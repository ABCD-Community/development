<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
</div>
<div class="page"><p/>
<p>Springer Texts in Business and Economics
</p>
<p>http://www.springer.com/series/10099
</p>
<p>For further volumes:</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Badi H. Baltagi 
</p>
<p>Econometrics 
</p>
<p>Fifth Edition 
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of 
translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilm or in any other way, and storage in data 
banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 
1965, in its current version, and permission for use must always be obtained from Springer. Violations are liable to prosecution under the 
German Copyright Law. 
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply, even in the absence of a specific 
statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. 
 
Cover design:
 
Printed on acid-free paper 
 
</p>
<p> 
</p>
<p>Prof. Badi H. Baltagi 
</p>
<p>Department of Economics 
</p>
<p>Center for Policy Research 
</p>
<p>Syracuse University 
</p>
<p>Eggers Hall 426 
</p>
<p>bbaltagi@maxwell.syr.edu 
</p>
<p>New York
</p>
<p>USA
</p>
<p>13244-1020 Syracuse 
</p>
<p> eStudio Calamar
</p>
<p> 
</p>
<p>ISBN 978-3-642-20058-8 e-ISBN 978-3-642-20059-5 
DOI 10.1007/978-3-642-20059-5 
Springer Heidelberg Dordrecht London New York 
</p>
<p> 
</p>
<p> 
Library of Congress Control Number: 2011926694
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg  1997, 1999, 2002, 2008, 2011 
</p>
<p>Springer is part of Springer Science+Business Media (www.springer.com)</p>
<p/>
</div>
<div class="page"><p/>
<p>To My Wife Phyllis</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>This book is intended for a first year graduate course in econometrics. I tried to strike a balance
between a rigorous approach that proves theorems, and a completely empirical approach where
no theorems are proved. Some of the strengths of this book lie in presenting some difficult
material in a simple, yet rigorous manner. For example, Chapter 12 on pooling time-series of
cross-section data is drawn from my area of expertise in econometrics and the intent here is to
make this material more accessible to the general readership of econometrics.
</p>
<p>This book teaches some of the basic econometric methods and the underlying assumptions
behind them. Estimation, hypotheses testing and prediction are three recurrent themes in
this book. Some uses of econometric methods include (i) empirical testing of economic the-
ory, whether it is the permanent income consumption theory or purchasing power parity, (ii)
forecasting, whether it is GNP or unemployment in the U.S. economy or future sales in the com-
puter industry. (iii) Estimation of price elasticities of demand, or returns to scale in production.
More importantly, econometric methods can be used to simulate the effect of policy changes
like a tax increase on gasoline consumption, or a ban on advertising on cigarette consumption.
It is left to the reader to choose among the available econometric/statistical software to use,
</p>
<p>like EViews, SAS, Stata, TSP, SHAZAM, Microfit, PcGive, LIMDEP, and RATS, to mention
a few. The empirical illustrations in the book utilize a variety of these software packages but
mostly with Stata and EViews. Of course, these packages have different advantages and disad-
vantages. However, for the basic coverage in this book, these differences may be minor and more
a matter of what software the reader is familiar or comfortable with. In most cases, I encourage
my students to use more than one of these packages and to verify these results using simple
programming languages like GAUSS, OX, R and MATLAB.
This book is not meant to be encyclopedic. I did not attempt the coverage of Bayesian
</p>
<p>econometrics simply because it is not my comparative advantage. The reader should consult
Koop (2003) for a more recent treatment of the subject. Nonparametrics and semiparametrics
are popular methods in today&rsquo;s econometrics, yet they are not covered in this book to keep
the technical difficulty at a low level. These are a must for a follow-up course in econometrics,
see Li and Racine (2007). Also, for a more rigorous treatment of asymptotic theory, see White
(1984). Despite these limitations, the topics covered in this book are basic and necessary in the
training of every economist. In fact, it is but a &lsquo;stepping stone&rsquo;, a &lsquo;sample of the good stuff&rsquo; the
reader will find in this young, energetic and ever evolving field.
I hope you will share my enthusiasm and optimism in the importance of the tools you will
</p>
<p>learn when you are through reading this book. Hopefully, it will encourage you to consult the
suggested readings on this subject that are referenced at the end of each chapter. In his inaugural
lecture at the University of Birmingham, entitled &ldquo;Econometrics: A View from the Toolroom,&rdquo;
Peter C.B. Phillips (1977) concluded:
</p>
<p>&ldquo;the toolroom may lack the glamour of economics as a practical art in government
or business, but it is every bit as important. For the tools (econometricians) fashion
provide the key to improvements in our quantitative information concerning matters
of economic policy.&rdquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>VIII Preface
</p>
<p>As a student of econometrics, I have benefited from reading Johnston (1984), Kmenta (1986),
Theil (1971), Klein (1974), Maddala (1977), and Judge, et al. (1985), to mention a few. As a
teacher of undergraduate econometrics, I have learned from Kelejian and Oates (1989), Wallace
and Silver (1988), Maddala (1992), Kennedy (1992), Wooldridge (2003) and Stock and Watson
(2003). As a teacher of graduate econometrics courses, Greene (1993), Judge, et al. (1985),
Fomby, Hill and Johnson (1984) and Davidson and MacKinnon (1993) have been my regular
companions. The influence of these books will be evident in the pages that follow. Courses
requiring matrix algebra as a pre-requisite to econometrics can start with Chapter 7. Chapter 2
has a quick refresher on some of the required background needed from statistics for the proper
understanding of the material in this book.
For an advanced undergraduate/masters class not requiring matrix algebra, one can structure
</p>
<p>a course based on Chapter 1; Section 2.6 on descriptive statistics; Chapters 3&ndash;6; Section 11.1
on simultaneous equations; and Chapter 14 on time-series analysis.
The exercises contain theoretical problems that should supplement the understanding of the
</p>
<p>material in each chapter. Some of these exercises are drawn from the Problems and Solutions
series of Econometric Theory (reprinted with permission of Cambridge University Press). In
addition, the book has a set of empirical illustrations demonstrating some of the basic results
learned in each chapter. Data sets from published articles are provided for the empirical exer-
cises. These exercises are solved using several econometric software packages and are available
in the Solution Manual. This book is by no means an applied econometrics text, and the reader
should consult Berndt&rsquo;s (1991) textbook for an excellent treatment of this subject. Instructors
and students are encouraged to get other data sets from the internet or journals that provide
backup data sets to published articles. The Journal of Applied Econometrics and the Jour-
nal of Business and Economic Statistics are two such journals. In fact, the Journal of Applied
Econometrics has a replication section for which I am serving as an editor. In my econometrics
course, I require my students to replicate an empirical paper. Many students find this experience
rewarding in terms of giving them hands on application of econometric methods that prepare
them for doing their own empirical work.
I would like to thank my teachers Lawrence R. Klein, Roberto S. Mariano and Robert Shiller
</p>
<p>who introduced me to this field; James M. Griffin who provided some data sets, empirical
exercises and helpful comments, and many colleagues who had direct and indirect influence
on the contents of this book including G.S. Maddala, Jan Kmenta, Peter Schmidt, Cheng
Hsiao, Tom Wansbeek, Walter Krämer, Maxwell King, Peter C. B. Phillips, Alberto Holly, Essie
Maasoumi, Aris Spanos, Farshid Vahid, Heather Anderson, Arnold Zellner and Bryan Brown.
Also, I would like to thank my students Wei-Wen Xiong, Ming-Jang Weng, Kiseok Nam, Dong
Li, Gustavo Sanchez, Long Liu and Liu Tian who read parts of this book and solved several of
the exercises. Martina Bihn at Springer for her continuous support and professional editorial
help. I have also benefited from my visits to the University of Arizona, University of California
San-Diego, Monash University, the University of Zurich, the Institute of Advanced Studies in
Vienna, and the University of Dortmund, Germany. A special thanks to my wife Phyllis whose
help and support were essential to completing this book.</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface IX
</p>
<p>References
</p>
<p>Berndt, E.R. (1991), The Practice of Econometrics: Classic and Contemporary (Addison-Wesley: Read-
ing, MA).
</p>
<p>Davidson, R. and J.G. MacKinnon (1993), Estimation and Inference In Econometrics (Oxford University
Press: Oxford, MA).
</p>
<p>Fomby, T.B., R.C. Hill and S.R. Johnson (1984), Advanced Econometric Methods (Springer-Verlag: New
York).
</p>
<p>Greene, W.H. (1993), Econometric Analysis (Macmillan: New York ).
</p>
<p>Johnston, J. (1984), Econometric Methods, 3rd. Ed., (McGraw-Hill: New York).
</p>
<p>Judge, G.G., W.E. Griffiths, R.C. Hill, H. Lütkepohl and T.C. Lee (1985), The Theory and Practice of
Econometrics 2nd Ed., (John Wiley: New York).
</p>
<p>Kelejian, H. and W. Oates (1989), Introduction to Econometrics: Principles and Applications 2nd Ed.,
(Harper and Row: New York).
</p>
<p>Kennedy, P. (1992), A Guide to Econometrics (The MIT Press: Cambridge, MA).
</p>
<p>Klein, L.R. (1974), A Textbook of Econometrics (Prentice-Hall: New Jersey).
</p>
<p>Kmenta, J. (1986), Elements of Econometrics 2nd Ed., (Macmillan: New York).
</p>
<p>Koop, G. (2003), Bayesian Econometrics (Wiley: New York).
</p>
<p>Li, Q. and J.S. Racine (2007), Nonparametric Econometrics, (Princeton University Press: New Jersey).
</p>
<p>Maddala, G.S. (1977), Econometrics (McGraw-Hill: New York).
</p>
<p>Maddala, G.S. (1992), Introduction to Econometrics (Macmillan: New York).
</p>
<p>Phillips, P.C.B. (1977), &ldquo;Econometrics: A View From the Toolroom,&rdquo; Inaugural Lecture, University of
Birmingham, Birmingham, England.
</p>
<p>Stock, J.H. and M.W. Watson (2003), Introduction to Econometrics (Addison-Wesley: New York).
</p>
<p>Theil, H. (1971), Principles of Econometrics (John Wiley: New York).
</p>
<p>Wallace, T.D. and L. Silver (1988), Econometrics: An Introduction (Addison-Wesley: New York).
</p>
<p>White, H. (1984), Asymptotic Theory for Econometrics (Academic Press: Florida).
</p>
<p>Wooldridge, J.M. (2003), Introductory Econometrics (South-Western: Ohio).
</p>
<p>Data
</p>
<p>The data sets used in this text can be downloaded from the Springer website in Germany.
The address is: http://www.springer.com/978-3-642-20058-8. Please select the link &ldquo;Samples &amp;
Supplements&rdquo; from the right-hand column.</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Table of Contents
</p>
<p>Preface VII
</p>
<p>Part I 1
</p>
<p>1 What Is Econometrics? 3
</p>
<p>1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
</p>
<p>1.2 A Brief History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
</p>
<p>1.3 Critiques of Econometrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
</p>
<p>1.4 Looking Ahead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
</p>
<p>Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
</p>
<p>2 Basic Statistical Concepts 13
</p>
<p>2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>2.2 Methods of Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>2.3 Properties of Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
</p>
<p>2.4 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
</p>
<p>2.5 Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
</p>
<p>2.6 Descriptive Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
</p>
<p>Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
</p>
<p>Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
</p>
<p>3 Simple Linear Regression 49
</p>
<p>3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
</p>
<p>3.2 Least Squares Estimation and the Classical Assumptions . . . . . . . . . . . . . 50
</p>
<p>3.3 Statistical Properties of Least Squares . . . . . . . . . . . . . . . . . . . . . . . . 55
</p>
<p>3.4 Estimation of σ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
</p>
<p>3.5 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
</p>
<p>3.6 A Measure of Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
</p>
<p>3.7 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>3.8 Residual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>3.9 Numerical Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
</p>
<p>3.10 Empirical Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
</p>
<p>Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
</p>
<p>4 Multiple Regression Analysis 73
</p>
<p>4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73</p>
<p/>
</div>
<div class="page"><p/>
<p>XII Table of Contents
</p>
<p>4.2 Least Squares Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
</p>
<p>4.3 Residual Interpretation of Multiple Regression Estimates . . . . . . . . . . . . . 75
</p>
<p>4.4 Overspecification and Underspecification of the Regression Equation . . . . . . . 76
</p>
<p>4.5 R-Squared Versus R-Bar-Squared . . . . . . . . . . . . . . . . . . . . . . . . . . 78
</p>
<p>4.6 Testing Linear Restrictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
</p>
<p>4.7 Dummy Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
</p>
<p>Note . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
</p>
<p>Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
</p>
<p>5 Violations of the Classical Assumptions 95
</p>
<p>5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
</p>
<p>5.2 The Zero Mean Assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
</p>
<p>5.3 Stochastic Explanatory Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 96
</p>
<p>5.4 Normality of the Disturbances . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
</p>
<p>5.5 Heteroskedasticity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
</p>
<p>5.6 Autocorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
</p>
<p>6 Distributed Lags and Dynamic Models 131
</p>
<p>6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>6.2 Infinite Distributed Lag . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
</p>
<p>6.2.1 Adaptive Expectations Model (AEM) . . . . . . . . . . . . . . . . . . . . 138
</p>
<p>6.2.2 Partial Adjustment Model (PAM) . . . . . . . . . . . . . . . . . . . . . . 138
</p>
<p>6.3 Estimation and Testing of Dynamic Models with Serial Correlation . . . . . . . 139
</p>
<p>6.3.1 A Lagged Dependent Variable Model with AR(1) Disturbances . . . . . 140
</p>
<p>6.3.2 A Lagged Dependent Variable Model with MA(1) Disturbances . . . . . 142
</p>
<p>6.4 Autoregressive Distributed Lag . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
</p>
<p>Note . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
</p>
<p>Part II 149
</p>
<p>7 The General Linear Model: The Basics 151
</p>
<p>7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>7.2 Least Squares Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>7.3 Partitioned Regression and the Frisch-Waugh-Lovell Theorem . . . . . . . . . . 154
</p>
<p>7.4 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
</p>
<p>7.5 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
</p>
<p>7.6 Confidence Intervals and Test of Hypotheses . . . . . . . . . . . . . . . . . . . . 160
</p>
<p>7.7 Joint Confidence Intervals and Test of Hypotheses . . . . . . . . . . . . . . . . . 160</p>
<p/>
</div>
<div class="page"><p/>
<p>Table of Contents XIII
</p>
<p>7.8 Restricted MLE and Restricted Least Squares . . . . . . . . . . . . . . . . . . . 161
</p>
<p>7.9 Likelihood Ratio, Wald and Lagrange Multiplier Tests . . . . . . . . . . . . . . . 162
</p>
<p>Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
</p>
<p>Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
</p>
<p>8 Regression Diagnostics and Specification Tests 179
</p>
<p>8.1 Influential Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>8.2 Recursive Residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
</p>
<p>8.3 Specification Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
</p>
<p>8.4 Nonlinear Least Squares and the Gauss-Newton Regression . . . . . . . . . . . . 206
</p>
<p>8.5 Testing Linear Versus Log-Linear Functional Form . . . . . . . . . . . . . . . . . 213
</p>
<p>Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
</p>
<p>9 Generalized Least Squares 223
</p>
<p>9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
</p>
<p>9.2 Generalized Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
</p>
<p>9.3 Special Forms of Ω . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
</p>
<p>9.4 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
</p>
<p>9.5 Test of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
</p>
<p>9.6 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
</p>
<p>9.7 Unknown Ω . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
</p>
<p>9.8 The W, LR and LM Statistics Revisited . . . . . . . . . . . . . . . . . . . . . . 228
</p>
<p>9.9 Spatial Error Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
</p>
<p>Note . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>10 Seemingly Unrelated Regressions 241
</p>
<p>10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
</p>
<p>10.2 Feasible GLS Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
</p>
<p>10.3 Testing Diagonality of the Variance-Covariance Matrix . . . . . . . . . . . . . . 246
</p>
<p>10.4 Seemingly Unrelated Regressions with Unequal Observations . . . . . . . . . . . 246
</p>
<p>10.5 Empirical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
</p>
<p>11 Simultaneous Equations Model 257
</p>
<p>11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
</p>
<p>11.1.1 Simultaneous Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
</p>
<p>11.1.2 The Identification Problem . . . . . . . . . . . . . . . . . . . . . . . . . . 260
</p>
<p>11.2 Single Equation Estimation: Two-Stage Least Squares . . . . . . . . . . . . . . . 263
</p>
<p>11.2.1 Spatial Lag Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . 271</p>
<p/>
</div>
<div class="page"><p/>
<p>XIV Table of Contents
</p>
<p>11.3 System Estimation: Three-Stage Least Squares . . . . . . . . . . . . . . . . . . . 272
11.4 Test for Over-Identification Restrictions . . . . . . . . . . . . . . . . . . . . . . . 273
11.5 Hausman&rsquo;s Specification Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
11.6 Empirical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
</p>
<p>12 Pooling Time-Series of Cross-Section Data 305
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
12.2 The Error Components Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
</p>
<p>12.2.1 The Fixed Effects Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
12.2.2 The Random Effects Model . . . . . . . . . . . . . . . . . . . . . . . . . 308
12.2.3 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . 312
</p>
<p>12.3 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
12.4 Empirical Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
12.5 Testing in a Pooled Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
12.6 Dynamic Panel Data Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
</p>
<p>12.6.1 Empirical Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
12.7 Program Evaluation and Difference-in-Differences Estimator . . . . . . . . . . . 326
</p>
<p>12.7.1 The Difference-in-Differences Estimator . . . . . . . . . . . . . . . . . . . 327
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
</p>
<p>13 Limited Dependent Variables 333
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
13.2 The Linear Probability Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
13.3 Functional Form: Logit and Probit . . . . . . . . . . . . . . . . . . . . . . . . . . 334
13.4 Grouped Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
13.5 Individual Data: Probit and Logit . . . . . . . . . . . . . . . . . . . . . . . . . . 341
13.6 The Binary Response Model Regression . . . . . . . . . . . . . . . . . . . . . . . 342
13.7 Asymptotic Variances for Predictions and Marginal Effects . . . . . . . . . . . . 344
13.8 Goodness of Fit Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
13.9 Empirical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
13.10 Multinomial Choice Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
</p>
<p>13.10.1 Ordered Response Models . . . . . . . . . . . . . . . . . . . . . . . . . . 350
13.10.2 Unordered Response Models . . . . . . . . . . . . . . . . . . . . . . . . . 354
</p>
<p>13.11 The Censored Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
13.12 The Truncated Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . . 359
13.13 Sample Selectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370</p>
<p/>
</div>
<div class="page"><p/>
<p>Table of Contents XV
</p>
<p>14 Time-Series Analysis 373
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
14.2 Stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
14.3 The Box and Jenkins Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
14.4 Vector Autoregression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
14.5 Unit Roots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
14.6 Trend Stationary Versus Difference Stationary . . . . . . . . . . . . . . . . . . . 383
14.7 Cointegration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
14.8 Autoregressive Conditional Heteroskedasticity . . . . . . . . . . . . . . . . . . . 387
Note . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
</p>
<p>Appendix 397
</p>
<p>List of Figures 403
</p>
<p>List of Tables 405
</p>
<p>Index 407</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Part I</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 1
</p>
<p>What Is Econometrics?
</p>
<p>1.1 Introduction
</p>
<p>What is econometrics? A few definitions are given below:
</p>
<p>The method of econometric research aims, essentially, at a conjunction of economic
theory and actual measurements, using the theory and technique of statistical infer-
ence as a bridge pier.
</p>
<p>Trygve Haavelmo (1944)
</p>
<p>Econometrics may be defined as the quantitative analysis of actual economic phe-
nomena based on the concurrent development of theory and observation, related by
appropriate methods of inference.
</p>
<p>Samuelson, Koopmans and Stone (1954)
</p>
<p>Econometrics is concerned with the systematic study of economic phenomena using
observed data.
</p>
<p>Aris Spanos (1986)
</p>
<p>Broadly speaking, econometrics aims to give empirical content to economic relations
for testing economic theories, forecasting, decision making, and for ex post deci-
sion/policy evaluation.
</p>
<p>J. Geweke, J. Horowitz, and M.H. Pesaran (2008)
</p>
<p>For other definitions of econometrics, see Tintner (1953).
An econometrician has to be a competent mathematician and statistician who is an economist
</p>
<p>by training. Fundamental knowledge of mathematics, statistics and economic theory are a nec-
essary prerequisite for this field. As Ragnar Frisch (1933) explains in the first issue of Econo-
metrica, it is the unification of statistics, economic theory and mathematics that constitutes
econometrics. Each view point, by itself is necessary but not sufficient for a real understanding
of quantitative relations in modern economic life.
Ragnar Frisch is credited with coining the term &lsquo;econometrics&rsquo; and he is one of the founders
</p>
<p>of the Econometrics Society, see Christ (1983). Econometrics aims at giving empirical content
to economic relationships. The three key ingredients are economic theory, economic data, and
statistical methods. Neither &lsquo;theory without measurement&rsquo;, nor &lsquo;measurement without theory&rsquo;
are sufficient for explaining economic phenomena. It is as Frisch emphasized their union that is
the key for success in the future development of econometrics.
Lawrence R. Klein, the 1980 recipient of the Nobel Prize in economics &ldquo;for the creation of
</p>
<p>econometric models and their application to the analysis of economic fluctuations and economic
policies,&rdquo;1 has always emphasized the integration of economic theory, statistical methods and
practical economics. The exciting thing about econometrics is its concern for verifying or refuting
economic laws, such as purchasing power parity, the life cycle hypothesis, the quantity theory of
money, etc. These economic laws or hypotheses are testable with economic data. In fact, David
F. Hendry (1980) emphasized this function of econometrics:
</p>
<p>3 B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_1, 
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 </p>
<p/>
</div>
<div class="page"><p/>
<p>4 Chapter 1: What Is Econometrics?
</p>
<p>The three golden rules of econometrics are test, test and test; that all three rules are
broken regularly in empirical applications is fortunately easily remedied. Rigorously
tested models, which adequately described the available data, encompassed previous
findings and were derived from well based theories would enhance any claim to be
scientific.
</p>
<p>Econometrics also provides quantitative estimates of price and income elasticities of demand,
returns to scale in production, technical efficiency in cost functions, wage elasticities, etc. These
are important for policy decision making. What is the effect of raising the tax on a pack of
cigarettes by 10% in reducing smoking? How much will it generate in tax revenues? What is the
effect of raising minimum wage by $1 per hour on unemployment? What is the effect of raising
beer tax on motor vehicle fatality?
Econometrics also provides predictions about future interest rates, unemployment, or GNP
</p>
<p>growth. Lawrence Klein (1971) emphasized this last function of econometrics:
</p>
<p>Econometrics had its origin in the recognition of empirical regularities and the sys-
tematic attempt to generalize these regularities into &ldquo;laws&rdquo; of economics. In a broad
sense, the use of such &ldquo;laws&rdquo; is to make predictions &ndash; about what might have or what
will come to pass. Econometrics should give a base for economic prediction beyond
experience if it is to be useful. In this broad sense it may be called the science of
economic prediction.
</p>
<p>Econometrics, while based on scientific principles, still retains a certain element of art. According
to Malinvaud (1966), the art in econometrics is trying to find the right set of assumptions
which are sufficiently specific, yet realistic to enable us to take the best possible advantage of
the available data. Data in economics are not generated under ideal experimental conditions
as in a physics laboratory. This data cannot be replicated and is most likely measured with
error. In some cases, the available data are proxies for variables that are either not observed or
cannot be measured. Many published empirical studies find that economic data may not have
enough variation to discriminate between two competing economic theories. Manski (1995, p. 8)
argues that
</p>
<p>Social scientists and policymakers alike seem driven to draw sharp conclusions, even
when these can be generated only by imposing much stronger assumptions than can
be defended. We need to develop a greater tolerance for ambiguity. We must face up
to the fact that we cannot answer all of the questions that we ask.
</p>
<p>To some, the &ldquo;art&rdquo; element in econometrics has left a number of distinguished economists doubt-
ful of the power of econometrics to yield sharp predictions. In his presidential address to the
American Economic Association, Wassily Leontief (1971, pp. 2&ndash;3) characterized econometrics
work as:
</p>
<p>an attempt to compensate for the glaring weakness of the data base available to us
by the widest possible use of more and more sophisticated techniques. Alongside the
mounting pile of elaborate theoretical models we see a fast growing stock of equally
intricate statistical tools. These are intended to stretch to the limit the meager supply
of facts.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 A Brief History 5
</p>
<p>Economic data can be of the cross-section type, for e.g., a sample of firms or households or
countries at a particular point in time. An important data source is the Current Population
Survey. This is a monthly survey of 50,000 households in the U.S. which is used to estimate the
unemployment rate. Data can also be of the time-series type, for e.g., macroeconomic variables
like Gross Domestic Product (GDP), Personal Disposable Income, Consumption, Government
Expenditures, etc. for the U.S. observed over the last 40 years. These can be found in the
Economic Report of the President. See Chapter 14 for some basic time-series methods in econo-
metrics. Data can also be following a group of households, firms, or countries over time, i.e.,
Longitudinal data or panel data. The National Longitudinal Survey of Youth, 1979 consists of a
nationally representative sample of 12686 young men and women who were 14&ndash;22 years old in
1979. These individuals were interviewed annually through 1994 and currently interviewed on
a biennial basis. The list of variables include information on schooling and career transitions,
marriage and fertility, training investments, child care usage and drug and alcohol use. See
Chapter 12 for some basic panel data methods in econometrics.
</p>
<p>Most of the time the data collected are not ideal for the economic question at hand because
they were posed to answer legal requirements or comply to regulatory agencies. Griliches (1986,
p. 1466) describes the situation as follows:
</p>
<p>Econometricians have an ambivilant attitude towards economic data. At one level,
the &lsquo;data&rsquo; are the world that we want to explain, the basic facts that economists
purport to elucidate. At the other level, they are the source of all our trouble. Their
imperfections make our job difficult and often impossible... We tend to forget that
these imperfections are what gives us our legitimacy in the first place... Given that
it is the &lsquo;badness&rsquo; of the data that provides us with our living, perhaps it is not all
that surprising that we have shown little interest in improving it, in getting involved
in the grubby task of designing and collecting original data sets of our own. Most of
our work is on &lsquo;found&rsquo; data, data that have been collected by somebody else, often
for quite different purposes.
</p>
<p>Even though economists are increasingly getting involved in collecting their data and measuring
variables more accurately and despite the increase in data sets and data storage and computa-
tional accuracy, some of the warnings given by Griliches (1986, p. 1468) are still valid today:
</p>
<p>The encounters between econometricians and data are frustrating and ultimately
unsatisfactory both because econometricians want too much from the data and hence
tend to be dissappointed by the answers, and because the data are incomplete and
imperfect. In part it is our fault, the appetite grows with eating. As we get larger
samples, we keep adding variables and expanding our models, until on the margin,
we come back to the same insignificance levels.
</p>
<p>1.2 A Brief History
</p>
<p>For a brief review of the origins of econometrics before World War II and its development in the
1940&ndash;1970 period, see Klein (1971). Klein gives an interesting account of the pioneering works
of Moore (1914) on economic cycles, Working (1927) on demand curves, Cobb and Douglas
(1928) on the theory of production, Schultz (1938) on the theory and measurement of demand,
and Tinbergen (1939) on business cycles. As Klein (1971, p. 415) adds:</p>
<p/>
</div>
<div class="page"><p/>
<p>6 Chapter 1: What Is Econometrics?
</p>
<p>The works of these men mark the beginnings of formal econometrics. Their analysis
was systematic, based on the joint foundations of statistical and economic theory,
and they were aiming at meaningful substantive goals - to measure demand elasticity,
marginal productivity and the degree of macroeconomic stability.
</p>
<p>The story of the early progress in estimating economic relationships in the U.S. is given in Christ
(1985). The modern era of econometrics, as we know it today, started in the 1940&rsquo;s. Klein (1971)
attributes the formulation of the econometrics problem in terms of the theory of statistical
inference to Haavelmo (1943, 1944) and Mann andWald (1943). This work was extended later by
T.C. Koopmans, J. Marschak, L. Hurwicz, T.W. Anderson and others at the Cowles Commission
in the late 1940&rsquo;s and early 1950&rsquo;s, see Koopmans (1950). Klein (1971, p. 416) adds:
</p>
<p>At this time econometrics and mathematical economics had to fight for academic
recognition. In retrospect, it is evident that they were growing disciplines and becom-
ing increasingly attractive to the new generation of economic students after World
War II, but only a few of the largest and most advanced universities offered formal
work in these subjects. The mathematization of economics was strongly resisted.
</p>
<p>This resistance is a thing of the past, with econometrics being an integral part of economics,
taught and practiced worldwide. Econometrica, the official journal of the Econometric Society
is one of the leading journals in economics, and today the Econometric Society boast a large
membership worldwide. Today, it is hard to read any professional article in leading economics
and econometrics journals without seeing mathematical equations. Students of economics and
econometrics have to be proficient in mathematics to comprehend this research. In an Econo-
metric Theory interview, professor J. D. Sargan of the London School of Economics looks back
at his own career in econometrics and makes the following observations: &ldquo;... econometric theo-
rists have really got to be much more professional statistical theorists than they had to be when
I started out in econometrics in 1948... Of course this means that the starting econometrician
hoping to do a Ph.D. in this field is also finding it more difficult to digest the literature as a
prerequisite for his own study, and perhaps we need to attract students of an increasing de-
gree of mathematical and statistical sophistication into our field as time goes by,&rdquo; see Phillips
(1985, pp. 134&ndash;135). This is also echoed by another giant in the field, professor T.W. Anderson
of Stanford, who said in an Econometric Theory interview: &ldquo;These days econometricians are
very highly trained in mathematics and statistics; much more so than statisticians are trained
in economics; and I think that there will be more cross-fertilization, more joint activity,&rdquo; see
Phillips (1986, p. 280).
Research at the Cowles Commission was responsible for providing formal solutions to the
</p>
<p>problems of identification and estimation of the simultaneous equations model, see Christ
(1985).2 Two important monographs summarizing much of the work of the Cowles Commis-
sion at Chicago, are Koopmans and Marschak (1950) and Koopmans and Hood (1953).3 The
creation of large data banks of economic statistics, advances in computing, and the general
acceptance of Keynesian theory, were responsible for a great flurry of activity in econometrics.
Macroeconometric modelling started to flourish beyond the pioneering macro models of Klein
(1950) and Klein and Goldberger (1955).
For the story of the founding of Econometrica and the Econometric Society, see Christ (1983).
</p>
<p>Suggested readings on the history of econometrics are Pesaran (1987), Epstein (1987) and</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Critiques of Econometrics 7
</p>
<p>Morgan (1990). In the conclusion of her book on The History of Econometric Ideas, Morgan
(1990; p. 264) explains:
</p>
<p>In the first half of the twentieth century, econometricians found themselves carrying
out a wide range of tasks: from the precise mathematical formulation of economic
theories to the development tasks needed to build an econometric model; from the ap-
plication of statistical methods in data preperation to the measurement and testing
of models. Of necessity, econometricians were deeply involved in the creative devel-
opment of both mathematical economic theory and statistical theory and techniques.
Between the 1920s and the 1940s, the tools of mathematics and statistics were in-
deed used in a productive and complementary union to forge the essential ideas of the
econometric approach. But the changing nature of the econometric enterprise in the
1940s caused a return to the division of labour favoured in the late nineteenth cen-
tury, with mathematical economists working on theory building and econometricians
concerned with statistical work. By the 1950s the founding ideal of econometrics, the
union of mathematical and statistical economics into a truly synthetic economics,
had collapsed.
</p>
<p>In modern day usage, econometrics have become the application of statistical methods to eco-
nomics, like biometrics and psychometrics. Although, the ideals of Frisch still live on in Econo-
metrica and the Econometric Society, Maddala (1999) argues that: &ldquo;In recent years the issues
of Econometrica have had only a couple of papers in econometrics (statistical methods in eco-
nomics) and the rest are all on game theory and mathematical economics. If you look at the
list of fellows of the Econometric Society, you find one or two econometricians and the rest
are game theorists and mathematical economists.&rdquo; This may be a little exagerated but it does
summarize the rift between modern day econometrics and mathematical economics. For a world
wide ranking of econometricians as well as academic institutions in the field of econometrics,
see Baltagi (2007).
</p>
<p>1.3 Critiques of Econometrics
</p>
<p>Econometrics has its critics. Interestingly, John Maynard Keynes (1940, p. 156) had the following
to say about Jan Tinbergen&rsquo;s (1939) pioneering work:
</p>
<p>No one could be more frank, more painstaking, more free of subjective bias or parti
pris than Professor Tinbergen. There is no one, therefore, so far as human qualities
go, whom it would be safer to trust with black magic. That there is anyone I would
trust with it at the present stage or that this brand of statistical alchemy is ripe to
become a branch of science, I am not yet persuaded. But Newton, Boyle and Locke
all played with alchemy. So let him continue.4
</p>
<p>In 1969, Jan Tinbergen shared the first Nobel Prize in economics with Ragnar Frisch.
Well cited critiques of econometrics include the Lucas (1976) critique which is based on the
</p>
<p>Rational Expectations Hypothesis (REH). As Pesaran (1990, p. 17) puts it:
</p>
<p>The message of the REH for econometrics was clear. By postulating that economic
agents form their expectations endogenously on the basis of the true model of the</p>
<p/>
</div>
<div class="page"><p/>
<p>8 Chapter 1: What Is Econometrics?
</p>
<p>economy and a correct understanding of the processes generating exogenous variables
of the model, including government policy, the REH raised serious doubts about the
invariance of the structural parameters of the mainstream macroeconometric models
in face of changes in government policy.
</p>
<p>Responses to this critique include Pesaran (1987). Other lively debates among econometricians
include Ed Leamer&rsquo;s (1983) article entitled &ldquo;Let&rsquo;s Take the Con Out of Econometrics,&rdquo; and the
response by McAleer, Pagan and Volker (1985). Rather than leave the reader with criticisms
of econometrics especially before we embark on the journey to learn the tools of the trade, we
conclude this section with the following quote from Pesaran (1990, pp. 25&ndash;26):
</p>
<p>There is no doubt that econometrics is subject to important limitations, which stem
largely from the incompleteness of the economic theory and the non-experimental
nature of economic data. But these limitations should not distract us from recog-
nizing the fundamental role that econometrics has come to play in the development
of economics as a scientific discipline. It may not be possible conclusively to re-
ject economic theories by means of econometric methods, but it does not mean that
nothing useful can be learned from attempts at testing particular formulations of a
given theory against (possible) rival alternatives. Similarly, the fact that economet-
ric modelling is inevitably subject to the problem of specification searches does not
mean that the whole activity is pointless. Econometric models are important tools for
forecasting and policy analysis, and it is unlikely that they will be discarded in the
future. The challenge is to recognize their limitations and to work towards turning
them into more reliable and effective tools. There seem to be no viable alternatives.
</p>
<p>1.4 Looking Ahead
</p>
<p>Econometrics have experienced phenomenal growth in the past 50 years. There are six volumes
of the Handbook of Econometrics, most of it dealing with post 1960&rsquo;s research. A lot of the
recent growth reflects the rapid advances in computing technology. The broad availability of
micro data bases is a major advance which facilitated the growth of panel data methods (see
Chapter 12) and microeconometric methods especially on sample selection and discrete choice
(see Chapter 13) and that also lead to the award of the Nobel Prize in Economics to James
Heckman and Daniel McFadden in 2000. The explosion in research in time series econometrics
which lead to the development of ARCH and GARCH and cointegration (see Chapter 14)
which also lead to the award of the Nobel Prize in Economics to Clive Granger and Robert
Engle in 2003. It is a different world than it was 30 years ago. The computing facilities changed
dramatically. The increasing accessibility of cheap and powerful computing facilities are helping
to make the latest econometric methods more readily available to applied researchers. Today,
there is hardly a field in economics which has not been intensive in its use of econometrics in
empirical work. Pagan (1987, p. 81) observed that the work of econometric theorists over the
period 1966&ndash;1986 have become part of the process of economic investigation and the training
of economists. Based on this criterion, he declares econometrics as an &ldquo;outstanding success.&rdquo;
He adds that:
</p>
<p>The judging of achievement inevitably involves contrast and comparison. Over a
period of twenty years this would be best done by interviewing a time-travelling</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Looking Ahead 9
</p>
<p>economist displaced from 1966 to 1986. I came into econometrics just after the be-
ginning of this period, so have some appreciation for what has occurred. But because
I have seen the events gradually unfolding, the effects upon me are not as dramatic.
Nevertheless, let me try to be a time-traveller and comment on the perceptions of a
1966&rsquo;er landing in 1986. My first impression must be of the large number of people
who have enough econometric and computer skills to formulate, estimate and sim-
ulate highly complex and non-linear models. Someone who could do the equivalent
tasks in 1966 was well on the way to a Chair. My next impression would be of the
widespread use and purchase of econometric services in the academic, government,
and private sectors. Quantification is now the norm rather than the exception. A
third impression, gleaned from a sounding of the job market, would be a persistent
tendency towards an excess demand for well-trained econometricians. The economist
in me would have to acknowledge that the market judges the products of the discipline
as a success.
</p>
<p>The challenge for the 21st century is to narrow the gap between theory and practice. Many
feel that this gap has been widening with theoretical research growing more and more abstract
and highly mathematical without an application in sight or a motivation for practical use.
Heckman (2001) argues that econometrics is useful only if it helps economists conduct and
interpret empirical research on economic data. He warns that the gap between econometric
theory and empirical practice has grown over the past two decades. Theoretical econometrics
becoming more closely tied to mathematical statistics. Although he finds nothing wrong, and
much potential value, in using methods and ideas from other fields to improve empirical work
in economics, he does warn of the risks involved in uncritically adopting the methods and mind
set of the statisticians:
</p>
<p>Econometric methods uncritically adapted from statistics are not useful in many re-
search activities pursued by economists. A theorem-proof format is poorly suited for
analyzing economic data, which requires skills of synthesis, interpretation and em-
pirical investigation. Command of statistical methods is only a part, and sometimes
a very small part, of what is required to do first class empirical research.
</p>
<p>In an Econometric Theory interview with Jan Tinbergen, Magnus and Morgan (1987, p. 117)
describe Tinbergen as one of the founding fathers of econometrics, publishing in the field from
1927 until the early 1950s. They add: &ldquo;Tinbergen&rsquo;s approach to economics has always been a
practical one. This was highly appropriate for the new field of econometrics, and enabled him
to make important contributions to conceptual and theoretical issues, but always in the context
of a relevant economic problem.&rdquo; The founding fathers of econometrics have always had the
practitioner in sight. This is a far cry from many theoretical econometricians who refrain from
applied work.
</p>
<p>Geweke, Horowitz, and Pesaran (2008) provide the following recommendations for the future:
</p>
<p>Econometric theory and practice seek to provide information required for informed
decision-making in public and private economic policy. This process is limited not
only by the adequacy of econometrics, but also by the development of economic theory
and the adequacy of data and other information. Effective progress, in the future as
in the past, will come from simultaneous improvements in econometrics, economic</p>
<p/>
</div>
<div class="page"><p/>
<p>10 Chapter 1: What Is Econometrics?
</p>
<p>theory, and data. Research that specifically addresses the effectiveness of the interface
between any two of these three in improving policy &mdash; to say nothing of all of them
&mdash; necessarily transcends traditional subdisciplinary boundaries within economics.
But it is precisely these combinations that hold the greatest promise for the social
contribution of academic economics.
</p>
<p>Notes
</p>
<p>1. See the interview of Professor L.R. Klein by Mariano (1987). Econometric Theory publishes inter-
views with some of the giants in the field. These interviews offer a wonderful glimpse at the life
and work of these giants.
</p>
<p>2. Simultaneous equations model is an integral part of econometrics and is studied in Chapter 11.
</p>
<p>3. Tjalling Koopmans was the joint recipient of the Nobel Prize in Economics in 1975. In addition
to his work on the identification and estimation of simultaneous equations models, he received the
Nobel Prize for his work in optimization and economic theory.
</p>
<p>4. I encountered this attack by Keynes on Tinbergen in the inaugural lecture that Peter C.B. Phillips
</p>
<p>(1977) gave at the University of Birmingham entitled &ldquo;Econometrics: A View From the Toolroom,&rdquo;
</p>
<p>and David F. Hendry&rsquo;s (1980) article entitled &ldquo;Econometrics - Alchemy or Science?&rdquo;
</p>
<p>References
</p>
<p>Baltagi, B.H. (2007), &ldquo;Worldwide Econometrics Rankings: 1989&ndash;2005,&rdquo; Econometric Theory, 23: 952&ndash;
1012.
</p>
<p>Christ, C.F. (1983), &ldquo;The Founding of the Econometric Society and Econometrica,&rdquo; Econometrica, 51:
3&ndash;6.
</p>
<p>Christ, C.F. (1985), &ldquo;Early Progress in Estimating Quantiative Economic Relations in America,&rdquo; Amer-
ican Economic Review, 12: 39&ndash;52.
</p>
<p>Cobb, C.W. and P.H. Douglas (1928), &ldquo;A Theory of Production,&rdquo; American Economic Review, Supple-
ment 18: 139&ndash;165.
</p>
<p>Epstein, R.J. (1987), A History of Econometrics (North-Holland: Amsterdam).
</p>
<p>Frisch, R. (1933), &ldquo;Editorial,&rdquo; Econometrica, 1: 1&ndash;14.
</p>
<p>Geweke, J., J. Horowitz, and M.H. Pesaran (2008), &ldquo;Econometrics,&rdquo; entry in S. Durlauf and L. Blume,
eds., The New Palgrave Dictionary of Economics, 2nd Edition, (Palgrave Macmillan: Basingstoke).
</p>
<p>Griliches, Z. (1986), &ldquo;Economic Data Issues,&rdquo; in Z. Griliches and M.D. Intriligator (eds), Handbook of
Econometrics Vol. III (North Holland: Amsterdam).
</p>
<p>Haavelmo, T. (1943), &ldquo;The Statistical Implications of a System of Simultaneous Equations,&rdquo; Economet-
rica, 11: 1&ndash;12.
</p>
<p>Haavelmo, T. (1944), &ldquo;The Probability Approach in Econometrics,&rdquo; Econometrica, Supplement to Vol-
ume 12: 1&ndash;118.
</p>
<p>Heckman, J.J. (2001), &ldquo;Econometrics and Empirical Economics,&rdquo; Journal of Econometrics, 100: 3&ndash;5.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 11
</p>
<p>Hendry, D.F. (1980), &ldquo;Econometrics - Alchemy or Science?&rdquo; Economica, 47: 387&ndash;406.
</p>
<p>Keynes, J.M. (1940), &ldquo;On Method of Statistical Research: Comment,&rdquo; Economic Journal, 50: 154&ndash;156.
</p>
<p>Klein, L.R. (1971), &ldquo;Whither Econometrics?&rdquo; Journal of the American Statistical Association, 66: 415&ndash;
421.
</p>
<p>Klein, L.R. (1950), Economic Fluctuations in the United States 1921&ndash;1941, Cowles Commission Mono-
graph, No. 11 (John Wiley: New York).
</p>
<p>Klein, L.R. and A.S. Goldberger (1955), An Econometric Model of the United States 1929&ndash;1952 (North-
Holland: Amsterdam).
</p>
<p>Koopmans, T.C. (1950), ed., Statistical Inference in Dynamic Economic Models (John Wiley: New York).
</p>
<p>Koopmans, T.C. and W.C. Hood (1953), Studies in Econometric Method (John Wiley: New York).
</p>
<p>Koopmans, T.C. and J. Marschak (1950), eds., Statistical Inference in Dynamic Economic Models (John
Wiley: New York).
</p>
<p>Leamer, E.E. (1983), &ldquo;Lets Take the Con Out of Econometrics,&rdquo; American Economic Review, 73: 31&ndash;43.
</p>
<p>Leontief, W. (1971), &ldquo;Theoretical Assumptions and Nonobserved Facts,&rdquo; American Economic Review,
61: 1&ndash;7.
</p>
<p>Lucas, R.E. (1976), &ldquo;Econometric Policy Evaluation: A Critique,&rdquo; in K. Brunner and A.M. Meltzer,
eds., The Phillips Curve and Labor Markets, Carnegie Rochester Conferences on Public Policy, 1:
19&ndash;46.
</p>
<p>Maddala, G.S. (1999), &ldquo;Econometrics in the 21st Century,&rdquo; in C.R. Rao and R. Szekeley, eds., Statistics
for the 21 stCentury (Marcel Dekker: New York).
</p>
<p>Magnus , J.R. and M.S. Morgan (1987), &ldquo;The ET Interview: Professor J. Tinbergen,&rdquo; Econometric
Theory, 3: 117&ndash;142.
</p>
<p>Malinvaud, E. (1966), Statistical Methods of Econometrics (North-Holland: Amsterdam).
</p>
<p>Manski, C.F. (1995), Identification Problems in the Social Sciences (Harvard University Press: Cam-
bridge).
</p>
<p>Mann, H.B. and A. Wald (1943), &ldquo;On the Statistical Treatment of Linear Stochastic Difference Equa-
tions,&rdquo; Econometrica, 11: 173&ndash;220.
</p>
<p>Mariano, R.S. (1987), &ldquo;The ET Interview: Professor L.R. Klein,&rdquo; Econometric Theory, 3: 409&ndash;460.
</p>
<p>McAleer, M., A.R. Pagan and P.A. Volker (1985), &ldquo;What Will Take The Con Out of Econometrics,&rdquo;
American Economic Review, 75: 293&ndash;307.
</p>
<p>Moore, H.L. (1914), Economic Cycles: Their Law and Cause (Macmillan: New York).
</p>
<p>Morgan, M. (1990), The History of Econometric Ideas (Cambridge University Press: Cambridge, MA).
</p>
<p>Pagan, A. (1987), &ldquo;Twenty Years After: Econometrics, 1966&ndash;1986,&rdquo; paper presented at CORE&rsquo;s 20th
Anniversary Conference, Louvain-la-Neuve.
</p>
<p>Pesaran, M.H. (1987), The Limits to Rational Expectations (Basil Blackwell: Oxford, MA).
</p>
<p>Pesaran, M.H. (1990), &ldquo;Econometrics,&rdquo; in J. Eatwell, M. Milgate and P. Newman; The New Palgrave:
Econometrics (W.W. Norton and Company: New York).
</p>
<p>Phillips, P.C.B. (1977), &ldquo;Econometrics: A View From the Toolroom,&rdquo; Inaugural Lecture, University of
Birmingham, Birmingham, England.</p>
<p/>
</div>
<div class="page"><p/>
<p>12 Chapter 1: What Is Econometrics?
</p>
<p>Phillips, P.C.B. (1985), &ldquo;ET Interviews: Professor J. D. Sargan,&rdquo; Econometric Theory, 1: 119&ndash;139.
</p>
<p>Phillips, P.C.B. (1986), &ldquo;The ET Interview: Professor T. W. Anderson,&rdquo; Econometric Theory, 2: 249&ndash;288.
</p>
<p>Samuelson, P.A., T.C. Koopmans and J.R.N. Stone (1954), &ldquo;Report of the Evaluative Committee for
Econometrica,&rdquo; Econometrica, 22: 141&ndash;146.
</p>
<p>Schultz, H. (1938), The Theory and Measurement of Demand (University of Chicago Press: Chicago,
IL).
</p>
<p>Spanos, A. (1986), Statistical Foundations of Econometric Modelling (Cambridge University Press: Cam-
bridge, MA).
</p>
<p>Tinbergen, J. (1939), Statistical Testing of Business Cycle Theories, Vol. II: Business Cycles in the USA,
1919&ndash;1932 (League of Nations: Geneva).
</p>
<p>Tintner, G. (1953), &ldquo;The Definition of Econometrics,&rdquo; Econometrica, 21: 31&ndash;40.
</p>
<p>Working, E.J. (1927), &ldquo;What Do Statistical &lsquo;Demand Curves&rsquo; Show?&rdquo; Quarterly Journal of Economics,
</p>
<p>41: 212&ndash;235.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 2
</p>
<p>Basic Statistical Concepts
</p>
<p>2.1 Introduction
</p>
<p>One chapter cannot possibly review what one learned in one or two pre-requisite courses in
statistics. This is an econometrics book, and it is imperative that the student have taken at
least one solid course in statistics. The concepts of a random variable, whether discrete or contin-
uous, and the associated probability function or probability density function (p.d.f.) are assumed
known. Similarly, the reader should know the following statistical terms: Cumulative distribu-
tion function, marginal, conditional and joint p.d.f.&rsquo;s. The reader should be comfortable with
computing mathematical expectations, and familiar with the concepts of independence, Bayes
Theorem and several continuous and discrete probability distributions. These distributions in-
clude: the Bernoulli, Binomial, Poisson, Geometric, Uniform, Normal, Gamma, Chi-squared
(χ2), Exponential, Beta, t and F distributions.
</p>
<p>Section 2.2 reviews two methods of estimation, while section 2.3 reviews the properties of
the resulting estimators. Section 2.4 gives a brief review of test of hypotheses, while section 2.5
discusses the meaning of confidence intervals. These sections are fundamental background for
this book, and the reader should make sure that he or she is familiar with these concepts. Also,
be sure to solve the exercises at the end of this chapter.
</p>
<p>2.2 Methods of Estimation
</p>
<p>Consider a Normal distribution with mean μ and variance σ2. This is the important &ldquo;Gaussian&rdquo;
distribution which is symmetric and bell-shaped and completely determined by its measure
of centrality, its mean μ and its measure of dispersion, its variance σ2. μ and σ2 are called
the population parameters. Draw a random sample X1, . . . , Xn independent and identically
distributed (IID) from this population. We usually estimate μ by μ̂ = X̄ and σ2 by
</p>
<p>s2 =
&sum;n
</p>
<p>i=1(Xi &minus; X̄)2/(n&minus; 1).
</p>
<p>For example, μ = mean income of a household in Houston. X̄ = sample average of incomes of
100 households randomly interviewed in Houston.
This estimator of μ could have been obtained by either of the following two methods of
</p>
<p>estimation:
</p>
<p>(i) Method of Moments
</p>
<p>Simply stated, this method of estimation uses the following rule: Keep equating population
moments to their sample counterpart until you have estimated all the population parameters.
</p>
<p>13 
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_2, </p>
<p/>
</div>
<div class="page"><p/>
<p>14 Chapter 2: Basic Statistical Concepts
</p>
<p>Population Sample
</p>
<p>E(X) = μ
&sum;n
</p>
<p>i=1Xi/n = X̄
E(X2) = μ2 + σ2
</p>
<p>&sum;n
i=1X
</p>
<p>2
i /n
</p>
<p>...
...
</p>
<p>E(Xr)
&sum;n
</p>
<p>i=1X
r
i /n
</p>
<p>The normal density is completely identified by μ and σ2, hence only the first 2 equations are
needed
</p>
<p>μ̂ = X̄ and μ̂2 + σ̂2 =
&sum;n
</p>
<p>i=1X
2
i /n
</p>
<p>Substituting the first equation in the second one obtains
</p>
<p>σ̂2 =
&sum;n
</p>
<p>i=1X
2
i /n&minus; X̄2 =
</p>
<p>&sum;n
i=1(Xi &minus; X̄)2/n
</p>
<p>(ii) Maximum Likelihood Estimation (MLE)
</p>
<p>For a random sample of size n from the Normal distribution Xi &sim; N(μ, σ
2), we have
</p>
<p>fi(Xi;μ, σ
2) = (1/σ
</p>
<p>&radic;
2π) exp
</p>
<p>{
&minus;(Xi &minus; μ)2/2σ2
</p>
<p>}
&minus;&infin; &lt; Xi &lt; +&infin;
</p>
<p>Since X1, . . . , Xn are independent and identically distributed, the joint probability density func-
tion is given as the product of the marginal probability density functions:
</p>
<p>f(X1, . . . , Xn;μ, σ
2) =
</p>
<p>n&prod;
i=1
</p>
<p>fi(Xi;μ, σ
2) = (1/2πσ2)n/2 exp
</p>
<p>{
&minus;&sum;ni=1(Xi &minus; μ)2/2σ2
</p>
<p>}
(2.1)
</p>
<p>Usually, we observe only one sample of n households which could have been generated by any
pair of (μ, σ2) with &minus;&infin; &lt; μ &lt; +&infin; and σ2 &gt; 0. For each pair, say (μ0, σ20), f(X1, . . . , Xn;μ0, σ20)
denotes the probability (or likelihood) of obtaining that sample. By varying (μ, σ2) we get differ-
ent probabilities of obtaining this sample. Intuitively, we choose the values of μ and σ2 that max-
imize the probability of obtaining this sample. Mathematically, we treat f(X1, . . . , Xn;μ, σ
</p>
<p>2) as
L(μ, σ2) and we call it the likelihood function. Maximizing L(μ, σ2) with respect to μ and σ2,
one gets the first-order conditions of maximization:
</p>
<p>(&part;L/&part;μ) = 0 and (&part;L/&part;σ2) = 0
</p>
<p>Equivalently, we can maximize logL(μ, σ2) rather than L(μ, σ2) and still get the same answer.
Usually, the latter monotonic transformation of the likelihood is easier to maximize and the
first-order conditions become
</p>
<p>(&part;logL/&part;μ) = 0 and (&part;logL/&part;σ2) = 0
</p>
<p>For the Normal distribution example, we get
</p>
<p>logL(μ;σ2) = &minus;(n/2)log σ2 &minus; (n/2)log 2π &minus; (1/2σ2)
&sum;n
</p>
<p>i=1(Xi &minus; μ)2
</p>
<p>&part;logL(μ;σ2)/&part;μ = (1/σ2)
&sum;n
</p>
<p>i=1(Xi &minus; μ) = 0 &rArr; μ̂MLE = X̄
</p>
<p>&part;logL(μ;σ2)/&part;σ2 = &minus;(n/2)(1/σ2) +
&sum;n
</p>
<p>i=1(Xi &minus; μ)2/2σ4 = 0
</p>
<p>&rArr; σ̂2MLE =
&sum;n
</p>
<p>i=1(Xi &minus; μ̂MLE)2/n =
&sum;n
</p>
<p>i=1(Xi &minus; X̄)2/n</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Methods of Estimation 15
</p>
<p>Note that the moments estimators and the maximum likelihood estimators are the same for
the Normal distribution example. In general, the two methods need not necessarily give the
same estimators. Also, note that the moments estimators will always have the same estimating
equations, for example, the first two equations are always
</p>
<p>E(X) = μ &equiv; &sum;ni=1Xi/n = X̄ and E(X2) = μ2 + σ2 &equiv;
&sum;n
</p>
<p>i=1X
2
i /n.
</p>
<p>For a specific distribution, we need only substitute the relationship between the population
moments and the parameters of that distribution. Again, the number of equations needed
depends upon the number of parameters of the underlying distribution. For e.g., the exponential
distribution has one parameter and needs only one equation whereas the gamma distribution has
two parameters and needs two equations. Finally, note that the maximum likelihood technique
is heavily reliant on the form of the underlying distribution, but it has desirable properties when
it exists. These properties will be discussed in the next section.
So far we have dealt with the Normal distribution to illustrate the two methods of estima-
</p>
<p>tion. We now apply these methods to the Bernoulli distribution and leave other distributions
applications to the exercises. We urge the student to practice on these exercises.
</p>
<p>Bernoulli Example: In various cases in real life the outcome of an event is binary, a worker may
join the labor force or may not. A criminal may return to crime after parole or may not. A
television off the assembly line may be defective or not. A coin tossed comes up head or tail,
and so on. In this case θ = Pr[Head] and 1 &minus; θ = Pr[Tail] with 0 &lt; θ &lt; 1 and this can be
represented by the discrete probability function
</p>
<p>f(X; θ) = θX(1&minus; θ)1&minus;X X = 0, 1
= 0 elsewhere
</p>
<p>The Normal distribution is a continuous distribution since it takes values for all X over the real
line. The Bernoulli distribution is discrete, because it is defined only at integer values for X.
Note that P [X = 1] = f(1; θ) = θ and P [X = 0] = f(0; θ) = 1 &minus; θ for all values of 0 &lt; θ &lt; 1.
A random sample of size n drawn from this distribution will have a joint probability function
</p>
<p>L(θ) = f(X1, . . . , Xn; θ) = θ
&sum;n
</p>
<p>i=1
Xi(1&minus; θ)n&minus;
</p>
<p>&sum;n
i=1
</p>
<p>Xi
</p>
<p>with Xi = 0, 1 for i = 1, . . . , n. Therefore,
</p>
<p>logL(θ) = (
&sum;n
</p>
<p>i=1Xi)logθ + (n&minus;
&sum;n
</p>
<p>i=1Xi)log(1&minus; θ)
&part;logL(θ)
</p>
<p>&part;θ
=
</p>
<p>&sum;n
i=1Xi
θ
</p>
<p>&minus; (n&minus;
&sum;n
</p>
<p>i=1Xi)
</p>
<p>(1&minus; θ)
Solving this first-order condition for θ, one gets
</p>
<p>(
&sum;n
</p>
<p>i=1Xi)(1&minus; θ)&minus; θ(n&minus;
&sum;n
</p>
<p>i=1Xi) = 0
</p>
<p>which reduces to
</p>
<p>θ̂MLE =
&sum;n
</p>
<p>i=1Xi/n = X̄.
</p>
<p>This is the frequency of heads in n tosses of a coin.</p>
<p/>
</div>
<div class="page"><p/>
<p>16 Chapter 2: Basic Statistical Concepts
</p>
<p>For the method of moments, we need
</p>
<p>E(X) =
&sum;1
</p>
<p>X=0Xf(X, θ) = 1.f(1, θ) + 0.f(0, θ) = f(1, θ) = θ
</p>
<p>and this is equated to X̄ to get θ̂ = X̄. Once again, the MLE and the method of moments yield
the same estimator. Note that only one parameter θ characterizes this Bernoulli distribution
and one does not need to equate second or higher population moments to their sample values.
</p>
<p>2.3 Properties of Estimators
</p>
<p>(i) Unbiasedness
</p>
<p>μ̂ is said to be unbiased for μ if and only if E(μ̂) = μ
</p>
<p>For μ̂ = X̄, we have E(X̄) =
&sum;n
</p>
<p>i=1E(Xi)/n = μ and X̄ is unbiased for μ. No distributional
assumption is needed as long as the Xi&rsquo;s are distributed with the same mean μ. Unbiasedness
means that &ldquo;on the average&rdquo; our estimator is on target. Let us explain this last statement. If
we repeat our drawing of a random sample of 100 households, say 200 times, then we get 200
X̄&rsquo;s. Some of these X̄ &rsquo;s will be above μ some below μ, but their average should be very close
to μ. Since in real life situations, we observe only one random sample, there is little consolation
if our observed X̄ is far from μ. But the larger n is the smaller is the dispersion of this X̄, since
var(X̄) = σ2/n and the lesser is the likelihood of this X̄ to be very far from μ. This leads us to
the concept of efficiency.
</p>
<p>(ii) Efficiency
</p>
<p>For two unbiased estimators, we compare their efficiencies by the ratio of their variances. We say
that the one with lower variance is more efficient. For example, taking μ̂1 = X1 versus μ̂2 = X̄,
both estimators are unbiased but var(μ̂1) = σ
</p>
<p>2 whereas, var(μ̂2) = σ
2/n and {the relative
</p>
<p>efficiency of μ̂1 with respect to μ̂2} = var(μ̂2)/var(μ̂1) = 1/n, see Figure 2.1. To compare all
unbiased estimators, we find the one with minimum variance. Such an estimator if it exists is
called the MVU (minimum variance unbiased estimator). A lower bound for the variance of
any unbiased estimator μ̂ of μ, is known in the statistical literature as the Cramér-Rao lower
bound, and is given by
</p>
<p>var(μ̂) &ge; 1/n{E(&part;logf(X;μ)/&part;μ)}2 = &minus;1/{nE(&part;2logf(X;μ)/&part;μ2)} (2.2)
</p>
<p>where we use either representation of the bound on the right hand side of (2.2) depending on
which one is the simplest to derive.
</p>
<p>Example 1: Consider the normal density
</p>
<p>logf(Xi;μ) = (&minus;1/2)logσ2 &minus; (1/2)log2π &minus; (1/2)(Xi &minus; μ)2/σ2
</p>
<p>&part;logf(Xi;μ)/&part;μ = (Xi &minus; μ)/σ2
</p>
<p>&part;2logf(Xi;μ)/&part;μ
2 = &minus;(1/σ2)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Properties of Estimators 17
</p>
<p>�
</p>
<p>),(~ 2��iX
</p>
<p>x
</p>
<p>)/,(~ 2 nX ��
</p>
<p>f(x)
</p>
<p>Figure 2.1 Efficiency Comparisons
</p>
<p>with E{&part;2logf(Xi;μ)/&part;μ2} = &minus;(1/σ2). Therefore, the variance of any unbiased estimator of μ,
say μ̂ satisfies the property that var(μ̂) &ge; σ2/n.
</p>
<p>Turning to σ2; let θ = σ2, then
</p>
<p>logf(Xi; θ) = &minus;(1/2)logθ &minus; (1/2)log2π &minus; (1/2)(Xi &minus; μ)2/θ
</p>
<p>&part;logf(Xi; θ)/&part;θ = &minus;1/2θ + (Xi &minus; μ)2/2θ2 = {(Xi &minus; μ)2 &minus; θ}/2θ2
</p>
<p>&part;2logf(Xi; θ)/&part;θ
2 = 1/2θ2 &minus; (Xi &minus; μ)2/θ3 = {θ &minus; 2(Xi &minus; μ)2}/2θ3
</p>
<p>E[&part;2logf(Xi; θ)/&part;θ
2] = &minus;(1/2θ2), since E(Xi &minus; μ)2 = θ. Hence, for any unbiased estimator of
</p>
<p>θ, say θ̂, its variance satisfies the following property var(θ̂) &ge; 2θ2/n, or var(σ̂2) &ge; 2σ4/n.
Note that, if one finds an unbiased estimator whose variance attains the Cramér-Rao lower
</p>
<p>bound, then this is the MVU estimator. It is important to remember that this is only a lower
bound and sometimes it is not necessarily attained. If the Xi&rsquo;s are normal, X̄ &sim; N(μ, σ
</p>
<p>2/n).
Hence, X̄ is unbiased for μ with variance σ2/n equal to the Cramér-Rao lower bound. Therefore,
X̄ is MVU for μ. On the other hand,
</p>
<p>σ̂2MLE =
&sum;n
</p>
<p>i=1(Xi &minus; X̄)2/n,
</p>
<p>and it can be shown that (nσ̂2MLE)/(n&minus;1) = s2 is unbiased for σ2. In fact, (n&minus;1)s2/σ2 &sim; χ2n&minus;1
and the expected value of a Chi-squared variable with (n&minus; 1) degrees of freedom is exactly its
degrees of freedom. Using this fact,
</p>
<p>E{(n&minus; 1)s2/σ2} = E(χ2n&minus;1) = n&minus; 1.
</p>
<p>Therefore, E(s2) = σ2.1 Also, the variance of a Chi-squared variable with (n &minus; 1) degrees of
freedom is twice these degrees of freedom. Using this fact,
</p>
<p>var{(n&minus; 1)s2/σ2} = var(χ2n&minus;1) = 2(n&minus; 1)</p>
<p/>
</div>
<div class="page"><p/>
<p>18 Chapter 2: Basic Statistical Concepts
</p>
<p>or
</p>
<p>{(n&minus; 1)2/σ4}var(s2) = 2(n&minus; 1).
</p>
<p>Hence, the var(s2) = 2σ4/(n&minus;1) and this does not attain the Cramér-Rao lower bound. In fact,
it is larger than (2σ4/n). Note also that var(σ̂2MLE) = {(n&minus; 1)2/n2}var(s2) = {2(n&minus; 1)}σ4/n2.
This is smaller than (2σ4/n)! How can that be? Remember that σ̂2MLE is a biased estimator
of σ2 and hence, var(σ̂2MLE) should not be compared with the Cramér-Rao lower bound. This
lower bound pertains only to unbiased estimators.
</p>
<p>Warning : Attaining the Cramér-Rao lower bound is only a sufficient condition for efficiency.
Failing to satisfy this condition does not necessarily imply that the estimator is not efficient.
</p>
<p>Example 2: For the Bernoulli case
</p>
<p>logf(Xi; θ) = Xilogθ + (1&minus;Xi)log(1&minus; θ)
</p>
<p>&part;logf(Xi, θ)/&part;θ = (Xi/θ)&minus; (1&minus;Xi)/(1&minus; θ)
</p>
<p>&part;2logf(Xi; θ)/&part;θ
2 = (&minus;Xi/θ2)&minus; (1&minus;Xi)/(1&minus; θ)2
</p>
<p>and E[&part;2logf(Xi; θ)/&part;θ
2] = (&minus;1/θ) &minus; 1/(1 &minus; θ) = &minus;1/[θ(1 &minus; θ)]. Therefore, for any unbiased
</p>
<p>estimator of θ, say θ̂, its variance satisfies the following property:
</p>
<p>var(θ̂) &ge; θ(1&minus; θ)/n.
</p>
<p>For the Bernoulli random sample, we proved that μ = E(Xi) = θ. Similarly, it can be easily
verified that σ2 = var(Xi) = θ(1&minus;θ). Hence, X̄ has mean μ = θ and var(X̄) = σ2/n = θ(1&minus;θ)/n.
This means that X̄ is unbiased for θ and it attains the Cramér-Rao lower bound. Therefore, X̄
is MVU for θ.
</p>
<p>Unbiasedness and efficiency are finite sample properties (in other words, true for any finite
sample size n). Once we let n tend to &infin; then we are in the realm of asymptotic properties.
Example 3: For a random sample from any distribution with mean μ it is clear that μ̃ =
(X̄ + 1/n) is not an unbiased estimator of μ since E(μ̃) = E(X̄ + 1/n) = μ+ 1/n. However, as
n &rarr; &infin; the lim E(μ̃) is equal to μ. We say, that μ̃ is asymptotically unbiased for μ.
Example 4: For the Normal case
</p>
<p>σ̂2MLE = (n&minus; 1)s2/n and E(σ̂2MLE) = (n&minus; 1)σ2/n.
</p>
<p>But as n &rarr; &infin;, lim E(σ̂2MLE) = σ2. Hence, σ̂2MLE is asymptotically unbiased for σ2.
Similarly, an estimator which attains the Cramér-Rao lower bound in the limit is asymp-
</p>
<p>totically efficient. Note that var(X̄) = σ2/n, and this tends to zero as n &rarr; &infin;. Hence, we
consider
</p>
<p>&radic;
nX̄ which has finite variance since var(
</p>
<p>&radic;
nX̄) = n var(X̄) = σ2. We say that the
</p>
<p>asymptotic variance of X̄ denoted by asymp.var(X̄) = σ2/n and that it attains the Cramér-
Rao lower bound in the limit. X̄ is therefore asymptotically efficient. Similarly,
</p>
<p>var(
&radic;
nσ̂2MLE) = n var(σ̂
</p>
<p>2
MLE) = 2(n&minus; 1)σ4/n
</p>
<p>which tends to 2σ4 as n &rarr; &infin;. This means that asymp.var(σ̂2MLE) = 2σ4/n and that it attains
the Cramér-Rao lower bound in the limit. Therefore, σ̂2MLE is asymptotically efficient.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Properties of Estimators 19
</p>
<p>(iii) Consistency
</p>
<p>Another asymptotic property is consistency. This says that as n &rarr; &infin; lim Pr[|X̄ &minus; μ| &gt; c] = 0
for any arbitrary positive constant c. In other words, X̄ will not differ from μ as n &rarr; &infin;.
</p>
<p>Proving this property uses the Chebyshev&rsquo;s inequality which states in this context that
</p>
<p>Pr[|X̄ &minus; μ| &gt; kσX̄ ] &le; 1/k2.
</p>
<p>If we let c = kσX̄ then 1/k
2 = σ2
</p>
<p>X̄
/c2 = σ2/nc2 and this tends to 0 as n &rarr; &infin;, since σ2 and c
</p>
<p>are finite positive constants. A sufficient condition for an estimator to be consistent is that it is
asymptotically unbiased and that its variance tends to zero as n &rarr; &infin;.2
</p>
<p>Example 1: For a random sample from any distribution with mean μ and variance σ2, E(X̄) = μ
and var(X̄) = σ2/n &rarr; 0 as n &rarr; &infin;, hence X̄ is consistent for μ.
Example 2: For the Normal case, we have shown that E(s2) = σ2 and var(s2) = 2σ4/(n&minus;1) &rarr; 0
as n &rarr; &infin;, hence s2 is consistent for σ2.
Example 3: For the Bernoulli case, we know that E(X̄) = θ and var(X̄) = θ(1 &minus; θ)/n &rarr; 0 as
n &rarr; &infin;, hence X̄ is consistent for θ.
Warning : This is only a sufficient condition for consistency. Failing to satisfy this condition
does not necessarily imply that the estimator is inconsistent.
</p>
<p>(iv) Sufficiency
</p>
<p>X̄ is sufficient for μ, if X̄ contains all the information in the sample pertaining to μ. In other
words, f(X1, . . . , Xn/X̄) is independent of μ. To prove this fact one uses the factorization
theorem due to Fisher and Neyman. In this context, X̄ is sufficient for μ, if and only if one can
factorize the joint p.d.f.
</p>
<p>f(X1, . . . , Xn;μ) = h(X̄;μ) &middot; g(X1, . . . , Xn)
</p>
<p>where h and g are any two functions with the latter being only a function of the X&rsquo;s and
independent of μ in form and in the domain of the X&rsquo;s.
</p>
<p>Example 1: For the Normal case, it is clear from equation (2.1) that by subtracting and adding
X̄ in the summation we can write after some algebra
</p>
<p>f(X1, .., Xn;μ, σ
2) = (1/2πσ2)n/2e&minus;{(1/2σ
</p>
<p>2)
&sum;n
</p>
<p>i=1
(Xi&minus;X̄)2}e&minus;{(n/2σ
</p>
<p>2)(X̄&minus;μ)2}
</p>
<p>Hence, h(X̄;μ) = e&minus;(n/2σ
2)(X̄&minus;μ2) and g(X1, . . . , Xn) is the remainder term which is independent
</p>
<p>of μ in form. Also &minus;&infin; &lt; Xi &lt; &infin; and hence independent of μ in the domain. Therefore, X̄ is
sufficient for μ.
</p>
<p>Example 2: For the Bernoulli case,
</p>
<p>f(X1, . . . , Xn; θ) = θ
nX̄(1&minus; θ)n(1&minus;X̄) Xi = 0, 1 for i = 1, . . . , n.
</p>
<p>Therefore, h(X̄, θ) = θnX̄(1&minus;θ)n(1&minus;X̄) and g(X1, . . . , Xn) = 1 which is independent of θ in form
and domain. Hence, X̄ is sufficient for θ.</p>
<p/>
</div>
<div class="page"><p/>
<p>20 Chapter 2: Basic Statistical Concepts
</p>
<p>Under certain regularity conditions on the distributions we are sampling from, one can show
that the MVU of any parameter θ is an unbiased function of a sufficient statistic for θ.3 Advan-
tages of the maximum likelihood estimators is that (i) they are sufficient estimators when they
exist. (ii) They are asymptotically efficient. (iii) If the distribution of the MLE satisfies certain
regularity conditions, then making the MLE unbiased results in a unique MVU estimator. A
prime example of this is s2 which was shown to be an unbiased estimator of σ2 for a random
sample drawn from the Normal distribution. It can be shown that s2 is sufficient for σ2 and that
(n&minus;1)s2/σ2 &sim; χ2n&minus;1. Hence, s2 is an unbiased sufficient statistic for σ2 and therefore it is MVU
for σ2, even though it does not attain the Cramér-Rao lower bound. (iv) Maximum likelihood
estimates are invariant with respect to continuous transformations. To explain the last property,
consider the estimator of eμ. Given μ̂MLE = X̄, an obvious estimator is e
</p>
<p>μ̂MLE = eX̄ . This is in
fact the MLE of eμ. In general, if g(μ) is a continuous function of μ, then g(μ̂MLE) is the MLE of
g(μ). Note that E(eμ̂MLE ) 	= eE(μ̂MLE) = eμ, in other words, expectations are not invariant to all
continuous transformations, especially nonlinear ones and hence the resulting MLE estimator
may not be unbiased. eX̄ is not unbiased for eμ even though X̄ is unbiased for μ.
In summary, there are two routes for finding the MVU estimator. One is systematically
</p>
<p>following the derivation of a sufficient statistic, proving that its distribution satisfies certain
regularity conditions, and then making it unbiased for the parameter in question. Of course,
MLE provides us with sufficient statistics, for example,
</p>
<p>X1, . . . , Xn &sim; IIN(μ, σ
2) &rArr; μ̂MLE = X̄ and σ̂2MLE =
</p>
<p>&sum;n
i=1(Xi &minus; X̄)2/n
</p>
<p>are both sufficient for μ and σ2, respectively. X̄ is unbiased for μ and X̄ &sim; N(μ, σ2/n). The
Normal distribution satisfies the regularity conditions needed for X̄ to be MVU for μ. σ̂2MLE is
biased for σ2, but s2 = nσ̂2MLE/(n&minus; 1) is unbiased for σ2 and (n&minus; 1)s2/σ2 &sim; χ2n&minus;1 which also
satisfies the regularity conditions for s2 to be a MVU estimator for σ2.
Alternatively, one finds the Cramér-Rao lower bound and checks whether the usual estimator
</p>
<p>(obtained from say the method of moments or the maximum likelihood method) achieves this
lower bound. If it does, this estimator is efficient, and there is no need to search further. If it
does not, the former strategy leads us to the MVU estimator. In fact, in the previous example
X̄ attains the Cramér-Rao lower bound, whereas s2 does not. However, both are MVU for μ
and σ2 respectively.
</p>
<p>(v) Comparing Biased and Unbiased Estimators
</p>
<p>Suppose we are given two estimators θ̂1 and θ̂2 of θ where the first is unbiased and has a large
variance and the second is biased but with a small variance. The question is which one of these
two estimators is preferable? θ̂1 is unbiased whereas θ̂2 is biased. This means that if we repeat
the sampling procedure many times then we expect θ̂1 to be on the average correct, whereas
θ̂2 would be on the average different from θ. However, in real life, we observe only one sample.
With a large variance for θ̂1, there is a great likelihood that the sample drawn could result in
a θ̂1 far away from θ. However, with a small variance for θ̂2, there is a better chance of getting
a θ̂2 close to θ. If our loss function is L(θ̂, θ) = (θ̂ &minus; θ)2 then our risk is
</p>
<p>R(θ̂, θ) = E[L(θ̂, θ)] = E(θ̂ &minus; θ)2 = MSE(θ̂)
= E[θ̂ &minus; E(θ̂) + E(θ̂)&minus; θ]2 = var(θ̂) + (Bias(θ̂))2.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Hypothesis Testing 21
</p>
<p>Minimizing the risk when the loss function is quadratic is equivalent to minimizing the Mean
Square Error (MSE). From its definition the MSE shows the trade-off between bias and variance.
MVU theory, sets the bias equal to zero and minimizes var(θ̂). In other words, it minimizes the
above risk function but only over θ̂&rsquo;s that are unbiased. If we do not restrict ourselves to
unbiased estimators of θ, minimizing MSE may result in a biased estimator such as θ̂2 which
beats θ̂1 because the gain from its smaller variance outweighs the loss from its small bias, see
Figure 2.2.
</p>
<p> Bias
</p>
<p>)ˆ( 2�f
</p>
<p>)ˆ( 2�E
</p>
<p>)ˆ( 1�f
</p>
<p>�
</p>
<p>)ˆ(�f
</p>
<p>�̂
</p>
<p>Figure 2.2 Bias Versus Variance
</p>
<p>2.4 Hypothesis Testing
</p>
<p>The best way to proceed is with an example.
</p>
<p>Example 1: The Economics Departments instituted a new program to teach micro-principles.
We would like to test the null hypothesis that 80% of economics undergraduate students will
pass the micro-principles course versus the alternative hypothesis that only 50% will pass. We
draw a random sample of size 20 from the large undergraduate micro-principles class and as
a simple rule we accept the null if x, the number of passing students is larger or equal to 13,
otherwise the alternative hypothesis will be accepted. Note that the distribution we are drawing
from is Bernoulli with the probability of success θ, and we have chosen only two states of the
world H0; θ0 = 0.80 and H1; θ1 = 0.5. This situation is known as testing a simple hypothesis
versus another simple hypothesis because the distribution is completely specified under the null
or alternative hypothesis. One would expect (E(x) = nθ0) 16 students under H0 and (nθ1) 10
students under H1 to pass the micro-principles exams. It seems then logical to take x &ge; 13 as
the cut-off point distinguishing H0 from H1. No theoretical justification is given at this stage
to this arbitrary choice except to say that it is the mid-point of [10, 16]. Figure 2.3 shows that
one can make two types of errors. The first is rejecting H0 when in fact it is true, this is known
as type I error and the probability of committing this error is denoted by α. The second is
accepting H0 when it is false. This is known as type II error and the corresponding probability
is denoted by β. For this example</p>
<p/>
</div>
<div class="page"><p/>
<p>22 Chapter 2: Basic Statistical Concepts
</p>
<p>α = Pr[rejecting H0/H0 is true] = Pr[x &lt; 13/θ = 0.8]
</p>
<p>= b(n = 20;x = 0; θ = 0.8) + ..+ b(n = 20;x = 12; θ = 0.8)
</p>
<p>= b(n = 20;x = 20; θ = 0.2) + ..+ b(n = 20;x = 8; θ = 0.2)
</p>
<p>= 0 + ..+ 0 + 0.0001 + 0.0005 + 0.0020 + 0.0074 + 0.0222 = 0.0322
</p>
<p>where we have used the fact that b(n;x; θ) = b(n;n&minus; x; 1&minus; θ) and b(n;x; θ) =
(
n
x
</p>
<p>)
θx(1&minus; θ)n&minus;x
</p>
<p>denotes the binomial distribution for x = 0, 1, . . . , n, see problem 4.
</p>
<p>True World
</p>
<p>θ0 = 0.80 θ1 = 0.50
</p>
<p>Decision θ0 No error Type II error
</p>
<p>θ1 Type I error No Error
</p>
<p>Figure 2.3 Type I and II Error
</p>
<p>β = Pr[accepting H0/H0 is false] = Pr[x &ge; 13/θ = 0.5]
= b(n = 20;x = 13; θ = 0.5) + ..+ b(n = 20;x = 20; θ = 0.5)
</p>
<p>= 0.0739 + 0.0370 + 0.0148 + 0.0046 + 0.0011 + 0.0002 + 0 + 0 = 0.1316
</p>
<p>The rejection region for H0, x &lt; 13, is known as the critical region of the test and α = Pr[Falling
in the critical region/H0 is true] is also known as the size of the critical region. A good test
is one which minimizes both types of errors α and β. For the above example, α is low but β
is high with more than a 13% chance of happening. This β can be reduced by changing the
critical region from x &lt; 13 to x &lt; 14, so that H0 is accepted only if x &ge; 14. In this case, one
can easily verify that
</p>
<p>α = Pr[x &lt; 14/θ = 0.8] = b(n = 20; x = 0; θ = 0.8) + ..+ b(n = 20, x = 13, θ = 0.8)
</p>
<p>= 0.0322 + b(n = 20;x = 13; θ = 0.8) = 0.0322 + 0.0545 = 0.0867
</p>
<p>and
</p>
<p>β = Pr[x &ge; 14/θ = 0.5] = b(n = 20; x = 14; θ = 0.5) + ..+ b(n = 20;x = 20; θ = 0.5)
= 0.1316&minus; b(n = 20;x = 13; θ = 0.5) = 0.0577
</p>
<p>By becoming more conservative on accepting H0 and more liberal on accepting H1, one reduces
β from 0.1316 to 0.0577 but the price paid is the increase in α from 0.0322 to 0.0867. The only
way to reduce both α and β is by increasing n. For a fixed n, there is a tradeoff between α and
β as we change the critical region. To understand this clearly, consider the real life situation of
trial by jury for which the defendant can be innocent or guilty. The decision of incarceration
or release implies two types of errors. One can make α = Pr[incarcerating/innocence] = 0 and
β = its maximum, by releasing every defendant. Or one can make β = Pr[release/guilty] = 0
and α = its maximum, by incarcerating every defendant. These are extreme cases but hopefully
they demonstrate the trade-off between α and β.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Hypothesis Testing 23
</p>
<p>The Neyman-Pearson Theory
</p>
<p>The classical theory of hypothesis testing, known as the Neyman-Pearson theory, fixes α =
Pr(type I error) &le; a constant and minimizes β or maximizes (1 &minus; β). The latter is known as
the Power of the test under the alternative.
</p>
<p>The Neyman-Pearson Lemma: If C is a critical region of size α and k is a constant such that
</p>
<p>(L0/L1) &le; k inside C
</p>
<p>and
</p>
<p>(L0/L1) &ge; k outside C
</p>
<p>then C is a most powerful critical region of size α for testing H0; θ = θ0, against H1; θ = θ1.
Note that the likelihood has to be completely specified under the null and alternative. Hence,
</p>
<p>this lemma applies only to testing a simple versus another simple hypothesis. The proof of this
lemma is given in Freund (1992). Intuitively, L0 is the likelihood function under the null H0
and L1 is the corresponding likelihood function under H1. Therefore, (L0/L1) should be small
for points inside the critical region C and large for points outside the critical region C. The
proof of the theorem shows that any other critical region, say D, of size α cannot have a smaller
probability of type II error than C. Therefore, C is the best or most powerful critical region of
size α. Its power (1&minus; β) is maximum at H1. Let us demonstrate this lemma with an example.
Example 2: Given a random sample of size n from N(μ, σ2 = 4), use the Neyman-Pearson
lemma to find the most powerful critical region of size α = 0.05 for testing H0;μ0 = 2 against
the alternative H1;μ1 = 4.
</p>
<p>Note that this is a simple versus simple hypothesis as required by the lemma, since σ2 = 4
is known and μ is specified by H0 and H1. The likelihood function for the N(μ, 4) density is
given by
</p>
<p>L(μ) = f(x1, . . . , xn;μ, 4) = (1/2
&radic;
2π)nexp
</p>
<p>{
&minus;
&sum;n
</p>
<p>i=1(xi &minus; μ)2/8
}
</p>
<p>so that
</p>
<p>L0 = L(μ0) = (1/2
&radic;
2π)nexp
</p>
<p>{
&minus;&sum;ni=1(xi &minus; 2)2/8
</p>
<p>}
</p>
<p>and
</p>
<p>L1 = L(μ1) = (1/2
&radic;
2π)nexp
</p>
<p>{
&minus;
&sum;n
</p>
<p>i=1(xi &minus; 4)2/8
}
</p>
<p>Therefore
</p>
<p>L0/L1 = exp
{
&minus;
[&sum;n
</p>
<p>i=1(xi &minus; 2)2 &minus;
&sum;n
</p>
<p>i=1(xi &minus; 4)2
]
/8
}
= exp {&minus;&sum;ni=1 xi/2 + 3n/2}
</p>
<p>and the critical region is defined by
</p>
<p>exp {&minus;&sum;ni=1 xi/2 + 3n/2} &le; k inside C
</p>
<p>Taking logarithms of both sides, subtracting (3/2)n and dividing by (&minus;1/2)n one gets
</p>
<p>x̄ &ge; K inside C</p>
<p/>
</div>
<div class="page"><p/>
<p>24 Chapter 2: Basic Statistical Concepts
</p>
<p>In practice, one need not keep track of K as long as one keeps track of the direction of the
inequality. K can be determined by making the size of C = α = 0.05. In this case
</p>
<p>α = Pr[x̄ &ge; K/μ = 2] = Pr[z &ge; (K &minus; 2)/(2/
&radic;
n)]
</p>
<p>where z = (x̄&minus; 2)/(2/&radic;n) is distributed N(0, 1) under H0. From the N(0, 1) tables, we have
</p>
<p>K &minus; 2
(2/
</p>
<p>&radic;
n)
</p>
<p>= 1.645
</p>
<p>Hence,
</p>
<p>K = 2 + 1.645(2/
&radic;
n)
</p>
<p>and x̄ &ge; 2 + 1.645(2/&radic;n) defines the most powerful critical region of size α = 0.05 for testing
H0;μ0 = 2 versus H1;μ1 = 4. Note that, in this case
</p>
<p>β = Pr[x̄ &lt; 2 + 1.645(2/
&radic;
n)/μ = 4]
</p>
<p>= Pr[z &lt; [&minus;2 + 1.645(2/
&radic;
n)]/(2/
</p>
<p>&radic;
n)] = Pr[z &lt; 1.645&minus;
</p>
<p>&radic;
n]
</p>
<p>For n = 4; β = Pr[z &lt; &minus;0.355] = 0.3613 shown by the shaded region in Figure 2.4. For n = 9;
β = Pr[z &lt; &minus;1.355] = 0.0877, and for n = 16; β = Pr[z &lt; &minus;2.355] = 0.00925.
</p>
<p>�
</p>
<p>3613.0��
</p>
<p>20 ��
x
</p>
<p>41 ��
</p>
<p>05.0��
</p>
<p>3.645
</p>
<p>Figure 2.4 Critical Region for Testing μ0 = 2 against μ1 = 4 for n = 4
</p>
<p>This gives us an idea of how, for a fixed α = 0.05, the minimum β decreases with larger sample
size n. As n increases from 4 to 9 to 16, the var(x̄) = σ2/n decreases and the two distributions
shown in Figure 2.4 shrink in dispersion still centered around μ0 = 2 and μ1 = 4, respectively.
This allows better decision making (based on larger sample size) as reflected by the critical
region shrinking from x &ge; 3.65 for n = 4 to x &ge; 2.8225 for n = 16, and the power (1&minus;β) rising
from 0.6387 to 0.9908, respectively, for a fixed α &le; 0.05. The power function is the probability
of rejecting H0. It is equal to α under H0 and 1&minus; β under H1. The ideal power function is zero
at H0 and one at H1. The Neyman-Pearson lemma allows us to fix α, say at 0.05, and find the
test with the best power at H1.
In example 2, both the null and alternative hypotheses are simple. In real life, one is more
</p>
<p>likely to be faced with testing H0;μ = 2 versus H1;μ 	= 2. Under the alternative hypothesis,
the distribution is not completely specified, since the mean μ is not known, and this is referred
to as a composite hypothesis. In this case, one cannot compute the probability of type II error</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Hypothesis Testing 25
</p>
<p>since the distribution is not known under the alternative. Also, the Neyman-Pearson lemma
cannot be applied. However, a simple generalization allows us to compute a Likelihood Ratio
test which has satisfactory properties but is no longer uniformly most powerful of size α. In this
case, one replaces L1, which is not known since H1 is a composite hypothesis, by the maximum
value of the likelihood, i.e.,
</p>
<p>λ =
maxL0
maxL
</p>
<p>Since max L0 is the maximum value of the likelihood under the null while maxL is the maximum
value of the likelihood over the whole parameter space, it follows that maxL0 &le;maxL and λ &le; 1.
Hence, if H0 is true, λ is close to 1, otherwise it is smaller than 1. Therefore, λ &le; k defines the
critical region for the Likelihood Ratio test, and k is determined such that the size of this test
is α.
</p>
<p>Example 3: For a random sample x1, . . . , xn drawn from a Normal distribution with mean μ
and variance σ2 = 4, derive the Likelihood Ratio test for H0;μ = 2 versus H1;μ 	= 2. In this
case
</p>
<p>maxL0 = (1/2
&radic;
2π)nexp
</p>
<p>{
&minus;
&sum;n
</p>
<p>i=1(xi &minus; 2)2/8
}
= L0
</p>
<p>and
</p>
<p>maxL = (1/2
&radic;
2π)nexp
</p>
<p>{
&minus;&sum;ni=1(xi &minus; x̄)2/8
</p>
<p>}
= L(μ̂MLE)
</p>
<p>where use is made of the fact that μ̂MLE = x̄. Therefore,
</p>
<p>λ = exp
{[
</p>
<p>&minus;
&sum;n
</p>
<p>i=1(xi &minus; 2)2 +
&sum;n
</p>
<p>i=1(xi &minus; x̄)2
]
/8
}
= exp
</p>
<p>{
&minus;n(x̄&minus; 2)2/8
</p>
<p>}
</p>
<p>Hence, the region for which λ &le; k, is equivalent after some simple algebra to the following
region
</p>
<p>(x̄&minus; 2)2 &ge; K or |x̄&minus; 2| &ge; K1/2
</p>
<p>where K is determined such that
</p>
<p>Pr[|x̄&minus; 2| &ge; K1/2/μ = 2] = α
</p>
<p>We know that x̄ &sim; N(2, 4/n) under H0. Hence, z = (x̄ &minus; 2)/(2/
&radic;
n) is N(0, 1) under H0, and
</p>
<p>the critical region of size α will be based upon |z| &ge; zα/2 where zα/2 is given in Figure 2.5 and
is the value of a N(0, 1) random variable such that the probability of exceeding it is α/2. For
α = 0.05, zα/2 = 1.96, and for α = 0.10, zα/2 = 1.645. This is a two-tailed test with rejection of
H0 obtained in case z &le; &minus;zα/2 or z &ge; zα/2.
Note that in this case
</p>
<p>LR &equiv; &minus;2logλ = (x̄&minus; 2)2/(4/n) = z2
</p>
<p>which is distributed as χ21 under H0. This is because it is the square of a N(0, 1) random variable
under H0. This is a finite sample result holding for any n. In general, other examples may lead
to more complicated λ statistics for which it is difficult to find the corresponding distributions
and hence the corresponding critical values. For these cases, we have an asymptotic result</p>
<p/>
</div>
<div class="page"><p/>
<p>26 Chapter 2: Basic Statistical Concepts
</p>
<p>Reject Ho Reject HoDo not reject Ho
</p>
<p>0
</p>
<p>2/�
</p>
<p>2/�
z
</p>
<p>2/�
</p>
<p>2/�
z&ndash;
</p>
<p>Figure 2.5 Critical Values
</p>
<p>which states that, for large n, LR = &minus;2logλ will be asymptotically distributed as χ2ν where ν
denotes the number of restrictions that are tested by H0. For example 2, ν = 1 and hence, LR
is asymptotically distributed as χ21. Note that we did not need this result as we found LR is
exactly distributed as χ21 for any n. If one is testing H0; μ = 2 and σ
</p>
<p>2 = 4 against the alternative
that H1; μ 	= 2 or σ2 	= 4, then the corresponding LR will be asymptotically distributed as χ22,
see problem 5, part (f).
</p>
<p>Likelihood Ratio, Wald and Lagrange Multiplier Tests
</p>
<p>Before we go into the derivations of these three tests we start by giving an intuitive graphical
explanation that will hopefully emphasize the differences among these tests. This intuitive
explanation is based on the article by Buse (1982).
Consider a quadratic log-likelihood function in a parameter of interest, say μ. Figure 2.6
</p>
<p>shows this log-likelihood logL(μ), with a maximum at μ̂. The Likelihood Ratio test, tests the
null hypothesis H0;μ = μ0 by looking at the ratio of the likelihoods λ = L(μ0)/L(μ̂) where
</p>
<p>C.
</p>
<p>B.
</p>
<p>A.
</p>
<p>�̂
</p>
<p>)(log 2 �L
</p>
<p>0�
�
</p>
<p>)(log 1 �L)(log 01 �L
</p>
<p>)(log 02 �L
</p>
<p>)ˆ(log �L
</p>
<p>)(log �L
</p>
<p>Figure 2.6 Wald Test</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Hypothesis Testing 27
</p>
<p>&minus;2logλ, twice the difference in log-likelihood, is distributed asymptotically as χ21 under H0. This
test differentiates between the top of the hill and a preassigned point on the hill by evaluating
the height at both points. Therefore, it needs both the restricted and unrestricted maximum
of the likelihood. This ratio is dependent on the distance of μ0 from μ̂ and the curvature of
the log-likelihood, C(μ) = |&part;2logL(μ)/&part;μ2|, at μ̂. In fact, for a fixed (μ̂&minus; μ0), the larger C(μ̂),
the larger is the difference between the two heights. Also, for a given curvature at μ̂, the larger
(μ̂&minus; μ0) the larger is the difference between the heights. The Wald test works from the top of
the hill, i.e., it needs only the unrestricted maximum likelihood. It tries to establish the distance
to μ0, by looking at the horizontal distance (μ̂&minus; μ0), and the curvature at μ̂. In fact the Wald
statistic is W = (μ̂&minus;μ0)2C(μ̂) and this is asymptotically distributed as χ21 under H0. The usual
form of W has I(μ) = &minus;E[&part;2logL(μ)/&part;μ2] the information matrix evaluated at μ̂, rather than
C(μ̂), but the latter is a consistent estimator of I(μ). The information matrix will be studied
in details in Chapter 7. It will be shown, under fairly general conditions, that μ̂ the MLE of
μ, has var(μ̂) = I&minus;1(μ̂). Hence W = (μ̂ &minus; μ0)2/var(μ̂) all evaluated at the unrestricted MLE.
The Lagrange-Multiplier test (LM), on the other hand, goes to the preassigned point μ0, i.e.,
it only needs the restricted maximum likelihood, and tries to determine how far it is from the
top of the hill by considering the slope of the tangent to the likelihood S(μ) = &part;logL(μ)/&part;μ at
μ0, and the rate at which this slope is changing, i.e., the curvature at μ0. As Figure 2.7 shows,
for two log-likelihoods with the same S(μ0), the one that is closer to the top of the hill is the
one with the larger curvature at μ0.
</p>
<p>�
</p>
<p>)ˆ(log 22 �L
</p>
<p>)(log �L
</p>
<p>�
</p>
<p>)(log 2 �L
</p>
<p>)(log 1 �L
</p>
<p>2�̂
</p>
<p>)ˆ(log 11 �L
</p>
<p>)(Llog 0�
</p>
<p>1�̂0�
</p>
<p>Figure 2.7 LM Test
</p>
<p>This suggests the following statistic: LM = S2(μ0){C(μ0)}&minus;1 where the curvature appears in
inverse form. In the Appendix to this chapter, we show that the E[S(μ)] = 0 and var[S(μ)] =
I(μ). Hence LM = S2(μ0)I
</p>
<p>&minus;1(μ0) = S
2(μ0)/var[S(μ0)] all evaluated at the restricted MLE.
</p>
<p>Another interpretation of the LM test is that it is a measure of failure of the restricted estimator,
in this case μ0, to satisfy the first-order conditions of maximization of the unrestricted likelihood.
We know that S(μ̂) = 0. The question is: to what extent does S(μ0) differ from zero? S(μ) is
known in the statistics literature as the score, and the LM test is also referred to as the score
test. For a more formal treatment of these tests, let us reconsider example 3 of a random sample
x1, . . . , xn from a N(μ, 4) where we are interested in testing H0;μ0 = 2 versus H1;μ 	= 2. The
likelihood function L(μ) as well as LR = &minus;2logλ = n(x̄ &minus; 2)2/4 were given in example 3. In</p>
<p/>
</div>
<div class="page"><p/>
<p>28 Chapter 2: Basic Statistical Concepts
</p>
<p>fact, the score function is given by
</p>
<p>S(μ) =
&part;logL(μ)
</p>
<p>&part;μ
=
</p>
<p>&sum;n
i=1(xi &minus; μ)
</p>
<p>4
=
</p>
<p>n(x̄&minus; μ)
4
</p>
<p>and under H0
</p>
<p>S(μ0) = S(2) =
n(x̄&minus; 2)
</p>
<p>4
</p>
<p>C(μ) = |&part;
2logL(μ)
</p>
<p>&part;μ2
| = | &minus; n
</p>
<p>4
| = n
</p>
<p>4
</p>
<p>and I(μ) = &minus;E
[
&part;2logL(μ)
</p>
<p>&part;μ2
</p>
<p>]
=
</p>
<p>n
</p>
<p>4
= C(μ).
</p>
<p>The Wald statistic is based on
</p>
<p>W = (μ̂MLE &minus; 2)2I(μ̂MLE) = (x̄&minus; 2)2 &middot;
(n
4
</p>
<p>)
</p>
<p>The LM statistic is based on
</p>
<p>LM = S2(μ0)I
&minus;1(μ0) =
</p>
<p>n2(x̄&minus; 2)2
16
</p>
<p>&middot; 4
n
=
</p>
<p>n(x̄&minus; 2)2
4
</p>
<p>Therefore, W = LM = LR for this example with known variance σ2 = 4. These tests are all
based upon the |x̄&minus; 2| &ge; k critical region, where k is determined such that the size of the test
is α. In general, these test statistics are not always equal, as is shown in the next example.
</p>
<p>Example 4: For a random sample x1, . . . , xn drawn from a N(μ, σ
2) with unknown σ2, test the
</p>
<p>hypothesis H0;μ = 2 versus H1;μ 	= 2. Problem 5, part (c), asks the reader to verify that
</p>
<p>LR = nlog
</p>
<p>[&sum;n
i=1(xi &minus; 2)2&sum;n
i=1(xi &minus; x̄)2
</p>
<p>]
whereas W =
</p>
<p>n2(x̄&minus; 2)2&sum;n
i=1(xi &minus; x̄)2
</p>
<p>and LM =
n2(x̄&minus; 2)2&sum;n
i=1(xi &minus; 2)2
</p>
<p>.
</p>
<p>One can easily show that LM/n = (W/n)/[1+(W/n)] and LR/n = log[1+(W/n)]. Let y = W/n,
then using the inequality y &ge; log(1 + y) &ge; y/(1 + y), one can conclude that W &ge; LR &ge; LM .
This inequality was derived by Berndt and Savin (1977), and will be considered again when we
study test of hypotheses in the general linear model. Note, however that all three test statistics
are based upon |x̄ &minus; 2| &ge; k and for finite n, the same exact critical value could be obtained
from the Normally distributed x̄. This section introduced the W, LR and LM test statistics, all
of which have the same asymptotic distribution. In addition, we showed that using the normal
distribution, when σ2 is known, W = LR = LM for testing H0;μ = 2 versus H1;μ 	= 2.
However, when σ2 is unknown, we showed that W &ge; LR &ge; LM for the same hypothesis.
Example 5: For a random sample x1, . . . , xn drawn from a Bernoulli distribution with parameter
θ, test the hypothesis H0; θ = θ0 versus H1; θ 	= θ0, where θ0 is a known positive fraction. This
example is based on Engle (1984). Problem 4, part (i), asks the reader to derive LR, W and
LM for H0; θ = 0.2 versus H1; θ 	= 0.2. The likelihood L(θ) and the Score S(θ) were derived in
section 2.2. One can easily verify that
</p>
<p>C(θ) = |&part;
2logL(θ)
</p>
<p>&part;θ2
| =
</p>
<p>&sum;n
i=1 xi
</p>
<p>θ2
+
</p>
<p>n&minus;&sum;ni=1 xi
(1&minus; θ)2</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Hypothesis Testing 29
</p>
<p>and
</p>
<p>I(θ) = &minus;E
[
&part;2logL(θ)
</p>
<p>&part;θ2
</p>
<p>]
=
</p>
<p>n
</p>
<p>θ(1&minus; θ)
The Wald statistic is based on
</p>
<p>W = (θ̂MLE &minus; θ0)2I(θ̂MLE) = (x̄&minus; θ0)2 &middot;
n
</p>
<p>x̄(1&minus; x̄) =
(x̄&minus; θ0)2
x̄(1&minus; x̄)/n
</p>
<p>using the fact that θ̂MLE = x̄. The LM statistic is based on
</p>
<p>LM = S2(θ0)I
&minus;1(θ0) =
</p>
<p>(x̄&minus; θ0)2
[θ0(1&minus; θ0)/n]2
</p>
<p>&middot; θ0(1&minus; θ0)
n
</p>
<p>=
(x̄&minus; θ0)2
</p>
<p>θ0(1&minus; θ0)/n
Note that the numerator of the W and LM are the same. It is the denominator which is the
var(x̄) = θ(1 &minus; θ)/n that is different. For Wald, this var(x̄) is evaluated at θ̂MLE , whereas for
LM, this is evaluated at θ0.
</p>
<p>The LR statistic is based on
</p>
<p>logL(θ̂MLE) =
&sum;n
</p>
<p>i=1 xilogx̄+ (n&minus;
&sum;n
</p>
<p>i=1 xi)log(1&minus; x̄)
</p>
<p>and
</p>
<p>logL(θ0) =
&sum;n
</p>
<p>i=1 xilogθ0 + (n&minus;
&sum;n
</p>
<p>i=1 xi)log(1&minus; θ0)
</p>
<p>so that
</p>
<p>LR = &minus;2logL(θ0) + 2logL(θ̂MLE) = &minus;2[
&sum;n
</p>
<p>i=1 xi(logθ0 &minus; logx̄)
+(n&minus;&sum;ni=1 xi)(log(1&minus; θ0)&minus; log(1&minus; x̄))]
</p>
<p>For this example, LR looks different from W and LM. However, a second-order Taylor-Series
expansion of LR around θ0 = x̄ yields the same statistic. Also, for n &rarr; &infin;, plim x̄ = θ and if
H0 is true, then all three statistics are asymptotically equivalent. Note also that all three test
statistics are based upon |x̄ &minus; θ0| &ge; k and for finite n, the same exact critical value could be
obtained from the binomial distribution. See problem 19 for more examples of the conflict in
test of hypotheses using the W, LR and LM test statistics.
Bera and Permaratne (2001, p. 58) tell the following amusing story that can bring home
</p>
<p>the interrelationship among the three tests: &ldquo;Once around 1946 Ronald Fisher invited Jerzy
Neyman, AbrahamWald, and C.R. Rao to his lodge for afternoon tea. During their conversation,
Fisher mentioned the problem of deciding whether his dog, who had been going to an &ldquo;obedience
school&rdquo; for some time, was disciplined enough. Neyman quickly came up with an idea: leave
the dog free for some time and then put him on his leash. If there is not much difference in
his behavior, the dog can be thought of as having completed the course successfully. Wald,
who lost his family in the concentration camps, was adverse to any restrictions and simply
suggested leaving the dog free and seeing whether it behaved properly. Rao, who had observed
the nuisances of stray dogs in Calcutta streets did not like the idea of letting the dog roam
freely and suggested keeping the dog on a leash at all times and observing how hard it pulls
on the leash. If it pulled too much, it needed more training. That night when Rao was back
in his Cambridge dormitory after tending Fisher&rsquo;s mice at the genetics laboratory, he suddenly
realized the connection of Neyman and Wald&rsquo;s recommendations to the Neyman-Pearson LR
and Wald tests. He got an idea and the rest is history.&rdquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>30 Chapter 2: Basic Statistical Concepts
</p>
<p>2.5 Confidence Intervals
</p>
<p>Estimation methods considered in section 2.2 give us a point estimate of a parameter, say μ,
and that is the best bet, given the data and the estimation method, of what μ might be. But
it is always good policy to give the client an interval, rather than a point estimate, where with
some degree of confidence, usually 95% confidence, we expect μ to lie. We have seen in Figure
2.5 that for a N(0, 1) random variable z, we have
</p>
<p>Pr[&minus;zα/2 &le; z &le; zα/2] = 1&minus; α
</p>
<p>and for α = 5%, this probability is 0.95, giving the required 95% confidence. In fact, zα/2 = 1.96
and
</p>
<p>Pr[&minus;1.96 &le; z &le; 1.96] = 0.95
</p>
<p>This says that if we draw 100 random numbers from a N(0, 1) density, (using a normal random
number generator) we expect 95 out of these 100 numbers to lie in the [&minus;1.96, 1.96] interval.
Now, let us get back to the problem of estimating μ from a random sample x1, . . . , xn drawn
from a N(μ, σ2) distribution. We found out that μ̂MLE = x̄ and x̄ &sim; N(μ, σ
</p>
<p>2/n). Hence,
z = (x̄&minus;μ)/(σ/&radic;n) is N(0, 1). The point estimate for μ is x̄ observed from the sample, and the
95% confidence interval for μ is obtained by replacing z by its value in the above probability
statement:
</p>
<p>Pr[&minus;zα/2 &le;
x̄&minus; μ
σ/
</p>
<p>&radic;
n
&le; zα/2] = 1&minus; α
</p>
<p>Assuming σ is known for the moment, one can rewrite this probability statement after some
simple algebraic manipulations as
</p>
<p>Pr[x̄&minus; zα/2(σ/
&radic;
n) &le; μ &le; x̄+ zα/2(σ/
</p>
<p>&radic;
n)] = 1&minus; α
</p>
<p>Note that this probability statement has random variables on both ends and the probability that
these random variables sandwich the unknown parameter μ is 1&minus;α. With the same confidence
of drawing 100 random N(0, 1) numbers and finding 95 of them falling in the (&minus;1.96, 1.96) range
we are confident that if we drew a 100 samples and computed a 100 x̄&rsquo;s, and a 100 intervals
(x̄&plusmn; 1.96 σ/&radic;n), μ will lie in these intervals in 95 out of 100 times.
</p>
<p>If σ is not known, and is replaced by s, then problem 12 shows that this is equivalent to
dividing a N(0, 1) random variable by an independent χ2n&minus;1 random variable divided by its
degrees of freedom, leading to a t-distribution with (n&minus;1) degrees of freedom. Hence, using the
t-tables for (n&minus; 1) degrees of freedom
</p>
<p>Pr[&minus;tα/2;n&minus;1 &le; tn&minus;1 &le; tα/2;n&minus;1] = 1&minus; α
</p>
<p>and replacing tn&minus;1 by (x̄&minus; μ)/(s/
&radic;
n) one gets
</p>
<p>Pr[x̄&minus; tα/2;n&minus;1(s/
&radic;
n) &le; μ &le; x̄+ tα/2;n&minus;1(s/
</p>
<p>&radic;
n)] = 1&minus; α
</p>
<p>Note that the degrees of freedom (n&minus;1) for the t-distribution come from s and the corresponding
critical value tn&minus;1;α/2 is therefore sample specific, unlike the corresponding case for the normal
density where zα/2 does not depend on n. For small n, the tα/2 values differ drastically from</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Descriptive Statistics 31
</p>
<p>Table 2.1 Descriptive Statistics for the Earnings Data
</p>
<p>Sample: 1 595
</p>
<p>LWAGE WKS ED EX MS FEM BLK UNION
</p>
<p>Mean 6.9507 46.4520 12.8450 22.8540 0.8050 0.1126 0.0723 0.3664
</p>
<p>Median 6.9847 48.0000 12.0000 21.0000 1.0000 0.0000 0.0000 0.0000
Maximum 8.5370 52.0000 17.0000 51.0000 1.0000 1.0000 1.0000 1.0000
Minimum 5.6768 5.0000 4.0000 7.0000 0.0000 0.0000 0.0000 0.0000
Std. Dev. 0.4384 5.1850 2.7900 10.7900 0.3965 0.3164 0.2592 0.4822
Skewness &ndash;0.1140 &ndash;2.7309 &ndash;0.2581 0.4208 &ndash;1.5400 2.4510 3.3038 0.5546
Kurtosis 3.3937 13.7780 2.7127 2.0086 3.3715 7.0075 11.9150 1.3076
</p>
<p>Jarque-Bera 5.13 3619.40 8.65 41.93 238.59 993.90 3052.80 101.51
Probability 0.0769 0.0000 0.0132 0.0000 0.0000 0.0000 0.0000 0.0000
</p>
<p>Observations 595 595 595 595 595 595 595 595
</p>
<p>zα/2, emphasizing the importance of using the t-density in small samples. When n is large the
difference between zα/2 and tα/2 diminishes as the t-density becomes more like a normal density.
For n = 20, and α = 0.05, tα/2;n&minus;1 = 2.093 as compared with zα/2 = 1.96. Therefore,
</p>
<p>Pr[&minus;2.093 &le; tn&minus;1 &le; 2.093] = 0.95
</p>
<p>and μ lies in x̄&plusmn; 2.093(s/&radic;n) with 95% confidence.
More examples of confidence intervals can be constructed, but the idea should be clear.
</p>
<p>Note that these confidence intervals are the other side of the coin for tests of hypotheses. For
example, in testing H0;μ = 2 versus H1;μ 	= 2 for a known σ, we discovered that the Likelihood
Ratio test is based on the same probability statement that generated the confidence interval
for μ. In classical tests of hypothesis, we choose the level of confidence α = 5% and compute
z = (x̄ &minus; μ)/(σ/&radic;n). This can be done since σ is known and μ = 2 under the null hypothesis
H0. Next, we do not reject H0 if z lies in the (&minus;zα/2, zα/2) interval and reject H0 otherwise. For
confidence intervals, on the other hand, we do not know μ, and armed with a level of confidence
(1&minus; α)% we construct the interval that should contain μ with that level of confidence. Having
done that, if μ = 2 lies in that 95% confidence interval, then we cannot reject H0;μ = 2 at the
5% level. Otherwise, we reject H0. This highlights the fact that any value of μ that lies in this
95% confidence interval (assuming it was our null hypothesis) cannot be rejected at the 5% level
by this sample. This is why we do not say &ldquo;accept H0&rdquo;, but rather we say &ldquo;do not reject H0&rdquo;.
</p>
<p>2.6 Descriptive Statistics
</p>
<p>In Chapter 4, we will consider the estimation of a simple wage equation based on 595 individuals
drawn from the Panel Study of Income Dynamics for 1982. This data is available on the Springer
web site as EARN.ASC. Table 2.1 gives the descriptive statistics using EViews for a subset of
the variables in this data set.</p>
<p/>
</div>
<div class="page"><p/>
<p>32 Chapter 2: Basic Statistical Concepts
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>50
</p>
<p>60
</p>
<p>70
</p>
<p>80
</p>
<p>6.0 6.5 7.0 7.5 8.0 8.5
</p>
<p>�������
��������
��������
</p>
<p>��������
��������
</p>
<p>���������
���������
</p>
<p>���������
���������
</p>
<p>������
������
������
������
������
</p>
<p>������
������
������
������
������
</p>
<p>�������
�������
�������
�������
�������
</p>
<p>�������
�������
�������
�������
�������
�������
</p>
<p>��������
��������
��������
��������
��������
��������
��������
��������
</p>
<p>��������
��������
��������
��������
��������
��������
��������
��������
��������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
</p>
<p>�����
�����
�����
�����
�����
�����
�����
�����
�����
�����
</p>
<p>������
������
������
������
������
������
������
������
������
������
������
������
������
������
������
������
</p>
<p>������
������
������
������
������
������
������
������
������
������
������
������
������
������
������
������
������
������
</p>
<p>�������
�������
�������
�������
�������
�������
�������
�������
�������
�������
�������
�������
�������
�������
</p>
<p>�������
�������
�������
�������
�������
�������
�������
�������
�������
�������
�������
</p>
<p>��������
��������
��������
��������
��������
��������
��������
��������
��������
</p>
<p>��������
��������
��������
��������
��������
��������
��������
</p>
<p>���������
���������
���������
</p>
<p>���������
���������
���������
���������
���������
</p>
<p>������
������
</p>
<p>������
������
������
</p>
<p>�������
�������
</p>
<p>�������
������� ��������� ������
</p>
<p>Series: LWAGE 
</p>
<p>Sample 1 595 
</p>
<p>Observations 595 
</p>
<p>Mean        6.950745 
</p>
<p>Median    6.984720 
</p>
<p>Maximum   8.537000 
</p>
<p>Minimum   5.676750 
</p>
<p>Std. Dev.    0.438403 
</p>
<p>Skewness   &ndash;0.114001 
</p>
<p>Kurtosis    3.393651 
</p>
<p>Jarque-Bera  5.130525 
</p>
<p>Probability  0.076899 
</p>
<p>Figure 2.8 Log (Wage) Histogram
</p>
<p>0
</p>
<p>50
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>250
</p>
<p>10 20 30 40 50
</p>
<p>���������
����������
����������
</p>
<p>���������
���������
</p>
<p>������������
������������
</p>
<p>�����������
�����������
</p>
<p>����������
����������
</p>
<p>��������
��������
��������
</p>
<p>�����������
�����������
</p>
<p>����������
����������
����������
����������
</p>
<p>���������
���������
���������
</p>
<p>��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
</p>
<p>�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
</p>
<p>����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
</p>
<p>Series: WKS
</p>
<p>Sample 1595
</p>
<p>Observations 595
</p>
<p>Mean  46.45210
</p>
<p>Median  48.00000
</p>
<p>Maximum  52.00000
</p>
<p>Minimum  5.000000
</p>
<p>Std. Dev.  5.185025
</p>
<p>Skewness &ndash;2.730880
</p>
<p>Kurtosis  13.77787
</p>
<p>Jarque-Bera  3619.416
</p>
<p>Probability  0.000000
</p>
<p>Figure 2.9 Weeks Worked Histogram
</p>
<p>The average log wage is $6.95 for this sample with a minimum of $5.68 and a maximum of
$8.54. The standard deviation of log wage is 0.44. A plot of the log wage histogram is given
in Figure 2.8. Weeks worked vary between 5 and 52 with an average of 46.5 and a standard
deviation of 5.2. This variable is highly skewed as evidenced by the histogram in Figure 2.9.
Years of education vary between 4 and 17 with an average of 12.8 and a standard deviation
of 2.79. There is the usual bunching up at 12 years, which is also the median, as is clear from
Figure 2.10.
Experience varies between 7 and 51 with an average of 22.9 and a standard deviation of 10.79.
</p>
<p>The distribution of this variable is skewed to the left, as shown in Figure 2.11.
Marital status is a qualitative variable indicating whether the individual is married or not.
</p>
<p>This information is recoded as a numeric (1, 0) variable, one if the individual is married and zero</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Descriptive Statistics 33
</p>
<p>0
</p>
<p>50
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>250
</p>
<p>4 6 8 10 12 14 16
</p>
<p>������� ��������� �������
���������
���������
</p>
<p>������
������
������
</p>
<p>��������
��������
��������
</p>
<p>������
������
������
</p>
<p>��������
��������
��������
</p>
<p>������
������
������
������
������
������
������
������
������
������
������
������
������
������
������
������
</p>
<p>�������
�������
�������
</p>
<p>���������
���������
���������
���������
</p>
<p>�������
�������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
</p>
<p>�������
�������
�������
�������
�������
�������
</p>
<p>Series: ED
</p>
<p>Sample 1595
</p>
<p>Observation 595
</p>
<p>Mean  12.84538
</p>
<p>Median   12.00000
</p>
<p>Maximum  17.00000
</p>
<p>Minimum  4.000000
</p>
<p>Std. Dev.   2.790006
</p>
<p>Skewness  &ndash;0.258116
</p>
<p>Kurtosis   2.712730
</p>
<p>Jarque-Bera  8.652780
</p>
<p>Probability  0.013215
</p>
<p>Figure 2.10 Years of Education Histogram
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>50
</p>
<p>60
</p>
<p>70
</p>
<p>80
</p>
<p>10 20 30 40 50
</p>
<p>��������
��������
��������
</p>
<p>�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
</p>
<p>����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
</p>
<p>��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
��������
</p>
<p>�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
</p>
<p>����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
����������
</p>
<p>����������
����������
����������
����������
����������
����������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
</p>
<p>��������
��������
��������
��������
��������
��������
</p>
<p>�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
</p>
<p>����������
����������
����������
����������
����������
����������
����������
����������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
</p>
<p>��������
��������
��������
��������
��������
</p>
<p>�����������
�����������
�����������
�����������
�����������
�����������
�����������
�����������
</p>
<p>����������
����������
����������
����������
</p>
<p>����������
����������
����������
</p>
<p>��������
��������
</p>
<p>Series: EX 
</p>
<p>Sample 1 595 
</p>
<p>Observation 595 
</p>
<p>Mean        22.85378 
</p>
<p>Median    21.00000 
</p>
<p>Maximum   51.00000 
</p>
<p>Minimum   7.000000 
</p>
<p>Std. Dev.    10.79018 
</p>
<p>Skewness    0.420826 
</p>
<p>Kurtosis    2.008578 
</p>
<p>Jarque-Bera  41.93007 
</p>
<p>Probability  0.000000 
</p>
<p>Figure 2.11 Years of Experience Histogram
</p>
<p>otherwise. This recoded variable is also known as a dummy variable. It is basically a switch
turning on when the individual is married and off when he or she is not. Female is another
dummy variable taking the value one when the individual is a female and zero otherwise. Black
is a dummy variable taking the value one when the individual is black and zero otherwise. Union
is a dummy variable taking the value one if the individual&rsquo;s wage is set by a union contract and
zero otherwise. The minimum and maximum values for these dummy variables are obvious. But
if they were not zero and one, respectively, you know that something is wrong. The average is a
meaningful statistic indicating the percentage of married individuals, females, blacks and union
contracted wages in the sample. These are 80.5, 11.3, 7.2 and 30.6%, respectively. We would
like to investigate the following claims: (i) women are paid less than men; (ii) blacks are paid
less than non-blacks; (iii) married individuals earn more than non-married individuals; and (iv)
union contracted wages are higher than non-union wages.</p>
<p/>
</div>
<div class="page"><p/>
<p>34 Chapter 2: Basic Statistical Concepts
</p>
<p>Table 2.2 Test for the Difference in Means
</p>
<p>Average
log wage Difference
</p>
<p>Male $7,004 &ndash;0.474
Female $6,530 (&ndash;8.86)
</p>
<p>Non-Black $6,978 &ndash;0.377
Black $6,601 (&ndash;5.57)
</p>
<p>Not Married $6,664 0.356
Married $7,020 (8.28)
</p>
<p>Non-Union $6,945 0.017
Union $6,962 (0.45)
</p>
<p>Table 2.3 Correlation Matrix
</p>
<p>LWAGE WKS ED EX MS FEM BLK UNION
</p>
<p>LWAGE 1.0000 0.0403 0.4566 0.0873 0.3218 &ndash;0.3419 &ndash;0.2229 0.0183
WKS 0.0403 1.0000 0.0002 &ndash;0.1061 0.0782 &ndash;0.0875 &ndash;0.0594 &ndash;0.1721
ED 0.4566 0.0002 1.0000 &ndash;0.2219 0.0184 &ndash;0.0012 &ndash;0.1196 &ndash;0.2719
EX 0.0873 &ndash;0.1061 &ndash;0.2219 1.0000 0.1570 &ndash;0.0938 0.0411 0.0689
MS 0.3218 0.0782 0.0184 0.1570 1.0000 &ndash;0.7104 &ndash;0.2231 0.1189
FEM &ndash;0.3419 &ndash;0.0875 &ndash;0.0012 &ndash;0.0938 &ndash;0.7104 1.0000 0.2086 &ndash;0.1274
BLK &ndash;0.2229 &ndash;0.0594 &ndash;0.1196 0.0411 &ndash;0.2231 0.2086 1.0000 0.0302
UNION 0.0183 &ndash;0.1721 &ndash;0.2719 0.0689 0.1189 &ndash;0.1274 0.0302 1.0000
</p>
<p>A simple first check could be based on computing the average log wage for each of these cat-
egories and testing whether the difference in means is significantly different from zero. This
can be done using a t-test, see Table 2.2. The average log wage for males and females is given
along with their difference and the corresponding t-statistic for the significance of this differ-
ence. Other rows of Table 2.2 give similar statistics for other groupings. In Chapter 4, we will
show that this t-test can be obtained from a simple regression of log wage on the categorical
dummy variable distinguishing the two groups. In this case, the Female dummy variable. From
Table 2.2, it is clear that only the difference between union and non-union contracted wages are
insignificant.
One can also plot log wage versus experience, see Figure 2.12, log wage versus education, see
</p>
<p>Figure 2.13, and log wage versus weeks, see Figure 2.14.
The data shows that, in general, log wage increases with education level, weeks worked, but
</p>
<p>that it exhibits a rising and then a declining pattern with more years of experience. Note that
the t-tests based on the difference in log wage across two groupings of individuals, by sex, race or
marital status, or the figures plotting log wage versus education, log wage versus weeks worked
are based on pairs of variables in each case. A nice summary statistic based also on pairwise com-
parisons of these variables is the correlation matrix across the data. This is given in Table 2.3.
The signs of this correlation matrix give the direction of linear relationship between the
</p>
<p>corresponding two variables, while the magnitude gives the strength of this correlation. In
Chapter 3, we will see that these simple correlations when squared give the percentage of</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Descriptive Statistics 35
L
</p>
<p>W
A
</p>
<p>G
E
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>9
</p>
<p> 0 10 20 30 40 50 60
</p>
<p>EX
</p>
<p>Figure 2.12 Log (Wage) Versus Experience
L
</p>
<p>W
A
</p>
<p>G
E
</p>
<p>ED
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>9
</p>
<p> 0 4 8 12 16 20
</p>
<p>Figure 2.13 Log (Wage) Versus Education
</p>
<p>5.5
</p>
<p>6.0
</p>
<p>6.5
</p>
<p>7.0
</p>
<p>7.5
</p>
<p>8.0
</p>
<p>8.5
</p>
<p>9.0
</p>
<p>0 10 20 30 40 50 60
</p>
<p>WKS
</p>
<p>L
W
</p>
<p>A
G
</p>
<p>E
 
</p>
<p>Figure 2.14 Log (Wage) Versus Weeks
</p>
<p>variation that one of these variables explain in the other. For example, the simple correlation
coefficient between log wage and marital status is 0.32. This means that marital status explains
(0.32)2 or 10% of the variation in log wage.
</p>
<p>One cannot emphasize enough how important it is to check one&rsquo;s data. It is important to
compute the descriptive statistics, simple plots of the data and simple correlations. A wrong
minimum or maximum could indicate some possible data entry errors. Troughs or peaks in these
plots may indicate important events for time series data, like wars or recessions, or influential
observations. More on this in Chapter 8. Simple correlation coefficients that equal one indicate
perfectly collinear variables and warn of the failure of a linear regression that has both variables
included among the regressors, see Chapter 4.</p>
<p/>
</div>
<div class="page"><p/>
<p>36 Chapter 2: Basic Statistical Concepts
</p>
<p>Notes
</p>
<p>1. Actually E(s2) = σ2 does not need the normality assumption. This fact along with the proof of
(n&minus; 1)s2/σ2 &sim; χ2n&minus;1, under Normality, can be easily shown using matrix algebra and is deferred
to Chapter 7.
</p>
<p>2. This can be proven using the Chebyshev&rsquo;s inequality, see Hogg and Craig (1995).
</p>
<p>3. See Hogg and Craig (1995) for the type of regularity conditions needed for these distributions.
</p>
<p>Problems
</p>
<p>1. Variance and Covariance of Linear Combinations of Random Variables. Let a, b, c, d, e and f be
arbitrary constants and let X and Y be two random variables.
</p>
<p>(a) Show that var(a+ bX) = b2 var(X).
</p>
<p>(b) var(a+ bX + cY ) = b2var(X) + c2 var(Y ) + 2bc cov(X,Y ).
</p>
<p>(c) cov[(a+ bX + cY ), (d+ eX + fY )] = be var(X) + cf var(Y ) + (bf + ce) cov(X,Y ).
</p>
<p>2. Independence and Simple Correlation.
</p>
<p>(a) Show that ifX and Y are independent, then E(XY ) = E(X)E(Y ) = μxμy where μx = E(X)
and μy = E(Y ). Therefore, cov(X,Y ) = E(X &minus; μx)(Y &minus; μy) = 0.
</p>
<p>(b) Show that if Y = a+ bX, where a and b are arbitrary constants, then ρxy = 1 if b &gt; 0 and
&minus;1 if b &lt; 0.
</p>
<p>3. Zero Covariance Does Not Necessarily Imply Independence. Let X = &minus;2,&minus;1, 0, 1, 2 with Pr[X =
x] = 1/5. Assume a perfect quadratic relationship between Y and X, namely Y = X2. Show that
cov(X,Y ) = E(X3) = 0. Deduce that ρXY = correlation (X,Y ) = 0. The simple correlation coef-
ficient ρXY measures the strength of the linear relationship between X and Y . For this example,
it is zero even though there is a perfect nonlinear relationship between X and Y . This is also an
example of the fact that if ρXY = 0, then X and Y are not necessarily independent. ρxy = 0 is a
necessary but not sufficient condition for X and Y to be independent. The converse, however, is
true, i.e., if X and Y are independent, then ρXY = 0, see problem 2.
</p>
<p>4. The Binomial Distribution is defined as the number of successes in n independent Bernoulli trials
with probability of success θ. This discrete probability function is given by
</p>
<p>f(X; θ) =
</p>
<p>(
n
</p>
<p>X
</p>
<p>)
θX(1&minus; θ)n&minus;X X = 0, 1, . . . , n
</p>
<p>and zero elsewhere, with
(
n
X
</p>
<p>)
= n!/[X!(n&minus;X)!].
</p>
<p>(a) Out of 20 candidates for a job with a probability of hiring of 0.1. Compute the probabilities
of getting X = 5 or 6 people hired?
</p>
<p>(b) Show that
(
n
X
</p>
<p>)
=
(
</p>
<p>n
n&minus;X
</p>
<p>)
and use that to conclude that b(n,X, θ) = b(n, n&minus;X, 1&minus; θ).
</p>
<p>(c) Verify that E(X) = nθ and var(X) = nθ(1&minus; θ).
(d) For a random sample of size n drawn from the Bernoulli distribution with parameter θ, show
</p>
<p>that X̄ is the MLE of θ.
</p>
<p>(e) Show that X̄, in part (d), is unbiased and consistent for θ.
</p>
<p>(f) Show that X̄, in part (d), is sufficient for θ.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 37
</p>
<p>(g) Derive the Cramér-Rao lower bound for any unbiased estimator of θ. Is X̄, in part (d), MVU
for θ?
</p>
<p>(h) For n = 20, derive the uniformly most powerful critical region of size α &le; 0.05 for testing
H0; θ = 0.2 versus H1; θ = 0.6. What is the probability of type II error for this test criteria?
</p>
<p>(i) Form the Likelihood Ratio test for testing H0; θ = 0.2 versus H1; θ 	= 0.2. Derive the Wald
and LM test statistics for testing H0 versus H1. When is the Wald statistic greater than the
LM statistic?
</p>
<p>5. For a random sample of size n drawn from the Normal distribution with mean μ and variance σ2.
</p>
<p>(a) Show that s2 is a sufficient statistic for σ2.
</p>
<p>(b) Using the fact that (n &minus; 1)s2/σ2 is χ2n&minus;1 (without proof), verify that E(s2) = σ2 and that
var(s2) = 2σ4/(n&minus; 1) as shown in the text.
</p>
<p>(c) Given that σ2 is unknown, form the Likelihood Ratio test statistic for testing H0;μ = 2
versus H1;μ 	= 2. Derive the Wald and Lagrange Multiplier statistics for testing H0 versus
H1. Verify that they are given by the expressions in example 4.
</p>
<p>(d) Another derivation of the W &ge; LR &ge; LM inequality for the null hypothesis given in part (c)
can be obtained as follows: Let μ̃, σ̃2 be the restricted maximum likelihood estimators under
H0;μ = μ0. Let μ̂, σ̂
</p>
<p>2 be the corresponding unrestricted maximum likelihood estimators
under the alternative H1;μ 	= μ0. Show that W = &minus;2log[L(μ̃, σ̂2)/L(μ̂, σ̂2)] and LM =
&minus;2log[L(μ̃, σ̃2)/L(μ̂, σ̃2)] where L(μ, σ2) denotes the likelihood function. Conclude that W &ge;
LR &ge; LM , see Breusch (1979). This is based on Baltagi (1994).
</p>
<p>(e) Given that μ is unknown, form the Likelihood Ratio test statistic for testing H0;σ = 3 versus
H1;σ 	= 3.
</p>
<p>(f) Form the Likelihood Ratio test statistic for testing H0;μ = 2, σ
2 = 4 against the alternative
</p>
<p>that H1;μ 	= 2 or σ2 	= 4.
(g) For n = 20, s2 = 9 construct a 95% confidence interval for σ2.
</p>
<p>6. The Poisson distribution can be defined as the limit of a Binomial distribution as n &rarr; &infin; and
θ &rarr; 0 such that nθ = λ is a positive constant. For example, this could be the probability of a
rare disease and we are random sampling a large number of inhabitants, or it could be the rare
probability of finding oil and n is the large number of drilling sights. This discrete probability
function is given by
</p>
<p>f(X;λ) =
e&minus;λλX
</p>
<p>X!
X = 0, 1, 2, . . .
</p>
<p>For a random sample from this Poisson distribution
</p>
<p>(a) Show that E(X) = λ and var(X) = λ.
</p>
<p>(b) Show that the MLE of λ is λ̂MLE = X̄.
</p>
<p>(c) Show that the method of moments estimator of λ is also X̄.
</p>
<p>(d) Show that X̄ is unbiased and consistent for λ.
</p>
<p>(e) Show that X̄ is sufficient for λ.
</p>
<p>(f) Derive the Cramér-Rao lower bound for any unbiased estimator of λ. Show that X̄ attains
that bound.
</p>
<p>(g) For n = 9, derive the Uniformly Most Powerful critical region of size α &le; 0.05 for testing
H0; λ = 2 versus H1; λ = 4.</p>
<p/>
</div>
<div class="page"><p/>
<p>38 Chapter 2: Basic Statistical Concepts
</p>
<p>(h) Form the Likelihood Ratio test for testing H0; λ = 2 versus H1; λ 	= 2. Derive the Wald and
LM statistics for testing H0 versus H1. When is the Wald test statistic greater than the LM
statistic?
</p>
<p>7. The Geometric distribution is known as the probability of waiting for the first success in indepen-
dent repeated trials of a Bernoulli process. This could occur on the 1st, 2nd, 3rd,.. trials.
</p>
<p>g(X; θ) = θ(1&minus; θ)X&minus;1 for X = 1, 2, 3, . . .
</p>
<p>(a) Show that E(X) = 1/θ and var(X) = (1&minus; θ)/θ2.
(b) Given a random sample from this Geometric distribution of size n, find the MLE of θ and
</p>
<p>the method of moments estimator of θ.
</p>
<p>(c) Show that X̄ is unbiased and consistent for 1/θ.
</p>
<p>(d) For n = 20, derive the Uniformly Most Powerful critical region of size α &le; 0.05 for testing
H0; θ = 0.5 versus H1; θ = 0.3.
</p>
<p>(e) Form the Likelihood Ratio test for testing H0; θ = 0.5 versus H1; θ 	= 0.5. Derive the Wald
and LM statistics for testing H0 versus H1. When is the Wald statistic greater than the LM
statistic?
</p>
<p>8. The Uniform density, defined over the unit interval [0, 1], assigns a unit probability for all values
of X in that interval. It is like a roulette wheel that has an equal chance of stopping anywhere
between 0 and 1.
</p>
<p>f(X) = 1 0 &le; X &le; 1
= 0 elsewhere
</p>
<p>Computers are equipped with a Uniform (0,1) random number generator so it is important to
understand these distributions.
</p>
<p>(a) Show that E(X) = 1/2 and var(X) = 1/12.
</p>
<p>(b) What is the Pr[0.1 &lt; X &lt; 0.3]? Does it matter if we ask for the Pr[0.1 &le; X &le; 0.3]?
</p>
<p>9. The Exponential distribution is given by
</p>
<p>f(X; θ) =
1
</p>
<p>θ
e&minus;X/θ X &gt; 0 and θ &gt; 0
</p>
<p>This is a skewed and continuous distribution defined only over the positive quadrant.
</p>
<p>(a) Show that E(X) = θ and var(X) = θ2.
</p>
<p>(b) Show that θ̂MLE = X̄.
</p>
<p>(c) Show that the method of moments estimator of θ is also X̄.
</p>
<p>(d) Show that X̄ is an unbiased and consistent estimator of θ.
</p>
<p>(e) Show that X̄ is sufficient for θ.
</p>
<p>(f) Derive the Cramér-Rao lower bound for any unbiased estimator of θ? Is X̄ MVU for θ?
</p>
<p>(g) For n = 20, derive the Uniformly Most Powerful critical region of size α &le; 0.05 for testing
H0; θ = 1 versus H1; θ = 2.
</p>
<p>(h) Form the Likelihood Ratio test for testing H0; θ = 1 versus H1; θ 	= 1. Derive the Wald and
LM statistics for testing H0 versus H1. When is the Wald statistic greater than the LM
statistic?</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 39
</p>
<p>10. The Gamma distribution is given by
</p>
<p>f(X;α, β) =
1
</p>
<p>Γ(α)βα
Xα&minus;1e&minus;X/β for X &gt; 0
</p>
<p>= 0 elsewhere
</p>
<p>where α and β &gt; 0 and Γ(α) = (α&minus; 1)! This is a skewed and continuous distribution.
</p>
<p>(a) Show that E(X) = αβ and var(X) = αβ2.
</p>
<p>(b) For a random sample drawn from this Gamma density, what are the method of moments
estimators of α and β?
</p>
<p>(c) Verify that for α = 1 and β = θ, the Gamma probability density function reverts to the
Exponential p.d.f. considered in problem 9.
</p>
<p>(d) We state without proof that for α = r/2 and β = 2, this Gamma density reduces to a χ2
</p>
<p>distribution with r degrees of freedom, denoted by χ2r. Show that E(χ
2
r) = r and var(χ
</p>
<p>2
r) = 2r.
</p>
<p>(e) For a random sample from the χ2r distribution, show that (X1X2, . . . , Xn) is a sufficient
statistic for r.
</p>
<p>(f) One can show that the square of a N(0, 1) random variable is a χ2 random variable with
1 degree of freedom, see the Appendix to the chapter. Also, one can show that the sum
of independent χ2&rsquo;s is a χ2 random variable with degrees of freedom equal the sum of the
corresponding degrees of freedom of the individual χ2&rsquo;s, see problem 15. This will prove useful
for testing later on. Using these results, verify that the sum of squares of m independent
N(0, 1) random variables is a χ2 with m degrees of freedom.
</p>
<p>11. The Beta distribution is defined by
</p>
<p>f(X) =
Γ(α+ β)
</p>
<p>Γ(α)Γ(β)
Xα&minus;1(1&minus;X)β&minus;1 for 0 &lt; X &lt; 1
</p>
<p>= 0 elsewhere
</p>
<p>where α &gt; 0 and β &gt; 0. This is a skewed continuous distribution.
</p>
<p>(a) For α = β = 1 this reverts back to the Uniform (0, 1) probability density function. Show
that E(X) = (α/α+ β) and var(X) = αβ/(α+ β)2(α+ β + 1).
</p>
<p>(b) Suppose that α = 1, find the estimators of β using the method of moments and the method
of maximum likelihood.
</p>
<p>12. The t-distribution with r degrees of freedom can be defined as the ratio of two independent random
variables. The numerator being a N(0, 1) random variable and the denominator being the square-
root of a χ2r random variable divided by its degrees of freedom. The t-distribution is a symmetric
distribution like the Normal distribution but with fatter tails. As r &rarr; &infin;, the t-distribution
approaches the Normal distribution.
</p>
<p>(a) Verify that if X1, . . . , Xn are a random sample drawn from a N(μ, σ
2) distribution, then
</p>
<p>z = (X̄ &minus; μ)/(σ/&radic;n) is N(0, 1).
(b) Use the fact that (n&minus; 1)s2/σ2 &sim; χ2n&minus;1 to show that t = z/
</p>
<p>&radic;
s2/σ2 = (X̄ &minus; μ)/(s/&radic;n) has a
</p>
<p>t-distribution with (n&minus; 1) degrees of freedom. We use the fact that s2 is independent of X̄
without proving it.
</p>
<p>(c) For n = 16, x̄ = 20 and s2 = 4, construct a 95% confidence interval for μ.</p>
<p/>
</div>
<div class="page"><p/>
<p>40 Chapter 2: Basic Statistical Concepts
</p>
<p>13. The F-distribution can be defined as the ratio of two independent χ2 random variables each divided
by its corresponding degrees of freedom. It is commonly used to test the equality of variances. Let
s21 be the sample variance from a random sample of size n1 drawn from N(μ1, σ
</p>
<p>2
1) and let s
</p>
<p>2
2 be
</p>
<p>the sample variance from another random sample of size n2 drawn from N(μ2, σ
2
2). We know that
</p>
<p>(n1 &minus; 1)s21/σ21 is χ2(n1&minus;1) and (n2 &minus; 1)s
2
2/σ
</p>
<p>2
2 is χ
</p>
<p>2
(n2&minus;1). Taking the ratio of those two independent
</p>
<p>χ2 random variables divided by their appropriate degrees of freedom yields
</p>
<p>F =
s21/σ
</p>
<p>2
1
</p>
<p>s22/σ
2
2
</p>
<p>which under the null hypothesis H0;σ
2
1 = σ
</p>
<p>2
2 gives F = s
</p>
<p>2
1/s
</p>
<p>2
2 and is distributed as F with (n1&minus;1)
</p>
<p>and (n2 &minus; 1) degrees of freedom. Both s21 and s22 are observable, so F can be computed and
compared to critical values for the F -distribution with the appropriate degrees of freedom. Two
inspectors drawing two random samples of size 25 and 31 from two shifts of a factory producing
steel rods, find that the sampling variance of the lengths of these rods are 15.6 and 18.9 inches
squared. Test whether the variances of the two shifts are the same.
</p>
<p>14. Moment Generating Function (MGF).
</p>
<p>(a) Derive the MGF of the Binomial distribution defined in problem 4. Show that it is equal to
[(1&minus; θ) + θet]n.
</p>
<p>(b) Derive the MGF of the Normal distribution defined in problem 5. Show that it is eμt+
1
</p>
<p>2
σ2t2
</p>
<p>.
</p>
<p>(c) Derive the MGF of the Poisson distribution defined in problem 6. Show that it is eλ(e
t&minus;1).
</p>
<p>(d) Derive the MGF of the Geometric distribution defined in problem 7. Show that it is θet/[1&minus;
(1&minus; θ)et].
</p>
<p>(e) Derive the MGF of the Exponential distribution defined in problem 9. Show that it is 1/(1&minus;
θt).
</p>
<p>(f) Derive the MGF of the Gamma distribution defined in problem 10. Show that it is (1&minus;βt)&minus;α.
Conclude that the MGF of a χ2r is (1&minus; 2t)&minus;
</p>
<p>r
2 .
</p>
<p>(g) Obtain the mean and variance of each distribution by differentiating the corresponding MGF
derived in parts (a) through (f).
</p>
<p>15. Moment Generating Function Method.
</p>
<p>(a) Show that if X1, . . . , Xn are independent Poisson distributed with parameters (λi) respec-
tively, then Y =
</p>
<p>&sum;n
i=1 Xi is Poisson with parameter
</p>
<p>&sum;n
i=1 λi.
</p>
<p>(b) Show that ifX1, . . . , Xn are independent Normally distributed with parameters (μi, σ
2
i ), then
</p>
<p>Y =
&sum;n
</p>
<p>i=1 Xi is Normal with mean
&sum;n
</p>
<p>i=1 μi and variance
&sum;n
</p>
<p>i=1 σ
2
i .
</p>
<p>(c) Deduce from part (b) that if X1, . . . , Xn are IIN(μ, σ
2), then X̄ &sim; N(μ, σ2/n).
</p>
<p>(d) Show that if X1, . . . , Xn are independent χ
2 distributed with parameters (ri) respectively,
</p>
<p>then Y =
&sum;n
</p>
<p>i=1 Xi is χ
2 distributed with parameter
</p>
<p>&sum;n
i=1 ri.
</p>
<p>16. Best Linear Prediction. (Problems 16 and 17 are based on Amemiya (1994)). Let X and Y be two
random variables with means μX and μY and variances σ
</p>
<p>2
X and σ
</p>
<p>2
Y , respectively. Suppose that
</p>
<p>ρ = correlation(X,Y ) = σXY /σXσY
</p>
<p>where σXY = cov(X,Y ). Consider the linear relationship Y = α+βX where α and β are scalars:
</p>
<p>(a) Show that the best linear predictor of Y based on X, where best in this case means the
minimum mean squared error predictor which minimizes E(Y &minus;α&minus; βX)2 with respect to α
and β is given by Ŷ = α̂+ β̂X where α̂ = μY &minus; β̂μX and β̂ = σXY /σ2X = ρσY /σX .</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 41
</p>
<p>(b) Show that the var(Ŷ ) = ρ2σ2Y and that û = Y &minus; Ŷ , the prediction error, has mean zero and
variance equal to (1&minus; ρ2)σ2Y . Therefore, ρ2 can be interpreted as the proportion of σ2Y that
is explained by the best linear predictor Ŷ .
</p>
<p>(c) Show that cov(Ŷ , û) = 0.
</p>
<p>17. The Best Predictor. Let X and Y be the two random variables considered in problem 16. Now
consider predicting Y by a general, possibly non-linear, function of X denoted by h(X).
</p>
<p>(a) Show that the best predictor of Y based on X, where best in this case means the minimum
mean squared error predictor that minimizes E[Y &minus; h(X)]2 is given by h(X) = E(Y/X).
Hint: Write E[Y &minus; h(X)]2 as E{[Y &minus; E(Y/X)] + [E(Y/X) &minus; h(X)]}2. Expand the square
and show that the cross-product term has zero expectation. Conclude that this mean squared
error is minimized at h(X) = E(Y/X).
</p>
<p>(b) If X and Y are bivariate Normal, show that the best predictor of Y based on X is identical
to the best linear predictor of Y based on X.
</p>
<p>18. Descriptive Statistics. Using the data used in section 2.6 based on 595 individuals drawn from the
Panel Study of Income Dynamics for 1982 and available on the Springer web site as EARN.ASC,
replicate the tables and graphs given in that section. More specifically
</p>
<p>(a) replicate Table 2.1 which gives the descriptive statistics for a subset of the variables in this
data set.
</p>
<p>(b) Replicate Figures 2.6&ndash;2.11 which plot the histograms for log wage, weeks worked, education
and experience.
</p>
<p>(c) Replicate Table 2.2 which gives the average log wage for various groups and test the difference
between these averages using a t-test.
</p>
<p>(d) Replicate Figure 2.12 which plots log wage versus experience. Figure 2.13 which plots log
wage versus education and Figure 2.14 which plots log wage versus weeks worked.
</p>
<p>(e) Replicate Table 2.3 which gives the correlation matrix among a subset of these variables.
</p>
<p>19. Conflict Among Criteria for Testing Hypotheses: Examples from Non-normal Distributions. This
is based on Baltagi (2000). Berndt and Savin (1977) showed that W &ge; LR &ge; LM for the case of
a multivariate regression model with normal distrubances. Ullah and Zinde-Walsh (1984) showed
that this inequality is not robust to non-normality of the disturbances. In the spirit of the latter
article, this problem considers simple examples from non-normal distributions and illustrates how
this conflict among criteria is affected.
</p>
<p>(a) Consider a random sample x1, x2, . . . , xn from a Poisson distribution with parameter λ. Show
that for testing λ = 3 versus λ 	= 3 yields W &ge; LM for x̄ &le; 3 and W &le; LM for x̄ &ge; 3.
</p>
<p>(b) Consider a random sample x1, x2, . . . , xn from an Exponential distribution with parameter
θ. Show that for testing θ = 3 versus θ 	= 3 yields W &ge; LM for 0 &lt; x̄ &le; 3 and W &le; LM for
x̄ &ge; 3.
</p>
<p>(c) Consider a random sample x1, x2, . . . , xn from a Bernoulli distribution with parameter θ.
Show that for testing θ = 0.5 versus θ 	= 0.5, we will always get W &ge; LM. Show also, that
for testing θ = (2/3) versus θ 	= (2/3) we get W &le; LM for (1/3) &le; x̄ &le; (2/3) and W &ge; LM
for (2/3) &le; x̄ &le; 1 or 0 &lt; x̄ &le; (1/3).</p>
<p/>
</div>
<div class="page"><p/>
<p>42 Chapter 2: Basic Statistical Concepts
</p>
<p>References
</p>
<p>More detailed treatment of the material in this chapter may be found in:
</p>
<p>Amemiya, T. (1994), Introduction to Statistics and Econometrics (Harvard University Press: Cambridge).
</p>
<p>Baltagi, B.H. (1994), &ldquo;The Wald, LR, and LM Inequality,&rdquo; Econometric Theory, Problem 94.1.2, 10:
223&ndash;224.
</p>
<p>Baltagi, B.H. (2000), &ldquo;Conflict Among Critera for Testing Hypotheses: Examples from Non-Normal
Distributions,&rdquo; Econometric Theory, Problem 00.2.4, 16: 288.
</p>
<p>Bera A.K. and G. Permaratne (2001), &ldquo;General Hypothesis Testing,&rdquo; Chapter 2 in Baltagi, B.H. (ed.),
A Companion to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Berndt, E.R. and N.E. Savin (1977), &ldquo;Conflict Among Criteria for Testing Hypotheses in the Multivariate
Linear Regression Model,&rdquo; Econometrica, 45: 1263&ndash;1278.
</p>
<p>Breusch, T.S. (1979), &ldquo;Conflict Among Criteria for Testing Hypotheses: Extensions and Comments,&rdquo;
Econometrica, 47: 203&ndash;207.
</p>
<p>Buse, A. (1982), &ldquo;The Likelihood Ratio, Wald, and Lagrange Multiplier Tests: An Expository Note,&rdquo;
The American Statistician, 36 :153&ndash;157.
</p>
<p>DeGroot, M.H. (1986), Probability and Statistics (Addison-Wesley: Mass.).
</p>
<p>Freedman, D., R. Pisani, R. Purves and A. Adhikari (1991), Statistics (Norton: New York).
</p>
<p>Freund, J.E. (1992), Mathematical Statistics (Prentice-Hall: New Jersey).
</p>
<p>Hogg, R.V. and A.T. Craig (1995), Introduction to Mathematical Statistics (Prentice Hall: New Jersey).
</p>
<p>Jollife, I.T. (1995), &ldquo;Sample Sizes and the Central Limit Theorem: The Poisson Distribution as an
Illustration,&rdquo; The American Statistician, 49: 269.
</p>
<p>Kennedy, P. (1992), A Guide to Econometrics (MIT Press: Cambridge).
</p>
<p>Mood, A.M., F.A. Graybill and D.C. Boes (1974), Introduction to the Theory of Statistics (McGraw-Hill:
New York).
</p>
<p>Spanos, A. (1986), Statistical Foundations of Econometric Modelling (Cambridge University Press: Cam-
bridge).
</p>
<p>Ullah, A. and V. Zinde-Walsh (1984), &ldquo;On the Robustness of LM, LR and W Tests in Regression Models,&rdquo;
Econometrica, 52: 1055&ndash;1065.
</p>
<p>Zellner, A. (1971), An Introduction to Bayesian Inference in Econometrics (Wiley: New York).
</p>
<p>Appendix
</p>
<p>Score and Information Matrix: The likelihood function of a sample X1, . . . , Xn drawn from f(Xi, θ)
is really the joint probability density function written as a function of θ:
</p>
<p>L(θ) = f(X1, . . . , Xn; θ)
</p>
<p>This probability density function has the property that
&int;
L(θ)dx = 1 where the integral is over all
</p>
<p>X1, . . . , Xn written compactly as one integral over x. Differentiating this multiple integral with respect</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 43
</p>
<p>to θ, one gets
&int;
</p>
<p>&part;L
</p>
<p>&part;θ
dx = 0
</p>
<p>Multiplying and dividing by L, one gets
</p>
<p>&int; (
1
</p>
<p>L
</p>
<p>&part;L
</p>
<p>&part;θ
</p>
<p>)
L dx =
</p>
<p>&int; (
&part;logL
</p>
<p>&part;θ
</p>
<p>)
L dx = 0
</p>
<p>But the score is by definition S(θ) = &part;logL/&part;θ. Hence E[S(θ)] = 0. Differentiating again with respect
to θ, one gets
</p>
<p>&int; [(
&part;2logL
</p>
<p>&part;θ2
</p>
<p>)
L+
</p>
<p>&int; (
&part;logL
</p>
<p>&part;θ
</p>
<p>)(
&part;L
</p>
<p>&part;θ
</p>
<p>)]
dx = 0
</p>
<p>Multiplying and dividing the second term by L one gets
</p>
<p>E
</p>
<p>[
&part;2logL
</p>
<p>&part;θ2
+
</p>
<p>(
&part;logL
</p>
<p>&part;θ
</p>
<p>)2]
= 0
</p>
<p>or
</p>
<p>E
</p>
<p>[
&minus;&part;
</p>
<p>2logL
</p>
<p>&part;θ2
</p>
<p>]
= E
</p>
<p>[(
&part;logL
</p>
<p>&part;θ
</p>
<p>)2]
= E[S(θ)]2
</p>
<p>But var[S(θ)] = E[S(θ)]2 since E[S(θ)] = 0. Hence I(θ) = var[S(θ)].
</p>
<p>Moment Generating Function (MGF): For the random variable X, the expected value of a special
function of X, namely eXt is denoted by
</p>
<p>MX(t) = E(e
Xt) = E(1 +Xt+X2
</p>
<p>t2
</p>
<p>2!
+X3
</p>
<p>t3
</p>
<p>3!
+ ..)
</p>
<p>where the second equality follows from the Taylor series expansion of eXt around zero. Therefore,
</p>
<p>MX(t) = 1 + E(X)t+ E(X
2)
t2
</p>
<p>2!
+ E(X3)
</p>
<p>t3
</p>
<p>3!
+ ..
</p>
<p>This function of t generates the moments of X as coefficients of an infinite polynomial in t. For example,
μ = E(X) = coefficient of t, and E(X2)/2 is the coefficient of t2, etc. Alternatively, one can differentiate
this MGF with respect to t and obtain μ = E(X) = M &prime;X(0), i.e., the first derivative of MX(t) with
respect to t evaluated at t = 0. Similarly, E(Xr) = MrX(0) which is the r-th derivative of MX(t) with
respect to t evaluated at t = 0. For example, for the Bernoulli distribution;
</p>
<p>MX(t) = E(e
Xt) =
</p>
<p>&sum;1
X=0 e
</p>
<p>XtθX(1&minus; θ)1&minus;X = θet + (1&minus; θ)
</p>
<p>so that M &prime;X(t) = θe
t and M &prime;X(0) = θ = E(X) and M
</p>
<p>&prime;&prime;
X(t) = θe
</p>
<p>t which means that E(X2) = M &prime;&prime;X(0) = θ.
Hence,
</p>
<p>var(X) = E(X2)&minus; (E(X))2 = θ &minus; θ2 = θ(1&minus; θ).
</p>
<p>For the Normal distribution, see problem 14, it is easy to show that if X &sim; N(μ, σ2), then MX(t) =
</p>
<p>eμt+
1
</p>
<p>2
σ2t2and M &prime;X(0) = E(X) = μ and M
</p>
<p>&prime;&prime;
X(0) = E(X
</p>
<p>2) = σ2 + μ2.
There is a one-to-one correspondence between the MGF when it exists and the corresponding p.d.f.
</p>
<p>This means that if Y has a MGF given by e2t+4t
2
</p>
<p>then Y is normally distributed with mean 2 and
variance 8. Similarly, if Z has a MGF given by (et + 1)/2, then Z is Bernoulli distributed with θ = 1/2.</p>
<p/>
</div>
<div class="page"><p/>
<p>44 Chapter 2: Basic Statistical Concepts
</p>
<p>Change of Variable: If X &sim; N(0, 1), then one can find the distribution function of Y = |X| by using
the Distribution Function method. By definition the distribution function of y is defined as
</p>
<p>G(y) = Pr[Y &le; y] = Pr[|X| &le; y] = Pr[&minus;y &le; X &le; y]
= Pr[X &le; y]&minus; Pr[X &le; &minus;y] = F (y)&minus; F (&minus;y)
</p>
<p>so that the distribution function of Y,G(y), can be obtained from the distribution function of X, F (x).
Since the N(0, 1) distribution is symmetric around zero, then F (&minus;y) = 1 &minus; F (y) and substituting
that in G(y) we get G(y) = 2F (y) &minus; 1. Recall, that the p.d.f. of Y is given by g(y) = G&prime;(y). Hence,
g(y) = f(y) + f(&minus;y) and this reduces to 2f(y) if the distribution is symmetric around zero. So that if
f(x) = e&minus;x
</p>
<p>2/2/
&radic;
2π for &minus;&infin; &lt; x &lt; +&infin; then g(y) = 2f(y) = 2e&minus;y2/2/
</p>
<p>&radic;
2π for y &ge; 0.
</p>
<p>Let us now find the distribution of Z = X2, the square of a N(0, 1) random variable. Note that
dZ/dX = 2X which is positive when X &gt; 0 and negative when X &lt; 0. The change of variable method
cannot be applied since Z = X2 is not a monotonic transformation over the entire domain ofX. However,
using Y = |X|, we get Z = Y 2 = (|X|)2 and dZ/dY = 2Y which is always non-negative since Y is non-
negative. In this case, the change of variable method states that the p.d.f. of Z is obtained from that
of Y by substituting the inverse transformation Y =
</p>
<p>&radic;
Z into the p.d.f. of Y and multiplying it by the
</p>
<p>absolute value of the derivative of the inverse transformation:
</p>
<p>h(z) = g(
&radic;
z) &middot; |dY
</p>
<p>dZ
| = 2&radic;
</p>
<p>2π
e&minus;z/2| 1
</p>
<p>2
&radic;
z
| = 1&radic;
</p>
<p>2π
z&minus;1/2e&minus;z/2 for z &ge; 0
</p>
<p>It is clear why this transformation will not work for X since Z = X2 has two solutions for the inverse
transformation, X = &plusmn;
</p>
<p>&radic;
Z, whereas, there is one unique solution for Y =
</p>
<p>&radic;
Z since it is non-negative.
</p>
<p>Using the results of problem 10, one can deduce that Z has a gamma distribution with α = 1/2 and
β = 2. This special Gamma density function is a χ2 distribution with 1 degree of freedom. Hence, we
have shown that the square of a N(0, 1) random variable has a χ21 distribution.
</p>
<p>Finally, if X1, . . . , Xn are independently distributed then the distribution function of Y =
&sum;n
</p>
<p>i=1 Xi
can be obtained from that of the Xi&rsquo;s using the Moment Generating Function (MGF) method:
</p>
<p>MY (t) = E(e
Y t) = E[e(
</p>
<p>&sum;n
i=1
</p>
<p>Xi)t] = E(eX1t)E(eX2t)..E(eXnt)
</p>
<p>= MX1(t)MX2(t)..MXn(t)
</p>
<p>If in addition these Xi&rsquo;s are identically distributed, then MXi(t) = MX(t) for i = 1, . . . , n and
</p>
<p>MY (t) = [MX(t)]
n
</p>
<p>For example, if X1, . . . , Xn are IID Bernoulli (θ), then MXi(t) = MX(t) = θe
t + (1&minus; θ) for i = 1, . . . , n.
</p>
<p>Hence the MGF of Y =
&sum;n
</p>
<p>i=1 Xi is given by
</p>
<p>MY (t) = [MX(t)]
n = [θet + (1&minus; θ)]n
</p>
<p>This can be easily shown to be the MGF of the Binomial distribution given in problem 14. This proves
that the sum of n independent and identically distributed Bernoulli random variables with parameter θ
is a Binomial random variable with same parameter θ.
</p>
<p>Central Limit Theorem: If X1, . . . , Xn are IID(μ, σ
2) from an unknown distribution, then Z =
</p>
<p>X̄ &minus; μ
σ/
</p>
<p>&radic;
nis asymptotically distributed as N(0, 1).
</p>
<p>Proof : We assume that the MGF of the Xi&rsquo;s exist and derive the MGF of Z. Next, we show that lim
MZ(t) as n &rarr; &infin; is e1/2t
</p>
<p>2
</p>
<p>which is the MGF of N(0, 1) distribution. First, note that
</p>
<p>Z =
</p>
<p>&sum;n
i=1 Xi &minus; nμ
</p>
<p>σ
&radic;
n
</p>
<p>=
Y &minus; nμ
σ
&radic;
n</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 45
</p>
<p>where Y =
&sum;n
</p>
<p>i=1 Xi with MY (t) = [MX(t)]
n. Therefore,
</p>
<p>MZ(t) = E(e
Zt) = E
</p>
<p>(
e(Y t&minus;nμt)/σ
</p>
<p>&radic;
n
)
= e&minus;nμt/σ
</p>
<p>&radic;
nE
</p>
<p>(
eY t/σ
</p>
<p>&radic;
n
)
</p>
<p>= e&minus;nμt/σ
&radic;
nMY (t/σ
</p>
<p>&radic;
n) = e&minus;nμt/σ
</p>
<p>&radic;
n[MX(t/σ
</p>
<p>&radic;
n)]n
</p>
<p>Taking log of both sides we get
</p>
<p>logMZ(t) =
&minus;nμt
σ
&radic;
n
</p>
<p>+ nlog[1 +
t
</p>
<p>σ
&radic;
n
E(X) +
</p>
<p>t2
</p>
<p>2σ2n
E(X2) +
</p>
<p>t3
</p>
<p>6σ3n
&radic;
n
E(X3) + ..]
</p>
<p>Using the Taylor series expansion log(1 + s) = s&minus; s
2
</p>
<p>2
+
</p>
<p>s3
</p>
<p>3
&minus; .. we get
</p>
<p>logMZ(t) = &minus;
&radic;
nμ
</p>
<p>σ
t+ n
</p>
<p>{[
μ
</p>
<p>t
</p>
<p>σ
&radic;
n
+
</p>
<p>t2
</p>
<p>2σ2n
E(X2) +
</p>
<p>t3
</p>
<p>6σ3n
&radic;
n
E(X3) + ..
</p>
<p>]
</p>
<p>&minus;1
2
</p>
<p>[
μ
</p>
<p>t
</p>
<p>σ
&radic;
n
+
</p>
<p>t2
</p>
<p>2σ2n
E(X2) +
</p>
<p>t3
</p>
<p>6σ3n
&radic;
n
E(X3) + ..
</p>
<p>]2
</p>
<p>+
1
</p>
<p>3
</p>
<p>[
μ
</p>
<p>t
</p>
<p>σ
&radic;
n
+
</p>
<p>t2
</p>
<p>2σ2
&radic;
n
E(X2) +
</p>
<p>t3
</p>
<p>6σ3n
&radic;
n
E(X3) + ..
</p>
<p>]3
&minus; ..
</p>
<p>}
</p>
<p>Collecting powers of t, we get
</p>
<p>logMZ(t) =
</p>
<p>(
&minus;
&radic;
nμ
</p>
<p>σ
+
</p>
<p>&radic;
nμ
</p>
<p>σ
</p>
<p>)
t+
</p>
<p>(
(E(X2)
</p>
<p>2σ2
&minus; μ
</p>
<p>2
</p>
<p>2σ2
</p>
<p>)
t2
</p>
<p>+
</p>
<p>(
E(X3)
</p>
<p>6σ3
&radic;
n
&minus; 1
</p>
<p>2
&middot; 2μE(X
</p>
<p>2)
</p>
<p>2σ3
&radic;
n
</p>
<p>+
1
</p>
<p>3
</p>
<p>μ3
</p>
<p>σ3
&radic;
n
</p>
<p>)
t3 + ..
</p>
<p>Therefore
</p>
<p>logMZ(t) =
1
</p>
<p>2
t2 +
</p>
<p>(
E(X3)
</p>
<p>6
&minus; μE(X
</p>
<p>2)
</p>
<p>2
+
</p>
<p>μ3
</p>
<p>3
</p>
<p>)
t3
</p>
<p>σ3
&radic;
n
+ ..
</p>
<p>note that the coefficient of t3 is 1/
&radic;
n times a constant. Therefore, this coefficient goes to zero as n &rarr; &infin;.
</p>
<p>Similarly, it can be shown that the coefficient of tr is 1/
&radic;
nr&minus;2 times a constant for r &ge; 3. Hence,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>logMZ(t) =
1
</p>
<p>2
t2 and lim
</p>
<p>n&rarr;&infin;
MZ(t) = e
</p>
<p>1
</p>
<p>2
t2
</p>
<p>which is the MGF of a standard normal distribution.
The Central Limit Theorem is a powerful tool for asymptotic inference. In real life we do not know
</p>
<p>what distribution we are sampling from, but as long as the sample drawn is random and we average (or
sum) and standardize then as n &rarr; &infin;, the resulting standardized statistic has an asymptotic N(0, 1)
distribution that can be used for inference.
</p>
<p>Using a random number generator from say the uniform distribution on the computer, one can generate
samples of size n = 20, 30, 50 from this distribution and show how the sampling distribution of the sum
(or average) when it is standardized closely approximates the N(0, 1) distribution.
</p>
<p>The real question for the applied researcher is how large n should be to invoke the Central Limit
Theorem. This depends on the distribution we are drawing from. For a Bernoulli distribution, a larger
n is needed the more asymmetric this distribution is i.e., if θ = 0.1 rather than 0.5.
</p>
<p>In fact, Figure 2.15 shows the Poisson distribution with mean = 15. This looks like a good approx-
imation for a Normal distribution even though it is a discrete probability function. Problem 15 shows
that the sum of n independent identically distributed Poisson random variables with parameter λ is
a Poisson random variable with parameter (nλ). This means that if λ = 0.15, an n of 100 will lead
to the distribution of the sum being Poisson (nλ = 15) and the Central Limit Theorem seems well
approximated.</p>
<p/>
</div>
<div class="page"><p/>
<p>46 Chapter 2: Basic Statistical Concepts
</p>
<p>Figure 2.15 Poisson Probability Distribution, Mean = 15
</p>
<p>Figure 2.16 Poisson Probability Distribution, Mean = 1.5
</p>
<p>However, if λ = 0.015, an n of 100 will lead to the distribution of the sum being Poisson (nλ = 1.5)
which is given in Figure 2.16. This Poisson probability function is skewed and discrete and does not
approximate well a normal density. This shows that one has to be careful in concluding that n = 100 is a
large enough sample for the Central Limit Theorem to apply. We showed in this simple example that this
depends on the distribution we are sampling from. This is true for Poisson (λ = 0.15) but not Poisson
(λ = 0.015), see Joliffe (1995). The same idea can be illustrated with a skewed Bernoulli distribution.
</p>
<p>Conditional Mean and Variance: Two random variables X and Y are bivariate Normal if they have
the following joint distribution:
</p>
<p>f(x, y) =
1
</p>
<p>2πσ
X
σ
</p>
<p>Y
</p>
<p>&radic;
1&minus; ρ2
</p>
<p>exp
</p>
<p>{
&minus; 1
2(1&minus; ρ2)
</p>
<p>[(
x&minus; μ
</p>
<p>X
</p>
<p>σ
X
</p>
<p>)2
+
</p>
<p>(
y &minus; μ
</p>
<p>Y
</p>
<p>σ
Y
</p>
<p>)2
</p>
<p>&minus;2ρ
(
x&minus; μX
σ
</p>
<p>X
</p>
<p>)(
y &minus; μ
</p>
<p>Y
</p>
<p>σ
Y
</p>
<p>)]}
</p>
<p>0.00
</p>
<p>0.02
</p>
<p>0.04
</p>
<p>0.06
</p>
<p>0.08
</p>
<p>0.10
</p>
<p>0.12
</p>
<p>0 5 10 15 20 25 30 x
</p>
<p>)(xp
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0 2 4 6 8 x10
</p>
<p>)(xp</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 47
</p>
<p>where &minus;&infin; &lt; x &lt; +&infin;, &minus;&infin; &lt; y &lt; +&infin;, E(X) = μX , E(Y ) = μY , var(X) = σ2X , var(Y ) = σ2Y and ρ =
correlation (X,Y ) = cov(X,Y )/σXσY . This joint density can be rewritten as
</p>
<p>f(x, y) =
1&radic;
</p>
<p>2πσ
Y
</p>
<p>&radic;
1&minus; ρ2
</p>
<p>exp
</p>
<p>{
&minus; 1
2σ2Y (1&minus; ρ2)
</p>
<p>[
y &minus; μY &minus; ρ
</p>
<p>σ
Y
</p>
<p>σ
X
</p>
<p>(x&minus; μX)
]2}
</p>
<p>&middot; 1&radic;
2πσX
</p>
<p>exp
</p>
<p>{
&minus; 1
2σ2X
</p>
<p>(x&minus; μX)2
}
</p>
<p>= f(y/x)f1(x)
</p>
<p>where f1(x) is the marginal density of X and f(y/x) is the conditional density of Y given X. In this
</p>
<p>case, X &sim; N(μX , σ
2
X) and Y/X is Normal with mean E(Y/X) = μY + ρ
</p>
<p>σ
Y
</p>
<p>σ
X
</p>
<p>(x&minus; μx) and variance given
by var(Y/X) = σ2Y (1&minus; ρ2).
</p>
<p>By symmetry, the roles of X and Y can be interchanged and one can write f(x, y) = f(x/y) f2(y)
where f2(y) is the marginal density of Y . In this case, Y &sim; N(μY , σ
</p>
<p>2
Y ) and X/Y is Normal with mean
</p>
<p>E(X/Y ) = μX + ρ
σ
</p>
<p>X
</p>
<p>σ
Y
</p>
<p>(y &minus; μY ) and variance given by var(X/Y ) = σ2X(1&minus; ρ2). If ρ = 0, then f(y/x) =
f2(y) and f(x, y) = f1(x)f2(y) proving that X and Y are independent. Therefore, if cov(X,Y ) = 0 and
X and Y are bivariate Normal, then X and Y are independent. In general, cov(X,Y ) = 0 alone does
not necessarily imply independence, see problem 3.
</p>
<p>One important and useful property is the law of iterated expectations. This says that the expectation
of any function of X and Y say h(X,Y ) can be obtained as follows:
</p>
<p>E[h(X,Y )] = EXEY/X [h(X,Y )]
</p>
<p>where the subscript Y/X on E means the conditional expectation of Y given that X is treated as a
constant. The next expectation EX treats X as a random variable. The proof is simple.
</p>
<p>E[h(X,Y )] =
</p>
<p>&int; +&infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; +&infin;
</p>
<p>&minus;&infin;
h(x, y)f(x, y)dxdy
</p>
<p>where f(x, y) is the joint density of X and Y . But f(x, y) can be written as f(y/x)f1(x), hence
</p>
<p>E[h(X,Y )] =
&int; +&infin;
&minus;&infin;
</p>
<p>[&int; +&infin;
&minus;&infin; h(x, y)f(y/x)dy
</p>
<p>]
f1(x)dx = EXEY/X [h(X,Y )].
</p>
<p>Example: This law of iterated expectation can be used to show that for the bivariate Normal density,
the parameter ρ is indeed the correlation coefficient of X and Y . In fact, let h(X,Y ) = XY , then
</p>
<p>E(XY ) = EXEY/X(XY/X) = EXXE(Y/X) = EXX[μY + ρ
σ
</p>
<p>Y
</p>
<p>σ
X
</p>
<p>(X &minus; μX)]
</p>
<p>= μXμY + ρ
σ
</p>
<p>Y
</p>
<p>σ
X
</p>
<p>σ2X = μXμY + ρσY σX
</p>
<p>Rearranging terms, one gets ρ = [E(XY )&minus; μXμY ]/σXσY = σXY /σXσY as required.
Another useful result pertains to the unconditional variance of h(X,Y ) being the sum of the mean of
</p>
<p>the conditional variance and the variance of the conditional mean:
</p>
<p>var(h(X,Y )) = EXvarY/X [h(X,Y )] + varXEY/X [h(X,Y )]
</p>
<p>Proof : We will write h(X,Y ) as h to simplify the presentation
</p>
<p>varY/X(h) = EY/X(h
2)&minus; [EY/X(h)]2
</p>
<p>and taking expectations with respect to X yields EXvarY/X(h) = EXEY/X(h
2)&minus; EX [EY/X(h)]2
</p>
<p>= E(h2)&minus; EX [EY/X(h)]2.
Also, varXEY/X(h) = EX [EY/X(h)]
</p>
<p>2 &minus; (EX [EY/X(h)])2 = EX [EY/X(h)]2 &minus; [E(h)]2 adding these two
terms yields
</p>
<p>E(h2)&minus; [E(h)]2 = var(h).</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 3
</p>
<p>Simple Linear Regression
</p>
<p>3.1 Introduction
</p>
<p>In this chapter, we study extensively the estimation of a linear relationship between two vari-
ables, Yi and Xi, of the form:
</p>
<p>Yi = α+ βXi + ui i = 1, 2, . . . , n (3.1)
</p>
<p>where Yi denotes the i-th observation on the dependent variable Y which could be consumption,
investment or output, and Xi denotes the i-th observation on the independent variable X which
could be disposable income, the interest rate or an input. These observations could be collected
on firms or households at a given point in time, in which case we call the data a cross-section.
Alternatively, these observations may be collected over time for a specific industry or country
in which case we call the data a time-series. n is the number of observations, which could be
the number of firms or households in a cross-section, or the number of years if the observations
are collected annually. α and β are the intercept and slope of this simple linear relationship
between Y and X. They are assumed to be unknown parameters to be estimated from the data.
A plot of the data, i.e., Y versus X would be very illustrative showing what type of relationship
exists empirically between these two variables. For example, if Y is consumption and X is
disposable income then we would expect a positive relationship between these variables and
the data may look like Figure 3.1 when plotted for a random sample of households. If α and
β were known, one could draw the straight line (α + βX) as shown in Figure 3.1. It is clear
that not all the observations (Xi, Yi) lie on the straight line (α + βX). In fact, equation (3.1)
states that the difference between each Yi and the corresponding (α+ βXi) is due to a random
error ui. This error may be due to (i) the omission of relevant factors that could influence
consumption, other than disposable income, like real wealth or varying tastes, or unforseen
events that induce households to consume more or less, (ii) measurement error, which could be
the result of households not reporting their consumption or income accurately, or (iii) wrong
choice of a linear relationship between consumption and income, when the true relationship
may be nonlinear. These different causes of the error term will have different effects on the
distribution of this error. In what follows, we consider only disturbances that satisfy some
restrictive assumptions. In later chapters we relax these assumptions to account for more general
kinds of error terms.
In real life, α and β are not known, and have to be estimated from the observed data {(Xi, Yi)
</p>
<p>for i = 1, 2, . . . , n}. This also means that the true line (α+βX) as well as the true disturbances
(the ui&rsquo;s) are unobservable. In this case, α and β could be estimated by the best fitting line
through the data. Different researchers may draw different lines through the same data. What
makes one line better than another? One measure of misfit is the amount of error from the
observed Yi to the guessed line, let us call the latter Ŷi = α̂ + β̂Xi, where the hat (ˆ) denotes
a guess on the appropriate parameter or variable. Each observation (Xi, Yi) will have a cor-
responding observable error attached to it, which we will call ei = Yi &minus; Ŷi, see Figure 3.2. In
other words, we obtain the guessed Yi, (Ŷi) corresponding to each Xi from the guessed line,
</p>
<p>49 
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_3, </p>
<p/>
</div>
<div class="page"><p/>
<p>50 Chapter 3: Simple Linear Regression
</p>
<p>α̂ + β̂Xi. Next, we find our error in guessing that Yi, by subtracting the actual Yi from the
guessed Ŷi. The only difference between Figure 3.1 and Figure 3.2 is the fact that Figure 3.1
draws the true consumption line which is unknown to the researcher, whereas Figure 3.2 is a
guessed consumption line drawn through the data. Therefore, while the ui&rsquo;s are unobservable,
the ei&rsquo;s are observable. Note that there will be n errors for each line, one error corresponding
to every observation.
Similarly, there will be another set of n errors for another guessed line drawn through the
</p>
<p>data. For each guessed line, we can summarize its corresponding errors by one number, the sum
of squares of these errors, which seems to be a natural criterion for penalizing a wrong guess.
Note that a simple sum of these errors is not a good choice for a measure of misfit since positive
errors end up canceling negative errors when both should be counted in our measure. However,
this does not mean that the sum of squared error is the only single measure of misfit. Other
measures include the sum of absolute errors, but this latter measure is mathematically more
difficult to handle. Once the measure of misfit is chosen, α and β could then be estimated by
minimizing this measure. In fact, this is the idea behind least squares estimation.
</p>
<p>3.2 Least Squares Estimation and the Classical Assumptions
</p>
<p>Least squares minimizes the residual sum of squares where the residuals are given by
</p>
<p>ei = Yi &minus; α̂&minus; β̂Xi i = 1, 2, . . . , n
</p>
<p>and α̂ and β̂ denote guesses on the regression parameters α and β, respectively. The residual
sum of squares denoted by RSS =
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i =
</p>
<p>&sum;n
i=1(Yi &minus; α̂ &minus; β̂Xi)2 is minimized by the two
</p>
<p>first-order conditions:
</p>
<p>&part;(
&sum;n
</p>
<p>i=1 e
2
i )/&part;α = &minus;2
</p>
<p>&sum;n
i=1 ei = 0; or
</p>
<p>&sum;n
i=1 Yi &minus; nα̂&minus; β̂
</p>
<p>&sum;n
i=1Xi = 0 (3.2)
</p>
<p>&part;(
&sum;n
</p>
<p>i=1 e
2
i )/&part;β = &minus;2
</p>
<p>&sum;n
i=1 eiXi = 0; or
</p>
<p>&sum;n
i=1 YiXi &minus; α̂
</p>
<p>n&sum;
i=1
</p>
<p>Xi &minus; β̂
&sum;n
</p>
<p>i=1X
2
i = 0 (3.3)
</p>
<p>Solving the least squares normal equations given in (3.2) and (3.3) for α and β one gets
</p>
<p>α̂OLS = Ȳ &minus; β̂OLSX̄ and β̂OLS =
&sum;n
</p>
<p>i=1 xiyi/
&sum;n
</p>
<p>i=1 x
2
i (3.4)
</p>
<p>where Ȳ =
&sum;n
</p>
<p>i=1 Yi/n, X̄ =
&sum;n
</p>
<p>i=1Xi/n, yi = Yi &minus; Ȳ , xi = Xi &minus; X̄,
&sum;n
</p>
<p>i=1 x
2
i =
</p>
<p>&sum;n
i=1X
</p>
<p>2
i &minus; nX̄2,&sum;n
</p>
<p>i=1 y
2
i =
</p>
<p>&sum;n
i=1 Y
</p>
<p>2
i &minus; nȲ 2 and
</p>
<p>&sum;n
i=1 xiyi =
</p>
<p>&sum;n
i=1XiYi &minus; nX̄Ȳ .
</p>
<p>These estimators are subscripted by OLS denoting the ordinary least squares estimators. The
OLS residuals ei = Yi &minus; α̂OLS &minus; β̂OLSXi automatically satisfy the two numerical relationships
given by (3.2) and (3.3). The first relationship states that (i)
</p>
<p>&sum;n
i=1 ei = 0, the residuals sum
</p>
<p>to zero. This is true as long as there is a constant in the regression. This numerical property
of the least squares residuals also implies that the estimated regression line passes through the
sample means (X̄, Ȳ ). To see this, average the residuals, or equation (3.2), this gives immediately
Ȳ = α̂OLS + β̂OLSX̄. The second relationship states that (ii)
</p>
<p>&sum;n
i=1 eiXi = 0, the residuals and
</p>
<p>the explanatory variable are uncorrelated. Other numerical properties that the OLS estimators
satisfy are the following: (iii)
</p>
<p>&sum;n
i=1 Ŷi =
</p>
<p>&sum;n
i=1 Yi and (iv)
</p>
<p>&sum;n
i=1 eiŶi = 0. Property (iii) states
</p>
<p>that the sum of the estimated Yi&rsquo;s or the predicted Yi&rsquo;s from the sample is equal to the sum of the</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Least Squares Estimation and the Classical Assumptions 51
</p>
<p>Y
</p>
<p>iX�� �
</p>
<p>iX�� �
</p>
<p>iX
</p>
<p>+  +
</p>
<p>+ +
</p>
<p>+   +
</p>
<p>+ +
</p>
<p>+
</p>
<p>+  +
</p>
<p>+   +
</p>
<p>+  +
</p>
<p>+
</p>
<p>+
</p>
<p>+   +   
</p>
<p>+   +
</p>
<p>+  +
</p>
<p>+      +
</p>
<p>+ +     +
</p>
<p>+   +
</p>
<p>+
</p>
<p>+
</p>
<p>+  +  +
</p>
<p>+
</p>
<p>Disposable Income
</p>
<p>C
o
</p>
<p>n
su
</p>
<p>m
p
</p>
<p>ti
o
</p>
<p>n
</p>
<p>iY
</p>
<p>X
</p>
<p>iu
</p>
<p>Figure 3.1 &lsquo;True&rsquo; Consumption Function
</p>
<p>Y
</p>
<p>iX��
ˆˆ �
</p>
<p>iX��
ˆˆ �
</p>
<p>iX
Disposable Income
</p>
<p>C
o
</p>
<p>n
su
</p>
<p>m
p
</p>
<p>ti
o
</p>
<p>n
</p>
<p>iY
</p>
<p>X
</p>
<p>ie
</p>
<p>+  +
</p>
<p>+ +
</p>
<p>+   +
</p>
<p>+ +
</p>
<p>+
</p>
<p>+  +
</p>
<p>+   +
</p>
<p>+  +
</p>
<p>+
</p>
<p>+
</p>
<p>+   +   
</p>
<p>+   +
</p>
<p>+  +
</p>
<p>+      +
</p>
<p>+ +     +
</p>
<p>+   +
</p>
<p>+
</p>
<p>+
</p>
<p>+  +  +
</p>
<p>+
</p>
<p>Figure 3.2 Estimated Consumption Function
</p>
<p>actual Yi&rsquo;s. Property (iv) states that the OLS residuals and the predicted Yi&rsquo;s are uncorrelated.
The proof of (iii) and (iv) follow from (i) and (ii) see problem 1. Of course, underlying our
estimation of (3.1) is the assumption that (3.1) is the true model generating the data. In this
case, (3.1) is linear in the parameters α and β, and contains only one explanatory variable
Xi besides the constant. The inclusion of other explanatory variables in the model will be
considered in Chapter 4, and the relaxation of the linearity assumption will be considered in
Chapters 8 and 13. In order to study the statistical properties of the OLS estimators of α and
β, we need to impose some statistical assumptions on the model generating the data.
</p>
<p>Assumption 1: The disturbances have zero mean, i.e., E(ui) = 0 for every i = 1, 2, . . . , n. This
assumption is needed to insure that on the average we are on the true line.
</p>
<p>To see what happens if E(ui) 	= 0, consider the case where households consistently under-report
their consumption by a constant amount of δ dollars, while their income is measured accurately,
say by cross-referencing it with their IRS tax forms. In this case,
</p>
<p>(Observed Consumption) = (True Consumption)&minus; δ
</p>
<p>and our regression equation is really
</p>
<p>(True Consumption)i = α+ β(Income)i + ui
</p>
<p>But we observe,
</p>
<p>(Observed Consumption)i = α+ β(Income)i + ui &minus; δ
</p>
<p>This can be thought of as the old regression equation with a new disturbance term u&lowast;i = ui&minus; δ.
Using the fact that δ &gt; 0 and E(ui) = 0, one gets E(u
</p>
<p>&lowast;
i ) = &minus;δ &lt; 0. This says that for
</p>
<p>all households with the same income, say $20, 000, their observed consumption will be on the
average below that predicted from the true line [α+β($20, 000)] by an amount δ. Fortunately, one</p>
<p/>
</div>
<div class="page"><p/>
<p>52 Chapter 3: Simple Linear Regression
</p>
<p>can deal with this problem of constant but non-zero mean of the disturbances by reparametizing
the model as
</p>
<p>(Observed Consumption)i = α
&lowast; + β(Income)i + ui
</p>
<p>where α&lowast; = α&minus; δ. In this case, E(ui) = 0 and α&lowast; and β can be estimated from the regression.
Note that while α&lowast; is estimable, α and δ are non-estimable. Also note that for all $20, 000
income households, their average consumption is [(α&minus; δ) + β($20, 000)].
Assumption 2: The disturbances have a constant variance, i.e., var(ui) = σ
</p>
<p>2 for every i =
1, 2, . . . , n. This insures that every observation is equally reliable.
</p>
<p>To see what this assumption means, consider the case where var(ui) = σ
2
i , for i = 1, 2, . . . , n.
</p>
<p>In this case, each observation has a different variance. An observation with a large variance is
less reliable than one with a smaller variance. But, how can this differing variance happen? In
the case of consumption, households with large disposable income (a large Xi, say $100, 000)
may be able to save more (or borrow more to spend more) than households with smaller income
(a small Xi, say $10, 000). In this case, the variation in consumption for the $100, 000 income
household will be much larger than that for the $10, 000 income household. Therefore, the
corresponding variance for the $100, 000 observation will be larger than that for the $10, 000
observation. Consequences of different variances for different observations will be studied more
rigorously in Chapter 5.
</p>
<p>Assumption 3: The disturbances are not correlated, i.e., E(uiuj) = 0 for i 	= j, i, j = 1, 2, . . . , n.
Knowing the i-th disturbance does not tell us anything about the j-th disturbance, for i 	= j.
For the consumption example, the unforseen disturbance which caused the i-th household to
consume more, (like a visit of a relative), has nothing to do with the unforseen disturbances of
any other household. This is likely to hold for a random sample of households. However, it is
less likely to hold for a time series study of consumption for the aggregate economy, where a
disturbance in 1945, a war year, is likely to affect consumption for several years after that. In
this case, we say that the disturbance in 1945 is related to the disturbances in 1946, 1947, and
so on. Consequences of correlated disturbances will be studied in Chapter 5.
</p>
<p>Assumption 4: The explanatory variable X is nonstochastic, i.e., fixed in repeated samples,
and hence, not correlated with the disturbances. Also,
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i /n 	= 0 and has a finite limit as
</p>
<p>n tends to infinity.
</p>
<p>This assumption states that we have at least two distinct values for X. This makes sense, since
we need at least two distinct points to draw a straight line. Otherwise X̄ = X, the common
value, and x = X &minus; X̄ = 0, which violates &sum;ni=1 x2i 	= 0. In practice, one always has several
distinct values of X. More importantly, this assumption implies that X is not a random variable
and hence is not correlated with the disturbances.
</p>
<p>In section 5.3, we will relax the assumption of a non-stochastic X. Basically, X becomes
a random variable and our assumptions have to be recast conditional on the set of X&rsquo;s that
are observed. This is the more realistic case with economic data. The zero mean assumption
becomes E(ui/X) = 0, the constant variance assumption becomes var(ui/X) = σ
</p>
<p>2, the no serial
correlation assumption becomes E(uiuj/X) = 0 for i 	= j. The conditional expectation here is
with respect to every observation on Xi from i = 1, 2, . . . n. Of course, one can show that if
E(ui/X) = 0 for all i, then Xi and ui are not correlated. The reverse is not necessarily true, see</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Least Squares Estimation and the Classical Assumptions 53
</p>
<p>Y
</p>
<p>+  +  
</p>
<p>+ +  +   +
</p>
<p>+   +
</p>
<p>+  +
</p>
<p>+  +
</p>
<p>+  +
</p>
<p>+  + + +
</p>
<p>+ +  +    +
</p>
<p>+        +
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>++  +
</p>
<p>+  +  + +
</p>
<p>+ +
</p>
<p>+  +
</p>
<p>+
</p>
<p>+  +
</p>
<p>X�� �
</p>
<p>Disposable Income 
x�
</p>
<p>C
o
</p>
<p>n
su
</p>
<p>m
p
</p>
<p>ti
o
</p>
<p>n
</p>
<p>X
</p>
<p>Figure 3.3 Consumption Function with Cov(X,u) &gt; 0
</p>
<p>problem 3 of Chapter 2. That problem shows that two random variables, say ui and Xi could be
uncorrelated, i.e., not linearly related when in fact they are nonlinearly related with ui = X
</p>
<p>2
i .
</p>
<p>Hence, E(ui/Xi) = 0 is a stronger assumption than ui and Xi are not correlated. By the law of
iterated expectations given in the Appendix of Chapter 2, E(ui/X) = 0 implies that E(ui) = 0.
It also implies that ui is uncorrelated with any function of Xi. This is a stronger assumption
than ui is uncorrelated with Xi. Therefore, conditional on Xi, the mean of the disturbances is
zero and does not depend on Xi. In this case, E(Yi/Xi) = α+ βXi is linear in α and β and is
assumed to be the true conditional mean of Y given X.
</p>
<p>To see what a violation of assumption 4 means, suppose that X is a random variable and that
X and u are positively correlated, then in the consumption example, households with income
above the average income will be associated with disturbances above their mean of zero, and
hence positive disturbances. Similarly, households with income below the average income will be
associated with disturbances below their mean of zero, and hence negative disturbances. This
means that the disturbances are systematically affected by values of the explanatory variable
and the scatter of the data will look like Figure 3.3. Note that if we now erase the true line
(α+ βX), and estimate this line from the data, the least squares line drawn through the data
is going to have a smaller intercept and a larger slope than those of the true line. The scatter
should look like Figure 3.4 where the disturbances are random variables, not correlated with
the Xi&rsquo;s, drawn from a distribution with zero mean and constant variance. Assumptions 1 and
4 insure that E(Yi/Xi) = α+βXi, i.e., on the average we are on the true line. Several economic
models will be studied where X and u are correlated. The consequences of this correlation will
be studied in Chapters 5 and 11.
We now generate a data set which satisfies all four classical assumptions. Let α and β take
</p>
<p>the arbitrary values, say 10 and 0.5 respectively, and consider a set of 20 fixed X&rsquo;s, say income
classes from $10 to $105 (in thousands of dollars), in increments of $5, i.e., $10, $15, $20,
$25, . . . , $105. Our consumption variable Yi is constructed as (10 + 0.5Xi + ui) where ui is a</p>
<p/>
</div>
<div class="page"><p/>
<p>54 Chapter 3: Simple Linear Regression
</p>
<p>)(uf
</p>
<p>X
</p>
<p>Y
</p>
<p>X�� �
</p>
<p>1X
</p>
<p>2X
</p>
<p>nX
. . .
</p>
<p>Figure 3.4 Random Disturbances Around the Regression
</p>
<p>disturbance which is a random draw from a distribution with zero mean and constant variance,
say σ2 = 9. Computers generate random numbers with various distributions.
In this case, Figure 3.4 would depict our data, with the true line being (10 + 0.5X) and ui
</p>
<p>being random draws from the computer which are by construction independent and identically
distributed with mean zero and variance 9. For every set of 20 ui&rsquo;s randomly generated, given
the fixed Xi&rsquo;s, we obtain a corresponding set of 20 Yi&rsquo;s from our linear regression model. This is
what we mean in assumption 4 when we say that the X&rsquo;s are fixed in repeated samples. Monte
Carlo experiments generate a large number of samples, say a 1000, in the fashion described
above. For each data set generated, least squares can be performed and the properties of the
resulting estimators which are derived analytically in the remainder of this chapter, can be
verified. For example, the average of the 1000 estimates of α and β can be compared to their
true values to see whether these least squares estimates are unbiased. Note what will happen to
Figure 3.4 if E(ui) = &minus;δ where δ &gt; 0, or var(ui) = σ2i for i = 1, 2, . . . , n. In the first case, the
mean of f(u), the probability density function of u, will shift off the true line (10+0.5X) by &minus;δ.
In other words, we can think of the distributions of the ui&rsquo;s, shown in Figure 3.4 , being centered
on a new imaginary line parallel to the true line but lower by a distance δ. This means that one
is more likely to draw negative disturbances than positive disturbances, and the observed Yi&rsquo;s
are more likely to be below the true line than above it. In the second case, each f(ui) will have
a different variance, hence the spread of this probability density function will vary with each
observation. In this case, Figure 3.4 will have a distribution for the ui&rsquo;s which has a different
spread for each observation. In other words, if the ui&rsquo;s are say normally distributed, then u1 is
drawn from a N(0, σ21) distribution, whereas u2 is drawn from a N(0, σ
</p>
<p>2
2) distribution, and so
</p>
<p>on. Violation of the classical assumptions can also be studied using Monte Carlo experiments,
see Chapter 5.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Statistical Properties of Least Squares 55
</p>
<p>3.3 Statistical Properties of Least Squares
</p>
<p>(i) Unbiasedness
</p>
<p>Given assumptions 1&ndash;4, it is easy to show that β̂OLS is unbiased for β. In fact, using equation
(3.4) one can write
</p>
<p>β̂OLS =
&sum;n
</p>
<p>i=1 xiyi/
&sum;n
</p>
<p>i=1 x
2
i =
</p>
<p>&sum;n
i=1 xiYi/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i = β +
</p>
<p>&sum;n
i=1 xiui/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i (3.5)
</p>
<p>where the second equality follows from the fact that yi = Yi&minus;Ȳ and
&sum;n
</p>
<p>i=1 xiȲ = Ȳ
&sum;n
</p>
<p>i=1 xi = 0.
The third equality follows from substituting Yi from (3.1) and using the fact that
</p>
<p>&sum;n
i=1 xi = 0.
</p>
<p>Taking expectations of both sides of (3.5) and using assumptions 1 and 4, one can show that
E(β̂OLS) = β. Furthermore, one can derive the variance of β̂OLS from (3.5) since
</p>
<p>var(β̂OLS) = E(β̂OLS &minus; β)2 = E(
&sum;n
</p>
<p>i=1 xiui/
&sum;n
</p>
<p>i=1 x
2
i )
</p>
<p>2 (3.6)
</p>
<p>= var(
&sum;n
</p>
<p>i=1 xiui/
&sum;n
</p>
<p>i=1 x
2
i ) = σ
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i
</p>
<p>where the last equality uses assumptions 2 and 3, i.e., that the ui&rsquo;s are not correlated with each
other and that their variance is constant, see problem 4. Note that the variance of the OLS
estimator of β depends upon σ2, the variance of the disturbances in the true model, and on
the variation in X. The larger the variation in X the larger is
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i and the smaller is the
</p>
<p>variance of β̂OLS .
</p>
<p>(ii) Consistency
</p>
<p>Next, we show that β̂OLS is consistent for β. A sufficient condition for consistency is that β̂OLS
is unbiased and its variance tends to zero as n tends to infinity. We have already shown β̂OLS
to be unbiased, it remains to show that its variance tends to zero as n tends to infinity.
</p>
<p>lim
n&rarr;&infin;
</p>
<p>var(β̂OLS) = limn&rarr;&infin;
[(σ2/n)/(
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i /n)] = 0
</p>
<p>where the second equality follows from the fact that (σ2/n) &rarr; 0 and (
&sum;n
</p>
<p>i=1 x
2
i /n) 	= 0 and has
</p>
<p>a finite limit, see assumption 4. Hence, plim β̂OLS = β and β̂OLS is consistent for β. Similarly
one can show that α̂OLS is unbiased and consistent for α with variance σ
</p>
<p>2
&sum;n
</p>
<p>i=1X
2
i /n
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i ,
</p>
<p>and cov(α̂OLS , β̂OLS) = &minus;X̄σ2/
&sum;n
</p>
<p>i=1 x
2
i , see problem 5.
</p>
<p>(iii) Best Linear Unbiased
</p>
<p>Using (3.5) one can write β̂OLS as
&sum;n
</p>
<p>i=1wiYi where wi = xi/
&sum;n
</p>
<p>i=1 x
2
i . This proves that β̂OLS
</p>
<p>is a linear combination of the Yi&rsquo;s, with weights wi satisfying the following properties:
</p>
<p>&sum;n
i=1wi = 0;
</p>
<p>&sum;n
i=1wiXi = 1;
</p>
<p>&sum;n
i=1w
</p>
<p>2
i = 1/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i (3.7)
</p>
<p>The next theorem shows that among all linear unbiased estimators of β, it is β̂OLS which has
the smallest variance. This is known as the Gauss-Markov Theorem.
</p>
<p>Theorem 1: Consider any arbitrary linear estimator β̃ =
&sum;n
</p>
<p>i=1 aiYi for β, where the ai&rsquo;s denote
</p>
<p>arbitrary constants. If β̃ is unbiased for β, and assumptions 1 to 4 are satisfied, then var(β̃) &ge;
var(β̂OLS).</p>
<p/>
</div>
<div class="page"><p/>
<p>56 Chapter 3: Simple Linear Regression
</p>
<p>Proof: Substituting Yi from (3.1) into β̃, one gets β̃ = α
&sum;n
</p>
<p>i=1 ai+β
&sum;n
</p>
<p>i=1 aiXi+
&sum;n
</p>
<p>i=1 aiui. For β̃
</p>
<p>to be unbiased for β it must follow that E(β̃) = α
&sum;n
</p>
<p>i=1 ai+β
&sum;n
</p>
<p>i=1 aiXi = β for all observations
i = 1, 2, . . . , n. This means that
</p>
<p>&sum;n
i=1 ai = 0 and
</p>
<p>&sum;n
i=1 aiXi = 1 for all i = 1, 2, . . . , n. Hence,
</p>
<p>β̃ = β +
&sum;n
</p>
<p>i=1 aiui with var(β̃) = var(
&sum;n
</p>
<p>i=1 aiui) = σ
2
&sum;n
</p>
<p>i=1 a
2
i where the last equality follows
</p>
<p>from assumptions 2 and 3. But the ai&rsquo;s are constants which differ from the wi&rsquo;s, the weights of
the OLS estimator, by some other constants, say di&rsquo;s, i.e., ai = wi+ di for i = 1, 2, . . . , n. Using
the properties of the ai&rsquo;s and wi one can deduce similar properties on the di&rsquo;s i.e.,
</p>
<p>&sum;n
i=1 di = 0
</p>
<p>and
&sum;n
</p>
<p>i=1 diXi = 0. In fact,
</p>
<p>&sum;n
i=1 a
</p>
<p>2
i =
</p>
<p>&sum;n
i=1 d
</p>
<p>2
i +
</p>
<p>&sum;n
i=1w
</p>
<p>2
i + 2
</p>
<p>&sum;n
i=1widi
</p>
<p>where
&sum;n
</p>
<p>i=1widi =
&sum;n
</p>
<p>i=1 xidi/
&sum;n
</p>
<p>i=1 x
2
i = 0. This follows from the definition of wi and the fact
</p>
<p>that
&sum;n
</p>
<p>i=1 di =
&sum;n
</p>
<p>i=1 diXi = 0. Hence,
</p>
<p>var(β̃) = σ2
&sum;n
</p>
<p>i=1 a
2
i = σ
</p>
<p>2&sum;n
i=1 d
</p>
<p>2
i + σ
</p>
<p>2&sum;n
i=1w
</p>
<p>2
i = var(β̂OLS) + σ
</p>
<p>2&sum;n
i=1 d
</p>
<p>2
i
</p>
<p>Since σ2
&sum;n
</p>
<p>i=1 d
2
i is non-negative, this proves that var(β̃) &ge; var(β̂OLS) with the equality holding
</p>
<p>only if di = 0 for all i = 1, 2, . . . , n, i.e., only if ai = wi, in which case β̃ reduces to β̂OLS .
Therefore, any linear estimator of β, like β̃ that is unbiased for β has variance at least as large
as var(β̂OLS). This proves that β̂OLS is BLUE, Best among all Linear Unbiased Estimators of
β.
Similarly, one can show that α̂OLS is linear in Yi and has the smallest variance among all
</p>
<p>linear unbiased estimators of α, if assumptions 1 to 4 are satisfied, see problem 6. This result
implies that the OLS estimator of α is also BLUE.
</p>
<p>3.4 Estimation of σ2
</p>
<p>The variance of the regression disturbances σ2 is unknown and has to be estimated. In fact,
both the variance of β̂OLS and that of α̂OLS depend upon σ
</p>
<p>2, see (3.6) and problem 5. An
unbiased estimator for σ2 is s2 =
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i /(n&minus; 2). To prove this, we need the fact that
</p>
<p>ei = Yi &minus; α̂OLS &minus; β̂OLSXi = yi &minus; β̂OLSxi = (β &minus; β̂OLS)xi + (ui &minus; ū)
</p>
<p>where ū =
&sum;n
</p>
<p>i=1 ui/n. The second equality substitutes α̂OLS = Ȳ &minus; β̂OLSX̄ and the third
equality substitutes yi = βxi + (ui &minus; ū). Hence,
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i = (β̂OLS &minus; β)2
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i +
</p>
<p>&sum;n
i=1(ui &minus; ū)2 &minus; 2(β̂OLS &minus; β)
</p>
<p>&sum;n
i=1 xi(ui &minus; ū),
</p>
<p>and
</p>
<p>E(
&sum;n
</p>
<p>i=1 e
2
i ) =
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i var(β̂OLS) + (n&minus; 1)σ2 &minus; 2E(
</p>
<p>&sum;n
i=1 xiui)
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i
</p>
<p>= σ2 + (n&minus; 1)σ2 &minus; 2σ2 = (n&minus; 2)σ2
</p>
<p>where the first equality uses the fact that E(
&sum;n
</p>
<p>i=1(ui &minus; ū)2) = (n &minus; 1)σ2 and β̂OLS &minus; β =
&sum;n
</p>
<p>i=1 xiui/
&sum;n
</p>
<p>i=1 x
2
i . The second equality uses the fact that var(β̂OLS) = σ
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i and
</p>
<p>E(
&sum;n
</p>
<p>i=1 xiui)
2 = σ2
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i .</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Maximum Likelihood Estimation 57
</p>
<p>Therefore, E(s2) = E(
&sum;n
</p>
<p>i=1 e
2
i /(n&minus; 2)) = σ2.
</p>
<p>Intuitively, the estimator of σ2 could be obtained from
&sum;n
</p>
<p>i=1(ui &minus; ū)2/(n &minus; 1) if the true
disturbances were known. Since the u&rsquo;s are not known, consistent estimates of them are used.
These are the ei&rsquo;s. Since
</p>
<p>&sum;n
i=1 ei = 0, our estimator of σ
</p>
<p>2 becomes
&sum;n
</p>
<p>i=1 e
2
i /(n &minus; 1). Taking
</p>
<p>expectations we find that the correct divisor ought to be (n&minus;2) and not (n&minus;1) for this estimator
to be unbiased for σ2. This is plausible, since we have estimated two parameters α and β in
obtaining the ei&rsquo;s, and there are only n&minus; 2 independent pieces of information left in the data.
To prove this fact, consider the OLS normal equations given in (3.2) and (3.3). These equations
represent two relationships involving the ei&rsquo;s. Therefore, knowing (n &minus; 2) of the ei&rsquo;s we can
deduce the remaining two ei&rsquo;s from (3.2) and (3.3).
</p>
<p>3.5 Maximum Likelihood Estimation
</p>
<p>Assumption 5: The ui&rsquo;s are independent and identically distributed N(0, σ
2).
</p>
<p>This assumption allows us to derive distributions of estimators and other test statistics. In
fact using (3.5) one can easily see that β̂OLS is a linear combination of the ui&rsquo;s. But, a linear
combination of normal random variables is itself a normal random variable, see Chapter 2,
problem 15. Hence, β̂OLS is N(β, σ
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i ). Similarly α̂OLS is N(α, σ
</p>
<p>2
&sum;n
</p>
<p>i=1X
2
i /n
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i ),
</p>
<p>and Yi is N(α+βXi, σ
2). Moreover, we can write the joint probability density function of the u&rsquo;s
</p>
<p>as f(u1, u2, . . . , un;α, β, σ
2) = (1/2πσ2)n/2exp(&minus;
</p>
<p>&sum;n
i=1 u
</p>
<p>2
i /2σ
</p>
<p>2). To get the likelihood function
we make the transformation ui = Yi&minus;α&minus;βXi and note that the Jacobian of the transformation
is 1. Therefore,
</p>
<p>f(Y1, Y2, . . . , Yn;α, β, σ
2) = (1/2πσ2)n/2exp{&minus;
</p>
<p>&sum;n
i=1(Yi &minus; α&minus; βXi)2/2σ2} (3.8)
</p>
<p>Taking the log of this likelihood, we get
</p>
<p>logL(α, β, σ2) = &minus;(n/2)log(2πσ2)&minus;
&sum;n
</p>
<p>i=1(Yi &minus; α&minus; βXi)2/2σ2 (3.9)
</p>
<p>Maximizing this likelihood with respect to α, β and σ2 one gets the maximum likelihood esti-
mators (MLE). However, only the second term in the log likelihood contains α and β and that
term (without the negative sign) has already been minimized with respect to α and β in (3.2)
and (3.3) giving us the OLS estimators. Hence, α̂MLE = α̂OLS and β̂MLE = β̂OLS . Similarly,
by differentiating logL with respect to σ2 and setting this derivative equal to zero one gets
σ̂2MLE =
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i /n, see problem 7. Note that this differs from s
</p>
<p>2 only in the divisor. In fact,
E(σ̂2MLE) = (n &minus; 2)σ2/n 	= σ2. Hence, σ̂2MLE is biased but note that it is still asymptotically
unbiased.
So far, the gains from imposing assumption 5 are the following: The likelihood can be formed,
</p>
<p>maximum likelihood estimators can be derived, and distributions can be obtained for these
estimators. One can also derive the Cramér-Rao lower bound for unbiased estimators of the
parameters and show that the α̂OLS and β̂OLS attain this bound whereas s
</p>
<p>2 does not. This
derivation is postponed until Chapter 7. In fact, one can show following the theory of complete
sufficient statistics that α̂OLS , β̂OLS and s
</p>
<p>2 are minimum variance unbiased estimators for α, β
and σ2, see Chapter 2. This is a stronger result (for α̂OLS and β̂OLS) than that obtained using
the Gauss-Markov Theorem. It says, that among all unbiased estimators of α and β, the OLS</p>
<p/>
</div>
<div class="page"><p/>
<p>58 Chapter 3: Simple Linear Regression
</p>
<p>estimators are the best. In other words, our set of estimators include now all unbiased estimators
and not just linear unbiased estimators. This stronger result is obtained at the expense of a
stronger distributional assumption, i.e., normality. If the distribution of the disturbances is not
normal, then OLS is no longer MLE. In this case, MLE will be more efficient than OLS as
long as the distribution of the disturbances is correctly specified. Some of the advantages and
disadvantages of MLE were discussed in Chapter 2.
</p>
<p>We found the distributions of α̂OLS , β̂OLS , now we give that of s
2. In Chapter 7, it is shown
</p>
<p>that
&sum;n
</p>
<p>i=1 e
2
i /σ
</p>
<p>2 is a chi-squared with (n &minus; 2) degrees of freedom. Also, s2 is independent of
α̂OLS and β̂OLS . This is useful for test of hypotheses. In fact, the major gain from assumption
5 is that we can perform test of hypotheses.
Standardizing the normal random variable β̂OLS , one gets z = (β̂OLS &minus;β)/(σ2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )
</p>
<p>1
2 &sim;
</p>
<p>N(0, 1). Also, (n &minus; 2)s2/σ2 is distributed as χ2n&minus;2. Hence, one can divide z, a N(0, 1) random
variable, by the square root of (n &minus; 2)s2/σ2 divided by its degrees of freedom (n &minus; 2) to
get a t-statistic with (n &minus; 2) degrees of freedom. The resulting statistic is tobs = (β̂OLS &minus;
β)/(s2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )
</p>
<p>1
2 &sim; tn&minus;2, see problem 8. This statistic can be used to test H0;β = β0, versus
</p>
<p>H1;β 	= β0, where β0 is a known constant. Under H0, tobs can be calculated and its value can be
compared to a critical value from a t-distribution with (n&minus;2) degrees of freedom, at a specified
critical value of α%. Of specific interest is the hypothesis H0;β = 0, which states that there is
no linear relationship between Yi and Xi. Under H0,
</p>
<p>tobs = β̂OLS/(s
2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )
</p>
<p>1
2 = β̂OLS/ŝe(β̂OLS)
</p>
<p>where ŝe(β̂OLS) = (s
2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )
</p>
<p>1
2 . If |tobs| &gt; tα/2;n&minus;2, then H0 is rejected at the α% significance
</p>
<p>level. tα/2;n&minus;2 represents a critical value obtained from a t-distribution with n &minus; 2 degrees of
freedom. It is determined such that the area to its right under a tn&minus;2 distribution is equal to
α/2.
</p>
<p>Similarly one can get a confidence interval for β by using the fact that, Pr[&minus;tα/2;n&minus;2 &lt; tobs &lt;
tα/2;n&minus;2] = 1 &minus; α and substituting for tobs its value derived above as (β̂OLS &minus; β)/ŝe(β̂OLS).
Since the critical values are known, β̂OLS and ŝe(β̂OLS) can be calculated from the data, the
following (1&minus; α)% confidence interval for β emerges
</p>
<p>β̂OLS &plusmn; tα/2;n&minus;2ŝe(β̂OLS).
</p>
<p>Tests of hypotheses and confidence intervals on α and σ2 can be similarly constructed using the
normal distribution of α̂OLS and the χ
</p>
<p>2
n&minus;2 distribution of (n&minus; 2)s2/σ2.
</p>
<p>3.6 A Measure of Fit
</p>
<p>We have obtained the least squares estimates of α, β and σ2 and found their distributions
under normality of the disturbances. We have also learned how to test hypotheses regarding
these parameters. Now we turn to a measure of fit for this estimated regression line. Recall, that
ei = Yi&minus; Ŷi where Ŷi denotes the predicted Yi from the least squares regression line at the value
Xi, i.e., α̂OLS + β̂OLSXi. Using the fact that
</p>
<p>&sum;n
i=1 ei = 0, we deduce that
</p>
<p>&sum;n
i=1 Yi =
</p>
<p>&sum;n
i=1 Ŷi,
</p>
<p>and therefore, Ȳ = Ŷ . The actual and predicted values of Y have the same sample mean, see
numerical properties (i) and (iii) of the OLS estimators discussed in section 2. This is true</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 A Measure of Fit 59
</p>
<p>as long as there is a constant in the regression. Adding and subtracting Ȳ from ei, we get
ei = yi &minus; ŷi, or yi = ei + ŷi. Squaring and summing both sides:
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i =
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i +
</p>
<p>&sum;n
i=1 ŷ
</p>
<p>2
i + 2
</p>
<p>&sum;n
i=1 eiŷi =
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i +
</p>
<p>&sum;n
i=1 ŷ
</p>
<p>2
i (3.10)
</p>
<p>where the last equality follows from the fact that ŷi = β̂OLSxi and
&sum;n
</p>
<p>i=1 eixi = 0. In fact,
</p>
<p>&sum;n
i=1 eiŷi =
</p>
<p>&sum;n
i=1 eiŶi = 0
</p>
<p>means that the OLS residuals are uncorrelated with the predicted values from the regression,
see numerical properties (ii) and (iv) of the OLS estimates discussed in section 3.2. In other
words, (3.10) says that the total variation in Yi, around its sample mean Ȳ i.e.,
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i , can be
</p>
<p>decomposed into two parts: the first is the regression sums of squares
&sum;n
</p>
<p>i=1 ŷ
2
i = β̂
</p>
<p>2
</p>
<p>OLS
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i ,
</p>
<p>and the second is the residual sums of squares
&sum;n
</p>
<p>i=1 e
2
i . In fact, regressing Y on a constant
</p>
<p>yields α̃OLS = Ȳ , see problem 2, and the unexplained residual sums of squares of this naive
model is
</p>
<p>&sum;n
i=1(Yi &minus; α̃OLS)2 =
</p>
<p>&sum;n
i=1(Yi &minus; Ȳ )2 =
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i .
</p>
<p>Therefore,
&sum;n
</p>
<p>i=1 ŷ
2
i in (3.10) gives the explanatory power of X after the constant is fit.
</p>
<p>Using this decomposition, one can define the explanatory power of the regression as the
ratio of the regression sums of squares to the total sums of squares. In other words, define
R2 =
</p>
<p>&sum;n
i=1 ŷ
</p>
<p>2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i and this value is clearly between 0 and 1. In fact, dividing (3.10) by&sum;n
</p>
<p>i=1 y
2
i one gets R
</p>
<p>2 = 1 &minus;
&sum;n
</p>
<p>i=1 e
2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i . The
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i is a measure of misfit which was
</p>
<p>minimized by least squares. If
&sum;n
</p>
<p>i=1 e
2
i is large, this means that the regression is not explaining
</p>
<p>a lot of the variation in Y and hence, the R2 value would be small. Alternatively, if the
&sum;n
</p>
<p>i=1 e
2
i
</p>
<p>is small, then the fit is good and R2 is large. In fact, for a perfect fit, where all the observations
lie on the fitted line, Yi = Ŷi and ei = 0, which means that
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i = 0 and R
</p>
<p>2 = 1. The other
extreme case is where the regression sums of squares
</p>
<p>&sum;n
i=1 ŷ
</p>
<p>2
i = 0. In other words, the linear
</p>
<p>regression explains nothing of the variation in Yi. In this case,
&sum;n
</p>
<p>i=1 y
2
i =
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i and R
</p>
<p>2 = 0.
</p>
<p>Note that since
&sum;n
</p>
<p>i=1 ŷ
2
i = 0 implies ŷi = 0 for every i, which in turn means that Ŷi = Ȳ for
</p>
<p>every i. The fitted regression line is a horizontal line drawn at Y = Ȳ , and the independent
variable X does not have any explanatory power in a linear relationship with Y .
</p>
<p>Note that R2 has two alternative meanings: (i) It is the simple squared correlation coefficient
between Yi and Ŷi, see problem 9. Also, for the simple regression case, (ii) it is the simple
squared correlation between X and Y . This means that before one runs the regression of Y on
X, one can compute r2xy which in turn tells us the proportion of the variation in Y that will
be explained by X. If this number is pretty low, we have a weak linear relationship between Y
and X and we know that a poor fit will result if Y is regressed on X. It is worth emphasizing
that R2 is a measure of linear association between Y and X. There could exist, for example, a
perfect quadratic relationship between X and Y , yet the estimated least squares line through
the data is a flat line implying that R2 = 0, see problem 3 of Chapter 2. One should also be
suspicious of least squares regressions with R2 that are too close to 1. In some cases, we may
not want to include a constant in the regression. In such cases, one should use an uncentered
R2 as a measure fit. The appendix to this chapter defines both centered and uncentered R2 and
explains the difference between them.</p>
<p/>
</div>
<div class="page"><p/>
<p>60 Chapter 3: Simple Linear Regression
</p>
<p>3.7 Prediction
</p>
<p>Let us now predict Y0 given X0. Usually this is done for a time series regression, where the
researcher is interested in predicting the future, say one period ahead. This new observation Y0
is generated by (3.1), i.e.,
</p>
<p>Y0 = α+ βX0 + u0 (3.11)
</p>
<p>What is the Best Linear Unbiased Predictor (BLUP) of E(Y0)? From (3.11), E(Y0) = α+ βX0
is a linear combination of α and β. Using the Gauss-Markov result, Ŷ0 = α̂OLS + β̂OLSX0 is
BLUE for α+βX0 and the variance of this predictor of E(Y0) is σ
</p>
<p>2[(1/n)+(X0&minus;X̄)2/
&sum;n
</p>
<p>i=1 x
2
i ],
</p>
<p>see problem 10. But, what if we are interested in the BLUP for Y0 itself ? Y0 differs from E(Y0)
by u0, and the best predictor of u0 is zero, so the BLUP for Y0 is still Ŷ0. The forecast error is
</p>
<p>Y0 &minus; Ŷ0 = [Y0 &minus; E(Y0)] + [E(Y0)&minus; Ŷ0] = u0 + [E(Y0)&minus; Ŷ0]
</p>
<p>where u0 is the error committed even if the true regression line is known, and E(Y0) &minus; Ŷ0 is
the difference between the sample and population regression lines. Hence, the variance of the
forecast error becomes:
</p>
<p>var(u0) + var[E(Y0)&minus; Ŷ0] + 2cov[u0, E(Y0)&minus; Ŷ0] = σ2[1 + (1/n) + (X0 &minus; X̄)2/
&sum;n
</p>
<p>i=1 x
2
i ]
</p>
<p>This says that the variance of the forecast error is equal to the variance of the predictor of
E(Y0) plus the var(u0) plus twice the covariance of the predictor of E(Y0) and u0. But, this last
covariance is zero, since u0 is a new disturbance and is not correlated with the disturbances in the
sample upon which Ŷi is based. Therefore, the predictor of the average consumption of a $20, 000
income household is the same as the predictor of consumption for a specific household whose
income is $20, 000. The difference is not in the predictor itself but in the variance attached
to it. The latter variance being larger only by σ2, the variance of u0. The variance of the
predictor therefore, depends upon σ2, the sample size, the variation in the X&rsquo;s, and how far X0
is from the sample mean of the observed data. To summarize, the smaller σ2 is, the larger n
and
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i are, and the closer X0 is to X̄, the smaller is the variance of the predictor. One
</p>
<p>can construct 95% confidence intervals to these predictions for every value of X0. In fact, this
is (α̂OLS + β̂OLSX0) &plusmn; t.025;n&minus;2{s[1 + (1/n) + (X0 &minus; X̄)2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i ]
</p>
<p>1
2 } where s replaces σ, and
</p>
<p>t.025;n&minus;2 represents the 2.5% critical value obtained from a t-distribution with n&minus; 2 degrees of
freedom. Figure 3.5 shows this confidence band around the estimated regression line. This is a
hyperbola which is the narrowest at X̄ as expected, and widens as we predict away from X̄.
</p>
<p>3.8 Residual Analysis
</p>
<p>A plot of the residuals of the regression is very important. The residuals are consistent estimates
of the true disturbances. But unlike the ui&rsquo;s, these ei&rsquo;s are not independent. In fact, the OLS
normal equations (3.2)and (3.3) give us two relationships between these residuals. Therefore,
knowing (n &minus; 2) of these residuals the remaining two residuals can be deduced. If we had the
true ui&rsquo;s, and we plotted them, they should look like a random scatter around the horizontal
axis with no specific pattern to them. A plot of the ei&rsquo;s that shows a certain pattern like a set
of positive residuals followed by a set of negative residuals as shown in Figure 3.6(a) may be</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Residual Analysis 61
</p>
<p>Figure 3.5 95% Confidence Bands
</p>
<p>indicative of a violation of one of the 5 assumptions imposed on the model, or simply indicating
a wrong functional form. For example, if assumption 3 is violated, so that the ui&rsquo;s are say
positively correlated, then it is likely to have a positive residual followed by a positive one, and
a negative residual followed by a negative one, as observed in Figure 3.6(b). Alternatively, if
we fit a linear regression line to a true quadratic relation between Y and X, then a scatter
of residuals like that in Figure 3.6(c) will be generated. We will study how to deal with this
violation and how to test for it in Chapter 5.
Large residuals are indicative of bad predictions in the sample. A large residual could be
</p>
<p>a typo, where the researcher entered this observation wrongly. Alternatively, it could be an
influential observation, or an outlier which behaves differently from the other data points in
the sample and therefore, is further away from the estimated regression line than the other
data points. The fact that OLS minimizes the sum of squares of these residuals means that a
large weight is put on this observation and hence it is influential. In other words, removing this
observation from the sample may change the estimates and the regression line significantly. For
more on the study of influential observations, see Belsely, Kuh and Welsch (1980). We will focus
on this issue in Chapter 8 of this book.
</p>
<p>Figure 3.6 Positively Correlated Residuals
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+ +
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>Y
</p>
<p>X
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>Y
</p>
<p>X
</p>
<p>+
+
</p>
<p>+ +
+
</p>
<p>ii XY
ˆˆˆ
</p>
<p>ii XY
ˆˆˆ
</p>
<p>ii XY
ˆˆˆ
</p>
<p>&gt;
</p>
<p>(c)(b)
</p>
<p>iY iY
</p>
<p>iX
0
</p>
<p>(a) 
</p>
<p>ie
</p>
<p>iX&gt;
+ +
</p>
<p>-
</p>
<p>_
</p>
<p>+
</p>
<p>&gt;
</p>
<p>+
</p>
<p>)( ii XfY
</p>
<p>iX</p>
<p/>
</div>
<div class="page"><p/>
<p>62 Chapter 3: Simple Linear Regression
</p>
<p>Figure 3.7 Residual Variation Growing with X
</p>
<p>One can also plot the residuals versus the Xi&rsquo;s. If a pattern like Figure 3.7 emerges, this could be
indicative of a violation of assumption 2 because the variation of the residuals is growing with
Xi when it should be constant for all observations. Alternatively, it could imply a relationship
between the Xi&rsquo;s and the true disturbances which is a violation of assumption 4.
</p>
<p>In summary, one should always plot the residuals to check the data, identify influential obser-
vations, and check violations of the 5 assumptions underlying the regression model. In the next
few chapters, we will study various tests of the violation of the classical assumptions. Most of
these tests are based on the residuals of the model. These tests along with residual plots should
help the researcher gauge the adequacy of his or her model.
</p>
<p>Table 3.1 Simple Regression Computations
</p>
<p>OBS
Consumption Income
</p>
<p>yi=Yi&minus;Ȳ xi=Xi-X̄ xiyi x2i Ŷi eiyi xi
1 4.6 5 &ndash;1.9 &ndash;2.5 4.75 6.25 4.476190 0.123810
2 3.6 4 &ndash;2.9 &ndash;3.5 10.15 12.25 3.666667 &ndash;0.066667
3 4.6 6 &ndash;1.9 &ndash;1.5 2.85 2.25 5.285714 &ndash;0.685714
4 6.6 8 0.1 0.5 0.05 0.25 6.904762 &ndash;0.304762
5 7.6 8 1.1 0.5 0.55 0.25 6.904762 0.695238
6 5.6 7 &ndash;0.9 &ndash;0.5 0.45 0.25 6.095238 &ndash;0.495238
7 5.6 6 &ndash;0.9 &ndash;1.5 1.35 2.25 5.285714 0.314286
8 8.6 9 2.1 1.5 3.15 2.25 7.714286 0.885714
9 8.6 10 2.1 2.5 5.25 6.25 8.523810 0.076190
10 9.6 12 3.1 4.5 13.95 20.25 10.142857 &ndash;0.542857
</p>
<p>SUM 6.5 75 0 0 42.5 52.5 65 0
</p>
<p>MEAN 6.5 7.5 6.5
</p>
<p>iX&gt;
</p>
<p>+
</p>
<p>+
</p>
<p>++
</p>
<p>+
+
</p>
<p>+
</p>
<p>+ ++
+
+
</p>
<p>+
+
</p>
<p>+
+
</p>
<p>+
+
</p>
<p>+
+
+
</p>
<p>+
+
</p>
<p>+
+
+
+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
+
</p>
<p>+
</p>
<p>+
+
</p>
<p>+
+
+
</p>
<p>+
</p>
<p>+
+
+
</p>
<p>+
</p>
<p>+ +
+
</p>
<p>+
+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>+
</p>
<p>ie
</p>
<p>0</p>
<p/>
</div>
<div class="page"><p/>
<p>3.9 Numerical Example 63
</p>
<p>3.9 Numerical Example
</p>
<p>Table 3.1 gives the annual consumption of 10 households each selected randomly from a group of
households with a fixed personal disposable income. Both income and consumption are measured
in $10, 000, so that the first household earns $50, 000 and consumes $46, 000 annually. It is
worthwhile doing the computations necessary to obtain the least squares regression estimates
of consumption on income in this simple case and to compare them with those obtained from
a regression package. In order to do this, we first compute Ȳ = 6.5 and X̄ = 7.5 and form two
new columns of data made up of yi = Yi&minus; Ȳ and xi = Xi&minus; X̄. To get β̂OLS we need
</p>
<p>&sum;n
i=1 xiyi,
</p>
<p>so we multiply these last two columns by each other and sum to get 42.5. The denominator of
β̂OLS is given by
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i . This is why we square the xi column to get x
</p>
<p>2
i and sum to obtain
</p>
<p>52.5. Our estimate of β̂OLS = 42.5/52.5 = 0.8095 which is the estimated marginal propensity to
consume. This is the extra consumption brought about by an extra dollar of disposable income.
</p>
<p>α̂OLS = Ȳ &minus; β̂OLSX̄ = 6.5&minus; (0.8095)(7.5) = 0.4286
</p>
<p>This is the estimated consumption at zero personal disposable income. The fitted values or
predicted values from this regression are computed from Ŷi = α̂OLS + β̂OLSXi = 0.4286 +
0.8095Xi and are given in Table 3.1. Note that the mean of Ŷi is equal to the mean of Yi
confirming one of the numerical properties of least squares. The residuals are computed from
ei = Yi &minus; Ŷi and they satisfy
</p>
<p>&sum;n
i=1 ei = 0. It is left to the reader to verify that
</p>
<p>&sum;n
i=1 eiXi = 0.
</p>
<p>The residual sum of squares is obtained by squaring the column of residuals and summing it.
This gives us
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i = 2.495238. This means that s
</p>
<p>2 =
&sum;n
</p>
<p>i=1 e
2
i /(n&minus;2) = 0.311905. Its square
</p>
<p>root is given by s = 0.558. This is known as the standard error of the regression. In this case,
the estimated var(β̂OLS) is s
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i = 0.311905/52.5 = 0.005941 and the estimated
</p>
<p>var(α̂) = s2
[
1
</p>
<p>n
+
</p>
<p>X̄2&sum;n
i=1 x
</p>
<p>2
i
</p>
<p>]
= 0.311905
</p>
<p>[
1
</p>
<p>10
+
</p>
<p>(7.5)2
</p>
<p>52.5
</p>
<p>]
= 0.365374
</p>
<p>Taking the square root of these estimated variances, we get the estimated standard errors of
α̂OLS and β̂OLS given by ŝe(α̂OLS) = 0.60446 and ŝe(β̂OLS) = 0.077078.
</p>
<p>Since the disturbances are normal, the OLS estimators are also the maximum likelihood
estimators, and are normally distributed themselves. For the null hypothesis Ha0 ;β = 0; the
observed t-statistic is
</p>
<p>tobs = (β̂OLS &minus; 0)/ŝe(β̂OLS) = 0.809524/0.077078 = 10.50
</p>
<p>and this is highly significant, since Pr[|t8| &gt; 10.5] &lt; 0.0001. This probability can be obtained
using most regression packages. It is also known as the p-value or probability value. It shows
that this t-value is highly unlikely and we reject Ha0 that β = 0. Similarly, the null hypothesis
Hb0;α = 0, gives an observed t-statistic of tobs = (α̂OLS &minus; 0)/ŝe(α̂OLS) = 0.428571/0.604462 =
0.709, which is not significant, since its p-value is Pr[|t8| &gt; 0.709] &lt; 0.498. Hence, we do not
reject the null hypothesis Hb0 that α = 0.
The total sum of squares is
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i =
</p>
<p>&sum;n
i=1(Yi&minus; Ȳ )2 which can be obtained by squaring the
</p>
<p>yi column in Table 3.1 and summing. This yields
&sum;n
</p>
<p>i=1 y
2
i = 36.9. Also, the regression sum of
</p>
<p>squares =
&sum;n
</p>
<p>i=1 ŷ
2
i =
</p>
<p>&sum;n
i=1(Ŷi &minus; Ȳ )2 which can be obtained by subtracting Ȳ = Ŷ = 6.5 from
</p>
<p>the Ŷi column, squaring that column and summing. This yields 34.404762. This could have also
been obtained as</p>
<p/>
</div>
<div class="page"><p/>
<p>64 Chapter 3: Simple Linear Regression
</p>
<p>&sum;n
i=1 ŷ
</p>
<p>2
i = β̂
</p>
<p>2
</p>
<p>OLS
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i = (0.809524)
</p>
<p>2(52.5) = 34.404762.
</p>
<p>A final check is that
&sum;n
</p>
<p>i=1 ŷ
2
i =
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i &minus;
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i = 36.9&minus; 2.495238 = 34.404762 as required.
</p>
<p>Recall, that R2 = r2xy = (
&sum;n
</p>
<p>i=1 xiyi)
2/(
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )(
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ) = (42.5)
</p>
<p>2/(52.5)(36.9) = 0.9324.
This could have also been obtained as R2 = 1 &minus; (
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ) = 1 &minus; (2.495238/36.9) =
</p>
<p>0.9324, or as
</p>
<p>R2 = r2yŷ =
&sum;n
</p>
<p>i=1 ŷ
2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i = 34.404762/36.9 = 0.9324.
</p>
<p>This means that personal disposable income explains 93.24% of the variation in consumption.
A plot of the actual, predicted and residual values versus time is given in Figure 3.8. This was
done using EViews.
</p>
<p>��
</p>
<p>��
</p>
<p>��
</p>
<p>���
</p>
<p>�� � � � � �� 	 � 
 � � � �� �� ��
</p>
<p>&#13;��������
</p>
<p>� ������
</p>
<p>� ������
</p>
<p>Figure 3.8 Residual Plot
</p>
<p>3.10 Empirical Example
</p>
<p>Table 3.2 gives (i) the logarithm of cigarette consumption (in packs) per person of smoking age
(&gt; 16 years) for 46 states in 1992, (ii) the logarithm of real price of cigarettes in each state,
and (iii) the logarithm of real disposable income per capita in each state. This is drawn from
Baltagi and Levin (1992) study on dynamic demand for cigarettes. It can be downloaded as
Cigarett.dat from the Springer web site.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Empirical Example 65
</p>
<p>Table 3.2 Cigarette Consumption Data
</p>
<p>LNC: log of consumption (in packs) per person of smoking age (&gt;16)
</p>
<p>LNP: log of real price (1983$/pack)
LNY: log of real disposable income per-capita (in thousand 1983$)
</p>
<p>OBS STATE LNC LNP LNY
</p>
<p>1 AL 4.96213 0.20487 4.64039
2 AZ 4.66312 0.16640 4.68389
3 AR 5.10709 0.23406 4.59435
4 CA 4.50449 0.36399 4.88147
5 CT 4.66983 0.32149 5.09472
6 DE 5.04705 0.21929 4.87087
7 DC 4.65637 0.28946 5.05960
8 FL 4.80081 0.28733 4.81155
9 GA 4.97974 0.12826 4.73299
</p>
<p>10 ID 4.74902 0.17541 4.64307
11 IL 4.81445 0.24806 4.90387
12 IN 5.11129 0.08992 4.72916
13 IA 4.80857 0.24081 4.74211
14 KS 4.79263 0.21642 4.79613
15 KY 5.37906 &ndash;0.03260 4.64937
16 LA 4.98602 0.23856 4.61461
17 ME 4.98722 0.29106 4.75501
18 MD 4.77751 0.12575 4.94692
19 MA 4.73877 0.22613 4.99998
20 MI 4.94744 0.23067 4.80620
21 MN 4.69589 0.34297 4.81207
22 MS 4.93990 0.13638 4.52938
23 MO 5.06430 0.08731 4.78189
24 MT 4.73313 0.15303 4.70417
25 NE 4.77558 0.18907 4.79671
26 NV 4.96642 0.32304 4.83816
27 NH 5.10990 0.15852 5.00319
28 NJ 4.70633 0.30901 5.10268
29 NM 4.58107 0.16458 4.58202
30 NY 4.66496 0.34701 4.96075
31 ND 4.58237 0.18197 4.69163
32 OH 4.97952 0.12889 4.75875
33 OK 4.72720 0.19554 4.62730
34 PA 4.80363 0.22784 4.83516
35 RI 4.84693 0.30324 4.84670
36 SC 5.07801 0.07944 4.62549
37 SD 4.81545 0.13139 4.67747
38 TN 5.04939 0.15547 4.72525
39 TX 4.65398 0.28196 4.73437
40 UT 4.40859 0.19260 4.55586
41 VT 5.08799 0.18018 4.77578
42 VA 4.93065 0.11818 4.85490
43 WA 4.66134 0.35053 4.85645
44 WV 4.82454 0.12008 4.56859
45 WI 4.83026 0.22954 4.75826
46 WY 5.00087 0.10029 4.71169
</p>
<p>Data: Cigarette Consumption of 46 States in 1992
</p>
<p>1</p>
<p/>
</div>
<div class="page"><p/>
<p>66 Chapter 3: Simple Linear Regression
</p>
<p>Table 3.3 Cigarette Consumption Regression
</p>
<p>Analysis of Variance
</p>
<p>Sum of Mean
Source DF Squares Square F Value Prob &gt; F
</p>
<p>Model 1 0.48048 0.48048 18.084 0.0001
Error 44 1.16905 0.02657
</p>
<p>Root MSE 0.16300 R-square 0.2913
Dep Mean 4.84784 Adj R-sq 0.2752
C.V. 3.36234
</p>
<p>Parameter Estimates
</p>
<p>Parameter Standard T for H0:
Variable DF Estimate Error Parameter=0 Prob &gt; |T|
INTERCEP 1 5.094108 0.06269897 81.247 0.0001
LNP 1 &ndash;1.198316 0.28178857 &ndash;4.253 0.0001
</p>
<p>&ndash;.6
</p>
<p>&ndash;.4
</p>
<p>&ndash;.2
</p>
<p>.0
</p>
<p>.2
</p>
<p>.4
</p>
<p>&ndash;.1 .0 .1 .2 .3 .4
</p>
<p> Log of Real Price (1983$/Pack) 
</p>
<p>R
es
</p>
<p>id
u
</p>
<p>al
s
</p>
<p>Figure 3.9 Residuals Versus LNP
</p>
<p>Table 3.3 gives the SAS output for the regression of logC on logP . The price elasticity of
demand for cigarettes in this simple model is (dlogC/logP ) which is the slope coefficient. This
is estimated to be &minus;1.198 with a standard error of 0.282. This says that a 10% increase in real
price of cigarettes has an estimated 12% drop in per capita consumption of cigarettes. The R2
</p>
<p>of this regression is 0.29, s2 is given by the Mean Square Error of the regression which is 0.0266.
Figure 3.9 plots the residuals of this regression versus the independent variable, while Figure
3.10 plots the predictions along with the 95% confidence interval band for these predictions.
One observation clearly stands out as an influential observation given its distance from the
rest of the data and that is the observation for Kentucky, a producer state with very low real
price. This observation almost anchors the straight line fit through the data. More on influential
observations in Chapter 8.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 67
</p>
<p>4.0
</p>
<p>4.4
</p>
<p>4.8
</p>
<p>5.2
</p>
<p>5.6
</p>
<p>&ndash;.1 .0 .1 .2 .3 .4
</p>
<p>Log of Real Price (1983$/Pack)
</p>
<p>P
re
</p>
<p>d
ic
</p>
<p>te
d
</p>
<p> V
al
</p>
<p>u
e 
</p>
<p>o
f 
</p>
<p>L
N
</p>
<p>C
</p>
<p>Figure 3.10 95% Confidence Band for Predicted Values
</p>
<p>Problems
</p>
<p>1. For the simple regression with a constant Yi = α + βXi + ui, given in equation (3.1) verify the
following numerical properties of the OLS estimator:
</p>
<p>&sum;n
i=1 ei = 0,
</p>
<p>&sum;n
i=1 eiXi = 0,
</p>
<p>&sum;n
i=1 eiŶi = 0,
</p>
<p>&sum;n
i=1 Ŷi =
</p>
<p>&sum;n
i=1 Yi
</p>
<p>2. For the regression with only a constant Yi = α + ui with ui &sim; IID(0, σ
2), show that the least
</p>
<p>squares estimate of α̂ is α̂OLS = Ȳ , var(α̂OLS) = σ
2/n, and the residual sums of squares is&sum;n
</p>
<p>i=1 y
2
i =
</p>
<p>&sum;n
i=1(Yi &minus; Ȳ )2.
</p>
<p>3. For the simple regression without a constant Yi = βXi + ui, with ui &sim; IID(0, σ
2).
</p>
<p>(a) Derive the OLS estimator of β and find its variance.
</p>
<p>(b) What numerical properties of the OLS estimators described in problem 1 still hold for this
model?
</p>
<p>(c) derive the maximum likelihood estimator of β and σ2 under the assumption ui &sim; IIN(0, σ
2).
</p>
<p>(d) Assume σ2 is known. Derive the Wald, LM and LR tests for H0; β = 1 versus H1; β 	= 1.
</p>
<p>4. Use the fact that E(
&sum;n
</p>
<p>i=1 xiui)
2 =
</p>
<p>&sum;n
i=1
</p>
<p>&sum;n
j=1 xixjE(uiuj); and assumptions 2 and 3 to prove
</p>
<p>equation (3.6).
</p>
<p>5. Using the regression given in equation (3.1):
</p>
<p>(a) Show that α̂OLS = α+ (β &minus; β̂OLS)X̄ + ū; and deduce that E(α̂OLS) = α.
(b) Using the fact that β̂OLS &minus;β =
</p>
<p>&sum;n
i=1 xiui/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i ; use the results in part (a) to show that
</p>
<p>var(α̂OLS) = σ
2[(1/n) + (X̄2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )] = σ
</p>
<p>2
&sum;n
</p>
<p>i=1 X
2
i /n
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i .
</p>
<p>(c) Show that α̂OLS is consistent for α.
</p>
<p>(d) Show that cov(α̂OLS , β̂OLS) = &minus;X̄var(β̂OLS) = &minus;σ2X̄/
&sum;n
</p>
<p>i=1 x
2
i . This means that the sign
</p>
<p>of the covariance is determined by the sign of X̄. If X̄ is positive, this covariance will be
negative. This also means that if α̂OLS is over-estimated, β̂OLS will be under-estimated.</p>
<p/>
</div>
<div class="page"><p/>
<p>68 Chapter 3: Simple Linear Regression
</p>
<p>6. Using the regression given in equation (3.1):
</p>
<p>(a) Prove that α̂OLS =
&sum;n
</p>
<p>i=1 λiYi where λi = (1/n)&minus; X̄wi and wi = xi/
&sum;n
</p>
<p>i=1 x
2
i .
</p>
<p>(b) Show that
&sum;n
</p>
<p>i=1 λi = 1 and
&sum;n
</p>
<p>i=1 λiXi = 0.
</p>
<p>(c) Prove that any other linear estimator of α, say α̃ =
&sum;n
</p>
<p>i=1 biYi must satisfy
&sum;n
</p>
<p>i=1 bi = 1 and&sum;n
i=1 biXi = 0 for α̃ to be unbiased for α.
</p>
<p>(d) Let bi = λi + fi; show that
&sum;n
</p>
<p>i=1 fi = 0 and
&sum;n
</p>
<p>i=1 fiXi = 0.
</p>
<p>(e) Prove that var(α̃) = σ2
&sum;n
</p>
<p>i=1 b
2
i = σ
</p>
<p>2
&sum;n
</p>
<p>i=1 λ
2
i + σ
</p>
<p>2
&sum;n
</p>
<p>i=1 f
2
i =var(α̂OLS) + σ
</p>
<p>2
&sum;n
</p>
<p>i=1 f
2
i .
</p>
<p>7. (a) Differentiate (3.9) with respect to α and β and show that α̂MLE = α̂OLS , β̂MLE = β̂OLS .
</p>
<p>(b) Differentiate (3.9) with respect to σ2 and show that σ̂2MLE =
&sum;n
</p>
<p>i=1 e
2
i /n.
</p>
<p>8. The t-Statistic in a Simple Regression. It is well known that a standard normal random variable
N(0, 1) divided by a square root of a chi-squared random variable divided by its degrees of freedom
</p>
<p>(χ2ν/ν)
1
</p>
<p>2 results in a random variable that is t-distributed with ν degrees of freedom, provided
the N(0, 1) and the χ2 variables are independent, see Chapter 2. Use this fact to show that
</p>
<p>(β̂OLS &minus; β)/[s/(
&sum;n
</p>
<p>i=1 x
2
i )
</p>
<p>1
</p>
<p>2 ] &sim; tn&minus;2.
</p>
<p>9. Relationship Between R2 and r2xy.
</p>
<p>(a) Using the fact that R2 =
&sum;n
</p>
<p>i=1 ŷ
2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ; ŷi = β̂OLSxi; and β̂OLS =
</p>
<p>&sum;n
i=1 xiyi/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i ,
</p>
<p>show that R2 = r2xy where,
</p>
<p>r2xy = (
&sum;n
</p>
<p>i=1 xiyi)
2/(
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )(
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ).
</p>
<p>(b) Using the fact that yi = ŷi + ei, show that
&sum;n
</p>
<p>i=1 ŷiyi =
&sum;n
</p>
<p>i=1 ŷ
2
i , and hence, deduce that
</p>
<p>r2yŷ = (
&sum;n
</p>
<p>i=1 yiŷi)
2/(
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i )(
</p>
<p>&sum;n
i=1 ŷ
</p>
<p>2
i ) is equal to R
</p>
<p>2.
</p>
<p>10. Prediction. Consider the problem of predicting Y0 from (3.11). Given X0,
</p>
<p>(a) Show that E(Y0) = α+ βX0.
</p>
<p>(b) Show that Ŷ0 is unbiased for E(Y0).
</p>
<p>(c) Show that var(Ŷ0) = var(α̂OLS)+X
2
0var(β̂OLS)+2X0cov(α̂OLS , β̂OLS). Deduce that var(Ŷ0)
</p>
<p>= σ2[(1/n) + (X0 &minus; X̄)2/
&sum;n
</p>
<p>i=1 x
2
i ].
</p>
<p>(d) Consider a linear predictor of E(Y0), say Ỹ0 =
&sum;n
</p>
<p>i=1 aiYi, show that
&sum;n
</p>
<p>i=1 ai = 1 and&sum;n
i=1 aiXi = X0 for this predictor to be unbiased for E(Y0).
</p>
<p>(e) Show that the var(Ỹ0) = σ
2
&sum;n
</p>
<p>i=1 a
2
i . Minimize
</p>
<p>&sum;n
i=1 a
</p>
<p>2
i subject to the restrictions given
</p>
<p>in (d). Prove that the resulting predictor is Ỹ0 = α̂OLS + β̂OLSX0 and that the minimum
variance is σ2[(1/n) + (X0 &minus; X̄)2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i ].
</p>
<p>11. Optimal Weighting of Unbiased Estimators. This is based on Baltagi (1995). For the simple re-
gression without a constant Yi = βXi + ui, i = 1, 2, . . . , N ; where β is a scalar and ui &sim; IID(0, σ
</p>
<p>2)
independent of Xi. Consider the following three unbiased estimators of β:
</p>
<p>β̂1 =
&sum;n
</p>
<p>i=1 XiYi/
&sum;n
</p>
<p>i=1 X
2
i , β̂2 = Ȳ /X̄
</p>
<p>and
</p>
<p>β̂3 =
&sum;n
</p>
<p>i=1(Xi &minus; X̄)(Yi &minus; Ȳ )/
&sum;n
</p>
<p>i=1(Xi &minus; X̄)2,
</p>
<p>where X̄ =
&sum;n
</p>
<p>i=1 Xi/n and Ȳ =
&sum;n
</p>
<p>i=1 Yi/n.
</p>
<p>(a) Show that cov(β̂1, β̂2) = var(β̂1) &gt; 0, and that ρ12 = (the correlation coefficient of β̂1 and
</p>
<p>β̂2) = [var(β̂1)/var(β̂2)]
1
</p>
<p>2 with 0 &lt; ρ12 &le; 1. Show that the optimal combination of β̂1 and
β̂2, given by β̂ = αβ̂1 + (1 &minus; α)β̂2 where &minus;&infin; &lt; α &lt; &infin; occurs at α&lowast; = 1. Optimality here
refers to minimizing the variance. Hint: Read the paper by Samuel-Cahn (1994).</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 69
</p>
<p>(b) Similarly, show that cov(β̂1, β̂3) = var(β̂1) &gt; 0, and that ρ13 = (the correlation coefficient of
</p>
<p>β̂1 and β̂3) = [var(β̂1)/var(β̂3)]
1
</p>
<p>2 = (1&minus; ρ212)
1
</p>
<p>2 with 0 &lt; ρ13 &lt; 1. Conclude that the optimal
</p>
<p>combination β̂1 and β̂3 is again α
&lowast; = 1.
</p>
<p>(c) Show that cov(β̂2, β̂3) = 0 and that optimal combination of β̂2 and β̂3 is β̂ = (1&minus; ρ212)β̂3 +
ρ212β̂2 = β̂1. This exercise demonstrates a more general result, namely that the BLUE of
</p>
<p>β in this case β̂1, has a positive correlation with any other linear unbiased estimator of β,
and that this correlation can be easily computed from the ratio of the variances of these two
estimators.
</p>
<p>12. Efficiency as Correlation. This is based on Oksanen (1993). Let β̂ denote the Best Linear Unbiased
</p>
<p>Estimator of β and let β̃ denote any linear unbiased estimator of β. Show that the relative efficiency
of β̃ with respect to β̂ is the squared correlation coefficient between β̂ and β̃. Hint: Compute the
variance of β̂ + λ(β̂ &minus; β̃) for any λ. This variance is minimized at λ = 0 since β̂ is BLUE. This
should give you the result that E(β̂
</p>
<p>2
) = E(β̂β̃) which in turn proves the required result, see Zheng
</p>
<p>(1994).
</p>
<p>13. For the numerical illustration given in section 3.9, what happens to the least squares regression
coefficient estimates (α̂OLS , β̂OLS), s
</p>
<p>2, the estimated se(α̂OLS) and se(β̂OLS), t-statistic for α̂OLS
and β̂OLS for H
</p>
<p>a
0 ;α = 0, and H
</p>
<p>b
0 ;β = 0 and R
</p>
<p>2 when:
</p>
<p>(a) Yi is regressed on Xi + 5 rather than Xi. In other words, we add a constant 5 to each
observation of the explanatory variable Xi and rerun the regression. It is very instructive to
see how the computations in Table 3.1 are affected by this simple transformation on Xi.
</p>
<p>(b) Yi + 2 is regressed on Xi. In other words, a constant 2 is added to Yi.
</p>
<p>(c) Yi is regressed on 2Xi. (A constant 2 is multiplied by Xi).
</p>
<p>14. For the cigarette consumption data given in Table 3.2.
</p>
<p>(a) Give the descriptive statistics for logC, logP and logY . Plot their histogram. Also, plot logC
versus logY and logC versus logP . Obtain the correlation matrix of these variables.
</p>
<p>(b) Run the regression of logC on logY . What is the income elasticity estimate? What is its
standard error? Test the null hypothesis that this elasticity is zero. What is the s and R2 of
this regression?
</p>
<p>(c) Show that the square of the simple correlation coefficient between logC and logY is equal to
R2. Show that the square of the correlation coefficient between the fitted and actual values
of logC is also equal to R2.
</p>
<p>(d) Plot the residuals versus income. Also, plot the fitted values along with their 95% confidence
band.
</p>
<p>15. Consider the simple regression with no constant: Yi = βXi + ui i = 1, 2, . . . , n
</p>
<p>where ui &sim; IID(0, σ
2) independent of Xi. Theil (1971) showed that among all linear estimators in
</p>
<p>Yi, the minimum mean square estimator for β, i.e., that which minimizes E(β̃ &minus; β)2 is given by
</p>
<p>β̃ = β2
&sum;n
</p>
<p>i=1 XiYi/(β
2 &sum;n
</p>
<p>i=1 X
2
i + σ
</p>
<p>2).
</p>
<p>(a) Show that E(β̃) = β/(1 + c), where c = σ2/β2
&sum;n
</p>
<p>i=1 X
2
i &gt; 0.
</p>
<p>(b) Conclude that the Bias (β̃) = E(β̃) &minus; β = &minus;[c/(1 + c)]β. Note that this bias is positive
(negative) when β is negative (positive). This also means that β̃ is biased towards zero.
</p>
<p>(c) Show that MSE(β̃) = E(β̃&minus;β)2 = σ2/[
&sum;n
</p>
<p>i=1 X
2
i +(σ
</p>
<p>2/β2)]. Conclude that it is smaller than
</p>
<p>the MSE(β̂OLS).</p>
<p/>
</div>
<div class="page"><p/>
<p>70 Chapter 3: Simple Linear Regression
</p>
<p>Table 3.4 Energy Data for 20 countries
</p>
<p>RGDP EN
Country (in 106 1975 U.S.$&rsquo;s) 106 Kilograms Coal Equivalents
</p>
<p>Malta 1251 456
Iceland 1331 1124
Cyprus 2003 1211
Ireland 11788 11053
Norway 27914 26086
Finland 28388 26405
Portugal 30642 12080
Denmark 34540 27049
Greece 38039 20119
Switzerland 42238 23234
Austria 45451 30633
Sweden 59350 45132
Belgium 62049 58894
Netherlands 82804 84416
Turkey 91946 32619
Spain 159602 88148
Italy 265863 192453
U.K. 279191 268056
France 358675 233907
W. Germany 428888 352.677
</p>
<p>16. Table 3.4 gives cross-section Data for 1980 on real gross domestic product (RGDP) and aggregate
energy consumption (EN) for 20 countries
</p>
<p>(a) Enter the data and provide descriptive statistics. Plot the histograms for RGDP and EN.
Plot EN versus RGDP.
</p>
<p>(b) Estimate the regression:
</p>
<p>log(En) = α+ βlog(RGDP ) + u.
</p>
<p>Be sure to plot the residuals. What do they show?
</p>
<p>(c) Test H0;β = 1.
</p>
<p>(d) One of your Energy data observations has a misplaced decimal. Multiply it by 1000. Now
repeat parts (a), (b) and (c).
</p>
<p>(e) Was there any reason for ordering the data from the lowest to highest energy consumption?
Explain.
</p>
<p>Lesson Learned : Always plot the residuals. Always check your data very carefully.
</p>
<p>17. Using the Energy Data given in Table 3.4, corrected as in problem 16 part (d), is it legitimate to
reverse the form of the equation?
</p>
<p>log(RDGP ) = γ + δlog(En) + ǫ
</p>
<p>(a) Economically, does this change the interpretation of the equation? Explain.
</p>
<p>(b) Estimate this equation and compare R2 of this equation with that of the previous problem.
</p>
<p>Also, check if δ̂ = 1/β̂. Why are they different?</p>
<p/>
</div>
<div class="page"><p/>
<p>References 71
</p>
<p>(c) Statistically, by reversing the equation, which assumptions do we violate?
</p>
<p>(d) Show that δ̂β̂ = R2.
</p>
<p>(e) Effects of changing units in which variables are measured. Suppose you measured energy in
BTU&rsquo;s instead of kilograms of coal equivalents so that the original series was multiplied by
60. How does it change α and β in the following equations?
</p>
<p>log(En) = α+ βlog(RDGP ) + u En = α&lowast; + β&lowast;RGDP + ν
</p>
<p>Can you explain why α̂ changed, but not β̂ for the log-log model, whereas both α̂&lowast;and
</p>
<p>β̂
&lowast;
changed for the linear model?
</p>
<p>(f) For the log-log specification and the linear specification, compare the GDP elasticity for
Malta and W. Germany. Are both equally plausible?
</p>
<p>(g) Plot the residuals from both linear and log-log models. What do you observe?
</p>
<p>(h) Can you compare the R2 and standard errors from both models in part (g)? Hint: Retrieve
</p>
<p>log(En) and l̂og(En) in the log-log equation, exponentiate, then compute the residuals and
s. These are comparable to those obtained from the linear model.
</p>
<p>18. For the model considered in problem 16: log(En) = α + βlog(RGDP ) + u and measuring energy
in BTU&rsquo;s (like part (e) of problem 17).
</p>
<p>(a) What is the 95% confidence prediction interval at the sample mean?
</p>
<p>(b) What is the 95% confidence prediction interval for Malta?
</p>
<p>(c) What is the 95% confidence prediction interval for West Germany?
</p>
<p>References
</p>
<p>Additional readings on the material covered in this chapter can be found in:
</p>
<p>Baltagi, B.H. (1995), &ldquo;Optimal Weighting of Unbiased Estimators,&rdquo; Econometric Theory, Problem 95.3.1,
11:637.
</p>
<p>Baltagi, B.H. and D. Levin (1992), &ldquo;Cigarette Taxation: Raising Revenues and Reducing Consumption,&rdquo;
Structural Change and Economic Dynamics, 3: 321&ndash;335.
</p>
<p>Belsley, D.A., E. Kuh and R.E. Welsch (1980), Regression Diagnostics (Wiley: New York).
</p>
<p>Greene, W. (1993), Econometric Analysis (Macmillian: New York).
</p>
<p>Gujarati, D. (1995), Basic Econometrics (McGraw-Hill: New York).
</p>
<p>Johnston, J. (1984), Econometric Methods (McGraw-Hill: New York).
</p>
<p>Kelejian, H. and W. Oates (1989), Introduction to Econometrics (Harper and Row: New York).
</p>
<p>Kennedy, P. (1992), A Guide to Econometrics (MIT Press: Cambridge).
</p>
<p>Kmenta, J. (1986), Elements of Econometrics (Macmillan: New York).
</p>
<p>Maddala, G.S. (1992), Introduction to Econometrics (Macmillan: New York).
</p>
<p>Oksanen, E.H. (1993), &ldquo;Efficiency as Correlation,&rdquo; Econometric Theory, Problem 93.1.3, 9: 146.
</p>
<p>Samuel-Cahn, E. (1994), &ldquo;Combining Unbiased Estimators,&rdquo; The American Statistician, 48: 34&ndash;36.
</p>
<p>Wallace, D. and L. Silver (1988), Econometrics: An Introduction (Addison Wesley: New York).
</p>
<p>Zheng, J.X. (1994), &ldquo;Efficiency as Correlation,&rdquo; Econometric Theory, Solution 93.1.3, 10: 228.</p>
<p/>
</div>
<div class="page"><p/>
<p>72 Chapter 3: Simple Linear Regression
</p>
<p>Appendix
Centered and Uncentered R2
</p>
<p>From the OLS regression on (3.1) we get
</p>
<p>Yi = Ŷi + ei i = 1, 2, . . . , n (A.1)
</p>
<p>where Ŷi = α̂OLS +Xiβ̂OLS . Squaring and summing the above equation we get
</p>
<p>&sum;n
i=1 Y
</p>
<p>2
i =
</p>
<p>&sum;n
i=1 Ŷ
</p>
<p>2
i +
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i (A.2)
</p>
<p>since
&sum;n
</p>
<p>i=1 Ŷiei = 0. The uncentered R
2 is given by
</p>
<p>uncentered R2 = 1&minus;&sum;ni=1 e2i /
&sum;n
</p>
<p>i=1 Y
2
i =
</p>
<p>&sum;n
i=1 Ŷ
</p>
<p>2
i /
</p>
<p>&sum;
i=1 Y
</p>
<p>2
i (A.3)
</p>
<p>Note that the total sum of squares for Yi is not expressed in deviation from the sample mean Ȳ .
In other words, the uncentered R2 is the proportion of variation of
</p>
<p>&sum;n
i=1 Y
</p>
<p>2
i that is explained
</p>
<p>by the regression on X. Regression packages usually report the centered R2 which was defined
in section 3.6 as 1 &minus; (
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ) where yi = Yi &minus; Ȳ . The latter measure focuses on
</p>
<p>explaining the variation in Yi after fitting the constant.
From section 3.6, we have seen that a naive model with only a constant in it gives Ȳ as the
</p>
<p>estimate of the constant, see also problem 2. The variation in Yi that is not explained by this
naive model is
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i =
</p>
<p>&sum;n
i=1(Yi &minus; Ȳ )2. Subtracting nȲ 2 from both sides of (A.2) we get
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i =
</p>
<p>&sum;n
i=1 Ŷ
</p>
<p>2
i &minus; nȲ 2 +
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i
</p>
<p>and the centered R2 is
</p>
<p>centered R2 = 1&minus; (
&sum;n
</p>
<p>i=1 e
2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ) = (
</p>
<p>&sum;n
i=1 Ŷ
</p>
<p>2
i &minus; nȲ 2)/
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i (A.4)
</p>
<p>If there is a constant in the model Ȳ = Ŷ , see section 3.6, and
&sum;n
</p>
<p>i=1 ŷ
2
i =
</p>
<p>&sum;n
i=1(Ŷi &minus; Ŷ )2 =&sum;n
</p>
<p>i=1 Ŷ
2
i &minus; nȲ 2. Therefore, the centered R2 =
</p>
<p>&sum;n
i=1 ŷ
</p>
<p>2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i which is the R
</p>
<p>2 reported by
regression packages. If there is no constant in the model, some regression packages give you the
option of (no constant) and the R2 reported is usually the uncentered R2. Check your regression
package documentation to verify what you are getting. We will encounter uncentered R2 again
in constructing test statistics using regressions, see for example Chapter 11.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 4
</p>
<p>Multiple Regression Analysis
</p>
<p>4.1 Introduction
</p>
<p>So far we have considered only one regressor X besides the constant in the regression equation.
Economic relationships usually include more than one regressor. For example, a demand equa-
tion for a product will usually include real price of that product in addition to real income as
well as real price of a competitive product and the advertising expenditures on this product. In
this case
</p>
<p>Yi = α+ β2X2i + β3X3i + ..+ βKXKi + ui i = 1, 2, . . . , n (4.1)
</p>
<p>where Yi denotes the i-th observation on the dependent variable Y, in this case the sales of
this product. Xki denotes the i-th observation on the independent variable Xk for k = 2, . . . ,K
in this case, own price, the competitor&rsquo;s price and advertising expenditures. α is the intercept
and β2, β3, . . . , βK are the (K &minus; 1) slope coefficients. The ui&rsquo;s satisfy the classical assumptions
1&ndash;4 given in Chapter 3. Assumption 4 is modified to include all the X&rsquo;s appearing in the
regression, i.e., every Xk for k = 2, . . . ,K, is uncorrelated with the ui&rsquo;s with the property that&sum;n
</p>
<p>i=1(Xki &minus; X̄k)2/n where X̄k =
&sum;n
</p>
<p>i=1Xki/n has a finite probability limit which is different
from zero.
Section 4.2 derives the OLS normal equations of this multiple regression model and discovers
</p>
<p>that an additional assumption is needed for these equations to yield a unique solution.
</p>
<p>4.2 Least Squares Estimation
</p>
<p>As explained in Chapter 3, least squares minimizes the residual sum of squares where the
residuals are now given by ei = Yi &minus; α̂ &minus;
</p>
<p>&sum;K
k=2 β̂kXki and α̂ and β̂k denote guesses on the
</p>
<p>regression parameters α and βk, respectively. The residual sum of squares
</p>
<p>RSS =
&sum;n
</p>
<p>i=1 e
2
i =
</p>
<p>&sum;n
i=1(Yi &minus; α̂&minus; β̂2X2i &minus; ..&minus; β̂KXKi)2
</p>
<p>is minimized by the following K first-order conditions:
</p>
<p>&part;(
&sum;n
</p>
<p>i=1 e
2
i )/&part;α̂ = &minus;2
</p>
<p>&sum;n
i=1 ei = 0
</p>
<p>&part;(
&sum;n
</p>
<p>i=1 e
2
i )/&part;β̂k = &minus;2
</p>
<p>&sum;n
i=1 eiXki = 0, for k = 2, . . . ,K. (4.2)
</p>
<p>or, equivalently
&sum;n
</p>
<p>i=1 Yi = α̂n+ β̂2
&sum;n
</p>
<p>i=1X2i + ..+ β̂K
&sum;n
</p>
<p>i=1Xki&sum;n
i=1 YiX2i = α̂
</p>
<p>&sum;n
i=1X2i + β̂2
</p>
<p>&sum;n
i=1X
</p>
<p>2
2i + ..+ β̂K
</p>
<p>&sum;n
i=1X2iXKi (4.3)
</p>
<p>...
...
</p>
<p>...
...&sum;n
</p>
<p>i=1 YiXKi = α̂
&sum;n
</p>
<p>i=1XKi + β̂2
&sum;n
</p>
<p>i=1X2iXKi + ..+ β̂K
&sum;n
</p>
<p>i=1X
2
Ki
</p>
<p>73 
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_4, </p>
<p/>
</div>
<div class="page"><p/>
<p>74 Chapter 4: Multiple Regression Analysis
</p>
<p>where the first equation multiplies the regression equation by the constant and sums, the second
equation multiplies the regression equation by X2 and sums, and the K-th equation multiplies
the regression equation by XK and sums.
</p>
<p>&sum;n
i=1 ui = 0 and
</p>
<p>&sum;n
i=1 uiXki = 0 for k = 2, . . . ,K
</p>
<p>are implicitly imposed to arrive at (4.3). Solving these K equations in K unknowns, we get the
OLS estimators. This can be done more succinctly in matrix form, see Chapter 7. Assumptions
1&ndash;4 insure that the OLS estimator is BLUE. Assumption 5 introduces normality and as a result
the OLS estimator is also (i) a maximum likelihood estimator, (ii) it is normally distributed,
and (iii) it is minimum variance unbiased. Normality also allows test of hypotheses. Without
the normality assumption, one has to appeal to the Central Limit Theorem and the fact that
the sample is large to perform hypotheses testing.
</p>
<p>In order to make sure we can solve for the OLS estimators in (4.3) we need to impose one
further assumption on the model besides those considered in Chapter 3.
</p>
<p>Assumption 6: No perfect multicollinearity, i.e., the explanatory variables are not perfectly
correlated with each other. This assumption states that, no explanatory variable Xk for k =
2, . . . ,K is a perfect linear combination of the other X&rsquo;s. If assumption 6 is violated, then
one of the equations in (4.2) or (4.3) becomes redundant and we would have K &minus; 1 linearly
independent equations in K unknowns. This means that we cannot solve uniquely for the OLS
estimators of the K coefficients.
</p>
<p>Example 1: If X2i = 3X4i &minus; 2X5i +X7i for i = 1, . . . , n, then multiplying this relationship by
ei and summing over i we get
</p>
<p>&sum;n
i=1X2iei = 3
</p>
<p>&sum;n
i=1X4iei &minus; 2
</p>
<p>&sum; n
i=1X5iei +
</p>
<p>&sum;n
i=1X7iei.
</p>
<p>This means that the second OLS normal equation in (4.2) can be represented as a perfect linear
combination of the fourth, fifth and seventh OLS normal equations. Knowing the latter three
equations, the second equation adds no new information. Alternatively, one could substitute
this relationship in the original regression equation (4.1). After some algebra, X2 would be
eliminated and the resulting equation becomes:
</p>
<p>Yi = α+ β3X3i + (3β2 + β4)X4i + (β5 &minus; 2β2)X5i + β6X6i + (β2 + β7)X7i (4.4)
+..+ βKXKi + ui.
</p>
<p>Note that the coefficients of X4i, X5i and X7i are now (3β2 + β4), (β5 &minus; 2β2) and (β2 + β7),
respectively. All of which are contaminated by β2. These linear combinations of β2, β4, β5 and
β7 can be estimated from regression (4.4) which excludes X2i. In fact, the other X&rsquo;s, not con-
taminated by this perfect linear relationship, will have coefficients that are not contaminated
by β2 and hence are themselves estimable using OLS. However, β2, β4, β5 and β7 cannot be
estimated separately. Perfect multicollinearity means that we cannot separate the influence on
Y of the independent variables that are perfectly related. Hence, assumption 6 of no perfect
multicollinearity is needed to guarantee a unique solution of the OLS normal equations. Note
that it applies to perfect linear relationships and does not apply to perfect non-linear relation-
ships among the independent variables. In other words, one can include X1i and X
</p>
<p>2
1i like (years
</p>
<p>of experience) and (years of experience)2 in an equation explaining earnings of individuals. Al-
though, there is a perfect quadratic relationship between these independent variables, this is
not a perfect linear relationship and therefore, does not cause perfect multicollinearity.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Residual Interpretation of Multiple Regression Estimates 75
</p>
<p>4.3 Residual Interpretation of Multiple Regression Estimates
</p>
<p>Although we did not derive an explicit solution for the OLS estimators of the β&rsquo;s, we know
that they are the solutions to (4.2) or (4.3). Let us focus on one of these estimators, say β̂2, the
OLS estimator of β2, the partial derivative of Yi with respect to X2i. As a solution to (4.2) or
(4.3), β̂2 is a multiple regression coefficient estimate of β2. Alternatively, we can interpret β̂2
as a simple linear regression coefficient.
</p>
<p>Claim 1: (i) Run the regression of X2 on all the other X&rsquo;s in (4.1), and obtain the residuals
ν̂2, i.e., X2 = X̂2 + ν̂2. (ii) Run the simple regression of Y on ν̂2, the resulting estimate of the
slope coefficient is β̂2.
The first regression essentially cleans out the effect of the other X&rsquo;s from X2, leaving the vari-
</p>
<p>ation unique to X2 in ν̂2. Claim 1 states that β̂2 can be interpreted as a simple linear regression
coefficient of Y on this residual. This is in line with the partial derivative interpretation of β2.
The proof of claim 1 is given in the Appendix. Using the results of the simple regression given
in (3.4) with the regressor Xi replaced by the residual ν̂2, we get
</p>
<p>β̂2 =
&sum;n
</p>
<p>i=1 ν̂2Yi/
&sum;n
</p>
<p>i=1 ν̂
2
2i (4.5)
</p>
<p>and from (3.6) we get
</p>
<p>var(β̂2) = σ
2/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
2i (4.6)
</p>
<p>An alternative interpretation of β̂2 as a simple regression coefficient is the following:
</p>
<p>Claim 2: (i) Run Y on all the other X&rsquo;s and get the predicted Ỹ and the residuals, say ω̃. (ii)
Run the simple linear regression of ω̃ on ν̂2. β̂2 is the resulting estimate of the slope coefficient.
This regression cleans both Y and X2 from the effect of the other X&rsquo;s and then regresses the
</p>
<p>cleaned out residuals of Y on those of X2. Once again this is in line with the partial derivative
interpretation of β2. The proof of claim 2 is simple and is given in the Appendix.
These two interpretations of β̂2 are important in that they provide an easy way of looking at
</p>
<p>a multiple regression in the context of a simple linear regression. Also, it says that there is no
need to clean the effects of one X from the other X&rsquo;s to find its unique effect on Y . All one has
to do is to include all these X&rsquo;s in the same multiple regression. Problem 1 verifies this result
with an empirical example. This will also be proved using matrix algebra in Chapter 7.
</p>
<p>Recall that R2 = 1 &minus; RSS/TSS for any regression. Let R22 be the R2 for the regression
of X2 on all the other X&rsquo;s, then R
</p>
<p>2
2 = 1 &minus;
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
2i/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
2i where x2i = X2i &minus; X̄2 and
</p>
<p>X̄2 =
&sum;n
</p>
<p>i=1X2i/n; TSS =
&sum;n
</p>
<p>i=1(X2i &minus; X̄2)2 =
&sum;n
</p>
<p>i=1 x
2
2i and RSS =
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
2i. Equivalently,&sum;n
</p>
<p>i=1 ν̂
2
2i =
</p>
<p>&sum;n
i=1 x
</p>
<p>2
2i(1&minus;R22) and the
</p>
<p>var(β̂2) = σ
2/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
2i = σ
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
2i(1&minus;R22) (4.7)
</p>
<p>This means that the larger R22, the smaller is (1 &minus; R22) and the larger is var(β̂2) holding σ2
and
</p>
<p>&sum;n
i=1 x
</p>
<p>2
2i fixed. This shows the relationship between multicollinearity and the variance of
</p>
<p>the OLS estimates. High multicollinearity between X2 and the other X&rsquo;s will result in high
R22 which in turn implies high variance for β̂2. Perfect multicollinearity is the extreme case
</p>
<p>where R22 = 1. This in turn implies an infinite variance for β̂2. In general, high multicollinearity
among the regressors yields imprecise estimates for these highly correlated variables. The least</p>
<p/>
</div>
<div class="page"><p/>
<p>76 Chapter 4: Multiple Regression Analysis
</p>
<p>squares regression estimates are still unbiased as long as assumptions 1 and 4 are satisfied,
but these estimates are unreliable as reflected by their high variances. However, it is important
to note that a low σ2 and a high
</p>
<p>&sum;
n
i=1x
</p>
<p>2
2i could counteract the effect of a high R
</p>
<p>2
2 leading
</p>
<p>to a significant t-statistic for β̂2. Maddala (2001) argues that high intercorrelation among the
explanatory variables are neither necessary nor sufficient to cause the multicollinearity problem.
In practice, multicollinearity is sensitive to the addition or deletion of observations. More on
this in Chapter 8. Looking at high intercorrelations among the explanatory variables is useful
only as a complaint. It is more important to look at the standard errors and t-statistics to assess
the seriousness of multicollinearity.
Much has been written on possible solutions to the multicollinearity problem, see Hill and
</p>
<p>Adkins (2001) for a good summary. Credible candidates include: (i) obtaining new and better
data, but this is rarely available; (ii) introducing nonsample information about the model pa-
rameters based on previous empirical research or economic theory. The problem with the latter
solution is that we never truly know whether the information we introduce is good enough to
reduce estimator Mean Square Error.
</p>
<p>4.4 Overspecification and Underspecification of the Regression
Equation
</p>
<p>So far we have assumed that the true linear regression relationship is always correctly specified.
This is likely to be violated in practice. In order to keep things simple, we consider the case
where the true model is a simple regression with one regressor X1.
</p>
<p>True model: Yi = α+ β1X1i + ui
</p>
<p>with ui &sim; IID(0, σ
2), but the estimated model is overspecified with the inclusion of an additional
</p>
<p>irrelevant variable X2, i.e.,
</p>
<p>Estimated model: Ŷi = α̂+ β̂1X1i + β̂2X2i
</p>
<p>From the previous section, it is clear that β̂1 =
&sum;n
</p>
<p>i=1 ν̂1iYi/
&sum;n
</p>
<p>i=1 ν̂
2
1i where ν̂1 is the OLS
</p>
<p>residuals of X1 on X2. Substituting the true model for Y we get
</p>
<p>β̂1 = β1
&sum;n
</p>
<p>i=1 ν̂1iX1i/
&sum;n
</p>
<p>i=1 ν̂
2
1i +
</p>
<p>&sum;n
i=1 ν̂1iui/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
1i
</p>
<p>since
&sum;n
</p>
<p>i=1 ν̂1i = 0. But, X1i = X̂1i + ν̂1i and
&sum;n
</p>
<p>i=1 X̂1iν̂1i = 0 implying that
&sum;n
</p>
<p>i=1 ν̂1iX1i =&sum;
n
i=1ν̂
</p>
<p>2
1i. Hence,
</p>
<p>β̂1 = β1 +
&sum;n
</p>
<p>i=1 ν̂1iui/
&sum;n
</p>
<p>i=1 ν̂
2
1i (4.8)
</p>
<p>and E(β̂1) = β1 since ν̂1 is a linear combination of the X&rsquo;s, and E(Xku) = 0 for k = 1, 2. Also,
</p>
<p>var(β̂1) = σ
2/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
1i = σ
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
1i(1&minus;R21) (4.9)
</p>
<p>where x1i = X1i &minus; X̄1 and R21 is the R2 of the regression of X1 on X2. Using the true model
to estimate β1, one would get b1 =
</p>
<p>&sum;n
i=1 x1iyi/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
1i with E(b1) = β1 and var(b1) =</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Overspecification and Underspecification of the Regression Equation 77
</p>
<p>σ2/
&sum;n
</p>
<p>i=1 x
2
1i. Hence, var(β̂1) &ge; var(b1). Note also that in the overspecified model, the estimate
</p>
<p>for β2 which has a true value of zero is given by
</p>
<p>β̂2 =
&sum;n
</p>
<p>i=1 ν̂2iYi/
&sum;n
</p>
<p>i=1 ν̂
2
2i (4.10)
</p>
<p>where ν̂2 is the OLS residual of X2 on X1. Substituting the true model for Y we get
</p>
<p>β̂2 =
&sum;n
</p>
<p>i=1 ν̂2iui/
&sum;n
</p>
<p>i=1 ν̂
2
2i (4.11)
</p>
<p>since
&sum;n
</p>
<p>i=1 ν̂2iX1i = 0 and
&sum;n
</p>
<p>i=1 ν̂2i = 0. Hence, E(β̂2) = 0 since ν̂2 is a linear combination
of the X&rsquo;s and E(Xku) = 0 for k = 1, 2. In summary, overspecification still yields unbiased
estimates of β1 and β2, but the price is a higher variance.
Similarly, the true model could be a two-regressors model
</p>
<p>True model: Yi = α+ β1X1i + β2X2i + ui
</p>
<p>where ui &sim; IID(0, σ
2) but the estimated model is
</p>
<p>Estimated model: Ŷi = α̂+ β̂1X1i
</p>
<p>The estimated model omits a relevant variable X2 and underspecifies the true relationship. In
this case
</p>
<p>β̂1 =
&sum;n
</p>
<p>i=1 x1iYi/
&sum;n
</p>
<p>i=1 x
2
1i (4.12)
</p>
<p>where x1i = X1i &minus; X̄1. Substituting the true model for Y we get
</p>
<p>β̂1 = β1 + β2
&sum;n
</p>
<p>i=1 x1iX2i/
&sum;n
</p>
<p>i=1 x
2
1i +
</p>
<p>&sum;n
i=1 x1iui/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
1i (4.13)
</p>
<p>Hence, E(β̂1) = β1 + β2b12 since E(x1u) = 0 with b12 =
&sum;n
</p>
<p>i=1 x1iX2i/
&sum;n
</p>
<p>i=1 x
2
1i. Note that b12
</p>
<p>is the regression slope estimate obtained by regressing X2 on X1 and a constant. Also, the
</p>
<p>var(β̂1) = E(β̂1 &minus; E(β̂1))2 = E(
&sum;n
</p>
<p>i=1 x1iui/
&sum;n
</p>
<p>i=1 x
2
1i)
</p>
<p>2 = σ2/
&sum;n
</p>
<p>i=1 x
2
1i
</p>
<p>which understates the variance of the estimate of β1 obtained from the true model, i.e., b1 =&sum;n
i=1 ν̂1iYi/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
1i with
</p>
<p>var(b1) = σ
2/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
1i = σ
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
1i(1&minus;R21) &ge; var(β̂1). (4.14)
</p>
<p>In summary, underspecification yields biased estimates of the regression coefficients and under-
states the variance of these estimates. This is also an example of imposing a zero restriction
on β2 when in fact it is not true. This introduces bias, because the restriction is wrong, but
reduces the variance because it imposes more information even if this information may be false.
We will encounter this general principle again when we discuss distributed lags in Chapter 6.</p>
<p/>
</div>
<div class="page"><p/>
<p>78 Chapter 4: Multiple Regression Analysis
</p>
<p>4.5 R-Squared Versus R-Bar-Squared
</p>
<p>Since OLS minimizes the residual sums of squares, adding one or more variables to the regres-
sion cannot increase this residual sums of squares. After all, we are minimizing over a larger
dimension parameter set and the minimum there is smaller or equal to that over a subset of the
parameter space, see problem 4. Therefore, for the same dependent variable Y , adding more vari-
ables makes
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i non-increasing and R
</p>
<p>2 non-decreasing, since R2 = 1&minus; (
&sum;n
</p>
<p>i=1 e
2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ).
</p>
<p>Hence, a criteria of selecting a regression that &ldquo;maximizes R2&rdquo; does not make sense, since we
can add more variables to this regression and improve on this R2 (or at worst leave it the same).
In order to penalize the researcher for adding an extra variable, one computes
</p>
<p>R̄2 = 1&minus; [
&sum;n
</p>
<p>i=1 e
2
i /(n&minus;K)]/[
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i /(n&minus; 1)] (4.15)
</p>
<p>where
&sum;n
</p>
<p>i=1 e
2
i and
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i have been adjusted by their degrees of freedom. Note that the
</p>
<p>numerator is the s2 of the regression and is equal to
&sum;n
</p>
<p>i=1 e
2
i /(n &minus; K). This differs from the
</p>
<p>s2 in Chapter 3 in the degrees of freedom. Here, it is n &minus; K, because we have estimated K
coefficients, or because (4.2) represents K relationships among the residuals. Therefore knowing
(n&minus;K) residuals we can deduce the other K residuals from (4.2).
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i is non-increasing as
</p>
<p>we add more variables, but the degrees of freedom decrease by one with every added variable.
Therefore, s2 will decrease only if the effect of the
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i decrease outweighs the effect of the
</p>
<p>one degree of freedom loss on s2. This is exactly the idea behind R̄2, i.e., penalizing each added
variable by decreasing the degrees of freedom by one. Hence, this variable will increase R̄2 only
if the reduction in
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i outweighs this loss, i.e., only if s
</p>
<p>2 is decreased. Using the definition
of R̄2, one can relate it to R2 as follows:
</p>
<p>(1&minus; R̄2) = (1&minus;R2)[(n&minus; 1)/(n&minus;K)] (4.16)
</p>
<p>4.6 Testing Linear Restrictions
</p>
<p>In the simple linear regression chapter, we proved that the OLS estimates are BLUE provided
assumptions 1 to 4 were satisfied. Then we imposed normality on the disturbances, assumption
5, and proved that the OLS estimators are in fact the maximum likelihood estimators. Then we
derived the Cramér-Rao lower bound, and proved that these estimates are efficient. This will
be done in matrix form in Chapter 7 for the multiple regression case. Under normality one can
test hypotheses about the regression. Basically, any regression package will report the OLS esti-
mates, their standard errors and the corresponding t-statistics for the null hypothesis that each
individual coefficient is zero. These are tests of significance for each coefficient separately. But
one may be interested in a joint test of significance for two or more coefficients simultaneously,
or simply testing whether linear restrictions on the coefficients of the regression are satisfied.
This will be developed more formally in Chapter 7. For now, all we assume is that the reader can
perform regressions using his or her favorite software like EViews, Stata, SAS, TSP, SHAZAM,
LIMDEP or GAUSS. The solutions to (4.2) or (4.3) result in the OLS estimates. These multiple
regression coefficient estimates can be interpreted as simple regression estimates as shown in
section 4.3. This allows a simple derivation of their standard errors. Now, we would like to use
these regressions to test linear restrictions. The strategy followed is to impose these restrictions
on the model and run the resulting restricted regression. The corresponding Restricted Residual</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Testing Linear Restrictions 79
</p>
<p>Sums of Squares is denoted by RRSS. Next, one runs the regression without imposing these
linear restrictions to obtain the Unrestricted Residual Sums of Squares, which we denote by
URSS. Finally, one forms the following F -statistic:
</p>
<p>F =
(RRSS &minus;URSS )/ℓ
URSS/(n&minus;K) &sim; Fℓ,n&minus;K (4.17)
</p>
<p>where ℓ denotes the number of restrictions, and n &minus; K gives the degrees of freedom of the
unrestricted model. The idea behind this test is intuitive. If the restrictions are true, then the
RRSS should not be much different from the URSS. If RRSS is different from URSS, then
we reject these restrictions. The denominator of the F -statistic is a consistent estimate of the
unrestricted regression variance. Dividing by the latter makes the F -statistic invariant to units
of measurement. Let us consider two examples:
</p>
<p>Example 2: Testing the joint significance of two regression coefficients. For e.g., let us test the
following null hypothesis H0;β2 = β3 = 0. These are two restrictions β2 = 0 and β3 = 0
and they are to be tested jointly. We know how to test for β2 = 0 alone or β3 = 0 alone
with individual t-tests. This is a test of joint significance of the two coefficients. Imposing this
restriction, means the removal of X2 and X3 from the regression, i.e., running the regression
of Y on X4, . . . , XK excluding X2 and X3. Hence, the number of parameters to be estimated
becomes (K &minus; 2) and the degrees of freedom of this restricted regression are n&minus; (K &minus; 2). The
unrestricted regression is the one including all the X&rsquo;s in the model. Its degrees of freedom
are (n&minus;K). The number of restrictions are 2 and this can also be inferred from the difference
between the degrees of freedom of the restricted and unrestricted regressions. All the ingredients
are now available for computing F in (4.17) and this will be distributed as F2,n&minus;K .
</p>
<p>Example 3: Test the equality of two regression coefficients H0;β3 = β4 against the alternative
that H1;β3 	= β4. Note that H0 can be rewritten as H0;β3&minus;β4 = 0. This can be tested using a
t-statistic that tests whether d = β3 &minus; β4 is equal to zero. From the unrestricted regression, we
can obtain d̂ = β̂3 &minus; β̂4 with var(d̂) = var(β̂3)+var(β̂4)&minus; 2cov(β̂3, β̂4). The variance-covariance
matrix of the regression coefficients can be printed out with any regression package. In section
4.3, we gave these variances and covariances a simple regression interpretation. This means
</p>
<p>that se(d̂) =
</p>
<p>&radic;
var(d̂) and the t-statistic is simply t = (d̂ &minus; 0)/se(d̂) which is distributed as
</p>
<p>tn&minus;K under H0. Alternatively, one can run an F -test with the RRSS obtained from running the
following regression
</p>
<p>Yi = α+ β2X2i + β3i(X3i +X4i) + β5X5i + ..+ βKXKi + ui
</p>
<p>with β3 = β4 substituted in for β4. This regression has the variable (X3i+X4i) rather than X3i
and X4i separately. The URSS is the regression of Y on all the X&rsquo;s in the model. The degrees
of freedom of the resulting F -statistic are 1 and n&minus;K. The numerator degree of freedom states
that there is only one restriction. It will be proved in Chapter 7 that the square of the t-statistic
is exactly equal to the F -statistic just derived. Both methods of testing are equivalent. The first
one computes only the unrestricted regression and involves some further variance computations,
while the latter involves running two regressions and computing the usual F -statistic.
</p>
<p>Example 4: Test the joint hypothesis H0;β3 = 1 and β2 &minus; 2β4 = 0. These two restrictions are
usually obtained from prior information or imposed by theory. The first restriction is β3 = 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>80 Chapter 4: Multiple Regression Analysis
</p>
<p>The value 1 could have been any other constant. The second restriction shows that a linear
combination of β2 and β4 is equal to zero. Substituting these restrictions in (4.1) we get
</p>
<p>Yi = α+ β2X2i +X3i +
1
2β2X4i + β5X5i + ..+ βKXKi + ui
</p>
<p>which can be written as
</p>
<p>Yi &minus;X3i = α+ β2(X2i + 12X4i) + β5X5i + ..+ βKXKi + ui
</p>
<p>Therefore, the RRSS can be obtained by regressing (Y &minus;X3) on (X2 + 12X4), X5, . . . , XK . This
regression has n &minus; (K &minus; 2) degrees of freedom. The URSS is the regression with all the X&rsquo;s
included. The resulting F -statistic has 2 and n&minus;K degrees of freedom.
</p>
<p>Example 5: Testing constant returns to scale in a Cobb-Douglas production function. Q =
AKαLβEγM δeu is a Cobb-Douglas production function with capital(K), labor(L), energy(E)
and material(M). Constant returns to scale means that a proportional increase in the inputs pro-
duces the same proportional increase in output. Let this proportional increase be λ, then K&lowast; =
λK, L&lowast; = λL, E&lowast; = λE and M&lowast; = λM . Q&lowast; = λ(α+β+γ+δ)AKαLβEγM δeu = λ(α+β+γ+δ)Q.
For this last term to be equal to λQ, the following restriction must hold: α + β + γ + δ = 1.
Hence, a test of constant returns to scale is equivalent to testing H0;α + β + γ + δ = 1. The
Cobb-Douglas production function is nonlinear in the variables, and can be linearized by taking
logs of both sides, i.e.,
</p>
<p>logQ = logA+ αlogK + βlogL+ γlogE + δlogM + u (4.18)
</p>
<p>This is a linear regression with Y = logQ, X2 = logK, X3 = logL, X4 = logE and X5 = logM .
Ordinary least squares is BLUE on this non-linear model as long as u satisfies assumptions
1&ndash;4. Note that these disturbances entered the original Cobb-Douglas production function mul-
tiplicatively as exp(ui). Had these disturbances entered additively as Q = AK
</p>
<p>αLβEγM δ + u
then taking logs does not simplify the right hand side and one has to estimate this with non-
linear least squares, see Chapter 8. Now we can test constant returns to scale as follows. The
unrestricted regression is given by (4.18) and its degrees of freedom are n &minus; 5. Imposing H0
means substituting the linear restriction by replacing say β by (1&minus;α&minus;γ&minus; δ). This results after
collecting terms in the following restricted regression with one less parameter
</p>
<p>log(Q/L) = logA+ αlog(K/L) + γlog(E/L) + δlog(M/L) + u (4.19)
</p>
<p>The degrees of freedom are n&minus; 4. Once again all the ingredients for the test in (4.17) are there
and this statistic is distributed as F1, n&minus; 5 under the null hypothesis.
</p>
<p>Example 6: Joint significance of all the slope coefficients. The null hypothesis is
</p>
<p>H0;β2 = β3 = .. = βK = 0
</p>
<p>against the alternative H1; at least one βk 	= 0 for k = 2, . . . ,K. Under the null, only the
constant is left in the regression. Problem 3.2 showed that for a regression of Y on a constant
only, the least squares estimate of α is Ȳ . This means that the corresponding residual sum of
squares is
</p>
<p>&sum;n
i=1(Yi&minus;Ȳ )2. Therefore,RRSS = Total sums of squares of regression (4.1) = Σni=1y2i .</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Dummy Variables 81
</p>
<p>The URSS is the usual residual sums of squares
&sum;n
</p>
<p>i=1 e
2
i from the unrestricted regression given
</p>
<p>by (4.1). Hence, the corresponding F -statistic for H0 is
</p>
<p>F =
(TSS &minus;RSS)/(K &minus; 1)
</p>
<p>RSS/(n&minus;K) =
(
&sum;n
</p>
<p>i=1 y
2
i &minus;
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i )/(K &minus; 1)&sum;n
</p>
<p>i=1 e
2
i /(n&minus;K)
</p>
<p>=
R2
</p>
<p>1&minus;R2 &middot;
n&minus;K
K &minus; 1 (4.20)
</p>
<p>where R2 = 1&minus;(
&sum;n
</p>
<p>i=1 e
2
i /
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ). This F -statistic has (K&minus;1) and (n&minus;K) degrees of freedom
</p>
<p>under H0, and is usually reported by regression packages.
</p>
<p>4.7 Dummy Variables
</p>
<p>Many explanatory variables are qualitative in nature. For example, the head of a household
could be male or female, white or non-white, employed or unemployed. In this case, one codes
these variables as &ldquo;M&rdquo; for male and &ldquo;F&rdquo; for female, or change this qualitative variable into a
quantitative variable called FEMALE which takes the value &ldquo;0&rdquo; for male and &ldquo;1&rdquo; for female.
This obviously begs the question: &ldquo;why not have a variable MALE that takes on the value 1 for
male and 0 for female?&rdquo; Actually, the variable MALE would be exactly 1-FEMALE. In other
words, the zero and one can be thought of as a switch, which turns on when it is 1 and off when
it is 0. Suppose that we are interested in the earnings of households, denoted by EARN, and
MALE and FEMALE are the only explanatory variables available, then problem 10 asks the
reader to verify that running OLS on the following model:
</p>
<p>EARN = αMMALE + αFFEMALE + u (4.21)
</p>
<p>gives α̂M = &ldquo;average earnings of the males in the sample&rdquo; and α̂F = &ldquo;average earnings of
the females in the sample.&rdquo; Notice that there is no intercept in (4.21), this is because of what
is known in the literature as the &ldquo;dummy variable trap.&rdquo; Briefly stated, there will be perfect
multicollinearity between MALE, FEMALE and the constant. In fact, MALE + FEMALE =
1. Some researchers may choose to include the intercept and exclude one of the sex dummy
variables, say MALE, then
</p>
<p>EARN = α+ βFEMALE + u (4.22)
</p>
<p>and the OLS estimates give α̂ = &ldquo;average earnings of males in the sample&rdquo; = α̂M , while β̂ =
α̂F &minus; α̂M = &ldquo;the difference in average earnings between females and males in the sample.&rdquo;
Regression (4.22) is more popular when one is interested in contrasting the earnings between
males and females and obtaining with one regression the markup or markdown in average earn-
ings (α̂F &minus; α̂M ) as well as the test of whether this difference is statistically different from zero.
This would be simply the t-statistic on β̂ in (4.22). On the other hand, if one is interested in
estimating the average earnings of males and females separately, then model (4.21) should be
the one to consider. In this case, the t-test for α̂F &minus; α̂M = 0 would involve further calcula-
tions not directly given from the regression in (4.21) but similar to the calculations given in
Example 3.
</p>
<p>What happens when another qualitative variable is included, to depict another classification
of the individuals in the sample, say for example, race? If there are three race groups in the
sample, WHITE, BLACK and HISPANIC. One could create a dummy variable for each of
these classifications. For example, WHITE will take the value 1 when the individual is white</p>
<p/>
</div>
<div class="page"><p/>
<p>82 Chapter 4: Multiple Regression Analysis
</p>
<p>and 0 when the individual is non-white. Note that the dummy variable trap does not allow
the inclusion of all three categories as they sum up to 1. Also, even if the intercept is dropped,
once MALE and FEMALE are included, perfect multicollinearity is still present because MALE
+ FEMALE = WHITE + BLACK + HISPANIC. Therefore, one category from race should
be dropped. Suits (1984) argues that the researcher should use the dummy variable category
omission to his or her advantage, in interpreting the results, keeping in mind the purpose of
the study. For example, if one is interested in comparing earnings across the sexes holding race
constant, the omission of MALE or FEMALE is natural, whereas, if one is interested in the race
differential in earnings holding gender constant, one of the race variables should be omitted.
Whichever variable is omitted, this becomes the base category for which the other earnings are
compared. Most researchers prefer to keep an intercept, although regression packages allow for
a no intercept option. In this case one should omit one category from each of the race and sex
classifications. For example, if MALE and WHITE are omitted:
</p>
<p>EARN = α+ βFFEMALE + βBBLACK + βHHISPANIC + u (4.23)
</p>
<p>Assuming the error u satisfies all the classical assumptions, and taking expected values of both
sides of (4.23), one can see that the intercept α = the expected value of earnings of the omitted
category which is &ldquo;white males&rdquo;. For this category, all the other switches are off. Similarly,
α + βF is the expected value of earnings of &ldquo;white females,&rdquo; since the FEMALE switch is
on. One can conclude that βF = difference in the expected value of earnings between white
females and white males. Similarly, one can show that α+βB is the expected earnings of &ldquo;black
males&rdquo; and α+ βF + βB is the expected earnings of &ldquo;black females.&rdquo; Therefore, βF represents
the difference in expected earnings between black females and black males. In fact, problem
11 asks the reader to show that βF represents the difference in expected earnings between
hispanic females and hispanic males. In other words, βF represents the differential in expected
earnings between females and males holding race constant. Similarly, one can show that βB is
the difference in expected earnings between blacks and whites holding sex constant, and βH is
the differential in expected earnings between hispanics and whites holding sex constant. The
main key to the interpretation of the dummy variable coefficients is to be able to turn on and
turn off the proper switches, and write the correct expectations.
The real regression will contain other quantitative and qualitative variables, like
</p>
<p>EARN = α+ βFFEMALE + βBBLACK + βHHISPANIC + γ1EXP (4.24)
</p>
<p>+γ2EXP
2 + γ3EDUC + γ4UNION + u
</p>
<p>where EXP is years of job experience, EDUC is years of education, and UNION is 1 if the
individual belongs to a union and 0 otherwise. EXP2 is the squared value of EXP. Once again,
one can interpret the coefficients of these regressions by turning on or off the proper switches. For
example, γ4 is interpreted as the expected difference in earnings between union and non-union
members holding all other variables included in (4.24) constant. Halvorsen and Palmquist (1980)
warn economists about the interpretation of dummy variable coefficients when the dependent
variable is in logs. For example, if the earnings equation is semi-logarithmic:
</p>
<p>log(Earnings) = α+ βUNION + γEDUC + u
</p>
<p>then γ = % change in earnings for one extra year of education, holding union membership
constant. But, what about the returns for union membership? If we let Y1 = log(Earnings)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Dummy Variables 83
</p>
<p>when the individual belongs to a union, and Y0 = log(Earnings) when the individual does not
belong to a union, then g = % change in earnings due to union membership = (eY1 &minus; eY0)/eY0 .
Equivalently, one can write that log(1 + g) = Y1 &minus; Y0 = β, or that g = eβ &minus; 1. In other words,
one should not hasten to conclude that β has the same interpretation as γ. In fact, the %
change in earnings due to union membership is eβ &minus; 1 and not β. The error involved in using β̂
rather than eβ̂&minus;1 to estimate g could be substantial, especially if β̂ is large. For example, when
β̂ = 0.5, 0.75, 1; ĝ = eβ̂ &minus; 1 = 0.65, 1.12, 1.72, respectively. Kennedy (1981) notes that if β̂ is
unbiased for β, ĝ is not necessarily unbiased for g. However, consistency of β̂ implies consistency
</p>
<p>for ĝ. If one assumes log-normal distributed errors, then E(eβ̂) = eβ+0.5Var(β̂). Based on this
</p>
<p>result, Kennedy (1981) suggests estimating g by g̃ = eβ̂+0.5V̂ar(β̂)-1, where V̂ar(β̂) is a consistent
estimate of Var(β̂).
</p>
<p>Another use of dummy variables is in taking into account seasonal factors, i.e., including
3 seasonal dummy variables with the omitted season becoming the base for comparison.1 For
example:
</p>
<p>Sales = α+ βWWinter + βSSpring + βFFall + γ1Price+ u (4.25)
</p>
<p>the omitted season being the Summer season, and if (4.25) models the sales of air-conditioning
units, then βF is the difference in expected sales between the Fall and Summer seasons, holding
the price of an air-conditioning unit constant. If these were heating units one may want to
change the base season for comparison.
Another use of dummy variables is for War years, where consumption is not at its normal
</p>
<p>level say due to rationing. Consider estimating the following consumption function
</p>
<p>Ct = α+ βYt + δWARt + ut t = 1, 2, . . . , T (4.26)
</p>
<p>where Ct denotes real per capita consumption, Yt denotes real per capita personal disposable
income, and WARt is a dummy variable taking the value 1 if it is a War time period and 0
otherwise. Note that the War years do not affect the slope of the consumption line with respect
to income, only the intercept. The intercept is α in non-War years and α + δ in War years. In
other words, the marginal propensity out of income is the same in War and non-War years, only
the level of consumption is different.
Of course, one can dummy other unusual years like periods of strike, years of natural disaster,
</p>
<p>earthquakes, floods, hurricanes, or external shocks beyond control, like the oil embargo of 1973.
If this dummy includes only one year like 1973, then the dummy variable for 1973, call it
D73, takes the value 1 for 1973 and zero otherwise. Including D73 as an extra variable in the
regression has the effect of removing the 1973 observation from estimation purposes, and the
resulting regression coefficients estimates are exactly the same as those obtained excluding
the 1973 observation and its corresponding dummy variable. In fact, using matrix algebra in
Chapter 7, we will show that the coefficient estimate of D73 is the forecast error for 1973,
using the regression that ignores the 1973 observations. In addition, the standard error of the
dummy coefficient estimates is the standard error of this forecast. This is a much easier way
of obtaining the forecast error and its standard error from the regression package without
additional computations, see Salkever (1976). More on this in Chapter 7.</p>
<p/>
</div>
<div class="page"><p/>
<p>84 Chapter 4: Multiple Regression Analysis
</p>
<p>Interaction Effects
</p>
<p>So far the dummy variables have been used to shift the intercept of the regression keeping
the slopes constant. One can also use the dummy variables to shift the slopes by letting them
interact with the explanatory variables. For example, consider the following earnings equation:
</p>
<p>EARN = α+ αFFEMALE + βEDUC + u (4.27)
</p>
<p>In this regression, only the intercept shifts from males to females. The returns to an extra year
of education is simply β, which is assumed to be the same for males as well as females. But if
we now introduce the interaction variable (FEMALE &times; EDUC), then the regression becomes:
</p>
<p>EARN = α+ αFFEMALE + βEDUC + γ(FEMALE &times; EDUC ) + u (4.28)
</p>
<p>In this case, the returns to an extra year of education depends upon the sex of the individual.
In fact, &part;(EARN )/&part;(EDUC ) = β + γ(FEMALE ) = β if male, and β + γ if female. Note that
the interaction variable = EDUC if the individual is female and 0 if the individual is male.
Estimating (4.28) is equivalent to estimating two earnings equations, one for males and an-
</p>
<p>other one for females, separately. The only difference is that (4.28) imposes the same variance
across the two groups, whereas separate regressions do not impose this, albeit restrictive, equal-
ity of the variances assumption. This set-up is ideal for testing the equality of slopes, equality
of intercepts, or equality of both intercepts and slopes across the sexes. This can be done with
the F -test described in (4.17). In fact, for H0; equality of slopes, given different intercepts,
the restricted residuals sum of squares (RRSS) is obtained from (4.27), while the unrestricted
residuals sum of squares (URSS) is obtained from (4.28). Problem 12 asks the reader to set
up the F -test for the following null hypothesis: (i) equality of slopes and intercepts, and (ii)
equality of intercepts given the same slopes.
</p>
<p>Dummy variables have many useful applications in economics. For example, several tests
including the Chow (1960) test, and Utts (1982) Rainbow test described in Chapter 8, can be
applied using dummy variable regressions. Additionally, they can be used in modeling splines,
see Poirier (1976) and Suits, Mason and Chan (1978), and fixed effects in panel data, see
Chapter 12. Finally, when the dependent variable is itself a dummy variable, the regression
equation needs special treatment, see Chapter 13 on qualitative limited dependent variables.
</p>
<p>Empirical Example: Table 4.1 gives the results of a regression on 595 individuals drawn from
the Panel Study of Income Dynamics (PSID) in 1982. This data is provided on the Springer
web site as EARN.ASC. A description of the data is given in Cornwell and Rupert (1988). In
particular, log wage is regressed on years of education (ED), weeks worked (WKS), years of
full-time work experience (EXP), occupation (OCC = 1, if the individual is in a blue-collar
occupation), residence (SOUTH = 1, SMSA = 1, if the individual resides in the South, or
in a standard metropolitan statistical area), industry (IND = 1, if the individual works in a
manufacturing industry), marital status (MS = 1, if the individual is married), sex and race
(FEM = 1, BLK = 1, if the individual is female or black), union coverage (UNION = 1, if the
individual&rsquo;s wage is set by a union contract). These results show that the returns to an extra year
of schooling is 5.7%, holding everything else constant. It shows that Males on the average earn
more than Females. Blacks on the average earn less than Whites, and Union workers earn more
than non-union workers. Individuals residing in the South earn less than those living elsewhere.
Those residing in a standard metropolitan statistical area earn more on the average than those</p>
<p/>
</div>
<div class="page"><p/>
<p>85
</p>
<p>Table 4.1 Earnings Regression for 1982
</p>
<p>Dependent Variable: LWAGE
Analysis of Variance
</p>
<p>Sum of Mean
Source DF Squares Square F Value Prob &gt; F
</p>
<p>Model 12 52.48064 4.37339 41.263 0.0001
Error 582 61.68465 0.10599
C Total 594 114.16529
</p>
<p>Root MSE 0.32556 R-square 0.4597
Dep Mean 6.95074 Adj R-sq 0.4485
C.V. 4.68377
</p>
<p>Parameter Estimates
</p>
<p>Parameter Standard T for H0:
</p>
<p>Variable DF Estimate Error Parameter=0 Prob &gt; |T|
INTERCEP 1 5.590093 0.19011263 29.404 0.0001
WKS 1 0.003413 0.00267762 1.275 0.2030
SOUTH 1 &ndash;0.058763 0.03090689 &ndash;1.901 0.0578
SMSA 1 0.166191 0.02955099 5.624 0.0001
MS 1 0.095237 0.04892770 1.946 0.0521
EXP 1 0.029380 0.00652410 4.503 0.0001
EXP2 1 &ndash;0.000486 0.00012680 &ndash;3.833 0.0001
OCC 1 &ndash;0.161522 0.03690729 &ndash;4.376 0.0001
IND 1 0.084663 0.02916370 2.903 0.0038
UNION 1 0.106278 0.03167547 3.355 0.0008
FEM 1 &ndash;0.324557 0.06072947 &ndash;5.344 0.0001
BLK 1 &ndash;0.190422 0.05441180 &ndash;3.500 0.0005
ED 1 0.057194 0.00659101 8.678 0.0001
</p>
<p>who do not. Individuals who work in a manufacturing industry or are not blue collar workers
or are married earn more on the average than those who are not. For EXP2 = (EXP )2, this
regression indicates a significant quadratic relationship between earnings and experience. All
the variables were significant at the 5% level except for WKS, SOUTH and MS.
</p>
<p>Note
</p>
<p>1. There are more sophisticated ways of seasonal adjustment than introducing seasonal dummies, see
Judge et al. (1985).
</p>
<p>Problems
</p>
<p>1. For the Cigarette Data given in Table 3.2. Run the following regressions:
</p>
<p>(a) Real per capita consumption of cigarettes on real price and real per capita income. (All
variables are in log form, and all regressions in this problem include a constant).
</p>
<p>Problems</p>
<p/>
</div>
<div class="page"><p/>
<p>86 Chapter 4: Multiple Regression Analysis
</p>
<p>(b) Real per capita consumption of cigarettes on real price.
</p>
<p>(c) Real per capita income on real price.
</p>
<p>(d) Real per capita consumption on the residuals of part (c).
</p>
<p>(e) Residuals from part (b) on the residuals in part (c).
</p>
<p>(f) Compare the regression slope estimates in parts (d) and (e) with the regression coefficient
estimate of the real income coefficient in part (a), what do you conclude?
</p>
<p>2. Simple Versus Multiple Regression Coefficients. This is based on Baltagi (1987b). Consider the
multiple regression
</p>
<p>Yi = α+ β2X2i + β3X3i + ui i = 1, 2, . . . , n
</p>
<p>along with the following auxiliary regressions:
</p>
<p>X2i = â+ b̂X3i + ν̂2i
</p>
<p>X3i = ĉ+ d̂X2i + ν̂3i
</p>
<p>In section 4.3, we showed that β̂2, the OLS estimate of β2 can be interpreted as a simple regression
</p>
<p>of Y on the OLS residuals ν̂2. A similar interpretation can be given to β̂3. Kennedy (1981, p. 416)
</p>
<p>claims that β̂2 is not necessarily the same as δ̂2, the OLS estimate of δ2 obtained from the regression
Y on ν̂2, ν̂3 and a constant, Yi = γ+ δ2ν̂2i + δ3ν̂3i +wi. Prove this claim by finding a relationship
between the β̂&rsquo;s and the δ̂&rsquo;s.
</p>
<p>3. For the simple regression Yi = α+ βXi + ui considered in Chapter 3, show that
</p>
<p>(a) β̂OLS =
&sum;n
</p>
<p>i=1 xiyi/
&sum;n
</p>
<p>i=1 x
2
i can be obtained using the residual interpretation by regressing
</p>
<p>X on a constant first, getting the residuals ν̂ and then regressing Y on ν̂.
</p>
<p>(b) α̂OLS = Ȳ &minus; β̂OLSX̄ can be obtained using the residual interpretation by regressing 1 on X
and obtaining the residuals ω̂ and then regressing Y on ω̂.
</p>
<p>(c) Check the var(α̂OLS) and var(β̂OLS) in parts (a) and (b) with those obtained from the
residualing interpretation.
</p>
<p>4. Effect of Additional Regressors on R2. This is based on Nieswiadomy (1986).
</p>
<p>(a) Suppose that the multiple regression given in (4.1) has K1 regressors in it. Denote the least
squares sum of squared errors by SSE1. Now add K2 regressors so that the total number of
regressors is K = K1 + K2. Denote the corresponding least squares sum of squared errors
by SSE2. Show that SSE2 &le; SSE1, and conclude that the corresponding R-squares satisfy
R22 &ge; R21.
</p>
<p>(b) Derive the equality given in (4.16) starting from the definition of R2 and R̄2.
</p>
<p>(c) Show that the corresponding R̄-squares satisfy R̄21 &ge; R̄22 when the F -statistic for the joint
significance of these additional K2 regressors is less than or equal to one.
</p>
<p>5. Perfect Multicollinearity. Let Y be the output and X2 = skilled labor and X3 = unskilled labor
in the following relationship:
</p>
<p>Yi = α+ β2X2i + β3X3i + β4(X2i +X3i) + β5X
2
2i + β6X
</p>
<p>2
3i + ui
</p>
<p>What parameters are estimable by OLS?
</p>
<p>6. Suppose that we have estimated the parameters of the multiple regression model:
</p>
<p>Yt = β1 + β2Xt2 + β3Xt3 + ut
</p>
<p>by Ordinary Least Squares (OLS) method. Denote the estimated residuals by (et, t = 1, . . . , T )
</p>
<p>and the predicted values by (Ŷt, t = 1, . . . , T ).</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 87
</p>
<p>(a) What is the R2 of the regression of e on a constant, X2 and X3?
</p>
<p>(b) If we regress Y on a constant and Ŷ , what are the estimated intercept and slope coeffi-
cients? What is the relationship between the R2 of this regression and the R2 of the original
regression?
</p>
<p>(c) If we regress Y on a constant and e, what are the estimated intercept and slope coeffi-
cients? What is the relationship between the R2 of this regression and the R2 of the original
regression?
</p>
<p>(d) Suppose that we add a new explanatory variable X4 to the original model and re-estimate
the parameters by OLS. Show that the estimated coefficient of X4 and its estimated standard
error will be the same as in the OLS regression of e on a constant, X2, X3 and X4.
</p>
<p>7. Consider the Cobb-Douglas production function in example 5. How can you test for constant
returns to scale using a t-statistic from the unrestricted regression given in (4.18).
</p>
<p>8. Testing Multiple Restrictions. For the multiple regression given in (4.1). Set up the F -statistic
described in (4.17) for testing
</p>
<p>(a) H0;β2 = β4 = β6.
</p>
<p>(b) H0;β2 = &minus;β3 and β5 &minus; β6 = 1.
</p>
<p>9. Monte Carlo Experiments. Hanushek and Jackson (1977, pp. 60&ndash;65) generated the following data
Yi = 15 + 1X2i + 2X3i + ui for i = 1, 2, . . . , 25 with a fixed set of X2i and X3i, and ui&rsquo;s that
are IID &sim; N(0, 100). For each set of 25 ui&rsquo;s drawn randomly from the normal distribution, a
corresponding set of 25 Yi&rsquo;s are created from the above equation. Then OLS is performed on the
resulting data set. This can be repeated as many times as we can afford. 400 replications were
performed by Hanushek and Jackson. This means that they generated 400 data sets each of size 25
and ran 400 regressions giving 400 OLS estimates of α, β2, β3 and σ
</p>
<p>2. The classical assumptions
are satisfied for this model, by construction, so we expect these OLS estimators to be BLUE, MLE
and efficient.
</p>
<p>(a) Replicate the Monte Carlo experiments of Hanushek and Jackson (1977) and generate the
means of the 400 estimates of the regression coefficients as well as σ2. Are these estimates
unbiased?
</p>
<p>(b) Compute the standard deviation of these 400 estimates and call this σ̂b. Also compute the
average of the 400 standard errors of the regression estimates reported by the regression.
Denote this mean by sb. Compare these two estimates of the standard deviation of the
regression coefficient estimates to the true standard deviation knowing the true σ2. What do
you conclude?
</p>
<p>(c) Plot the frequency of these regression coefficients estimates? Does it resemble its theoretical
distribution.
</p>
<p>(d) Increase the sample size form 25 to 50 and repeat the experiment. What do you observe?
</p>
<p>10. Female and Male Dummy Variables.
</p>
<p>(a) Derive the OLS estimates of αF and αM for Yi = αFFi + αMMi + ui where Y is Earnings,
F is FEMALE and M is MALE, see (4.21). Show that α̂F = ȲF , the average of the Yi&rsquo;s only
for females, and α̂M = ȲM , the average of the Yi&rsquo;s only for males.
</p>
<p>(b) Suppose that the regression is Yi = α + βFi + ui, see (4.22). Show that α̂ = α̂M , and
</p>
<p>β̂ = α̂F &minus; α̂M .
(c) Substitute M = 1&minus; F in (4.21) and show that α = αM and β = αF &minus; αM .
(d) Verify parts (a), (b) and (c) using the earnings data underlying Table 4.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>88 Chapter 4: Multiple Regression Analysis
</p>
<p>11. Multiple Dummy Variables. For equation (4.23)
</p>
<p>EARN = α+ βFFEMALE + βBBLACK + βHHISPANIC + u
</p>
<p>Show that
</p>
<p>(a) E(Earnings/Hispanic Female) = α + βF + βH ; also E(Earnings/Hispanic Male) = α + βH .
Conclude that βF = E(Earnings/Hispanic Female) &ndash; E(Earnings/Hispanic Male).
</p>
<p>(b) E(Earnings/Hispanic Female) &ndash; E(Earnings/White Female) = E(Earnings/Hispanic Male) &ndash;
E(Earnings/White Male) = βH .
</p>
<p>(c) E(Earnings/Black Female) &ndash; E(Earnings/White Female) = E(Earnings/Black Male) &ndash;
E(Earnings/White Male) = βB.
</p>
<p>12. For the earnings equation given in (4.28), how would you set up the F -test and what are the
restricted and unrestricted regressions for testing the following hypotheses:
</p>
<p>(a) The equality of slopes and intercepts for Males and Females.
</p>
<p>(b) The equality of intercepts given the same slopes for Males and Females. Show that the
resulting F -statistic is the square of a t-statistic from the unrestricted regression.
</p>
<p>(c) The equality of intercepts allowing for different slopes for Males and Females. Show that the
resulting F -statistic is the square of a t-statistic from the unrestricted regression.
</p>
<p>(d) Apply your results in parts (a), (b) and (c) to the earnings data underlying Table 4.1.
</p>
<p>13. For the earnings data regression underlying Table 4.1.
</p>
<p>(a) Replicate the regression results given in Table 4.1.
</p>
<p>(b) Verify that the joint significance of all slope coefficients can be obtained from (4.20).
</p>
<p>(c) How would you test the joint restriction that expected earnings are the same for Males and
Females whether Black or Non-Black holding everything else constant?
</p>
<p>(d) How would you test the joint restriction that expected earnings are the same whether the
individual is married or not and whether this individual belongs to a Union or not?
</p>
<p>(e) From Table 4.1 what is your estimate of the % change in earnings due to Union membership?
If the disturbances are assumed to be log-normal, what would be the estimate suggested by
Kennedy (1981) for this % change in earnings?
</p>
<p>(f) What is your estimate of the % change in earnings due to the individual being married?
</p>
<p>14. Crude Quality. Using the data set of U.S. oil field postings on crude prices ($/barrel), gravity
(degree API) and sulphur (% sulphur) given in the CRUDES.ASC file on the Springer web site.
</p>
<p>(a) Estimate the following multiple regression model: POIL = β1+β2GRAVITY + β3 SULPHUR
+ ǫ.
</p>
<p>(b) Regress GRAVITY = α0+α1SULPHUR + νt then compute the residuals (ν̂t). Now perform
the regression
</p>
<p>POIL = γ1 + γ2ν̂t + ǫ
</p>
<p>Verify that γ̂2 is the same as β̂2 in part (a). What does this tell you?
</p>
<p>(c) Regress POIL = φ1 + φ2SULPHUR + w. Compute the residuals (ŵ). Now regress ŵ on ν̂
</p>
<p>obtained from part (b), to get ŵt = δ̂1 + δ̂2ν̂t+ residuals. Show that δ̂2 = β̂2 in part (a).
Again, what does this tell you?
</p>
<p>(d) To illustrate how additional data affects multicollinearity, show how your regression in part
(a) changes when the sample is restricted to the first 25 crudes.
</p>
<p>(e) Delete all crudes with sulphur content outside the range of 1 to 2 percent and run the multiple
regression in part (a). Discuss and interpret these results.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 89
</p>
<p>Table 4.2 U.S. Gasoline Data: 1950&ndash;1987
</p>
<p>QMG PMG POP RGNP
</p>
<p>Year CAR (1,000 Gallons) ($ ) (1,000) (Billion) PGNP
</p>
<p>1950 49195212 40617285 0.272 152271 1090.4 26.1
</p>
<p>1951 51948796 43896887 0.276 154878 1179.2 27.9
</p>
<p>1952 53301329 46428148 0.287 157553 1226.1 28.3
</p>
<p>1953 56313281 49374047 0.290 160184 1282.1 28.5
</p>
<p>1954 58622547 51107135 0.291 163026 1252.1 29.0
</p>
<p>1955 62688792 54333255 0.299 165931 1356.7 29.3
</p>
<p>1956 65153810 56022406 0.310 168903 1383.5 30.3
</p>
<p>1957 67124904 57415622 0.304 171984 1410.2 31.4
</p>
<p>1958 68296594 59154330 0.305 174882 1384.7 32.1
</p>
<p>1959 71354420 61596548 0.311 177830 1481.0 32.6
</p>
<p>1960 73868682 62811854 0.308 180671 1517.2 33.2
</p>
<p>1961 75958215 63978489 0.306 183691 1547.9 33.6
</p>
<p>1962 79173329 62531373 0.304 186538 1647.9 34.0
</p>
<p>1963 82713717 64779104 0.304 189242 1711.6 34.5
</p>
<p>1964 86301207 67663848 0.312 191889 1806.9 35.0
</p>
<p>1965 90360721 70337126 0.321 194303 1918.5 35.7
</p>
<p>1966 93962030 73638812 0.332 196560 2048.9 36.6
</p>
<p>1967 96930949 76139326 0.337 198712 2100.3 37.8
</p>
<p>1968 101039113 80772657 0.348 200706 2195.4 39.4
</p>
<p>1969 103562018 85416084 0.357 202677 2260.7 41.2
</p>
<p>1970 106807629 88684050 0.364 205052 2250.7 43.4
</p>
<p>1971 111297459 92194620 0.361 207661 2332.0 45.6
</p>
<p>1972 117051638 95348904 0.388 209896 2465.5 47.5
</p>
<p>1973 123811741 99804600 0.524 211909 2602.8 50.2
</p>
<p>1974 127951254 100212210 0.572 213854 2564.2 55.1
</p>
<p>1975 130918918 102327750 0.595 215973 2530.9 60.4
</p>
<p>1976 136333934 106972740 0.631 218035 2680.5 63.5
</p>
<p>1977 141523197 110023410 0.657 220239 2822.4 67.3
</p>
<p>1978 146484336 113625960 0.678 222585 3115.2 72.2
</p>
<p>1979 149422205 107831220 0.857 225055 3192.4 78.6
</p>
<p>1980 153357876 100856070 1.191 227757 3187.1 85.7
</p>
<p>1981 155907473 100994040 1.311 230138 3248.8 94.0
</p>
<p>1982 156993694 100242870 1.222 232520 3166.0 100.0
</p>
<p>1983 161017926 101515260 1.157 234799 3279.1 103.9
</p>
<p>1984 163432944 102603690 1.129 237001 3489.9 107.9
</p>
<p>1985 168743817 104719230 1.115 239279 3585.2 111.5
</p>
<p>1986 173255850 107831220 0.857 241613 3676.5 114.5
</p>
<p>1987 177922000 110467980 0.897 243915 3847.0 117.7
</p>
<p>CAR: Stock of Cars POP: Population
</p>
<p>RMG: Motor Gasoline Consumption RGNP: Real GNP in 1982 dollars
</p>
<p>PMG: Retail Price of Motor Gasoline PGNP: GNP Deflator (1982=100)</p>
<p/>
</div>
<div class="page"><p/>
<p>90 Chapter 4: Multiple Regression Analysis
</p>
<p>15. Consider the U.S. gasoline data from 1950&ndash;1987 given in Table 4.2, and obtained from the file
USGAS.ASC on the Springer web site.
</p>
<p>(a) For the period 1950&ndash;1972 estimate models (1) and (2):
</p>
<p>logQMG = β1 + β2logCAR+ β3logPOP + β4logRGNP (1)
</p>
<p>+β5logPGNP + β6logPMG+ u
</p>
<p>log
QMG
</p>
<p>CAR
= γ1 + γ2log
</p>
<p>RGNP
</p>
<p>POP
+ γ3log
</p>
<p>CAR
</p>
<p>POP
+ γ4log
</p>
<p>PMG
</p>
<p>PGNP
+ ν (2)
</p>
<p>(b) What restrictions should the β&rsquo;s satisfy in model (1) in order to yield the γ&rsquo;s in model (2)?
</p>
<p>(c) Compare the estimates and the corresponding standard errors from models (1) and (2).
</p>
<p>(d) Compute the simple correlations among the X&rsquo;s in model (1). What do you observe?
</p>
<p>(e) Use the Chow-F test to test the parametric restrictions obtained in part (b).
</p>
<p>(f) Estimate equations (1) and (2) now using the full data set 1950&ndash;1987. Discuss briefly the
effects on individual parameter estimates and their standard errors of the larger data set.
</p>
<p>(g) Using a dummy variable, test the hypothesis that gasoline demand per CAR permanently
shifted downward for model (2) following the Arab Oil Embargo in 1973?
</p>
<p>(h) Construct a dummy variable regression that will test whether the price elasticity has changed
after 1973.
</p>
<p>16. Consider the following model for the demand for natural gas by residential sector, call it model
(1):
</p>
<p>logConsit = β0 + β1logPgit + β2logPoit + β3logPeit + β4logHDDit + β5logPIit + uit
</p>
<p>where i = 1, 2, . . . , 6 states and t = 1, 2, . . . , 23 years. Cons is the consumption of natural gas by
residential sector, Pg, Po and Pe are the prices of natural gas, distillate fuel oil, and electricity
of the residential sector. HDD is heating degree days and PI is real per capita personal income.
The data covers 6 states: NY, FL, MI, TX, UT and CA over the period 1967&ndash;1989. It is given in
the NATURAL.ASC file on the Springer web site.
</p>
<p>(a) Estimate the above model by OLS. Call this model (1). What do the parameter estimates
imply about the relationship between the fuels?
</p>
<p>(b) Plot actual consumption versus the predicted values. What do you observe?
</p>
<p>(c) Add a dummy variable for each state except California and run OLS. Call this model (2).
Compute the parameter estimates and standard errors and compare to model (1). Do any
of the interpretations of the price coefficients change? What is the interpretation of the New
York dummy variable? What is the predicted consumption of natural gas for New York in
1989?
</p>
<p>(d) Test the hypothesis that the intercepts of New York and California are the same.
</p>
<p>(e) Test the hypothesis that all the states have the same intercept.
</p>
<p>(f) Add a dummy variable for each state and run OLS without an intercept. Call this model
(3). Compare the parameter estimates and standard errors to the first two models. What is
the interpretation of the coefficient of the New York dummy variable? What is the predicted
consumption of natural gas for New York in 1989?
</p>
<p>(g) Using the regression in part (f), test the hypothesis that the intercepts of New York and
California are the same.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 91
</p>
<p>References
</p>
<p>This chapter draws upon the material in Kelejian and Oates (1989) and Wallace and Silver (1988).
</p>
<p>Several econometrics books have an excellent discussion on dummy variables, see Gujarati (1978), Judge
</p>
<p>et al. (1985), Kennedy (1992), Johnston (1984) and Maddala (2001), to mention a few. Other readings
</p>
<p>referenced in this chapter include:
</p>
<p>Baltagi, B.H. (1987a), &ldquo;To Pool or Not to Pool: The Quality Bank Case,&rdquo; The American Statistician,
41: 150&ndash;152.
</p>
<p>Baltagi, B.H. (1987b), &ldquo;Simple versus Multiple Regression Coefficients,&rdquo; Econometric Theory, Problem
87.1.1, 3: 159.
</p>
<p>Chow, G.C. (1960), &ldquo;Tests of Equality Between Sets of Coefficients in Two Linear Regressions,&rdquo; Econo-
metrica, 28: 591&ndash;605.
</p>
<p>Cornwell, C. and P. Rupert (1988), &ldquo;Efficient Estimation with Panel Data: An Empirical Comparison of
Instrumental Variables Estimators,&rdquo; Journal of Applied Econometrics, 3: 149&ndash;155.
</p>
<p>Dufour, J.M. (1980), &ldquo;Dummy Variables and Predictive Tests for Structural Change,&rdquo; Economics Letters,
6: 241&ndash;247.
</p>
<p>Dufour, J.M. (1982), &ldquo;Recursive Stability of Linear Regression Relationships,&rdquo; Journal of Econometrics,
19: 31&ndash;76.
</p>
<p>Gujarati, D. (1970), &ldquo;Use of Dummy Variables in Testing for Equality Between Sets of Coefficients in
Two Linear Regressions: A Note,&rdquo; The American Statistician, 24: 18&ndash;21.
</p>
<p>Gujarati, D. (1970), &ldquo;Use of Dummy Variables in Testing for Equality Between Sets of Coefficients in
Two Linear Regressions: A Generalization,&rdquo; The American Statistician, 24: 50&ndash;52.
</p>
<p>Halvorsen, R. and R. Palmquist (1980), &ldquo;The Interpretation of Dummy Variables in Semilogarithmic
Equations,&rdquo; American Economic Review, 70: 474&ndash;475.
</p>
<p>Hanushek, E.A. and J.E. Jackson (1977), Statistical Methods for Social Scientists (Academic Press: New
York).
</p>
<p>Hill, R. Carter and L.C. Adkins (2001), &ldquo;Collinearity,&rdquo; Chapter 12 in B.H. Baltagi (ed.) A Companion
to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Kennedy, P.E. (1981), &ldquo;Estimation with Correctly Interpreted Dummy Variables in Semilogarithmic
Equations,&rdquo; American Economic Review, 71: 802.
</p>
<p>Kennedy, P.E. (1981), &ldquo;The Balentine: A Graphical Aid for Econometrics,&rdquo; Australian Economic Papers,
20: 414&ndash;416.
</p>
<p>Kennedy, P.E. (1986), &ldquo;Interpreting Dummy Variables,&rdquo; Review of Economics and Statistics, 68: 174&ndash;175.
</p>
<p>Nieswiadomy, M. (1986), &ldquo;Effect of an Additional Regressor on R2,&rdquo; Econometric Theory, Problem
86.3.1, 2:442.
</p>
<p>Poirier, D. (1976), The Econometrics of Structural Change(North Holland: Amsterdam).
</p>
<p>Salkever, D. (1976), &ldquo;The Use of Dummy Variables to Compute Predictions, Prediction Errors, and
Confidence Intervals,&rdquo; Journal of Econometrics, 4: 393&ndash;397.
</p>
<p>Suits, D. (1984), &ldquo;Dummy Variables: Mechanics vs Interpretation,&rdquo; Review of Economics and Statistics,
66: 132&ndash;139.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 Chapter 4: Multiple Regression Analysis
</p>
<p>Suits, D.B., A. Mason and L. Chan (1978), &ldquo;Spline Functions Fitted by Standard Regression Methods,&rdquo;
Review of Economics and Statistics, 60: 132&ndash;139.
</p>
<p>Utts, J. (1982), &ldquo;The Rainbow Test for Lack of Fit in Regression,&rdquo;Communications in Statistics-Theory
</p>
<p>and Methods, 11: 1801&ndash;1815.
</p>
<p>Appendix
Residual Interpretation of Multiple Regression Estimates
</p>
<p>Proof of Claim 1: Regressing X2 on all the other X&rsquo;s yields residuals ν̂2 that satisfy the usual properties
of OLS residuals similar to those in (4.2), i.e.,
</p>
<p>&sum;n
i=1 ν̂2i = 0,
</p>
<p>&sum;n
i=1 ν̂2iX3i =
</p>
<p>&sum;n
i=1 ν̂2iX4i = .. =
</p>
<p>&sum;n
i=1 ν̂2iXKi = 0 (A.1)
</p>
<p>Note that X2 is the dependent variable of this regression, and X̂2 is the predicted value from this
regression. The latter satisfies
</p>
<p>&sum;n
i=1 ν̂2iX̂2i = 0. This holds because X̂2 is a linear combination of the
</p>
<p>other X&rsquo;s, all of which satisfy (A.1). Turn now to the estimated regression equation:
</p>
<p>Yi = α̂+ β̂2X2i + ..+ β̂KXKi + ei (A.2)
</p>
<p>Multiply (A.2) by X2i and sum
</p>
<p>&sum;n
i=1 X2iYi = α̂
</p>
<p>&sum;n
i=1 X2i + β̂2
</p>
<p>&sum;n
i=1 X
</p>
<p>2
2i + ..+ β̂K
</p>
<p>&sum;n
i=1 X2iXKi (A.3)
</p>
<p>This uses the fact that
&sum;n
</p>
<p>i=1 X2iei = 0. Alternatively, (A.3) is just the second equation from (4.3).
</p>
<p>Substituting X2i = X̂2i + ν̂2i, in (A.3) one gets
</p>
<p>&sum;n
i=1 X̂2iYi +
</p>
<p>&sum;n
i=1 ν̂2iYi = α̂
</p>
<p>&sum;n
i=1 X̂2i + β̂2
</p>
<p>&sum;n
i=1 X̂
</p>
<p>2
2i + ..
</p>
<p>+ β̂K
&sum;n
</p>
<p>i=1 X̂2iXKi + β̂2
&sum;n
</p>
<p>i=1 ν̂
2
2i
</p>
<p>(A.4)
</p>
<p>using (A.1) and the fact that Σni=1X̂2iν̂2i = 0. Multiply (A.2) by X̂2i and sum, we get
</p>
<p>&sum;n
i=1 X̂2iYi = α̂
</p>
<p>&sum;n
i=1 X̂2i + β̂2
</p>
<p>&sum;n
i=1 X̂2iX2i + ..+ β̂K
</p>
<p>&sum;n
i=1 X̂2iXKi +
</p>
<p>&sum;n
i=1 X̂2iei (A.5)
</p>
<p>But
&sum;n
</p>
<p>i=1 X̂2ei = 0 since X̂2 is a linear combination of all the other X&rsquo;s, all of which satisfy (4.2). Also,&sum;n
i=1 X̂2iX2i =
</p>
<p>&sum;n
i=1 X̂
</p>
<p>2
2i since
</p>
<p>&sum;n
i=1 X̂2iν̂2i = 0. Hence (A.5) reduces to
</p>
<p>&sum;n
i=1 X̂2iYi = α̂
</p>
<p>&sum;n
i=1 X̂2i + β̂2
</p>
<p>&sum;n
i=1 X̂
</p>
<p>2
2i + ..+ β̂K
</p>
<p>&sum;n
i=1 X̂2iXKi (A.6)
</p>
<p>Subtracting (A.6) from (A.4), we get
</p>
<p>&sum;n
i=1 ν̂2iYi = β̂2
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
2i (A.7)
</p>
<p>and β̂2 is the slope estimate of the simple regression of Y on ν̂2 as given in (4.5).
By substituting for Yi its expression from equation (4.1) in (4.5) we get
</p>
<p>β̂2 = β2
&sum;n
</p>
<p>i=1 X2iν̂2i/
&sum;n
</p>
<p>i=1 ν̂
2
2i +
</p>
<p>&sum;n
i=1 ν̂2iui/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
2i (A.8)
</p>
<p>where
&sum;n
</p>
<p>i=1 X1iν̂2i = 0 and
&sum;n
</p>
<p>i=1 ν̂2i = 0. But, X2i = X̂2i + ν̂2i and
&sum;n
</p>
<p>i=1 X̂2iν̂2i = 0, which implies
</p>
<p>that
&sum;n
</p>
<p>i=1 X2iν̂2i =
&sum;n
</p>
<p>i=1 ν̂
2
2i and β̂2 = β2 +
</p>
<p>&sum;n
i=1 ν̂2iui/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
2i. This means that β̂2 is unbiased with</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 93
</p>
<p>E(β̂2) = β2 since ν̂2 is a linear combination of the X&rsquo;s and these in turn are not correlated with the u&rsquo;s.
Also,
</p>
<p>var(β̂2) = E(β̂2 &minus; β2)2 = E(
&sum;n
</p>
<p>i=1 ν̂2iui/
&sum;n
</p>
<p>i=1 ν̂
2
2i)
</p>
<p>2 = σ2/
&sum;n
</p>
<p>i=1 ν̂
2
2i
</p>
<p>The same results apply for any β̂k for k = 2, . . . ,K, i.e.,
</p>
<p>β̂k =
&sum;n
</p>
<p>i=1 ν̂kiYi/
&sum;n
</p>
<p>i=1 ν̂
2
ki (A.9)
</p>
<p>where ν̂k is the OLS residual of Xk on all the other X&rsquo;s in the regression. Similarly,
</p>
<p>β̂k = βk +
&sum;n
</p>
<p>i=1 ν̂kiui/
&sum;n
</p>
<p>i=1 ν̂
2
ki (A.10)
</p>
<p>and E(β̂k) = βk with var(β̂k) = σ
2/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
ki for k = 2, . . . ,K. Note also that
</p>
<p>cov(β̂2, β̂k) = E(β̂2 &minus; β2)(β̂k &minus; βk) = E(
&sum;n
</p>
<p>i=1 ν̂2iui/
&sum;n
</p>
<p>i=1 ν̂
2
2i)(
</p>
<p>&sum;n
i=1 ν̂kiui/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
ki)
</p>
<p>= σ2
&sum;n
</p>
<p>i=1 ν̂2iν̂ki/
&sum;n
</p>
<p>i=1 ν̂
2
2i
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
ki
</p>
<p>Proof of Claim 2: Regressing Y on all the other X&rsquo;s yields, Yi = Ỹi + ω̃i. Substituting this expression
for Yi in (4.5) one gets
</p>
<p>β̂2 = (
&sum;n
</p>
<p>i=1 ν̂2iỸi +
&sum;n
</p>
<p>i=1 ν̂2iω̃i)/
&sum;n
</p>
<p>i=1 ν̂
2
2i =
</p>
<p>&sum;n
i=1 ν̂2iω̃i/
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
2i (A.11)
</p>
<p>where the last equality follows from the fact that Ỹ is a linear combination of all X&rsquo;s excluding X2, all
of which satisfy (A.1). Hence β̂2 is the estimate of the slope coefficient in the linear regression of ω̃ on
ν̂2.
</p>
<p>Simple, Partial and Multiple Correlation Coefficients
</p>
<p>In Chapter 3, we interpreted the square of the simple correlation coefficient, r2Y,X2 , as the proportion of
</p>
<p>the variation in Y that is explained by X2. Similarly, r
2
Y,Xk
</p>
<p>is the R-squared of the simple regression
of Y on Xk for k = 2, . . . ,K. In fact, one can compute these simple correlation coefficients and find
out which Xk is most correlated with Y , say it is X2. If one is selecting regressors to include in the
regression equation, X2 would be the best one variable candidate. In order to determine what variable
to include next, we look at partial correlation coefficients of the form rY,Xk.X2 for k 	= 2. The square of
this first-order partial gives the proportion of the residual variation in Y , not explained by X2, that is
explained by the addition of Xk. The maximum first-order partial (&lsquo;first&rsquo; because it has only one variable
after the dot) determines the best candidate to follow X2. Let us assume it is X3. The first-order partial
correlation coefficients can be computed from simple correlation coefficients as follows:
</p>
<p>rY,X3.X2 =
rY,X3 &minus; rY,X2rX2,X3&radic;
1&minus; r2Y,X2
</p>
<p>&radic;
1&minus; r2X2,X3
</p>
<p>see Johnston (1984). Next we look at second-order partials of the form rY,Xk.X2,X3 for k 	= 2, 3, and so
on. This method of selecting regressors is called forward selection. Suppose there is only X2, X3 and X4
in the regression equation. In this case (1&minus; r2Y,X2) is the proportion of the variation in Y , i.e.,
</p>
<p>&sum;n
i=1 y
</p>
<p>2
i ,
</p>
<p>that is not explained by X2. Also (1&minus; r2Y,X3.X2)(1&minus; r2Y,X2) denotes the proportion of the variation in Y
not explained after the inclusion of both X2 and X3. Similarly (1&minus; r2Y,X4.X2,X3)(1&minus; r2Y,X3.X2)(1&minus; r2Y,X2)
is the proportion of the variation in Y unexplained after the inclusion of X2, X3 and X4. But this is
exactly (1&minus;R2), where R2 denotes the R-squared of the multiple regression of Y on a constant, X2, X3
and X4. This R
</p>
<p>2 is called the multiple correlation coefficient, and is also written as R2Y.X2,X3,X4 . Hence
</p>
<p>(1&minus;R2Y.X2,X3,X4) = (1&minus; r
2
Y,X2)(1&minus; r
</p>
<p>2
Y,X3.X2)(1&minus; r
</p>
<p>2
Y,X4.X2,X3)
</p>
<p>and similar expressions relating the multiple correlation coefficient to simple and partial correlation
</p>
<p>coefficients can be written by including say X3 first then X4 and X2 in that order.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 5
</p>
<p>Violations of the Classical Assumptions
</p>
<p>5.1 Introduction
</p>
<p>In this chapter, we relax the assumptions made in Chapter 3 one by one and study the effect
of that on the OLS estimator. In case the OLS estimator is no longer a viable estimator, we
derive an alternative estimator and propose some tests that will allow us to check whether this
assumption is violated.
</p>
<p>5.2 The Zero Mean Assumption
</p>
<p>Violation of assumption 1 implies that the mean of the disturbances is no longer zero. Two
cases are considered:
</p>
<p>Case 1: E(ui) = μ 	= 0
</p>
<p>The disturbances have a common mean which is not zero. In this case, one can subtract μ from
the ui&rsquo;s and get new disturbances u
</p>
<p>&lowast;
i = ui &minus; μ which have zero mean and satisfy all the other
</p>
<p>assumptions imposed on the ui&rsquo;s. Having subtracted μ from ui we add it to the constant α
leaving the regression equation intact:
</p>
<p>Yi = α
&lowast; + βXi + u
</p>
<p>&lowast;
i i = 1, 2, . . . , n (5.1)
</p>
<p>where α&lowast; = α + μ. It is clear that only α&lowast; and β can be estimated, and not α nor μ. In other
words, one cannot retrieve α and μ from an estimate of α&lowast; without additional assumptions or
further information, see problem 10. With this reparameterization, equation (5.1) satisfies the
four classical assumptions, and therefore OLS gives the BLUE estimators of α&lowast; and β. Hence,
a constant non-zero mean for the disturbances affects only the intercept estimate but not the
slope. Fortunately, in most economic applications, it is the slope coefficients that are of interest
and not the intercept.
</p>
<p>Case 2: E(ui) = μi
</p>
<p>The disturbances have a mean which varies with every observation. In this case, one can trans-
form the regression equation as in (5.1) by adding and subtracting μi. The problem, however,
is that α&lowast;i = α+μi now varies with each observation, and hence we have more parameters than
observations. In fact, there are n intercepts and one slope to be estimated with n observations.
Unless we have repeated observations like in panel data, see Chapter 12 or we have some prior
information on these α&lowast;i , we cannot estimate this model.
</p>
<p>95
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_5, </p>
<p/>
</div>
<div class="page"><p/>
<p>96 Chapter 5: Violations of the Classical Assumptions
</p>
<p>5.3 Stochastic Explanatory Variables
</p>
<p>Sections 5.5 and 5.6 will study violations of assumptions 2 and 3 in detail. This section deals with
violations of assumption 4 and its effect on the properties of the OLS estimators. In this case,
X is a random variable which may be (i) independent; (ii) contemporaneously uncorrelated; or
(iii) simply correlated with the disturbances.
</p>
<p>Case 1: If X is independent of u, then all the results of Chapter 3 still hold, but now they are
conditional on the particular set of X&rsquo;s drawn in the sample. To illustrate this result, recall
that for the simple linear regression:
</p>
<p>β̂OLS = β +
&sum;n
</p>
<p>i=1wiui where wi = xi/
&sum;n
</p>
<p>i=1 x
2
i (5.2)
</p>
<p>Hence, when we take expectations E(
&sum;n
</p>
<p>i=1wiui) =
&sum;n
</p>
<p>i=1E(wi)E(ui) = 0. The first equality
holds because X and u are independent and the second equality holds because the u&rsquo;s have zero
mean. In other words the unbiasedness property of the OLS estimator still holds. However, the
</p>
<p>var(β̂OLS) = E(
&sum;n
</p>
<p>i=1wiui)
2 =
</p>
<p>&sum;n
i=1
</p>
<p>&sum;n
j=1E(wiwj)E(uiuj) = σ
</p>
<p>2&sum;n
i=1E(w
</p>
<p>2
i )
</p>
<p>where the last equality follows from assumptions 2 and 3, homoskedasticity and no serial correla-
tion. The only difference between this result and that of Chapter 3 is that we have expectations
on the X&rsquo;s rather than the X&rsquo;s themselves. Hence, by conditioning on the particular set of X&rsquo;s
that are observed, we can use all the results of Chapter 3. Also, maximizing the likelihood in-
volves both the X&rsquo;s and the u&rsquo;s. But, as long as the distribution of the X&rsquo;s does not involve the
parameters we are estimating, i.e., α, β and σ2, the same maximum likelihood estimators are
obtained. Why? Because f(x1, x2, . . . , xn, u1, u2, . . . , un) = f1(x1, x2, . . . , xn)f2(u1, u2, . . . , un)
since the X&rsquo;s and the u&rsquo;s are independent. Maximizing f with respect to (α, β, σ2) is the same
as maximizing f2 with respect to (α, β, σ
</p>
<p>2) as long as f1 is not a function of these parameters.
</p>
<p>Case 2: Consider a simple model of consumption, where Yt, current consumption, is a function
of Yt&minus;1, consumption in the previous period. This is the case for a habit forming consumption
good like cigarette smoking. In this case our regression equation becomes
</p>
<p>Yt = α+ βYt&minus;1 + ut t = 2, . . . , T (5.3)
</p>
<p>where we lost one observation due to lagging. It is obvious that Yt is correlated to ut, but the
question here is whether Yt&minus;1 is correlated to ut. After all, Yt&minus;1 is our explanatory variable
Xt. As long as assumption 3 is not violated, i.e., the u&rsquo;s are not correlated across periods, ut
represents a freshly drawn disturbance independent of previous disturbances and hence is not
correlated with the already predetermined Yt&minus;1. This is what we mean by contemporaneously
uncorrelated, i.e., ut is correlated with Yt, but it is not correlated with Yt&minus;1. The OLS estimator
of β is
</p>
<p>β̂OLS =
&sum;T
</p>
<p>t=2 ytyt&minus;1/
&sum;T
</p>
<p>t=2 y
2
t&minus;1 = β +
</p>
<p>&sum;T
t=2 yt&minus;1ut/
</p>
<p>&sum;T
t=2 y
</p>
<p>2
t&minus;1 (5.4)
</p>
<p>and the expected value of (5.4) is not β because in general,
</p>
<p>E(
&sum;T
</p>
<p>t=2 yt&minus;1ut/
&sum;T
</p>
<p>t=2 y
2
t&minus;1) 	= E(
</p>
<p>&sum;T
t=2 yt&minus;1ut)/E(
</p>
<p>&sum;T
t=2 y
</p>
<p>2
t&minus;1).
</p>
<p>The expected value of a ratio is not the ratio of expected values. Also, even if E(Yt&minus;1ut) = 0,
one can easily show that E(yt&minus;1ut) 	= 0. In fact, yt&minus;1 = Yt&minus;1 &minus; Ȳ , and Ȳ contains Yt in it, and</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Stochastic Explanatory Variables 97
</p>
<p>we know that E(Ytut) 	= 0. Hence, we lost the unbiasedness property of OLS. However, all the
asymptotic properties still hold. In fact, β̂OLS is consistent because
</p>
<p>plim β̂OLS = β + cov(Yt&minus;1, ut)/var(Yt&minus;1) = β (5.5)
</p>
<p>where the second equality follows from (5.4) and the fact that plim(
&sum;T
</p>
<p>t=2 yt&minus;1ut/T ) is
cov(Yt&minus;1, ut) which is zero, and plim(
</p>
<p>&sum;T
t=2 y
</p>
<p>2
t&minus;1/T ) = var(Yt&minus;1) which is positive and finite.
</p>
<p>Case 3: X and u are correlated, in this case OLS is biased and inconsistent. This can be
easily deduced from (5.2) since plim(
</p>
<p>&sum;n
i=1 xiui/n) is the cov(X,u) 	= 0, and plim(
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i /n)
</p>
<p>is positive and finite. This means that OLS is no longer a viable estimator, and an alternative
estimator that corrects for this bias has to be derived. In fact we will study three specific cases
where this assumption is violated. These are: (i) the errors in measurement case; (ii) the case
of a lagged dependent variable with correlated errors; and (iii) simultaneous equations.
Briefly, the errors in measurement case involves a situation where the true regression model
</p>
<p>is in terms of X&lowast;, but X&lowast; is measured with error, i.e., Xi = X&lowast;i + νi, so we observe Xi but not
X&lowast;i . Hence, when we substitute this Xi for X
</p>
<p>&lowast;
i in the regression equation, we get
</p>
<p>Yi = α+ βX
&lowast;
i + ui = α+ βXi + (ui &minus; βνi) (5.6)
</p>
<p>where the composite error term is now correlated with Xi because Xi is correlated with νi.
After all, Xi = X
</p>
<p>&lowast;
i + νi and E(Xiνi) = E(ν
</p>
<p>2
i ) if X
</p>
<p>&lowast;
i and νi are uncorrelated.
</p>
<p>Similarly, in case (ii) above, if the u&rsquo;s were correlated across time, i.e., ut&minus;1 is correlated with
ut, then Yt&minus;1, which is a function of ut&minus;1, will also be correlated with ut, and E(Yt&minus;1ut) 	= 0.
More on this and how to test for serial correlation in the presence of a lagged dependent variable
in Chapter 6.
Finally, if one considers a demand and supply equations where quantity Qt is a function of
</p>
<p>price Pt in both equations
</p>
<p>Qt = α+ βPt + ut (demand) (5.7)
</p>
<p>Qt = δ + γPt + νt (supply) (5.8)
</p>
<p>The question here is whether Pt is correlated with the disturbances ut and νt in both equations.
The answer is yes, because (5.7) and (5.8) are two equations in two unknowns Pt and Qt. Solving
for these variables, one gets Pt as well as Qt as a function of a constant and both ut and νt. This
means that E(Ptut) 	= 0 and E(Ptνt) 	= 0 and OLS performed on either (5.7) or (5.8) is biased
and inconsistent. We will study this simultaneous bias problem more rigorously in Chapter 11.
</p>
<p>For all situations where X and u are correlated, it would be illuminating to show graphically
why OLS is no longer a consistent estimator. Let us consider the case where the disturbances
are, say, positively correlated with the explanatory variable. Figure 3.3 of Chapter 3 shows the
true regression line α+βXi. It also shows that when Xi and ui are positively correlated then an
Xi higher than its mean will be associated with a disturbance ui above its mean, i.e., a positive
disturbance. Hence, Yi = α + βXi + ui will always be above the true regression line whenever
Xi is above its mean. Similarly Yi would be below the true regression line for every Xi below
its mean. This means that not knowing the true regression line, a researcher fitting OLS on
this data will have a biased intercept and slope. In fact, the intercept will be understated and
the slope will be overstated. Furthermore, this bias does not disappear with more data, since</p>
<p/>
</div>
<div class="page"><p/>
<p>98 Chapter 5: Violations of the Classical Assumptions
</p>
<p>this new data will be generated by the same mechanism described above. Hence these OLS
estimates are inconsistent.
Similarly, if Xi and ui are negatively correlated, the intercept will be overstated and the
</p>
<p>slope will be understated. This story applies to any equation with at least one of its right hand
side variables correlated with the disturbance term. Correlation due to the lagged dependent
variable with autocorrelated errors, is studied in Chapter 6, whereas the correlation due to the
simultaneous equations problem is studied in Chapter 11.
</p>
<p>5.4 Normality of the Disturbances
</p>
<p>If the disturbance are not normal, OLS is still BLUE provided assumptions 1&ndash;4 still hold. Nor-
mality made the OLS estimators minimum variance unbiased MVU and these OLS estimators
turn out to be identical to the MLE. Normality allowed the derivation of the distribution of
these estimators and this in turn allowed testing of hypotheses using the t and F -tests consid-
ered in the previous chapter. If the disturbances are not normal, yet the sample size is large,
one can still use the normal distribution for the OLS estimates asymptotically by relying on the
Central Limit Theorem, see Theil (1978). Theil&rsquo;s proof is for the case of fixed X&rsquo;s in repeated
samples, zero mean and constant variance on the disturbances. A simple asymptotic test for
the normality assumption is given by Jarque and Bera (1987). This is based on the fact that
the normal distribution has a skewness measure of zero and a kurtosis of 3. Skewness (or lack
of symmetry) is measured by
</p>
<p>S =
[E(X &minus; μ)3]2
[E(X &minus; μ)2]3 =
</p>
<p>Square of the 3rd moment about the mean
</p>
<p>Cube of the variance
</p>
<p>Kurtosis (a measure of flatness) is measured by
</p>
<p>κ =
E(X &minus; μ)4
[E(X &minus; μ)2]2 =
</p>
<p>4th moment about the mean
</p>
<p>Square of the variance
</p>
<p>For the normal distribution S = 0 and κ = 3. Hence, the Jarque-Bera (JB) statistic is given by
</p>
<p>JB = n
</p>
<p>[
S2
</p>
<p>6
+
</p>
<p>(κ&minus; 3)2
24
</p>
<p>]
</p>
<p>where S represents skewness and κ represents kurtosis of the OLS residuals. This statistic is
asymptotically distributed as χ2 with two degrees of freedom under H0. Rejecting H0, rejects
normality of the disturbances but does not offer an alternative distribution. In this sense,
the test is non-constructive. In addition, not rejecting H0 does not necessarily mean that the
distribution of the disturbances is normal, it only means we do not reject that the distribution
of the disturbances is symmetric and has a kurtosis of 3. See the empirical example in section
5.5 for an illustration. The Jarque-Bera test is part of the standard output using EViews.
</p>
<p>5.5 Heteroskedasticity
</p>
<p>Violation of assumption 2, means that the disturbances have a varying variance, i.e., E(u2i ) = σ
2
i ,
</p>
<p>i = 1, 2, . . . , n. First, we study the effect of this violation on the OLS estimators. For the simple</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Heteroskedasticity 99
</p>
<p>linear regression it is obvious that β̂OLS given in equation (5.2) is still unbiased and consistent
because these properties depend upon assumptions 1 and 4, and not assumption 2. However,
the variance of β̂OLS is now different
</p>
<p>var(β̂OLS) = var(
&sum;n
</p>
<p>i=1wiui) =
&sum;n
</p>
<p>i=1w
2
i σ
</p>
<p>2
i =
</p>
<p>&sum;n
i=1 x
</p>
<p>2
iσ
</p>
<p>2
i /(
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )
</p>
<p>2 (5.9)
</p>
<p>where the second equality follows from assumption 3 and the fact that var(ui) is now σ
2
i .
</p>
<p>Note that if σ2i = σ
2, this reverts back to σ2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i , the usual formula for var(β̂OLS) under
</p>
<p>homoskedasticity. Furthermore, one can show that E(s2) will involve all of the σ2i &rsquo;s and not one
common σ2, see problem 1. This means that the regression package reporting s2/
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i as the
</p>
<p>estimate of the variance of β̂OLS is committing two errors. One, it is not using the right formula
for the variance, i.e., equation (5.9). Second, it is using s2 to estimate a common σ2 when in
fact the σ2i &rsquo;s are different. The bias from using s
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i as an estimate of var(β̂OLS) will
</p>
<p>depend upon the nature of the heteroskedasticity and the regressor. In fact, if σ2i is positively
related to x2i , one can show that s
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i understates the true variance and hence the t-
</p>
<p>statistic reported for β = 0 is overblown, and the confidence interval for β is tighter than it is
supposed to be, see problem 2. This means that the t-statistic in this case is biased towards
rejecting H0;β = 0, i.e., showing significance of the regression slope coefficient, when it may
not be significant.
</p>
<p>The OLS estimator of β is linear unbiased and consistent, but is it still BLUE? In order to
answer this question, we note that the only violation we have is that the var(ui) = σ
</p>
<p>2
i . Hence, if
</p>
<p>we divided ui by σi/σ, the resulting u
&lowast;
i = σui/σi will have a constant variance σ
</p>
<p>2. It is easy to
show that u&lowast; satisfies all the classical assumptions including homoskedasticity. The regression
model becomes
</p>
<p>σYi/σi = ασ/σi + βσXi/σi + u
&lowast;
i (5.10)
</p>
<p>and OLS on this model (5.10) is BLUE. The OLS normal equations on (5.10) are
&sum;n
</p>
<p>i=1(Yi/σ
2
i ) = α
</p>
<p>&sum;n
i=1(1/σ
</p>
<p>2
i ) + β
</p>
<p>&sum;n
i=1(Xi/σ
</p>
<p>2
i )
</p>
<p>&sum;n
i=1(YiXi/σ
</p>
<p>2
i ) = α
</p>
<p>&sum;n
i=1(Xi/σ
</p>
<p>2
i ) + β
</p>
<p>&sum;n
i=1(X
</p>
<p>2
i /σ
</p>
<p>2
i )
</p>
<p>(5.11)
</p>
<p>Note that σ2 drops out of these equations. Solving (5.11), see problem 3, one gets
</p>
<p>α̃ = [
&sum;n
</p>
<p>i=1(Y1i/σ
2
i )/
</p>
<p>&sum;n
i=1(1/σ
</p>
<p>2
i )]&minus; β̃[
</p>
<p>&sum;n
i=1(Xi/σ
</p>
<p>2
i )/
</p>
<p>&sum;n
i=1(1/σ
</p>
<p>2
i )] = Ȳ
</p>
<p>&lowast; &minus; β̃X̄&lowast; (5.12a)
</p>
<p>with Ȳ &lowast; = [
&sum;n
</p>
<p>i=1(Yi/σ
2
i )/
</p>
<p>&sum;n
i=1(1/σ
</p>
<p>2
i )] =
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
i Yi/
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
i and
</p>
<p>X̄&lowast; = [
&sum;n
</p>
<p>i=1(Xi/σ
2
i )/
</p>
<p>&sum;n
i=1(1/σ
</p>
<p>2
i )] =
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
iXi/
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
i
</p>
<p>where w&lowast;i = (1/σ
2
i ). Similarly,
</p>
<p>β̃ =
[
&sum;n
</p>
<p>i=1(1/σ
2
i )][
</p>
<p>&sum;n
i=1(YiXi/σ
</p>
<p>2
i )]&minus; [
</p>
<p>&sum;n
i=1(Xi/σ
</p>
<p>2
i )][
</p>
<p>&sum;n
i=1(Yi/σ
</p>
<p>2
i )]
</p>
<p>[
&sum;n
</p>
<p>i=1X
2
i /σ
</p>
<p>2
i ][
&sum;n
</p>
<p>i=1(1/σ
2
i )]&minus; [
</p>
<p>&sum;n
i=1(Xi/σ
</p>
<p>2
i )]
</p>
<p>2
</p>
<p>=
(
&sum;n
</p>
<p>i=1w
&lowast;
i )(
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
iXiYi)&minus; (
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
iXi)(
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
i Yi)
</p>
<p>(
&sum;n
</p>
<p>i=1w
&lowast;
i )(
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
iX
</p>
<p>2
i )&minus; (
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
iXi)
</p>
<p>2
(5.12b)
</p>
<p>=
</p>
<p>&sum;n
i=1w
</p>
<p>&lowast;
i (Xi &minus; X̄&lowast;)(Yi &minus; Ȳ &lowast;)&sum;n
i=1w
</p>
<p>&lowast;
i (Xi &minus; X̄&lowast;)2</p>
<p/>
</div>
<div class="page"><p/>
<p>100 Chapter 5: Violations of the Classical Assumptions
</p>
<p>It is clear that the BLU estimators α̃ and β̃, obtained from the regression in (5.10), are different
from the usual OLS estimators α̂OLS and β̂OLS since they depend upon the σ
</p>
<p>2
i &rsquo;s. It is also true
</p>
<p>that when σ2i = σ
2 for all i = 1, 2, . . . , n, i.e., under homoskedasticity, (5.12) reduces to the
</p>
<p>usual OLS estimators given by equation (3.4) of Chapter 3. The BLU estimators weight the
i-th observation by (1/σi) which is a measure of precision of that observation. The more precise
the observation, i.e., the smaller σi, the larger is the weight attached to that observation. α̃
and β̃ are also known as Weighted Least Squares (WLS) estimators which are a specific form
of Generalized Least Squares (GLS). We will study GLS in details in Chapter 9, using matrix
notation.
Under heteroskedasticity, OLS looses efficiency in that it is no longer BLUE. However, be-
</p>
<p>cause it is still unbiased and consistent and because the true σ2i &rsquo;s are never known some re-
searchers compute OLS as an initial consistent estimator of the regression coefficients. It is
important to emphasize however, that the standard errors of these estimates as reported by the
regression package are biased and any inference based on these estimated variances including
the reported t-statistics are misleading. White (1980) proposed a simple procedure that would
yield heteroskedasticity consistent standard errors of the OLS estimators. In equation (5.9), this
amounts to replacing σ2i by e
</p>
<p>2
i , the square of the i-th OLS residual, i.e.,
</p>
<p>White&rsquo;s var(β̂OLS) =
&sum;n
</p>
<p>i=1 x
2
i e
</p>
<p>2
i /(
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )
</p>
<p>2 (5.13)
</p>
<p>Note that we can not consistently estimate σ2i by e
2
i , since there is one observation per parameter
</p>
<p>estimated. As the sample size increases, so does the number of unknown σ2i &rsquo;s. What White (1980)
</p>
<p>consistently estimates is the var(β̂OLS) which is a weighted average of the e
2
i . The same analysis
</p>
<p>applies to the multiple regression OLS estimates. In this case, White&rsquo;s (1980) heteroskedasticity
consistent estimate of the variance of the k-th OLS regression coefficient βk, is given by
</p>
<p>White&rsquo;s var(β̂k) =
&sum;n
</p>
<p>i=1 ν̂
2
kie
</p>
<p>2
i /(
</p>
<p>&sum;n
i=1 ν̂
</p>
<p>2
ki)
</p>
<p>2
</p>
<p>where ν̂2k is the squared OLS residual obtained from regressing Xk on the remaining regres-
sors in the equation being estimated. ei is the i-th OLS residual from this multiple regression
equation. Many regression packages provide White&rsquo;s heteroskedasticity-consistent estimates of
the variances and their corresponding robust t-statistics. For example, using EViews, one clicks
on Quick, choose Estimate Equation. Now click on Options, a menu appears where one selects
White to obtain the heteroskedasticity-consistent estimates of the variances.
While the regression packages correct for heteroskedasticity in the t-statistics they do not
</p>
<p>usually do that for the F -statistics studied, say in Example 2 in Chapter 4. Wooldridge (1991)
suggests a simple way of obtaining a robust LM statistic for H0; β2 = β3 = 0 in the multiple
regression (4.1). This involves the following steps:
</p>
<p>(1) Run OLS on the restricted model without X2 and X3 and obtain the restricted least
squares residuals ũ.
</p>
<p>(2) Regress each of the independent variables excluded under the null (i.e., X2 and X3) on all
of the other included independent variables (i.e., X4, X5, . . . , XK) including the constant.
Get the corresponding residuals v̂2 and v̂3, respectively.
</p>
<p>(3) Regress the dependent variable equal to 1 for all observations on v̂2ũ, v̂3ũ without a con-
stant and obtain the robust LM statistic equal to the n- the sum of squared residuals of</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Heteroskedasticity 101
</p>
<p>this regression. This is exactly nR2u of this last regression. Under H0 this LM statistic is
distributed as χ22.
</p>
<p>Since OLS is no longer BLUE, one should compute α̃ and β̃. The only problem is that the σi&rsquo;s
are rarely known. One example where the σi&rsquo;s are known up to a scalar constant is the following
simple example of aggregation.
</p>
<p>Example 5.1: Aggregation and Heteroskedasticity. Let Yij be the observation on the j-th firm
in the i-th industry, and consider the following regression:
</p>
<p>Yij = α+ βXij + uij j = 1, 2, . . . , ni; i = 1, 2, . . . ,m (5.14)
</p>
<p>If only aggregate observations on each industry are available, then (5.14) is summed over firms,
i.e.,
</p>
<p>Yi = αni + βXi + ui i = 1, 2, . . . ,m (5.15)
</p>
<p>where Yi =
&sum;ni
</p>
<p>j=1 Yij , Xi =
&sum;ni
</p>
<p>j=1Xij , ui =
&sum;ni
</p>
<p>j=1 uij for i = 1, 2, . . . ,m. Note that although the
</p>
<p>uij &rsquo;s are IID(0, σ
2), by aggregating, we get ui &sim; (0, niσ
</p>
<p>2). This means that the disturbances in
(5.15) are heteroskedastic. However, σ2i = niσ
</p>
<p>2 and is known up to a scalar constant. In fact,
σ/σi is 1/(ni)
</p>
<p>1/2. Therefore, premultiplying (5.15) by 1/(ni)
1/2 and performing OLS on the
</p>
<p>transformed equation results in BLU estimators of α and β. In other words, BLU estimation
reduces to performing OLS of Yi/(ni)
</p>
<p>1/2 on (ni)
1/2 and Xi/(ni)
</p>
<p>1/2, without an intercept.
There may be other special cases in practice where σi is known up to a scalar, but in general,
</p>
<p>σi is usually unknown and will have to be estimated. This is hopeless with only n observations,
since there are n σi&rsquo;s, so we either have to have repeated observations, or know more about the
σi&rsquo;s. Let us discuss these two cases.
</p>
<p>Case 1: Repeated Observations
</p>
<p>Suppose that ni households are selected randomly with income Xi for i = 1, 2, . . . ,m. For
each household j = 1, 2, . . . , ni, we observe its consumption expenditures on food, say Yij . The
regression equation is
</p>
<p>Yij = α+ βXi + uij i = 1, 2, . . . ,m ; j = 1, 2, . . . , ni (5.16)
</p>
<p>where m is the number of income groups selected. Note that Xi has only one subscript, whereas
Yij has two subscripts denoting the repeated observations on households with the same income
Xi. The uij &rsquo;s are independently distributed (0, σ
</p>
<p>2
i ) reflecting the heteroskedasticity in consump-
</p>
<p>tion expenditures among the different income groups. In this case, there are n =
&sum;m
</p>
<p>i=1 ni obser-
vations and m σ2i &rsquo;s to be estimated. This is feasible, and there are two methods for estimating
these σ2i &rsquo;s. The first is to compute
</p>
<p>ŝ2i =
&sum;ni
</p>
<p>j=1(Yij &minus; Ȳi)2/(ni &minus; 1)
</p>
<p>where Ȳi =
&sum;ni
</p>
<p>j=1 Yij/ni. The second is to compute s̃
2
i =
</p>
<p>&sum;ni
j=1 e
</p>
<p>2
ij/ni where eij is the OLS
</p>
<p>residual given by
</p>
<p>eij = Yij &minus; α̂OLS &minus; β̂OLSXi</p>
<p/>
</div>
<div class="page"><p/>
<p>102 Chapter 5: Violations of the Classical Assumptions
</p>
<p>Both estimators of σ2i are consistent. Substituting either s̃
2
i or ŝ
</p>
<p>2
i for σ
</p>
<p>2
i in (5.12) will result
</p>
<p>in feasible estimators of α and β. However, the resulting estimates are no longer BLUE. The
substitution of the consistent estimators of σ2i is justified on the basis that the resulting α
and β estimates will be asymptotically efficient, see Chapter 9. Of course, this step could have
been replaced by a regression of Yij/ŝi on (1/ŝi) and (Xi/ŝi) without a constant, or the similar
regression in terms of s̃i. For this latter estimate, s̃
</p>
<p>2
i , one can iterate, i.e., obtaining new residuals
</p>
<p>based on the new regression estimates and therefore new s̃2i . The process continues until the
estimates obtained from the r-th iteration do not differ from those of the (r + 1)th iteration
in absolute value by more than a small arbitrary positive number chosen as the convergence
criterion. Once the estimates converge, the final round estimators are the maximum likelihood
estimators, see Oberhofer and Kmenta (1974).
</p>
<p>Case 2: Assuming More Information on the Form of Heteroskedasticity
</p>
<p>If we do not have repeated observations, it is hopeless to try and estimate n variances and α
and β with only n observations. More structure on the form of heteroskedasticity is needed to
estimate this model, but not necessarily to test it. Heteroskedasticity is more likely to occur
with cross-section data where the observations may be on firms with different size. For example,
a regression relating profits to sales might have heteroskedasticity, because larger firms have
more resources to draw upon, can borrow more, invest more, and loose or gain more than
smaller firms. Therefore, we expect the form of heteroskedasticity to be related to the size of
the firm, which is reflected in this case by the regressor, sales, or some other variable that
measures size, like assets. Hence, for this regression we can write σ2i = σ
</p>
<p>2Z2i , where Zi denotes
the sales or assets of firm i. Once again the form of heteroskedasticity is known up to a scalar
constant and the BLU estimators of α and β can be obtained from (5.12), assuming Zi is known.
Alternatively, one can run the regression of Yi/Zi on 1/Zi and Xi/Zi without a constant to get
the same result. Special cases of Zi are Xi and E(Yi). (i) If Zi = Xi the regression becomes that
of Yi/Xi on 1/Xi and a constant. Note that the regression coefficient of 1/Xi is the estimate of
α, while the constant of the regression is now the estimate of β. But, is it possible to have ui
uncorrelated with Xi when we are assuming var(ui) related to Xi? The answer is yes, as long
as E(ui/Xi) = 0, i.e., the mean of ui is zero for every value of Xi, see Figure 3.4 of Chapter
3. This, in turn, implies that the overall mean of the ui&rsquo;s is zero, i.e., E(ui) = 0 and that
cov(Xi, ui) = 0. If the latter is not satisfied and say cov(Xi, ui) is positive, then large values
of Xi imply large values of ui. This would mean that for these values of Xi, we have a non-
zero mean for the corresponding ui&rsquo;s. This contradicts E(ui/Xi) = 0. Hence, if E(ui/Xi) = 0,
then cov(Xi, ui) = 0. (ii) If Zi = E(Yi) = α + βXi, then σ
</p>
<p>2
i is proportional to the population
</p>
<p>regression line, which is a linear function of α and β. Since the OLS estimates are consistent
one can estimate E(Yi) by Ŷi = α̂OLS + β̂OLSXi use Ẑi = Ŷi instead of E(Yi). In other words,
run the regression of Yi/Ŷi on 1/Ŷi and Xi/Ŷi without a constant. The resulting estimates are
asymptotically efficient, see Amemiya (1973).
One can generalize σ2i = σ
</p>
<p>2Z2i to σ
2
i = σ
</p>
<p>2Zδi where δ is an unknown parameter to be es-
timated. Hence rather than estimating n σ2i &rsquo;s one has to estimate only σ
</p>
<p>2 and δ. Assuming
normality one can set up the likelihood function and derive the first-order conditions by dif-
ferentiating that likelihood with respect to α, β, σ2 and δ. The resulting equations are highly
nonlinear. Alternatively, one can search over possible values for δ = 0, 0.1, 0.2, . . . , 4, and get the
</p>
<p>corresponding estimates of α, β, and σ2 from the regression of Yi/Z
δ/2
i on 1/Z
</p>
<p>δ/2
i and Xi/Z
</p>
<p>δ/2
i</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Heteroskedasticity 103
</p>
<p>without a constant. This is done for every δ and the value of the likelihood function is reported.
Using this search procedure one can get the maximum value of the likelihood and corresponding
to it the MLE of α, β, σ2 and δ. Note that as δ increases so does the degree of heteroskedasticity.
Problem 4 asks the reader to compute the relative efficiency of the OLS estimator with respect
to the BLU estimator for Zi = Xi for various values of δ. As expected the relative efficiency of
the OLS estimator declines as the degree of heteroskedasticity increases.
One can also generalize σ2i = σ
</p>
<p>2Zδi to include more Z variables. In fact, a general form of
this multiplicative heteroskedasticity is
</p>
<p>logσ2i = logσ
2 + δ1logZ1i + δ2logZ2i + . . .+ δrlogZri (5.17)
</p>
<p>with r &lt; n, otherwise one cannot estimate with n observations. Z1, Z2, . . . , Zr are known vari-
ables determining the heteroskedasticity. Note that if δ2 = δ3 = . . . = δr = 0, we revert
back to σ2i = σ
</p>
<p>2Zδi , where δ = δ1. For the estimation of this general multiplicative form of
heteroskedasticity, see Harvey (1976).
Another form for heteroskedasticity, is the additive form
</p>
<p>σ2i = a+ b1Z1i + b2Z2i + . . .+ brZri (5.18)
</p>
<p>where r &lt; n, see Goldfeld and Quandt (1972). Special cases of (5.18) include
</p>
<p>σ2i = a+ b1Xi + b2X
2
i (5.19)
</p>
<p>where if a and b1 are zero we have a simple form of multiplicative heteroskedasticity. In order
to estimate the regression model with additive heteroskedasticity of the type given in (5.19),
one can get the OLS residuals, the ei&rsquo;s, and run the following regression
</p>
<p>e2i = a+ b1Xi + b2X
2
i + vi (5.20)
</p>
<p>where vi = e
2
i &minus; σ2i . The vi&rsquo;s are heteroskedastic, and the OLS estimates of (5.20) yield the
</p>
<p>following estimates of σ2i
</p>
<p>σ̂2i = âOLS + b̂1,OLSXi + b̂2,OLSX
2
i (5.21)
</p>
<p>One can obtain a better estimate of the σ2i &rsquo;s by computing the following regression which
corrects for the heteroskedasticity in the vi&rsquo;s
</p>
<p>(e2i /σ̂i) = a(1/σ̂i) + b1(Xi/σ̂i) + b2(X
2
i /σ̂i) + wi (5.22)
</p>
<p>The new estimates of σ2i are
</p>
<p>σ̃2i = ã+ b̃1Xi + b̃2X
2
i (5.23)
</p>
<p>where ã, b̃1 and b̃2 are the OLS estimates from (5.22). Using the σ̃
2
i &rsquo;s one can run the regression of
</p>
<p>Yi/σ̃i on (1/σ̃i) and Xi/σ̃i without a constant to get asymptotically efficient estimates of α and
β. These have the same asymptotic properties as the MLE estimators derived in Rutemiller and
Bowers (1968), see Amemiya (1977) and Buse (1984). The problem with this iterative procedure
is that there is no guarantee that the σ̃2i &rsquo;s are positive, which means that the square root σ̃i
may not exist. This problem would not occur if σ2i = (a + b1Xi + b2X
</p>
<p>2
i )
</p>
<p>2 because in this case
one regresses |ei| on a constant, Xi and X2i and the predicted value from this regression would
be an estimate of σi. It would not matter if this predictor is negative, because we do not have
to take its square root and because its sign cancels in the OLS normal equations of the final
regression of Yi/σ̂i on (1/σ̂i) and (Xi/σ̂i) without a constant.</p>
<p/>
</div>
<div class="page"><p/>
<p>104 Chapter 5: Violations of the Classical Assumptions
</p>
<p>Testing for Homoskedasticity
</p>
<p>In the repeated observation&rsquo;s case, one can perform Bartlett&rsquo;s (1937) test. The null hypothesis
is H0;σ
</p>
<p>2
1 = σ
</p>
<p>2
2 = . . . = σ
</p>
<p>2
m. Under the null there is one variance σ
</p>
<p>2 which can be estimated
by the pooled variance s2 =
</p>
<p>&sum;m
i=1 νis̃
</p>
<p>2
i /ν where ν =
</p>
<p>&sum;m
i=1 νi, and νi = ni &minus; 1. Under the
</p>
<p>alternative hypothesis there are m different variances estimated by s̃2i for i = 1, 2, . . . ,m. The
Likelihood Ratio test, which computes the ratio of the likelihoods under the null and alternative
hypotheses, reduces to computing
</p>
<p>B = [νlogs2 &minus;
&sum;m
</p>
<p>i=1 νi logs
2
i ]/c (5.24)
</p>
<p>where c = 1 + [
&sum;m
</p>
<p>i=1(1/νi)&minus; 1/ν] /3(m&minus; 1). Under H0, B is distributed χ2m&minus;1. Hence, a large
p-value for the B-statistic given in (5.24) means that we do not reject homoskedasticity whereas,
a small p-value leads to rejection of H0 in favor of heteroskedasticity.
</p>
<p>In case of no repeated observations, several tests exist in the literature. Among these are the
following:
</p>
<p>(1) Glejser&rsquo;s (1969) Test: In this case one regresses |ei| on a constant and Zδi for δ =
1,&minus;1, 0.5 and &minus;0.5. If the coefficient of Zδi is significantly different from zero, this would lead
to a rejection of homoskedasticity. The power of this test depends upon the true form of het-
eroskedasticity. One important result however, is that this power is not seriously impaired if
the wrong value of δ is chosen, see Ali and Giaccotto (1984) who confirmed this result using
extensive Monte Carlo experiments.
</p>
<p>(2) The Goldfeld and Quandt (1965) Test: This is a simple and intuitive test. One orders
the observations according to Xi and omits c central observations. Next, two regressions are run
on the two separated sets of observations with (n &minus; c)/2 observations in each. The c omitted
observations separate the low value X&rsquo;s from the high value X&rsquo;s, and if heteroskedasticity
exists and is related to Xi, the estimates of σ
</p>
<p>2 reported from the two regressions should be
different. Hence, the test statistic is s22/s
</p>
<p>2
1 where s
</p>
<p>2
1 and s
</p>
<p>2
2 are the Mean Square Error of the
</p>
<p>two regressions, respectively. Their ratio would be the same as that of the two residual sums of
squares because the degrees of freedom of the two regressions are the same. This statistic is F -
distributed with ((n&minus;c)/2)&minus;K degrees of freedom in the numerator as well as the denominator.
The only remaining question for performing this test is the magnitude of c. Obviously, the larger
c is, the more central observations are being omitted and the more confident we feel that the
two samples are distant from each other. The loss of c observations should lead to loss of power.
However, separating the two samples should give us more confidence that the two variances
are in fact the same if we do not reject homoskedasticity. This trade off in power was studied
by Goldfeld and Quandt using Monte Carlo experiments. Their results recommend the use of
c = 8 for n = 30 and c = 16 for n = 60. This is a popular test, but assumes that we know
how to order the heteroskedasticity. In this case, using Xi. But what if there are more than one
regressor on the right hand side? In that case one can order the observations using Ŷi.
</p>
<p>(3) Spearman&rsquo;s Rank Correlation Test: This test ranks the Xi&rsquo;s and the absolute value of
the OLS residuals, the ei&rsquo;s. Then it computes the difference between these rankings, i.e., di =
rank(|ei|)&minus; rank(Xi). The Spearman-Correlation coefficient is r = 1 &minus;
</p>
<p>[
6
&sum;n
</p>
<p>i=1 d
2
i /(n
</p>
<p>3 &minus; n)
]
.
</p>
<p>Finally, test H0; the correlation coefficient between the rankings is zero, by computing t =</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Heteroskedasticity 105
</p>
<p>[r2(n&minus; 2)/(1&minus; r2)]1/2 which is t-distributed with (n&minus; 2) degrees of freedom. If this t-statistic
has a large p-value we do not reject homoskedasticity. Otherwise, we reject homoskedasticity in
favor of heteroskedasticity.
</p>
<p>(4) Harvey&rsquo;s (1976) Multiplicative Heteroskedasticity Test: If heteroskedasticity is re-
lated to Xi, it looks like the Goldfeld and Quandt test or the Spearman rank correlation test
would detect it, and the Glejser test would establish its form. In case the form of heteroskedas-
ticity is of the multiplicative type, Harvey (1976) suggests the following test which rewrites
(5.17) as
</p>
<p>loge2i = logσ
2 + δ1logZ1i + . . .+ δrlogZri + vi (5.25)
</p>
<p>where vi = log(e
2
i /σ
</p>
<p>2
i ). This disturbance term has an asymptotic distribution that is logχ
</p>
<p>2
1. This
</p>
<p>random variable has mean &minus;1.2704 and variance 4.9348. Therefore, Harvey suggests performing
the regression in (5.25) and testing H0; δ1 = δ2 = . . . = δr = 0 by computing the regression
sum of squares divided by 4.9348. This statistic is distributed asymptotically as χ2r . This is also
asymptotically equivalent to an F -test that tests for δ1 = δ2 = . . . = δr = 0 in the regression
given in (5.25). See the F -test described in example 6 of Chapter 4.
</p>
<p>(5) Breusch and Pagan (1979) Test: If one knows that σ2i = f(a+ b1Z1+ b2Z2+ ..+ brZr)
but does not know the form of this function f , Breusch and Pagan (1979) suggest the following
test for homoskedasticity, i.e., H0; b1 = b2 = . . . = br = 0. Compute σ̂
</p>
<p>2 =
&sum;n
</p>
<p>i=1 e
2
i /n, which
</p>
<p>would be the MLE estimator of σ2 under homoskedasticity. Run the regression of e2i /σ̂
2on the
</p>
<p>Z variables and a constant, and compute half the regression sum of squares. This statistic is
distributed as χ2r. This is a more general test than the ones discussed earlier in that f does not
have to be specified.
</p>
<p>(6) White&rsquo;s (1980) Test: Another general test for homoskedasticity where nothing is known
about the form of this heteroskedasticity is suggested by White (1980). This test is based on
the difference between the variance of the OLS estimates under homoskedasticity and that
under heteroskedasticity. For the case of a simple regression with a constant, White shows that
this test compares White&rsquo;s var(β̂OLS) given by (5.13) with the usual var(β̂OLS) = s
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i
</p>
<p>under homoskedasticity. This test reduces to running the regression of e2i on a constant, Xi
and X2i and computing nR
</p>
<p>2. This statistic is distributed as χ22 under the null hypothesis of
homoskedasticity. The degrees of freedom correspond to the number of regressors without the
constant. If this statistic is not significant, then e2i is not related to Xi and X
</p>
<p>2
i and we can not
</p>
<p>reject that the variance is constant. Note that if there is no constant in the regression, we run
e2i on a constant and X
</p>
<p>2
i only, i.e., Xi is no longer in this regression and the degree of freedom
</p>
<p>of the test is 1. In general, White&rsquo;s test is based on running e2i on the cross-product of all the
X&rsquo;s in the regression being estimated, computing nR2, and comparing it to the critical value
of χ2r where r is the number of regressors in this last regression excluding the constant. For the
case of two regressors, X2 and X3 and a constant, White&rsquo;s test is again based on nR
</p>
<p>2 for the
regression of e2i on a constant, X2, X3, X
</p>
<p>2
2 , X2X3, X
</p>
<p>2
3 . This statistic is distributed as χ
</p>
<p>2
5. White&rsquo;s
</p>
<p>test is standard using EViews. After running the regression, click on residuals tests then choose
White. This software gives the user a choice between including or excluding the cross-product
terms like X2X3 from the regression. This may be useful when there are many regressors.
</p>
<p>A modified Breusch-Pagan test was suggested by Koenker (1981) and Koenker and Bassett
(1982). This attempts to improve the power of the Breusch-Pagan test, and make it more robust</p>
<p/>
</div>
<div class="page"><p/>
<p>106 Chapter 5: Violations of the Classical Assumptions
</p>
<p>to the non-normality of the disturbances. This amounts to multiplying the Breusch-Pagan
statistic (half the regression sum of squares) by 2σ̂4, and dividing it by the second sample
moment of the squared residuals, i.e.,
</p>
<p>&sum;n
i=1(e
</p>
<p>2
i &minus; σ̂2)2/n, where σ̂2 =
</p>
<p>&sum;n
i=1 e
</p>
<p>2
i /n. Waldman
</p>
<p>(1983) showed that if the Zi&rsquo;s in the Breusch-Pagan test are in fact the Xi&rsquo;s and their cross-
products, as in White&rsquo;s test, then this modified Breusch-Pagan test is exactly the nR2 statistic
proposed by White.
White&rsquo;s (1980) test for heteroskedasticity without specifying its form lead to further work
</p>
<p>on estimators that are more efficient than OLS while recognizing that the efficiency of GLS
may not be achievable, see Cragg (1992). Adaptive estimators have been developed by Carroll
(1982) and Robinson (1987). These estimators assume no particular form of heteroskedasticity
but nevertheless have the same asymptotic distribution as GLS based on the true σ2i .
</p>
<p>Many Monte Carlo experiments were performed to study the performance of these and other
tests of homoskedasticity. One such extensive study is that of Ali and Giaccotto (1984). Six
types of heteroskedasticity specifications were considered;
</p>
<p>(i) σ2i = σ
2 (ii) σ2i = σ
</p>
<p>2|Xi| (iii) σ2i = σ2|E(Yi)|
(iv) σ2i = σ
</p>
<p>2X2i (v) σ
2
i = σ
</p>
<p>2[E(Yi)]
2 (vi) σ2i = σ
</p>
<p>2 for i &le; n/2
and σ2i = 2σ
</p>
<p>2 for i &gt; n/2
</p>
<p>Six data sets were considered, the first three were stationary and the last three were nonsta-
tionary (Stationary versus non-stationary regressors, are discussed in Chapter 14). Five models
were entertained, starting with a model with one regressor and no intercept and finishing with
a model with an intercept and 5 variables. Four types of distributions were imposed on the
disturbances. These were normal, t, Cauchy and log normal. The first three are symmetric, but
the last one is skewed. Three sample sizes were considered, n = 10, 25, 40. Various correlations
between the disturbances were also entertained. Among the tests considered were tests 1, 2, 5
and 6 discussed in this section. The results are too numerous to summarize, but some of the
major findings are the following: (1) The power of these tests increased with sample size and
trendy nature or the variability of the regressors. It also decreased with more regressors and
for deviations from the normal distribution. The results were mostly erratic when the errors
were autocorrelated. (2) There were ten distributionally robust tests using OLS residuals named
TROB which were variants of Glejser&rsquo;s, White&rsquo;s and Bickel&rsquo;s tests. The last one being a non-
parametric test not considered in this chapter. These tests were robust to both long-tailed and
skewed distributions. (3) None of these tests has any significant power to detect heteroskedas-
ticity which deviates substantially from the true underlying heteroskedasticity. For example,
none of these tests was powerful in detecting heteroskedasticity of the sixth kind, i.e., σ2i =
σ2 for i &le; n/2 and σ2i = 2σ2 for i &gt; n/2. In fact, the maximum power was 9%. (4) Ali and
Giaccotto (1984) recommend any of the TROB tests for practical use. They note that the sim-
ilarity among these tests is the use of squared residuals rather than the absolute value of the
residuals. In fact, they argue that tests of the same form that use absolute value rather than
squared residuals are likely to be non-robust and lack power.
</p>
<p>Empirical Example: For the Cigarette Consumption Data given in Table 3.2, the OLS regression
yields:
</p>
<p>logC = 4.30
</p>
<p>(0.91)
</p>
<p>&minus; 1.34
(0.32)
</p>
<p>logP + 0.17
</p>
<p>(0.20)
</p>
<p>logY R̄2 = 0.27</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Heteroskedasticity 107
</p>
<p>&ndash;0.4
</p>
<p>&ndash;0.2
</p>
<p>0.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>4.4 4.6 4.8 5.0 5.2 
</p>
<p>R
es
</p>
<p>id
u
</p>
<p>al
s
</p>
<p>LNY
</p>
<p>Figure 5.1 Plots of Residuals Versus Log Y
</p>
<p>Suspecting heteroskedasticity, we plotted the residuals from this regression versus logY in Figure
5.1. This figure shows the dispersion of the residuals to decrease with increasing logY . Next,
we performed several tests for heteroskedasticity studied in this chapter. The first is Glejser&rsquo;s
(1969) test. We ran the following regressions:
</p>
<p>|ei| = 1.16
(0.46)
</p>
<p>&minus; 0.22
(0.10)
</p>
<p>logY
</p>
<p>|ei| =&minus;0.95
(0.47)
</p>
<p>+ 5.13
</p>
<p>(2.23)
</p>
<p>(logY )&minus;1
</p>
<p>|ei| =&minus;2.00
(0.93)
</p>
<p>+ 4.65
</p>
<p>(2.04)
</p>
<p>(logY )&minus;0.5
</p>
<p>|ei| = 2.21
(0.93)
</p>
<p>&minus; 0.96
(0.42)
</p>
<p>(logY )0.5
</p>
<p>The t-statistics on the slope coefficient in these regressions are &minus;2.24, 2.30, 2.29 and &minus;2.26,
respectively. All are significant with p-values of 0.03, 0.026, 0.027 and 0.029, respectively, indi-
cating the rejection of homoskedasticity.
The second test is the Goldfeld and Quandt (1965) test. The observations are ordered accord-
</p>
<p>ing to logY and c = 12 central observations are omitted. Two regressions are run on the first and
last 17 observations. The first regression yields s21 = 0.04881 and the second regression yields
s22 = 0.01554. This is a test of equality of variances and it is based on the ratio of two χ
</p>
<p>2 ran-
dom variables with 17&minus; 3 = 14 degrees of freedom. In fact, s21/s22 = 0.04881/0.01554 = 3.141 &sim;
F14,14 under H0. This has a p-value of 0.02 and rejects H0 at the 5% level. The third test
is the Spearman rank correlation test. First one obtains the rank(logYi) and rank(|ei|) and
compute di = rank|ei|&minus; rank|logYi|. From these r = 1 &minus;
</p>
<p>[
6
&sum;46
</p>
<p>i=1 d
2
i /(n
</p>
<p>3 &minus; n)
]
= &minus;0.282 and</p>
<p/>
</div>
<div class="page"><p/>
<p>108 Chapter 5: Violations of the Classical Assumptions
</p>
<p>t = [r2(n&minus; 2)/(1&minus; r2)]1/2 = 1.948. This is distributed as a t with 44 degrees of freedom. This
t-statistic has a p-value of 0.058.
</p>
<p>The fourth test is Harvey&rsquo;s (1976) multiplicative heteroskedasticity test which is based upon
regressing log e2i on log(log Yi)
</p>
<p>log e2i = 24.85
</p>
<p>(17.25)
</p>
<p>&minus; 19.08
(11.03)
</p>
<p>log(log Y )
</p>
<p>Harvey&rsquo;s (1976) statistic divides the regression sum of squares which is 14.360 by 4.9348. This
yields 2.91 which is asymptotically distributed as χ21 under the null. This has a p-value of 0.088
and does not reject the null of homoskedasticity at the 5% significance level.
The fifth test is the Breusch and Pagan (1979) test which is based on the regression of e2i /σ̂
</p>
<p>2
</p>
<p>(where σ̂2 =
&sum;46
</p>
<p>i=1 e
2
i /46 = 0.024968) on log Yi. The test-statistic is half the regression sum of
</p>
<p>squares = (10.971 &divide; 2) = 5.485. This is distributed as χ21 under the null hypothesis. This has
a p-value of 0.019 and rejects the null of homoskedasticity. This can be generated with Stata
after running the OLS regression reported above, and then issuing the command estat hettest
lnrdi
</p>
<p>.estat hettest lnrdi
</p>
<p>Breusch-Pagan / Cook-Weisberg test for heteroskedasticity
</p>
<p>Ho: Constant variance
</p>
<p>Variables: lnrdi
</p>
<p>chi2(1) = 5.49
</p>
<p>Prob &gt; chi2 = 0.0192
</p>
<p>Finally, White&rsquo;s (1980) test for heteroskedasticity is performed which is based on the regression
of e2i on logP , logY , (logP )
</p>
<p>2, (logY )2, (logP )(logY ) and a constant. This is shown in Table
5.1 using EViews. The test-statistic is nR2 = (46)(0.3404) = 15.66 which is distributed as χ25.
This has a p-value of 0.008 and rejects the null of homoskedasticity. Except for Harvey&rsquo;s test,
all the tests performed indicate the presence of heteroskedasticity. This is true despite the fact
that the data are in logs, and both consumption and income are expressed in per capita terms.
White&rsquo;s heteroskedasticity-consistent estimates of the variances are as follows:
</p>
<p>logC = 4.30
</p>
<p>(1.10)
</p>
<p>&minus; 1.34
(0.34)
</p>
<p>logP + 0.17
</p>
<p>(0.24)
</p>
<p>logY
</p>
<p>These are given in Table 5.2 using EViews. Note that in this case all of the heteroskedasticity-
consistent standard errors are larger than those reported using a standard OLS package, but
this is not necessarily true for other data sets.
In section 5.4, we described the Jarque and Bera (1987) test for normality of the disturbances.
</p>
<p>For this cigarette consumption regression, Figure 5.2 gives the histogram of the residuals along
with descriptive statistics of these residuals including their mean, median, skewness and kurtosis.
</p>
<p>This is done using EViews. The measure of skewness S is estimated to be &minus;0.184 and the
measure of kurtosis κ is estimated to be 2.875 yielding a Jarque-Bera statistic of
</p>
<p>JB = 46
</p>
<p>[
(&minus;0.184)2
</p>
<p>6
+
</p>
<p>(2.875&minus; 3)2
24
</p>
<p>]
= 0.29.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Heteroskedasticity 109
</p>
<p>Table 5.1 White Heteroskedasticity Test
</p>
<p>F-statistic 4.127779 Probability 0.004073
Obs*R-squared 15.65644 Probability 0.007897
</p>
<p>Test Equation:
Dependent Variable: RESIDˆ2
Method: Least Squares
Sample: 1 46
Included observations: 46
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C 18.22199 5.374060 3.390730 0.0016
LNP 9.506059 3.302570 2.878382 0.0064
LNPˆ2 1.281141 0.656208 1.952340 0.0579
LNP*LNY &ndash;2.078635 0.727523 &ndash;2.857139 0.0068
LNY &ndash;7.893179 2.329386 &ndash;3.388523 0.0016
LNYˆ2 0.855726 0.253048 3.381670 0.0016
</p>
<p>R-squared 0.340357 Mean dependent var 0.024968
Adjusted R-squared 0.257902 S.D. dependent var 0.034567
S.E. of regression 0.029778 Akaike info criterion &ndash;4.068982
Sum squared resid 0.035469 Schwarz criterion &ndash;3.830464
Log likelihood 99.58660 F-statistic 4.127779
Durbin-Watson stat 1.853360 Prob (F-statistic) 0.004073
</p>
<p>Table 5.2 White Heteroskedasticity-Consistent Standard Errors
</p>
<p>Dependent Variable: LNC
Method: Least Squares
Sample: 1 46
Included observations: 46
</p>
<p>White Heteroskedasticity-Consistent Standard Errors &amp; Covariance
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C 4.299662 1.095226 3.925821 0.0003
LNP &ndash;1.338335 0.343368 &ndash;3.897671 0.0003
LNY 0.172386 0.236610 0.728565 0.4702
</p>
<p>R-squared 0.303714 Mean dependent var 4.847844
Adjusted R-squared 0.271328 S.D. dependent var 0.191458
S.E. of regression 0.163433 Akaike info criterion &ndash;0.721834
Sum squared resid 1.148545 Schwarz criterion &ndash;0.602575
Log likelihood 19.60218 F-statistic 9.378101
Durbin-Watson stat 2.315716 Prob (F-statistic) 0.000417</p>
<p/>
</div>
<div class="page"><p/>
<p>110 Chapter 5: Violations of the Classical Assumptions
</p>
<p>�����������
��������������������������������
</p>
<p>����������
����������
����������
����������
</p>
<p>����������
����������
����������
����������
</p>
<p>����������
����������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
���������
���������
</p>
<p>���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
���������
</p>
<p>��������
��������
��������
��������
��������
��������
</p>
<p>������������
������������
������������
������������
������������
������������
������������
������������
������������
</p>
<p>������������
������������
</p>
<p>������������
������������
������������
������������
������������
������������
������������
</p>
<p>�����������
�����������
�����������
�����������
</p>
<p>�����������
�����������
</p>
<p>Series: Residuals 
</p>
<p>Sample 1 46 
</p>
<p>Observations 46 
</p>
<p>Mean       &ndash;9.90E-16 
</p>
<p>Median   0.007568 
</p>
<p>Maximum  0.328677 
</p>
<p>Minimum  &ndash;0.418675 
</p>
<p>Std. Dev.   0.159760 
</p>
<p>Skewness   &ndash;0.183945 
</p>
<p>Kurtosis   2.875020 
</p>
<p>Jarque-Bera 0.289346 
</p>
<p>Probability 0.865305 
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>&ndash;0.4 &ndash;0.3 &ndash;0.2 &ndash;0.1 0.0 0.1 0.2 0.3 
</p>
<p>Figure 5.2 Normality Test (Jarque-Bera)
</p>
<p>This is distributed as χ22 under the null hypothesis of normality and has a p-value of 0.865. Hence
we do not reject that the distribution of the disturbances is symmetric and has a kurtosis of 3.
</p>
<p>5.6 Autocorrelation
</p>
<p>Violation of assumption 3 means that the disturbances are correlated, i.e., E(uiuj) = σij 	= 0, for
i 	= j, and i, j = 1, 2, . . . , n. Since ui has zero mean, E(uiuj) = cov(ui, uj) and this is denoted by
σij . This correlation is more likely to occur in time-series than in cross-section studies. Consider
estimating the consumption function of a random sample of households. An unexpected event,
like a visit of family members will increase the consumption of this household. However, this
positive disturbance need not be correlated to the disturbances affecting consumption of other
randomly drawn households. However, if we were estimating this consumption function using
aggregate time-series data for the U.S., then it is very likely that a recession year affecting
consumption negatively this year may have a carry over effect to the next few years. A shock
to the economy like an oil embargo in 1973 is likely to affect the economy for several years. A
labor strike this year may affect production for the next few years. Therefore, we will switch
the i and j subscripts to t and s denoting time-series observations t, s = 1, 2, . . . , T and the
sample size will be denoted by T rather than n. This covariance term is symmetric, so that
σ12 = E(u1u2) = E(u2u1) = σ21. Hence, only T (T &minus; 1)/2 distinct σts&rsquo;s have to be estimated.
For example, if T = 3, then σ12, σ13 and σ23 are the distinct covariance terms. However, it is
hopeless to estimate T (T &minus; 1)/2 covariances (σts) with only T observations. Therefore, more
structure on these σts&rsquo;s need to be imposed. A popular assumption is that the ut&rsquo;s follow a
first-order autoregressive process denoted by AR(1):
</p>
<p>ut = ρut&minus;1 + ǫt t = 1, 2, . . . , T (5.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Autocorrelation 111
</p>
<p>where ǫt is IID(0, σ
2
ǫ ). It is autoregressive because ut is related to its lagged value ut&minus;1. One
</p>
<p>can also write (5.26), for period t&minus; 1, as
</p>
<p>ut&minus;1 = ρut&minus;2 + ǫt&minus;1 (5.27)
</p>
<p>and substitute (5.27) in (5.26) to get
</p>
<p>ut = ρ
2ut&minus;2 + ρǫt&minus;1 + ǫt (5.28)
</p>
<p>Note that the power of ρ and the subscript of u or ǫ always sum to t. By continuous substitution
of this form, one ultimately gets
</p>
<p>ut = ρ
tu0 + ρ
</p>
<p>t&minus;1ǫ1 + ..+ ρǫt&minus;1 + ǫt (5.29)
</p>
<p>This means that ut is a function of current and past values of ǫt and u0 where u0 is the initial
value of ut. If u0 has zero mean, then ut has zero mean. This follows from (5.29) by taking
expectations. Also, from (5.26)
</p>
<p>var(ut) = ρ
2var(ut&minus;1) + var(ǫt) + 2ρcov(ut&minus;1, ǫt) (5.30)
</p>
<p>Using (5.29), ut&minus;1 is a function of ǫt&minus;1, past values of ǫt&minus;1 and u0. Since u0 is independent of the
ǫ&rsquo;s, and the ǫ&rsquo;s are themselves not serially correlated, then ut&minus;1 is independent of ǫt. This means
that cov(ut&minus;1, ǫt) = 0. Furthermore, for ut to be homoskedastic, var(ut) = var(ut&minus;1) = σ2u, and
(5.30) reduces to σ2u = ρ
</p>
<p>2σ2u + σ
2
ǫ , which when solved for σ
</p>
<p>2
u gives:
</p>
<p>σ2u = σ
2
ǫ/(1&minus; ρ2) (5.31)
</p>
<p>Hence, u0 &sim; (0, σ
2
ǫ/(1 &minus; ρ2)) for the u&rsquo;s to have zero mean and homoskedastic disturbances.
</p>
<p>Multiplying (5.26) by ut&minus;1 and taking expected values, one gets
</p>
<p>E(utut&minus;1) = ρE(u
2
t&minus;1) + E(ut&minus;1ǫt) = ρσ
</p>
<p>2
u (5.32)
</p>
<p>since E(u2t&minus;1) = σ
2
u and E(ut&minus;1ǫt) = 0. Therefore, cov(ut, ut&minus;1) = ρσ
</p>
<p>2
u, and the correlation coef-
</p>
<p>ficient between ut and ut&minus;1 is correl(ut, ut&minus;1) = cov(ut, ut&minus;1)/
&radic;
</p>
<p>var(ut)var(ut&minus;1) = ρσ2u/σ
2
u = ρ.
</p>
<p>Since ρ is a correlation coefficient, this means that &minus;1 &le; ρ &le; 1. In general, one can show that
</p>
<p>cov(ut, us) = ρ
|t&minus;s|σ2u t, s = 1, 2, . . . , T (5.33)
</p>
<p>see problem 6. This means that the correlation between ut and ut&minus;r is ρr, which is a fraction
raised to an integer power, i.e., the correlation is decaying between the disturbances the further
apart they are. This is reasonable in economics and may be the reason why this autoregressive
form (5.26) is so popular. One should note that this is not the only form that would correlate the
disturbances across time. In Chapter 14, we will consider other forms like the Moving Average
(MA) process, and higher order Autoregressive Moving Average (ARMA) processes, but these
are beyond the scope of this chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>112 Chapter 5: Violations of the Classical Assumptions
</p>
<p>Consequences for OLS
</p>
<p>How is the OLS estimator affected by the violation of the no autocorrelation assumption among
the disturbances? The OLS estimator is still unbiased and consistent since these properties rely
on assumptions 1 and 4 and have nothing to do with assumption 3. For the simple linear
regression, using (5.2), the variance of β̂OLS is now
</p>
<p>var(β̂OLS) = var(
&sum;T
</p>
<p>t=1wtut) =
&sum;T
</p>
<p>t=1
</p>
<p>&sum;T
s=1wtwscov(ut, us) (5.34)
</p>
<p>= σ2u/
&sum;T
</p>
<p>t=1 x
2
t +
</p>
<p>&sum;&sum;
t �=s
</p>
<p>wtwsρ
|t&minus;s|σ2u
</p>
<p>where cov(ut, us) = ρ
|t&minus;s|σ2u as explained in (5.33). Note that the first term in (5.34) is the
</p>
<p>usual variance of β̂OLS under the classical case. The second term in (5.34) arises because of the
correlation between the ut&rsquo;s. Hence, the variance of OLS computed from a regression package,
i.e., s2/
</p>
<p>&sum;T
t=1 x
</p>
<p>2
t is a wrong estimate of the variance of β̂OLS for two reasons. First, it is using
</p>
<p>the wrong formula for the variance, i.e., σ2u/
&sum;T
</p>
<p>t=1 x
2
t rather than (5.34). The latter depends on ρ
</p>
<p>through the extra term in (5.34). Second, one can show, see problem 7, that E(s2) 	= σ2u and will
involve ρ as well as σ2u. Hence, s
</p>
<p>2 is not unbiased for σ2u and s
2/
</p>
<p>&sum;T
t=1 x
</p>
<p>2
t is a biased estimate of
</p>
<p>var(β̂OLS). The direction and magnitude of this bias depends on ρ and the regressor. In fact, if ρ
is positive, and the xt&rsquo;s are themselves positively autocorrelated, then s
</p>
<p>2/
&sum;T
</p>
<p>t=1 x
2
t understates
</p>
<p>the true variance of β̂OLS . This means that the confidence interval for β is tighter than it should
be and the t-statistic for H0; β = 0 is overblown, see problem 8. As in the heteroskedastic
case, but for completely different reasons, any inference based on var(β̂OLS) reported from the
standard regression packages will be misleading if the ut&rsquo;s are serially correlated.
</p>
<p>Newey and West (1987) suggested a simple heteroskedasticity and autocorrelation-consistent
covariance matrix for the OLS estimator without specifying the functional form of the serial
correlation. The basic idea extends White&rsquo;s (1980) replacement of heteroskedastic variances
with squared OLS residuals e2t by additionally including products of least squares residuals
etet&minus;s for s = 0,&plusmn;1, . . . ,&plusmn;p where p is the maximum order of serial correlation we are willing to
assume. The consistency of this procedure relies on p being very small relative to the number
of observations T . This is consistent with popular serial correlation specifications considered
in this chapter where the autocorrelation dies out quickly as j increases. Newey and West
(1987) allow the higher order covariance terms to receive diminishing weights. This Newey-
West option for the least squares estimator is available using EViews. Andrews (1991) warns
about the unreliability of such standard error corrections in some circumstances. Wooldridge
(1991) shows that it is possible to construct serially correlated robust F -statistics for testing
joint hypotheses as considered in Chapter 4. However, these are beyond the scope of this book.
Is OLS still BLUE? In order to determine the BLU estimator in this case, we lag the regression
</p>
<p>equation once, multiply it by ρ, and subtract it from the original regression equation, we get
</p>
<p>Yt &minus; ρYt&minus;1 = α(1&minus; ρ) + β(Xt &minus; ρXt&minus;1) + ǫt t = 2, 3, . . . , T (5.35)
</p>
<p>This transformation, known as the Cochrane-Orcutt (1949) transformation, reduces the dis-
turbances to classical errors. Therefore, OLS on the resulting regression renders the estimates
BLU, i.e., run Ỹt = Yt &minus; ρYt&minus;1 on a constant and X̃t = Xt &minus; ρXt&minus;1, for t = 2, 3, . . . , T . Note
that we have lost one observation by lagging, and the resulting estimators are BLUE only for
linear combinations of (T &minus; 1) observations in Y .1 Prais and Winsten (1954) derive the BLU</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Autocorrelation 113
</p>
<p>estimators for linear combinations of T observations in Y . This entails recapturing the initial
observation as follows: (i) Multiply the first observation of the regression equation by
</p>
<p>&radic;
1&minus; ρ2;
</p>
<p>&radic;
1&minus; ρ2Y1 = α
</p>
<p>&radic;
1&minus; ρ2 + β
</p>
<p>&radic;
1&minus; ρ2X1 +
</p>
<p>&radic;
1&minus; ρ2u1
</p>
<p>(ii) add this transformed initial observation to the Cochrane-Orcutt transformed observations
for t = 2, . . . , T and run the regression on the T observations rather than the (T&minus;1) observations.
See Chapter 9, for a formal proof of this result. Note that
</p>
<p>Ỹ1 =
&radic;
1&minus; ρ2Y1
</p>
<p>and
</p>
<p>Ỹt = Yt &minus; ρYt&minus;1 for t = 2, . . . , T
</p>
<p>Similarly, X̃1=
&radic;
1&minus; ρ2X1 and X̃t = Xt &minus; ρXt&minus;1 for t = 2, . . . , T . The constant variable Ct = 1
</p>
<p>for t = 1, . . . , T is now a new variable C̃t which takes the values C̃1=
&radic;
1&minus; ρ2 and C̃t = (1&minus; ρ)
</p>
<p>for t = 2, . . . , T . Hence, the Prais-Winsten procedure is the regression of Ỹt on C̃t and X̃t
without a constant. It is obvious that the resulting BLU estimators will involve ρ and are
therefore, different from the usual OLS estimators except in the case where ρ = 0. Hence, OLS
is no longer BLUE. Furthermore, we need to know ρ in order to obtain the BLU estimators.
In applied work, ρ is not known and has to be estimated, in which case the Prais-Winsten
regression is no longer BLUE since it is based on an estimate of ρ rather than the true ρ itself.
However, as long as ρ̂ is a consistent estimate for ρ then this is a sufficient condition for the
corresponding estimates of α and β in the next step to be asymptotically efficient, see Chapter
9. We now turn to various methods of estimating ρ.
</p>
<p>(1) The Cochrane-Orcutt (1949) Method: This method starts with an initial estimate of
ρ, the most convenient is 0, and minimizes the residual sum of squares in (5.35). This gives us
the OLS estimates of α and β. Then we substitute α̂OLS and β̂OLS in (5.35) and we get
</p>
<p>et = ρet&minus;1 + ǫt t = 2, . . . , T (5.36)
</p>
<p>where et denotes the OLS residual. An estimate of ρ can be obtained by minimizing the residual
sum of squares in (5.36) or running the regression of et on et&minus;1 without a constant. The resulting
estimate of ρ is ρ̂co =
</p>
<p>&sum;T
t=2 etet&minus;1/
</p>
<p>&sum;T
t=2 e
</p>
<p>2
t&minus;1 where both summations run over t = 2, 3, . . . , T .
</p>
<p>The second step of the Cochrane-Orcutt procedure (2SCO) is to perform the regression in
(5.35) with ρ̂co instead of ρ. One can iterate this procedure (ITCO) by computing new residuals
based on the new estimates of α and β and hence a new estimate of ρ from (5.36), and so on,
until convergence. Both the 2SCO and the ITCO are asymptotically efficient, the argument for
iterating must be justified in terms of small sample gains.
</p>
<p>(2) The Hilderth-Lu (1960) Search Procedure: ρ is between &minus;1 and 1. Therefore, this
procedure searches over this range, i.e., using values of ρ say between &minus;0.9 and 0.9 in intervals
of 0.1. For each ρ, one computes the regression in (5.35) and reports the residual sum of squares
corresponding to that ρ. The minimum residual sum of squares gives us our choice of ρ and the
corresponding regression gives us the estimates of α, β and σ2. One can refine this procedure
around the best ρ found in the first stage of the search. For example, suppose that ρ = 0.6 gave</p>
<p/>
</div>
<div class="page"><p/>
<p>114 Chapter 5: Violations of the Classical Assumptions
</p>
<p>the minimum residual sum of squares, one can search next between 0.51 and 0.69 in intervals
of 0.01. This search procedure guards against a local minimum. Since the likelihood in this case
contains ρ as well as σ2 and α and β, this search procedure can be modified to maximize the
likelihood rather than minimize the residual sum of squares, since the two criteria are no longer
equivalent. The maximum value of the likelihood will give our choice of ρ and the corresponding
estimates of α, β and σ2.
</p>
<p>(3) Durbin&rsquo;s (1960) Method: One can rearrange (5.35) by moving Yt&minus;1 to the right hand
side, i.e.,
</p>
<p>Yt = ρYt&minus;1 + α(1&minus; ρ) + βXt &minus; ρβXt&minus;1 + ǫt (5.37)
and running OLS on (5.37). The error in (5.37) is classical, and the presence of Yt&minus;1 on the
right hand side reminds us of the contemporaneously uncorrelated case discussed under the
violation of assumption 4. For that violation, we have shown that unbiasedness is lost, but not
consistency. Hence, the estimate of ρ as a coefficient of Yt&minus;1 is biased but consistent. This is
the Durbin estimate of ρ, call it ρ̂D. Next, the second step of the Cochrane-Orcutt procedure
is performed using this estimate of ρ.
</p>
<p>(4) Beach-MacKinnon (1978) Maximum Likelihood Procedure: Beach and MacKinnon
(1978) derived a cubic equation in ρ which maximizes the likelihood function concentrated with
respect to α, β, and σ2. With this estimate of ρ, denoted by ρ̂BM , one performs the Prais-
Winsten procedure in the next step.
Correcting for serial correlation is not without its critics. Mizon (1995) argues this point
</p>
<p>forcefully in his article entitled &ldquo;A simple message for autocorrelation correctors: Don&rsquo;t.&rdquo; The
main point being that serial correlation is a symptom of dynamic misspecification which is
better represented using a general unrestricted dynamic specification.
</p>
<p>Monte Carlo Results
</p>
<p>Rao and Griliches (1969) performed a Monte Carlo study using an autoregressive Xt, and
various values of ρ. They found that OLS is still a viable estimator as long as |ρ| &lt; 0.3, but
if |ρ| &gt; 0.3, then it pays to perform procedures that correct for serial correlation based on an
estimator of ρ. Their recommendation was to compute a Durbin&rsquo;s estimate of ρ in the first step
and to do the Prais-Winsten procedure in the second step. Maeshiro (1976, 1979) found that
if the Xt series is trended, which is usual with economic data, then OLS outperforms 2SCO,
but not the two-step Prais-Winsten (2SPW) procedure that recaptures the initial observation.
These results were confirmed by Park and Mitchell (1980) who performed an extensive Monte
Carlo using trended and untrended Xt&rsquo;s. Their basic findings include the following: (i) For
trended Xt&rsquo;s, OLS beats 2SCO, ITCO and even a Cochrane-Orcutt procedure that is based on
the true ρ. However, OLS was beaten by 2SPW, iterative Prais-Winsten (ITPW), and Beach-
MacKinnon (BM). Their conclusion is that one should not use regressions based on (T &minus; 1)
observations as in Cochrane and Orcutt. (ii) Their results find that the ITPW procedure is the
recommended estimator beating 2SPW and BM for high values of true ρ, for both trended as
well as nontrended Xt&rsquo;s. (iii) Test of hypotheses regarding the regression coefficients performed
miserably for all estimators based on an estimator of ρ. The results indicated less bias in
standard error estimation for ITPW, BM and 2SPW than OLS. However, the tests based on
these standard errors still led to a high probability of type I error for all estimation procedures.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Autocorrelation 115
</p>
<p>Testing for Autocorrelation
</p>
<p>So far, we have studied the properties of OLS under the violation of assumption 3. We have
derived asymptotically efficient estimators of the coefficients based on consistent estimators of
ρ and studied their small sample properties using Monte Carlo experiments. Next, we focus on
the problem of detecting this autocorrelation between the disturbances. A popular diagnostic
for detecting such autocorrelation is the Durbin and Watson (1951) statistic2
</p>
<p>d =
&sum;T
</p>
<p>t=2(et &minus; et&minus;1)2/
&sum;T
</p>
<p>t=1 e
2
t (5.38)
</p>
<p>If this was based on the true ut&rsquo;s and T was very large then d can be shown to tend in the limit
as T gets large to 2(1 &minus; ρ), see problem 9. This means that if ρ &rarr; 0, then d &rarr; 2; if ρ &rarr; 1,
then d &rarr; 0 and if ρ &rarr; &minus;1, then d &rarr; 4. Therefore, a test for H0; ρ = 0, can be based on
whether d is close to 2 or not. Unfortunately, the critical values of d depend upon the Xt&rsquo;s, and
these vary from one data set to another. To get around this, Durbin and Watson established
upper (dU ) and lower (dL) bounds for this critical value. If the observed d is less than dL, or
larger than 4 &minus; dL, we reject H0. If the observed d is between dU and 4 &minus; dU , then we do not
reject H0. If d lies in any of the two indeterminant regions, then one should compute the exact
critical values which depend on the data. Most regression packages report the Durbin-Watson
statistic, but few give the exact p-value for this d-statistic. If one is interested in a single sided
test, say H0; ρ = 0 versus H1; ρ &gt; 0 then one would reject H0 if d &lt; dL, and not reject H0 if
d &gt; dU . If dL &lt; d &lt; dU , then the test is inconclusive. Similarly for testing H0; ρ = 0 versus
H1; ρ &lt; 0, one computes (4&minus;d) and follow the steps for testing against positive autocorrelation.
Durbin and Watson tables for dL and dU covered samples sizes from 15 to 100 and a maximum
of 5 regressors. Savin and White (1977) extended these tables for 6 &le; T &le; 200 and up to 10
regressors.
The Durbin-Watson statistic has several limitations. We discussed the inconclusive region and
</p>
<p>the computation of exact critical values. The Durbin-Watson statistic is appropriate when there
is a constant in the regression. In case there is no constant in the regression, see Farebrother
(1980). Also, the Durbin-Watson statistic is inappropriate when there are lagged values of the
dependent variable among the regressors. We now turn to an alternative test for serial correlation
that does not have these limitations and that is also easy to apply. This test was derived by
Breusch (1978) and Godfrey (1978) and is known as the Breusch-Godfrey test for zero first-order
serial correlation. This is a Lagrange Multiplier test that amounts to running the regression of
the OLS residuals et on et&minus;1 and the original regressors in the model. The test statistic is TR2.
Its distribution under the null is χ21. In this case, the regressors are a constant and Xt, and the
test checks whether the coefficient of et&minus;1 is significant. The beauty of this test is that (i) it
is the same test for first-order serial correlation, whether the disturbances are Moving Average
of order one MA(1) or AR(1). (ii) This test is easily generalizable to higher autoregressive or
Moving Average schemes. For second-order serial correlation, like MA(2) or AR(2) one includes
two lags of the residuals on the right hand side; i.e., both et&minus;1 and et&minus;2. (iii) This test is still
valid even when lagged values of the dependent variable are present among the regressors, see
Chapter 6. The Breusch and Godfrey test is standard using EViews and it prompts the user
with a choice of the number of lags of the residuals to include among the regressors to test
for serial correlation. You click on residuals, then tests and choose Breusch-Godfrey. Next, you
input the number of lagged residuals you want to include.</p>
<p/>
</div>
<div class="page"><p/>
<p>116 Chapter 5: Violations of the Classical Assumptions
</p>
<p>What about first differencing the data as a possible solution for getting rid of serial correla-
tion? Some economic behavioral equations are specified with variables in first difference form,
like GDP growth, but other equations are first differenced for estimation purposes. In the latter
case, if the original disturbances were not autocorrelated, (or even correlated, with ρ 	= 1), then
the transformed disturbances are serially correlated. After all, first differencing the disturbances
is equivalent to setting ρ = 1 in ut&minus;ρut&minus;1, and this new disturbance u&lowast;t = ut&minus;ut&minus;1 has ut&minus;1 in
common with u&lowast;t&minus;1 = ut&minus;1 &minus; ut&minus;2, making E(u&lowast;tu&lowast;t&minus;1) = &minus;E(u2t&minus;1) = &minus;σ2u. However, one could
argue that if ρ is large and positive, first differencing the data may not be a bad solution. Rao
and Miller (1971) calculated the variance of the BLU estimator correcting for serial correlation,
for various guesses of ρ. They assume a true ρ of 0.2, and an autoregressive Xt
</p>
<p>Xt = λXt&minus;1 + wt with λ = 0, 0.4, 0.8. (5.39)
</p>
<p>They find that OLS (or a guess of ρ = 0), performs better than first differencing the data,
and is pretty close in terms of efficiency to the true BLU estimator for trended Xt (λ = 0.8).
However, the performance of OLS deteriorates as λ declines to 0.4 and 0, with respect to the true
BLU estimator. This supports the Monte Carlo finding by Rao and Griliches that for |ρ| &lt; 0.3,
OLS performs reasonably well relative to estimators that correct for serial correlation. However,
the first-difference estimator, i.e., a guess of ρ = 1, performs badly for trended Xt (λ = 0.8)
giving the worst efficiency when compared to any other guess of ρ. Only when the Xt&rsquo;s are
less trended (λ = 0.4) or random (λ = 0), does the efficiency of the first-difference estimator
improve. However, even for those cases one can do better by guessing ρ. For example, for λ = 0,
one can always do better than first differencing by guessing any positive ρ less than 1. Similarly,
for true ρ = 0.6, a higher degree of serial correlation, Rao and Miller (1971) show that the
performance of OLS deteriorates, while that of the first difference improves. However, one can
still do better than first differencing by guessing in the interval (0.4, 0.9). This gain in efficiency
increases with trended Xt&rsquo;s.
</p>
<p>Empirical Example: Table 5.3 gives the U.S. Real Personal Consumption Expenditures (C)
and Real Disposable Personal Income (Y ) from the Economic Report of the President over the
period 1959&ndash;2007. This data set is available as CONSUMP.DAT on the Springer web site.
The OLS regression yields:
</p>
<p>Ct =&minus;1343.31
(219.56)
</p>
<p>+ 0.979
</p>
<p>(0.011)
</p>
<p>Yt + residuals
</p>
<p>Figure 5.3 plots the actual, fitted and residuals using EViews 6.0. This shows positive serial
correlation with a string of positive residuals followed by a string of negative residuals followed
by positive residuals. The Durbin-Watson statistic is d = 0.181 which is much smaller than the
lower bound d = 1.497 for T = 49 and one regressor. Therefore, we reject the null hypothesis
of H0; ρ = 0 at the 5% significance level.
</p>
<p>The Breusch (1978) and Godfrey (1978) regression that tests for first-order serial correlation
is given in Table 5.4. This is done using EViews 6.0.
This yields
</p>
<p>et = &minus;54.41
(102.77)
</p>
<p>+ 0.004
</p>
<p>(0.005)
</p>
<p>Yt + 0.909
</p>
<p>(0.070)
</p>
<p>et&minus;1 + residuals
</p>
<p>The test statistic is TR2 which yields 49&times; (0.786) = 38.5. This is distributed as χ21 under H0;
ρ = 0. This rejects the null hypothesis of no first order serial correlation with a p-value of 0.0000
shown in Table 5.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Autocorrelation 117
</p>
<p>Table 5.3 U.S. Consumption Data, 1959&ndash;2007
</p>
<p>C = Real Personal Consumption Expenditures (in 1987 dollars)
</p>
<p>Y = Real Disposable Personal Income (in 1987 dollars)
</p>
<p>YEAR Y C
</p>
<p>1959 8776 9685
1960 8837 9735
1961 8873 9901
1962 9170 10227
1963 9412 10455
1964 9839 11061
1965 10331 11594
1966 10793 12065
1967 10994 12457
1968 11510 12892
1969 11820 13163
1970 11955 13563
1971 12256 14001
1972 12868 14512
1973 13371 15345
1974 13148 15094
1975 13320 15291
1976 13919 15738
1977 14364 16128
1978 14837 16704
1979 15030 16931
1980 14816 16940
1981 14879 17217
1982 14944 17418
1983 15656 17828
</p>
<p>YEAR Y C
</p>
<p>1984 16343 19011
1985 17040 19476
1986 17570 19906
1987 17994 20072
1988 18554 20740
1989 18898 21120
1990 19067 21281
1991 18848 21109
1992 19208 21548
1993 19593 21493
1994 20082 21812
1995 20382 22153
1996 20835 22546
1997 21365 23065
1998 22183 24131
1999 23050 24564
2000 23862 25472
2001 24215 25697
2002 24632 26238
2003 25073 26566
2004 25750 27274
2005 26290 27403
2006 26835 28098
2007 27319 28614
</p>
<p>Source: Economic Report of the President
</p>
<p>Regressing the OLS residuals on their lagged values yields
</p>
<p>et = 0.906
</p>
<p>(0.062)
</p>
<p>et&minus;1 + residuals
</p>
<p>The two-step Cochrane-Orcutt (1949) procedure based on ρ̂ = 0.906 using Stata 11 yields the
results given in Table 5.5.
</p>
<p>The Prais-Winsten (1954) procedure using Stata 11 yields the results given in Table 5.6. The
estimate of the marginal propensity to consume is 0.979 for OLS, 0.989 for two-step Cochrane-
Orcutt, and 0.912 for iterative Prais-Winsten. All of these estimates are significant.
The Newey-West heteroskedasticity and autocorrelation-consistent standard errors for least
</p>
<p>squares with a three-year lag truncation are given in Table 5.7 using EViews 6. Note that both
standard errors are now larger than those reported by least squares. But once again, this is not
necessarily the case for other data sets.</p>
<p/>
</div>
<div class="page"><p/>
<p>118 Chapter 5: Violations of the Classical Assumptions
</p>
<p>-1,000
</p>
<p>-500
</p>
<p>0
</p>
<p>500
</p>
<p>1,000
</p>
<p>5,000
</p>
<p>10,000
</p>
<p>15,000
</p>
<p>20,000
</p>
<p>25,000
</p>
<p>30,000
</p>
<p>5 10 15 20 25 30 35 40 45
</p>
<p>Residual Actual Fitted
</p>
<p>Figure 5.3 Residual Plot: Consumption Regression
</p>
<p>Table 5.4 Breusch-Godfrey LM Test
</p>
<p>F-statistic 168.9023 Prob. F(1,46) 0.0000
Obs*R-squared 38.51151 Prob. Chi-Square(1) 0.0000
</p>
<p>Test Equation:
Dependent Variable: RESID
Method: Least Squares
</p>
<p>Sample 1959 2007
Included observations: 49
Presample missing value lagged residuals set to zero
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;54.41017 102.7650 &ndash;0.529462 0.5990
Y 0.003590 0.005335 0.673044 0.5043
RESID(&ndash;1) 0.909272 0.069964 12.99624 0.0000
R-squared 0.785949 Mean dependent var &ndash;5.34E-13
Adjusted R-squared 0.776643 S.D. dependent var 433.0451
S.E. of regression 204.6601 Akaike info criterion 13.53985
Sum squared resid 1926746. Schwarz criterion 13.65567
Log likelihood &ndash;328.7263 Hannan-Quinn criter. 13.58379
F-statistic 84.45113 Durbin-Watson stat 2.116362
Prob(F-statistic) 0.000000</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Autocorrelation 119
</p>
<p>Table 5.5 Cochrane-Orcutt AR(1) Regression &ndash; Twostep
</p>
<p>. prais c y, corc two
</p>
<p>Iteration 0: rho = 0.0000
</p>
<p>Iteration 1: rho = 0.9059
</p>
<p>Cochrane-Orcutt AR(1) regression &ndash; twostep estimates
</p>
<p>Source SS df MS Number of obs = 48
</p>
<p>F(1, 46) = 519.58
</p>
<p>Model 17473195 1 17473195 Prob &gt; F = 0.0000
</p>
<p>Residual 1546950.74 46 33629.364 R-squared = 0.9187
</p>
<p>Adj R-squared = 0.9169
</p>
<p>Total 19020145.7 47 404683.951 Root MSE = 183.38
</p>
<p>c Coef. Std. Err. t P &gt; |t| [95% Conf. Interval]
</p>
<p>y .9892295 .0433981 22.79 0.000 .9018738 1.076585
</p>
<p>cons &ndash;1579.722 1014.436 &ndash;1.56 0.126 &ndash;3621.676 462.2328
</p>
<p>rho .9059431
</p>
<p>Durbin-Watson statistic (original) 0.180503
</p>
<p>Durbin-Watson statistic (transformed) 2.457550
</p>
<p>Table 5.6 The Iterative Prais-Winsten AR(1) Regression
</p>
<p>. prais c y
</p>
<p>Prais-Winsten AR(1) regression &ndash; iterated estimates
</p>
<p>Source SS df MS Number of obs = 49
</p>
<p>F(1, 47) = 119.89
</p>
<p>Model 3916565.48 1 3916565.48 Prob &gt; F = 0.0000
</p>
<p>Residual 1535401.45 47 32668.1159 R-squared = 0.7184
</p>
<p>Adj R-squared = 0.7124
</p>
<p>Total 5451966.93 48 113582.644 Root MSE = 180.74
</p>
<p>c Coef. Std. Err. t P &gt; |t| [95% Conf. Interval]
</p>
<p>y .912147 .047007 19.40 0.000 .8175811 1.006713
</p>
<p>cons 358.9638 1174.865 0.31 0.761 &ndash;2004.56 2722.488
</p>
<p>rho .9808528
</p>
<p>Durbin-Watson statistic (original) 0.180503
</p>
<p>Durbin-Watson statistic (transformed) 2.314703</p>
<p/>
</div>
<div class="page"><p/>
<p>120 Chapter 5: Violations of the Classical Assumptions
</p>
<p>Table 5.7 The Newey-West HAC Standard Errors
</p>
<p>Dependent Variable: CONSUM
</p>
<p>Method: Least Squares
</p>
<p>Sample: 1959 2007
</p>
<p>Included observations: 49
</p>
<p>Newey-West HAC Standard Errors &amp; Covariance (lag truncation=3)
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;1343.314 422.2947 &ndash;3.180987 0.0026
</p>
<p>Y 0.979228 0.022434 43.64969 0.0000
</p>
<p>R-squared 0.993680 Mean dependent var 16749.10
</p>
<p>Adjusted R-squared 0.993545 S.D. dependent var 5447.060
</p>
<p>S.E. of regression 437.6277 Akaike info criterion 15.04057
</p>
<p>Sum squared resid 9001348. Schwarz criterion 15.11779
</p>
<p>Log likelihood &ndash;366.4941 Hannan-Quinn criter. 15.06987
</p>
<p>F-statistic 7389.281 Durbin-Watson stat 0.180503
</p>
<p>Prob(F-statistic) 0.000000
</p>
<p>Notes
</p>
<p>1. A computational warning is in order when one is applying the Cochrane-Orcutt transformation
to cross-section data. Time-series data has a natural ordering which is generally lacking in cross-
section data. Therefore, one should be careful in applying the Cochrane-Orcutt transformation to
cross-section data since it is not invariant to the ordering of the observations.
</p>
<p>2. Another test for serial correlation can be obtained as a by-product of maximum likelihood estima-
</p>
<p>tion. The maximum likelihood estimator of ρ has a normal limiting distribution with mean ρ and
</p>
<p>variance (1&minus; ρ2)/T . Hence, one can compute ρ̂MLE/[(1&minus; ρ̂2MLE)/T ]1/2and compare it to critical
values from the normal distribution.
</p>
<p>Problems
</p>
<p>1. s2 Is Biased Under Heteroskedasticity. For the simple linear regression with heteroskedasticity,
i.e., E(u2i ) = σ
</p>
<p>2
i , show that E(s
</p>
<p>2) is a function of the σ2i &rsquo;s?
</p>
<p>2. OLS Variance Is Biased Under Heteroskedasticity. For the simple linear regression with het-
eroskedasticity of the form E(u2i ) = σ
</p>
<p>2
i = bx
</p>
<p>2
i where b &gt; 0, show that E(s
</p>
<p>2/
&sum;n
</p>
<p>i=1 x
2
i ) understates
</p>
<p>the variance of β̂OLS which is
</p>
<p>&sum;n
i=1 x
</p>
<p>2
iσ
</p>
<p>2
i /(
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )
</p>
<p>2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 121
</p>
<p>3. Weighted Least Squares. This is based on Kmenta (1986).
</p>
<p>(a) Solve the two equations in (5.11) and show that the solution is given by (5.12).
</p>
<p>(b) Show that
</p>
<p>var(β̃) =
</p>
<p>&sum;n
i=1(1/σ
</p>
<p>2
i )
</p>
<p>[
&sum;n
</p>
<p>i=1 X
2
i /σ
</p>
<p>2
i ][
</p>
<p>&sum;n
i=1(1/σ
</p>
<p>2
i )]&minus; [
</p>
<p>&sum;n
i=1(Xi/σ
</p>
<p>2
i )]
</p>
<p>2
</p>
<p>=
</p>
<p>&sum;n
i=1 w
</p>
<p>&lowast;
i
</p>
<p>(
&sum;n
</p>
<p>i=1 w
&lowast;
iX
</p>
<p>2
i )(
</p>
<p>&sum;n
i=1 w
</p>
<p>&lowast;
i )&minus; (
</p>
<p>&sum;n
i=1 w
</p>
<p>&lowast;
iXi)
</p>
<p>2
</p>
<p>=
1&sum;n
</p>
<p>i=1 w
&lowast;
i (Xi &minus; X̄&lowast;)2
</p>
<p>where w&lowast;i = (1/σ
2
i ) and X̄
</p>
<p>&lowast; =
&sum;n
</p>
<p>i=1 w
&lowast;
iXi/
</p>
<p>&sum;n
i=1 w
</p>
<p>&lowast;
i .
</p>
<p>4. Relative Efficiency of OLS Under Heteroskedasticity. Consider the simple linear regression with
heteroskedasticity of the form σ2i = σ
</p>
<p>2Xδi where Xi = 1, 2, . . . , 10.
</p>
<p>(a) Compute var(β̂OLS) for δ = 0.5, 1, 1.5 and 2.
</p>
<p>(b) Compute var(β̃BLUE) for δ = 0.5, 1, 1.5 and 2.
</p>
<p>(c) Compute the efficiency of β̂OLS = var(β̃BLUE)/var(β̂OLS) for δ = 0.5, 1, 1.5 and 2. What
happens to this efficiency measure as δ increases?
</p>
<p>5. Consider the simple regression with only a constant yi = α + ui for i = 1, 2, . . . , n; where the
ui&rsquo;s are independent with mean zero and var(ui) = σ
</p>
<p>2
1 for i = 1, 2, . . . , n1; and var(ui) = σ
</p>
<p>2
2 for
</p>
<p>i = n1 + 1, . . . , n1 + n2 with n = n1 + n2.
</p>
<p>(a) Derive the OLS estimator of α along with its mean and variance.
</p>
<p>(b) Derive the GLS estimator of α along with its mean and variance.
</p>
<p>(c) Obtain the relative efficiency of OLS with respect to GLS. Compute their relative efficiency
for various values of σ22/σ
</p>
<p>2
1 = 0.2, 0.4, 0.6, 0.8, 1, 1.25, 1.33, 2.5, 5; and n1/n = 0.2, 0.3, 0.4, . . . ,
</p>
<p>0.8. Plot this relative efficiency.
</p>
<p>(d) Assume that ui is N(0, σ
2
1) for i = 1, 2, . . . , n1; and N(0, σ
</p>
<p>2
1) for i = n1+1, . . . , n1+n2; with
</p>
<p>ui&rsquo;s being independent. What is the maximum likelihood estimator of α, σ
2
1 and σ
</p>
<p>2
2?
</p>
<p>(e) Derive the LR test for testing H0;σ
2
1 = σ
</p>
<p>2
2 in part (d).
</p>
<p>6. Show that for an AR(1) model given in (5.26), E(utus) = ρ
|t&minus;s|σ2u for t, s = 1, 2, . . . , T .
</p>
<p>7. Relative Efficiency of OLS Under the AR(1) Model. This problem is based on Johnston (1984, pp.
310&ndash;312). For the simple regression without a constant yt = βxt + ut with ut = ρut&minus;1 + ǫt and
ǫt &sim; IID(0, σ
</p>
<p>2
ǫ)
</p>
<p>(a) Show that
</p>
<p>var(β̂OLS) =
σ2u&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>(
1 + 2ρ
</p>
<p>&sum;T&minus;1
t=1 xtxt+1&sum;T
</p>
<p>t=1 x
2
t
</p>
<p>+ 2ρ2
&sum;T&minus;2
</p>
<p>t=1 xtxt+2&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>+ . . .+ 2ρT&minus;1
x1xT&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>)
</p>
<p>and that the Prais-Winsten estimator β̂PW has variance
</p>
<p>var(β̂PW ) =
σ2u&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>[
1&minus; ρ2
</p>
<p>1 + ρ2 &minus; 2ρ&sum;T&minus;1t=1 xtxt+1/
&sum;T
</p>
<p>t=1 x
2
t
</p>
<p>]
</p>
<p>These expressions are easier to prove using matrix algebra, see Chapter 9.</p>
<p/>
</div>
<div class="page"><p/>
<p>122 Chapter 5: Violations of the Classical Assumptions
</p>
<p>(b) Let xt itself follow an AR(1) scheme with parameter λ, i.e., xt = λxt&minus;1+vt, and let T &rarr; &infin;.
Show that
</p>
<p>asy eff(β̂OLS) = lim
T&rarr;&infin;
</p>
<p>var(β̂PW )
</p>
<p>var(β̂OLS)
=
</p>
<p>1&minus; ρ2
(1 + ρ2 &minus; 2ρλ)(1 + 2ρλ+ 2ρ2λ2 + . . .)
</p>
<p>=
(1&minus; ρ2)(1&minus; ρλ)
</p>
<p>(1 + ρ2 &minus; 2ρλ)(1 + ρλ)
</p>
<p>(c) Tabulate this asy eff(β̂OLS) for various values of ρ and λ where ρ varies between &minus;0.9 to
+0.9 in increments of 0.1, while λ varies between 0 and 0.9 in increments of 0.1. What do you
conclude? How serious is the loss in efficiency in using OLS rather than the PW procedure?
</p>
<p>(d) Ignoring this autocorrelation one would compute σ2u/
&sum;T
</p>
<p>t=1 x
2
t as the var(β̂OLS). The differ-
</p>
<p>ence between this wrong formula and that derived in part (a) gives us the bias in estimating
</p>
<p>the variance of β̂OLS . Show that as T &rarr; &infin;, this asymptotic proportionate bias is given by
&minus;2ρλ/(1 + ρλ). Tabulate this asymptotic bias for various values of ρ and λ as in part (c).
What do you conclude? How serious is the asymptotic bias of using the wrong variances for
β̂OLS when the disturbances are first-order autocorrelated?
</p>
<p>(e) Show that
</p>
<p>E(s2) = σ2u
</p>
<p>{
T &minus;
</p>
<p>(
1 + 2ρ
</p>
<p>&sum;T&minus;1
t=1 xtxt+1&sum;T
</p>
<p>t=1 x
2
t
</p>
<p>+ 2ρ2
&sum;T&minus;2
</p>
<p>t=1 xtxt+2&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>+ . . .+ 2ρT&minus;1
x1xT&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>)}
/(T &minus; 1)
</p>
<p>Conclude that if ρ = 0, then E(s2) = σ2u. If xt follows an AR(1) scheme with parameter λ,
then for a large T , we get
</p>
<p>E(s2) = σ2u
</p>
<p>(
T &minus; 1 + ρλ
</p>
<p>1&minus; ρλ
</p>
<p>)
/(T &minus; 1)
</p>
<p>Compute this E(s2) for T = 101 and various values of ρ and λ as in part (c). What do you
conclude? How serious is the bias in using s2 as an unbiased estimator for σ2u?
</p>
<p>8. OLS Variance Is Biased Under Serial Correlation. For the AR(1) model given in (5.26), show that
</p>
<p>if ρ &gt; 0 and the xt&rsquo;s are positively autocorrelated that E(s
2/
</p>
<p>&sum;
x2t ) understates the var(β̂OLS)
</p>
<p>given in (5.34).
</p>
<p>9. Show that for the AR(1) model, the Durbin-Watson statistic has plimd &rarr; 2(1&minus; ρ).
</p>
<p>10. Regressions with Non-zero Mean Disturbances. Consider the simple regression with a constant
</p>
<p>Yi = α+ βXi + ui i = 1, 2, . . . , n
</p>
<p>where α and β are scalars and ui is independent of the Xi&rsquo;s. Show that:
</p>
<p>(a) If the ui&rsquo;s are independent and identically gamma distributed with f(ui) =
1
</p>
<p>Γ(θ)u
θ&minus;1
i e
</p>
<p>&minus;ui
</p>
<p>where ui &ge; 0 and θ &gt; 0, then α̂OLS &minus; s2 is unbiased for α.
(b) If the ui&rsquo;s are independent and identically χ
</p>
<p>2 distributed with ν degrees of freedom, then
α̂OLS &minus; s2/2 is unbiased for α.
</p>
<p>(c) If the ui&rsquo;s are independent and identically exponentially distributed with f(ui) =
1
θ e
</p>
<p>&minus;ui/θ
</p>
<p>where ui &ge; 0 and θ &gt; 0, then α̂OLS &minus; s is consistent for α.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 123
</p>
<p>11. The Heteroskedastic Consequences of an Arbitrary Variance for the Initial Disturbance of an AR(1)
Model. This is based on Baltagi and Li (1990, 1992). Consider a simple AR(1) model
</p>
<p>ut = ρut&minus;1 + ǫt t = 1, 2, . . . , T |ρ| &lt; 1
</p>
<p>with ǫt &sim; IID(0, σ
2
ǫ) independent of u0 &sim; (0, σ
</p>
<p>2
ǫ/τ), and τ is an arbitrary positive parameter.
</p>
<p>(a) Show that this arbitrary variance on the initial disturbance u0 renders the disturbances, in
general, heteroskedastic.
</p>
<p>(b) Show that var(ut) = σ
2
t is increasing if τ &gt; (1&minus; ρ2) and decreasing if τ &lt; (1&minus; ρ2). When is
</p>
<p>the process homoskedastic?
</p>
<p>(c) Show that cov(ut, ut&minus;s) = ρsσ2t&minus;s for t &ge; s. Hint: See the solution by Kim (1991).
(d) Consider the simple regression model
</p>
<p>yt = βxt + ut t = 1, 2 . . . , T
</p>
<p>with ut following the AR(1) process described above. Consider the common case where
ρ &gt; 0 and the xt&rsquo;s are positively autocorrelated. For this case, it is a standard result that
the var(β̂OLS) is understated under the stationary case (i.e., (1 &minus; ρ2) = τ), see problem 8.
This means that OLS rejects too often the hypothesis H0;β = 0. Show that OLS will reject
more often than the stationary case if τ &lt; 1 &minus; ρ2 and less often than the stationary case if
τ &gt; (1&minus; ρ2). Hint: See the solution by Koning (1992).
</p>
<p>12. ML Estimation of Linear Regression Model with AR(1) Errors and Two Observations. This is
based on Magee (1993). Consider the regression model yi = xiβ + ui, with only two observations
i = 1, 2, and the nonstochastic |x1| 	= |x2| are scalars. Assume that ui &sim; N(0, σ2) and u2 = ρu1+ ǫ
with |ρ| &lt; 1. Also, ǫ &sim; N [0, (1&minus; ρ2)σ2] where ǫ and u1 are independent.
</p>
<p>(a) Show that the OLS estimator of β is (x1y1 + x2y2)/(x
2
1 + x
</p>
<p>2
2).
</p>
<p>(b) Show that the ML estimator of β is (x1y1 &minus; x2y2)/(x21 &minus; x22).
(c) Show that the ML estimator of ρ is 2x1x2/(x
</p>
<p>2
1 + x
</p>
<p>2
2) and thus is nonstochastic.
</p>
<p>(d) How do the ML estimates of β and ρ behave as x1 &rarr; x2 and x1 &rarr; &minus;x2? Assume x2 	= 0.
Hint: See the solution by Baltagi and Li (1995).
</p>
<p>13. For the empirical example in section 5.5 based on the Cigarette Consumption Data in Table 3.2.
</p>
<p>(a) Replicate the OLS regression of logC on logP , logY and a constant. Plot the residuals versus
logY and verify Figure 5.1.
</p>
<p>(b) Run Glejser&rsquo;s (1969) test by regressing |ei| the absolute value of the residuals from part (a),
on (logYi)
</p>
<p>δ for δ = 1,&minus;1,&minus;0.5 and 0.5. Verify the t-statistics reported in the text.
(c) Run Goldfeld and Quandt&rsquo;s (1965) test by ordering the observations according to logYi and
</p>
<p>omitting 12 central observations. Report the two regressions based on the first and last 17
observations and verify the F -test reported in the text.
</p>
<p>(d) Verify the Spearman rank correlation test based on the rank (logYi) and rank |ei|.
(e) Verify Harvey&rsquo;s (1976) multiplicative heteroskedasticity test based on regressing loge2i on
</p>
<p>log(logYi).
</p>
<p>(f) Run the Breusch and Pagan (1979) test based on the regression of e2i /σ̂
2 on logYi, where
</p>
<p>σ̂2 =
&sum;46
</p>
<p>i=1 e
2
i /46.
</p>
<p>(g) Run White&rsquo;s (1980) test for heteroskedasticity.
</p>
<p>(h) Run the Jarque and Bera (1987) test for normality of the disturbances.
</p>
<p>(i) ComputeWhite&rsquo;s (1980) heteroskedasticity robust standard errors for the regression in part (a).</p>
<p/>
</div>
<div class="page"><p/>
<p>124 Chapter 5: Violations of the Classical Assumptions
</p>
<p>14. A Simple Linear Trend Model with AR(1) Disturbances. This is based on Krämer (1982).
</p>
<p>(a) Consider the following simple linear trend model
</p>
<p>Yt = α+ βt+ ut
</p>
<p>where ut = ρut&minus;1 + ǫt with |ρ| &lt; 1, ǫt &sim; IID(0, σ2ǫ) and var(ut) = σ2u = σ2ǫ/(1 &minus; ρ2). Our
interest is focused on the estimates of the trend coefficient, β, and the estimators to be
considered are OLS, CO (assuming that the true value of ρ is known), the first-difference
estimator (FD), and the Generalized Least Squares (GLS), which is Best Linear Unbiased
(BLUE) in this case.
</p>
<p>In the context of the simple linear trend model, the formulas for the variances of these
estimators reduce to
</p>
<p>V (OLS) = 12σ2{&minus;6ρT+1[(T &minus; 1)ρ&minus; (T + 1)]2 &minus; (T 3 &minus; T )ρ4
+2(T 2 &minus; 1)(T &minus; 3)ρ3 + 12(T 2 + 1)ρ2 &minus; 2(T 2 &minus; 1)(T + 3)ρ
+(T 3 &minus; T )}/(1&minus; ρ2)(1&minus; ρ)4(T 3 &minus; T )2
</p>
<p>V (CO) = 12σ2(1&minus; ρ)2(T 3 &minus; 3T 2 + 2T ),
V (FD) = 2σ2(1&minus; ρT&minus;1)/(1&minus; ρ2)(T &minus; 1)2,
V (GLS) = 12σ2/(T &minus; 1)[(T &minus; 3)(T &minus; 2)ρ2 &minus; 2(T &minus; 3)(T &minus; 1)ρ+ T (T + 1)].
</p>
<p>(b) Compute these variances and their relative efficiency with respect to the GLS estimator for
T = 10, 20, 30, 40 and ρ between &minus;0.9 and 0.9 in 0.1 increments.
</p>
<p>(c) For a given T , show that the limit of var(OLS)/var(CO) is zero as ρ &rarr; 1. Prove that
var(FD) and var(GLS) both tend in the limit to σ2ǫ/(T &minus; 1) &lt; &infin; as ρ &rarr; 1. Conclude
that var(GLS)/var(FD) tend to 1 as ρ &rarr; 1. Also, show that lim
</p>
<p>ρ&rarr;1
[var(GLS)/var(OLS)] =
</p>
<p>5(T 2 + T )/6(T 2 + 1) &lt; 1 provided T &gt; 3.
</p>
<p>(d) For a given ρ, show that var(FD) = O(T&minus;2) whereas the variance of the remaining estimators
is O(T&minus;3). Conclude that lim
</p>
<p>T&rarr;&infin;
[var(FD)/var(CO)] = &infin; for any given ρ.
</p>
<p>15. Consider the empirical example in section 5.6, based on the Consumption-Income data in Table 5.3.
Obtain this data set from the CONSUMP.DAT file on the Springer web site.
</p>
<p>(a) Replicate the OLS regression of Ct on Yt and a constant, and compute the Durbin-Watson
statistic. Test H0; ρ = 0 versus H1; ρ &gt; 0 at the 5% significance level.
</p>
<p>(b) Test for first-order serial correlation using the Breusch and Godfrey test.
</p>
<p>(c) Perform the two-step Cochrane-Orcutt procedure and verify Table 5.5. What happens if we
iterate the Cochrane-Orcutt procedure?
</p>
<p>(d) Perform the Prais-Winsten procedure and verify Table 5.6.
</p>
<p>(e) Compute the Newey-West heteroskedasticity and autocorrelation-consistent standard errors
for the least squares estimates in part (a).
</p>
<p>16. Benderly and Zwick (1985) considered the following equation
</p>
<p>RSt = α+ βQt+1 + γPt + ut
</p>
<p>where RSt = the real return on stocks in year t, Qt+1 = the annual rate of growth of real GNP
in year t + 1, and Pt = the rate of inflation in year t. The data is provided on the Springer web
site and labeled BENDERLY.ASC. This data covers 31 annual observations for the U.S. over the
period 1952&ndash;1982. This was obtained from Lott and Ray (1991). This equation is used to test the
significance of the inflation rate in explaining real stock returns. Use the sample period 1954&ndash;1976
to answer the following questions:</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 125
</p>
<p>(a) Run OLS to estimate the above equation. Remember to use Qt+1. Is Pt significant in this
equation? Plot the residuals against time. Compute the Newey-West heteroskedasticity and
autocorrelation-consistent standard errors for these least squares estimates.
</p>
<p>(b) Test for serial correlation using the D.W. test.
</p>
<p>(c) Would your decision in (b) change if you used the Breusch-Godfrey test for first-order serial
correlation?
</p>
<p>(d) Run the Cochrane-Orcutt procedure to correct for first-order serial correlation. Report your
estimate of ρ.
</p>
<p>(e) Run a Prais-Winsten procedure accounting for the first observation and report your estimate
of ρ. Plot the residuals against time.
</p>
<p>17. Using our cross-section Energy/GDP data set in Chapter 3, problem 3.16 consider the following
two models:
</p>
<p>Model 1: logEn = α+ βlogRGDP + u
Model 2: En = α+ βRGDP + v
</p>
<p>Make sure you have corrected the W. Germany observation on EN as described in problem 3.16
part (d).
</p>
<p>(a) Run OLS on both Models 1 and 2. Test for heteroskedasticity using the Goldfeldt/Quandt
Test. Omit c = 6 central observations. Why is heteroskedasticity a problem in Model 2, but
not Model 1?
</p>
<p>(b) For Model 2, test for heteroskedasticity using the Glejser Test.
</p>
<p>(c) Now use the Breusch-Pagan Test to test for heteroskedasticity on Model 2.
</p>
<p>(d) Apply White&rsquo;s Test to Model 2.
</p>
<p>(e) Do all these tests give the same decision?
</p>
<p>(f) Propose and estimate a simple transformation of Model 2, assuming heteroskedasticity of
the form σ2i = σ
</p>
<p>2RGDP 2.
</p>
<p>(g) Propose and estimate a simple transformation of Model 2, assuming heteroskedasticity of
the form σ2i = σ
</p>
<p>2(a+ bRGDP )2.
</p>
<p>(h) Now suppose that heteroskedasticity is of the form σ2i = σ
2RGDP γ where γ is an unknown
</p>
<p>parameter. Propose and estimate a simple transformation for Model 2. Hint: You can write
σ2i as exp{α+ γlogRGDP} where α = logσ2.
</p>
<p>(i) Compare the standard errors of the estimates for Model 2 from OLS, also obtain White&rsquo;s
heteroskedasticity-consistent standard errors. Compare them with the simple Weighted Least
Squares estimates of the standard errors in parts (f), (g) and (h). What do you conclude?
</p>
<p>18. You are given quarterly data from the first quarter of 1965 (1965.1) to the fourth quarter of 1983
(1983.4) on employment in Orange County California (EMP) and real gross national product
(RGNP). The data set is in a file called ORANGE.DAT on the Springer web site.
</p>
<p>(a) Generate the lagged variable of real GNP, call it RGNPt&minus;1 and estimate the following model
by OLS: EMPt = α+ βRGNPt&minus;1 + ut.
</p>
<p>(b) What does inspection of the residuals and the Durbin-Watson statistic suggest?
</p>
<p>(c) Assuming ut = ρut&minus;1 + ǫt where |ρ| &lt; 1 and ǫt &sim; IIN(0, σ2ǫ), use the Cochrane-Orcutt
procedure to estimate ρ, α and β. Compare the latter estimates and their standard errors
with those of OLS.
</p>
<p>(d) The Cochrane-Orcutt procedure omits the first observation. Perform the Prais-Winsten ad-
justment. Compare the resulting estimates and standard error with those in part (c).</p>
<p/>
</div>
<div class="page"><p/>
<p>126 Chapter 5: Violations of the Classical Assumptions
</p>
<p>(e) Apply the Breusch-Godfrey test for first and second order autoregression. What do you
conclude?
</p>
<p>(f) Compute the Newey-West heteroskedasticity and autocorrelation-consistent covariance stan-
dard errors for the least squares estimates in part (a).
</p>
<p>19. Consider the earning data underlying the regression in Table 4.1 and available on the Springer
web site as EARN.ASC.
</p>
<p>(a) Apply White&rsquo;s test for heteroskedasticity to the regression residuals.
</p>
<p>(b) Compute White&rsquo;s heteroskedasticity-consistent standard errors.
</p>
<p>(c) Test the least squares residuals for normality using the Jarque-Bera test.
</p>
<p>20. Hedonic Housing. Harrison and Rubinfield (1978) collected data on 506 census tracts in the Boston
area in 1970 to study hedonic housing prices and the willingness to pay for clean air. This
data is available on the Springer web site as HEDONIC.XLS. The dependent variable is the
Median Value (MV) of owner-occupied homes. The regressors include two structural variables,
RM the average number of rooms, and AGE representing the proportion of owner units built prior
to 1940. In addition there are eight neighborhood variables: B, the proportion of blacks in the
population; LSTAT, the proportion of population that is lower status; CRIM, the crime rate; ZN,
the proportion of 25000 square feet residential lots; INDUS, the proportion of nonretail business
acres; TAX, the full value property tax rate ($/$10000); PTRATIO, the pupil-teacher ratio; and
CHAS represents the dummy variable for Charles River: = 1 if a tract bounds the Charles. There
are also two accessibility variables, DIS the weighted distances to five employment centers in the
Boston region, and RAD the index of accessibility to radial highways. One more regressor is an
air pollution variable NOX, the annual average nitrogen oxide concentration in parts per hundred
million.
</p>
<p>(a) Run OLS of MV on the 13 independent variables and a constant. Plot the residuals.
</p>
<p>(b) Apply White&rsquo;s tests for heteroskedasticity.
</p>
<p>(c) Obtain the White heteroskedasticity-consistent standard errors.
</p>
<p>(d) Test the least squares residuals for normality using the Jarque-Bera test.
</p>
<p>21. Agglomeration Economies, Diseconomies, and Growth. Wheeler (2003) uses data on 3106 counties
of the contiguous USA to fit a fourth-order polynomial relating County population (employment)
growth (over the period 1980 to 1990) as a function of log(size), where size is measured as total
resident population or total civilian employment. Other control variables include the proportion
of the adult resident population (i.e. of age 25 or older) with a bachelor&rsquo;s degree or more; the
proportion of total employment in manufacturing; and the unemployment rate, all for the year
1980; Per capita income in 1979; the proportion of the resident population belonging to non-
white racial categories in 1980, and the share of local government expenditures going to each of
three public goods-education, roads and highways, police protection-in 1982. This data can be
downloaded from the JAE archive data web site.
</p>
<p>(a) Replicate the OLS regressions reported in Tables VIII and IX of Wheeler (2003, pp. 88&ndash;89).
</p>
<p>(b) Apply White&rsquo;s and Breusch-Pagan tests for heteroskedasticity.
</p>
<p>(c) Test the least squares residuals for normality using the Jarque-Bera test.
</p>
<p>References
</p>
<p>For additional readings consult the econometrics books cited in the Preface. Also the chapter on het-
</p>
<p>eroskedasticity by Griffiths (2001), and the chapter on serial correlation by King (2001):</p>
<p/>
</div>
<div class="page"><p/>
<p>References 127
</p>
<p>Ali, M.M. and C. Giaccotto (1984), &ldquo;A study of Several New and Existing Tests for Heteroskedasticity
in the General Linear Model,&rdquo; Journal of Econometrics, 26: 355&ndash;373.
</p>
<p>Amemiya, T. (1973), &ldquo;Regression Analysis When the Variance of the Dependent Variable is Proportional
to the Square of its Expectation,&rdquo; Journal of the American Statistical Association, 68: 928&ndash;934.
</p>
<p>Amemiya, T. (1977), &ldquo;A Note on a Heteroskedastic Model,&rdquo; Journal of Econometrics, 6: 365&ndash;370.
</p>
<p>Andrews, D.W.K. (1991), &ldquo;Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Esti-
mation,&rdquo; Econometrica, 59: 817&ndash;858.
</p>
<p>Baltagi, B. and Q. Li (1990), &ldquo;The Heteroskedastic Consequences of an Arbitrary Variance for the Initial
Disturbance of an AR(1) Model,&rdquo; Econometric Theory, Problem 90.3.1, 6: 405.
</p>
<p>Baltagi, B. and Q. Li (1992), &ldquo;The Bias of the Standard Errors of OLS for an AR(1) process with an
Arbitrary Variance on the Initial Observations,&rdquo; Econometric Theory, Problem 92.1.4, 8: 146.
</p>
<p>Baltagi, B. and Q. Li (1995), &ldquo;ML Estimation of Linear Regression Model with AR(1) Errors and Two
Observations,&rdquo; Econometric Theory, Solution 93.3.2, 11: 641&ndash;642.
</p>
<p>Bartlett&rsquo;s test, M.S. (1937), &ldquo;Properties of Sufficiency and Statistical Tests,&rdquo; Proceedings of the Royal
Statistical Society, A, 160: 268&ndash;282.
</p>
<p>Beach, C.M. and J.G. MacKinnon (1978), &ldquo;A Maximum Likelihood Procedure for Regression with Au-
tocorrelated Errors,&rdquo; Econometrica, 46: 51&ndash;58.
</p>
<p>Benderly, J. and B. Zwick (1985), &ldquo;Inflation, Real Balances, Output and Real Stock Returns,&rdquo; American
Economic Review, 75: 1115&ndash;1123.
</p>
<p>Breusch, T.S. (1978), &ldquo;Testing for Autocorrelation in Dynamic Linear Models,&rdquo; Australian Economic
Papers, 17: 334&ndash;355.
</p>
<p>Breusch, T.S. and A.R. Pagan (1979), &ldquo;A Simple Test for Heteroskedasticity and Random Coefficient
Variation,&rdquo; Econometrica, 47: 1287&ndash;1294.
</p>
<p>Buse, A. (1984), &ldquo;Tests for Additive Heteroskedasticity: Goldfeld and Quandt Revisited,&rdquo; Empirical
Economics, 9: 199&ndash;216.
</p>
<p>Carroll, R.H. (1982), &ldquo;Adapting for Heteroskedasticity in Linear Models,&rdquo; Annals of Statistics, 10: 1224&ndash;
1233.
</p>
<p>Cochrane, D. and G. Orcutt (1949), &ldquo;Application of Least Squares Regression to Relationships Contain-
ing Autocorrelated Error Terms,&rdquo; Journal of the American Statistical Association, 44: 32&ndash;61.
</p>
<p>Cragg, J.G. (1992), &ldquo;Quasi-Aitken Estimation for Heteroskedasticity of Unknown Form,&rdquo; Journal of
Econometrics, 54: 197&ndash;202.
</p>
<p>Durbin, J. (1960), &ldquo;Estimation of Parameters in Time-Series Regression Model,&rdquo; Journal of the Royal
Statistical Society, Series B, 22: 139&ndash;153.
</p>
<p>Durbin, J. and G. Watson (1950), &ldquo;Testing for Serial Correlation in Least Squares Regression-I,&rdquo; Bio-
metrika, 37: 409&ndash;428.
</p>
<p>Durbin, J. and G. Watson (1951), &ldquo;Testing for Serial Correlation in Least Squares Regression-II,&rdquo;
Biometrika, 38: 159&ndash;178.
</p>
<p>Evans, M.A., and M.L. King (1980) &ldquo;A Further Class of Tests for Heteroskedasticity,&rdquo; Journal of Econo-
metrics, 37: 265&ndash;276.
</p>
<p>Farebrother, R.W. (1980), &ldquo;The Durbin-Watson Test for Serial Correlation When There is No Intercept
in the Regression,&rdquo; Econometrica, 48: 1553&ndash;1563.</p>
<p/>
</div>
<div class="page"><p/>
<p>128 Chapter 5: Violations of the Classical Assumptions
</p>
<p>Glejser, H. (1969), &ldquo;A New Test for Heteroskedasticity,&rdquo; Journal of the American Statistical Association,
64: 316&ndash;323.
</p>
<p>Godfrey, L.G. (1978), &ldquo;Testing Against General Autoregressive and Moving Average Error Models When
the Regressors Include Lagged Dependent Variables,&rdquo; Econometrica, 46: 1293&ndash;1302.
</p>
<p>Goldfeld, S.M. and R.E. Quandt (1965), &ldquo;Some Tests for Homoscedasticity,&rdquo; Journal of the American
Statistical Association, 60: 539&ndash;547.
</p>
<p>Goldfeld, S.M. and R.E. Quandt (1972), Nonlinear Methods in Econometrics (North-Holland: Amster-
dam).
</p>
<p>Griffiths, W.E. (2001), &ldquo;Heteroskedasticity,&rdquo; Chapter 4 in B.H. Baltagi, (ed.), A Companion to Theo-
retical Econometrics (Blackwell: Massachusetts).
</p>
<p>Harrison, M. and B.P. McCabe (1979), &ldquo;A Test for Heteroskedasticity Based On Ordinary Least Squares
Residuals,&rdquo; Journal of the American Statistical Association, 74: 494&ndash;499.
</p>
<p>Harrison, D. and D.L. Rubinfeld (1978), &ldquo;Hedonic Housing Prices and the Demand for Clean Air,&rdquo;
Journal of Environmental Economics and Management, 5: 81&ndash;102.
</p>
<p>Harvey, A.C. (1976), &ldquo;Estimating Regression Models With Multiplicative Heteroskedasticity,&rdquo; Econo-
metrica, 44: 461&ndash;466.
</p>
<p>Hilderth, C. and J. Lu (1960), &ldquo;Demand Relations with Autocorrelated Disturbances,&rdquo; Technical Bulletin
276 (Michigan State University, Agriculture Experiment Station).
</p>
<p>Jarque, C.M. and A.K. Bera (1987), &ldquo;A Test for Normality of Observations and Regression Residuals,&rdquo;
International Statistical Review, 55: 163&ndash;177.
</p>
<p>Kim, J.H. (1991), &ldquo;The Heteroskedastic Consequences of an Arbitrary Variance for the Initial Distur-
bance of an AR(1) Model,&rdquo; Econometric Theory, Solution 90.3.1, 7: 544&ndash;545.
</p>
<p>King, M. (2001), &ldquo;Serial Correlation,&rdquo; Chapter 2 in B.H. Baltagi, (ed.), A Companion to Theoretical
Econometrics (Blackwell: Massachusetts).
</p>
<p>Koenker, R. (1981), &ldquo;A Note on Studentizing a Test for Heteroskedasticity,&rdquo; Journal of Econometrics,
17: 107&ndash;112.
</p>
<p>Koenker, R. and G.W. Bassett, Jr. (1982), &ldquo;Robust Tests for Heteroskedasticity Based on Regression
Quantiles,&rdquo; Econometrica, 50:43&ndash;61.
</p>
<p>Koning, R.H. (1992), &ldquo;The Bias of the Standard Errors of OLS for an AR(1) process with an Arbitrary
Variance on the Initial Observations,&rdquo; Econometric Theory, Solution 92.1.4, 9: 149&ndash;150.
</p>
<p>Krämer, W. (1982), &ldquo;Note on Estimating Linear Trend When Residuals are Autocorrelated,&rdquo; Economet-
rica, 50: 1065&ndash;1067.
</p>
<p>Lott, W.F. and S.C. Ray (1992), Applied Econometrics: Problems With Data Sets (The Dryden Press:
New York).
</p>
<p>Maddala, G.S. (1977), Econometrics (McGraw-Hill: New York).
</p>
<p>Maeshiro, A. (1976), &ldquo;Autoregressive Transformation, Trended Independent Variables and Autocorre-
lated Disturbance Terms,&rdquo; The Review of Economics and Statistics, 58: 497&ndash;500.
</p>
<p>Maeshiro, A. (1979), &ldquo;On the Retention of the First Observations in Serial Correlation Adjustment of
Regression Models,&rdquo; International Economic Review, 20: 259&ndash;265.
</p>
<p>Magee L. (1993), &ldquo;ML Estimation of Linear Regression Model with AR(1) Errors and Two Observations,&rdquo;
Econometric Theory, Problem 93.3.2, 9: 521&ndash;522.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 129
</p>
<p>Mizon, G.E. (1995), &ldquo;A Simple Message for Autocorrelation Correctors: Don&rsquo;t,&rdquo; Journal of Econometrics
69: 267&ndash;288.
</p>
<p>Newey, W.K. and K.D. West (1987), &ldquo;A Simple, Positive Semi-definite, Heteroskedasticity and Autocor-
relation Consistent Covariance Matrix,&rdquo; Econometrica, 55: 703&ndash;708.
</p>
<p>Oberhofer, W. and J. Kmenta (1974), &ldquo;A General Procedure for Obtaining Maximum Likelihood Esti-
mates in Generalized Regression Models,&rdquo; Econometrica, 42: 579&ndash;590.
</p>
<p>Park, R.E. and B.M. Mitchell (1980), &ldquo;Estimating the Autocorrelated Error Model With Trended Data,&rdquo;
Journal of Econometrics, 13: 185&ndash;201.
</p>
<p>Prais, S. and C. Winsten (1954), &ldquo;Trend Estimation and Serial Correlation,&rdquo; Discussion Paper 383
(Cowles Commission: Chicago).
</p>
<p>Rao, P. and Z. Griliches (1969), &ldquo;Some Small Sample Properties of Several Two-Stage Regression Meth-
ods in the Context of Autocorrelated Errors,&rdquo; Journal of the American Statistical Association, 64:
253&ndash;272.
</p>
<p>Rao, P. and R. L. Miller (1971), Applied Econometrics (Wadsworth: Belmont).
</p>
<p>Robinson, P.M. (1987), &ldquo;Asymptotically Efficient Estimation in the Presence of Heteroskedasticity of
Unknown Form,&rdquo; Econometrica, 55: 875&ndash;891.
</p>
<p>Rutemiller, H.C. and D.A. Bowers (1968), &ldquo;Estimation in a Heteroskedastic Regression Model,&rdquo; Journal
of the American Statistical Association, 63: 552&ndash;557.
</p>
<p>Savin, N.E. and K.J. White (1977), &ldquo;The Durbin-Watson Test for Serial Correlation with Extreme
Sample Sizes or Many Regressors,&rdquo; Econometrica, 45: 1989&ndash;1996.
</p>
<p>Szroeter, J. (1978), &ldquo;A Class of Parametric Tests for Heteroskedasticity in Linear Econometric Models,&rdquo;
Econometrica, 46: 1311&ndash;1327.
</p>
<p>Theil, H. (1978), Introduction to Econometrics (Prentice-Hall: Englewood Cliffs, NJ).
</p>
<p>Waldman, D.M. (1983), &ldquo;A Note on Algebraic Equivalence of White&rsquo;s Test and a Variation of the
Godfrey/Breusch-Pagan Test for Heteroskedasticity,&rdquo; Economics Letters, 13: 197&ndash;200.
</p>
<p>Wheeler, C. (2003), &ldquo;Evidence on Agglomeration Economies, Diseconomies, and Growth,&rdquo; Journal of
Applied Econometrics, 18: 79&ndash;104.
</p>
<p>White, H. (1980), &ldquo;A Heteroskedasticity Consistent Covariance Matrix Estimator and a Direct Test for
Heteroskedasticity,&rdquo; Econometrica, 48: 817&ndash;838.
</p>
<p>Wooldridge, J.M. (1991), &ldquo;On the Application of Robust, Regression-Based Diagnostics to Models of
</p>
<p>Conditional Means and Conditional Variances,&rdquo; Journal of Econometrics, 47: 5&ndash;46.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 6
</p>
<p>Distributed Lags and Dynamic Models
</p>
<p>6.1 Introduction
</p>
<p>Many economic models have lagged values of the regressors in the regression equation. For
example, it takes time to build roads and highways. Therefore, the effect of this public investment
on growth in GNP will show up with a lag, and this effect will probably linger on for several
years. It takes time before investment in research and development pays off in new inventions
which in turn take time to develop into commercial products. In studying consumption behavior,
a change in income may affect consumption over several periods. This is true in the permanent
income theory of consumption, where it may take the consumer several periods to determine
whether the change in real disposable income was temporary or permanent. For example, is
the extra consulting money earned this year going to continue next year? Also, lagged values
of real disposable income appear in the regression equation because the consumer takes into
account his life time earnings in trying to smooth out his consumption behavior. In turn, one&rsquo;s
life time income may be guessed by looking at past as well as current earnings. In other words,
the regression relationship would look like
</p>
<p>Yt = α+ β0Xt + β1Xt&minus;1 + ..+ βsXt&minus;s + ut t = 1, 2, . . . , T (6.1)
</p>
<p>where Yt denotes the t-th observation on the dependent variable Y and Xt&minus;s denotes the (t-s)th
observation on the independent variable X. α is the intercept and β0, β1, . . . , βs are the current
and lagged coefficients of Xt. Equation (6.1) is known as a distributed lag since it distributes the
effect of an increase in income on consumption over s periods. Note that the short-run effect of
a unit change in X on Y is given by βo, while the long-run effect of a unit change in X on Y
is (β0 + β1 + ..+ βs).
Suppose that you observeXt from 1959 to 2007. Xt&minus;1 is the same variable but for the previous
</p>
<p>period, i.e., 1958&ndash;2006. Since 1958 is not available in this data, the software you are using will
start from 1959 for Xt&minus;1, and end at 2006. This means that when we lag once, the current
Xt series will have to start at 1960 and end at 2007. For practical purposes, this means that
when we lag once we loose one observation from the sample. So if we lag s periods, we loose
s observations. Furthermore, we are estimating one extra β with every lag. Therefore, there
is double jeopardy with respect to loss of degrees of freedom. The number of observations fall
(because we are lagging the same series), and the number of parameters to be estimated increase
with every lagging variable introduced. Besides the loss of degrees of freedom, the regressors
in (6.1) are likely to be highly correlated with each other. In fact most economic time series
are usually trended and very highly correlated with their lagged values. This introduces the
problem of among the regressors and as we saw in Chapter 4, the higher the multicollinearity
among these regressors, the lower is the reliability of the regression estimates.
In this model, OLS is still BLUE because the classical assumptions are still satisfied. All we
</p>
<p>have done in (6.1) is introduce the additional regressors (Xt&minus;1, . . . , Xt&minus;s). These regressors are
uncorrelated with the disturbances since they are lagged values of Xt, which are by assumption
not correlated with ut for every t.
</p>
<p>131
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_6, </p>
<p/>
</div>
<div class="page"><p/>
<p>132 Chapter 6: Distributed Lags and Dynamic Models
</p>
<p>�
�
</p>
<p>�)1( �s
</p>
<p>s )1( �s
�
</p>
<p>�
</p>
<p>0 1     2   3 
</p>
<p>Figure 6.1 Linear Distributed Lag
</p>
<p>In order to reduce the degrees of freedom problem, one could impose more structure on the β&rsquo;s.
One of the simplest forms imposed on these coefficients is the linear arithmetic lag, (see Figure
6.1), which can be written as
</p>
<p>βi = [(s+ 1)&minus; i]β for i = 0, 1, . . . , s (6.2)
</p>
<p>The lagged coefficients of X follow a linear distributed lag declining arithmetically from (s+1)β
for Xt to β for Xt&minus;s. Substituting (6.2) in (6.1) one gets
</p>
<p>Yt = α+
&sum;s
</p>
<p>i=0 βiXt&minus;i + ut = α+ β
&sum;s
</p>
<p>i=0[(s+ 1)&minus; i]Xt&minus;i + ut (6.3)
</p>
<p>where the latter equation can be estimated by the regression of Yt on a constant and Zt, where
</p>
<p>Zt =
&sum;s
</p>
<p>i=0[(s+ 1)&minus; i]Xt&minus;i
</p>
<p>This Zt can be calculated given s andXt. Hence, we have reduced the estimation of β0, β1, . . . , βs
into the estimation of just one β. Once β̂ is obtained, β̂i can be deduced from (6.2), for i =
0, 1, . . . , s. Despite its simplicity, this lag is too restrictive to impose on the regression and is
not usually used in practice.
Alternatively, one can think of βi = f(i) for i = 0, 1, . . . , s. If f(i) is a continuous function,
</p>
<p>over a closed interval, then it can be approximated by an r-th degree polynomial,
</p>
<p>f(i) = a0 + a1i+ . . .+ ari
r
</p>
<p>For example, if r = 2, then
</p>
<p>βi = a0 + a1i+ a2i
2 for i = 0, 1, 2, . . . , s</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Introduction 133
</p>
<p>so that
</p>
<p>β0 = a0
</p>
<p>β1 = a0 + a1 + a2
</p>
<p>β2 = a0 + 2a1 + 4a2
...
</p>
<p>...
...
</p>
<p>βs = a0 + sa1 + s
2a2
</p>
<p>Once a0, a1, and a2 are estimated, β0, β1, . . . , βs can be deduced. In fact, substituting βi =
a0 + a1i+ a2i
</p>
<p>2 in (6.1) we get
</p>
<p>Yt = α+
&sum;s
</p>
<p>i=0(a0 + a1i+ a2i
2)Xt&minus;i + ut (6.4)
</p>
<p>= α+ a0
&sum;s
</p>
<p>i=0Xt&minus;i + a1
&sum;s
</p>
<p>i=0 iXt&minus;i + a2
&sum;s
</p>
<p>i=0 i
2Xt&minus;i + ut
</p>
<p>This last equation, shows that α, a0, a1 and a2 can be estimated from the regression of Yt
on a constant, Z0 =
</p>
<p>&sum;s
i=0Xt&minus;i, Z1 =
</p>
<p>&sum;s
i=0 iXt&minus;i and Z2 =
</p>
<p>&sum;s
i=0 i
</p>
<p>2Xt&minus;i. This procedure was
proposed by Almon (1965) and is known as the Almon lag . One of the problems with this
procedure is the choice of s and r, the number of lags on Xt, and the degree of the polynomial,
respectively. In practice, neither is known. Davidson and MacKinnon (1993) suggest starting
with a maximum reasonable lag s&lowast; that is consistent with the theory and then based on the
unrestricted regression, given in (6.1), checking whether the fit of the model deteriorates as s&lowast;
</p>
<p>is reduced. Some criteria suggested for this choice include: (i) maximizing R̄2; (ii) minimizing
Akaike&rsquo;s (1973) Information Criterion (AIC) with respect to s. This is given by AIC(s) =
(RSS/T )e2s/T ; or (iii) minimizing Schwarz (1978) Bayesian Information Criterion (BIC) with
respect to s. This is given by BIC(s) = (RSS/T )T s/T where RSS denotes the residual sum
of squares. Note that the AIC and BIC criteria, like R̄2, reward good fit but penalize loss
of degrees of freedom associated with a high value of s. These criteria are printed by most
regression software including SHAZAM, EViews and SAS. Once the lag length s is chosen it is
straight forward to determine r, the degree of the polynomial. Start with a high value of r and
construct the Z variables as described in (6.4). If r = 4 is the highest degree polynomial chosen
and a4, the coefficient of Z4 =
</p>
<p>&sum;s
i=0 i
</p>
<p>4Xt&minus;4 is insignificant, drop Z4 and run the regression for
r = 3. Stop, if the coefficient of Z3 is significant, otherwise drop Z3 and run the regression for
r = 2.
Applied researchers usually impose end point constraints on this Almon lag. A near end
</p>
<p>point constraint means that β&minus;1 = 0 in equation (6.1). This means that for equation (6.4),
this constraint yields the following restriction on the second degree polynomial in a&rsquo;s: β&minus;1 =
f(&minus;1) = a0 &minus; a1 + a2 = 0. This restriction allows us to solve for a0 given a1 and a2. In fact,
substituting a0 = a1 &minus; a2 into (6.4), the regression becomes
</p>
<p>Yt = α+ a1(Z1 + Z0) + a2(Z2 &minus; Z0) + ut (6.5)
</p>
<p>and once a1 and a2 are estimated, a0 is deduced, and hence the βi&rsquo;s. This restriction essentially
states that Xt+1 has no effect on Yt. This may not be a plausible assumption, especially in our
consumption example, where income next year enters the calculation of permanent income or
life time earnings. A more plausible assumption is the far end point constraint, where βs+1 = 0.
This means that Xt&minus;(s+1) does not affect Yt. The further you go back in time, the less is the
effect on the current period. All we have to be sure of is that we have gone far back enough</p>
<p/>
</div>
<div class="page"><p/>
<p>134 Chapter 6: Distributed Lags and Dynamic Models
</p>
<p>���
</p>
<p>�� ��
</p>
<p>��
</p>
<p>��
 &ndash;1 0 1 2 3 )1( ��
</p>
<p>�
�
</p>
<p>�
�
</p>
<p>Figure 6.2 A Polynomial Lag with End Point Constraints
</p>
<p>to reach an insignificant effect. This far end point constraint is imposed by removing Xt&minus;(s+1)
from the equation as we have done above. But, some researchers impose this restriction on
βi = f(i), i.e., by restricting βs+1 = f(s+1) = 0. This yields for r = 2 the following constraint:
a0+(s+1)a1+(s+1)
</p>
<p>2a2 = 0. Solving for a0 and substituting in (6.4), the constrained regression
becomes
</p>
<p>Yt = α+ a1[Z1 &minus; (s+ 1)Z0] + a2[Z2 &minus; (s+ 1)2Z0] + ut (6.6)
</p>
<p>One can also impose both end point constraints and reduce the regression into the estimation
of one a rather than three a&rsquo;s. Note that β&minus;1 = βs+1 = 0 can be imposed by not including
Xt+1 and Xt&minus;(s+1) in the regression relationship. However, these end point restrictions impose
the additional restrictions that the polynomial on which the a&rsquo;s lie should pass through zero at
i = &minus;1 and i = (s+ 1), see Figure 6.2.
These additional restrictions on the polynomial may not necessarily be true. In other words,
</p>
<p>the polynomial could intersect the X-axis at points other than &minus;1 or (s + 1). Imposing a
restriction, whether true or not, reduces the variance of the estimates, and introduces bias if the
restriction is untrue. This is intuitive, because this restriction gives additional information which
should increase the reliability of the estimates. The reduction in variance and the introduction of
bias naturally lead to Mean Square Error criteria that help determine whether these restrictions
should be imposed, see Wallace (1972). These criteria are beyond the scope of this chapter. In
general, one should be careful in the use of restrictions that may not be plausible or even valid. In
fact, one should always test these restrictions before using them. See Schmidt and Waud (1975).
</p>
<p>Empirical Example: Using the Consumption-Income data from the Economic Report of the
President over the period 1959&ndash;2007, given in Table 5.3, we estimate a consumption-income
regression imposing a five year lag on income. In this case, all variables are in log and s = 5
in equation (6.1). Table 6.1 gives the Stata output imposing the linear arithmetic lag given in
equation (6.2).
</p>
<p>The regression output reports β̂ = 0.0498 which is statistically significant with a t-value
of 64.4. One can test the arithmetic lag restrictions jointly using an F -test. The Unrestricted</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Introduction 135
</p>
<p>Table 6.1 Regression with Arithmetic Lag Restriction
</p>
<p>. tsset year
time variable: year, 1959 to 2007
</p>
<p>delta: 1 unit
. gen ly=ln(y)
. gen lc=ln(c)
. gen z=6*ly+5*l.ly+4*l2.ly+3*l3.ly+2*l4.ly+l5.ly
(5 missing values generated)
. reg lc z
</p>
<p>Source SS df MS Number of obs = 44
F(1, 42) = 4149.96
</p>
<p>Model 3.5705689 1 3.5705689 Prob &gt; F = 0.0000
Residual .036136249 42 .000860387 R-squared = 0.9900
</p>
<p>Adj R-squared = 0.9897
Total 3.60670515 43 .083876864 Root MSE = .02933
</p>
<p>lc Coef. Std. Err. t P &gt; |t| [95% Conf. Interval]
</p>
<p>Z .049768 .0007726 64.42 0.000 .0482089 .0513271
cons &ndash;.5086255 .1591019 &ndash;3.20 0.003 &ndash;.8297061 &ndash;.1875449
</p>
<p>Table 6.2 Almon Polynomial, r = 2, s = 5 and Near End-Point Constraint
</p>
<p>Dependent Variable = LNC
Method: Least Squares
Sample (adjusted): 1964 2007
Included observations: 44 after adjustments
</p>
<p>Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;0.770611 0.201648 3.821563 0.0004
PDL01 0.342152 0.056727 6.031589 0.0000
PDL02 0.067215 0.012960 &ndash;5.186494 0.0000
</p>
<p>R-squared 0.990054 Mean dependent var 9.736786
Adjusted R-squared 0.989568 S.D. dependent var 0.289615
S.E. of regression 0.029580 Akaike info criterion &ndash;4.137705
Sum squared resid 0.035874 Schwarz criterion &ndash;4.016055
Log likelihood 94.02950 Hannan-Quinn criter. &ndash;4.092591
F-statistic 2040.559 Durbin-Watson stat 0.382851
Prob(F-statistic) 0.000000
</p>
<p>Lag Distribution of LNY i Coefficient Std. Error t-Statistic
</p>
<p>. ⋆ | 0 0.27494 0.04377 6.28161
</p>
<p>. ⋆| 1 0.41544 0.06162 6.74167
</p>
<p>. ⋆| 2 0.42152 0.05358 7.86768
</p>
<p>. ⋆ | 3 0.29317 0.01976 14.8332
⋆ | 4 0.03039 0.04056 0.74937
</p>
<p>⋆ . | 5 0.36682 0.12630 2.90445
Sum of Lags 1.06865 0.01976 54.0919</p>
<p/>
</div>
<div class="page"><p/>
<p>136 Chapter 6: Distributed Lags and Dynamic Models
</p>
<p>Residual Sum of Squares (URSS) is obtained by regressing Ct on Yt, Yt&minus;1, . . . , Yt&minus;5 and a con-
stant. This yields URSS = 0.016924. The RRSS is given in Table 6.1 as 0.036136 and it involves
imposing 5 restrictions given in (6.2). Therefore,
</p>
<p>F =
(0.036136249&minus; 0.016924337)/5
</p>
<p>0.016924337/37
= 8.40
</p>
<p>and this is distributed as F5,37 under the null hypothesis. This rejects the linear arithmetic lag
restrictions.
Next we impose an Almon lag based on a second degree polynomial as described in equation
</p>
<p>(6.4). Table 6.2 reports the EViews output for s = 5 imposing the near end point constraint.
To do this using EViews, one replaces the regressor Y by PDL(Y, 5, 2, 1) indicating a request
to fit a five year Almon lag on Y that is of the second-order degree, with a near end point
constraint. In this case, the estimated regression coefficients rise and then fall becoming negative:
β̂0 = 0.275, β̂1 = 0.415, . . . , β̂5 = &minus;0.367. Note that β̂4 is statistically insignificant. The Almon
lag restrictions can be jointly tested using Chow&rsquo;s F -statistic. The URSS is obtained from the
unrestricted regression of Ct on Yt, Yt&minus;1, . . . , Yt&minus;5 and a constant. This was reported above as
URSS = 0.016924.
</p>
<p>Table 6.3 Almon Polynomial, r = 2, s = 5 and Far End-Point Constraint
</p>
<p>Dependent Variable = LNC
</p>
<p>Method: Least Squares
Sample (adjusted): 1964 2007
Included observations: 44 after adjustments
</p>
<p>Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;1.107052 0.158405 &ndash;6.988756 0.0000
PDL01 0.134206 0.011488 11.68247 0.0000
PDL02 &ndash;0.259490 0.036378 &ndash;7.133152 0.0000
</p>
<p>R-squared 0.994467 Mean dependent var 9.736786
Adjusted R-squared 0.994197 S.D. dependent var 0.289615
S.E. of regression 0.022062 Akaike info criterion &ndash;4.724198
Sum squared resid 0.019956 Schwarz criterion &ndash;4.602548
Log likelihood 106.9324 Hannan-Quinn criter. &ndash;4.679084
F-statistic 3684.610 Durbin-Watson stat 0.337777
Prob(F-statistic) 0.000000
</p>
<p>Lag Distribution of LNY i Coefficient Std. Error t-Statistic
</p>
<p>. ⋆| 0 0.87912 0.10074 8.72642
</p>
<p>. ⋆ | 1 0.45018 0.03504 12.8475
</p>
<p>. ⋆ | 2 0.13421 0.01149 11.6825
⋆. | 3 &ndash;0.06880 0.03787 &ndash;1.81686
</p>
<p>⋆ . | 4 &ndash;0.15884 0.04483 &ndash;3.54337
⋆ . | 5 &ndash;0.13590 0.03221 &ndash;4.21962
</p>
<p>Sum of Lags 1.09997 0.01547 71.0964</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Infinite Distributed Lag 137
</p>
<p>The RRSS, given in Table 6.2, is 0.035874 and involves four restrictions. Therefore,
</p>
<p>F =
(0.03587367&minus; 0.016924337)/4
</p>
<p>0.016924337/37
= 10.357
</p>
<p>and this is distributed as F4,37 under the null hypothesis. This rejects the second degree poly-
nomial Almon lag specification with a near end point constraint.
Table 6.3 reports the EViews output for s = 5, imposing the far end point constraint. To do
</p>
<p>this using EViews, one replaces the regressor Y by PDL(Y, 5, 2, 2) indicating a request to fit a five
year Almon lag on Y that is of the second-order degree, with a far end point constraint. In this
case, the β̂&rsquo;s are positive, then becoming negative, β̂0 = 0.879, β̂1 = 0.450, . . . , β̂5 = &minus;0.136, all
being statistically significant. This second degree polynomial Almon lag specification with a far
end point constraint can be tested against the unrestricted lag model using Chow&rsquo;s F -statistic.
The RRSS, given in Table 6.3, is 0.019955101 and involves four restrictions. Therefore,
</p>
<p>F =
(0.019955101&minus; 0.016924337)/4
</p>
<p>0.016924337/37
= 1.656
</p>
<p>and this is distributed as F4,37 under the null hypothesis. This does not reject the restrictions
imposed by this model.
</p>
<p>6.2 Infinite Distributed Lag
</p>
<p>So far we have been dealing with a finite number of lags imposed on Xt. Some lags may be
infinite. For example, the investment in building highways and roads several decades ago may
still have an effect on today&rsquo;s growth in GNP. In this case, we write equation (6.1) as
</p>
<p>Yt = α+
&sum;&infin;
</p>
<p>i=0 βiXt&minus;i + ut t = 1, 2, . . . , T. (6.7)
</p>
<p>There are an infinite number of βi&rsquo;s to estimate with only T observations. This can only be
feasible if more structure is imposed on the βi&rsquo;s. First, we normalize these βi&rsquo;s by their sum,
i.e., let wi = βi/β where β =
</p>
<p>&sum;&infin;
i=0 βi. If all the βi&rsquo;s have the same sign, then the βi&rsquo;s take
</p>
<p>the sign of β and 0 &le; wi &le; 1 for all i, with
&sum;&infin;
</p>
<p>i=0wi = 1. This means that the wi&rsquo;s can be
interpreted as probabilities. In fact, Koyck (1954) imposed the geometric lag on the wi&rsquo;s, i.e.,
wi = (1&minus; λ)λi for i = 0, 1, . . . ,&infin;1. Substituting
</p>
<p>βi = βwi = β(1&minus; λ)λi
</p>
<p>in (6.7) we get
</p>
<p>Yt = α+ β(1&minus; λ)
&sum;&infin;
</p>
<p>i=0 λ
iXt&minus;i + ut (6.8)
</p>
<p>Equation (6.8) is known as the infinite distributed lag form of the Koyck lag. The short-run
effect of a unit change in Xt on Yt is given by β(1 &minus; λ); whereas the long-run effect of a unit
change in Xt on Yt is
</p>
<p>&sum;&infin;
i=0 βi = β
</p>
<p>&sum;&infin;
i=0wi = β. Implicit in the Koyck lag structure is that
</p>
<p>the effect of a unit change in Xt on Yt declines the further back we go in time. For example, if
λ = 1/2, then β0 = β/2, β1 = β/4, β2 = β/8, etc. Defining LXt = Xt&minus;1, as the lag operator,
we have LiXt = Xt&minus;i, and (6.8) reduces to
</p>
<p>Yt = α+ β(1&minus; λ)
&sum;&infin;
</p>
<p>i=0(λL)
iXt + ut = α+ β(1&minus; λ)Xt/(1&minus; λL) + ut (6.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>138 Chapter 6: Distributed Lags and Dynamic Models
</p>
<p>where we have used the fact that
&sum;&infin;
</p>
<p>i=0 c
i = 1/(1&minus; c). Multiplying the last equation by (1&minus;λL)
</p>
<p>one gets
</p>
<p>Yt &minus; λYt&minus;1 = α(1&minus; λ) + β(1&minus; λ)Xt + ut &minus; λut&minus;1
</p>
<p>or
</p>
<p>Yt = λYt&minus;1 + α(1&minus; λ) + β(1&minus; λ)Xt + ut &minus; λut&minus;1 (6.10)
</p>
<p>This is the autoregressive form of the infinite distributed lag. It is autoregressive because it
includes the lagged value of Yt as an explanatory variable. Note that we have reduced the
problem of estimating an infinite number of βi&rsquo;s into estimating λ and β from (6.10). However,
OLS would lead to biased and inconsistent estimates, because (6.10) contains a lagged dependent
variable as well as serially correlated errors. In fact the error in (6.10) is a Moving Average
process of order one, i.e., MA(1), see Chapter 14. We digress at this stage to give two econometric
models which would lead to equations resembling (6.10).
</p>
<p>6.2.1 Adaptive Expectations Model (AEM)
</p>
<p>Suppose that output Yt is a function of expected sales X
&lowast;
t and that the latter is unobservable,
</p>
<p>i.e.,
</p>
<p>Yt = α+ βX
&lowast;
t + ut
</p>
<p>where expected sales are updated according to the following method
</p>
<p>X&lowast;t &minus;X&lowast;t&minus;1 = δ(Xt &minus;X&lowast;t&minus;1) (6.11)
</p>
<p>that is, expected sales at time t is a weighted combination of expected sales at time t&minus; 1 and
actual sales at time t. In fact,
</p>
<p>X&lowast;t = δXt + (1&minus; δ)X&lowast;t&minus;1 (6.12)
</p>
<p>Equation (6.11) is also an error learning model, where one learns from past experience and adjust
expectations after observing current sales. Using the lag operator L, (6.12) can be rewritten as
X&lowast;t = δXt/[1&minus; (1&minus; δ)L]. Substituting this last expression in the above relationship, we get
</p>
<p>Yt = α+ βδXt/[1&minus; (1&minus; δ)L] + ut (6.13)
</p>
<p>Multiplying both sides of (6.13) by [1&minus; (1&minus; δ)L], we get
</p>
<p>Yt &minus; (1&minus; δ)Yt&minus;1 = α[(1&minus; (1&minus; δ)] + βδXt + ut &minus; (1&minus; δ)ut&minus;1 (6.14)
</p>
<p>(6.14) looks exactly like (6.10) with λ = (1&minus; δ).
</p>
<p>6.2.2 Partial Adjustment Model (PAM)
</p>
<p>Under this model there is a cost of being out of equilibrium and a cost of adjusting to that
equilibrium, i.e.,
</p>
<p>Cost = a(Yt &minus; Y &lowast;t )2 + b(Yt &minus; Yt&minus;1)2 (6.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Estimation and Testing of Dynamic Models with Serial Correlation 139
</p>
<p>where Y &lowast;t is the target or equilibrium level for Y , whereas Yt is the current level of Y . The
first term of (6.15) gives a quadratic loss function proportional to the distance of Yt from the
equilibrium level Y &lowast;t . The second quadratic term represents the cost of adjustment. Minimizing
this quadratic cost function with respect to Y , we get Yt = γY
</p>
<p>&lowast;
t +(1&minus;γ)Yt&minus;1, where γ = a/(a+b).
</p>
<p>Note that if the cost of adjustment was zero, then b = 0, γ = 1, and the target is reached
immediately. However, there are costs of adjustment, especially in building the desired capital
stock. Hence,
</p>
<p>Yt = γY
&lowast;
t + (1&minus; γ)Yt&minus;1 + ut (6.16)
</p>
<p>where we made this relationship stochastic. If the true relationship is Y &lowast;t = α+ βXt, then from
(6.16)
</p>
<p>Yt = γα+ γβXt + (1&minus; γ)Yt&minus;1 + ut (6.17)
</p>
<p>and this looks like (6.10) with λ = (1 &minus; γ), except for the error term, which is not necessarily
MA(1) with the Moving Average parameter λ.
</p>
<p>6.3 Estimation and Testing of Dynamic Models with
Serial Correlation
</p>
<p>Both the AEM and the PAM give equations resembling the autoregressive form of the infinite
distributed lag. In all cases, we end up with a lagged dependent variable and an error term
that is either Moving Average of order one as in (6.10), or just classical or autoregressive as in
(6.17). In this section we study the testing and estimation of such autoregressive or dynamic
models.
If there is a Yt&minus;1 in the regression equation and the ut&rsquo;s are classical disturbances, as may
</p>
<p>be the case in equation (6.17), then Yt&minus;1 is said to be contemporaneously uncorrelated with
the disturbance term ut. In fact, the disturbances satisfy assumptions 1&ndash;4 of Chapter 3 and
E(Yt&minus;1ut) = 0 even though E(Yt&minus;1ut&minus;1) 	= 0. In other words, Yt&minus;1 is not correlated with the
current disturbance ut but it is correlated with the lagged disturbance ut&minus;1. In this case, as
long as the disturbances are not serially correlated, OLS will be biased, but remains consistent
and asymptotically efficient. This case is unlikely with economic data given that most macro
time-series variables are highly trended. More likely, the ut&rsquo;s are serially correlated. In this case,
OLS is biased and inconsistent. Intuitively, Yt is related to ut, so Yt&minus;1 is related to ut&minus;1. If ut
and ut&minus;1 are correlated, then Yt&minus;1 and ut are correlated. This means that one of the regressors,
lagged Y , is correlated with ut and we have the problem of endogeneity. Let us demonstrate
what happens to OLS for the simple autoregressive model with no constant
</p>
<p>Yt = βYt&minus;1 + νt |β| &lt; 1 t = 1, 2, . . . , T (6.18)
</p>
<p>with νt = ρνt&minus;1 + ǫt, |ρ| &lt; 1 and ǫt &sim; IIN(0, σ2ǫ ). One can show, see problem 3, that
</p>
<p>β̂OLS =
&sum;T
</p>
<p>t=2 YtYt&minus;1/
&sum;T
</p>
<p>t=2 Y
2
t&minus;1 = β +
</p>
<p>&sum;T
t=2 Yt&minus;1νt/
</p>
<p>&sum;T
t=2 Y
</p>
<p>2
t&minus;1
</p>
<p>with plim(β̂OLS&minus;β) = asymp. bias(β̂OLS) = ρ(1&minus;β2)/(1+ρβ). This asymptotic bias is positive
if ρ &gt; 0 and negative if ρ &lt; 0. Also, this asymptotic bias can be large for small values of β and</p>
<p/>
</div>
<div class="page"><p/>
<p>140 Chapter 6: Distributed Lags and Dynamic Models
</p>
<p>large values of ρ. For example, if ρ = 0.9 and β = 0.2, the asymptotic bias for β is 0.73. This is
more than 3 times the value of β.
</p>
<p>Also, ρ̂ =
&sum;T
</p>
<p>t=2 ν̂tν̂t&minus;1/
&sum;T
</p>
<p>t=2 ν̂
2
t&minus;1 where ν̂t = Yt &minus; β̂OLSYt&minus;1 has
</p>
<p>plim(ρ̂&minus; ρ) = &minus;ρ(1&minus; β2)/(1 + ρβ) = &minus; asymp.bias(β̂OLS)
</p>
<p>This means that if ρ &gt; 0, then ρ̂ would be negatively biased. However, if ρ &lt; 0, then ρ̂ is
positively biased. In both cases, ρ̂ is biased towards zero. In fact, the asymptotic bias of the
D.W. statistic is twice the asymptotic bias of β̂OLS , see problem 3. This means that the D.W.
statistic is biased towards not rejecting the null hypothesis of zero serial correlation. Therefore,
if the D.W. statistic rejects the null of ρ = 0, it is doing that when the odds are against it,
and therefore confirming our rejection of the null and the presence of serial correlation. If on
the other hand it does not reject the null, then the D.W. statistic is uninformative and has
to be replaced by another conclusive test for serial correlation. Such an alternative test in the
presence of a lagged dependent variable has been developed by Durbin (1970), and the statistic
computed is called Durbin&rsquo;s h. Using (6.10) or (6.17), one computes OLS ignoring its possible
bias and ρ̂ from OLS residuals as shown above. Durbin&rsquo;s h is given by
</p>
<p>h = ρ̂[n/(1&minus; n v̂ar(coeff. of Yt&minus;1))]1/2. (6.19)
</p>
<p>This is asymptotically distributed N(0, 1) under null hypothesis of ρ = 0. If n[v̂ar(coeff. of
Yt&minus;1)] is greater than one, then h cannot be computed, and Durbin suggests running the OLS
residuals et on et&minus;1 and the regressors in the model (including the lagged dependent variable),
and testing whether the coefficient of et&minus;1 in this regression is significant. In fact, this test can
be generalized to higher order autoregressive errors. Let ut follow an AR(p) process
</p>
<p>ut = ρ1ut&minus;1 + ρ2ut&minus;2 + ..+ ρput&minus;p + ǫt
</p>
<p>then this test involves running et on et&minus;1, et&minus;2, . . . , et&minus;p and the regressors in the model including
Yt&minus;1. The test statistic for H0; ρ1 = ρ2 = .. = ρp = 0; is TR
</p>
<p>2 which is distributed χ2p. This is the
Lagrange multiplier test developed independently by Breusch (1978) and Godfrey (1978) and
discussed in Chapter 5. In fact, this test has other useful properties. For example, this test is the
same whether the null imposes an AR(p) model or an MA(p) model on the disturbances, see
Chapter 14. Kiviet (1986) argues that even though these are large sample tests, the Breusch-
Godfrey test is preferable to Durbin&rsquo;s h in small samples.
</p>
<p>6.3.1 A Lagged Dependent Variable Model with AR(1) Disturbances
</p>
<p>A model with a lagged dependent variable and an autoregressive error term is estimated using
instrumental variables (IV). This method will be studied extensively in Chapter 11. In short, the
IV method corrects for the correlation between Yt&minus;1 and the error term by replacing Yt&minus;1 with
its predicted value Ŷt&minus;1. The latter is obtained by regressing Yt&minus;1 on some exogenous variables,
say a set of Z&rsquo;s, which are called a set of instruments for Yt&minus;1. Since these variables are exogenous
and uncorrelated with ut, Ŷt&minus;1 will not be correlated with ut. Suppose the regression equation
is
</p>
<p>Yt = α+ βYt&minus;1 + γXt + ut t = 2, . . . , T (6.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Estimation and Testing of Dynamic Models with Serial Correlation 141
</p>
<p>and that at least one exogenous variable Zt exists which will be our instrument for Yt&minus;1. Re-
gressing Yt&minus;1 on Xt, Zt and a constant, we get
</p>
<p>Yt&minus;1 = Ŷt&minus;1 + ν̂t = â1 + â2Zt + â3Xt + ν̂t. (6.21)
</p>
<p>Then Ŷt&minus;1 = â1 + â2Zt + â3Xt and is independent of ut, because it is a linear combination of
exogenous variables. But, Yt&minus;1 is correlated with ut. This means that ν̂t is the part of Yt&minus;1 that
is correlated with ut. Substituting Yt&minus;1 = Ŷt&minus;1 + ν̂t in (6.20) we get
</p>
<p>Yt = α+ βŶt&minus;1 + γXt + (ut + βν̂t) (6.22)
</p>
<p>Ŷt&minus;1 is uncorrelated with the new error term (ut+ βν̂t) because ΣŶt&minus;1ν̂t = 0 from (6.21). Also,
Xt is uncorrelated with ut by assumption. But, from (6.21), Xt also satisfies ΣXtν̂t = 0. Hence,
Xt is uncorrelated with the new error term (ut + βν̂t). This means that OLS applied to (6.22)
will lead to consistent estimates of α, β and γ. The only remaining question is where do we find
instruments like Zt? This Zt should be (i) uncorrelated with ut, (ii) preferably predicting Yt&minus;1
fairly well, but, not predicting it perfectly, otherwise Ŷt&minus;1 = Yt&minus;1. If this happens, we are back to
OLS which we know is inconsistent, (iii) Σz2t /T should be finite and different from zero. Recall
that zt = Zt &minus; Z̄. In this case, Xt&minus;1 seems like a natural instrumental variable candidate. It is
an exogenous variable which would very likely predict Yt&minus;1 fairly well, and satisfies Σx2t&minus;1/T
being finite and different from zero. In other words, (6.21) regresses Yt&minus;1 on a constant, Xt&minus;1
and Xt, and gets Ŷt&minus;1. Additional lags on Xt can be used as instruments to improve the small
sample properties of this estimator. Substituting Ŷt&minus;1 in equation (6.22) results in consistent
estimates of the regression parameters. Wallis (1967) substituted these consistent estimates in
the original equation (6.20) and obtained the residuals ũt. Then he computed
</p>
<p>ρ̂ = [
&sum;T
</p>
<p>t=2 ũtũt&minus;1/(T &minus; 1)]/[
&sum;T
</p>
<p>t=1 ũ
2
t /T ] + (3/T )
</p>
<p>where the last term corrects for the bias in ρ̂. At this stage, one can perform a Prais-Winsten
procedure on (6.20) using ρ̂ instead of ρ, see Fomby and Guilkey (1983).
An alternative two-step procedure has been proposed by Hatanaka (1974). After estimating
</p>
<p>(6.22) and obtaining the residuals ũt from (6.20), Hatanaka (1974) suggests running Y
&lowast;
t = Yt &minus;
</p>
<p>ρ̃Yt&minus;1 on Y &lowast;t&minus;1 = Yt&minus;1&minus; ρ̃Yt&minus;2, X&lowast;t = Xt&minus; ρ̃Xt&minus;1 and ũt&minus;1. Note that this is the Cochrane-Orcutt
transformation which ignores the first observation. Also, ρ̃ =
</p>
<p>&sum;T
t=3 ũtũt&minus;1/
</p>
<p>&sum;T
t=3 ũ
</p>
<p>2
t ignores the
</p>
<p>small sample bias correction factor suggested by Wallis (1967). Let δ̃ be the coefficient of ũt&minus;1,
then the efficient estimator of ρ is given by ˜̃ρ = ρ̃ + δ̃. Hatanaka shows that the resulting
estimators are asymptotically equivalent to the MLE in the presence of Normality.
</p>
<p>Empirical Example: Consider the Consumption-Income data from the Economic Report of the
President over the period 1959&ndash;2007 given in Table 5.3. Problem 5 asks the reader to verify
that Durbin&rsquo;s h obtained from the lagged dependent variable model described in (6.20) yields
a value of 3.50. This is asymptotically distributed as N(0, 1) under the null hypothesis of no
serial correlation of the disturbances. This null is soundly rejected. The Bruesch and Godfrey
test runs the regression of OLS residuals on their lagged values and the regressors in the model.
This yields a TR2 = 13.37. This is distributed as χ21 under the null and has a p-value of 0.0003.
Therefore, we reject the hypothesis of no first-order serial correlation. Next, we estimate (6.20)
using current and lagged values of income (Yt, Yt&minus;1 and Yt&minus;2) as a set of instruments for lagged</p>
<p/>
</div>
<div class="page"><p/>
<p>142 Chapter 6: Distributed Lags and Dynamic Models
</p>
<p>consumption (Ct&minus;1). The regression given by (6.22), yields:
</p>
<p>Ct =&minus;0.831
(0.280)
</p>
<p>+ 0.104
</p>
<p>(0.303)
</p>
<p>Ĉt&minus;1 + 1.177
(0.326)
</p>
<p>Yt + residuals
</p>
<p>Substituting these estimates in (6.20), one gets the residuals ũt. Based on these ũt&rsquo;s, the Wallis
(1967) estimate of ρ yields ρ̂ = 0.907 and Hatanaka&rsquo;s (1974) estimate of ρ yields ρ̃ = 0.828.
Running the Hatanaka regression gives
</p>
<p>C&lowast;t =&minus;0.142
(0.053)
</p>
<p>+ 0.233
</p>
<p>(0.083)
</p>
<p>C&lowast;t&minus;1 + 0.843
(0.095)
</p>
<p>Y &lowast;t + 0.017
(0.058)
</p>
<p>ũt&minus;1 + residuals
</p>
<p>where C&lowast;t = Ct &minus; ρ̃Ct&minus;1. The efficient estimate of ρ is given by ˜̃ρ = ρ̃+ 0.017 = 0.846.
</p>
<p>6.3.2 A Lagged Dependent Variable Model with MA(1) Disturbances
</p>
<p>Zellner and Geisel (1970) estimated the Koyck autoregressive representation of the infinite
distributed lag, given in (6.10). In fact, we saw that this could also arise from the AEM, see
(6.14). In particular, it is a regression with a lagged dependent variable and an MA(1) error
term with the added restriction that the coefficient of Yt&minus;1 is the same as the MA(1) parameter.
For simplicity, we write
</p>
<p>Yt = α+ λYt&minus;1 + βXt + (ut &minus; λut&minus;1) (6.23)
Let wt = Yt &minus; ut, then (6.23) becomes
</p>
<p>wt = α+ λwt&minus;1 + βXt (6.24)
</p>
<p>By continuous substitution of lagged values of wt in (6.24) we get
</p>
<p>wt = α(1 + λ+ λ
2 + ..+ λt&minus;1) + λtw0 + β(Xt + λXt&minus;1 + ..+ λ
</p>
<p>t&minus;1X1)
</p>
<p>and replacing wt by (Yt &minus; ut), we get
Yt = α(1 + λ+ λ
</p>
<p>2 + ..+ λt&minus;1) + λtw0 + β(Xt + λXt&minus;1 + ..+ λ
t&minus;1X1) + ut (6.25)
</p>
<p>knowing λ, this equation can be estimated via OLS assuming that the disturbances ut are not
serially correlated. Since λ is not known, Zellner and Geisel (1970) suggest a search procedure
over λ, where 0 &lt; λ &lt; 1. The regression with the minimum residual sums of squares gives the
optimal λ, and the corresponding regression gives the estimates of α, β and w0. The last coeffi-
cient wo = Yo&minus;uo = E(Yo) can be interpreted as the expected value of the initial observation on
the dependent variable. Klein (1958) considered the direct estimation of the infinite Koyck lag,
given in (6.8) and arrived at (6.25). The search over λ results in MLEs of the coefficients. Note,
however, that the estimate of wo is not consistent. Intuitively, as t tends to infinity, λ
</p>
<p>t tends to
zero implying no new information to estimate wo. In fact, some applied researchers ignore the
variable λt in the regression given in (6.25). This practice, known as truncating the remainder, is
not recommended since the Monte Carlo experiments of Maddala and Rao (1971) and Schmidt
(1975) have shown that even for T = 60 or 100, it is not desirable to omit λt from (6.25).
In summary, we have learned how to estimate a dynamic model with a lagged dependent
</p>
<p>variable and serially correlated errors. In case the error is autoregressive of order one, we have
outlined the steps to implement the Wallis Two-Stage estimator and Hatanaka&rsquo;s two-step proce-
dure. In case the error is Moving Average of order one, we have outlined the steps to implement
the Zellner-Geisel procedure.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Autoregressive Distributed Lag 143
</p>
<p>6.4 Autoregressive Distributed Lag
</p>
<p>So far, section 6.1 considered finite distributed lags on the explanatory variables, whereas section
6.2 considered an autoregressive relation including the first lag of the dependent variable and
current values of the explanatory variables. In general, economic relationships may be generated
by an Autoregressive Distributed Lag (ADL) scheme. The simplest form is the ADL (1,1) model
which is given by
</p>
<p>Yt = α+ λYt&minus;1 + β0Xt + β1Xt&minus;1 + ut (6.26)
</p>
<p>where both Yt and Xt are lagged once. By specifying higher order lags for Yt and Xt, say an
ADL (p, q) with p lags on Yt and q lags on Xt, one can test whether the specification now is
general enough to ensure White noise disturbances. Next, one can test whether some restrictions
can be imposed on this general model, like reducing the order of the lags to arrive at a simpler
ADL model, or estimating the simpler static model with the Cochrane-Orcutt correction for
serial correlation, see problem 20 in Chapter 7. This general to specific modelling strategy is
prescribed by David Hendry and is utilized by the econometric software PC-Give, see Gilbert
(1986).
</p>
<p>Returning to the ADL (1, 1) model in (6.26) one can invert the autoregressive form as follows:
</p>
<p>Yt = α(1 + λ+ λ
2 + ..) + (1 + λL+ λ2L2 + ..)(β0Xt + β1Xt&minus;1 + ut) (6.27)
</p>
<p>provided |λ| &lt; 1. This equation gives the effect of a unit change in Xt on future values of
Yt. In fact, &part;Yt/&part;Xt = β0 while &part;Yt+1/&part;Xt = β1 + λβ0, etc. This gives the immediate short-
run responses with the long-run effect being the sum of all these partial derivatives yielding
(β0+β1)/(1&minus;λ). This can be alternatively derived from (6.26) at the long-run static equilibrium
(Y &lowast;, X&lowast;) where Yt = Yt&minus;1 = Y &lowast;, Xt = Xt&minus;1 = X&lowast; and the disturbance is set equal to zero, i.e.,
</p>
<p>Y &lowast; =
α
</p>
<p>1&minus; λ +
β0 + β1
1&minus; λ X
</p>
<p>&lowast; (6.28)
</p>
<p>Replacing Yt by Yt&minus;1 +ΔYt and Xt by Xt&minus;1 +ΔXt in (6.26) one gets
</p>
<p>ΔYt = α+ β0ΔXt &minus; (1&minus; λ)Yt&minus;1 + (β0 + β1)Xt&minus;1 + ut
</p>
<p>This can be rewritten as
</p>
<p>ΔYt = β0ΔXt &minus; (1&minus; λ)
[
Yt&minus;1 &minus;
</p>
<p>α
</p>
<p>1&minus; λ &minus;
β0 + β1
1&minus; λ Xt&minus;1
</p>
<p>]
+ ut (6.29)
</p>
<p>Note that the term in brackets contains the long-run equilibrium parameters derived in (6.28).
In fact, the term in brackets represents the deviation of Yt&minus;1 from the long-run equilibrium
term corresponding to Xt&minus;1. Equation (6.29) is known as the Error Correction Model (ECM),
see Davidson, Hendry, Srba and Yeo (1978). Yt is obtained from Yt&minus;1 by adding the short-run
effect of the change in Xt and a long-run equilibrium adjustment term. Since, the disturbances
are White noise, this model is estimated by OLS.</p>
<p/>
</div>
<div class="page"><p/>
<p>144 Chapter 6: Distributed Lags and Dynamic Models
</p>
<p>Note
</p>
<p>1. Other distributions besides the geometric distribution can be considered. In fact, a Pascal distri-
bution was considered by Solow (1960), a rational-lag distribution was considered by Jorgenson
</p>
<p>(1966), and a Gamma distribution was considered by Schmidt (1974, 1975). See Maddala (1977)
</p>
<p>for an excellent review.
</p>
<p>Problems
</p>
<p>1. Consider the Consumption-Income data given in Table 5.3 and provided on the Springer web site
as CONSUMP.DAT. Estimate a Consumption-Income regression in logs that allows for a six year
lag on income as follows:
</p>
<p>(a) Use the linear arithmetic lag given in equation (6.2). Show that this result can also be
obtained as an Almon lag first-degree polynomial with a far end point constraint.
</p>
<p>(b) Use an Almon lag second-degree polynomial, described in equation (6.4), imposing the near
end point constraint.
</p>
<p>(c) Use an Almon lag second-degree polynomial imposing the far end point constraint.
</p>
<p>(d) Use an Almon lag second-degree polynomial imposing both end point constraints.
</p>
<p>(e) Using Chow&rsquo;s F -statistic, test the arithmetic lag restrictions given in part (a).
</p>
<p>(f) Using Chow&rsquo;s F -statistic, test the Almon lag restrictions implied by the model in part (b).
</p>
<p>(g) Repeat part (f) for the restrictions imposed in parts (c) and (d).
</p>
<p>2. Consider fitting an Almon lag third degree polynomial βi = a0+a1i+a2i
2+a3i
</p>
<p>3 for i = 0, 1, . . . , 5,
on the Consumption-Income relationship in logarithms. In this case, there are five lags on income,
i.e., s = 5.
</p>
<p>(a) Set up the estimating equation for the ai&rsquo;s and report the estimates using OLS.
</p>
<p>(b) What is your estimate of β3? What is the standard error? Can you relate the var(β̂3) to the
variances and covariances of the ai&rsquo;s?
</p>
<p>(c) How would the OLS regression in part (a) change if we impose the near end point constraint
β&minus;1 = 0?
</p>
<p>(d) Test the near end point constraint.
</p>
<p>(e) Test the Almon lag specification given in part (a) against an unrestricted five year lag spec-
ification on income.
</p>
<p>3. For the simple dynamic model with AR(1) disturbances given in (6.18),
</p>
<p>(a) Verify that plim(β̂OLS&minus;β) = ρ(1&minus;β2)/(1+ρβ). Hint: From (6.18), Yt&minus;1 = βYt&minus;2+νt&minus;1 and
ρYt&minus;1 = ρβYt&minus;2 + ρνt&minus;1. Subtracting this last equation from (6.18) and re-arranging terms,
</p>
<p>one gets Yt = (β+ρ)Yt&minus;1&minus;ρβYt&minus;2+ǫt. Multiply both sides by Yt&minus;1 and sum
&sum;T
</p>
<p>t=2 YtYt&minus;1 =
</p>
<p>(β + ρ)
&sum;T
</p>
<p>t=2 Y
2
t&minus;1 &minus; ρβ
</p>
<p>&sum;T
t=2 Yt&minus;1Yt&minus;2 +
</p>
<p>&sum;T
t=2 Yt&minus;1ǫt. Now divide by
</p>
<p>&sum;T
t=2 Y
</p>
<p>2
t&minus;1 and take
</p>
<p>probability limits. See Griliches (1961).
</p>
<p>(b) For various values of |ρ| &lt; 1 and |β| &lt; 1, tabulate the asymptotic bias computed in part (a).
(c) Verify that plim(ρ̂&minus; ρ) = &minus;ρ(1&minus; β2)/(1 + ρβ) = &minus;plim(β̂OLS &minus; β).</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 145
</p>
<p>(d) Using part (c), show that plim d = 2(1&minus; plim ρ̂) = 2[1 &minus; βρ(β + ρ)
1 + βρ
</p>
<p>] where d =
&sum;T
</p>
<p>t=2(ν̂t &minus;
</p>
<p>ν̂t&minus;1)2/
&sum;T
</p>
<p>t=1 ν̂
2
t denotes the Durbin-Watson statistic.
</p>
<p>(e) Knowing the true disturbances, the Durbin-Watson statistic would be d&lowast; =
&sum;T
</p>
<p>t=2(νt &minus;
νt&minus;1)2/
</p>
<p>&sum;T
t=1 ν
</p>
<p>2
t and its plim d
</p>
<p>&lowast; = 2(1 &minus; ρ). Using part (d), show that plim (d &minus; d&lowast;) =
2ρ(1&minus; β2)
1 + βρ
</p>
<p>= 2plim(β̂OLS &minus; β) obtained in part (a). See Nerlove and Wallis (1966). For
various values of |ρ| &lt; 1 and |β| &lt; 1, tabulate d&lowast; and d and the asymptotic bias in part (d).
</p>
<p>4. For the simple dynamic model given in (6.18), let the disturbances follow an MA(1) process
νt = ǫt + θǫt&minus;1 with ǫt &sim; IIN(0, σ2ǫ).
</p>
<p>(a) Show that plim(β̂OLS &minus; β) =
δ(1&minus; β2)
1 + 2βδ
</p>
<p>where δ = θ/(1 + θ2).
</p>
<p>(b) Tabulate this asymptotic bias for various values of |β| &lt; 1 and 0 &lt; θ &lt; 1.
</p>
<p>(c) Show that plim(
1
</p>
<p>T
</p>
<p>&sum;T
t=2 ν̂
</p>
<p>2
t ) = σ
</p>
<p>2
ǫ [1 + θ(θ &minus; θ&lowast;)] where θ&lowast; = δ(1 &minus; β2)/(1 + 2βδ) and ν̂t =
</p>
<p>Yt &minus; β̂OLSYt&minus;1.
</p>
<p>5. Consider the lagged dependent variable model given in (6.20). Using the Consumption-Income
data from the Economic Report of the President over the period 1950&ndash;1993 which is given in
Table 5.3.
</p>
<p>(a) Test for first-order serial correlation in the disturbances using Durbin&rsquo;s h given in (6.19).
</p>
<p>(b) Test for first-order serial correlation in the disturbances using the Breusch (1978) and Godfrey
(1978) test.
</p>
<p>(c) Test for second-order serial correlation in the disturbances.
</p>
<p>6. Using the U.S. gasoline data in Chapter 4, problem 15 given in Table 4.2 and obtained from the
USGAS.ASC file, estimate the following two models:
</p>
<p>Static: log
</p>
<p>(
QMG
</p>
<p>CAR
</p>
<p>)
</p>
<p>t
</p>
<p>= γ1 + γ2log
</p>
<p>(
RGNP
</p>
<p>POP
</p>
<p>)
</p>
<p>t
</p>
<p>+ γ3log
</p>
<p>(
CAR
</p>
<p>POP
</p>
<p>)
</p>
<p>t
</p>
<p>+γ4log
</p>
<p>(
PMG
</p>
<p>PGNP
</p>
<p>)
</p>
<p>t
</p>
<p>+ ǫt
</p>
<p>Dynamic: log
</p>
<p>(
QMG
</p>
<p>CAR
</p>
<p>)
</p>
<p>t
</p>
<p>= γ1 + γ2log
</p>
<p>(
RGNP
</p>
<p>POP
</p>
<p>)
</p>
<p>t
</p>
<p>+ γ3
</p>
<p>(
CAR
</p>
<p>POP
</p>
<p>)
</p>
<p>t
</p>
<p>+γ4log
</p>
<p>(
PMG
</p>
<p>PGNP
</p>
<p>)
</p>
<p>t
</p>
<p>+ λlog
</p>
<p>(
QMG
</p>
<p>CAR
</p>
<p>)
</p>
<p>t&minus;1
+ ǫt
</p>
<p>(a) Compare the implied short-run and long-run elasticities for price (PMG) and income (RGNP ).
</p>
<p>(b) Compute the elasticities after 3, 5 and 7 years. Do these lags seem plausible?
</p>
<p>(c) Can you apply the Durbin-Watson test for serial correlation to the dynamic version of this
model? Perform Durbin&rsquo;s h-test for the dynamic gasoline model. Also, the Breusch-Godfrey
test for first-order serial correlation.
</p>
<p>7. Using the U.S. gasoline data in Chapter 4, problem 15, given in Table 4.2 estimate the following
model with a six year lag on prices:
</p>
<p>log
</p>
<p>(
QMG
</p>
<p>CAR
</p>
<p>)
</p>
<p>t
</p>
<p>= γ1 + γ2log
</p>
<p>(
RGNP
</p>
<p>POP
</p>
<p>)
</p>
<p>t
</p>
<p>+ γ3log
</p>
<p>(
CAR
</p>
<p>POP
</p>
<p>)
</p>
<p>t
</p>
<p>+γ4
&sum; 6
</p>
<p>i=0wi log
</p>
<p>(
PMG
</p>
<p>PGNP
</p>
<p>)
</p>
<p>t&minus;i</p>
<p/>
</div>
<div class="page"><p/>
<p>146 Chapter 6: Distributed Lags and Dynamic Models
</p>
<p>(a) Report the unrestricted OLS estimates.
</p>
<p>(b) Now, estimate a second degree polynomial lag for the same model. Compare the results with
part (a) and explain why you got such different results.
</p>
<p>(c) Re-estimate part (b) comparing the six year lag to a four year, and eight year lag. Which
one would you pick?
</p>
<p>(d) For the six year lag model, does a third degree polynomial give a better fit?
</p>
<p>(e) For the model outlined in part (b), reestimate with a far end point constraint. Now, reestimate
with only a near end point constraint. Are such restrictions justified in this case?
</p>
<p>References
</p>
<p>This chapter is based on the material in Maddala (1977), Johnston (1984), Kelejian and Oates
(1989) and Davidson and MacKinnon (1993). Additional references on the material in this
chapter include:
</p>
<p>Akaike, H. (1973), &ldquo;Information Theory and an Extension of the Maximum Likelihood Principle,&rdquo; in
B. Petrov and F. Csake, eds. 2nd. International Symposium on Information Theory, Budapest:
Akademiai Kiado.
</p>
<p>Almon, S. (1965), &ldquo;The Distributed Lag Between Capital Appropriations and Net Expenditures,&rdquo; Econo-
metrica, 30: 407&ndash;423.
</p>
<p>Breusch, T.S. (1978), &ldquo;Testing for Autocorrelation in Dynamic Linear Models,&rdquo; Australian Economic
Papers, 17: 334&ndash;355.
</p>
<p>Davidson, J.E.H., D.F. Hendry, F. Srba and S. Yeo (1978), &ldquo;Econometric Modelling of the Aggregate
Time-Series Relationship Between Consumers&rsquo; Expenditure and Income in the United Kingdom,&rdquo;
Economic Journal, 88: 661&ndash;692.
</p>
<p>Dhrymes, P.J. (1971), Distributed Lags: Problems of Estimation and Formulation (Holden-Day: San
Francisco).
</p>
<p>Durbin, J. (1970), &ldquo;Testing for Serial Correlation in Least Squares Regression when Some of the Regres-
sors are Lagged Dependent Variables,&rdquo; Econometrica, 38: 410&ndash;421.
</p>
<p>Fomby, T.B. and D.K. Guilkey (1983), &ldquo;An Examination of Two-Step Estimators for Models with Lagged
Dependent and Autocorrelated Errors,&rdquo; Journal of Econometrics, 22: 291&ndash;300.
</p>
<p>Gilbert, C.L. (1986), &ldquo;Professor Hendry&rsquo;s Econometric Methodology,&rdquo; Oxford Bulletin of Economics and
Statistics, 48: 283&ndash;307.
</p>
<p>Godfrey, L.G. (1978), &ldquo;Testing Against General Autoregressive and Moving Average Error Models when
the Regressors Include Lagged Dependent Variables,&rdquo; Econometrica, 46: 1293&ndash;1302.
</p>
<p>Griliches, Z. (1961), &ldquo;A Note on Serial Correlation Bias in Estimates of Distributed Lags,&rdquo; Econometrica,
29: 65&ndash;73.
</p>
<p>Hatanaka, M. (1974), &ldquo;An Efficient Two-Step Estimator for the Dynamic Adjustment Model with Au-
tocorrelated Errors,&rdquo; Journal of Econometrics, 2: 199&ndash;220.
</p>
<p>Jorgenson, D.W. (1966), &ldquo;Rational Distributed Lag Functions,&rdquo; Econometrica, 34: 135&ndash;149.
</p>
<p>Kiviet, J.F. (1986), &ldquo;On The Vigor of Some Misspecification Tests for Modelling Dynamic Relationships,&rdquo;
Review of Economic Studies, 53: 241&ndash;262.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 147
</p>
<p>Klein, L.R. (1958), &ldquo;The Estimation of Distributed Lags,&rdquo; Econometrica, 26: 553&ndash;565.
</p>
<p>Koyck, L.M. (1954), Distributed Lags and Investment Analysis (North-Holland: Amsterdam).
</p>
<p>Maddala, G.S. and A.S. Rao (1971), &ldquo;Maximum Likelihood Estimation of Solow&rsquo;s and Jorgenson&rsquo;s Dis-
tributed Lag Models,&rdquo; Review of Economics and Statistics, 53: 80&ndash;88.
</p>
<p>Nerlove, M. and K.F. Wallis (1967), &ldquo;Use of the Durbin-Watson Statistic in Inappropriate Situations,&rdquo;
Econometrica, 34: 235&ndash;238.
</p>
<p>Schwarz, G. (1978), &ldquo;Estimating the Dimension of a Model,&rdquo; Annals of Statistics, 6: 461&ndash;464.
</p>
<p>Schmidt, P. (1974), &ldquo;An Argument for the Usefulness of the Gamma Distributed Lag Model,&rdquo; Interna-
tional Economic Review, 15: 246&ndash;250.
</p>
<p>Schmidt, P. (1975), &ldquo;The Small Sample Effects of Various Treatments of Truncation Remainders on the
Estimation of Distributed Lag Models,&rdquo; Review of Economics and Statistics, 57: 387&ndash;389.
</p>
<p>Schmidt, P. and R. N. Waud (1973), &ldquo;The Almon lag Technique and the Monetary versus Fiscal Policy
Debate,&rdquo; Journal of the American Statistical Association, 68: 11&ndash;19.
</p>
<p>Solow, R.M. (1960), &ldquo;On a Family of Lag Distributions,&rdquo; Econometrica, 28: 393&ndash;406.
</p>
<p>Wallace, T.D. (1972), &ldquo;Weaker Criteria and Tests for Linear Restrictions in Regression,&rdquo; Econometrica,
40: 689&ndash;698.
</p>
<p>Wallis, K.F. (1967), &ldquo;Lagged Dependent Variables and Serially Correlated Errors: A Reappraisal of
Three-Pass Least Squares, &rdquo; Review of Economics and Statistics, 49: 555&ndash;567.
</p>
<p>Zellner, A. and M.Geisel (1970), &ldquo;Analysis of Distributed Lag Models with Application to Consumption
</p>
<p>Function Estimation,&rdquo; Econometrica, 38: 865&ndash;888.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part II</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 7
</p>
<p>The General Linear Model: The Basics
</p>
<p>7.1 Introduction
</p>
<p>Consider the following regression equation
</p>
<p>y = Xβ + u (7.1)
</p>
<p>where
</p>
<p>y =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>Y1
Y2
:
Yn
</p>
<p>⎤
⎥⎥⎦ ;X =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>X11 X12 . . . X1k
X21 X22 . . . X2k
: : : :
</p>
<p>Xn1 Xn2 . . . Xnk
</p>
<p>⎤
⎥⎥⎦ ;β =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>β1
β2
:
βk
</p>
<p>⎤
⎥⎥⎦ ;u =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>u1
u2
:
un
</p>
<p>⎤
⎥⎥⎦
</p>
<p>with n denoting the number of observations and k the number of variables in the regression,
with n &gt; k. In this case, y is a column vector of dimension (n&times;1) andX is a matrix of dimension
(n &times; k). Each column of X denotes a variable and each row of X denotes an observation on
these variables. If y is log(wage) as in the empirical example in Chapter 4, see Table 4.1 then
the columns of X contain a column of ones for the constant (usually the first column), weeks
worked, years of full time experience, years of education, sex, race, marital status, etc.
</p>
<p>7.2 Least Squares Estimation
</p>
<p>Least squares minimizes the residual sum of squares where the residuals are given by e = y&minus;Xβ̂
and β̂ denotes a guess on the regression parameters β. The residual sum of squares
</p>
<p>RSS =
&sum;n
</p>
<p>i=1
e2i = e
</p>
<p>&prime;e = (y &minus;Xβ)&prime;(y &minus;Xβ) = y&prime;y &minus; y&prime;Xβ &minus; β&prime;X &prime;y + β&prime;X &prime;Xβ
</p>
<p>The last four terms are scalars as can be verified by their dimensions. It is essential that the
reader keep track of the dimensions of the matrices used. This will insure proper multiplication,
addition, subtraction of matrices and help the reader obtain the right answers. In fact the middle
two terms are the same because the transpose of a scalar is a scalar. For a quick review of some
matrix properties, see the Appendix to this chapter. Differentiating the RSS with respect to β
one gets
</p>
<p>&part;RSS/&part;β = &minus;2X &prime;y + 2X &prime;Xβ (7.2)
</p>
<p>where use is made of the following two rules of differentiating matrices. The first is that
&part;a&prime;b/&part;b = a and the second is
</p>
<p>&part;(b&prime;Ab)/&part;b = (A+A&prime;)b = 2Ab
</p>
<p>where the last equality holds if A is a symmetric matrix. In the RSS equation a is y&prime;X and A is
X &prime;X. The first-order condition for minimization equates the expression in (7.2) to zero. This yields
</p>
<p>X &prime;Xβ = X &prime;y (7.3)
</p>
<p>151
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_7, </p>
<p/>
</div>
<div class="page"><p/>
<p>152 Chapter 7: The General Linear Model: The Basics
</p>
<p>which is known as the OLS normal equations. As long as X is of full column rank , i.e., of rank k,
then X &prime;X is nonsingular and the solution to the above equations is β̂OLS = (X
</p>
<p>&prime;X)&minus;1X &prime;y. Full
column rank means that no column of X is a perfect linear combination of the other columns.
In other words, no variable in the regression can be obtained from a linear combination of the
other variables. Otherwise, at least one of the OLS normal equations becomes redundant. This
means that we only have (k &minus; 1) linearly independent equations to solve for k unknown β&rsquo;s.
This yields no solution for β̂OLS and we say that X
</p>
<p>&prime;X is singular. X &prime;X is the sum of squares
cross product matrix (SSCP). If it has a column of ones then it will contain the sums, the sum
of squares, and the cross-product sum between any two variables
</p>
<p>X &prime;X =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>n
&sum;n
</p>
<p>i=1Xi2 . . .
&sum;n
</p>
<p>i=1Xik&sum;n
i=1Xi2
</p>
<p>&sum;n
i=1X
</p>
<p>2
i2 . . .
</p>
<p>&sum;n
i=1Xi2Xik
</p>
<p>: : :&sum;n
i=1Xik
</p>
<p>&sum;n
i=1XikXi2 . . .
</p>
<p>&sum;n
i=1X
</p>
<p>2
ik
</p>
<p>⎤
⎥⎥⎦
</p>
<p>Of course y could be added to this matrix as another variable which will generate X &prime;y and
y&prime;y automatically for us, i.e., the column pertaining to the variable y will generate
</p>
<p>&sum;n
i=1 yi,&sum;n
</p>
<p>i=1Xi1yi, . . . ,
&sum;n
</p>
<p>i=1Xikyi, and
&sum;n
</p>
<p>i=1 y
2
i . To see this, let
</p>
<p>Z = [y,X] then Z &prime;Z =
</p>
<p>[
y&prime;y y&prime;X
X &prime;y X &prime;X
</p>
<p>]
</p>
<p>This matrix summarizes the data and we can compute any regression of one variable in Z on
any subset of the remaining variables in Z using only Z &prime;Z. Denoting the least squares residuals
by e = y &minus;Xβ̂OLS , the OLS normal equations given in (7.3) can be written as
</p>
<p>X &prime;(y &minus;Xβ̂OLS) = X &prime;e = 0 (7.4)
</p>
<p>Note that if the regression includes a constant, the first column of X will be a vector of ones
and the first equation of (7.4) becomes
</p>
<p>&sum;n
i=1 ei = 0. This proves the well known result that
</p>
<p>if there is a constant in the regression, the OLS residuals sum to zero. Equation (7.4) also
indicates that the regressor matrix X is orthogonal to the residuals vector e. This will become
clear when we define e in terms of the orthogonal projection matrix on X. This representation
allows another interpretation of OLS as a method of moments estimator which was considered
in Chapter 2. This follows from the classical assumptions where X satisfies E(X &prime;u) = 0. The
sample counterpart of this condition yields X &prime;e/n = 0. These are the OLS normal equations
and therefore, yield the OLS estimates without minimizing the residual sums of squares.
Since data in economics are not generated using experiments like the physical sciences, the
</p>
<p>X&rsquo;s are stochastic and we only observe one realization of this data. Consider for example, annual
observations for GNP, money supply, unemployment rate, etc. One cannot repeat draws for this
data in the real world or fix the X&rsquo;s to generate new y&rsquo;s (unless one is performing a Monte
Carlo study). So we have to condition on the set of X&rsquo;s observed, see Chapter 5.
</p>
<p>Classical Assumptions: u &sim; (0, σ2In) which means that (i) each disturbance ui has zero mean,
(ii) constant variance, and (iii) ui and uj for i 	= j are not correlated. The u&rsquo;s are known as
spherical disturbances. Also, (iv) the conditional expectation of u given X is zero, E(u/X) = 0.
Note that the conditioning here is with respect to every regressor in X and for all observations
i = 1, 2, . . . n. In other words, it is conditional on all the elements of the matrix X. Using</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Least Squares Estimation 153
</p>
<p>(7.1), this implies that E(y/X) = Xβ is linear in β, var(ui/X) = σ
2 and cov(ui, uj/X) = 0.
</p>
<p>Additionally, we assume that plim X &prime;X/n is finite and positive definite and plim X &prime;u/n = 0 as
n &rarr; &infin;.
</p>
<p>Given these classical assumptions, and conditioning on the X&rsquo;s observed, it is easy to show
that β̂OLS is unbiased for β. In fact using (7.1) one can write
</p>
<p>β̂OLS = β + (X
&prime;X)&minus;1X &prime;u (7.5)
</p>
<p>Taking expectations, conditioning on the X&rsquo;s, and using assumptions (i) and (iv), one attains
the unbiasedness result. Furthermore, one can derive the variance-covariance matrix of β̂OLS
from (7.5) since
</p>
<p>var(β̂OLS) = E(β̂OLS &minus; β)(β̂OLS &minus; β)&prime; = E(X &prime;X)&minus;1X &prime;uu&prime;X(X &prime;X)&minus;1 = σ2(X &prime;X)&minus;1 (7.6)
</p>
<p>this uses assumption (iv) along with the fact that E(uu&prime;) = σ2In. This variance-covariance
matrix is (k &times; k) and gives the variances of the β̂i&rsquo;s across the diagonal and the pairwise
covariances of say β̂i and β̂j off the diagonal. The next theorem shows that among all linear
</p>
<p>unbiased estimators of c&prime;β, it is c&prime;β̂OLS which has the smallest variance. This is known as the
Gauss-Markov Theorem.
</p>
<p>Theorem 1: Consider the linear estimator a&prime;y for c&prime;β, where both a and c are arbitrary vectors
of constants. If a&prime;y is unbiased for c&prime;β then var(a&prime;y) &ge; var(c&prime;β̂OLS).
</p>
<p>Proof: For a&prime;y to be unbiased for c&prime;β it must follow from (7.1) that E(a&prime;y) = a&prime;Xβ +E(a&prime;u) =
a&prime;Xβ = c&prime;β which means that a&prime;X = c&prime; . Also, var(a&prime;y) = E(a&prime;y&minus;c&prime;β)(a&prime;y&minus;c&prime;β)&prime; = E(a&prime;uu&prime;a) =
σ2a&prime;a. Comparing this variance with that of c&prime;β̂OLS , one gets var(a
</p>
<p>&prime;y)&minus; var(c&prime;β̂OLS) = σ2a&prime;a&minus;
σ2c&prime;(X &prime;X)&minus;1c. But, c&prime; = a&prime;X, therefore this difference becomes σ2[a&prime;a &minus; a&prime;PXa] = σ2a&prime;P̄Xa
where PX is a projection matrix on the X-plane defined as X(X
</p>
<p>&prime;X)&minus;1X &prime; and P̄X is defined as
In &minus; PX . In fact, PXy = Xβ̂OLS = ŷ and P̄Xy = y &minus; PXy = y &minus; ŷ = e. So that ŷ projects
the vector y on the X-plane and e is the projection of y on the plane orthogonal to X or
perpendicular to X, see Figure 7.1. Both PX and P̄X are idempotent which means that the
above difference σ2a&prime;P̄Xa is greater or equal to zero since P̄X is positive semi-definite. To see
this, define z = P̄Xa, then the above difference is equal to σ
</p>
<p>2z&prime;z &ge; 0.
The implications of the theorem are important. It means for example, that for the choice
</p>
<p>of c&prime; = (1, 0, . . . , 0) one can pick β1 = c
&prime;β for which the best linear unbiased estimator would
</p>
<p>be β̂1,OLS = c
&prime;β̂OLS . Similarly any βj can be chosen by using c
</p>
<p>&prime; = (0, . . . , 1, . . . , 0) which has
</p>
<p>1 in the j-th position and zero elsewhere. Again, the BLUE of βj = c
&prime;β is β̂j,OLS = c
</p>
<p>&prime;β̂OLS .
</p>
<p>Furthermore, any linear combination of these β&rsquo;s such as their sum
&sum;k
</p>
<p>j=1 βj which corresponds
</p>
<p>to c&prime; = (1, 1, . . . , 1) has the sum
&sum;k
</p>
<p>j=1 β̂j,OLS as its BLUE.
</p>
<p>The disturbance variance σ2 is unknown and has to be estimated. Note that E(u&prime;u) =
E(tr(uu&prime;)) = tr(E(uu&prime;)) = tr(σ2In) = nσ2, so that u&prime;u/n seems like a natural unbiased es-
timator for σ2. However, u is not observed and is estimated by the OLS residuals e. It is
therefore, natural to investigate E(e&prime;e). In what follows, we show that s2 = e&prime;e/(n &minus; k) is an
unbiased estimator for σ2. To prove this, we need the fact that
</p>
<p>e = y &minus;Xβ̂OLS = y &minus;X(X &prime;X)&minus;1X &prime;y = P̄Xy = P̄Xu (7.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>154 Chapter 7: The General Linear Model: The Basics
</p>
<p>��� � �
</p>
<p>x
</p>
<p>y
</p>
<p>���
�
</p>
<p>�ˆ
</p>
<p>Figure 7.1 The Orthogonal Decomposition of y
</p>
<p>where the last equality follows from the fact that P̄XX = 0. Hence,
</p>
<p>E(e&prime;e) = E(u&prime;P̄Xu) = E(tr{u&prime;P̄Xu}) = E(tr{uu&prime;P̄X})
= tr(σ2P̄X) = σ
</p>
<p>2tr(P̄X) = σ
2(n&minus; k)
</p>
<p>where the second equality follows from the fact that the trace of a scalar is a scalar. The
third equality from the fact that tr(ABC) = tr(CAB). The fourth equality from the fact that
E(trace) = trace{E(.)}, and E(uu&prime;) = σ2In. The last equality from the fact that
</p>
<p>tr(P̄X) = tr(In)&minus; tr(PX) = n&minus; tr(X(X &prime;X)&minus;1X &prime;)
= n&minus; tr(X &prime;X(X &prime;X)&minus;1) = n&minus; tr(Ik) = n&minus; k.
</p>
<p>Hence, an unbiased estimator of var(β̂OLS) = σ
2(X &prime;X)&minus;1 is given by s2(X &prime;X)&minus;1.
</p>
<p>So far we have shown that β̂OLS is BLUE. It can also be shown that it is consistent for β. In
fact, taking probability limits of (7.5) as n &rarr; &infin;, one gets
</p>
<p>plim(β̂OLS) = plim(β) + plim(X
&prime;X/n)&minus;1(X &prime;u/n) = β
</p>
<p>The first equality uses the fact that the plim of a sum is the sum of the plims. The second
equality follows from assumption 1 and the fact that plim of a product is the product of plims.
</p>
<p>7.3 Partitioned Regression and the Frisch-Waugh-Lovell Theorem
</p>
<p>In Chapter 4, we studied a useful property of least squares which allows us to interpret multiple
regression coefficients as simple regression coefficients. This was called the residualing inter-
pretation of multiple regression coefficients. In general, this property applies whenever the k
regressors given byX can be separated into two sets of variablesX1 andX2 of dimension (n&times;k1)
and (n&times; k2) respectively, with X = [X1, X2] and k = k1 + k2. The regression in equation (7.1)
becomes a partitioned regression given by
</p>
<p>y = Xβ + u = X1β1 +X2β2 + u (7.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Partitioned Regression and the Frisch-Waugh-Lovell Theorem 155
</p>
<p>One may be interested in the least squares estimates of β2 corresponding to X2, but one has
to control for the presence of X1 which may include seasonal dummy variables or a time trend,
see Frisch and Waugh (1933) and Lovell (1963)1.
The OLS normal equations from (7.8) are as follows:
</p>
<p>[
X &prime;1X1 X
</p>
<p>&prime;
1X2
</p>
<p>X &prime;2X1 X
&prime;
2X2
</p>
<p>] [
β̂1,OLS
β̂2,OLS
</p>
<p>]
=
</p>
<p>[
X &prime;1y
X &prime;2y
</p>
<p>]
(7.9)
</p>
<p>These can be solved by partitioned inversion of the matrix on the left, see the Appendix to this
chapter, or by solving two equations in two unknowns. Problem 2 asks the reader to verify that
</p>
<p>β̂2,OLS = (X
&prime;
2P̄X1X2)
</p>
<p>&minus;1X &prime;2P̄X1y (7.10)
</p>
<p>where P̄X1 = In &minus; PX1 and PX1 = X1(X &prime;1X1)&minus;1X &prime;1. P̄X1 is the orthogonal projection matrix of
X1 and P̄X1X2 generates the least squares residuals of each column of X2 regressed on all the
variables in X1. In fact, if we denote by X̃2 = P̄X1X2 and ỹ = P̄X1y, then (7.10) can be written
as
</p>
<p>β̂2,OLS = (X̃
&prime;
2X̃2)
</p>
<p>&minus;1X̃ &prime;2ỹ (7.11)
</p>
<p>using the fact that P̄X1 is idempotent. This implies that β̂2,OLS can be obtained from the
</p>
<p>regression of ỹ on X̃2. In words, the residuals from regressing y on X1 are in turn regressed
upon the residuals from each column of X2 regressed on all the variables in X1. This was
illustrated in Chapter 4 with some examples. Following Davidson and MacKinnon (1993) we
denote this result more formally as the Frisch-Waugh-Lovell (FWL) Theorem. In fact, if we
premultiply (7.8) by P̄X1 and use the fact that P̄X1X1 = 0, one gets
</p>
<p>P̄X1y = P̄X1X2β2 + P̄X1u (7.12)
</p>
<p>The FWL Theorem states that: (1) The least squares estimates of β2 from equations (7.8)
and (7.12) are numerically identical and (2) The least squares residuals from equations (7.8)
and (7.12) are identical.
</p>
<p>Using the fact that P̄X1 is idempotent, it immediately follows that, OLS on (7.12) yields β̂2,OLS
as given by equation (7.10). Alternatively, one can start from equation (7.8) and use the result
that
</p>
<p>y = PXy + P̄Xy = Xβ̂OLS + P̄Xy = X1β̂1,OLS +X2β̂2,OLS + P̄Xy (7.13)
</p>
<p>where PX = X(X
&prime;X)&minus;1X &prime; and P̄X = In &minus; PX . Premultiplying equation (7.13) by X &prime;2P̄X1 and
</p>
<p>using the fact that P̄X1X1 = 0, one gets
</p>
<p>X &prime;2P̄X1y = X
&prime;
2P̄X1X2β̂2,OLS +X
</p>
<p>&prime;
2P̄X1P̄Xy (7.14)
</p>
<p>But, PX1PX = PX1 . Hence, P̄X1P̄X = P̄X . Using this fact along with P̄XX = P̄X [X1, X2] = 0,
</p>
<p>the last term of equation (7.14) drops out yielding the result that β̂2,OLS from (7.14) is identical
to the expression in (7.10). Note that no partitioned inversion was used in this proof. This proves
part (1) of the FWL Theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>156 Chapter 7: The General Linear Model: The Basics
</p>
<p>Also, premultiplying equation (7.13) by P̄X1 and using the fact that P̄X1P̄X = P̄X , one gets
</p>
<p>P̄X1y = P̄X1X2β̂2,OLS + P̄Xy (7.15)
</p>
<p>Now β̂2,OLS was shown to be numerically identical to the least squares estimate obtained from
equation (7.12). Hence, the first term on the right hand side of equation (7.15) must be the
fitted values from equation (7.12). Since the dependent variables are the same in equations
(7.15) and (7.12), P̄Xy in equation (7.15) must be the least squares residuals from regression
(7.12). But, P̄Xy is the least squares residuals from regression (7.8). Hence, the least squares
residuals from regressions (7.8) and (7.12) are numerically identical. This proves part (2) of the
FWL Theorem.
Several applications of the FWL Theorem will be given in this book. Problem 2 shows that if
</p>
<p>X1 is the vector of ones indicating the presence of a constant in the regression, then regression
(7.15) is equivalent to running (yi&minus; ȳ) on the set of variables in X2 expressed as deviations from
their respective sample means. Problem 3 shows that the FWL Theorem can be used to prove
that including a dummy variable for one of the observations in the regression is equivalent to
omitting that observation from the regression.
</p>
<p>7.4 Maximum Likelihood Estimation
</p>
<p>In Chapter 2, we introduced the method of maximum likelihood estimation which is based on
specifying the distribution we are sampling from and writing the joint density of our sample.
This joint density is then referred to as the likelihood function because it gives for a given
set of parameters specifying the distribution, the probability of obtaining the observed sample.
See Chapter 2 for several examples. For the regression equation, specifying the distribution of
the disturbances in turn specifies the likelihood function. These disturbances could be Poisson,
Exponential, Normal, etc. Once this distribution is chosen, the likelihood function is maximized
and the MLE of the regression parameters are obtained. Maximum likelihood estimators are
desirable because they are (1) consistent under fairly general conditions,2 (2) asymptotically
normal, (3) asymptotically efficient and (4) invariant to reparameterizations of the model3. Some
of the undesirable properties of MLE are that (1) it requires explicit distributional assumptions
on the disturbances, and (2) their finite sample properties can be quite different from their
asymptotic properties. For example, MLE can be biased even though they are consistent, and
their covariance estimates can be misleading for small samples. In this section, we derive the
MLE under normality of the disturbances.
</p>
<p>The Normality Assumption: u &sim; N(0, σ2In). This additional assumption allows us to
derive distributions of estimators and other random variables. This is important for constructing
confidence intervals and tests of hypotheses. In fact using (7.5) one can easily see that β̂OLS is
a linear combination of the u&rsquo;s. But, a linear combination of normal random variables is itself
a normal random variable. Hence, β̂OLS is N(β, σ
</p>
<p>2(X &prime;X)&minus;1). Similarly y is N(Xβ, σ2In) and
e is N(0, σ2P̄X). Moreover, we can write the joint probability density function of the u&rsquo;s as
f(u1, u2, . . . , un;σ
</p>
<p>2) = (1/2πσ2)n/2exp(&minus;u&prime;u/2σ2). To get the likelihood function we make the
transformation u = y &minus;Xβ and note that the Jacobian of the transformation is one. Hence
</p>
<p>f(y1, y2, . . . , yn;β, σ
2) = (1/2πσ2)n/2exp{&minus;(y &minus;Xβ)&prime;(y &minus;Xβ)/2σ2} (7.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Maximum Likelihood Estimation 157
</p>
<p>Taking the log of this likelihood, we get
</p>
<p>logL(β, σ2) = &minus;(n/2)log(2πσ2)&minus; (y &minus;Xβ)&prime;(y &minus;Xβ)/2σ2 (7.17)
</p>
<p>Maximizing this likelihood with respect to β and σ2 one gets the maximum likelihood estimators
(MLE). Let θ = σ2 and Q = (y &minus;Xβ)&prime;(y &minus;Xβ), then
</p>
<p>&part;logL(β, θ)
</p>
<p>&part;β
=
</p>
<p>2X &prime;y &minus; 2X &prime;Xβ
2θ
</p>
<p>&part;logL(β, θ)
</p>
<p>&part;θ
=
</p>
<p>Q
</p>
<p>2θ2
&minus; n
</p>
<p>2θ
</p>
<p>Setting these first-order conditions equal to zero, one gets
</p>
<p>β̂MLE = β̂OLS and θ̂ = σ̂
2
MLE = Q/n = RSS/n = e
</p>
<p>&prime;e/n.
</p>
<p>Intuitively, only the second term in the log likelihood contains β and that term (without the
negative sign) has already been minimized with respect to β in (7.2) giving us the OLS estimator.
Note that σ̂2MLE differs from s
</p>
<p>2 only in the degrees of freedom. It is clear that β̂MLE is unbiased
for β while σ̂2MLE is not unbiased for σ
</p>
<p>2. Substituting these MLE&rsquo;s into (7.17) one gets the
maximum value of logL which is
</p>
<p>logL(β̂MLE , σ̂
2
MLE) = &minus;(n/2)log(2πσ̂2MLE)&minus; e&prime;e/2σ̂2MLE
</p>
<p>= &minus;(n/2)log(2π)&minus; (n/2)log(e&prime;e/n)&minus; n/2
= constant&minus; (n/2)log(e&prime;e).
</p>
<p>In order to get the Cramér-Rao lower bound for the unbiased estimators of β and σ2 one first
computes the information matrix
</p>
<p>I(β, σ2) = &minus;E
[
&part;2logL/&part;β&part;β&prime; &part;2logL/&part;β&part;σ2
</p>
<p>&part;2logL/&part;σ2&part;β&prime; &part;2logL/&part;σ2&part;σ2
</p>
<p>]
(7.18)
</p>
<p>Recall, that θ = σ2 and Q = (y &minus;Xβ)&prime;(y &minus;Xβ). It is easy to show (see problem 4) that
</p>
<p>&part;2logL(β, θ)
</p>
<p>&part;β&part;θ
=
</p>
<p>1
</p>
<p>2θ2
&part;Q
</p>
<p>&part;β
and
</p>
<p>&part;2logL(β, θ)
</p>
<p>&part;θ&part;β
=
</p>
<p>&minus;X &prime;(y &minus;Xβ)
θ2
</p>
<p>Therefore,
</p>
<p>E
</p>
<p>(
&part;2logL(β, θ)
</p>
<p>&part;θ&part;β
</p>
<p>)
=
</p>
<p>&minus;E(X &prime;u)
θ2
</p>
<p>= 0
</p>
<p>Also
</p>
<p>&part;2logL(β, θ)
</p>
<p>&part;β&part;β&prime;
=
</p>
<p>&minus;X &prime;X
θ
</p>
<p>and
&part;2logL(β, θ)
</p>
<p>&part;θ2
=
</p>
<p>&minus;4Q
4θ3
</p>
<p>+
2n
</p>
<p>4θ2
=
</p>
<p>&minus;Q
θ3
</p>
<p>+
n
</p>
<p>2θ2
</p>
<p>so that
</p>
<p>E
</p>
<p>(
&part;2logL(β, θ)
</p>
<p>&part;θ2
</p>
<p>)
=
</p>
<p>&minus;nθ
θ3
</p>
<p>+
n
</p>
<p>2θ2
=
</p>
<p>&minus;2n+ n
2θ2
</p>
<p>=
&minus;n
2θ2</p>
<p/>
</div>
<div class="page"><p/>
<p>158 Chapter 7: The General Linear Model: The Basics
</p>
<p>using the fact that E(Q) = nσ2 = nθ. Hence,
</p>
<p>I(β, σ2) =
</p>
<p>[
X &prime;X/σ2 0
</p>
<p>0 n/2σ4
</p>
<p>]
(7.19)
</p>
<p>The information matrix is block-diagonal between β and σ2. This is an important property for
regression models with normal disturbances. It implies that the Cramér-Rao lower bound is
</p>
<p>I&minus;1(β, σ2) =
</p>
<p>[
σ2(X &prime;X)&minus;1 0
</p>
<p>0 2σ4/n
</p>
<p>]
(7.20)
</p>
<p>Note that β̂MLE = β̂OLS attains the Cramér-Rao lower bound. Under normality, β̂OLS is
MVU (minimum variance unbiased). This is best among all unbiased estimators not only linear
unbiased estimators. By assuming more (in this case normality) we get more (MVU rather than
BLUE)4.
</p>
<p>Problem 5 derives the variance of s2 under normality of the disturbances. This is found to
be 2σ4/(n &minus; k). This means that s2 does not attain the Cramér-Rao lower bound. However,
following the theory of complete sufficient statistics one can show that both β̂OLS and s
</p>
<p>2 are
MVU for their respective parameters and therefore both are small sample efficient. Note also
that σ̂2MLE is biased, therefore it is not meaningful to compare its variance to the Cramér-Rao
lower bound. There is a trade-off between bias and variance in estimating σ2. Problem 6 looks
at all estimators of σ2 of the type e&prime;e/r and derives r such that the mean squared error (MSE)
is minimized. The choice of r turns out to be (n&minus; k + 2).
We found the distribution of β̂OLS , now we derive the distribution of s
</p>
<p>2. In order to do that
we need a result from matrix algebra, which is stated without proof, see Graybill (1961).
</p>
<p>Lemma 1: For every symmetric idempotent matrix A of rank r, there exists an orthogonal
matrix P such that P &prime;AP = Jr where Jr is a diagonal matrix with the first r elements equal to
one and the rest equal to zero.
</p>
<p>We use this lemma to show that the RSS/σ2 is a chi-squared with (n&minus;k) degrees of freedom.
To see this note that e&prime;e/σ2 = u&prime;P̄Xu/σ2 and that P̄X is symmetric and idempotent of rank
(n&minus;k). Using the lemma there exists a matrix P such that P &prime;P̄XP = Jn&minus;k is a diagonal matrix
with the first (n&minus; k) elements on the diagonal equal to 1 and the last k elements equal to zero.
Now make the change of variable v = P &prime;u. This makes v &sim; N(0, σ2In) since the v&rsquo;s are linear
combinations of the u&rsquo;s and P &prime;P = In. Replacing u by v in RSS/σ2 we get
</p>
<p>v&prime;PP̄XPv/σ
2 = v&prime;Jn&minus;kv/σ
</p>
<p>2 =
&sum;n&minus;k
</p>
<p>i=1 v
2
i /σ
</p>
<p>2
</p>
<p>where the last sum is only over i = 1, 2, . . . , n &minus; k. But, the v&rsquo;s are independent identically
distributed N(0, σ2), hence v2i /σ
</p>
<p>2 is the square of a standardized N(0, 1) random variable which
is distributed as a χ21. Moreover, the sum of independent χ
</p>
<p>2 random variables is a χ2 random
variable with degrees of freedom equal to the sum of the respective degrees of freedom. Hence,
RSS/σ2 is distributed as χ2n&minus;k.
</p>
<p>The beauty of the above result is that it applies to all quadratic forms u&prime;Au where A is
symmetric and idempotent. We will use this result again in the test of hypotheses section.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Prediction 159
</p>
<p>7.5 Prediction
</p>
<p>Let us now predict To periods ahead. Those new observations are assumed to satisfy (7.1). In
other words
</p>
<p>yo = Xoβ + uo (7.21)
</p>
<p>What is the Best Linear Unbiased Predictor (BLUP) of E(yo)? From (7.21), E(yo) = Xoβ which
is a linear combination of the β&rsquo;s. Using the Gauss-Markov result ŷo = Xoβ̂OLS is BLUE for
Xoβ and the variance of this predictor of E(yo) is Xovar(β̂OLS)X
</p>
<p>&prime;
o = σ
</p>
<p>2Xo(X
&prime;X)&minus;1X &prime;o. But,
</p>
<p>what if we are interested in the predictor for yo? The best predictor of uo is zero, so the predictor
for yo is still ŷo but its MSE is
</p>
<p>E(ŷo &minus; yo)(ŷo &minus; yo)&prime; = E{Xo(β̂OLS &minus; β)&minus; uo}{Xo(β̂OLS &minus; β)&minus; uo}&prime;
= Xovar(β̂OLS)X
</p>
<p>&prime;
o + σ
</p>
<p>2ITo &minus; 2cov{Xo(β̂OLS &minus; β), uo}
= σ2Xo(X
</p>
<p>&prime;X)&minus;1X &prime;o + σ
2ITo
</p>
<p>(7.22)
</p>
<p>the last equality follows from the fact that (β̂OLS &minus; β) = (X &prime;X)&minus;1X &prime;u and uo have zero co-
variance. The latter holds because uo and u have zero covariance. Intuitively this says that the
future To disturbances are not correlated with the current sample disturbances.
</p>
<p>Therefore, the predictor of the average consumption of a $20, 000 income household is the
same as the predictor of consumption of a specific household whose income is $20, 000. The
difference is not in the predictor itself but in the MSE attached to it. The latter MSE being
larger.
</p>
<p>Salkever (1976) suggested a simple way to compute these forecasts and their standard errors.
The basic idea is to augment the usual regression in (7.1) with a matrix of observation-specific
dummies, i.e., a dummy variable for each period where we want to forecast:
</p>
<p>[
y
yo
</p>
<p>]
=
</p>
<p>[
X 0
Xo ITo
</p>
<p>] [
β
γ
</p>
<p>]
+
</p>
<p>[
u
uo
</p>
<p>]
(7.23)
</p>
<p>or
</p>
<p>y&lowast; = X&lowast;δ + u&lowast; (7.24)
</p>
<p>where δ&prime; = (β&prime;, γ&prime;). X&lowast; has in its second part a matrix of dummy variables one for each of
the To periods for which we are forecasting. Since these To observations do not serve in the
</p>
<p>estimation, problem 7 asks the reader to verify that OLS on (7.23) yields δ̂
&prime;
= (β̂
</p>
<p>&prime;
, γ̂&prime;) where
</p>
<p>β̂ = (X &prime;X)&minus;1X &prime;y, γ̂ = yo &minus; ŷo, and ŷo = Xoβ̂. In other words, OLS on (7.23) yields the
OLS estimate of β without the To observations, and the coefficients of the To dummies are
the forecast errors. This also means that the first n residuals are the usual OLS residuals
e = y &minus; Xβ̂ based on the first n observations, whereas the next To residuals are all zero.
Therefore, s&lowast;2 = s2 = e&prime;e/(n&minus; k), and the variance covariance matrix of δ̂ is given by
</p>
<p>s2(X&lowast;&prime;X&lowast;)&minus;1 = s2
[
(X &prime;X)&minus;1
</p>
<p>[ITo +Xo(X
&prime;X)&minus;1X &prime;o]
</p>
<p>]
(7.25)
</p>
<p>and the off-diagonal elements are of no interest. This means that the regression package gives
the estimated variance of β̂ and the estimated variance of the forecast error in one stroke. Note
that if the forecasts rather than the forecast errors are needed, one can replace yo by zero, and
ITo by &minus;ITo in (7.23). The resulting estimate of γ will be ŷo = Xoβ̂, as required. The variance
of this forecast will be the same as that given in (7.25), see problem 7.</p>
<p/>
</div>
<div class="page"><p/>
<p>160 Chapter 7: The General Linear Model: The Basics
</p>
<p>7.6 Confidence Intervals and Test of Hypotheses
</p>
<p>We start by constructing a confidence interval for any linear combination of β, say c&prime;β. We
know that c&prime;β̂OLS &sim; N(c
</p>
<p>&prime;β, σ2c&prime;(X &prime;X)&minus;1c) and it is a scalar. Hence,
</p>
<p>zobs = (c
&prime;β̂OLS &minus; c&prime;β)/σ(c&prime;(X &prime;X)&minus;1c)1/2 (7.26)
</p>
<p>is a standardized N(0, 1) random variable. Replacing σ by s is equivalent to dividing zobs by
the square root of a χ2 random variable divided by its degrees of freedom. The latter random
variable is (n&minus; k)s2/σ2 = RSS/σ2 which was shown to be a χ2n&minus;k. Problem 8 shows that zobs
and RSS/σ2 are independent. This means that
</p>
<p>tobs = (c
&prime;β̂OLS &minus; c&prime;β)/s(c&prime;(X &prime;X)&minus;1c)1/2 (7.27)
</p>
<p>is a N(0, 1) random variable divided by the square root of an independent χ2n&minus;k/(n&minus; k). This
is a t-statistic with (n&minus; k) degrees of freedom. Hence, a 100(1&minus;α)% confidence interval for c&prime;β
is
</p>
<p>c&prime;β̂OLS &plusmn; tα/2s(c&prime;(X &prime;X)&minus;1c)1/2 (7.28)
</p>
<p>Example: Let us say we are predicting one year ahead so that To = 1 and xo is a (1&times; k) vector
of next year&rsquo;s observations on the exogenous variables. The 100(1 &minus; α) confidence interval for
next year&rsquo;s forecast of yo will be ŷo &plusmn; tα/2s(1 + x&prime;o(X &prime;X)&minus;1xo)1/2. Similarly (7.28) allows us to
construct confidence intervals or test any single hypothesis on any single βj (again by picking
c to have 1 in its j-th position and zero elsewhere). In this case we get the usual t-statistic
reported in any regression package. More importantly, this allows us to test any hypothesis
concerning any linear combination of the β&rsquo;s, e.g., testing that the sum of coefficients of input
variables in a Cobb-Douglas production function is equal to one. This is known as a test for
constant returns to scale, see Chapter 4.
</p>
<p>7.7 Joint Confidence Intervals and Test of Hypotheses
</p>
<p>We have learned how to test any single hypothesis involving any linear combination of the
β&rsquo;s. But what if we are interested in testing two or three or more hypotheses involving linear
combinations of the β&rsquo;s. For example, testing that β2 = β4 = 0, i.e., that variables X2 and X4
are not significant in the model. This can be written as c&prime;2β = c
</p>
<p>&prime;
4β = 0 where c
</p>
<p>&prime;
j is a row vector
</p>
<p>of zeros with a one in the j-th position. In order to test these two hypotheses simultaneously,
we rearrange these restrictions on the β&rsquo;s in matrix form Rβ = 0 where R&prime; = [c2, c4]. In a
similar fashion, we can rearrange g restrictions on the β&rsquo;s into this matrix R which will now be
of dimension (g &times; k). Also these restrictions need not be of the form Rβ = 0 and can be of the
more general form Rβ = r where r is a (g &times; 1) vector of constants. For example, β1 + β2 = 1
and 3β3 + 2β4 = 5 are two such restrictions. Since Rβ is a collection of linear combinations
of the β&rsquo;s, the BLUE of these is Rβ̂OLS and the latter is distributed N(Rβ, σ
</p>
<p>2R(X &prime;X)&minus;1R&prime;).
Standardization of the form encountered with the scalar c&prime;β gives us the following:
</p>
<p>(Rβ̂OLS &minus;Rβ)&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus;Rβ)/σ2 (7.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.8 Restricted MLE and Restricted Least Squares 161
</p>
<p>rather than divide by the variance we multiply by its inverse, and since we divided by the
variance rather than the standard deviation we square the numerator which means in vector
form premultiplying by its transpose. Problem 9 replaces the matrix R by the vector c&prime; and
shows that (7.29) reduces to the square of the z-statistic observed in (7.26). This also proves
that the resulting statistic is distributed as χ21. But, what is the distribution of (7.29)? The
trick is to write it in terms of the original disturbances, i.e.,
</p>
<p>u&prime;X(X &prime;X)&minus;1R&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1R(X &prime;X)&minus;1X &prime;u/σ2 (7.30)
</p>
<p>where (Rβ̂OLS &minus; Rβ) is replaced by R(X &prime;X)&minus;1X &prime;u. Note that (7.30) is quadratic in the dis-
turbances u of the form u&prime;Au/σ2. Problem 10 shows that A is symmetric and idempotent and
of rank g. Applying the same proof as given below lemma 1 we get the result that (7.30) is
distributed as χ2g. Again σ
</p>
<p>2 is unobserved, so we divide by (n &minus; k)s2/σ2 which is χ2n&minus;k. This
becomes a ratio of two χ2&rsquo;s random variables. If we divide the numerator and denominator χ2&rsquo;s
by their respective degrees of freedom and prove that they are independent (see problem 11)
the resulting statistic
</p>
<p>(Rβ̂OLS &minus; r)&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r)/gs2 (7.31)
is distributed under the null Rβ = r as an F (g, n&minus; k).
</p>
<p>7.8 Restricted MLE and Restricted Least Squares
</p>
<p>Maximizing the likelihood function given in (7.16) subject to Rβ = r is equivalent to minimizing
the residual sum of squares subject to Rβ = r. Forming the Lagrangian function
</p>
<p>Ψ(β, μ) = (y &minus;Xβ)&prime;(y &minus;Xβ) + 2μ&prime;(Rβ &minus; r) (7.32)
and differentiating with respect to β and μ one gets
</p>
<p>&part;Ψ(β, μ)/&part;β = &minus;2X &prime;y + 2X &prime;Xβ + 2R&prime;μ = 0 (7.33)
&part;Ψ(β, μ)/&part;μ = 2(Rβ &minus; r) = 0 (7.34)
</p>
<p>Solving for μ, we premultiply (7.33) by R(X &prime;X)&minus;1 and use (7.34)
</p>
<p>μ̂ = [R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r) (7.35)
Substituting (7.35) in (7.33) we get
</p>
<p>β̂RLS = β̂OLS &minus; (X &prime;X)&minus;1R&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r) (7.36)
The restricted least squares estimator of β differs from that of the unrestricted OLS estimator
by the second term in (7.36) with the term in parentheses showing the extent to which the
unrestricted OLS estimator satisfies the constraint. Problem 12 shows that β̂RLS is biased
unless the restriction Rβ = r is satisfied. However, its variance is always less than that of β̂OLS .
This brings in the trade-off between bias and variance and the MSE criteria which was discussed
in Chapter 2.
The Lagrange Multiplier estimator μ̂ is distributed N(0, σ2[R(X &prime;X)&minus;1R&prime;]&minus;1) under the null
</p>
<p>hypothesis. Therefore, to test μ = 0, we use
</p>
<p>μ̂&prime;[R(X &prime;X)&minus;1R&prime;]μ̂/σ2 = (Rβ̂OLS &minus; r)&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r)/σ2 (7.37)
Since μ measures the cost of imposing the restriction Rβ = r, it is no surprise that the right
hand side of (7.37) was already encountered in (7.29) and is distributed as χ2g.</p>
<p/>
</div>
<div class="page"><p/>
<p>162 Chapter 7: The General Linear Model: The Basics
</p>
<p>7.9 Likelihood Ratio, Wald and Lagrange Multiplier Tests
</p>
<p>Before we go into the derivations of these three classical tests for the null hypothesis H0;Rβ = r,
it is important for the reader to review the intuitive graphical explanation of these tests given
in Chapter 2.
The Likelihood Ratio test of H0;Rβ = r is based upon the ratio λ = maxℓr/maxℓu, where
</p>
<p>maxℓu and maxℓr are the maximum values of the unrestricted and restricted likelihoods, re-
spectively. Let us assume for simplicity that σ2 is known, then
</p>
<p>maxℓu = (1/2πσ
2)n/2exp{&minus;(y &minus;Xβ̂MLE)&prime;(y &minus;Xβ̂MLE)/2σ2}
</p>
<p>where β̂MLE = β̂OLS . Denoting the unrestricted residual sum of squares by URSS, we have
</p>
<p>maxℓu = (1/2πσ
2)n/2exp{&minus;URSS/2σ2}
</p>
<p>Similarly, maxℓr is given by
</p>
<p>maxℓr = (1/2πσ
2)n/2exp{&minus;(y &minus;Xβ̂RMLE)&prime;(y &minus;Xβ̂RMLE)/2σ2}
</p>
<p>where β̂RMLE = β̂RLS . Denoting the restricted residual sum of squares by RRSS, we have
</p>
<p>maxℓr = (1/2πσ
2)n/2exp{&minus;RRSS/2σ2}
</p>
<p>Therefore, &minus;2logλ = (RRSS &minus; URSS )/σ2. Let us find the relationship between these residual
sums of squares.
</p>
<p>er = y &minus;Xβ̂RLS = y &minus;Xβ̂OLS &minus;X(β̂RLS &minus; β̂OLS) = e&minus;X(β̂RLS &minus; β̂OLS) (7.38)
e&prime;rer = e
</p>
<p>&prime;e+ (β̂RLS &minus; β̂OLS)&prime;X &prime;X(β̂RLS &minus; β̂OLS)
where er denotes the restricted residuals and e
</p>
<p>&prime;
rer the RRSS. The cross-product terms drop out
</p>
<p>because X &prime;e = 0. Substituting the value of (β̂RLS &minus; β̂OLS) from (7.36) into (7.38), we get:
</p>
<p>RRSS &minus;URSS = (Rβ̂OLS &minus; r)&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r) (7.39)
</p>
<p>It is now clear that &minus;2logλ is the right hand side of (7.39) divided by σ2. In fact, this Likelihood
Ratio (LR) statistic is the same as that given in (7.37) and (7.29). Under the null hypothesis
Rβ = r , this was shown to be a χ2g.
The Wald test of Rβ = r is based upon the unrestricted estimator and the extent of which it
</p>
<p>satisfies the restriction. More formally, if r(β) = 0 denote the vector of g restrictions on β and
R(β̂MLE) denotes the (g &times; k) matrix of partial derivatives &part;r(β)/&part;β&prime; evaluated at β̂MLE , then
the Wald statistic is given by
</p>
<p>W = r(β̂MLE)
&prime;[R(β̂MLE)I(β̂MLE)
</p>
<p>&minus;1R(β̂MLE)
&prime;]&minus;1r(β̂MLE) (7.40)
</p>
<p>where I(β) = &minus;E(&part;2logL/&part;β&part;β&prime;). In this case, r(β) = Rβ &minus; r,R(β̂MLE) = R and I(β̂MLE) =
(X &prime;X)/σ2 as seen in (7.19). Therefore,
</p>
<p>W = (Rβ̂MLE &minus; r)&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂MLE &minus; r)/σ2 (7.41)
</p>
<p>which is the same as the LR statistic5.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9 Likelihood Ratio, Wald and Lagrange Multiplier Tests 163
</p>
<p>The Lagrange Multiplier test is based upon the restricted estimator. In section 7.8, we derived
the restricted estimator and the estimated Lagrange Multiplier μ̂. The Lagrange Multiplier μ is
the cost or shadow price of imposing the restrictions Rβ = r. If these restrictions are true, one
would expect the estimated Lagrange Multiplier μ̂ to have mean zero. Therefore, a test for the
null hypothesis that μ = 0, is called the LM test and the corresponding test statistic is given
in equation (7.37). Alternatively, one can derive the LM test as a score&rsquo; test based on the score
or the first derivative of the log-likelihood function i.e., S(β) = &part;logL/&part;β. The score is zero for
the unrestricted MLE, and the score test is based upon the departure of S(β), evaluated at the
restricted estimator β̂RMLE , from zero. In this case, the score form of the LM statistic is given
by
</p>
<p>LM = S(β̂RMLE)
&prime;I(β̂RMLE)
</p>
<p>&minus;1S(β̂RMLE) (7.42)
</p>
<p>For our model, S(β) = (X &prime;y &minus;X &prime;Xβ)/σ2 and from equation (7.36) we have
S(β̂RMLE) = X
</p>
<p>&prime;(y &minus;Xβ̂RMLE)/σ2
</p>
<p>= {X &prime;y &minus;X &prime;Xβ̂OLS +R&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r)}/σ2
</p>
<p>= R&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r)/σ2
</p>
<p>Using (7.20), one gets I&minus;1(β̂RMLE) = σ
2(X &prime;X)&minus;1. Therefore, the score form of the LM test
</p>
<p>becomes
</p>
<p>LM = (Rβ̂OLS &minus; r)&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1R(X &prime;X)&minus;1R&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r)/σ2
</p>
<p>= (Rβ̂OLS &minus; r)&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1(Rβ̂OLS &minus; r)/σ2 (7.43)
This is numerically identical to the LM test derived in equation (7.37) and to the W and LR
statistics derived above. Note that S(β̂RMLE) = R
</p>
<p>&prime;μ̂/σ2 from (7.35), so it is clear why the Score
and the Lagrangian Multiplier tests are identical.
</p>
<p>The score form of the LM test can also be obtained as a by-product of an artificial regression.
In fact, S(β) evaluated as β̂RMLE is given by
</p>
<p>S(β̂RMLE) = X
&prime;(y &minus;Xβ̂RMLE)/σ2
</p>
<p>where y &minus; Xβ̂RMLE is the vector of restricted residuals. If H0 is true, then this converges
asymptotically to u and the asymptotic variance of the vector of scores becomes (X &prime;X)/σ2.
The score test is then based upon
</p>
<p>(y &minus;Xβ̂RMLE)&prime;X(X &prime;X)&minus;1X &prime;(y &minus;Xβ̂RMLE)/σ2 (7.44)
</p>
<p>This expression is the explained sum of squares from the artificial regression of (y&minus;Xβ̂RMLE)/σ
on X. To see that this is exactly identical to the LM test in equation (7.37), recall from equation
(7.33) that R&prime;μ̂ = X &prime;(y &minus;Xβ̂RMLE) and substituting this expression for R&prime;μ̂ on the left hand
side of equation (7.37) we get equation (7.44). In practice, σ2 is estimated by s̃2 the Mean
Square Error of the restricted regression. This is an example of the Gauss-Newton Regression
which will be discussed in Chapter 8.
</p>
<p>An alternative approach to testing H0, is to estimate the restricted and unrestricted models
and compute the following F -statistic
</p>
<p>Fobs =
(RRSS &minus;URSS )/g
</p>
<p>URSS/(n&minus; k) (7.45)</p>
<p/>
</div>
<div class="page"><p/>
<p>164 Chapter 7: The General Linear Model: The Basics
</p>
<p>This statistic is known in the econometric literature as the Chow (1960) test and was encoun-
tered in Chapter 4. Note that from equation (7.39), if we divide the numerator by σ2 we get a
χ2g statistic divided by its degrees of freedom. Also, using the fact that (n &minus; k)s2/σ2 is χ2n&minus;k,
the denominator divided by σ2 is a χ2n&minus;k statistic divided by its degrees of freedom. Problem
11 shows independence of the numerator and denominator and completes the proof that Fobs is
distributed F (g, n&minus; k) under H0.
</p>
<p>Chow&rsquo;s (1960) Test for Regression Stability
</p>
<p>Chow (1960) considered the problem of testing the equality of two sets of regression coefficients
</p>
<p>y1 = X1β1 + u1 and y2 = X2β2 + u2 (7.46)
</p>
<p>where X1 is n1&times;k and X2 is n2&times;k with n1 and n2 &gt; k. In this case, the unrestricted regression
can be written as
</p>
<p>[
y1
y2
</p>
<p>]
=
</p>
<p>[
X1 0
0 X2
</p>
<p>] [
β1
β2
</p>
<p>]
+
</p>
<p>[
u1
u2
</p>
<p>]
(7.47)
</p>
<p>under the null hypothesis H0;β1 = β2 = β, the restricted model becomes
</p>
<p>[
y1
y2
</p>
<p>]
=
</p>
<p>[
X1
X2
</p>
<p>]
β +
</p>
<p>[
u1
u2
</p>
<p>]
(7.48)
</p>
<p>The URSS and the RRSS are obtained from these two regressions by stacking the n1 + n2
observations. It is easy to show that the URSS= e&prime;1e1+ e
</p>
<p>&prime;
2e2 where e1 is the OLS residuals from
</p>
<p>y1 on X1 and e2 is the OLS residuals from y2 on X2. In other words, the URSS is the sum of two
residual sums of squares from the separate regressions, see problem 13. The Chow F -statistic
given in equation (7.45) has k and (n1+n2&minus;2k) degrees of freedom, respectively. Equivalently,
one can obtain this Chow F -statistic from running
</p>
<p>[
y1
y2
</p>
<p>]
=
</p>
<p>[
X1
X2
</p>
<p>]
β1 +
</p>
<p>[
0
X2
</p>
<p>]
(β2 &minus; β1) +
</p>
<p>[
u1
u2
</p>
<p>]
(7.49)
</p>
<p>Note that the second set of explanatory variables whose coefficients are (β2&minus;β1) are interaction
variables obtained by multiplying each independent variable in equation (7.48) by a dummy
variable, say D2, that takes on the value 1 if the observation is from the second regression and
0 if it is from the first regression. A test for H0; β1 = β2 becomes a joint test of significance
for the coefficients of these interaction variables. Gujarati (1970) points out that this dummy
variable approach has the additional advantage of giving the estimates of (β2 &minus; β1) and their
t-statistics. If the Chow F -test rejects stability, these individual interaction dummy variable
coefficients may point to the source of instability. Of course, one has to be careful with the
interpretation of these individual t-statistics, after all they can all be insignificant with the joint
F -statistic still being significant, see Maddala (1992).
In case one of the two regressions does not have sufficient observations to estimate a separate
</p>
<p>regression say n2 &lt; k, then one can proceed by running the regression on the full data set to
get the RRSS. This is the restricted model because the extra n2 observations are assumed to be
generated by the same regression as the first n1 observations. The URSS is the residual sums</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9 Likelihood Ratio, Wald and Lagrange Multiplier Tests 165
</p>
<p>of squares based only on the longer period (n1 observations). In this case, the Chow F -statistic
given in equation (7.45) has n2 and n1 &minus; k degrees of freedom, respectively. This is known
as Chow&rsquo;s predictive test since it tests whether the shorter n2 observations are different from
their predictions using the model with the longer n1 observations. This predictive test can be
performed with dummy variables as follows: Introduce n2 observation specific dummies, one for
each of the observations in the second regression. Test the joint significance of these n2 dummy
variables. Salkever&rsquo;s (1976) result applies and each dummy variable will have as its estimated
coefficient the prediction error with its corresponding standard error and its t-statistic. Once,
again, the individual dummies may point out possible outliers, but it is their joint significance
that is under question.
</p>
<p>The W, LR and LM Inequality
</p>
<p>We have shown that the LR = W = LM for linear restrictions if the log-likelihood is quadratic.
However, this is not necessarily the case for more general situations. In fact, in the next chapter
where we consider more general variance covariance structure on the disturbances, estimating
this variance-covariance matrix destroys this equality and may lead to conflict in hypotheses
testing as noted by Berndt and Savin (1977). In this case, W &ge; LR &ge; LM . See also the
problems at the end of this chapter. The LR, W and LM tests are based on the efficient MLE.
When consistent rather than efficient estimators are used, an alternative way of constructing
the score-type test is known as Neyman&rsquo;s C(α). For details, see Bera and Permaratne (2001).
Although, these three tests are asymptotically equivalent, one test may be more convenient
</p>
<p>than another for a particular problem. For example, when the model is linear but the restriction
is nonlinear, the unrestricted model is easier to estimate than the restricted model. So the Wald
test suggests itself in that it relies only on the unrestricted estimator. Unfortunately, the Wald
test has a drawback that the LR and LM test do not have. In finite samples, the Wald test is not
invariant to testing two algebraically equivalent formulations of the nonlinear restriction. This
fact has been pointed out in the econometric literature by Gregory and Veall (1985, 1986) and
Lafontaine and White (1986). In what follows, we review some of Gregory and Veall&rsquo;s (1985)
findings:
</p>
<p>Consider the linear regression with two regressors
</p>
<p>yt = βo + β1x1t + β2x2t + ut (7.50)
</p>
<p>where the ut&rsquo;s are IIN(0, σ
2), and the nonlinear restriction β1β2 = 1. Two algebraically equiv-
</p>
<p>alent formulation of the null hypothesis are: HA; rA(β) = β1 &minus; 1/β2 = 0, and HB; rB(β) =
β1β2 &minus; 1 = 0. The unrestricted maximum likelihood estimator is β̂OLS and the Wald statistic
given in (7.40) is
</p>
<p>W = r(β̂OLS)
&prime;[R(β̂OLS)V̂ (β̂OLS)R
</p>
<p>&prime;(β̂OLS)]
&minus;1r(β̂OLS) (7.51)
</p>
<p>where V̂ (β̂OLS) is the usual estimated variance-covariance matrix of β̂OLS . Problem 19 asks the
reader to verify that the Wald statistics corresponding to HA and HB using (7.51) are
</p>
<p>WA = (β̂1β̂2 &minus; 1)2/(β̂
2
</p>
<p>2v11 + 2v12 + v22/β̂
2
</p>
<p>2) (7.52)
</p>
<p>and
</p>
<p>WB = (β̂1β̂2 &minus; 1)2/(β̂
2
</p>
<p>2v11 + 2β̂1β̂2v12 + β̂
2
</p>
<p>1v22) (7.53)</p>
<p/>
</div>
<div class="page"><p/>
<p>166 Chapter 7: The General Linear Model: The Basics
</p>
<p>where the vij &rsquo;s are the elements of V̂ (β̂OLS) for i, j = 0, 1, 2. These Wald statistics are clearly not
identical, and other algebraically equivalent formulations of the null hypothesis can be generated
with correspondingly different Wald statistics. Monte Carlo experiments were performed with
1000 replications on the model given in (7.50) with various values for β1 and β2, and for a
sample size n = 20, 30, 50, 100, 500. The experiments were run when the null hypothesis is true
and when it is false. For n = 20 and β1 = 10, β2 = 0.1, so that H0 is satisfied, W
</p>
<p>A rejects the
null when it is true 293 times out of a 1000, while WB rejects the null 65 times out of a 1000. At
the 5% level one would expect 50 rejections with a 95% confidence interval [36, 64]. Both WA
</p>
<p>and WB reject too often but WA performs worse than WB. When n is increased to 500, WA
</p>
<p>rejects 78 times while WB rejects 39 times out of a 1000. WA still rejects too often although
its performance is better than that for n = 20, while WB performs well and is within the 95%
confidence region. When n = 20, β1 = 1 and β2 = 0.5, so that H0 is not satisfied, W
</p>
<p>A rejects
the null when it is false 65 times out of a 1000 whereas WB rejects it 584 times out of a 1000. For
n = 500, both test statistics reject the null in 1000 out of 1000 times. Even in cases where the
empirical sizes of the tests appear similar, see Table 1 of Gregory and Veall (1985), in particular
the case where β1 = β2 = 1, Gregory and Veall find that W
</p>
<p>A and WB are in conflict about 5%
of the time for n = 20, and this conflict drops to 0.5% at n = 500. Problem 20 asks the reader
to derive four Wald statistics corresponding to four algebraically equivalent formulations of the
common factor restriction analyzed by Hendry and Mizon (1978). Gregory and Veall (1986)
give Monte Carlo results on the performance of these Wald statistics for various sample sizes.
Once again they find conflict among these tests even when their empirical sizes appear to be
similar. Also, the differences among the Wald statistics are much more substantial, and persist
even when n is as large as 500.
</p>
<p>Lafontaine and White (1985) consider a simple regression
</p>
<p>y = α+ βx+ γz + u
</p>
<p>where y is log of per capita consumption of textiles, x is log of per capita real income and z is
log of relative prices of textiles, with the data taken from Theil (1971, p. 102). The estimated
equation is:
</p>
<p>ŷ = 1.37
</p>
<p>(0.31)
</p>
<p>+ 1.14x
</p>
<p>(0.16)
</p>
<p>&minus; 0.83z
(0.04)
</p>
<p>with σ̂2 = 0.0001833, and n = 17, with standard errors shown in parentheses. Consider the
null hypothesis H0; β = 1. Algebraically equivalent formulations of H0 are Hk; β
</p>
<p>k = 1 for any
exponent k. Applying (7.40) with r(β) = βk &minus; 1 and R(β) = kβk&minus;1, one gets the Wald statistic
</p>
<p>Wk = (β̂
k &minus; 1)2/[(kβ̂k&minus;1)2V (β̂)] (7.54)
</p>
<p>where β̂ is the OLS estimate of β and V (β̂)is its corresponding estimated variance. For every k,
Wk has a limiting χ
</p>
<p>2
1 distribution under H0. The critical values are χ
</p>
<p>2
1,.05 = 3.84 and F
</p>
<p>.05
1,14 = 4.6.
</p>
<p>The latter is an exact distribution test for β = 1 under H0. Lafontaine and White (1985) try
different integer exponents (&plusmn;k) where k = 1, 2, 3, 6, 10, 20, 40. Using β̂ = 1.14 and V (β̂) =
(0.16)2 one gets W&minus;20 = 24.56, W1 = 0.84, and W20 = 0.12. The authors conclude that one
could get any Wald statistic desired by choosing an appropriate exponent. Since β &gt; 1, Wk is
inversely related to k. So, we can find a Wk that exceeds the critical values given by the χ
</p>
<p>2 and
F distributions. In fact, W&minus;20 leads to rejection whereas W1 and W20 do not reject H0.</p>
<p/>
</div>
<div class="page"><p/>
<p>167
</p>
<p>For testing nonlinear restrictions, the Wald test is easy to compute. However, it has a serious
problem in that it is not invariant to the way the null hypothesis is formulated. In this case,
the score test may be difficult to compute, but Neyman&rsquo;s C(α) test is convenient to use and
provide the invariance that is needed, see Dagenais and Dufour (1991).
</p>
<p>Notes
</p>
<p>1. For example, in a time-series setting, including the time trend in the multiple regression is equiv-
alent to detrending each variable first, by residualing out the effect of time, and then running the
regression on these residuals.
</p>
<p>2. Two exceptions noted in Davidson and MacKinnon (1993) are the following: One, if the model
is not identified asymptotically. For example, yt = β(1/t) + ut for t = 1, 2, . . . , T , will have (1/t)
tend to zero as T &rarr; &infin;. This means that as the sample size increase, there is no information on β.
Two, if the number of parameters in the model increase as the sample size increase. For example,
the fixed effects model in panel data discussed in Chapter 12.
</p>
<p>3. If the MLE of β is β̂MLE , then the MLE of (1/β) is (1/β̂MLE). Note that this invariance property
</p>
<p>implies that MLE cannot be in general unbiased. For example, even if β̂MLE is unbiased for β, by
</p>
<p>the above reparameterization, (1/β̂MLE) is not unbiased for (1/β).
</p>
<p>4. If the distribution of disturbances is not normal, then OLS is still BLUE as long as the assumptions
underlying the Gauss-Markov Theorem are satisfied. The MLE in this case will be in general more
efficient than OLS as long as the distribution of the errors is correctly specified.
</p>
<p>5. Using the Taylor Series approximation of r(β̂MLE) around the true parameter vector β, one gets
</p>
<p>r(β̂MLE) ≃ r(β)+R(β)(β̂MLE &minus;β). Under the null hypothesis, r(β) = 0 and the var[r(β̂MLE)] ≃
R(β) var(β̂MLE)R
</p>
<p>&prime;(β).
</p>
<p>Problems
</p>
<p>1. Invariance of the Fitted Values and Residuals to Nonsingular Transformations of the Independent
Variables. Post-multiply the independent variables in (7.1) by a nonsingular transformation C, so
that X&lowast; = XC.
</p>
<p>(a) Show that PX&lowast; = PX and P̄X&lowast; = P̄X . Conclude that the regression of y on X has the same
fitted values and the same residuals as the regression of y on X&lowast;.
</p>
<p>(b) As an application of these results, suppose that every X was multiplied by a constant, say,
a change in the units of measurement. Would the fitted values or residuals change when we
rerun this regression?
</p>
<p>(c) Suppose that X contains two regressors X1 and X2 each of dimension n &times; 1. If we run the
regression of y on (X1 &minus;X2) and (X1 +X2), will this yield the same fitted values and the
same residuals as the original regression of y on X1 and X2?
</p>
<p>2. The FWL Theorem.
</p>
<p>(a) Using partitioned inverse results from the Appendix, show that the solution to (7.9) yields
</p>
<p>β̂2,OLS given in (7.10).
</p>
<p>Problems</p>
<p/>
</div>
<div class="page"><p/>
<p>168 Chapter 7: The General Linear Model: The Basics
</p>
<p>(b) Alternatively, write (7.9) as a system of two equations in two unknowns β̂1,OLS and β̂2,OLS .
</p>
<p>Solve, by eliminating β̂1,OLS and show that the resulting solution is given by (7.10).
</p>
<p>(c) Using the FWL Theorem, show that if X1 = ιn a vector of ones indicating the presence of
</p>
<p>the constant in the regression, and X2 is a set of economic variables, then (i) β̂2,OLS can
be obtained by running yi &minus; ȳ on the set of variables in X2 expressed as deviations from
their respective sample means. (ii) The least squares estimate of the constant β̂1,OLS can
</p>
<p>be retrieved as ȳ &minus; X̄ &prime;2β̂2,OLS where X̄ &prime;2 = ι&prime;nX2/n is the vector of sample means of the
independent variables in X2.
</p>
<p>3. Let y = Xβ+Diγ+u where y is n&times;1, X is n&times;k and Di is a dummy variable that takes the value
1 for the i-th observation and 0 otherwise. Using the FWL Theorem, prove that the least squares
estimates of β and γ from this regression are β̂OLS = (X
</p>
<p>&lowast;&prime;X&lowast;)&minus;1X&lowast;&prime;y&lowast; and γ̂OLS = yi &minus; x&prime;iβ̂OLS ,
where X&lowast; denotes the X matrix without the i-th observation and y&lowast; is the y vector without
the i-th observation and (yi, x
</p>
<p>&prime;
i) denotes the i-th observation on the dependent and independent
</p>
<p>variables. This means that γ̂OLS is the forecasted OLS residual from the regression of y
&lowast; on X&lowast;
</p>
<p>for the i-th observation which was essentially excluded from the regression by the inclusion of the
dummy variable Di.
</p>
<p>4. Maximum Likelihood Estimation. Given the log-likelihood in (7.17),
</p>
<p>(a) Derive the first-order conditions for maximization and show that β̂MLE = β̂OLS and that
σ̂2MLE = RSS/n.
</p>
<p>(b) Calculate the second derivatives given in (7.18) and verify that the information matrix re-
duces to (7.19).
</p>
<p>5. Given that u &sim; N(0, σ2In), we showed that (n&minus; k)s2/σ2 &sim; χ2n&minus;k. Use this fact to prove that,
</p>
<p>(a) s2 is unbiased for σ2.
</p>
<p>(b) var(s2) = 2σ4/(n&minus; k). Hint: E(χ2r) = r and var(χ2r) = 2r.
</p>
<p>6. Consider all estimators of σ2 of the type σ̃2 = e&prime;e/r = u&prime;P̄Xu/r with u &sim; N(0, σ2In).
</p>
<p>(a) Find E(σ̂2MLE) and the bias(σ̂
2
MLE).
</p>
<p>(b) Find var(σ̂2MLE) and the MSE(σ̂
2
MLE).
</p>
<p>(c) Compute MSE(σ̃2) and minimize it with respect to r. Compare with the MSE of s2 and
σ̂2MLE .
</p>
<p>7. Computing Forecasts and Forecast Standard Errors Using a Regression Package. This is based on
Salkever (1976). From equations (7.23) and (7.24), show that
</p>
<p>(a) δ̂
&prime;
OLS = (β̂
</p>
<p>&prime;
OLS , γ̂
</p>
<p>&prime;
OLS) where β̂OLS = (X
</p>
<p>&prime;X)&minus;1X &prime;y, and γ̂OLS = yo&minus;Xoβ̂OLS . Hint: Set up
the OLS normal equations and solve two equations in two unknowns. Alternatively, one can
use the FWL Theorem to residual out the additional To dummy variables.
</p>
<p>(b) e&lowast;OLS = (e
&prime;
OLS , 0
</p>
<p>&prime;)&prime; and s&lowast;2 = s2.
</p>
<p>(c) s&lowast;2(X&lowast;&prime;X&lowast;)&minus;1 is given by the expression in (7.25). Hint: Use partitioned inverse.
</p>
<p>(d) Replace yo by 0 and ITo by &minus;ITo in (7.23) and show that γ̂ = ŷo = Xoβ̂OLS whereas all the
results in parts (a), (b) and (c) remain the same.
</p>
<p>8. (a) Show that cov(β̂OLS , e) = 0. (Since both random variables are normally distributed, this
proves their independence).</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 169
</p>
<p>(b) Show that β̂OLS and s
2 are independent. Hint: A linear (Bu) and quadratic (u&prime;Au) forms
</p>
<p>in normal random variables are independent if BA = 0. See Graybill (1961) Theorem 4.17.
</p>
<p>9. (a) Show that if one replaces R by c&prime; in (7.29) one gets the square of the z-statistic given in
(7.26).
</p>
<p>(b) Show that when we replace σ2 by s2, the χ21 statistic given in part (a) becomes the square
of a t-statistic which is distributed as F (1, n&minus;K). Hint: The square of a N(0, 1) is χ21. Also
the ratio of two independent χ2 random variables divided by their degrees of freedom is an
F -statistic with these corresponding degrees of freedom, see Chapter 2.
</p>
<p>10. (a) Show that the matrix A defined in (7.30) by u&prime;Au/σ2 is symmetric, idempotent and of rank
g.
</p>
<p>(b) Using the same proof given below lemma 1, show that (7.30) is χ2g.
</p>
<p>11. (a) Show that the two quadratic forms s2 = u&prime;P̄Xu/(n &minus; k) and that given in (7.30) are inde-
pendent. Hint: Two positive semi-definite quadratic forms u&prime;Au and u&prime;Bu are independent
if and only if AB = 0, see Graybill (1961) Theorem 4.10.
</p>
<p>(b) Conclude that (7.31) is distributed as an F (g, n&minus; k).
</p>
<p>12. Restricted Least Squares.
</p>
<p>(a) Show that β̂RLS given by (7.36) is biased unless Rβ = r.
</p>
<p>(b) Show that the var(β̂RLS) = var(A(X
&prime;X)&minus;1X &prime;u) where
</p>
<p>A = IK &minus; (X &prime;X)&minus;1R&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1R.
</p>
<p>Prove that A2 = A, but A&prime; 	= A. Conclude that
var(β̂RLS) = σ
</p>
<p>2A(X &prime;X)&minus;1A&prime; = σ2{(X &prime;X)&minus;1
</p>
<p>&minus;(X &prime;X)&minus;1R&prime;[R(X &prime;X)&minus;1R&prime;]&minus;1R(X &prime;X)&minus;1}.
</p>
<p>(c) Show that var(β̂OLS)&minus; var(β̂RLS) is a positive semi-definite matrix.
</p>
<p>13. The Chow Test.
</p>
<p>(a) Show that OLS on (7.47) yields OLS on each equation separately in (7.46). In other words,
</p>
<p>β̂1,OLS = (X
&prime;
1X1)
</p>
<p>&minus;1X &prime;1y1 and β̂2,OLS = (X
&prime;
2X2)
</p>
<p>&minus;1X &prime;2y2.
</p>
<p>(b) Show that the residual sum of squares for equation (7.47) is given by RSS1 + RSS2, where
RSSi is the residual sum of squares from running yi on Xi for i = 1, 2.
</p>
<p>(c) Show that the Chow F -statistic can be obtained from (7.49) by testing for the joint signifi-
cance of Ho; β2 &minus; β1 = 0.
</p>
<p>14. Suppose we would like to test Ho; β2 = 0 in the following unrestricted model given also in (7.8)
</p>
<p>y = Xβ + u = X1β1 +X2β2 + u
</p>
<p>(a) Using the FWL Theorem, show that the URSS is identical to the residual sum of squares
obtained from P̄X1y = P̄X1X2β2 + P̄X1u. Conclude that
</p>
<p>URSS = y&prime;P̄Xy = y
&prime;P̄X1y &minus; y&prime;P̄X1X2(X &prime;2P̄X1X2)&minus;1X &prime;2P̄X1y.
</p>
<p>(b) Show that the numerator of the F -statistic for testing Ho; β2 = 0 which is given in (7.45),
is y&prime;P̄X1X2(X
</p>
<p>&prime;
2P̄X1X2)
</p>
<p>&minus;1X &prime;2P̄X1y/k2.
</p>
<p>Substituting y = X1β1+u under the null hypothesis, show that the above expression reduces
to u&prime;P̄X1X2(X
</p>
<p>&prime;
2P̄X1X2)
</p>
<p>&minus;1X &prime;2P̄X1u/k2.</p>
<p/>
</div>
<div class="page"><p/>
<p>170 Chapter 7: The General Linear Model: The Basics
</p>
<p>(c) Let v = X &prime;2P̄X1u, show that if u &sim; IIN(0, σ
2) then v &sim; N(0, σ2X &prime;2P̄X1X2). Conclude that
</p>
<p>the numerator of the F -statistic given in part (b) when divided by σ2 can be written as
v&prime;[var(v)]&minus;1v/k2 where v&prime;[var(v)]&minus;1v is distributed as χ2k2under Ho. Hint: See the discussion
below lemma 1.
</p>
<p>(d) Using the result that (n &minus; k)s2/σ2 &sim; χ2n&minus;k where s2 is the URSS/(n &minus; k), show that the
F -statistic given by (7.45) is distributed as F (k2, n&minus; k) under Ho. Hint: You need to show
that u&prime;P̄Xu is independent of the quadratic term given in part (b), see problem 11.
</p>
<p>(e) Show that the Wald Test for Ho; β2 = 0, given in (7.41), reduces in this case to W =
</p>
<p>β̂
&prime;
2[R(X
</p>
<p>&prime;X)&minus;1R&prime;]&minus;1β̂2/s
2 were R = [0, Ik2 ], β̂2 denotes the OLS or equivalently the MLE
</p>
<p>of β2 from the unrestricted model and s
2 is the corresponding estimate of σ2 given by
</p>
<p>URSS/(n&minus; k). Using partitioned inversion or the FWL Theorem, show that the numerator
of W is k2 times the expression in part (b).
</p>
<p>(f) Show that the score form of the LM statistic, given in (7.42) and (7.44), can be obtained
as the explained sum of squares from the artificial regression of the restricted residuals
(y&minus;X1β̂1,RLS) deflated by s̃ on the matrix of regressors X. In this case, s̃2 = RRSS/(n&minus;k1)
is the Mean Square Error of the restricted regression. In other words, obtain the explained
sum of squares from regressing P̄X1y/s̃ on X1 and X2.
</p>
<p>15. Iterative Estimation in Partitioned Regression Models. This is based on Fiebig (1995). Consider the
partitioned regression model given in (7.8) and let X2 be a single regressor, call it x2 of dimension
n &times; 1 so that β2 is a scalar. Consider the following strategy for estimating β2: Estimate β1 from
the shortened regression of y on X1. Regress the residuals from this regression on x2 to yield b
</p>
<p>(1)
2 .
</p>
<p>(a) Prove that b
(1)
2 is biased.
</p>
<p>Now consider the following iterative strategy for re-estimating β2:
</p>
<p>Re-estimate β1 by regressing y &minus; x2b
(1)
2 on X1 to yield b
</p>
<p>(1)
1 . Next iterate according to the
</p>
<p>following scheme:
</p>
<p>b
(j)
1 = (X
</p>
<p>&prime;
1X1)
</p>
<p>&minus;1X &prime;1(y &minus; x2b
(j)
2 )
</p>
<p>b
(j+1)
2 = (x
</p>
<p>&prime;
2x2)
</p>
<p>&minus;1x&prime;2(y &minus;X1b
(j)
1 ), j = 1, 2, ...
</p>
<p>(b) Determine the behavior of the bias of b
(j+1)
2 as j increases.
</p>
<p>(c) Show that as j increases b
(j+1)
2 converges to the estimator of β2 obtained by running OLS
</p>
<p>on (7.8).
</p>
<p>16. Maddala (1992, pp. 120&ndash;127). Consider the simple linear regression
</p>
<p>Yi = α+ βXi + ui i = 1, 2, . . . , n.
</p>
<p>where α and β are scalars and ui &sim; IIN(0, σ
2). For Ho; β = 0,
</p>
<p>(a) Derive the Likelihood Ratio (LR) statistic and show that it can be written as nlog[1/(1&minus;r2)]
where r2 is the square of the correlation coefficient between X and y.
</p>
<p>(b) Derive the Wald (W) statistic for testing Ho; β = 0. Show that it can be written as nr
2/(1&minus;
</p>
<p>r2). This is the square of the usual t-statistic on β̂ with σ̂2MLE =
&sum;
</p>
<p>n
i=1e
</p>
<p>2
i /n used instead of
</p>
<p>s2 in estimating σ2. β̂ is the unrestricted MLE which is OLS in this case, and the ei&rsquo;s are
the usual least squares residuals.
</p>
<p>(c) Derive the Lagrange Multiplier (LM) statistic for testing Ho; β = 0. Show that it can be
</p>
<p>written as nr2. This is the square of the usual t-statistic on β̂ with σ̃2RMLE =
&sum;n
</p>
<p>i=1(Yi&minus;Y )2/n
used instead of s2 in estimating σ2. The σ̃2RMLE is restricted MLE of σ
</p>
<p>2 (i.e., imposing Ho
and maximizing the likelihood with respect to σ2).</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 171
</p>
<p>(d) Show that LM/n = (W/n)/[1 + (W/n)], and LR/n = log[1 + (W/n)]. Using the following
inequality x &ge; log(1 + x) &ge; x/(1 + x), conclude that W &ge; LR &ge; LM . Hint: Use x = W/n.
</p>
<p>(e) For the cigarette consumption data given in Table 3.2, compute the W, LR, LM for the
simple regression of logC on logP and demonstrate the above inequality given in part (d)
for testing that the price elasticity is zero?
</p>
<p>17. Engle (1984, pp. 785&ndash;786). Consider a set of T independent observations on a Bernoulli random
variable which takes on the values yt = 1 with probability θ, and yt = 0 with probability (1&minus; θ).
</p>
<p>(a) Derive the log-likelihood function, the MLE of θ, the score S(θ), and the information I(θ).
</p>
<p>(b) Compute the LR, W and LM test statistics for testing Ho; θ = θo, versus HA; θ 	= θo for
θǫ(0, 1).
</p>
<p>18. Engle (1984, pp. 787&ndash;788). Consider the linear regression model
</p>
<p>y = Xβ + u = X1β1 +X2β2 + u
</p>
<p>given in (7.8), where u &sim; N(0, σ2IT ).
</p>
<p>(a) Write down the log-likelihood function, find the MLE of β and σ2.
</p>
<p>(b) Write down the score S(β) and show that the information matrix is block-diagonal between
β and σ2.
</p>
<p>(c) Derive the W, LR and LM test statistics in order to test Ho; β1 = β
o
1, versus HA; β1 	= βo1,
</p>
<p>where β1 is say the first k1 elements of β. Show that if X = [X1, X2], then
</p>
<p>W = (βo1 &minus; β̂1)&prime;[X &prime;1P̄X2X1](βo1 &minus; β̂1)/σ̂2
</p>
<p>LM = ũ&prime;X1[X
&prime;
1P̄X2X1]
</p>
<p>&minus;1X &prime;1ũ/σ̃
2
</p>
<p>LR = T log(ũ&prime;ũ/û&prime;û)
</p>
<p>where û = y &minus;Xβ̂, ũ = y &minus;Xβ̃ and σ̂2 = û&prime;û/T , σ̃2 = ũ&prime;ũ/T . β̂ is the unrestricted MLE,
whereas β̃ is the restricted MLE.
</p>
<p>(d) Using the above results, show that
</p>
<p>W = T (ũ&prime;ũ&minus; û&prime;û)/û&prime;û
LM = T (ũ&prime;ũ&minus; û&prime;û)/ũ&prime;ũ
</p>
<p>Also, that LR = T log[1 + (W/T )]; LM = W/[1 + (W/T )]; and (T &minus; k)W/Tk1 &sim; Fk1,T&minus;k
under Ho. As in problem 16, we use the inequality x &ge; log(1 + x) &ge; x/(1 + x) to conclude
that W &ge; LR &ge; LM. Hint: Use x = W/T . However, it is important to note that all the test
statistics are monotonic functions of the F -statistic and exact tests for each would produce
identical critical regions.
</p>
<p>(e) For the cigarette consumption data given in Table 3.2, run the following regression:
</p>
<p>logC = α+ βlogP + γlogY + u
</p>
<p>compute the W, LR, LM given in part (c) for the null hypothesis Ho; β = &minus;1.
(f) Compute the Wald statistics for HAo ; β = &minus;1, HBo ; β5 = &minus;1 and HCo ; β&minus;5 = &minus;1. How do
</p>
<p>these statistics compare?
</p>
<p>19. Gregory and Veall (1985). Using equation (7.51) and the two formulations of the null hypothe-
sis HA and HB given below (7.50), verify that the Wald statistics corresponding to these two
formulations are those given in (7.52) and (7.53), respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>172 Chapter 7: The General Linear Model: The Basics
</p>
<p>20. Gregory and Veall (1986). Consider the dynamic equation
</p>
<p>yt = ρyt&minus;1 + β1xt + β2xt&minus;1 + ut
</p>
<p>where |ρ| &lt; 1, and ut &sim; NID(0, σ2). Note that for this equation to be the Cochrane-Orcutt
transformation
</p>
<p>yt &minus; ρyt&minus;1 = β1(xt &minus; ρxt&minus;1) + ut
</p>
<p>the following nonlinear restriction must be satisfied &minus;β1ρ = β2 called the common factor restric-
tion by Hendry and Mizon (1978). Now consider the following four formulations of this restriction
HA; β1ρ+ β2 = 0; H
</p>
<p>B ; β1 + (β2/ρ) = 0; H
C ; ρ+ (β2/β1) = 0 and H
</p>
<p>D; (β1ρ/β2) + 1 = 0.
</p>
<p>(a) Using equation (7.51) derive the four Wald statistics corresponding to the four formulations
of the null hypothesis.
</p>
<p>(b) Apply these four Wald statistics to the equation relating real personal consumption expen-
ditures to real disposable personal income in the U.S. over the post World War II period
1959&ndash;2007, see Table 5.3.
</p>
<p>21. Effect of Additional Regressors on R2. This problem was considered in non-matrix form in Chapter
</p>
<p>4, problem 4. Regress y on X1 which is T &times;K1 and compute SSE1. Add X2 which is T &times;K2 so
that the number of regressors in now K = K1 +K2. Regress y on X = [X1, X2] and get SSE2.
</p>
<p>Show that SSE2 &le; SSE1. Conclude that the corresponding R-squares satisfy R22 &ge; R21. Hint:
Show that PX &minus; PX1 is a positive semi-definite matrix.
</p>
<p>References
</p>
<p>Additional readings for the material covered in this chapter can be found in Davidson and MacKinnon
</p>
<p>(1993), Kelejian and Oates (1989), Maddala (1992), Fomby, Hill and Johnson (1984), Greene (1993),
</p>
<p>Johnston (1984), Judge et al. (1985) and Theil (1971). These econometrics texts were cited earlier.
</p>
<p>Other references cited in this chapter are the following:
</p>
<p>Bera A.K. and G. Permaratne (2001), &ldquo;General Hypothesis Testing,&rdquo; Chapter 2 in Baltagi, B.H. (ed.),
A Companion to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Berndt, E.R. and N.E. Savin (1977), &ldquo;Conflict Among Criteria for Testing Hypotheses in the Multivariate
Linear Regression Model,&rdquo; Econometrica, 45: 1263&ndash;1278.
</p>
<p>Buse, A.(1982), &ldquo;The Likelihood Ratio, Wald, and Lagrange Multiplier Tests: An Expository Note,&rdquo; The
American Statistician, 36: 153&ndash;157.
</p>
<p>Chow, G.C. (1960), &ldquo;Tests of Equality Between Sets of Coefficients in Two Linear Regressions,&rdquo; Econo-
metrica, 28: 591&ndash;605.
</p>
<p>Dagenais, M.G. and J. M. Dufour (1991), &ldquo;Invariance, Nonlinear Models, and Asymptotic Tests,&rdquo; Econo-
metrica, 59: 1601&ndash;1615.
</p>
<p>Engle, R.F. (1984), &ldquo;Wald, Likelihood Ratio, and Lagrange Multiplier Tests in Econometrics,&rdquo; In:
Griliches, Z. and M.D. Intrilligator (eds) Handbook of Econometrics (North-Holland: Amsterdam).
</p>
<p>Fiebig, D.G. (1995), &ldquo;Iterative Estimation in Partitioned Regression Models,&rdquo; Econometric Theory,
Problem 95.5.1, 11:1177.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 173
</p>
<p>Frisch, R., and F.V. Waugh (1933), &ldquo;Partial Time Regression as Compared with Individual Trends,&rdquo;
Econometrica, 1: 387&ndash;401.
</p>
<p>Graybill, F.A.(1961), An Introduction to Linear Statistical Models, Vol. 1 (McGraw-Hill: New York).
</p>
<p>Gregory, A.W. and M.R. Veall (1985), &ldquo;Formulating Wald Tests of Nonlinear Restrictions,&rdquo; Economet-
rica, 53: 1465&ndash;1468.
</p>
<p>Gregory, A.W. and M.R. Veall (1986), &ldquo;Wald Tests of Common Factor Restrictions,&rdquo; Economics Letters,
22: 203&ndash;208.
</p>
<p>Gujarati, D. (1970), &ldquo;Use of Dummy Variables in Testing for Equality Between Sets of Coefficients in
Two Linear Regressions: A Generalization,&rdquo; The American Statistician, 24: 50&ndash;52.
</p>
<p>Hendry, D.F. and G.E. Mizon (1978), &ldquo;Serial Correlation as a Convenient Simplification, Not as a
Nuisance: A Comment on A Study of the Demand for Money by the Bank of England,&rdquo; Economic
Journal, 88: 549&ndash;563.
</p>
<p>Lafontaine, F. and K.J. White (1986), &ldquo;Obtaining Any Wald Statistic You Want,&rdquo; Economics Letters,
21: 35&ndash;40.
</p>
<p>Lovell, M.C. (1963), &ldquo;Seasonal Adjustment of Economic Time Series,&rdquo; Journal of the American Statistical
Association, 58: 993&ndash;1010.
</p>
<p>Salkever, D. (1976), &ldquo;The Use of Dummy Variables to Compute Predictions, Prediction Errors, and
</p>
<p>Confidence Intervals,&rdquo; Journal of Econometrics, 4: 393&ndash;397.
</p>
<p>Appendix
Some Useful Matrix Properties
</p>
<p>This book assumes that the reader has encountered matrices before, and knows how to add, subtract
and multiply conformable matrices. In addition, that the reader is familiar with the transpose, trace,
rank, determinant and inverse of a matrix. Unfamiliar readers should consult standard texts like Bellman
(1970) or Searle (1982). The purpose of this Appendix is to review some useful matrix properties that are
used in the text and provide easy access to these properties. Most of these properties are given without
proof.
</p>
<p>Starting with Chapter 7, our data matrix X is organized such that it has n rows and k columns, so
that each row denotes an observation on k variables and each column denotes n observations on one
variable. This matrix is of dimension n &times; k. The rank of an n &times; k matrix is always less than or equal
to its smaller dimension. Since n &gt; k, the rank (X) &le; k. When there is no perfect multicollinearity
among the variables in X, this matrix is said to be of full column rank k. In this case, X &prime;X, the matrix
of cross-products is of dimension k &times; k. It is square, symmetric and of full rank k. This uses the fact
that the rank(X &prime;X) = rank(X) = k. Therefore, (X &prime;X) is nonsingular and the inverse (X &prime;X)&minus;1 exists.
This is needed for the computation of Ordinary Least Squares. In fact, for least squares to be feasible,
X should be of full column rank k and no variable in X should be a perfect linear combination of the
other variables in X. If we write
</p>
<p>X =
</p>
<p>⎡
⎢⎣
</p>
<p>x&prime;1
...
x&prime;n
</p>
<p>⎤
⎥⎦
</p>
<p>where x&prime;i denotes the i-th observation, in the data, then X
&prime;X =
</p>
<p>&sum;n
i=1 xix
</p>
<p>&prime;
i where xi is a column vector
</p>
<p>of dimension k &times; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>174 Chapter 7: The General Linear Model: The Basics
</p>
<p>An important and widely encountered matrix is the Identity matrix which will be denoted by In and
subscripted by its dimension n. This is a square n&times;n matrix whose diagonal elements are all equal to one
and its off diagonal elements are all equal to zero. Also, σ2In will be a familiar scalar covariance matrix,
with every diagonal element equal to σ2 reflecting homoskedasticity or equal variances (see Chapter 5),
and zero covariances or no serial correlation (see Chapter 5). Let
</p>
<p>Ω = diag[σ2i ] =
</p>
<p>⎡
⎢⎣
</p>
<p>σ21 0
. . .
</p>
<p>0 σ2n
</p>
<p>⎤
⎥⎦
</p>
<p>be an (n&times;n) diagonal matrix with the i-th diagonal element equal to σ2i for i = 1, 2, . . . , n. This matrix
will be encountered under heteroskedasticity, see Chapter 9. Note that tr(Ω) =
</p>
<p>&sum;n
i=1 σ
</p>
<p>2
i is the sum of its
</p>
<p>diagonal elements. Also, tr(In) = n and tr(σ
2In) = nσ
</p>
<p>2. Another useful matrix is the projection matrix
PX = X(X
</p>
<p>&prime;X)&minus;1X &prime; which is of dimension n&times; n. This matrix is encountered in Chapter 7. If y denotes
the n &times; 1 vector of observations on the dependent variable, then PXy generates the predicted values ŷ
from the least squares regression of y on X. This matrix PX is symmetric and idempotent. This means
that P &prime;X = PX and P
</p>
<p>2
X = PXPX = PX as can be easily verified. Some of the properties of idempotent
</p>
<p>matrices is that their rank is equal to their trace. Hence, rank(PX) = tr(PX) = tr[X(X
&prime;X)&minus;1X &prime;] =
</p>
<p>tr[X &prime;X(X &prime;X&minus;1)] = tr(Ik) = k.
Here, we used the fact that tr(ABC) = tr(CAB) = tr(BCA). In other words, the trace is unaffected by
</p>
<p>the cyclical permutation of the product. Of course, these matrices should be conformable and the product
should result in a square matrix. Note that P̄X = In &minus; PX is also a symmetric and idempotent matrix.
In this case, P̄Xy = y &minus; PXy = y &minus; ŷ = e where e denotes the least squares residuals, y &minus;Xβ̂OLS where
β̂OLS = (X
</p>
<p>&prime;X)&minus;1X &prime;y, see Chapter 7. Some properties of these projection matrices are the following:
</p>
<p>PXX = X, P̄XX = 0, P̄Xe = e and PXe = 0.
</p>
<p>In fact, X &prime;e = 0 means that the matrix X is orthogonal to the vector of least squares residuals e. Note
that X &prime;e = 0 means that X &prime;(y &minus;Xβ̂OLS) = 0 or X &prime;y = X &prime;Xβ̂OLS . These k equations are known as the
OLS normal equations and their solution yields the least squares estimates β̂OLS . By the definition of
P̄X , we have (i) PX + P̄X = In. Also, (ii) PX and P̄X are idempotent and (iii) PX P̄X = 0. In fact, any
two of these properties imply the third. The rank(P̄X) = tr(P̄X) = tr(In &minus; PX) = n&minus; k. Note that PX
and P̄X are of rank k and (n &minus; k), respectively. Both matrices are not of full column rank. In fact, the
only full rank, symmetric idempotent matrix is the identity matrix.
</p>
<p>Matrices not of full rank are singular, and their inverse do not exist. However, one can find a generalized
inverse of a matrix Ω which we will call Ω&minus; which satisfies the following requirements:
</p>
<p>(i) ΩΩ&minus;Ω =Ω (ii) Ω&minus;ΩΩ&minus; = Ω&minus;
</p>
<p>(iii) Ω&minus;Ω is symmetric and (iv) ΩΩ&minus; is symmetric.
</p>
<p>Even if Ω is not square, a unique Ω&minus; can be found for Ω which satisfies the above four properties. This
is called the Moore-Penrose generalized inverse.
</p>
<p>Note that a symmetric idempotent matrix is its own Moore-Penrose generalized inverse. For example, it
is easy to verify that if Ω = PX , then Ω
</p>
<p>&minus; = PX and that it satisfies the above four properties. Idempotent
matrices have characteristic roots that are either zero or one. The number of non-zero characteristic roots
is equal to the rank of this matrix. The characteristic roots of Ω&minus;1 are the reciprocals of the characteristic
roots of Ω, but the characteristic vectors of both matrices are the same.
</p>
<p>The determinant of a matrix is non-zero if and only if it has full rank. Therefore, if A is singular,
then |A| = 0. Also, the determinant of a matrix is equal to the product of its characteristic roots.
For two square matrices A and B, the determinant of the product is the product of the determinants
|AB| = |A| &middot; |B|. Therefore, the determinant of Ω&minus;1 is the reciprocal of the determinant of Ω. This follows
from the fact that |Ω||Ω&minus;1| = |ΩΩ&minus;1| = |I| = 1. This property is used in writing the likelihood function
for Generalized Least Squares (GLS) estimation, see Chapter 9. The determinant of a triangular matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 175
</p>
<p>is equal to the product of its diagonal elements. Of course, it immediately follows that the determinant
of a diagonal matrix is the product of its diagonal elements.
</p>
<p>The constant in the regression corresponds to a vector of ones in the matrix of regressors X. This
vector of ones is denoted by ιn where n is the dimension of this column vector. Note that ι
</p>
<p>&prime;
nιn = n and
</p>
<p>ιnι
&prime;
n = Jn where Jn is a matrix of ones of dimension n &times; n. Note also that Jn is not idempotent, but
</p>
<p>J̄n = Jn/n is idempotent as can be easily verified. The rank(J̄n) = tr(J̄n) = 1. Note also that In &minus; J̄n
is idempotent with rank (n &minus; 1). J̄ny has a typical element ȳ =
</p>
<p>&sum;n
i=1 yi/n whereas (In &minus; J̄n)y has a
</p>
<p>typical element (yi &minus; ȳ). So that J̄n is the averaging matrix, whereas premultiplying by (In &minus; J̄n) results
in deviations from the mean.
</p>
<p>For two nonsingular matrices A and B
</p>
<p>(AB)&minus;1 = B&minus;1A&minus;1
</p>
<p>Also, the transpose of a product of two conformable matrices, (AB)&prime; = B&prime;A&prime;. In fact, for the product of
three conformable matrices this becomes (ABC)&prime; = C &prime;B&prime;A&prime;. The transpose of the inverse is the inverse
of the transpose, i.e., (A&minus;1)&prime; = (A&prime;)&minus;1.
</p>
<p>The inverse of a partitioned matrix
</p>
<p>A =
</p>
<p>[
A11 A12
A21 A22
</p>
<p>]
</p>
<p>is
</p>
<p>A&minus;1 =
</p>
<p>[
E &minus;EA12A&minus;122
</p>
<p>&minus;A&minus;122 A21E A&minus;122 +A&minus;122 A21EA12A&minus;122
</p>
<p>]
</p>
<p>where E = (A11 &minus;A12A&minus;122 A21)&minus;1. Alternatively, it can be expressed as
</p>
<p>A&minus;1 =
</p>
<p>[
A&minus;111 +A
</p>
<p>&minus;1
11 A12FA21A
</p>
<p>&minus;1
11 &minus;A&minus;111 A12F
</p>
<p>&minus;FA21A&minus;111 F
</p>
<p>]
</p>
<p>where F = (A22 &minus; A21A&minus;111 A12)&minus;1. These formulas are used in partitioned regression models, see for
example the Frisch-Waugh Lovell Theorem and the computation of the variance-covariance matrix of
forecasts from a multiple regression in Chapter 7.
</p>
<p>An n &times; n symmetric matrix Ω has n distinct characteristic vectors c1, . . . , cn. The corresponding n
characteristic roots λ1, . . . , λn may not be distinct but they are all real numbers. The number of non-
zero characteristic roots of Ω is equal to the rank of Ω. The characteristic roots of a positive definite
matrix are positive. The characteristic vectors of the symmetric matrix Ω are orthogonal to each other,
i.e., c&prime;icj = 0 for i 	= j and can be made orthonormal with c&prime;ici = 1 for i = 1, 2, . . . , n. Hence, the
matrix of characteristic vectors C = [c1, c2, . . . , cn] is an orthogonal matrix, such that CC
</p>
<p>&prime; = C &prime;C = In
with C &prime; = C&minus;1. By definition Ωci = λici or ΩC = CΛ where Λ = diag[λi]. Premultiplying the last
equation by C &prime; we get C &prime;ΩC = C &prime;CΛ = Λ. Therefore, the matrix of characteristic vectors C diagonalizes
the symmetric matrix Ω. Alternatively, we can write Ω = CΛC &prime; =
</p>
<p>&sum;n
i=1 λicic
</p>
<p>&prime;
i which is the spectral
</p>
<p>decomposition of Ω.
A real symmetric n&times; n matrix Ω is positive semi-definite if for every n&times; 1 non-negative vector y, we
</p>
<p>have y&prime;Ωy &ge; 0. If y&prime;Ωy is strictly positive for any non-zero y then Ω is said to be positive definite. A
necessary and sufficient condition for Ω to be positive definite is that all the characteristic roots of Ω are
positive. One important application is the comparison of efficiency of two unbiased estimators of a vector
of parameters β. In this case, we subtract the variance-covariance matrix of the inefficient estimator from
the more efficient one and show that the resulting difference yields a positive semi-definite matrix, see
the Gauss-Markov Theorem in Chapter 7.
</p>
<p>If Ω is a symmetric and positive definite matrix, there exists a nonsingular matrix P such that
Ω = PP &prime;. In fact, using the spectral decomposition of Ω given above, one choice for P = CΛ1/2 so</p>
<p/>
</div>
<div class="page"><p/>
<p>176 Chapter 7: The General Linear Model: The Basics
</p>
<p>that Ω = CΛC &prime; = PP &prime;. This is a useful result which we use in Chapter 9 to obtain Generalized
Least Squares (GLS) as a least squares regression after transforming the original regression model by
P&minus;1 = (CΛ1/2)&minus;1 = Λ&minus;1/2C &prime;. In fact, if u &sim; (0, σ2Ω), then P&minus;1u has zero mean and var(P&minus;1u) =
P&minus;1&prime;var(u)P&minus;1&prime; = σ2P&minus;1ΩP&minus;1&prime; = σ2P&minus;1PP &prime;P&minus;1&prime; = σ2In.
</p>
<p>From Chapter 2, we have seen that if u &sim; N(0, σ2In), then ui/σ &sim; N(0, 1), so that u
2
i /σ
</p>
<p>2
&sim; χ21
</p>
<p>and u&prime;u/σ2 =
&sum;n
</p>
<p>i=1 u
2
i /σ
</p>
<p>2
&sim; χ2n. Therefore, u
</p>
<p>&prime;(σ2In)&minus;1u &sim; χ2n. If u &sim; N(0, σ
2Ω) where Ω is positive
</p>
<p>definite, then u&lowast; = P&minus;1u &sim; N(0, σ2In) and u&lowast;&prime;u&lowast;/σ2 &sim; χ2n. But u
&lowast;&prime;u&lowast; = u&prime;P&minus;1&prime;P&minus;1u = u&prime;Ω&minus;1u. Hence,
</p>
<p>u&prime;Ω&minus;1u/σ2 &sim; χ2n . This is used in Chapter 9.
Note that the OLS residuals are denoted by e = P̄Xu. If u &sim; N(0, σ
</p>
<p>2In), then e has mean zero and
var(e) = σ2P̄XInP̄X = σ
</p>
<p>2P̄X so that e &sim; N(0, σ
2P̄X). Our estimator of σ
</p>
<p>2 in Chapter 7 is s2 = e&prime;e/(n&minus;k)
so that (n &minus; k)s2/σ2 = e&prime;e/σ2. The last term can also be written as u&prime;P̄Xu/σ2. In order to find the
distribution of this quadratic form in Normal variables, we use the following result stated as lemma 1 in
Chapter 7.
</p>
<p>Lemma 1: For every symmetric idempotent matrix A of rank r, there exists an orthogonal matrix P
such that P &prime;AP = Jr where Jr is a diagonal matrix with the first r elements equal to one and the rest
equal to zero.
</p>
<p>We use this lemma to show that the e&prime;e/σ2 is a chi-squared with (n &minus; k) degrees of freedom. To see
this note that e&prime;e/σ2 = u&prime;P̄Xu/σ2 and that P̄X is symmetric and idempotent of rank (n &minus; k). Using
the lemma there exists a matrix P such that P &prime;P̄XP = Jn&minus;k is a diagonal matrix with the first (n&minus; k)
elements on the diagonal equal to 1 and the last k elements equal to zero. An orthogonal matrix P is
by definition a matrix whose inverse, is its own transpose, i.e., P &prime;P = In. Let v = P &prime;u then v has mean
zero and var(v) = σ2P &prime;P = σ2In so that v is N(0, σ2In) and u = Pv. Therefore,
</p>
<p>e&prime;e/σ2 = u&prime;P̄Xu/σ
2 = v&prime;P &prime;P̄XPv/σ
</p>
<p>2 = v&prime;Jn&minus;kv/σ
2 =
</p>
<p>&sum;n&minus;k
i=1 v
</p>
<p>2
i /σ
</p>
<p>2
</p>
<p>But, the v&rsquo;s are independent identically distributed N(0, σ2), hence v2i /σ
2 is the square of a standardized
</p>
<p>N(0, 1) random variable which is distributed as a χ21. Moreover, the sum of independent χ
2 random
</p>
<p>variables is a χ2 random variable with degrees of freedom equal to the sum of the respective degrees of
freedom, see Chapter 2. Hence, e&prime;e/σ2 is distributed as χ2n&minus;k.
</p>
<p>The beauty of the above result is that it applies to all quadratic forms u&prime;Au where A is symmetric
and idempotent. In general, for u &sim; N(0, σ2I), a necessary and sufficient condition for u&prime;Au/σ2 to
be distributed χ2k is that A is idempotent of rank k, see Theorem 4.6 of Graybill (1961). Another
useful theorem on quadratic forms in normal random variables is the following: If u &sim; N(0, σ2Ω), then
u&prime;Au/σ2 is χ2k if and only if AΩ is an idempotent matrix of rank k, see Theorem 4.8 of Graybill (1961). If
u &sim; N(0, σ2I), the two positive semi-definite quadratic forms in normal random variables say u&prime;Au and
u&prime;Bu are independent if and only if AB = 0, see Theorem 4.10 of Graybill (1961). A sufficient condition
is that tr(AB) = 0, see Theorem 4.15 of Graybill (1961). This is used in Chapter 7 to construct F -
statistics to test hypotheses, see for example problem 11. For u &sim; N(0, σ2I), the quadratic form u&prime;Au
is independent of the linear form Bu if BA = 0, see Theorem 4.17 of Graybill (1961). This is used in
</p>
<p>Chapter 7 to prove the independence of s2 and β̂ols, see problem 8. In general, if u &sim; N(0,Σ), then u
&prime;Au
</p>
<p>and u&prime;Bu are independent if and only if AΣB = 0, see Theorem 4.21 of Graybill (1961). Many other
useful matrix properties can be found. This is only a sample of them that will be implicitly or explicitly
used in this book.
</p>
<p>The Kronecker product of two matrices say Σ&otimes; In where Σ is m&times;m and In is the identity matrix of
dimension n is defined as follows:
</p>
<p>Σ&otimes; In =
</p>
<p>⎡
⎣
</p>
<p>σ11In . . . σ1mIn
: :
</p>
<p>σm1In . . . σmmIn
</p>
<p>⎤
⎦
</p>
<p>In other words, we place an In next to every element of Σ = [σij ]. The dimension of the resulting matrix
is mn&times;mn. This is useful when we have a system of equations like Seemingly Unrelated Regressions in
Chapter 10. In general, if A is m&times;n and B is p&times; q then A&otimes;B is mp&times;nq. Some properties of Kronecker</p>
<p/>
</div>
<div class="page"><p/>
<p>References 177
</p>
<p>products include (A &otimes; B)&prime; = A&prime; &otimes; B&prime;. If both A and B are square matrices of order m &times;m and p &times; p
then (A&otimes;B)&minus;1 = A&minus;1 &otimes;B&minus;1, |A&otimes;B| = |A|m|B|p and tr(A&otimes;B) = tr(A)tr(B). Applying this result to
Σ&otimes; In we get
</p>
<p>(Σ&otimes; In)&minus;1 = Σ&minus;1 &otimes; In and |Σ&otimes; In| = |Σ|m|In|n = |Σ|m
</p>
<p>and tr(Σ&otimes; In) = tr(Σ)tr(In) = n tr(Σ).
Some useful properties of matrix differentiation are the following:
</p>
<p>&part;x&prime;b
</p>
<p>&part;b
= x where x&prime; is 1&times; k and b is k &times; 1.
</p>
<p>Also
</p>
<p>&part;b&prime;Ab
</p>
<p>&part;b
= (A+A&prime;) where A is k &times; k.
</p>
<p>If A is symmetric, then &part;b&prime;Ab/&part;b = 2Ab. These two properties will be used in Chapter 7 in deriving the
</p>
<p>least squares estimator.
</p>
<p>References
</p>
<p>Bellman, R. (1970), Introduction to Matrix Analysis (McGraw Hill: New York).
</p>
<p>Searle, S.R. (1982), Matrix Algebra Useful for Statistics (John Wiley and Sons: New York).</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 8
</p>
<p>Regression Diagnostics and Specification Tests
</p>
<p>8.1 Influential Observations1
</p>
<p>Sources of influential observations include: (i) improperly recorded data, (ii) observational errors
in the data, (iii) misspecification and (iv) outlying data points that are legitimate and contain
valuable information which improve the efficiency of the estimation. It is constructive to isolate
extreme points and to determine the extent to which the parameter estimates depend upon
these desirable data.
One should always run descriptive statistics on the data, see Chapter 2. This will often reveal
</p>
<p>outliers, skewness or multimodal distributions. Scatter diagrams should also be examined, but
these diagnostics are only the first line of attack and are inadequate in detecting multivariate
discrepant observations or the way each observation affects the estimated regression model.
</p>
<p>In regression analysis, we emphasize the importance of plotting the residuals against the ex-
planatory variables or the predicted values ŷ to identify patterns in these residuals that may
indicate nonlinearity, heteroskedasticity, serial correlation, etc, see Chapter 3. In this section,
we learn how to identify significantly large residuals and compute regression diagnostics that
may identify influential observations. We study the extent to which the deletion of any observa-
tion affects the estimated coefficients, the standard errors, predicted values, residuals and test
statistics. These represent the core of diagnostic tools in regression analysis.
Accordingly, Belsley, Kuh and Welsch (1980, p. 11) define an influential observation as &ldquo;..one
</p>
<p>which, either individually or together with several other observations, has demonstrably larger
impact on the calculated values of various estimates (coefficients, standard errors, t-values, etc.)
than is the case for most of the other observations.&rdquo;
</p>
<p>First, what is a significantly large residual? We have seen that the least squares residuals of
y on X are given by e = (In &minus; PX)u, see equation (7.7). y is n &times; 1 and X is n &times; k. If u &sim;
IID(0, σ2In), then e has zero mean and variance σ
</p>
<p>2(In &minus;PX). Therefore, the OLS residuals are
correlated and heteroskedastic with var(ei) = σ
</p>
<p>2(1&minus; hii) where hii is the i-th diagonal element
of the hat matrix H = PX , since ŷ = Hy.
</p>
<p>The diagonal elements hii have the following properties:
</p>
<p>&sum;n
i=1 hii = tr(PX) = k and hii =
</p>
<p>&sum;n
j=1 h
</p>
<p>2
ij &ge; h2ii &ge; 0.
</p>
<p>The last property follows from the fact that PX is symmetric and idempotent. Therefore, h
2
ii &minus;
</p>
<p>hii &le; 0 or hii(hii &minus; 1) &le; 0. Hence, 0 &le; hii &le; 1, (see problem 1). hii is called the leverage of the
i-th observation. For a simple regression with a constant,
</p>
<p>hii = (1/n) + (x
2
i /
</p>
<p>&sum;n
i=1 x
</p>
<p>2
i )
</p>
<p>where xi = Xi &minus; X̄;hii can be interpreted as a measure of the distance between X values of
the i-th observation and their mean over all n observations. A large hii indicates that the i-th
observation is distant from the center of the observations. This means that the i-th observation
with large hii (a function only of Xi values) exercises substantial leverage in determining the
fitted value ŷi. Also, the larger hii, the smaller the variance of the residual ei. Since observations
</p>
<p>179
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_8, </p>
<p/>
</div>
<div class="page"><p/>
<p>180 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>with high leverage tend to have smaller residuals, it may not be possible to detect them by an
examination of the residuals alone. But, what is a large leverage? hii is large if it is more
than twice the mean leverage value 2h̄ = 2k/n. Hence, hii &ge; 2k/n are considered outlying
observations with regards to X values.
</p>
<p>An alternative representation of hii is simply hii = d
&prime;
iPXdi = ||PXdi||2 = x&prime;i(X &prime;X)&minus;1xi where
</p>
<p>di denotes the i-th observation&rsquo;s dummy variable, i.e., a vector of dimension n with 1 in the i-th
position and 0 elsewhere. x&prime;i is the i-th row of X and ||.|| denotes the Euclidian length. Note
that d&prime;iX = x
</p>
<p>&prime;
i.
</p>
<p>Let us standardize the i-th OLS residual by dividing it by an estimate of its variance. A
standardized residual would then be:
</p>
<p>ẽi = ei/s
&radic;
1&minus; hii (8.1)
</p>
<p>where σ2 is estimated by s2, the MSE of the regression. This is an internal studentization of the
residuals, see Cook and Weisberg (1982). Alternatively, one could use an estimate of σ2 that
is independent of ei. Defining s
</p>
<p>2
(i) as the MSE from the regression computed without the i-th
</p>
<p>observation, it can be shown, see equation (8.18) below, that
</p>
<p>s2(i) =
(n&minus; k)s2 &minus; e2i /(1&minus; hii)
</p>
<p>(n&minus; k &minus; 1) = s
2
</p>
<p>(
n&minus; k &minus; ẽ2i
n&minus; k &minus; 1
</p>
<p>)
(8.2)
</p>
<p>Under normality, s2(i) and ei are independent and the externally studentized residuals are defined
by
</p>
<p>e&lowast;i = ei/s(i)
&radic;
1&minus; hii &sim; tn&minus;k&minus;1 (8.3)
</p>
<p>Thus, if the normality assumption holds, we can readily assess the significance of any single
studentized residual. Of course, the e&lowast;i will not be independent. Since this is a t-statistic, it is
natural to think of e&lowast;i as large if its value exceeds 2 in absolute value.
Substituting (8.2) into (8.3) and comparing the result with (8.1), it is easy to show that e&lowast;i
</p>
<p>is a monotonic transformation of ẽi
</p>
<p>e&lowast;i = ẽi
</p>
<p>(
n&minus; k &minus; 1
n&minus; k &minus; ẽ2i
</p>
<p>) 1
2
</p>
<p>(8.4)
</p>
<p>Cook and Wiesberg (1982) show that e&lowast;i can be obtained as a t-statistic from the following
augmented regression:
</p>
<p>y = Xβ&lowast; + diϕ+ u (8.5)
</p>
<p>where di is the dummy variable for the i-th observation. In fact, ϕ̂ = ei/(1&minus; hii) and e&lowast;i is the
t-statistic for testing that ϕ = 0. (see problem 4 and the proof given below). Hence, whether
the i-th residual is large can be simply determined by the regression (8.5). A dummy variable
for the i-th observation is included in the original regression and the t-statistic on this dummy
tests whether this i-th residual is large. This is repeated for all observations i = 1, . . . , n.
</p>
<p>This can be generalized easily to testing for a group of significantly large residuals:
</p>
<p>y = Xβ&lowast; +Dpϕ
&lowast; + u (8.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Influential Observations 181
</p>
<p>where Dp is an n&times; p matrix of dummy variables for the p-suspected observations. One can test
ϕ&lowast; = 0 using the Chow test described in (4.17) as follows:
</p>
<p>F =
[Residual SS(no dummies)&minus; Residual SS(Dp dummies used)]/p
</p>
<p>Residual SS(Dp dummies used)/(n&minus; k &minus; p)
(8.7)
</p>
<p>This will be distributed as Fp,n&minus;k&minus;p under the null, see Gentleman and Wilk (1975). Let
</p>
<p>ep = D
&prime;
pe, then E(ep) = 0 and var(ep) = σ
</p>
<p>2D&prime;pP̄XDp (8.8)
</p>
<p>Then one can show, (see problem 5), that
</p>
<p>F =
[e&prime;p(D
</p>
<p>&prime;
pP̄XDp)
</p>
<p>&minus;1ep]/p
</p>
<p>[(n&minus; k)s2 &minus; e&prime;p(D&prime;pP̄XDp)&minus;1ep]/(n&minus; k &minus; p)
&sim; Fp,n&minus;k&minus;p (8.9)
</p>
<p>Another refinement comes from estimating the regression without the i-th observation:
</p>
<p>β̂(i) = [X
&prime;
(i)X(i)]
</p>
<p>&minus;1X &prime;(i)y(i) (8.10)
</p>
<p>where the (i) subscript notation indicates that the i-th observation has been deleted. Using the
updating formula
</p>
<p>(A&minus; a&prime;b)&minus;1 = A&minus;1 +A&minus;1a&prime;(I &minus; bA&minus;1a&prime;)&minus;1bA&minus;1 (8.11)
</p>
<p>with A = (X &prime;X) and a = b = x&prime;i, one gets
</p>
<p>[X &prime;(i)X(i)]
&minus;1 = (X &prime;X)&minus;1 + (X &prime;X)&minus;1xix
</p>
<p>&prime;
i(X
</p>
<p>&prime;X)&minus;1/(1&minus; hii) (8.12)
</p>
<p>Therefore
</p>
<p>β̂ &minus; β̂(i) = (X &prime;X)&minus;1xiei/(1&minus; hii) (8.13)
</p>
<p>Since the estimated coefficients are often of primary interest, (8.13) describes the change in the
estimated regression coefficients that would occur if the i-th observation is deleted. Note that
a high leverage observation with hii large will be influential in (8.13) only if the corresponding
residual ei is not small. Therefore, high leverage implies a potentially influential observation,
but whether this potential is actually realized depends on yi.
Alternatively, one can obtain this result from the augmented regression given in (8.5). Note
</p>
<p>that Pdi = di(d
&prime;
idi)
</p>
<p>&minus;1d&prime;i = did
&prime;
i is an n &times; n matrix with 1 in the i-th diagonal position and 0
</p>
<p>elsewhere. P̄di = In &minus;Pdi , has the effect when post-multiplied by a vector y of deleting the i-th
observation. Hence, premultiplying (8.5) by P̄di one gets
</p>
<p>P̄diy =
</p>
<p>(
y(i)
0
</p>
<p>)
=
</p>
<p>(
X(i)
0
</p>
<p>)
β&lowast; +
</p>
<p>(
u(i)
0
</p>
<p>)
(8.14)
</p>
<p>where the i-th observation is moved to the bottom of the data, without loss of generality. The
last observation has no effect on the least squares estimate of β&lowast; since both the dependent and
independent variables are zero. This regression will yield β̂
</p>
<p>&lowast;
= β̂(i), and the i-th observation&rsquo;s
</p>
<p>residual is clearly zero. By the Frisch-Waugh-Lovell Theorem given in section 7.3, the least
squares estimates and the residuals from (8.14) are numerically identical to those from (8.5).</p>
<p/>
</div>
<div class="page"><p/>
<p>182 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Therefore, β̂
&lowast;
= β̂(i) in (8.5) and the i-th observation residual from (8.5) must be zero. This
</p>
<p>implies that ϕ̂ = yi&minus;x&prime;iβ̂(i), and the fitted values from this regression are given by ŷ = Xβ̂(i)+diϕ̂
whereas those from the original regression (7.1) are given by Xβ̂. The difference in residuals is
therefore
</p>
<p>e&minus; e(i) = Xβ̂(i) + diϕ̂&minus;Xβ̂ (8.15)
</p>
<p>premultiplying (8.15) by P̄X and using the fact that P̄XX = 0, one gets P̄X(e&minus; e(i)) = P̄Xdiϕ̂.
But, P̄Xe = e and P̄Xe(i) = e(i), hence P̄Xdiϕ̂ = e&minus;e(i). Premultiplying both sides by d&prime;i one gets
d&prime;iP̄Xdiϕ̂ = ei since the i-th residual of e(i) from (8.5) is zero. By definition, d
</p>
<p>&prime;
iP̄Xdi = 1 &minus; hii,
</p>
<p>therefore
</p>
<p>ϕ̂ = ei/(1&minus; hii) (8.16)
</p>
<p>premultiplying (8.15) by (X &prime;X)&minus;1X &prime; one gets 0 = β̂(i) &minus; β̂ + (X &prime;X)&minus;1X &prime;diϕ̂. This uses the fact
that both residuals are orthogonal to X. Rearranging terms and substituting ϕ̂ from (8.16), one
gets
</p>
<p>β̂ &minus; β̂(i) = (X &prime;X)&minus;1xiϕ̂ = (X &prime;X)&minus;1xiei/(1&minus; hii)
</p>
<p>as given in (8.13).
Note that s2(i) given in (8.2) can now be written in terms of β̂(i):
</p>
<p>s2(i) =
&sum;
</p>
<p>t �=i(yt &minus; x&prime;tβ̂(i))2/(n&minus; k &minus; 1) (8.17)
</p>
<p>upon substituting (8.13) in (8.17) we get
</p>
<p>(n&minus; k &minus; 1)s2(i) =
&sum;n
</p>
<p>t=1
</p>
<p>(
et +
</p>
<p>hitei
1&minus; hii
</p>
<p>)2
&minus; e
</p>
<p>2
i
</p>
<p>(1&minus; hii)2
</p>
<p>= (n&minus; k)s2 + 2ei
1&minus; hi
</p>
<p>&sum;n
t=1 ethit +
</p>
<p>e2i
(1&minus; hii)2
</p>
<p>&sum;n
t=1 h
</p>
<p>2
it &minus;
</p>
<p>e2i
(1&minus; hii)2
</p>
<p>= (n&minus; k)s2 &minus; e
2
i
</p>
<p>1&minus; hii
(8.18)
</p>
<p>which is (8.2). This uses the fact that He = 0 and H2 = H. Hence,
&sum;n
</p>
<p>t=1 ethit = 0 and&sum;n
t=1 h
</p>
<p>2
it = hii.
</p>
<p>To assess whether the change in β̂j (the j-th component of β̂) that results from the deletion
</p>
<p>of the i-th observation, is large or small, we scale by the variance of β̂j , σ
2(X &prime;X)&minus;1jj . This is
</p>
<p>denoted by
</p>
<p>DFBETAS ij = (β̂j &minus; β̂j(i))/s(i)
&radic;
(X &prime;X)&minus;1jj (8.19)
</p>
<p>Note that s(i) is used in order to make the denominator stochastically independent of the
numerator in the Gaussian case. Absolute values of DFBETAS larger than 2 are considered
influential. However, Belsley, Kuh, and Welsch (1980) suggest 2/
</p>
<p>&radic;
n as a size-adjusted cutoff.
</p>
<p>In fact, it would be most unusual for the removal of a single observation from a sample of 100
or more to result in a change in any estimate by two or more standard errors. The size-adjusted</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Influential Observations 183
</p>
<p>cutoff tend to expose approximately the same proportion of potentially influential observations,
regardless of sample size. The size-adjusted cutoff is particularly important for large data sets.
In case of Normality, it can also be useful to look at the change in the t-statistics, as a means
</p>
<p>of assessing the sensitivity of the regression output to the deletion of the i-th observation:
</p>
<p>DFSTAT ij =
β̂j
</p>
<p>s
&radic;
(X &prime;X)&minus;1jj
</p>
<p>&minus;
β̂j(i)
</p>
<p>s(i)
</p>
<p>&radic;
(X &prime;(i)X(i))
</p>
<p>&minus;1
jj
</p>
<p>(8.20)
</p>
<p>Another way to summarize coefficient changes and gain insight into forecasting effects when the
i-th observation is deleted is to look at the change in fit, defined as
</p>
<p>DFFIT i = ŷi &minus; ŷ(i) = x&prime;i[β̂ &minus; β̂(i)] = hiiei/(1&minus; hii) (8.21)
</p>
<p>where the last equality is obtained from (8.13).
We scale this measure by the variance of ŷ(i), i.e., σ
</p>
<p>&radic;
hii, giving
</p>
<p>DFFITS i =
</p>
<p>(
hii
</p>
<p>1&minus; hii
</p>
<p>)1/2 ei
s(i)
</p>
<p>&radic;
1&minus; hii
</p>
<p>=
</p>
<p>(
hii
</p>
<p>1&minus; hii
</p>
<p>)1/2
e&lowast;i (8.22)
</p>
<p>where σ has been estimated by s(i) and e
&lowast;
i denotes the externally studentized residual given
</p>
<p>in (8.3). Values of DFFITS larger than 2 in absolute value are considered influential. A size-
adjusted cutoff for DFFITS suggested by Belsley, Kuh and Welsch (1980) is 2
</p>
<p>&radic;
k/n.
</p>
<p>In (8.3), the studentized residual e&lowast;i was interpreted as a t-statistic that tests for the sig-
nificance of the coefficient ϕ of di, the dummy variable which takes the value 1 for the i-th
observation and 0 otherwise, in the regression of y on X and di. This can now be easily proved
as follows:
Consider the Chow test for the significance of ϕ. The RRSS = (n &minus; k)s2, the URSS =
</p>
<p>(n&minus; k &minus; 1)s2(i) and the Chow F -test described in (4.17) becomes
</p>
<p>F1,n&minus;k&minus;1 =
[(n&minus; k)s2 &minus; (n&minus; k &minus; 1)s2(i)]/1
(n&minus; k &minus; 1)s2(i)/(n&minus; k &minus; 1)
</p>
<p>=
e2i
</p>
<p>s2(i)(1&minus; hii)
(8.23)
</p>
<p>The square root of (8.23) is e&lowast;i &sim; tn&minus;k&minus;1. These studentized residuals provide a better way to
examine the information in the residuals, but they do not tell the whole story, since some of
the most influential data points can have small e&lowast;i (and very small ei).
</p>
<p>One overall measure of the impact of the i-th observation on the estimated regression co-
efficients is Cook&rsquo;s (1977) distance measure D2i . Recall, that the confidence region for all k
</p>
<p>regression coefficients is (β̂ &minus; β)&prime;X &prime;X(β̂ &minus; β)/ks2 &sim; F (k, n &minus; k). Cook&rsquo;s (1977) distance mea-
sure D2i uses the same structure for measuring the combined impact of the differences in the
estimated regression coefficients when the i-th observation is deleted:
</p>
<p>D2i (s) = (β̂ &minus; β̂(i))&prime;X &prime;X(β̂ &minus; β̂(i))/ks2 (8.24)
</p>
<p>Even though D2i (s) does not follow the above F -distribution, Cook suggests computing the per-
centile value from this F -distribution and declaring an influential observation if this percentile
value &ge; 50%. In this case, the distance between β̂ and β̂(i) will be large, implying that the i-th</p>
<p/>
</div>
<div class="page"><p/>
<p>184 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>observation has a substantial influence on the fit of the regression. Cook&rsquo;s distance measure can
be equivalently computed as:
</p>
<p>D2i (s) =
e2i
ks2
</p>
<p>(
hii
</p>
<p>(1&minus; hii)2
)
</p>
<p>(8.25)
</p>
<p>D2i (s) depends on ei and hii; the larger ei or hii the larger is D
2
i (s). Note the relationship
</p>
<p>between Cook&rsquo;s D2i (s) and Belsley, Kuh, and Welsch (1980) DFFITS i(σ) in (8.22), i.e.,
</p>
<p>DFFITS i(σ) =
&radic;
kDi(σ) = (ŷi &minus; x&prime;iβ̂(i))/(σ
</p>
<p>&radic;
hii)
</p>
<p>Belsley, Kuh, and Welsch (1980) suggest nominating DFFITS based on s(i) exceeding 2
&radic;
k/n
</p>
<p>for special attention. Cook&rsquo;s 50 percentile recommendation is equivalent to DFFITS &gt;
&radic;
k,
</p>
<p>which is more conservative, see Velleman and Welsch (1981).
Next, we study the influence of the i-th observation deletion on the covariance matrix of the
</p>
<p>regression coefficients. One can compare the two covariance matrices using the ratio of their
determinants:
</p>
<p>COVRATIO i =
det(s2(i)[X
</p>
<p>&prime;
(i)X(i)]
</p>
<p>&minus;1)
</p>
<p>det(s2[X &prime;X]&minus;1)
=
</p>
<p>s2k(i)
</p>
<p>s2k
</p>
<p>(
det[X &prime;(i)X(i)]
</p>
<p>&minus;1
</p>
<p>det[X &prime;X]&minus;1
</p>
<p>)
(8.26)
</p>
<p>Using the fact that
</p>
<p>det[X &prime;(i)X(i)] = (1&minus; hii)det[X &prime;X] (8.27)
</p>
<p>see problem 8, one obtains
</p>
<p>COVRATIO i =
</p>
<p>(
s2(i)
</p>
<p>s2
</p>
<p>)k
&times; 1
</p>
<p>1&minus; hii
=
</p>
<p>1
(
n&minus;k&minus;1
n&minus;k +
</p>
<p>e&lowast;2i
n&minus;k
</p>
<p>)k
(1&minus; hii)
</p>
<p>(8.28)
</p>
<p>where the last equality follows from (8.18) and the definition of e&lowast;i in (8.3). Values of COVRA-
TIO not near unity identify possible influential observations and warrant further investigation.
Belsley, Kuh and Welsch (1980) suggest investigating points with |COVRATIO&minus;1| near to or
larger than 3k/n. The COVRATIO depends upon both hii and e
</p>
<p>&lowast;2
i . In fact, from (8.28), COV-
</p>
<p>RATIO is large when hii is large and small when e
&lowast;
i is large. The two factors can offset each
</p>
<p>other, that is why it is important to look at hii and e
&lowast;
i separately as well as in combination as
</p>
<p>in COVRATIO.
Finally, one can look at how the variance of ŷi changes when an observation is deleted.
</p>
<p>var(ŷi) = s
2hii and var(ŷ(i)) = var(x
</p>
<p>&prime;
iβ̂(i)) = s
</p>
<p>2
(i)(hii/(1&minus; hii))
</p>
<p>and the ratio is
</p>
<p>FVARATIO i = s
2
(i)/s
</p>
<p>2(1&minus; hii) (8.29)
</p>
<p>This expression is similar to COVRATIO except that [s2(i)/s
2] is not raised to the k-th power.
</p>
<p>As a diagnostic measure it will exhibit the same patterns of behavior with respect to different
configurations of hii and the studentized residual as described for COVRATIO.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Influential Observations 185
</p>
<p>Table 8.1 Cigarette Regression
</p>
<p>Dependent Variable: LNC
Analysis of Variance
</p>
<p>Sum of Mean
Source DF Squares Square F Value Prob&gt;F
</p>
<p>Model 2 0.50098 0.25049 9.378 0.0004
Error 43 1.14854 0.02671
C Total 45 1.64953
</p>
<p>Root MSE 0.16343 R-square 0.3037
Dep Mean 4.84784 Adj R-sq 0.2713
C.V. 3.37125
</p>
<p>Parameter Estimates
</p>
<p>Parameter Standard T for H0:
Variable DF Estimate Error Parameter=0 Prob &gt; |T|
INTERCEP 1 4.299662 0.90892571 4.730 0.0001
LNP 1 &ndash;1.338335 0.32460147 &ndash;4.123 0.0002
LNY 1 0.172386 0.19675440 0.876 0.3858
</p>
<p>Example 1: For the cigarette data given in Table 3.2, Table 8.1 gives the SAS least squares
regression for logC on logP and logY .
</p>
<p>logC = 4.30
</p>
<p>(0.909)
</p>
<p>&minus; 1.34
(0.325)
</p>
<p>logP + 0.172
</p>
<p>(0.197)
</p>
<p>logY + residuals
</p>
<p>The standard error of the regression is s = 0.16343 and R̄2 = 0.271. Table 8.2 gives the data
along with the predicted values of logC, the least squares residuals e, the internal studentized
residuals ẽ given in (8.1), the externally studentized residuals e&lowast; given in (8.3), the Cook statis-
tic given in (8.25), the leverage of each observation h, the DFFITS given in (8.22) and the
COVRATIO given in (8.28).
Using the leverage column, one can identify four potential observations with high leverage, i.e.,
</p>
<p>greater than 2h̄ = 2k/n = 6/46 = 0.13043. These are the observations belonging to the following
states: Connecticut (CT), Kentucky (KY), New Hampshire (NH) and New Jersey (NJ) with
leverage 0.13535, 0.19775, 0.13081 and 0.13945, respectively. Note that the corresponding OLS
residuals are &minus;0.078, 0.234, 0.160 and &minus;0.059, which are not necessarily large. The internally
studentized residuals are computed using equation (8.1). For KY this gives
</p>
<p>ẽKY =
eKY
</p>
<p>s
&radic;
1&minus; hKY
</p>
<p>=
0.23428
</p>
<p>0.16343
&radic;
1&minus; 0.19775
</p>
<p>= 1.6005
</p>
<p>From Table 8.2, two observations with a high internally studentized residuals are those belonging
to Arkansas (AR) and Utah (UT) with values of 2.102 and &minus;2.679 respectively, both larger than
2 in absolute value.
The externally studentized residuals are computed from (8.3). For KY, we first compute
</p>
<p>s2(KY ), the MSE from the regression computed without the KY observation. From (8.2), this is</p>
<p/>
</div>
<div class="page"><p/>
<p>186 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>given by
</p>
<p>s2(KY ) =
(n&minus; k)s2 &minus; e2KY /(1&minus; hKY )
</p>
<p>(n&minus; k &minus; 1)
</p>
<p>=
(46&minus; 3)(0.16343)2 &minus; (0.23428)2/(1&minus; 0.19775)
</p>
<p>(46&minus; 3&minus; 1) = 0.025716
</p>
<p>From (8.3) we get
</p>
<p>e&lowast;(KY ) =
eKY
</p>
<p>s(KY )
&radic;
1&minus; hKY
</p>
<p>=
0.23428
</p>
<p>0.16036
&radic;
1&minus; 0.19775
</p>
<p>= 1.6311
</p>
<p>This externally studentized residual is distributed as a t-statistic with 42 degrees of freedom.
However, e&lowast;KY does not exceed 2 in absolute value. Again, e
</p>
<p>&lowast;
AR and e
</p>
<p>&lowast;
UT are 2.193 and &minus;2.901
</p>
<p>both larger than 2 in absolute value. From (8.13), the change in the regression coefficients due
to the omission of the KY observation is given by
</p>
<p>β̂ &minus; β̂(KY ) = (X &prime;X)&minus;1xKY eKY /(1&minus; hKY )
</p>
<p>Using the fact that
</p>
<p>(X &prime;X)&minus;1 =
</p>
<p>⎡
⎣
</p>
<p>30.929816904 4.8110214655 &minus;6.679318415
4.81102114655 3.9447686638 &minus;1.177208398
&minus;6.679318415 &minus;1.177208398 1.4493372835
</p>
<p>⎤
⎦
</p>
<p>and x&prime;KY = (1,&minus;0.03260, 4.64937) with eKY = 0.23428 and hKY = 0.19775 one gets
</p>
<p>(β̂ &minus; β̂(KY ))&prime; = (&minus;0.082249,&minus;0.230954, 0.028492)
</p>
<p>In order to assess whether this change is large or small, we compute DFBETAS given in (8.19).
For the KY observation, these are given by
</p>
<p>DFBETASKY,1 =
β̂1 &minus; β̂1(KY )
</p>
<p>s(KY )
</p>
<p>&radic;
(X &prime;X)&minus;111
</p>
<p>=
&minus;0.082449
</p>
<p>0.16036
&radic;
30.9298169
</p>
<p>= &minus;0.09222
</p>
<p>Similarly,DFBETASKY,2 = &minus;0.7251 and DFBETASKY,3 = 0.14758. These are not larger than 2
in absolute value. However, DFBETASKY,2 is larger than 2/
</p>
<p>&radic;
n = 2/
</p>
<p>&radic;
46 = 0.2949 in absolute
</p>
<p>value. This is the size-adjusted cutoff recommended by Belsley, Kuh and Welsch (1980) for
large n.
</p>
<p>The change in the fit due to the omission of the KY observation is given by (8.21). In fact,
</p>
<p>DFFITKY = ŷKY &minus; ŷ(KY ) = x&prime;KY [β̂ &minus; β̂(KY )]
</p>
<p>= (1,&minus;0.03260, 4.64937)
⎛
⎜⎝
</p>
<p>&minus;0.082249
&minus;0.230954
&minus;0.028492
</p>
<p>⎞
⎟⎠ = 0.05775
</p>
<p>or simply
</p>
<p>DFFITKY =
hKY eKY
(1&minus; hKY )
</p>
<p>=
(0.19775)(0.23428)
</p>
<p>1&minus; 0.19775 = 0.05775</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Recursive Residuals 187
</p>
<p>Scaling it by the variance of ŷ(KY ) we get from (8.22)
</p>
<p>DFFITSKY =
</p>
<p>(
hKY
</p>
<p>1&minus; hKY
</p>
<p>)1/2
e&lowast;KY =
</p>
<p>(
0.19775
</p>
<p>1&minus; 0.19775
</p>
<p>)1/2
(1.6311) = 0.8098
</p>
<p>This is not larger than 2 in absolute value, but it is larger than the size-adjusted cutoff of
2
&radic;
</p>
<p>k/n = 2
&radic;
3/46 = 0.511. Note also that both DFFITSAR = 0.667 and DFFITSUT = &minus;0.888
</p>
<p>are larger than 0.511 in absolute value.
Cook&rsquo;s distance measure is given in (8.25) and for KY can be computed as
</p>
<p>D2KY (s) =
e2KY
ks2
</p>
<p>(
hKY
</p>
<p>(1&minus; hKY )2
)
</p>
<p>=
</p>
<p>(
(0.23428)2
</p>
<p>3(0.16343)2
</p>
<p>)(
0.19775
</p>
<p>(1&minus; 0.19775)2
)
</p>
<p>= 0.21046
</p>
<p>The other two large Cook&rsquo;s distance measures are DR2AR(s) = 0.13623 and D
2
UT (s) = 0.22399,
</p>
<p>respectively. COVRATIO omitting the KY observation can be computed from (8.28) as
</p>
<p>COVRATIOKY =
</p>
<p>(
s2(KY )
</p>
<p>s2
</p>
<p>)k
1
</p>
<p>1&minus; hKY
=
</p>
<p>(
0.025716
</p>
<p>(0.16343)2
</p>
<p>)3( 1
(1&minus; 0.019775)
</p>
<p>)
= 1.1125
</p>
<p>which means that COVRATIOKY &minus; 1/ = 0.1125 is less than 3k/n = 9/46 = 0.1956.
Finally, FVARATIO omitting the KY observation can be computed from (8.29) as
</p>
<p>FVARATIOKY =
s2(KY )
</p>
<p>s2(1&minus; hKY )
=
</p>
<p>0.025716
</p>
<p>(0.16343)2(1&minus; 0.19775) = 1.2001
</p>
<p>By several diagnostic measures, AR, KY and UT are influential observations that deserve special
attention. The first two states are characterized with large sales of cigarettes. KY is a producer
state with a very low price on cigarettes, while UT is a low consumption state due to its high
percentage of Mormon population (a religion that forbids smoking). Table 8.3 gives the predicted
consumption along with the 95% confidence band, the OLS residuals, and the internalized
student residuals, Cook&rsquo;s D-statistic and a plot of these residuals. This last plot highlights the
fact that AR, UT and KY have large studentized residuals.
</p>
<p>8.2 Recursive Residuals
</p>
<p>In Section 8.1, we showed that the least squares residuals are heteroskedastic with non-zero co-
variances, even when the true disturbances have a scalar covariance matrix. This section studies
recursive residuals which are a set of linear unbiased residuals with a scalar covariance matrix.
They are independent and identically distributed when the true disturbances themselves are
independent and identically distributed.2 These residuals are natural in time-series regressions
and can be constructed as follows:
</p>
<p>1. Choose the first t &ge; k observations and compute β̂t = (X &prime;tXt)&minus;1X &prime;tYt where Xt denotes
the t &times; k matrix of t observations on k variables and Y &prime;t = (y1, . . . , yt). The recursive
residuals are basically standardized one-step ahead forecast residuals:
</p>
<p>wt+1 = (yt+1 &minus; x&prime;t+1β̂t)/
&radic;
</p>
<p>1 + x&prime;t+1(X
&prime;
tXt)
</p>
<p>&minus;1xt+1 (8.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>188 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Table 8.2 Diagnostic Statistics for the Cigarettes Example
</p>
<p>OBS STATE LNC LNP LNY PREDICTED e ẽ e⋆ Cook&rsquo;s D Leverage DFFITS COVRATIO
</p>
<p>1 AL 4.96213 0.20487 4.64039 4.8254 0.1367 0.857 0.8546 0.012 0.0480 0.1919 1.0704
</p>
<p>2 AZ 4.66312 0.16640 4.68389 4.8844 &ndash;0.2213 &ndash;1.376 &ndash;1.3906 0.021 0.0315 &ndash;0.2508 0.9681
</p>
<p>3 AR 5.10709 0.23406 4.59435 4.7784 0.3287 2.102 2.1932 0.136 0.0847 0.6670 0.8469
</p>
<p>4 CA 4.50449 0.36399 4.88147 4.6540 &ndash;0.1495 &ndash;0.963 &ndash;0.9623 0.033 0.0975 &ndash;0.3164 1.1138
</p>
<p>5 CT 4.66983 0.32149 5.09472 4.7477 &ndash;0.0778 &ndash;0.512 &ndash;0.5077 0.014 0.1354 &ndash;0.2009 1.2186
</p>
<p>6 DE 5.04705 0.21929 4.87087 4.8458 0.2012 1.252 1.2602 0.018 0.0326 0.2313 0.9924
</p>
<p>7 DC 4.65637 0.28946 5.05960 4.7845 &ndash;0.1281 &ndash;0.831 &ndash;0.8280 0.029 0.1104 &ndash;0.2917 1.1491
</p>
<p>8 FL 4.80081 0.28733 4.81155 4.7446 0.0562 0.352 0.3482 0.002 0.0431 0.0739 1.1118
</p>
<p>9 GA 4.97974 0.12826 4.73299 4.9439 0.0358 0.224 0.2213 0.001 0.0402 0.0453 1.1142
</p>
<p>10 ID 4.74902 0.17541 4.64307 4.8653 &ndash;0.1163 &ndash;0.727 &ndash;0.7226 0.008 0.0413 &ndash;0.1500 1.0787
</p>
<p>11 IL 4.81445 0.24806 4.90387 4.8130 0.0014 0.009 0.0087 0.000 0.0399 0.0018 1.1178
</p>
<p>12 IN 5.11129 0.08992 4.72916 4.9946 0.1167 0.739 0.7347 0.013 0.0650 0.1936 1.1046
</p>
<p>13 IA 4.80857 0.24081 4.74211 4.7949 0.0137 0.085 0.0843 0.000 0.0310 0.0151 1.1070
</p>
<p>14 KS 4.79263 0.21642 4.79613 4.8368 &ndash;0.0442 &ndash;0.273 &ndash;0.2704 0.001 0.0223 &ndash;0.0408 1.0919
</p>
<p>15 KY 5.37906 &ndash;0.03260 4.64937 5.1448 0.2343 1.600 1.6311 0.210 0.1977 0.8098 1.1126
</p>
<p>16 LA 4.98602 0.23856 4.61461 4.7759 0.2101 1.338 1.3504 0.049 0.0761 0.3875 1.0224
</p>
<p>17 ME 4.98722 0.29106 4.75501 4.7298 0.2574 1.620 1.6527 0.051 0.0553 0.4000 0.9403
</p>
<p>18 MD 4.77751 0.12575 4.94692 4.9841 &ndash;0.2066 &ndash;1.349 &ndash;1.3624 0.084 0.1216 &ndash;0.5070 1.0731
</p>
<p>19 MA 4.73877 0.22613 4.99998 4.8590 &ndash;0.1202 &ndash;0.769 &ndash;0.7653 0.018 0.0856 &ndash;0.2341 1.1258
</p>
<p>20 MI 4.94744 0.23067 4.80620 4.8195 0.1280 0.792 0.7890 0.005 0.0238 0.1232 1.0518
</p>
<p>21 MN 4.69589 0.34297 4.81207 4.6702 0.0257 0.165 0.1627 0.001 0.0864 0.0500 1.1724
</p>
<p>22 MS 4.93990 0.13638 4.52938 4.8979 0.0420 0.269 0.2660 0.002 0.0883 0.0828 1.1712
</p>
<p>23 MO 5.06430 0.08731 4.78189 5.0071 0.0572 0.364 0.3607 0.004 0.0787 0.1054 1.1541
</p>
<p>24 MT 4.73313 0.15303 4.70417 4.9058 &ndash;0.1727 &ndash;1.073 &ndash;1.0753 0.012 0.0312 &ndash;0.1928 1.0210
</p>
<p>25 NE 4.77558 0.18907 4.79671 4.8735 &ndash;0.0979 &ndash;0.607 &ndash;0.6021 0.003 0.0243 &ndash;0.0950 1.0719
</p>
<p>26 NV 4.96642 0.32304 4.83816 4.7014 0.2651 1.677 1.7143 0.065 0.0646 0.4504 0.9366
</p>
<p>27 NH 5.10990 0.15852 5.00319 4.9500 0.1599 1.050 1.0508 0.055 0.1308 0.4076 1.1422
</p>
<p>28 NJ 4.70633 0.30901 5.10268 4.7657 &ndash;0.0594 &ndash;0.392 &ndash;0.3879 0.008 0.1394 &ndash;0.1562 1.2337
</p>
<p>29 NM 4.58107 0.16458 4.58202 4.8693 &ndash;0.2882 &ndash;1.823 &ndash;1.8752 0.076 0.0639 &ndash;0.4901 0.9007
</p>
<p>30 NY 4.66496 0.34701 4.96075 4.6904 &ndash;0.0254 &ndash;0.163 &ndash;0.1613 0.001 0.0888 &ndash;0.0503 1.1755
</p>
<p>31 ND 4.58237 0.18197 4.69163 4.8649 &ndash;0.2825 &ndash;1.755 &ndash;1.7999 0.031 0.0295 &ndash;0.3136 0.8848
</p>
<p>32 OH 4.97952 0.12889 4.75875 4.9475 0.0320 0.200 0.1979 0.001 0.0423 0.0416 1.1174
</p>
<p>33 OK 4.72720 0.19554 4.62730 4.8356 &ndash;0.1084 &ndash;0.681 &ndash;0.6766 0.008 0.0505 &ndash;0.1560 1.0940
</p>
<p>34 PA 4.80363 0.22784 4.83516 4.8282 &ndash;0.0246 &ndash;0.153 &ndash;0.1509 0.000 0.0257 &ndash;0.0245 1.0997
</p>
<p>35 RI 4.84693 0.30324 4.84670 4.7293 0.1176 0.738 0.7344 0.010 0.0504 0.1692 1.0876
</p>
<p>36 SC 5.07801 0.07944 4.62549 4.9907 0.0873 0.555 0.5501 0.008 0.0725 0.1538 1.1324
</p>
<p>37 SD 4.81545 0.13139 4.67747 4.9301 &ndash;0.1147 &ndash;0.716 &ndash;0.7122 0.007 0.0402 &ndash;0.1458 1.0786
</p>
<p>38 TN 5.04939 0.15547 4.72525 4.9062 0.1432 0.890 0.8874 0.008 0.0294 0.1543 1.0457
</p>
<p>39 TX 4.65398 0.28196 4.73437 4.7384 &ndash;0.0845 &ndash;0.532 &ndash;0.5271 0.005 0.0546 &ndash;0.1267 1.1129
</p>
<p>40 UT 4.40859 0.19260 4.55586 4.8273 &ndash;0.4187 &ndash;2.679 &ndash;2.9008 0.224 0.0856 &ndash;0.8876 0.6786
</p>
<p>41 VT 5.08799 0.18018 4.77578 4.8818 0.2062 1.277 1.2869 0.014 0.0243 0.2031 0.9794
</p>
<p>42 VA 4.93065 0.11818 4.85490 4.9784 &ndash;0.0478 &ndash;0.304 &ndash;0.3010 0.003 0.0773 &ndash;0.0871 1.1556
</p>
<p>43 WA 4.66134 0.35053 4.85645 4.6677 &ndash;0.0064 &ndash;0.041 &ndash;0.0404 0.000 0.0866 &ndash;0.0124 1.1747
</p>
<p>44 WV 4.82454 0.12008 4.56859 4.9265 &ndash;0.1020 &ndash;0.647 &ndash;0.6429 0.011 0.0709 &ndash;0.1777 1.1216
</p>
<p>45 WI 4.83026 0.22954 4.75826 4.8127 0.0175 0.109 0.1075 0.000 0.0254 0.0174 1.1002
</p>
<p>46 WY 5.00087 0.10029 4.71169 4.9777 0.0232 0.146 0.1444 0.000 0.0555 0.0350 1.1345</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Recursive Residuals 189
T
a
b
le
</p>
<p>8
.3
</p>
<p>R
eg
re
ss
io
n
o
f
R
ea
l
P
er
-C
</p>
<p>a
p
it
a
C
o
n
su
m
p
ti
o
n
o
f
C
ig
a
re
tt
es
</p>
<p>D
ep
</p>
<p>V
a
r
</p>
<p>P
re
d
ic
t
</p>
<p>S
td
</p>
<p>E
rr
</p>
<p>L
ow
</p>
<p>er
9
5
%
</p>
<p>U
p
p
er
9
5
%
</p>
<p>L
ow
</p>
<p>er
9
5
%
</p>
<p>U
p
p
er
9
5
%
</p>
<p>S
td
</p>
<p>E
rr
</p>
<p>S
tu
d
en
t
</p>
<p>O
b
s
</p>
<p>L
N
C
</p>
<p>V
a
lu
e
</p>
<p>P
re
d
ic
t
</p>
<p>M
ea
n
</p>
<p>M
ea
n
</p>
<p>P
re
d
ic
t
</p>
<p>P
re
d
ic
t
</p>
<p>R
es
id
u
a
l
</p>
<p>R
es
id
u
a
l
</p>
<p>R
es
id
u
a
l
</p>
<p>&ndash;
2
</p>
<p>&ndash;
1
</p>
<p>0
1
</p>
<p>2
C
o
o
k
&rsquo;s
</p>
<p>D
</p>
<p>1
4
.9
6
2
1
</p>
<p>4
.8
2
5
4
</p>
<p>0
.0
3
6
</p>
<p>4
.7
5
3
2
</p>
<p>4
.8
9
7
6
</p>
<p>4
.4
8
8
0
</p>
<p>5
.1
6
2
8
</p>
<p>0
.1
3
6
7
</p>
<p>0
.1
5
9
</p>
<p>0
.8
5
7
</p>
<p>⋆
0
.0
1
2
</p>
<p>2
4
.6
6
3
1
</p>
<p>4
.8
8
4
4
</p>
<p>0
.0
2
9
</p>
<p>4
.8
2
5
9
</p>
<p>4
.9
4
2
9
</p>
<p>4
.5
4
9
7
</p>
<p>5
.2
1
9
1
</p>
<p>&ndash;
0
.2
2
1
3
</p>
<p>0
.1
6
1
</p>
<p>&ndash;
1
.3
7
6
</p>
<p>⋆
⋆
</p>
<p>0
.0
2
1
</p>
<p>3
5
.1
0
7
1
</p>
<p>4
.7
7
8
4
</p>
<p>0
.0
4
8
</p>
<p>4
.6
8
2
5
</p>
<p>4
.8
7
4
3
</p>
<p>4
.4
3
5
1
</p>
<p>5
.1
2
1
7
</p>
<p>0
.3
2
8
7
</p>
<p>0
.1
5
6
</p>
<p>2
.1
0
2
</p>
<p>⋆
⋆
⋆
⋆
</p>
<p>0
.1
3
6
</p>
<p>4
4
.5
0
4
5
</p>
<p>4
.6
5
4
0
</p>
<p>0
.0
5
1
</p>
<p>4
.5
5
1
1
</p>
<p>4
.7
5
7
0
</p>
<p>4
.3
0
8
7
</p>
<p>4
.9
9
9
3
</p>
<p>&ndash;
0
.1
4
9
5
</p>
<p>0
.1
5
5
</p>
<p>&ndash;
0
.9
6
3
</p>
<p>⋆
0
.0
3
3
</p>
<p>5
4
.6
6
9
8
</p>
<p>4
.7
4
7
7
</p>
<p>0
.0
6
0
</p>
<p>4
.6
2
6
4
</p>
<p>4
.8
6
8
9
</p>
<p>4
.3
9
6
5
</p>
<p>5
.0
9
8
9
</p>
<p>&ndash;
0
.0
7
7
8
</p>
<p>0
.1
5
2
</p>
<p>&ndash;
0
.5
1
2
</p>
<p>⋆
0
.0
1
4
</p>
<p>6
5
.0
4
7
1
</p>
<p>4
.8
4
5
8
</p>
<p>0
.0
3
0
</p>
<p>4
.7
8
6
3
</p>
<p>4
.9
0
5
3
</p>
<p>4
.5
1
0
9
</p>
<p>5
.1
8
0
8
</p>
<p>0
.2
0
1
2
</p>
<p>0
.1
6
1
</p>
<p>1
.2
5
2
</p>
<p>⋆
⋆
</p>
<p>0
.0
1
8
</p>
<p>7
4
.6
5
6
4
</p>
<p>4
.7
8
4
5
</p>
<p>0
.0
5
4
</p>
<p>4
.6
7
5
0
</p>
<p>4
.8
9
4
0
</p>
<p>4
.4
3
7
2
</p>
<p>5
.1
3
1
8
</p>
<p>&ndash;
0
.1
2
8
1
</p>
<p>0
.1
5
4
</p>
<p>&ndash;
0
.8
3
1
</p>
<p>⋆
0
.0
2
9
</p>
<p>8
4
.8
0
0
8
</p>
<p>4
.7
4
4
6
</p>
<p>0
.0
3
4
</p>
<p>4
.6
7
6
1
</p>
<p>4
.8
1
3
0
</p>
<p>4
.4
0
7
9
</p>
<p>5
.0
8
1
2
</p>
<p>0
.0
5
6
2
</p>
<p>0
.1
6
0
</p>
<p>0
.3
5
2
</p>
<p>0
.0
0
2
</p>
<p>9
4
.9
7
9
7
</p>
<p>4
.9
4
3
9
</p>
<p>0
.0
3
3
</p>
<p>4
.8
7
7
8
</p>
<p>5
.0
1
0
0
</p>
<p>4
.6
0
7
8
</p>
<p>5
.2
8
0
1
</p>
<p>0
.0
3
5
8
</p>
<p>0
.1
6
0
</p>
<p>0
.2
2
4
</p>
<p>0
.0
0
1
</p>
<p>1
0
</p>
<p>4
.7
4
9
0
</p>
<p>4
.8
6
5
3
</p>
<p>0
.0
3
3
</p>
<p>4
.7
9
8
3
</p>
<p>4
.9
3
2
3
</p>
<p>4
.5
2
9
0
</p>
<p>5
.2
0
1
6
</p>
<p>&ndash;
0
.1
1
6
3
</p>
<p>0
.1
6
0
</p>
<p>&ndash;
0
.7
2
7
</p>
<p>⋆
0
.0
0
8
</p>
<p>1
1
</p>
<p>4
.8
1
4
5
</p>
<p>4
.8
1
3
0
</p>
<p>0
.0
3
3
</p>
<p>4
.7
4
7
2
</p>
<p>4
.8
7
8
9
</p>
<p>4
.4
7
6
9
</p>
<p>5
.1
4
9
1
</p>
<p>0
.0
0
1
4
2
</p>
<p>0
.1
6
0
</p>
<p>0
.0
0
9
</p>
<p>0
.0
0
0
</p>
<p>1
2
</p>
<p>5
.1
1
1
3
</p>
<p>4
.9
9
4
6
</p>
<p>0
.0
4
2
</p>
<p>4
.9
1
0
6
</p>
<p>5
.0
7
8
6
</p>
<p>4
.6
5
4
4
</p>
<p>5
.3
3
4
7
</p>
<p>0
.1
1
6
7
</p>
<p>0
.1
5
8
</p>
<p>0
.7
3
9
</p>
<p>⋆
0
.0
1
3
</p>
<p>1
3
</p>
<p>4
.8
0
8
6
</p>
<p>4
.7
9
4
9
</p>
<p>0
.0
2
9
</p>
<p>4
.7
3
6
8
</p>
<p>4
.8
5
2
9
</p>
<p>4
.4
6
0
2
</p>
<p>5
.1
2
9
5
</p>
<p>0
.0
1
3
7
</p>
<p>0
.1
6
1
</p>
<p>0
.0
8
5
</p>
<p>0
.0
0
0
</p>
<p>1
4
</p>
<p>4
.7
9
2
6
</p>
<p>4
.8
3
6
8
</p>
<p>0
.0
2
4
</p>
<p>4
.7
8
7
6
</p>
<p>4
.8
8
6
0
</p>
<p>4
.5
0
3
6
</p>
<p>5
.1
7
0
1
</p>
<p>&ndash;
0
.0
4
4
2
</p>
<p>0
.1
6
2
</p>
<p>&ndash;
0
.2
7
3
</p>
<p>0
.0
0
1
</p>
<p>1
5
</p>
<p>5
.3
7
9
1
</p>
<p>5
.1
4
4
8
</p>
<p>0
.0
7
3
</p>
<p>4
.9
9
8
2
</p>
<p>5
.2
9
1
3
</p>
<p>4
.7
8
4
1
</p>
<p>5
.5
0
5
5
</p>
<p>0
.2
3
4
3
</p>
<p>0
.1
4
6
</p>
<p>1
.6
0
0
</p>
<p>⋆
⋆
⋆
</p>
<p>0
.2
1
0
</p>
<p>1
6
</p>
<p>4
.9
8
6
0
</p>
<p>4
.7
7
5
9
</p>
<p>0
.0
4
5
</p>
<p>4
.6
8
5
0
</p>
<p>4
.8
6
6
8
</p>
<p>4
.4
3
4
0
</p>
<p>5
.1
1
7
8
</p>
<p>0
.2
1
0
1
</p>
<p>0
.1
5
7
</p>
<p>1
.3
3
8
</p>
<p>⋆
⋆
</p>
<p>0
.0
4
9
</p>
<p>1
7
</p>
<p>4
.9
8
7
2
</p>
<p>4
.7
2
9
8
</p>
<p>0
.0
3
8
</p>
<p>4
.6
5
2
3
</p>
<p>4
.8
0
7
4
</p>
<p>4
.3
9
1
2
</p>
<p>5
.0
6
8
4
</p>
<p>0
.2
5
7
4
</p>
<p>0
.1
5
9
</p>
<p>1
.6
2
0
</p>
<p>⋆
⋆
⋆
</p>
<p>0
.0
5
1
</p>
<p>1
8
</p>
<p>4
.7
7
7
5
</p>
<p>4
.9
8
4
1
</p>
<p>0
.0
5
7
</p>
<p>4
.8
6
9
2
</p>
<p>5
.0
9
9
1
</p>
<p>4
.6
3
5
1
</p>
<p>5
.3
3
3
2
</p>
<p>&ndash;
0
.2
0
6
6
</p>
<p>0
.1
5
3
</p>
<p>&ndash;
1
.3
4
9
</p>
<p>⋆
⋆
</p>
<p>0
.0
8
4
</p>
<p>1
9
</p>
<p>4
.7
3
8
8
</p>
<p>4
.8
5
9
0
</p>
<p>0
.0
4
8
</p>
<p>4
.7
6
2
5
</p>
<p>4
.9
5
5
4
</p>
<p>4
.5
1
5
5
</p>
<p>5
.2
0
2
4
</p>
<p>&ndash;
0
.1
2
0
2
</p>
<p>0
.1
5
6
</p>
<p>&ndash;
0
.7
6
9
</p>
<p>⋆
0
.0
1
8
</p>
<p>2
0
</p>
<p>4
.9
4
7
4
</p>
<p>4
.8
1
9
5
</p>
<p>0
.0
2
5
</p>
<p>4
.7
6
8
6
</p>
<p>4
.8
7
0
3
</p>
<p>4
.4
8
6
0
</p>
<p>5
.1
5
3
0
</p>
<p>0
.1
2
8
0
</p>
<p>0
.1
6
1
</p>
<p>0
.7
9
2
</p>
<p>⋆
0
.0
0
5
</p>
<p>2
1
</p>
<p>4
.6
9
5
9
</p>
<p>4
.6
7
0
2
</p>
<p>0
.0
4
8
</p>
<p>4
.5
7
3
3
</p>
<p>4
.7
6
7
1
</p>
<p>4
.3
2
6
7
</p>
<p>5
.0
1
3
7
</p>
<p>0
.0
2
5
7
</p>
<p>0
.1
5
6
</p>
<p>0
.1
6
5
</p>
<p>0
.0
0
1
</p>
<p>2
2
</p>
<p>4
.9
3
9
9
</p>
<p>4
.8
9
7
9
</p>
<p>0
.0
4
9
</p>
<p>4
.8
0
0
0
</p>
<p>4
.9
9
5
9
</p>
<p>4
.5
5
4
1
</p>
<p>5
.2
4
1
8
</p>
<p>0
.0
4
2
0
</p>
<p>0
.1
5
6
</p>
<p>0
.2
6
9
</p>
<p>0
.0
0
2
</p>
<p>2
3
</p>
<p>5
.0
6
4
3
</p>
<p>5
.0
0
7
1
</p>
<p>0
.0
4
6
</p>
<p>4
.9
1
4
7
</p>
<p>5
.0
9
9
6
</p>
<p>4
.6
6
4
8
</p>
<p>5
.3
4
9
5
</p>
<p>0
.0
5
7
2
</p>
<p>0
.1
5
7
</p>
<p>0
.3
6
4
</p>
<p>0
.0
0
4
</p>
<p>2
4
</p>
<p>4
.7
3
3
1
</p>
<p>4
.9
0
5
8
</p>
<p>0
.0
2
9
</p>
<p>4
.8
4
7
6
</p>
<p>4
.9
6
4
0
</p>
<p>4
.5
7
1
1
</p>
<p>5
.2
4
0
5
</p>
<p>&ndash;
0
.1
7
2
7
</p>
<p>0
.1
6
1
</p>
<p>&ndash;
1
.0
7
3
</p>
<p>⋆
⋆
</p>
<p>0
.0
1
2
</p>
<p>2
5
</p>
<p>4
.7
7
5
6
</p>
<p>4
.8
7
3
5
</p>
<p>0
.0
2
5
</p>
<p>4
.8
2
2
1
</p>
<p>4
.9
2
4
9
</p>
<p>4
.5
3
9
9
</p>
<p>5
.2
0
7
1
</p>
<p>&ndash;
0
.0
9
7
9
</p>
<p>0
.1
6
1
</p>
<p>&ndash;
0
.6
0
7
</p>
<p>⋆
0
.0
0
3
</p>
<p>2
6
</p>
<p>4
.9
6
6
4
</p>
<p>4
.7
0
1
4
</p>
<p>0
.0
4
2
</p>
<p>4
.6
1
7
6
</p>
<p>4
.7
8
5
1
</p>
<p>4
.3
6
1
3
</p>
<p>5
.0
4
1
4
</p>
<p>0
.2
6
5
1
</p>
<p>0
.1
5
8
</p>
<p>1
.6
7
7
</p>
<p>⋆
⋆
⋆
</p>
<p>0
.0
6
5
</p>
<p>2
7
</p>
<p>5
.1
0
9
9
</p>
<p>4
.9
5
0
0
</p>
<p>0
.0
5
9
</p>
<p>4
.8
3
0
8
</p>
<p>5
.0
6
9
2
</p>
<p>4
.5
9
9
5
</p>
<p>5
.3
0
0
5
</p>
<p>0
.1
5
9
9
</p>
<p>0
.1
5
2
</p>
<p>1
.0
5
0
</p>
<p>⋆
⋆
</p>
<p>0
.0
5
5
</p>
<p>2
8
</p>
<p>4
.7
0
6
3
</p>
<p>4
.7
6
5
7
</p>
<p>0
.0
6
1
</p>
<p>4
.6
4
2
7
</p>
<p>4
.8
8
8
8
</p>
<p>4
.4
1
3
9
</p>
<p>5
.1
1
7
6
</p>
<p>&ndash;
0
.0
5
9
4
</p>
<p>0
.1
5
2
</p>
<p>&ndash;
0
.3
9
2
</p>
<p>0
.0
0
8
</p>
<p>2
9
</p>
<p>4
.5
8
1
1
</p>
<p>4
.8
6
9
3
</p>
<p>0
.0
4
1
</p>
<p>4
.7
8
5
9
</p>
<p>4
.9
5
2
6
</p>
<p>4
.5
2
9
3
</p>
<p>5
.2
0
9
2
</p>
<p>&ndash;
0
.2
8
8
2
</p>
<p>0
.1
5
8
</p>
<p>&ndash;
1
.8
2
3
</p>
<p>⋆
⋆
⋆
</p>
<p>0
.0
7
6
</p>
<p>3
0
</p>
<p>4
.6
6
5
0
</p>
<p>4
.6
9
0
4
</p>
<p>0
.0
4
9
</p>
<p>4
.5
9
2
2
</p>
<p>4
.7
8
8
6
</p>
<p>4
.3
4
6
5
</p>
<p>5
.0
3
4
3
</p>
<p>&ndash;
0
.0
2
5
4
</p>
<p>0
.1
5
6
</p>
<p>&ndash;
0
.1
6
3
</p>
<p>0
.0
0
1
</p>
<p>3
1
</p>
<p>4
.5
8
2
4
</p>
<p>4
.8
6
4
9
</p>
<p>0
.0
2
8
</p>
<p>4
.8
0
8
3
</p>
<p>4
.9
2
1
5
</p>
<p>4
.5
3
0
5
</p>
<p>5
.1
9
9
3
</p>
<p>&ndash;
0
.2
8
2
5
</p>
<p>0
.1
6
1
</p>
<p>&ndash;
1
.7
5
5
</p>
<p>⋆
⋆
⋆
</p>
<p>0
.0
3
1
</p>
<p>3
2
</p>
<p>4
.9
7
9
5
</p>
<p>4
.9
4
7
5
</p>
<p>0
.0
3
4
</p>
<p>4
.8
7
9
7
</p>
<p>5
.0
1
5
3
</p>
<p>4
.6
1
1
0
</p>
<p>5
.2
8
4
0
</p>
<p>0
.0
3
2
0
</p>
<p>0
.1
6
0
</p>
<p>0
.2
0
0
</p>
<p>0
.0
0
1
</p>
<p>3
3
</p>
<p>4
.7
2
7
2
</p>
<p>4
.8
3
5
6
</p>
<p>0
.0
3
7
</p>
<p>4
.7
6
1
6
</p>
<p>4
.9
0
9
7
</p>
<p>4
.4
9
7
8
</p>
<p>5
.1
7
3
5
</p>
<p>&ndash;
0
.1
0
8
4
</p>
<p>0
.1
5
9
</p>
<p>&ndash;
0
.6
8
1
</p>
<p>⋆
0
.0
0
8
</p>
<p>3
4
</p>
<p>4
.8
0
3
6
</p>
<p>4
.8
2
8
2
</p>
<p>0
.0
2
6
</p>
<p>4
.7
7
5
4
</p>
<p>4
.8
8
1
1
</p>
<p>4
.4
9
4
4
</p>
<p>5
.1
6
2
1
</p>
<p>&ndash;
0
.0
2
4
6
</p>
<p>0
.1
6
1
</p>
<p>&ndash;
0
.1
5
3
</p>
<p>0
.0
0
0
</p>
<p>3
5
</p>
<p>4
.8
4
6
9
</p>
<p>4
.7
2
9
3
</p>
<p>0
.0
3
7
</p>
<p>4
.6
5
5
3
</p>
<p>4
.8
0
3
3
</p>
<p>4
.3
9
1
5
</p>
<p>5
.0
6
7
1
</p>
<p>0
.1
1
7
6
</p>
<p>0
.1
5
9
</p>
<p>0
.7
3
8
</p>
<p>⋆
0
.0
1
0
</p>
<p>3
6
</p>
<p>5
.0
7
8
0
</p>
<p>4
.9
9
0
7
</p>
<p>0
.0
4
4
</p>
<p>4
.9
0
2
0
</p>
<p>5
.0
7
9
5
</p>
<p>4
.6
4
9
4
</p>
<p>5
.3
3
2
0
</p>
<p>0
.0
8
7
3
</p>
<p>0
.1
5
7
</p>
<p>0
.5
5
5
</p>
<p>⋆
0
.0
0
8
</p>
<p>3
7
</p>
<p>4
.8
1
5
5
</p>
<p>4
.9
3
0
1
</p>
<p>0
.0
3
3
</p>
<p>4
.8
6
4
0
</p>
<p>4
.9
9
6
3
</p>
<p>4
.5
9
4
0
</p>
<p>5
.2
6
6
3
</p>
<p>&ndash;
0
.1
1
4
7
</p>
<p>0
.1
6
0
</p>
<p>&ndash;
0
.7
1
6
</p>
<p>⋆
0
.0
0
7
</p>
<p>3
8
</p>
<p>5
.0
4
9
4
</p>
<p>4
.9
0
6
2
</p>
<p>0
.0
2
8
</p>
<p>4
.8
4
9
7
</p>
<p>4
.9
6
2
6
</p>
<p>4
.5
7
1
8
</p>
<p>5
.2
4
0
6
</p>
<p>0
.1
4
3
2
</p>
<p>0
.1
6
1
</p>
<p>0
.8
9
0
</p>
<p>⋆
0
.0
0
8
</p>
<p>3
9
</p>
<p>4
.6
5
4
0
</p>
<p>4
.7
3
8
4
</p>
<p>0
.0
3
8
</p>
<p>4
.6
6
1
4
</p>
<p>4
.8
1
5
5
</p>
<p>4
.4
0
0
0
</p>
<p>5
.0
7
6
9
</p>
<p>&ndash;
0
.0
8
4
5
</p>
<p>0
.1
5
9
</p>
<p>&ndash;
0
.5
3
2
</p>
<p>⋆
0
.0
0
5
</p>
<p>4
0
</p>
<p>4
.4
0
8
6
</p>
<p>4
.8
2
7
3
</p>
<p>0
.0
4
8
</p>
<p>4
.7
3
0
8
</p>
<p>4
.9
2
3
7
</p>
<p>4
.4
8
3
9
</p>
<p>5
.1
7
0
7
</p>
<p>&ndash;
0
.4
1
8
7
</p>
<p>0
.1
5
6
</p>
<p>&ndash;
2
.6
7
9
</p>
<p>⋆
⋆
⋆
⋆
⋆
</p>
<p>0
.2
2
4
</p>
<p>4
1
</p>
<p>5
.0
8
8
0
</p>
<p>4
.8
8
1
8
</p>
<p>0
.0
2
5
</p>
<p>4
.8
3
0
4
</p>
<p>4
.9
3
3
2
</p>
<p>4
.5
4
8
2
</p>
<p>5
.2
1
5
4
</p>
<p>0
.2
0
6
2
</p>
<p>0
.1
6
1
</p>
<p>1
.2
7
7
</p>
<p>⋆
⋆
</p>
<p>0
.0
1
4
</p>
<p>4
2
</p>
<p>4
.9
3
0
7
</p>
<p>4
.9
7
8
4
</p>
<p>0
.0
4
5
</p>
<p>4
.8
8
6
8
</p>
<p>5
.0
7
0
1
</p>
<p>4
.6
3
6
3
</p>
<p>5
.3
2
0
5
</p>
<p>&ndash;
0
.0
4
7
8
</p>
<p>0
.1
5
7
</p>
<p>&ndash;
0
.3
0
4
</p>
<p>0
.0
0
3
</p>
<p>4
3
</p>
<p>4
.6
6
1
3
</p>
<p>4
.6
6
7
7
</p>
<p>0
.0
4
8
</p>
<p>4
.5
7
0
8
</p>
<p>4
.7
6
4
7
</p>
<p>4
.3
2
4
2
</p>
<p>5
.0
1
1
3
</p>
<p>&ndash;
0
.0
0
6
3
8
</p>
<p>0
.1
5
6
</p>
<p>&ndash;
0
.0
4
1
</p>
<p>0
.0
0
0
</p>
<p>4
4
</p>
<p>4
.8
2
4
5
</p>
<p>4
.9
2
6
5
</p>
<p>0
.0
4
4
</p>
<p>4
.8
3
8
7
</p>
<p>5
.0
1
4
3
</p>
<p>4
.5
8
5
4
</p>
<p>5
.2
6
7
6
</p>
<p>&ndash;
0
.1
0
2
0
</p>
<p>0
.1
5
8
</p>
<p>&ndash;
0
.6
4
7
</p>
<p>⋆
0
.0
1
1
</p>
<p>4
5
</p>
<p>4
.8
3
0
3
</p>
<p>4
.8
1
2
7
</p>
<p>0
.0
2
6
</p>
<p>4
.7
6
0
2
</p>
<p>4
.8
6
5
3
</p>
<p>4
.4
7
9
0
</p>
<p>5
.1
4
6
5
</p>
<p>0
.0
1
7
5
</p>
<p>0
.1
6
1
</p>
<p>0
.1
0
9
</p>
<p>0
.0
0
0
</p>
<p>4
6
</p>
<p>5
.0
0
0
9
</p>
<p>4
.9
7
7
7
</p>
<p>0
.0
3
9
</p>
<p>4
.9
0
0
0
</p>
<p>5
.0
5
5
3
</p>
<p>4
.6
3
9
1
</p>
<p>5
.3
1
6
3
</p>
<p>0
.0
2
3
2
</p>
<p>0
.1
5
9
</p>
<p>0
.1
4
6
</p>
<p>0
.0
0
0
</p>
<p>S
u
m
</p>
<p>o
f
R
es
id
u
a
ls
</p>
<p>0
S
u
m
</p>
<p>o
f
S
q
u
a
re
d
R
es
id
u
a
ls
</p>
<p>1
.1
4
8
5
</p>
<p>P
re
d
ic
te
d
R
es
id
</p>
<p>S
S
(P
</p>
<p>re
ss
)
</p>
<p>1
.3
4
0
6</p>
<p/>
</div>
<div class="page"><p/>
<p>190 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>2. Add the (t + 1)-th observation to the data and obtain β̂t+1 = (X
&prime;
t+1Xt+1)
</p>
<p>&minus;1X &prime;t+1Yt+1.
Compute wt+2.
</p>
<p>3. Repeat step 2, adding one observation at a time. In time-series regressions, one usually
starts with the first k-observations and obtain (T &minus; k) forward recursive residuals. These
recursive residuals can be computed using the updating formula given in (8.11) with
A = (X &prime;tXt) and a = &minus;b = x&prime;t+1. Therefore,
</p>
<p>(X &prime;t+1Xt+1)
&minus;1 = (X &prime;tXt)
</p>
<p>&minus;1&minus;(X &prime;tXt)&minus;1xt+1x&prime;t+1(X &prime;tXt)&minus;1/[1+x&prime;t+1(X &prime;tXt)&minus;1xt+1] (8.31)
</p>
<p>and only (X &prime;tXt)
&minus;1 have to be computed. Also,
</p>
<p>β̂t+1 = β̂t + (X
&prime;
tXt)
</p>
<p>&minus;1xt+1(yt+1 &minus; x&prime;t+1β̂t)/ft+1 (8.32)
</p>
<p>where ft+1 = 1 + x
&prime;
t+1(X
</p>
<p>&prime;
tXt)
</p>
<p>&minus;1xt+1, see problem 13.
</p>
<p>Alternatively, one can compute these residuals by regressing Yt+1 on Xt+1 and dt+1 where
dt+1 = 1 for the (t + 1)-th observation, and zero otherwise, see equation (8.5). The estimated
coefficient of dt+1 is the numerator of wt+1. The standard error of this estimate is st+1 times
the denominator of wt+1, where st+1 is the standard error of this regression. Hence, wt+1 can
be retrieved as st+1 multiplied by the t-statistic corresponding to dt+1. This computation has
to be performed sequentially, in each case generating the corresponding recursive residual. This
may be computationally inefficient, but it is simple to generate using regression packages.
</p>
<p>It is obvious from (8.30) that if ut &sim; IIN(0, σ
2), then wt+1 has zero mean and var(wt+1) = σ
</p>
<p>2.
Furthermore, wt+1 is linear in the y&rsquo;s. Therefore, it is normally distributed. It remains to show
that the recursive residuals are independent. Given normality, it is sufficient to show that
</p>
<p>cov(wt+1, ws+1) = 0 for t 	= s; t, s = k, . . . , T &minus; 1 (8.33)
</p>
<p>This is left as an exercise for the reader, see problem 13.
Alternatively, one can express the T &minus; k vector of recursive residuals as w = Cy where C is
</p>
<p>of dimension (T &minus; k)&times; T as follows:
</p>
<p>C =
</p>
<p>⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
</p>
<p>&minus;x
&prime;
</p>
<p>k+1(X
&prime;
</p>
<p>k
Xk)
</p>
<p>&minus;1X&prime;
k&radic;
</p>
<p>fk+1
</p>
<p>1&radic;
fk+1
</p>
<p>0....0
</p>
<p>...
. . .
</p>
<p>&minus;x
&prime;
</p>
<p>t(X
&prime;
</p>
<p>t&minus;1Xt&minus;1)
&minus;1X&prime;t&minus;1&radic;
</p>
<p>ft
1&radic;
ft
</p>
<p>0....0
...
</p>
<p>. . .
</p>
<p>&minus;x
&prime;
</p>
<p>T
(X&prime;
</p>
<p>T&minus;1XT&minus;1)
&minus;1X&prime;
</p>
<p>T&minus;1&radic;
fT
</p>
<p>1&radic;
fT
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
</p>
<p>(8.34)
</p>
<p>Problem 14 asks the reader to verify that w = Cy, using (8.30). Also, that the matrix C satisfies
the following properties:
</p>
<p>(i) CX = 0 (ii) CC &prime; = IT&minus;k (iii) C
&prime;C = P̄X (8.35)
</p>
<p>This means that the recursive residuals w are (LUS) linear in y, unbiased with mean zero and
have a scalar variance-covariance matrix: var(w) = CE(uu&prime;)C &prime; = σ2IT&minus;k. Property (iii) also</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Recursive Residuals 191
</p>
<p>means that w&prime;w = y&prime;C &prime;Cy = y&prime;P̄Xy = e&prime;e. This means that the sum of squares of (T &minus; k)
recursive residuals is equal to the sum of squares of T least squares residuals. One can also show
from (8.32) that
</p>
<p>RSSt+1 = RSSt + w
2
t+1 for t = k, . . . , T &minus; 1 (8.36)
</p>
<p>where RSSt = (Yt &minus; Xtβ̂t)&prime;(Yt &minus; Xtβ̂t), see problem 14. Note that for t = k; RSS = 0, since
with k observations one gets a perfect fit and zero residuals. Therefore
</p>
<p>RSST =
&sum;T
</p>
<p>t=k+1w
2
t =
</p>
<p>&sum;T
t=1 e
</p>
<p>2
t (8.37)
</p>
<p>Applications of Recursive Residuals
</p>
<p>Recursive residuals have been used in several important applications:
</p>
<p>(1) Harvey (1976) used these recursive residuals to give an alternative proof of the fact that
Chow&rsquo;s post-sample predictive test has an F -distribution. Recall, from Chapter 7, that when
the second sample n2 had fewer than k observations, Chow&rsquo;s test becomes
</p>
<p>F =
(e&prime;e&minus; e&prime;1e1)/n2
e&prime;1e1/(n1 &minus; k)
</p>
<p>&sim; F (n2, n1 &minus; k) (8.38)
</p>
<p>where e&prime;e = RSS from the total sample (n1 + n2 = T observations), and e&prime;1e1 = RSS from the
first n1 observations. Recursive residuals can be computed for t = k + 1, . . . , n1, and continued
on for the extra n2 observations. From (8.36) we have
</p>
<p>e&prime;e =
&sum;n1+n2
</p>
<p>t=k+1 w
2
t and e
</p>
<p>&prime;
1e1 =
</p>
<p>&sum;n1
t=k+1w
</p>
<p>2
t (8.39)
</p>
<p>Therefore,
</p>
<p>F =
</p>
<p>&sum;n1+n2
t=n1+1
</p>
<p>w2t /n2&sum;n1
t=k+1w
</p>
<p>2
t /(n1 &minus; k)
</p>
<p>(8.40)
</p>
<p>But the wt&rsquo;s are &sim;IIN(0, σ
2) under the null, therefore the F -statistic in (8.38) is a ratio of two
</p>
<p>independent chi-squared variables, each divided by the appropriate degrees of freedom. Hence,
F &sim; F (n2, n1 &minus; k) under the null, see Chapter 2.
</p>
<p>(2) Harvey and Phillips (1974) used recursive residuals to test the null hypothesis of
homoskedasticity. If the alternative hypothesis is that σ2i varies with Xj , the proposed test is
as follows:
</p>
<p>1) Order the data according to Xj and choose a base of at least k observations from among
the central observations.
</p>
<p>2) From the first m observations compute the vector of recursive residuals w1 using the base
constructed in step 1. Also, compute the vector of recursive residuals w2 from the last m
observations. The maximum m can be is (T &minus; k)/2.</p>
<p/>
</div>
<div class="page"><p/>
<p>192 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>3) Under the null hypothesis, it follows that
</p>
<p>F = w&prime;2w2/w
&prime;
1w1 &sim; Fm,m (8.41)
</p>
<p>Harvey and Phillips suggest setting m at approximately (n/3) provided n &gt; 3k. This test
has the advantage over the Goldfeld-Quandt test in that if one wanted to test whether
σ2i varies with some other variable Xs, one could simply regroup the existing recursive
residuals according to low and high values of Xs and compute (8.41) afresh, whereas the
Goldfeld-Quandt test would require the computation of two new regressions.
</p>
<p>(3) Phillips and Harvey (1974) suggest using the recursive residuals to test the null hy-
pothesis of no serial correlation using a modified von Neuman ratio:
</p>
<p>MVNR =
</p>
<p>&sum;T
t=k+2(wt &minus; wt&minus;1)2/(T &minus; k &minus; 1)&sum;T
</p>
<p>t=k+1w
2
t /(T &minus; k)
</p>
<p>(8.42)
</p>
<p>This is the ratio of the mean-square successive difference to the variance. It is arithmetically
closely related to the DW statistic, but given that w &sim; N(0, σ2IT&minus;k) one has an exact test
available and no inconclusive regions. Phillips and Harvey (1974) provide tabulations of the
significance points. If the sample size is large, a satisfactory approximation is obtained from a
normal distribution with mean 2 and variance 4/(T &minus; k).
</p>
<p>(4) Harvey and Collier (1977) suggest a test for functional misspecification based on
recursive residuals. This is based on the fact that w &sim; N(0, σ2IT&minus;k). Therefore,
</p>
<p>w̄/(sw/
&radic;
T &minus; k) &sim; tT&minus;k&minus;1 (8.43)
</p>
<p>where w̄ =
&sum;T
</p>
<p>t=k+1wt/(T &minus; k) and s2w =
&sum;T
</p>
<p>t=k+1(wt &minus; w̄)2/(T &minus; k &minus; 1). Suppose that the true
functional form relating y to a single explanatory variable X is concave (convex) and the data
are ordered by X. A simple linear regression is estimated by regressing y on X. The recursive
residuals would be expected to be mainly negative (positive) and the computed t-statistic will be
large in absolute value. When there are multiple X&rsquo;s, one could carry out this test based on any
single explanatory variable. Since several specification errors might have a self-cancelling effect
on the recursive residuals, this test is not likely to be very effective in multivariate situations.
Wu (1993) suggested performing this test using the following augmented regression:
</p>
<p>y = Xβ + zγ + v (8.44)
</p>
<p>where z = C &prime;ιT&minus;k is one additional regressor with C defined in (8.34) and ιT&minus;k denoting a
vector of ones of dimension T &minus; k. In fact, the F -statistic for testing H0; γ = 0 turns out to be
the square of the Harvey and Collier (1977) t-statistic given in (8.43), see problem 15.
</p>
<p>Alternatively, a Sign test may be used to test the null hypothesis of no functional misspecifi-
cation. Under the null hypothesis, the expected number of positive recursive residuals is equal
to (T &minus; k)/2. A critical region may therefore be constructed from the binomial distribution.
However, Harvey and Collier (1977) suggest that the Sign test tends to lack power compared
with the t-test described in (8.43). Nevertheless, it is very simple and it may be more robust to
non-normality.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Recursive Residuals 193
</p>
<p>(5) Brown, Durbin and Evans (1975) used recursive residuals to test for structural change
over time. The null hypothesis is
</p>
<p>H0;
</p>
<p>{
β1 = β2 = .. = βT = β
σ21 = σ
</p>
<p>2
2 = .. = σ
</p>
<p>2
T = σ
</p>
<p>2 (8.45)
</p>
<p>where βt is the vector of coefficients in period t and σ
2
t is the disturbance variance for that
</p>
<p>period. The authors suggest a pair of tests. The first is the CUSUM test which computes
</p>
<p>Wr =
&sum;r
</p>
<p>t=k+1wt/sw for r = k + 1, . . . , T (8.46)
</p>
<p>where s2w is an estimate of the variance of the wt&rsquo;s, given below (8.43). Wr is a cumulative sum
and should be plotted against r. Under the null, E(Wr) = 0. But, if there is a structural break,
Wr will tend to diverge from the horizontal line. The authors suggest checking whether Wr
cross a pair of straight lines (see Figure 8.1) which pass through the points
</p>
<p>{
k,&plusmn;a
</p>
<p>&radic;
T &minus; k
</p>
<p>}
and{
</p>
<p>T,&plusmn;3a
&radic;
T &minus; k
</p>
<p>}
where a depends upon the chosen significance level α. For example, a =
</p>
<p>0.850, 0.948, and 1.143 for α = 10%, 5%, and 1% levels, respectively.
If the coefficients are not constant, there may be a tendency for a disproportionate number
</p>
<p>of recursive residuals to have the same sign and to push Wr across the boundary. The second
test is the cumulative sum of squares (CUSUMSQ) which is based on plotting
</p>
<p>W &lowast;r =
&sum;r
</p>
<p>t=k+1w
2
t /
</p>
<p>&sum;T
t=k+1w
</p>
<p>2
t for t = k + 1, . . . , T (8.47)
</p>
<p>against r. Under the null, E(W &lowast;r ) = (r &minus; k)/(T &minus; k) which varies from 0 for r = k to 1
for r = T . The significance of the departure of W &lowast;r from its expected value is assessed by
whether W &lowast;r crosses a pair of lines parallel to E(W
</p>
<p>&lowast;
r ) at a distance co above and below this line.
</p>
<p>Brown, Durbin and Evans (1975) provide values of c0 for various sample sizes T and levels of
significance α.
The CUSUM and CUSUMSQ should be regarded as data analytic techniques; i.e., the value of
</p>
<p>the plots lie in the information to be gained simply by inspecting them. The plots contain more
information than can be summarized in a single test statistic. The significance lines constructed
are, to paraphrase the authors, best regarded as &lsquo;yardsticks&rsquo; against which to assess the observed
plots rather than as formal tests of significance. See Brown et al. (1975) for various examples.
Note that the CUSUM and CUSUMSQ are quite general tests for structural change in that
they do not require a prior determination of where the structural break takes place. If this is
known, the Chow-test will be more powerful. But, if this break is not known, the CUSUM and
CUSUMSQ are more appropriate.
</p>
<p>Example 2: Table 8.5 reproduces the consumption-income data, over the period 1959&ndash;2007,
taken from the Economic Report of the President. In addition, the recursive residuals are
computed as in (8.30) and exhibited in column 5, starting with 1961 and ending in 2007. The
CUSUM given by Wr in (8.46) is plotted against r in Figure 8.2. The CUSUM crosses the
upper 5% line in 1998, showing structural instability in the latter years. This was done using
EViews 6.
The post-sample predictive test for 1998, can be obtained from (8.38) by computing the RSS
</p>
<p>from 1950&ndash;1997 and comparing it with the RSS from 1950-2007. The observed F -statistic is
5.748 which is distributed as F (10, 37). Using EViews, one clicks on stability diagnostics and
then selects Chow forecast test. You will be prompted to enter the break point period which</p>
<p/>
</div>
<div class="page"><p/>
<p>194 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>���3
</p>
<p>�
�
</p>
<p>n
0
</p>
<p>k t
</p>
<p>���
</p>
<p>���2
</p>
<p>���3
</p>
<p>���
</p>
<p>&ndash;
</p>
<p>&ndash;
</p>
<p>&ndash;
</p>
<p>&ndash;
</p>
<p>&ndash;
</p>
<p>&ndash;
</p>
<p>&ndash;
</p>
<p>Figure 8.1 CUSUM Critical Values
</p>
<p>Table 8.4 Chow Forecast Test
</p>
<p>Specification: CONSUM C Y
Test predictions for observations from 1998 to 2007
</p>
<p>Value df Probability
</p>
<p>F-statistic 5.747855 (10,37) 0.0000
Likelihood ratio 45.93529 10 0.0000
F-test summary:
</p>
<p>Sum of Sq. df Mean Squares
</p>
<p>Test SSR 5476210. 10 547621.0
Restricted SSR 9001348. 47 191518.0
Unrestricted SSR 3525138. 37 95273.99
</p>
<p>LR test summary:
Value df
</p>
<p>Restricted LogL &ndash;366.4941 47
Unrestricted LogL &ndash;343.5264 37
</p>
<p>in this case is 1998. EViews gives the back up regression which is not shown here, and also
performs a likelihood ratio test, see Table 8.4.
</p>
<p>The reader can verify that the same F -statistic can be obtained from (8.40) using the recursive
residuals in Table 8.5. In fact,
</p>
<p>F = (
&sum;2007
</p>
<p>t=1998 w
2
t /10)/(
</p>
<p>&sum;1997
t=1961w
</p>
<p>2
t /37) = 5.748</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Recursive Residuals 195
</p>
<p>Table 8.5 Recursive Residuals for the Consumption Regression
</p>
<p>Year CONSUM Income RESID Recursive RES
</p>
<p>1959 8776 9685 635.4909 NA
1960 8837 9735 647.5295 NA
1961 8873 9901 520.9776 &ndash;30.06109
1962 9170 10227 498.7493 53.63333
1963 9412 10455 517.4853 57.07454
1964 9839 11061 351.0732 &ndash;14.42043
1965 10331 11594 321.1447 40.23840
1966 10793 12065 321.9283 72.59054
1967 10994 12457 139.0709 &ndash;58.72718
1968 11510 12892 229.1068 88.63871
1969 11820 13163 273.7360 125.0883
1970 11955 13563 17.04481 &ndash;88.54736
1971 12256 14001 &ndash;110.8570 &ndash;123.0740
1972 12868 14512 0.757470 68.23355
1973 13371 15345 &ndash;311.9394 &ndash;118.2972
1974 13148 15094 &ndash;289.1532 &ndash;100.8288
1975 13320 15291 &ndash;310.0611 &ndash;72.86693
1976 13919 15738 &ndash;148.7760 148.9270
1977 14364 16128 &ndash;85.67493 231.2810
1978 14837 16704 &ndash;176.7102 178.9840
1979 15030 16931 &ndash;205.9950 147.8067
1980 14816 16940 &ndash;428.8080 &ndash;80.37207
1981 14879 17217 &ndash;637.0542 &ndash;229.1660
1982 14944 17418 &ndash;768.8790 &ndash;296.0910
1983 15656 17828 &ndash;458.3625 86.49899
1984 16343 19011 &ndash;929.7892 &ndash;205.6594
1985 17040 19476 &ndash;688.1302 111.3357
1986 17570 19906 &ndash;579.1982 251.5306
1987 17994 20072 &ndash;317.7500 479.8759
1988 18554 20740 &ndash;411.8743 405.8181
1989 18898 21120 &ndash;439.9809 366.8060
1990 19067 21281 &ndash;428.6367 347.8156
1991 18848 21109 &ndash;479.2094 243.0261
1992 19208 21548 &ndash;549.0905 195.0177
1993 19593 21493 &ndash;110.2330 588.3097
1994 20082 21812 66.39330 731.2551
1995 20382 22153 32.47656 660.7508
1996 20835 22546 100.6400 696.2055
1997 21365 23065 122.4207 689.6197
1998 22183 24131 &ndash;103.4364 474.3981
1999 23050 24564 339.5579 870.8977
2000 23862 25472 262.4189 751.2861
2001 24215 25697 395.0926 808.6041
2002 24632 26238 282.3303 639.0555
2003 25073 26566 402.1435 700.0686
2004 25750 27274 385.8501 633.1310
2005 26290 27403 799.5297 970.8717
2006 26835 28098 663.9663 760.6385
2007 27319 28614 642.6847 673.7335</p>
<p/>
</div>
<div class="page"><p/>
<p>196 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>-20
</p>
<p>-10
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>65 70 75 80 85 90 95 00 05
</p>
<p>CUSUM 5% Significance
</p>
<p>Figure 8.2 CUSUM Plot of the Consumption Regression
</p>
<p>8.3 Specification Tests
</p>
<p>Specification tests are an important part of model specification in econometrics. In this section,
we only study a few of these diagnostic tests. For an excellent summary on this topic, see
Wooldridge (2001).
</p>
<p>(1) Ramsey&rsquo;s (1969) RESET (Regression Specification Error Test)
</p>
<p>Ramsey suggests testing the specification of the linear regression model yt = X
&prime;
tβ + ut by
</p>
<p>augmenting it with a set of regressors Zt so that the augmented model is
</p>
<p>yt = X
&prime;
tβ + Z
</p>
<p>&prime;
tγ + ut (8.48)
</p>
<p>If the Zt&rsquo;s are available then the specification test would reduce to the F -test for H0; γ = 0.
The crucial issue is the choice of Zt variables. This depends upon the true functional form under
the alternative, which is usually unknown. However, this can be often well approximated by
higher powers of the initial regressors, as in the case where the true form is quadratic or cubic.
Alternatively, one might approximate it with higher moments of ŷt = X
</p>
<p>&prime;
tβ̂OLS . The popular
</p>
<p>Ramsey RESET test is carried out as follows:
</p>
<p>(1) Regress yt on Xt and get ŷt.
</p>
<p>(2) Regress yt on Xt, ŷ
2
t , ŷ
</p>
<p>3
t and ŷ
</p>
<p>4
t and test that the coefficients of all the powers of ŷt are
</p>
<p>zero. This is an F3,T&minus;k&minus;3 under the null.
</p>
<p>Note that ŷt is not included among the regressors because it would be perfectly multicollinear
with Xt.
</p>
<p>3 Different choices of Zt&rsquo;s may result in more powerful tests when H0 is not true.
Thursby and Schmidt (1977) carried out an extensive Monte Carlo and concluded that the test
based on Zt = [X
</p>
<p>2
t , X
</p>
<p>3
t , X
</p>
<p>4
t ] seems to be generally the best choice.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Specification Tests 197
</p>
<p>(2) Utts&rsquo; (1982) Rainbow Test
</p>
<p>The basic idea behind the Rainbow test is that even when the true relationship is nonlinear, a
good linear fit can still be obtained over subsets of the sample. The test therefore rejects the
null hypothesis of linearity whenever the overall fit is markedly inferior to the fit over a properly
selected sub-sample of the data, see Figure 8.3.
</p>
<p>��
��
</p>
<p>��
</p>
<p>��
</p>
<p>��
��
</p>
<p>��
��
</p>
<p>��
</p>
<p>��
</p>
<p>��
</p>
<p>��
</p>
<p>��
</p>
<p>��
��
</p>
<p>��
��
</p>
<p>��
</p>
<p>��
</p>
<p>��
</p>
<p>��
</p>
<p>�� ��
</p>
<p>��
</p>
<p>��
��
</p>
<p>��
</p>
<p>��
</p>
<p>��
</p>
<p>y
</p>
<p>X
</p>
<p>Figure 8.3 The Rainbow Test
</p>
<p>Let e&prime;e be the OLS residuals sum of squares from all available n observations and let ẽ&prime;ẽ be the
OLS residual sum of squares from the middle half of the observations (T/2). Then
</p>
<p>F =
(e&prime;e&minus; ẽ&prime;ẽ)/
</p>
<p>(
T
2
</p>
<p>)
</p>
<p>ẽ&prime;ẽ/
(
T
2 &minus; k
</p>
<p>) is distributed as FT
2
,(T2 &minus;k)
</p>
<p>under H0 (8.49)
</p>
<p>Under H0; E(e
&prime;e/(T&minus;k)) = σ2 = E
</p>
<p>[
ẽ&prime;ẽ/
</p>
<p>(
T
2 &minus; k
</p>
<p>)]
, while in general underHA; E(e
</p>
<p>&prime;e/(T&minus;k)) &gt;
E
[
ẽ&prime;ẽ/
</p>
<p>(
T
2 &minus; k
</p>
<p>)]
&gt; σ2. The RRSS is e&prime;e because all the observations are forced to fit the straight
</p>
<p>line, whereas the URSS is ẽ&prime;ẽ because only a part of the observations are forced to fit a straight
line. The crucial issue of the Rainbow test is the proper choice of the subsample (the middle T/2
observations in case of one regressor). This affects the power of the test and not the distribution
of the test statistic under the null. Utts (1982) recommends points close to X̄, since an incorrect
linear fit will in general not be as far off there as it is in the outer region. Closeness to X̄ is
measured by the magnitude of the corresponding diagonal elements of PX . Close points are
those with low leverage hii, see section 8.1. The optimal size of the subset depends upon the
alternative. Utts recommends about 1/2 of the data points in order to obtain some robustness
to outliers. The F -test in (8.49) looks like a Chow test, but differs in the selection of the
sub-sample. For example, using the post-sample predictive Chow test, the data are arranged
according to time and the first T observations are selected. The Rainbow test arranges the data
according to their distance from X̄ and selects the first T/2 of them.
</p>
<p>(3) Plosser, Schwert and White (1982) (PSW) Differencing Test
</p>
<p>The differencing test is a general test for misspecification (like Hausman&rsquo;s (1978) test, which
will be introduced in the simultaneous equation chapter) but for time-series data only. This test
compares OLS and First Difference (FD) estimates of β. Let the differenced model be</p>
<p/>
</div>
<div class="page"><p/>
<p>198 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>ẏ = Ẋβ + u̇ (8.50)
</p>
<p>where ẏ = Dy, Ẋ = DX and u̇ = Du where
</p>
<p>D =
</p>
<p>⎡
⎢⎢⎢⎢⎣
</p>
<p>1 &minus;1 0 0 . . . 0 0
0 1 &minus;1 0 . . . 0 0
0 0 1 &minus;1 . . . 0 0
. . . . . . . . .
0 0 0 0 . . . 1 &minus;1
</p>
<p>⎤
⎥⎥⎥⎥⎦
</p>
<p>is the familiar (T &minus; 1)&times; T differencing matrix.
</p>
<p>Wherever there is a constant in the regression, the first column of X becomes zero and is
dropped. From (8.50), the FD estimator is given by
</p>
<p>β̃FD = (Ẋ
&prime;Ẋ)&minus;1Ẋ &prime;ẏ (8.51)
</p>
<p>with var(β̃FD) = σ
2(Ẋ &prime;Ẋ)&minus;1Ẋ &prime;DD&prime;Ẋ(Ẋ &prime;Ẋ)&minus;1 since var(u̇) = σ2(DD)&prime; and
</p>
<p>DD&prime; =
</p>
<p>⎡
⎢⎢⎢⎢⎣
</p>
<p>2 &minus;1 0 . . . 0 0
&minus;1 2 &minus;1 . . . 0 0
: : : : : :
0 0 0 . . . 2 &minus;1
0 0 0 . . . &minus;1 2
</p>
<p>⎤
⎥⎥⎥⎥⎦
</p>
<p>The differencing test is based on
</p>
<p>q̂ = β̃FD &minus; β̂OLS with V (q̂) = σ2[V (β̃FD)&minus; V (β̂OLS)] (8.52)
</p>
<p>A consistent estimate of V (q̂) is
</p>
<p>V̂ (q̂) = σ̂2
</p>
<p>⎡
⎣
(
Ẋ &prime;Ẋ
T
</p>
<p>)&minus;1(
Ẋ &prime;DD&prime;Ẋ
</p>
<p>T
</p>
<p>)(
Ẋ &prime;Ẋ
T
</p>
<p>)&minus;1
&minus;
(
X &prime;X
T
</p>
<p>)&minus;1
⎤
⎦ (8.53)
</p>
<p>where σ̂2 is a consistent estimate of σ2. Therefore,
</p>
<p>Δ = T q̂&prime;[V̂ (q̂)]&minus;1q̂ &sim; χ2k under H0 (8.54)
</p>
<p>where k is the number of slope parameters if V̂ (q̂) is nonsingular. V̂ (q̂) could be singular, in
which case we use a generalized inverse V̂ &minus;(q̂) of V̂ (q̂) and in this case is distributed as χ2
</p>
<p>with degrees of freedom equal to the rank(V̂ (q̂)). This is a special case of the general Hausman
(1978) test which will be studied extensively in Chapter 11.
</p>
<p>Davidson, Godfrey, and MacKinnon (1985) show that, like the Hausman test, the PSW test
is equivalent to a much simpler omitted variables test, the omitted variables being the sum of
the lagged and one-period ahead values of the regressors.
</p>
<p>Thus if the regression equation we are considering is
</p>
<p>yt = β1x1t + β2x2t + ut (8.55)
</p>
<p>the PSW test involves estimating the expanded regression equation
</p>
<p>yt = β1x1t + β2x2t + γ1z1t + γ2z2t + ut (8.56)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Specification Tests 199
</p>
<p>where z1t = x1,t+1 + x1,t&minus;1 and z2t = x2,t+1 + x2,t&minus;1 and testing the hypothesis γ1 = γ2 = 0 by
the usual F -test.
</p>
<p>If there are lagged dependent variables in the equation, the test needs a minor modification.
Suppose that the model is
</p>
<p>yt = β1yt&minus;1 + β2xt + ut (8.57)
</p>
<p>Now the omitted variables would be defined as z1t = yt+yt&minus;2 and z2t = xt+1+xt&minus;1. There is no
problem with z2t but z1t would be correlated with the error term ut because of the presence of
yt in it. The solution would be simply to transfer it to the left hand side and write the expanded
regression equation in (8.56) as
</p>
<p>(1&minus; γ1)yt = β1yt&minus;1 + β2xt + γ1yt&minus;2 + γ2z2t + ut (8.58)
</p>
<p>This equation can be written as
</p>
<p>yt = β
&lowast;
1yt&minus;1 + β
</p>
<p>&lowast;
2xt + γ
</p>
<p>&lowast;
1yt&minus;2 + γ
</p>
<p>&lowast;
2z2t + u
</p>
<p>&lowast;
t (8.59)
</p>
<p>where all the starred parameters are the corresponding unstarred ones divided by (1&minus; γ1).
The PSW now tests the hypothesis γ&lowast;1 = γ
</p>
<p>&lowast;
2 = 0. Thus, in the case where the model involves
</p>
<p>the lagged dependent variable yt&minus;1 as an explanatory variable, the only modification needed
is that we should use yt&minus;2 as the omitted variable, not (yt + yt&minus;2). Note that it is only yt&minus;1
that creates a problem, not higher-order lags of yt, like yt&minus;2, yt&minus;3, and so on. For yt&minus;2, the
corresponding zt will be obtained by adding yt&minus;1 to yt&minus;3. This zt is not correlated with ut as
long as the disturbances are not serially correlated.
</p>
<p>(4) Tests for Non-nested Hypothesis
</p>
<p>Consider the following two competing non-nested models:
</p>
<p>H1; y = X1β1 + ǫ1 (8.60)
</p>
<p>H2; y = X2β2 + ǫ2 (8.61)
</p>
<p>These are non-nested because the explanatory variables under one model are not a subset of
the other model even though X1 and X2 may share some common variables. In order to test H1
versus H2, Cox (1961) modified the LR-test to allow for the non-nested case. The idea behind
Cox&rsquo;s approach is to consider to what extent Model I under H1, is capable of predicting the
performance of Model II, under H2.
Alternatively, one can artificially nest the 2 models
</p>
<p>H3; y = X1β1 +X
&lowast;
2β
</p>
<p>&lowast;
2 + ǫ3 (8.62)
</p>
<p>where X&lowast;2 excludes from X2 the common variables with X1. A test for H1 is simply the F -test
for H0; β
</p>
<p>&lowast;
2 = 0.
</p>
<p>Criticism: This tests H1 versus H3 which is a (Hybrid) of H1 and H2 and not H1 versus H2.
Davidson andMacKinnon (1981) proposed (testingα=0) in the linear combination ofH1 andH2:
</p>
<p>y = (1&minus; α)X1β1 + αX2β2 + ǫ (8.63)
</p>
<p>where α is an unknown scalar. Since α is not identified, we replace β2 by β̂2,OLS = (X
&prime;
2X2/T )
</p>
<p>&minus;1
</p>
<p>(X &prime;2y/T ) the regression coefficient estimate obtained from running y on X2 under H2, i.e., (1)</p>
<p/>
</div>
<div class="page"><p/>
<p>200 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Run y on X2 get ŷ2 = X2β̂2,OLS ; (2) Run y on X1 and ŷ2 and test that the coefficient of ŷ2 is
zero. This is known as the J-test and this is asymptotically N(0, 1) under H1.
</p>
<p>Fisher and McAleer (1981) suggested a modification of the J-test known as the JA test.
</p>
<p>Under H1; plimβ̂2 = plim(X
&prime;
2X2/T )
</p>
<p>&minus;1plim(X &prime;2X1/T )β1 + 0 (8.64)
</p>
<p>Therefore, they propose replacing β̂2 by β̃2 = (X
&prime;
2X2)
</p>
<p>&minus;1(X &prime;2X1)β̂1,OLS where β̂1,OLS = (X
&prime;
1X1)
</p>
<p>&minus;1
</p>
<p>X &prime;1y. The steps for the JA-test are as follows:
</p>
<p>1. Run y on X1 get ŷ1 = X1β̂1,OLS .
</p>
<p>2. Run ŷ1 on X2 get ỹ2 = X2(X
&prime;
2X2)
</p>
<p>&minus;1X &prime;2ŷ1.
</p>
<p>3. Run y on X1 and ỹ2 and test that the coefficient of ỹ2 is zero. This is the simple t-statistic
on the coefficient of ỹ2. The J and JA tests are asymptotically equivalent.
</p>
<p>Criticism: Note the asymmetry of H1 and H2. Therefore one should reverse the role of these
hypotheses and test again.
In this case one can get the four scenarios depicted in Table 8.6. In case both hypotheses are
</p>
<p>not rejected, the data are not rich enough to discriminate between the two hypotheses. In case
both hypotheses are rejected neither model is useful in explaining the variation in y. In case
one hypothesis is rejected while the other is not, one should remember that the non-rejected
hypothesis may still be brought down by another challenger hypothesis.
</p>
<p>Small Sample Properties: (i) The J-test tends to reject the null more frequently than it
should. Also, the JA test has relatively low power when K1, the number of parameters in H1 is
larger than K2, the number of parameters in H2. Therefore, one should use the JA test when
K1 is about the same size as K2, i.e., the same number of non-overlapping variables. (ii) If both
H1 and H2 are false, these tests are inferior to the standard diagnostic tests. In practice, use
higher significance levels for the J-test, and supplement it with the artificially nested F -test
and standard diagnostic tests.
</p>
<p>Table 8.6 Non-nested Hypothesis Testing
</p>
<p>α = 0
</p>
<p>Not Rejected Rejected
</p>
<p>Both H1 and H2 H1 rejectedα = 1 Not Rejected
are not rejected H2 not rejected
</p>
<p>H1 not rejected Both H1 and H2Rejected
H2 rejected are rejected
</p>
<p>Note: J and JA tests are one degree of freedom tests, whereas the artificially nested F -test is
not.
For a recent summary of non-nested hypothesis testing, see Pesaran and Weeks (2001). Exam-
</p>
<p>ples of non-nested hypothesis encountered in empirical economic research include linear versus
log-linear models, see section 8.5. Also, logit versus probit models in discrete choice, see Chap-
ter 13 and exponential versus Weibull distributions in the analysis of duration data. In the</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Specification Tests 201
</p>
<p>logit versus probit specification, the set of regressors is most likely to be the same. It is only
the form of the distribution functions that separate the two models. Pesaran and Weeks (2001,
p. 287) emphasize the differences between hypothesis testing and model selection:
</p>
<p>The model selection process treats all models under consideration symmetrically,
while hypothesis testing attributes a different status to the null and to the alternative
hypotheses and by design treats the models asymmetrically. Model selection always
ends in a definite outcome, namely one of the models under consideration is selected
for use in decision making. Hypothesis testing on the other hand asks whether there
is any statistically significant evidence (in the Neyman-Pearson sense) of departure
from the null hypothesis in the direction of one or more alternative hypotheses.
Rejection of the null hypothesis does not necessarily imply acceptance of any one of
the alternative hypotheses; it only warns the investigator of possible shortcomings of
the null that is being advocated. Hypothesis testing does not seek a definite outcome
and if carried out with due care need not lead to a favorite model. For example, in the
case of nonnested hypothesis testing it is possible for all models under consideration
to be rejected, or all models to be deemed as observationally equivalent.
</p>
<p>They conclude that the choice between hypothesis testing and model selection depends on the
primary objective of one&rsquo;s study. Model selection may be more appropriate when the objective
is decision making, while hypothesis testing is better suited to inferential problems.
</p>
<p>A model may be empirically adequate for a particular purpose, but of little relevance
for another use... In the real world where the truth is elusive and unknowable both
approaches to model evaluation are worth pursuing.
</p>
<p>(5) White&rsquo;s (1982) Information-Matrix (IM) Test
</p>
<p>This is a general specification test much like the Hausman (1978) specification test which will
be considered in details in Chapter 11. The latter is based on two different estimates of the
regression coefficients, while the former is based on two different estimates of the Information
Matrix I(θ) where θ&prime; = (β&prime;, σ2) in the case of the linear regression studied in Chapter 7. The
first estimate of I(θ) evaluates the expectation of the second derivatives of the log-likelihood
at the MLE, i.e., &minus;E(&part;2logL/&part;θ&part;θ&prime;) at θ̂mle while the second sum up the outer products of
the score vectors
</p>
<p>&sum;n
i=1(&part;logLi(θ)/&part;θ)(&part;logLi(θ)/&part;θ)
</p>
<p>&prime; evaluated at θ̂mle. This is based on the
fundamental identity that
</p>
<p>I(θ) = &minus;E(&part;2logL/&part;θ&part;θ&prime;) = E(&part;logL/&part;θ)(&part;logL/&part;θ)&prime;
</p>
<p>If the model estimated by MLE is not correctly specified, this equality will not hold. From Chap-
ter 7, equation (7.19), we know that for the linear regression model with normal disturbances,
the first estimate of I(θ) denoted by I1(θ̂mle) is given by
</p>
<p>I1(θ̂MLE) =
</p>
<p>[
X &prime;X/σ̂2 0
</p>
<p>0 n/2σ̂4
</p>
<p>]
(8.65)
</p>
<p>where σ̂2 = e&prime;e/n is the MLE of σ2 and e denotes the OLS residuals.</p>
<p/>
</div>
<div class="page"><p/>
<p>202 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Similarly, one can show that the second estimate of I(θ) denoted by I2(θ) is given by
</p>
<p>I2(θ) =
&sum;n
</p>
<p>i=1
</p>
<p>(
&part;logLi(θ)
</p>
<p>&part;θ
</p>
<p>)(
&part;logLi(θ)
</p>
<p>&part;θ
</p>
<p>)&prime;
</p>
<p>=
&sum;n
</p>
<p>i=1
</p>
<p>⎡
⎢⎣
</p>
<p>u2ixix
&prime;
i
</p>
<p>σ4
&minus;uixi
2σ4
</p>
<p>+
u3ixi
2σ6
</p>
<p>&minus;uix
&prime;
i
</p>
<p>2σ4
+
</p>
<p>u3ix
&prime;
i
</p>
<p>2σ6
1
</p>
<p>4σ4
&minus; u
</p>
<p>2
i
</p>
<p>2σ6
+
</p>
<p>u4i
4σ8
</p>
<p>⎤
⎥⎦ (8.66)
</p>
<p>where xi is the i-th row of X. Substituting the MLE we get
</p>
<p>I2(θ̂MLE) =
</p>
<p>⎡
⎢⎣
</p>
<p>&sum;n
i=1 e
</p>
<p>2
ixix
</p>
<p>&prime;
i
</p>
<p>σ̂4
</p>
<p>&sum;n
i=1 e
</p>
<p>3
ixi
</p>
<p>2σ̂6&sum;n
i=1 e
</p>
<p>3
ix
</p>
<p>&prime;
i
</p>
<p>2σ̂6
&minus; n
4σ̂4
</p>
<p>+
</p>
<p>&sum;n
i=1 e
</p>
<p>4
i
</p>
<p>4σ̂8
</p>
<p>⎤
⎥⎦ (8.67)
</p>
<p>where we used the fact that
&sum;n
</p>
<p>i=1 eixi = 0. If the model is correctly specified and the distur-
bances are normal then
</p>
<p>plim I1(θ̂MLE)/n = plim I2(θ̂MLE)/n = I(θ)
</p>
<p>Therefore, the Information Matrix (IM) test rejects the model when
</p>
<p>[I2(θ̂MLE)&minus; I1(θ̂MLE)]/n (8.68)
</p>
<p>is too large. These are two matrices with (k + 1) by (k + 1) elements since β is k &times; 1 and σ2
is a scalar. However, due to symmetry, this reduces to (k + 2)(k + 1)/2 unique elements. Hall
(1987) noted that the first k(k + 1)/2 unique elements obtained from the first k &times; k block of
(8.68) have a typical element
</p>
<p>&sum;n
i=1(e
</p>
<p>2
i &minus; σ̂2)xirxis/nσ̂4 where r and s denote the r-th and s-th
</p>
<p>explanatory variables with r, s = 1, 2, . . . , k. This term measures the discrepancy between the
OLS estimates of the variance-covariance matrix of β̂OLS and its robust counterpart suggested
by White (1980), see Chapter 5. The next k unique elements correspond to the off-diagonal block&sum;n
</p>
<p>i=1 e
3
ixi/2nσ̂
</p>
<p>6and this measures the discrepancy between the estimates of the cov(β̂, σ̂2). The
last element correspond to the difference in the bottom right elements, i.e., the two estimates
of σ̂2. This is given by
</p>
<p>[
&minus; 3
4σ̂4
</p>
<p>+
1
</p>
<p>n
</p>
<p>&sum;n
i=1 e
</p>
<p>4
i /4σ̂
</p>
<p>8
</p>
<p>]
</p>
<p>These (k+1)(k+2)/2 unique elements can be arranged in vector form D(θ) which has a limiting
normal distribution with zero mean and some covariance matrix V (θ) under the null. One can
show, see Hall (1987) or Krämer and Sonnberger (1986) that if V (θ) is estimated from the
sample moments of these terms, that the IM test statistic is given by
</p>
<p>m = nD&prime;(θ)[V (θ)]&minus;1D(θ)
H0&rarr; χ2(k+1)(k+2)/2 (8.69)
</p>
<p>In fact, Hall (1987) shows that this statistic is the sum of three asymptotically independent
terms
</p>
<p>m = m1 +m2 +m3 (8.70)
</p>
<p>where m1 = a particular version of White&rsquo;s heteroskedasticity test; m2 = n times the explained
sum of squares from the regression of e3i on xi divided by 6σ̂
</p>
<p>6; and</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Specification Tests 203
</p>
<p>m3 =
n
</p>
<p>24σ̂8
(&sum;n
</p>
<p>i=1 e
4
i /n&minus; 3σ̂4
</p>
<p>)2
</p>
<p>which is similar to the Jarque-Bera test for normality of the disturbances given in Chapter 5.
It is clear that the IM test will have power whenever the disturbances are non-normal or
</p>
<p>heteroskedastic. However, Davidson and MacKinnon (1992) demonstrated that the IM test
considered above will tend to reject the model when true, much too often, in finite samples. This
problem gets worse as the number of degrees of freedom gets large. In Monte Carlo experiments,
Davidson and MacKinnon (1992) showed that for a linear regression model with ten regressors,
the IM test rejected the null at the 5% level, 99.9% of the time for n = 200. This problem did
not disappear when n increased. In fact, for n = 1000, the IM test still rejected the null 92.7%
of the time at the 5% level.
</p>
<p>These results suggest that it may be more useful to run individual tests for non-normality,
heteroskedasticity and other misspecification tests considered above rather than run the IM
test. These tests may be more powerful and more informative than the IM test. Alternative
methods of calculating the IM test with better finite-sample properties are suggested in Orme
(1990), Chesher and Spady (1991) and Davidson and MacKinnon (1992).
</p>
<p>Example 3: For the consumption-income data given in Table 5.3, we first compute the RESET
test from the consumption-income regression given in Chapter 5. Using EViews, one clicks on
stability tests and then selects RESET. You will be prompted with the option of the number
of fitted terms to include (i.e., powers of ŷ). Table 8.7 shows the RESET test including ŷ2 and
ŷ3. The F -statistic for their joint-significance is equal to 94.94. This is significant and indicates
misspecification.
</p>
<p>Table 8.7 Ramsey RESET Test
</p>
<p>F-statistic 94.93796 Prob. F(2,45) 0.00000
Log likelihood ratio 80.96735 Prob. Chi-Square(2) 0.00000
</p>
<p>Test Equation:
Dependent Variable: CONSUM
Method: Least Squares
Sample: 1959 2007
Included observations: 49
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C 3519.599 1141.261 3.083956 0.0035
Y 0.421587 0.173597 2.428540 0.0192
FITTEDˆ2 1.99E-05 1.09E-05 1.834317 0.0732
FITTEDˆ3 &ndash;1.18E-10 2.10E-10 &ndash;0.560377 0.5780
</p>
<p>R-squared 0.998789 Mean dependent var 16749.10
Adjusted R-squared 0.998708 S.D. dependent var 5447.060
S.E. of regression 195.7648 Akaike info criterion 13.46981
Sum squared resid 1724573. Schwarz criterion 13.62425
Log likelihood &ndash;326.0104 Hannan-Quinn criter. 13.52840
F-statistic 12372.26 Durbin-Watson stat 1.001605
Prob(F-statistic) 0.000000</p>
<p/>
</div>
<div class="page"><p/>
<p>204 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Table 8.8 Consumption Regression 1971&ndash;1995
</p>
<p>Dependent Variable: CONSUM
</p>
<p>Method: Least Squares
</p>
<p>Sample: 1971 1995
</p>
<p>Included observations: 25
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;1410.425 371.3812 &ndash;3.797783 0.0009
</p>
<p>Y 0.963780 0.020036 48.10199 0.0000
</p>
<p>R-squared 0.990157 Mean dependent var 16279.48
</p>
<p>Adjusted R-squared 0.989730 S.D. dependent var 2553.097
</p>
<p>S.E. of regression 258.7391 Akaike info criterion 14.02614
</p>
<p>Sum squared resid 1539756. Schwarz criterion 14.12365
</p>
<p>Log likelihood &ndash;173.3267 Hannan-Quinn criter. 14.05318
</p>
<p>F-statistic 2313.802 Durbin-Watson stat 0.613064
</p>
<p>Prob(F-statistic) 0.000000
</p>
<p>Next, we compute Utts (1982) Rainbow test. Table 8.8 gives the middle 25 observations of our
data, i.e., 1971-1995, and the EViews 6 regression using this data. The RSS of these middle
observations is given by ẽ&prime;ẽ = 1539756.14, while the RSS for the entire sample is given by
e&prime;e = 9001347.76 so that the observed F -statistic given in (8.49) can be computed as follows:
</p>
<p>F =
(9001347.76&minus; 1539756.14)/25
</p>
<p>1539756.14/23
= 4.46
</p>
<p>This is distributed as F25,23 under the null hypothesis and rejects the hypothesis of linearity.
The PSW differencing test is computed using the artificial regression given in (8.56) with
</p>
<p>Zt = Yt+1 + Yt&minus;1. The results are given in Table 8.9 using EViews 6. The t-statistic for Zt is
1.19 and has a p-value of 0.24 which is insignificant.
Now consider the two competing non-nested models:
</p>
<p>H1; Ct = β0 + β1Yt + β2Yt&minus;1 + ut H2; Ct = γ0 + γ1Yt + γ2Ct&minus;1 + vt
</p>
<p>The two non-nested models share Yt as a common variable. The artificial model that nests these
two models is given by:
</p>
<p>H3; Ct = δ0 + δ1Yt + δ2Yt&minus;1 + δ3Ct&minus;1 + ǫt
</p>
<p>Table 8.10, runs regression (1) given by H2 and obtains the predicted values Ĉ2(C2HAT ).
Regression (2) runs consumption on a constant, income, lagged income and C2HAT. The coef-
ficient of this last variable is 1.18 and is statistically significant with a t-value of 16.99. This is
the Davidson and MacKinnon (1981) J-test. In this case, H1 is rejected but H2 is not rejected.
The JA-test, given by Fisher and McAleer (1981) runs the regression in H1 and keeps the pre-
dicted values Ĉ1(C1HAT ). This is done in regression (3). Then C1HAT is run on a constant,
income and lagged consumption and the predicted values are stored as C̃2(C2TILDE ). This is
done in regression (5). The last step runs consumption on a constant, income, lagged income
and C2TILDE, see regression (6). The coefficient of this last variable is 97.43 and is statistically
significant with a t-value of 16.99. Again H1 is rejected but H2 is not rejected.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Specification Tests 205
</p>
<p>Table 8.9 Artificial Regression to compute the PSW Differencing Test
</p>
<p>Dependent Variable: CONSUM
Method: Least Squares
Sample (adjusted): 1960 2006
Included observations: 47 after adjustments
</p>
<p>Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;1373.390 226.1376 &ndash;6.073251 0.0000
Y 0.596293 0.321464 1.854930 0.0703
Z 0.191494 0.160960 1.189700 0.2405
</p>
<p>R-squared 0.993678 Mean dependent var 16693.85
Adjusted R-squared 0.993390 S.D. dependent var 5210.244
S.E. of regression 423.5942 Akaike info criterion 14.99713
Sum squared resid 7895011. Schwarz criterion 15.11523
Log likelihood &ndash;349.4326 Hannan-Quinn criter. 15.04157
F-statistic 3457.717 Durbin-Watson stat 0.119325
Prob(F-statistic) 0.000000
</p>
<p>Reversing the roles of H1 and H2, the J and JA-tests are repeated. In fact, regression (4) runs
consumption on a constant, income, lagged consumption and Ĉ1 (which was obtained from
regression (3)). The coefficient on Ĉ1 is &minus;15.20 and is statistically significant with a t-value
of &minus;6.5. This J-test rejects H2 but does not reject H1. Regression (7) runs Ĉ2 on a constant,
income and lagged income and the predicted values are stored as C̃1(C1TILDE). The last step of
the JA test runs consumption on a constant, income, lagged consumption and C̃1, see regression
(8). The coefficient of this last variable is &minus;1.11 and is statistically significant with a t-value of
&minus;6.5. This JA test rejects H2 but not H1. The artificial model, given in H3, is also estimated,
see regression (9). One can easily check that the corresponding F -tests reject H1 against H3
and also H2 against H3. In sum, all evidence indicates that both Ct&minus;1 and Yt&minus;1 are important
to include along with Yt. Of course, the true model is not known and could include higher lags
of both Yt and Ct.
</p>
<p>Stata 11 performs White&rsquo;s (1982) Information matrix test by issuing the command estat
imtest after running the regression of consumption on income. The results yield:
</p>
<p>. estat imtest
</p>
<p>Cameron &amp; Trivedi&rsquo;s decomposition of IM-test
</p>
<p>----------------------------------------------
</p>
<p>Source | chi2 df p
</p>
<p>--------------------+-------------------------
</p>
<p>Heteroskedasticity | 2.64 2 0.2677
</p>
<p>Skewness | 0.45 1 0.5030
</p>
<p>Kurtosis | 4.40 1 0.0359
</p>
<p>--------------------+-------------------------
</p>
<p>Total | 7.48 4 0.1124
</p>
<p>This does not reject the null even though Kurtosis seems to be a problem. Note that the IM
test is split into its components following Hall (1987) as described above.</p>
<p/>
</div>
<div class="page"><p/>
<p>206 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Table 8.10 Non-nested J and JA Tests for the Consumption Regression
</p>
<p>Regression 1
</p>
<p>Dependent Variable: CONSUM
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;254.5241 155.2906 &ndash;1.639019 0.1082
Y 0.211505 0.068310 3.096256 0.0034
CONSUM(&ndash;1) 0.800004 0.070537 11.34159 0.0000
</p>
<p>R-squared 0.998367 Mean dependent var 16915.21
Adjusted R-squared 0.998294 S.D. dependent var 5377.825
S.E. of regression 222.1108 Akaike info criterion 13.70469
Sum squared resid 2219995. Schwarz criterion 13.82164
Log likelihood &ndash;325.9126 Hannan-Quinn criter. 13.74889
F-statistic 13754.09 Durbin-Watson stat 0.969327
Prob(F-statistic) 0.000000
</p>
<p>Regression 2
</p>
<p>Dependent Variable: CONSUM
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C 144.3306 125.5929 1.149194 0.2567
Y 0.425354 0.090692 4.690091 0.0000
Y(&ndash;1) &ndash;0.613631 0.094424 -6.498678 0.0000
C2HAT 1.184853 0.069757 16.98553 0.0000
</p>
<p>R-squared 0.999167 Mean dependent var 16915.21
Adjusted R-squared 0.999110 S.D. dependent var 5377.825
S.E. of regression 160.4500 Akaike info criterion 13.07350
Sum squared resid 1132745. Schwarz criterion 13.22943
Log likelihood &ndash;309.7639 Hannan-Quinn criter. 13.13242
F-statistic 17585.25 Durbin-Watson stat 1.971939
Prob(F-statistic) 0.000000
</p>
<p>8.4 Nonlinear Least Squares and the Gauss-Newton Regression4
</p>
<p>So far we have been dealing with linear regressions. But, in reality, one might face a nonlinear
regression of the form:
</p>
<p>yt = xt(β) + ut for t = 1, 2, . . . , T (8.71)
</p>
<p>where ut &sim; IID(0, σ
2) and xt(β) is a scalar nonlinear regression function of k unknown param-
</p>
<p>eters β. It can be interpreted as the expected value of yt conditional on the values of the inde-</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Nonlinear Least Squares and the Gauss-Newton Regression 207
</p>
<p>Table 8.10 (continued)
</p>
<p>Regression 3
</p>
<p>Dependent Variable: CONSUM
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;1424.802 231.2843 &ndash;6.160393 0.0000
Y 0.943371 0.232170 4.063283 0.0002
Y(&ndash;1) 0.040368 0.234363 0.172244 0.8640
</p>
<p>R-squared 0.993702 Mean dependent var 16915.21
Adjusted R-squared 0.993423 S.D. dependent var 5377.825
S.E. of regression 436.1488 Akaike info criterion 15.05431
Sum squared resid 8560159. Schwarz criterion 15.17126
Log likelihood &ndash;358.3033 Hannan-Quinn criter. 15.09850
F-statistic 3550.327 Durbin-Watson stat 0.174411
Prob(F-statistic) 0.000000
</p>
<p>Regression 4
</p>
<p>Dependent Variable: CONSUM
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;21815.80 3319.691 &ndash;6.571637 0.0000
Y 15.01623 2.278648 6.589974 0.0000
CONSUM(&ndash;1) 0.947887 0.055806 16.98553 0.0000
C1HAT &ndash;15.20110 2.339106 &ndash;6.498678 0.0000
</p>
<p>R-squared 0.999167 Mean dependent var 16915.21
Adjusted R-squared 0.999110 S.D. dependent var 5377.825
S.E. of regression 160.4500 Akaike info criterion 13.07350
Sum squared resid 1132745. Schwarz criterion 13.22943
Log likelihood &ndash;309.7639 Hannan-Quinn criter. 13.13242
F-statistic 17585.25 Durbin-Watson stat 1.971939
Prob(F-statistic) 0.000000
</p>
<p>pendent variables. Nonlinear least squares minimizes
&sum;T
</p>
<p>t=1(yt&minus;xt(β))2 = (y&minus;x(β))&prime;(y&minus;x(β)).
The first-order conditions for minimization yield
</p>
<p>X &prime;(β̂)(y &minus; x(β̂)) = 0 (8.72)
</p>
<p>where X(β) is a T &times; k matrix with typical element Xtj(β) = &part;xt(β)/&part;βj for j = 1, . . . , k. The
solution to these k equations yield the Nonlinear Least Squares (NLS) estimates of β denoted by
β̂NLS . These normal equations given in (8.72) are similar to those in the linear case in that they</p>
<p/>
</div>
<div class="page"><p/>
<p>208 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Table 8.10 (continued)
</p>
<p>Regression 5
</p>
<p>Dependent Variable: C1HAT
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;1418.403 7.149223 &ndash;198.3996 0.0000
Y 0.973925 0.003145 309.6905 0.0000
CONSUM(&ndash;1) 0.009728 0.003247 2.995785 0.0044
</p>
<p>R-squared 0.999997 Mean dependent var 16915.21
Adjusted R-squared 0.999996 S.D. dependent var 5360.865
S.E. of regression 10.22548 Akaike info criterion 7.548103
Sum squared resid 4705.215 Schwarz criterion 7.665053
Log likelihood &ndash;178.1545 Hannan-Quinn criter. 7.592298
F-statistic 6459057. Durbin-Watson stat 1.678118
Prob(F-statistic) 0.000000
</p>
<p>Regression 6
</p>
<p>Dependent Variable: CONSUM
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C 138044.4 8211.501 16.81111 0.0000
Y &ndash;94.21814 5.603155 &ndash;16.81519 0.0000
Y(&ndash;1) &ndash;0.613631 0.094424 &ndash;6.498678 0.0000
C2TILDE 97.43471 5.736336 16.98553 0.0000
</p>
<p>R-squared 0.999167 Mean dependent var 16915.21
Adjusted R-squared 0.999110 S.D. dependent var 5377.825
S.E. of regression 160.4500 Akaike info criterion 13.07350
Sum squared resid 1132745. Schwarz criterion 13.22943
Log likelihood &ndash;309.7639 Hannan-Quinn criter. 13.13242
F-statistic 17585.25 Durbin-Watson stat 1.971939
Prob(F-statistic) 0.000000
</p>
<p>require the vector of residuals y&minus;x(β̂) to be orthogonal to the matrix of derivativesX(β̂). In the
linear case, x(β̂) = Xβ̂OLS and X(β̂) = X where the latter is independent of β̂. Because of this
dependence of the fitted values x(β̂) as well as the matrix of derivativesX(β̂) on β̂, one in general
cannot get explicit analytical solution to these NLS first-order equations. Under fairly general
conditions, see Davidson and MacKinnon (1993), one can show that the β̂NLS has asymptotically</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Nonlinear Least Squares and the Gauss-Newton Regression 209
</p>
<p>Table 8.10 (continued)
</p>
<p>Regression 7
</p>
<p>Dependent Variable: C2HAT
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;1324.328 181.8276 &ndash;7.283424 0.0000
Y 0.437200 0.182524 2.395306 0.0208
Y(&ndash;1) 0.551966 0.184248 2.995785 0.0044
</p>
<p>R-squared 0.996101 Mean dependent var 16915.21
Adjusted R-squared 0.995928 S.D. dependent var 5373.432
S.E. of regression 342.8848 Akaike info criterion 14.57313
Sum squared resid 5290650. Schwarz criterion 14.69008
Log likelihood &ndash;346.7551 Hannan-Quinn criter. 14.61732
F-statistic 5748.817 Durbin-Watson stat 0.127201
Prob(F-statistic) 0.000000
</p>
<p>Regression 8
</p>
<p>Dependent Variable: CONSUM
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;1629.522 239.4806 &ndash;6.804403 0.0000
Y 1.161999 0.154360 7.527865 0.0000
CONSUM(&ndash;1) 0.947887 0.055806 16.98553 0.0000
C1TILDE &ndash;1.111718 0.171068 &ndash;6.498678 0.0000
</p>
<p>R-squared 0.999167 Mean dependent var 16915.21
Adjusted R-squared 0.999110 S.D. dependent var 5377.825
S.E. of regression 160.4500 Akaike info criterion 13.07350
Sum squared resid 1132745. Schwarz criterion 13.22943
Log likelihood &ndash;309.7639 Hannan-Quinn criter. 13.13242
F-statistic 17585.25 Durbin-Watson stat 1.971939
Prob(F-statistic) 0.000000
</p>
<p>a normal distribution with mean β0 and asymptotic variance σ
2
0(X
</p>
<p>&prime;(β0)X(β0))
&minus;1, where β0 and
</p>
<p>σ0 are the true values of the parameters generating the data. Similarly, defining
</p>
<p>s2 = (y &minus; x(β̂NLS))&prime;(y &minus; x(β̂NLS))/(T &minus; k)
</p>
<p>we get a feasible estimate of this covariance matrix as s2(X &prime;(β̂)X(β̂))&minus;1. If the disturbances
are normally distributed then NLS is MLE and therefore asymptotically efficient as long as the
model is correctly specified, see Chapter 7.</p>
<p/>
</div>
<div class="page"><p/>
<p>210 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Table 8.10 (continued)
</p>
<p>Regression 9
</p>
<p>Dependent Variable: CONSUM
Method: Least Squares
Sample (adjusted): 1960 2007
Included observations: 48 after adjustments
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;157.2430 113.1743 &ndash;1.389389 0.1717
Y 0.675956 0.086849 7.783091 0.0000
Y(&ndash;1) &ndash;0.613631 0.094424 &ndash;6.498678 0.0000
CONSUM(&ndash;1) 0.947887 0.055806 16.98553 0.0000
</p>
<p>R-squared 0.999167 Mean dependent var 16915.21
Adjusted R-squared 0.999110 S.D. dependent var 5377.825
S.E. of regression 160.4500 Akaike info criterion 13.07350
Sum squared resid 1132745. Schwarz criterion 13.22943
Log likelihood &ndash;309.7639 Hannan-Quinn criter. 13.13242
F-statistic 17585.25 Durbin-Watson stat 1.971939
Prob(F-statistic) 0.000000
</p>
<p>Taking the first-order Taylor series approximation around some arbitrary parameter vector β&lowast;,
we get
</p>
<p>y = x(β&lowast;) +X(β&lowast;)(β &minus; β&lowast;) + higher-order terms + u (8.73)
</p>
<p>or
</p>
<p>y &minus; x(β&lowast;) = X(β&lowast;)b+ residuals (8.74)
</p>
<p>This is the simplest version of the Gauss-Newton Regression, see Davidson and MacKinnon
(1993). In this case the higher-order terms and the error term are combined in the residuals
and (β &minus; β&lowast;) is replaced by b, a parameter vector that can be estimated. If the model is linear,
X(β&lowast;) is the matrix of regressors X and the GNR regresses a residual on X. If β&lowast;=β̂NLS , the
unrestricted NLS estimator of β, then the GNR becomes
</p>
<p>y &minus; x̂ = X̂b+ residuals (8.75)
</p>
<p>where x̂ &equiv; x(β̂NLS) and X̂ &equiv; X(β̂NLS). From the first-order conditions of NLS we get (y &minus;
x̂)&prime;X̂ = 0. In this case, OLS on this GNR yields b̂OLS = (X̂ &prime;X̂)&minus;1X̂ &prime;(y &minus; x̂) = 0 and this GNR
has no explanatory power. However, this regression can be used to (i) check that the first-
order conditions given in (8.72) are satisfied. For example, one could check that the t-statistics
are of the 10&minus;3 order, and that R2 is zero up to several decimal places; (ii) compute estimated
covariance matrices. In fact, this GNR prints out s2(X̂ &prime;X̂)&minus;1, where s2 = (y&minus; x̂)&prime;(y&minus; x̂)/(T &minus;k)
is the OLS estimate of the regression variance. This can be verified easily using the fact that this
GNR has no explanatory power. This method of computing the estimated variance-covariance
matrix is useful especially in cases where β̂ has been obtained by some method other than NLS.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Nonlinear Least Squares and the Gauss-Newton Regression 211
</p>
<p>For example, sometimes the model is nonlinear only in one or two parameters which are known
to be in a finite range, say between zero and one. One can then search over this range, running
OLS regressions and minimizing the residual sum of squares. This search procedure can be
repeated over finer grids to get more accuracy. Once the final parameter estimate is found, one
can run the GNR to get estimates of the variance-covariance matrix.
</p>
<p>Testing Restrictions (GNR Based on the Restricted NLS Estimates)
</p>
<p>The best known use for the GNR is to test restrictions. These are based on the LM principle
which requires only the restricted estimator. In particular, consider the following competing
hypotheses:
</p>
<p>H0; y = x(β1, 0) + u H1; y = x(β1, β2) + u
</p>
<p>where u &sim; IID(0, σ2I) and β1 and β2 are k&times;1 and r&times;1, respectively. Denote by β̃ the restricted
NLS estimator of β, in this case β̃
</p>
<p>&prime;
= (β̃
</p>
<p>&prime;
1, 0).
</p>
<p>The GNR evaluated at this restricted NLS estimator of β is
</p>
<p>(y &minus; x̃) = X̃1b1 + X̃2b2 + residuals (8.76)
</p>
<p>where x̃ = x(β̃) and X̃i = Xi(β̃) with Xi(β) = &part;x/&part;βi for i = 1, 2.
By the FWL Theorem this yields the same estimate of b2 as
</p>
<p>P̄
X̃1
</p>
<p>(y &minus; x̃) = P̄
X̃1
</p>
<p>X̃2b2 + residuals (8.77)
</p>
<p>But P̄
X̃1
</p>
<p>(y &minus; x̃) = (y &minus; x̃) &minus; P
X̃1
</p>
<p>(y &minus; x̃) = (y &minus; x̃) since X̃ &prime;1(y &minus; x̃) = 0 from the first-order
conditions of restricted NLS. Hence, (8.77) reduces to
</p>
<p>(y &minus; x̃) = P̄
X̃1
</p>
<p>X̃2b2 + residuals (8.78)
</p>
<p>Therefore,
</p>
<p>b2,OLS = (X̃
&prime;
2P̄X̃1X̃2)
</p>
<p>&minus;1X̃ &prime;2P̄X̃1(y &minus; x̃) = (X̃
&prime;
2P̄X̃1X̃2)
</p>
<p>&minus;1X̃ &prime;2(y &minus; x̃) (8.79)
</p>
<p>and the residual sums of squares is (y &minus; x̃)&prime;(y &minus; x̃)&minus; (y &minus; x̃)&prime;X̃2(X̃ &prime;2P̄X̃1X̃2)
&minus;1X̃ &prime;2(y &minus; x̃).
</p>
<p>If X̃2 was excluded from the regression in (8.76), (y&minus; x̃)&prime;(y&minus; x̃) would be the residual sum of
squares. Therefore, the reduction in the residual sum of squares brought about by the inclusion
of X̃2 is
</p>
<p>(y &minus; x̃)&prime;X̃2(X̃ &prime;2P̄X̃1X̃2)
&minus;1X̃ &prime;2(y &minus; x̃)
</p>
<p>This is also equal to the explained sum of squares from (8.76) since X̃1 has no explanatory
power. This sum of squares divided by a consistent estimate of σ2 is asymptotically distributed
as χ2r under the null.
</p>
<p>Different consistent estimates of σ2 yield different test statistics. The two most common
test statistics for H0 based on this regression are the following: (1) TR
</p>
<p>2
u where R
</p>
<p>2
u is the
</p>
<p>uncentered R2 of (8.76) and (2) the F -statistic for b2 = 0. The first statistic is given by TR
2
u =
</p>
<p>T (y&minus; x̃)&prime;X̃2(X̃ &prime;2P̄X̃1X̃2)
&minus;1X̃ &prime;2(y&minus; x̃)/(y&minus; x̃)&prime;(y&minus; x̃) where the uncentered R2 was defined in the</p>
<p/>
</div>
<div class="page"><p/>
<p>212 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Appendix to Chapter 3. This statistic implicitly divides the explained sum of squares term by
σ̃2 = (restricted residual sums of squares)/T . This is equivalent to the LM-statistic obtained
by running the artificial regression (y &minus; x̃)/σ̃ on X̃ and getting the explained sum of squares.
Regression packages print the centered R2. This is equal to the uncentered R2u as long as there
is a constant in the restricted regression so that (y &minus; x̃) sum to zero.
</p>
<p>The F -statistic for b2 = 0 from (8.76) is
</p>
<p>(RRSS &minus;URSS )/r
URSS/(T &minus; k) =
</p>
<p>(y &minus; x̃)&prime;X̃2(X̃ &prime;2P̄X̃1X̃2)
&minus;1X̃ &prime;2(y &minus; x̃)/r
</p>
<p>[(y &minus; x̃)&prime;(y &minus; x̃)&minus; (y &minus; x̃)&prime;X̃2(X̃ &prime;2P̄X̃1X̃2)
&minus;1X̃ &prime;2(y &minus; x̃)]/(T &minus; k)
</p>
<p>(8.80)
</p>
<p>The denominator is the OLS estimate of σ2 from (8.76) which tends to σ20 as T &rarr; &infin;. Hence
(rF -statistic &rarr; χ2r ). In small samples, use the F -statistic.
</p>
<p>Diagnostic Tests for Linear Regression Models
</p>
<p>Variable addition tests suggested by Pagan and Hall (1983) consider the additional variables
Z of dimension (T &times; r) and test whether their coefficients are zero using an F -test from the
regression
</p>
<p>y = Xβ + Zγ + u (8.81)
</p>
<p>If H0; γ = 0 is true, the model is y = Xβ + u and there is no misspecification. The GNR for
this restriction would run the following regression:
</p>
<p>P̄Xy = Xb+ Zc+ residuals (8.82)
</p>
<p>and test that c is zero. By the FWL Theorem, (8.82) yields the same residual sum of squares as
</p>
<p>P̄Xy = P̄XZc+ residuals (8.83)
</p>
<p>Applying the FWL Theorem to (8.81) we get the same residual sum of squares as the regression
in (8.83). The F -statistic for γ = 0 from (8.81) is therefore identical to the F -statistic for c = 0
from the GNR given in (8.82). Hence, &ldquo;Tests based on the GNR are equivalent to variable
addition tests when the latter are applicable,&rdquo; see Davidson and MacKinnon (1993, p. 194).
</p>
<p>Note also, that the nR2u test statistic for H0; γ = 0 based on the GNR in (8.82) is exactly the
LM statistic based on running the restricted least squares residuals of y onX on the unrestricted
set of regressors X and Z in (8.81). If X has a constant, then the uncentered R2 is equal to
the centered R2 printed by the regression.
</p>
<p>Computational Warning: It is tempting to base tests on the OLS residuals û = P̄Xy by
simply regressing them on the test regressors Z. This is equivalent to running the GNR without
the X variables on the right hand side of (8.82) yielding test-statistics that are too small.
</p>
<p>Functional Form
</p>
<p>Davidson and MacKinnon (1993, p. 195) show that the RESET with yt = Xtβ + ŷ
2
t c+ residual
</p>
<p>which is based on testing for c = 0 is equivalent to testing for θ = 0 using the nonlinear model
yt = Xtβ(1 + θXtβ) + ut. In this case, it is easy to verify from (8.74) that the GNR is
</p>
<p>yt &minus;Xtβ(1 + θXtβ) = (2θ(Xtβ)Xt +Xt)b+ (Xtβ)2c+ residual</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Testing Linear Versus Log-Linear Functional Form 213
</p>
<p>At θ = 0 and β = β̂OLS , the GNR becomes (yt &minus;Xtβ̂OLS) = Xtb+ (Xtβ̂OLS)2c+ residual. The
t-statistic on c = 0 is equivalent to that from the RESET regression given in section 8.3, see
problem 25.
</p>
<p>Testing for Serial Correlation
</p>
<p>Suppose that the null hypothesis is the nonlinear regression model given in (8.71), and the
alternative is the model yt = xt(β)+νt with νt = ρνt&minus;1+ut where ut &sim; IID(0, σ2). Conditional
on the first observation, the alternative model can be written as
</p>
<p>yt = xt(β) + ρ(yt&minus;1 &minus; xt&minus;1(β)) + ut
The GNR test for H0; ρ = 0, computes the derivatives of this regression function with respect
to β and ρ evaluated at the restricted estimates under the null hypothesis, i.e., ρ = 0 and
β = β̂NLS (the nonlinear least squares estimate of β assuming no serial correlation). Those yield
Xt(β̂NLS) and (yt&minus;1&minus;xt&minus;1(β̂NLS)) respectively. Therefore, the GNR runs ût = yt&minus;xt(β̂NLS) =
Xt(β̂NLS)b+cût&minus;1+ residual, and tests that c = 0. If the regression model is linear, this reduces
to running ordinary least squares residuals on their lagged values in addition to the regressors
in the model. This is exactly the Breusch and Godfrey test for first-order serial correlation
considered in Chapter 5. For other applications as well as benefits and limitations of the GNR,
see Davidson and MacKinnon (1993).
</p>
<p>8.5 Testing Linear Versus Log-Linear Functional Form5
</p>
<p>In many economic applications where the explanatory variables take only positive values, econo-
metricians must decide whether a linear or log-linear regression model is appropriate. In general,
the linear model is given by
</p>
<p>yi =
&sum;k
</p>
<p>j=1 βjXij +
&sum;ℓ
</p>
<p>s=1 γsZis + ui i = 1, 2, . . . , n (8.84)
</p>
<p>and the log-linear model is
</p>
<p>logyi =
&sum;k
</p>
<p>j=1 βj logXij +
&sum;ℓ
</p>
<p>s=1 γsZis + ui i = 1, 2, . . . , n (8.85)
</p>
<p>with ui &sim; NID(0, σ
2). Note that, the log-linear model is general in that only the dependent
</p>
<p>variable y and a subset of the regressors, i.e., the X variables are subject to the logarithmic
transformation. Of course, one could estimate both models and compare their log-likelihood
values. This would tell us which model fits best, but not whether either is a valid specification.
Box and Cox (1964) suggested the following transformation
</p>
<p>B(yi, λ) =
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>yλi &minus;1
λ when λ 	= 0
</p>
<p>logyi when λ = 0
</p>
<p>(8.86)
</p>
<p>where yi &gt; 0. Note that for λ = 1, as long as there is constant in the regression, subjecting the
linear model to a Box-Cox transformation is equivalent to not transformation yields the log-
linear regression. Therefore, the following Box-Cox model regression. Therefore, the following
Box-Cox model
</p>
<p>B(yi, λ) =
&sum;k
</p>
<p>j=1 βjB(Xij , λ) +
&sum;ℓ
</p>
<p>s=1 γsZis + ui (8.87)</p>
<p/>
</div>
<div class="page"><p/>
<p>214 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>encompasses as special cases the linear and log-linear models given in (8.84) and (8.85), respec-
tively. Box and Cox (1964) suggested estimating these models by ML and using the LR test to
test (8.84) and (8.85) against (8.87). However, estimation of (8.87) is computationally burden-
some, see Davidson and MacKinnon (1993). Instead, we give an LM test involving a Double
Length Regression (DLR) due to Davidson and MacKinnon (1985) that is easier to compute.
In fact, Davidson and MacKinnon (1993, p. 510) point out that &ldquo;everything that one can do
with the Gauss-Newton Regression for nonlinear regression models can be done with the DLR
for models involving transformations of the dependent variable.&rdquo; The GNR is not applicable in
cases where the dependent variable is subjected to a nonlinear transformation, so one should
use a DLR in these cases. Conversely, in cases where the GNR is valid, there is no need to run
the DLR, since in these cases the latter is equivalent to the GNR.
For the linear model (8.84), the null hypothesis is that λ = 1. In this case, Davidson and
</p>
<p>MacKinnon suggest running a regression with 2n observations where the dependent variable
has observations (e1/σ̂, . . . , en/σ̂, 1, . . . , 1)
</p>
<p>&prime;, i.e., the first n observations are the OLS residuals
from (8.84) divided by the MLE of σ, where σ̂2mle = e
</p>
<p>&prime;e/n. The second n observations are all
equal to 1. The 2n observations for the regressors have typical elements:
</p>
<p>for βj : Xij &minus; 1 for i = 1, . . . , n and 0 for the second n elements
for γs: Zis for i = 1, . . . , n and 0 for the second n elements
for σ: ei/σ̂ for i = 1, . . . , n and &minus;1 for the second n elements
for λ:
</p>
<p>&sum;k
j=1 β̂j(Xij logXij &minus;Xij + 1)&minus; (yilog yi &minus; yi+1) for i = 1, . . . , n
</p>
<p>and σ̂logyi for the second n elements
</p>
<p>The explained sum of squares for this DLR provides an asymptotically valid test for λ = 1.
This will be distributed as χ21 under the null hypothesis.
</p>
<p>Similarly, when testing the log-linear model (8.85), the null hypothesis is that λ = 0. In this
case, the dependent variable of the DLR has observations (ẽ1/σ̃, ẽ2/σ̃, . . . , ẽn/σ̃, 1, . . . , 1)
</p>
<p>&prime;, i.e.,
the first n observations are the OLS residuals from (8.85) divided by the MLE for σ, i.e., σ̃
where σ̃2 = ẽ&prime;ẽ/n. The second n observations are all equal to 1. The 2n observations for the
regressors have typical elements:
</p>
<p>for βj : logXij for i = 1, . . . , n and 0 for the second n elements
</p>
<p>for γs: Zis for i = 1, . . . , n and 0 for the second n elements
for σ: ẽi/σ̃ for i = 1, . . . , n and &minus;1 for the second n elements
for λ: 12
</p>
<p>&sum;k
j=1 β̃j(logXij)
</p>
<p>2 &minus; 12(logyi)2 for i = 1, . . . , n
and σ̃logyi for the second n elements
</p>
<p>The explained sum of squares from this DLR provides an asymptotically valid test for λ = 0.
This will be distributed as χ21 under the null hypothesis.
</p>
<p>For the cigarette data given in Table 3.2, the linear model is given by C = β0+β1P +β2Y +u
whereas the log-linear model is given by logC = γ0 + γ1logP + γ2logY + ǫ and the Box-Cox
model is given by B(C, λ) = δ0 + δ1B(P, λ) + δ2B(Y, λ) + ν, where B(C, λ) is defined in (8.86).
In this case, the DLR which tests the hypothesis that H0; λ = 1, i.e., the model is linear, gives
an explained sum of squares equal to 15.55. This is greater than a χ21,0.05 = 3.84 and is therefore
significant at the 5% level. Similarly the DLR that tests the hypothesis that H0; λ = 0, i.e.,
the model is log-linear, gives an explained sum of squares equal to 8.86. This is also greater
than χ21,0.05 = 3.84 and is therefore significant at the 5% level. In this case, both the linear and
log-linear models are rejected by the data.</p>
<p/>
</div>
<div class="page"><p/>
<p>215
</p>
<p>Finally, it is important to note that there are numerous other tests for testing linear and
log-linear models and the interested reader should refer to Davidson and MacKinnon (1993).
</p>
<p>Notes
</p>
<p>1. This section is based on Belsley, Kuh and Welsch (1980).
</p>
<p>2. Other residuals that are linear unbiased with a scalar covariance matrix (LUS) are the BLUS
residuals suggested by Theil (1971). Since we are explicitly dealing with time-series data, we use
</p>
<p>subscript t rather than i to index observations and T rather than n to denote the sample size.
</p>
<p>3. Ramsey&rsquo;s (1969) initial formulation was based on BLUS residuals, but Ramsey and Schmidt (1976)
showed that this is equivalent to using OLS residuals.
</p>
<p>4. This section is based on Davidson and MacKinnon (1993, 2001).
</p>
<p>5. This section is based on Davidson and MacKinnon (1993, pp. 502&ndash;510).
</p>
<p>Problems
</p>
<p>1. We know that H = PX is idempotent. Also, (In &minus;PX) is idempotent. Therefore, b&prime;Hb &ge; 0 for any
arbitrary vector b. Using these facts, show for b&prime; = (1, 0, . . . , 0) that 0 &le; h11 &le; 1. Deduce that
0 &le; hii &le; 1 for i = 1, . . . , n.
</p>
<p>2. For the simple regression with no constant yi = xiβ + ui for i = 1, . . . , n
</p>
<p>(a) What is hii? Verify that
&sum;n
</p>
<p>i=1 hii = 1.
</p>
<p>(b) What is β̂ &minus; β̂(i), see (8.13)? What is s2(i) in terms of s2 and e2i , see (8.18)? What is DFBE-
TAS ij , see (8.19)?
</p>
<p>(c) What are DFFIT i and DFFITS i, see (8.21) and (8.22)?
</p>
<p>(d) What is Cook&rsquo;s distance measure D2i (s) for this simple regression with no intercept, see
(8.24)?
</p>
<p>(e) Verify that (8.27) holds for this simple regression with no intercept. What is COVRATIO i,
see (8.26)?
</p>
<p>3. From the definition of s2(i) in (8.17), substitute (8.13) in (8.17) and verify (8.18).
</p>
<p>4. Consider the augmented regression given in (8.5) y = Xβ&lowast;+diϕ+u where ϕ is a scalar and di = 1
for the i-th observation and 0 otherwise. Using the Frisch-Waugh Lovell Theorem given in section
7.3, verify that
</p>
<p>(a) β̂
&lowast;
= (X &prime;(i)X(i))
</p>
<p>&minus;1X &prime;(i)y(i) = β̂(i).
</p>
<p>(b) ϕ̂ = (d&prime;iP̄Xdi)
&minus;1d&prime;iP̄Xy = ei/(1&minus; hii) where P̄X = I &minus; PX .
</p>
<p>(c) Residual Sum of Squares from (8.5) = (Residual Sum of Squares with di deleted) &minus; e2i /(1&minus;
hii).
</p>
<p>(d) Assuming Normality of u, show that the t-statistic for testing ϕ = 0 is t = ϕ̂/s.e.(ϕ̂) = e&lowast;i as
given in (8.3).
</p>
<p>Problems</p>
<p/>
</div>
<div class="page"><p/>
<p>216 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>5. Consider the augmented regression y = Xβ&lowast; + P̄XDpϕ&lowast; + u, where Dp is an n &times; p matrix of
dummy variables for the p suspected observations. Note that P̄XDp rather than Dp appear in this
equation. Compare with (8.6). Let ep = D
</p>
<p>&prime;
pe, then E(ep) = 0, var(ep) = σ
</p>
<p>2D&prime;pP̄XDp. Verify that
</p>
<p>(a) β̂
&lowast;
= (X &prime;X)&minus;1X &prime;y = β̂OLS and
</p>
<p>(b) ϕ̂&lowast; = (D&prime;pP̄XDp)
&minus;1D&prime;pP̄Xy = (D
</p>
<p>&prime;
pP̄XDp)
</p>
<p>&minus;1D&prime;pe = (D
&prime;
pP̄XDp)
</p>
<p>&minus;1ep.
</p>
<p>(c) Residual Sum of Squares = (Residual Sum of Squares with Dp deleted) &minus; e&prime;p(D&prime;pP̄X)D&minus;1p ep.
Using the Frisch-Waugh Lovell Theorem show this residual sum of squares is the same as
that for (8.6).
</p>
<p>(d) Assuming normality of u, verify (8.7) and (8.9).
</p>
<p>(e) Repeat this exercise for problem 4 with P̄Xdi replacing di. What do you conclude?
</p>
<p>6. Using the updating formula in (8.11), verify (8.12) and deduce (8.13).
</p>
<p>7. Verify that Cook&rsquo;s distance measure given in (8.25) is related to DFFITS i(σ) as follows: DF-
FITS i(σ) =
</p>
<p>&radic;
kDi(σ).
</p>
<p>8. Using the matrix identity det(Ik &minus; ab&prime;) = 1&minus; b&prime;a, where a and b are column vectors of dimension
k, prove (8.27). Hint: Use a = xi and b
</p>
<p>&prime; = x&prime;i(X
&prime;X)&minus;1 and the fact that det[X &prime;(i)X(i)] =det[{Ik &minus;
</p>
<p>xix
&prime;
i(X
</p>
<p>&prime;X)&minus;1}X &prime;X].
</p>
<p>9. For the cigarette data given in Table 3.2
</p>
<p>(a) Replicate the results in Table 8.2.
</p>
<p>(b) For the New Hampshire observation (NH), compute ẽNH , e
&lowast;
NH , β̂&minus; β̂(NH), DFBETASNH ,
</p>
<p>DFFITNH , DFFITSNH , D
2
NH(s), COVRATIONH , and FVARATIONH .
</p>
<p>(c) Repeat the calculations in part (b) for the following states: AR, CT, NJ and UT.
</p>
<p>(d) What about the observations for NV, ME, NM and ND? Are they influential?
</p>
<p>10. For the Consumption-Income data given in Table 5.3, compute
</p>
<p>(a) The internal studentized residuals ẽ given in (8.1).
</p>
<p>(b) The externally studentized residuals e&lowast; given in (8.3).
</p>
<p>(c) Cook&rsquo;s statistic given in (8.25).
</p>
<p>(d) The leverage of each observation h.
</p>
<p>(e) The DFFITS given in (8.22).
</p>
<p>(f) The COVRATIO given in (8.28).
</p>
<p>(g) Based on the results in parts (a) to (f), identify the observations that are influential.
</p>
<p>11. Repeat problem 10 for the 1982 data on earnings used in Chapter 4. This data is provided on the
Springer web site as EARN.ASC.
</p>
<p>12. Repeat problem 10 for the Gasoline data provided on the Springer web site as GASOLINE.DAT.
Use the gasoline demand model given in Chapter 10, section 5. Do this for Austria and Belgium
separately.
</p>
<p>13. Independence of Recursive Residuals.
</p>
<p>(a) Using the updating formula given in (8.11) with A = (X &prime;tXt) and a = &minus;b = x&prime;t+1, verify
(8.31).</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 217
</p>
<p>(b) Using (8.31), verify (8.32).
</p>
<p>(c) For ut &sim; IIN(0, σ
2) and wt+1 defined in (8.30) verify (8.33). Hint: define vt+1=
</p>
<p>&radic;
ft+1wt+1.
</p>
<p>From (8.30), we have
</p>
<p>vt+1 =
&radic;
</p>
<p>ft+1wt+1 = yt+1 &minus; x&prime;t+1β̂t = x&prime;t+1(β &minus; β̂t) + ut+1 for t = k, . . . , T &minus; 1
</p>
<p>Since ft+1 is fixed, it suffices to show that cov(vt+1, vs+1) = 0 for t 	= s.
</p>
<p>14. Recursive Residuals are Linear Unbiased With Scalar Covariance Matrix (LUS).
</p>
<p>(a) Verify that the (T &minus; k) recursive residuals defined in (8.30) can be written in vector form as
w = Cy where C is defined in (8.34). This shows that the recursive residuals are linear in y.
</p>
<p>(b) Show that C satisfies the three properties given in (8.35) i.e., CX = 0, CC &prime; = IT&minus;k, and
C &prime;C = P̄X . Prove that CX = 0 means that the recursive residuals are unbiased with zero
mean. Prove that the CC &prime; = IT&minus;k means that the recursive residuals have a scalar covariance
matrix. Prove that C &prime;C = P̄X means that the sum of squares of (T &minus; k) recursive residuals
is equal to the sum of squares of T least squares residuals.
</p>
<p>(c) If the true disturbances u &sim; N(0, σ2IT ), prove that the recursive residuals w &sim; N(0, σ
2IT&minus;k)
</p>
<p>using parts (a) and (b).
</p>
<p>(d) Verify (8.36), i.e., show that RSSt+1 = RSSt + w
2
t+1 for t = k, . . . , T &minus; 1 where RSSt =
</p>
<p>(Yt &minus;Xtβ̂t)&prime;(Yt &minus;Xtβ̂t).
</p>
<p>15. The Harvey and Collier (1977) Misspecification t-Test as a Variable Additions Test. This is based
on Wu (1993).
</p>
<p>(a) Show that the F -statistic for testing H0; γ = 0 versus γ 	= 0 in (8.44) is given by
</p>
<p>F =
y&prime;P̄Xy &minus; y&prime;P̄[X,z]y
</p>
<p>y&prime;P̄[X,z]y/(T &minus; k &minus; 1)
=
</p>
<p>y&prime;Pzy
</p>
<p>y&prime;(P̄X &minus; Pz)y/(T &minus; k &minus; 1)
</p>
<p>and is distributed as F (1, T &minus; k &minus; 1) under the null hypothesis.
(b) Using the properties of C given in (8.35), show that the F -statistic given in part (a) is the
</p>
<p>square of the Harvey and Collier (1977) t-statistic given in (8.43).
</p>
<p>16. For the Gasoline data for Austria given on the Springer web site as GASOLINE.DAT and the
model given in Chapter 10, section 5, compute:
</p>
<p>(a) The recursive residuals given in (8.30).
</p>
<p>(b) The CUSUM given in (8.46) and plot it against r.
</p>
<p>(c) Draw the 5% upper and lower lines given below (8.46) and see whether the CUSUM crosses
these boundaries.
</p>
<p>(d) The post-sample predictive test for 1978. Verify that computing it from (8.38) or (8.40) yields
the same answer.
</p>
<p>(e) The modified von Neuman ratio given in (8.42).
</p>
<p>(f) The Harvey and Collier (1977) functional misspecification test given in (8.43).
</p>
<p>17. The Differencing Test in a Regression with Equicorrelated Disturbances. This is based on Baltagi
(1990). Consider the time-series regression
</p>
<p>Y = ιTα+Xβ + u (1)</p>
<p/>
</div>
<div class="page"><p/>
<p>218 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>where ιT is a vector of ones of dimension T .X is T&times;K and [ιT , X] is of full column rank. u &sim; (0,Ω)
where Ω is positive definite. Differencing this model, we get
</p>
<p>DY = DXβ +Du (2)
</p>
<p>where D is a (T &minus; 1)&times;T matrix given below (8.50). Maeshiro and Wichers (1989) show that GLS
on (1) yields through partitioned inverse:
</p>
<p>β̂ = (X &prime;LX)&minus;1X &prime;LY (3)
</p>
<p>where L = Ω&minus;1 &minus; Ω&minus;1ιT (ι&prime;TΩ&minus;1ιT )&minus;1ι&prime;TΩ&minus;1. Also, GLS on (2) yields
</p>
<p>β̃ = (X &prime;MX)&minus;1X &prime;MY (4)
</p>
<p>where M = D&prime;(DΩD&prime;)&minus;1D. Finally, they show that M = L, and GLS on (2) is equivalent to GLS
on (1) as long as there is an intercept in (1).
</p>
<p>Consider the special case of equicorrelated disturbances
</p>
<p>Ω = σ2[(1&minus; ρ)IT + ρJT ] (5)
</p>
<p>where IT is an identity matrix of dimension T and JT is a matrix of ones of dimension T .
</p>
<p>(a) Derive the L andM matrices for the equicorrelated case, and verify the Maeshiro andWichers
result for this special case.
</p>
<p>(b) Show that for the equicorrelated case, the differencing test given by Plosser, Schwert, and
White (1982) can be obtained as the difference between the OLS and GLS estimators of the
differenced equation (2). Hint: See the solution by Koning (1992).
</p>
<p>18. For the 1982 data on earnings used in Chapter 4, provided as EARN.ASC on the Springer web
site, (a) compute Ramsey&rsquo;s (1969) RESET. (b) Compute White&rsquo;s (1982) information matrix test
given in (8.69) and (8.70).
</p>
<p>19. Repeat problem 18 for the Hedonic housing data given on the Springer web site as HEDONIC.XLS.
</p>
<p>20. Repeat problem 18 for the cigarette data given in Table 3.2.
</p>
<p>21. Repeat problem 18 for the Gasoline data for Austria given on the Springer web site as GASO-
LINE.DAT. Use the model given in Chapter 10, section 5. Also compute the PSW differencing
test given in (8.54).
</p>
<p>22. Use the 1982 data on earnings used in Chapter 4, and provided on the Springer web site as
EARN.ASC. Consider the two competing non-nested models
</p>
<p>H0; log(wage) = β0 + β1ED + β2EXP + β3EXP
2 + β4WKS
</p>
<p>+β5MS + β6FEM + β7BLK + β8UNION + u
</p>
<p>H1; log(wage) = γ0 + γ1ED + γ2EXP + γ3EXP
2 + γ4WKS
</p>
<p>+γ5OCC + γ6SOUTH + γ7SMSA+ γ8IND + ǫ
</p>
<p>Compute:
</p>
<p>(a) The Davidson and MacKinnon (1981) J-test for H0 versus H1.
</p>
<p>(b) The Fisher and McAleer (1981) JA-test for H0 versus H1.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 219
</p>
<p>(c) Reverse the roles of H0 and H1 and repeat parts (a) and (b).
</p>
<p>(d) Both H0 and H1 can be artificially nested in the model used in Chapter 4. Using the F -
test given in (8.62), test for H0 versus this augmented model. Repeat for H1 versus this
augmented model. What do you conclude?
</p>
<p>23. For the Consumption-Income data given in Table 5.3,
</p>
<p>(a) Test the hypothesis that the Consumption model is linear against a general Box-Cox alter-
native.
</p>
<p>(b) Test the hypothesis that the Consumption model is log-linear against a general Box-Cox
alternative.
</p>
<p>24. Repeat problem 23 for the Cigarette data given in Table 3.2.
</p>
<p>25. RESET as a Gauss-Newton Regression. This is based on Baltagi (1998). Davidson and MacKinnon
(1993) showed that Ramsey&rsquo;s (1969) regression error specification test (RESET) can be derived
as a Gauss-Newton Regression. This problem is a simple extension of their results. Suppose that
the linear regression model under test is given by:
</p>
<p>yt = X
&prime;
tβ + ut t = 1, 2, . . . , T (1)
</p>
<p>where β is a k &times; 1 vector of unknown parameters. Suppose that the alternative is the nonlinear
regression model between yt and Xt:
</p>
<p>yt = X
&prime;
tβ[1 + θ(X
</p>
<p>&prime;
tβ) + γ(X
</p>
<p>&prime;
tβ)
</p>
<p>2 + λ(X &prime;tβ)
3] + ut, (2)
</p>
<p>where θ, γ, and λ are unknown scalar parameters. It is well known that Ramsey&rsquo;s (1969) RESET
</p>
<p>is obtained by regressing yt on Xt, ŷ
2
t , ŷ
</p>
<p>3
t and ŷ
</p>
<p>4
t and by testing that the coefficients of all powers
</p>
<p>of ŷt are jointly zero. Show that this RESET can be derived from a Gauss-Newton Regression on
</p>
<p>(2), which tests θ = γ = λ = 0.
</p>
<p>References
</p>
<p>This chapter is based on Belsley, Kuh and Welsch (1980), Johnston (1984), Maddala (1992) and Davidson
</p>
<p>and MacKinnon (1993). Additional references are the following:
</p>
<p>Baltagi, B.H. (1990), &ldquo;The Differencing Test in a Regression with Equicorrelated Disturbances,&rdquo; Econo-
metric Theory, Problem 90.4.5, 6: 488.
</p>
<p>Baltagi, B.H. (1998), &ldquo;Regression Specification Error Test as A Gauss-Newton Regression,&rdquo; Econometric
Theory, Problem 98.4.3, 14: 526.
</p>
<p>Belsley, D.A., E. Kuh and R.E. Welsch (1980), Regression Diagnostics (Wiley: New York).
</p>
<p>Box, G.E.P. and D.R. Cox (1964), &ldquo;An Analysis of Transformations,&rdquo; Journal of the Royal Statistical
Society, Series B, 26: 211&ndash;252.
</p>
<p>Brown, R.L., J. Durbin, and J.M. Evans (1975), &ldquo;Techniques for Testing the Constancy of Regression
Relationships Over Time,&rdquo; Journal of the Royal Statistical Society 37:149&ndash;192.
</p>
<p>Chesher, A. and R. Spady (1991), &ldquo;Asymptotic Expansions of the Information Matrix Test Statistic,&rdquo;
Econometrica 59: 787&ndash;815.</p>
<p/>
</div>
<div class="page"><p/>
<p>220 Chapter 8: Regression Diagnostics and Specification Tests
</p>
<p>Cook, R.D. (1977), &ldquo;Detection of Influential Observations in Linear Regression,&rdquo; Technometrics 19:15&ndash;
18.
</p>
<p>Cook, R.D. and S. Weisberg (1982), Residuals and Influences in Regression (Chapman and Hall: New
York).
</p>
<p>Cox, D.R. (1961), &ldquo;Tests of Separate Families of Hypotheses,&rdquo; Proceedings of the Fourth Berkeley Sym-
posium on Mathematical Statistics and Probability, 1: 105&ndash;123.
</p>
<p>Davidson, R., L.G. Godfrey and J.G. MacKinnon (1985), &ldquo;A Simplified Version of the Differencing Test,&rdquo;
International Economic Review, 26: 639&ndash;47.
</p>
<p>Davidson, R. and J.G. MacKinnon (1981), &ldquo;Several Tests for Model Specification in the Presence of
Alternative Hypotheses,&rdquo; Econometrica, 49: 781&ndash;793.
</p>
<p>Davidson, R. and J.G. MacKinnon (1985), &ldquo;Testing Linear and Loglinear Regressions Against Box-Cox
Alternatives,&rdquo; Canadian Journal of Economics, 18: 499&ndash;517.
</p>
<p>Davidson, R. and J.G. MacKinnon (1992), &ldquo;A New Form of the Information Matrix Test,&rdquo; Econometrica,
60: 145&ndash;157.
</p>
<p>Davidson, R. and J.G. MacKinnon (2001), &ldquo;Artificial Regressions,&rdquo; Chapter 1 in Baltagi, B.H. (ed.) A
Companion to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Fisher, G.R. and M. McAleer (1981), &ldquo;Alternative Procedures and Associated Tests of Significance for
Non-Nested Hypotheses,&rdquo; Journal of Econometrics, 16: 103&ndash;119.
</p>
<p>Gentleman, J.F. and M.B. Wilk (1975), &ldquo;Detecting Outliers II: Supplementing the Direct Analysis of
Residuals,&rdquo; Biometrics, 31: 387&ndash;410.
</p>
<p>Godfrey, L.G. (1988), Misspecification Tests in Econometrics: The Lagrange Multiplier Principle and
Other Approaches (Cambridge University Press: Cambridge).
</p>
<p>Hall, A. (1987), &ldquo;The Information Matrix Test for the Linear Model,&rdquo; Review of Economic Studies, 54:
257&ndash;263.
</p>
<p>Harvey, A.C. (1976), &ldquo;An Alternative Proof and Generalization of a Test for Structural Change,&rdquo; The
American Statistician, 30: 122&ndash;123.
</p>
<p>Harvey, A.C. (1990), The Econometric Analysis of Time Series (MIT Press: Cambridge).
</p>
<p>Harvey, A.C. and P. Collier (1977), &ldquo;Testing for Functional Misspecification in Regression Analysis,&rdquo;
Journal of Econometrics, 6: 103&ndash;119.
</p>
<p>Harvey, A.C. and G.D.A. Phillips (1974), &ldquo;A Comparison of the Power of Some Tests for Heteroskedas-
ticity in the General Linear Model,&rdquo; Journal of Econometrics, 2: 307&ndash;316.
</p>
<p>Hausman, J. (1978), &ldquo;Specification Tests in Econometrics,&rdquo; Econometrica, 46: 1251&ndash;1271.
</p>
<p>Koning, R.H. (1992), &ldquo;The Differencing Test in a Regression with Equicorrelated Disturbances,&rdquo; Econo-
metric Theory, Solution 90.4.5, 8: 155&ndash;156.
</p>
<p>Krämer, W. and H. Sonnberger (1986), The Linear Regression Model Under Test (Physica-Verlag: Hei-
delberg).
</p>
<p>Krasker, W.S., E. Kuh and R.E. Welsch (1983), &ldquo;Estimation for Dirty Data and Flawed Models,&rdquo; Chapter
11 in Handbook of Econometrics, Vol. I, eds. Z. Griliches and M.D. Intrilligator, Amsterdam, North-
Holland.
</p>
<p>Maeshiro, A. and R. Wichers (1989), &ldquo;On the Relationship Between the Estimates of Level Models and
Difference Models,&rdquo; American Journal of Agricultural Economics, 71: 432&ndash;434.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 221
</p>
<p>Orme, C. (1990), &ldquo;The Small Sample Performance of the Information Matrix Test,&rdquo; Journal of Econo-
metrics, 46: 309&ndash;331.
</p>
<p>Pagan, A.R. and A.D. Hall (1983), &ldquo;Diagnostic Tests as Residual Analysis,&rdquo; Econometric Reviews, 2:
159&ndash;254.
</p>
<p>Pesaran, M.H. and M. Weeks (2001), &ldquo;Nonnested Hypothesis Testing: A Overview,&rdquo; Chapter 13 in
Baltagi, B.H. (ed.) A Companion to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Phillips, G.D.A. and A.C. Harvey (1974), &ldquo;A Simple Test for Serial Correlation in Regression Analysis,&rdquo;
Journal of the American Statistical Association, 69: 935&ndash;939.
</p>
<p>Plosser, C.I., G.W. Schwert, and H. White (1982), &ldquo;Differencing as a Test of Specification,&rdquo; International
Economic Review, 23: 535&ndash;552.
</p>
<p>Ramsey, J.B. (1969), &ldquo;Tests for Specification Errors in Classical Linear Least-Squares Regression Anal-
ysis,&rdquo; Journal of the Royal Statistics Society, Series B, 31: 350&ndash;371.
</p>
<p>Ramsey, J.B. and P. Schmidt (1976), &ldquo;Some Further Results in the Use of OLS and BLUS Residuals in
Error Specification Tests,&rdquo; Journal of the American Statistical Association, 71: 389&ndash;390.
</p>
<p>Schmidt, P. (1976), Econometrics (Marcel Dekker: New York).
</p>
<p>Theil, H. (1971), Principles of Econometrics (Wiley: New York).
</p>
<p>Thursby, J. and P. Schmidt (1977), &ldquo;Some Properties of Tests for Specification Error in a Linear Regres-
sion Model,&rdquo; Journal of the American Statistical Association, 72: 635&ndash;641.
</p>
<p>Utts, J.M. (1982), &ldquo;The Rainbow Test for Lack of Fit in Regression,&rdquo; Communications in Statistics, 11:
2801&ndash;2815.
</p>
<p>Velleman, P. and R. Welsch (1981), &ldquo;Efficient Computing of Regression Diagnostics,&rdquo; The American
Statistician, 35: 234&ndash;242.
</p>
<p>White, H. (1980), &ldquo;A Heteroskedasticity&ndash;Consistent Covariance Matrix Estimator and a Direct Test for
Heteroskedasticity,&rdquo; Econometrica, 48: 817&ndash;838.
</p>
<p>White, H. (1982), &ldquo;Maximum Likelihood Estimation of Misspecified Models,&rdquo; Econometrica, 50: 1&ndash;25.
</p>
<p>Wooldridge, J.M. (2001), &ldquo;Diagnostic Testing,&rdquo; Chapter 9 in B.H. Baltagi (ed.) A Companion to Theo-
retical Econometrics (Blackwell: Massachusetts).
</p>
<p>Wu, P. (1993), &ldquo;Variable Addition Test,&rdquo; Econometric Theory, Problem 93.1.2, 9: 145&ndash;146.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 9
</p>
<p>Generalized Least Squares
</p>
<p>9.1 Introduction
</p>
<p>This chapter considers a more general variance covariance matrix for the disturbances. In other
words, u &sim; (0, σ2In) is relaxed so that u &sim; (0, σ
</p>
<p>2Ω) where Ω is a positive definite matrix of
dimension (n&times;n). First Ω is assumed known and the BLUE for β is derived. This estimator turns
out to be different from β̂OLS , and is denoted by β̂GLS , the Generalized Least Squares estimator
of β. Next, we study the properties of β̂OLS under this nonspherical form of the disturbances. It
turns out that the OLS estimates are still unbiased and consistent, but their standard errors as
computed by standard regression packages are biased and inconsistent and lead to misleading
inference. Section 9.3 studies some special forms of Ω and derive the corresponding BLUE for β.
It turns out that heteroskedasticity and serial correlation studied in Chapter 5 are special cases
of Ω. Section 9.4 introduces normality and derives the maximum likelihood estimator. Sections
9.5 and 9.6 study the way in which test of hypotheses and prediction get affected by this general
variance-covariance assumption on the disturbances. Section 9.7 studies the properties of this
BLUE for β when Ω is unknown, and is replaced by a consistent estimator. Section 9.8 studies
what happens to the W, LR and LM statistics when u &sim; N(0, σ2Ω). Section 9.9 gives another
application of GLS to spatial autocorrelation.
</p>
<p>9.2 Generalized Least Squares
</p>
<p>The regression equation did not change, only the variance-covariance matrix of the disturbances.
It is now σ2Ω rather than σ2In. However, we can rely once again on a result from matrix algebra
to transform our nonspherical disturbances back to spherical form, see the Appendix to Chapter
7. This result states that for every positive definite matrix Ω, there exists a nonsingular matrix
P such that PP &prime; = Ω. In order to use this result, we transform the original model
</p>
<p>y = Xβ + u (9.1)
</p>
<p>by premultiplying it by P&minus;1. We get
</p>
<p>P&minus;1y = P&minus;1Xβ + P&minus;1u (9.2)
</p>
<p>Defining y&lowast; as P&minus;1y and X&lowast; and u&lowast; similarly, we have
</p>
<p>y&lowast; = X&lowast;β + u&lowast; (9.3)
</p>
<p>with u&lowast; having 0 mean and var(u&lowast;) = P&minus;1var(u)P&minus;1&prime; = σ2P&minus;1ΩP &prime;&minus;1 = σ2P&minus;1PP &prime;P &prime;&minus;1 = σ2In.
Hence, the variance-covariance of the disturbances in (9.3) is a scalar times an identity matrix.
Therefore, using the results of Chapter 7, the BLUE for β in (9.1) is OLS on the transformed
model in (9.3)
</p>
<p>β̂BLUE = (X
&lowast;&prime;X&lowast;)&minus;1X&lowast;&prime;y&lowast; = (X &prime;P&minus;1&prime;P&minus;1X)&minus;1X &prime;P&minus;1&prime;P&minus;1y = (X &prime;Ω&minus;1X)&minus;1X &prime;Ω&minus;1y (9.4)
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>223B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_9, </p>
<p/>
</div>
<div class="page"><p/>
<p>224 Chapter 9: Generalized Least Squares
</p>
<p>with var(β̂BLUE) = σ
2(X&lowast;&prime;X&lowast;)&minus;1 = σ2(X &prime;Ω&minus;1X)&minus;1. This β̂BLUE is known as β̂GLS . Define
</p>
<p>Σ = E(uu&prime;) = σ2Ω, then Σ differs from Ω only by the positive scalar σ2. One can easily verify
that β̂GLS can be alternatively written as β̂GLS = (X
</p>
<p>&prime;Σ&minus;1X)&minus;1X &prime;Σ&minus;1y and that var(β̂GLS) =
(X &prime;Σ&minus;1X)&minus;1. Just substitute Σ&minus;1 = Ω&minus;1/σ2 in this last expression for β̂GLS and verify that
this yields (9.4).
</p>
<p>It is clear that β̂GLS differs from β̂OLS . In fact, since β̂OLS is still a linear unbiased estimator
of β, the Gauss-Markov Theorem states that it must have a variance larger than that of β̂GLS .
Using equation (7.5) from Chapter 7, i.e., β̂OLS = β + (X
</p>
<p>&prime;X)&minus;1X &prime;u it is easy to show that
</p>
<p>var(β̂OLS) = σ
2(X &prime;X)&minus;1(X &prime;ΩX)(X &prime;X)&minus;1 (9.5)
</p>
<p>Problem 1 shows that var(β̂OLS)&minus; var(β̂GLS) is a positive semi-definite matrix. Note that
var(β̂OLS) is no longer σ
</p>
<p>2(X &prime;X)&minus;1, and hence a regression package that is programmed to
compute s2(X &prime;X)&minus;1 as an estimate of the variance of β̂OLS is using the wrong formula. Fur-
thermore, problem 2 shows that E(s2) is not in general σ2. Hence, the regression package is also
wrongly estimating σ2 by s2. Two wrongs do not make a right, and the estimate of var(β̂OLS)
is biased. The direction of this bias depends upon the form of Ω and the X matrix. (We saw
some examples of this bias under heteroskedasticity and serial correlation in Chapter 5). Hence,
the standard errors and t-statistics computed using this OLS regression are biased. Under het-
eroskedasticity, one can use the White (1980) robust standard errors for OLS. In this case,
Σ = σ2Ω in (9.5) is estimated by Σ̂ = diag[e2i ] where ei denotes the least squares residuals.
The resulting t-statistics are robust to heteroskedasticity. Similarly Wald type statistics for
H0; Rβ = r can be obtained based on β̂OLS by replacing σ
</p>
<p>2(X &prime;X)&minus;1 in (7.41) by (9.5) with
Σ̂ = diag[e2i ]. In the presence of both serial correlation and heteroskedasticity, one can use the
consistent covariance matrix estimate suggested by Newey and West (1987). This was discussed
in Chapter 5.
</p>
<p>To summarize, β̂OLS is no longer BLUE whenever Ω 	= In. However, it is still unbiased and
consistent. The last two properties do not rely upon the form of the variance-covariance matrix
of the disturbances but rather on E(u/X) = 0 and plim X &prime;u/n = 0. The standard errors of
β̂OLS as computed by the regression package are biased and any test of hypothesis based on
this OLS regression may be misleading.
So far we have not derived an estimator for σ2. We know however, from the results in Chapter
</p>
<p>7, that the transformed regression (9.3) yields a mean squared error that is an unbiased estimator
for σ2. Denote this by s&lowast;2 which is equal to the transformed OLS residual sum of squares
divided by (n &minus; K). Let e&lowast; denote the vector of OLS residuals from (9.3), this means that
e&lowast; = y&lowast; &minus;X&lowast;β̂GLS = P&minus;1(y &minus;Xβ̂GLS) = P&minus;1eGLS and
</p>
<p>s&lowast;2 = e&lowast;&prime;e&lowast;/(n&minus;K) = (y &minus;Xβ̂GLS)&prime;Ω&minus;1(y &minus;Xβ̂GLS)/(n&minus;K) (9.6)
= e&prime;GLSΩ
</p>
<p>&minus;1eGLS/(n&minus;K)
Note that s&lowast;2 now depends upon Ω&minus;1.
</p>
<p>Necessary and Sufficient Conditions for OLS to be Equivalent to GLS
</p>
<p>There are several necessary and sufficient conditions for OLS to be equivalent to GLS, see
Puntanen and Styan (1989) for a historical survey. For pedagogical reasons, we focus on the
derivation of Milliken and Albohali (1984). Note that y = PXy + P̄Xy. Therefore, replacing y</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Special Forms of Ω 225
</p>
<p>in β̂GLS by this expression we get
</p>
<p>β̂GLS = (X
&prime;Ω&minus;1X)&minus;1X &prime;Ω&minus;1[PXy + P̄Xy] = β̂OLS + (X
</p>
<p>&prime;Ω&minus;1X)&minus;1X &prime;Ω&minus;1P̄Xy
</p>
<p>The last term is zero for every y if and only if
</p>
<p>X &prime;Ω&minus;1P̄X = 0 (9.7)
</p>
<p>Therefore, β̂GLS = β̂OLS if and only if (9.7) is true.
Another easy necessary and sufficient condition to check in practice is the following:
</p>
<p>PXΩ = ΩPX (9.8)
</p>
<p>see Zyskind (1967). This involves Ω rather than Ω&minus;1. There are several applications in economics
where these conditions are satisfied and can be easily verified, see Balestra (1970) and Baltagi
(1989). We will apply these conditions in Chapter 10 on Seemingly Unrelated Regressions,
Chapter 11 on simultaneous equations and Chapter 12 on panel data. See also problem 9.
</p>
<p>9.3 Special Forms of Ω
</p>
<p>If the disturbances are heteroskedastic but not serially correlated, then Ω = diag[σ2i ]. In this case,
P = diag[σi], P
</p>
<p>&minus;1 = Ω&minus;1/2 = diag[1/σi] and Ω&minus;1= diag[1/σ2i ]. Premultiplying the regression
equation by Ω&minus;1/2 is equivalent to dividing the i-th observation of this model by σi. This makes
the new disturbance ui/σi have 0 mean and homoskedastic variance σ
</p>
<p>2, leaving properties
like no serial correlation intact. The new regression runs y&lowast;i = yi/σi on X
</p>
<p>&lowast;
ik = Xik/σi for
</p>
<p>i = 1, 2, . . . , n, and k = 1, 2, . . . ,K. Specific assumptions on the form of these σi&rsquo;s were studied
in the heteroskedasticity chapter.
If the disturbances follow an AR(1) process ut = ρut&minus;1 + ǫt for t = 1, 2, . . . , T ; with |ρ| &le; 1
</p>
<p>and ǫt &sim;IID(0, σ
2
ǫ ), then cov(ut, ut&minus;s) = ρ
</p>
<p>sσ2u with σ
2
u = σ
</p>
<p>2
ǫ/(1&minus; ρ2). This means that
</p>
<p>Ω =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>1 ρ ρ2 . . . ρT&minus;1
</p>
<p>ρ 1 ρ . . . ρT&minus;2
</p>
<p>: : : :
ρT&minus;1 ρT&minus;2 ρT&minus;3 . . . 1
</p>
<p>⎤
⎥⎥⎦ (9.9)
</p>
<p>and
</p>
<p>Ω&minus;1 =
</p>
<p>(
1
</p>
<p>1&minus; ρ2
)
⎡
⎢⎢⎢⎢⎣
</p>
<p>1 &minus;ρ 0 . . . 0 0 0
&minus;ρ 1 + ρ2 &minus;ρ . . . 0 0 0
: : : : : :
0 0 0 . . . &minus;ρ 1 + ρ2 &minus;ρ
0 0 0 . . . 0 &minus;ρ 1
</p>
<p>⎤
⎥⎥⎥⎥⎦
</p>
<p>(9.10)
</p>
<p>Then
</p>
<p>P&minus;1 =
</p>
<p>⎡
⎢⎢⎢⎢⎢⎢⎣
</p>
<p>&radic;
1&minus; ρ2 0 0 . . . 0 0 0
&minus;ρ 1 0 . . . 0 0 0
0 &minus;ρ 1 . . . 0 0 0
: : : : : :
0 0 0 . . . &minus;ρ 1 0
0 0 0 . . . 0 &minus;ρ 1
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎦
</p>
<p>(9.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>226 Chapter 9: Generalized Least Squares
</p>
<p>is the matrix that satisfies the following condition P&minus;1&prime;P&minus;1 = (1 &minus; ρ2)Ω&minus;1. Premultiplying
the regression model by P&minus;1 is equivalent to performing the Prais-Winsten transformation. In
particular the first observation on y becomes y&lowast;1 =
</p>
<p>&radic;
1&minus; ρ2y1 and the remaining observations are
</p>
<p>given by y&lowast;t = (yt&minus;ρyt&minus;1) for t = 2, 3, . . . , T , with similar terms for theX&rsquo;s and the disturbances.
Problem 3 shows that the variance covariance matrix of the transformed disturbances u&lowast; = P&minus;1u
is σ2ǫIT .
Other examples where an explicit form for P&minus;1 has been derived include, (i) the MA(1) model,
</p>
<p>see Balestra (1980); (ii) the AR(2) model, see Lempers and Kloek (1973); (iii) the specialized
AR(4) model for quarterly data, see Thomas and Wallis (1971); and (iv) the error components
model, see Fuller and Battese (1974) and Chapter 12.
</p>
<p>9.4 Maximum Likelihood Estimation
</p>
<p>Assuming that u &sim; N(0, σ2Ω), the new likelihood function can be derived keeping in mind that
u&lowast; = P&minus;1u = Ω&minus;1/2u and u&lowast; &sim; N(0, σ2In). In this case
</p>
<p>f(u&lowast;1, . . . , u
&lowast;
n;σ
</p>
<p>2) = (1/2πσ2)n/2 exp{&minus;u&lowast;&prime;u&lowast;/2σ2} (9.12)
</p>
<p>Making the transformation u = Pu&lowast; = Ω1/2u&lowast;, we get
</p>
<p>f(u1, . . . , un;σ
2) = (1/2πσ2)n/2|Ω&minus;1/2| exp{&minus;u&prime;Ω&minus;1u/2σ2} (9.13)
</p>
<p>where |Ω&minus;1/2| is the Jacobian of the inverse transformation. Finally, substituting y = Xβ + u
in (9.13), one gets the likelihood function
</p>
<p>L(β, σ2; Ω) = (1/2πσ2)n/2|Ω&minus;1/2| exp{&minus;(y &minus;Xβ)&prime;Ω&minus;1(y &minus;Xβ)/2σ2} (9.14)
</p>
<p>since the Jacobian of this last transformation is 1. Knowing Ω, maximizing (9.14) with respect to
β is equivalent to minimizing u&lowast;&prime;u&lowast; with respect to β. This means that β̂MLE is the OLS estimate
on the transformed model, i.e., β̂GLS . From (9.14), we see that this RSS is a weighted one with
the weight being the inverse of the variance covariance matrix of the disturbances. Similarly,
maximizing (9.14) with respect to σ2 gets σ̂2MLE = the OLS residual sum of squares of the
transformed regression (9.3) divided by n. From (9.6) this can be written as σ̂2MLE = e
</p>
<p>&lowast;&prime;e&lowast;/n =
(n &minus; K)s&lowast;2/n. The distributions of these maximum likelihood estimates can be derived from
the transformed model using the results in Chapter 7. In fact, β̂GLS &sim; N(β, σ
</p>
<p>2(X &prime;Ω&minus;1X)&minus;1)
and (n&minus;K)s&lowast;2/σ2 &sim; χ2n&minus;K .
</p>
<p>9.5 Test of Hypotheses
</p>
<p>In order to test H0; Rβ = r, under the general variance-covariance matrix assumption, one can
revert to the transformed model (9.3) which has a scalar identity variance-covariance matrix
and use the test statistic derived in Chapter 7
</p>
<p>(Rβ̂GLS &minus; r)&prime;[R(X&lowast;&prime;X&lowast;)&minus;1R&prime;]&minus;1(Rβ̂GLS &minus; r)/σ2 &sim; χ2g (9.15)
</p>
<p>Note that β̂GLS replaces β̂OLS and X
&lowast; replaces X. Replacing X&lowast; by P&minus;1X, we get
</p>
<p>(Rβ̂GLS &minus; r)&prime;[R(X &prime;Ω&minus;1X)&minus;1R&prime;]&minus;1(Rβ̂GLS &minus; r)/σ2 &sim; χ2g (9.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>227
</p>
<p>This differs from its counterpart in the spherical disturbances model in two ways. β̂GLS replaces
β̂OLS , and (X
</p>
<p>&prime;Ω&minus;1X) takes the place of X &prime;X. One can also derive the restricted estimator based
on the transformed model by simply replacing X&lowast; by P&minus;1X and the OLS estimator of β by its
GLS counterpart. Problem 4 asks the reader to verify that the restricted GLS estimator is
</p>
<p>β̂RGLS = β̂GLS &minus; (X &prime;Ω&minus;1X)&minus;1R&prime;[R(X &prime;Ω&minus;1X)&minus;1R&prime;]&minus;1(Rβ̂GLS &minus; r) (9.17)
</p>
<p>Furthermore, using the same analysis given in Chapter 7, one can show that (9.15) is in fact
the Likelihood Ratio statistic and is equal to the Wald and Lagrangian Multiplier statistics,
see Buse (1982). In order to operationalize these tests, we replace σ2 by its unbiased estimate
s&lowast;2, and divide by g the number of restrictions. The resulting statistic is an F (g, n&minus;K) for the
same reasons given in Chapter 7.
</p>
<p>9.6 Prediction
</p>
<p>How is prediction affected by nonspherical disturbances? Suppose we want to predict one period
ahead. What has changed with a general Ω? For one thing, we now know that the period (T +1)
disturbance is correlated with the sample disturbances. Let us assume that this correlation is
given by the (T &times; 1) vector ω = E(uT+1u), problem 5 shows that the BLUP for yT+1 is
</p>
<p>ŷT+1 = x
&prime;
T+1β̂GLS + ω
</p>
<p>&prime;Ω&minus;1(y &minus;Xβ̂GLS)/σ2 (9.18)
</p>
<p>The first term is as expected, however it is the second term that highlights the difference
between the spherical and nonspherical model predictions. To illustrate this, let us look at the
AR(1) case where cov(ut, ut&minus;s) = ρsσ2u. This implies that ω
</p>
<p>&prime; = σ2u(ρ
T , ρT&minus;1, . . . , ρ). Using Ω
</p>
<p>which is given in (9.9), one can show that ω is equal to ρσ2u multiplied by the last column of
Ω. But Ω&minus;1Ω = IT , therefore, Ω&minus;1 times the last column of Ω gives the last column of the
identity matrix, i.e., (0, 0, . . . , 1)&prime; . Substituting for the last column of Ω its expression (ω/ρσ2u)
one gets, Ω&minus;1(ω/ρσ2u) = (0, 0, . . . , 1)
</p>
<p>&prime;. Transposing and rearranging this last expression, we get
ω&prime;Ω&minus;1/σ2u = ρ(0, 0, . . . , 1). This means that the last term in (9.18) is equal to ρ(0, 0, . . . , 1)(y&minus;
Xβ̂GLS) = ρeT,GLS , where eT,GLS is the T -th GLS residual. This differs from the spherical
model prediction in that next year&rsquo;s disturbance is not independent of the sample disturbances
and hence, is not predicted by its mean which is zero. Instead, one uses the fact that uT+1 =
ρuT + ǫT+1 and predicts uT+1 by ρeT,GLS . Only ǫT+1 is predicted by its zero mean but uT is
predicted by eT,GLS .
</p>
<p>9.7 Unknown Ω
</p>
<p>If Ω is unknown, the practice is to get a consistent estimate of Ω, say Ω̂ and substitute that in
β̂GLS . The resulting estimator is
</p>
<p>β̂FGLS = (X
&prime;Ω̂&minus;1X)&minus;1X &prime;Ω̂&minus;1y (9.19)
</p>
<p>and is called a feasible GLS estimator of β. Once Ω̂ replaces Ω the Gauss-Markov Theorem
no longer necessarily holds. In other words, β̂FGLS is not BLUE, although it is still consistent.
</p>
<p>9.7 Unknown Ω</p>
<p/>
</div>
<div class="page"><p/>
<p>228 Chapter 9: Generalized Least Squares
</p>
<p>The finite sample properties of β̂FGLS are in general difficult to derive. However, we have the
following asymptotic results.
</p>
<p>Theorem 1:
&radic;
n(β̂GLS &minus; β) and
</p>
<p>&radic;
n(β̂FGLS &minus; β) have the same asymptotic distribution
</p>
<p>N(0, σ2Q&minus;1), where Q = lim(X &prime;Ω&minus;1X)/n as n &rarr; &infin;, if (i) plim X &prime;(Ω̂&minus;1 &minus; Ω&minus;1)X/n = 0
and (ii) plim X &prime;(Ω̂&minus;1 &minus; Ω&minus;1)u/n = 0. A sufficient condition for this theorem to hold is that Ω̂
is a consistent estimator of Ω and X has a satisfactory limiting behavior.
</p>
<p>Lemma 1: If in addition plim u&prime;(Ω̂&minus;1 &minus; Ω&minus;1)u/n = 0, then s&lowast;2 = e&prime;GLSΩ&minus;1eGLS/(n &minus;K) and
ŝ&lowast;2 = e&prime;FGLSΩ̂
</p>
<p>&minus;1eFGLS/(n &minus;K) are both consistent for σ2. This means that one can perform
test of hypotheses based on asymptotic arguments using β̂FGLS and ŝ
</p>
<p>&lowast;2 rather than β̂GLS and
s&lowast;2, respectively. For a proof of Theorem 1 and Lemma 1, see Theil (1971), Schmidt (1976) or
Judge et al. (1985).
Monte Carlo evidence under heteroskedasticity or serial correlation suggest that there is gain
</p>
<p>in performing feasible GLS rather than OLS in finite samples. However, we have also seen in
Chapter 5 that performing a two-step Cochrane-Orcutt procedure is not necessarily better than
OLS if the X&rsquo;s are trended. This says that feasible GLS omitting the first observation (in this
case Cochrane-Orcutt) may not be better in finite samples than OLS using all the observations.
</p>
<p>9.8 The W, LR and LM Statistics Revisited
</p>
<p>In this section we present a simplified and more general proof of W &ge; LR &ge; LM due to
Breusch (1979). For the general linear model given in (9.1) with u &sim; N(0,Σ) and H0; Rβ = r.
The likelihood function given in (9.14) with Σ = σ2Ω, can be maximized with respect to
β and Σ without imposing H0, yielding the unrestricted estimators β̂u and Σ̂, where β̂u =
(X &prime;Σ̂&minus;1X)&minus;1X &prime;Σ̂&minus;1y. Similarly, this likelihood can be maximized subject to the restriction H0,
yielding β̃r and Σ̃, where
</p>
<p>β̃r = (X
&prime;Σ̃&minus;1X)&minus;1X &prime;Σ̃&minus;1y &minus; (X &prime;Σ̃&minus;1X)&minus;1R&prime;μ̃ (9.20)
</p>
<p>as in (9.17), where μ̃ = Ã&minus;1(Rβ̂r &minus; r) is the Lagrange multiplier described in equation (7.35) of
Chapter 7 and Ã = [R(X &prime;Σ̃&minus;1X)&minus;1R&prime;]. The major distinction from Chapter 7 is that Σ is un-
known and has to be estimated. Let β̂r denote the unrestricted maximum likelihood estimator of
β conditional on the restricted variance-covariance estimator Σ̃ and let β̃u denote the restricted
maximum likelihood of β (satisfying H0) conditional on the unrestricted variance-covariance
estimator Σ̂. More explicitly,
</p>
<p>β̂r = (X
&prime;Σ̃&minus;1X)&minus;1X &prime;Σ̃&minus;1y (9.21)
</p>
<p>and
</p>
<p>β̃u = β̂u &minus; (X &prime;Σ̂&minus;1X)&minus;1R&prime;Â&minus;1(Rβ̂u &minus; r) (9.22)
</p>
<p>Knowing Σ, the Likelihood Ratio statistic is given by
</p>
<p>LR = &minus;2log[max L
Rβ=r
</p>
<p>(β/Σ)/max L
β
</p>
<p>(β/Σ)] = &minus;2log[L(β̃,Σ)/L(β̂,Σ)] (9.23)
</p>
<p>= ũ&prime;Σ&minus;1ũ&minus; û&prime;Σ&minus;1û</p>
<p/>
</div>
<div class="page"><p/>
<p>9.8 The W, LR and LM Statistics Revisited 229
</p>
<p>where û = y &minus;Xβ̂ and ũ = y &minus;Xβ̃, both estimators of β are conditional on a known Σ.
Rβ̂u &sim; N(Rβ,R(X
</p>
<p>&prime;Σ̂&minus;1X)&minus;1R&prime;)
</p>
<p>and the Wald statistic is given by
</p>
<p>W = (Rβ̂u &minus; r)&prime;Â&minus;1(Rβ̂u &minus; r) where Â = [R(X &prime;Σ̂&minus;1X)&minus;1R&prime;] (9.24)
Using (9.22), it is easy to show that ũu = y &minus;Xβ̃u and ûu = y &minus;Xβ̂u are related as follows:
</p>
<p>ũu = ûu +X(X
&prime;Σ̂&minus;1X)&minus;1R&prime;Â&minus;1(Rβ̂u &minus; r) (9.25)
</p>
<p>and
</p>
<p>ũ&prime;uΣ̂
&minus;1ũu = û
</p>
<p>&prime;
uΣ̂
</p>
<p>&minus;1ûu + (Rβ̂u &minus; r)&prime;Â&minus;1(Rβ̂u &minus; r) (9.26)
The cross-product terms are zero because X &prime;Σ̂&minus;1ûu = 0. Therefore,
</p>
<p>W = ũ&prime;uΣ̂
&minus;1ũu &minus; û&prime;uΣ̂&minus;1ûu = &minus;2log[L(β̃, Σ̂)/L(β̂, Σ̂)] (9.27)
</p>
<p>= &minus;2log[max L
Rβ=r
</p>
<p>(β/Σ̂)/max L
β
</p>
<p>(β/Σ̂)]
</p>
<p>and the Wald statistic can be interpreted as a LR statistic conditional on Σ̂, the unrestricted
maximum likelihood estimator of Σ.
Similarly, the Lagrange multiplier statistic, which tests that μ = 0, is given by
</p>
<p>LM = μ&prime;Ãμ = (Rβ̂r &minus; r)&prime;Ã&minus;1(Rβ̂r &minus; r) (9.28)
Using (9.20) one can easily show that
</p>
<p>ũr = ûr +X(X
&prime;Σ̃&minus;1X)&minus;1R&prime;Ã&minus;1(Rβ̂r &minus; r) (9.29)
</p>
<p>and
</p>
<p>ũ&prime;rΣ̃
&minus;1ũr = û
</p>
<p>&prime;
rΣ̃
</p>
<p>&minus;1ûr + μ
&prime;Ãμ (9.30)
</p>
<p>The cross-product terms are zero because X &prime;Σ̃&minus;1ûr = 0. Therefore,
</p>
<p>LM = ũ&prime;rΣ̃
&minus;1ũr &minus; û&prime;rΣ̃&minus;1ûr = &minus;2log[L(β̃r, Σ̃)/L(β̂r, Σ̃)] (9.31)
</p>
<p>= &minus;2log[max L
Rβ=r
</p>
<p>(β/Σ̃)/max
β
</p>
<p>L(β/Σ̃)]
</p>
<p>and the Lagrange multiplier statistic can be interpreted as a LR statistic conditional on Σ̃ the
restricted maximum likelihood of Σ. Given that
</p>
<p>max L
β
</p>
<p>(β/Σ̃) &le; max L
β,Σ
</p>
<p>(β,Σ) = max L
β
</p>
<p>(β/Σ̂) (9.32)
</p>
<p>max L
Rβ=r
</p>
<p>(β/Σ̂) &le; max L
Rβ=r,Σ
</p>
<p>(β,Σ) = max L
Rβ=r
</p>
<p>(β/Σ̃) (9.33)
</p>
<p>it can be easily shown that the likelihood ratio statistic given by
</p>
<p>LR = &minus;2log[max L
Rβ=r,Σ
</p>
<p>(β,Σ)/max L
β,Σ
</p>
<p>(β,Σ)] (9.34)
</p>
<p>satisfies the following inequality
</p>
<p>W &ge; LR &ge; LM (9.35)
The proof is left to the reader, see problem 6.
This general and simple proof holds as long as the maximum likelihood estimator of β is
</p>
<p>uncorrelated with the maximum likelihood estimator of Σ, see Breusch (1979).</p>
<p/>
</div>
<div class="page"><p/>
<p>230 Chapter 9: Generalized Least Squares
</p>
<p>9.9 Spatial Error Correlation1
</p>
<p>Unlike time-series, there is typically no unique natural ordering for cross-sectional data. Spatial
autocorrelation permit correlation of the disturbance terms across cross-sectional units. There
is an extensive literature on spatial models in regional science, urban economics, geography
and statistics, see Anselin (1988). Examples in economics usually involve spillover effects or
externalities due to geographical proximity. For example, the productivity of public capital, like
roads and highways, on the output of neighboring states. Also, the pricing of welfare in one
state that pushes recipients to other states. Spatial correlation could relate directly to the model
dependent variable y, the exogenous variables X, the disturbance term u, or to a combination
of all three. Here we consider spatial correlation in the disturbances and leave the remaining
literature on spatial dependence to the motivated reader to pursue in Anselin (1988, 2001) and
Anselin and Bera (1998) to mention a few.
For the cross-sectional disturbances, the spatial autocorrelation is specified as
</p>
<p>u = λWu+ ǫ (9.36)
</p>
<p>where λ is the spatial autoregressive coefficient satisfying |λ| &lt; 1, and ǫ &sim; IIN(0, σ2). W is a
known spatial weight matrix with diagonal elements equal to zero. W also satisfies some other
regularity conditions like the fact that In &minus; λW must be nonsingular.
</p>
<p>The regression model given in (9.1) can be written as
</p>
<p>y = Xβ + (In &minus; λW )&minus;1ǫ (9.37)
</p>
<p>with the variance-covariance matrix of the disturbances given by
</p>
<p>Σ = σ2Ω = σ2(In &minus; λW )&minus;1(In &minus; λW &prime;)&minus;1 (9.38)
</p>
<p>Under normality of the disturbances, Ord (1975) derived the maximum likelihood estimators
</p>
<p>lnL = &minus;1
2
ln|Ω| &minus; n
</p>
<p>2
ln2πσ2 &minus; (y &minus;Xβ)&prime;Ω&minus;1(y &minus;Xβ)/2σ2 (9.39)
</p>
<p>The Jacobian term simplifies by using
</p>
<p>ln|Ω| = &minus;2ln|I &minus; λW | = &minus;2&sum;ni=1 ln(1&minus; λwi) (9.40)
</p>
<p>where wi are the eigenvalues of the spatial weight matrix W . The first-order conditions yield
the familiar GLS estimator of β and the associated estimator of σ2:
</p>
<p>β̂MLE = (X
&prime;Ω&minus;1X)&minus;1X &prime;Ω&minus;1y and σ̂2MLE = e
</p>
<p>&prime;
MLEΩ
</p>
<p>&minus;1eMLE/n (9.41)
</p>
<p>where eMLE = y &minus; Xβ̂MLE . An estimate of λ can be obtained using the iterative solution of
the first-order conditions in Magnus (1978, p. 283):
</p>
<p>&minus;1
2
tr
</p>
<p>[(
&part;Ω&minus;1
</p>
<p>&part;λ
</p>
<p>)
Ω
</p>
<p>]
= e&prime;MLE
</p>
<p>(
&part;Ω&minus;1
</p>
<p>&part;λ
</p>
<p>)
eMLE (9.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>Note 231
</p>
<p>where
</p>
<p>&part;Ω&minus;1/&part;λ = &minus;W &minus;W &prime; + λW &prime;W (9.43)
</p>
<p>Alternatively, one can substitute β̂MLE and σ̂
2
MLE from (9.41) into the log-likelihood in (9.39)
</p>
<p>to get the concentrated log-likelihood which will be a nonlinear function of λ, see Anselin (1988)
for details.
</p>
<p>Testing for zero spatial autocorrelation i.e., H0; λ = 0 is usually based on the Moran I-test
which is similar to the Durbin-Watson statistic in time-series. This is given by
</p>
<p>MI =
n
</p>
<p>S0
</p>
<p>(
e&prime;We
e&prime;e
</p>
<p>)
(9.44)
</p>
<p>where e denotes the vector of OLS residuals and S0 is a standardization factor equal to the sum
of the spatial weights
</p>
<p>&sum;n
i=1
</p>
<p>&sum;n
j=1wij . For a row-standardized weights matrix W where each row
</p>
<p>sums to one, S0 = n and the Moran I-statistic simplifies to e
&prime;We/e&prime;e. In practice the test is
</p>
<p>implemented by standardizing it and using the asymptotic N(0, 1) critical values, see Anselin
and Bera (1988). In fact, for a row-standardized W matrix, the mean and variance of the Moran
I-statistic is obtained from
</p>
<p>E(MI) = E
</p>
<p>(
e&prime;We
e&prime;e
</p>
<p>)
= tr(P̄XW )/(n&minus; k) (9.45)
</p>
<p>and
</p>
<p>E(MI)2 =
tr(P̄XWP̄XW
</p>
<p>&prime;) + tr(P̄XW )2 + {tr(P̄X̄W )}2
(n&minus; k)(n&minus; k + 2)
</p>
<p>Alternatively, one can derive the Lagrange Multiplier test for H0; λ = 0 using the result that
&part;lnL/&part;λ evaluated under the null of λ = 0 is equal to u&prime;Wu/σ2 and the fact that the Information
matrix is block-diagonal between β and (σ2, λ), see problem 14. In fact, one can show that
</p>
<p>LMλ =
(e&prime;We/σ̃2)2
</p>
<p>tr[(W &prime; +W )W ]
(9.46)
</p>
<p>with σ̃2 = e&prime;e/n. Under H0, LMλ is asymptotically distributed as χ21. One can clearly see the
connection between Moran&rsquo;s I-statistic and LMλ. Computationally, the W and LR tests are
more demanding since the require ML estimation under spatial autocorrelation.
This is only a brief introduction into the spatial dependence literature. Hopefully, it will moti-
</p>
<p>vate the reader to explore alternative formulations of spatial dependence, alternative estimation
and testing methods discussed in this literature and the numerous applications in economics on
hedonic housing, crime rates, police expenditures and R&amp;D spillovers, to mention a few.
</p>
<p>Note
</p>
<p>1. This section is based on Anselin (1988, 2001) and Anselin and Bera (1998).</p>
<p/>
</div>
<div class="page"><p/>
<p>232 Chapter 9: Generalized Least Squares
</p>
<p>Problems
</p>
<p>1. GLS Is More Efficient than OLS.
</p>
<p>(a) Using equation (7.5) of Chapter 7, verify that var(β̂OLS) is that given in (9.5).
</p>
<p>(b) Show that var(β̂OLS)&minus; var(β̂GLS) = σ2AΩA&prime; where
</p>
<p>A = [(X &prime;X)&minus;1X &prime; &minus; (X &prime;Ω&minus;1X)&minus;1X &prime;Ω&minus;1].
</p>
<p>Conclude that this difference in variances is positive semi-definite.
</p>
<p>2. s2 Is No Longer Unbiased for σ2.
</p>
<p>(a) Show that E(s2) = σ2tr(ΩP̄X)/(n &minus; K) 	= σ2. Hint: Follow the same proof given below
equation (7.6) of Chapter 7, but substitute σ2Ω instead of σ2In.
</p>
<p>(b) Use the fact that PX and Σ are non-negative definite matrices with tr(ΣPX) &ge; 0 to show
that 0 &le; E(s2) &le; tr(Σ)/(n &minus; K) where tr(Σ) =
</p>
<p>&sum;n
i=1 σ
</p>
<p>2
i with σ
</p>
<p>2
i = var(ui) &ge; 0. This
</p>
<p>bound was derived by Dufour (1986). Under homoskedasticity, show that this bound becomes
0 &le; E(s2) &le; nσ2/(n &minus;K). In general, 0 &le; {mean of n &minus;K smallest characteristic roots of
Σ} &le; E(s2) &le; {mean of n&minus;K largest characteristic roots of Σ} &le; tr(Σ)/(n&minus;K), see Sathe
and Vinod (1974) and Neudecker (1977, 1978).
</p>
<p>(c) Show that a sufficient condition for s2 to be consistent for σ2 irrespective of X is that
λmax = the largest characteristic root of Ω is o(n), i.e., λmax/n &rarr; 0 as n &rarr; &infin; and plim
(u&prime;u/n) = σ2. Hint: s2 = u&prime;P̄Xu/(n&minus;K) = u&prime;u/(n&minus;K)&minus; u&prime;PXu/(n&minus;K). By assumption,
the first term tends in probability limits to σ2 as n &rarr; &infin;. The second term has expectation
σ2tr(PXΩ)/(n&minus;K). Now PXΩ has rank K and therefore exactly K non-zero characteristic
roots each of which cannot exceed λmax. This means that E[u
</p>
<p>&prime;PXu/(n&minus;K)] &le; σ2Kλmax/(n&minus;
K). Using the condition that λmax/n &rarr; 0 proves the result. See Krämer and Berghoff (1991).
</p>
<p>(d) Using the same reasoning in part (a), show that s&lowast;2 given in (9.6) is unbiased for σ2.
</p>
<p>3. The AR(1) Model. See Kadiyala (1968).
</p>
<p>(a) Verify that ΩΩ&minus;1 = IT for Ω and Ω&minus;1 given in (9.9) and (9.10), respectively.
</p>
<p>(b) Show that P&minus;1&prime;P&minus;1 = (1&minus; ρ2)Ω&minus;1 for P&minus;1 defined in (9.11).
(c) Conclude that var(P&minus;1u) = σ2ǫIT . Hint: Ω = (1&minus;ρ2)PP &prime; as can be easily derived from part
</p>
<p>(b).
</p>
<p>4. Restricted GLS. Using the derivation of the restricted least squares estimator for u &sim; (0, σ2In) in
Chapter 7, verify equation (9.17) for the restricted GLS estimator based on u &sim; (0, σ2Ω). Hint:
Apply restricted least squares results to the transformed model given in (9.3).
</p>
<p>5. Best Linear Unbiased Prediction. This is based on Goldberger (1962). Consider all linear predictors
of yT+s = x
</p>
<p>&prime;
T+sβ + uT+s of the form ŷT+s = c
</p>
<p>&prime;y, where u &sim; (0,Σ) and Σ = σ2Ω.
</p>
<p>(a) Show that c&prime;X = x&prime;T+s for ŷT+s to be unbiased.
</p>
<p>(b) Show that var(ŷT+s) = c
&prime;Σc+ σ2T+s &minus; 2c&prime;ω where var(uT+s) = σ2T+s and ω = E(uT+su).
</p>
<p>(c) Minimize var(ŷT+s) given in part (b) subject to c
&prime;X = x&prime;T+s and show that
</p>
<p>ĉ = Σ&minus;1[IT &minus;X(X &prime;Σ&minus;1X)&minus;1X &prime;Σ&minus;1]ω +Σ&minus;1X(X &prime;Σ&minus;1X)&minus;1xT+s
</p>
<p>This means that ŷT+s = ĉ
&prime;y = x&prime;T+sβ̂GLS + ω
</p>
<p>&prime;Σ&minus;1eGLS = x&prime;T+sβ̂GLS + ω
&prime;Ω&minus;1eGLS/σ2. For
</p>
<p>s = 1, i.e., predicting one period ahead, this verifies equation (9.18). Hint: Use partitioned
inverse in solving the first-order minimization equations.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 233
</p>
<p>(d) Show that ŷT+s = x
&prime;
T+sβ̂GLS +ρ
</p>
<p>seT,GLS for the stationary AR(1) disturbances with autore-
gressive parameter ρ, and |ρ| &lt; 1.
</p>
<p>6. The W, LR and LM Inequality. Using the inequalities given in equations (9.32) and (9.33) verify
equation (9.35) which states that W &ge; LR &ge; LM . Hint: Use the conditional likelihood ratio
interpretations of W and LM given in equations (9.27) and (9.31) respectively.
</p>
<p>7. Consider the simple linear regression
</p>
<p>yi = α+ βXi + ui i = 1, 2, . . . , n
</p>
<p>with ui &sim; IIN(0, σ
2). ForH0; β = 0, derive the LR, W and LM statistics in terms of conditional like-
</p>
<p>lihood ratios as described in Breusch (1979). In other words, computeW = &minus;2 log[max L
H0
</p>
<p>(α, β/σ̂2)/
</p>
<p>max L
α,β
</p>
<p>(α, β/σ̂2)], LM =&minus;2log[max L
H0
</p>
<p>(α, β/σ̃2)/max L
α,β
</p>
<p>(α, β/σ̃2)] andLR=&minus;2log[max L
H0
</p>
<p>(α, β, σ2)/
</p>
<p>max L
α,β,σ2
</p>
<p>(α, β, σ2)] where σ̂2 is the unrestricted MLE of σ2 while σ̃2 is the restricted MLE of σ2 under
</p>
<p>H0. Use these results to infer that W &ge; LR &ge; LM .
</p>
<p>8. Sampling Distributions and Efficiency Comparison of OLS and GLS. Consider the following re-
gression model yt = βxt + ut for (t = 1, 2), where β = 2 and xt takes on the fixed values x1 = 1,
x2 = 2. The ut&rsquo;s have the following discrete joint probability distribution:
</p>
<p>(u1, u2) Probability
(&minus;1,&minus;2) 1/8
(1,&minus;2) 3/8
(&minus;1, 2) 3/8
(1, 2) 1/8
</p>
<p>(a) What is the variance-covariance matrix of the disturbances? Are the disturbances het-
eroskedastic? Are they correlated?
</p>
<p>(b) Find the sampling distributions of β̂OLS and β̃GLS and verify that var(β̂OLS) &gt; var(β̃GLS).
</p>
<p>(c) Find the sampling distribution of the OLS residuals and verify that the estimated var(β̂OLS)
is biased. Also, find the sampling distribution of the GLS residuals and verify that the MSE
of the GLS regression is an unbiased estimator of the GLS regression variance. Hint: Read
Oksanen (1991) and Phillips and Wickens (1978), pp. 3&ndash;4. This problem is based on Baltagi
(1992). See also the solution by Im and Snow (1993).
</p>
<p>9. Equi-correlation. This problem is based on Baltagi (1998). Consider the regression model given
in (9.1) with equi-correlated disturbances, i.e., equal variances and equal covariances: E(uu&prime;) =
σ2Ω = σ2[(1 &minus; ρ)IT + ριT ι&prime;T ] where ιT is a vector of ones of dimension T and IT is the identity
matrix. In this case, var(ut) = σ
</p>
<p>2 and cov(ut, us) = ρσ
2 for t 	= s with t = 1, 2, . . . , T . Assume
</p>
<p>that the regression has a constant.
</p>
<p>(a) Show that OLS on this model is equivalent to GLS. Hint: Verify Zyskind&rsquo;s condition given
in (9.8) using the fact that PXιT = ιT if ιT is a column of X.
</p>
<p>(b) Show that E(s2) = σ2(1&minus;ρ). Also, that Ω is positive semi-definite when &minus;1/(T &minus;1) &le; ρ &le; 1.
Conclude that if &minus;1/(T &minus; 1) &le; ρ &le; 1, then 0 &le; E(s2) &le; [T/(T &minus; 1)]σ2. The lower and upper
bounds are attained at ρ = 1 and ρ = &minus;1/(T &minus; 1), respectively, see Dufour (1986). Hint: Ω
is positive semi-definite if for every arbitrary non-zero vector a we have a&prime;Ωa &ge; 0. What is
this expression for a = ιT ?
</p>
<p>(c) Show that for this equi-correlated regression model, the BLUP of yT+1 = x
&prime;
T+1β + uT+1 is
</p>
<p>ŷT+1 = x
&prime;
T+1β̂OLS as long as there is a constant in the model.</p>
<p/>
</div>
<div class="page"><p/>
<p>234 Chapter 9: Generalized Least Squares
</p>
<p>10. Consider the simple regression with no regressors and equi-correlated disturbances:
</p>
<p>yi = α+ ui i = 1, . . . , n
</p>
<p>where E(ui) = 0 and
</p>
<p>cov(ui, uj) = ρσ
2 for i 	= j
</p>
<p>= σ2 for i = j
</p>
<p>with 1(n&minus;1) &le; ρ &le; 1 for the variance-covariance matrix of the disturbances to be positive definite.
</p>
<p>(a) Show that the OLS and GLS estimates of α are identical. This is based on Kruskal (1968).
</p>
<p>(b) Show that the bias in s2, the OLS estimator of σ2, is given by &minus;ρσ2.
(c) Show that the GLS estimator of σ2 is unbiased.
</p>
<p>(d) Show that the E[estimated var(α̂)&minus; true var(α̂OLS)] is also &minus;ρσ2.
</p>
<p>11. Prediction Error Variances Under Heteroskedasticity. This is based on Termayne (1985). Consider
the t-th observation of the linear regression model given in (9.1).
</p>
<p>yt = x
&prime;
tβ + ut t = 1, 2, . . . , T
</p>
<p>where yt is a scalar x
&prime;
t is 1&times;K and β is a K &times; 1 vector of unknown coefficients. ut is assumed to
</p>
<p>have zero mean, heteroskedastic variances E(u2t ) = (z
&prime;
tγ)
</p>
<p>2 where z&prime;t is a 1 &times; r vector of observed
variables and γ is an r&times;1 vector of parameters. Furthermore, these ut&rsquo;s are not serially correlated,
so that E(utus) = 0 for t 	= s.
</p>
<p>(a) Find the var(β̂OLS) and var(β̃GLS) for this model.
</p>
<p>(b) Suppose we are forecasting y for period f in the future knowing xf , i.e., yf = x
&prime;
fβ+ uf with
</p>
<p>f &gt; T . Let êf and ẽf be the forecast errors derived using OLS and GLS, respectively. Show
that the prediction error variances of the point predictions of yf are given by
</p>
<p>var(êf ) = x
&prime;
f (
&sum;T
</p>
<p>t=1 xtx
&prime;
t)
</p>
<p>&minus;1[
&sum;T
</p>
<p>t=1 xtx
&prime;
t(z
</p>
<p>&prime;
tγ)
</p>
<p>2](
&sum;T
</p>
<p>t=1 xtx
&prime;
t)
</p>
<p>&minus;1xf + (z
&prime;
fγ)
</p>
<p>2
</p>
<p>var(ẽf ) = x
&prime;
f [
&sum;T
</p>
<p>t=1 xtx
&prime;
t(z
</p>
<p>&prime;
tγ)
</p>
<p>2]&minus;1xf + (z
&prime;
fγ)
</p>
<p>2
</p>
<p>(c) Show that the variances of the two forecast errors of conditional mean E(yf/xf ) based
</p>
<p>upon β̂OLS and β̃GLS and denoted by ĉf and c̃f , respectively are the first two terms of the
corresponding expressions in part (b).
</p>
<p>(d) Now assume that K = 1 and r = 1 so that there is only one single regressor xt and one
zt variable determining the heteroskedasticity. Assume also for simplicity that the empirical
moments of xt match the population moments of a Normal random variable with mean zero
and variance θ. Show that the relative efficiency of the OLS to the GLS predictor of yf is
equal to (T +1)/(T +3), whereas the relative efficiency of the corresponding ratio involving
the two predictions of the conditional mean is (1/3).
</p>
<p>12. Estimation of Time Series Regressions with Autoregressive Disturbances and Missing Observations.
This is based on Baltagi and Wu (1997). Consider the following time series regression model,
</p>
<p>yt = x
&prime;
tβ + ut t = 1, . . . , T,
</p>
<p>where β is a K&times;1 vector of regression coefficients including the intercept. The disturbances follow
a stationary AR(1) process, that is,
</p>
<p>ut = ρut&minus;1 + ǫt,</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 235
</p>
<p>with |ρ| &lt; 1, ǫt is IIN(0, σ2ǫ), and u0 &sim; N(0, σ2ǫ/(1&minus; ρ2)). This model is only observed at times tj
for j = 1, . . . , n with 1 = t1 &lt; . . . &lt; tn = T and n &gt; K. The typical covariance element of ut for
the observed periods tj and ts is given by
</p>
<p>cov(utj , uts) =
σ2ǫ
</p>
<p>1&minus; ρ2 ρ
|tj&minus;ts| for s, j = 1, . . . , n
</p>
<p>Knowing ρ, derive a simple Prais-Winsten-type transformation that will obtain GLS as a simple
least squares regression.
</p>
<p>13. Multiplicative Heteroskedasticity. This is based on Harvey (1976). Consider the linear model given
in (9.1) and let u &sim; N(0,Σ) where Σ = diag[σ2i ]. Assume that σ
</p>
<p>2
i = σ
</p>
<p>2hi(θ) with θ
&prime; = (θ1, . . . , θs)
</p>
<p>and hi(θ) = exp(θ1z1i + . . .+ θszsi) = exp(z
&prime;
iθ) with z
</p>
<p>&prime;
i = (z1i, . . . , zsi).
</p>
<p>(a) Show that log-likelihood function is given by
</p>
<p>log L(β, θ, σ2) = &minus;N
2
</p>
<p>log 2πσ2 &minus; 1
2
</p>
<p>&sum;N
i=1 log hi(θ)&minus;
</p>
<p>1
</p>
<p>2σ2
&sum;N
</p>
<p>i=1
</p>
<p>(yi &minus; x&prime;iβ)2
hi(θ)
</p>
<p>and the score with respect to θ is
</p>
<p>&part;log L/&part;θ = &minus;1
2
</p>
<p>&sum;N
i=1
</p>
<p>1
</p>
<p>hi(θ)
</p>
<p>&part;hi
&part;θ
</p>
<p>+
1
</p>
<p>2σ2
&sum;N
</p>
<p>i=1
</p>
<p>(yi &minus; x&prime;iβ)2
(hi(θ))2
</p>
<p>&middot; &part;hi
&part;θ
</p>
<p>Conclude that for multiplicative heteroskedasticity, equating this score to zero yields
</p>
<p>&sum;N
i=1
</p>
<p>(yi &minus; x&prime;iβ)2
exp(z&prime;iθ)
</p>
<p>zi = σ
2 &sum;N
</p>
<p>i=1 zi.
</p>
<p>(b) Show that the Information matrix is given by
</p>
<p>I(β, θ, σ2) =
</p>
<p>⎡
⎢⎢⎢⎢⎢⎢⎢⎣
</p>
<p>X &prime;Σ&minus;1X 0 0
</p>
<p>0
1
</p>
<p>2
</p>
<p>&sum;N
i=1
</p>
<p>1
</p>
<p>(hi(θ))2
&part;hi
&part;θ
</p>
<p>&part;hi
</p>
<p>&part;θ&prime;
1
</p>
<p>2σ2
&sum;N
</p>
<p>i=1
</p>
<p>1
</p>
<p>hi(θ)
</p>
<p>&part;hi
&part;θ
</p>
<p>0
1
</p>
<p>2σ2
&sum;N
</p>
<p>i=1
</p>
<p>1
</p>
<p>hi(θ)
</p>
<p>&part;hi
</p>
<p>&part;θ&prime;
N
</p>
<p>2σ4
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎥⎦
</p>
<p>and for multiplicative heteroskedasticity this becomes
</p>
<p>I(β, θ, σ2) =
</p>
<p>⎡
⎢⎢⎢⎢⎢⎢⎣
</p>
<p>X &prime;Σ&minus;1X 0 0
</p>
<p>0
1
</p>
<p>2
Z &prime;Z
</p>
<p>1
</p>
<p>2σ2
&sum;N
</p>
<p>i=1 zi
</p>
<p>0
1
</p>
<p>2σ2
&sum;N
</p>
<p>i=1 z
&prime;
i
</p>
<p>N
</p>
<p>2σ4
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎦
</p>
<p>where Z &prime;i = (z1, . . . , zN ).
</p>
<p>(c) Assume that hi(θ) satisfies hi(0) = 1, then the test for heteroskedasticity is H0; θ = 0 versus
H1; θ 	= 0. Show that the score with respect to θ and σ2 evaluated under the null hypothesis,
i.e., at θ = 0 and σ̃2 = e&prime;e/N is given by
</p>
<p>S̃ =
</p>
<p>⎛
⎝
</p>
<p>1
</p>
<p>2
</p>
<p>&sum;N
i=1 zi
</p>
<p>(
e2i
σ̃2
</p>
<p>&minus; 1
)
</p>
<p>0
</p>
<p>⎞
⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>236 Chapter 9: Generalized Least Squares
</p>
<p>where e denotes the vector of OLS residuals. The Information matrix with respect to θ and
σ2can be obtained from the bottom right block of I(β, θ, σ2) given in part (b). Conclude that
the score test for H0 is given by
</p>
<p>LM =
</p>
<p>&sum;N
i=1 z
</p>
<p>&prime;
i(e
</p>
<p>2
i &minus; σ̃2)
</p>
<p>(&sum;N
i=1(zi &minus; z̄) (zi &minus; z̄)
</p>
<p>&prime;
)&minus;1 &sum;N
</p>
<p>i=1 zi(e
2
i &minus; σ̃2)
</p>
<p>2σ̃4
</p>
<p>This statistic is asymptotically distributed as χ2s under H0. From Chapter 5, we can see that
this is a special case of the Breusch and Pagan (1979) test-statistic which can be obtained as
one-half the regression sum of squares of e2/σ̃2 on a constant and Z. Koenker and Bassett
</p>
<p>(1982) suggested replacing the denominator 2σ̃4 by
&sum;N
</p>
<p>i=1(e
2&minus; σ̃2)2/N to make this test more
</p>
<p>robust to departures from normality.
</p>
<p>14. Spatial Autocorrelation. Consider the regression model given in (9.1) with spatial autocorrelation
defined in (9.36).
</p>
<p>(a) Verify that the first-order conditions of maximization of the log-likelihood function given in
(9.39) yield (9.41).
</p>
<p>(b) Show that for testing H0; λ = 0, the score &part;lnL/&part;λ evaluated under the null, i.e., at λ = 0,
is given by u&prime;Wu/σ2.
</p>
<p>(c) Show that the Information matrix with respect to σ2and λ, evaluated under the null of λ = 0,
is given by
</p>
<p>⎡
⎢⎢⎢⎣
</p>
<p>n
</p>
<p>2σ4
tr(W )
</p>
<p>σ2
</p>
<p>tr(W )
</p>
<p>σ2
tr(W 2) + tr(W &prime;W )
</p>
<p>⎤
⎥⎥⎥⎦
</p>
<p>(d) Conclude from parts (b) and (c) that the Lagrange Multiplier for H0; λ = 0 is given by LMλ
in (9.46). Hint: Use the fact that the diagonal elements of W are zero, hence tr(W ) = 0.
</p>
<p>15. Neighborhood Effects and Housing Demand. Ioannides and Zabel (2003) use data from the Amer-
ican Housing Survey to estimate a model of housing demand with neighborhood effects. The
number of observations on housing units used were 1947 in 1985, 2318 in 1989 and 2909 in 1993.
The housing survey has detailed information for each of these housing units and their owners,
including: the owner&rsquo;s schooling, whether the owner is white, whether the owner is married, the
number of persons in the household, household income, and whether the house has changed owners
(&ldquo;changed hands&rdquo;) in the last 5 years. In addition, the current owner&rsquo;s evaluation of the housing
unit&rsquo;s market value, as well as various structural characteristics of the housing unit (such as num-
ber of bedrooms, bathrooms, and whether the house has a garage). The variable definitions are
given in Table VI of Ioannides and Zabel (2003, p. 568) and the data is available from the Journal
of Applied Econometrics archive:
</p>
<p>(a) Replicate Table VII of Ioannides and Zabel (2003, p. 569) which displays the means and
standard deviations for some of the variables by year and for the pooled sample. Note that
the price and income variables are different from the numbers reported in the paper.
</p>
<p>(b) Replicate Table VIII of Ioannides and Zabel (2003, p. 577) which reports regression results of
housing demand. Note that these regressions are different from those reported in the paper.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 237
</p>
<p>References
</p>
<p>Additional readings on GLS can be found in the econometric texts cited in the Preface.
</p>
<p>Anselin, L. (2001), &ldquo;Spatial Econometrics,&rdquo; Chapter 14 in B.H. Baltagi (ed.) A Companion to Theoretical
Econometrics (Blackwell: Massachusetts).
</p>
<p>Anselin, L. (1988), Spatial Econometrics: Methods and Models (Kluwer: Dordrecht).
</p>
<p>Anselin, L. and A.K. Bera (1998), &ldquo;Spatial Dependence in Linear Regression Models with an Introduction
to Spatial Econometrics,&rdquo; in A. Ullah and D.E.A. Giles (eds.) Handbook of Applied Economic
Statistics (Marcel Dekker: New York).
</p>
<p>Balestra, P. (1970), &ldquo;On the Efficiency of Ordinary Least Squares in Regression Models,&rdquo; Journal of the
American Statistical Association, 65: 1330&ndash;1337.
</p>
<p>Balestra, P. (1980), &ldquo;A Note on the Exact Transformation Associated with First-Order Moving Average
Process,&rdquo; Journal of Econometrics, 14: 381&ndash;394.
</p>
<p>Baltagi, B.H. (1989), &ldquo;Applications of a Necessary and Sufficient Condition for OLS to be BLUE,&rdquo;
Statistics and Probability Letters, 8: 457&ndash;461.
</p>
<p>Baltagi, B.H. (1992), &ldquo;Sampling Distributions and Efficiency Comparisons of OLS and GLS in the
Presence of Both Serial Correlation and Heteroskedasticity,&rdquo; Econometric Theory, Problem 92.2.3,
8: 304&ndash;305.
</p>
<p>Baltagi, B.H. and P.X. Wu (1997), &ldquo;Estimation of Time Series Regressions with Autoregressive Distur-
bances and Missing Observations,&rdquo; Econometric Theory, Problem 97.5.1, 13: 889.
</p>
<p>Baltagi, B.H. (1998), &ldquo;Prediction in the Equicorrelated Regression Model,&rdquo; Econometric Theory, Problem
98.3.3, 14: 382.
</p>
<p>Breusch, T.S. (1979), &ldquo;Conflict Among Criteria for Testing Hypotheses: Extensions and Comments,&rdquo;
Econometrica, 47: 203&ndash;207.
</p>
<p>Breusch, T.S. and A.R. Pagan (1979), &ldquo;A Simple Test for Heteroskedasticity and Random Coefficient
Variation,&rdquo; Econometrica, 47: 1287&ndash;1294.
</p>
<p>Buse, A. (1982), &ldquo;The Likelihood Ratio, Wald, and Lagrange Multiplier Tests: An Expository Note,&rdquo;
The American Statistician, 36: 153&ndash;157.
</p>
<p>Dufour, J.M. (1986), &ldquo;Bias of s2 in Linear Regressions with Dependent Errors,&rdquo; The American Statisti-
cian, 40: 284&ndash;285.
</p>
<p>Fuller, W.A. and G.E. Battese (1974), &ldquo;Estimation of Linear Models with Crossed-Error Structure,&rdquo;
Journal of Econometrics, 2: 67&ndash;78.
</p>
<p>Goldberger, A.S. (1962), &ldquo;Best Linear Unbiased Prediction in the Generalized Linear Regression Model,&rdquo;
Journal of the American Statistical Association, 57: 369&ndash;375.
</p>
<p>Harvey, A.C. (1976), &ldquo;Estimating Regression Models With Multiplicative Heteroskedasticity,&rdquo; Econo-
metrica, 44: 461&ndash;466.
</p>
<p>Im, E.I. and M.S. Snow (1993), &ldquo;Sampling Distributions and Efficiency Comparisons of OLS and GLS
in the Presence of Both Serial Correlation and Heteroskedasticity,&rdquo; Econometric Theory, Solution
92.2.3, 9: 322&ndash;323.
</p>
<p>Ioannides, Y.M. and J.E. Zabel (2003), &ldquo;Neighbourhood Effects and Housing Demand,&rdquo; Journal of
Applied Econometrics 18: 563&ndash;584.</p>
<p/>
</div>
<div class="page"><p/>
<p>238 Chapter 9: Generalized Least Squares
</p>
<p>Kadiyala, K.R. (1968), &ldquo;A Transformation Used to Circumvent the Problem of Autocorrelation,&rdquo; Econo-
metrica, 36: 93&ndash;96.
</p>
<p>Koenker, R. and G. Bassett, Jr. (1982), &ldquo;Robust Tests for Heteroskedasticity Based on Regression Quan-
tiles,&rdquo; Econometrica, 50: 43&ndash;61.
</p>
<p>Krämer, W. and S. Berghoff (1991), &ldquo;Consistency of s2 in the Linear Regression Model with Correlated
Errors,&rdquo; Empirical Economics, 16: 375&ndash;377.
</p>
<p>Kruskal, W. (1968), &ldquo;When are Gauss-Markov and Least Squares Estimators Identical? A Coordinate-
Free Approach,&rdquo; The Annals of Mathematical Statistics, 39: 70&ndash;75.
</p>
<p>Lempers, F.B. and T. Kloek (1973), &ldquo;On a Simple Transformation for Second-Order Autocorrelated
Disturbances in Regression Analysis,&rdquo; Statistica Neerlandica, 27: 69&ndash;75.
</p>
<p>Magnus, J. (1978), &ldquo;Maximum Likelihood Estimation of the GLS Model with Unknown Parameters in
the Disturbance Covariance Matrix,&rdquo; Journal of Econometrics, 7: 281&ndash;312.
</p>
<p>Milliken, G.A. and M. Albohali (1984), &ldquo;On Necessary and Sufficient Conditions for Ordinary Least
Squares Estimators to be Best Linear Unbiased Estimators,&rdquo; The American Statistician, 38: 298&ndash;
299.
</p>
<p>Neudecker, H. (1977), &ldquo;Bounds for the Bias of the Least Squares Estimator of σ2 in Case of a First-Order
Autoregressive Process (positive autocorrelation),&rdquo; Econometrica, 45: 1257&ndash;1262.
</p>
<p>Neudecker, H. (1978), &ldquo;Bounds for the Bias of the LS Estimator in the Case of a First-Order (positive)
Autoregressive Process Where the Regression Contains a Constant Term,&rdquo; Econometrica, 46: 1223&ndash;
1226.
</p>
<p>Newey, W. and K. West (1987), &ldquo;A Simple Positive Semi-Definite, Heteroskedasticity and Autocorrelation
Consistent Covariance Matrix,&rdquo; Econometrica, 55: 703&ndash;708.
</p>
<p>Oksanen, E.H. (1991), &ldquo;A Simple Approach to Teaching Generalized Least Squares Theory,&rdquo; The Amer-
ican Statistician, 45: 229&ndash;233.
</p>
<p>Ord, J.K. (1975), &ldquo; Estimation Methods for Models of Spatial Interaction,&rdquo; Journal of the American
Statistical Association, 70: 120&ndash;126.
</p>
<p>Phillips, P.C.B. and M.R. Wickens (1978), Exercises in Econometrics, Vol. 1 (Philip Allan/Ballinger:
Oxford).
</p>
<p>Puntanen S. and G.P.H. Styan (1989), &ldquo;The Equality of the Ordinary Least Squares Estimator and the
Best Linear Unbiased Estimator,&rdquo; (with discussion), The American Statistician, 43: 153&ndash;161.
</p>
<p>Sathe, S.T. and H.D. Vinod (1974), &ldquo;Bounds on the Variance of Regression Coefficients Due to Het-
eroskedastic or Autoregressive Errors,&rdquo; Econometrica, 42: 333&ndash;340.
</p>
<p>Schmidt, P. (1976), Econometrics (Marcell-Decker: New York).
</p>
<p>Termayne, A.R. (1985), &ldquo;Prediction Error Variances Under Heteroskedasticity,&rdquo; Econometric Theory,
Problem 85.2.3, 1: 293&ndash;294.
</p>
<p>Theil, H. (1971), Principles of Econometrics (Wiley: New York).
</p>
<p>Thomas, J.J and K.F. Wallis (1971), &ldquo;Seasonal Variation in Regression Analysis,&rdquo; Journal of the Royal
Statistical Society, Series A, 134: 67&ndash;72.
</p>
<p>White, H. (1980), &ldquo;A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for
Heteroskedasticity,&rdquo; Econometrica, 48: 817&ndash;838.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 239
</p>
<p>Zyskind, G. (1967), &ldquo;On Canonical Forms, Non-Negative Covariance Matrices and Best and Simple
</p>
<p>Least Squares Linear Estimators in Linear Models,&rdquo; The Annals of Mathematical Statistics, 38:
</p>
<p>1092&ndash;1109.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 10
</p>
<p>Seemingly Unrelated Regressions
</p>
<p>When asked &ldquo;How did you get the idea for SUR?&rdquo; Zellner responded: &ldquo;On a rainy
night in Seattle in about 1956 or 1957, I somehow got the idea of algebraically writing
a multivariate regression model in single equation form. When I figured out how to do
that, everything fell into place because then many univariate results could be carried
over to apply to the multivariate system and the analysis of the multivariate system
is much simplified notationally, algebraically and, conceptually.&rdquo; Read the interview
of Professor Arnold Zellner by Rossi (1989, p. 292).
</p>
<p>10.1 Introduction
</p>
<p>Consider two regression equations corresponding to two different firms
</p>
<p>yi = Xiβi + ui i = 1, 2 (10.1)
</p>
<p>where yi and ui are T &times; 1 and Xi is (T &times; Ki) with ui &sim; (0, σiiIT ). OLS is BLUE on each
equation separately. Zellner&rsquo;s (1962) idea is to combine these Seemingly Unrelated Regressions
in one stacked model, i.e.,
</p>
<p>[
y1
y2
</p>
<p>]
=
</p>
<p>[
X1 0
0 X2
</p>
<p>](
β1
β2
</p>
<p>)
+
</p>
<p>(
u1
u2
</p>
<p>)
(10.2)
</p>
<p>which can be written as
</p>
<p>y = Xβ + u (10.3)
</p>
<p>where y&prime; = (y&prime;1, y
&prime;
2) and X and u are obtained similarly from (10.2). y and u are 2T &times; 1, X is
</p>
<p>2T &times; (K1 +K2) and β is (K1 +K2)&times; 1. The stacked disturbances have a variance-covariance
matrix
</p>
<p>Ω =
</p>
<p>[
σ11IT σ12IT
σ21IT σ22IT
</p>
<p>]
= Σ&otimes; IT (10.4)
</p>
<p>where Σ = [σij ] for i, j = 1, 2; with ρ = σ12/
&radic;
σ11σ22 measuring the extent of correlation
</p>
<p>between the two regression equations. The Kronecker product operator &otimes; is defined in the Ap-
pendix to Chapter 7. Some important applications of SUR models in economics include the
estimation of a system of demand equations or a translog cost function along with its share
equations, see Berndt (1991). Briefly, a system of demand equations explains household con-
sumption of several commodities. The correlation among equations could be due to unobservable
household specific attributes that influence the consumption of these commodities. Similarly, in
estimating a cost equation along with the corresponding input share equations based on firm
level data. The correlation among equations could be due to unobservable firm-specific effects
that influence input choice and cost in production decisions.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_10, 241</p>
<p/>
</div>
<div class="page"><p/>
<p>242 Chapter 10: Seemingly Unrelated Regressions
</p>
<p>Problem 1 asks the reader to verify that OLS on the system of two equations in (10.2) yields
the same estimates as OLS on each equation in (10.1) taken separately. If ρ is large we expect
gain in efficiency in performing GLS rather than OLS on (10.3). In this case
</p>
<p>β̂GLS = (X
&prime;Ω&minus;1X)&minus;1X &prime;Ω&minus;1y (10.5)
</p>
<p>where Ω&minus;1 = Σ&minus;1 &otimes; IT . GLS will be BLUE for the system of two equations estimated jointly.
Note that we only need to invert Σ to obtain Ω&minus;1. Σ is of dimension 2 &times; 2 whereas, Ω is of
dimension 2T &times; 2T . In fact, if we denote by Σ&minus;1 = [σij ], then
</p>
<p>β̂GLS =
</p>
<p>[
σ11X &prime;1X1 σ
</p>
<p>12X &prime;1X2
σ21X &prime;2X1 σ
</p>
<p>22X &prime;2X2
</p>
<p>]&minus;1 [
σ11X &prime;1y1 + σ
</p>
<p>12X &prime;1y2
σ21X &prime;2y1 + σ
</p>
<p>22X &prime;2y2
</p>
<p>]
(10.6)
</p>
<p>Zellner (1962) gave two sufficient conditions where it does not pay to perform GLS, i.e., GLS
on this system of equations turns out to be OLS on each equation separately. These are the
following:
</p>
<p>Case 1: Zero correlation among the disturbances of the i-th and j-th equations, i.e., σij = 0 for
i 	= j. This means that Σ is diagonal which in turn implies that Σ&minus;1 is diagonal with σii = 1/σii
for i = 1, 2, and σij = 0 for i 	= j. Therefore, (10.6) reduces to
</p>
<p>β̂GLS =
</p>
<p>[
σ11(X
</p>
<p>&prime;
1X1)
</p>
<p>&minus;1 0
0 σ22(X
</p>
<p>&prime;
2X2)
</p>
<p>&minus;1
</p>
<p>] [
X &prime;1y1/σ11
X &prime;2y2/σ22
</p>
<p>]
=
</p>
<p>[
β̂1,OLS
β̂2,OLS
</p>
<p>]
(10.7)
</p>
<p>Case 2: Same regressors across all equations. This means that all the Xi&rsquo;s are the same, i.e.,
</p>
<p>X1 = X2 = X
&lowast;. This rules out different number of regressors in each equation and all the Xi&rsquo;s
</p>
<p>must have the same dimension, i.e., K1 = K2 = K. Hence, X = I2 &otimes;X&lowast; and (10.6) reduces to
β̂GLS = [(I2 &otimes;X&lowast;&prime;)(Σ&minus;1 &otimes; IT )(I2 &otimes;X&lowast;)]&minus;1[(I2 &otimes;X&lowast;&prime;)(Σ&minus;1 &otimes; IT )y] (10.8)
</p>
<p>= [Σ&otimes; (X&lowast;&prime;X&lowast;)&minus;1][(Σ&minus;1 &otimes;X&lowast;&prime;)y] = [I2 &otimes; (X&lowast;&prime;X&lowast;)&minus;1X&lowast;&prime;]y = β̂OLS
These results generalize to the case of M regression equations, but for simplicity of exposition
we considered the case of two equations only.
A necessary and sufficient condition for SUR(GLS) to be equivalent to OLS, was derived by
</p>
<p>Dwivedi and Srivastava (1978). An alternative derivation based on the Milliken and Albohali
(1984) necessary and sufficient condition for OLS to be equivalent to GLS, is presented here,
see Baltagi (1988). In Chapter 9, we saw that GLS is equivalent to OLS, for every y, if and only
if
</p>
<p>X &prime;Ω&minus;1P̄X = 0 (10.9)
</p>
<p>In this case, X = diag[Xi], Ω
&minus;1 = Σ&minus;1 &otimes; IT , and P̄X = diag[P̄Xi ]. Hence, the typical element
</p>
<p>of (10.9), see problem 1, is
</p>
<p>σijX &prime;iP̄Xj = 0 (10.10)
</p>
<p>This is automatically satisfied for i = j. For i 	= j, this holds if σij = 0 or X &prime;iP̄Xj = 0. Note
that σij = 0 is the first sufficient condition provided by Zellner (1962). The latter condition
X &prime;iP̄Xj = 0 implies that the set of regressors in the i-th equation are a perfect linear combination</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Feasible GLS Estimation 243
</p>
<p>of those in the j-th equation. Since X &prime;jP̄Xi = 0 has to hold also, Xj has to be a perfect linear
combination of the regressors in the i-th equation. Xi and Xj span the same space. Both Xi and
Xj have full column rank for OLS to be feasible, hence they have to be of the same dimension
for X &prime;iP̄Xj = X
</p>
<p>&prime;
jP̄Xi = 0. In this case, X
</p>
<p>&prime;
i = CX
</p>
<p>&prime;
j , where C is a nonsingular matrix, i.e., the
</p>
<p>regressors in the i-th equation are a perfect linear combination of those in the j-th equation.
This includes the second sufficient condition derived by Zellner (1962). In practice, different
economic behavioral equations contain different number of right hand side variables. In this
case, one rearranges the SUR into blocks where each block has the same number of right hand
side variables. For two equations (i and j) belonging to two different blocks (i 	= j), (10.10)
is satisfied if the corresponding σij is zero, i.e., Σ has to be block diagonal. However, in this
case, GLS performed on the whole system is equivalent to GLS performed on each block taken
separately. Hence, (10.10) is satisfied for SUR if it is satisfied for each block taken separately.
Revankar (1974) considered the case whereX2 is a subset ofX1. In this case, there is no gain in
</p>
<p>using SUR for estimating β2. In fact, problem 2 asks the reader to verify that β̂2,SUR = β̂2,OLS .
</p>
<p>However, this is not the case for β1. It is easy to show that β̂1,SUR = β̂1,OLS &minus;Ae2,OLS , where
A is a matrix defined in problem 2, and e2,OLS are the OLS residuals for the second equation.
</p>
<p>Telser (1964) suggested an iterative least squares procedure for SUR equations. For the two
equations model given in (10.1), this estimation method involves the following:
</p>
<p>1. Compute the OLS residuals e1 and e2 from both equations.
</p>
<p>2. Include e1 as an extra regressor in the second equation and e2 as an extra regressor
in the first equation. Compute the new least squares residuals and iterate this step until
convergence of the estimated coefficients. The resulting estimator has the same asymptotic
distribution as Zellner&rsquo;s (1962) SUR estimator.
</p>
<p>Conniffe (1982) suggests stopping at the second step because in small samples this provides most
of the improvement in precision. In fact, Conniffe (1982) argues that it may be unnecessary and
even disadvantageous to calculate Zellner&rsquo;s estimator proper. Extensions to multiple equations
is simple. Step 1 is the same where one computes least squares residuals of every equation. Step
2 adds the residuals of all other equations in the equation of interest. OLS is run and the new
residuals are computed. One can stop at this second step or iterate until convergence.
</p>
<p>10.2 Feasible GLS Estimation
</p>
<p>In practice, Σ is not known and has to be estimated. Zellner (1962) recommended the following
feasible GLS estimation procedure:
</p>
<p>sii =
&sum;T
</p>
<p>t=1 e
2
it/(T &minus;Ki) for i = 1, 2 (10.11)
</p>
<p>and
</p>
<p>sij =
&sum;T
</p>
<p>t=1 eitejt/(T &minus;Ki)1/2(T &minus;Kj)1/2 for i, j = 1, 2 and i 	= j (10.12)
where eit denotes OLS residuals of the i-th equation. sii is the s
</p>
<p>2 of the regression for the i-th
equation. This is unbiased for σii. However, sij for i 	= j is not unbiased for σij . In fact, the
unbiased estimate is
</p>
<p>s̃ij =
&sum;T
</p>
<p>t=1 eitejt/[T &minus;Ki &minus;Kj + tr (B)] for i, j = 1, 2 (10.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>244 Chapter 10: Seemingly Unrelated Regressions
</p>
<p>where B = Xi(X
&prime;
iXi)
</p>
<p>&minus;1X &prime;iXj(X
&prime;
jXj)
</p>
<p>&minus;1X &prime;j = PXiPXj , see problem 4. Using this last estimator
may lead to a variance-covariance matrix that is not positive definite. For consistency, however,
all we need is a division by T , however this leaves us with a biased estimator:
</p>
<p>ŝij =
&sum;T
</p>
<p>t=1 eitejt/T for i, j = 1, 2 (10.14)
</p>
<p>Using this consistent estimator of Σ will result in feasible GLS estimates that are asymptoti-
cally efficient. In fact, if one iterates this procedure, i.e., compute feasible GLS residuals and
second round estimates of Σ using these GLS residuals in (10.14), and continue iterating, until
convergence, this will lead to maximum likelihood estimates of the regression coefficients, see
Oberhofer and Kmenta (1974).
</p>
<p>Relative Efficiency of OLS in the Case of Simple Regressions
</p>
<p>To illustrate the gain in efficiency of Zellner&rsquo;s SUR compared to performing OLS on each
equation separately, Kmenta(1986, pp. 641&ndash;643) considers the following two simple regression
equations:
</p>
<p>Y1t = β11 + β12X1t + u1t (10.15)
</p>
<p>Y2t = β21 + β22X2t + u2t for t = 1, 2, . . . T ;
</p>
<p>and proves that
</p>
<p>var(β̂12,GLS)/var(β̂12,OLS) = (1&minus; ρ2)/[1&minus; ρ2r2] (10.16)
</p>
<p>where ρ is the correlation coefficient between u1 and u2, and r is the sample correlation coefficient
between X1 and X2. Problem 5 asks the reader to verify (10.16). In fact, the same relative
efficiency ratio holds for β22, i.e., var(β̂22,GLS)/var(β̂22,OLS) is given by that in (10.16). This
confirms the two results obtained above, namely, that as ρ increases this relative efficiency ratio
decreases and OLS is less efficient than GLS. Also, as r increases this relative efficiency ratio
increases and there is less gain in performing GLS rather than OLS. For ρ = 0 or r = 1, the
efficiency ratio is 1, and OLS is equivalent to GLS. However, if ρ is large, say 0.9 and r is small,
say 0.1 then (10.16) gives a relative efficiency of 0.11. For a tabulation of (10.16) for various
values of ρ2 and r2, see Table 12-1 of Kmenta (1986, p. 642).
</p>
<p>Relative Efficiency of OLS in the Case of Multiple Regressions
</p>
<p>With more regressors in each equation, the relative efficiency story has to be modified, as
indicated by Binkley and Nelson (1988). In the two equation model considered in (10.2) with
K1 regressors X1 in the first equation and K2 regressors X2 in the second equation
</p>
<p>var(β̂GLS) = (X
&prime;Ω&minus;1X)&minus;1 =
</p>
<p>[
σ11X &prime;1X1 σ
</p>
<p>12X &prime;1X2
σ21X &prime;2X1 σ
</p>
<p>22X &prime;2X2
</p>
<p>]&minus;1
= [Aij ] (10.17)
</p>
<p>If we focus on the regression estimates of the first equation, we get var(β̂1,GLS) = A11 =
[σ11X &prime;1X1 &minus; σ12X &prime;1X2(σ22X &prime;2X2)&minus;1σ21X &prime;2X1]&minus;1 see problem 6. Using the fact that
</p>
<p>Σ&minus;1 = [1/(1&minus; ρ2)]
[
</p>
<p>1/σ11 &minus;ρ2/σ12
&minus;ρ2/σ21 1/σ22
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Feasible GLS Estimation 245
</p>
<p>where ρ2 = σ212/σ11σ22, one gets
</p>
<p>var(β̂1,GLS) = [σ11(1&minus; ρ2)]{X &prime;1X1 &minus; ρ2(X &prime;1PX2X1)}&minus;1 (10.18)
</p>
<p>Add and subtract ρ2X &prime;1X1 from the expression to be inverted, one gets
</p>
<p>var(β̂1,GLS) = σ11{X &prime;1X1 + [ρ2/(1&minus; ρ2)]E&prime;E}&minus;1 (10.19)
</p>
<p>where E = P̄X2X1 is the matrix whose columns are the OLS residuals of each variable in
X1 regressed on X2. If E = 0, there is no gain in SUR over OLS for the estimation of β1.
X1 = X2 or X1 is a subset of X2 are two such cases. One can easily verify that (10.19) is the
variance-covariance matrix of an OLS regression with regressor matrix
</p>
<p>W =
</p>
<p>[
X1
θE
</p>
<p>]
</p>
<p>where θ2 = ρ2/(1 &minus; ρ2). Now let us focus on the efficiency of the estimated coefficient of the
q-th variable, Xq in X1. Recall, from Chapter 4, that for the regression of y on X1
</p>
<p>var(β̂q,OLS) = σ11/
{&sum;T
</p>
<p>t=1 x
2
tq(1&minus;R2q)
</p>
<p>}
(10.20)
</p>
<p>where the denominator is the residual sum of squares of Xq on the other (K1 &minus; 1) regressors in
X1 and R
</p>
<p>2
q is the corresponding R
</p>
<p>2 of that regression. Similarly, from (10.19),
</p>
<p>var(β̂q,SUR) = σ11/
{&sum;T
</p>
<p>t=1 x
2
tq + θ
</p>
<p>2&sum;T
t=1 e
</p>
<p>2
tq
</p>
<p>}
(1&minus;R&lowast;2q ) (10.21)
</p>
<p>where the denominator is the residual sum of squares of
</p>
<p>[
Xq
θeq
</p>
<p>]
on the other (K1&minus;1) regressors
</p>
<p>in W , and R&lowast;2q is the corresponding R
2 of that regression. Add and subtract
</p>
<p>&sum;T
t=1 x
</p>
<p>2
tq(1&minus;R2q)
</p>
<p>to the denominator of (10.21), we get
</p>
<p>var(β̂q,SUR) =
σ11{&sum;T
</p>
<p>t=1 x
2
tq(1&minus;R2q) +
</p>
<p>&sum;T
t=1 x
</p>
<p>2
tq(R
</p>
<p>2
q &minus;R&lowast;2q ) + θ2
</p>
<p>&sum;T
t=1 e
</p>
<p>2
tq(1&minus;R&lowast;2q )
</p>
<p>} (10.22)
</p>
<p>This variance differs from var(β̂q,OLS) in (10.20) by the two extra terms in the denominator. If
</p>
<p>ρ = 0, then θ2 = 0, so that W &prime; = [X &prime;1, 0] and R
2
q = R
</p>
<p>&lowast;2
q . In this case, (10.22) reduces to (10.20).
</p>
<p>If Xq also appears in the second equation, or in general is spanned by the variables in X2, then
</p>
<p>etq = 0,
&sum;T
</p>
<p>t=1 e
2
tq = 0 and from (10.22) there is gain in efficiency only if R
</p>
<p>2
q &ge; R&lowast;2q . R2q is a
</p>
<p>measure of multicollinearity ofXq with the other (K1&minus;1) regressors in the first equation, i.e.,X1.
If this is high, then it is more likely for R2q &ge; R&lowast;2q . Therefore, the higher the multicollinearity
within X1, the greater the potential for a decrease in variance of OLS by SUR. Note that
R2q = R
</p>
<p>&lowast;2
q when θE = 0. This is true if θ = 0, or E = 0. The latter occurs when X1 is
</p>
<p>spanned by the sub-space of X2. Problem 7 asks the reader to verify that R
2
q = R
</p>
<p>&lowast;2
q when X1
</p>
<p>is orthogonal to X2. Therefore, with more regressors in each equation, one has to consider the
correlation between the X&rsquo;s within each equation as well as that across equations. Even when
the X&rsquo;s across equations are highly correlated, there may still be gains from joint estimation
using SUR when there is high mulicollinearity within each equation.</p>
<p/>
</div>
<div class="page"><p/>
<p>246 Chapter 10: Seemingly Unrelated Regressions
</p>
<p>10.3 Testing Diagonality of the Variance-Covariance Matrix
</p>
<p>Since the diagonality of Σ is at the heart of using SUR estimation methods, it is important
to look at tests for H0: Σ is diagonal. Breusch and Pagan (1980) derived a simple and easy
to use Lagrange multiplier statistic for testing H0. This is based upon the sample correlation
coefficients of the OLS residuals:
</p>
<p>LM = T
&sum;M
</p>
<p>i=2
</p>
<p>&sum;i&minus;1
j=1 r
</p>
<p>2
ij (10.23)
</p>
<p>where M denotes the number of equations and rij = ŝij/(ŝiiŝjj)
1/2. The ŝij &rsquo;s are computed
</p>
<p>from OLS residuals as in (10.14). Under the null hypothesis, λLM has an asymptotic χ
2
M(M&minus;1)/2
</p>
<p>distribution. Note that the ŝij &rsquo;s are needed for feasible GLS estimation. Therefore, it is easy
to compute the rij &rsquo;s and λLM by summing the squares of half the number of off-diagonal
elements of R = [rij ] and multiplying the sum by T . For example, for the two equations case,
λLM = Tr
</p>
<p>2
21 which is asymptotically distributed as χ
</p>
<p>2
1 under H0. For the three equations case,
</p>
<p>λLM = T (r
2
21 + r
</p>
<p>2
31 + r
</p>
<p>2
32) which is asymptotically distributed as χ
</p>
<p>2
3 under H0.
</p>
<p>Alternatively, the Likelihood Ratio test can also be used to test for diagonality of Σ. This
is based on the determinants of the variance covariance matrices estimated by MLE for the
restricted and unrestricted models:
</p>
<p>λLR = T
(&sum;M
</p>
<p>i=1 logŝii &minus; log|Σ̂|
)
</p>
<p>(10.24)
</p>
<p>where ŝii is the restricted MLE of σii obtained from the OLS residuals as in (10.14). The matrix
Σ̂ denotes the unrestricted MLE of Σ. This may be adequately approximated with an estimator
based on the feasible GLS estimator β̂FGLS , see Judge et al. (1982). Under H0, λLR has an
asymptotic χ2M(M&minus;1)/2 distribution.
</p>
<p>10.4 Seemingly Unrelated Regressions with Unequal Observations
</p>
<p>Srivastava and Dwivedi (1979) surveyed the developments in the SUR model and described
the extensions of this model to the serially correlated case, the nonlinear case, the misspecified
case, and that with unequal number of observations. Srivastava and Giles (1988) dedicated a
monograph to SUR models, and surveyed the finite sample as well as asymptotic results. More
recently, Fiebig (2001) gives a concise and up to date account of research in this area. In this
section, we consider one extension to focus upon. This is the case of SUR with unequal number
of observations considered by Schmidt (1977), Baltagi, Garvin and Kerman (1989) and Hwang
(1990).
Let the first firm have T observations common with the second firm, but allow the latter to
</p>
<p>have N extra observations. In this case, (10.2) will have y1 of dimension T &times; 1 whereas y2 will
be of dimension (T + N) &times; 1. In fact, y&prime;2 = (y&lowast;&prime;2 , yo&prime;2 ) and X &prime;2 = (X&lowast;&prime;2 , Xo&prime;2 ) with &lowast; denoting the
T common observations for the second firm, and o denoting the extra N observations for the
second firm. The disturbances will now have a variance-covariance matrix
</p>
<p>Ω =
</p>
<p>⎡
⎣
</p>
<p>σ11IT σ12IT 0
σ12IT σ22IT 0
0 0 σ22IN
</p>
<p>⎤
⎦ (10.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Seemingly Unrelated Regressions with Unequal Observations 247
</p>
<p>GLS on (10.2) will give
</p>
<p>β̂GLS =
</p>
<p>[
σ11X &prime;1X1 σ
</p>
<p>12X &prime;1X
&lowast;
2
</p>
<p>σ12X&lowast;&prime;2 X1 σ
22X&lowast;&prime;2 X
</p>
<p>&lowast;
2 + (X
</p>
<p>o&prime;
2 X
</p>
<p>o
2)/σ22)
</p>
<p>]&minus;1
</p>
<p>(10.26)[
σ11X &prime;1y1 + σ
</p>
<p>12X &prime;1y
&lowast;
2
</p>
<p>σ12X&lowast;&prime;2 y1 + σ
22X&lowast;&prime;2 y
</p>
<p>&lowast;
2 + (X
</p>
<p>o&prime;
2 y
</p>
<p>o
2)/σ22)
</p>
<p>]
</p>
<p>where Σ&minus;1 = [σij ] for i, j = 1, 2. If we run OLS on each equation (T for the first equation, and
T + N for the second equation) and denote the residuals for the two equations by e1 and e2,
respectively, then we can partition the latter residuals into e&prime;2 = (e
</p>
<p>&lowast;&prime;
2 , e
</p>
<p>o&prime;
2 ). In order to estimate
</p>
<p>Ω, Schmidt (1977) considers the following procedures:
</p>
<p>(1) Ignore the extra N observations in estimating Ω. In this case
</p>
<p>σ̂11 = s11 = e
&prime;
1e1/T ; σ̂12 = s12 = e
</p>
<p>&prime;
1e
</p>
<p>&lowast;
2/T and σ̂22 = s
</p>
<p>&lowast;
22 = e
</p>
<p>&lowast;&prime;
2 e
</p>
<p>&lowast;
2/T (10.27)
</p>
<p>(2) Use T + N observations to estimate σ22. In other words, use s11, s12 and σ̂22 = s22 =
e&prime;2e2/(T +N). This procedure is attributed to Wilks (1932) and has the disadvantage of
giving estimates of Ω that are not positive definite.
</p>
<p>(3) Use s11 and s22, but modify the estimate of σ12 such that Ω̂ is positive definite. Srivastava
and Zaatar (1973) suggest σ̂12 = s12(s22/s
</p>
<p>&lowast;
22)
</p>
<p>1/2.
</p>
<p>(4) Use all (T + N) observations in estimating Ω. Hocking and Smith (1968) suggest using
σ̂11 = s11 &minus; (N/N + T )(s12/s&lowast;22)2(s&lowast;22 &minus; so22) where so22 = eo&prime;2 eo2/N ; σ̂12 = s12(s22/s&lowast;22) and
σ̂22 = s22.
</p>
<p>(5) Use a maximum likelihood procedure.
</p>
<p>All estimators of Ω are consistent, and β̂FGLS based on any of these estimators will be asymp-
totically efficient. Schmidt considers their small sample properties by means of Monte Carlo
experiments. Using the set up of Kmenta and Gilbert (1968) he finds for T = 10, 20, 50 and
N = 5, 10, 20 and various correlation of the X&rsquo;s and the disturbances across equations the fol-
lowing disconcerting result: &ldquo;..it is certainly remarkable that procedures that essentially ignore
the extra observations in estimating Σ (e.g., Procedure 1) do not generally do badly relative to
procedures that use the extra observations fully (e.g., Procedure 4 or MLE). Except when the
disturbances are highly correlated across equations, we may as well just forget about the extra
observations in estimating Σ. This is not an intuitively reasonable procedure.&rdquo;
Hwang (1990) re-parametrizes these estimators in terms of the elements of Σ&minus;1 rather than Σ.
</p>
<p>After all, it is Σ&minus;1 rather than Σ that appears in the GLS estimator of β. This re-parametrization
shows that the estimators of Σ&minus;1 no longer have the ordering in terms of their use of the extra
observations as that reported by Schmidt (1977). However, regardless of the parametrization
chosen, it is important to point out that all the observations are used in the estimation of
β whether at the first stage for obtaining the least squares residuals, or in the final stage in
computing GLS. Baltagi et al. (1989) show using Monte Carlo experiments that better estimates
of Σ or its inverse Σ&minus;1 in Mean Square Error sense, do not necessarily lead to better GLS
estimates of β.</p>
<p/>
</div>
<div class="page"><p/>
<p>248 Chapter 10: Seemingly Unrelated Regressions
</p>
<p>10.5 Empirical Examples
</p>
<p>Example 1: Baltagi and Griffin (1983) considered the following gasoline demand equation:
</p>
<p>log
Gas
</p>
<p>Car
= α+ β1log
</p>
<p>Y
</p>
<p>N
+ β2log
</p>
<p>PMG
PGDP
</p>
<p>+ β3log
Car
</p>
<p>N
+ u
</p>
<p>where Gas/Car is motor gasoline consumption per auto, Y/N is real per capita income,
PMG/PGDP is real motor gasoline price and Car/N denotes the stock of cars per capita.
This data consists of annual observations across 18 OECD counties, covering the period 1960&ndash;
1978. It is provided as GASOLINE.DAT on the Springer web site. We consider the first two
countries: Austria and Belgium. OLS on this data yields
</p>
<p>Austria log
Gas
</p>
<p>Car
= 3.727
</p>
<p>(0.373)
</p>
<p>+ 0.761
</p>
<p>(0.211)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.793
</p>
<p>(0.150)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.520
(0.113)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>Belgium log
Gas
</p>
<p>Car
= 3.042
</p>
<p>(0.453)
</p>
<p>+ 0.845
</p>
<p>(0.170)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.042
</p>
<p>(0.158)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.673
(0.093)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>where the standard errors are shown in parentheses. Based on these OLS residuals, the estimate
of Σ is given by
</p>
<p>Σ̂ =
</p>
<p>[
0.0012128 0.00023625
</p>
<p>0.00092367
</p>
<p>]
</p>
<p>The Seemingly Unrelated Regression estimates based on this Σ̂, i.e., after one iteration, are
given by
</p>
<p>Austria log
Gas
</p>
<p>Car
= 3.713
</p>
<p>(0.372)
</p>
<p>+ 0.721
</p>
<p>(0.209)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.754
</p>
<p>(0.146)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.496
(0.111)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>Belgium log
Gas
</p>
<p>Car
= 2.843
</p>
<p>(0.445)
</p>
<p>+ 0.835
</p>
<p>(0.170)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.131
</p>
<p>(0.154)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.686
(0.093)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>The Breusch-Pagan (1980) Lagrange multiplier test for diagonality of Σ is Tr221 = 0.947 which is
distributed as χ21 under the null hypothesis. The Likelihood Ratio test for the diagonality of Σ,
given in (10.23), yields a value of 1.778 which is also distributed as χ21 under the null hypothesis.
Both test statistics do not reject H0. These SUR results were run using SHAZAM and could be
iterated further. Note the reduction in the standard errors of the estimated regression coefficients
is minor as we compare the OLS and SUR estimates.
Suppose that we only have the first 15 observations (1960&ndash;1974) on Austria and all 19 ob-
</p>
<p>servations (1960&ndash;1978) on Belgium. We now apply the four feasible GLS procedures described
by Schmidt (1977). The first procedure which ignores the extra 4 observations in estimating Σ
yields s11 = 0.00086791, s12 = 0.00026357 and s
</p>
<p>&lowast;
22 = 0.00109947 as described in (10.27). The</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Empirical Examples 249
</p>
<p>resulting SUR estimates are given by
</p>
<p>Austria log
Gas
</p>
<p>Car
= 4.484
</p>
<p>(0.438)
</p>
<p>+ 0.817
</p>
<p>(0.168)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.580
</p>
<p>(0.176)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.487
(0.098)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>Belgium log
Gas
</p>
<p>Car
= 2.936
</p>
<p>(0.436)
</p>
<p>+ 0.848
</p>
<p>(0.164)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.095
</p>
<p>(0.151)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.686
(0.090)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>The second procedure, due to Wilks (1932) uses the same s11 and s12 in procedure 1, but
σ̂22 = s22 = e
</p>
<p>&prime;
2e2/19 = 0.00092367. The resulting SUR estimates are given by
</p>
<p>Austria log
Gas
</p>
<p>Car
= 4.521
</p>
<p>(0.437)
</p>
<p>+ 0.806
</p>
<p>(0.167)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.554
</p>
<p>(0.174)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.476
(0.098)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>Belgium log
Gas
</p>
<p>Car
= 2.937
</p>
<p>(0.399)
</p>
<p>+ 0.848
</p>
<p>(0.150)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.094
</p>
<p>(0.138)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.685
(0.082)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>The third procedure based on Srivastava and Zaatar (1973) use the same s11 and s22 as proce-
dure 2, but modify σ̂12 = s12(s22/s
</p>
<p>&lowast;
22)
</p>
<p>1/2 = 0.00024158. The resulting SUR estimates are given
by
</p>
<p>Austria log
Gas
</p>
<p>Car
= 4.503
</p>
<p>(0.438)
</p>
<p>+ 0.812
</p>
<p>(0.168)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.567
</p>
<p>(0.176)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.481
(0.098)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>Belgium log
Gas
</p>
<p>Car
= 2.946
</p>
<p>(0.400)
</p>
<p>+ 0.847
</p>
<p>(0.151)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.090
</p>
<p>(0.139)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.684
(0.082)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>The fourth procedure due to Hocking and Smith (1968) yields σ̂11 = 0.00085780, σ̂12 =
0.0022143 and σ̂22 = s22 = 0.00092367. The resulting SUR estimates are given by
</p>
<p>Austria log
Gas
</p>
<p>Car
= 4.485
</p>
<p>(0.437)
</p>
<p>+ 0.817
</p>
<p>(0.168)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.579
</p>
<p>(0.176)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.487
(0.098)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>Belgium log
Gas
</p>
<p>Car
= 2.952
</p>
<p>(0.400)
</p>
<p>+ 0.847
</p>
<p>(0.151)
</p>
<p>log
Y
</p>
<p>N
&minus; 0.086
</p>
<p>(0.139)
</p>
<p>log
PMG
PGDP
</p>
<p>&minus; 0.684
(0.082)
</p>
<p>log
Car
</p>
<p>N
</p>
<p>In this case, there is not much difference among these four alternative estimates.
</p>
<p>Example 2: Growth and Inequality. Lundberg and Squire (2003) estimate a two equation model
of growth and inequality using SUR. The first equation relates Growth (dly) to education (adult
years schooling: yrt), the share of government consumption in GDP (gov), M2/GDP (m2y),
Inflation (inf), Sachs-Warner measure of openness (swo), changes in the terms of trade (dtot),
initial income (f pcy), dummy for 1980s (d80) and dummy for 1990s (d90). The second equation
relates the Gini coefficient (gih) to education, M2/GDP, civil liberties index (civ), mean land
Gini (mlg), mean land Gini interacted with a dummy for developing countries (mlgldc). The
data contains 119 observations for 38 countries over the period 1965-1990, and can be obtained
from http://www.res.org.uk/economic/datasets/datasetlist.asp.</p>
<p/>
</div>
<div class="page"><p/>
<p>250 Chapter 10: Seemingly Unrelated Regressions
</p>
<p>Table 10.1 Growth and Inequality: SUR Estimates
</p>
<p>. sureg (Growth: dly = yrt gov m2y inf swo dtot f pcy d80 d90)
</p>
<p>(Inequality: gih = yrt m2y civ mlg mlgldc), corr
</p>
<p>Seemingly unrelated regression
</p>
<p>Equation Obs Parms RMSE &ldquo;R-sq&rdquo; chi2 P
</p>
<p>Growth 119 9 2.313764 0.4047 80.36 0.0000
</p>
<p>Inequality 119 5 6.878804 0.4612 102.58 0.0000
</p>
<p>Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
</p>
<p>Growth
</p>
<p>yrt &ndash;.0497042 .1546178 &ndash;0.32 0.748 &ndash;.3527496 .2533412
</p>
<p>gov &ndash;.0345058 .0354801 &ndash;0.97 0.331 &ndash;.1040455 .0350338
</p>
<p>m2y .0084999 .0163819 0.52 0.604 &ndash;.023608 .0406078
</p>
<p>inf &ndash;.0020648 .0013269 &ndash;1.56 0.120 &ndash;.0046655 .000536
</p>
<p>swo 3.263209 .60405 5.40 0.000 2.079292 4.447125
</p>
<p>dtot 17.74543 21.9798 0.81 0.419 &ndash;25.33419 60.82505
</p>
<p>f pcy &ndash;1.038173 .4884378 &ndash;2.13 0.034 &ndash;1.995494 &ndash;.0808529
</p>
<p>d80 &ndash;1.615472 .5090782 &ndash;3.17 0.002 &ndash;2.613247 &ndash;.6176976
</p>
<p>d90 &ndash;3.339514 .6063639 &ndash;5.51 0.000 &ndash;4.527965 &ndash;2.151063
</p>
<p>cons 10.60415 3.471089 3.05 0.002 3.800944 17.40736
</p>
<p>Inequality
</p>
<p>yrt &ndash;1.000843 .3696902 &ndash;2.71 0.007 &ndash;1.725422 &ndash;.2762635
</p>
<p>m2y &ndash;.0570365 .0471514 &ndash;1.21 0.226 &ndash;.1494516 .0353785
</p>
<p>civ .0348434 .5533733 0.06 0.950 &ndash;1.049748 1.119435
</p>
<p>mlg .1684692 .0625023 2.70 0.007 .0459669 .2909715
</p>
<p>mlgldc .0344093 .0421904 0.82 0.415 &ndash;.0482823 .117101
</p>
<p>cons 33.96115 4.471626 7.59 0.000 25.19693 42.72538
</p>
<p>Correlation matrix of residuals:
</p>
<p>Growth Inequality
</p>
<p>Growth 1.0000
</p>
<p>Inequality 0.0872 1.0000
</p>
<p>Breusch-Pagan test of independence: chi2(1) = 0.905, Pr = 0.3415.
</p>
<p>Table 10.1 gives the SUR estimates reported in Table 1 of Lundberg and Squire (2003, p.
334) using the sureg command in Stata. Among other things, these results show that openness
enhances growth and education reduces inequality. The correlation among the residuals of the
two equations is weak (0.0872) and the Breusch-Pagan test for diagonality of the variance-
covariance matrix of the disturbances of the two equations is statistically insignificant, not
rejecting zero correlation among the two equations.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 251
</p>
<p>Problems
</p>
<p>1. When Is OLS as Efficient as Zellner&rsquo;s SUR?
</p>
<p>(a) Show that OLS on a system of two Zellner&rsquo;s SUR equations given in (10.2) is the same as OLS
on each equation taken separately. What about the estimated variance-covariance matrix of
the coefficients? Will they be the same?
</p>
<p>(b) In the General Linear Model, we found a necessary and sufficient condition for OLS to be
equivalent to GLS is that X &prime;Ω&minus;1P̄X = 0 for every y where P̄X = I &minus;PX . Show that a neces-
sary and sufficient condition for Zellner&rsquo;s GLS to be equivalent to OLS is that σijX &prime;iP̄Xj = 0
for i 	= j as described in (10.10). This is based on Baltagi (1988).
</p>
<p>(c) Show that the two sufficient conditions given by Zellner for SUR to be equivalent to OLS
both satisfy the necessary and sufficient condition given in part (b).
</p>
<p>(d) Show that if Xi = XjC
&prime; where C is an arbitrary nonsingular matrix, then the necessary and
</p>
<p>sufficient condition given in part (b) is satisfied.
</p>
<p>2. What Happens to Zellner&rsquo;s SUR Estimator when the Set of Regressors in One Equation Are a
Subset of Those in the Second Equation? Consider the two SUR equations given in (10.2). Let
X1 = (X2, Xe), i.e., X2 is a subset of X1. Prove that
</p>
<p>(a) β̂2,SUR = β̂2,OLS .
</p>
<p>(b) β̂1,SUR = β̂1,OLS &minus; Ae2,OLS , where A = ŝ12(X &prime;1X1)&minus;1X &prime;1/ŝ22. e2,OLS are the OLS residuals
from the second equation, and the ŝij &rsquo;s are defined in (10.14).
</p>
<p>3. What Happens to Zellner&rsquo;s SUR Estimator when the Set of Regressors in One Equation Are Or-
thogonal to Those in the Second Equation? Consider the two SUR equations given in (10.2). Let
X1 and X2 be orthogonal, i.e., X
</p>
<p>&prime;
1X2 = 0. Show that knowing the true Σ we get
</p>
<p>(a) β̂1,GLS = β̂1,OLS + (σ
12/σ11)(X &prime;1X1)
</p>
<p>&minus;1X &prime;1y2 and β̂2,GLS = β̂2,OLS + (σ
21/σ22)(X &prime;2X2)
</p>
<p>&minus;1
</p>
<p>X &prime;2y1.
</p>
<p>(b) What are the variances of these estimates?
</p>
<p>(c) If X1 and X2 are single regressors, what are the relative efficiencies of β̂i,OLS with respect
</p>
<p>to β̂i,GLS for i = 1, 2?
</p>
<p>4. An Unbiased Estimate of σij . Verify that s̃ij , given in (10.13), is unbiased for σij . Note that for
computational purposes tr(B) = tr(PXiPXj ).
</p>
<p>5. Relative Efficiency of OLS in the Case of Simple Regressions. This is based on Kmenta (1986, pp.
641&ndash;643). For the system of two equations given in (10.15), show that
</p>
<p>(a) var(β̂12,OLS) = σ11/mx1x1 and var(β̂22,OLS) = σ22/mx2x2 where mxixj =
&sum;T
</p>
<p>t=1(Xit &minus;
X̄i)(Xjt &minus; X̄j) for i, j = 1, 2.
</p>
<p>(b) var
</p>
<p>(
β̂12,GLS
β̂22,GLS
</p>
<p>)
= (σ11σ22 &minus; σ212)
</p>
<p>[
σ22mx1x1 &minus;σ12mx1x2
&minus;σ12mx1x2 σ11mx2x2
</p>
<p>]&minus;1
.
</p>
<p>Deduce that var
(
β̂12,GLS
</p>
<p>)
= (σ11σ22 &minus; σ212)σ11mx2x2/[σ11σ22mx2x2mx1x1 &minus; σ212m2x1x2 ] and
</p>
<p>var
(
β̂22,GLS
</p>
<p>)
= (σ11σ22 &minus; σ212)σ22mx1x1/[σ11σ22mx1x1mx2x2 &minus; σ212m2x1x2 ].
</p>
<p>(c) Using ρ = σ12/(σ11σ22)
1/2 and r = mx1x2/(mx1x1mx2x2)
</p>
<p>1/2 and the results in parts (a) and
</p>
<p>(b), show that (10.16) holds, i.e., var(β̂12,GLS)/var(β̂12,OLS) = (1&minus; ρ2)/[1&minus; ρ2r2].</p>
<p/>
</div>
<div class="page"><p/>
<p>252 Chapter 10: Seemingly Unrelated Regressions
</p>
<p>(d) Differentiate (10.16) with respect to θ = ρ2 and show that (10.16) is a non-increasing function
of θ. Similarly, differentiate (10.16) with respect to λ = r2 and show that (10.16) is a non-
decreasing function of λ. Finally, compute this efficiency measure (10.16) for various values
of ρ2 and r2 between 0 and 1 at 0.1 intervals, see Kmenta&rsquo;s (1986) Table 12-1, p. 642.
</p>
<p>6. Relative Efficiency of OLS in the Case of Multiple Regressions. This is based on Binkley and
Nelson (1988). Using partitioned inverse formulas, verify that var(β̂1,GLS) = A11 given below
(10.17). Deduce (10.18) and (10.19).
</p>
<p>7. Consider the multiple regression case with orthogonal regressors across the two equations, i.e.,
X &prime;1X2 = 0. Verify that R
</p>
<p>2
q = R
</p>
<p>&lowast;2
q , where R
</p>
<p>2
q and R
</p>
<p>&lowast;2
q are defined below (10.20) and (10.21),
</p>
<p>respectively.
</p>
<p>8. (a) SUR With Unequal Number of Observations. This is based on Schmidt (1977). Derive the
GLS estimator for SUR with unequal number of observations given in (10.26).
</p>
<p>(b) Show that if σ12 = 0, SUR with unequal number of observations reduces to OLS on each
equation separately.
</p>
<p>9. Grunfeld (1958) considered the following investment equation:
</p>
<p>Iit = α+ β1Fit + β2Cit + uit
</p>
<p>where Iit denotes real gross investment for firm i in year t, Fit is the real value of the firm (shares
outstanding) and Cit is the real value of the capital stock. This data set consists of 10 large U.S.
manufacturing firms over 20 years, 1935&ndash;1954, and are given in Boot and de Witt (1960). It is
provided as GRUNFELD.DAT on the Springer web site. Consider the first three firms: G.M., U.S.
Steel and General Electric.
</p>
<p>(a) Run OLS of I on a constant, F and C for each of the 3 firms separately. Plot the residuals
against time. Print the variance-covariance of the estimates.
</p>
<p>(b) Test for serial correlation in each regression.
</p>
<p>(c) Run Seemingly Unrelated Regressions (SUR) for the first two firms. Compare with OLS.
</p>
<p>(d) Run SUR for the three assigned firms. Compare these results with those in part (c).
</p>
<p>(e) Test for the diagonality of Σ across these three equations.
</p>
<p>(f) Test for the equality of all coefficients across the 3 firms.
</p>
<p>10. (Continue Problem 9). Consider the first two firms again and focus on the coefficient of F . Refer
to the Binkley and Nelson (1988) article in The American Statistician, and compute R2q , R
</p>
<p>&lowast;2
q , Σe
</p>
<p>2
tq
</p>
<p>and Σx2tq.
</p>
<p>(a) What would be equations (10.20) and (10.21) for your data set?
</p>
<p>(b) Substitute estimates of σ11 and θ
2 and verify that the results are the same as those obtained
</p>
<p>in problems 9(a) and 9(c).
</p>
<p>(c) Compare the results from equations (10.20) and (10.21) in part (a). What do you conclude?
</p>
<p>11. (Continue Problem 9). Consider the first two firms once more. Now you only have the first 15
observations on the first firm and all 20 observations on the second firm. Apply Schmidt&rsquo;s (1977)
feasible GLS estimators and compare the resulting estimates.
</p>
<p>12. For the Baltagi and Griffin (1983) Gasoline Data considered in section 10.5, the model is
</p>
<p>log
Gas
</p>
<p>Car
= α+ β1log
</p>
<p>Y
</p>
<p>N
+ β2log
</p>
<p>PMG
PGDP
</p>
<p>+ β3log
Car
</p>
<p>N
+ u
</p>
<p>where Gas/Car is motor gasoline consumption per auto, Y/N is real per capita income, PMG/PGDP
is real motor gasoline price and Car/N denotes the stock of cars per capita.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 253
</p>
<p>(a) Run Seemingly Unrelated Regressions (SUR) for the first two countries. Compare with OLS.
</p>
<p>(b) Run SUR for the first three countries. Comment on the results and compare with those of
part (a). (Are there gains in efficiency?)
</p>
<p>(c) Test for Diagonality of Σ across the three equations using the Breusch and Pagan (1980) LM
test and the Likelihood Ratio test.
</p>
<p>(d) Test for the equality of all coefficients across the 3 countries.
</p>
<p>(e) Consider the first 2 countries once more. Now you only have the first 15 observations on the
first country and all 19 observations on the second country. Apply Schmidt&rsquo;s (1977) feasible
GLS estimators, and compare the results.
</p>
<p>13. Trace Minimization of Singular Systems with Cross-Equation Restrictions. This is based on Baltagi
(1993). Berndt and Savin (1975) demonstrated that when certain cross-equation restrictions are
imposed, restricted least squares estimation of a singular set of SUR equations will not be invariant
to which equation is deleted. Consider the following set of three equations with the same regressors:
</p>
<p>yi = αiιT + βiX + ǫi i = 1, 2, 3.
</p>
<p>where yi = (yi1, yi2, . . . , yiT )
&prime;, X = (x1, x2, . . . , xT )&prime;, and ǫi for (i = 1, 2, 3) are T &times; 1 vectors
</p>
<p>and ιT is a vector of ones of dimension T . αi and βi are scalars, and these equations satisfy the
adding up restriction
</p>
<p>&sum;3
i=1 yit = 1 for every t = 1, 2, . . . , T . Additionally, we have a cross-equation
</p>
<p>restriction: β1 = β2.
</p>
<p>(a) Denote the unrestricted OLS estimates of βi by bi where bi =
&sum;T
</p>
<p>t=1(xt&minus;x̄)yit/
&sum;T
</p>
<p>t=1(xt&minus;x̄)2
for i = 1, 2, 3, and x̄ =
</p>
<p>&sum;T
t=1 xt/T . Show that these unrestricted bi&rsquo;s satisfy the adding up
</p>
<p>restriction β1 + β2 + β3 = 0 on the true parameters automatically.
</p>
<p>(b) Show that if one drops the first equation for i = 1 and estimate the remaining system by
</p>
<p>trace minimization subject to β1 = β2, one gets β̂1 = 0.4b1 + 0.6b2.
</p>
<p>(c) Now drop the second equation for i = 2, and show that estimating the remaining system by
</p>
<p>trace minimization subject to β1 = β2, gives β̂1 = 0.6b1 + 0.4b2.
</p>
<p>(d) Finally, drop the third equation for i = 3, and show that estimating the remaining system
</p>
<p>by trace minimization subject to β1 = β2 gives β̂1 = 0.5b1 + 0.5b2.
</p>
<p>Note that this also means the variance of β̂1 is not invariant to the deleted equation. Also, this
non-invariancy affects Zellner&rsquo;s SUR estimation if the restricted least squares residuals are used
rather than the unrestricted least squares residuals in estimating the variance covariance matrix
of the disturbances. Hint: See the solution by Im (1994).
</p>
<p>14. For the Natural Gas data considered in Chapter 4, problem 16. The model is
</p>
<p>logConsit = β0 + β1logPgit + β2logPoit + β3logPeit + β4logHDDit
</p>
<p>+β5logPIit + uit
</p>
<p>where i = 1, 2, . . . , 6 states and t = 1, 2, . . . , 23 years.
</p>
<p>(a) Run Seemingly Unrelated Regressions (SUR) for the first two states. Compare with OLS.
</p>
<p>(b) Run SUR for all six states. Comment on the results and compare with those of part (a). (Are
there gains in efficiency?)
</p>
<p>(c) Test for Diagonality of Σ across the six states using the Breusch and Pagan (1980) LM test
and the Likelihood Ratio test.
</p>
<p>(d) Test for the equality of all coefficients across the six states.</p>
<p/>
</div>
<div class="page"><p/>
<p>254 Chapter 10: Seemingly Unrelated Regressions
</p>
<p>15. Equivalence of LR Test and Hausman Test. This is based on Qian (1998). Suppose that we have
the following two equations:
</p>
<p>ygt = αg + ugt g = 1, 2, t = 1, 2, . . . T
</p>
<p>where (u1t, u2t) is normally distributed with mean zero and variance Ω = Σ&otimes; IT where Σ = [σgs]
for g, s = 1, 2. This is a simple example of the same regressors across two equations.
</p>
<p>(a) Show that the OLS estimator of αg is the same as the GLS estimator of αg and both are
</p>
<p>equal to ȳg =
&sum;T
</p>
<p>t=1 ygt/T for g = 1, 2.
</p>
<p>(b) Derive the maximum likelihood estimators of αg and σgs for g, s,= 1, 2. Compute the log-
likelihood function evaluated at these unrestricted estimates.
</p>
<p>(c) Compute the maximum likelihood estimators of αg and σgs for g, s = 1, 2 under the null
hypothesis H0; σ11 = σ22.
</p>
<p>(d) Using parts (b) and (c) compute the LR test for H0; σ11 = σ22.
</p>
<p>(e) Show that the LR test for H0 derived in part (c) is asymptotically equivalent to the Hausman
test based on the difference in estimators obtained in parts (b) and (c). Hausman&rsquo;s test is
studied in Chapter 12.
</p>
<p>16. Estimation of a Triangular, Seemingly Unrelated Regression System by OLS. This is based on
Sentana (1997). Consider a system of three SUR equations in which the explanatory variables for
the first equation are a subset of the explanatory variables for the second equation, which are in
turn a subset of the explanatory variables for the third equation.
</p>
<p>(a) Show that SUR applied to the first two equations is the same (for those equations) as SUR
applied to all three equations. Hint: See Schmidt (1978).
</p>
<p>(b) Using part (a) show that SUR for the first equation is equivalent to OLS.
</p>
<p>(c) Using parts (a) and (b) show that SUR for the second equation is equivalent to OLS on the
second equation with one additional regressor. The extra regressor is the OLS residuals from
the first equation. Hint: Use Telser&rsquo;s (1964) results.
</p>
<p>(d) Using parts (a), (b) and (c) show that SUR for the third equation is equivalent to OLS on the
third equation with the residuals from the regressions in parts (b) and (c) as extra regressors.
</p>
<p>17. Growth and Inequality. Lundberg and Squire (2003). See example 2, section 10.5. The data con-
tains 119 observations for 38 countries over the period 1965&ndash;1990, and can be obtained from
http://www.res.org.uk/economic/datasets/datasetlist.asp.
</p>
<p>(a) Estimate these equations using SUR, see Table 10.1, and verify the results reported in Table
1 of Lundberg and Squire (2003, p. 334). These results show that openness enhances growth
and education reduces inequality.
</p>
<p>(b) Report the Breusch-Pagan test for diagonality of the variance-covariance matrix of the dis-
turbances of the two equations. Compare the SUR estimates in part (a) to OLS on each
equation separately.
</p>
<p>References
</p>
<p>This chapter is based on Zellner(1962), Kmenta(1986), Baltagi (1988), Binkley and Nelson (1988),
</p>
<p>Schmidt (1977) and Judge et al. (1982). References cited are:</p>
<p/>
</div>
<div class="page"><p/>
<p>References 255
</p>
<p>Baltagi, B.H. (1988), &ldquo;The Efficiency of OLS in a Seemingly Unrelated Regressions Model,&rdquo; Econometric
Theory, Problem 88.3.4, 4: 536&ndash;537.
</p>
<p>Baltagi, B.H. (1993), &ldquo;Trace Minimization of Singular Systems With Cross-Equation Restrictions,&rdquo;
Econometric Theory, Problem 93.2.4, 9: 314&ndash;315.
</p>
<p>Baltagi, B., S. Garvin and S. Kerman (1989), &ldquo;Further Monte Carlo Evidence on Seemingly Unrelated
Regressions with Unequal Number of Observations,&rdquo; Annales D &prime;Economie et de Statistique, 14:
103&ndash;115.
</p>
<p>Baltagi, B.H. and J.M. Griffin (1983), &ldquo;Gasoline Demand in the OECD: An Application of Pooling and
Testing Procedures,&rdquo; European Economic Review, 22: 117&ndash;137.
</p>
<p>Berndt, E.R. (1991), The Practice of Econometrics: Classic and Contemporary (Addison- Wesley: Read-
ing, MA).
</p>
<p>Berndt, E.R. and N.E. Savin (1975), &ldquo;Estimation and Hypothesis Testing in Singular Equation Systems
With Autoregressive Disturbances,&rdquo; Econometrica, 43: 937&ndash;957.
</p>
<p>Binkley, J.K. and C.H. Nelson (1988), &ldquo;A Note on the Efficiency of Seemingly Unrelated Regression,&rdquo;
The American Statistician, 42: 137&ndash;139.
</p>
<p>Boot, J. and G. de Witt (1960), &ldquo;Investment Demand: An Empirical Contribution to the Aggregation
Problem,&rdquo; International Economic Review, 1: 3&ndash;30.
</p>
<p>Breusch, T.S. and A.R. Pagan (1980), &ldquo;The Lagrange Multiplier Test and Its Applications to Model
Specification in Econometrics,&rdquo; Review of Economic Studies, 47: 239&ndash;253.
</p>
<p>Conniffe, D. (1982), &ldquo;A Note on Seemingly Unrelated Regressions,&rdquo; Econometrica, 50: 229&ndash;233.
</p>
<p>Dwivedi, T.D. and V.K. Srivastava (1978), &ldquo;Optimality of Least Squares in the Seemingly Unrelated
Regression Equations Model,&rdquo; Journal of Econometrics, 7: 391&ndash;395.
</p>
<p>Fiebig, D.G. (2001), &ldquo;Seemingly Unrelated Regression,&rdquo; Chapter 5 in Baltagi, B.H. (ed.), A Companion
to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Grunfeld, Y. (1958), &ldquo;The Determinants of Corporate Investment,&rdquo; unpublished Ph.D. dissertation (Uni-
versity of Chicago: Chicago, IL).
</p>
<p>Hocking, R.R. and W.B. Smith (1968), &ldquo;Estimation of Parameters in the Multivariate Normal Distribu-
tion with Missing Observations,&rdquo; Journal of the American Statistical Association, 63: 154&ndash;173.
</p>
<p>Hwang, H.S. (1990), &ldquo;Estimation of Linear SUR Model With Unequal Numbers of Observations,&rdquo; Review
of Economics and Statistics, 72: 510&ndash;515.
</p>
<p>Im, Eric Iksoon (1994), &ldquo;Trace Minimization of Singular Systems With Cross-Equation Restrictions,&rdquo;
Econometric Theory, Solution 93.2.4, 10: 450.
</p>
<p>Kmenta, J. and R. Gilbert (1968), &ldquo;Small Sample Properties of Alternative Estimators of Seemingly
Unrelated Regressions,&rdquo; Journal of the American Statistical Association, 63: 1180&ndash;1200.
</p>
<p>Lundberg, M. and L. Squire (2003), &ldquo;The Simultaneous Evolution of Growth and Inequality,&rdquo; The
Economic Journal, 113: 326&ndash;344.
</p>
<p>Milliken, G.A. and M. Albohali (1984), &ldquo;On Necessary and Sufficient Conditions for Ordinary Least
Squares Estimators to be Best Linear Unbiased Estimators,&rdquo; The American Statistician, 38: 298&ndash;
299.
</p>
<p>Oberhofer, W. and J. Kmenta (1974), &ldquo;A General Procedure for Obtaining Maximum Likelihood Esti-
mates in Generalized Regression Models,&rdquo; Econometrica, 42: 579&ndash;590.</p>
<p/>
</div>
<div class="page"><p/>
<p>256 Chapter 10: Seemingly Unrelated Regressions
</p>
<p>Qian, H. (1998), &ldquo;Equivalence of LR Test and Hausman Test,&rdquo; Econometric Theory, Problem 98.1.3, 14:
151.
</p>
<p>Revankar, N.S. (1974), &ldquo;Some Finite Sample Results in the Context of Two Seemingly Unrelated Re-
gression Equations,&rdquo; Journal of the American Statistical Association, 71: 183&ndash;188.
</p>
<p>Rossi, P.E. (1989), &ldquo;The ET Interview: Professor Arnold Zellner,&rdquo; Econometric Theory, 5: 287&ndash;317.
</p>
<p>Schmidt, P. (1977), &ldquo;Estimation of Seemingly Unrelated Regressions With Unequal Numbers of Obser-
vations,&rdquo; Journal of Econometrics, 5: 365&ndash;377.
</p>
<p>Schmidt, P. (1978), &ldquo;A Note on the Estimation of Seemingly Unrelated Regression Systems,&rdquo; Journal of
Econometrics, 7: 259&ndash;261.
</p>
<p>Sentana, E. (1997), &ldquo;Estimation of a Triangular, Seemingly Unrelated, Regression System by OLS,&rdquo;
Econometric Theory, Problem 97.2.2, 13: 463.
</p>
<p>Srivastava, V.K. and T.D. Dwivedi (1979), &ldquo;Estimation of Seemingly Unrelated Regression Equations:
A Brief Survey,&rdquo; Journal of Econometrics, 10: 15&ndash;32.
</p>
<p>Srivastava, V.K. and D.E.A. Giles (1987), Seemingly Unrelated Regression Equations Models: Estimation
and Inference (Marcel Dekker: New York).
</p>
<p>Srivastava, J.N. and M.K. Zaatar (1973), &ldquo;Monte Carlo Comparison of Four Estimators of Dispersion
Matrix of a Bivariate Normal Population, Using Incomplete Data,&rdquo; Journal of the American Sta-
tistical Association, 68: 180&ndash;183.
</p>
<p>Telser, L. (1964), &ldquo;Iterative Estimation of a Set of Linear Regression Equations,&rdquo; Journal of the American
Statistical Association, 59: 845&ndash;862.
</p>
<p>Wilks, S.S. (1932), &ldquo;Moments and Distributions of Estimates of Population Parameters From Fragmen-
tary Samples,&rdquo; Annals of Mathematical Statistics, 3: 167&ndash;195.
</p>
<p>Zellner, A. (1962), &ldquo;An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for
</p>
<p>Aggregation Bias,&rdquo; Journal of the American Statistical Association, 57: 348&ndash;368.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 11
</p>
<p>Simultaneous Equations Model
</p>
<p>11.1 Introduction
</p>
<p>Economists formulate models for consumption, production, investment, money demand and
money supply, labor demand and labor supply to attempt to explain the workings of the econ-
omy. These behavioral equations are estimated equation by equation or jointly as a system
of equations. These are known as simultaneous equations models. Much of today&rsquo;s economet-
rics have been influenced and shaped by a group of economists and econometricians known as
the Cowles Commission who worked together at the University of Chicago in the late 1940&rsquo;s,
see Chapter 1. Simultaneous equations models had their genesis in economics during that pe-
riod. Haavelmo&rsquo;s (1944) work emphasized the use of the probability approach to formulating
econometric models. Koopmans and Marschak (1950) and Koopmans and Hood (1953) in two
influential Cowles Commission monographs provided the appropriate statistical procedures for
handling simultaneous equations models. In this chapter, we first give simple examples of simul-
taneous equations models and show why the least squares estimator is no longer appropriate.
Next, we discuss the important problem of identification and give a simple necessary but not
sufficient condition that helps check whether a specific equation is identified. Sections 11.2 and
11.3 give the estimation of a single and a system of equations using instrumental variable pro-
cedures. Section 11.4 gives a test of over-identification restrictions whereas, section 11.5 gives a
Hausman specification test. Section 11.6 concludes with an empirical example. The Appendix
revisits the identification problem and gives a necessary and sufficient condition for identifica-
tion.
</p>
<p>11.1.1 Simultaneous Bias
</p>
<p>Example 1: Consider a simple Keynesian model with no government
</p>
<p>Ct = α+ βYt + ut t = 1, 2, . . . , T (11.1)
</p>
<p>Yt = Ct + It (11.2)
</p>
<p>where Ct denotes consumption, Yt denotes disposable income, and It denotes autonomous in-
vestment. This is a system of two simultaneous equations, also known as structural equations
with the second equation being an identity. The first equation can be estimated by OLS giving
</p>
<p>β̂OLS =
&sum;T
</p>
<p>t=1 ytct/
&sum;T
</p>
<p>t=1 y
2
t and α̂OLS = C̄ &minus; β̂OLSȲ (11.3)
</p>
<p>with yt and ct denoting Yt and Ct in deviation form, i.e., yt = Yt &minus; Ȳ , and Ȳ =
&sum;T
</p>
<p>t=1 Yt/T .
Since It is autonomous, it is an exogenous variable determined outside the system, whereas Ct
and Yt are endogenous variables determined by the system. Let us solve for Yt and Ct in terms
of the constant and It. The resulting two equations are known as the reduced form equations
</p>
<p>Ct = α/(1&minus; β) + βIt(1&minus; β) + ut/(1&minus; β) (11.4)
Yt = α/(1&minus; β) + It/(1&minus; β) + ut/(1&minus; β) (11.5)
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_11, 257</p>
<p/>
</div>
<div class="page"><p/>
<p>258 Chapter 11: Simultaneous Equations Model
</p>
<p>These equations express each endogenous variable in terms of exogenous variables and the error
terms. Note that both Yt and Ct are a function of ut, and hence both are correlated with ut. In
fact, Yt &minus; E(Yt) = ut/(1&minus; β), and
</p>
<p>cov(Yt, ut) = E[(Yt &minus; E(Yt))ut] = σ2u/(1&minus; β) &ge; 0 if 0 &le; β &le; 1 (11.6)
</p>
<p>This holds because ut &sim; (0, σ
2
u) and It is exogenous and independent of the error term. Equation
</p>
<p>(11.6) shows that the right hand side regressor in (11.1) is correlated with the error term. This
causes the OLS estimates to be biased and inconsistent. In fact, from (11.1),
</p>
<p>ct = Ct &minus; C̄ = βyt + (ut &minus; ū)
</p>
<p>and substituting this expression in (11.3), we get
</p>
<p>β̂OLS = β +
&sum;T
</p>
<p>t=1 ytut/
&sum;T
</p>
<p>t=1 y
2
t (11.7)
</p>
<p>From (11.7), it is clear that E(β̂OLS) 	= β, since the expected value of the second term is not
necessarily zero. Also, using (11.5) one gets
</p>
<p>yt = Yt &minus; Ȳ = [it + (ut &minus; ū)]/(1&minus; β)
</p>
<p>where it = It &minus; Ī and Ī =
&sum;T
</p>
<p>t=1 It/T . Defining myy =
&sum;T
</p>
<p>t=1 y
2
t /T , we get
</p>
<p>myy = (mii + 2miu +muu)/(1&minus; β)2 (11.8)
</p>
<p>where mii =
&sum;T
</p>
<p>t=1 i
2
t /T , miu =
</p>
<p>&sum;T
t=1 it(ut &minus; ū)/T and muu =
</p>
<p>&sum;T
t=1(ut &minus; ū)2/T . Also,
</p>
<p>myu = (miu +muu)/(1&minus; β) (11.9)
</p>
<p>Using the fact that plim miu = 0 and plim muu = σ
2
u, we get
</p>
<p>plim β̂OLS = β + plim (myu/myy) = β + [σ
2
u(1&minus; β)/(plim mii + σ2u)]
</p>
<p>which shows that β̂OLS overstates β if 0 &le; β &le; 1.
Example 2: Consider a simple demand and supply model
</p>
<p>Qdt = α+ βPt + u1t (11.10)
</p>
<p>Qst = γ + δPt + u2t (11.11)
</p>
<p>Qdt = Q
s
t = Qt t = 1, 2, . . . , T (11.12)
</p>
<p>Substituting the equilibrium condition (11.12) in (11.10) and (11.11), we get
</p>
<p>Qt = α+ βPt + u1t (11.13)
</p>
<p>Qt = γ + δPt + u2t t = 1, 2, . . . , T (11.14)
</p>
<p>For the demand equation (11.13), the sign of β is expected to be negative, while for the supply
equation (11.14), the sign of δ is expected to be positive. However, we only observe one equilib-
rium pair (Qt, Pt) and these are not labeled demand or supply quantities and prices. When we
run the OLS regression of Qt on Pt we do not know what we are estimating, demand or supply?
In fact, any linear combination of (11.13) and (11.14) looks exactly like (11.13) or (11.14). It</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Introduction 259
</p>
<p>will have a constant, Price, and a disturbance term in it. Since demand or supply cannot be
distinguished from this &lsquo;mongrel&rsquo; we have what is known as an identification problem. If the
demand equation (or the supply equation) looked different from this mongrel, then this partic-
ular equation would be identified. More on this later. For now let us examine the properties of
the OLS estimates of the demand equation. It is well known that
</p>
<p>β̂OLS =
&sum;T
</p>
<p>t=1 qtpt/
&sum;T
</p>
<p>t=1 p
2
t = β +
</p>
<p>&sum;T
t=1 pt(u1t &minus; ū1)/
</p>
<p>&sum;T
t=1 p
</p>
<p>2
t (11.15)
</p>
<p>where qt and pt denote Qt and Pt in deviation form, i.e., qt = Qt &minus; Q̄. This estimator is
unbiased depending on whether the last term in (11.15) has zero expectations. In order to find
this expectation we solve the structural equations in (11.13) and (11.14) for Qt and Pt
</p>
<p>Qt = (αδ &minus; γβ)/(δ &minus; β) + (δu1t &minus; βu2t)/(δ &minus; β) (11.16)
Pt = (α&minus; γ)/(δ &minus; β) + (u1t &minus; u2t)/(δ &minus; β) (11.17)
</p>
<p>(11.16) and (11.17) are known as the reduced form equations. Note that both Qt and Pt are
functions of both errors u1 and u2. Hence, Pt is correlated with u1t. In fact,
</p>
<p>pt = (u1t &minus; ū1)/(δ &minus; β)&minus; (u2t &minus; ū2)/(δ &minus; β) (11.18)
</p>
<p>and
</p>
<p>plim
&sum;T
</p>
<p>t=1 pt(u1t &minus; ū1)/T = (σ11 &minus; σ12)/(δ &minus; β) (11.19)
</p>
<p>plim
&sum;T
</p>
<p>t=1 p
2
t /T = (σ11 + σ22 &minus; 2σ12)/(δ &minus; β)2 (11.20)
</p>
<p>where σij = cov(uit, ujt) for i, j = 1, 2; and t = 1, . . . , T . Hence, from (11.15)
</p>
<p>plim β̂OLS = β + (σ11 &minus; σ12)(δ &minus; β)/(σ11 + σ22 &minus; 2σ12) (11.21)
</p>
<p>and the last term is not necessarily zero, implying that β̂OLS is not consistent for β. Similarly,
one can show that the OLS estimator for δ is not consistent, see problem 1. This simultaneous
bias is once again due to the correlation of the right hand side variable (price) with the error
term u1. This correlation could be due to the fact that Pt is a function of u2t, from (11.17),
and u2t and u1t are correlated, making Pt correlated with u1t. Alternatively, Pt is a function of
Qt, from (11.13) or (11.14), and Qt is a function of u1t, from (11.13), making Pt a function of
u1t. Intuitively, if a shock in demand (i.e., a change in u1t) shifts the demand curve, the new
intersection of demand and supply determines a new equilibrium price and quantity. This new
price is therefore, affected by the change in u1t, and is correlated with it.
In general, whenever a right hand side variable is correlated with the error term, the OLS
</p>
<p>estimates are biased and inconsistent. We refer to this as an endogeneity problem. Recall, Figure
3 of Chapter 3 with cov(Pt, u1t) &gt; 0. This shows that Pt&rsquo;s above their mean are on the average
associated with u1t&rsquo;s above their mean, (i.e., u1t &gt; 0). This implies that the quantity Qt asso-
ciated with this particular Pt is on the average above the true line (α + βPt). This is true for
all observations to the right of E(Pt). Similarly, any Pt to the left of E(Pt) is on the average
associated with a u1t below its mean, (i.e., u1t &lt; 0). This implies that quantities associated with
prices below their mean E(Pt) are on the average data points that lie below the true line. With
this observed data, the estimated line using OLS will always be biased. In this case, the intercept
estimate is biased downwards, whereas the slope estimate is biased upwards. This bias does not</p>
<p/>
</div>
<div class="page"><p/>
<p>260 Chapter 11: Simultaneous Equations Model
</p>
<p>disappear with more data, as any new observation will on the average be either above the true
line if Pt &gt; E(Pt) or below the line if Pt &lt; E(Pt). Hence, these OLS estimates are inconsistent.
Deaton (1997, p. 95) has a nice discussion of endogeneity problems in development economics.
</p>
<p>One important example pertains to farm size and farm productivity. Empirical studies using
OLS have found an inverse relationship between productivity as measured by log(Output/Acre)
and farm size as measured by (Acreage). This seems counter-intuitive as it suggests that smaller
farms are more productive than larger farms. Economic explanations of this phenomenon in-
clude the observation that hired labor (which is typically used on large farms) is of lower quality
than family labor (which is typically used on small farms). The latter needs less monitoring
and can be entrusted with valuable animals and machinery. Another explanation is that this
phenomenon is an optimal response by small farmers to uncertainty. It could also be a sign of
inefficiency as farmers work too much on their own farms pushing their marginal productivity
below market wage. How could this be an endogeneity problem? After all, the amount of
land is outside the control of the farmer. This is true, but that does not mean that acreage
is uncorrelated with the disturbance term. After all, size is unlikely to be independent of the
quality of land. &ldquo;Desert farms that are used for low-intensity animal grazing are typically larger
than garden farms, where the land is rich and output/acre is high.&rdquo; In this case, land quality is
negatively correlated with land size. It takes more acres to sustain a cow in West Texas than in
less arid areas. This negative correlation between acres, the explanatory variable and quality
of land which is an omitted variable included in the error term introduces endogeneity. This
in turn results in downward bias of the OLS estimate of acreage on productivity.
</p>
<p>Endogeneity can also be caused by sample selection. Gronau (1973) observed that women with
small children had higher wages than women with no children. An economic explanation is that
women with children have higher reservation wages and as a result fewer of them work. Of those
that work, their observed wages are higher than those without children. The endogeneity works
through the unobserved component in the working women&rsquo;s wage that induces her to work. This
is positively correlated with the number of children she has and therefore introduces upward
biases in the OLS estimate of the effect of the number of children on wages.
</p>
<p>11.1.2 The Identification Problem
</p>
<p>In general, we can think of any structural equation, say the first, as having one left hand
side endogenous variable y1, g1 right hand side endogenous variables, and k1 right hand side
exogenous variables. The right hand side endogenous variables are correlated with the error
term rendering OLS on this equation biased and inconsistent. Normally, for each endogenous
variable, there exists a corresponding structural equation explaining its behavior in the model.
We say that a system of simultaneous equations is complete if there are as many endogenous
variables as there are equations. To correct for the simultaneous bias we need to replace the
right hand side endogenous variables in this equation by variables which are highly correlated
with the ones they are replacing but not correlated with the error term. Using the method of
instrumental variable estimation, discussed below, we will see that these variables turn out to
be the predictors obtained by regressing each right hand side endogenous variable on a subset of
all the exogenous variables in the system. Let us assume that there are K exogenous variables
in the simultaneous system. What set of exogenous variables should we use that would lead
to consistent estimates of this structural equation? A search for the minimum set needed for
consistency leads us to the order condition for identification.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Introduction 261
</p>
<p>The Order Condition for Identification: A necessary condition for identification of any
structural equation is that the number of excluded exogenous variables from this equation are
greater than or equal to the number of right hand side included endogenous variables. Let K
be the number of exogenous variables in the system, then this condition requires k2 &ge; g1, where
k2 = K &minus; k1.
Let us consider the demand and supply equations given in (11.13) and (11.14) but assume
</p>
<p>that the supply equation has in it an extra variable Wt denoting weather conditions. In this
case the demand equation has one right hand side endogenous variable Pt, i.e., g1 = 1 and
one excluded exogenous variable Wt, making k2 = 1. Since k2 &ge; g1, this order condition is
satisfied, in other words, based on the order condition alone we cannot conclude that the
demand equation is unidentified. The supply equation, however, has g1 = 1 and k2 = 0, making
this equation unidentified, since it does not satisfy the order condition for identification. Note
that this condition is only necessary but not sufficient for identification. In other words, it
is useful only if it is not satisfied, in which case the equation in question is not identified.
Note that any linear combination of the new supply and demand equations would have a
constant, price and weather. This looks like the supply equation but not like demand. This
is why the supply equation is not identified. In order to prove once and for all whether the
demand equation is identified, we need the rank condition for identification and this will be
discussed in details in the Appendix to this chapter. Adding a third variable to the supply
equation like the amount of fertilizer used Ft will not help the supply equation any, since a
linear combination of supply and demand will still look like supply. However, it does help the
identification of the demand equation. Denote by ℓ = k2 &minus; g1, the degree of over-identification.
In (11.13) and (11.14) both equations are unidentified (or under-identified) with ℓ = &minus;1. When
Wt is added to the supply equation, ℓ = 0 for the demand equation, and it is just-identified.
When both Wt and Ft are included in the supply equation, ℓ = 1 and the demand equation is
over-identified.
Without the use of matrices, we can describe a two-stage least squares method that will
</p>
<p>estimate the demand equation consistently. First, we run the right hand side endogenous variable
Pt on a constant andWt and get P̂t, then replace Pt in the demand equation with P̂t and perform
this second stage regression. In other words, the first step regression is
</p>
<p>Pt = π11 + π12Wt + vt (11.22)
</p>
<p>with v̂t = Pt &minus; P̂t satisfying the OLS normal equations
&sum;T
</p>
<p>t=1 v̂t =
&sum;T
</p>
<p>t=1 v̂tWt = 0. The second
stage regression is
</p>
<p>Qt = α+ βP̂t + ǫt (11.23)
</p>
<p>with
&sum;T
</p>
<p>t=1 ǫ̂t =
&sum;T
</p>
<p>t=1 ǫ̂tP̂t = 0. Using (11.13) and (11.23), we can write
</p>
<p>ǫt = β(Pt &minus; P̂t) + u1t = βv̂t + u1t (11.24)
</p>
<p>so that
&sum;T
</p>
<p>t=1 ǫt =
&sum;T
</p>
<p>t=1 u1t and
&sum;T
</p>
<p>t=1 ǫtP̂t =
&sum;T
</p>
<p>t=1 u1tP̂t using the fact that
&sum;T
</p>
<p>t=1 v̂t =&sum;T
t=1 v̂tP̂t = 0. So the new error ǫt behaves as the original disturbance u1t. However, our
</p>
<p>right hand side variable is now P̂t which is independent of u1t since it is a linear combination
of exogenous variables only. We essentially decomposed Pt into two parts, the first part P̂t is a
linear combination of exogenous variables and therefore, independent of the u1t&rsquo;s. The second</p>
<p/>
</div>
<div class="page"><p/>
<p>262 Chapter 11: Simultaneous Equations Model
</p>
<p>part is v̂t which is correlated with u1t. In fact, this is the source of simultaneous bias. The
two parts P̂t and v̂t are orthogonal to each other by construction. Hence when the v̂t&rsquo;s become
part of the new error ǫt, they are orthogonal to the new regressor P̂t. Furthermore, P̂t is also
independent of u1t.
</p>
<p>Why would this procedure not work on the estimation of (11.13) if the model is given by
equations (11.13) and (11.14). The answer is that in (11.22) we will only have a constant, and
no Wt. When we try to run the second-stage regression in (11.23) the regression will fail because
of perfect multicollinearity between the constant and P̂t. This will happen whenever the order
condition is not satisfied and the equation is not identified, see Kelejian and Oates (1989).
Hence, in order for it to succeed in the second stage we need at least one excluded exogenous
variable from the demand equation that is in the supply equation, i.e., variables like Wt or
Ft. Therefore, whenever the second-stage regression fails because of perfect multicollinearity
between the right hand side regressors, this implies that the order condition of identification is
not satisfied.
In general, if we are given an equation like
</p>
<p>y1 = α12y2 + β11X1 + β12X2 + u1 (11.25)
</p>
<p>the order condition requires the existence of at least one exogenous variable excluded from
(11.25), say X3. These extra exogenous variables like X3 usually appear in other equations of
our simultaneous equation model. In the first step regression we run
</p>
<p>y2 = π21X1 + π22X2 + π23X3 + v2 (11.26)
</p>
<p>with the OLS residuals v̂2 satisfying
</p>
<p>&sum;T
t=1 v̂2tX1t = 0;
</p>
<p>&sum;T
t=1 v̂2tX2t = 0;
</p>
<p>&sum;T
t=1 v̂2tX3t = 0 (11.27)
</p>
<p>and in the second step, we run the regression
</p>
<p>y1 = α12ŷ2 + β11X1 + β12X2 + ǫ1 (11.28)
</p>
<p>where ǫ1 = α12(y2 &minus; ŷ2) + u1 = α12v̂2 + u1. This regression will lead to consistent estimates,
because
</p>
<p>&sum;T
t=1 ŷ2tǫ1t =
</p>
<p>&sum;T
t=1 ŷ2tu1t;
</p>
<p>&sum;T
t=1X1tǫ1t =
</p>
<p>&sum;T
t=1X1tu1t;&sum;T
</p>
<p>t=1X2tǫ1t =
&sum;T
</p>
<p>t=1X2tu1t
(11.29)
</p>
<p>and u1t is independent of the exogenous variables. In order to solve for 3 structural parameters
α12, β11 and β12 one needs three linearly independent OLS normal equations.
</p>
<p>&sum;T
t=1 ŷ2tǫ̂1t = 0
</p>
<p>is a new piece of information provided y2 is regressed on at least one extra variable besides
X1 and X2. Otherwise,
</p>
<p>&sum;T
t=1X1tǫ̂1t =
</p>
<p>&sum;T
t=1X2tǫ̂1t = 0 are the only two linearly independent
</p>
<p>normal equations in three structural parameters.
What happens if there is another right hand side endogenous variable, say y3? In that case
</p>
<p>(11.25) becomes
</p>
<p>y1 = α12y2 + α13y3 + β11X1 + β12X2 + u1 (11.30)
</p>
<p>Now we need at least two exogenous variables that are excluded from (11.30) for the order
condition to be satisfied, and the second stage regression to run. Otherwise, we will have less</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Single Equation Estimation: Two-Stage Least Squares 263
</p>
<p>linearly independent equations than there are structural parameters to estimate, and the second
stage regression will fail. Also, y2 and y3 should be regressed on the same set of exogenous vari-
ables. Furthermore, this set of second-stage regressors should always include the right hand side
exogenous variables of (11.30). These two conditions will ensure consistency of the estimates.
Let X3 and X4 be the excluded exogenous variables from (11.30). Our first step regression
would regress y2 and y3 on X1, X2, X3 and X4 to get ŷ2 and ŷ3, respectively. The second stage
regression would regress y1 on ŷ2, ŷ3, X1 and X2. From the first step regressions we have
</p>
<p>y2 = ŷ2 + v̂2 and y3 = ŷ3 + v̂3 (11.31)
</p>
<p>where ŷ2 and ŷ3 are linear combinations of the X&rsquo;s, and v̂2 and v̂3 are the residuals. The second
stage regression has the following normal equations
</p>
<p>&sum;T
t=1 ŷ2tǫ̂1t =
</p>
<p>&sum;T
t=1 ŷ3tǫ̂1t =
</p>
<p>&sum;T
t=1X1tǫ̂1t =
</p>
<p>&sum;T
t=1X2tǫ̂1t = 0 (11.32)
</p>
<p>where ǫ̂1 denotes the residuals from the second stage regression. In fact
</p>
<p>ǫ1 = α12v̂2 + α13v̂3 + u1 (11.33)
</p>
<p>Now
&sum;T
</p>
<p>t=1 ǫ1tŷ2t =
&sum;T
</p>
<p>t=1 u1tŷ2t because
&sum;T
</p>
<p>t=1 v̂2tŷ2t =
&sum;T
</p>
<p>t=1 v̂3tŷ2t = 0. The latter holds because
ŷ2, the predictor, is orthogonal to v̂2, the residual. Also, ŷ2 is orthogonal to v̂3 if y2 is regressed on
a set ofX&rsquo;s that are a subset of the regressors included in the first step regression of y3. Similarly,&sum;T
</p>
<p>t=1 ǫ1tŷ3t =
&sum;T
</p>
<p>t=1 u1tŷ3t if y3 is regressed on a set of exogenous variables that are a subset of
the X&rsquo;s included in the first step regression of y2. Combining these two conditions leads to the
following fact: y2 and y3 have to be regressed on the same set of exogenous variables for the
composite error term to behave like the original error. Furthermore these exogenous variables
should include the included X&rsquo;s on the right hand side of the equation to be estimated, i.e., X1
and X2, otherwise,
</p>
<p>&sum;T
t=1 ǫ1tX1t is not necessarily equal to
</p>
<p>&sum;T
t=1 u1tX1t, because
</p>
<p>&sum;T
t=1 v̂2tX1t
</p>
<p>or
&sum;T
</p>
<p>t=1 v̂3tX1t are not necessarily zero. For further analysis along these lines, see problem 2.
</p>
<p>11.2 Single Equation Estimation: Two-Stage Least Squares
</p>
<p>In matrix form, we can write the first structural equation as
</p>
<p>y1 = Y1α1 +X1β1 + u1 = Z1δ1 + u1 (11.34)
</p>
<p>where y1 and u1 are (T &times; 1), Y1 denotes the right hand side endogenous variables which is
(T &times; g1) and X1 is the set of right hand side included exogenous variables which is (T &times; k1), α1
is of dimension g1 and β1 is of dimension k1. Z1 = [Y1, X1] and δ
</p>
<p>&prime;
1 = (α
</p>
<p>&prime;
1, β
</p>
<p>&prime;
1). We require the
</p>
<p>existence of excluded exogenous variables, from (11.34), call them X2, enough to identify this
equation. These excluded exogenous variables appear in the other equations in the simultaneous
model. Let the set of all exogenous variables be X = [X1, X2] where X is of dimension (T &times; k).
For the order condition to be satisfied for equation (11.34) we must have (k&minus;k1) &ge; g1. If all the
exogenous variables in the system are included in the first step regression, i.e., Y1 is regressed on
X to get Ŷ1, the resulting second stage least squares estimator obtained from regressing y1 on
Ŷ1 and X1 is called two-stage least squares (2SLS). This method was proposed independently
by Basmann (1957) and Theil (1953). In matrix form Ŷ1 = PXY1 is the predictor of the right</p>
<p/>
</div>
<div class="page"><p/>
<p>264 Chapter 11: Simultaneous Equations Model
</p>
<p>hand side endogenous variables, where PX is the projection matrix X(X
&prime;X)&minus;1X &prime;. Replacing Y1
</p>
<p>by Ŷ1 in (11.34), we get
</p>
<p>y1 = Ŷ1α1 +X1β1 + w1 = Ẑ1δ1 + w1 (11.35)
</p>
<p>where Ẑ1 = [Ŷ1, X1] and w1 = u1 + (Y1 &minus; Ŷ1)α1. Running OLS on (11.35) one gets
</p>
<p>δ̂1,2SLS = (Ẑ
&prime;
1Ẑ1)
</p>
<p>&minus;1Ẑ &prime;1y1 = (Z
&prime;
1PXZ1)
</p>
<p>&minus;1Z &prime;1PXy1 (11.36)
</p>
<p>where the second equality follows from the fact that Ẑ1 = PXZ1 and the fact that PX is
idempotent. The former equality holds because PXX = X, hence PXX1 = X1, and PXY1 = Ŷ1.
If there is only one right hand side endogenous variable, running the first-stage regression y2
on X1 and X2 and testing that the coefficients of X2 are all zero against the hypothesis that
at least one of these coefficients is different from zero is a test for rank identification. In case
of several right hand side endogenous variables, things get complicated, see Cragg and Donald
(1996), but one can still run the first-stage regressions for each right hand side endogenous
variable to make sure that at least one element of X2 is significantly different from zero.
</p>
<p>1 This
is not sufficient for the rank condition but it is a good diagnostic for whether the rank condition
fails. If we fail to meet this requirement we should question our 2SLS estimator.
Two-stage least squares can also be thought of as a simple instrumental variables estimator
</p>
<p>with the set of instruments W = Ẑ1 = [Ŷ1, X1]. Recall that Y1 is correlated with u1, rendering
OLS inconsistent. The idea of simple instrumental variables is to find a set of instruments,
say W for Z1 with the following properties: (1) plim W
</p>
<p>&prime;u1/T = 0, the instruments have to
be exogenous, i.e., uncorrelated with the error term, otherwise this defeats the purpose of the
instruments and result in inconsistent estimates. (2) plim W &prime;W/T = Qw 	= 0, where Qw is
finite and positive definite, the W &prime;s should not be perfectly multicollinear. (3) W should be
highly correlated with Z1, i.e., the instruments should be highly relevant, not weak instruments
as we will explain shortly. In fact, plim W &prime;Z1/T should be finite and of full rank (k1 + g1).
Premultiplying (11.34) by W &prime;, we get
</p>
<p>W &prime;y1 = W
&prime;Z1δ1 +W
</p>
<p>&prime;u1 (11.37)
</p>
<p>In this case, W = Ẑ1 is of the same dimension as Z1, and since plim W
&prime;Z1/T is square and of
</p>
<p>full rank (k1 + g1), the simple instrumental variable (IV) estimator of δ1 becomes
</p>
<p>δ̂1,IV = (W
&prime;Z1)
</p>
<p>&minus;1W &prime;y1 = δ1 + (W
&prime;Z1)
</p>
<p>&minus;1W &prime;u1 (11.38)
</p>
<p>with plim δ̂1,IV = δ1 which follows from (11.37) and the fact that plim W
&prime;u1/T = 0.
</p>
<p>Digression: In the general linear model, y = Xβ + u, X is the set of instruments for X.
Premultiplying by X &prime; we get X &prime;y = X &prime;Xβ +X &prime;u and using the fact that plim X &prime;u/T = 0, one
gets
</p>
<p>β̂IV = (X
&prime;X)&minus;1X &prime;y = β̂OLS .
</p>
<p>This estimator is consistent as long as X and u are uncorrelated. In the simultaneous equation
model for the first structural equation given in (11.34), the right hand side regressors Z1 include
endogenous variables Y1 that are correlated with u1. Therefore OLS on (11.34) will lead to</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Single Equation Estimation: Two-Stage Least Squares 265
</p>
<p>inconsistent estimates, since the matrix of instruments W = Z1, and Z1 is correlated with u1.
In fact,
</p>
<p>δ̂1,OLS = (Z
&prime;
1Z1)
</p>
<p>&minus;1Z &prime;1y1 = δ1 + (Z
&prime;
1Z1)
</p>
<p>&minus;1Z &prime;1u1
</p>
<p>with plim δ̂1,OLS 	= δ1 since plim Z &prime;1u1/T 	= 0.
Denote by e1,OLS = y1&minus;Z1δ̂1,OLS as the OLS residuals on the first structural equation, then
</p>
<p>plim s21 =
e&prime;1,OLSe1,OLS
T &minus; (g1 + k1)
</p>
<p>= plim
u&prime;1P̄Z1u1
</p>
<p>T &minus; (g1 + k1)
= σ11 &minus; plim
</p>
<p>u&prime;1Z1(Z
&prime;
1Z1)
</p>
<p>&minus;1Z &prime;1u1
T &minus; (g1 + k1)
</p>
<p>&le; σ11,
</p>
<p>since the last term is positive. Only if plim Z &prime;1u1/T is zero will plim s
2
1 = σ11, otherwise it is
</p>
<p>smaller. OLS fits very well, it minimizes (y1&minus;Z1δ1)&prime;(y1&minus;Z1δ1). Since Z1 and u1 are correlated,
OLS attributes part of the variation in y1 that is due to u1 incorrectly to the regressor Z1.
</p>
<p>Both the simple IV and OLS estimators can be interpreted as method of moments estimators.
These were discussed in Chapter 2. For OLS, the population moment conditions are given
by E(X &prime;u) = 0 and the corresponding sample moment conditions yield X &prime;(y &minus; Xβ̂)/T = 0.
Solving for β̂ results in β̂OLS . Similarly, the population moment conditions for the simple IV
estimator in (11.37) are E(W &prime;u1) = 0 and the corresponding sample moment conditions yield
W &prime;(y1 &minus; Z1δ̂1)/T = 0. Solving for δ̂1 results in δ̂1,IV given in (11.38).
If W = [Ŷ1, X1], then (11.38) results in
</p>
<p>δ̂1,IV =
</p>
<p>[
Ŷ &prime;1Y1 Ŷ
</p>
<p>&prime;
1X1
</p>
<p>X &prime;1Y1 X
&prime;
1X1
</p>
<p>]&minus;1 [
Ŷ &prime;1y1
X &prime;1y1
</p>
<p>]
(11.39)
</p>
<p>which is the same as (11.36)
</p>
<p>δ̂1,2SLS =
</p>
<p>[
Ŷ &prime;1 Ŷ1 Ŷ
</p>
<p>&prime;
1X1
</p>
<p>X &prime;1Ŷ1 X
&prime;
1X1
</p>
<p>]&minus;1 [
Ŷ &prime;1y1
X &prime;1y1
</p>
<p>]
(11.40)
</p>
<p>provided Ŷ &prime;1 Ŷ1 = Ŷ
&prime;
1Y1, and X
</p>
<p>&prime;
1Y1 = X
</p>
<p>&prime;
1Ŷ1. The latter conditions hold because Ŷ1 = PXY1, and
</p>
<p>PXX1 = X1.
In general, let X&lowast; be our set of first stage regressors. An IV estimator with Ŷ &lowast;1 = PX&lowast;Y1, i.e.,
</p>
<p>with every right hand side y regressed on the same set of regressors X&lowast;, will satisfy
</p>
<p>Ŷ &lowast;&prime;1 Ŷ
&lowast;
1 = Ŷ
</p>
<p>&lowast;&prime;
1 PX&lowast;Y1 = Ŷ
</p>
<p>&lowast;&prime;
1 Y1
</p>
<p>In addition, for X &prime;1Ŷ
&lowast;
1 to equal X
</p>
<p>&prime;
1Y1, X1 has to be a subset of the regressors in X
</p>
<p>&lowast;. Therefore
X&lowast; should include X1 and at least as many X&rsquo;s from X2 as is required for identification, i.e.,
(at least g1 of the X&rsquo;s from X2). In this case, the IV estimator using W
</p>
<p>&lowast; = [Ŷ &lowast;1 , X1] will result
in the same estimator as that obtained by a two stage regression where in the first step Ŷ &lowast;1
is obtained by regressing Y1 on X
</p>
<p>&lowast;, and in the second step y1 is regressed on W &lowast;. Note that
these are the same conditions required for consistency of an IV estimator. Note also, that if this
equation is just-identified, then there is exactly g1 of the X&rsquo;s excluded from that equation. In
other words, X2 is of dimension (T &times; g1), and X&lowast; = X is of dimension T &times; (g1 + k1). Problem
3 shows that 2SLS in this case reduces to an IV estimator with W = X, i.e.
</p>
<p>δ̂1,2SLS = δ̂1,IV = (X
&prime;Z1)
</p>
<p>&minus;1X &prime;y1 (11.41)</p>
<p/>
</div>
<div class="page"><p/>
<p>266 Chapter 11: Simultaneous Equations Model
</p>
<p>Note that if the first equation is over-identified, then X &prime;Z1 is not square and (11.41) cannot be
computed.
</p>
<p>Rather than having W , the matrix of instruments, be of exactly the same dimension as Z1
which is required for the expression in (11.38), one can define a generalized instrumental variable
in terms of a general matrix W of dimension T &times; ℓ where ℓ &ge; g1 + k1. The latter condition is
the order condition for identification. In this case, δ̂1,IV is obtained as GLS on (11.37). Using
the fact that
</p>
<p>plim W &prime;u1u
&prime;
1W/T = σ11 plim W
</p>
<p>&prime;W/T,
</p>
<p>one gets
</p>
<p>δ̂1,IV = (Z
&prime;
1PWZ1)
</p>
<p>&minus;1Z &prime;1PW y1 = δ1 + (Z
&prime;
1PWZ1)
</p>
<p>&minus;1Z &prime;1PWu1
</p>
<p>with plim δ̂1,IV = δ1 and limiting covariance matrix σ11 plim (Z
&prime;
1PWZ1/T )
</p>
<p>&minus;1. Therefore, 2SLS
can be obtained as a generalized instrumental variable estimator with W = X. This also means
that 2SLS of δ1 can be obtained as GLS on (11.34) after premultiplication by X
</p>
<p>&prime;, see problem
4. Note that GLS on (11.37) minimizes (y1 &minus; Z1δ1)&prime;PW (y1 &minus; Z1δ1) which yields the first-order
conditions
</p>
<p>Z &prime;1PW (y1 &minus; Z1δ̂1,IV ) = 0
</p>
<p>the solution of which is δ̂1,IV = (Z
&prime;
1PWZ1)
</p>
<p>&minus;1Z &prime;1PW y1. It can also be shown that 2SLS and
the generalized instrumental variables estimators are special cases of a Generalized Method of
Moments (GMM) estimator considered by Hansen (1982). See Davidson and MacKinnon (1993)
and Hall (1993) for an introduction to GMM.
</p>
<p>For the matrix Z &prime;1PWZ1 to be of full rank and invertible, a necessary condition is thatW must
be of full rank ℓ &ge; (g1 + k1). This is in fact, the order condition of identification. If ℓ = g1 + k1,
then this equation is just-identified. Also, W &prime;Z1 is square and nonsingular. Problem 10 asks
the reader to verify that the generalized instrumental variable estimator reduces to the simple
instrumental variable estimator given in (11.38). Also, under just-identification the minimized
value of the criterion function is zero.
One of the biggest problems with IV estimation is the choice of the instrumental variables
</p>
<p>W . We have listed some necessary conditions for this set of instruments to yield consistent
estimators of the structural coefficients. However, different choices by different researchers may
yield different estimates in finite samples. Using more instruments will yield more efficient IV
estimation. Let W1 and W2 be two sets of IV&rsquo;s with W1 being spanned by the space of W2. In
this case, PW2W1 = W1 and therefore, PW2PW1 = PW1 . For the corresponding IV estimators
</p>
<p>δ̂1,Wi = (Z
&prime;
1PWiZ1)
</p>
<p>&minus;1Z &prime;1PWiy1 for i = 1, 2
</p>
<p>are both consistent for δ1 as long as plim W
&prime;
iu1/T = 0 and have asymptotic covariance matrices
</p>
<p>σ11 plim (Z
&prime;
1PWiZ1/T )
</p>
<p>&minus;1
</p>
<p>Note that δ̂1,W2 is at least as efficient as δ̂1,W1 , if the difference in their asymptotic covariance
matrices is positive semi-definite, i.e., if
</p>
<p>σ11
</p>
<p>[
plim
</p>
<p>Z &prime;1PW1Z1
T
</p>
<p>]&minus;1
&minus; σ11
</p>
<p>[
plim
</p>
<p>Z &prime;1PW2Z1
T
</p>
<p>]&minus;1</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Single Equation Estimation: Two-Stage Least Squares 267
</p>
<p>is p.s.d. This holds, if Z &prime;1PW2Z1&minus;Z &prime;1PW1Z1 is p.s.d. This last condition holds since PW2 &minus;PW1 is
idempotent. Problem 11 asks the reader to verify this result. δ̂1,W2 is more efficient than δ̂1,W1
since W2 explains Z1 at least as well as W1. This seems to suggest that one should use as many
instruments as possible. If T is large this is a good strategy. But, if T is finite, there will be a
trade-off between this gain in asymptotic efficiency and the introduction of more finite sample
bias in our IV estimator.
In fact, the more instruments we use, the more will Ŷ1 resemble Y1 and the more bias is
</p>
<p>introduced in this second stage regression. The extreme case where Y1 is perfectly predicted
by Ŷ1 returns us to OLS which we know is biased. On the other hand, if our set of instru-
ments have little ability in predicting Y1, then the resulting instrumental variable estimator
will be inefficient and its asymptotic distribution will not resemble its finite sample distribu-
tion, see Nelson and Startz (1990). If the number of instruments is fixed and the coefficients
of the instruments in the first stage regression go to zero at the rate 1/
</p>
<p>&radic;
T , indicating weak
</p>
<p>correlation, Staiger and Stock (1997) find that even as T increases, IV estimation is not consis-
tent and has a nonstandard asymptotic distribution. Bound et al. (1995) recommend reporting
the R2 or the F -statistic of the first stage regression as a useful indicator of the quality of IV
estimates.
</p>
<p>Instrumental variables are important for obtaining consistent estimates when endogeneity is
suspected. However, invalid instruments can produce meaningless results. How do we know
whether our instruments are valid? Stock and Watson (2003) draw an analogy between a
relevant instrument and a large sample. The more relevant the instrument, i.e., the more the
variation in the right hand side endogenous variable that is explained by this instrument, the
more accurate the resulting estimator. This is similar to the observation that the larger the
sample size, the more accurate the estimator. They argue that the instruments should not just
be relevant, but highly relevant if the normal distribution is to provide a good approximation to
the sampling distribution of 2SLS. Weak instruments explain little of the variation in the right
hand side endogenous variable they are instrumenting. This renders the normal distribution as a
poor approximation to the sampling distribution of 2SLS, even if the sample size is large. Stock
and Watson (2003) suggest a simple rule of thumb to check for weak instruments. If there is one
right hand side endogenous variable, the first-stage regression can test for the significance of the
excluded exogenous variables (or instruments) using an F-statistic. This first-stage F -statistic
should be larger than 10.2 Stock and Watson (2003) suggest that a first-stage F -statistic less
than 10 indicates weak instruments which casts doubt on the validity of 2SLS, since with weak
instruments, 2SLS will be biased even in large samples and the corresponding t-statistics and
confidence intervals will be unreliable. Finding weak instruments, one can search for additional
stronger instruments, or use alternative estimators than 2SLS which are less sensitive to weak
instruments like LIML. Deaton (1997, p. 112) argues that it is difficult to find instruments that
are exogenous while at the same time highly correlated with the endogenous variables they
are instrumenting. He argues that it is easy to generate 2SLS estimates that are different from
OLS but much harder to make the case that these 2SLS estimates are necessarily better than
OLS. &ldquo;Credible identification and estimation of structural equations almost always requires
real creativity, and creativity cannot be reduced to a formula.&rdquo; Stock and Watson (2003, p.
371) show that for the case of a single right hand side endogenous variable with no included
exogenous variables and one weak instrument, the distribution of the 2SLS estimators is non-
normal even for large samples, with the mean of the sampling distribution of the 2SLS estimator
approximately equal to the true coefficient plus the asymptotic bias of the OLS estimator divided</p>
<p/>
</div>
<div class="page"><p/>
<p>268 Chapter 11: Simultaneous Equations Model
</p>
<p>by (E(F )&minus; 1) where F is the first-stage F -statistic. If E(F ) = 10, then the large sample bias
of 2SLS is (1/9) that of the large sample bias of OLS. They argue that this rule of thumb is
an acceptable cutoff for most empirical applications.
</p>
<p>2SLS is a single equation estimator. The focus is on a particular equation. [y1, Y1, X1] is spec-
ified and therefore all that is needed to perform 2SLS is the matrix X of all exogenous variables
in the system. If a researcher is interested in a particular behavioral economic relationship which
may be a part of a big model consisting of several equations, one need not specify the whole
model to perform 2SLS on that equation, all that is needed is the matrix of all exogenous vari-
ables in that system. Empirical studies involving one structural equation, specify which right
hand side variables are endogenous and proceed by estimating this equation via an IV procedure
that usually includes all the feasible exogenous variables available to the researcher. If this set
of exogenous variables does not include all the X&rsquo;s in the system, this estimation method is not
2SLS. However, it is a consistent IV method which we will call feasible 2SLS.
Substituting (11.34) in (11.36), we get
</p>
<p>δ̂1,2SLS = δ1 + (Z
&prime;
1PXZ1)
</p>
<p>&minus;1Z &prime;1PXu1 (11.42)
</p>
<p>with plim δ̂1,2SLS = δ1 and an asymptotic variance covariance matrix given by σ11 plim
</p>
<p>(Z &prime;1PXZ1/T )
&minus;1. σ11 is estimated from the 2SLS residuals û1 = y1 &minus; Z1δ̂1,2SLS , by comput-
</p>
<p>ing s11 = û
&prime;
1û1/(T &minus; g1 &minus; k1). It is important to emphasize that s11 is obtained from the 2SLS
</p>
<p>residuals of the original equation (11.34), not (11.35). In other words, s11 is not the mean
squared error (i.e., s2) of the second stage regression given in (11.35). The latter regression has
Ŷ1 in it and not Y1. Therefore, the asymptotic variance covariance matrix of 2SLS can be esti-
mated by s11(Z
</p>
<p>&prime;
1PXZ1)
</p>
<p>&minus;1 = s11(Ẑ &prime;1Ẑ1)
&minus;1. The t-statistics reported by 2SLS packages are based
</p>
<p>on the standard errors obtained from the square root of the diagonal elements of this matrix.
These standard errors and t-statistics can be made robust for heteroskedasticity by computing
(Ẑ &prime;1Ẑ1)
</p>
<p>&minus;1(Ẑ &prime;1diag[û
2
i ]Ẑ1)(Ẑ
</p>
<p>&prime;
1Ẑ1)
</p>
<p>&minus;1 where ûi denotes the i-th 2SLS residual. Wald type statistics
for Ho;Rδ1 = r based on 2SLS estimates of δ1 can be obtained as in equation (7.41) with
δ̂1,2SLS replacing β̂OLS and var(δ̂1,2SLS) = s11(Ẑ
</p>
<p>&prime;
1Ẑ1)
</p>
<p>&minus;1 replacing var(β̂OLS) = s11(X
&prime;X)&minus;1.
</p>
<p>This can be made robust for heteroskedasticity by using the robust variance covariance matrix
of δ̂1,2SLS described above. The resulting Wald statistic is asymptotically distributed as χ
</p>
<p>2
q
</p>
<p>under the null hypothesis, with q being the number of restrictions imposed by Rδ1 = r.
LM type tests for exclusion restrictions, like a subset of δ1 set equal to zero can be performed
</p>
<p>by running the restricted 2SLS residuals on the matrix of unrestricted second stage regressors Ẑ1.
The test statistic is given by TR2u where R
</p>
<p>2
u denotes the uncentered R
</p>
<p>2. This is asymptotically
distributed as χ2q under the null hypothesis, where q is the number of coefficients in δ1 set equal
to zero. Note that it does not matter whether the exclusion restrictions are imposed on β1 or
α1, i.e., whether the excluded variables to be tested are endogenous or exogenous. An F-test for
these exclusion restrictions can be constructed based on the restricted and unrestricted residual
sums of squares from the second stage regression. The denominator of this F-statistic, however,
is based on the unrestricted 2SLS residual sum of squares as reported by the 2SLS package. Of
course, one has to adjust the numerator and denominator by the appropriate degrees of freedom.
Under the null, this is asymptotically distributed as F (q, T &minus; (g1+k1)). See Wooldridge (1990)
for details. Also, see the over-identification test in Section 11.5.
</p>
<p>Finite sample properties of 2SLS are model specific, see Mariano (2001) for a useful summary.
One important result is that the absolute moments of positive order for 2SLS are finite up to
the order of over-identification. So, for the 2SLS estimator to have a mean and variance, we</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Single Equation Estimation: Two-Stage Least Squares 269
</p>
<p>need the degree of over-identification to be at least 2. This also means that for a just-identified
model, no moments for 2SLS exist. For 2SLS, the absolute bias is an increasing function of the
degree of over-identification. For the case of one right hand side included endogenous regressor,
like equation (11.25), the size of OLS bias relative to 2SLS gets larger, the lower the degree of
over-identification, the bigger the sample size, the higher the absolute value of the correlation
between the disturbances and the endogenous regressor y2 and the higher the concentration
parameter μ2. The latter is defined as μ2 = E(y2)
</p>
<p>&prime;(PX &minus; PX1)E(y2)/ω2 and ω2 = var(y2t). In
terms of MSE, larger values of μ2 and large sample size favor 2SLS over OLS.
Another important single equation estimator is the Limited Information Maximum Likelihood
</p>
<p>(LIML) estimator which as the name suggests maximizes the likelihood function pertaining to
the endogenous variables appearing in the estimated equation only. Excluded exogenous vari-
ables from this equation as well as the identifiability restrictions on other equations in the
system are disregarded in the likelihood maximization. For details, see Anderson and Rubin
(1950). LIML is invariant to the normalization choice of the dependent variable whereas 2SLS
is not. This invariancy of LIML is in the spirit of a simultaneous equation model where nor-
malization should not matter. Under just-identification 2SLS and LIML are equivalent. LIML
is also known as the Least Variance Ratio (LVR) method, since the LIML estimates can be
obtained by minimizing a ratio of two variances or equivalently the ratio of two residual sum of
squares. Using equation (11.34), one can write
</p>
<p>y&lowast;1 = y1 &minus; Y1α = X1β1 + u1
</p>
<p>For a choice of α1 one can compute y
&lowast;
1 and regress it on X1 to get the residual sum of squares
</p>
<p>RSS1. Now regress y
&lowast;
1 on X1 and X2 and compute the residual sum of squares RSS2. Equation
</p>
<p>(11.34) states that X2 does not enter the specification of that equation. In fact, this is where
our identifying restrictions come from and the excluded exogenous variables that are used as
instrumental variables. If these identifying restrictions are true, adding X2 to the regression
of y&lowast;1 and X1 should lead to minimal reduction in RSS1. Therefore, the LVR method finds
the α1 that will minimize the ratio (RSS1/RSS2). After α1 is estimated, β1 is obtained from
regressing y&lowast;1 on X1. In contrast, it can be shown that 2SLS minimizes RSS1 &minus; RSS2. For
details, see Johnston (1984) or Mariano (2001). Estimator bias is less of a problem for LIML
than 2SLS. In fact as the number of instruments increase with the sample size such that their
ratio is a constant, Bekker (1994) shows that 2SLS becomes inconsistent while LIML remains
consistent. Both estimators are special cases of the following estimator:
</p>
<p>δ̂1 = (Z
&prime;
1PXZ1 &minus; θ̂Z &prime;1Z1)&minus;1(Z &prime;1PXy1 &minus; θ̂Z &prime;1y1)
</p>
<p>with θ̂ = 0 yielding 2SLS, and θ̂ = the smallest eigenvalue of {(D&prime;1D1)&minus;1D&prime;1PXD1} yielding
LIML, where D1 = [y1, Z1].
</p>
<p>Example 3: Simple Keynesian Model
For the data from the Economic Report of the President, given in Table 5.3, consider the simple
Keynesian model with no government
</p>
<p>Ct = α+ βYt + ut t = 1, 2, . . . , T
</p>
<p>with Yt = Ct + It.</p>
<p/>
</div>
<div class="page"><p/>
<p>270 Chapter 11: Simultaneous Equations Model
</p>
<p>Table 11.1 Two-Stage Least Squares
</p>
<p>Dependent Variable: CONSUMP
Method: Two-Stage Least Squares
Sample: 1959 2007
Included observations: 49
Instrument specification: INV
Constant added to instrument list
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C 4081.653 3194.839 1.277577 0.2077
Y 0.685609 0.172415 3.976513 0.0002
</p>
<p>R-squared 0.904339 Mean dependent var 16749.10
Adjusted R-squared 0.902304 S.D. dependent var 5447.060
S.E. of regression 1702.552 Sum squared resid 1.36E+08
F-statistic 15.81265 Durbin-Watson stat 0.014554
Prob (F-statistic) 0.000240 Second-Stage SSR 1.38E+09
J-statistic 0.000000 Instrument rank 2
</p>
<p>The OLS estimates of the consumption function yield:
</p>
<p>Ct =&minus;1343.31
(219.56)
</p>
<p>+ 0.979
</p>
<p>(0.011)
</p>
<p>Yt + residuals
</p>
<p>The 2SLS estimates assuming that It is exogenous and is the only instrument available, yield
</p>
<p>Ct = 4081.65
</p>
<p>(3194.8)
</p>
<p>+ 0.686
</p>
<p>(0.172)
</p>
<p>Yt + residuals
</p>
<p>Table 11.1 reports these 2SLS results using EViews. Note that the OLS estimate of the intercept
is understated, while that of the slope estimate is overstated indicating positive correlation
between Yt and the error as described in (11.6). The standard errors of 2SLS are bigger than
those of OLS. This is always the case for an instrumental variable estimator as will be shown
analytically for a simple regression in Example 4 below.
OLS on the reduced form equations yield
</p>
<p>Ct = 12982.72
</p>
<p>(3110.4)
</p>
<p>+ 2.18
</p>
<p>(1.74)
</p>
<p>It + residuals and Yt = 12982.72
</p>
<p>(3110.4)
</p>
<p>+ 3.18
</p>
<p>(1.74)
</p>
<p>It + residuals
</p>
<p>From example (A.5) in the Appendix, we see that β̂ = π̂12/π̂22 = 2.18/3.18 = 0.686 as described
in (A.24). Also, β̂ = (π̂22&minus;1)/π̂22 = (3.18&minus;1)/3.18 = 2.18/3.18 = 0.686 as described in (A.25).
Similarly, α̂ = π̂11/π̂22 = π̂21/π̂22 = 12982.72/3.18 = 4081.65 as described in (A.22).
This confirms that under just-identification, the 2SLS estimates of the structural coefficients
</p>
<p>are identical to the Indirect Least Squares (ILS) estimates. The latter estimates uniquely solve
for the structural parameter estimates from the reduced form estimates under just-identification.
Note that in this case both 2SLS and ILS estimates of the consumption equation are identical
to the simple IV estimator using It as an instrument for Yt; i.e., β̂IV = mci/myi as shown in
(A.24).</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Single Equation Estimation: Two-Stage Least Squares 271
</p>
<p>11.2.1 Spatial Lag Dependence
</p>
<p>An alternative popular model for spatial lag dependence considered in Section 9.9 is given by:
</p>
<p>y = ρWy +Xβ + ǫ
</p>
<p>where ǫ &sim; IIN(0, σ2), see Anselin (1988). Here yi may denote output in region i which is affected
by output of its neighbors through the spatial coefficient ρ and the weight matrix W. Recall
from section 9.9, W is a known weight matrix with zero elements along its diagonal. It could
be a contiguity matrix having elements 1 if its a neighboring region and zero otherwise. Usually
this is normalized such that each row sums to 1. Alternatively, W could be based on distances
from neighbors again normalized such that each row sums to 1. It is clear that the presence of
Wy as a regressor introduces endogeneity. Assuming (In &minus; ρW ) nonsingular, one can solve for
the reduced form model:
</p>
<p>y = (In &minus; ρW )&minus;1Xβ + ǫ&lowast;
</p>
<p>where ǫ&lowast; = (In &minus; ρW )&minus;1ǫ has mean zero and variance covariance matrix which has the same
form as (9.38), i.e.,
</p>
<p>Σ = E(ǫ&lowast;ǫ&lowast;&prime;) = σ2Ω = σ2(In &minus; ρW )&minus;1(In &minus; ρW &prime;)&minus;1
</p>
<p>For |ρ| &lt; 1, one obtains
</p>
<p>(In &minus; ρW )&minus;1 = In + ρW + ρ2W 2 + ρ3W 3 + ...
</p>
<p>Hence
</p>
<p>E(y/X) = (In &minus; ρW )&minus;1Xβ = Xβ + ρWXβ + ρ2W 2Xβ + ρ3W 3Xβ + ...
</p>
<p>This also means that
</p>
<p>E(Wy/X) = W (In &minus; ρW )&minus;1Xβ = WXβ + ρW 2Xβ + ρ2W 3Xβ + ρ3W 4Xβ + ...
</p>
<p>Based on this last expression, Kelejian and Robinson (1993) and Kelejian and Prucha (1998)
suggest the use of a subset of the following instrumental variables:
</p>
<p>{X,WX,W 2X,W 3X,W 4X, ...}
</p>
<p>Lee (2003) suggested using the optimal instrument matrix:
</p>
<p>{X,W (In &minus; ρ̂W )&minus;1Xβ̂}
</p>
<p>where the values for ρ̂ and β̂ are obtained from a first stage IV estimator, using {X,WX}
as instruments, possibly augmented with W 2X. Note that Lee&rsquo;s (2003) instruments involve
inverting a matrix of dimension n. Kelejian, et al. (2004) suggest an approximation based upon:
</p>
<p>{X,
r&sum;
</p>
<p>s=0
ρ̂sW s+1Xβ̂}
</p>
<p>where r, the highest order of this approximation depends upon the sample size, with r = o(n1/2).
In their Monte Carlo experiments, they set r = nc where c = 0.25, 0.35, and 0.45. This is a
natural application of 2SLS to deal with the problem of spatial lag dependence.</p>
<p/>
</div>
<div class="page"><p/>
<p>272 Chapter 11: Simultaneous Equations Model
</p>
<p>11.3 System Estimation: Three-Stage Least Squares
</p>
<p>If the entire simultaneous equations model is to be estimated, then one should consider system
estimators rather than single equation estimators. System estimators take into account the zero
restrictions in every equation as well as the variance-covariance matrix of the disturbances of
the whole system. One such system estimator is Three-Stage Least Squares (3SLS) where the
structural equations are stacked on top of each other, just like a set of SUR equations,
</p>
<p>y = Zδ + u (11.43)
</p>
<p>where
</p>
<p>y =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>y1
y2
:
yG
</p>
<p>⎤
⎥⎥⎦ ; Z =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>Z1 0 . . . 0
0 Z2 . . . 0
: . . . : . . .
0 . . . ZG
</p>
<p>⎤
⎥⎥⎦ ; δ =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>δ1
δ2
:
δG
</p>
<p>⎤
⎥⎥⎦ ; u =
</p>
<p>⎡
⎢⎢⎣
</p>
<p>u1
u2
:
uG
</p>
<p>⎤
⎥⎥⎦
</p>
<p>and u has zero mean and variance-covariance matrix Σ&otimes; IT , indicating the possible correlation
among the disturbances of the different structural equations. Σ = [σij ], with E(uiu
</p>
<p>&prime;
j) = σijIT ,
</p>
<p>for i, j = 1, 2, . . . , G. This &otimes; notation was used in Chapter 9 and defined in the Appendix
to Chapter 7. Problem 4 shows that premultiplying the i-th structural equation by X &prime; and
performing GLS on the transformed equation results in 2SLS. For the system given in (11.43),
the analogy is obtained by premultiplying by (IG&otimes;X &prime;), i.e., each equation byX &prime;, and performing
GLS on the whole system. The transformed error (IG &otimes; X &prime;)u has a zero mean and variance-
covariance matrix Σ&otimes; (X &prime;X). Hence, GLS on the entire system obtains
</p>
<p>δ̂GLS = {Z &prime;(IG &otimes;X)[Σ&minus;1 &otimes; (X &prime;X)&minus;1](IG &otimes;X &prime;)Z}&minus;1
</p>
<p>{Z &prime;(IG &otimes;X)[Σ&minus;1 &otimes; (X &prime;X)&minus;1](IG &otimes;X &prime;)y (11.44)
which upon simplifying yields
</p>
<p>δ̂GLS = {Z &prime;[Σ&minus;1 &otimes; PX ]Z}&minus;1{Z &prime;[Σ&minus;1 &otimes; PX ]y} (11.45)
</p>
<p>Σ has to be estimated to make this estimator operational. Zellner and Theil (1962), suggest
getting the 2SLS residuals for the i-th equation, say ûi = yi &minus; Ziδ̂i,2SLS and estimating Σ by
Σ̂ = [σ̂ij ] where
</p>
<p>σ̂ij = [û
&prime;
iûj/(T &minus; gi &minus; ki)1/2(T &minus; gj &minus; kj)1/2] for i, j = 1, 2, . . . , G.
</p>
<p>If Σ̂ is substituted for Σ in (11.45), the resulting estimator is called 3SLS:
</p>
<p>δ̂3SLS = {Z &prime;[Σ̂&minus;1 &otimes; PX ]Z}&minus;1{Z &prime;[Σ̂&minus;1 &otimes; PX ]y} (11.46)
</p>
<p>The asymptotic variance-covariance matrix of δ̂3SLS can be estimated by {Z &prime;[Σ̂&minus;1 &otimes; PX ]Z}&minus;1.
If the system of equations (11.43) is properly specified, 3SLS is more efficient than 2SLS. But
if say, the second equation is improperly specified while the first equation is properly specified,
then a system estimator like 3SLS will be contaminated by this misspecification whereas a single
equation estimator like 2SLS on the first equation is not. So, if the first equation is of interest
it does not pay to go to a system estimator in this case.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Test for Over-Identification Restrictions 273
</p>
<p>Two sufficient conditions exist for the equivalence of 2SLS and 3SLS, these are the following:
(i) Σ is diagonal, and (ii) every equation is just identified. Problem 5 leads you step by step
through these results. It is also easy to show, see problem 5, that a necessary and sufficient
condition for 3SLS to be equivalent to 2SLS on each equation is given by
</p>
<p>σijẐ &prime;iP̄Ẑj = 0 for i, j = 1, 2, . . . , G
</p>
<p>where Ẑi = PXZi, see Baltagi (1989). This is similar to the condition derived in the seemingly
unrelated regressions case except it involves the set of second stage regressors of 2SLS. One can
easily see that besides the two sufficient conditions given above, Ẑ &prime;iP̄Ẑj = 0 states that the set
</p>
<p>of second stage regressors of the i-th equation have to be a perfect linear combination of those
in the j-th equation and vice versa. A similar condition was derived by Kapteyn and Fiebig
(1981). If some equations in the system are over-identified while others are just-identified, the
3SLS estimates of the over-identified equations can be obtained by running 3SLS ignoring the
just-identified equations. The 3SLS estimates of each just-identified equation differ from those of
2SLS by a vector which is a linear function of the 3SLS residuals of the over-identified equations,
see Theil (1971) and problem 17.
</p>
<p>11.4 Test for Over-Identification Restrictions
</p>
<p>We emphasized instrument relevance, now we turn to instrument exogeneity. Under just-
identification, one cannot statistically test instruments for exogeneity. This choice of exogenous
instruments requires making an expert judgement based on knowledge of the empirical applica-
tion. However, if the first structural equation is over-identified, i.e., the number of instruments
ℓ is larger than the number of right hand side variables (g1 + k1), then one can test these over-
identifying restrictions. A likelihood ratio test for this over-identification condition based on
maximum likelihood procedures was given by Anderson and Rubin (1950). This version of the
test requires the computation of LIML. This was later modified by Basmann (1960) so that it
could be based on the 2SLS procedure. Here we present a simpler alternative based on Davidson
and MacKinnon (1993) and Hausman (1983). In essence, one is testing
</p>
<p>Ho; y1 = Z1δ1 + u1 versus H1; y1 = Z1δ1 +W
&lowast;γ + u1 (11.47)
</p>
<p>where u1 &sim; IID(0, σ11IT ). Let W be the matrix of instruments of full rank ℓ. Also, let W
&lowast; be a
</p>
<p>subset of instruments W , of dimension (ℓ&minus;k1&minus;g1), that are linearly independent of Ẑ1 = PWZ1.
In this case, the matrix [Ẑ1,W
</p>
<p>&lowast;] has full rank ℓ and therefore, spans the same space as W . A
test for over-identification is a test for γ = 0. In other words, W &lowast; has no ability to explain any
variation in y1 that is not explained by Z1 using the matrix of instruments W .
If W &lowast; is correlated with u1 or the first structural equation (11.34) is misspecified, say by Z1
</p>
<p>not including some variables in W &lowast;, then γ 	= 0. Hence, testing γ = 0 should be interpreted as a
joint test for the validity of the matrix of instruments W and the proper specification of (11.34)
see, Davidson and MacKinnon (1993). Testing Ho; γ = 0 can be obtained as an asymptotic
F -test as follows:
</p>
<p>(RRSS &lowast; &minus;URSS &lowast;)/(ℓ&minus; (g1 + k1))
URSS/(T &minus; ℓ) (11.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>274 Chapter 11: Simultaneous Equations Model
</p>
<p>This is asymptotically distributed as F (ℓ &minus; (g1 + k1), T &minus; ℓ) under Ho. Using instruments W,
we regress Z1 on W and get Ẑ1, then obtain the restricted 2SLS estimate δ̃1,2SLS by regressing
</p>
<p>y1 on Ẑ1. The restricted residual sum of squares from the second stage regression is RRSS
&lowast; =
</p>
<p>(y1&minus;Ẑ1δ̃1,2SLS)&prime;(y1&minus;Ẑ1δ̃1,2SLS). Next we regress y1 on Ẑ1 and W &lowast; to get the unrestricted 2SLS
estimates δ̂1,2SLS and γ̂2SLS . The unrestricted residual sum of squares from the second stage
</p>
<p>regression is URSS &lowast; = (y1 &minus; Ẑ1δ̂1,2SLS &minus; W &lowast;γ̂2SLS)&prime;(y1 &minus; Ẑ1δ̂1,2SLS &minus; W &lowast;γ̂2SLS). The URSS
in (11.48) is the 2SLS residuals sum of squares from the unrestricted model which is obtained
as follows (y1 &minus;Z1δ̂1,2SLS &minus;W &lowast;γ̂2SLS)&prime;(y1 &minus;Z1δ̂1,2SLS &minus;W &lowast;γ̂2SLS). URSS differs from URSS &lowast;
in that Z1 rather than Ẑ1 is used in obtaining the residuals. Note that this differs from the
Chow-test in that the denominator is not based on URSS &lowast;, see Wooldridge (1990).
</p>
<p>This test does not require the construction of W &lowast; for its implementation. This is because the
model under H1 is just-identified with as many regressor as there are instruments. This means
that its
</p>
<p>URSS &lowast; = y&prime;1P̄W y1 = y
&prime;
1y1 &minus; y&prime;1PW y1
</p>
<p>see problem 10. It is easy to show, see problem 12, that
</p>
<p>RRSS &lowast; = y&prime;1P̄Ẑ1y1 = y
&prime;
1y1 &minus; y&prime;1PẐ1y1
</p>
<p>where Ẑ1 = PWZ1. Hence,
</p>
<p>RRSS &lowast; &minus;URSS &lowast; = y&prime;1PW y1 &minus; y&prime;1PẐ1y1 (11.49)
</p>
<p>The test for over-identification can therefore be based on RRSS&lowast; &minus; URSS&lowast; divided by a con-
sistent estimate of σ11, say,
</p>
<p>σ̃11 = (y1 &minus; Z1δ̃1,2SLS)&prime;(y1 &minus; Z1δ̃1,2SLS)/T (11.50)
</p>
<p>Problem 12 shows that the resulting test statistic is exactly that proposed by Hausman (1983).
In a nutshell, the Hausman over-identification test regresses the 2SLS residuals y1 &minus; Z1δ̃1,2SLS
on the matrix W of all pre-determined variables in the model. The test statistic is T times the
uncentered R2 of this regression. See the Appendix to Chapter 3 for a definition of uncentered
R2. This test statistic is asymptotically distributed as χ2 with ℓ&minus; (g1 + k1) degrees of freedom.
Large values of this statistic reject the null hypothesis.
Alternatively, one can get this test statistic as a Gauss-Newton Regression (GNR) on the
</p>
<p>unrestricted model in (11.47). To see this, recall from section 8.4 that the GNR applies to a
general nonlinear model yt = xt(β) + ut. Using the set of instruments W , the GNR becomes
</p>
<p>y &minus; x(β̃) = PWX(β̃)b+ residuals
</p>
<p>where β̃ denotes the restricted instrumental variable estimate of β under the null hypothesis and
X(β) is the matrix of derivatives with typical elements Xij(β) = &part;xi(β)/&part;βj for j = 1, . . . , k.
Thus, the only difference between this GNR and that in Chapter 8 is that the regressors are
multiplied by PW , see Davidson and MacKinnon (1993, p. 226). Therefore, the GNR for (11.47)
yields
</p>
<p>y1 &minus; Z1δ̃1,2SLS = Ẑ1b1 +W &lowast;b2 + residuals (11.51)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Hausman&rsquo;s Specification Test 275
</p>
<p>since PW [Z1,W
&lowast;] = [Ẑ1,W &lowast;] and δ̃1,2SLS is the restricted estimator under Ho; γ = 0. But,
</p>
<p>[Ẑ1,W
&lowast;] spans the same space as W , see problem 12. Hence, the GNR in (11.51) is equivalent
</p>
<p>to running the 2SLS residuals on W and computing T times the uncentered R2 as described
above. Once again, it is clear that W &lowast; need not be constructed.
The basic intuition behind the test for over-identification restriction rests on the fact that
</p>
<p>one can compute several legitimate IV estimators if all these instruments are relevant and
exogenous. For example, suppose there are two instruments and one right hand side endogenous
variable. Then one can compute two IV estimators using each instrument separately. If these
IV estimators produce very different estimates, then may be one instrument or the other or
both are not exogenous. The over-identification test we just described implicitly makes this
comparison without actually computing all possible IV estimates. Exogenous instruments have
to be uncorrelated with the disturbances. This suggests that the 2SLS residuals have to be
uncorrelated with the instruments. This is the basis for the TR2u test statistic. If all the
instruments are exogenous, the regression coefficient estimates should all be not significantly
different from zero and the R2u should be low.
</p>
<p>11.5 Hausman&rsquo;s Specification Test3
</p>
<p>A critical assumption for the linear regression model y = Xβ + u is that the set of regressors
X are uncorrelated with the error term u. Otherwise, we have simultaneous bias and OLS is
inconsistent. Hausman (1978) proposed a general specification test for Ho; E(u/X) = 0 versus
H1; E(u/X) 	= 0. Two estimators are needed to implement this test. The first estimator must
be a consistent and efficient estimator of β under Ho which becomes inconsistent under H1. Let
us denote this efficient estimator under Ho by β̂o. The second estimator, denoted by β̂1, must
be consistent for β under both Ho and H1, but inefficient under Ho. Hausman&rsquo;s test is based
on the difference between these two estimators q̂ = β̂1 &minus; β̂o. Under Ho; plim q̂ is zero, while
under H1; plim q̂ 	= 0. Hausman (1978) shows that var(q̂) = var(β̂1)&minus; var(β̂o) and Hausman&rsquo;s
test becomes
</p>
<p>m = q̂&prime;[var(q̂)]&minus;1q̂ (11.52)
</p>
<p>which is asymptotically distributed under Ho as χ
2
k where k is the dimension of β.
</p>
<p>It remains to show that var(q̂) is the difference between the two variances. This can be
illustrated for a single regressor case without matrix algebra, see Maddala (1992, page 507).
First, one shows that cov(β̂o, q̂) = 0. To prove this, consider a new estimator of β defined as
̂̂
β = β̂o + λq̂ where λ is an arbitrary constant. Under Ho, plim
</p>
<p>̂̂
β = β for every λ and
</p>
<p>var(
̂̂
β) = var(β̂o) + λ
</p>
<p>2var(q̂) + 2λcov(β̂o, q̂)
</p>
<p>Since β̂o is efficient, var(
̂̂
β) &ge; var(β̂o) which means that λ2 var(q̂)+2λ cov(β̂o, q̂) &ge; 0 for every λ.
</p>
<p>If cov(β̂o, q̂) &gt; 0, then for λ = &minus;cov(β̂o, q̂)/var(q̂) the above inequality is violated. Similarly, if
cov(β̂o, q̂) &lt; 0, then for λ=-cov(β̂o, q̂)/var(q̂) the above inequality is violated. Therefore, under
Ho, for the above inequality to be satisfied for every λ, it must be the case that cov(β̂o, q̂) = 0.
Now, q̂ = β̂1&minus;β̂o can be rewritten as β̂1 = q̂+β̂o with var(β̂1) = var(q̂)+ var(β̂o)+2cov(q̂, β̂o).
</p>
<p>Using the fact that the last term is zero, we get the required result: var(q̂) = var(β̂1)&minus; var(β̂o).</p>
<p/>
</div>
<div class="page"><p/>
<p>276 Chapter 11: Simultaneous Equations Model
</p>
<p>Example 4: Consider the simple regression without a constant
</p>
<p>yt = βxt + ut t = 1, 2, . . . , T with ut &sim; IIN(0, σ
2)
</p>
<p>and where β is a scalar. Under Ho; E(ut/xt) = 0 and OLS is efficient and consistent. Under
H1; OLS is not consistent. Using wt as an instrumental variable, with wt uncorrelated with ut
and preferably highly correlated with xt, yields the following IV estimator of β:
</p>
<p>β̂IV =
&sum;T
</p>
<p>t=1 ytwt/
&sum;T
</p>
<p>t=1 xtwt = β +
&sum;T
</p>
<p>t=1wtut/
&sum;T
</p>
<p>t=1 xtwt
</p>
<p>with plim β̂IV = β under Ho and H1 and
</p>
<p>var(β̂IV ) = σ
2&sum;T
</p>
<p>t=1w
2
t /(
</p>
<p>&sum;T
t=1 xtwt)
</p>
<p>2 =
σ2
</p>
<p>&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>[
1
</p>
<p>r2xw
</p>
<p>]
</p>
<p>where r2xw = (
&sum;T
</p>
<p>t=1 xtwt)
2/
</p>
<p>&sum;T
t=1w
</p>
<p>2
t
</p>
<p>&sum;T
t=1 x
</p>
<p>2
t . Also,
</p>
<p>β̂OLS =
&sum;T
</p>
<p>t=1 xtyt/
&sum;T
</p>
<p>t=1 x
2
t = β +
</p>
<p>&sum;T
t=1 xtut/
</p>
<p>&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>with plim β̂OLS = β under Ho but plim β̂OLS 	= β under H1, with var(β̂OLS) = σ2/
&sum;T
</p>
<p>t=1 x
2
t .
</p>
<p>One can see that var(β̂OLS) &le; var(β̂IV ) since 0 &le; r2xw &le; 1. In fact for a weak IV, r2xw is small
and close to zero and the var(β̂IV ) blows up. Strong IV is where r
</p>
<p>2
xw is close to 1 and the closer
</p>
<p>is the var(β̂IV ) to var(β̂OLS). In this case, q̂ = β̂IV &minus; β̂OLS and plim q̂ 	= 0 under H1, while
plim q̂ = 0 under Ho, with
</p>
<p>var(q̂) = var(β̂IV )&minus; var(β̂OLS) =
σ2
</p>
<p>&sum;T
t=1 x
</p>
<p>2
t
</p>
<p>[
1
</p>
<p>r2xw
&minus; 1
</p>
<p>]
= var(β̂OLS)
</p>
<p>[
1&minus; r2xw
r2xw
</p>
<p>]
</p>
<p>Therefore, Hausman&rsquo;s test statistic is
</p>
<p>m = q̂2r2xw/var(β̂OLS)(1&minus; r2xw)
</p>
<p>which is asymptotically distributed as χ21 under Ho. Note that the same estimator of σ
2 is used
</p>
<p>for var(β̂IV ) and var(β̂OLS). This is the estimator of σ
2 obtained under Ho.
</p>
<p>The Hausman-test can also be obtained from the following augmented regression:
</p>
<p>yt = βxt + γx̂t + ǫt
</p>
<p>where x̂t is the predicted value of xt from regressing it on the instrumental variable wt. Problem
13 asks the reader to show that Hausman&rsquo;s test statistic can be obtained by testing γ = 0.
</p>
<p>The IV estimator assumes that wt and ut are uncorrelated. If this is violated in practice, then
the IV estimator is inconsistent and the asymptotic bias can be aggravated if in addition this
is a weak instrument. To see this,
</p>
<p>plimβ̂IV = β + plim(
&sum;T
</p>
<p>t=1wtut/
&sum;T
</p>
<p>t=1 xtwt) = β +
cov(wt, ut)
</p>
<p>cov(xt, wt)
= β +
</p>
<p>corr(wt, ut)
</p>
<p>corr(xt, wt)
</p>
<p>σx
σu
</p>
<p>where cov(wt, ut) and corr(xt, wt) denote the population covariance and correlation, respec-
tively. Also, σx and σu denote the population standard deviations. If corr(wt, ut) 	= 0, there is
an asymptotic bias in β̂IV . If corr(wt, ut) is small and the instrument is strong (with a large
corr(xt, wt)), this bias could be small. However, this bias could be large, even if corr(wt, ut) is</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Hausman&rsquo;s Specification Test 277
</p>
<p>small, in the case of weak instruments (small corr(xt, wt)). This warns researchers of using an
instrument which they deem much better than xt because it is less correlated with ut. If this
instrument is additionally weak, the bias of the resulting IV estimator will be enlarged due to
the smaller corr(xt, wt). In sum, weak instruments may have large asymptotic bias in practice
even if they are slightly correlated with the error term.
In matrix form, the Durbin-Wu-Hausman test for the first structural equation is based upon
</p>
<p>the difference between OLS and IV estimation of (11.34) using the matrix of instruments W .
In particular, the vector of contrasts is given by
</p>
<p>q̂ = δ̂1,IV &minus; δ̂1,OLS = (Z &prime;1PWZ1)&minus;1[Z &prime;1PW y1 &minus; (Z &prime;1PWZ1)(Z &prime;1Z1)&minus;1Z &prime;1y1] (11.53)
= (Z &prime;1PWZ1)
</p>
<p>&minus;1[Z &prime;1PW P̄Z1y1]
</p>
<p>Under the null hypothesis, q̂ = (Z &prime;1PWZ1)
&minus;1Z &prime;1PW P̄Z1u1. The test for q̂ = 0 can be based on the
</p>
<p>test for Z &prime;1PW P̄Z1u1 having mean zero asymptotically. This last vector is of dimension (g1+k1).
However, not all of its elements are necessarily random variables since P̄Z1may annihilate some
columns of the second stage regressors Ẑ1 = PWZ1. In fact, all the included X&rsquo;s which are part
of W , i.e., X1, will be annihilated by P̄Z1 . Only the g1 linearly independent variables Ŷ1 = PWY1
are not annihilated by P̄Z1 .
</p>
<p>Our test focuses on the vector Ŷ
&prime;
</p>
<p>1 P̄Z1u1 having mean zero asymptotically. Now consider the
artificial regression
</p>
<p>y1 = Z1δ1 + Ŷ1γ + residuals (11.54)
</p>
<p>Since [Z1, Ŷ1], [Z1, Ẑ1], [Z1, Z1 &minus; Ẑ1] and [Z1, Y1 &minus; Ŷ1] all span the same column space, this
regression has the same sum of squares residuals as
</p>
<p>y1 = Z1δ1 + (Y1 &minus; Ŷ1)η + residuals (11.55)
The DWH-test may be based on either of these regressions. It is equivalent to testing γ = 0
in (11.54) or η = 0 in (11.55) using an F -test. This is asymptotically distributed as F (g1, T &minus;
2g1 &minus; k1). Davidson and MacKinnon (1993, p. 239) warn about interpreting this test as one of
exogeneity of Y1 (the variables in Z1 not in the space spanned by W ). They argue that what
is being tested is the consistency of the OLS estimates of δ1, not that every column of Z1 is
independent of u1.
</p>
<p>In practice, one may be sure about using W2 as a set of IV&rsquo;s but is not sure whether some
r additional variables in Z1 are legitimate as instruments. The DWH-test in this case will be
based upon the difference between two IV estimators for δ1. The first is δ̂2,IV based on W2 and
</p>
<p>the second is δ̂1,IV based on W1. The latter set includes W2 and the additional r variables in Z1.
</p>
<p>δ̂2,IV &minus; δ̂1,IV = (Z &prime;1PW2Z1)&minus;1[Z &prime;1PW2y1 &minus; (Z &prime;1PW2Z1)(Z &prime;1PW1Z1)&minus;1Z &prime;1PW1y1] (11.56)
= (Z &prime;1PW2Z1)
</p>
<p>&minus;1Z &prime;1PW2(P̄PW1Z1 )y1
</p>
<p>since PW2PW1 = PW2 . The DWH-test is based on this contrast having mean zero asymptotically.
Once again this last vector has dimension g1 + k1 and not all its elements are necessarily
random variables since P̄PW1Z1 annihilates some columns of PW2Z1. This test can be based on
the following artificial regression:
</p>
<p>y1 = Z1δ1 + PW2Z
&lowast;
1γ + residuals (11.57)
</p>
<p>where PW2Z
&lowast;
1 consists of the r columns of PW2Z1 that are not annihilated by P̄W1 . Regression
</p>
<p>(11.57) is performed with W1 as the set of IV&rsquo;s and γ = 0 is tested using an F -test.</p>
<p/>
</div>
<div class="page"><p/>
<p>278 Chapter 11: Simultaneous Equations Model
</p>
<p>11.6 Empirical Examples
</p>
<p>Example 1: Crime in North Carolina. Cornwell and Trumbull (1994) estimated an economic
model of crime using data on 90 counties in North Carolina observed over the years 1981&ndash;87.
This data set is available on the Springer web site as CRIME.DAT. Here, we consider cross-
section data for 1987 and reconsider the full panel data set in Chapter 12. Table 11.2 gives the
OLS estimates relating the crime rate (which is an FBI index measuring the number of crimes
divided by the county population) to a set of explanatory variables. This was done using Stata.
All variables are in logs except for the regional dummies. The explanatory variables consist of the
probability of arrest (which is measured by ratio of arrests to offenses), probability of conviction
given arrest (which is measured by the ratio of convictions to arrests), probability of a prison
sentence given a conviction (measured by the proportion of total convictions resulting in prison
sentences); average prison sentence in days as a proxy for sanction severity. The number of police
per capita as a measure of the county&rsquo;s ability to detect crime, the population density which
is the county population divided by county land area, a dummy variable indicating whether
the county is in the SMSA with population larger than 50,000. Percent minority, which is the
</p>
<p>Table 11.2 Least Squares Estimates: Crime in North Carolina
</p>
<p>Source SS df MS Number of obs = 90
</p>
<p>F(20,69) = 19.71
Model 22.8072483 20 1.14036241 Prob &gt; F = 0.0000
Residual 3.99245334 69 .057861643 R-squared = 0.8510
</p>
<p>Adj R-squared = 0.8078
Total 26.7997016 89 .301120243 Root MSE = .24054
</p>
<p>lcrmrte Coef. Std. Err. t P &gt; |t| [95% Conf. Interval]
</p>
<p>lprbarr &minus;.4522907 .0816261 &minus;5.54 0.000 &minus;.6151303 &minus;.2894511
lprbconv &minus;.3003044 .0600259 &minus;5.00 0.000 &minus;.4200527 &minus;.180556
lprbpris &minus;.0340435 .1251096 &minus;0.27 0.786 &minus;.2836303 .2155433
lavgsen &minus;.2134467 .1167513 &minus;1.83 0.072 &minus;.4463592 .0194659
lpolpc .3610463 .0909534 3.97 0.000 .1795993 .5424934
ldensity .3149706 .0698265 4.51 0.000 .1756705 .4542707
lwcon .2727634 .2198714 1.24 0.219 &minus;.165868 .7113949
lwtuc .1603777 .1666014 0.96 0.339 &minus;.171983 .4927385
lwtrd .1325719 .3005086 0.44 0.660 &minus;.4669263 .7320702
lwfir &minus;.3205858 .251185 &minus;1.28 0.206 &minus;.8216861 .1805146
lwser &minus;.2694193 .1039842 &minus;2.59 0.012 &minus;.4768622 &minus;.0619765
lwmfg .1029571 .1524804 0.68 0.502 &minus;.2012331 .4071472
lwfed .3856593 .3215442 1.20 0.234 &minus;.2558039 1.027123
lwsta &minus;.078239 .2701264 &minus;0.29 0.773 &minus;.6171264 .4606485
lwloc &minus;.1774064 .4251793 &minus;0.42 0.678 &minus;1.025616 .670803
lpctymle .0326912 .1580377 0.21 0.837 &minus;.2825855 .3479678
lpctmin .2245975 .0519005 4.33 0.000 .1210589 .3281361
west &minus;.087998 .1243235 &minus;0.71 0.481 &minus;.3360167 .1600207
central &minus;.1771378 .0739535 &minus;2.40 0.019 &minus;.3246709 &minus;.0296046
urban &minus;.0896129 .1375084 &minus;0.65 0.517 &minus;.3639347 .184709
cons &minus;3.395919 3.020674 &minus;1.12 0.265 &minus;9.421998 2.630159</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Empirical Examples 279
</p>
<p>Table 11.3 Instrumental Variables (2SLS) Regression: Crime in North Carolina
</p>
<p>Source SS df MS Number of obs = 90
</p>
<p>F(20,69) = 17.35
Model 22.6350465 20 1.13175232 Prob &gt; F = 0.0000
Residual 4.16465515 69 .060357321 R-squared = 0.8446
</p>
<p>Adj R-squared = 0.7996
Total 26.7997016 89 .301120243 Root MSE = .24568
</p>
<p>lcrmrte Coef. Std. Err. t P &gt; |t| [95% Conf. Interval]
</p>
<p>lprbarr &minus;.4393081 .2267579 &minus;1.94 0.057 &minus;.8916777 .0130615
lpolpc .5136133 .1976888 2.60 0.011 .1192349 .9079918
lprbconv &minus;.2713278 .0847024 &minus;3.20 0.002 &minus;.4403044 &minus;.1023512
lprbpris &minus;.0278416 .1283276 &minus;0.22 0.829 &minus;.2838482 .2281651
lavgsen &minus;.280122 .1387228 &minus;2.02 0.047 &minus;.5568663 &minus;.0033776
ldensity .3273521 .0893292 3.66 0.000 .1491452 .505559
lwcon .3456183 .2419206 1.43 0.158 &minus;.137 .8282366
lwtuc .1773533 .1718849 1.03 0.306 &minus;.1655477 .5202542
lwtrd .212578 .3239984 0.66 0.514 &minus;.433781 .8589371
lwfir &minus;.3540903 .2612516 &minus;1.36 0.180 &minus;.8752731 .1670925
lwser &minus;.2911556 .1122454 &minus;2.59 0.012 &minus;.5150789 &minus;.0672322
lwmfg .0642196 .1644108 0.39 0.697 &minus;.263771 .3922102
lwfed .2974661 .3425026 0.87 0.388 &minus;.3858079 .9807402
lwsta .0037846 .3102383 0.01 0.990 &minus;.615124 .6226931
lwloc &minus;.4336541 .5166733 &minus;0.84 0.404 &minus;1.464389 .597081
lpctymle .0095115 .1869867 0.05 0.960 &minus;.3635166 .3825397
lpctmin .2285766 .0543079 4.21 0.000 .1202354 .3369179
west &minus;.0952899 .1301449 &minus;0.73 0.467 &minus;.3549219 .1643422
central &minus;.1792662 .0762815 &minus;2.35 0.022 &minus;.3314437 &minus;.0270888
urban &minus;.1139416 .143354 &minus;0.79 0.429 &minus;.3999251 .1720419
cons &minus;1.159015 3.898202 &minus;0.30 0.767 &minus;8.935716 6.617686
</p>
<p>Instrumented: lprbarr lpolpc
Instruments: lprbconv lprbpris lavgsen ldensity lwcon lwtuc lwtrd lwfir lwser lwmfg lwfed
</p>
<p>lwsta lwloc lpctymle lpctmin west central ltaxpc lmix
</p>
<p>proportion of the county&rsquo;s population that is minority or non-white. Percent young male which
is the proportion of the county&rsquo;s population that is males and between the ages of 15 and 24.
Regional dummies for western and central counties. Opportunities in the legal sector captured
by the average weekly wage in the county by industry. These industries are: construction;
transportation, utilities and communication; wholesale and retail trade; finance, insurance and
real estate; services; manufacturing; and federal, state and local government.
Results show that the probability of arrest as well as conviction given arrest have a negative
</p>
<p>and significant effect on the crime rate with estimated elasticities of &minus;0.45 and &minus;0.30 respec-
tively. The probability of imprisonment given conviction as well as the sentence severity have a
negative but insignificant effect on the crime rate. The greater the number of police per capita,
the greater the number of reported crimes per capita. The estimated elasticity is 0.36 and it is
significant. This could be explained by the fact that the larger the police force, the larger the
reported crime. Alternatively, this could be an endogeneity problem with more crime resulting</p>
<p/>
</div>
<div class="page"><p/>
<p>280 Chapter 11: Simultaneous Equations Model
</p>
<p>Table 11.4 Hausman&rsquo;s Test: Crime in North Carolina
</p>
<p>Coefficients
</p>
<p>(b) (B) (b&ndash;B) sqrt(diag(V b&ndash;V B))
b2sls bols Difference S.E.
</p>
<p>lprbarr &minus;.4393081 &minus;.4522907 .0129826 .2115569
lpolpc .5136133 .3610463 .152567 .1755231
lprbconv &minus;.2713278 &minus;.3003044 .0289765 .0597611
lprbpris &minus;.0278416 &minus;.0340435 .0062019 .0285582
lavgsen &minus;.280122 &minus;.2134467 &minus;.0666753 .0749208
ldensity .3273521 .3149706 .0123815 .0557132
lwcon .3456183 .2727634 .0728548 .1009065
lwtuc .1773533 .1603777 .0169755 .0422893
lwtrd .212578 .1325719 .0800061 .1211178
lwfir &minus;.3540903 &minus;.3205858 &minus;.0335045 .0718228
lwser &minus;.2911556 &minus;.2694193 &minus;.0217362 .0422646
lwmfg .0642196 .1029571 &minus;.0387375 .0614869
lwfed .2974661 .3856593 &minus;.0881932 .1179718
lwsta .0037846 &minus;.078239 .0820236 .1525764
lwloc &minus;.4336541 &minus;.1774064 &minus;.2562477 .293554
lpctymle .0095115 .0326912 &minus;.0231796 .0999404
lpctmin .2285766 .2245975 .0039792 .0159902
west &minus;.0952899 &minus;.087998 &minus;.0072919 .0384885
central &minus;.1792662 &minus;.1771378 &minus;.0021284 .0187016
urban &minus;.1139416 &minus;.0896129 &minus;.0243287 .0405192
</p>
<p>b = consistent under Ho and Ha; obtained from ivreg
</p>
<p>B = inconsistent under Ha, efficient under Ho; obtained from regress
</p>
<p>Test: Ho: difference in coefficients not systematic
</p>
<p>chi2(20) = (b&minus;B)&rsquo;[(V b&minus;V B)ˆ(&minus;1)](b&minus;B)
= 0.87
</p>
<p>Prob &gt; chi2 = 1.0000
</p>
<p>in the hiring of more police. The higher the density of the population the higher the crime rate.
The estimated elasticity is 0.31 and it is significant. Returns to legal activity are insignificant
except for wages in the service sector. This has a negative and significant effect on crime with
an estimated elasticity of &minus;0.27. Percent young male is insignificant, while percent minority
is positive and significant with an estimated elasticity of 0.22. The central dummy variable is
negative and significant while the western dummy variable is not significant. Also, the urban
dummy variable is insignificant. Cornwell and Trumbull (1994) worried about the endogeneity
of police per capita and the probability of arrest. They used as instruments two additional vari-
ables. Offense mix which is the ratio of crimes involving face to face contact (such as robbery,
assault and rape) to those that do not. The rationale for using this variable is that arrest is
facilitated by positive identification of the offender. The second instrument is per capita tax
revenue. This is justified on the basis that counties with preferences for law enforcement will
vote for higher taxes to fund a larger police force.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Empirical Examples 281
</p>
<p>The 2SLS estimates are reported in Table 11.3. The probability of arrest has an estimated
elasticity of &minus;0.44 but now with a p-value of 0.057. The probability of conviction given arrest
has an estimated elasticity of &minus;0.27 still significant. The probability of imprisonment given
conviction is still insignificant while the sentence severity is now negative and significant with
an estimated elasticity of &minus;0.28. Police per capita has a higher elasticity of 0.51 still signif-
icant. The remaining estimates are slightly affected. In fact, the Hausman test based on the
difference between the OLS and 2SLS estimates is shown in Table 11.4. This is computed us-
ing Stata and it contrasts 20 slope coefficient estimates. The Hausman test statistic is 0.87
and is asymptotically distributed as χ220. This is insignificant, and shows that the 2SLS and
OLS estimates are not significantly different given this model specification and the specific
choice of instruments. Note that this is a just-identified equation and one cannot test for over-
identification.
Note that the 2sls estimates and the Hausman test are not robust to heteroskedasticity.
</p>
<p>Table 11.5 gives the 2sls estimates by running ivregress in Stata with the robust variance-
covariance matrix option.
Note that the estimates remain the same as Table 11.3 but the standard errors for the right
</p>
<p>hand side endogenous variables Y1 are now larger. The option (estat endogenous) generates the
Hausman test which tests whether 2sls is different from OLS based now on the robust variance-
covariance estimate. The F(2,67) statistic observed is 0.455 and has a p-value 0.636 which is
not significant. This F-statistic could have been generated by an artificial regression as follows:
Obtain the residuals from the first stage regressions, see Tables 11.6 and 11.7 below. Call these
residuals v1hat and v2hat. Include them as additional variables in the original equation and run
robust least squares. The robust Hausman test is equivalent to testing that the coefficients of
these two residuals are jointly zero. The Stata commands (without showing the output of this
artificial regression) and the resulting test statistic are shown below:
</p>
<p>. quietly regress lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc ldensity
</p>
<p>lwcon lwtuc lwtrd lwfir lwser lwmfg lwfed lwsta lwloc lpctymle lpctmin west
</p>
<p>central urban v1hat v2hat if year==87, vce(robust)
</p>
<p>. test v1hat v2hat
</p>
<p>(1) v1hat = 0
</p>
<p>(2) v2hat = 0
</p>
<p>F(2,67) = 0.46
</p>
<p>Prob &gt; F = 0.6361
</p>
<p>This is the same statistic obtained above with estat endogenous.
Tables 11.6 and 11.7 give the first-stage regressions for the probability of arrest and police
</p>
<p>per capita. The R2 of these regressions are 0.47 and 0.56, respectively. The F -statistics for the
significance of all slope coefficients are 3.11 and 4.42, respectively. The additional instruments
(offense mix and per capita tax revenue) are jointly significant in both regressions yielding F -
statistics of 5.78 and 10.56 with p-values of 0.0048 and 0.0001, respectively. Although there are
two right hand side endogenous regressors in the crime equation rather than one, the Stock and
Watson &lsquo;rule of thumb&rsquo; suggest that these instruments may be weak.</p>
<p/>
</div>
<div class="page"><p/>
<p>282 Chapter 11: Simultaneous Equations Model
</p>
<p>Table 11.5 Robust Variance Covariance (2SLS) Regression: Crime in North Carolina
</p>
<p>Instrumental variables (2SLS) regression Number of obs = 90
Wald chi2(20) = 1094.07
Prob &gt; chi2 = 0.0000
R-squared = 0.8446
Root MSE = .21511
</p>
<p>Robust
lcrmrte Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
</p>
<p>lprbarr &ndash;.4393081 .311466 &ndash;1.41 0.158 &ndash;1.04977 .1711541
lpolpc .5136133 .2483426 &ndash;2.07 0.039 .0268707 1.000356
</p>
<p>lprbconv &ndash;.2713278 .1138502 &ndash;2.38 0.017 &ndash;.4944701 &ndash;.0481855
lprbpris &ndash;.0278416 .1339361 &ndash;0.21 0.835 &ndash;.2903516 .2346685
lavgsen &ndash;.280122 .1204801 &ndash;2.33 0.020 &ndash;.5162587 &ndash;.0439852
ldensity .3273521 .0983388 3.33 0.001 .1346116 .5200926
lwcon .3456183 .1961291 1.76 0.078 &ndash;.0387877 .7300243
lwtuc .1773533 .1942597 0.91 0.361 &ndash;.2033887 .5580952
lwtrd .212578 .2297782 0.93 0.355 &ndash;.2377789 .6629349
lwfir &ndash;.3540903 .2299624 &ndash;1.54 0.124 &ndash;.8048082 .0966276
lwser &ndash;.2911556 .0865243 &ndash;3.37 0.001 &ndash;.4607401 &ndash;.121571
lwmfg .0642196 .1459929 0.44 0.660 &ndash;.2219213 .3503605
lwfed .2974661 .3089013 0.96 0.336 &ndash;.3079692 .9029015
lwsta .0037846 .2861629 0.01 0.989 &ndash;.5570843 .5646535
lwloc &ndash;.4336541 .4840087 &ndash;0.90 0.370 &ndash;1.382294 .5149856
</p>
<p>lpctymle .0095115 .2232672 0.04 0.966 &ndash;.4280842 .4471073
lpctmin .2285766 .0531983 4.30 0.000 .1243099 .3328434
</p>
<p>west &ndash;.0952899 .1293715 &ndash;0.74 0.461 &ndash;.3488534 .1582736
central &ndash;.1792662 .0651109 &ndash;2.75 0.006 &ndash;.3068813 &ndash;.0516512
urban &ndash;.1139416 .1065919 &ndash;1.07 0.285 &ndash;.3228579 .0949747
cons &ndash;1.159015 3.791608 &ndash;0.31 0.760 &ndash;8.59043 6.2724
</p>
<p>Instrumented: lprbarr lpolpc
Instruments: lprbconv lprbpris lavgsen ldensity lwcon lwtuc lwtrd lwfir lwser lwmfg lwfed lwsta
</p>
<p>lwloc lpctymle lpctmin west central urban ltaxpc lmix
</p>
<p>One could obtain a lot of diagnostics on weak instruments after (ivregress 2sls) in Stata by
issuing the command (estat firststage). This is done in Table 11.8. The option forcenonrobust
is forcing these diagnostics to be done for a robust regression where the econometric theory
behind their derivation need not apply.
This is a just-identified equation, so we cannot test over-identification. We have already seen
</p>
<p>the R-squared of the first stage regressions (0.47 and 0.56). These are not low enough to flag
possible weak instruments. But these R-squared measures may be due mostly to the inclusion
of the right hand side exogenous variables X1. We want to know the additional contribution
of the instruments X2 = (ltaxpc lmix) over and above X1. The partial R-squared provide such
measures and yield lower numbers. For lprbarr, this is 0.32. This is the correlation between
lprbarr and the instruments X2 = (ltaxpc lmix) after including the right hand side exogenous
variables X1. The F(2,69) statistic tests the joint significance of the excluded instruments X2
in the first stage regressions. These F-statistics of 6.58 and 6.68 are certainly less than 10.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Empirical Examples 283
</p>
<p>Table 11.6 First Stage Regression: Probability of Arrest
</p>
<p>Source SS df MS Number of obs = 90
</p>
<p>F(20,69) = 3.11
Model 6.84874028 20 0.342437014 Prob &gt; F = 0.0002
Residual 7.59345096 69 0.110050014 R-squared = 0.4742
</p>
<p>Adj R-squared = 0.3218
Total 14.4421912 89 0.162271812 Root MSE = 0.33174
</p>
<p>lprbarr Coef. Std. Err. t P &gt; |t| [95% Conf. Interval]
</p>
<p>lprbconv &ndash;.1946392 .0877581 &ndash;2.22 0.030 &ndash;.3697119 &ndash;.0195665
lprbpris &ndash;.0240173 .1732583 &ndash;0.14 0.890 &ndash;.3696581 .3216236
lavgsen .1565061 .1527134 1.02 0.309 &ndash;.1481488 .4611611
ldensity &ndash;.2211654 .0941026 &ndash;2.35 0.022 &ndash;.408895 &ndash;.0334357
lwcon &ndash;.2024569 .3020226 &ndash;0.67 0.505 &ndash;.8049755 .4000616
lwtuc &ndash;.0461931 .230479 &ndash;0.20 0.842 &ndash;.5059861 .4135999
lwtrd .0494793 .4105612 0.12 0.904 &ndash;.769568 .8685266
lwfir .050559 .3507405 0.14 0.886 &ndash;.6491492 .7502671
lwser .0551851 .1500094 0.37 0.714 &ndash;.2440754 .3544456
lwmfg .0550689 .2138375 0.26 0.798 &ndash;.3715252 .481663
lwfed .2622408 .4454479 0.59 0.558 &ndash;.6264035 1.150885
lwsta &ndash;.4843599 .3749414 &ndash;1.29 0.201 &ndash;1.232347 .2636277
lwloc .7739819 .5511607 1.40 0.165 &ndash;.3255536 1.873517
lpctymle &ndash;.3373594 .2203286 &ndash;1.53 &ndash;0.130 .776903 .1021842
lpctmin &ndash;.0096724 .0729716 &ndash;0.13 &ndash;0.895 .1552467 .1359019
west .0701236 .1756211 0.40 0.691 &ndash;.280231 .4204782
central .0112086 .1034557 0.11 0.914 &ndash;.1951798 .217597
urban &ndash;.0150372 .2026425 &ndash;0.07 0.941 &ndash;.4192979 .3892234
ltaxpc &ndash;.1938134 .1755345 &ndash;1.10 0.273 &ndash;.5439952 .1563684
lmix .2682143 .0864373 3.10 0.003 .0957766 .4406519
cons &ndash;4.319234 3.797113 &ndash;1.14 0.259 &ndash;11.89427 3.255799
</p>
<p>This is the rule of thumb suggested by Stock and Watson for the case of one right hand side
endogenous variable. Note that we computed these statistics above but without the robust
variance-covariance matrix option. Shea&rsquo;s partial R-squared of 0.135 for lprbarr is the R-squared
from running a regression of residuals on residuals. The first residuals come from regressing
lprbarr on the right hand side included exogenous variablesX1. The second set of residuals come
from regressing the right hand side included exogenous variables X1 on the set of instruments
X2 = (ltaxpc lmix).
</p>
<p>Because we have more than one right hand side endogenous variable, Stock and Yogo (2005)
suggest using the minimum eigenvalue of a matrix analog of the F-statistic originally proposed
by Cragg and Donald (1993) to test for identification. A low minimum eigenvalue statistic
indicate weak instruments. If there is only one right hand side endogenous variable, this reverts
back to the F-statistic which is compared to 10 by the ad hoc rule of Stock and Watson. The
critical values for this minimum eigenvalue statistic are dependent on how much relative bias
we are willing to tolerate relative to OLS in case of weak instruments. This is available only
when the degree of over-identification is two or more. This is why Stata does not report it in
this just-identified example. However, Stata does report a second test which applies even for the</p>
<p/>
</div>
<div class="page"><p/>
<p>284 Chapter 11: Simultaneous Equations Model
</p>
<p>Table 11.7 First Stage Regression: Police per Capita
</p>
<p>Source SS df MS Number of obs = 90
</p>
<p>F(20,69) = 4.42
Model 6.99830344 20 0.349915172 Prob &gt; F = 0.0000
Residual 5.46683312 69 0.079229465 R-squared = 0.5614
</p>
<p>Adj R-squared = 0.4343
Total 12.4651366 89 0.140057714 Root MSE = 0.28148
</p>
<p>lpolpc Coef. Std. Err. t P &gt; |t| [95% Conf. Interval]
</p>
<p>lprbconv .0037744 .0744622 0.05 0.960 &ndash;.1447736 .1523223
lprbpris &ndash;.0487064 .1470085 &ndash;0.33 0.741 &ndash;.3419802 .2445675
lavgsen .3958972 .1295763 3.06 0.003 .1373996 .6543948
ldensity .0201292 .0798454 0.25 0.802 &ndash;.1391581 .1794165
lwcon &ndash;.5368469 .2562641 &ndash;2.09 0.040 &ndash;1.04808 &ndash;.025614
lwtuc &ndash;.0216638 .1955598 &ndash;0.11 0.912 &ndash;.411795 .3684674
lwtrd &ndash;.4207274 .3483584 &ndash;1.21 0.231 &ndash;1.115683 .2742286
lwfir .0001257 .2976009 0.00 1.000 &ndash;.5935718 .5938232
lwser .0973089 .1272819 0.76 0.447 &ndash;.1566116 .3512293
lwmfg .1710295 .1814396 0.94 0.349 &ndash;.1909327 .5329916
lwfed .8555422 .3779595 2.26 0.027 .1015338 1.609551
lwsta &ndash;.1118764 .3181352 &ndash;0.35 0.726 &ndash;.7465387 .5227859
lwloc 1.375102 .4676561 2.94 0.004 .4421535 2.30805
lpctymle .4186939 .1869473 2.24 0.028 .0457442 .7916436
lpctmin &ndash;.0517966 .0619159 &ndash;0.84 0.406 &ndash;.1753154 .0717222
west .1458865 .1490133 0.98 0.331 &ndash;.151387 .4431599
central .0477227 .0877814 0.54 0.588 &ndash;.1273964 .2228419
urban &ndash;.1192027 .1719407 &ndash;0.69 0.490 &ndash;.4622151 .2238097
ltaxpc .5601989 .1489398 3.76 0.000 .2630721 .8573258
lmix .2177256 .0733414 2.97 0.004 .0714135 .3640378
cons &ndash;16.33148 3.221824 &ndash;5.07 0.000 &ndash;22.75884 &ndash;9.904113
</p>
<p>just-identified case. This is based on size distortions of the Wald test for the joint significance of
the right hand side endogenous variables Y1 at the 5% level. The observed minimum eigenvalue
statistic of 5.31 is between the critical values of 7.03 and 4.58 and indicates a 2SLS relative bias
of more than 10% and less than 15% when it should be 5%.
</p>
<p>Example 2: Growth and Inequality Reconsidered. Lundberg and Squire (2003) estimate a two
equation model of growth and inequality using 3SLS, see section 10.5 for the SUR specification
where all the explanatory variables were assumed to be exogenous. The first equation relates
Growth (dly) to education (adult years schooling: yrt), the share of government consumption in
GDP (gov), M2/GDP (m2y), Inflation (inf), Sachs-Warner measure of openness (swo), changes
in the terms of trade (dtot), initial income (f pcy), dummy for 1980s (d80) and dummy for
1990s (d90). The second equation relates the Gini coefficient (gih) to education, M2/GDP,
civil liberties index (civ), mean land Gini (mlg), mean land Gini interacted with a dummy for
developing countries (mlgldc). The data contains 119 observations for 38 countries over the
period 1965-1990, and can be obtained from
</p>
<p>http://www.res.org.uk/economic/datasets/datasetlist.asp.</p>
<p/>
</div>
<div class="page"><p/>
<p>Notes 285
</p>
<p>Table 11.8 Weak IV Diagnostics: The Crime Example
</p>
<p>. estat firststage, forcenonrobust all
First-stage regression summary statistics
</p>
<p>Adjusted Partial Robust
Variable R-sq. R-sq. R-sq. F (2, 69) Prob &gt; F
</p>
<p>lprbarr 0.4742 0.3218 0.1435 6.57801 0.0024
lpolpc 0.5614 0.4343 0.2344 6.68168 0.0022
</p>
<p>Shea&rsquo;s partial R-squared
</p>
<p>Shea&rsquo;s Shea&rsquo;s
Variable Partial R-sq. Adj. Partial R-sq.
</p>
<p>lprbarr 0.1352 &ndash;0.0996
lpolpc 0.2208 0.0093
</p>
<p>Minimum eigenvalue statistic = 5.31166
</p>
<p>Critical Values # of endogenous regressors: 2
Ho: Instruments are weak # of excluded instruments: 2
</p>
<p>5% 10% 20% 30%
</p>
<p>2SLS relative bias (not available)
</p>
<p>10% 15% 20% 25%
</p>
<p>2SLS Size of nominal 5% Wald test 7.03 4.58 3.95 3.63
LIML Size of nominal 5% Wald test 7.03 4.58 3.95 3.63
</p>
<p>Education, government, M2/GDP, inflation, Sachs-Warner measure of openness, civil liberties
index, mean land Gini, mean land Gini interacted with a dummy for developing countries(ldc)
are assumed to be endogenous. Instruments include initial values of all variables (except land
Gini and income), population, urban share, life expectancy, fertility, initial female literacy and
democracy, arable area, dummies for oil and non-oil commodity exporters, and legal origin.
Table 11.9 reports the 3SLS estimates of these two equations using the reg3 command in Stata.
The results replicate those reported in Table 1 of Lundberg and Squire (2003, p. 334). Allowing
for endogeneity, these results still show that openness enhances growth and education reduces
inequality.
</p>
<p>Notes
</p>
<p>1. A heteroskedasticity-robust statistic is recommended especially if y2 has discrete characteristics.
</p>
<p>2. Why 10? See the proof in Appendix 10.4 of Stock and Watson (2003).
</p>
<p>3. This test is also known as the Durbin-Wu-Hausman test, following the work of Durbin (1954), Wu
(1973) and Hausman (1978).</p>
<p/>
</div>
<div class="page"><p/>
<p>286 Chapter 11: Simultaneous Equations Model
</p>
<p>Table 11.9 Growth and Inequality: 3SLS Estimates
</p>
<p>reg3 (Growth: dly = yrt gov m2y inf swo dtot f pcy d80 d90) (Inequality: gih = yrt m2y civ mlg
mlgldc), exog(commod f civ f dem f dtot f flit f gov f inf f m2y f swo f yrt pop urb lex lfr marea oil
legor fr legor ge legor mx legor sc legor uk) endog(yrt gov m2y inf swo civ mlg mlgldc)
</p>
<p>Three-stage least-squares regression
</p>
<p>Equation Obs Parms RMSE &ldquo;R-sq&rdquo; chi2 P
</p>
<p>Growth 119 9 2.34138 0.3905 65.55 0.0000
Inequality 119 5 7.032975 0.4368 94.28 0.0000
</p>
<p>Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
Growth
</p>
<p>yrt &ndash;.0280625 .1827206 &ndash;0.15 0.878 &ndash;.3861882 .3300632
gov &ndash;.0533221 .0447711 &ndash;1.19 0.234 &ndash;.1410718 .0344276
m2y .0085368 .0199759 0.43 0.669 &ndash;.0306152 .0476889
inf &ndash;.0008174 .0025729 &ndash;0.32 0.751 &ndash;.0058602 .0042254
swo 4.162776 .9499015 4.38 0.000 2.301003 6.024548
dtot 26.03736 23.05123 1.13 0.259 &ndash;19.14221 71.21694
f pcy &ndash;1.38017 .5488437 &ndash;2.51 0.012 &ndash;2.455884 &ndash;.3044564
d80 &ndash;1.560392 .545112 &ndash;2.86 0.004 &ndash;2.628792 &ndash;.4919922
d90 &ndash;3.413661 .6539689 &ndash;5.22 0.000 &ndash;4.695417 &ndash;2.131906
cons 13.00837 3.968276 3.28 0.001 5.230693 20.78605
</p>
<p>Inequality
yrt &ndash;1.244464 .4153602 &ndash;3.00 0.003 &ndash;2.058555 &ndash;.4303731
</p>
<p>m2y &ndash;.120124 .0581515 &ndash;2.07 0.039 &ndash;.2340989 &ndash;.0061492
civ .2531189 .7277433 0.35 0.728 &ndash;1.173232 1.67947
mlg .292672 .0873336 3.35 0.001 .1215012 .4638428
</p>
<p>mlgldc &ndash;.0547843 .0576727 &ndash;0.95 0.342 &ndash;.1678207 .0582522
cons 33.13231 5.517136 6.01 0.000 22.31893 43.9457
</p>
<p>Endogenous variables: dly gih yrt gov m2y inf swo civ mlg mlgldc
Exogenous variables: dtot f pcy d80 d90 commod f civ f dem f dtot f flit f gov f inf f m2y f swo
</p>
<p>f yrt pop urb lex lfr marea oil legor fr legor ge legor mx legor sc legor uk
</p>
<p>Problems
</p>
<p>1. The Inconsistency of OLS. Show that the OLS estimator of δ in (11.14), which can be written as
</p>
<p>δ̂OLS =
&sum;T
</p>
<p>t=1 ptqt/
&sum;T
</p>
<p>t=1 p
2
t
</p>
<p>is not consistent for δ. Hint: Write δ̂OLS = δ +
&sum;T
</p>
<p>t=1 pt(u2t &minus; ū2)/
&sum;T
</p>
<p>t=1 p
2
t , and use (11.18) to
</p>
<p>show that
</p>
<p>plim δ̂OLS = δ + (σ12 &minus; σ22)(δ &minus; β)/[σ11 + σ22 &minus; 2σ12].
</p>
<p>2. When Is the IV Estimator Consistent? Consider equation (11.30) and let X3 and X4 be the only
two other exogenous variables in this system.
</p>
<p>(a) Show that a two-stage estimator which regresses y2 on X1, X2 and X3 to get y2 = ŷ2 + v̂2,
and y3 on X1, X2 and X4 to get y3 = ŷ3 + v̂3, and then regresses y1 on ŷ2, ŷ3 and X1 and</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 287
</p>
<p>X2 does not necessarily yield consistent estimators. Hint: Show that the composite error is
ǫ1 = (u1+α12v̂2+α13v̂3) and
</p>
<p>&sum;T
t=1 ǫ̂1tŷ2t 	= 0, because
</p>
<p>&sum;T
t=1 ŷ2tv̂3t 	= 0. The latter does not
</p>
<p>hold because
&sum;T
</p>
<p>t=1 X3tv̂3t 	= 0. (This shows that if both y&rsquo;s are not regressed on the same
set of X&rsquo;s, the resulting two stage regression estimates are not consistent).
</p>
<p>(b) Show that the two-stage estimator which regresses y2 and y3 on X2, X3 and X4 to get
y2 = ŷ2 + v̂2 and y3 = ŷ3 + v̂3 and then regresses y1 on ŷ2, ŷ3 and X1 and X2 is not
necessarily consistent. Hint: Show that the composite error term ǫ1 = u1 + α12v̂2 + α13v̂3
does not satisfy
</p>
<p>&sum;T
t=1 ǫ̂1tX1t = 0, since
</p>
<p>&sum;T
t=1 v̂2tX1t 	= 0 and
</p>
<p>&sum;T
t=1 v̂3tX1t 	= 0. (This shows
</p>
<p>that if one of the included X&rsquo;s is not included in the first stage regression, then the resulting
two-stage regression estimates are not consistent).
</p>
<p>3. Just-Identification and Simple IV. If equation (11.34) is just-identified, then X2 is of the same
dimension as Y1, i.e., both are T &times;g1. Hence, Z1 is of the same dimension as X, both of dimension
T &times; (g1 + k1). Therefore, X &prime;Z1 is a square nonsingular matrix of dimension (g1 + k1). Hence,
(Z &prime;1X)
</p>
<p>&minus;1 exists. Using this fact, show that δ̂1,2SLS given by (11.36) reduces to (X &prime;Z1)&minus;1X &prime;y1.
This is exactly the IV estimator with W = X, given in (11.41). Note that this is only feasible if
X &prime;Z1 is square and nonsingular.
</p>
<p>4. 2SLS Can Be Obtained as GLS. Premultiply equation (11.34) byX &prime; and show that the transformed
disturbances X &prime;u1 &sim; (0, σ11
(X &prime;X)). Perform GLS on the resulting transformed equation and show that δ̂1,GLS is δ̂1,2SLS ,
given by (11.36).
</p>
<p>5. The Equivalence of 3SLS and 2SLS.
</p>
<p>(a) Show that δ̂3SLS given in (11.46) reduces to δ̂2SLS , when (i) Σ is diagonal, or (ii) every equa-
</p>
<p>tion in the system is just-identified. Hint: For (i); show that Σ̂&minus;1&otimes;PX is block-diagonal with
the i-th block consisting of PX/σ̂ii. Also, Z is block-diagonal, therefore, {Z &prime;[Σ̂&minus;1&otimes;PX ]Z}&minus;1
is block-diagonal with the i-th block consisting of σ̂ii(Z
</p>
<p>&prime;
iPXZi)
</p>
<p>&minus;1. Similarly, computing
</p>
<p>Z &prime;[Σ̂&minus;1 &otimes; PX ]y, one can show that the i-th element of δ̂3SLS is (Z &prime;iPXZi)&minus;1Z &prime;iPXyi =
δ̂i,2SLS . For (ii); show that Z
</p>
<p>&prime;
iX is square and nonsingular under just-identification. There-
</p>
<p>fore, δ̂i,2SLS = (X
&prime;Zi)&minus;1X &prime;yi from problem 3. Also, from (11.44), we get
</p>
<p>δ̂3SLS = {diag[Z &prime;iX](Σ̂&minus;1 &otimes; (X &prime;X)&minus;1)diag[X &prime;Zi]}&minus;1
</p>
<p>{diag[Z &prime;iX](Σ̂&minus;1 &otimes; (X &prime;X)&minus;1)(IG &otimes;X &prime;)y}.
</p>
<p>Using the fact that Z &prime;iX is square, one can show that δ̂i,3SLS = (X
&prime;Zi)&minus;1X &prime;yi.
</p>
<p>(b) Premultiply the system of equations in (11.43) by (IG &otimes; PX) and let y&lowast; = (IG &otimes; PX)y,
Z&lowast; = (IG &otimes; PX)Z and u&lowast; = (IG &otimes; PX)u, then y&lowast; = Z&lowast;δ + u&lowast;.Show that OLS on this
transformed model yields 2SLS on each equation in (11.43). Show that GLS on this model
yields 3SLS (knowing the true Σ) given in (11.45). Note that var(u&lowast;) = Σ &otimes; PX and its
generalized inverse is Σ&minus;1&otimes;PX . Use the Milliken and Albohali condition for the equivalence
of OLS and GLS given in equation (9.7) of Chapter 9 to deduce that 3SLS is equivalent
to 2SLS if Z&lowast;&prime;(Σ&minus;1 &otimes; PX)P̄Z&lowast; = 0. Show that this reduces to the following necessary and
sufficient condition σijẐ &prime;iP̄Ẑj = 0 for i 	= j, see Baltagi (1989). Hint: Use the fact that
</p>
<p>Z&lowast; = diag[PXZi] = diag[Ẑi] and P̄Z&lowast; = diag[P̄Ẑi ].
</p>
<p>Verify that the two sufficient conditions given in part (a) satisfy this necessary and sufficient
condition.</p>
<p/>
</div>
<div class="page"><p/>
<p>288 Chapter 11: Simultaneous Equations Model
</p>
<p>6. Consider the following demand and supply equations:
</p>
<p>Q = a&minus; bP + u1
Q = c+ dP + eW + fL+ u2
</p>
<p>where W denotes weather conditions affecting supply, and L denotes the supply of immigrant
workers available at harvest time.
</p>
<p>(a) Write this system in the matrix form given by equation (A.1) in the Appendix.
</p>
<p>(b) What does the order-condition for identification say about these two equations?
</p>
<p>(c) Premultiply this system by a nonsingular matrix F = [fij ], for i, j = 1, 2. What restrictions
must the matrix F satisfy if the transformed model is to satisfy the same restrictions of the
original model? Show that the first row of F is in fact the first row of an identity matrix, but
the second row of F is not the second row of an identity matrix. What do you conclude?
</p>
<p>7. Answer the same questions in problem 6 for the following model:
</p>
<p>Q = a&minus; bP + cY + dA+ u1
Q = e+ fP + gW + hL+ u2
</p>
<p>where Y is real income and A is real assets.
</p>
<p>8. Consider example (A.1) in the Appendix. Recall, that system of equations (A.3) and (A.4) are
just-identified.
</p>
<p>(a) Construct φ for the demand equation (A.3) and show that Aφ = (0,&minus;f)&prime; which is of rank
1 as long as f 	= 0. Similarly, construct φ for the supply equation (A.4) and show that
Aφ = (&minus;c, 0)&prime; which is of rank 1 as long as c 	= 0.
</p>
<p>(b) Using equation (A.17), show how the structural parameters can be retrieved from the reduced
form parameters. Derive the reduced form equations for this system and verify the above
relationships relating the reduced form and structural form parameters.
</p>
<p>9. Derive the reduced form equations for the model given in problem 6, and show that the structural
parameters of the second equation cannot be derived from the reduced form parameters. Also, show
that there are more than one way of expressing the structural parameters of the first equation in
terms of the reduced form parameters.
</p>
<p>10. Just-Identified Model. Consider the just-identified equation
</p>
<p>y1 = Z1δ1 + u1
</p>
<p>with W , the matrix of instruments for this equation of dimension T &times; ℓ where ℓ = g1 + k1 the
dimension of Z1. In this case, W
</p>
<p>&prime;Z1 is square and nonsingular.
</p>
<p>(a) Show that the generalized instrumental variable estimator given below (11.41) reduces to the
simple instrumental variable estimator given in (11.38).
</p>
<p>(b) Show that the minimized value of the criterion function for this just-identified model is zero,
</p>
<p>i.e., show that (y1 &minus; Z1δ̂1,IV )&prime;PW (y1 &minus; Z1δ̂1,IV ) = 0.
(c) Conclude that the residual sum of squares of the second stage regression of this just-identified
</p>
<p>model is the same as that obtained by regressing y1 on the matrix of instruments W , i.e.,
show that (y1 &minus; Ẑ1δ̂1,IV )&prime;(y1 &minus; Ẑ1δ̂1,IV ) = y&prime;1P̄W y1 where Ẑ1 = PWZ1. Hint: Show that
PẐ1 = PPW Z1 = PW , under just-identification.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 289
</p>
<p>11. The More Valid Instruments the Better. Let W1 and W2 be two sets of instrumental variables for
the first structural equation given in (11.34). Suppose that W1 is spanned by the space of W2.
Verify that the resulting IV estimator of δ1 based on W2 is at least as efficient as that based
on W1. Hint: Show that PW2W1 = W1 and that PW2 &minus; PW1 is idempotent. Conclude that the
difference in the corresponding asymptotic covariances of these IV estimators is positive semi-
definite. (This shows that increasing the number of legitimate instruments should improve the
asymptotic efficiency of an IV estimator).
</p>
<p>12. Testing for Over-Identification. In testing Ho; γ = 0 versus H1; γ 	= 0 in section 11.4, equation
(11.47):
</p>
<p>(a) Show that the second stage regression of 2SLS on the unrestricted model y1 = Z1δ1+W
&lowast;γ+u1
</p>
<p>with the matrix of instruments W yields the following residual sum of squares:
</p>
<p>URSS &lowast; = y&prime;1P̄W y1 = y
&prime;
1y1 &minus; y&prime;1PW y1
</p>
<p>Hint: Use the results of problem 10 for the just-identified case.
</p>
<p>(b) Show that the second stage regression of 2SLS on the restricted model y1 = Z1δ1 + u1 with
the matrix of instruments W yields the following residual sum of squares:
</p>
<p>RRSS &lowast; = y&prime;1P̄Ẑ1y1 = y
&prime;
1y1 &minus; y&prime;1PẐ1y1
</p>
<p>where Ẑ1 = PWZ1 and PẐ1 = PWZ1(Z
&prime;
1PWZ1)
</p>
<p>&minus;1Z &prime;1PW . Conclude that RRSS
&lowast;&minus; URSS&lowast;
</p>
<p>yields (11.49).
</p>
<p>(c) Consider the test statistic (RRSS&lowast;&minus; URSS&lowast;)/σ̃11 where σ̃11 is given by (11.50) as the usual
2SLS residual sum of squares under Ho divided by T . Show that it can be written as Haus-
man&rsquo;s (1983) test statistic, i.e., TR2u where R
</p>
<p>2
u is the uncentered R
</p>
<p>2 of the regression of 2SLS
</p>
<p>residuals (y1&minus;Z1δ̃1,2SLS) on the matrix of all pre-determined variables W . Hint: Show that
the regression sum of squares (y1 &minus; Z1δ̃1,2SLS)&prime;PW (y1 &minus; Z1δ̃1,2SLS) = (RRSS&lowast;&minus; URSS&lowast;)
given in (11.49).
</p>
<p>(d) Verify that the test for Ho based on the GNR for the model given in part (a) yields the same
TR2u test statistic described in part (c).
</p>
<p>13. Hausman&rsquo;s Specification Test: OLS versus 2SLS. This is based on Maddala (1992, page 511). For
the simple regression
</p>
<p>yt = βxt + ut t = 1, 2 . . . , T
</p>
<p>where β is scalar and ut &sim; IIN(0, σ
2). Let wt be an instrumental variable for xt. Run xt on wt
</p>
<p>and get xt = π̂wt + ν̂t or xt = x̂t + ν̂t where x̂t = π̂wt.
</p>
<p>(a) Show that in the augmented regression yt = βxt + γx̂t + ǫt a test for γ = 0 based on OLS
from this regression yields Hausman&rsquo;s test-statistic. Hint: Show that γ̂OLS = q̂/(1 &minus; r2xw)
where
</p>
<p>r2xw =
(&sum;T
</p>
<p>t=1 xtwt
</p>
<p>)2
/
&sum;T
</p>
<p>t=1 w
2
t
</p>
<p>&sum;T
t=1 x
</p>
<p>2
t .
</p>
<p>Next, show that var(γ̂OLS) = var(β̂OLS)/r
2
xw(1&minus; r2xw). Conclude that
</p>
<p>γ̂2OLS/var(γ̂OLS) = q̂
2r2xw/[var(β̂OLS)(1&minus; r2xw)]
</p>
<p>is the Hausman (1978) test statistic m given in section 11.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>290 Chapter 11: Simultaneous Equations Model
</p>
<p>(b) Show that the same result in part (a) could have been obtained from the augmented regression
</p>
<p>yt = βxt + γν̂t + ηt
</p>
<p>where ν̂t is the residual from the regression of xt on wt.
</p>
<p>14. Consider the following structural equation: y1 = α12y2 + α13y3 + β11X1 + β12X2 + u1 where y2
and y3 are endogenous and X1 and X2 are exogenous. Also, suppose that the excluded exogenous
variables include X3 and X4.
</p>
<p>(a) Show that Hausman&rsquo;s test statistic can be obtained from the augmented regression:
</p>
<p>y1 = α12y2 + α13y3 + β11X1 + β12X2 + γ2ŷ2 + γ3ŷ3 + ǫ1
</p>
<p>where ŷ2 and ŷ3 are predicted values from regressing y2 and y3 on X = [X1,X2,X3,X4].
Hausman&rsquo;s test is equivalent to testing Ho; γ2 = γ3 = 0. See equation (11.54).
</p>
<p>(b) Show that the same results in part (a) hold if we had used the following augmented regression:
</p>
<p>y1 = α12y2 + α13y3 + β11X1 + β12X2 + γ2ν̂2 + γ3ν̂3 + η1
</p>
<p>where ν̂2 and ν̂3 are the residuals from running y2 and y3 on X = [X1, X2, X3, X4]. See
equation (11.55). Hint: Show that the regressions in (a) and (b) have the same residual sum
of squares.
</p>
<p>15. For the artificial regression given in (11.55):
</p>
<p>(a) Show that OLS on this model yields δ̂1,OLS = δ̂1,IV = (Z
&prime;
1PWZ1)
</p>
<p>&minus;1Z &prime;1PW y1. Hint: Y1&minus;Ŷ1 =
P̄WY1. Use the FWL Theorem to residual out these variables in (11.55) and use the fact that
P̄WZ1 = [P̄WY1, 0].
</p>
<p>(b) Show that the var(δ̂1,OLS) = s̃11(Z
&prime;
1PWZ1)
</p>
<p>&minus;1 where s̃11 is the mean squared error of the
OLS regression in (11.55). Note that when η 	= 0 in (11.55), IV estimation is necessary and
s̃11 underestimates σ11 and will have to be replaced by (y1 &minus; Z1δ̂1,IV )&prime;(y1 &minus; Z1δ̂1,IV )/T .
</p>
<p>16. Recursive Systems. A recursive system has two crucial features: B is a triangular matrix and
Σ is a diagonal matrix. For this special case of the simultaneous equations model, OLS is still
consistent, and under normality of the disturbances still maximum likelihood. Let us consider a
specific example:
</p>
<p>y1t + γ11x1t + γ12x2t = u1t
</p>
<p>β21y1t + y2t + γ23x3t = u2t
</p>
<p>In this case, B =
</p>
<p>[
1 0
β21 1
</p>
<p>]
is triangular and
</p>
<p>&sum;
=
</p>
<p>[
σ11 0
0 σ22
</p>
<p>]
is assumed diagonal.
</p>
<p>(a) Check the identifiability conditions of this recursive system.
</p>
<p>(b) Solve for the reduced form and show that y1t is only a function of the xt&rsquo;s and u1t, while y2t
is a function of the xt&rsquo;s and a linear combination of u1t and u2t.
</p>
<p>(c) Show that OLS on the first structural equation yields consistent estimates. Hint: There are
no right hand side y&rsquo;s for the first equation. Show that despite the presence of y1 in the second
equation, OLS of y2 on y1 and x3 yields consistent estimates. Note that y1 is a function of
u1 only and u1 and u2 are not correlated.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 291
</p>
<p>(d) Under the normality assumption on the disturbances, the likelihood function conditional on
the x&rsquo;s is given by
</p>
<p>L(B,Γ,Σ) = (2π)&minus;T/2|B|T |Σ|&minus;T/2 exp(&minus;1
2
</p>
<p>&sum;T
t=1 u
</p>
<p>&prime;
tΣ
</p>
<p>&minus;1ut)
</p>
<p>where in this two equation case u&prime;t = (u1t, u2t). Since B is triangular, |B| = 1. Show that
maximizing L with respect to B and Γ is equivalent to minimizing Q =
</p>
<p>&sum;T
t=1 u
</p>
<p>&prime;
tΣ
</p>
<p>&minus;1ut.
</p>
<p>Conclude that when Σ is diagonal, Σ&minus;1 is diagonal and Q =
&sum;T
</p>
<p>t=1 u
2
1t/σ11 +
</p>
<p>&sum;T
t=1 u
</p>
<p>2
2t/σ22.
</p>
<p>Hence, maximizing the likelihood with respect to B and Γ is equivalent to running OLS on
each equation separately.
</p>
<p>17. Hausman&rsquo;s Specification Test: 2SLS Versus 3SLS. This is based on Holly (1988). Consider the
two-equations model,
</p>
<p>y1 = αy2 + β1x1 + β2x2 + u1
</p>
<p>y2 = γy1 + β3x3 + u2
</p>
<p>where y1 and y2 are endogenous; x1, x2 and x3 are exogenous (the y&rsquo;s and the x&rsquo;s are n &times; 1
vectors). The standard assumptions are made on the disturbance vectors u1 and u2. With the
usual notation, the model can also be written as
</p>
<p>y1 = Z1δ1 + u1
</p>
<p>y2 = Z2δ2 + u2
</p>
<p>The following notation will be used: δ̃ = 2SLS,
˜̃
δ = 3SLS, and the corresponding residuals will
</p>
<p>be denoted as ũ and ˜̃u, respectively.
</p>
<p>(a) Assume that αγ 	= 1. Show that the 3SLS estimating equations reduce to
</p>
<p>σ̃11X &prime;˜̃u1 + σ̃12X &prime;˜̃u2 = 0
σ̃12Z &prime;2PX
</p>
<p>˜̃u1 + σ̃22Z &prime;2PX ˜̃u2 = 0
</p>
<p>where X = (x1, x2, x3), Σ = [σij ] is the structural form covariance matrix, and Σ
&minus;1 = [σij ]
</p>
<p>for i, j = 1, 2.
</p>
<p>(b) Deduce that
˜̃
δ2 = δ̃2 and
</p>
<p>˜̃
δ1 = δ̃1 &minus; (σ̃12/σ̃22)(Z &prime;1PXZ1)&minus;1Z &prime;1PX ũ2. This proves that the
</p>
<p>3SLS estimator of the over-identified second equation is equal to its 2SLS counterpart. Also,
the 3SLS estimator of the just-identified first equation differs from its 2SLS (or indirect
least squares) counterpart by a linear combination of the 2SLS (or 3SLS) residuals of the
over-identified equation, see Theil (1971).
</p>
<p>(c) How would you interpret a Hausman-type test where you compare
˜̃
δ1 and δ̃1? Show that it
</p>
<p>is nR2 where R2 is the R-squared of the regression of ũ2 on the set of second stage regressors
of both equations Ẑ1 and Ẑ2. Hint: See the solution by Baltagi (1989).
</p>
<p>18. For the two-equation simultaneous model
</p>
<p>y1t = β12y2t + γ11x1t + u1t
</p>
<p>y2t = β21y1t + γ22x2t + γ23x3t + u2t
</p>
<p>With
</p>
<p>X &prime;X =
</p>
<p>⎡
⎣
</p>
<p>20 0 0
0 20 0
0 0 10
</p>
<p>⎤
⎦X &prime;Y =
</p>
<p>⎡
⎣
</p>
<p>5 10
40 20
20 30
</p>
<p>⎤
⎦Y &prime;Y =
</p>
<p>[
3 4
4 8
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>292 Chapter 11: Simultaneous Equations Model
</p>
<p>(a) Determine the identifiability of each equation with the aid of the order and rank conditions
for identification.
</p>
<p>(b) Obtain the OLS normal equations for both equations. Solve for the OLS estimates.
</p>
<p>(c) Obtain the 2SLS normal equations for both equations. Solve for the 2SLS estimates.
</p>
<p>(d) Can you estimate these equations using Indirect Least Squares? Explain.
</p>
<p>19. Laffer (1970) considered the following supply and demand equations for Traded Money:
</p>
<p>log(TM/P ) = αo + α1log(RM/P ) + α2log i+ u1
</p>
<p>log(TM/P ) = βo + β1log(Y/P ) + β2log i+ β3log(S1) + β4log(S2) + u2
</p>
<p>where
</p>
<p>TM = Nominal total trade money
</p>
<p>RM = Nominal effective reserve money
</p>
<p>Y = GNP in current dollars
</p>
<p>S2 = Degree of market utilization
</p>
<p>i = short-term rate of interest
</p>
<p>S1 = Mean real size of the representative economic unit (1939 = 100)
</p>
<p>P = GNP price deflator (1958 = 100)
</p>
<p>The basic idea is that trade credit is a line of credit and the unused portion represents purchasing
power which can be used as a medium of exchange for goods and services. Hence, Laffer (1970)
suggests that trade credit should be counted as part of the money supply. Besides real income and
the short-term interest rate, the demand for real traded money includes log(S1) and log(S2). S1
is included to capture economies of scale. As S1 increases, holding everything else constant, the
presence of economies of scale would mean that the demand for traded money would decrease.
Also, the larger S2, the larger the degree of market utilization and the more money is needed for
transaction purposes.
</p>
<p>The data are provided on the Springer web site as LAFFER.ASC. This data covers 21 annual
observations over the period 1946&ndash;1966. This was obtained from Lott and Ray (1992). Assume
that (TM/P ) and i are endogenous and the rest of the variables in this model are exogenous.
</p>
<p>(a) Using the order condition for identification, determine whether the demand and supply equa-
tions are identified? What happens if you used the rank condition of identification?
</p>
<p>(b) Estimate this model using OLS.
</p>
<p>(c) Estimate this model using 2SLS.
</p>
<p>(d) Estimate this model using 3SLS. Compare the estimates and their standard errors for parts
(b), (c) and (d).
</p>
<p>(e) Test the over-identification restriction of each equation.
</p>
<p>(f) Run Hausman&rsquo;s specification test on each equation basing it on OLS and 2SLS.
</p>
<p>(g) Run Hausman&rsquo;s specification test on each equation basing it on 2SLS and 3SLS.
</p>
<p>20. The market for a certain good is expressed by the following equations:
</p>
<p>Dt = α0 &minus; α1Pt + α2Xt + u1t (α1, α2 &gt; 0)
St = β0 + β1Pt + u2t (β1 &gt; 0)
</p>
<p>Dt = St = Qt</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 293
</p>
<p>where Dt is the quantity demanded, St is the quantity supplied, Xt is an exogenous demand shift
variable. (u1t, u2t) is an IID random vector with zero mean and covariance matrix Σ = [σij ] for
i, j = 1, 2.
</p>
<p>(a) Examine the identifiability of the model under the assumptions given above using the order
and rank conditions of identification.
</p>
<p>(b) Assuming the moment matrix of exogenous variables converge to a finite non-zero matrix,
derive the simultaneous equation bias in the OLS estimator of β1.
</p>
<p>(c) If σ12 = 0 would you expect this bias to be positive or negative? Explain.
</p>
<p>21. Consider the following three equations simultaneous model
</p>
<p>y1 = α1 + β2y2 + γ1X1 + u1 (1)
</p>
<p>y2 = α2 + β1y1 + β3y3 + γ2X2 + u2 (2)
</p>
<p>y3 = α3 + γ3X3 + γ4X4 + γ5X5 + u3 (3)
</p>
<p>where the X&rsquo;s are exogenous and the y&rsquo;s are endogenous.
</p>
<p>(a) Examine the identifiability of this system using the order and rank conditions.
</p>
<p>(b) How would you estimate equation (2) by 2SLS? Describe your procedure step by step.
</p>
<p>(c) Suppose that equation (1) was estimated by running y2 on a constant X2 and X3 and the
resulting predicted ŷ2 was substituted in (1), and OLS performed on the resulting model.
Would this estimating procedure yield consistent estimates of α1, β2 and γ1? Explain your
answer.
</p>
<p>(d) How would you test for the over-identification restrictions in equation (1)?
</p>
<p>22. Equivariance of Instrumental Variables Estimators. This is based on Sapra (1997). For the struc-
tural equation given in (11.34), let the matrix of instruments W be of dimension T &times; ℓ where
ℓ &ge; g1 + k1 as described below (11.41). Then the corresponding instrumental variable estimator
of δ1 given below (11.41) is δ̂1,IV (y1) = (Z
</p>
<p>&prime;
1PWZ1)
</p>
<p>&minus;1Z &prime;1PW y1.
</p>
<p>(a) Show that this IV estimator is an equivariant estimator of δ1, i.e., show that for any linear
transformation y&lowast;1 = ay1+Z1b where a is a positive scalar and b is an (ℓ&times; 1) real vector, the
following relationship holds:
</p>
<p>δ̂1,IV (y
&lowast;
1) = aδ̂1,IV (y1) + b.
</p>
<p>(b) Show that the variance estimator
</p>
<p>σ̂2(y1) = (y1 &minus; Z1δ̂1,IV (y1))&prime;(y1 &minus; Z1δ̂1,IV (y1))/T
</p>
<p>is equivariant for σ2, i.e., show that σ̂2(y&lowast;1) = a
2σ̂2(y1).
</p>
<p>23. Identification and Estimation of a Simple Two-Equation Model. This is based on Holly (1987).
Consider the following two equation model
</p>
<p>yt1 = α+ βyt2 + ut1
</p>
<p>yt2 = γ + yt1 + ut2
</p>
<p>where the y&rsquo;s are endogenous variables and the u&rsquo;s are serially independent disturbances that
are identically distributed with zero means and nonsingular covariance matrix Σ = [σij ] where
E(utiutj) = σij for i, j = 1, 2, and all t = 1, 2, . . . , T. The reduced form equations are given by
</p>
<p>yt1 = π11 + νt1 and yt2 = π21 + νt2
</p>
<p>with Ω = [ωij ] where E(νtiνtj) = ωij for i, j = 1, 2 and all t = 1, 2, . . . , T.</p>
<p/>
</div>
<div class="page"><p/>
<p>294 Chapter 11: Simultaneous Equations Model
</p>
<p>(a) Examine the identification of this system of two equations when no further information is
available.
</p>
<p>(b) Repeat part (a) when σ12 = 0.
</p>
<p>(c) Assuming σ12 = 0, show that β̂OLS , the OLS estimator of β in the first equation is not
consistent.
</p>
<p>(d) Assuming σ12 = 0, show that an alternative consistent estimator of β is an IV estimator
using zt = [(yt2 &minus; ȳ2)&minus; (yt1 &minus; ȳ1)] as an instrument for yt2.
</p>
<p>(e) Show that the IV estimator of β obtained from part (d) is also an indirect least squares
estimator of β. Hint: See the solution by Singh and Bhat (1988).
</p>
<p>24. Errors in Measurement and the Wald (1940) Estimator. This is based on Farebrother (1985). Let
y&lowast;i be permanent consumption and X
</p>
<p>&lowast; be permanent income, both are measured with error:
</p>
<p>y&lowast;i = βx
&lowast;
i where yi = y
</p>
<p>&lowast;
i + ǫi and xi = x
</p>
<p>&lowast;
i + ui for i = 1, 2, . . . , n.
</p>
<p>Let x&lowast;i , ǫi and ui be independent normal random variables with zero means and variances σ
2
&lowast;, σ
</p>
<p>2
ǫ
</p>
<p>and σ2u, respectively. Wald (1940) suggested the following estimator of β: Order the sample by
the xi&rsquo;s and split the sample into two. Let (ȳ1, x̄1) be the sample mean of the first half of the
sample and (ȳ2, x̄2) be the sample mean of the second half of this sample. Wald&rsquo;s estimator of β is
</p>
<p>β̂W = (ȳ2 &minus; ȳ1)/(x̄2 &minus; x̄1). It is the slope of the line joining these two sample mean observations.
</p>
<p>(a) Show that β̂W can be interpreted as a simple IV estimator with instrument
</p>
<p>zi = 1 for xi &ge; median(x)
= &minus;1 for xi &lt; median(x)
</p>
<p>where median(x) is the sample median of x1, x2, . . . , xn.
</p>
<p>(b) Define wi = ρ
2x&lowast;i &minus; τ2ui where ρ2 = σ2u/(σ2u + σ2&lowast;) and τ2 = σ2&lowast;/(σ2u + σ2&lowast;). Show that
</p>
<p>E(xiwi) = 0 and that wi &sim; N(0, σ
2
&lowast;σ
</p>
<p>2
u/(σ
</p>
<p>2
&lowast; + σ
</p>
<p>2
u)).
</p>
<p>(c) Show that x&lowast;i = τ
2xi + wi and use it to show that
</p>
<p>E(β̂W /x1, . . . , xn) = E(β̂OLS/x1, . . . , xn) = βτ
2.
</p>
<p>Conclude that the exact small sample bias of β̂OLS and β̂W are the same.
</p>
<p>25. Comparison of t-ratios. This is based on Holly (1990). Consider the two equations model
</p>
<p>y1 = αy2 +Xβ + u1 and y2 = γy1 +Xβ + u2
</p>
<p>where α and γ are scalars, y1 and y2 are T &times; 1 and X is a T &times; (K &minus; 1) matrix of exogenous
variables. Assume that ui &sim; N(0, σ
</p>
<p>2
i IT ) for i = 1, 2. Show that the t-ratios for H
</p>
<p>a
o ; α = 0 and H
</p>
<p>b
o ;
</p>
<p>γ = 0 using α̂OLS and γ̂OLS are the same. Comment on this result. Hint: See the solution by
Farebrother (1991).
</p>
<p>26. Degeneration of Feasible GLS to 2SLS in a Limited Information Simultaneous Equations Model.
This is based on Gao and Lahiri (2000). Consider a simple limited information simultaneous
equations model,
</p>
<p>y1 = γy2 + u, (1)
</p>
<p>y2 = Xβ + v, (2)
</p>
<p>where y1, y2 are N &times; 1 vectors of observations on two endogenous variables. X is N &times;K matrix
of predetermined variables of the system, and K &ge; 1 such that (1) is identified. Each row of</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 295
</p>
<p>(u, v) is assumed to be i.i.d. (0,Σ), and Σ is p.d. In this case, γ̂2SLS = (y
&prime;
2PXy2)
</p>
<p>&minus;1y&prime;2PXy1, where
</p>
<p>PX = X(X
&prime;X)&minus;1X &prime;. The residuals û = y1&minus; γ̂2SLSy2 and v̂ = My2, where M = IN &minus;PX are used
</p>
<p>to generate a consistent estimate for Σ
</p>
<p>Σ̂ =
1
</p>
<p>N
</p>
<p>[
û&prime;û û&prime;v̂
v̂&prime;û v̂&prime;v̂
</p>
<p>]
.
</p>
<p>Show that a feasible GLS estimate of γ using Σ̂ degenerates to γ̂2SLS .
</p>
<p>27. Equality of Two IV Estimators. This is based on Qian (1999). Consider the following linear re-
gression model:
</p>
<p>yi = x
&prime;
iβ + ǫ = x
</p>
<p>&prime;
1iβ1 + x
</p>
<p>&prime;
2iβ2 + ǫi, i = 1, 2, . . . , N, (1)
</p>
<p>where the dimensions of x&prime;i, x
&prime;
1i and x
</p>
<p>&prime;
2i are 1&times;K, 1&times;K1 and 1&times;K2, respectively, withK = K1+K2.
</p>
<p>xi may be correlated with ǫi, but we have instruments zi such that E(ǫi|z&prime;i) = 0 and E(ǫ2i |z&prime;i) = σ2.
Partition the instruments into two subsets: z&prime;i = (z
</p>
<p>&prime;
1i, z
</p>
<p>&prime;
2i), where the dimensions of zi, z1i, and z2i
</p>
<p>are L,L1 and L2, with L = L1+L2. Assume that E(z1ix
&prime;
i) has full column rank (so L1 &ge; K); this
</p>
<p>ensures that β can be estimated consistently using the subset of instruments z1i only or using the
entire set z&prime;i = (z
</p>
<p>&prime;
1i, z
</p>
<p>&prime;
2i). We also assume that (yi, x
</p>
<p>&prime;
i, z
</p>
<p>&prime;
i) is covariance stationary.
</p>
<p>Define PA = A(A
&prime;A)&minus;1A&prime; and MA = I &minus; PA for any matrix A with full column rank. Let X =
</p>
<p>(x1, . . . , xN )
&prime;, and similarly for X1, X2, y, Z1, Z2 and Z. Define X̂ = P[Z1]X and β̂ = (X̂
</p>
<p>&prime;X̂)&minus;1X̂ &prime;y,
</p>
<p>so that β̂ is the instrumental variables (IV) estimator of (1) using Z1 as instruments. Similarly,
</p>
<p>define X̃ = PZX and β̃ = (X̃
&prime;X̃)&minus;1X̃ &prime;y, so that β̃ is the IV estimator of (1) using Z as instruments.
</p>
<p>Show that β̂1 = β̃1 if Z
&prime;
2M[Z1][X1 &minus;X2(X &prime;2P1X2)&minus;1X &prime;2P1X1] = 0.
</p>
<p>28. For the crime in North Carolina example given in section 11.6, replicate the results in Tables
11.2&ndash;11.6 using the data for 1987. Do the same using the data for 1981. Are there any notable
differences in the results as we compare 1981 to 1987?
</p>
<p>29. Spatial Lag Test with Equal Weights. This is based on Baltagi and Liu (2009). Consider the spatial
lag dependence model described in Section 11.2.1, with the N&times;N spatial weight matrix W having
zero elements across the diagonal and equal elements 1/ (N &minus; 1) off the diagonal. The LM test for
zero spatial lag, i.e., H0 : ρ = 0 versus H1 : ρ 	= 0, is given in Anselin (1988). This takes the form
</p>
<p>LM =
[û&prime;Wy/ (û&prime;û/N)]2(
</p>
<p>WXβ̂
)&prime;
</p>
<p>PXWXβ̂/σ̂
2 + tr (W 2 +W &prime;W )
</p>
<p>.
</p>
<p>where PX = I &minus; X (X &prime;X)&minus;1 X &prime;, β̂ is the restricted mle, which in this case is the least squares
estimator of β. Similarly, σ̂2 is the corresponding restricted mle of σ2, which in this case is the least
squares residual sums of squares divided by N, i.e., (û&prime;û/N) , where û denotes the least squares
residuals. Show that for the equal weight spatial matrix, this LM test statistic will always equal
N/2 (N &minus; 1) no matter what ρ is.
</p>
<p>30. Growth and Inequality Reconsidered. For the Lundberg and Squire (2003) Growth and Inequality
example considered in section 11.6.
</p>
<p>(a) Estimate these equations using 3SLS and verify the results reported in Table 1 of Lundberg
and Squire (2003, p. 334). These results show that openness enhances growth and education
reduces inequality, see Table 11.9. How do these 3SLS results compare with 2SLS? Are the
over-identification restrictions rejected by the data?</p>
<p/>
</div>
<div class="page"><p/>
<p>296 Chapter 11: Simultaneous Equations Model
</p>
<p>(b) Lundberg and Squire (2003) allow growth to enter the inequality equation, and inequality to
enter the growth equation. Estimate these respecified equations using 3SLS and verify the
results reported in Table 2 of Lundberg and Squire (2003, p. 336). How do these 3SLS results
compare with 2SLS? Are the over-identification restrictions rejected by the data?
</p>
<p>31. Married Women Labor Supply. Mroz (1987) questions the exogeneity assumption of the wife&rsquo;s
wage rate in a simple specification of married women labor supply. Using the PSID for 1975, his
sample consists of 753 married white women between the ages of 30 and 60 in 1975, with 428
working at some time during the year. The wife&rsquo;s annual hours of work (hours) is regressed on the
logarithm of her wage rate (lwage); the nonwife income (nwifeinc); the wife&rsquo;s age (age), her years
of schooling (educ), the number of children less than six years old in the household (kidslt6), and
the number of children between the ages of five and nineteen (kidsge6). The data set was obtained
from the data web site of Wooldridge (2009).
</p>
<p>(a) Replicate Table III of Mroz (1987, p. 769) which gives the descriptive statistics of the data.
</p>
<p>(b) Replicate Table IV of Mroz (1987, p. 770) which runs OLS and 2SLS using a variety of
instrumental variables for lwage. These instrumental variables are described in Table V of
Mroz (1987, p. 771).
</p>
<p>(c) Run the over-identification test for each 2sls regression in (b), as well as the diagnostics for
the first stage regression.
</p>
<p>References
</p>
<p>This chapter is influenced by Johnston (1984), Kelejian and Oates (1989), Maddala (1992),
Davidson and MacKinnon (1993), Mariano (2001) and Stock and Watson (2003). Additional
references include the econometric texts cited in Chapter 3 and the following:
</p>
<p>Anderson, T.W. and H. Rubin (1950), &ldquo;The Asymptotic Properties of Estimates of the Parameters
of a Single Equation in a Complete System of Stochastic Equations,&rdquo; Annals of Mathematical
Statistics, 21: 570&ndash;582.
</p>
<p>Baltagi, B.H. (1989), &ldquo;A Hausman Specification Test in a Simultaneous Equations Model,&rdquo; Econometric
Theory, Solution 88.3.5, 5: 453&ndash;467.
</p>
<p>Baltagi, B.H. and L. Liu (2009), &ldquo;Spatial Lag Test with Equal Weights,&rdquo; Economics Letters, 104: 81&ndash;82.
</p>
<p>Basmann, R.L. (1957), &ldquo;A Generalized Classical Method of Linear Estimation of Coefficients in a Struc-
tural Equation,&rdquo; Econometrica, 25: 77&ndash;83.
</p>
<p>Basmann, R.L. (1960), &ldquo;On Finite Sample Distributions of Generalized Classical Linear Identifiability
Tests Statistics,&rdquo; Journal of the American Statistical Association, 55: 650&ndash;659.
</p>
<p>Bekker, P.A. (1994), &ldquo;Alternative Approximations to the Distribution of Instrumental Variable Estima-
tors,&rdquo; Econometrica 62: 657&ndash;681.
</p>
<p>Bekker, P.A. and T.J. Wansbeek (2001), &ldquo;Identification in Parametric Models,&rdquo; Chapter 7 in Baltagi,
B.H. (ed.) A Companion to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Bound, J., D.A. Jaeger and R.M. Baker (1995), &ldquo;Problems with Instrumental Variables Estimation When
the Correlation Between the Instruments and the Exogenous Explanatory Variable is Weak,&rdquo;
Journal of American Statistical Association 90: 443&ndash;450.
</p>
<p>Cornwell, C. and W.N. Trumbull (1994), &ldquo;Estimating the Economic Model of Crime Using Panel Data,&rdquo;
Review of Economics and Statistics, 76: 360&ndash;366.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 297
</p>
<p>Cragg, J.G. and S.G. Donald (1996), &ldquo;Inferring the Rank of Matrix,&rdquo; Journal of Econometrics, 76:
223&ndash;250.
</p>
<p>Deaton, A. (1997), The Analysis of Household Surveys: A Microeconometric Approach to Development
Policy (Johns Hopkins University Press: Baltimore).
</p>
<p>Durbin, J. (1954), &ldquo;Errors in Variables,&rdquo; Review of the International Statistical Institute, 22: 23&ndash;32.
</p>
<p>Farebrother, R.W. (1985), &ldquo;The Exact Bias of Wald&rsquo;s Estimator,&rdquo; Econometric Theory, Problem 85.3.1,
1: 419.
</p>
<p>Farebrother, R.W. (1991), &ldquo;Comparison of t-Ratios,&rdquo; Econometric Theory, Solution 90.1.4, 7: 145&ndash;146.
</p>
<p>Fisher, F.M. (1966), The Identification Problem in Econometrics (McGraw-Hill: New York).
</p>
<p>Gao, C. and K. Lahiri (2000), &ldquo;Degeneration of Feasible GLS to 2SLS in a Limited Information Simul-
taneous Equations Model,&rdquo; Econometric Theory, Problem 00.2.1, 16: 287.
</p>
<p>Gronau, R. (1973), &ldquo;The Effects of Children on the Housewife&rsquo;s Value of Time,&rdquo; Journal of Political
Economy, 81: S168&ndash;199.
</p>
<p>Haavelmo, T. (1944), &ldquo;The Probability Approach in Econometrics,&rdquo; Supplement to Econometrica, 12.
</p>
<p>Hall, A. (1993), &ldquo;Some Aspects of Generalized Method of Moments Estimation,&rdquo; Chapter 15 in Handbook
of Statistics, Vol. 11 (North Holland: Amsterdam).
</p>
<p>Hansen, L. (1982), &ldquo;Large Sample Properties of Generalized Method of Moments Estimators,&rdquo; Econo-
metrica, 50: 646&ndash;660.
</p>
<p>Hausman, J.A. (1978), &ldquo;Specification Tests in Econometrics,&rdquo; Econometrica, 46: 1251&ndash;1272.
</p>
<p>Hausman, J.A. (1983), &ldquo;Specification and Estimation of Simultaneous Equation Models,&rdquo; Chapter 7
in Griliches, Z. and Intriligator, M.D. (eds.) Handbook of Econometrics, Vol. I (North Holland:
Amsterdam).
</p>
<p>Holly, A. (1987), &ldquo;Identification and Estimation of a Simple Two-Equation Model,&rdquo; Econometric Theory,
Problem 87.3.3, 3: 463&ndash;466.
</p>
<p>Holly, A. (1988), &ldquo;A Hausman Specification Test in a Simultaneous Equations Model,&rdquo; Econometric
Theory, Problem 88.3.5, 4: 537&ndash;538.
</p>
<p>Holly, A. (1990), &rdquo;Comparison of t-ratios,&rdquo; Econometric Theory, Problem 90.1.4, 6: 114.
</p>
<p>Kapteyn, A. and D.G. Fiebig (1981), &ldquo;When are Two-Stage and Three-Stage Least Squares Estimators
Identical?,&rdquo; Economics Letters, 8: 53&ndash;57.
</p>
<p>Koopmans, T.C. and J. Marschak (1950), Statistical Inference in Dynamic Economic Models (John Wiley
and Sons: New York).
</p>
<p>Koopmans, T.C. and W.C. Hood (1953), Studies in Econometric Method (John Wiley and Sons: New
York).
</p>
<p>Laffer, A.B., (1970), &ldquo;Trade Credit and the Money Market,&rdquo; Journal of Political Economy, 78: 239&ndash;267.
</p>
<p>Lott, W.F. and S.C. Ray (1992), Applied Econometrics: Problems with Data Sets (The Dryden Press:
New York).
</p>
<p>Lundberg, M. and L. Squire (2003), &ldquo;The Simultaneous Evolution of Growth and Inequality,&rdquo; The
Economic Journal, 113: 326&ndash;344.
</p>
<p>Manski, C.F. (1995), Identification Problems in the Social Sciences (Harvard University Press: Cam-
bridge)</p>
<p/>
</div>
<div class="page"><p/>
<p>298 Chapter 11: Simultaneous Equations Model
</p>
<p>Mariano, R.S. (2001), &ldquo;Simultaneous Equation Model Estimators: Statistical Properties and Practical
Implications,&rdquo; Chapter 6 in Baltagi, B.H. (ed.) A Companion to Theoretical Econometrics (Black-
well: Massachusetts).
</p>
<p>Mroz, T.A. (1987), &ldquo;The Sensitivity of an Empirical Model of Married Women&rsquo;s Hours of Work to
Economic and Statistical Assumptions,&rdquo; Econometrica, 55: 765&ndash;799.
</p>
<p>Nelson, C.R. and R. Startz (1990), &ldquo;The Distribution of the Instrumental Variables Estimator and its
t-Ratio when the Instrument is a Poor One,&rdquo; Journal of Business, 63: S125&ndash;140.
</p>
<p>Sapra, S.K. (1997), &ldquo;Equivariance of an Instrumental Variable (IV) Estimator in the Linear Regression
Model,&rdquo; Econometric Theory, Problem 97.2.5, 13: 464.
</p>
<p>Singh, N. and A N. Bhat (1988), &ldquo;Identification and Estimation of a Simple Two-Equation Model,&rdquo;
Econometric Theory, Solution 87.3.3, 4: 542&ndash;545.
</p>
<p>Staiger, D. and J. Stock (1997), &ldquo;Instrumental Variables Regression With Weak Instruments,&rdquo; Econo-
metrica, 65: 557&ndash;586.
</p>
<p>Stock, J.H. and M.W. Watson (2003), Introduction to Econometrics (Addison Wesley: Boston).
</p>
<p>Theil, H. (1953), &ldquo;Repeated Least Squares Applied to Complete Equation Systems,&rdquo; The Hague, Central
Planning Bureau (Mimeo).
</p>
<p>Theil, H. (1971), Principles of Econometrics (Wiley: New York).
</p>
<p>Wald, A. (1940), &ldquo;Fitting of Straight Lines if Both Variables are Subject to Error,&rdquo; Annals of Mathe-
matical Statistics, 11: 284&ndash;300.
</p>
<p>Wooldridge, J.M. (1990), &ldquo;A Note on the Lagrange Multiplier and F Statistics for Two Stage Least
Squares Regression,&rdquo; Economics Letters, 34: 151&ndash;155.
</p>
<p>Wooldridge, J.M. (2009), Introductory Econometrics: A Modern Approach (South-Western: Ohio).
</p>
<p>Wu, D.M. (1973), &ldquo;Alternative Tests of Independence Between Stochastic Regressors and Disturbances,&rdquo;
Econometrica, 41: 733&ndash;740.
</p>
<p>Zellner, A. and Theil, H. (1962), &ldquo;Three-Stage Least Squares: Simultaneous Estimation of Simultaneous
</p>
<p>Equations,&rdquo; Econometrica, 30: 54&ndash;78.
</p>
<p>Appendix: The Identification Problem Revisited: The Rank
Condition of Identification
</p>
<p>In section 11.1.2, we developed a necessary but not sufficient condition for identification. In
this section we emphasize that model identification is crucial because only then can we get
meaningful estimates of the parameters. For an under-identified model, different sets of param-
eter values agree well with the statistical evidence. As Bekker and Wansbeek (2001, p. 144)
put it, preference for one set of parameter values over other ones becomes arbitrary. Therefore,
&ldquo;Scientific conclusions drawn on the basis of such arbitrariness are in the best case void and in
the worst case dangerous.&rdquo; Manski (1995, p. 6) also warns that &ldquo;negative identification findings
imply that statistical inference is fruitless. It makes no sense to try to use a sample of finite size
to infer something that could not be learned even if a sample of infinite size were available.&rdquo;
Consider the simultaneous equation model
</p>
<p>Byt + Γxt = ut t = 1, 2, . . . , T. (A.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 299
</p>
<p>which displays the whole set of equations at time t. B is G&times;G, Γ is G&times;K and ut is G&times;1. B is
square and nonsingular indicating that the system is complete, i.e., there are as many equations
as there are endogenous variables. Premultiplying (A.1) by B&minus;1 and solving for yt in terms of
the exogenous variables and the vector of errors, we get
</p>
<p>yt = Πxt + vt t = 1, 2, . . . , T. (A.2)
</p>
<p>where Π = &minus;B&minus;1Γ, is G &times; K, and vt = B&minus;1ut. Note that if we premultiply the structural
model in (A.1) by an arbitrary nonsingular G &times; G matrix F , then the new structural model
has the same reduced form given in (A.2). In this case, each new structural equation is a
linear combination of the original structural equations, but the reduced form equations are the
same. One idea for identification, explored by Fisher (1966), is to note that (A.1) is completely
defined by B, Γ and the probability density function of the disturbances p(ut). The specification
of the structural model which comes from economic theory, imposes a lot of zero restrictions
on the B and Γ coefficients. In addition, there may be cross-equations or within equation
restrictions. For example, constant returns to scale of the production function, or homogeneity
of a demand equation, or symmetry conditions. In addition, the probability density function of
the disturbances may itself contain some zero covariance restrictions. The structural model given
in (A.1) is identified if these restrictions are enough to distinguish it from any other structural
model. This is operationalized by proving that the only nonsingular matrix F which results in
a new structural model that satisfies the same restrictions on the original model is the identity
matrix. If after imposing the restrictions, only certain rows of F resemble the corresponding
rows of an identity matrix, up to a scalar of normalization, then the corresponding equations of
the system are identified. The remaining equations are not identified. This is the same concept
of taking a linear combination of the demand and supply equations and seeing if the linear
combination is different from demand or supply. If it is, then both equations are identified. If
it looks like demand but not supply, then supply is identified and demand is not identified. Let
us look at an example.
</p>
<p>Example (A.1): Consider a demand and supply equations with
</p>
<p>Qt = a&minus; bPt + cYt + u1t (A.3)
Qt = d+ ePt + fWt + u2t (A.4)
</p>
<p>where Y is income and W is weather. Writing (A.3) and (A.4) in matrix form (A.1), we get
</p>
<p>B =
</p>
<p>[
1 b
1 &minus;e
</p>
<p>]
Γ =
</p>
<p>[
&minus;a &minus;c 0
&minus;d 0 &minus;f
</p>
<p>]
yt =
</p>
<p>(
Qt
Pt
</p>
<p>)
(A.5)
</p>
<p>x&prime;t = [1, Yt,Wt] u
&prime;
t = (u1t, u2t)
</p>
<p>There are two zero restrictions on Γ. The first is that income does not appear in the supply
equation and the second is that weather does not appear in the demand equation. Therefore, the
order condition of identification is satisfied for both equations. In fact, for each equation, there
is one excluded exogenous variable and only one right hand side included endogenous variable.
Therefore, both equations are just-identified. Let F = [fij ] for i, j = 1, 2, be a nonsingular
matrix. Premultiply this system by F . The new matrix B is now FB and the new matrix Γ
is now FΓ. In order for the transformed system to satisfy the same restrictions as the original
model, FB must satisfy the following normalization restrictions:
</p>
<p>f11 + f12 = 1 f21 + f22 = 1 (A.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>300 Chapter 11: Simultaneous Equations Model
</p>
<p>Also, FΓ should satisfy the following zero restrictions
</p>
<p>&minus;f21c+ f220 = 0 f110&minus; f12f = 0 (A.7)
</p>
<p>Since c 	= 0, and f 	= 0, then (A.7) implies that f21 = f12 = 0. Using (A.6), we get f11 = f22 = 1.
Hence, the only nonsingular F that satisfies the same restrictions on the original model is the
identity matrix, provided c 	= 0 and f 	= 0. Therefore, both equations are identified.
Example (A.2): If income does not appear in the demand equation (A.3), i.e., c = 0, the model
looks like
</p>
<p>Qt = a&minus; bPt + u1t (A.8)
Qt = d+ ePt + fWt + u2t (A.9)
</p>
<p>In this case, only the second restriction given in (A.7) holds. Therefore, f 	= 0 implies f12 = 0,
however f21 is not necessarily zero without additional restrictions. Using (A.6), we get f11 = 1
and f21 + f22 = 1. This means that only the first row of F looks like the first row of an identity
matrix, and only the demand equation is identified. In fact, the order condition for identification
is not met for the supply equation since there are no excluded exogenous variables from that
equation but there is one right hand side included endogenous variable. See problems 6 and 7
for more examples of this method of identification.
</p>
<p>Example (A.3): Suppose that u &sim; (0,Ω) where Ω = Σ &otimes; IT , and Σ = [σij ] for i, j = 1, 2. This
example shows how a variance-covariance restriction can help identify an equation. Let us take
the model defined in (A.8), (A.9) and add the restriction that σ12 = σ21 = 0. In this case, the
transformed model disturbances will be Fu &sim; (0,Ω&lowast;), where Ω&lowast; = Σ&lowast;&otimes; IT , and Σ&lowast; = FΣF &prime;. In
fact, since Σ is diagonal, FΣF &prime; should also be diagonal. This imposes the following restriction
on the elements of F :
</p>
<p>f11σ11f21 + f12σ22f22 = 0 (A.10)
</p>
<p>But, f11 = 1 and f12 = 0 from the zero restrictions imposed on the demand equation, see
example 4. Hence, (A.10) reduces to σ11f21 = 0. Since σ11 	= 0, this implies that f21 = 0,
and the normalization restriction given in (A.6), implies that f22 = 1. Therefore, the second
equation is also identified.
</p>
<p>Example (A.4): In this example, we demonstrate how cross-equation restrictions can help iden-
tify equations. Consider the following simultaneous model
</p>
<p>y1 = a+ by2 + cx1 + u1 (A.11)
</p>
<p>y2 = d+ ey1 + fx1 + gx2 + u2 (A.12)
</p>
<p>and add the restriction c = f . It can be easily shown that the first equation is identified with
f11 = 1, and f12 = 0. The second equation has no zero restrictions, but the cross-equation
restriction c = f implies:
</p>
<p>&minus;cf11 &minus; ff12 = &minus;cf21 &minus; ff22
</p>
<p>Using c = f , we get
</p>
<p>f11 + f12 = f21 + f22 (A.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 301
</p>
<p>But, the first equation is identified with f11 = 1 and f12 = 0. Hence, (A.13) reduces to f21+f22 =
1, which together with the normalization condition &minus;f21b + f22 = 1, gives f21(b + 1) = 0. If
b 	= &minus;1, then f21 = 0 and f22 = 1. The second equation is also identified provided b 	= &minus;1.
</p>
<p>Alternatively, one can look at the problem of identification by asking whether the structural
parameters in B and Γ can be obtained from the reduced form parameters. It will be clear
from the following discussion that this task is impossible if there are no restrictions on this
simultaneous model. In this case, the model is hopelessly unidentified. However, in the usual
case where there are a lot of zeros in B and Γ, we may be able to retrieve the remaining non-zero
coefficients from Π. More rigorously, Π = &minus;B&minus;1Γ, which implies that
</p>
<p>BΠ+ Γ = 0 (A.14)
</p>
<p>or
</p>
<p>AW = 0 where A = [B,Γ] and W &prime; = [Π&prime;, IK ] (A.15)
</p>
<p>For the first equation, this implies that
</p>
<p>α&prime;1W = 0 where α
&prime;
1 is the first row of A. (A.16)
</p>
<p>W is known (or can be estimated) and is of rankK. If the first equation has no restrictions on its
structural parameters, then α&prime;1 contains (G+K) unknown coefficients. These coefficients satisfy
K homogeneous equations given in (A.16). Without further restrictions, we cannot solve for
(G+K) coefficients (α&prime;1) with only K equations. Let φ denote the matrix of R zero restrictions
on the first equation, i.e., α&prime;1φ = 0. This together with (A.16) implies that
</p>
<p>α&prime;1[W,φ] = 0 (A.17)
</p>
<p>and we can solve uniquely for α&prime;1 (up to a scalar of normalization) provided the
</p>
<p>rank [W,φ] = G+K &minus; 1 (A.18)
</p>
<p>Economists specify each structural equation with the left hand side endogenous variable having
the coefficient one. This normalization identifies one coefficient of α&prime;1, therefore, we require only
(G +K &minus; 1) more restrictions to uniquely identify the remaining coefficients of α1. [W,φ] is a
(G + K) &times; (K + R) matrix. Its rank is less than any of its two dimensions, i.e., (K + R) &ge;
(G + K &minus; 1), which results in R &ge; G &minus; 1, or the order condition of identification. Note that
this is a necessary but not sufficient condition for (A.18) to hold. It states that the number
of restrictions on the first equation must be greater than the number of endogenous variables
minus one. If all R restrictions are zero restrictions, then it means that the number of excluded
exogenous plus the number of excluded endogenous variables should be greater than (G &minus; 1).
But the G endogenous variables are made up of the left hand side endogenous variable y1, the
g1 right hand side included endogenous variables Y1, and (G &minus; g1 &minus; 1) excluded endogenous
variables. Therefore, R &ge; (G&minus; 1) can be written as k2 + (G&minus; g1 &minus; 1) &ge; (G&minus; 1) which reduces
to k2 &ge; g1, which was discussed earlier in this chapter.
</p>
<p>The necessary and sufficient condition for identification can now be obtained as follows: Using
(A.1) one can write
</p>
<p>Azt = ut where z
&prime;
t = (y
</p>
<p>&prime;
t, x
</p>
<p>&prime;
t) (A.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>302 Chapter 11: Simultaneous Equations Model
</p>
<p>and from the first definition of identification we make the transformation FAzt = Fut, where
F is a G &times; G nonsingular matrix. The first equation satisfies the restrictions α&prime;1φ = 0, which
can be rewritten as ι&prime;Aφ = 0, where ι&prime; is the first row of an identity matrix IG. F must satisfy
the restriction that (first row of FA)φ = 0. But the first row of FA is the first row of F , say
f &prime;1, times A. This means that f
</p>
<p>&prime;
1(Aφ) = 0. For the first equation to be identified, this condition
</p>
<p>on the transformed first equation must be equivalent to ι&prime;Aφ = 0, up to a scalar constant. This
holds if and only if f &prime;1 is a scalar multiple of ι
</p>
<p>&prime;, and the latter condition holds if and only if the
rank (Aφ) = G&minus; 1. The latter is known as the rank condition for identification.
Example (A.5): Consider the simple Keynesian model given in example 1. The second equation
is an identity and the first equation satisfies the order condition of identification, since It is the
excluded exogenous variable from that equation, and there is only one right hand side included
endogenous variable Yt. In fact, the first equation is just-identified. Note that
</p>
<p>A = [B,Γ] =
</p>
<p>[
β11 β12 γ11 γ12
β21 β22 γ21 γ22
</p>
<p>]
=
</p>
<p>[
1 &minus;β &minus;α 0
&minus;1 1 0 &minus;1
</p>
<p>]
(A.20)
</p>
<p>and φ for the first equation consists of only one restriction, namely that It is not in that
equation, or γ12 = 0. This makes φ
</p>
<p>&prime; = (0, 0, 0, 1), since α&prime;1φ = 0 gives γ12 = 0. From (A.20),
Aφ = (γ12, γ22)
</p>
<p>&prime; = (0,&minus;1)&prime; and the rank (Aφ) = 1 = G &minus; 1. Hence, the rank condition holds
for the first equation and it is identified. Problem 8 reconsiders example (A.1), where both
equations are just-identified by the order condition of identification and asks the reader to show
that both satisfy the rank condition of identification as long as c 	= 0 and f 	= 0.
The reduced form of the simple Keynesian model is given in equations (11.4) and (11.5). In
</p>
<p>fact,
</p>
<p>Π =
</p>
<p>[
π11 π12
π21 π22
</p>
<p>]
=
</p>
<p>[
α β
α 1
</p>
<p>]
/(1&minus; β (A.21)
</p>
<p>Note that
</p>
<p>π11/π22 = α and π21/π22 = α (A.22)
</p>
<p>π12/π22 = β and (π22 &minus; 1)/π22 = β
Therefore, the structural parameters of the consumption equation can be retrieved from the
reduced form coefficients. However, what happens if we replace Π by its OLS estimate Π̂OLS?
Would the solution in (A.22) lead to two estimates of (α, β) or would this solution lead to a
unique estimate? In this case, the consumption equation is just-identified and the solution in
(A.22) is unique. To show this, recall that
</p>
<p>π̂12 = mci/mii; and π̂22 = myi/mii (A.23)
</p>
<p>Solving for β̂, using (A.22), one gets
</p>
<p>β̂ = π̂12/π̂22 = mci/myi (A.24)
</p>
<p>and
</p>
<p>β̂ = (π̂22 &minus; 1)/π̂22 = (mci &minus;myi)/myi (A.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 303
</p>
<p>(A.24) and (A.25) lead to a unique solution because equation (11.2) gives
</p>
<p>myi = mci +mii (A.26)
</p>
<p>In general, we would not be able to solve for the structural parameters of an unidentified
equation in terms of the reduced form parameters. However, when this equation is identified,
replacing the reduced form parameters by their OLS estimates would lead to a unique estimate
of the structural parameters, only if this equation is just-identified, and to more than one
estimate depending upon the degree of over-identification. Problem 8 gives another example of
the just-identified case, while problem 9 considers a model with one unidentified and another
over-identified equation.
</p>
<p>Example (A.6): Equations (11.13) and (11.14) give an unidentified demand and supply model
with
</p>
<p>B =
</p>
<p>[
1 &minus;β
1 &minus;δ
</p>
<p>]
and Γ =
</p>
<p>[
&minus;α
&minus;γ
</p>
<p>]
(A.27)
</p>
<p>The reduced form equations given by (11.16) and (11.17) yield
</p>
<p>Π = &minus;B&minus;1Γ =
[
π11
π21
</p>
<p>]
=
</p>
<p>[
αδ &minus; γβ
α&minus; γ
</p>
<p>]
/(δ &minus; β) (A.28)
</p>
<p>Note that one cannot solve for (α, β) nor (γ, δ) in terms of (π11, π21) without further restric-
tions.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 12
</p>
<p>Pooling Time-Series of Cross-Section Data
</p>
<p>12.1 Introduction
</p>
<p>In this chapter, we will consider pooling time-series of cross-sections. This may be a panel
of households or firms or simply countries or states followed over time. Two well known ex-
amples of panel data in the U.S. are the Panel Study of Income Dynamics (PSID) and the
National Longitudinal Survey (NLS). The PSID began in 1968 with 4802 families, including
an over-sampling of poor households. Annual interviews were conducted and socioeconomic
characteristics of each of the families and of roughly 31000 individuals who have been in these
or derivative families were recorded. The list of variables collected is over 5000. The NLS, fol-
lowed five distinct segments of the labor force. The original samples include 5020 older men,
5225 young men, 5083 mature women, 5159 young women and 12686 youths. There was an
over-sampling of blacks, hispanics, poor whites and military in the youths survey. The list of
variables collected runs into the thousands. An inventory of national studies using panel data is
given at http://www.isr.umich.edu/src/psid/panelstudies.html. Pooling this data gives a richer
source of variation which allows for more efficient estimation of the parameters. With additional,
more informative data, one can get more reliable estimates and test more sophisticated behav-
ioral models with less restrictive assumptions. Another advantage of panel data sets are their
ability to control for individual heterogeneity. Not controlling for these unobserved individual
specific effects leads to bias in the resulting estimates. Panel data sets are also better able to
identify and estimate effects that are simply not detectable in pure cross-sections or pure time-
series data. In particular, panel data sets are better able to study complex issues of dynamic
behavior. For example, with a cross-section data set one can estimate the rate of unemployment
at a particular point in time. Repeated cross-sections can show how this proportion changes
over time. Only panel data sets can estimate what proportion of those who are unemployed in
one period remain unemployed in another period. Some of the benefits and limitations of using
panel data sets are listed in Hsiao (2003) and Baltagi (2008). Section 12.2 studies the error com-
ponents model focusing on fixed effects, random effects and maximum likelihood estimation.
Section 12.3 considers the question of prediction in a random effects model, while Section 12.4
illustrates the estimation methods using an empirical example. Section 12.5 considers testing
the poolability assumption, the existence of random individual effects and the consistency of
the random effects estimator using a Hausman test. Section 12.6 studies the dynamic panel
data model and illustrates the methods used with an empirical example. Section 12.7 concludes
with a short presentation of program evaluation and the difference-in-differences estimator.
</p>
<p>12.2 The Error Components Model
</p>
<p>The regression model is still the same, but it now has double subscripts
</p>
<p>yit = α+X
&prime;
itβ + uit (12.1)
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_12, 305</p>
<p/>
</div>
<div class="page"><p/>
<p>306 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>where i denotes cross-sections and t denotes time-periods with i = 1, 2, . . . , N , and t = 1, 2,
. . . , T. α is a scalar, β is K &times; 1 and Xit is the it-th observation on K explanatory variables.
The observations are usually stacked with i being the slower index, i.e., the T observations on
the first household followed by the T observations on the second household, and so on, until we
get to the N -th household. Under the error components specification, the disturbances take the
form
</p>
<p>uit = μi + νit (12.2)
</p>
<p>where the μi&rsquo;s are cross-section specific components and νit are remainder effects. For example,
μi may denote individual ability in an earnings equation, or managerial skill in a production
function or simply a country specific effect. These effects are time-invariant.
In vector form, (12.1) can be written as
</p>
<p>y = αιNT +Xβ + u = Zδ + u (12.3)
</p>
<p>where y is NT &times; 1, X is NT &times;K, Z = [ιNT , X], δ&prime; = (α&prime;, β&prime;), and ιNT is a vector of ones of
dimension NT . Also, (12.2) can be written as
</p>
<p>u = Zμμ+ ν (12.4)
</p>
<p>where u&prime; = (u11, . . . , u1T , u21, . . . , u2T , . . . , uN1, . . . , uNT ) and Zμ = IN &otimes; ιT . IN is an identity
matrix of dimension N, ιT is a vector of ones of dimension T , and &otimes; denotes Kronecker product
defined in the Appendix to Chapter 7. Zμ is a selector matrix of ones and zeros, or simply the ma-
trix of individual dummies that one may include in the regression to estimate the μi&rsquo;s if they are
assumed to be fixed parameters. μ&prime; = (μ1, . . . , μN ) and ν
</p>
<p>&prime; = (ν11, . . . , ν1T , . . . , νN1, . . . , νNT ).
Note that ZμZ
</p>
<p>&prime;
μ = IN&otimes;JT where JT is a matrix of ones of dimension T , and P = Zμ(Z &prime;μZμ)&minus;1Z &prime;μ,
</p>
<p>the projection matrix on Zμ, reduces to P = IN&otimes;J̄T where J̄T = JT /T . P is a matrix which aver-
ages the observation across time for each individual, and Q = INT &minus;P is a matrix which obtains
the deviations from individual means. For example, Pu has a typical element ūi. =
</p>
<p>&sum;T
t=1 uit/T
</p>
<p>repeated T times for each individual and Qu has a typical element (uit &minus; ūi.). P and Q are (i)
symmetric idempotent matrices, i.e., P &prime; = P and P 2 = P . This means that the rank (P ) =
tr(P ) = N and rank (Q) = tr(Q) = N(T &minus; 1). This uses the result that rank of an idempotent
matrix is equal to its trace, see Graybill (1961, Theorem 1.63) and the Appendix to Chapter
7. Also, (ii) P and Q are orthogonal, i.e., PQ = 0 and (iii) they sum to the identity matrix
P +Q = INT . In fact, any two of these properties imply the third, see Graybill (1961, Theorem
1.68).
</p>
<p>12.2.1 The Fixed Effects Model
</p>
<p>If the μi&rsquo;s are thought of as fixed parameters to be estimated, then equation (12.1) becomes
</p>
<p>yit = α+X
&prime;
itβ +
</p>
<p>&sum;N
i=1 μiDi + νit (12.5)
</p>
<p>where Di is a dummy variable for the i-th household. Not all the dummies are included so as
not to fall in the dummy variable trap. One is usually dropped or equivalently, we can say that
there is a restriction on the μ&rsquo;s given by
</p>
<p>&sum;N
i=1 μi = 0. The νit&rsquo;s are the usual classical IID ran-
</p>
<p>dom variables with 0 mean and variance σ2ν . OLS on equation (12.5) is BLUE, but we have two</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 The Error Components Model 307
</p>
<p>problems, the first is the loss of degrees of freedom since in this case, we are estimating N +K
parameters. Also, with a lot of dummies we could be running into multicollinearity problems
and a large X &prime;X matrix to invert. For example, if N = 50 states, T = 10 years and we have
two explanatory variables, then with 500 observations we are estimating 52 parameters. Alter-
natively, we can think of this in an analysis of variance context and rearrange our observations,
say, on y in an (N &times; T ) matrix where rows denote firms and columns denote time periods.
</p>
<p>t
1 2 .. T
</p>
<p>1 y11 y12 .. y1T y1.
i 2 y21 y22 .. y2T y2.
</p>
<p>: : : .. : :
N yN1 yN2 .. yNT yN.
</p>
<p>where yi. =
&sum;T
</p>
<p>t=1 yit and ȳi. = yi./T . For the simple regression with one regressor, the model
given in (12.1) becomes
</p>
<p>yit = α+ βxit + μi + νit (12.6)
</p>
<p>averaging over time gives
</p>
<p>ȳi. = α+ βx̄i. + μi + ν̄i. (12.7)
</p>
<p>and averaging over all observations gives
</p>
<p>ȳ.. = α+ βx̄.. + ν̄.. (12.8)
</p>
<p>where ȳ.. =
&sum;N
</p>
<p>i=1
</p>
<p>&sum;T
t=1 yit/NT . Equation (12.8) follows because the μi&rsquo;s sum to zero. Defining
</p>
<p>ỹit = (yit &minus; ȳi.) and x̃it and ν̃it similarly, we get
</p>
<p>yit &minus; ȳi. = β(xit &minus; x̄i.) + (νit &minus; ν̄i.)
</p>
<p>or
</p>
<p>ỹit = βx̃it + ν̃it (12.9)
</p>
<p>Running OLS on equation (12.9) leads to the same estimator of β as that obtained from equation
(12.5). This is called the least squares dummy variable estimator (LSDV) or β̃ in our notation.
It is also known as the Within estimator since
</p>
<p>&sum;N
i=1
</p>
<p>&sum;T
t=1 x̃
</p>
<p>2
it is the within sum of squares in an
</p>
<p>analysis of variance framework. One can then retrieve an estimate of α from equation (12.8) as
α̃ = ȳ.. &minus; β̃x̄... Similarly, if we are interested in the μi&rsquo;s, those can also be retrieved from (12.7)
and (12.8) as follows:
</p>
<p>μ̃i = (ȳi. &minus; ȳ..)&minus; β̃(x̄i. &minus; x̄..) (12.10)
</p>
<p>In matrix form, one can substitute the disturbances given by (12.4) into (12.3) to get
</p>
<p>y = αιNT +Xβ + Zμμ+ ν = Zδ + Zμμ+ ν (12.11)
</p>
<p>and then perform OLS on (12.11) to get estimates of α, β and μ. Note that Z is NT &times; (K +1)
and Zμ, the matrix of individual dummies is NT &times; N . If N is large, (12.11) will include too</p>
<p/>
</div>
<div class="page"><p/>
<p>308 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>many individual dummies, and the matrix to be inverted by OLS is large and of dimension
(N +K). In fact, since α and β are the parameters of interest, one can obtain the least squares
dummy variables (LSDV) estimator from (12.11), by residualing out the Zμ variables, i.e., by
premultiplying the model by Q, the orthogonal projection of Zμ, and performing OLS
</p>
<p>Qy = QXβ +Qν (12.12)
</p>
<p>This uses the fact that QZμ = QιNT = 0, since PZμ = Zμ. In other words, the Q matrix
wipes out the individual effects. Recall, the FWL Theorem in Chapter 7. This is a regression
of ỹ = Qy with typical element (yit &minus; ȳi.) on X̃ = QX with typical element (Xit,k &minus; X̄i.,k) for
the k-th regressor, k = 1, 2, . . . ,K. This involves the inversion of a (K &times;K) matrix rather than
(N +K)&times; (N +K) as in (12.11). The resulting OLS estimator is
</p>
<p>β̃ = (X &prime;QX)&minus;1X &prime;Qy (12.13)
</p>
<p>with var(β̃) = σ2ν(X
&prime;QX)&minus;1 = σ2ν(X̃
</p>
<p>&prime;X̃)&minus;1.
Note that this fixed effects (FE) estimator cannot estimate the effect of any time-invariant
</p>
<p>variable like sex, race, religion, schooling, or union participation. These time-invariant variables
are wiped out by the Q transformation, the deviations from means transformation. Alterna-
tively, one can see that these time-invariant variables are spanned by the individual dummies
in (12.5) and therefore any regression package attempting (12.5) will fail, signaling perfect mul-
ticollinearity. If (12.5) is the true model, LSDV is BLUE as long as νit is the standard classical
disturbance with mean 0 and variance covariance matrix σ2νINT . Note that as T &rarr; &infin;, the FE
estimator is consistent. However, if T is fixed and N &rarr; &infin; as typical in short labor panels, then
only the FE estimator of β is consistent, the FE estimators of the individual effects (α + μi)
are not consistent since the number of these parameters increase as N increases.
</p>
<p>Testing for Fixed Effects: One could test the joint significance of these dummies, i.e., H0;
μ1 = μ2 = .. = μN&minus;1 = 0, by performing an F -test. This is a simple Chow test given in (4.17)
with the restricted residual sums of squares (RRSS) being that of OLS on the pooled model
and the unrestricted residual sums of squares (URSS) being that of the LSDV regression. If N
is large, one can perform the within transformation and use that residual sum of squares as the
URSS. In this case
</p>
<p>F0 =
(RRSS &minus;URSS )/(N &minus; 1)
URSS/(NT &minus;N &minus;K)
</p>
<p>H0
&sim; FN&minus;1,N(T&minus;1)&minus;K (12.14)
</p>
<p>Computational Warning: One computational caution for those using theWithin regression given
by (12.12). The s2 of this regression as obtained from a typical regression package divides the
residual sums of squares by NT &minus; K since the intercept and the dummies are not included.
The proper s2, say s&lowast;2 from the LSDV regression in (12.5) would divide the same residual sums
of squares by N(T &minus; 1) &minus; K. Therefore, one has to adjust the variances obtained from the
within regression (12.12) by multiplying the variance-covariance matrix by (s&lowast;2/s2) or simply
by multiplying by [NT &minus;K]/[N(T &minus; 1)&minus;K].
</p>
<p>12.2.2 The Random Effects Model
</p>
<p>There are too many parameters in the fixed effects model and the loss of degrees of freedom
can be avoided if the μi&rsquo;s can be assumed random. In this case μi &sim; IID(0, σ
</p>
<p>2
μ), νit &sim; IID(0, σ
</p>
<p>2
ν)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 The Error Components Model 309
</p>
<p>and the μi&rsquo;s are independent of the νit&rsquo;s. In addition, the Xit&rsquo;s are independent of the μi&rsquo;s and
νit&rsquo;s for all i and t. The random effects model is an appropriate specification if we are drawing
N individuals randomly from a large population.
</p>
<p>This specification implies a homoskedastic variance var(uit) = σ
2
μ + σ
</p>
<p>2
ν for all i and t, and
</p>
<p>an equi-correlated block-diagonal covariance matrix which exhibits serial correlation over time
only between the disturbances of the same individual. In fact,
</p>
<p>cov(uit, ujs) = σ
2
μ + σ
</p>
<p>2
ν for i = j, t = s (12.15)
</p>
<p>= σ2μ for i = j, t 	= s
and zero otherwise. This also means that the correlation coefficient between uit and ujs is
</p>
<p>ρ = correl(uit, ujs) = 1 for i = j, t = s (12.16)
</p>
<p>= σ2μ/(σ
2
μ + σ
</p>
<p>2
ν) for i = j, t 	= s
</p>
<p>and zero otherwise. From (12.4), one can compute the variance-covariance matrix
</p>
<p>Ω = E(uu&prime;) = ZμE(μμ
&prime;)Z &prime;μ + E(νν
</p>
<p>&prime;) = σ2μ(IN &otimes; JT ) + σ2ν(IN &otimes; IT ) (12.17)
</p>
<p>In order to obtain the GLS estimator of the regression coefficients, we need Ω&minus;1. This is a huge
matrix for typical panels and is of dimension (NT &times;NT ). No brute force inversion should be
attempted even if the researcher&rsquo;s application has a small N and T . For example, if we observe
N = 20 firms over T = 5 time periods, Ω will be 100 by 100. We will follow a simple trick devised
by Wansbeek and Kapteyn (1982) that allows the deviation of Ω&minus;1 and Ω&minus;1/2. Essentially, one
replaces JT by T J̄T , and IT by (ET + J̄T ) where ET is by definition (IT &minus; J̄T ). In this case:
</p>
<p>Ω = Tσ2μ(IN &otimes; J̄T ) + σ2ν(IN &otimes; ET ) + σ2ν(IN &otimes; J̄T )
</p>
<p>collecting terms with the same matrices, we get
</p>
<p>Ω = (Tσ2μ + σ
2
ν)(IN &otimes; J̄T ) + σ2ν(IN &otimes; ET ) = σ21P + σ2νQ (12.18)
</p>
<p>where σ21 = Tσ
2
μ + σ
</p>
<p>2
ν . (12.18) is the spectral decomposition representation of Ω, with σ
</p>
<p>2
1
</p>
<p>being the first unique characteristic root of Ω of multiplicity N and σ2ν is the second unique
characteristic root of Ω of multiplicity N(T &minus; 1). It is easy to verify, using the properties of P
and Q, that
</p>
<p>Ω&minus;1 =
1
</p>
<p>σ21
P +
</p>
<p>1
</p>
<p>σ2ν
Q (12.19)
</p>
<p>and
</p>
<p>Ω&minus;1/2 =
1
</p>
<p>σ1
P +
</p>
<p>1
</p>
<p>σν
Q (12.20)
</p>
<p>In fact, Ωr = (σ21)
rP + (σ2ν)
</p>
<p>rQ where r is an arbitrary scalar. Now we can obtain GLS as
a weighted least squares. Fuller and Battese (1974) suggested premultiplying the regression
equation given in (12.3) by σνΩ
</p>
<p>&minus;1/2 = Q + (σν/σ1)P and performing OLS on the resulting
transformed regression. In this case, y&lowast; = σνΩ&minus;1/2y has a typical element yit &minus; θȳi. where
θ = 1&minus; (σν/σ1). This transformed regression inverts a matrix of dimension (K +1) and can be
easily implemented using any regression package.</p>
<p/>
</div>
<div class="page"><p/>
<p>310 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>The Best Quadratic Unbiased (BQU) estimators of the variance components arise naturally
from the spectral decomposition of Ω. In fact, Pu &sim; (0, σ21P ) and Qu &sim; (0, σ
</p>
<p>2
νQ) and
</p>
<p>σ̂21 =
u&prime;Pu
tr(P )
</p>
<p>= T
&sum;N
</p>
<p>i=1 ū
2
i./N (12.21)
</p>
<p>and
</p>
<p>σ̂2ν =
u&prime;Qu
tr(Q)
</p>
<p>= T
&sum;N
</p>
<p>i=1
</p>
<p>&sum;T
t=1(uit &minus; ūi.)2/N(T &minus; 1) (12.22)
</p>
<p>provide the BQU estimators of σ21 and σ
2
ν , respectively, see Balestra (1973).
</p>
<p>These are analysis of variance type estimators of the variance components and are MVU under
normality of the disturbances, see Graybill (1961). The true disturbances are not known and
therefore (12.21) and (12.22) are not feasible. Wallace and Hussain (1969) suggest substituting
OLS residuals ûOLS instead of the true u&rsquo;s. After all, the OLS estimates are still unbiased and
consistent, but no longer efficient. Amemiya (1971) shows that these estimators of the variance
components have a different asymptotic distribution from that knowing the true disturbances.
He suggests using the LSDV residuals instead of the OLS residuals. In this case ũ = y&minus; α̃ιNT &minus;
Xβ̃ where α̃ = ȳ..&minus;X̄ &prime;..β̃ and X̄ &prime;.. is a 1&times;K vector of averages of all regressors. Substituting these
ũ&rsquo;s for u in (12.21) and (12.22) we get the Amemiya-type estimators of the variance components.
The resulting estimates of the variance components have the same asymptotic distribution as
that knowing the true disturbances.
Swamy and Arora (1972) suggest running two regressions to get estimates of the variance
</p>
<p>components from the corresponding mean square errors of these regressions. The first regression
is the Within regression, given in (12.12), which yields the following s2:
</p>
<p>̂̂σ2ν = [y&prime;Qy &minus; y&prime;QX(X &prime;QX)&minus;1X &prime;Qy]/[N(T &minus; 1)&minus;K] (12.23)
</p>
<p>The second regression is the Between regression which runs the regression of averages across
time, i.e.,
</p>
<p>ȳi. = α+ X̄
&prime;
i.β + ūi. i = 1, . . . , N (12.24)
</p>
<p>This is equivalent to premultiplying the model in (12.11) by P and running OLS. The only
caution is that the latter regression has NT observations because it repeats the averages T times
for each individual, while the cross-section regression in (12.24) is based on N observations. To
remedy this, one can run the cross-section regression
</p>
<p>yi./
&radic;
T = α(
</p>
<p>&radic;
T ) + (X &prime;i./
</p>
<p>&radic;
T )β + ui./
</p>
<p>&radic;
T (12.25)
</p>
<p>where one can easily verify that var(ui./
&radic;
T ) = σ21. This regression will yield an s
</p>
<p>2 given by
</p>
<p>̂̂σ21 = (y&prime;Py &minus; y&prime;PZ(Z &prime;PZ)&minus;1Z &prime;Py)/(N &minus;K &minus; 1) (12.26)
</p>
<p>Note that stacking the following two transformed regressions we just performed yields
</p>
<p>(
Qy
Py
</p>
<p>)
=
</p>
<p>(
QZ
PZ
</p>
<p>)
δ +
</p>
<p>(
Qu
Pu
</p>
<p>)
(12.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 The Error Components Model 311
</p>
<p>and the transformed error has mean 0 and variance-covariance matrix given by
</p>
<p>(
σ2νQ 0
0 σ21P
</p>
<p>)
</p>
<p>Problem 6 asks the reader to verify that OLS on this system of 2NT observations yields OLS
on the pooled model (12.3). Also, GLS on this system yields GLS on (12.3). Alternatively, one
could get rid of the constant α by running the following stacked regressions:
</p>
<p>(
Qy
(P &minus; J̄NT )y
</p>
<p>)
=
</p>
<p>(
QX
(P &minus; J̄NT )X
</p>
<p>)
β +
</p>
<p>(
Qu
(P &minus; J̄NT )u
</p>
<p>)
(12.28)
</p>
<p>This follows from the fact the QιNT = 0 and (P &minus; J̄NT )ιNT = 0. The transformed error has
zero mean and variance-covariance matrix
</p>
<p>(
σ2νQ 0
0 σ21(P &minus; J̄NT )
</p>
<p>)
(12.29)
</p>
<p>OLS on this system, yields OLS on (12.3) and GLS on (12.28) yields GLS on (12.3). In fact,
</p>
<p>β̂GLS = [(X
&prime;QX/σ2ν) +X
</p>
<p>&prime;(P &minus; J̄NT )X/σ21]&minus;1[(X &prime;Qy/σ2ν) + (X &prime;(P &minus; J̄NT )y/σ21)]
= [WXX + φ
</p>
<p>2BXX ]
&minus;1[WXy + φ
</p>
<p>2BXy] (12.30)
</p>
<p>with var(β̂GLS) = σ
2
ν [WXX + φ
</p>
<p>2BXX ]
&minus;1. Note that WXX = X &prime;QX, BXX = X &prime;(P &minus; J̄NT )X
</p>
<p>and φ2 = σ2ν/σ
2
1. Also, the Within estimator of β is β̃Within = W
</p>
<p>&minus;1
XXWXy and the Between
</p>
<p>estimator β̂Between = B
&minus;1
XXBXy. This shows that β̂GLS is a matrix weighted average of β̃Within
</p>
<p>and β̂Between weighing each estimate by the inverse of its corresponding variance. In fact
</p>
<p>β̂GLS = W1β̃Within +W2β̂Between (12.31)
</p>
<p>where W1 = [WXX + φ
2BXX ]
</p>
<p>&minus;1WXX and W2 = [WXX + φ
2BXX ]
</p>
<p>&minus;1(φ2BXX) = I &minus;W1. This
was demonstrated by Maddala (1971). Note that (i) if σ2μ = 0, then φ
</p>
<p>2 = 1 and β̂GLS reduces
</p>
<p>to β̂OLS . (ii) If T &rarr; &infin;, then φ2 &rarr; 0 and β̂GLS tends to β̃Within. (iii) If φ2 &rarr; &infin;, then β̂GLS
tends to β̂Between. In other words, the Within estimator ignores the between variation, and
the Between estimator ignores the within variation. The OLS estimator gives equal weight to
the between and within variations. From (12.30), it is clear that var(β̃Within)&minus; var(β̂GLS) is a
positive semi-definite matrix, since φ2 is positive. However as T &rarr; &infin; for any fixed N , φ2 &rarr; 0
and both β̂GLS and β̃Within have the same asymptotic variance.
Another estimator of the variance components was suggested by Nerlove (1971). His sugges-
</p>
<p>tion is to estimate σ̂2μ =
&sum;N
</p>
<p>i=1(μ̂i &minus; μ̂)2/(N &minus; 1) where μ̂i are the dummy coefficients estimates
from the LSDV regression. σ̂2ν is estimated from the within residual sums of squares divided by
NT without correction for degrees of freedom.
</p>
<p>Note that, except for Nerlove&rsquo;s (1971) method, one has to retrieve σ̂2μ as (σ̂
2
1&minus; σ̂2ν)/T . In this
</p>
<p>case, there is no guarantee that the estimate of σ̂2μ would be non-negative. Searle (1971) has
an extensive discussion of the problem of negative estimates of the variance components in the
biometrics literature. One solution is to replace these negative estimates by zero. This in fact is
the suggestion of the Monte Carlo study by Maddala and Mount (1973). This study finds that
negative estimates occurred only when the true σ2μ was small and close to zero. In these cases</p>
<p/>
</div>
<div class="page"><p/>
<p>312 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>OLS is still a viable estimator. Therefore, replacing negative σ̂2μ by zero is not a bad sin after
all, and the problem is dismissed as not being serious.
Under the random effects model, GLS based on the true variance components is BLUE, and
</p>
<p>all the feasible GLS estimators considered are asymptotically efficient as either N or T &rarr; &infin;.
Maddala and Mount (1973) compared OLS, Within, Between, feasible GLS methods, true GLS
and MLE using their Monte Carlo study. They found little to choose among the various feasible
GLS estimators in small samples and argued in favor of methods that were easier to compute.
Taylor (1980) derived exact finite sample results for the one-way error components model. He
</p>
<p>compared the Within estimator with the Swamy-Arora feasible GLS estimator. He found the
following important results: (1) Feasible GLS is more efficient than FE for all but the fewest
degrees of freedom. (2) The variance of feasible GLS is never more than 17% above the Cramér-
Rao lower bound. (3) More efficient estimators of the variance components do not necessarily
yield more efficient feasible GLS estimators. These finite sample results are confirmed by the
Monte Carlo experiments carried out by Maddala and Mount (1973) and Baltagi (1981).
</p>
<p>12.2.3 Maximum Likelihood Estimation
</p>
<p>Under normality of the disturbances, one can write the log-likelihood function as
</p>
<p>L(α, β, φ2, σ2ν) = constant&minus;
NT
</p>
<p>2
logσ2ν +
</p>
<p>N
</p>
<p>2
logφ2 &minus; 1
</p>
<p>2σ2ν
u&prime;Σ&minus;1u (12.32)
</p>
<p>where Ω = σ2νΣ, φ
2 = σ2ν/σ
</p>
<p>2
1 and Σ = Q + φ
</p>
<p>&minus;2P from (12.18). This uses the fact that |Ω| =
product of its characteristic roots = (σ2ν)
</p>
<p>N(T&minus;1)(σ21)
N = (σ2ν)
</p>
<p>NT (φ2)&minus;N . Note that there is a
one-to-one correspondence between φ2 and σ2μ. In fact, 0 &le; σ2μ &lt; &infin; translates into 0 &lt; φ2 &le; 1.
Brute force maximization of (12.32) leads to nonlinear first-order conditions, see Amemiya
(1971). Instead, Breusch (1987) concentrates the likelihood with respect to α and σ2ν . In this
case, α̂MLE = ȳ.. &minus; X̄ &prime;..β̂MLE and σ̂2ν,MLE = û&prime;Σ̂&minus;1û/NT where û and Σ̂ are based on MLE&rsquo;s of
β, φ2 and α. Let d = y&minus;Xβ̂MLE then α̂MLE = ι&prime;NTd/NT and û = d&minus; ιNT α̂ = d&minus; J̄NTd. This
implies that σ̂2ν,MLE can be rewritten as
</p>
<p>σ̂2ν,MLE = d
&prime;[Q+ φ2(P &minus; J̄NT )]d/NT (12.33)
</p>
<p>and the concentrated log-likelihood becomes
</p>
<p>Lc(β, φ
2) = constant&minus; NT
</p>
<p>2
log{d&prime;[Q+ φ2(P &minus; J̄NT )]d}+
</p>
<p>N
</p>
<p>2
logφ2 (12.34)
</p>
<p>Maximizing (12.34), over φ2 given β, yields
</p>
<p>φ̂
2
=
</p>
<p>d&prime;Qd
</p>
<p>(T &minus; 1)d&prime;(P &minus; J̄NT )d
=
</p>
<p>&sum;N
i=1
</p>
<p>&sum;T
t=1(dit &minus; d̄i.)2
</p>
<p>T (T &minus; 1)&sum;Ni=1(d̄i. &minus; d̄..)2
(12.35)
</p>
<p>Maximizing (12.34) over β, given φ2, yields
</p>
<p>β̂MLE =
{
X &prime;[Q+ φ2(P &minus; J̄NT )]X
</p>
<p>}&minus;1
X &prime;[Q+ φ2(P &minus; J̄NT )]y (12.36)
</p>
<p>One can iterate between β and φ2 until convergence. Breusch (1987) shows that provided T &gt; 1,
any i-th iteration β, call it βi, gives 0 &lt; φ
</p>
<p>2
i+1 &lt; &infin; in the (i+1)th iteration. More importantly,</p>
<p/>
</div>
<div class="page"><p/>
<p>313
</p>
<p>Breusch (1987) shows that these φ2i &rsquo;s have a remarkable property of forming a monotonic se-
quence. In fact, starting from the Within estimator of β, for φ2 = 0, the next φ2 is finite and
positive and starts a monotonically increasing sequence of φ2&rsquo;s. Similarly, starting from the
Between estimator of β, for (φ2 &rarr; &infin;) the next φ2 is finite and positive and starts a monotoni-
cally decreasing sequence of φ2&rsquo;s. Hence, to guard against the possibility of a local maximum,
Breusch (1987) suggests starting with β̃Within and β̂Between and iterating. If these two sequences
converge to the same maximum, then this is the global maximum. If one starts with β̂OLS for
φ2 = 1, and the next iteration obtains a larger φ2, then we have a local maximum at the bound-
ary φ2 = 1. Maddala (1971) finds that there are at most two maxima for the likelihood L(φ2)
for 0 &lt; φ2 &le; 1. Hence, we have to guard against one local maximum.
</p>
<p>12.3 Prediction
</p>
<p>Suppose we want to predict S periods ahead for the i-th individual. For the random effects
model, the BLU estimator is GLS. Using the results in Chapter 9 on GLS, Goldberger&rsquo;s (1962)
Best Linear Unbiased Predictor (BLUP) of yi,T+S is
</p>
<p>ŷi,T+S = Z
&prime;
i,T+S δ̂GLS + w
</p>
<p>&prime;Ω&minus;1ûGLS for S &ge; 1 (12.37)
</p>
<p>where ûGLS = y &minus; Zδ̂GLS and w = E(ui,T+Su). Note that
</p>
<p>ui,T+S = μi + νi,T+S (12.38)
</p>
<p>and w = σ2μ(ℓi &otimes; ιT ) where ℓi is the i-th column of IN , i.e., ℓi is a vector that has 1 in the i-th
position and zero elsewhere. In this case
</p>
<p>w&prime;Ω&minus;1 = σ2μ(ℓ
&prime;
i &otimes; ι&prime;T )
</p>
<p>[
1
</p>
<p>σ21
P +
</p>
<p>1
</p>
<p>σ2ν
Q
</p>
<p>]
=
</p>
<p>σ2μ
σ21
</p>
<p>(ℓ&prime;i &otimes; ι&prime;T ) (12.39)
</p>
<p>since (ℓ&prime;i &otimes; ι&prime;T )P = (ℓ&prime;i &otimes; ι&prime;T ) and (ℓ&prime;i &otimes; ι&prime;T )Q = 0. The typical element of w&prime;Ω&minus;1ûGLS is
(Tσ2μ/σ
</p>
<p>2
1)̂̄ui.,GLS where ̂̄ui.,GLS =
</p>
<p>&sum;T
t=1 ûit,GLS/T . Therefore, in (12.37), the BLUP for yi,T+S
</p>
<p>corrects the GLS prediction by a fraction of the mean of the GLS residuals corresponding to
that i-th individual. This predictor was considered by Wansbeek and Kapteyn (1978) and Taub
(1979).
</p>
<p>12.4 Empirical Example
</p>
<p>Baltagi and Griffin (1983) considered the following gasoline demand equation:
</p>
<p>log
Gas
</p>
<p>Car
= α+ β1log
</p>
<p>Y
</p>
<p>N
+ β2log
</p>
<p>PMG
PGDP
</p>
<p>+ β3log
Car
</p>
<p>N
+ u (12.40)
</p>
<p>where Gas/Car is motor gasoline consumption per auto, Y/N is real income per capita, PMG/
PGDP is real motor gasoline price and Car/N denotes the stock of cars per capita. This panel
consists of annual observations across eighteen OECD countries, covering the period 1960&ndash;1978.
The data for this example are provided on the Springer web site as GASOLINE.DAT. Table 12.1
</p>
<p>12.4 Empirical Example</p>
<p/>
</div>
<div class="page"><p/>
<p>314 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>gives the Stata output for the Within estimator using xtreg, fe. This is the regression described
in (12.5) and computed as in (12.9). The Within estimator gives a low price elasticity for
gasoline demand of -.322. The F -statistic for the significance of the country effects described in
(12.14) yields an observed value of 83.96. This is distributed under the null as an F (17, 321)
and is statistically significant. This F -statistic is printed by Stata below the fixed effects output.
In EViews, one invokes the test for redundant effects after running the fixed effects regression.
</p>
<p>Table 12.1 Fixed Effects Estimator &ndash; Gasoline Demand Data
</p>
<p>Coef. Std. Err. T P &gt; |t| [95% Conf. Interval]
</p>
<p>log(Y/N) 0.6622498 0.073386 9.02 0.000 0.5178715 0.8066282
log(PMG/PGDP ) &ndash;0.3217025 0.0440992 &ndash;7.29 0.000 &ndash;0.4084626 &ndash;0.2349425
log(Car/N) &ndash;0.6404829 0.0296788 &ndash;21.58 0.000 &ndash;0.6988725 &ndash;0.5820933
Constant 2.40267 0.2253094 10.66 0.000 1.959401 2.84594
</p>
<p>sigma u 0.34841289
sigma e 0.09233034
Rho 0.93438173 (fraction of variance due to u i)
</p>
<p>Table 12.2 gives the Stata output for the Between estimator using xtreg, be.This is based on
the regression given in (12.24). The Between estimator yields a high price elasticity of gasoline
demand of -.964. These results were also verified using TSP.
</p>
<p>Table 12.2 Between Estimator &ndash; Gasoline Demand Data
</p>
<p>Coef. Std. Err. T P &gt; |t| [95% Conf. Interval]
</p>
<p>log(Y/N) 0.9675763 0.1556662 6.22 0.000 0.6337055 1.301447
log(PMG/PGDP ) &ndash;0.9635503 0.1329214 &ndash;7.25 0.000 &ndash;1.248638 &ndash;0.6784622
log(Car/N) &ndash;0.795299 0.0824742 &ndash;9.64 0.000 &ndash;0.9721887 &ndash;0.6184094
Constant 2.54163 0.5267845 4.82 0.000 1.411789 3.67147
</p>
<p>Table 12.3 gives the Stata output for the random effect model using xtreg, re. This is the
Swamy and Arora (1972) estimator which yields a price elasticity of -.420. This is closer to the
Within estimator than the Between estimator.
</p>
<p>Table 12.3 Random Effects Estimator &ndash; Gasoline Demand Data
</p>
<p>Coef. Std. Err. T P &gt; |t| [95% Conf. Interval]
</p>
<p>log(Y/N) 0.5549858 0.0591282 9.39 0.000 0.4390967 0.6708749
log(PMG/PGDP ) &ndash;0.4203893 0.0399781 &ndash;10.52 0.000 &ndash;0.498745 &ndash;0.3420336
log(Car/N) &ndash;0.6068402 0.025515 &ndash;23.78 0.000 &ndash;0.6568487 &ndash;0.5568316
Constant 1.996699 0.184326 10.83 0.000 1.635427 2.357971
</p>
<p>sigma u 0.19554468
sigma e 0.09233034
Rho 0.81769 (fraction of variance due to u i)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Empirical Example 315
</p>
<p>Table 12.4 Gasoline Demand Data. One-way Error Component Results
</p>
<p>β1 β2 β3 ρ
OLS 0.890
</p>
<p>(0.036)&lowast;
&minus;0.892
(0.030)&lowast;
</p>
<p>&minus;0.763
(0.019)&lowast;
</p>
<p>0
</p>
<p>WALHUS 0.545
(0.066)
</p>
<p>&minus;0.447
(0.046)
</p>
<p>&minus;0.605
(0.029)
</p>
<p>0.75
</p>
<p>AMEMIYA 0.602
(0.066)
</p>
<p>&minus;0.366
(0.042)
</p>
<p>&minus;0.621
(0.029)
</p>
<p>0.93
</p>
<p>SWAR 0.555
(0.059)
</p>
<p>&minus;0.402
(0.042)
</p>
<p>&minus;0.607
(0.026)
</p>
<p>0.82
</p>
<p>IMLE 0.588
(0.066)
</p>
<p>&minus;0.378
(0.046)
</p>
<p>&minus;0.616
(0.029)
</p>
<p>0.91
</p>
<p>&lowast; These are biased standard errors when the true model has error component disturbances (see Moulton, 1986).
</p>
<p>Source: Baltagi and Griffin (1983). Reproduced by permission of Elsevier Science Publishers B.V. (North-Holland).
</p>
<p>Table 12.5 Gasoline Demand Data. Wallace and Hussain (1969) Estimator
</p>
<p>Dependent Variable: GAS
Method: Panel EGLS (Cross-section random effects)
</p>
<p>Sample: 1960 1978
Periods included: 19
Cross-sections included: 18
Total panel (balanced) observations: 342
Wallace and Hussain estimator of component variances
</p>
<p>Coefficient Std. Error t-Statistic Prob.
</p>
<p>C 1.938318 0.201817 9.604333 0.0000
log(Y/N) 0.545202 0.065555 8.316682 0.0000
log(PMG/PGDP ) &minus;0.447490 0.045763 &minus;9.778438 0.0000
log(Car/N) &minus;0.605086 0.028838 &minus;20.98191 0.0000
</p>
<p>Effects Specification
S.D. Rho
</p>
<p>Cross-section random 0.196715 0.7508
Idiosyncratic random 0.113320 0.2492
</p>
<p>Table 12.4 gives the parameter estimates for OLS and three feasible GLS estimates of the slope
coefficients along with their standard errors, and the corresponding estimate of ρ defined in
(12.16). These were obtained using EViews by invoking the random effects estimation on the
individual effects and choosing the estimation method from the options menu. Breusch&rsquo;s (1987)
iterative maximum likelihood was computed using Stata(xtreg,mle) and TSP.
</p>
<p>Table 12.5 gives the EViews output for the Wallace and Hussain (1969) random effects estima-
tor, while Table 12.6 gives the EViews output for the Amemiya (1971) random effects estimator.
Note that EViews calls the Amemiya estimator Wansbeek and Kapteyn (1989) since the latter
paper generalizes this method to deal with unbalanced panels with missing observations, see
Baltagi (2008) for details. Table 12.6 gives the Stata maximum likelihood output.</p>
<p/>
</div>
<div class="page"><p/>
<p>316 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>Table 12.6 Gasoline Demand Data. Wansbeek and Kapteyn (1989) Estimator
</p>
<p>Dependent Variable: GAS
Method: Panel EGLS (Cross-section random effects)
</p>
<p>Sample: 1960 1978
Periods included: 19
Cross-sections included: 18
Total panel (balanced) observations: 342
Wallace and Hussain estimator of component variances
</p>
<p>Coefficient Std. Error t-Statistic Prob.
</p>
<p>C 2.188322 0.216372 10.11372 0.0000
log(Y/N) 0.601969 0.065876 9.137941 0.0000
log(PMG/PGDP ) &minus;0.365500 0.041620 &minus;8.781832 0.0000
log(Car/N) &minus;0.620725 0.027356 &minus;22.69053 0.0000
</p>
<p>Effects Specification
S.D. Rho
</p>
<p>Cross-section random 0.343826 0.9327
Idiosyncratic random 0.092330 0.0673
</p>
<p>Table 12.7 Gasoline Demand Data. Random Effects Maximum Likelihood Estimator
</p>
<p>. xtreg c y p car,mle
</p>
<p>Random-effects ML regression Number of obs = 342
Group variable (i): coun Number of groups = 18
</p>
<p>Random effects u i &sim; Gaussian Obs per group: min = 19
avg = 19.0
max = 19
</p>
<p>LR chi2(3) = 609.75
</p>
<p>Log likelihood = 282.47697 Prob &gt; chi2 = 0.0000
</p>
<p>c Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
</p>
<p>log(Y/N) .5881334 .0659581 8.92 0.000 .4588578 .717409
log(PMG/PGDP ) &minus;.3780466 .0440663 &minus;8.58 0.000 &minus;.464415 &minus;.2916782
log(Car/N) &minus;.6163722 .0272054 &minus;22.66 0.000 &minus;.6696938 &minus;.5630506
cons 2.136168 .2156039 9.91 0.000 1.713593 2.558744
</p>
<p>sigma u .2922939 .0545496 .2027512 .4213821
sigma e .0922537 .0036482 .0853734 .0996885
rho .9094086 .0317608 .8303747 .9571561
</p>
<p>Likelihood-ratio test of sigma u = 0: chibar2(01)= 463.97 Prob &gt;= chibar2 = 0.000</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Testing in a Pooled Model 317
</p>
<p>12.5 Testing in a Pooled Model
</p>
<p>(1) The Chow-Test
</p>
<p>Before pooling the data one may be concerned whether the data is poolable. This hypothesis
is also known as the stability of the regression equation across firms or across time. It can be
formulated in terms of an unrestricted model which involves a separate regression equation for
each firm
</p>
<p>yi = Ziδi + ui for i = 1, 2, . . . , N (12.41)
</p>
<p>where y&prime;i = (yi1, . . . , yiT ), Zi = [ιT , Xi] and Xi is (T &times; K). δ&prime;i is 1 &times; (K + 1) and ui is T &times; 1.
The important thing to notice is that δi is different for every regional equation. We want to
test the hypothesis H0; δi=δ for all i, versus H1; δi 	= δ for some i. Under H0 we can write the
restricted model given in (12.41) as:
</p>
<p>y = Zδ + u (12.42)
</p>
<p>where Z &prime; = (Z &prime;1, Z
&prime;
2, . . . , Z
</p>
<p>&prime;
N ) and u
</p>
<p>&prime; = (u&prime;1, u
&prime;
2, . . . , u
</p>
<p>&prime;
N ). The unrestricted model can also be
</p>
<p>written as
</p>
<p>y =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>Z1 0 . . . 0
0 Z2 . . . 0
...
</p>
<p>. . .
...
</p>
<p>0 0 . . . ZN
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>δ1
δ2
...
δN
</p>
<p>⎞
⎟⎟⎟⎠+ u = Z
</p>
<p>&lowast;δ&lowast; + u (12.43)
</p>
<p>where δ&lowast;&prime; = (δ&prime;1, δ
&prime;
2, . . . , δ
</p>
<p>&prime;
N ) and Z = Z
</p>
<p>&lowast;I&lowast; with I&lowast; = (ιN &otimes; IK&prime;), an NK &prime; &times; K &prime; matrix, with
K &prime; = K + 1. Hence the variables in Z are all linear combinations of the variables in Z&lowast;. Under
the assumption that u &sim; N(0, σ2INT ), the MVU estimator for δ in equation (12.42) is
</p>
<p>δ̂OLS = δ̂MLE = (Z
&prime;Z)&minus;1Z &prime;y (12.44)
</p>
<p>and therefore
</p>
<p>y = Zδ̂OLS + e (12.45)
</p>
<p>implying that e = (INT &minus; Z(Z &prime;Z)&minus;1Z &prime;)y = My = M(Zδ + u) = Mu since MZ = 0. Similarly,
under the alternative, the MVU for δi is given by
</p>
<p>δ̂i,OLS = δ̂i,MLE = (Z
&prime;
iZi)
</p>
<p>&minus;1Z &prime;iyi (12.46)
</p>
<p>and therefore
</p>
<p>yi = Ziδ̂i,OLS + ei (12.47)
</p>
<p>implying that ei = (IT &minus; Zi(Z &prime;iZi)&minus;1Z &prime;i)yi = Miyi = Mi(Ziδi + ui) = Miui since MiZi = 0, and
this is true for i = 1, 2, . . . , N . Also, let
</p>
<p>M&lowast; = INT &minus; Z&lowast;(Z&lowast;&prime;Z&lowast;)&minus;1Z&lowast;&prime; =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>M1 0 . . . 0
0 M2 . . . 0
...
</p>
<p>. . .
...
</p>
<p>0 0 . . . MN
</p>
<p>⎞
⎟⎟⎟⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>318 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>One can easily deduce that y = Z&lowast;δ̂
&lowast;
+ e&lowast; with e&lowast; = M&lowast;y = M&lowast;u and δ̂
</p>
<p>&lowast;
= (Z&lowast;&prime;Z&lowast;)&minus;1Z&lowast;&prime;y.
</p>
<p>Note that both M and M&lowast; are symmetric and idempotent with MM&lowast; = M&lowast;. This easily follows
since
</p>
<p>Z(Z &prime;Z)&minus;1Z &prime;Z&lowast;(Z&lowast;&prime;Z&lowast;)&minus;1Z&lowast;&prime; = Z(Z &prime;Z)&minus;1I&lowast;&prime;Z&lowast;&prime;Z&lowast;(Z&lowast;&prime;Z&lowast;)&minus;1Z&lowast;&prime; = Z(Z &prime;Z)&minus;1Z &prime;
</p>
<p>This uses the fact that Z = Z&lowast;I&lowast;. Now, e&prime;e &minus; e&lowast;&prime;e&lowast; = u&prime;(M &minus; M&lowast;)u and e&lowast;&prime;e&lowast; = u&prime;M&lowast;u are
independent since (M &minus; M&lowast;)M&lowast; = 0. Also, both quadratic forms when divided by σ2 are
distributed as χ2 since (M &minus; M&lowast;) and M&lowast; are idempotent, see Judge et al. (1985). Dividing
these quadratic forms by their respective degrees of freedom, and taking their ratio leads to the
following test statistic:
</p>
<p>Fobs =
(e&prime;e&minus; e&lowast;&prime;e&lowast;)/(tr(M)&minus; tr(M&lowast;))
</p>
<p>e&lowast;&prime;e&lowast;/tr(M&lowast;)
(12.48)
</p>
<p>=
(e&prime;e&minus; e&prime;1e1 &minus; e&prime;2e2 &minus; ..&minus; e&prime;NeN )/(N &minus; 1)K &prime;
</p>
<p>(e&prime;1e1 + e
&prime;
2e2 + ..+ e
</p>
<p>&prime;
NeN )/N(T &minus;K &prime;)
</p>
<p>Under H0, Fobs is distributed as an F ((N &minus; 1)K &prime;, N(T &minus;K &prime;)), see lemma 2.2 of Fisher (1970).
This is exactly the Chow&rsquo;s (1960) test extended to the case of N linear regressions.
The URSS in this case is the sum of the N residual sum of squares obtained by applying
</p>
<p>OLS to (12.41), i.e., on each firm equation separately. The RRSS is simply the RSS from OLS
performed on the pooled regression given by (12.42). In this case, there are (N&minus;1)K &prime; restrictions
and the URSS has N(T &minus; K &prime;) degrees of freedom. Similarly, one can test the stability of the
regression across time. In this case, the degrees of freedom are (T &minus; 1)K &prime; and N(T &minus; K &prime;)
respectively. Both tests target the whole set of regression coefficients including the constant. If
the LSDV model is suspected to be the proper specification, then the intercepts are allowed to
vary but the slopes remain the same. To test the stability of the slopes only, the same Chow-
test can be utilized, however the RRSS is now that of the LSDV regression with firm (or time)
dummies only. The number of restrictions becomes (N &minus; 1)K for testing the stability of the
slopes across firms and (T &minus; 1)K for testing their stability across time.
</p>
<p>The Chow-test however is proper under spherical disturbances, and if that hypothesis is not
correct it will lead to improper inference. Baltagi (1981) showed that if the true specification of
the disturbances is an error components structure then the Chow-test tend to reject poolability
too often when in fact it is true. However, a generalization of the Chow-test which takes care
of the general variance-covariance matrix is available in Zellner (1962). This is exactly the test
of the null hypothesis H0; Rβ = r when Ω is that of the error components specification, see
Chapter 9. Baltagi (1981) shows that this test performs well in Monte Carlo experiments. In this
case, all we need to do is transform our model (under both the null and alternative hypotheses)
such that the transformed disturbances have a variance of σ2INT , then apply the Chow-test on
the transformed model. The later step is legitimate because the transformed disturbances have
homoskedastic variances and the usual Chow-test is legitimate. Given Ω = σ2Σ, we premultiply
the restricted model given in (12.42) by Σ&minus;1/2 and we call Σ&minus;1/2y = ẏ, Σ&minus;1/2Z = Ż and
Σ&minus;1/2u = u̇. Hence
</p>
<p>ẏ = Żδ + u̇ (12.49)
</p>
<p>with E(u̇u̇&prime;) = Σ&minus;1/2E(uu&prime;)Σ&minus;1/2&prime; = σ2INT . Similarly, we premultiply the unrestricted model
given in (12.43) by Σ&minus;1/2 and we call Σ&minus;1/2Z&lowast; = Ż&lowast;. Therefore</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Testing in a Pooled Model 319
</p>
<p>ẏ = Ż&lowast;δ&lowast; + u̇ (12.50)
</p>
<p>with E(u̇u̇&prime;) = σ2INT .
At this stage, we can test H0; δi = δ for every i = 1, 2, . . . , N , simply by using the Chow-
</p>
<p>statistic, only now on the transformed models (12.49) and (12.50) since they satisfy u̇ &sim;
N(0, σ2INT ). Note that Ż = Ż
</p>
<p>&lowast;I&lowast; which is simply obtained from Z = Z&lowast;I&lowast; by premulti-
plying by Σ&minus;1/2. Defining Ṁ = INT &minus; Ż(Ż &prime;Ż)&minus;1Ż &prime;, and Ṁ&lowast; = INT &minus; Ż&lowast;(Ż&lowast;&prime;Ż&lowast;)&minus;1Ż&lowast;&prime;, it is easy
to show that Ṁ and Ṁ&lowast; are both symmetric, idempotent and such that ṀṀ&lowast; = Ṁ&lowast;. Once
again the conditions for lemma 2.2 of Fisher (1970) are satisfied, and the test-statistic
</p>
<p>Ḟobs =
(ė&prime;ė&minus; ė&lowast;&prime;ė&lowast;)/(tr(Ṁ)&minus; tr(Ṁ&lowast;))
</p>
<p>ė&lowast;&prime;ė&lowast;/tr(Ṁ&lowast;)
&sim; F ((N &minus; 1)K &prime;, N(T &minus;K &prime;)) (12.51)
</p>
<p>where ė = ẏ &minus; Ż̂̇δOLS and ̂̇δOLS = (Ż &prime;Ż)&minus;1Ż &prime;ẏ implying that ė = Ṁẏ = Ṁu̇. Similarly,
ė&lowast; = ẏ &minus; Ż&lowast;̂̇δ
</p>
<p>&lowast;
OLS and
</p>
<p>̂̇
δ
&lowast;
OLS = (Ż
</p>
<p>&lowast;&prime;Ż&lowast;)&minus;1Ż&lowast;&prime;ẏ implying that ė&lowast; = Ṁ&lowast;ẏ = Ṁ&lowast;u̇. This is the
Chow-test after premultiplying the model by Σ&minus;1/2 or simply applying the Fuller and Battese
(1974) transformation. See Baltagi (2008) for details.
For the gasoline data in Baltagi and Griffin (1983), Chow&rsquo;s test for poolability across countries
</p>
<p>yields an observed F -statistic of 129.38 and is distributed as F (68, 270) under H0; δi = δ for
i = 1, . . . , N . This tests the stability of four time-series regression coefficients across 18 countries.
The unrestricted SSE is based upon 18 OLS time-series regressions, one for each country. For
the stability of the slope coefficients only, H0; βi = β, an observed F -value of 27.33 is obtained
which is distributed as F (51, 270) under the null. Chow&rsquo;s test for poolability across time yields an
F -value of 0.276 which is distributed as F (72, 266) under H0; δt = δ for t = 1, . . . , T . This tests
the stability of four cross-section regression coefficients across 19 time periods. The unrestricted
SSE is based upon 19 OLS cross-section regressions, one for each year. This does not reject
poolability across time-periods. The test for poolability across countries, allowing for a one-way
error components model yields an F -value of 21.64 which is distributed as F (68, 270) under H0;
δi = δ for i = 1, . . . , N . The test for poolability across time yields an F -value of 1.66 which is
distributed as F (72, 266) under H0; δt = δ for t = 1, . . . , T . This rejects H0 at the 5% level.
</p>
<p>(2) The Breusch-Pagan Test
</p>
<p>Next, we look at a Lagrange Multiplier test developed by Breusch and Pagan (1980), which
tests whether H0; σ
</p>
<p>2
μ = 0. The test statistic is given by
</p>
<p>LM = (NT/2(T &minus; 1))
[
(
&sum;N
</p>
<p>i=1 e
2
i./
</p>
<p>&sum;N
i=1
</p>
<p>&sum;T
t=1 e
</p>
<p>2
it)&minus; 1
</p>
<p>]2
(12.52)
</p>
<p>where eit denotes the OLS residuals on the pooled model, ei. denote their sum over t, respec-
tively. Under the null hypothesis H0 this LM statistic is distributed as a χ
</p>
<p>2
1. For the gasoline
</p>
<p>data in Baltagi and Griffin (1983), the Breusch and Pagan LM test yields an LM statistic of
1465.6. This is obtained using the Stata command xtest0 after estimating the model with ran-
dom effects. This is significant and rejects the null hypothesis. The corresponding likelihood
ratio test assuming Normal disturbances is also reported by Stata maximum likelihood output
for the random effects model. This yields an LR statistic of 463.97 which is asymptotically
distributed as χ21 under the null hypothesis H0 and is also significant.</p>
<p/>
</div>
<div class="page"><p/>
<p>320 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>One problem with the Breusch-Pagan test is that it assumes that the alternative hypothesis
is two-sided when we know that σ2μ &gt; 0. A one-sided version of this test is given by Honda
(1985):
</p>
<p>HO =
</p>
<p>&radic;
NT
</p>
<p>2(T &minus; 1)
</p>
<p>[
e&prime;(IN &otimes; JT )e
</p>
<p>e&prime;e
&minus; 1
</p>
<p>]
H0&rarr; N(0, 1) (12.53)
</p>
<p>where e denotes the vector of OLS residuals. Note that the square of this N(0, 1) statistic is
the Breusch and Pagan (1980) LM test-statistic. Honda (1985) finds that this test statistic is
uniformly most powerful and robust to non-normality. However, Moulton and Randolph (1989)
showed that the asymptotic N(0, 1) approximation for this one-sided LM statistic can be poor
even in large samples. They suggest an alternative Standardized Lagrange Multiplier (SLM)
test whose asymptotic critical values are generally closer to the exact critical values than those
of the LM test. This SLM test statistic centers and scales the one-sided LM statistic so that its
mean is zero and its variance is one.
</p>
<p>SLM =
HO &minus; E(HO)&radic;
</p>
<p>var(HO)
=
</p>
<p>d&minus; E(d)&radic;
var(d)
</p>
<p>(12.54)
</p>
<p>where d = e&prime;De/e&prime;e and D = (IN &otimes; JT ). Using the results on moments of quadratic forms in
regression residuals, see for e.g., Evans and King (1985), we get
</p>
<p>E(d) = tr(DP̄Z)/p
</p>
<p>and
</p>
<p>var(d) = 2{p tr(DP̄Z)2 &minus; [tr(DP̄Z)]2}/p2(p+ 2) (12.55)
</p>
<p>where p = n &minus; (K + 1) and P̄Z = In &minus; Z(Z &prime;Z)&minus;1Z &prime;. Under the null hypothesis, SLM has an
asymptotic N(0, 1) distribution.
</p>
<p>(3) The Hausman-Test
</p>
<p>A critical assumption in the error components regression model is that E(uit/Xit) = 0. This is
important given that the disturbances contain individual effects (the μi&rsquo;s) which are unobserved
and may be correlated with theXit&rsquo;s. For example, in an earnings equation these μi&rsquo;s may denote
unobservable ability of the individual and this may be correlated with the schooling variable
included on the right hand side of this equation. In this case, E(uit/Xit) 	= 0 and the GLS
estimator β̂GLS becomes biased and inconsistent for β. However, the within transformation
wipes out these μi&rsquo;s and leaves the Within estimator β̃Within unbiased and consistent for β.
Hausman (1978) suggests comparing β̂GLS and β̃Within, both of which are consistent under the
null hypothesis H0; E(uit/Xit) = 0, but which will have different probability limits if H0 is not
true. In fact, β̃Within is consistent whetherH0 is true or not, while β̂GLS is BLUE, consistent and
asymptotically efficient under H0, but is inconsistent when H0 is false. A natural test statistic
would be based on q̂ = β̂GLS &minus; β̃Within. Under H0, plim q̂ = 0, and cov(q̂, β̂GLS) = 0.
Using the fact that β̂GLS &minus; β = (X &prime;Ω&minus;1X)&minus;1X &prime;Ω&minus;1u and β̃Within &minus; β = (X &prime;QX)&minus;1X &prime;Qu,
</p>
<p>one gets E(q̂) = 0 and
</p>
<p>cov(β̂GLS , q̂) = var(β̂GLS)&minus; cov(β̂GLS , β̃Within)
= (X &prime;Ω&minus;1X)&minus;1 &minus; (X &prime;Ω&minus;1X)&minus;1X &prime;Ω&minus;1E(uu&prime;)QX(X &prime;QX)&minus;1 = 0</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Dynamic Panel Data Models 321
</p>
<p>Using the fact that β̃Within = β̂GLS &minus; q̂, one gets
</p>
<p>var(β̃Within) = var(β̂GLS) + var(q̂),
</p>
<p>since cov(β̂GLS , q̂) = 0. Therefore,
</p>
<p>var(q̂) = var(β̃Within)&minus; var(β̂GLS) = σ2ν(X &prime;QX)&minus;1 &minus; (X &prime;Ω&minus;1X)&minus;1 (12.56)
</p>
<p>Hence, the Hausman test statistic is given by
</p>
<p>m = q̂&prime;[var(q̂)]&minus;1q̂ (12.57)
</p>
<p>and under H0 is asymptotically distributed as χ
2
K , where K denotes the dimension of slope
</p>
<p>vector β. In order to make this test operational, Ω is replaced by a consistent estimator Ω̂, and
GLS by its corresponding FGLS. An alternative asymptotically equivalent test can be obtained
from the augmented regression
</p>
<p>y&lowast; = X&lowast;β + X̃γ + w (12.58)
</p>
<p>where y&lowast; = σνΩ&minus;1/2y, X&lowast; = σνΩ&minus;1/2X and X̃ = QX. Hausman&rsquo;s test is now equivalent to
testing whether γ = 0. This is a standard Wald test for the omission of the variables X̃ from
(12.58).
This test was generalized by Arellano (1993) to make it robust to heteroskedasticity and
</p>
<p>autocorrelation of arbitrary forms. In fact, if either heteroskedasticity or serial correlation is
present, the variances of the Within and GLS estimators are not valid and the corresponding
Hausman test statistic is inappropriate. For the Baltagi and Griffin (1983) gasoline data, the
Hausman test statistic based on the difference between the Within estimator and that of feasible
GLS based on Swamy and Arora (1972) yields a χ23 value of m = 306.1 which rejects the null
hypothesis. This is obtained using the Stata command hausman.
</p>
<p>12.6 Dynamic Panel Data Models
</p>
<p>The dynamic error components regression is characterized by the presence of a lagged dependent
variable among the regressors, i.e.,
</p>
<p>yit = δyi,t&minus;1 + x
&prime;
itβ + μi + νit, i = 1, . . . , N ; t = 1, . . . , T (12.59)
</p>
<p>where δ is a scalar, x&prime;it is 1 &times; K and β is K &times; 1. This model has been extensively studied by
Anderson and Hsiao (1982). Since yit is a function of μi, yi,t&minus;1 is also a function of μi. Therefore,
yi,t&minus;1, a right hand regressor in (12.59), is correlated with the error term. This renders the
OLS estimator biased and inconsistent even if the νit&rsquo;s are not serially correlated. For the FE
estimator, the within transformation wipes out the μi&rsquo;s, but ỹi,t&minus;1 will still be correlated with
ν̃it even if the νit&rsquo;s are not serially correlated. In fact, the Within estimator will be biased of
O(1/T ) and its consistency will depend upon T being large, see Nickell (1981). An alternative
transformation that wipes out the individual effects, yet does not create the above problem
is the first difference (FD) transformation. In fact, Anderson and Hsiao (1982) suggested first
differencing the model to get rid of the μi&rsquo;s and then using Δyi,t&minus;2 = (yi,t&minus;2 &minus; yi,t&minus;3) or
simply yi,t&minus;2 as an instrument for Δyi,t&minus;1 = (yi,t&minus;1 &minus; yi,t&minus;2). These instruments will not be</p>
<p/>
</div>
<div class="page"><p/>
<p>322 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>correlated with Δνit = νi,t&minus;νi,t&minus;1, as long as the νit&rsquo;s themselves are not serially correlated. This
instrumental variable (IV) estimation method leads to consistent but not necessarily efficient
estimates of the parameters in the model. This is because it does not make use of all the
available moment conditions, see Ahn and Schmidt (1995), and it does not take into account the
differenced structure on the residual disturbances (Δνit). Arellano (1989) finds that for simple
dynamic error components models the estimator that uses differences Δyi,t&minus;2 rather than levels
yi,t&minus;2 for instruments has a singularity point and very large variances over a significant range of
parameter values. In contrast, the estimator that uses instruments in levels, i.e., yi,t&minus;2, has no
singularities and much smaller variances and is therefore recommended. Additional instruments
can be obtained in a dynamic panel data model if one utilizes the orthogonality conditions that
exist between lagged values of yit and the disturbances νit.
Let us illustrate this with the simple autoregressive model with no regressors:
</p>
<p>yit = δyi,t&minus;1 + uit i = 1, . . . , N t = 1, . . . , T (12.60)
</p>
<p>where uit = μi + νit with μi &sim; IID(0, σ
2
μ) and νit &sim; IID(0, σ
</p>
<p>2
ν), independent of each other and
</p>
<p>among themselves. In order to get a consistent estimate of δ as N &rarr; &infin; with T fixed, we first
difference (12.60) to eliminate the individual effects
</p>
<p>yit &minus; yi,t&minus;1 = δ(yi,t&minus;1 &minus; yi,t&minus;2) + (νit &minus; νi,t&minus;1) (12.61)
</p>
<p>and note that (νit &minus; νi,t&minus;1) is MA(1) with unit root. For the first period we observe this
relationship, i.e., t = 3, we have
</p>
<p>yi3 &minus; yi2 = δ(yi2 &minus; yi1) + (νi3 &minus; νi2)
</p>
<p>In this case, yi1 is a valid instrument, since it is highly correlated with (yi2 &minus; yi1) and not
correlated with (νi3 &minus; νi2) as long as the νit are not serially correlated. But note what happens
for t = 4, the second period we observe (12.61):
</p>
<p>yi4 &minus; yi3 = δ(yi3 &minus; yi2) + (νi4 &minus; νi3)
</p>
<p>In this case, yi2 as well as yi1 are valid instruments for (yi3 &minus; yi2), since both yi2 and yi1
are not correlated with (νi4 &minus; νi3). One can continue in this fashion, adding an extra valid
instrument with each forward period, so that for period T , the set of valid instruments becomes
(yi1, yi2, . . . , yi,T&minus;2).
</p>
<p>This instrumental variable procedure still does not account for the differenced error term in
(12.61). In fact,
</p>
<p>E(Δνi Δν
&prime;
i) = σ
</p>
<p>2
νG (12.62)
</p>
<p>where Δν &prime;i = (νi3 &minus; νi2, . . . , νiT &minus; νi,T&minus;1) and
</p>
<p>G =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>2 &minus;1 0 &middot; &middot; &middot; 0 0 0
&minus;1 2 &minus;1 &middot; &middot; &middot; 0 0 0
</p>
<p>...
...
</p>
<p>...
. . .
</p>
<p>...
...
</p>
<p>...
0 0 0 &middot; &middot; &middot; &minus;1 2 &minus;1
0 0 0 &middot; &middot; &middot; 0 &minus;1 2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Dynamic Panel Data Models 323
</p>
<p>is (T &minus; 2)&times; (T &minus; 2), since Δνi is MA(1) with unit root. Define
</p>
<p>Wi =
</p>
<p>⎡
⎢⎢⎢⎣
</p>
<p>[yi1] 0
[yi1, yi2]
</p>
<p>. . .
</p>
<p>0 [yi1, . . . , yi,T&minus;2]
</p>
<p>⎤
⎥⎥⎥⎦ (12.63)
</p>
<p>Then, the matrix of instruments is W = [W &prime;1, . . . ,W
&prime;
N ]
</p>
<p>&prime; and the moment equations described
above are given by E(W &prime;iΔνi) = 0. Premultiplying the differenced equation (12.61) in vector
form by W &prime;, one gets
</p>
<p>W &prime;Δy = W &prime;(Δy&minus;1)δ +W
&prime;Δν (12.64)
</p>
<p>Performing GLS on (12.64) one gets the Arellano and Bond (1991) preliminary one-step con-
sistent estimator
</p>
<p>δ̂1 = [(Δy&minus;1)
&prime;W (W &prime;(IN &otimes;G)W )&minus;1W &prime;(Δy&minus;1)]&minus;1 (12.65)
</p>
<p>&times;[(Δy&minus;1)&prime;W (W &prime;(IN &otimes;G)W )&minus;1W &prime;(Δy)]
The optimal generalized method of moments (GMM) estimator of δ1 à la Hansen (1982) for
N &rarr; &infin; and T fixed using only the above moment restrictions yields the same expression as in
(12.65) except that
</p>
<p>W &prime;(IN &otimes;G)W =
N&sum;
</p>
<p>i=1
</p>
<p>W &prime;iGWi
</p>
<p>is replaced by
</p>
<p>VN =
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>W &prime;i (Δνi)(Δνi)
&prime;Wi
</p>
<p>This GMM estimator requires no knowledge concerning the initial conditions or the distributions
of νi and μi. To operationalize this estimator, Δν is replaced by differenced residuals obtained
from the preliminary consistent estimator δ̂1. The resulting estimator is the two-step Arellano
and Bond (1991) GMM estimator:
</p>
<p>δ̂2 = [(Δy&minus;1)
&prime;WV̂ &minus;1N W
</p>
<p>&prime;(Δy&minus;1)]
&minus;1[(Δy&minus;1)
</p>
<p>&prime;WV̂ &minus;1N W
&prime;(Δy)] (12.66)
</p>
<p>A consistent estimate of the asymptotic var(δ̂2) is given by the first term in (12.66),
</p>
<p>v̂ar(δ̂2) = [(Δy&minus;1)
&prime;WV̂ &minus;1N W
</p>
<p>&prime;(Δy&minus;1)]
&minus;1 (12.67)
</p>
<p>Note that δ̂1 and δ̂2 are asymptotically equivalent if the νit are IID(0, σ
2
ν).
</p>
<p>If there are additional strictly exogenous regressors xit as in (12.59) with E(xitνis) = 0 for
all t, s = 1, 2, . . . , T, but where all the xit are correlated with μi, then all the xit are valid
instruments for the first differenced equation of (12.59). Therefore, [x&prime;i1, x
</p>
<p>&prime;
i2, . . . , x
</p>
<p>&prime;
iT ] should be
</p>
<p>added to each diagonal element of Wi in (12.63). In this case, (12.64) becomes
</p>
<p>W &prime;Δy = W &prime;(Δy&minus;1)δ +W
&prime;(ΔX)β +W &prime;Δν</p>
<p/>
</div>
<div class="page"><p/>
<p>324 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>where ΔX is the stacked N(T &minus; 2) &times; K matrix of observations on Δxit. One- and two-step
estimators of (δ, β&prime;)can be obtained from
</p>
<p>(
δ̂
</p>
<p>β̂
</p>
<p>)
= ([Δy&minus;1,ΔX]
</p>
<p>&prime;WV̂ &minus;1N W
&prime;[Δy&minus;1,ΔX])
</p>
<p>&minus;1([Δy&minus;1,ΔX]
&prime;WV̂ &minus;1N W
</p>
<p>&prime;Δy) (12.68)
</p>
<p>as in (12.65) and (12.66).
Arellano and Bond (1991) suggest Sargan&rsquo;s (1958) test of over-identifying restrictions given
</p>
<p>by
</p>
<p>m = (Δν̂)&prime;W
</p>
<p>[
N&sum;
</p>
<p>i=1
</p>
<p>W &prime;i (Δν̂i)(Δν̂i)
&prime;Wi
</p>
<p>]&minus;1
W &prime;(Δν̂) &sim; χ2p&minus;K&minus;1
</p>
<p>where p refers to the number of columns of W and Δν̂ denote the residuals from a two-step
estimation given in (12.68).
</p>
<p>To summarize, dynamic panel data estimation of equation (12.59) with individual fixed effects
suffers from the Nickell (1981) bias. This disappears only if T tends to infinity. Alternatively,
a GMM estimator was suggested by Arellano and Bond (1991) which basically differences the
model to get rid of the individual specific effects and along with it any time invariant regressor.
This also gets rid of any endogeneity that may be due to the correlation of these individual effects
and the right hand side regressors. The moment conditions utilize the orthogonality conditions
between the differenced errors and lagged values of the dependent variable. This assumes that
the original disturbances are serially uncorrelated. In fact, two diagnostics are computed using
the Arellano and Bond GMM procedure to test for first order and second order serial correlation
in the disturbances. One should reject the null of the absence of first order serial correlation
and not reject the absence of second order serial correlation. A special feature of dynamic panel
data GMM estimation is that the number of moment conditions increase with T. Therefore, a
Sargan test is performed to test the over-identification restrictions. There is convincing evidence
that too many moment conditions introduce bias while increasing efficiency. It is even suggested
that a subset of these moment conditions be used to take advantage of the trade-off between
the reduction in bias and the loss in efficiency, see Baltagi (2008) for details.
Arellano and Bond (1991) apply their GMM estimation and testing methods to a model of
</p>
<p>employment using a panel of 140 quoted UK companies for the period 1979&ndash;84. This is the
benchmark data set used in Stata to obtain the one-step and two-step estimators described
in (12.65) and (12.66) as well as the Sargan test for over-identification using the command
(xtabond,twostep).The reader is asked to replicate their results in problem 22.
</p>
<p>12.6.1 Empirical Illustration
</p>
<p>Baltagi, Griffin and Xiong (2000) estimate a dynamic demand model for cigarettes based on
panel data from 46 American states over 30 years 1963&ndash;1992. The estimated equation is
</p>
<p>lnCit = α+ β1 lnCi,t&minus;1 + β2 lnPi,t + β3 lnYit + β4 lnPnit + uit (12.69)
</p>
<p>where the subscript i denotes the ith state (i = 1, . . . , 46), and the subscript t denotes the tth
year (t = 1, . . . , 30). Cit is real per capita sales of cigarettes by persons of smoking age (14</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Dynamic Panel Data Models 325
</p>
<p>years and older). This is measured in packs of cigarettes per head. Pit is the average retail price
of a pack of cigarettes measured in real terms. Yit is real per capita disposable income. Pnit
denotes the minimum real price of cigarettes in any neighboring state. This last variable is a
proxy for the casual smuggling effect across state borders. It acts as a substitute price attracting
consumers from high-tax states like Massachusetts to cross over to New Hampshire where the
tax is low. The disturbance term is specified as a two-way error component model:
</p>
<p>uit = μi + λt + νit i = 1, . . . , 46 t = 1, . . . , 30 (12.70)
</p>
<p>where μi denotes a state-specific effect, and λt denotes a year-specific effect. The time-period
effects (the λt) are assumed fixed parameters to be estimated as coefficients of time dummies
for each year in the sample. This can be justified given the numerous policy interventions as
well as health warnings and Surgeon General&rsquo;s reports. For example:
</p>
<p>(1) the imposition of warning labels by the Federal Trade Commission effective January 1965;
</p>
<p>(2) the application of the Fairness Doctrine Act to cigarette advertising in June 1967, which
subsidized antismoking messages from 1968 to 1970;
</p>
<p>(3) the Congressional ban on broadcast advertising of cigarettes effective January 1971.
</p>
<p>The μi are state-specific effects which can represent any state-specific characteristic including
the following:
</p>
<p>(1) States with Indian reservations like Montana, New Mexico and Arizona are among the
biggest losers in tax revenues from non-Indians purchasing tax-exempt cigarettes from the
reservations.
</p>
<p>(2) Florida, Texas, Washington and Georgia are among the biggest losers of revenues due to
the purchasing of cigarettes from tax-exempt military bases in these states.
</p>
<p>(3) Utah, which has a high percentage of Mormon population (a religion which forbids smok-
ing), has a per capita sales of cigarettes in 1988 of 55 packs, a little less than half the
national average of 113 packs.
</p>
<p>(4) Nevada, which is a highly touristic state, has a per capita sales of cigarettes of 142 packs
in 1988, 29 more packs than the national average.
</p>
<p>These state-specific effects may be assumed fixed, in which case one includes state dummy
variables in equation (12.69). The resulting estimator is the Within estimator reported in Table
12.8. Comparing these estimates with OLS without state or time dummies, one can see that
the coefficient of lagged consumption drops from 0.97 to 0.83 and the price elasticity goes up
in absolute value from &minus;0.09 to &minus;0.30. The income elasticity switches sign from negative to
positive going from &minus;0.03 to 0.10.
The OLS and Within estimators do not take into account the endogeneity of the lagged de-
</p>
<p>pendent variable, and therefore 2SLS and Within-2SLS are performed. The instruments used
are one lag on price, neighboring price and income. These give lower estimates of lagged con-
sumption and higher own price elasticities in absolute value. The Arellano and Bond (1991)
two-step estimator yields an estimate of lagged consumption of 0.70 and a price elasticity of
&minus;0.40, both of which are significant. Sargan&rsquo;s test for over-identification yields an observed
value of 32.3. This is asymptotically distributed as χ227 and is not significant. This was ob-
tained using the Stata command (xtabond2, twostep) with the collapse option to reduce the
number of moment conditions used for estimation.</p>
<p/>
</div>
<div class="page"><p/>
<p>326 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>Table 12.8 Dynamic Demand for Cigarettes: 1963&ndash;92&lowast;
</p>
<p>lnCi,t&minus;1 lnPit lnYit lnPnit
</p>
<p>OLS 0.97
(157.7)
</p>
<p>&minus;0.090
(6.2)
</p>
<p>&minus;0.03
(5.1)
</p>
<p>0.024
(1.8)
</p>
<p>Within 0.83
(66.3)
</p>
<p>&minus;0.299
(12.7)
</p>
<p>0.10
(4.2)
</p>
<p>0.034
(1.2)
</p>
<p>2SLS 0.85
(25.3)
</p>
<p>&minus;0.205
(5.8)
</p>
<p>&minus;0.02
(2.2)
</p>
<p>0.052
(3.1)
</p>
<p>Within-2SLS 0.60
(17.0)
</p>
<p>&minus;0.496
(13.0)
</p>
<p>0.19
(6.4)
</p>
<p>&minus;0.016
(0.5)
</p>
<p>Arellano and Bond (two-step) 0.70
(10.2)
</p>
<p>&minus;0.396
(6.0)
</p>
<p>0.13
(3.5)
</p>
<p>&minus;0.003
(0.1)
</p>
<p>&lowast; Numbers in parentheses are t-statistics.
</p>
<p>Source: Some of the results in this Table are reported in Baltagi, Griffin and Xiong (2000).
</p>
<p>12.7 Program Evaluation and Difference-in-Differences Estimator
</p>
<p>Suppose we want to study the effect of job training programs on earnings. An ideal experiment
would assign individuals randomly, by a flip of a coin, to training and non-training camps, and
then compare their earnings, holding other factors constant. This is a necessary experiment
before the approval of any drug. Patients are randomly assigned to receive the drug or a
placebo and the drug is approved or disapproved depending on the difference in the outcome
between these two groups. In this case, the FDA is concerned with the drug&rsquo;s safety and its
effectiveness. However, we run into problems in setting this experiment. How can we hold other
factors constant? Even twins which have been used in economic studies are not identical and
may have different life experiences.
The individual&rsquo;s prior work experience will affect one&rsquo;s chances in getting a job after training.
</p>
<p>But as long as the individuals are randomly assigned, the distribution of work experience is the
same in the treatment and control group, i.e., participation in the job training is independent
of prior work experience. In this case, omitting previous work experience from the analysis will
not cause omitted variable bias in the estimator of the effect of the training program on future
employment. Stock and Watson (2003) discuss threats to the internal and external validity of
such experiments. The former include: (i) failure to randomize, or (ii) to follow the treatment
protocol. These failures can cause bias in estimating the effect of the treatment. The first
can happen when individuals are assigned non-randomly to the treatment and non-treatment
groups. The second can happen, for example, when some people in the training program do
not show up for all training sessions; or when some people who are not supposed to be in the
training program are allowed to attend some of these training sessions. Attrition caused by
people dropping out of the experiment in either group can cause bias especially if the cause
of attrition is related to their acquiring or not acquiring training. In addition, small samples,
usually associated with expensive experiments, can affect the precision of the estimates. There
can also be experimental effects, brought about by people trying harder simply because the
worker being trained feels noticed or because the trainer has a stake in the success of the
program. Stock and Watson (2003, p. 380) argue that &ldquo;threats to external validity compromise
the ability to generalize the results of the experiment to other populations and settings. Two</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 327
</p>
<p>such threats are when the experimental sample is not representative of the population of interest
and when the treatment being studied is not representative of the treatment that would be
implemented more broadly.&rdquo;
They also warn about &ldquo;general equilibrium effects&rdquo;where, for example, turning a small, tem-
</p>
<p>porary experimental program into a widespread, permanent program might change the economic
environment sufficiently that the results of the experiment cannot be generalized. For example,
it could displace employer-provided training, thereby reducing the net benefits of the program.
</p>
<p>12.7.1 The Difference-in-Differences Estimator
</p>
<p>With panel data, observations on the same subjects before and after the training program allow
us to estimate the effect of this program on earnings. In simple regression form, assuming
the assignment to the training program is random, one regresses the change in earnings before
and after training is completed on a dummy variable which takes the value 1 if the individual
received training and zero if they did not. This regression computes the average change in
earnings for the treatment group before and after the training program and subtracts that from
the average change in earnings for the control group. One can include additional regressors
which measure the individual characteristics prior to training. Examples are gender, race,
education and age of the individual.
Card (1990) used a quasi-experiment to see whether immigration reduces wages. Taking
</p>
<p>advantage of the &ldquo;Mariel boatlift&rdquo; where a large number of Cuban immigrants entered Miami.
Card (1990) used the difference-in-differences estimator, comparing the change in wages of low-
skilled workers in Miami to the change in wages of similar workers in other comparable U.S.
cities over the same period. Card concluded that the influx of Cuban immigrants had a negligible
effect on wages of less-skilled workers.
</p>
<p>Problems
</p>
<p>1. Fixed Effects and the Within Transformation.
</p>
<p>(a) Premultiply (12.11) by Q and verify that the transformed equation reduces to (12.12). Show
that the new disturbances Qν have zero mean and variance-covariance matrix σ2νQ.
Hint: QZμ = 0.
</p>
<p>(b) Show that the GLS estimator is the same as the OLS estimator on this transformed regression
equation. Hint: Use one of the necessary and sufficient conditions for GLS to be equivalent
to OLS given in Chapter 9.
</p>
<p>(c) Using the Frisch-Waugh-Lovell Theorem given in Chapter 7, show that the estimator derived
</p>
<p>in part (b) is the Within estimator and is given by β̃ = (X &prime;QX)&minus;1X &prime;Qy.
</p>
<p>2. Variance-Covariance Matrix of Random Effects.
</p>
<p>(a) Show that Ω given in (12.17) can be written as (12.18).
</p>
<p>(b) Show that P and Q are symmetric, idempotent, orthogonal and sum to the identity matrix.
</p>
<p>(c) For Ω&minus;1 given by (12.19), verify that ΩΩ&minus;1 = Ω&minus;1Ω = INT .
</p>
<p>(d) For Ω&minus;1/2 given by (12.20), verify that Ω&minus;1/2Ω&minus;1/2 = Ω&minus;1.</p>
<p/>
</div>
<div class="page"><p/>
<p>328 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>3. Fuller and Battese (1974) Transformation. Premultiply y by σνΩ
&minus;1/2 where Ω&minus;1/2 is defined in
</p>
<p>(12.20) and show that the resulting y&lowast; has a typical element y&lowast;it = yit&minus;θȳi., where the θ = 1&minus;σν/σ1
and σ21 = Tσ
</p>
<p>2
μ + σ
</p>
<p>2
ν .
</p>
<p>4. Unbiased Estimates of the Variance-Components. Using (12.21) and (12.22), show that E(σ̂21) = σ
2
1
</p>
<p>and E(σ̂2ν) = σ
2
ν . Hint: E(u
</p>
<p>&prime;Qu) = E{tr(u&prime;Qu)} = E{tr(uu&prime;Q)} = tr{E(uu&prime;)Q} = tr(ΩQ).
</p>
<p>5. Swamy and Arora (1972) Estimates of the Variance-Components.
</p>
<p>(a) Show that ̂̂σ2ν given in (12.23) is unbiased for σ2ν .
</p>
<p>(b) Show that ̂̂σ21 given in (12.26) is unbiased for σ21.
</p>
<p>6. System Estimation.
</p>
<p>(a) Perform OLS on the system of equations given in (12.27) and show that the resulting esti-
</p>
<p>mator is δ̂OLS = (Z
&prime;Z)&minus;1Z &prime;y.
</p>
<p>(b) Perform GLS on this system of equations and show that the resulting estimator is δ̂GLS =
(Z &prime;Ω&minus;1Z)&minus;1Z &prime;Ω&minus;1y where Ω&minus;1 is given in (12.19).
</p>
<p>7. Random Effects Is More Efficient than Fixed Effects. Using the var(β̂GLS) expression below (12.30)
</p>
<p>and var(β̃Within) = σ
2
νW
</p>
<p>&minus;1
XX , show that
</p>
<p>(var(β̂GLS))
&minus;1 &minus; (var(β̃Within))&minus;1 = φ2BXX/σ2ν
</p>
<p>which is positive semi-definite. Conclude that var(β̃Within)&minus; var(β̂GLS) is positive semi-definite.
</p>
<p>8. Maximum Likelihood Estimation of the Random Effects Model.
</p>
<p>(a) Using the concentrated likelihood function in (12.34), solve &part;Lc/&part;φ
2 = 0 and verify (12.35).
</p>
<p>(b) Solve &part;Lc/&part;β = 0 and verify (12.36).
</p>
<p>9. Prediction in the Random Effects Model.
</p>
<p>(a) For the predictor of yi,T+S given in (12.37), compute E(ui,T+Suit) for t = 1, 2, . . . , T and
verify that w = E(ui,T+Su) = σ
</p>
<p>2
μ(ℓi &otimes; ιT ) where ℓi is the i-th column of IN .
</p>
<p>(b) Verify (12.39) by showing that (ℓ&prime;i &otimes; ι&prime;T )P = (ℓ&prime;i &otimes; ι&prime;T ).
</p>
<p>10. Using the gasoline demand data of Baltagi and Griffin (1983), given on the Springer web site as
GASOLINE.DAT, reproduce Tables 12.1 through 12.7.
</p>
<p>11. Bounds on s2 in the Random Effects Model. For the random one-way error components model
given in (12.1) and (12.2), consider the OLS estimator of var(uit) = σ
</p>
<p>2, which is given by s2 =
e&prime;e/(n&minus;K &prime;), where e denotes the vector of OLS residuals, n = NT and K &prime; = K + 1.
</p>
<p>(a) Show that E(s2) = σ2 + σ2μ[K
&prime;&minus; tr(IN &otimes; JT )PX ]/(n&minus;K &prime;).
</p>
<p>(b) Consider the inequalities given by Kiviet and Krämer (1992) which state that 0 &le; mean of
n &minus;K &prime; smallest roots of Ω &le; E(s2) &le; mean of n &minus;K &prime; largest roots of Ω &le; tr(Ω)/(n &minus;K &prime;)
where Ω = E(uu&prime;). Show that for the one-way error components model, these bounds are
</p>
<p>0 &le; σ2ν + (n&minus; TK &prime;)σ2μ/(n&minus;K &prime;) &le; E(s2) &le; σ2ν + nσ2μ/(n&minus;K &prime;) &le; nσ2/(n&minus;K &prime;).
</p>
<p>As n &rarr; &infin;, both bounds tend to σ2, and s2 is asymptotically unbiased, irrespective of the
particular evolution of X, see Baltagi and Krämer (1994) for a proof of this result.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 329
</p>
<p>12. Verify the relationship between M and M&lowast;, i.e., MM&lowast; = M&lowast;, given below (12.47). Hint: Use
the fact that Z = Z&lowast;I&lowast; with I&lowast; = (ιN &otimes; IK&prime;).
</p>
<p>13. Verify that Ṁ and Ṁ&lowast; defined below (12.50) are both symmetric, idempotent and satisfy ṀṀ&lowast; =
Ṁ&lowast;.
</p>
<p>14. For the gasoline data used in problem 10, verify the Chow-test results given below equation (12.51).
</p>
<p>15. For the gasoline data, compute the Breusch-Pagan, Honda and Standardized LM tests for H0;
σ2μ = 0.
</p>
<p>16. If β̃ denotes the LSDV estimator and β̂GLS denotes the GLS estimator, then
</p>
<p>(a) Show that q̂ = β̂GLS &minus; β̃ satisfies cov(q̂, β̂GLS) = 0.
(b) Verify equation (12.56).
</p>
<p>17. For the gasoline data used in problem 10, replicate the Hausman test results given below equation
(12.58).
</p>
<p>18. For the cigarette data given as CIGAR.TXT on the Springer web site, reproduce the results given
in Table 12.8. See also Baltagi, Griffin and Xiong (2000).
</p>
<p>19. Heteroskedastic Fixed Effects Models. This is based on Baltagi (1996). Consider the fixed effects
model
</p>
<p>yit = αi + uit i = 1, 2, . . . , N ; t = 1, 2, . . . , Ti
</p>
<p>where yit denotes output in industry i at time t and αi denotes the industry fixed effect. The
disturbances uit are assumed to be independent with heteroskedastic variances σ
</p>
<p>2
i . Note that the
</p>
<p>data are unbalanced with different number of observations for each industry.
</p>
<p>(a) Show that OLS and GLS estimates of αi are identical.
</p>
<p>(b) Let σ2 =
&sum;N
</p>
<p>i=1 Tiσ
2
i /n where n =
</p>
<p>&sum;N
i=1 Ti, be the average disturbance variance. Show that
</p>
<p>the GLS estimator of σ2 is unbiased, whereas the OLS estimator of σ2 is biased. Also show
that this bias disappears if the data are balanced or the variances are homoskedastic.
</p>
<p>(c) Define λ2i = σ
2
i /σ
</p>
<p>2 for i = 1, 2 . . . , N . Show that for α&prime; = (α1, α2, . . . , αN )
</p>
<p>E[estimated var(α̂OLS)&minus; true var(α̂OLS)]
</p>
<p>= σ2[(n&minus;
N&sum;
</p>
<p>i=1
</p>
<p>λ2i )/(n&minus;N)] diag (1/Ti)&minus; σ2 diag (λ2i /Ti)
</p>
<p>This problem shows that in case there are no regressors in the unbalanced panel data model,
fixed effects with heteroskedastic disturbances can be estimated by OLS, but one has to
correct the standard errors.
</p>
<p>20. The Relative Efficiency of the Between Estimator with Respect to the Within Estimator. This is
based on Baltagi (1999). Consider the simple panel data regression model
</p>
<p>yit = α+ βxit + uit i = 1, 2, . . . , N ; t = 1, 2, . . . , T (1)
</p>
<p>where α and β are scalars. Subtract the mean equation to get rid of the constant
</p>
<p>yit &minus; ȳ.. = β(xit &minus; x̄..) + uit &minus; ū.., (2)</p>
<p/>
</div>
<div class="page"><p/>
<p>330 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>where x̄.. = Σ
N
i=1Σ
</p>
<p>T
t=1xit/NT and ȳ.. and ū.. are similarly defined. Add and subtract x̄i. from the
</p>
<p>regressor in parentheses and rearrange
</p>
<p>yit &minus; ȳ.. = β(xit &minus; x̄i.) + β(x̄i. &minus; x̄..) + uit &minus; ū.. (3)
</p>
<p>where x̄i. = Σ
T
t=1xit/T . Now run the unrestricted least squares regression
</p>
<p>yit &minus; ȳ.. = βw(xit &minus; x̄i.) + βb(x̄i. &minus; x̄..) + uit &minus; ū.. (4)
</p>
<p>where βw is not necessarily equal to βb.
</p>
<p>(a) Show that the least squares estimator of βw from (4) is the Within estimator and that of βb
is the Between estimator.
</p>
<p>(b) Show that if uit = μi + νit where μi &sim; IID(0, σ
2
μ) and νit &sim; IID(0, σ
</p>
<p>2
ν) independent of each
</p>
<p>other and among themselves, then ordinary least squares (OLS) is equivalent to generalized
least squares (GLS) on (4).
</p>
<p>(c) Show that for model (1), the relative efficiency of the Between estimator with respect to
the Within estimator is equal to (BXX/WXX)[(1 &minus; ρ)/(Tρ + (1 &minus; ρ))], where WXX =
ΣNi=1Σ
</p>
<p>T
t=1(xit &minus; x̄i.)2 denotes the Within variation and BXX = TΣNi=1(x̄i. &minus; x̄..)2 denotes
</p>
<p>the Between variation. Also, ρ = σ2μ/(σ
2
μ + σ
</p>
<p>2
ν) denotes the equicorrelation coefficient.
</p>
<p>(d) Show that the square of the t-statistic used to test H0; βw = βb in (4) yields exactly Haus-
man&rsquo;s (1978) specification test.
</p>
<p>21. For the crime example of Cornwell and Trumbull (1994) studied in Chapter 11. Use the panel data
given as CRIME.DAT on the Springer web site to replicate the Between and Within estimates
given in Table 1 of Cornwell and Trumbull (1994). Compute 2SLS and Within-2SLS (2SLS with
county dummies) using offense mix and per capita tax revenue as instruments for the probability
of arrest and police per capita. Comment on the results.
</p>
<p>22. Consider the Arellano and Bond (1991) dynamic employment equation for 140 UK companies over
</p>
<p>the period 1979&ndash;1984. Replicate all the estimation results in Table 4 of Arellano and Bond (1991,
</p>
<p>p. 290).
</p>
<p>References
</p>
<p>This chapter is based on Baltagi (2008).
</p>
<p>Ahn, S.C. and P. Schmidt (1995), &ldquo;Efficient Estimation of Models for Dynamic Panel Data,&rdquo; Journal of
Econometrics, 68: 5&ndash;27.
</p>
<p>Amemiya, T. (1971), &ldquo;The Estimation of the Variances in a Variance-Components Model,&rdquo; International
Economic Review, 12: 1&ndash;13.
</p>
<p>Anderson, T.W. and C. Hsiao (1982), &ldquo;Formulation and Estimation of Dynamic Models Using Panel
Data, Journal of Econometrics, 18: 47&ndash;82.
</p>
<p>Arellano, M. (1989), &ldquo;A Note on the Anderson-Hsiao Estimator for Panel Data,&rdquo; Economics Letters, 31:
337&ndash;341.
</p>
<p>Arellano, M. (1993), &ldquo;On the Testing of Correlated Effects With Panel Data,&rdquo; Journal of Econometrics,
59: 87&ndash;97.
</p>
<p>Arellano, M. and S. Bond (1991), &ldquo;Some Tests of Specification for Panel Data: Monte Carlo Evidence
and An Application to Employment Equations,&rdquo; Review of Economic Studies, 58: 277&ndash;297.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 331
</p>
<p>Balestra, P. (1973), &ldquo;Best Quadratic Unbiased Estimators of the Variance-Covariance Matrix in Normal
Regression,&rdquo; Journal of Econometrics, 2: 17&ndash;28.
</p>
<p>Baltagi, B.H. (1981), &ldquo;Pooling: An Experimental Study of Alternative Testing and Estimation Procedures
in a Two-Way Errors Components Model,&rdquo; Journal of Econometrics, 17: 21&ndash;49.
</p>
<p>Baltagi, B.H. (1996), &ldquo;Heteroskedastic Fixed Effects Models,&rdquo; Problem 96.5.1, Econometric Theory, 12:
867.
</p>
<p>Baltagi, B.H. (1999), &ldquo;The Relative Efficiency of the Between Estimator with Respect to the Within
Estimator,&rdquo; Problem 99.4.3, Econometric Theory, 15: 630&ndash;631.
</p>
<p>Baltagi, B.H. (2008), Econometric Analysis of Panel Data (Wiley: Chichester).
</p>
<p>Baltagi, B.H. and J.M. Griffin (1983), &ldquo;Gasoline Demand in the OECD: An Application of Pooling and
Testing Procedures,&rdquo; European Economic Review, 22: 117&ndash;137.
</p>
<p>Baltagi, B.H., J.M. Griffin and W. Xiong (2000), &ldquo;To Pool or Not to Pool: Homogeneous Versus Het-
erogeneous Estimators Applied to Cigarette Demand,&rdquo; Review of Economics and Statistics, 82:
117&ndash;126.
</p>
<p>Baltagi, B.H. and W. Krämer (1994), &ldquo;Consistency, Asymptotic Unbiasedness and Bounds on the Bias
of s2 in the Linear Regression Model with Error Components Disturbances,&rdquo; Statistical Papers,
35: 323&ndash;328.
</p>
<p>Breusch, T.S. (1987), &ldquo;Maximum Likelihood Estimation of Random Effects Models,&rdquo; Journal of Econo-
metrics, 36: 383&ndash;389.
</p>
<p>Breusch, T.S. and A.R. Pagan (1980), &ldquo;The Lagrange Multiplier Test and its Applications to Model
Specification in Econometrics,&rdquo; Review of Economic Studies, 47: 239&ndash;253.
</p>
<p>Card (1990), &ldquo;The Impact of the Mariel Boat Lift on the Miami Labor Market,&rdquo; Industrial and Labor
Relations Review, 43: 245&ndash;253.
</p>
<p>Chow, G.C. (1960), &ldquo;Tests of Equality Between Sets of Coefficients in Two Linear Regressions,&rdquo; Econo-
metrica, 28: 591&ndash;605.
</p>
<p>Cornwell, C. and W.N. Trumbull (1994), &ldquo;Estimating the Economic Model of Crime with Panel Data,&rdquo;
Review of Economics and Statistics 76: 360&ndash;366.
</p>
<p>Evans, M.A. and M.L. King (1985), &ldquo;Critical Value Approximations for Tests of Linear Regression
Disturbances,&rdquo; Australian Journal of Statistics, 27: 68&ndash;83.
</p>
<p>Fisher, F.M. (1970), &ldquo;Tests of Equality Between Sets of Coefficients in Two Linear Regressions: An
Expository Note,&rdquo; Econometrica, 38: 361&ndash;366.
</p>
<p>Fuller, W.A. and G.E. Battese (1974), &ldquo;Estimation of Linear Models with Cross-Error Structure,&rdquo; Jour-
nal of Econometrics, 2: 67&ndash;78.
</p>
<p>Goldberger, A.S. (1962), &ldquo;Best Linear Unbiased Prediction in the Generalized Linear Regression Model,&rdquo;
Journal of the American Statistical Association, 57: 369&ndash;375.
</p>
<p>Graybill, F.A. (1961), An Introduction to Linear Statistical Models (McGraw-Hill: New York).
</p>
<p>Hansen, L.P. (1982), &ldquo;Large Sample Properties of Generalized Method of Moments Estimators,&rdquo; Econo-
metrica, 50: 1029&ndash;1054.
</p>
<p>Hausman, J.A. (1978), &ldquo;Specification Tests in Econometrics,&rdquo; Econometrica, 46: 1251&ndash;1271.
</p>
<p>Honda, Y. (1985), &ldquo;Testing the Error Components Model with Non-Normal Disturbances,&rdquo; Review of
Economic Studies, 52: 681&ndash;690.</p>
<p/>
</div>
<div class="page"><p/>
<p>332 Chapter 12: Pooling Time-Series of Cross-Section Data
</p>
<p>Hsiao, C. (2003), Analysis of Panel Data (Cambridge University Press: Cambridge).
</p>
<p>Judge, G.G., W.E. Griffiths, R.C. Hill, H. Lutkepohl and T.C. Lee (1985), The Theory and Practice of
Econometrics (Wiley: New York).
</p>
<p>Kiviet, J.F. and W. Krämer (1992), &ldquo;Bias of s2 in the Linear Regression Model with Correlated Errors,&rdquo;
Empirical Economics, 16: 375&ndash;377.
</p>
<p>Maddala, G.S. (1971), &ldquo;The Use of Variance Components Models in Pooling Cross Section and Time
Series Data,&rdquo; Econometrica, 39: 341&ndash;358.
</p>
<p>Maddala, G.S. and T. Mount (1973), &ldquo;A Comparative Study of Alternative Estimators for Variance
Components Models Used in Econometric Applications,&rdquo; Journal of the American Statistical As-
sociation, 68: 324&ndash;328.
</p>
<p>Moulton, B.R. and W.C. Randolph (1989), &ldquo;Alternative Tests of the Error Components Model,&rdquo; Econo-
metrica, 57: 685&ndash;693.
</p>
<p>Nerlove, M. (1971), &ldquo;A Note on Error Components Models,&rdquo; Econometrica, 39: 383&ndash;396.
</p>
<p>Nickell, S. (1981), &ldquo;Biases in Dynamic Models with Fixed Effects,&rdquo;Econometrica, 49: 1417&ndash;1426.
</p>
<p>Searle, S.R. (1971), Linear Models (Wiley: New York).
</p>
<p>Sargan, J. (1958), &ldquo;The Estimation of Economic Relationships Using Instrumental Variables,&rdquo; Econo-
metrica, 26: 393&ndash;415.
</p>
<p>Swamy, P.A.V.B. and S.S. Arora (1972), &ldquo;The Exact Finite Sample Properties of the Estimators of
Coefficients in the Error Components Regression Models,&rdquo; Econometrica, 40: 261&ndash;275.
</p>
<p>Taub, A.J. (1979), &ldquo;Prediction in the Context of the Variance-Components Model,&rdquo; Journal of Econo-
metrics, 10: 103&ndash;108.
</p>
<p>Taylor, W.E. (1980), &ldquo;Small Sample Considerations in Estimation from Panel Data,&rdquo; Journal of Econo-
metrics, 13: 203&ndash;223.
</p>
<p>Wallace, T. and A. Hussain (1969), &ldquo;The Use of Error Components Models in Combining Cross-Section
and Time-Series Data,&rdquo; Econometrica, 37: 55&ndash;72.
</p>
<p>Wansbeek, T.J. and A. Kapteyn (1978), &ldquo;The Separation of Individual Variation and Systematic Change
in the Analysis of Panel Data,&rdquo; Annales de l&rsquo;INSEE, 30&ndash;31: 659&ndash;680.
</p>
<p>Wansbeek, T.J. and A. Kapteyn (1982), &ldquo;A Simple Way to Obtain the Spectral Decomposition of
Variance Components Models for Balanced Data,&rdquo; Communications in Statistics All, 2105&ndash;2112.
</p>
<p>Wansbeek, T.J. and A. Kapteyn, (1989), &ldquo;Estimation of the error components model with incomplete
panels,&rdquo; Journal of Econometrics 41: 341&ndash;361.
</p>
<p>Zellner, A. (1962), &ldquo;An Efficient Method of Estimating Seemingly Unrelated Regression and Tests for
</p>
<p>Aggregation Bias,&rdquo; Journal of the American Statistical Association, 57: 348&ndash;368.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 13
</p>
<p>Limited Dependent Variables
</p>
<p>13.1 Introduction
</p>
<p>In labor economics, one is faced with explaining the decision to participate in the labor force,
the decision to join a union, or the decision to migrate from one region to the other. In finance,
a consumer defaults on a loan or a credit card debt, or purchases a stock or an asset like a house
or a car. In these examples, the dependent variable is usually a dummy variable with values 1 if
the worker participates (or consumer defaults on a loan) and 0 if he or she does not participate
(or default). We dealt with dummy variables as explanatory variables on the right hand side of
the regression, but what additional problems arise when this dummy variable appears on the
left hand side of the equation? As we have done in previous chapters, we first study its effects
on the usual least squares estimator, and then consider alternative estimators that are more
appropriate for models of this nature.
</p>
<p>13.2 The Linear Probability Model
</p>
<p>What is wrong with running OLS on this model? After all, it is a feasible procedure. For the
labor force participation example one regresses the dummy variable for participation on age,
sex, race, marital status, number of children, experience and education, etc. The prediction
from this OLS regression is interpreted as the likelihood of participating in the labor force. The
problems with this interpretation are the following:
</p>
<p>(i) We are predicting probabilities of participation for each individual, whereas the actual values
observed are 0 or 1.
</p>
<p>(ii) There is no guarantee that ŷi, the predicted value of yi is going to be between 0 and 1. In
fact, one can always find values of the explanatory variables that would generate a corresponding
prediction outside the (0, 1) range.
</p>
<p>(iii) Even if one is willing to assume that the true model is a linear regression given by
</p>
<p>yi = x
&prime;
iβ + ui i = 1, 2, . . . , n. (13.1)
</p>
<p>what properties does this entail on the disturbances? It is obvious that yi = 1 only when
ui = 1 &minus; x&prime;iβ, let us say with probability πi, where πi is to be determined. Then yi = 0 only
when ui = &minus;x&prime;iβ with probability (1&minus; πi). For the disturbances to have zero mean
</p>
<p>E(ui) = πi(1&minus; x&prime;iβ) + (1&minus; πi)(&minus;x&prime;iβ) = 0 (13.2)
</p>
<p>Solving for πi, one gets that πi = x
&prime;
iβ. This also means that
</p>
<p>var(ui) = πi(1&minus; πi) = x&prime;iβ(1&minus; x&prime;iβ) (13.3)
</p>
<p>which is heteroskedastic. Goldberger (1964) suggests correcting for this heteroskedasticity by
first running OLS to estimate β, and estimating σ2i = var(ui) by σ̂
</p>
<p>2
i = x
</p>
<p>&prime;
iβ̂OLS(1 &minus; x&prime;iβ̂OLS) =
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_13, 333</p>
<p/>
</div>
<div class="page"><p/>
<p>334 Chapter 13: Limited Dependent Variables
</p>
<p>ŷi(1 &minus; ŷi). In the next step a Weighted Least Squares (WLS) procedure is run on (13.1) with
the original observations divided by σ̂i. One cannot compute σ̂i if OLS predicts ŷi larger than
1 or smaller than 0. Suggestions in the literature include substituting 0.005 instead of ŷi &lt; 0,
and 0.995 for ŷi &gt; 1. However, these procedures do not perform well, and the WLS predictions
themselves are not guaranteed to fall in the (0, 1) range. Therefore, one should use the robust
White heteroskedastic variance-covariance matrix option when estimating linear probability
models, otherwise the standard errors are biased and inference is misleading.
This brings us to the fundamental problem with OLS, i.e., its functional form. We are trying
</p>
<p>to predict
</p>
<p>yi = F (x
&prime;
iβ) + ui (13.4)
</p>
<p>with a linear regression equation, see Figure 13.1, where the more reasonable functional form
for this probability is an S-shaped cumulative distribution functional form. This was justified
in the biometrics literature as follows: An insect has a tolerance to an insecticide I&lowast;i , which is
an unobserved random variable with cumulative distribution function (c.d.f.) F . If the dosage
of insecticide administered induces a stimulus Ii that exceeds I
</p>
<p>&lowast;
i , the insect dies, i.e., yi = 1.
</p>
<p>Therefore
</p>
<p>Pr(yi = 1) = Pr(I
&lowast;
i &le; Ii) = F (Ii) (13.5)
</p>
<p>To put it in an economic context, I&lowast;i could be the unobserved reservation wage of a worker, and if
we increase the offered wage beyond that reservation wage, the worker participates in the labor
force. In general, Ii could be represented as a function of the individuals characteristics, i.e., the
xi&rsquo;s. F (x
</p>
<p>&prime;
iβ) is by definition between zero and 1 for all values of xi. Also, the linear probability
</p>
<p>model yields the result that &part;πi/&part;xk = βk, for every i. This means that the probability of
participating (πi) always changes at the same rate with respect to unit increases in the offer
wage xk. However, this probability model gives
</p>
<p>&part;πi/&part;xk = [&part;F (zi)/&part;zi] &middot; [&part;zi/&part;xk] = f(x&prime;iβ) &middot; βk (13.6)
</p>
<p>where zi = x
&prime;
iβ, and f is the probability density function (p.d.f.). Equation (13.6) makes more
</p>
<p>sense because if xk denotes the offered wage, changing the probability of participation πi from
0.96 to 0.97 requires a larger change in xk than changing πi from 0.23 to 0.24.
</p>
<p>If F (x&prime;iβ) is the true probability function, assuming it is linear introduces misspecification, and
as Figure 13.1 indicates, for xi &lt; xℓ, all the ui&rsquo;s generated by a linear probability approximation
are positive. Similarly for all xi &gt; xu, all the ui&rsquo;s generated by a linear probability approximation
are negative.
</p>
<p>13.3 Functional Form: Logit and Probit
</p>
<p>Having pointed out the problems with considering the functional form F as linear, we turn to
two popular functional forms of F , the logit and the probit. These two c.d.f.&rsquo;s differ only in the
tails, and the logit resembles the c.d.f. of a t-distribution with 7 degrees of freedom, whereas
the probit is the normal c.d.f., or that of a t with &infin; degrees of freedom. Therefore, these two
forms will give similar predictions unless there are an extreme number of observations in the
tails.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Functional Form: Logit and Probit 335
</p>
<p>)Pr(�
</p>
<p>1
</p>
<p>��� �
</p>
<p>)/���������
</p>
<p>+
</p>
<p>&ndash;
</p>
<p>+
</p>
<p>&ndash;
</p>
<p>�
� x0 ��
</p>
<p>Figure 13.1 Linear Probability Model
</p>
<p>We will use the conventional notation Φ(z) =
&int; z
&minus;&infin; φ(u)du, where φ(z) = e
</p>
<p>&minus;z2/2/
&radic;
2π for
</p>
<p>&minus;&infin; &lt; z &lt; &infin;, for the probit. Also, Λ(z) = ez/(1 + ez) = 1/(1 + e&minus;z) for &minus;&infin; &lt; z &lt; +&infin;,
for the logit. Some results that we will use quite often in our derivations are the following:
dΦ/dz = φ, and dΛ/dz = Λ(1 &minus; Λ). The p.d.f. of the logistic distribution is the product of its
c.d.f. and one minus this c.d.f. Therefore, the marginal effects considered above for a general F
are respectively,
</p>
<p>&part;Φ(x&prime;iβ)/&part;xk = φiβk (13.7)
</p>
<p>and
</p>
<p>&part;Λ(x&prime;iβ)/&part;xk = Λi(1&minus; Λi)βk (13.8)
</p>
<p>where φi = φ(x
&prime;
iβ) and Λi = Λ(x
</p>
<p>&prime;
iβ).
</p>
<p>One has to be careful with the computation of partial derivatives in case there is a dummy
variable among the explanatory variables. For such models, one should compute the marginal
effects of a change in one unit of a continuous variable xk for both values of the dummy variable.
</p>
<p>Illustrative Example: Using the probit model, suppose that the probability of joining a union
is estimated as follows: π̂i = Φ(2.5 &minus; 0.06 WKS i + 0.95 OCC i) where WKS is the number of
weeks worked and OCC = 1, if the individual is in a blue-collar occupation, and zero otherwise.
Weeks worked in this sample range from 20 to 50. From (13.7), the marginal effect of one extra
week of work on the probability of joining the union is given by:</p>
<p/>
</div>
<div class="page"><p/>
<p>336 Chapter 13: Limited Dependent Variables
</p>
<p>For Blue-Collar Workers: &minus;0.06 φ[2.5&minus; 0.06 WKS + 0.95]
= &minus;0.06 φ[2.25] = &minus;0.002 at WKS = 20
= &minus;0.06 φ[1.35] = &minus;0.010 at WKS = 35
= &minus;0.06 φ[0.45] = &minus;0.022 at WKS = 50
</p>
<p>For Non Blue-Collar Workers: &minus;0.06 φ[2.5&minus; 0.06 WKS ]
= &minus;0.06 φ[1.3] = &minus;0.010 at WKS = 20
= &minus;0.06 φ[0.4] = &minus;0.022 at WKS = 35
= &minus;0.06 φ[&minus;0.5] = &minus;0.021 at WKS = 50
</p>
<p>Note how different these marginal effects are for blue-collar versus non blue-collar workers even
for the same weeks worked. Increasing weeks worked from 20 to 21 reduces the probability of
joining the union by 0.002 for a Blue-Collar worker. This is compared to five times that amount
for a Non Blue-Collar worker.
</p>
<p>13.4 Grouped Data
</p>
<p>In the biometrics literature, grouped data is very likely from laboratory experiments, see Cox
(1970). In the insecticide example, every dosage level xi is administered to a group of insects of
size ni, and the proportion of insects that die are recorded (pi). This is done for i = 1, 2, . . . ,M
dosage levels.
</p>
<p>P [yi = 1] = πi = P [I
&lowast;
i &le; Ii] = Φ(α+ βxi)
</p>
<p>where I&lowast;i is the tolerance and Ii = α + βxi is the stimulus. In economics, observations may be
grouped by income levels or age and we observe the labor participation rate for each income
or age group. For this type of grouped data, we estimate the probability of participating in
the labor force πi with pi, the proportion from the sample. This requires a large number of
observations in each group, i.e., a large ni for i = 1, 2, . . . ,M . In this case, the approximation is
</p>
<p>zi = Φ
&minus;1(pi) &sim;= α+ βxi (13.9)
</p>
<p>for each pi, we compute the standardized normal variates, the zi&rsquo;s, and we have an estimate of
α+ βxi. Note that the standard normal distribution assumption is not restrictive in the sense
that if I&lowast;i is N(μ, σ
</p>
<p>2) rather than N(0, 1), then one standardizes the P [I&lowast;i &le; Ii] by subtracting
μ and dividing by σ, in which case the new I&lowast;i is N(0, 1) and the new α is (α&minus; μ)/σ, whereas
the new β is β/σ. This also implies that μ and σ are not separately estimable. A plot of the zi&rsquo;s
versus the xi&rsquo;s would give estimates of α and β. For the biometrics example, one can compute
LD50, which is the dosage level that will kill 50% of the insect population. This corresponds to
zi = 0, which solves for xi = &minus;α̂/β̂. Similarly, LD95 corresponds to zi = 1.645, which solves for
xi = (1.645&minus; α̂)/β̂. Alternatively, for the economic example, LD50 is the minimum reservation
wage that is necessary for a 50% labor participation rate.
One could improve on this method by including more x&rsquo;s on the right hand side of (13.9).
</p>
<p>In this case, one can no longer plot the zi values versus the x variables. However, one can run</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Grouped Data 337
</p>
<p>OLS of z on these x&rsquo;s. One problem remains, OLS ignores the heteroskedasticity in the error
term. To see this:
</p>
<p>pi = πi + ǫi = F (x
&prime;
iβ) + ǫi (13.10)
</p>
<p>where F is a general c.d.f. and πi = F (x
&prime;
iβ). Using the properties of the binomial distribution,
</p>
<p>E(pi) = πi and var(pi) = πi(1&minus; πi)/ni. Defining zi = F&minus;1(pi), we obtain from (13.10)
</p>
<p>zi = F
&minus;1(pi) = F
</p>
<p>&minus;1(πi + ǫi) &sim;= F&minus;1(πi) + [dF&minus;1(πi)/dπi]ǫi (13.11)
</p>
<p>where the approximation &sim;= is a Taylor series expansion around πi with ǫi &rarr; 0. Since F is
monotonic πi = F (F
</p>
<p>&minus;1(πi)). Let wi = F&minus;1(πi) = x&prime;iβ, differentiating with respect to π gives
</p>
<p>1 = [dF (wi)/dwi]dwi/dπi (13.12)
</p>
<p>Alternatively, this can be rewritten as
</p>
<p>dF&minus;1(πi)/dπi = dwi/dπi = 1/{dF (wi)/dwi} = 1/f(wi) = 1/f(x&prime;iβ) (13.13)
</p>
<p>where f is the probability density function corresponding to F . Using (13.13), equation (13.11)
can be rewritten as
</p>
<p>zi = F
&minus;1(pi) &sim;= F&minus;1(πi) + ǫi/f(x&prime;iβ) (13.14)
</p>
<p>= F&minus;1(F (x&prime;iβ)) + ǫi/f(x
&prime;
iβ) = x
</p>
<p>&prime;
iβ + ǫi/f(x
</p>
<p>&prime;
iβ)
</p>
<p>From (13.14), it is clear that the regression disturbances of zi on xi are given by ui &sim;= ǫi/f(x&prime;iβ),
with E(ui) = 0 and σ
</p>
<p>2
i = var(ui) = var(ǫi)/f
</p>
<p>2(x&prime;iβ) = πi(1 &minus; πi)/(nif2i ) = Fi(1 &minus; Fi)/(nif2i )
since πi = Fi where the subscript i on f or F denotes that the argument of that function is x
</p>
<p>&prime;
iβ .
</p>
<p>This heteroskedasticity in the disturbances renders OLS on (13.14) consistent but inefficient. For
the probit, σ2i = Φi(1&minus;Φi)/(niφ2i ), and for the logit, σ2i = 1/[niΛi(1&minus;Λi)], since fi = Λi(1&minus;Λi).
Using 1/σi as weights, a WLS procedure can be performed on (13.14). Note that F
</p>
<p>&minus;1(p) for
the logit is simply log[p/(1 &minus; p)]. This is one more reason why the logistic functional form is
so popular. In this case one regresses log[p/(1&minus; p)] on x correcting for heteroskedasticity using
WLS. This procedure is also known as the minimum logit chi-square method and is due to
Berkson (1953).
</p>
<p>In order to obtain feasible estimates of the σi&rsquo;s, one could use the OLS estimates of β from
(13.14), to estimate the weights. Greene (1993) argues that one should not use the proportions
pi&rsquo;s as estimates for the πi&rsquo;s because this is equivalent to using the y
</p>
<p>2
i &rsquo;s instead of σ
</p>
<p>2
i in the het-
</p>
<p>eroskedastic regression. These will lead to inefficient estimates. If OLS on (13.14) is reported one
should use the robust White heteroskedastic variance-covariance option, otherwise the standard
errors are biased and inference is misleading.
</p>
<p>Example 1: Beer Taxes and Motor Vehicle Fatality. Ruhm (1996) used grouped logit analysis
with fixed time and state effects to study the impact of beer taxes and a variety of alcohol-control
policies on motor vehicle fatality rates. Ruhm collected panel data for 48 states (excluding
Alaska, Hawaii and the District of Columbia) over the period 1982-1988. The dependent variable
is a proportion p, denoting the total vehicle fatality rate per capita for state i at time t. One
can perform the inverse logit transformation, log[p/(1&minus; p)], provided p is not zero or one, and
run the usual fixed effects regression described in Chapter 12. Denote this dependent variable</p>
<p/>
</div>
<div class="page"><p/>
<p>338 Chapter 13: Limited Dependent Variables
</p>
<p>by (LFVR). The explanatory variables included the real beer tax rate on 24 (12 oz.) containers
of beer (BEERTAX), the minimum legal drinking age (MLDA) in years, the percentage of the
population living in dry counties (DRY), the average number of vehicle miles per person aged
16 and over (VMILES), and the percentage of young drivers (15-24 years old) (YNGDRV). Also
some dummy variables indicating the presence of alcohol regulations. These include BREATH
test laws which is a dummy variable that takes the value 1 if the state authorized the police
to administer pre-arrest breath test to establish probable cause for driving under the influence
(DUI). JAILD which takes the value of 1 if the state passed legislation mandating jail or
community service (COMSERD) for the first DUI conviction. Other variables included are the
unemployment rate, real per capita income, and state and time dummy variables. Details on
these variables are given in Table 1 of Ruhm (1996). Some of the variables in this data set
can be downloaded from the Stock and Watson (2003) web site at www.aw.com/stock watson.
Table 13.1 replicate to the extent possible the grouped logit regression results in column (d)
of Table 2, p. 444 of Ruhm (1996). This regression does not include some of the other alcohol
regulations that were not provided in the data set. These were replicated using the robust White
cross-section option in EViews.
</p>
<p>Table 13.1 Grouped Logit, Beer Tax and Motor Vehicle Fatality
</p>
<p>Dependent Variable: LVFR
Method: Panel Least Squares
Sample: 1982 1988
Cross-sections included: 48
Total panel (unbalanced) observations: 335
White cross-section standard errors &amp; covariance (d.f. corrected)
</p>
<p>Variable Coefficient Std. Error t-Statistic Prob.
</p>
<p>C &ndash;9.361589 0.132383 &ndash;70.71570 0.0000
BEERTAX &ndash;0.183533 0.077658 &ndash;2.363344 0.0188
MLDA &ndash;0.004465 0.007814 &ndash;0.571427 0.5682
DRY 0.008677 0.002611 3.323523 0.0010
YNGDRV 0.493472 0.450802 1.094651 0.2746
VMILES 6.91E-06 4.23E-06 1.632849 0.1037
BREATH &ndash;0.015930 0.027952 &ndash;0.569893 0.5692
JAILD &ndash;0.012623 0.038973 &ndash;0.323888 0.7463
COMSERD 0.020238 0.024505 0.825867 0.4096
PERINC 0.060305 0.010275 5.868945 0.0000
</p>
<p>Effects Specification
</p>
<p>Cross-section fixed (dummy variables)
Period fixed (dummy variables)
</p>
<p>R-squared 0.931852 Mean dependent var &ndash;8.534768
Adjusted R-squared 0.916318 S.D. dependent var 0.276471
S.E. of regression 0.079977 Akaike info criterion &ndash;2.046359
Sum squared resid 1.739808 Schwarz criterion &ndash;1.329075
Log likelihood 405.7652 F-statistic 59.98854
Durbin-Watson stat 1.433777 Prob(F-statistic) 0.000000</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Grouped Data 339
</p>
<p>Table 13.1 shows that the beer tax is negative and significant, while the minimum legal drinking
age is not significant. Neither is the breath test law, JAILD or COMSERD variables, all of which
represent state alcohol safety related legislation. Income per capita and the percentage of the
population living in dry counties have a positive and significant effect on motor vehicle fatality
rates. The state dummy variables are jointly significant with an observed F -value of 34.9 which is
distributed as F (47, 272). The year dummies are jointly significant with an observed F -value of
2.97 which is distributed as F (6, 272). Problem 12 asks the reader to replicate Table 13.1. These
results imply that increasing the minimum legal drinking age, or imposing stiffer punishments
like mandating jail or community service are not effective policy tools for decreasing traffic
related deaths. However, increasing the real tax on beer is an effective policy for reducing traffic
related deaths.
For grouped data, the sample sizes ni for each group have to be sufficiently large. Also, the
</p>
<p>pi&rsquo;s cannot be zero or one. One modification suggested in the literature is to add (1/2ni) to pi
when computing the log of odds ratio, see Cox (1970).
</p>
<p>Example 2: Fractional Response. Papke and Wooldridge (1996) argue that in many economic
settings pi may be 0 or 1 for a large number of observations. For example, when studying par-
ticipation rates in pension plans or when studying high school graduation rates. They propose a
fractional logit regression which handles fractional response variables based on quasi-likelihood
methods. Fractional response variables are bounded variables. Without loss of generality, they
could be restricted to lie between 0 and 1. Examples include the proportion of income spent on
charitable contributions, the fraction of total weekly hours spent working. Papke andWooldridge
(1996) propose modeling the E(yi/xi) as a logistic function Λ(x
</p>
<p>&prime;
iβ). This insures that the pre-
</p>
<p>dicted value of yi lies in the interval (0, 1). It is also well defined even if yi takes the values 0 or
1 with positive probability. It is important to note that in case yi is a proportion from a group
of known size ni, the quasi maximum likelihood method ignores the information on ni. Using
the Bernoulli log-likelihood function, one gets
</p>
<p>Li(β) = yi log[Λ(x
&prime;
iβ)] + (1&minus; yi) log[1&minus; Λ(x&prime;iβ)]
</p>
<p>for i = 1, 2, . . . , n, with 0 &lt; Λ(x&prime;iβ) &lt; 1.
Maximizing
</p>
<p>&sum;n
i=1 Li(β) with respect to β yields the quasi-MLE which is consistent and
</p>
<p>&radic;
n
</p>
<p>asymptotically normal regardless of the distribution of yi conditional on xi, see Gourieroux,
Monfort and Trognon (1984) and McCullagh and Nelder (1989). The latter proposed the gen-
eralized linear models (GLM) approach to this problem in statistics. Logit QMLE can be done
in Stata using the GLM command with the Binary family function indicating Bernoulli and the
Link function indicating the logistic distribution.
</p>
<p>Papke and Wooldridge (1996) derive robust asymptotic variance of the QMLE of β and sug-
gest some specification tests based on Wooldridge (1991). They apply their methods to the
participation in 401(K) pension plans. The data are from the 1987 IRS Form 5500 reports of
pension plans with more than 100 participants. This data set containing 4734 observations
can be downloaded from the Journal of Applied Econometrics Data Archive. We focus on a
subset of their data which includes 3874 observations of plans with match rates less than or
equal to one. Match rates above one may be indicating end-of-plan year employer contribu-
tions made to avoid IRS disqualification. Participation rates (PRATE) in this sample are high</p>
<p/>
</div>
<div class="page"><p/>
<p>340 Chapter 13: Limited Dependent Variables
</p>
<p>Table 13.2 Logit Quasi-MLE of Participation Rates in 401(K) Plan
</p>
<p>glm prate mrate log emp log emp2 age age2 sole if one==1, f(bin) l(logit) robust
note: prate has non-integer values
</p>
<p>Iteration 0: log pseudo-likelihood = &minus;1200.8698
Iteration 1: log pseudo-likelihood = &minus;1179.3843
Iteration 2: log pseudo-likelihood = &minus;1179.2785
Iteration 3: log pseudo-likelihood = &minus;1179.2785
Generalized linear models Number of obs = 3784
Optimization : ML: Newton-Raphson Residual df = 3777
</p>
<p>Scale parameter = 1
Deviance = 1273.60684 (1/df) Deviance = .3372006
Pearson = 724.4199889 (1/df) Pearson = .1917977
</p>
<p>Variance function : V(u) = u*(1-u) [Bernoulli]
Link function : g(u) = ln(u/(1-u)) [Logit]
Standard errors : Sandwich
</p>
<p>Log pseudo-likelihood = &minus;1179.278516
BIC = &minus;29843.34715 AIC = .6269971
</p>
<p>Robust
prate Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
</p>
<p>mrate 1.39008 .1077064 12.91 0.000 1.17898 1.601181
log emp &minus;1.001874 .1104365 &minus;9.07 0.000 &minus;1.218326 &minus;.7854229
log emp2 .0521864 .0071278 7.32 0.000 .0382161 .0661568
age .0501126 .0088451 5.67 0.000 .0327766 .0674486
age2 &minus;.0005154 .0002117 &minus;2.43 0.015 &minus;.0009303 &minus;.0001004
sole .0079469 .0502025 0.16 0.874 &minus;.0904482 .1063421
cons 5.057997 .4208646 12.02 0.000 4.233117 5.882876
</p>
<p>averaging 84.8%. Over 40% of the plans have a participation proportion of one. This makes
the log-odds ratio approach awkward since adjustments have to be made to more than 40%
of the observations. The plan match rate (MRATE) averages about 41 cents on the dollar.
Other explanatory variables include total firm employment (EMP), age of the plan (AGE),
a dummy variable (SOLE) which takes the value of 1 if the 401(K) plan is the only pension
plan offered by the employer. The 401(K) plans average 12 years in age, they are the SOLE
plan in 37% of the sample. The average employment is 4622. Problem 14 asks the reader
to replicate the descriptive statistic given in Table I of Papke and Wooldridge (1996, p. 627).
Table 13.2 gives the Stata output for logit QMLE using the same specification given in Table II
of Papke and Wooldridge (1996, p. 628). Note that it uses the GLM command, the Bernoulli
variance function and the logit link function. The results show that there is a positive and
significant relationship between match rate and participation rate. All the other variables in-
cluded are significant except for SOLE. Problem 14 asks the reader to replicate this result
and compare with OLS. The latter turns out to have a lower R2 and fails a RESET test, see
Chapter 8.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Individual Data: Probit and Logit 341
</p>
<p>13.5 Individual Data: Probit and Logit
</p>
<p>When the number of observations ni in each group is small, one cannot obtain reliable esti-
mates of the πi&rsquo;s with the pi&rsquo;s. In this case, one should not group the observations, instead
these observations should be treated as individual observations and the model estimated by the
maximum likelihood procedure. The likelihood is obtained as independent random draws from
a Bernoulli distribution with probability of success πi = F (x
</p>
<p>&prime;
iβ) = P [yi = 1]. Hence
</p>
<p>ℓ =
&prod;n
</p>
<p>i=1[F (x
&prime;
iβ)]
</p>
<p>yi [1&minus; F (x&prime;iβ)]1&minus;yi (13.15)
</p>
<p>and the log-likelihood
</p>
<p>logℓ =
&sum;n
</p>
<p>i=1{yilogF (x&prime;iβ) + (1&minus; yi)log[1&minus; F (x&prime;iβ)]} (13.16)
</p>
<p>The first-order conditions for maximization require the score S(β) = &part;logℓ/&part;β to be zero:
</p>
<p>S(β) = &part;logℓ/&part;β =
&sum;n
</p>
<p>i=1{[fiyi/Fi]&minus; (1&minus; yi)[fi/(1&minus; Fi)]}xi (13.17)
=
</p>
<p>&sum;n
i=1(yi &minus; Fi)fixi/[Fi(1&minus; Fi)] = 0
</p>
<p>where the subscript i on f or F denotes that the argument of that function is x&prime;iβ . For the
logit model (13.17) reduces to
</p>
<p>S(β) =
&sum;n
</p>
<p>i=1(yi &minus; Λi)xi = 0 since fi = Λi(1&minus; Λi) (13.18)
</p>
<p>If there is a constant in the model, the solution to (13.18) for xi = 1 implies that
&sum;n
</p>
<p>i=1 yi =&sum;n
i=1 Λ̂i. This means that the number of participants in the sample, i.e., those with yi = 1, will
</p>
<p>always be equal to the predicted number of participants from the logit model. Similarly, if xi
is a dummy variable which is 1 if the individual is male and zero if the individual is female,
then (13.18) states that the predicted frequency is equal to the actual frequency for males and
females. Note that (13.18) resembles the OLS normal equations if we interpret (yi &minus; Λ̂i) as
residuals. For the probit model (13.17) reduces to
</p>
<p>S(β) =
&sum;n
</p>
<p>i=1(yi &minus; Φi)φixi/[Φi(1&minus; Φi)] (13.19)
=
</p>
<p>&sum;
yi=0
</p>
<p>λoixi +
&sum;
</p>
<p>yi=1
λ1ixi = 0
</p>
<p>where λoi = &minus;φi/[1&minus;Φi] for yi = 0 and λ1i = φi/Φi for yi = 1. Also, Σyi=0 denotes the sum over
all zero values of yi. These λi&rsquo;s are thought of as generalized residuals which are orthogonal to
xi. Note that unlike the logit, the probit does not necessarily predict the number of participants
to be exactly equal to the number of ones in the sample.
</p>
<p>Equations (13.17) are highly nonlinear and may be solved using the scoring method, i.e.,
starting with some initial value βo we revise this estimate as follows:
</p>
<p>β1 = βo + [I
&minus;1(βo)]S(βo) (13.20)
</p>
<p>where S(β) = &part;logℓ/&part;β and I(β) = E[&minus;&part;2logℓ/&part;β&part;β&prime;]. This process is repeated until conver-
gence. For the logit and probit models, logF (x&prime;iβ) and log[1&minus; F (x&prime;iβ)] are concave. Hence, the
log-likelihood function given by (13.16) is globally concave, see Pratt (1981). Hence, for both
the logit and probit, [&part;2logℓ/&part;β&part;β&prime;] is negative definite for all values of β and the iterative
procedure will converge to the unique maximum likelihood estimate β̂MLE no matter what</p>
<p/>
</div>
<div class="page"><p/>
<p>342 Chapter 13: Limited Dependent Variables
</p>
<p>starting values we use. In this case, the asymptotic covariance matrix of β̂MLE is estimated by
I&minus;1(β̂MLE) from the last iteration.
</p>
<p>Amemiya (1981, p. 1495) derived I(β) by differentiating (13.17), multiplying by a negative
sign and taking the expected value, the result is given by:
</p>
<p>I(β) = &minus;E[&part;2logℓ/&part;β&part;β&prime;] =
&sum;n
</p>
<p>i=1 f
2
i xix
</p>
<p>&prime;
i/Fi(1&minus; Fi) (13.21)
</p>
<p>For the logit, (13.21) reduces to
</p>
<p>I(β) =
&sum;n
</p>
<p>i=1 Λi(1&minus; Λi)xix&prime;i (13.22)
</p>
<p>For the probit, (13.21) reduces to
</p>
<p>I(β) =
&sum;n
</p>
<p>i=1 φ
2
ixix
</p>
<p>&prime;
i/Φi(1&minus; Φi) (13.23)
</p>
<p>Alternative maximization may use the Newton-Raphson iterative procedure which uses the
Hessian itself rather than its expected value in (13.20), i.e., I(β) is replaced by H(β) =
[&minus;&part;2logℓ/&part;β&part;β&prime;]. For the logit model, H(β) = I(β) and is given in (13.22). For the probit
model, H(β) =
</p>
<p>&sum;n
i=1[λ
</p>
<p>2
i + λix
</p>
<p>&prime;
iβ]xix
</p>
<p>&prime;
i which is different from (13.23). Note that λi = λoi if
</p>
<p>yi = 0; and λi = λ1i if yi = 1. These were defined below (13.19).
A third method, suggested by Berndt, Hall, Hall and Hausman (1974) uses the outer product
</p>
<p>of the first derivatives in place of I(β), i.e., G(β) = S(β)S&prime;(β). For the logit model, this is
G(β) =
</p>
<p>&sum;n
i=1(yi &minus; Λi)2xix&prime;i. For the probit model, G(β) =
</p>
<p>&sum;n
i=1 λ
</p>
<p>2
ixix
</p>
<p>&prime;
i. As in the method of
</p>
<p>scoring, one iterates starting from initial estimates βo, and the asymptotic variance-covariance
matrix is estimated from the inverse of G(β̂), H(β̂) or I(β̂) in the last iteration.
</p>
<p>Test of hypotheses can be carried out from the asymptotic standard errors using t-statistics.
For Rβ = r type restrictions, the usual Wald test W = (Rβ̂ &minus; r)&prime;[RV (β̂)R&prime;]&minus;1(Rβ̂ &minus; r) can
be used with V (β̂) obtained from the last iteration as described above. Likelihood ratio and
Lagrange Multiplier statistics can also be computed. LR = &minus;2[logℓrestricted&minus; logℓunrestricted],
whereas, the Lagrange Multiplier statistic is LM = S&prime;(β)V (β)S(β), where S(β) is the score
evaluated at the restricted estimator. Davidson and MacKinnon (1984) suggest that V (β̂) based
on I(β̂) is the best of the three estimators to use. In fact, Monte Carlo experiments show that
the estimate of V (β̂) based on the outer product of the first derivatives usually performs the
worst and is not recommended in practice. All three statistics are asymptotically equivalent
and are asymptotically distributed as χ2q where q is the number of restrictions. The next section
discusses tests of hypotheses using an artificial regression.
</p>
<p>13.6 The Binary Response Model Regression1
</p>
<p>Davidson and MacKinnon (1984) suggest a modified version of the Gauss-Newton regression
(GNR) considered in Chapter 8 which is useful in the context of a binary response model
described in (13.5).2 In fact, we have shown that this model can be written as a nonlinear
regression
</p>
<p>yi = F (x
&prime;
iβ) + ui (13.24)
</p>
<p>with ui having zero mean and var(ui) = Fi(1&minus;Fi). The GNR ignoring heteroskedasticity yields
</p>
<p>(yi &minus; Fi) = fix&prime;ib+ residual</p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 The Binary Response Model Regression 343
</p>
<p>where b is the regression estimates when we regress (yi &minus; Fi) on fix&prime;i.
Correcting for heteroskedasticity by dividing each observation by its standard deviation we
</p>
<p>get the Binary Response Model Regression (BRMR):
</p>
<p>(yi &minus; Fi)&radic;
Fi(1&minus; Fi)
</p>
<p>=
fi&radic;
</p>
<p>Fi(1&minus; Fi)
x&prime;ib+ residual (13.25)
</p>
<p>For the logit model with fi = Λi(1&minus; Λi), this simplifies further to
</p>
<p>yi &minus; Λi&radic;
fi
</p>
<p>=
&radic;
</p>
<p>fix
&prime;
ib+ residual (13.26)
</p>
<p>For the probit model, the BRMR is given by
</p>
<p>yi &minus; Φi&radic;
Φi(1&minus; Φi)
</p>
<p>=
φi&radic;
</p>
<p>Φi(1&minus; Φi)
x&prime;ib+ residual (13.27)
</p>
<p>Like the GNR considered in Chapter 8, the BRMR given in (13.25) can be used for obtaining
parameter and covariance matrix estimates as well as test of hypotheses. In fact, Davidson and
MacKinnon point out that the transpose of the dependent variable in (13.25) times the matrix
of regressors in (13.25) yields a vector whose typical element is exactly that of S(β) given in
(13.17). Also, the transpose of the matrix of regressors in (13.25) multiplied by itself yields a
matrix whose typical element is exactly that of I(β) given in (13.21).
Let us consider how the BRMR is used to test hypotheses. Suppose that β&prime; = (β&prime;1, β
</p>
<p>&prime;
2) where
</p>
<p>β1 is of dimension k&minus; r and β2 is of dimension r. We want to test Ho; β2 = 0. Let β̃
&prime;
= (β̃1, 0)
</p>
<p>be the restricted MLE of β subject to Ho. In order to test Ho, we run the BRMR:
</p>
<p>yi &minus; F̃i&radic;
F̃i(1&minus; F̃i)
</p>
<p>=
f̃i&radic;
</p>
<p>F̃i(1&minus; F̃i)
x&prime;i1b1 +
</p>
<p>f̃i&radic;
F̃i(1&minus; F̃i)
</p>
<p>x&prime;i2b2 + residual (13.28)
</p>
<p>where x&prime;i = (x
&prime;
i1, x
</p>
<p>&prime;
i2) has been partitioned into vectors conformable with the corresponding
</p>
<p>partition of β. Also, F̃i = F (x
&prime;
iβ̃) and f̃i = f(x
</p>
<p>&prime;
iβ̃). The suggested test statistic for Ho is the
</p>
<p>explained sum of squares of the regression (13.28). This is asymptotically distributed as χ2r
under Ho.
</p>
<p>3 A special case of this BRMR is that of testing the null hypothesis that all the slope
coefficients are zero. In this case, xi1 = 1 and β1 is the constant α. Problem 2 shows that the
restricted MLE in this case is F̃ (α) = ȳ or α̃ = F&minus;1(ȳ), where ȳ is the proportion of the sample
with yi = 1. Therefore, the BRMR in (13.25) reduces to
</p>
<p>yi &minus; ȳ&radic;
ȳ(1&minus; ȳ)
</p>
<p>=
fi(α̃)&radic;
ȳ(1&minus; ȳ)
</p>
<p>b1 +
fi(α̃)&radic;
ȳ(1&minus; ȳ)
</p>
<p>x&prime;i2b2 + residual (13.29)
</p>
<p>Note that ȳ(1&minus; ȳ) is constant for all observations. The test for b2 = 0 is not affected by dividing
the dependent variable or the regressors by this constant, nor is it affected by subtracting a
constant from the dependent variable. Hence, the test for b2 = 0 can be carried out by regressing
yi on a constant and xi2 and testing that the slope coefficients of xi2 are zero using the usual
least squares F -statistic. This is a simpler alternative to the likelihood ratio test proposed in
the previous section and described in the empirical example in section 13.9. For other uses of
the BRMR, see Davidson and MacKinnon (1993).</p>
<p/>
</div>
<div class="page"><p/>
<p>344 Chapter 13: Limited Dependent Variables
</p>
<p>13.7 Asymptotic Variances for Predictions and Marginal Effects
</p>
<p>Two results of interest after estimating the model are: the predictions F (x&prime;β̂) and the marginal
effects &part;F/&part;x = f(x&prime;β̂)β̂. For example, given the characteristics of an individual x, we can
predict his or her probability of purchasing a car. Also, given a change in x, say income,
one can estimate the marginal effect this will have on the probability of purchasing a car.
The latter effect is constant for the linear probability model and is given by the regression
coefficient of income, whereas for the probit and logit models this marginal effect will vary
with the xi&rsquo;s, see (13.7) and (13.8). These marginal effects can be computed with Stata using
the dprobit command. The default is to compute them at the sample mean x. There is also
the additional problem of computing variances for these predictions and marginal effects. Both
F (x&prime;β̂) and f(x&prime;β̂)β̂ are nonlinear functions of the β̂&rsquo;s. To compute standard errors, we can use
the following linear approximation which states that whenever θ̂ = F (β̂) then the asy.var(θ̂) =
(&part;F/&part;β̂)&prime;V (β̂)(&part;F/&part;β̂). For the predictions, let z = x&prime;β̂ and denote by F̂ = F (x&prime;β̂) and f̂ =
f(x&prime;β̂), then
</p>
<p>&part;F̂ /&part;β̂ = (&part;F̂ /&part;z)(&part;z/&part;β̂) = f̂x and asy.var(F̂ ) = f̂2x&prime;V (β̂)x.
</p>
<p>For the marginal effects, let γ̂ = f̂ β̂, then
</p>
<p>asy.var(γ̂) = (&part;γ̂/&part;β̂
&prime;
)V (β̂)(&part;γ̂/&part;β̂
</p>
<p>&prime;
)&prime; (13.30)
</p>
<p>where &part;γ̂/&part;β̂
&prime;
= f̂ Ik + β̂(&part;f̂/&part;z)(&part;z/&part;β̂
</p>
<p>&prime;
) = f̂ Ik + (&part;f̂/&part;z)(β̂x
</p>
<p>&prime;).
</p>
<p>For the probit model, &part;f̂/&part;z = &part;φ̂/&part;z = &minus;zφ̂. So, &part;γ̂/&part;β̂&prime; = φ̂[Ik &minus; zβ̂x&prime;] and
</p>
<p>asy.var(γ̂) = φ̂
2
[Ik &minus; x&prime;β̂β̂x&prime;]V (β̂)[Ik &minus; x&prime;β̂β̂x&prime;]&prime; (13.31)
</p>
<p>For the logit model, f̂ = Λ̂(1&minus; Λ̂), so
&part;f̂/&part;z = (1&minus; 2Λ̂)(&part;Λ̂/&part;z) = (1&minus; 2Λ̂)(f̂) = (1&minus; 2Λ̂)Λ̂(1&minus; Λ̂)
&part;γ̂/&part;β̂
</p>
<p>&prime;
= Λ̂(1&minus; Λ̂)[Ik + (1&minus; 2Λ̂)β̂x&prime;]
</p>
<p>and (13.30) becomes
</p>
<p>asy.var(γ̂) = [Λ̂(1&minus; Λ̂)]2[Ik + (1&minus; 2Λ̂)β̂x&prime;]V (β̂)[Ik + (1&minus; 2Λ̂)β̂x&prime;]&prime; (13.32)
</p>
<p>13.8 Goodness of Fit Measures
</p>
<p>There are problems with the use of conventional R2-type measures when the explained variable
y takes on two values, see Maddala (1983, pp. 37&ndash;41). The predicted values ŷ are probabilities
and the actual values of y are either 0 or 1 so the usual R2 is likely to be very low. Also, if there
is a constant in the model the linear probability and logit models satisfy
</p>
<p>&sum;n
i=1 yi =
</p>
<p>&sum;n
i=1 ŷi.
</p>
<p>However, the probit model does not necessarily satisfy this exact relationship although it is
approximately valid.
Several R2-type measures have been suggested in the literature, some of these are the follow-
</p>
<p>ing:
</p>
<p>(i) The squared correlation between y and ŷ: R21 = r
2
y,ŷ.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.9 Empirical Examples 345
</p>
<p>(ii) Measures based on the residual sum of squares: Effron (1978) suggested using
</p>
<p>R22 = 1&minus; [
&sum;n
</p>
<p>i=1(yi &minus; ŷi)2/
&sum;n
</p>
<p>i=1(yi &minus; ȳ)2] = 1&minus; [n
&sum;n
</p>
<p>i=1(yi &minus; ŷi)2/n1n2]
</p>
<p>since
&sum;n
</p>
<p>i=1(yi&minus; ȳ)2 =
&sum;n
</p>
<p>i=1 y
2
i &minus;nȳ2 = n1&minus;n(n1/n)2 = n1n2/n, where n1 =
</p>
<p>&sum;n
i=1 yi and
</p>
<p>n2 = n&minus; n1.
Amemiya (1981, p. 1504) suggests using
</p>
<p>[&sum;n
i=1(yi &minus; ŷi)2/ŷi(1&minus; ŷi)
</p>
<p>]
as the residual sum
</p>
<p>of squares. This weights each squared error by the inverse of its variance.
</p>
<p>(iii) Measures based on likelihood ratios: R23 = 1&minus; (ℓr/ℓu)2/n where ℓr is the restricted likeli-
hood and ℓu is the unrestricted likelihood. This tests that all the slope coefficients are zero
in the standard linear regression model. For the limited dependent variable model however,
the likelihood function has a maximum of 1. This means that ℓr &le; ℓu &le; 1 or ℓr &le; (ℓr/ℓu) &le;
1 or ℓ
</p>
<p>2/n
r &le; 1 &minus; R23 &le; 1 or 0 &le; R23 &le; 1 &minus; ℓ
</p>
<p>2/n
r . Hence, Cragg and Uhler (1970) suggest a
</p>
<p>pseudo-R2 that lies between 0 and 1, and is given by R24 = (ℓ
2/n
u &minus; ℓ2/nr )/[(1&minus; ℓ2/nr )/ℓ2/nu ].
</p>
<p>Another measure suggested by McFadden (1974) is R25 = 1&minus; (logℓu/logℓr).
</p>
<p>(iv) Proportion of correct predictions: After computing ŷ, one classifies the i-th observation
as a success if ŷi &gt; 0.5, and a failure if ŷi &lt; 0.5. This measure is useful but may not have
enough discriminatory power.
</p>
<p>13.9 Empirical Examples
</p>
<p>Example 1: Union Participation
To illustrate the logit and probit models, we consider the PSID data for 1982 used in Chapter 4.
In this example, we are interested in modelling union participation. Out of the 595 individuals
observed in 1982, 218 individuals had their wage set by a union and 377 did not. The explanatory
variables used are: years of education (ED), weeks worked (WKS), years of full-time work
experience (EXP), occupation (OCC = 1, if the individual is in a blue-collar occupation),
residence (SOUTH = 1, SMSA = 1, if the individual resides in the South, or in a standard
metropolitan statistical area), industry (IND = 1, if the individual works in a manufacturing
industry), marital status (MS = 1, if the individual is married), sex and race (FEM = 1,
BLK = 1, if the individual is female or black). A full description of the data is given in Cornwell
and Rupert (1988). The results of the linear probability, logit and probit models are given in
Table 13.3. These were computed using EViews. In fact Table 13.4 gives the probit output.
We have already mentioned that the probit model normalizes σ to be 1. But, the logit model
has variance π2/3. Therefore, the logit estimates tend to be larger than the probit estimates
although by a factor less than π/
</p>
<p>&radic;
3. In order to make the logit results comparable to those of
</p>
<p>the probit, Amemiya (1981) suggests multiplying the logit coefficient estimates by 0.625.
Similarly, to make the linear probability estimates comparable to those of the probit model
</p>
<p>one needs to multiply these coefficients by 2.5 and then subtract 1.25 from the constant term.
For this example, both logit and probit procedures converged quickly in 4 iterations. The log-
likelihood values and McFadden&rsquo;s (1974) R2 obtained for the last iteration are recorded.</p>
<p/>
</div>
<div class="page"><p/>
<p>346 Chapter 13: Limited Dependent Variables
</p>
<p>Table 13.3 Comparison of the Linear Probability, Logit and Probit Models: Union Participation&lowast;
</p>
<p>Variable OLS Logit Probit
</p>
<p>EXP &minus;.005 (1.14) &minus;.007 (1.15) &minus;.007 (1.21)
WKS &minus;.045 (5.21) &minus;.068 (5.05) &minus;.061 (5.16)
OCC .795 (6.85) 1.036 (6.27) .955 (6.28)
IND .075 (0.79) .114 (0.89) .093 (0.76)
SOUTH &minus;.425 (4.27) &minus;.653 (4.33) &minus;.593 (4.26)
SMSA .211 (2.20) .280 (2.05) .261 (2.03)
MS .247 (1.55) .378 (1.66) .351 (1.62)
FEM &minus;.272 (1.37) &minus;.483 (1.58) &minus;.407 (1.47)
ED &minus;.040 (1.88) &minus;.057 (1.85) &minus;.057 (1.99)
BLK .125 (0.71) .222 (0.90) .226 (0.99)
Const 1.740 (5.27) 2.738 (3.27) 2.517 (3.30)
Log-likelihood &minus;312.337 &minus;313.380
McFadden&rsquo;s R2 0.201 0.198
χ210 157.2 155.1
</p>
<p>&lowast; Figures in parentheses are t-statistics
</p>
<p>Note that the logit and probit estimates yield similar results in magnitude, sign and significance.
One would expect different results from the logit and probit only if there are several observations
in the tails. The following variables were insignificant at the 5% level: EXP, IND, MS, FEM
and BLK. The results show that union participation is less likely if the individual resides in the
South and more likely if he or she resides in a standard metropolitan statistical area. Union
participation is also less likely the more the weeks worked and the higher the years of education.
Union participation is more likely for blue-collar than non blue-collar occupations. The linear
probability model yields different estimates from the logit and probit results. OLS predicts two
observations with ŷi &gt; 1, and 29 observations with ŷi &lt; 0. Table 13.5 gives the actual versus
predicted values of union participation for the linear probability, logit and probit models. The
percentage of correct predictions is 75% for the linear probability and probit model and 76%
for the logit model.
One can test the significance of all slope coefficients by computing the LR based on the
</p>
<p>unrestricted log-likelihood value (logℓu) reported in Table 13.3, and the restricted log-likelihood
value including only the constant. The latter is the same for both the logit and probit models
and is given by
</p>
<p>logℓr = n[ȳlogȳ + (1&minus; ȳ)log(1&minus; ȳ)] (13.33)
</p>
<p>where ȳ is the proportion of the sample with yi = 1, see problem 2. In this example, ȳ =
218/595 = 0.366 and n = 595 with logℓr = &minus;390.918. Therefore, for the probit model,
</p>
<p>LR = &minus;2[logℓr &minus; logℓu] = &minus;2[&minus;390.918 + 313.380] = 155.1
</p>
<p>which is distributed as χ210 under the null of zero slope coefficients. This is highly significant
and the null is rejected. Similarly, for the logit model this LR statistic is 157.2. For the linear
probability model, the same null hypothesis of zero slope coefficients can be tested using a</p>
<p/>
</div>
<div class="page"><p/>
<p>13.9 Empirical Examples 347
</p>
<p>Table 13.4 Probit Estimates: Union Participation
</p>
<p>Dependent Variable: UNION
Method: ML &ndash; Binary Probit
Sample: 1 595
Included observations: 595
Convergence achieved after 5 iterations
Covariance matrix computed using second derivatives
</p>
<p>Variable Coefficient Std. Error z-Statistic Prob.
</p>
<p>EX &ndash;0.006932 0.005745 &ndash;1.206491 0.2276
WKS &ndash;0.060829 0.011785 &ndash;5.161666 0.0000
OCC 0.955490 0.152137 6.280476 0.0000
IND 0.092827 0.122774 0.756085 0.4496
SOUTH &ndash;0.592739 0.139102 &ndash;4.261183 0.0000
SMSA 0.260700 0.128630 2.026741 0.0427
MS 0.350520 0.216284 1.620648 0.1051
FEM &ndash;0.407026 0.277038 &ndash;1.469203 0.1418
ED &ndash;0.057382 0.028842 &ndash;1.989515 0.0466
BLK 0.226482 0.228845 0.989675 0.3223
C 2.516784 0.762612 3.300217 0.0010
</p>
<p>Mean dependent var 0.366387 S.D. dependent var 0.482222
S.E. of regression 0.420828 Akaike info criterion 1.090351
Sum squared resid 103.4242 Schwarz criterion 1.171484
Log likelihood &ndash;313.3795 Hannan-Quinn criter. 1.121947
Restr. log likelihood &ndash;390.9177 Avg. log likelihood &ndash;0.526688
LR statistic (10 df) 155.0763 McFadden R-squared 0.198349
Probability(LR stat) 0.000000
</p>
<p>Obs with Dep=0 377 Total obs 595
Obs with Dep=1 218
</p>
<p>Table 13.5 Actual Versus Predicted: Union Participation
</p>
<p>Predicted
</p>
<p>Union=0 Union=1
Total
</p>
<p>Union=0 OLS = 312 OLS = 65 377
LOGIT = 316 LOGIT = 61
Probit = 314 Probit = 63
</p>
<p>Actual
</p>
<p>Union=1 OLS = 83 OLS = 135 218
LOGIT = 82 LOGIT = 136
Probit = 86 Probit = 132
</p>
<p>OLS = 395 OLS = 200 595
Total LOGIT = 398 LOGIT = 197
</p>
<p>Probit = 400 Probit = 195</p>
<p/>
</div>
<div class="page"><p/>
<p>348 Chapter 13: Limited Dependent Variables
</p>
<p>Chow F -statistic. This yields an observed value of 17.80 which is distributed as F (10, 584)
under the null hypothesis. Again, the null is soundly rejected. This F -test is in fact the BRMR
test considered in section 13.6. As described in section 13.8, McFadden&rsquo;s R2 is given by R25 =
1&minus; [logℓu/logℓr] which for the probit model yields
</p>
<p>R25 = 1&minus; (313.380/390.918) = 0.198.
</p>
<p>For the logit model, McFadden&rsquo;s R25 is 0.201.
</p>
<p>Example 2: Employment and Problem Drinking
Mullahy and Sindelar (1996) estimate a linear probability model relating employment and mea-
sures of problem drinking. The analysis is based on the 1988 Alcohol Supplement of the National
Health Interview Survey. This regression was performed for Males and Females separately since
the authors argue that women are less likely than men to be alcoholic, are more likely to ab-
stain from consumption, and have lower mean alcohol consumption levels. They also report
that women metabolize ethanol faster than do men and experience greater liver damage for
the same level of consumption of ethanol. The dependent variable takes the value 1 if the
individual was employed in the past two weeks and zero otherwise. The explanatory variables
included the 90th percentile of ethanol consumption in the sample (18 oz. for males and 10.8 oz.
for females) and zero otherwise. This variables is denoted by hvdrnk90. The state unemploy-
ment rate in 1988 (UE88), Age, Age2, schooling, married, family size, and white. Health status
dummies indicating whether the individual&rsquo;s health was excellent, very good, fair. Region of
residence, whether the individual resided in the northeast, midwest or south. Also, whether
he or she resided in center city (msa1) or other metropolitan statistical area (not center city,
msa2). Three additional dummy variables were included for the quarters in which the survey
was conducted. Details on the definitions of these variables are given in Table 1 of Mullahy
and Sindelar (1996). Table 13.6 gives the probit results based on n = 9822 males using Stata.
These results show a negative relationship between the 90th percentile alcohol variable and
the probability of being employed, but this has a p-value of 0.075. Mullahy and Sindelar find
that for both men and women, problem drinking results in reduced employment and increased
unemployment. Table 13.7 gives the marginal effects computed in Stata using the mfx option
after probit estimation. The marginal effects are computed at the sample mean of the variables,
except in the case of dummy variables where it is done for a discrete change from 0 to 1. For
example, the marginal effect of being a heavy drinker in the upper 90th percentile of ethanol
consumption in the sample, (given that all the other variables are evaluated at their mean and
dummy variables are changing from 0 to 1), is to decrease the probability of employment by
1.6%. These can also be computed at particular values of the explanatory variables with the
option at in Stata. In fact Table 13.8 gives the average marginal effect for all males. This can
be computed using the margeff command in Stata. In this case the average marginal effect for
a heavy drinker (-.0165) did not change much from the marginal effect computed at the sample
mean (-.0162) and neither did the standard error (.0096 compared with .0093). The goodness
of fit as measured by how well this probit classifies the predicted probabilities is given in Ta-
ble 13.9 using the estat classification option in Stata. The percentage of correct predictions is
90.79%. Problem 13 asks the reader to verify these results as well as those in the original article
by Mullahy and Sindelar (1996).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.9 Empirical Examples 349
</p>
<p>Table 13.6 Probit Estimates: Employment and Problem Drinking
</p>
<p>. probit emp hvdrnk90 ue88 age agesq educ married famsize white hlstat1 hlstat2 hlstat3 hlstat4
region1 region2 region3 msa1 msa2 q1 q2 q3, robust
</p>
<p>Probit regression Number of obs = 9822
Wald chi2(20) = 928.33
Prob &gt; chi2 = 0.0000
</p>
<p>Log pseudolikelihood = &ndash;2698.1797 Pseudo R2 = 0.1651
</p>
<p>Robust
emp Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
</p>
<p>hvdrnk90 &ndash;.1049465 .0589881 &ndash;1.78 0.075 &ndash;.2205612 .0106681
ue88 &ndash;.0532774 .0142025 &ndash;3.75 0.000 &ndash;.0811137 &ndash;.0254411
age .0996338 .0171185 5.82 0.000 .0660821 .1331855
</p>
<p>agesq &ndash;.0013043 .0002051 &ndash;6.36 0.000 &ndash;.0017062 &ndash;.0009023
educ .0471834 .0066739 7.07 0.000 .0341029 .0602639
</p>
<p>married .2952921 .0540858 5.46 0.000 .189286 .4012982
famsize .0188906 .0140463 1.34 0.179 &ndash;.0086398 .0464209
white .3945226 .0483381 8.16 0.000 .2997818 .4892634
</p>
<p>hlstat1 1.816306 .0983447 18.47 0.000 1.623554 2.009058
hlstat2 1.778434 .0991531 17.94 0.000 1.584098 1.972771
hlstat3 1.547836 .0982637 15.75 0.000 1.355243 1.74043
hlstat4 1.043363 .1077279 9.69 0.000 .8322205 1.254506
region1 .0343123 .0620021 0.55 0.580 &ndash;.0872096 .1558341
region2 .0604907 .0537885 1.12 0.261 &ndash;.0449327 .1659142
region3 .1821206 .0542346 3.36 0.001 .0758227 .2884185
msa1 &ndash;.0730529 .0518719 &ndash;1.41 0.159 &ndash;.1747199 .0286141
msa2 .0759533 .0513092 1.48 0.139 &ndash;.0246109 .1765175
</p>
<p>q1 &ndash;.1054844 .0527728 &ndash;2.00 0.046 &ndash;.2089171 &ndash;.0020516
q2 &ndash;.0513229 .0528185 &ndash;0.97 0.331 &ndash;.1548453 .0521995
q3 &ndash;.0293419 .0543751 &ndash;0.54 0.589 &ndash;.1359152 .0772313
</p>
<p>cons &ndash;3.017454 .3592321 &ndash;8.40 0.000 &ndash;3.721536 &ndash;2.313372
</p>
<p>Example 3: Fertility and Same Sex of Previous Children
Carrasco (2001) estimated a probit equation for fertility using PSID data over the period 1986&ndash;
1989. The sample consists of 1,442 married or cohabiting women between the ages of 18 and
55 in 1986. The dependent variable fertility (f) is specified by a dummy variable that equals 1
if the age of the youngest child in the next year is 1. The explanatory variables are: (ags26l)
which is a dummy variable that equals 1 if the woman has a child between 2 and 6 years
old; education which has three levels (educ 1, educ 2 and educ 3), the female&rsquo;s age, race, and
husband&rsquo;s income. An indicator of same sex of previous children (dsex), and its components:
(dsexf) for girls, and (dsexm) for boys. This variable exploits the widely observed phenomenon
of parental preferences for a mixed sibling-sex composition in developed countries. Therefore, a
dummy for whether the sex of the next child matches the sex of the previous children provides a
plausible predictor for additional childbearing. The data set can be obtained from the Journal of
Business &amp; Economic Statistics archive data web site. Problem 15 asks the reader to replicate
some of the results obtained in the original article by Carrasco (2001). The estimates reveal
that having children of the same sex has a significant and positive effect on the probability of
having an additional child. The marginal effect of same sex children increases the probability
of fertility by 3%, see Table 13.10. These are obtained using the dprobit command in Stata.</p>
<p/>
</div>
<div class="page"><p/>
<p>350 Chapter 13: Limited Dependent Variables
</p>
<p>Table 13.7 Marginal Effects: Employment and Problem Drinking
</p>
<p>. mfx compute
</p>
<p>Marginal effects after probit
y = Pr(emp) (predict)
= .92244871
</p>
<p>variable dy/dx Std. Err. z P &gt; |z| [95% Conf. Interval] X
</p>
<p>hvdrnk90* &ndash;.0161704 .00962 &ndash;1.68 0.093 &ndash;.035034 .002693 .099165
ue88 &ndash;.0077362 .00205 &ndash;3.78 0.000 &ndash;.011747 &ndash;.003725 5.56921
age .0144674 .00248 5.83 0.000 .009607 .019327 39.1757
</p>
<p>agesq &ndash;.0001894 .00003 &ndash;6.37 0.000 &ndash;.000248 &ndash;.000131 1627.61
educ .0068513 .00096 7.12 0.000 .004966 .008737 13.3096
</p>
<p>married* .0488911 .01009 4.85 0.000 .029119 .068663 .816432
famsize .002743 .00204 1.35 0.179 &ndash;.001253 .006739 2.7415
white* .069445 .01007 6.90 0.000 .049709 .089181 .853085
</p>
<p>hlstat1* .2460794 .01484 16.58 0.000 .216991 .275167 .415903
hlstat2* .1842432 .00992 18.57 0.000 .164799 .203687 .301873
hlstat3* .130786 .00661 19.80 0.000 .11784 .143732 .205254
hlstat4* .0779836 .00415 18.77 0.000 .069841 .086126 .053451
region1* .0049107 .00875 0.56 0.575 &ndash;.012233 .022054 .203014
region2* .0086088 .0075 1.15 0.251 &ndash;.006092 .023309 .265628
region3* .0252543 .00715 3.53 0.000 .011247 .039262 .318265
msa1* &ndash;.0107946 .00779 &ndash;1.39 0.166 &ndash;.026061 .004471 .333232
msa2* .0109542 .00735 1.49 0.136 &ndash;.003456 .025365 .434942
</p>
<p>q1* &ndash;.0158927 .00825 &ndash;1.93 0.054 &ndash;.032053 .000268 .254632
q2* &ndash;.0075883 .00795 &ndash;0.95 0.340 &ndash;.023167 .007991 .252698
q3* &ndash;.0043066 .00807 &ndash;0.53 0.594 &ndash;.020121 .011508 .242822
</p>
<p>(*) dy/dx is for discrete change of dummy variable from 0 to 1
</p>
<p>13.10 Multinomial Choice Models
</p>
<p>In many economic situations, the choice may be among m alternatives where m &gt; 2. These may
be unordered alternatives like the selection of a mode of transportation, bus, car or train, or
an occupational choice like lawyer, carpenter, teacher, etc., or they may be ordered alternatives
like bond ratings, or the response to an opinion survey, which could vary from strongly agree to
strongly disagree. Ordered response multinomial models utilize the extra information implicit in
the ordinal nature of the dependent variable. Therefore, these models have a different likelihood
than unordered response multinomial models and have to be treated separately.
</p>
<p>13.10.1 Ordered Response Models
</p>
<p>Suppose there are three bond ratings, A, AA and AAA. We sample n bonds and the i-th bond
is rated A (which we record as yi = 0) if its performance index I
</p>
<p>&lowast;
i &lt; 0, where 0 is again not
</p>
<p>restrictive. I&lowast;i = x
&prime;
iβ + ui, so the probability of an A rating or the Pr[yi = 0] is
</p>
<p>π1i = Pr[yi = 0] = P [I
&lowast;
i &lt; 0] = P [ui &lt; &minus;x&prime;iβ] = F (&minus;x&prime;iβ) (13.34)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.10 Multinomial Choice Models 351
</p>
<p>Table 13.8 Average Marginal Effects: Employment and Problem Drinking
</p>
<p>. margeff
</p>
<p>Average partial effects after probit
y = Pr(emp)
</p>
<p>variable Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
</p>
<p>hvdrnk90 &ndash;.0164971 .009264 &ndash;1.78 0.075 &ndash;.0346543 .00166
ue88 &ndash;.0078854 .0019748 &ndash;3.99 0.000 &ndash;.011756 &ndash;.0040149
age .0147633 .0024012 6.15 0.000 .010057 .0194697
</p>
<p>agesq &ndash;.000193 .0000287 &ndash;6.73 0.000 &ndash;.0002493 &ndash;.0001368
educ .0069852 .0009316 7.50 0.000 .0051593 .0088112
</p>
<p>married .048454 .0070149 6.91 0.000 .0347051 .0622028
famsize .002796 .0019603 1.43 0.154 &ndash;.0010461 .0066382
white .0685255 .0062822 10.91 0.000 .0562127 .0808383
</p>
<p>hlstat1 .2849987 .0059359 48.01 0.000 .2733645 .2966328
hlstat2 .2318828 .0049776 46.59 0.000 .2221269 .2416386
hlstat3 .1725703 .0049899 34.58 0.000 .1627903 .1823502
hlstat4 .0914458 .0048387 18.90 0.000 .0819621 .1009295
region1 .0050178 .0083778 0.60 0.549 &ndash;.0114025 .021438
region2 .0088116 .0071262 1.24 0.216 &ndash;.0051556 .0227787
region3 .0259534 .0064999 3.99 0.000 .0132139 .0386929
msa1 &ndash;.0109515 .007632 &ndash;1.43 0.151 &ndash;.02591 .0040071
msa2 .0111628 .0067952 1.64 0.100 &ndash;.0021556 .0244811
</p>
<p>q1 &ndash;.0160925 .0080458 &ndash;2.00 0.045 &ndash;.0318619 &ndash;.0003231
q2 &ndash;.0077086 .0076973 &ndash;1.00 0.317 &ndash;.0227951 .0073779
q3 &ndash;.0043814 .0077835 &ndash;0.56 0.573 &ndash;.0196368 .010874
</p>
<p>Table 13.9 Actual vs Predicted: Employment and Problem Drinking
</p>
<p>. estat class
Probit model for emp
</p>
<p>True
Classified D &sim;D Total
+ 8743 826 9569
&ndash; 79 174 253
</p>
<p>Total 8822 1000 9822
</p>
<p>Classified + if predicted Pr(D) &gt;= .5
True D defined as emp != 0
</p>
<p>Sensitivity Pr( +| D) 99.10%
Specificity Pr( &ndash;|&sim;D) 17.40%
Positive predictive value Pr( D| +) 91.37%
Negative predictive value Pr(&sim;D| &ndash;) 68.77%
False + rate for true &sim;D Pr( +|&sim;D) 82.60%
False &ndash; rate for true D Pr( &ndash;| D) 0.90%
False + rate for classified Pr(&sim;D| +) 8.63%
False &ndash; rate for classified Pr( D| &ndash;) 31.23%
Correctly classified 90.79%</p>
<p/>
</div>
<div class="page"><p/>
<p>352 Chapter 13: Limited Dependent Variables
</p>
<p>Table 13.10 Marginal Effects: Fertility and Same Sex of Previous Children
</p>
<p>. dprobit f dsex ags26l educ 2 educ 3 age drace inc
</p>
<p>Probit regression, reporting marginal effects Number of obs = 5768
LR chi2(7) = 964.31
Prob &gt; chi2 = 0.0000
</p>
<p>Log likelihood = 1561.1312 Pseudo R2 = 0.2360
</p>
<p>f dF/dx Std. Err. z P &gt; |z| x-bar [95% Conf. Interval]
</p>
<p>dsex* .0302835 .0069532 5.40 0.000 .256415 .016655 .043912
ags26l* &ndash;.1618148 .0066629 &ndash;13.22 0.000 .377601 &ndash;.174874 &ndash;.148756
educ 2* .0022157 .0090239 0.24 0.808 .717753 &ndash;.015471 .019902
educ 3* .0288636 .0140083 2.45 0.014 .223994 .001408 .056319
</p>
<p>age &ndash;.0065031 .0007644 &ndash;16.65 0.000 32.8024 &ndash;.008001 &ndash;.005005
drace* &ndash;.0077119 .0055649 &ndash;1.45 0.146 .773232 &ndash;.018619 .003195
</p>
<p>inc .0002542 .000241 1.06 0.289 12.8582 &ndash;.000218 .000727
</p>
<p>obs. P .1137309
pred. P .0367557 (at xbar)
</p>
<p>(*) dF/dx is for discrete change of dummy variable from 0 to 1
z and P &gt; |z| correspond to the test of the underlying coefficient being 0
</p>
<p>The i-th bond is rated AA (which we record as yi = 1) if its performance index I
&lowast;
i is between
</p>
<p>0 and c where c is a positive number, with probability
</p>
<p>π2i = Pr[yi = 1] = P [0 &le; I&lowast;i &lt; c] (13.35)
= P [0 &le; x&prime;iβ + ui &lt; c] = F (c&minus; x&prime;iβ)&minus; F (&minus;x&prime;iβ)
</p>
<p>The i-th bond is rated AAA (which we record as yi = 2) if I
&lowast;
i &ge; c, with probability
</p>
<p>π3i = Pr[yi = 2] = P [I
&lowast;
i &ge; c] = P [x&prime;iβ + ui &ge; c] = 1&minus; F (c&minus; x&prime;iβ) (13.36)
</p>
<p>F can be the logit or probit function. The log-likelihood function for the ordered probit is given
by
</p>
<p>logℓ(β, c) =
&sum;
</p>
<p>yi=0
log(Φ(&minus;x&prime;iβ)) +
</p>
<p>&sum;
yi=1
</p>
<p>log[Φ(c&minus; x&prime;iβ)&minus; Φ(&minus;x&prime;iβ)] (13.37)
+
&sum;
</p>
<p>yi=2
log[1&minus; Φ(c&minus; x&prime;iβ)].
</p>
<p>For the probabilities given in (13.34), (13.35) and (13.36), the marginal effects of changes in the
regressors are:
</p>
<p>&part;π1i/&part;xi = &minus;f(&minus;x&prime;iβ)β (13.38)
&part;π2i/&part;xi = [f(&minus;x&prime;iβ)&minus; f(c&minus; x&prime;iβ)]β (13.39)
&part;π3i/&part;xi = f(c&minus; x&prime;iβ)β (13.40)
</p>
<p>Generalizing this model to m bond ratings is straight forward. The likelihood, the score and
the Hessian for the m-ordered probit model are given in Maddala (1983, pp. 47&ndash;49).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.10 Multinomial Choice Models 353
</p>
<p>Illustrative Example: Corporate Bond Rating. This data set is obtained from Baum(2006) by
issuing the following command in Stata:
</p>
<p>.use http://www.stata-press.com/data/imeus/panel84extract, clear
</p>
<p>This data set contains ratings of 98 corporate bonds coded as 2 to 5 (rating83c). The rating 2
corresponds to the lowest rating BA B C and 5 to the highest rating AAA. These are given in
Table 13.11.
</p>
<p>Table 13.11 Corporate Bond Rating
</p>
<p>. tab rating83c
</p>
<p>Bond rating, 1982 Freq. Percent Cum.
</p>
<p>BA B C 26 26.53 26.53
BAA 28 28.57 55.10
AA A 15 15.31 70.41
AAA 29 29.59 100.00
</p>
<p>Total 98 100.00
</p>
<p>This is modeled as an ordered logit with two explanatory variables: ia83, the income to asset
ratio in 1983, and the change in that ratio from 1982 to 1983 (dia). The summary statistics are
given in Table 13.12.
</p>
<p>Table 13.12 Ordered Logit
</p>
<p>Variable Obs Mean Std. Dev. Min Max
</p>
<p>rating83c 98 3.479592 1.17736 2 5
ia83 98 10.11473 7.441946 &ndash;13.08016 30.74564
dia 98 .7075242 4.711211 &ndash;10.79014 20.05367
</p>
<p>. ologit rating83c ia83 dia
</p>
<p>Ordered logistic regression Number of obs = 98
LR chi2(2) = 11.54
Prob &gt; chi2 = 0.0021
</p>
<p>Log likelihood = &ndash;127.27146 Pseudo R2 = 0.0434
</p>
<p>rating83c Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
</p>
<p>ia83 .0939166 .0296196 3.17 0.002 .0358633 .1519699
dia &ndash;.0866925 .0449789 &ndash;1.93 0.054 &ndash;.1748496 .0014646
</p>
<p>/cut1 &ndash;.1853053 .3571432 &ndash;.8852932 .5146826
/cut2 1.185726 .3882099 .4248488 1.946604
/cut3 1.908412 .4164896 1.092108 2.724717
</p>
<p>Income/assets has a positive effect on the rating, while the change in that ratio has a negative
effect! Both are significant.</p>
<p/>
</div>
<div class="page"><p/>
<p>354 Chapter 13: Limited Dependent Variables
</p>
<p>Table 13.13 Predicted Bond Rating
</p>
<p>. predict pbabc pbaa paa paaa, pr
</p>
<p>. sum pbabc pbaa paa paaa, separator(0)
</p>
<p>Variable Obs Mean Std. Dev. Min Max
</p>
<p>pbabc 98 .2729981 .1224448 .0388714 .7158453
pbaa 98 .2950074 .0456984 .0985567 .3299373
paa 98 .1496219 .0274841 .0449056 .1787291
</p>
<p>paaa 98 .2823726 .1304381 .0466343 .7528986
</p>
<p>The /cut1 to /cut3 give the estimated thresholds of the ratings categories using a logit specifi-
cation. The first one is not significant, while the other two are. Note that the 95% confidence
interval for these thresholds overlap.
We can predict the probability of achieving this rating using the predict command naming
</p>
<p>the values we want it to take for each category, see Table 13.13. The average of these predictions
is pretty close to the actual frequencies observed in each category.
</p>
<p>13.10.2 Unordered Response Models
</p>
<p>There are m choices each with probability πi1, πi2, . . . , πim for individual i. yij = 1 if individual
i chooses alternative j, otherwise it is 0. This means that
</p>
<p>&sum;m
j=1 yij = 1 and
</p>
<p>&sum;m
j=1 πij = 1. The
</p>
<p>likelihood function for n individuals is a multinomial given by:
</p>
<p>ℓ =
&prod; n
</p>
<p>i=1(πi1)
yi1(πi2)
</p>
<p>yi2 ..(πim)
yim (13.41)
</p>
<p>This model can be motivated by a utility maximization story where the utility that individual
i derives from say the occupational choice j is denoted by Uij and is a function of the job
attributes for the i-th individual, i.e., some xij &rsquo;s like the present value of potential earnings,
and training cost/net worth for that job choice for individual i, see Boskin (1974).
</p>
<p>Uij = x
&prime;
ijβ + ǫij (13.42)
</p>
<p>where β is a vector of implicit prices for these occupational characteristics. Therefore, the
probability of choosing the first occupation is given by:
</p>
<p>πi1 = Pr[Ui1 &gt; Ui2, Ui1 &gt; Ui3, . . . , Ui1 &gt; Uim] (13.43)
</p>
<p>= Pr[ǫi2 &minus; ǫi1 &lt; (x&prime;i1 &minus; x&prime;i2)β, ǫi3 &minus; ǫi1
&lt; (x&prime;i1 &minus; x&prime;i3)β, . . . , ǫim &minus; ǫi1 &lt; (x&prime;i1 &minus; x&prime;im)β]
</p>
<p>The normality assumption involves a number of integrals but has the advantage of not neces-
sarily assuming the ǫ&rsquo;s to be independent. The more popular assumption computationally is
the multinomial logit model. This arises if and only if the ǫ&rsquo;s are independent and identically
distributed as a Weibull density function, see McFadden (1974). The latter is given by F (z) =
exp(&minus;exp(&minus;z)). The difference between any two random variables with a Weibull distribution
has a logistic distribution Λ(z) = ez/1 + ez, giving the conditional logit model:
</p>
<p>πij = Pr[yi = j] = exp[(xij &minus; xim)&prime;β]/{1 +
&sum;m&minus;1
</p>
<p>j=1 exp[(xij &minus; xim)&prime;β]} (13.44)
= exp[x&prime;ijβ]/
</p>
<p>&sum;m
j=1 exp[x
</p>
<p>&prime;
ijβ] for j = 1, 2, . . . ,m&minus; 1</p>
<p/>
</div>
<div class="page"><p/>
<p>13.10 Multinomial Choice Models 355
</p>
<p>and πim = Pr[yi = m] = 1/{1 +
&sum;m&minus;1
</p>
<p>j=1 exp[(xij &minus; xim)&prime;β]} = exp[x&prime;imβ]/
&sum;m
</p>
<p>j=1exp[x
&prime;
ijβ]. There
</p>
<p>are two consequences of this conditional logit specification. The first is that the odds of any two
alternative occupations, say 1 and 2 is given by
</p>
<p>πi1/πi2 = exp[(xi1 &minus; xi2)&prime;β]
</p>
<p>and this does not change when the number of alternatives change from m to m&lowast;, since the
denominators divide out. Therefore, the odds are unaffected by an additional alternative. This
is known as the independence of irrelevant alternatives property and can represent a serious
weakness in the conditional logit model. For example, suppose the choices are between a pony
and a bicycle, and children choose a pony two-thirds of the time. Suppose that an additional
alternative is made available, an additional bicycle but of a different color, then one would still
expect two-thirds of the children to choose the pony and the remaining one-third to split choices
among the bicycles according to their color preference. In the conditional logit model, however,
the proportion choosing the pony must fall to one half if the odds relative to either bicycle is to
remain two to one in favor of the pony. This illustrates the point that when two or more of the
m alternatives are close substitutes, the conditional logit model may not produce reasonable
results. This feature is a consequence of assuming the errors ǫij &rsquo;s as independent. Hausman and
McFadden (1984) proposed a Hausman type test to check for the independence of these errors.
They suggest that if a subset of the choices is truly irrelevant then omitting it from the model
altogether will not change the parameter estimates systematically. Including them if they are
irrelevant preserves consistency but is inefficient. The test statistic is
</p>
<p>q = (β̂s &minus; β̂f )&prime;[V̂s &minus; V̂f ]&minus;1(β̂s &minus; β̂f ) (13.45)
</p>
<p>where s indicates the estimators based on the restricted subset and f denotes the estimator
based on the full set of choices. This is asymptotically distributed as χ2k, where k is the dimension
of β.
</p>
<p>Second, in this specification, none of the xij &rsquo;s can be constant across different alternatives,
because the corresponding β will not be identified. This means that we cannot include individual
specific variables that do not vary across alternatives like race, sex, age, experience, income,
etc. The latter type of data is more frequent in economics, see Schmidt and Strauss (1975). In
this case the specification can be modified to allow for a differential impact of the explanatory
variables upon the odds of choosing one alternative rather than the other:
</p>
<p>πij = Pr[yi = j] = exp(x
&prime;
ijβj)/
</p>
<p>&sum;m
j=1 exp(x
</p>
<p>&prime;
ijβj) for j = 1, . . . ,m (13.46)
</p>
<p>where now the parameter vector is indexed by j. If the xij &rsquo;s are the same for every j, then
</p>
<p>πij = Pr[yi = j] = exp(x
&prime;
iβj)/
</p>
<p>&sum;m
j=1 exp(x
</p>
<p>&prime;
iβj) for j = 1, . . . ,m (13.47)
</p>
<p>This is the model used by Schmidt and Strauss (1975). A normalization would be to take
βm = 0, in which case, we get the multinomial logit model
</p>
<p>πim = 1/
&sum;m
</p>
<p>j=1 exp(x
&prime;
iβj) (13.48)
</p>
<p>and
</p>
<p>πij = exp(x
&prime;
iβj)/[1 +
</p>
<p>&sum;m&minus;1
j=1 exp(x
</p>
<p>&prime;
iβj)] for j = 1, 2, . . . ,m&minus; 1. (13.49)</p>
<p/>
</div>
<div class="page"><p/>
<p>356 Chapter 13: Limited Dependent Variables
</p>
<p>The likelihood function, score equations, Hessian and information matrices are given in Maddala
(1983, pp. 36&ndash;37).
Multinomial Logit Model. Terza (2002) reconsidered the Mullahy and Sindelar (1996) data
</p>
<p>set for problem drinking described in section 13.9. However, Terza reclassified the dependent
variable as follows: y=1 when the individual is out of the labor force, y=2 when this individual
is unemployed, and y=3 when this individual is employed. Table 13.14 runs a multinomial
logit model which replicates some of the results in Terza (2002, p. 399) for males using Stata.
Although the health variables are still significant the problem drinking variable is not significant.
Problem 16 asks the reader to replicate these results for females.
</p>
<p>13.11 The Censored Regression Model
</p>
<p>Suppose one is interested in the amount one is willing to spend on the purchase of a durable
good. For example, a car. In this case, one would observe the expenditures only if the car is
bought, so
</p>
<p>y&lowast;i = x
&prime;
iβ + ui if y
</p>
<p>&lowast;
i &gt; 0 (13.50)
</p>
<p>where xi denotes a vector of household characteristics, such as income, number of children or
education. y&lowast;i is a latent variable, in this case the amount one is willing to spend on a car. We
observe yi = y
</p>
<p>&lowast;
i only if y
</p>
<p>&lowast;
i &gt; 0 and we set yi = 0 if y
</p>
<p>&lowast;
i &le; 0. The censoring at zero is of course
</p>
<p>arbitrary, and the ui&rsquo;s are assumed to be IIN(0, σ
2). This is known as the Tobit model after
</p>
<p>Tobin (1958). In this case, we have censored observations since we do not observe any y&lowast; that is
negative. All we observe is the fact that this household did not buy a car and a corresponding
vector xi of this household&rsquo;s characteristics. Without loss of generality, we assume that the first
n1 observations have positive y
</p>
<p>&lowast;
i &rsquo;s and the remaining n0 = n&minus;n1 observations have non-positive
</p>
<p>y&lowast;i &rsquo;s. In this case, OLS on the first n1 observations, i.e., using only the positive observed y
&lowast;
i &rsquo;s
</p>
<p>would be biased since ui does not have zero mean. In fact, by omitting observations for which
y&lowast;i &le; 0 from the sample, one is only considering disturbances from (13.50) such that ui &gt; &minus;x
</p>
<p>&prime;
</p>
<p>iβ.
The distribution of these ui&rsquo;s is a truncated normal density given in Figure 13.2. The mean of
this density is not zero and is dependent on β, σ2 and xi. More formally, the regression function
can be written as:
</p>
<p>E(y&lowast;i /xi, y
&lowast;
i &gt; 0) = x
</p>
<p>&prime;
iβ + E[ui/y
</p>
<p>&lowast;
i &gt; 0] = x
</p>
<p>&prime;
iβ + E[ui/ui &gt; &minus;x&prime;iβ] (13.51)
</p>
<p>= x&prime;iβ + σγi for i = 1, 2, . . . , n1
</p>
<p>where γi = φ(&minus;zi)/[1 &minus; Φ(&minus;zi)] and zi = x&prime;iβ/σ. See Greene (1993, p. 685) for the moments
of a truncated normal density or the Appendix to this chapter. OLS on the positive y&lowast;i &rsquo;s omits
the second term in (13.51), and is therefore biased and inconsistent.
A simple two-step can be used to estimate (13.51). First, we define a dummy variable di
</p>
<p>which takes the value 1 if y&lowast;i is observed and 0 otherwise. This allows us to perform probit
estimation on the whole sample, and provides us with a consistent estimator of (β/σ). Also,
P [di = 1] = P [y
</p>
<p>&lowast;
i &gt; 0] = P [ui &gt; &minus;x&prime;iβ] and P [di = 0] = P [y&lowast;i &le; 0] = P [ui &le; &minus;x&prime;iβ]. Therefore,
</p>
<p>the likelihood function is given by
</p>
<p>ℓ =
&prod;n
</p>
<p>i=1[P (ui &le; &minus;x&prime;iβ)]1&minus;di [P (ui &gt; &minus;x&prime;iβ)]di (13.52)
=
</p>
<p>&prod;n
i=1Φ(zi)
</p>
<p>di [1&minus; Φ(zi)]1&minus;di where zi = x&prime;iβ/σ</p>
<p/>
</div>
<div class="page"><p/>
<p>13.11 The Censored Regression Model 357
</p>
<p>Table 13.14 Multinomial Logit Results: Problem Drinking
</p>
<p>. mlogit y alc90th ue88 age agesq schooling married famsize white excellent verygood good fair
northeast midwest south centercity othermsa q1 q2 q3, baseoutcome(1)
</p>
<p>Multinomial logistic regression Number of obs = 9822
Wald chi2(20) = 1276.47
Prob &gt; chi2 = 0.0000
</p>
<p>Log likelihood = &ndash;3217.481 Pseudo R2 = 0.1655
</p>
<p>y Coef. Std. Err. z P &gt; |z| [95% Conf. Interval]
</p>
<p>2
alc90th .1270931 .21395 0.59 0.552 &ndash;.2922412 .5464274
</p>
<p>ue88 .0458099 .051355 0.89 0.372 &ndash;.0548441 .1464639
age .1617634 .0663205 2.44 0.015 .0317776 .2917492
</p>
<p>agesq &ndash;.0024377 .0007991 &ndash;3.05 0.002 &ndash;.004004 &ndash;.0008714
schooling &ndash;.0092135 .0245172 &ndash;0.38 0.707 &ndash;.0572664 .0388393
married .4004928 .1927458 2.08 0.038 .022718 .7782677
famsize .0622453 .0503686 1.24 0.217 &ndash;.0364753 .1609659
white .0391309 .1705625 0.23 0.819 &ndash;.2951653 .3734272
</p>
<p>excellent 2.91833 .4486757 6.50 0.000 2.038942 3.797719
verygood 2.978336 .4505932 6.61 0.000 2.09519 3.861483
</p>
<p>good 2.493939 .4446815 5.61 0.000 1.622379 3.365499
fair 1.460263 .4817231 3.03 0.002 .5161027 2.404422
</p>
<p>northeast .0849125 .2374365 0.36 0.721 &ndash;.3804545 .5502796
midwest .0158816 .2037486 0.08 0.938 &ndash;.3834583 .4152215
</p>
<p>south .1750244 .2027444 0.86 0.388 &ndash;.2223474 .5723962
centercity &ndash;.2717445 .1911074 &ndash;1.42 0.155 &ndash;.6463081 .1028192
othermsa &ndash;.0921566 .1929076 &ndash;0.48 0.633 &ndash;.4702486 .2859354
</p>
<p>q1 .422405 .1978767 2.13 0.033 .0345738 .8102362
q2 &ndash;.0219499 .2056751 &ndash;0.11 0.915 &ndash;.4250657 .3811659
q3 &ndash;.0365295 .2109049 &ndash;0.17 0.862 &ndash;.4498954 .3768364
</p>
<p>cons &ndash;6.113244 1.427325 &ndash;4.28 0.000 &ndash;8.910749 &ndash;3.315739
</p>
<p>3
alc90th &ndash;.1534987 .1395003 &ndash;1.10 0.271 &ndash;.4269144 .1199169
</p>
<p>ue88 &ndash;.0954848 .033631 &ndash;2.84 0.005 &ndash;.1614004 &ndash;.0295693
age .227164 .0409884 5.54 0.000 .1468282 .3074999
</p>
<p>agesq &ndash;.0030796 .0004813 &ndash;6.40 0.000 &ndash;.0040228 &ndash;.0021363
schooling .0890537 .0152314 5.85 0.000 .0592008 .1189067
married .7085708 .1219565 5.81 0.000 .4695405 .9476012
famsize .0622447 .0332365 1.87 0.061 &ndash;.0028975 .127387
white .7380044 .1083131 6.81 0.000 .5257147 .9502941
</p>
<p>excellent 3.702792 .1852415 19.99 0.000 3.339725 4.065858
verygood 3.653313 .1894137 19.29 0.000 3.282069 4.024557
</p>
<p>good 2.99946 .1786747 16.79 0.000 2.649264 3.349656
fair 1.876172 .1885159 9.95 0.000 1.506688 2.245657
</p>
<p>northeast .088966 .1491191 0.60 0.551 &ndash;.203302 .3812341
midwest .1230169 .1294376 0.95 0.342 &ndash;.130676 .3767099
</p>
<p>south .4393047 .1298054 3.38 0.001 .1848908 .6937185
centercity &ndash;.2689532 .1231083 &ndash;2.18 0.029 &ndash;.510241 &ndash;.0276654
othermsa .0978701 .1257623 0.78 0.436 &ndash;.1486195 .3443598
</p>
<p>q1 &ndash;.0274086 .1286695 &ndash;0.21 0.831 &ndash;.2795961 .224779
q2 &ndash;.110751 .126176 &ndash;0.88 0.380 &ndash;.3580514 .1365494
q3 &ndash;.0530835 .1296053 &ndash;0.41 0.682 &ndash;.3071052 .2009382
</p>
<p>cons &ndash;6.237275 .8886698 &ndash;7.02 0.000 &ndash;7.979036 &ndash;4.495515
</p>
<p>(y==1 is the base outcome)</p>
<p/>
</div>
<div class="page"><p/>
<p>358 Chapter 13: Limited Dependent Variables
</p>
<p>�
�
</p>
<p>�
�&ndash;
</p>
<p>Figure 13.2 Truncated Normal Distribution
</p>
<p>and once β/σ is estimated, we substitute these estimates in zi and γi given below (13.51) to get
γ̂i. The second step is to estimate (13.51) using only the positive y
</p>
<p>&lowast;
i &rsquo;s with γ̂i substituted for
</p>
<p>γi. The resulting estimator of β is consistent and asymptotically normal, see Heckman (1976,
1979).
Alternatively, one can use maximum likelihood procedures to estimate the Tobit model. Note
</p>
<p>that we have two sets of observations: (i) the positive y&lowast;i &rsquo;s with yi = y
&lowast;
i , for which we can write
</p>
<p>the density function N(x&prime;iβ, σ
2), and (ii) the non-positive y&lowast;i &rsquo;s for which we assign yi = 0 with
</p>
<p>probability
</p>
<p>Pr[yi = 0] = Pr[y
&lowast;
i &lt; 0] = Pr[ui &lt; &minus;x&prime;iβ] = Φ(&minus;x&prime;iβ/σ) = 1&minus; Φ(x&prime;iβ/σ) (13.53)
</p>
<p>The probability over the entire censored region gets assigned to the censoring point. This allows
us to write the following log-likelihood:
</p>
<p>logℓ = &minus;(1/2)
&sum;n1
</p>
<p>i=1 log(2πσ
2)&minus; (1/2σ2)
</p>
<p>&sum;n1
i=1(yi &minus; x&prime;iβ)2 (13.54)
</p>
<p>+
&sum;n
</p>
<p>i=n1+1
log[1&minus; Φ(x&prime;iβ/σ)]
</p>
<p>Differentiating with respect to β and σ2, see Maddala (1983, p. 153), one gets
</p>
<p>&part;logℓ/&part;β =
&sum;n1
</p>
<p>i=1(yi &minus; x&prime;iβ)xi/σ2 &minus;
&sum;n
</p>
<p>i=n1+1
φixi/σ[1&minus; Φi] (13.55)
</p>
<p>&part;logℓ/&part;σ2 =
&sum;n1
</p>
<p>i=1(yi &minus; x&prime;iβ)2/2σ4 &minus; (n1/2σ2) +
&sum;n
</p>
<p>i=n1+1
φix
</p>
<p>&prime;
iβ/[2σ
</p>
<p>3(1&minus; Φi)] (13.56)
where Φi and φi are evaluated at zi = x
</p>
<p>&prime;
iβ/σ.
</p>
<p>Premultiplying (13.55) by β&prime;/2σ2 and adding the result to (13.56), one gets
</p>
<p>σ̂2MLE =
&sum;n1
</p>
<p>i=1(yi &minus; x&prime;iβ)yi/n1 = Y &prime;1(Y1 &minus;X1β)/n1 (13.57)
</p>
<p>where Y1 denotes the n1 &times; 1 vector of non-zero observations on yi, X1 is the n1 &times; k matrix
of values of xi for the non-zero yi&rsquo;s. Also, after multiplying throughout by σ, (13.55) can be
written as:
</p>
<p>&minus;X &prime;0γ0 +X &prime;1(Y1 &minus;X1β)/σ = 0 (13.58)
</p>
<p>where X0 denotes the n0 &times; k matrix of xi&rsquo;s for which yi is zero, γ0 is an n0 &times; 1 vector of γi&rsquo;s
= φi/[1&minus;Φi] evaluated at zi = x&prime;iβ/σ for the observations for which yi = 0. Solving (13.58) one
gets
</p>
<p>β̂MLE = (X
&prime;
1X1)
</p>
<p>&minus;1X &prime;1Y1 &minus; σ(X &prime;1X1)&minus;1X &prime;0γ0 (13.59)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.12 The Truncated Regression Model 359
</p>
<p>Note that the first term in (13.59) is the OLS estimator for the first n1 observations for which
y&lowast;i is positive.
</p>
<p>One can use the Newton-Raphson procedure or the method of scoring, for the second deriva-
tives of the log-likelihood, see Maddala (1983, pp. 154&ndash;156). These can be computed with the
tobit command in Stata. Note that for the Tobit specification, both β and σ2 are identified.
This is contrasted to the logit and probit specifications where only the ratio (β/σ2) is identified.
Wooldridge (2009, Chapter 17) recommends one obtain the estimates of (β/σ2) from a probit
and comparing those with the Tobit estimates generated by dividing β̂ by σ̂2. If these estimates
are different or have different signs, then the Tobit estimation may not be appropriate. Problem
13.17 illustrates the Tobit estimation for married women labor supply example using the Mroz
(1987) data.
Maddala warns that the Tobit specification is not necessarily the right specification every time
</p>
<p>we have zero observations. It is applicable only in those cases where the latent variable can,
in principle, take negative values and the observed zero values are a consequence of censoring
and non-observability. In fact, one cannot have negative expenditures on a car, negative hours
of work or negative wages. However, one can enter employment and earn wages when one&rsquo;s
observed wage is larger than the reservation wage. Let y&lowast; be the difference between observed
wage and reservation wage. Only if y&lowast; is positive will wages be observed. Final warning: The
Tobit specification is heavily reliant on the normality and homoskedasticity assumptions. Failure
of these assumptions leads to misleading inference.
</p>
<p>13.12 The Truncated Regression Model
</p>
<p>The truncated regression model excludes or truncates some observations from the sample. For
example, in studying poverty we exclude the rich, say with earnings larger than some upper
limit yu from our sample. The sample is therefore not random and applying least squares to
the truncated sample lead to biased and inconsistent results, see Figure 13.3. This differs from
censoring. In the latter case, no data is excluded. In fact, we observe the characteristics of all
households even those that do not actually purchase a car. The truncated regression model is
given by
</p>
<p>y&lowast;i = x
&prime;
iβ + ui i = 1, 2, . . . , n with ui &sim; IIN(0, σ
</p>
<p>2) (13.60)
</p>
<p>where y&lowast;i is for example earnings of the i-th household and xi contains determinants of earnings
like education, experience, etc. The sample contains observations on individuals with y&lowast;i &le; yu.
The probability that y&lowast;i will be observed is
</p>
<p>Pr[y&lowast;i &le; yu] = Pr[x&prime;iβ + ui &le; yu] = Pr[ui &lt; yu &minus; x&prime;iβ] = Φ(
1
</p>
<p>σ
(yu &minus; x&prime;iβ)) (13.61)
</p>
<p>In addition, using the results of a truncated normal density, see Greene (1993, p. 685)
</p>
<p>E(ui/y
&lowast;
i &le; yu) =
</p>
<p>&minus;σφ((yu &minus; x&prime;iβ)/σ)
Φ((yu &minus; x&prime;iβ)/σ)
</p>
<p>(13.62)
</p>
<p>which is not necessarily zero. From (13.60) one can see that E(y&lowast;i /y
&lowast;
i &le; yu) = x&prime;iβ +E(ui/y&lowast;i &lt;
</p>
<p>yu). Therefore, OLS on (13.60) using the observed y&lowast;i is biased and inconsistent because it
ignores the term in (13.62).</p>
<p/>
</div>
<div class="page"><p/>
<p>360 Chapter 13: Limited Dependent Variables
</p>
<p>y
</p>
<p>Estimated line
</p>
<p>True line
</p>
<p>x
</p>
<p>�
�
</p>
<p>Figure 13.3 Truncated Regression Model
</p>
<p>The density of y&lowast;i is normal but its total area is given by (13.61). A proper density function
has to have an area of 1. Therefore, the density of y&lowast;i conditional on y
</p>
<p>&lowast;
i &le; yu is simply the
</p>
<p>conditional density of y&lowast;i restricted to values of y
&lowast;
i &le; yu divided by the Pr[y&lowast;i &le; yu], see the
</p>
<p>Appendix to this chapter:
</p>
<p>f(y&lowast;i ) =
φ((y&lowast;i &minus; x&prime;iβ/σ)
</p>
<p>σΦ((yu &minus; x&prime;iβ)/σ)
if y&lowast;i &le; yu (13.63)
</p>
<p>= 0 otherwise
</p>
<p>The log-likelihood function is therefore
</p>
<p>logℓ = &minus;n
2
log2π &minus; n
</p>
<p>2
logσ2 &minus; 1
</p>
<p>2σ2
&sum;n
</p>
<p>i=1(y
&lowast;
i &minus; x&prime;iβ)2 (13.64)
</p>
<p>&minus;&sum;ni=1 logΦ
(
yu &minus; x&prime;iβ
</p>
<p>σ
</p>
<p>)
</p>
<p>It is the last term which makes MLE differ from OLS on the observed sample. Hausman and
Wise (1977) applied the truncated regression model to data from the New Jersey negative-
income-tax experiment where families with incomes higher than 1.5 times the 1967 poverty line
were eliminated from the sample.
</p>
<p>13.13 Sample Selectivity
</p>
<p>In labor economics, one observes the market wages of individuals only if the worker participates
in the labor force. This happens when the worker&rsquo;s market wage exceeds his or her reservation
wage. In a study of earnings, one does not observe the reservation wage and for non-participants
in the labor force we record a zero market wage. This sample is censored because we observe
the characteristics of these non-labor participants. If we restrict our attention to labor market
participants only, then the sample is truncated. This example needs special attention because
the censoring is not based directly on the dependent variable, as in section 13.11. Rather, it is
based on the difference between market wage and reservation wage. This latent variable which
determines the sample selection is correlated with the dependent variable. Hence, least squares</p>
<p/>
</div>
<div class="page"><p/>
<p>13.13 Sample Selectivity 361
</p>
<p>on this model results in selectivity bias, see Heckman (1976, 1979). A sample generated by this
type of self-selection may not represent the true population distribution no matter how big
the sample size. However, one can correct for self-selection bias if the underlying sampling
generating process can be understood and relevant identification conditions are available, see
Lee (2001) for a excellent summary and the references cited there. In order to demonstrate this,
let the earnings equation be given by
</p>
<p>w&lowast;i = x
&prime;
1iβ + ui i = 1, 2, . . . , n (13.65)
</p>
<p>and the labor participation (or selection) equation be given by
</p>
<p>y&lowast;i = x
&prime;
2iγ + vi i = 1, 2, . . . , n (13.66)
</p>
<p>where ui and vi are bivariate normally distributed with mean zero and variance-covariance
</p>
<p>var
</p>
<p>(
ui
vi
</p>
<p>)
=
</p>
<p>[
σ2 ρσ
ρσ 1
</p>
<p>]
(13.67)
</p>
<p>Normalizing the variance of vi to be 1 is not restrictive, since only the sign of y
&lowast;
i is observed.
</p>
<p>In fact, we only observe wi and yi where
</p>
<p>wi = w
&lowast;
i if y
</p>
<p>&lowast;
i &gt; 0 (13.68)
</p>
<p>= 0 otherwise
</p>
<p>and
</p>
<p>yi = 1 if y
&lowast;
i &gt; 0
</p>
<p>= 0 otherwise
</p>
<p>We observe (yi = 0, wi = 0) and (yi = 1, wi = w
&lowast;
i ) only. The log-likelihood for this model is
</p>
<p>&sum;
yi=0
</p>
<p>log Pr[yi = 0] +
&sum;
</p>
<p>yi=1
log Pr[yi = 1]f(w
</p>
<p>&lowast;
i /yi = 1) (13.69)
</p>
<p>where f(w&lowast;i /yi = 1) is the conditional density of w
&lowast;
i given that yi = 1. The second term can also
</p>
<p>be written as Σyi=1log Pr[yi = 1/w
&lowast;
i ]f(w
</p>
<p>&lowast;
i ) which is another way of factoring the joint density
</p>
<p>function. f(w&lowast;i ) is in fact a normal density with conditional mean x
&prime;
1iβ and variance σ
</p>
<p>2. Using
properties of the bivariate normal density, one can write
</p>
<p>y&lowast;i = x
&prime;
2iγ + ρ
</p>
<p>(
1
</p>
<p>σ
(w&lowast;i &minus; x&prime;1iβ)
</p>
<p>)
+ ǫi (13.70)
</p>
<p>where ǫi &sim; IIN(0, σ
2(1&minus; ρ2)). Therefore,
</p>
<p>Pr[yi = 1] = Pr[y
&lowast;
i &gt; 0] = Φ
</p>
<p>(
x&prime;2iγ + ρ((wi &minus; x&prime;1iβ)/σ)&radic;
</p>
<p>1&minus; ρ2
</p>
<p>)
(13.71)
</p>
<p>where wi has been substituted for w
&lowast;
i since yi = 1. The likelihood function in (13.69) becomes
</p>
<p>&sum;
yi=0
</p>
<p>log(Φ(&minus;x&prime;2iγ)) +
&sum;
</p>
<p>yi=1
log
</p>
<p>(
1
</p>
<p>σ
φ(wi &minus; x&prime;1iβ)
</p>
<p>)
(13.72)
</p>
<p>+
&sum;
</p>
<p>yi=1
logΦ
</p>
<p>(
x&prime;2iγ + ρ((wi &minus; x&prime;1iβ)/σ&radic;
</p>
<p>1&minus; ρ2
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>362 Chapter 13: Limited Dependent Variables
</p>
<p>MLE may be computationally burdensome. Heckman (1976) suggested a two-step procedure
which is based on rewriting (13.65) as
</p>
<p>w&lowast;i = βx
&prime;
1i + ρσvi + ηi (13.73)
</p>
<p>and replacing w&lowast;i by wi and vi by its conditional mean E[vi/yi = 1]. Using the results on
truncated density, this conditional mean is given by φ(x&prime;2iγ)/Φ(&minus;x&prime;2iγ) known also as the inverse
Mills ratio. Hence, (13.73) becomes
</p>
<p>wi = x
&prime;
1iβ + ρσ
</p>
<p>φ(x&prime;2iγ)
Φ(&minus;x&prime;2iγ)
</p>
<p>+ residual (13.74)
</p>
<p>Heckman&rsquo;s (1976) two-step estimator consists of (i) running a probit on (13.66) in the first step
to get a consistent estimate of γ, (ii) substituting the estimated inverse Mills ratio in (13.74)
and running OLS. Since σ is positive, this second stage regression provides a test for sample
selectivity, i.e., for ρ = 0, by checking whether the t-statistic on the estimated inverse Mills ratio
is significant. This statistic is asymptotically distributed as N(0, 1). Rejecting Ho implies there
is a selectivity problem and one should not rely on OLS on (13.65) which ignores the selectivity
bias term in (13.74). Davidson and MacKinnon (1993) suggest performing MLE using (13.72)
rather than relying on the two-step results in (13.74) if the former is not computationally bur-
densome. Note that the Tobit model for car purchases given in (13.50) can be thought of as
a special case of the sample selectivity model given by (13.65) and (13.66). In fact, the Tobit
model assumes that the selection equation (the decision to buy a car) and the car expenditure
equation (conditional on the decision to buy) are identical. Therefore, if one thinks that the
specification of the selection equation is different from that of the expenditure equation, then
one should not use the Tobit model. Instead, one should proceed with the two equation sam-
ple selectivity model discussed in this section. It is also important to emphasize that for the
censored, truncated and sample selectivity models, normality and homoskedasticity are crucial
assumptions. Suggested tests for these assumptions are given in Bera, Jarque and Lee (1984),
Lee and Maddala (1985) and Pagan and Vella (1989). Alternative estimation methods that are
more robust to violations of normality and heteroskedasticity include symmetrically trimmed
least squares for Tobit models and least absolute deviations estimation for censored regression
models. These were suggested by Powell (1984, 1986).
</p>
<p>Notes
</p>
<p>1. This is based on Davidson and MacKinnon (1993, pp. 523&ndash;526).
</p>
<p>2. A binary response model attempts to explain a zero-one (or binary) dependent variable.
</p>
<p>3. One should not use nR2 as the test statistic because the total sum of squares in this case is not n.
</p>
<p>Problems
</p>
<p>1. The Linear Probability Model.
</p>
<p>(a) For the linear probability model described in (13.1), show that for E(ui) to equal zero, we
must have Pr[yi = 1] = x
</p>
<p>&prime;
iβ.
</p>
<p>(b) Show that ui is heteroskedastic with var(ui) = x
&prime;
iβ(1&minus; x&prime;iβ).</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 363
</p>
<p>2. Consider the general log-likelihood function given in (13.16). Assume that all the regression slopes
are zero except for the constant α.
</p>
<p>(a) Show that maximizing logℓ with respect to the constant α yields F̂ (α) = ȳ, where ȳ is the
proportion of the sample with yi = 1.
</p>
<p>(b) Conclude that the value of the maximized likelihood is logℓr = n[ȳlogȳ + (1&minus; ȳ)log(1&minus; ȳ)].
(c) Verify that for the union participation example in section 13.9 that logℓr = &minus;390.918.
</p>
<p>3. For the union participation example considered in section 13.9:
</p>
<p>(a) Replicate Tables 13.3 and 13.5.
</p>
<p>(b) Using the measures of fit considered in section 13.8, compute R21, R
2
2, . . . , R
</p>
<p>2
5 for the logit and
</p>
<p>probit models.
</p>
<p>(c) Compute the predicted value for the 10th observation using OLS, logit and probit models.
Also, the corresponding standard errors.
</p>
<p>(d) The industry variable (IND) was not significant in all models. Drop this variable and run
OLS, logit and probit. How do the results change? Compare with Tables 13.3 and 13.5.
</p>
<p>(e) Using the model results in part (d), test that the slope coefficients are all zero for the logit,
probit, and linear probability models.
</p>
<p>(f) Test that the coefficients of IND, FEM and BLK in Table 13.3 are jointly insignificant using
a LR test, a Wald test and a BRMR using OLS, logit and probit.
</p>
<p>4. For the data used in the union participation example in section 13.9:
</p>
<p>(a) Run OLS, logit and probit using as the dependent variable OCC which is one if the individual
is in a blue-collar occupation, and zero otherwise. For the independent variables use ED,
WKS, EXP, SOUTH, SMSA, IND, MS, FEM and BLK. Compare the coefficient estimates
across the three models. What variables are significant?
</p>
<p>(b) Using the measures of fit considered in section 13.8, compute R21, R
2
2, . . . , R
</p>
<p>2
5 for the logit and
</p>
<p>probit models.
</p>
<p>(c) Tabulate the actual versus predicted values for OCC from all three model results, like Ta-
ble 13.5 for Union. What is the proportion of correct decisions for OLS, logit and probit?
</p>
<p>(d) Test that the slope coefficients are all zero for the logit, probit, and linear probability models.
</p>
<p>5. Truncated Uniform Density. Let x be a uniformly distributed random variable with density
</p>
<p>f(x) =
1
</p>
<p>2
for &minus; 1 &lt; x &lt; 1
</p>
<p>(a) What is the density function of f (x/x &gt; &minus;1/2)? Hint: Use the definition of a conditional
density
</p>
<p>f (x/x &gt; &minus;1/2) = f(x)/Pr[x &gt; &minus;1/2] for &minus; 1
2
&lt; x &lt; 1.
</p>
<p>(b) What is the conditional mean E(x/x &gt; &minus;1/2)? How does it compare with the unconditional
mean of x? Note that because we truncated the density from below, the new mean should
shift to the right.
</p>
<p>(c) What is the conditional variance var(x/x &gt; &minus;1/2)? How does it compare to the unconditional
var(x)? (Truncation reduces the variance).
</p>
<p>6. Truncated Normal Density. Let x be N(1, 1). Using the results in the Appendix, show that:</p>
<p/>
</div>
<div class="page"><p/>
<p>364 Chapter 13: Limited Dependent Variables
</p>
<p>(a) The conditional density f(x/x &gt; 1) = 2φ(x &minus; 1) for x &gt; 1 and f(x/x &lt; 1) = 2φ(x &minus; 1) for
x &lt; 1.
</p>
<p>(b) The conditional mean E(x/x &gt; 1) = 1 + 2φ(0) and E(x/x &lt; 1) = 1&minus; 2φ(0). Compare with
the unconditional mean of x.
</p>
<p>(c) The conditional variance var(x/x &gt; 1) = var(x/x &lt; 1) = 1 &minus; 4φ2(0). Compare with the
unconditional variance of x.
</p>
<p>7. Censored Normal Distribution. This is based on Greene (1993, pp. 692&ndash;693). Let y&lowast; be N(μ, σ2)
and define y = y&lowast; if y&lowast; &gt; c and y = c if y&lowast; &lt; c for some constant c.
</p>
<p>(a) Verify the E(y) expression given in (A.7).
</p>
<p>(b) Derive the var(y) expression given in (A.8). Hint: Use the fact that
</p>
<p>var(y) = E(conditional variance) + var(conditional mean)
</p>
<p>and the formulas given in the Appendix for conditional and unconditional means of a trun-
cated normal random variable.
</p>
<p>(c) For the special case of c = 0, show that (A.7) simplifies to E(y) = Φ(μ/σ)
</p>
<p>[
μ+
</p>
<p>σφ(μ/σ)
</p>
<p>Φ(μ/σ)
</p>
<p>]
</p>
<p>and (A.8) simplifies to
</p>
<p>var(y) = σ2Φ
(μ
σ
</p>
<p>)[
1&minus; δ
</p>
<p>(&minus;μ
σ
</p>
<p>)
+
</p>
<p>(
&minus;μ
σ
&minus; φ(/σ)
</p>
<p>Φ(μ/σ)
</p>
<p>)2
Φ
(
&minus;μ
σ
</p>
<p>)]
</p>
<p>where δ
</p>
<p>(&minus;μ
σ
</p>
<p>)
=
</p>
<p>φ(μ/σ)
</p>
<p>Φ(μ/σ)
</p>
<p>[
φ(μ/σ)
</p>
<p>Φ(μ/σ)
+
</p>
<p>μ
</p>
<p>σ
</p>
<p>]
. Similar expressions can be derived for censoring
</p>
<p>of the upper part rather than the lower part of the distribution.
</p>
<p>8. Fixed and Adjustable Rate Mortgages. Dhillon, Shilling and Sirmans (1987) considered the eco-
nomic decision of choosing between fixed and adjustable rate mortgages. The data consisted of 78
households borrowing money from a Louisiana mortgage banker. Of these, 46 selected fixed rate
mortgages and 32 selected uncapped adjustable rate mortgages. This data set can be downloaded
from the Springer web site and is labelled DHILLON.ASC. It was obtained from Lott and Ray
(1992). These variables include:
</p>
<p>Y = 0 if adjustable rate and 1 if fixed rate.
BA = Age of the borrower.
BS = Years of schooling for the borrower.
NW = Net worth of the borrower.
FI = Fixed interest rate.
PTS = Ratio of points paid on adjustable to fixed rate mortgages.
MAT = Ratio of maturities on adjustable to fixed rate mortgages.
MOB = Years at the present address.
MC = 1 if the borrower is married and 0 otherwise.
FTB = 1 if the borrower is a first-time home buyer and 0 otherwise.
SE = 1 if the borrower is self-employed and 0 otherwise.
YLD = The difference between the 10-year treasury rate less the 1-year treasury rate.
MARG = The margin on the adjustable rate mortgage.
CB = 1 if there is a co-borrower and 0 otherwise.
STL = Short-term liabilities.
LA = Liquid assets.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 365
</p>
<p>The probability of choosing a variable rate mortgage is a function of personal borrower charac-
teristics as well as mortgage characteristics. The efficient market hypothesis state that only cost
variables and not personal borrower characteristics influence the borrower&rsquo;s decision between fixed
and adjustable rate mortgages. Cost variables include FI, MARG, YLD, PTS and MAT. The rest
of the variables are personal characteristics variables. The principal agent theory suggests that in-
formation is asymmetric between lender and borrower. Therefore, one implication of this theory is
that the personal characteristics of the borrower will be significant in the choice of mortgage loan.
</p>
<p>(a) Run OLS of Y on all variables in the data set. For this linear probability model what does
the F -statistic for the significance of all slope coefficients yield? What is the R2? How many
predictions are less than zero or larger than one?
</p>
<p>(b) Using only the cost variables in the restricted regression, test that personal characteristics
are jointly insignificant. Hint: Use the Chow-F statistic. Do you find support for the efficient
market hypothesis?
</p>
<p>(c) Run the above model using the logit specification. Test the efficient market hypothesis. Does
your conclusion change from that in part (b)? Hint: Use the likelihood ratio test or the
BRMR.
</p>
<p>(d) Do part (c) using the probit specification.
</p>
<p>9. Sampling Distribution of OLS Under a Logit Model. This is based on Baltagi (2000).
</p>
<p>Consider a simple logit regression model
</p>
<p>yt = Λ(βxt) + ut
</p>
<p>for t = 1, 2, where Λ(z) = ez/(1 + ez) for &minus;&infin; &lt; z &lt; &infin;. Let β = 1, x1 = 1, x2 = 2 and assume
that the ut&rsquo;s are independent with mean zero.
</p>
<p>(a) Derive the sampling distribution of the least squares estimator of β, i.e., assuming a linear
probability model when the true model is a logit model.
</p>
<p>(b) Derive the sampling distribution of the least squares residuals and verify the estimated vari-
</p>
<p>ance of β̂OLS is biased.
</p>
<p>10. Sample Selection and Non-response. This is based on Manski (1995), see the Appendix to this
chapter. Suppose we are interested in estimating the probability than an individual who is home-
less at a given date has a home six months later. Let y = 1 if the individual has a home six months
later and y = 0 if the individual remains homeless. Let x be the sex of the individual and let z = 1
if the individual was located and interviewed and zero otherwise. 100 men and 31 women were
initially interviewed. Six months later, only 64 men and 14 women were located and interviewed.
Of the 64 men, 21 exited homelessness. Of the 14 women only 3 exited homelessness.
</p>
<p>(a) Compute Pr[y = 1/Male, z = 1], Pr[z = 1/Male] and the bound on Pr[y = 1/Male].
</p>
<p>(b) Compute Pr[y = 1/Female, z = 1], Pr[z = 1/Female] and the bound on Pr[y = 1/Female].
</p>
<p>(c) Show that the width of the bound is equal to the probability of attrition. Which bound is
tighter? Why?
</p>
<p>11. Does the Link Matter? This is based on Santos Silva (1999). Consider a binary random variable
Yi such that
</p>
<p>P (Yi = 1|x) = F (β0 + β1xi), i = 1, . . . , n,
</p>
<p>where the link F (&middot;) is a continuous distribution function.</p>
<p/>
</div>
<div class="page"><p/>
<p>366 Chapter 13: Limited Dependent Variables
</p>
<p>(a) Write down the log-likelihood function and the first-order conditions of maximization with
respect to β0 and β1.
</p>
<p>(b) Consider the case where xi only assumes two different values, without loss of generality, let
</p>
<p>it be 0 and 1. Show that F̂ (1) =
&sum;
</p>
<p>xi=1
yi/n1, where n1 is the number of observations for
</p>
<p>which xi = 1. Also, show that F̂ (0) =
&sum;
</p>
<p>xi=0
yi/(n&minus; n1).
</p>
<p>(c) What are the maximum likelihood estimates of β0 and β1?
</p>
<p>(d) Show that the value of the log-likelihood function evaluated at the maximum likelihood
estimates of β0 and β1 is the same, independent of the form of the link function.
</p>
<p>12. Beer Taxes and Motor Vehicle Fatality. Ruhm (1996) considered the effect of beer taxes and a
variety of alcohol-control policies on motor vehicle fatality rates, see section 13.4. The data is
for 48 states (excluding Alaska, Hawaii and the District of Columbia) over the period 1982&ndash;1988.
This data set can be downloaded from the Stock and Watson (2003) web site at www.aw.com/
stock watson. Using this data set replicate the results in Table 13.1.
</p>
<p>13. Problem Drinking and Employment. Mullahy and Sindelar (1996) considered the effect of problem
drinking on employment and unemployment. The data set is based on the 1988 Alcohol Sup-
plement of the National Health Interview Survey. This can be downloaded from the Journal of
Applied Econometrics web site at http://qed.econ.queensu.ca/jae/2002-v17.4/terza/.
</p>
<p>(a) Replicate the probit results in Table 13.6 and run also the logit and OLS regressions with ro-
bust White standard errors. The OLS results should match those given in Table 5 of Mullahy
and Sindelar (1996).
</p>
<p>(b) Compute marginal effects as reported in Table 13.7 and average marginal effects as reported
in Table 13.8. Compute the classifications of actual vs predicted as reported in Table 13.9.
Repeat these calculations for OLS and logit.
</p>
<p>(c) Mullahy and Sindelar (1996) performed similar regressions for females and for the dependent
variable taking the value of 1 if the individual is unemployed and zero otherwise. Replicate
the OLS results in Tables 5 and 6 of Mullahy and Sindelar (1996) and perform the corre-
sponding logit and probit regressions. Repeat part (b) for the female data set. What is your
conclusion on the relationship between problem drinking and unemployment?
</p>
<p>14. Fractional Response. Papke and Wooldridge (1996) studied the effect of match rates on partic-
ipation rates in 401(K) pension plans. The data are from the 1987 IRS Form 5500 reports of
pension plans with more than 100 participants. This data set can be downloaded from the Journal
of Applied Econometrics web site at http://qed.econ.queensu.ca/jae/1996-V11.6/papke.
</p>
<p>(a) Replicate Tables I and II of Papke and Wooldridge (1996).
</p>
<p>(b) Run the specification tests (RESET) described in Papke and Wooldridge (1996).
</p>
<p>(c) Compare OLS and logit QMLE using R2, specification tests, and predictions for various
values of MRATE as done in Figure 1 of Papke and Wooldridge (1996, p. 630).
</p>
<p>15. Fertility and Female Labor Supply. Carrasco (2001) estimated a probit equation for fertility using
PSID data over the period 1986-1989, see Table 13.10. The sample consists of 1,442 married or
cohabiting women between the ages of 18 and 55 in 1986. The data set can be obtained from the
Journal of Business &amp; Economic Statistics archive data web site.
</p>
<p>(a) Replicate Table 4, columns 1 and 2, of Carrasco (2001, p. 391). Show that having children of
the same sex has a significant and positive effect on the probability of having an additional
child.
</p>
<p>(b) Compute the predicted probabilities from these regression and the percentage of correct
decisions. Also compute the marginal effects.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 367
</p>
<p>(c) Replicate Table 5, columns 1 and 4, of Carrasco (2001, p. 392) which run a female labor force
participation equation using OLS and probit. Compute the predicted probabilities from the
probit regression and the percentage of correct decisions. Also compute the marginal effects.
</p>
<p>(d) Replicate Table 5, column 5, of Carrasco (2001, p. 392) which runs 2sls on the female labor
force participation equation using as instruments the same sex variables and their interactions
with ags26l. Compute the over-identification test for this 2sls.
</p>
<p>(e) Replicate Table 7, column 4, of Carrasco (2001, p. 393) which runs fixed effects on the fe-
male labor force participation equation with robust standard errors. Run also fixed effects
2sls using as instruments the same sex variables and their interactions with ags26l.
</p>
<p>16. Multinomial Logit. Terza (2002) ran a multinomial logit model on the Mullahy and Sindelar (1996)
data set for problem drinking described in problem 13, by explicitly accounting for the multinomial
classification of the dependent variable. In particular, y = 1 when the individual is out of the labor
force, y = 2 when this individual is unemployed, and y = 3 when this individual is employed.
</p>
<p>(a) Replicate the multinomial logit estimates for the male data reported in Table II of Terza
(2002, p. 399) columns 3,4, 9 and 10.
</p>
<p>(b) Obtain the multinomial logit estimates for the female data. How does your conclusion on the
relationship between problem drinking and employment/unemployment change from that in
problem 13?
</p>
<p>17. Tobit Estimation of Married Women Labor Supply. Wooldridge (2009, p. 593) estimated a Tobit
equation for the Mroz (1987) data considered in problem 11.31. Using the PSID for 1975, Mroz&rsquo;s
sample consists of 753 married white women between the ages of 30 and 60 in 1975, with 428 work-
ing at some time during the year. The wife&rsquo;s annual hours of work (hours) is regressed on nonwife
income (nwifeinc); the wife&rsquo;s age (age), her years of schooling (educ), the number of children less
than six years old in the household (kidslt6 ), and the number of children between the ages of five
and nineteen (kidsge6 ). The data set was obtained from Wooldridge&rsquo;s (2009) data web site.
</p>
<p>(a) Give a detailed summary of hours of work and determine the extent of skewness and kurtosis.
</p>
<p>(b) Run OLS and Tobit estimation as described above and replicate Table 17.2 of Wooldridge
(2009, p. 593).
</p>
<p>(c) Using the variable in the labor force (inlf ), run OLS, logit and probit using the same ex-
planatory variables given above and replicate Table 17.1 of Wooldridge (2009, p. 585).
</p>
<p>Give the predicted classification and the marginal effects at the mean as well as the average
marginal effects for these three specifications.
</p>
<p>(d) Compare the estimates of (β/σ2) from the probit in part (c) with the Tobit estimates given
in part (b).
</p>
<p>18. Heckit Estimation of Married Women&rsquo;s Earnings. Wooldridge (2009, p. 611) estimated a log wage
equation for the Mroz (1987) data considered in problem 13.7. The wife&rsquo;s log wage (lwage) is
regressed on her years of schooling (educ), her experience (exper) and its square (expersq). The
probit equation to correct for sample selection includes the regressors plus the following additional
variables: the number of children less than six years old in the household (kidslt6 ), the number of
children between the ages of five and nineteen (kidsge6 ), nonwife income (nwifeinc) and the wife&rsquo;s
age (age).
</p>
<p>(a) Run OLS and Heckit estimation as described above and replicate Table 17.5 of Wooldridge
(2009, p. 611).
</p>
<p>(b) Test that the inverse Mills ratio is not significant. What do you conclude?
</p>
<p>(c) Run the MLE of this Heckman (1976) sample selection model.</p>
<p/>
</div>
<div class="page"><p/>
<p>368 Chapter 13: Limited Dependent Variables
</p>
<p>References
</p>
<p>This chapter is based on the material in Hanushek and Jackson (1977), Maddala (1983), David-
son and MacKinnon (1993) and Greene (1993). Additional references include:
</p>
<p>Amemiya, T. (1981), &ldquo;Qualitative Response Models: A Survey,&rdquo; Journal of Economic Literature, 19:
1481&ndash;1536.
</p>
<p>Amemiya, T. (1984), &ldquo;Tobit Models: A Survey,&rdquo; Journal of Econometrics, 24: 3&ndash;61.
</p>
<p>Baltagi, B.H. (2000), &ldquo;Sampling Distribution of OLS Under a Logit Model,&rdquo; Problem 00.3.1, Econometric
Theory, 16: 451.
</p>
<p>Bera, A.K., C. Jarque and L.F. Lee (1984), &ldquo;Testing the Normality Assumption in Limited Dependent
Variable Models,&rdquo; International Economic Review, 25: 563&ndash;578.
</p>
<p>Berkson, J. (1953), &ldquo;A Statistically Precise and Relatively Simple Method of Estimating the Bio-Assay
with Quantal Response, Based on the Logistic Function,&rdquo; Journal of the American Statistical
Association, 48: 565&ndash;599.
</p>
<p>Berndt, E., B. Hall, R. Hall and J. Hausman (1974), &ldquo;Estimation and Inference in Nonlinear Structural
Models,&rdquo; Annals of Economic and Social Measurement, 3/4: 653&ndash;665.
</p>
<p>Boskin, M. (1974), &ldquo;A Conditional Logit Model of Occupational Choice,&rdquo; Journal of Political Economy,
82: 389&ndash;398.
</p>
<p>Carrasco, R. (2001), &ldquo;Binary Choice with Binary Endogenous Regressors in Panel Data: Estimating the
Effect Fertility on Female Labor Participation,&rdquo; Journal of Business &amp; Economic Statistics, 19:
385&ndash;394.
</p>
<p>Cornwell, C. and P. Rupert (1988), &ldquo;Efficient Estimation with Panel Data: An Empirical Comparison of
Instrumental Variables Estimators,&rdquo; Journal of Applied Econometrics, 3: 149&ndash;155.
</p>
<p>Cox, D.R. (1970), The Analysis of Binary Data (Chapman and Hall: London).
</p>
<p>Cragg, J. and R. Uhler (1970), &ldquo;The Demand for Automobiles,&rdquo; Canadian Journal of Economics, 3:
386&ndash;406.
</p>
<p>Davidson, R. and J. MacKinnon (1984), &ldquo;Convenient Specification Tests for Logit and Probit Models,&rdquo;
Journal of Econometrics, 25: 241&ndash;262.
</p>
<p>Dhillon, U.S., J.D. Shilling and C.F. Sirmans (1987), &ldquo;Choosing Between Fixed and Adjustable Rate
Mortgages,&rdquo; Journal of Money, Credit and Banking, 19: 260&ndash;267.
</p>
<p>Effron, B. (1978), &ldquo;Regression and ANOVA with Zero-One Data: Measures of Residual Variation,&rdquo;
Journal of the American Statistical Association, 73: 113&ndash;121.
</p>
<p>Goldberger, A. (1964), Econometric Theory (Wiley: New York).
</p>
<p>Gourieroux, C., A. Monfort and A. Trognon (1984), &ldquo;Pseudo-Maximum Likelihood Methods: Theory,&rdquo;
Econometrica, 52: 681&ndash;700.
</p>
<p>Hanushek, E.A. and J.E. Jackson (1977), Statistical Methods for Social Scientists (Academic Press: New
York).
</p>
<p>Hausman, J. and D. McFadden (1984), &ldquo;A Specification Test for Multinomial Logit Model,&rdquo; Economet-
rica, 52: 1219&ndash;1240.
</p>
<p>Hausman, J.A. and D.A. Wise (1977), &ldquo;Social Experimentation, Truncated Distributions, and Efficient
Estimation,&rdquo; Econometrica, 45: 919&ndash;938.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 369
</p>
<p>Heckman, J. (1976), &ldquo;The Common Structure of Statistical Models of Truncation, Sample Selection, and
Limited Dependent Variables and a Simple Estimator for Such Models,&rdquo; Annals of Economic and
Social Measurement, 5: 475&ndash;492.
</p>
<p>Heckman, J. (1979), &ldquo;Sample Selection Bias as a Specification Error,&rdquo; Econometrica, 47: 153&ndash;161.
</p>
<p>Lee, L.F. (2001), &ldquo;Self-Selection,&rdquo; Chapter 18 in B.H. Baltagi (ed.) A Companion to Theoretical Econo-
metrics (Blackwell: Massachusetts).
</p>
<p>Lee, L.F. and G.S. Maddala (1985), &ldquo;The Common Structure of Tests for Selectivity Bias, Serial Correla-
tion, Heteroskedasticity and Non-Normality in the Tobit Model,&rdquo; International Economic Review,
26: 1&ndash;20.
</p>
<p>Lott, W.F. and S.C. Ray (1992), Applied Econometrics: Problems with Data Sets (The Dryden Press:
New York).
</p>
<p>Maddala, G. (1983), Limited Dependent and Qualitative Variables in Econometrics (Cambridge Univer-
sity Press: Cambridge).
</p>
<p>Manski, C.F. (1995), Identification Problems in the Social Sciences (Harvard University Press: Cam-
bridge).
</p>
<p>McFadden, D. (1974), &ldquo;The Measurement of Urban Travel Demand,&rdquo; Journal of Public Economics, 3:
303&ndash;328.
</p>
<p>McCullagh, P. and J.A. Nelder (1989), Generalized Linear Models (Chapman and Hall: New York).
</p>
<p>Mroz, T.A. (1987), &ldquo;The Sensitivity of an Empirical Model of Married Women&rsquo;s Hours of Work to
Economic and Statistical Assumptions,&rdquo; Econometrica, 55: 765&ndash;799.
</p>
<p>Mullahy, J. and J. Sindelar (1996), &ldquo;Employment, Unemployment, and Problem Drinking,&rdquo; Journal of
Health Economics, 15: 409&ndash;434.
</p>
<p>Pagan, A.R. and F. Vella (1980), &ldquo;Diagnostic Tests for Models Based on Individual Data: A Survey,&rdquo;
Journal of Applied Econometrics, 4: S29-S59.
</p>
<p>Papke, L.E. and J.M. Wooldridge (1996), &ldquo;Econometric Methods for Fractional Response Variables with
An Application to 401(K) Plan Participation Rates,&rdquo;Journal of Applied Econometrics, 11: 619&ndash;
632.
</p>
<p>Powell, J. (1984), &ldquo;Least Absolute Deviations Estimation of the Censored Regression Model,&rdquo; Journal
of Econometrics, 25: 303&ndash;325.
</p>
<p>Powell, J. (1986), &ldquo;Symmetrically Trimmed Least Squares Estimation for Tobit Models,&rdquo; Econometrica,
54: 1435&ndash;1460.
</p>
<p>Pratt, J.W. (1981), &ldquo;Concavity of the Log-Likelihood,&rdquo; Journal of the American Statistical Association,
76: 103&ndash;109.
</p>
<p>Ruhm, C.J. (1996), &ldquo;Alcohol Policies and Highway Vehicle Fatalities,&rdquo; Journal of Health Economics, 15:
435&ndash;454.
</p>
<p>Schmidt, P. and R. Strauss (1975), &ldquo;Estimation of Models With Jointly Dependent Qualitative Variables:
A Simultaneous Logit Approach,&rdquo; Econometrica, 43: 745&ndash;755.
</p>
<p>Terza, J. (2002), &ldquo;Alcohol Abuse and Employment: A Second Look,&rdquo; Journal of Applied Econometrics,
17: 393&ndash;404.
</p>
<p>Wooldridge, J.M. (1991), &ldquo;Specification Testing and Quasi-Maximum Likelihood Estimation,&rdquo; Journal
</p>
<p>of Econometrics, 48: 29&ndash;55.</p>
<p/>
</div>
<div class="page"><p/>
<p>370 Chapter 13: Limited Dependent Variables
</p>
<p>Appendix
</p>
<p>1. Truncated Normal Distribution
</p>
<p>Let x be N(μ, σ2), then for a constant c, the truncated density is given by
</p>
<p>f(x/x &gt; c) =
f(x)
</p>
<p>Pr[x &gt; c]
=
</p>
<p>1
</p>
<p>σ
φ([x&minus; μ]/σ)
</p>
<p>1&minus; Φ
(
c&minus; μ
σ
</p>
<p>) c &lt; x &lt; &infin; (A.1)
</p>
<p>where φ(z) denotes the p.d.f. and Φ denotes the c.d.f. of a N(0, 1) random variable. If the
truncation is from above
</p>
<p>f(x/x &lt; c) =
f(x)
</p>
<p>Pr[x &lt; c]
=
</p>
<p>1
</p>
<p>σ
φ([x&minus; μ]/σ)
</p>
<p>Φ
</p>
<p>(
c&minus; μ
σ
</p>
<p>) &minus;&infin; &lt; x &lt; c (A.2)
</p>
<p>The conditional means are given by
</p>
<p>E(x/x &gt; c) = μ+ σ
φ(c&lowast;)
</p>
<p>1&minus; Φ(c&lowast;) (A.3)
</p>
<p>where c&lowast; =
c&minus; μ
σ
</p>
<p>, and
</p>
<p>E(x/x &lt; c) = μ&minus; σ φ(c
&lowast;)
</p>
<p>Φ(c&lowast;)
(A.4)
</p>
<p>In other words, the truncated mean shifts to the right (left) if truncation is from below (above).
The conditional variances are given by σ2(1&minus; δ(c&lowast;)) with 0 &lt; δ(c&lowast;) &lt; 1 for all values of c&lowast;.
</p>
<p>δ(c&lowast;) =
φ(c&lowast;)
</p>
<p>1&minus; Φ(c&lowast;)
</p>
<p>[
φ(c&lowast;)
</p>
<p>1&minus; Φ(c&lowast;) &minus; c
&lowast;
]
</p>
<p>for x &gt; c (A.5)
</p>
<p>=
&minus;φ(c&lowast;)
Φ(c&lowast;)
</p>
<p>[&minus;φ(c&lowast;)
Φ(c&lowast;)
</p>
<p>&minus; c&lowast;
]
</p>
<p>for x &lt; c (A.6)
</p>
<p>In other words, the truncated variance is always less than the unconditional or untruncated
variance. For more details, see Maddala (1983, p. 365) or Greene (1993, p. 685).
</p>
<p>2. The Censored Normal Distribution
</p>
<p>Let y&lowast; be N(μ, σ2), then for a constant c, define y = y&lowast; if y&lowast; &gt; c and y = c if y&lowast; &lt; c. Unlike the
truncated normal density, the censored density assigns the entire probability of the censored
region to the censoring point, i.e., y = c. So that Pr[y = c] = Pr[y&lowast; &lt; c] = Φ((c&minus;μ)/σ) = Φ(c&lowast;)
where c&lowast; = (c &minus; μ)/σ. For the uncensored region the probability of y&lowast; remains the same and
can be obtained from the normal density.
It is easy to show, see Greene (1993, p. 692) that
</p>
<p>E(y) = Pr[y = c]E(y/y = c) + Pr[y &gt; c]E(y/y &gt; c) (A.7)
</p>
<p>= cΦ(c&lowast;) + (1&minus; Φ(c&lowast;))E(y&lowast;/y&lowast; &gt; c)
</p>
<p>= cΦ(c&lowast;) + (1&minus; Φ(c&lowast;))
[
μ+ σ
</p>
<p>φ(c&lowast;)
1&minus; Φ(c&lowast;)
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 371
</p>
<p>where E(y&lowast;/y&lowast; &gt; c) is obtained from the mean of a truncated normal density, see (A.3).
Similarly, one can show, see problem 7 or Greene (1993, p. 693) that
</p>
<p>var(y) = σ2 [1&minus; Φ(c&lowast;)]
[
1&minus; δ(c&lowast;) +
</p>
<p>(
c&lowast; &minus; φ(c
</p>
<p>&lowast;)
1&minus; Φ(c&lowast;)
</p>
<p>)2
Φ(c&lowast;)
</p>
<p>]
(A.8)
</p>
<p>where δ(c&lowast;) was defined in (A.5).
</p>
<p>3. Sample Selection and Non-response
</p>
<p>Non-response is a big problem plaguing survey data. Some individuals refuse to respond and
some do not answer all the questions, especially on relevant economic variables like income.
Suppose we interviewed randomly 150 individuals upon their graduation from high school.
Among these, 50 were female and 100 were male. A year later, we try to re-interview these
individuals to find out whether they are employed or not. Only 70 out of 100 males and 40
out of 50 females were located and interviewed a year later. Out of those re-interviewed, 60
males and 20 females were found to be employed. Let y = 1 if the individual is employed and
zero if not. Let x be the sex of this individual and let z = 1 if this individual is located and
interviewed a year later and zero otherwise.
Conditioning on sex of the respondent one can compute the probability of being employed a
</p>
<p>year after high school graduation as follows:
</p>
<p>Pr[y = 1/x] = Pr[y = 1/x, z = 1] Pr[z = 1/x] + Pr[y = 1/x, z = 0] Pr[z = 0/x]
</p>
<p>In this case, Pr[y = 1/Male, z = 1] = 60/70, Pr[z = 1/Male] = 70/100 and Pr[z = 0/Male] =
30/100. But the sampling process is uninormative about the non-respondents or the censored
observations, i.e., Pr[y = 1/Male, z = 0]. Therefore, in the absence of other information
</p>
<p>Pr[y = 1/Male] = (0.6) + (0.3) Pr[y = 1/Male, z = 0]
</p>
<p>Manski (1995) argues that one can estimate bounds on this probability. In fact, replacing 0 &le;
Pr[y = 1/Male, z = 0] &le; 1 by its bounds, yields
</p>
<p>0.6 &le; Pr[y = 1/Male] &le; 0.9
</p>
<p>with the width of the bound equal to the probability of non-response conditioning on Males,
i.e., Pr[z = 0/Male] = 0.3. Similarly, 0.4 &le; Pr[y = 1/Female] &le; 0.6 with the width of the bound
equal to the probability of non-response conditioning on Females, i.e., Pr[z = 0/Female] =
10/50 = 0.2. Manski (1995) argues that these bounds are informative and should be the starting
point of empirical analysis. Researchers assuming that non-response is ignorable or exogenous
are imposing the following restriction
</p>
<p>Pr[y = 1/Male, z = 1] = Pr[y = 1/Male, z = 0] = Pr[y = 1/Male] = 60/70
Pr[y = 1/Female, z = 1] = Pr[y = 1/Female, z = 0] = Pr[y = 1/Female] = 20/40
</p>
<p>To the extent that these probabilities are different casts doubt on the ignorable non-response
assumption.</p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 14
</p>
<p>Time-Series Analysis
</p>
<p>14.1 Introduction
</p>
<p>There has been an enormous amount of research in time-series econometrics, and many eco-
nomics departments have required a time-series econometrics course in their graduate sequence.
Obviously, one chapter on this topic will not do it justice. Therefore, this chapter will focus on
some of the basic concepts needed for such a course. Section 14.2 defines what is meant by a
stationary time-series, while sections 14.3 and 14.4 briefly review the Box-Jenkins and Vector
Autoregression (VAR) methods for time-series analysis. Section 14.5 considers a random walk
model and various tests for the existence of a unit root. Section 14.6 studies spurious regressions
and trend stationary versus difference stationary models. Section 14.7 gives a simple explanation
of the concept of cointegration and illustrates it with an economic example. Finally, section 14.8
looks at Autoregressive Conditionally Heteroskedastic (ARCH) time-series.
</p>
<p>14.2 Stationarity
</p>
<p>Figure 14.1 plots the consumption and personal disposable income data considered in Chapter
5. This was done using EViews. This is annual data from 1959 to 2007 expressed in real terms.
Both series seem to be trending upwards over time. This may be an indication that these
time-series are non-stationary. Having a time-series xt that is trending upwards over time may
invalidate all the standard asymptotic theory we have been relying upon in this book. In fact,&sum;T
</p>
<p>t=1 x
2
t /T may not tend to a finite limit as T &rarr; &infin; and using regressors such as xt means that
</p>
<p>X &prime;X/T does not tend in probability limits to a finite positive definite matrix, see problem 6.
</p>
<p>8,000
</p>
<p>12,000
</p>
<p>16,000
</p>
<p>20,000
</p>
<p>24,000
</p>
<p>28,000
</p>
<p>32,000
</p>
<p>60 65 70 75 80 85 90 95 00 05
</p>
<p>C Y
</p>
<p>Figure 14.1 U.S. Consumption and Income, 1959&ndash;2007
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5_14, 373</p>
<p/>
</div>
<div class="page"><p/>
<p>374 Chapter 14: Time-Series Analysis
</p>
<p>Non-standard asymptotic theory will have to be applied which is beyond the scope of this book,
see problem 8.
</p>
<p>Definition: A time-series process xt is said to be covariance stationary (or weakly stationary)
if its mean and variance are constant and independent of time and the covariances given by
cov(xt, xt&minus;s) = γs depend only upon the distance between the two time periods, but not the
time periods per se.
</p>
<p>In order to check the time-series for weak stationarity one can compute its autocorrelation
function. This is given by ρs= correlation (xt, xt&minus;s) = γs/γo. These are correlation coefficients
taking values between &minus;1 and +1.
</p>
<p>The sample counterparts of the variance and covariances are given by
</p>
<p>γ̂o =
&sum;T
</p>
<p>t=1(xt &minus; x̄)2/T
</p>
<p>γ̂s =
&sum;T&minus;s
</p>
<p>t=1 (xt &minus; x̄)(xt+s &minus; x̄)/T
and the sample autocorrelation function is given by ρ̂s = γ̂s/γ̂o. Figure 14.2 plots ρ̂s against
s for the consumption series. This is called the sample correlogram. For a stationary process,
ρs declines sharply as the number of lags s increase. This is not necessarily the case for a
nonstationary series. In the next section, we briefly review a popular method for the analysis
of time-series known as the Box and Jenkins (1970) technique. This method utilizes the sample
autocorrelation function to establish whether a series is stationary or not.
</p>
<p>Sample: 1959 2007
Included observations: 49
</p>
<p>Autocorrelation Partial Correlation AC PAC Q-Stat Prob
</p>
<p>&middot; | ⋆⋆⋆⋆⋆⋆⋆ &middot; | ⋆⋆⋆⋆⋆⋆⋆ 1 0.935 0.935 45.500 0.000
&middot; | ⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 2 0.868 &ndash;0.050 85.527 0.000
&middot; | ⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 3 0.800 &ndash;0.042 120.26 0.000
&middot; | ⋆⋆⋆⋆⋆ | &middot; | &middot; | 4 0.733 &ndash;0.029 150.08 0.000
&middot; | ⋆⋆⋆⋆⋆ | &middot; | &middot; | 5 0.668 &ndash;0.024 175.39 0.000
&middot; | ⋆⋆⋆⋆ | &middot; | &middot; | 6 0.604 &ndash;0.030 196.57 0.000
&middot; | ⋆⋆⋆⋆ | &middot; | &middot; | 7 0.541 &ndash;0.029 214.00 0.000
&middot; | ⋆⋆⋆ | &middot; | &middot; | 8 0.480 &ndash;0.033 228.02 0.000
&middot; | ⋆⋆⋆ | &middot; | &middot; | 9 0.421 &ndash;0.015 239.12 0.000
&middot; | ⋆⋆⋆ | &middot; | &middot; | 10 0.369 0.004 247.83 0.000
&middot; | ⋆⋆ | &middot; | &middot; | 11 0.320 &ndash;0.009 254.57 0.000
&middot; | ⋆⋆ | &middot; | &middot; | 12 0.272 &ndash;0.033 259.57 0.000
&middot; | ⋆⋆ | &middot; | &middot; | 13 0.226 &ndash;0.027 263.10 0.000
&middot; | ⋆&middot; | &middot; | &middot; | 14 0.181 &ndash;0.021 265.45 0.000
&middot; | ⋆&middot; | &middot; | &middot; | 15 0.140 &ndash;0.013 266.88 0.000
&middot; | ⋆&middot; | &middot; | &middot; | 16 0.097 &ndash;0.052 267.60 0.000
&middot; | &middot; | &middot; | &middot; | 17 0.055 &ndash;0.036 267.83 0.000
&middot; | &middot; | &middot; | &middot; | 18 0.011 &ndash;0.052 267.84 0.000
&middot; | &middot; | &middot; | &middot; | 19 &ndash;0.032 &ndash;0.034 267.93 0.000
&middot;⋆| &middot; | &middot; | &middot; | 20 &ndash;0.073 &ndash;0.026 268.39 0.000
</p>
<p>Figure 14.2 Correlogram of Consumption</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 The Box and Jenkins Method 375
</p>
<p>14.3 The Box and Jenkins Method
</p>
<p>This method fits Autoregressive Integrated Moving Average (ARIMA) type models. We have
already considered simple AR and MA type models in Chapters 5 and 6. The Box-Jenkins
methodology differences the series and looks at the sample correlogram of the differenced series
to see whether stationarity is achieved. As will be clear shortly, if we have to difference the
series once, twice or three times to make it stationary, this series is integrated of order 1, 2
or 3, respectively. Next, the Box-Jenkins method looks at the autocorrelation function and
the partial autocorrelation function (synonymous with partial correlation coefficients) of the
resulting stationary series to identify the order of the AR and MA process that is needed. The
partial correlation between yt and yt&minus;s is the correlation between those two variables holding
constant the effect of all intervening lags, see Box and Jenkins (1970) for details. Figure 14.3
plots an AR(1) process of size T = 250 generated as yt = 0.7yt&minus;1+ǫt with ǫt &sim; IIN(0, 4). Figure
14.4 shows that the correlogram of this AR(1) process declines geometrically as s increases.
Similarly, Figure 14.5 plots an MA(1) process of size T = 250 generated as yt = ǫt + 0.4ǫt&minus;1
with ǫt &sim; IIN(0, 4). Figure 14.6 shows that the correlogram of this MA(1) process is zero after
the first lag, see also problems 1 and 2 for further analysis. Identifying the right ARIMA model
is not an exact science, but potential candidates emerge. These models are estimated using
maximum likelihood techniques. Next, these models are subjected to some diagnostic checks.
One commonly used check is to see whether the residuals are White noise. If they fail this test,
these models are dropped from the list of viable candidates.
</p>
<p>50
</p>
<p>10
</p>
<p>5
</p>
<p>0
</p>
<p>5
</p>
<p>&ndash;10
</p>
<p>&ndash;15
</p>
<p>100 150 200 250
</p>
<p>Figure 14.3 AR(1) Process, ρ = 0.7
</p>
<p>If the time-series is White noise, i.e., purely random with constant mean and variance and zero
autocorrelation, then ρs = 0 for s &gt; 0. In fact, for a White noise series, if T &rarr; &infin;,
</p>
<p>&radic;
T ρ̂s will be
</p>
<p>asymptotically distributed N(0, 1). A joint test for Ho; ρs = 0 for s = 1, 2, . . . ,m lags, is given</p>
<p/>
</div>
<div class="page"><p/>
<p>376 Chapter 14: Time-Series Analysis
</p>
<p>Sample: 1 250
Included observations: 250
</p>
<p>Autocorrelation Partial Correlation AC PAC Q-Stat Prob
</p>
<p>&middot; | ⋆⋆⋆⋆⋆⋆ | &middot; | ⋆⋆⋆⋆⋆⋆ | 1 0.725 0.725 132.99 0.000
&middot; | ⋆⋆⋆⋆ | &middot; | &middot; | 2 0.503 &ndash;0.048 197.27 0.000
&middot; | ⋆⋆⋆ | &middot; | &middot; | 3 0.330 &ndash;0.037 225.05 0.000
&middot; | ⋆⋆ | &middot; | &middot; | 4 0.206 &ndash;0.016 235.92 0.000
&middot; | ⋆ | &middot; | &middot; | 5 0.115 &ndash;0.022 239.33 0.000
&middot; | &middot; | &middot; | &middot; | 6 0.036 &ndash;0.050 239.67 0.000
&middot; | &middot; | &middot; | &middot; | 7 &ndash;0.007 0.004 239.68 0.000
&middot; | &middot; | &middot; | &middot; | 8 &ndash;0.003 0.050 239.68 0.000
&middot; | &middot; | &middot; | &middot; | 9 &ndash;0.017 &ndash;0.041 239.75 0.000
⋆ | &middot; | ⋆ | &middot; | 10 &ndash;0.060 &ndash;0.083 240.71 0.000
⋆ | &middot; | ⋆ | &middot; | 11 &ndash;0.110 &ndash;0.063 243.91 0.000
&middot; | &middot; | &middot; | ⋆ | 12 &ndash;0.040 0.191 244.32 0.000
</p>
<p>Figure 14.4 Correlogram of AR(1)
</p>
<p>50
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>0
</p>
<p>&ndash;6
</p>
<p>&ndash;8
</p>
<p>100 150 200 250
</p>
<p>&ndash;4
</p>
<p>&ndash;2
</p>
<p>Figure 14.5 MA(1) Process, θ = 0.4
</p>
<p>by the Box and Pierce (1970) statistic
</p>
<p>Q = T
&sum;m
</p>
<p>s=1 ρ̂
2
s (14.1)
</p>
<p>This is asymptotically distributed under the null as χ2m. A refinement of the Box-Pierce Q-
statistic that performs better, i.e., have more power in small samples is the Ljung and Box
(1978) QLB statistic</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 The Box and Jenkins Method 377
</p>
<p>Sample: 1 250
Included observations: 250
</p>
<p>Autocorrelation Partial Correlation AC PAC Q-Stat Prob
</p>
<p>&middot; | ⋆⋆⋆ | &middot; | ⋆⋆⋆ | 1 0.399 0.399 40.240 0.000
&middot; | &middot; | ⋆ | &middot; | 2 0.033 &ndash;0.150 40.520 0.000
&middot; | &middot; | &middot; | ⋆ | 3 0.010 0.066 40.545 0.000
&middot; | &middot; | &middot; | &middot; | 4 &ndash;0.002 &ndash;0.033 40.547 0.000
&middot; | ⋆ | &middot; | ⋆ | 5 0.090 0.127 42.624 0.000
&middot; | &middot; | &middot; | &middot; | 6 0.055 &ndash;0.045 43.417 0.000
&middot; | &middot; | &middot; | &middot; | 7 0.028 0.042 43.625 0.000
&middot; | &middot; | ⋆ | &middot; | 8 &ndash;0.031 &ndash;0.075 43.881 0.000
&middot; | &middot; | &middot; | &middot; | 9 &ndash;0.034 0.023 44.190 0.000
&middot; | &middot; | &middot; | &middot; | 10 &ndash;0.027 &ndash;0.045 44.374 0.000
&middot; | &middot; | &middot; | &middot; | 11 &ndash;0.013 0.020 44.421 0.000
&middot; | ⋆ | &middot; | ⋆ | 12 0.082 0.086 46.190 0.000
</p>
<p>Figure 14.6 Correlogram of MA(1)
</p>
<p>QLB = T (T + 2)
&sum;m
</p>
<p>j=1 ρ̂
2
j/(T &minus; j) (14.2)
</p>
<p>This is also distributed asymptotically as χ2m under the null hypothesis. Maddala (1992, p. 540)
warns about the inappropriateness of the Q and QLB statistics for autoregressive models. The
arguments against their use are the same as those for not using the Durbin-Watson statistic in
autoregressive models. Maddala (1992) suggests the use of LM statistics of the type proposed
by Godfrey (1979) to evaluate the adequacies of the ARMA model proposed.
For the consumption series, T = 49 and the 95% confidence interval for ρ̂s is 0&plusmn;1.96 (1/
</p>
<p>&radic;
49)
</p>
<p>which is &plusmn;0.28. Figure 14.2 plots this 95% confidence interval as two solid lines around zero. It
is clear that the sample correlogram declines slowly as the number of lags s increase. Moreover,
the QLB statistics which are reported for lags 1, 2, up to 13 are all statistically significant. These
were computed using EViews. Based on the sample correlogram and the Ljung-Box statistic, the
consumption series is not purely random white noise. Figure 14.7 plots the sample correlogram
for ΔCt = Ct &minus; Ct&minus;1. Note that this sample correlogram dies out abruptly after the first lag.
Also, the QLB statistics are not significant after the first lag. This indicates stationarity of the
first-differenced consumption series. Problem 3 asks the reader to plot the sample correlogram
for personal disposable income and its first difference, and to compute the Ljung-Box QLB
statistic to test for purely White noise based on 13 lags.
A difficult question when modeling economic behavior is to decide on what lags should be in
</p>
<p>the ARIMA model, or the dynamic regression model. Granger et al. (1995) argue that there are
disadvantages in using hypothesis testing to help make model specification decisions based on
the data. They recommend instead the use of model selection criteria to make those decisions.
The Box-Jenkins methodology has been popular primarily among forecasters who claimed
</p>
<p>better performance than simultaneous equations models based upon economic theory. Box-
Jenkins models are general enough to allow for nonstationarity and can handle seasonality.
However, the Box-Jenkins models suffer from the fact that they are devoid of economic theory
and as such they are not designed to test economic hypothesis, or provide estimates of key
elasticity parameters. As a consequence, this method cannot be used for simulating the effects</p>
<p/>
</div>
<div class="page"><p/>
<p>378 Chapter 14: Time-Series Analysis
</p>
<p>Sample: 1959 2007
Included observations: 48
</p>
<p>Autocorrelation Partial Correlation AC PAC Q-Stat Prob
</p>
<p>&middot; | ⋆⋆⋆⋆ &middot; | ⋆⋆⋆⋆ 1 0.465 0.465 11.059 0.001
&middot; | &middot; ⋆⋆| &middot; 2 0.065 &ndash;0.194 11.277 0.004
&middot;⋆| &middot; &middot;⋆| &middot; 3 &ndash;0.129 &ndash;0.099 12.160 0.007
&middot; | &middot; &middot; | ⋆&middot; 4 &ndash;0.048 0.101 12.288 0.015
&middot; | &middot; &middot; | &middot; 5 &ndash;0.012 &ndash;0.053 12.296 0.031
&middot; | &middot; &middot; | &middot; 6 0.015 0.015 12.309 0.055
&middot; | &middot; &middot; | &middot; 7 &ndash;0.016 &ndash;0.027 12.324 0.090
&middot;⋆| &middot; &middot;⋆| &middot; 8 &ndash;0.115 &ndash;0.133 13.123 0.108
&middot;⋆| &middot; &middot; | &middot; 9 &ndash;0.081 0.056 13.528 0.140
&middot; | ⋆&middot; &middot; | ⋆&middot; 10 0.124 0.194 14.501 0.151
&middot; | ⋆&middot; &middot; | &middot; 11 0.194 0.001 16.944 0.110
&middot; | ⋆&middot; &middot;⋆| &middot; 12 0.098 &ndash;0.027 17.589 0.129
&middot; | &middot; &middot; | ⋆&middot; 13 0.063 0.120 17.861 0.163
&middot; | &middot; &middot; | &middot; 14 0.034 &ndash;0.017 17.941 0.209
&middot; | &middot; &middot; | &middot; 15 0.027 0.016 17.994 0.263
&middot; | &middot; &middot; | &middot; 16 0.028 0.034 18.051 0.321
&middot; | &middot; &middot; | &middot; 17 0.026 &ndash;0.026 18.105 0.382
&middot;⋆| &middot; ⋆⋆| &middot; 18 &ndash;0.152 &ndash;0.193 19.959 0.335
&middot; | &middot; &middot; | ⋆⋆ 19 &ndash;0.029 0.279 20.027 0.393
&middot; | ⋆&middot; &middot; | &middot; 20 0.101 0.016 20.902 0.403
</p>
<p>Figure 14.7 Correlogram of First Difference of Consumption
</p>
<p>of a tax change or a Federal Reserve policy change. One lesson that economists learned from
the Box-Jenkins methodology is that they have to take a hard look at the time-series properties
of their variables and properly specify the dynamics of their economic models. Another popular
forecasting technique in economics is the Vector Autoregression (VAR) methodology proposed
by Sims (1980). This will be briefly discussed next.
</p>
<p>14.4 Vector Autoregression
</p>
<p>Sims (1980) criticized the simultaneous equation literature for the ad hoc restrictions needed
for identification and for the ad hoc classification of exogenous and endogenous variables in the
system, see Chapter 11. Instead, Sims (1980) suggested Vector Autoregression (VAR) models for
forecasting macro time-series. VAR assumes that all the variables are endogenous. For example,
consider the following three macro-series: money supply, interest rate, and output. VAR models
this vector of three endogenous variables as an autoregressive function of their lagged values.
VAR models can include some exogenous variables like trends and seasonal dummies, but the
whole point is that it does not have to classify variables as endogenous or exogenous. If we
allow 5 lags on each endogenous variable, each equation will have 16 parameters to estimate
if we include a constant. For example, the money supply equation will be a function of 5 lags
on money, 5 lags on the interest rate and 5 lags on output. Since the parameters are different
for each equation the total number of parameters in this unrestricted VAR is 3 &times; 16 = 48</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Unit Roots 379
</p>
<p>parameters. This degrees of freedom problem becomes more serious as the number of lags m
and number of equations g increase. In fact, the number of parameters to be estimated becomes
g +mg2. With small samples, individual parameters may not be estimated precisely. So, only
simple VAR models, can be considered for a short sample. Since this system of equations has
the same set of variables in each equation SUR on the system is equivalent to OLS on each
equation, see Chapter 10. Under normality of the disturbances, MLE as well as Likelihood Ratio
tests can be performed. One important application of LR tests in the context of VAR is its use
in determining the choice of lags to be used. In this case, one obtains the log-likelihood for the
restricted model with m lags and the unrestricted model with q &gt; m lags. This LR test will be
asymptotically distributed as χ2(q&minus;m)g2 . Once again, the sample size T should be large enough
</p>
<p>to estimate the large number of parameters (qg2 + g) for the unrestricted model.
One can of course impose restrictions to reduce the number of parameters to be estimated,
</p>
<p>but this reintroduces the problem of ad hoc restrictions which VAR was supposed to cure in
the first place. Bayesian VAR procedures claim success with forecasting, see Litterman (1986),
but again these models have been criticized because they are devoid of economic theory.
</p>
<p>VAR models have also been used to test the hypothesis that some variables do not Granger
cause some other variables.1 For a two-equation VAR, as long as this VAR is correctly specified
and no variables are omitted, one can test, for example, that y1 does not Granger cause y2. This
hypothesis cannot be rejected if all the m lagged values of y1 are insignificant in the equation
for y2. This is a simple F -test for the joint significance of the lagged coefficients of y1 in the
y2 equation. This is asymptotically distributed as Fm,T&minus;(2m+1). The problem with the Granger
test for non-causality is that it may be sensitive to the number of lags m, see Gujarati (1995).
For an extensive analysis of nonstationary VAR models as well as testing and estimation of
cointegrating relationships in VAR models, see Hamilton (1994) and Lütkepohl (2001).
</p>
<p>14.5 Unit Roots
</p>
<p>If xt = xt&minus;1 + ut where ut is IID(0, σ2), then xt is a random walk. Some stock market analysts
believe that stock prices follow a random walk, i.e., the price of a stock today is equal to its
price yesterday plus a random shock. This is a nonstationary time-series. Any shock to the price
of this stock is permanent and does not die out like an AR(1) process. In fact, if the initial price
of the stock is xo = μ, then
</p>
<p>x1 = μ+ u1, x2 = μ+ u1 + u2, . . . , and xt = μ+
&sum;t
</p>
<p>j=1 uj
</p>
<p>with E(xt) = μ and var(xt) = tσ
2 since u &sim; IID(0, σ2). Therefore, the variance of xt is dependent
</p>
<p>on t and xt is not covariance-stationary. In fact, as t &rarr; &infin;, so does var(xt). However, first
differencing xt we get ut which is stationary. Figure 14.8 plots the graph of a random walk
of size T = 250 generated as xt = xt&minus;1 + ǫt with ǫt &sim; IIN(0, 4). Figure 14.9 shows that the
autocorrelation function of this random walk process is persistent as s increases. Note that a
random walk is an AR(1) model xt = ρxt&minus;1+ut with ρ = 1. Therefore, a test for nonstationarity
is a test for ρ = 1 or a test for a unit root.
Using the lag operator L we can write the random walk as (1&minus;L)xt = ut and in general, any
</p>
<p>autoregressive model in xt can be written as A(L)xt = ut where A(L) is a polynomial in L. If
A(L) has (1&minus; L) as one of its roots, then xt has a unit root.</p>
<p/>
</div>
<div class="page"><p/>
<p>380 Chapter 14: Time-Series Analysis
</p>
<p>50
</p>
<p>60
</p>
<p>40
</p>
<p>20
</p>
<p>0
</p>
<p>&ndash;20
</p>
<p>100 150 200 250
</p>
<p>Figure 14.8 Random Walk Process
</p>
<p>Subtracting xt&minus;1 from both sides of the AR(1) model we get
</p>
<p>Δxt = (ρ&minus; 1)xt&minus;1 + ut = δxt&minus;1 + ut (14.3)
where δ = ρ &minus; 1 and Δxt = xt &minus; xt&minus;1 is the first-difference of xt. A test for Ho; ρ = 1 can be
obtained by regressing Δxt on xt&minus;1 and testing that Ho; δ = 0. Since ut is stationary then if
δ = 0, Δxt = ut and xt is difference stationary, i.e., it becomes stationary after differencing it
once. In this case, the original undifferenced series xt is said to be integrated of order 1 or I(1).
If we need to difference xt twice to make it stationary, then xt is I(2). A stationary process
is by definition I(0). Dickey and Fuller (1979) showed that the usual regression t-statistic for
Ho; δ = 0 from (14.3) does not have a t-distribution under Ho. In fact, this t-statistic has a
non-standard distribution, see Bierens (2001) for a simple derivation of these results. Dickey and
Fuller tabulated the critical values of the t-statistic = (ρ̂ &minus; 1)/s.e.(ρ̂) = δ̂/s.e.(δ̂) using Monte
Carlo experiments. These tables have been extended by MacKinnon (1991). If |t| exceeds the
critical values, we reject Ho that ρ = 1 which also means that we do not reject the hypothesis
of stationarity of the time-series. Non-rejection of Ho; ρ = 1 means that we do not reject the
presence of a unit root and hence the nonstationarity of the time-series. Note that non-rejection
of Ho may also be a non-rejection of ρ = 0.99. More formally stated, a weakness of unit root
tests in general is that they have low power discriminating between a unit root process and
a borderline stationary process. In practice, the Dickey-Fuller test has been applied in the
following three forms:
</p>
<p>Δxt = δxt&minus;1 + ut (14.4)
</p>
<p>Δxt = α+ δxt&minus;1 + ut (14.5)
</p>
<p>Δxt = α+ βt+ δxt&minus;1 + ut (14.6)
</p>
<p>where t is a time trend. The null hypothesis of the existence of a unit root is Ho; δ = 0. This is
the same for (14.4), (14.5) and (14.6), but the critical values for the corresponding t-statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Unit Roots 381
</p>
<p>Sample: 1 250
Included observations: 250
</p>
<p>Autocorrelation Partial Correlation AC PAC Q-Stat Prob
</p>
<p>&middot; | ⋆⋆⋆⋆⋆⋆⋆⋆ | &middot; | ⋆⋆⋆⋆⋆⋆⋆⋆ | 1 0.980 0.980 242.76 0.000
&middot; | ⋆⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 2 0.959 &ndash;0.003 476.56 0.000
&middot; | ⋆⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 3 0.940 0.004 701.83 0.000
&middot; | ⋆⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 4 0.920 &ndash;0.013 918.61 0.000
&middot; | ⋆⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 5 0.899 &ndash;0.044 1126.4 0.000
&middot; | ⋆⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 6 0.876 &ndash;0.053 1324.6 0.000
&middot; | ⋆⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 7 0.855 0.028 1514.2 0.000
&middot; | ⋆⋆⋆⋆⋆⋆ | &middot; | ⋆ | 8 0.837 0.067 1696.7 0.000
&middot; | ⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 9 0.821 0.032 1872.8 0.000
&middot; | ⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 10 0.804 &ndash;0.006 2042.6 0.000
&middot; | ⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 11 0.788 &ndash;0.007 2206.3 0.000
&middot; | ⋆⋆⋆⋆⋆⋆ | &middot; | &middot; | 12 0.774 0.030 2364.8 0.000
</p>
<p>Figure 14.9 Correlogram of a Random Walk Process
</p>
<p>are different in each case. Standard time-series software like EViews give the proper critical
values for the Dickey-Fuller statistic. For alternative unit root tests, see Phillips and Perron
(1988) and Bierens and Guo (1993). In practice, one should run (14.6) if the series is trended
with drift and (14.5) if it is trended without drift. Not including a constant or trend as in (14.4)
is unlikely for economic data. The Box-Jenkins approach differences the series and looks at
the sample correlogram of the differenced series. The Dickey-Fuller test is a more formal test
for the existence of a unit root. Maddala (1992, p. 585) warns the reader to perform both the
visual inspection and the unit root test before deciding on whether the time-series process is
nonstationary.
If the disturbance term ut follows a stationary AR(1) process, then the augmented Dickey-
</p>
<p>Fuller test runs the following modified version of (14.6) by including one additional regressor,
Δxt&minus;1:
</p>
<p>Δxt = α+ βt+ δxt&minus;1 + λΔxt&minus;1 + ǫt (14.7)
</p>
<p>In this case, the t-statistic for δ = 0 is a unit root test allowing for first-order serial correlation.
This augmented Dickey-Fuller test in (14.7) has the same asymptotic distribution as the corre-
sponding Dickey-Fuller test in (14.6) and the same critical values can be used. Similarly, if ut
follows a stationary AR(p) process, this amounts to adding p extra regressors in (14.6) consist-
ing of Δxt&minus;1,Δxt&minus;2, . . . ,Δxt&minus;p and testing that the coefficient of xt&minus;1 is zero. In practice, one
does not know the process generating the serial correlation in ut and the general practice is to
include as many lags of Δxt as is necessary to render the ǫt term in (14.7) serially uncorrelated.
More lags may be needed if the disturbance term contains Moving Average terms, since a MA
term can be thought of as an infinite autoregressive process, see Ng and Perron (1995) for an
extensive Monte Carlo on the selection of the truncation lag. Two other important complica-
tions when doing unit root tests are: (i) structural breaks in the time-series, like the oil embargo
of 1973, tend to bias the standard unit root tests against rejecting the null hypothesis of a unit
root, see Perron (1989). (ii) Seasonally adjusted data also tend to bias the standard unit root
tests against rejecting the null hypothesis of a unit root, see Ghysels and Perron (1992). For</p>
<p/>
</div>
<div class="page"><p/>
<p>382 Chapter 14: Time-Series Analysis
</p>
<p>this reason, Davidson and MacKinnon (1993, p. 714) suggest using seasonally unadjusted data
whenever available.
For the trended consumption series with drift, the Augmented Dickey-Fuller test using EViews
</p>
<p>yields the following regression:
</p>
<p>ΔCt = 665.60
</p>
<p>(1.80)
</p>
<p>+ 30.57
</p>
<p>(1.60)
</p>
<p>t&minus; 0.072
(1.42)
</p>
<p>Ct&minus;1+ 0.449
(3.17)
</p>
<p>ΔCt&minus;1 +residuals
(14.8)
</p>
<p>where the numbers in parentheses are the usually reported t-statistics. The null hypothesis
is that the coefficient of Ct&minus;1 in this regression is zero. Table 14.1 gives the Dickey-Fuller t-
statistic (&minus;1.42) and the corresponding 5% critical value (&minus;3.508) tabulated by MacKinnon
(1996). Note that @TREND(1959) is the time-trend starting at 1959. The Schwarz criterion
found the optimal number of lags of ΔCt&minus;1 to be included in this regression is one. Since the
p-value is 0.84, we do not reject the null hypothesis of the existence of a unit root. We conclude
that Ct is nonstationary. This confirms our finding from the sample correlogram of Ct given in
Figure 14.2.
</p>
<p>Table 14.1 Dickey-Fuller Test
</p>
<p>Null Hypothesis: CONSUMP has a unit root
Exogenous: Constant, Linear Trend
Lag Length: 1 (Automatic based on SIC, MAXLAG=10)
</p>
<p>t-Statistic Prob.⋆
</p>
<p>Augmented Dickey-Fuller test statistic &ndash;1.418937 0.8424
</p>
<p>Test critical values: 1% level &ndash;4.165756
5% level &ndash;3.508508
10% level &ndash;3.184230
</p>
<p>⋆ MacKinnon (1996) one-sided p-values.
</p>
<p>Augmented Dickey-Fuller Test Equation
</p>
<p>Dependent Variable: D(CONSUMP)
Method: Least Squares
Sample (adjusted): 1961 2007
Included observations: 47 after adjustments
</p>
<p>Coefficient Std. Error t-Statistic Prob.
</p>
<p>CONSUMP(&ndash;1) &ndash;0.072427 0.051043 &ndash;1.418937 0.1631
D(CONSUMP(&ndash;1)) 0.448898 0.141516 3.172064 0.0028
C 665.6031 370.5970 1.796030 0.0795
@TREND(1959) 30.56963 19.13925 1.597221 0.1175
</p>
<p>R-squared 0.291770 Mean dependent var 393.2340
Adjusted R-squared 0.242359 S.D. dependent var 254.9362
S.E. of regression 221.9031 Akaike info criterion 13.72362
Sum squared resid 2117363. Schwarz criterion 13.88108
Log likelihood &ndash;318.5052 Hannah-Quinn criter. 13.78288
F-statistic 5.904911 Durbin-Watson stat 1.841198
Prob(F-statistic) 0.001816</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 Trend Stationary Versus Difference Stationary 383
</p>
<p>One can check whether the first-differenced series is stationary by performing a unit root test
on the first-differenced model. Let C̃t = ΔCt, then run the following regression:
</p>
<p>ΔC̃t = 213.77
</p>
<p>(3.59)
</p>
<p>&minus; 0.533
(4.13)
</p>
<p>C̃t&minus;1 + residuals
(14.9)
</p>
<p>the coefficient of C̃t&minus;1 has a t-statistic of &minus;4.13 which is smaller than the 5% critical value of
&minus;2.925. In other words, we reject the null hypothesis of unit root for the first-differenced series
ΔCt. The same conclusion would have been reached if a linear trend was included besides the
constant. We conclude that Ct is I(1).
So far, all tests for unit root have the hypothesis of nonstationarity as the null with the
</p>
<p>alternative being that the series is stationary. Two unit roots tests with stationarity as the null
and nonstationarity as the alternative are given by Kwaitowski et al. (1992) and Leybourne and
McCabe (1994). The first test known as KPSS is an analog of the Phillips-Perron test whereas
the Leybourne-McCabe test is an analog of the augmented Dickey-Fuller test. Reversing the null
may lead to confirmation of stationarity or nonstationarity or may yield conflicting decisions.
</p>
<p>14.6 Trend Stationary Versus Difference Stationary
</p>
<p>Many macroeconomic time-series that are trending upwards have been characterized as either
</p>
<p>Trend Stationary: xt = α+ βt+ ut (14.10)
</p>
<p>or
</p>
<p>Difference Stationary: xt = γ + xt&minus;1 + ut (14.11)
</p>
<p>where ut is stationary. The first model (14.10) says that the macro-series is stationary except for
a deterministic trend. E(xt) = α+βt which varies with t. In contrast, the second model (14.11)
says that the macro-series is a random walk with drift. The drift parameter γ in (14.11) plays
the same role as the β parameter in (14.10), since both cause xt to trend upwards over time.
Model (14.10) is consistent with economists introducing a time trend in the regression. This
has the same effect as detrending each variable in the regression rendering it stationary, see the
Frisch-Waugh-Lovell Theorem in Chapter 7. This detrending is valid only if model (14.10) is
true for every series in the regression. Model (14.11) on the other hand, requires differencing to
obtain a stationary series. Detrending and differencing are two completely different remedies.
What is valid for one model is not valid for the other. The choice between (14.10) and (14.11) is
based on a test for the existence of a unit root. Essential reading on these two models are Nelson
and Plosser (1982) and Stock and Watson (1988). Nelson and Plosser applied the Dickey-Fuller
test to a wide range of historical macro time-series for the U.S. economy and found that all of
these series were difference stationary, with the exception of the unemployment rate. Plosser and
Schwert (1978) argued that for most economic macro time-series, it is best to difference the data
rather than work with levels. The reasoning is that if these series are difference stationary and
we run regressions in levels, the usual properties of our estimators as well as the distributions of
the associated test statistics are invalid. On the other hand, if the true model is a regression in
levels with the data series being trend stationary, differencing the model will produce a Moving</p>
<p/>
</div>
<div class="page"><p/>
<p>384 Chapter 14: Time-Series Analysis
</p>
<p>Average error term and at worst, ignoring it will lead to loss in efficiency. It is important to
emphasize, that for nonstationary variables, the standard asymptotic theory does not apply, see
problems 6 and 7, and that t and F -statistics obtained from regressions using these variables
may have non-standard distributions, see Durlauf and Phillips (1988).
Granger and Newbold (1974) demonstrated some of the problems associated with regressing
</p>
<p>nonstationary time-series on each other. In fact, they showed that if xt and yt are independent
random walks, then one should expect to find no evidence of a relationship when one regresses
yt on xt. In other words, the estimate of β in the regression yt = α+βxt+ut should be near zero
and its associated t-statistic insignifiant. In fact, for a sample size of 50 and a 100 replications,
Granger and Newbold found |t| &le; 2 on only 23 occasions, 2 &lt; |t| &le; 4 on 24 occasions, and |t| &gt; 4
on 53 occasions. Granger and Newbold (1974) called this phenomenon spurious regression since
it finds a significant relationship between the two time-series when none exists. Hence, one
should be cautious when running time-series regressions involving unit root processes. High R2
</p>
<p>and significant t-statistics from OLS regressions may be hiding nonsense results. Phillips (1986)
studied the asymptotic properties of the least squares spurious regression model and confirmed
these simulation results. In fact, Phillips showed that the t-statistic for Ho; β = 0 converges
in probability to &infin; as T &rarr; &infin;. This means that the t-statistic will reject Ho; β = 0 with
probability 1 as T &rarr; &infin;. If both xt and yt are independent trend stationary series generated as
described in (14.10), then the R2 of the regression of yt on xt will tend to one as T &rarr; &infin;, see
Davidson and MacKinnon (1993, p. 671). For a summary of several extensions of these results,
see Granger (2001).
</p>
<p>14.7 Cointegration
</p>
<p>Let us continue with our consumption-income example. In Chapter 5, we regressed Ct on Yt
and obtained
</p>
<p>Ct =&minus;1343.31
(219.56)
</p>
<p>+ 0.979
</p>
<p>(0.011)
</p>
<p>Yt + residuals
(14.12)
</p>
<p>with R2 = 0.994 and D.W. = 0.18. We have shown that Ct and Yt are nonstationary series and
that both are I(1), see also problem 3. The regression in (14.12) could be a spurious regression
owing to the fact that we regressed a nonstationary series on another. This invalidates the t
and F -statistics of regression (14.12). Since both Ct and Yt are integrated of the same order,
and Figure 14.1 shows that they are trending upwards together, this random walk may be in
unison. This is the idea behind cointegration. Ct and Yt are cointegrated if there exists a linear
combination of Ct and Yt that yields a stationary series. More formally, if Ct and Yt are both
I(1) but there exist a linear combination Ct &minus; α &minus; βYt = ut which is I(0), then Ct and Yt are
cointegrated and β is the cointegrating parameter. This idea can be extended to a vector of more
than two time-series. This vector is cointegrated if the components of this vector have a unit
root and there exists a linear combination of this vector that is stationary. Such a cointegrat-
ing relationship can be interpreted as a stable long-run relationship between the components
of this time-series vector. Economic examples of long-run relationship include the quantity
theory of money, purchasing power parity and the permanent income theory of consumption.
The important point to emphasize here is that differencing these nonstationary time-series de-
stroys potential valuable information about the long-run relationship between these economic</p>
<p/>
</div>
<div class="page"><p/>
<p>14.7 Cointegration 385
</p>
<p>variables. The theory of cointegration tries to estimate this long-run relationship using the non-
stationary series themselves, rather than their first differences. In order to explain this, we state
(without proof) one of the implications of the Granger Representation Theorem, namely, that
a set of cointegrated variables will have an Error-Correction Model (ECM) representation. Let
us illustrate with an example.
</p>
<p>A Cointegration Example
</p>
<p>This is based on Engle and Granger (1987). Assume that Ct and Yt for t = 1, 2, . . . , T are I(1)
processes generated as follows:
</p>
<p>Ct &minus; βYt = ut with ut = ρut&minus;1 + ǫt and |ρ| &lt; 1 (14.13)
Ct &minus; αYt = νt with νt = νt&minus;1 + ηt and α 	= β (14.14)
</p>
<p>In other words, ut follows a stationary AR(1) process, while νt follows a random walk. Suppose
</p>
<p>that
</p>
<p>(
ǫt
ηt
</p>
<p>)
are independent bivariate normal random variables with mean zero and variance
</p>
<p>Σ = [σij ] for i, j = 1, 2. First, we obtain the reduced form representation of Yt and Ct in terms
of ut and νt. This is given by
</p>
<p>Ct =
α
</p>
<p>(α&minus; β)ut +
β
</p>
<p>(α&minus; β)νt (14.15)
</p>
<p>Yt =
1
</p>
<p>(α&minus; β)ut &minus;
1
</p>
<p>(α&minus; β)νt (14.16)
</p>
<p>Since ut is I(0) and νt is I(1), we conclude from (14.15) and (14.16) that Ct and Yt are in
fact I(1) series. In terms of the usual order condition for identification considered in Chapter
11, the system of equations given by (14.13) and (14.14) are not identified because there are
no exclusion restrictions on either equation. However, if we take a linear combination of the
two structural equations given in (14.13) and (14.14), the disturbance of the resulting linear
combination is neither a stationary AR(1) process nor a random walk. Hence, both (14.13) and
(14.14) are identified. Note that if ρ = 1, then ut is a random walk and the linear combination
of ut and νt is also a random walk. In this case, neither (14.13) nor (14.14) are identified.
</p>
<p>In the Engle-Granger terminology, Ct&minus;βYt is the cointegrating relationship and (1,&minus;β) is the
cointegrating vector. This cointegrating relationship is unique. The proof is by contradiction.
Assume there is another cointegrating relationship Ct &minus; γYt that is I(0), then the difference
between the two cointegrating relationships yields (γ &minus; β)Yt. This is also I(0). This can only
happen for every value of Yt, which is I(1), if and only if β = γ.
Difference both equations in (14.13) and (14.14) and write both differenced equations as a
</p>
<p>system of two equations in (ΔCt,ΔYt)
&prime;, one gets:
</p>
<p>[
1 &minus;β
1 &minus;α
</p>
<p>] [
ΔCt
ΔYt
</p>
<p>]
=
</p>
<p>[
Δut
Δνt
</p>
<p>]
=
</p>
<p>[
ǫt + (ρ&minus; 1)Ct&minus;1 &minus; β(ρ&minus; 1)Yt&minus;1
</p>
<p>ηt
</p>
<p>]
(14.17)
</p>
<p>where the second equality is obtained by replacing Δνt by ηt, Δut by (ρ &minus; 1)ut&minus;1 + ǫt, and
substituting for ut&minus;1 its value (Ct&minus;1 &minus; βYt&minus;1). Post-multiplying (14.17) by the inverse of the</p>
<p/>
</div>
<div class="page"><p/>
<p>386 Chapter 14: Time-Series Analysis
</p>
<p>first matrix, one can show, see problem 9, that the resulting solution is the following VAR
model:
</p>
<p>[
ΔCt
ΔYt
</p>
<p>]
=
</p>
<p>1
</p>
<p>(β &minus; α)
</p>
<p>[
&minus;α(ρ&minus; 1) αβ(ρ&minus; 1)
&minus;(ρ&minus; 1) β(ρ&minus; 1)
</p>
<p>](
Ct&minus;1
Yt&minus;1
</p>
<p>)
+
</p>
<p>(
ht
gt
</p>
<p>)
(14.18)
</p>
<p>where ht and gt are linear combinations of ǫt and ηt. Note that if ρ = 1, then the level variables
Ct&minus;1 and Yt&minus;1 drop from the VAR equations. Let Zt = Ct&minus;βYt and define δ = (ρ&minus; 1)/(β&minus;α).
Then the VAR representation in (14.18) can be written as follows:
</p>
<p>ΔCt = &minus;αδZt&minus;1 + ht (14.19)
ΔYt = &minus;δZt&minus;1 + gt (14.20)
</p>
<p>This is the Error-Correction Model (ECM) representation of the original model. Zt&minus;1 is the
error correction term. It represents a disequilibrium term showing the departure from long-run
equilibrium, see section 6.4. Note that if ρ = 1, then δ = 0 and Zt&minus;1 drops from both ECM
equations. As Banerjee et al. (1993, p. 139) explain, this ECM representation is a noteworthy
&ldquo;...contribution to resolving, or synthesizing, the debate between time-series analysts and those
favoring econometric methods.&rdquo; The former considered only differenced time-series that can be
legitimately assumed stationary, while the latter focused on equilibrium relationships expressed
in levels. The former wiped out important long-run relationships by first differencing them,
while the latter ignored the spurious regression problem. In contrast, the ECM allows the use
of first differences and levels from the cointegrating relationship. For more details, see Banerjee
et al. (1993). A simple two-step procedure for estimating cointegrating relationships is given by
Engle and Granger (1987). In the first step, the OLS estimator of β is obtained by regressing Ct
on Yt. This can be shown to be superconsistent, i.e., plim T (β̂OLS &minus; β) &rarr; 0 as T &rarr; &infin;. Using
β̂OLS one obtains Ẑt = Ct &minus; β̂OLSYt. In the second step, using Ẑt&minus;1 rather than Zt&minus;1, apply
OLS to estimate the ECM in (14.19) and (14.20). Extensive Monte Carlo experiments have
been conducted by Banerjee et al. (1993) to investigate the bias of β in small samples. This is
pursued further in problem 9. An alternative estimation procedure is the maximum likelihood
approach suggested by Johansen (1988). This is beyond the scope of this book. See Dolado et
al. (2001) for a lucid summary of the cointegration literature.
A formal test for cointegration is given by Engle and Granger (1987) who suggest running
</p>
<p>regression (14.12) and testing that the residuals do not have a unit root. In other words, run a
Dickey-Fuller test or its augmented version on the resulting residuals from (14.12). In fact, if
Ct and Yt are not cointegrated, then any linear combination of them would be nonstationary
including the residuals of (14.12). Since these tests are based on residuals, their asymptotic dis-
tributions are not the same as those of the corresponding ordinary unit roots tests. Asymptotic
critical values for these tests can be found in Davidson and MacKinnon (1993, p. 722). For our
consumption regression the following Dickey-Fuller test is obtained on the residuals:
</p>
<p>Δût =&minus;1.111
(0.04)
</p>
<p>&minus; 0.094
(1.50)
</p>
<p>ût + residuals
(14.21)
</p>
<p>the Davidson and MacKinnon (1993) asymptotic 5% critical value for this t-statistic is &minus;2.92
and its p-value is 0.52. Therefore, we cannot reject the hypothesis that ût is nonstationary.
We have also included a trend and one lag of the first-differenced residuals. The resulting
augmented Dickey-Fuller test did not reject the existence of a unit root. Therefore, Ct and
Yt are not cointegrated. This suggests that the relationship estimated in (14.12) is spurious.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.8 Autoregressive Conditional Heteroskedasticity 387
</p>
<p>Table 14.2 Johansen Cointegration Test
</p>
<p>Sample (adjusted): 1961 2007
Included observations: 47 after adjustments
Trend assumption: Linear deterministic trend
Series: CONSUMP Y
Lags interval (in first differences): 1 to 1
</p>
<p>Unrestricted Cointegration Rank Test (Trace)
</p>
<p>Hypothesized Trace 0.05
No. of CE(s) Eigenvalue Statistic Critical Value Prob.⋆⋆
</p>
<p>None 0.120482 6.322057 15.49471 0.6575
At most 1 0.006112 0.288141 3.841466 0.5914
</p>
<p>Trace test indicates no cointegration at the 0.05 level
⋆ denotes rejection of the hypothesis at the 0.05 level
⋆⋆ MacKinnon-Haug-Michelis (1999) p-values
</p>
<p>Unrestricted Cointegration Rank Test (Maximum Eigenvalue)
</p>
<p>Hypothesized Max-Eigen 0.05
No. of CE(s) Eigenvalue Statistic Critical Value Prob.⋆⋆
</p>
<p>None 0.120482 6.033916 14.26460 0.6089
At most 1 0.006112 0.288141 3.841466 0.5914
</p>
<p>Max-eigenvalue test indicates no cointegration at the 0.05 level
⋆ denotes rejection of the hypothesis at the 0.05 level
⋆⋆ MacKinnon-Haug-Michelis (1999) p-values
</p>
<p>Regressing an I(1) series on another lead to spurious results unless they are cointegrated. Of
course, other I(1) series may have been erroneously excluded from (14.12) which when included
may result in a cointegrating relationship among the resulting variables. In other words, Ct
and Yt may not be cointegrated because of an omitted variables problem. Table 14.2 gives the
Johansen (1995) cointegration test reported by EViews which is beyond the scope of this book.
The null hypothesis is that of no cointegration or at most one cointegration relationship. Both
hypotheses are not rejected by the trace and maximum eigenvalue tests.
</p>
<p>14.8 Autoregressive Conditional Heteroskedasticity
</p>
<p>Financial time-series such as foreign exchange rates, inflation rates and stock prices may exhibit
some volatility which varies over time. In the case of inflation or foreign exchange rates this
could be due to changes in the Federal Reserve&rsquo;s policies. In the case of stock prices this could
be due to rumors about a certain company&rsquo;s merger or takeover. This suggests that the variance
of these time-series may be heteroskedastic. Engle (1982) modeled this heteroskedasticity by
relating the conditional variance of the disturbance term at time t to the size of the squared
disturbance terms in the recent past. A simple Autoregressive Conditionally Heteroskedastic</p>
<p/>
</div>
<div class="page"><p/>
<p>388 Chapter 14: Time-Series Analysis
</p>
<p>(ARCH) model is given by
</p>
<p>σ2t = E(u
2
t /ζt) = γo + γ1u
</p>
<p>2
t&minus;1 + ..+ γpu
</p>
<p>2
t&minus;p (14.22)
</p>
<p>where ζt denotes the information set upon which the variance of ut is to be conditioned. This
typically includes all the information available prior to period t. In (14.22), the variance of
ut conditional on the information prior to period t is an autoregressive function of order p in
squared lagged values of ut. This is called an ARCH(p) process. Since (14.22) is a variance, this
means that all the γi&rsquo;s for i = 0, 1, . . . , p have to be non-negative. Engle (1982) showed that a
simple test for homoskedasticity, i.e., Ho; γ1 = γ2 = .. = γp = 0, can be based upon an ordinary
F -test which regresses the squared OLS residuals (e2t ) on their lagged values (e
</p>
<p>2
t&minus;1, . . . , e
</p>
<p>2
t&minus;p)
</p>
<p>and a constant. The F -statistic tests the joint significance of the regressors and is reported
by most regression packages. Alternatively, one can compute T times the centered R2 of this
regression and this is distributed as χ2p under the null hypothesis Ho. This test resembles the
usual homoskedasticity tests studied in Chapter 5 except that the squared OLS residuals are
regressed upon their lagged values rather than some explanatory variables.
The simple ARCH(1) process
</p>
<p>σ2t = γo + γ1u
2
t&minus;1 (14.23)
</p>
<p>can be generated as follows: ut = [γo + γ1u
2
t&minus;1]
</p>
<p>1/2ǫt where ǫt &sim; IID(0, 1). Note that the sim-
plifying variance of unity for ǫt can be achieved by rescaling the parameters γo and γ1. In this
case, the conditional mean of ut is given by
</p>
<p>E(ut/ζt) = [γo + γ1u
2
t&minus;1]
</p>
<p>1/2E(ǫt/ζt) = 0
</p>
<p>since u2t&minus;1 is known at time t. Similarly, the conditional variance can be easily obtained from
</p>
<p>E(u2t /ζt) = [γo + γ1u
2
t&minus;1]E(ǫ
</p>
<p>2
t /ζt) = γo + γ1u
</p>
<p>2
t&minus;1
</p>
<p>since E(ǫ2t ) = 1. Also, the conditional covariances can be easily shown to be zero since
</p>
<p>E(utut&minus;s/ζt) = ut&minus;sE(ut/ζt) = 0 for s = 1, 2, . . . , t.
</p>
<p>The unconditional mean can be obtained by taking repeated conditional expectations period
by period until we reach the initial period, see the Appendix to Chapter 2. For example, taking
the conditional expectation of E(ut/ζt) based on information prior to period t&minus; 1, we get
</p>
<p>E[E(ut/ζt)/ζt&minus;1] = E(0/ζt&minus;1) = 0
</p>
<p>It is clear that all prior conditional expectations of zero will be zero so that E(ut) = 0. Similarly,
taking the conditional expectations of E(u2t /ζt) based on information prior to period t&minus; 1, we
get
</p>
<p>E[E(u2t /ζt)/ζt&minus;1] = γo + γ1E[u
2
t&minus;1/ζt&minus;1] = γo + γ1(γo + γ1u
</p>
<p>2
t&minus;2) = γo(1 + γ1) + γ
</p>
<p>2
1u
</p>
<p>2
t&minus;2
</p>
<p>By taking repeated conditional expectations one period at a time we finally get
</p>
<p>E(u2t ) = γo(1 + γ1 + γ
2
1 + ..+ γ
</p>
<p>t&minus;1
1 ) + γ
</p>
<p>t
1u
</p>
<p>2
o (14.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.8 Autoregressive Conditional Heteroskedasticity 389
</p>
<p>As t &rarr; &infin;, the unconditional variance of ut is given by σ2 = var(ut) = γo/(1&minus; γ1) for |γ1| &lt; 1
and γo &gt; 0. Therefore, the ARCH(1) process is homoskedastic.
ARCH models can be estimated using feasible GLS or maximum likelihood methods. Alterna-
</p>
<p>tively, one can use a double-length regression procedure suggested by Davidson and MacKinnon
(1993) to obtain (i) one-step efficient estimates starting from OLS estimates or (ii) the max-
imum likelihood estimates. Here we focus on the feasible GLS procedure suggested by Engle
(1982). For the regression model
</p>
<p>y = Xβ + u (14.25)
</p>
<p>where y is T &times; 1 and X is T &times; k. First, obtain the OLS estimates β̂OLS and the OLS residuals
e. Second, perform the following regression: e2t = ao + a1e
</p>
<p>2
t&minus;1+ residuals. This yields a test for
</p>
<p>homoskedasticity. Third, compute σ̂2t = ao + a1e
2
t&minus;1 and regress [(e
</p>
<p>2
t /σ̂t) &minus; 1] on (1/σ̂t) and
</p>
<p>(e2t&minus;1/σ̂t). Call the regression estimates da. One updates a
&prime; = (ao, a1) by computing â = a+ da.
</p>
<p>Fourth, recompute σ̂2t using the updated â from step 3, and form the set of regressors xtjrt for
j = 1, . . . , k, where
</p>
<p>rt =
</p>
<p>[
1
</p>
<p>σ̂t
+ 2
</p>
<p>(
â1et
σ̂t+1
</p>
<p>)2]1/2
(14.26)
</p>
<p>Finally, regress (etst/rt) where
</p>
<p>st =
1
</p>
<p>σ̂t
&minus; â1
</p>
<p>σ̂t+1
</p>
<p>(
e2t+1
σ̂t+1
</p>
<p>&minus; 1
)
</p>
<p>on xtjrt for j = 1, . . . , k and obtain the least squares coefficients dβ . Update the estimate of
</p>
<p>β by computing β̂ = β̂OLS + dβ . This procedure can run into problems if the σ̂
2
t are not all
</p>
<p>positive, see Judge et al. (1985) and Engle (1982) for details.
The ARCH model has been generalized by Bollerslev (1986). The Generalized ARCH
</p>
<p>(GARCH (p, q)) model can be written as
</p>
<p>σ2t = γo +
&sum;p
</p>
<p>i=1 γiu
2
t&minus;i +
</p>
<p>&sum;q
j=1 δjσ
</p>
<p>2
t&minus;j (14.27)
</p>
<p>In this case, the conditional variance of ut depends upon q of its lagged values as well as p
squared lagged values of ut. The simple GARCH (1, 1) model is given by
</p>
<p>σ2t = γo + γ1u
2
t&minus;1 + δ1σ
</p>
<p>2
t&minus;1 (14.28)
</p>
<p>An LM test for GARCH (p, q) turns out to be equivalent to testing ARCH (p+ q). This simply
regresses squared OLS residuals on (p + q) of its squared lagged values. The test statistic
is T times the uncentered R2 and is asymptotically distributed as χ2p+q under the null of
homoskedasticity.
In conclusion, a lot of basic concepts have been introduced in this chapter and we barely
</p>
<p>scratched the surface. Hopefully, this will motivate the reader to take the next econometrics
time series course.</p>
<p/>
</div>
<div class="page"><p/>
<p>390 Chapter 14: Time-Series Analysis
</p>
<p>Table 14.3 GARCH (1,1) model
</p>
<p>Dependent Variable: CONSUMP
Method: ML &ndash; ARCH (Marquardt) &ndash; Normal distribution
Sample: 1959 2007
Included observations: 49
Convergence achieved after 19 iterations
Presample variance: backcast (parameter = 0.7)
GARCH = C(3) + C(4)⋆RESID(&ndash;1)ˆ2 + C(5)⋆GARCH(&ndash;1)
</p>
<p>Coefficient Std. Error z-Statistic Prob.
</p>
<p>C &ndash;1435.888 226.3933 &ndash;6.342449 0.0000
Y 0.986813 0.011923 82.76452 0.0000
</p>
<p>Variance Equation
</p>
<p>C 118728.2 87402.35 1.358410 0.1743
RESID(&ndash;1)ˆ 2 1.068561 0.326091 3.276885 0.0010
GARCH(&ndash;1) &ndash;0.380949 0.187656 &ndash;2.030036 0.0424
</p>
<p>R-squared 0.993542 Mean dependent var 16749.10
Adjusted R-squared 0.992955 S.D. dependent var 5447.060
S.E. of regression 457.1938 Akaike info criterion 14.86890
Sum squared resid 9197153. Schwarz criterion 15.06194
Log likelihood &ndash;359.2880 Hannah-Quinn criter. 14.94214
F-statistic 1692.353 Durbin-Watson stat 0.178409
Prob(F-statistic) 0.000000
</p>
<p>Note
</p>
<p>1. Granger causality has been developed by Granger (1969). For another definition of causality,
see Sims (1972). Also, Chamberlain (1982) for a discussion on when these two definitions are
</p>
<p>equivalent.
</p>
<p>Problems
</p>
<p>1. For the AR(1) model
</p>
<p>yt = ρyt&minus;1 + ǫt t = 1, 2, . . . , T ; with |ρ| &lt; 1 and ǫt &sim; IIN(0, σ2ǫ)
</p>
<p>(a) Show that if yo &sim; N(0, σ
2
ǫ/1&minus; ρ2), then E(yt) = 0 for all t and var(yt) = σ2ǫ/(1&minus; ρ2) so that
</p>
<p>the mean and variance are independent of t. Note that if ρ = 1 then var(yt) is &infin;. If |ρ| &gt; 1
then var(yt) is negative!
</p>
<p>(b) Show that cov(yt, yt&minus;s) = ρsσ2 which is only dependent on s, the distance between the two
time periods. Conclude from parts (a) and (b) that this AR(1) model is weakly stationary.
</p>
<p>(c) Generate the above AR(1) series for T = 250, σ2ǫ = 0.25 and various values of ρ = &plusmn;0.9, &plusmn;0.8,
&plusmn;0.5, &plusmn;0.3 and &plusmn;0.1. Plot the AR(1) series and the autocorrelation function ρs versus s.
</p>
<p>2. For the MA(1) model
</p>
<p>yt = ǫt + θǫt&minus;1 t = 1, 2, . . . , T ; with ǫt &sim; IIN(0, σ
2
ǫ)</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 391
</p>
<p>(a) Show that E(yt) = 0 and var(yt) = σ
2
ǫ(1+θ
</p>
<p>2) so that the mean and variance are independent
of t.
</p>
<p>(b) Show that cov(yt, yt&minus;1) = θσ2ǫ and cov(yt, yt&minus;s) = 0 for s &gt; 1 which is only dependent on s,
the distance between the two time periods. Conclude from parts (a) and (b) that this MA(1)
model is weakly stationary.
</p>
<p>(c) Generate the above MA(1) series for T = 250, σ2ǫ = 0.25 and various values of θ = 0.9, 0.8,
0.5, 0.3 and 0.1. Plot the MA(1) series and the autocorrelation function versus s.
</p>
<p>3. Using the consumption-personal disposable income data for the U.S. used in this chapter:
</p>
<p>(a) Compute the sample autocorrelation function for personal disposable income (Yt). Plot the
sample correlogram. Repeat for the first-differenced series (ΔYt). Compute the Ljung-Box
QLB statistic, test that Ho; ρs = 0 for s = 1, . . . , 20.
</p>
<p>(b) Run the Augmented Dickey-Fuller test for the existence of a unit root in personal disposable
income (Yt).
</p>
<p>(c) Define Ỹt = ΔYt and run ΔỸt on Ỹt&minus;1 and a constant and trend. Test that the first-differenced
series of personal disposable income is stationary. What do you conclude? Is Yt an I(1)
process?
</p>
<p>(d) Replicate the regression in (14.21) and verify the Engle-Granger (1987) test for cointegration.
</p>
<p>(e) Replicate the GARCH(1,1) model given in Table 14.3.
</p>
<p>(f) Repeat parts (a) through (e) using logC and logY . Are there any changes in the above
results?
</p>
<p>4. (a) Generate T = 25 observations on xt and yt as independent random walks with IIN(0, 1)
disturbances. Run the regression yt = α + βxt + ut and test the null hypothesis Ho; β = 0
using the usual t-statistic at the 1%, 5% and 10% levels. Repeat this experiment 1000 times
and report the frequency of rejections at each significance level. What do you conclude?
</p>
<p>(b) Repeat part (a) for T = 100 and T = 500.
</p>
<p>(c) Repeat parts (a) and (b) generating xt and yt as independent random walks with drift as
described in (14.11), using IIN(0, 1) disturbances. Let γ = 0.2 for both series.
</p>
<p>(d) Repeat parts (a) and (b) generating xt and yt as independent trend stationary series as
described in (14.10), using IIN(0, 1) disturbances. Let α = 1 and β = 0.04 for both series.
</p>
<p>(e) Report the frequency distributions of the R2 statistics obtained in parts (a) through (d) for
each sample size and method of generating the time-series. What do you conclude? Hint:
See the Monte Carlo experiments in Granger and Newbold (1974), Davidson and MacKinnon
(1993) and Banerjee, Dolado, Galbraith and Hendry (1993).
</p>
<p>5. For the Money Supply, GNP and interest rate series data for the U.S. given on the Springer web
site as MACRO.ASC, fit a VAR three equation model using:
</p>
<p>(a) Two lags on each variable.
</p>
<p>(b) Three lags on each variable.
</p>
<p>(c) Compute the Likelihood Ratio test for part (a) versus part (b).
</p>
<p>(d) For the two-equation VAR of Money Supply and interest rate with three lags on each variable,
test that the interest rate does not Granger cause the money supply?
</p>
<p>(e) How sensitive are the tests in part (d) if we had used only two lags on each variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>392 Chapter 14: Time-Series Analysis
</p>
<p>6. For the simple Deterministic Time Trend Model
</p>
<p>yt = α+ βt+ ut t = 1, .., T
</p>
<p>where ut &sim; IIN(0, σ
2).
</p>
<p>(a) Show that
</p>
<p>(
α̂OLS &minus; α
β̂OLS &minus; β
</p>
<p>)
= (X &prime;X)X &prime;u =
</p>
<p>[
T
</p>
<p>&sum;T
t=1 t&sum;T
</p>
<p>t=1 t
&sum;T
</p>
<p>t=1 t
2
</p>
<p>]&minus;1 [ &sum;T
t=1 ut&sum;T
t=1 tut
</p>
<p>]
</p>
<p>where the t-th observation of X, the matrix of regressors, is [1, t].
</p>
<p>(b) Use the results that
&sum;T
</p>
<p>t=1 t = T (T + 1)/2 and
&sum;T
</p>
<p>t=1 t
2 = T (T + 1)(2T + 1)/6 to show that
</p>
<p>plim (X &prime;X/T ) as T &rarr; &infin; is not a positive definite matrix.
(c) Use the fact that
</p>
<p>( &radic;
T (α̂OLS &minus; α)
</p>
<p>T
&radic;
T (β̂OLS &minus; β)
</p>
<p>)
= A(X &prime;X)&minus;1AA&minus;1(X &prime;u) = (A&minus;1(X &prime;X)A&minus;1)&minus;1A&minus;1(X &prime;u)
</p>
<p>where A =
</p>
<p>( &radic;
T 0
</p>
<p>0 T
&radic;
T
</p>
<p>)
</p>
<p>is the 2 &times; 2 nonsingular matrix, to show that plim (A&minus;1(X &prime;X)A&minus;1) is the finite positive
definite matrix
</p>
<p>Q =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1
1
</p>
<p>2
</p>
<p>1
</p>
<p>2
</p>
<p>1
</p>
<p>3
</p>
<p>⎞
⎟⎟⎠ and A
</p>
<p>&minus;1(X &prime;u) =
</p>
<p>⎛
⎝
</p>
<p>&sum;T
t=1 ut/
</p>
<p>&radic;
T
</p>
<p>&sum;T
t=1 tut/T
</p>
<p>&radic;
T
</p>
<p>⎞
⎠
</p>
<p>(d) Show that z1 =
&sum;T
</p>
<p>t=1 ut/
&radic;
T is N(0, σ2) and z2 =
</p>
<p>&sum;T
t=1 tut/T
</p>
<p>&radic;
T is N(0, σ2(T + 1)(2T +
</p>
<p>1)/6T 2) with cov(z1, z2) = (T + 1)σ
2/2T , so that
</p>
<p>(
z1
z2
</p>
<p>)
&sim; N
</p>
<p>⎛
⎜⎜⎜⎝0, σ
</p>
<p>2
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1
T + 1
</p>
<p>2T
</p>
<p>T + 1
</p>
<p>2T
</p>
<p>(T + 1)(2T + 1)
</p>
<p>6T 2
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>Conclude that as T &rarr; &infin;, the asymptotic distribution of
(
</p>
<p>z1
z2
</p>
<p>)
is N(0, σ2Q).
</p>
<p>(e) Using the results in parts (c) and (d), conclude that the asymptotic distribution of( &radic;
T (α̂OLS &minus; α)
</p>
<p>T
&radic;
T (β̂OLS &minus; β)
</p>
<p>)
is N(0, σ2Q&minus;1). Since β̂OLS has the factor T
</p>
<p>&radic;
T rather than the usual
</p>
<p>&radic;
T , it is said to be superconsistent. This means that not only does (β̂OLS &minus; β) converge to
</p>
<p>zero in probability limits, but so does T (β̂OLS &minus; β). Note that the normality assumption
is not needed for this result. Using the central limit theorem, all that is needed is that ut
is White noise with finite fourth moments, see Sims, Stock and Watson (1990) or Hamilton
(1994).
</p>
<p>7. Test of Hypothesis with a Deterministic Time Trend Model. This is based on Hamilton (1994). In
</p>
<p>problem 6, we showed that α̂OLS and β̂OLS converged at different rates,
&radic;
T and T
</p>
<p>&radic;
T respectively.
</p>
<p>Despite this fact, the usual least squares t and F -statistics are asymptotically valid even when the
ut&rsquo;s are not Normally distributed.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 393
</p>
<p>(a) Show that s2 =
&sum;T
</p>
<p>t=1(yt &minus; α̂OLS &minus; β̂OLSt)2/(T &minus; 2) has plim s2 = σ2.
(b) In order to test Ho; α = αo, the usual least squares package computes
</p>
<p>tα = (α̂OLS &minus; αo)/[s2(1, 0)(X &prime;X)&minus;1(1, 0)&prime;]1/2
</p>
<p>where (X &prime;X) is given in problem 6. Multiply the numerator and denominator by
&radic;
T and
</p>
<p>use the results of part (c) of problem 6 to show that this t-statistic has the same asymptotic
</p>
<p>distribution as t&lowast;α =
&radic;
T (α̂OLS&minus;αo)/σ
</p>
<p>&radic;
q11 where q11 is the (1, 1) element of Q&minus;1 defined in
</p>
<p>problem 6. t&lowast;α has an asymptotic N(0, 1) distribution using the results of part (e) in problem
6.
</p>
<p>(c) Similarly, to test Ho; β = βo, the usual least squares package computes
</p>
<p>tβ = (β̂OLS &minus; β)/[s2(0, 1)(X &prime;X)&minus;1(0, 1)&prime;]1/2.
</p>
<p>Multiply the numerator and denominator by T
&radic;
T and use the results of part (c) of problem
</p>
<p>6 to show that this t-statistic has the same asymptotic distribution as t&lowast;β = T
&radic;
T (β̂OLS &minus;
</p>
<p>β)/σ
&radic;
</p>
<p>q22 where q22 is the (2, 2) element of Q&minus;1 defined in problem 6. t&lowast;β has an asymptotic
N(0, 1) distribution using the results of part (e) in problem 6.
</p>
<p>8. A Random Walk Model. This is based on Fuller (1976) and Hamilton (1994). Consider the following
random walk model
</p>
<p>yt = yt&minus;1 + ut t = 0, 1, . . . , T where ut &sim; IIN(0, σ
2) and yo = 0.
</p>
<p>(a) Show that yt can be written as yt = u1 + u2 + .. + ut with E(yt) = 0 and var(yt) = tσ
2 so
</p>
<p>that yt &sim; N(0, tσ
2).
</p>
<p>(b) Square the random walk equation y2t = (yt&minus;1 + ut)
2 and solve for yt&minus;1ut. Sum this over
</p>
<p>t = 1, 2, . . . , T and show that
</p>
<p>&sum;T
t=1 yt&minus;1ut = (y
</p>
<p>2
T /2)&minus;
</p>
<p>&sum;T
t=1 u
</p>
<p>2
t/2
</p>
<p>Divide by Tσ2 and show that
&sum;T
</p>
<p>t=1 yt&minus;1ut/Tσ
2 is asymptotically distributed as (χ21 &minus; 1)/2.
</p>
<p>Hint: Use the fact that yT &sim; N(0, Tσ
2).
</p>
<p>(c) Using the fact that yt&minus;1 &sim; N(0, (t&minus; 1)σ2) show that E
(&sum;T
</p>
<p>t=1 y
2
t&minus;1
</p>
<p>)
= σ2T (T &minus; 1)/2. Hint:
</p>
<p>Use the expression for
&sum;T
</p>
<p>t=1 t in problem 6.
</p>
<p>(d) Suppose we had estimated an AR(1) model rather than a random walk, i.e., yt = ρyt&minus;1 + ut
when the true ρ = 1. The OLS estimate is
</p>
<p>ρ̂ =
&sum;T
</p>
<p>t=1 yt&minus;1yt/
&sum;T
</p>
<p>t=1 y
2
t&minus;1 = ρ+
</p>
<p>&sum;T
t=1 yt&minus;1ut/
</p>
<p>&sum;T
t=1 y
</p>
<p>2
t&minus;1
</p>
<p>Show that
</p>
<p>plim T (ρ̂&minus; ρ) = plim
&sum;T
</p>
<p>t=1 yt&minus;1ut/Tσ
2
</p>
<p>&sum;T
t=1 y
</p>
<p>2
t&minus;1/T
</p>
<p>2σ2
= 0
</p>
<p>Note that the numerator was considered in part (b), while the denominator was considered in part
(c). One can see that the asymptotic distribution of ρ̂ when ρ = 1 is a ratio of (χ21 &minus; 1)/2 random
variable to a non-standard distribution in the denominator which is beyond the scope of this book,
see Hamilton (1994) or Fuller (1976) for further details. The object of this exercise is to show that
if ρ = 1,
</p>
<p>&radic;
T (ρ̂&minus; ρ) is no longer normal as in the standard stationary least squares regression with
</p>
<p>|ρ| &lt; 1. Also, to show that for the nonstationary (random walk) model, ρ̂ converges at a faster
rate (T ) than for the stationary case (
</p>
<p>&radic;
T ). From part (c) it is clear that one has to divide the
</p>
<p>denominator of ρ̂ by T 2 rather than T to get a convergent distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>394 Chapter 14: Time-Series Analysis
</p>
<p>9. Consider the cointegration example given in (14.13) and (14.14).
</p>
<p>(a) Verify equations (14.15)-(14.20).
</p>
<p>(b) Show that the OLS estimator of β obtained by regressing Ct on Yt is superconsistent , i.e.,
</p>
<p>show that plim T (β̂OLS &minus; β) &rarr; 0 as T &rarr; &infin;.
</p>
<p>References
</p>
<p>This chapter draws on the material in Davidson and MacKinnon (1993), Maddala (1992), Hamilton
</p>
<p>(1994), Banerjee et al. (1993) and Gujarati (1995). Advanced readings include Fuller (1976) and Hamilton
</p>
<p>(1994). Easier readings include Mills (1990) and Enders (1995).
</p>
<p>Banerjee, A., J.J. Dolado, J.W. Galbraith and D.F. Hendry (1993), Co-Integration, Error-Correction,
and The Econometric Analysis of Non-stationary Data (Oxford University Press: Oxford).
</p>
<p>Bierens, H.J. (2001), &ldquo; Unit Roots,&rdquo; Chapter 29 in B.H. Baltagi (ed.) A Companion to Theoretical
Econometrics (Blackwell: Massachusetts).
</p>
<p>Bierens, H.J. and S. Guo (1993), &ldquo;Testing for Stationarity and Trend Stationarity Against the Unit Root
Hypothesis,&rdquo; Econometric Reviews, 12: 1&ndash;32.
</p>
<p>Bollerslev, T. (1986), &ldquo;Generalized Autoregressive Heteroskedasticity,&rdquo; Journal of Econometrics, 31:
307&ndash;327.
</p>
<p>Box, G.E.P. and G.M. Jenkins (1970), Time Series Analysis, Forecasting and Control (Holden Day: San
Francisco).
</p>
<p>Box, G.E.P. and D.A. Pierce (1970), &ldquo;The Distribution of Residual Autocorrelations in Auto-regressive-
Integrated Moving Average Time Series Models,&rdquo; Journal of American Statistical Association, 65:
1509&ndash;1526.
</p>
<p>Chamberlain, G. (1982), &ldquo;The General Equivalence of Granger and Sims Causality,&rdquo; Econometrica, 50:
569&ndash;582.
</p>
<p>Davidson, R. and J.G. MacKinnon (1993), Estimation and Inference in Econometrics (Oxford University
Press: Oxford).
</p>
<p>Dickey, D.A. and W.A. Fuller (1979), &ldquo;Distribution of the Estimators for Autoregressive Time Series
with A Unit Root,&rdquo; Journal of the American Statistical Association, 74: 427&ndash;431.
</p>
<p>Dolado, J.J., J. Gonzalo and F. Marmol (2001), &ldquo;Cointegration,&rdquo; Chapter 30 in B.H. Baltagi (ed.) A
Companion to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Durlauf, S.N. and P.C.B. Phillips (1988), &ldquo;Trends versus Random Walks in Time Series Analysis,&rdquo;
Econometrica, 56: 1333&ndash;1354.
</p>
<p>Enders, W. (1995), Applied Econometric Time Series (Wiley: New York).
</p>
<p>Engle, R.F. (1982), &ldquo;Autogressive Conditional Heteroskedasticity with Estimates of the Variance of
United Kingdom Inflation,&rdquo; Econometrica, 50: 987&ndash;1007.
</p>
<p>Engle, R.F. and C.W.J. Granger (1987), &ldquo;Co-Integration and Error Correction: Representation, Estima-
tion and Testing,&rdquo; Econometrica, 55: 251&ndash;276.
</p>
<p>Fuller, W.A. (1976), Introduction to Statistical Time Series (John Wiley and Sons: New York).</p>
<p/>
</div>
<div class="page"><p/>
<p>References 395
</p>
<p>Geweke, J., R. Meese and W. Dent (1983), &ldquo;Comparing Alternative Tests of Causality in Temporal
Systems: Analytic Results and Experimental Evidence,&rdquo; Journal of Econometrics, 21: 161&ndash;194.
</p>
<p>Ghysels, E. and P. Perron (1993), &ldquo;The Effect of Seasonal Adjustment Filters on Tests for a Unit Root,&rdquo;
Journal of Econometrics, 55: 57&ndash;98.
</p>
<p>Godfrey, L.G. (1979), &ldquo;Testing the Adequacy of a Time Series Model,&rdquo; Biometrika, 66: 67&ndash;72.
</p>
<p>Granger, C.W.J. (1969), &ldquo;Investigating Causal Relations by Econometric Models and Cross-Spectral
Methods,&rdquo; Econometrica, 37: 424&ndash;438.
</p>
<p>Granger, C.W.J. (2001), &ldquo;Spurious Regressions in Econometrics,&rdquo; Chapter 26 in B.H. Baltagi (ed.) A
Companion to Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>Granger, C.W.J., M.L. King and H. White (1995), &ldquo;Comments on Testing Economic Theories and the
Use of Model Selection Criteria,&rdquo; Journal of Econometrics, 67: 173&ndash;187.
</p>
<p>Granger, C.W.J. and P. Newbold (1974), &ldquo;Spurious Regressions in Econometrics,&rdquo; Journal of Econo-
metrics, 2: 111&ndash;120.
</p>
<p>Gujarati, D.N. (1995), Basic Econometrics (McGraw Hill: New York).
</p>
<p>Hamilton, J.D. (1994), Time Series Analysis (Princeton University Press: Princeton, New Jersey).
</p>
<p>Johansen, S. (1988), &ldquo;Statistical Analysis of Cointegrating Vectors,&rdquo; Journal of Economic Dynamics and
Control, 12: 231&ndash;254.
</p>
<p>Judge, G.G., R.C. Hill, W.E. Griffiths, H. Lütkepohl and T.C. Lee (1985), The Theory and Practice of
Econometrics (John Wiley and Sons: New York).
</p>
<p>Kwaitowski, D., P.C.B. Phillips, P. Schmidt and Y. Shin (1992), &ldquo;Testing the Null Hypothesis of Sta-
tionarity Against the Alternative of a Unit Root,&rdquo; Journal of Econometrics, 54: 159&ndash;178.
</p>
<p>Leybourne, S.J. and B.P.M. McCabe (1994), &ldquo;A Consistent Test for a Unit Root,&rdquo; Journal of Business
and Economic Statistics, 12: 157&ndash;166.
</p>
<p>Litterman, R.B. (1986), &ldquo;Forecasting with Bayesian Vector Autoregressions-Five Years of Experience,&rdquo;
Journal of Business and Economic Statistics, 4: 25&ndash;38.
</p>
<p>Ljung, G.M. and G.E.P. Box (1978), &ldquo;On a Measure of Lack of Fit in Time-Series Models,&rdquo; Biometrika,
65: 297&ndash;303.
</p>
<p>Lütkepohl, H. (2001), &ldquo;Vector Autoregressions,&rdquo; Chapter 32 in B.H. Baltagi (ed.) A Companion to
Theoretical Econometrics (Blackwell: Massachusetts).
</p>
<p>MacKinnon, J.G. (1991), &rdquo;Critical Values for Cointegration Tests,&rdquo; Ch. 13 in Long-Run Economic Re-
lationships: Readings in Cointegration, eds. R.F. Engle and C.W.J. Granger (Oxford University
Press: Oxford ).
</p>
<p>Maddala, G.S. (1992), Introduction to Econometrics (Macmillan: New York).
</p>
<p>Mills, T.C. (1990), Time Series Techniques for Economists (Cambridge University Press: Cambridge).
</p>
<p>Nelson, C.R. and C.I. Plosser (1982), &ldquo;Trends and Random Walks in Macroeconomic Time Series: Some
Evidence and Implications,&rdquo; Journal of Monetary Economics, 10: 139&ndash;162.
</p>
<p>Ng, S. and P. Perron (1995), &ldquo;Unit Root Tests in ARMA Models With Data-Dependent Methods for the
Selection of the Truncation Lag,&rdquo; Journal of the American Statistical Association, 90: 268&ndash;281.
</p>
<p>Perron, P. (1989), &ldquo;The Great Cash, The Oil Price Shock, and the Unit Root Hypothesis,&rdquo; Econometrica,
57: 1361&ndash;1401.</p>
<p/>
</div>
<div class="page"><p/>
<p>396 Chapter 14: Time-Series Analysis
</p>
<p>Phillips, P.C.B. (1986), &ldquo;Understanding Spurious Regressions in Econometrics,&rdquo; Journal of Economet-
rics, 33: 311&ndash;340.
</p>
<p>Phillips, P.C.B. and P. Perron (1988), &ldquo;Testing for A Unit Root in Time Series Regression,&rdquo; Biometrika,
75: 335&ndash;346.
</p>
<p>Plosser, C.I. and G.W. Shwert (1978), &ldquo;Money, Income and Sunspots: Measuring Economic Relationships
and the Effects of Differencing,&rdquo; Journal of Monetary Economics, 4: 637&ndash;660.
</p>
<p>Sims, C.A. (1972), &ldquo;Money, Income and Causality,&rdquo; American Economic Review, 62: 540&ndash;552.
</p>
<p>Sims, C.A. (1980), &ldquo;Macroeconomics and Reality,&rdquo; Econometrica, 48: 1&ndash;48.
</p>
<p>Sims, C.A., J.H. Stock and M.W. Watson (1990), &ldquo;Inference in Linear Time Series Models with Some
Unit Roots,&rdquo; Econometrica, 58: 113&ndash;144.
</p>
<p>Stock, J.H. and M.W. Watson (1988), &ldquo;Variable Trends in Economic Time Series,&rdquo; Journal of Economic
</p>
<p>Perspectives, 2: 147&ndash;174.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix
</p>
<p>0 z
</p>
<p>Φ(1.65) = pr[z &le; 1.65] = 0.9505
</p>
<p>Table A Area under the Standard Normal Distribution
</p>
<p>z 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
</p>
<p>0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359
</p>
<p>0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753
</p>
<p>0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141
</p>
<p>0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517
</p>
<p>0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879
</p>
<p>0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224
</p>
<p>0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549
</p>
<p>0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852
</p>
<p>0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133
</p>
<p>0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389
</p>
<p>1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621
</p>
<p>1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830
</p>
<p>1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015
</p>
<p>1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177
</p>
<p>1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319
</p>
<p>1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441
</p>
<p>1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545
</p>
<p>1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633
</p>
<p>1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706
</p>
<p>1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767
</p>
<p>2.0 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817
</p>
<p>2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857
</p>
<p>2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890
</p>
<p>2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916
</p>
<p>2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936
</p>
<p>2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952
</p>
<p>2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964
</p>
<p>2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974
</p>
<p>2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981
</p>
<p>2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986
</p>
<p>3.0 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990
</p>
<p>Source: The SAS r&copy; function PROBNORM was used to generate this table.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2011 
</p>
<p>B.H. Baltagi, Econometrics, Springer Texts in Business and Economics, DOI 10.1007/978-3-642-20059-5, 397</p>
<p/>
</div>
<div class="page"><p/>
<p>398 Appendix
</p>
<p>0 tα
</p>
<p>α
</p>
<p>Pr[t8 &gt; tα = 2.306] = 0.025
</p>
<p>Table B Right-Tail Critical Values for the t-Distribution
</p>
<p>DF α=0.1 α=0.05 α=0.025 α=0.01 α=0.005
</p>
<p>1 3.0777 6.3138 12.7062 31.8205 63.6567
</p>
<p>2 1.8856 2.9200 4.3027 6.9646 9.9248
</p>
<p>3 1.6377 2.3534 3.1824 4.5407 5.8409
</p>
<p>4 1.5332 2.1318 2.7764 3.7469 4.6041
</p>
<p>5 1.4759 2.0150 2.5706 3.3649 4.0321
</p>
<p>6 1.4398 1.9432 2.4469 3.1427 3.7074
</p>
<p>7 1.4149 1.8946 2.3646 2.9980 3.4995
</p>
<p>8 1.3968 1.8595 2.3060 2.8965 3.3554
</p>
<p>9 1.3830 1.8331 2.2622 2.8214 3.2498
</p>
<p>10 1.3722 1.8125 2.2281 2.7638 3.1693
</p>
<p>11 1.3634 1.7959 2.2010 2.7181 3.1058
</p>
<p>12 1.3562 1.7823 2.1788 2.6810 3.0545
</p>
<p>13 1.3502 1.7709 2.1604 2.6503 3.0123
</p>
<p>14 1.3450 1.7613 2.1448 2.6245 2.9768
</p>
<p>15 1.3406 1.7531 2.1314 2.6025 2.9467
</p>
<p>16 1.3368 1.7459 2.1199 2.5835 2.9208
</p>
<p>17 1.3334 1.7396 2.1098 2.5669 2.8982
</p>
<p>18 1.3304 1.7341 2.1009 2.5524 2.8784
</p>
<p>19 1.3277 1.7291 2.0930 2.5395 2.8609
</p>
<p>20 1.3253 1.7247 2.0860 2.5280 2.8453
</p>
<p>21 1.3232 1.7207 2.0796 2.5176 2.8314
</p>
<p>22 1.3212 1.7171 2.0739 2.5083 2.8188
</p>
<p>23 1.3195 1.7139 2.0687 2.4999 2.8073
</p>
<p>24 1.3178 1.7109 2.0639 2.4922 2.7969
</p>
<p>25 1.3163 1.7081 2.0595 2.4851 2.7874
</p>
<p>26 1.3150 1.7056 2.0555 2.4786 2.7787
</p>
<p>27 1.3137 1.7033 2.0518 2.4727 2.7707
</p>
<p>28 1.3125 1.7011 2.0484 2.4671 2.7633
</p>
<p>29 1.3114 1.6991 2.0452 2.4620 2.7564
</p>
<p>30 1.3104 1.6973 2.0423 2.4573 2.7500
</p>
<p>31 1.3095 1.6955 2.0395 2.4528 2.7440
</p>
<p>32 1.3086 1.6939 2.0369 2.4487 2.7385
</p>
<p>33 1.3077 1.6924 2.0345 2.4448 2.7333
</p>
<p>34 1.3070 1.6909 2.0322 2.4411 2.7284
</p>
<p>35 1.3062 1.6896 2.0301 2.4377 2.7238
</p>
<p>36 1.3055 1.6883 2.0281 2.4345 2.7195
</p>
<p>37 1.3049 1.6871 2.0262 2.4314 2.7154
</p>
<p>38 1.3042 1.6860 2.0244 2.4286 2.7116
</p>
<p>39 1.3036 1.6849 2.0227 2.4258 2.7079
</p>
<p>40 1.3031 1.6839 2.0211 2.4233 2.7045
</p>
<p>Source: The SAS r&copy; function TINV was used to generate this table.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 399
</p>
<p>T
a
b
le
</p>
<p>C
R
ig
h
t-
T
a
il
C
ri
ti
ca
l
V
a
lu
es
</p>
<p>fo
r
th
e
F
-D
</p>
<p>is
tr
ib
u
ti
o
n
:
U
p
p
er
</p>
<p>5
%
</p>
<p>P
o
in
ts
</p>
<p>v
2
/v
</p>
<p>1
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>1
0
</p>
<p>1
2
</p>
<p>1
5
</p>
<p>2
0
</p>
<p>2
5
</p>
<p>3
0
</p>
<p>4
0
</p>
<p>1
16
1.
4
4
8
</p>
<p>19
9.
50
0
</p>
<p>2
15
.7
07
</p>
<p>2
2
4
.5
8
3
</p>
<p>2
30
.1
62
</p>
<p>2
33
.9
8
6
</p>
<p>2
36
.7
68
</p>
<p>2
38
.8
8
3
</p>
<p>2
4
0.
54
3
</p>
<p>2
4
1.
8
8
2
</p>
<p>2
4
3.
90
6
</p>
<p>2
4
5.
95
0
</p>
<p>2
4
8
.0
13
</p>
<p>2
4
9.
2
60
</p>
<p>2
50
.0
95
</p>
<p>2
51
.1
4
3
</p>
<p>2
18
.5
13
</p>
<p>19
.0
00
</p>
<p>19
.1
64
</p>
<p>19
.2
4
7
</p>
<p>19
.2
96
</p>
<p>19
.3
30
</p>
<p>19
.3
53
</p>
<p>19
.3
7
1
</p>
<p>19
.3
8
5
</p>
<p>19
.3
96
</p>
<p>19
.4
13
</p>
<p>19
.4
2
9
</p>
<p>19
.4
4
6
</p>
<p>19
.4
56
</p>
<p>19
.4
62
</p>
<p>19
.4
7
1
</p>
<p>3
10
.1
2
8
</p>
<p>9.
55
2
</p>
<p>9.
2
7
7
</p>
<p>9.
11
7
</p>
<p>9.
01
3
</p>
<p>8
.9
4
1
</p>
<p>8
.8
8
7
</p>
<p>8
.8
4
5
</p>
<p>8
.8
12
</p>
<p>8
.7
8
6
</p>
<p>8
.7
4
5
</p>
<p>8
.7
03
</p>
<p>8
.6
60
</p>
<p>8
.6
34
</p>
<p>8
.6
17
</p>
<p>8
.5
94
</p>
<p>4
7
.7
09
</p>
<p>6.
94
4
</p>
<p>6.
59
1
</p>
<p>6.
38
8
</p>
<p>6.
2
56
</p>
<p>6.
16
3
</p>
<p>6.
09
4
</p>
<p>6.
04
1
</p>
<p>5.
99
9
</p>
<p>5.
96
4
</p>
<p>5.
91
2
</p>
<p>5.
8
58
</p>
<p>5.
8
03
</p>
<p>5.
7
69
</p>
<p>5.
7
4
6
</p>
<p>5.
7
17
</p>
<p>5
6.
60
8
</p>
<p>5.
7
8
6
</p>
<p>5.
4
09
</p>
<p>5.
19
2
</p>
<p>5.
05
0
</p>
<p>4
.9
50
</p>
<p>4
.8
7
6
</p>
<p>4
.8
18
</p>
<p>4
.7
7
2
</p>
<p>4
.7
35
</p>
<p>4
.6
7
8
</p>
<p>4
.6
19
</p>
<p>4
.5
58
</p>
<p>4
.5
2
1
</p>
<p>4
.4
96
</p>
<p>4
.4
64
</p>
<p>6
5.
98
7
</p>
<p>5.
14
3
</p>
<p>4
.7
57
</p>
<p>4
.5
34
</p>
<p>4
.3
8
7
</p>
<p>4
.2
8
4
</p>
<p>4
.2
07
</p>
<p>4
.1
4
7
</p>
<p>4
.0
99
</p>
<p>4
.0
60
</p>
<p>4
.0
00
</p>
<p>3.
93
8
</p>
<p>3.
8
7
4
</p>
<p>3.
8
35
</p>
<p>3.
8
08
</p>
<p>3.
7
7
4
</p>
<p>7
5.
59
1
</p>
<p>4
.7
37
</p>
<p>4
.3
4
7
</p>
<p>4
.1
2
0
</p>
<p>3.
97
2
</p>
<p>3.
8
66
</p>
<p>3.
7
8
7
</p>
<p>3.
7
2
6
</p>
<p>3.
67
7
</p>
<p>3.
63
7
</p>
<p>3.
57
5
</p>
<p>3.
51
1
</p>
<p>3.
4
4
5
</p>
<p>3.
4
04
</p>
<p>3.
37
6
</p>
<p>3.
34
0
</p>
<p>8
5.
31
8
</p>
<p>4
.4
59
</p>
<p>4
.0
66
</p>
<p>3.
8
38
</p>
<p>3.
68
7
</p>
<p>3.
58
1
</p>
<p>3.
50
0
</p>
<p>3.
4
38
</p>
<p>3.
38
8
</p>
<p>3.
34
7
</p>
<p>3.
2
8
4
</p>
<p>3.
2
18
</p>
<p>3.
15
0
</p>
<p>3.
10
8
</p>
<p>3.
07
9
</p>
<p>3.
04
3
</p>
<p>9
5.
11
7
</p>
<p>4
.2
56
</p>
<p>3.
8
63
</p>
<p>3.
63
3
</p>
<p>3.
4
8
2
</p>
<p>3.
37
4
</p>
<p>3.
2
93
</p>
<p>3.
2
30
</p>
<p>3.
17
9
</p>
<p>3.
13
7
</p>
<p>3.
07
3
</p>
<p>3.
00
6
</p>
<p>2
.9
36
</p>
<p>2
.8
93
</p>
<p>2
.8
64
</p>
<p>2
.8
2
6
</p>
<p>10
4
.9
65
</p>
<p>4
.1
03
</p>
<p>3.
7
08
</p>
<p>3.
4
7
8
</p>
<p>3.
32
6
</p>
<p>3.
2
17
</p>
<p>3.
13
5
</p>
<p>3.
07
2
</p>
<p>3.
02
0
</p>
<p>2
.9
7
8
</p>
<p>2
.9
13
</p>
<p>2
.8
4
5
</p>
<p>2
.7
7
4
</p>
<p>2
.7
30
</p>
<p>2
.7
00
</p>
<p>2
.6
61
</p>
<p>11
4
.8
4
4
</p>
<p>3.
98
2
</p>
<p>3.
58
7
</p>
<p>3.
35
7
</p>
<p>3.
2
04
</p>
<p>3.
09
5
</p>
<p>3.
01
2
</p>
<p>2
.9
4
8
</p>
<p>2
.8
96
</p>
<p>2
.8
54
</p>
<p>2
.7
8
8
</p>
<p>2
.7
19
</p>
<p>2
.6
4
6
</p>
<p>2
.6
01
</p>
<p>2
.5
7
0
</p>
<p>2
.5
31
</p>
<p>12
4
.7
4
7
</p>
<p>3.
8
8
5
</p>
<p>3.
4
90
</p>
<p>3.
2
59
</p>
<p>3.
10
6
</p>
<p>2
.9
96
</p>
<p>2
.9
13
</p>
<p>2
.8
4
9
</p>
<p>2
.7
96
</p>
<p>2
.7
53
</p>
<p>2
.6
8
7
</p>
<p>2
.6
17
</p>
<p>2
.5
4
4
</p>
<p>2
.4
98
</p>
<p>2
.4
66
</p>
<p>2
.4
2
6
</p>
<p>13
4
.6
67
</p>
<p>3.
8
06
</p>
<p>3.
4
11
</p>
<p>3.
17
9
</p>
<p>3.
02
5
</p>
<p>2
.9
15
</p>
<p>2
.8
32
</p>
<p>2
.7
67
</p>
<p>2
.7
14
</p>
<p>2
.6
7
1
</p>
<p>2
.6
04
</p>
<p>2
.5
33
</p>
<p>2
.4
59
</p>
<p>2
.4
12
</p>
<p>2
.3
8
0
</p>
<p>2
.3
39
</p>
<p>14
4
.6
00
</p>
<p>3.
7
39
</p>
<p>3.
34
4
</p>
<p>3.
11
2
</p>
<p>2
.9
58
</p>
<p>2
.8
4
8
</p>
<p>2
.7
64
</p>
<p>2
.6
99
</p>
<p>2
.6
4
6
</p>
<p>2
.6
02
</p>
<p>2
.5
34
</p>
<p>2
.4
63
</p>
<p>2
.3
8
8
</p>
<p>2
.3
4
1
</p>
<p>2
.3
08
</p>
<p>2
.2
66
</p>
<p>15
4
.5
4
3
</p>
<p>3.
68
2
</p>
<p>3.
2
8
7
</p>
<p>3.
05
6
</p>
<p>2
.9
01
</p>
<p>2
.7
90
</p>
<p>2
.7
07
</p>
<p>2
.6
4
1
</p>
<p>2
.5
8
8
</p>
<p>2
.5
4
4
</p>
<p>2
.4
7
5
</p>
<p>2
.4
03
</p>
<p>2
.3
2
8
</p>
<p>2
.2
8
0
</p>
<p>2
.2
4
7
</p>
<p>2
.2
04
</p>
<p>16
4
.4
94
</p>
<p>3.
63
4
</p>
<p>3.
2
39
</p>
<p>3.
00
7
</p>
<p>2
.8
52
</p>
<p>2
.7
4
1
</p>
<p>2
.6
57
</p>
<p>2
.5
91
</p>
<p>2
.5
38
</p>
<p>2
.4
94
</p>
<p>2
.4
2
5
</p>
<p>2
.3
52
</p>
<p>2
.2
7
6
</p>
<p>2
.2
2
7
</p>
<p>2
.1
94
</p>
<p>2
.1
51
</p>
<p>17
4
.4
51
</p>
<p>3.
59
2
</p>
<p>3.
19
7
</p>
<p>2
.9
65
</p>
<p>2
.8
10
</p>
<p>2
.6
99
</p>
<p>2
.6
14
</p>
<p>2
.5
4
8
</p>
<p>2
.4
94
</p>
<p>2
.4
50
</p>
<p>2
.3
8
1
</p>
<p>2
.3
08
</p>
<p>2
.2
30
</p>
<p>2
.1
8
1
</p>
<p>2
.1
4
8
</p>
<p>2
.1
04
</p>
<p>18
4
.4
14
</p>
<p>3.
55
5
</p>
<p>3.
16
0
</p>
<p>2
.9
2
8
</p>
<p>2
.7
7
3
</p>
<p>2
.6
61
</p>
<p>2
.5
7
7
</p>
<p>2
.5
10
</p>
<p>2
.4
56
</p>
<p>2
.4
12
</p>
<p>2
.3
4
2
</p>
<p>2
.2
69
</p>
<p>2
.1
91
</p>
<p>2
.1
4
1
</p>
<p>2
.1
07
</p>
<p>2
.0
63
</p>
<p>19
4
.3
8
1
</p>
<p>3.
52
2
</p>
<p>3.
12
7
</p>
<p>2
.8
95
</p>
<p>2
.7
4
0
</p>
<p>2
.6
2
8
</p>
<p>2
.5
4
4
</p>
<p>2
.4
7
7
</p>
<p>2
.4
2
3
</p>
<p>2
.3
7
8
</p>
<p>2
.3
08
</p>
<p>2
.2
34
</p>
<p>2
.1
55
</p>
<p>2
.1
06
</p>
<p>2
.0
7
1
</p>
<p>2
.0
2
6
</p>
<p>2
0
</p>
<p>4
.3
51
</p>
<p>3.
4
93
</p>
<p>3.
09
8
</p>
<p>2
.8
66
</p>
<p>2
.7
11
</p>
<p>2
.5
99
</p>
<p>2
.5
14
</p>
<p>2
.4
4
7
</p>
<p>2
.3
93
</p>
<p>2
.3
4
8
</p>
<p>2
.2
7
8
</p>
<p>2
.2
03
</p>
<p>2
.1
2
4
</p>
<p>2
.0
7
4
</p>
<p>2
.0
39
</p>
<p>1.
99
4
</p>
<p>2
1
</p>
<p>4
.3
2
5
</p>
<p>3.
4
67
</p>
<p>3.
07
2
</p>
<p>2
.8
4
0
</p>
<p>2
.6
8
5
</p>
<p>2
.5
7
3
</p>
<p>2
.4
8
8
</p>
<p>2
.4
2
0
</p>
<p>2
.3
66
</p>
<p>2
.3
2
1
</p>
<p>2
.2
50
</p>
<p>2
.1
7
6
</p>
<p>2
.0
96
</p>
<p>2
.0
4
5
</p>
<p>2
.0
10
</p>
<p>1.
96
5
</p>
<p>2
2
</p>
<p>4
.3
01
</p>
<p>3.
4
4
3
</p>
<p>3.
04
9
</p>
<p>2
.8
17
</p>
<p>2
.6
61
</p>
<p>2
.5
4
9
</p>
<p>2
.4
64
</p>
<p>2
.3
97
</p>
<p>2
.3
4
2
</p>
<p>2
.2
97
</p>
<p>2
.2
2
6
</p>
<p>2
.1
51
</p>
<p>2
.0
7
1
</p>
<p>2
.0
2
0
</p>
<p>1.
98
4
</p>
<p>1.
93
8
</p>
<p>2
3
</p>
<p>4
.2
7
9
</p>
<p>3.
4
2
2
</p>
<p>3.
02
8
</p>
<p>2
.7
96
</p>
<p>2
.6
4
0
</p>
<p>2
.5
2
8
</p>
<p>2
.4
4
2
</p>
<p>2
.3
7
5
</p>
<p>2
.3
2
0
</p>
<p>2
.2
7
5
</p>
<p>2
.2
04
</p>
<p>2
.1
2
8
</p>
<p>2
.0
4
8
</p>
<p>1.
99
6
</p>
<p>1.
96
1
</p>
<p>1.
91
4
</p>
<p>2
4
</p>
<p>4
.2
60
</p>
<p>3.
4
03
</p>
<p>3.
00
9
</p>
<p>2
.7
7
6
</p>
<p>2
.6
2
1
</p>
<p>2
.5
08
</p>
<p>2
.4
2
3
</p>
<p>2
.3
55
</p>
<p>2
.3
00
</p>
<p>2
.2
55
</p>
<p>2
.1
8
3
</p>
<p>2
.1
08
</p>
<p>2
.0
2
7
</p>
<p>1.
97
5
</p>
<p>1.
93
9
</p>
<p>1.
8
92
</p>
<p>2
5
</p>
<p>4
.2
4
2
</p>
<p>3.
38
5
</p>
<p>2
.9
91
</p>
<p>2
.7
59
</p>
<p>2
.6
03
</p>
<p>2
.4
90
</p>
<p>2
.4
05
</p>
<p>2
.3
37
</p>
<p>2
.2
8
2
</p>
<p>2
.2
36
</p>
<p>2
.1
65
</p>
<p>2
.0
8
9
</p>
<p>2
.0
07
</p>
<p>1.
95
5
</p>
<p>1.
91
9
</p>
<p>1.
8
7
2
</p>
<p>2
6
</p>
<p>4
.2
2
5
</p>
<p>3.
36
9
</p>
<p>2
.9
7
5
</p>
<p>2
.7
4
3
</p>
<p>2
.5
8
7
</p>
<p>2
.4
7
4
</p>
<p>2
.3
8
8
</p>
<p>2
.3
2
1
</p>
<p>2
.2
65
</p>
<p>2
.2
2
0
</p>
<p>2
.1
4
8
</p>
<p>2
.0
7
2
</p>
<p>1.
99
0
</p>
<p>1.
93
8
</p>
<p>1.
90
1
</p>
<p>1.
8
53
</p>
<p>2
7
</p>
<p>4
.2
10
</p>
<p>3.
35
4
</p>
<p>2
.9
60
</p>
<p>2
.7
2
8
</p>
<p>2
.5
7
2
</p>
<p>2
.4
59
</p>
<p>2
.3
7
3
</p>
<p>2
.3
05
</p>
<p>2
.2
50
</p>
<p>2
.2
04
</p>
<p>2
.1
32
</p>
<p>2
.0
56
</p>
<p>1.
97
4
</p>
<p>1.
92
1
</p>
<p>1.
8
8
4
</p>
<p>1.
8
36
</p>
<p>2
8
</p>
<p>4
.1
96
</p>
<p>3.
34
0
</p>
<p>2
.9
4
7
</p>
<p>2
.7
14
</p>
<p>2
.5
58
</p>
<p>2
.4
4
5
</p>
<p>2
.3
59
</p>
<p>2
.2
91
</p>
<p>2
.2
36
</p>
<p>2
.1
90
</p>
<p>2
.1
18
</p>
<p>2
.0
4
1
</p>
<p>1.
95
9
</p>
<p>1.
90
6
</p>
<p>1.
8
69
</p>
<p>1.
8
2
0
</p>
<p>2
9
</p>
<p>4
.1
8
3
</p>
<p>3.
32
8
</p>
<p>2
.9
34
</p>
<p>2
.7
01
</p>
<p>2
.5
4
5
</p>
<p>2
.4
32
</p>
<p>2
.3
4
6
</p>
<p>2
.2
7
8
</p>
<p>2
.2
2
3
</p>
<p>2
.1
7
7
</p>
<p>2
.1
04
</p>
<p>2
.0
2
7
</p>
<p>1.
94
5
</p>
<p>1.
8
91
</p>
<p>1.
8
54
</p>
<p>1.
8
06
</p>
<p>30
4
.1
7
1
</p>
<p>3.
31
6
</p>
<p>2
.9
2
2
</p>
<p>2
.6
90
</p>
<p>2
.5
34
</p>
<p>2
.4
2
1
</p>
<p>2
.3
34
</p>
<p>2
.2
66
</p>
<p>2
.2
11
</p>
<p>2
.1
65
</p>
<p>2
.0
92
</p>
<p>2
.0
15
</p>
<p>1.
93
2
</p>
<p>1.
8
7
8
</p>
<p>1.
8
4
1
</p>
<p>1.
7
92
</p>
<p>31
4
.1
60
</p>
<p>3.
30
5
</p>
<p>2
.9
11
</p>
<p>2
.6
7
9
</p>
<p>2
.5
2
3
</p>
<p>2
.4
09
</p>
<p>2
.3
2
3
</p>
<p>2
.2
55
</p>
<p>2
.1
99
</p>
<p>2
.1
53
</p>
<p>2
.0
8
0
</p>
<p>2
.0
03
</p>
<p>1.
92
0
</p>
<p>1.
8
66
</p>
<p>1.
8
2
8
</p>
<p>1.
7
7
9
</p>
<p>32
4
.1
4
9
</p>
<p>3.
2
95
</p>
<p>2
.9
01
</p>
<p>2
.6
68
</p>
<p>2
.5
12
</p>
<p>2
.3
99
</p>
<p>2
.3
13
</p>
<p>2
.2
4
4
</p>
<p>2
.1
8
9
</p>
<p>2
.1
4
2
</p>
<p>2
.0
7
0
</p>
<p>1.
99
2
</p>
<p>1.
90
8
</p>
<p>1.
8
54
</p>
<p>1.
8
17
</p>
<p>1.
7
67
</p>
<p>33
4
.1
39
</p>
<p>3.
2
8
5
</p>
<p>2
.8
92
</p>
<p>2
.6
59
</p>
<p>2
.5
03
</p>
<p>2
.3
8
9
</p>
<p>2
.3
03
</p>
<p>2
.2
35
</p>
<p>2
.1
7
9
</p>
<p>2
.1
33
</p>
<p>2
.0
60
</p>
<p>1.
98
2
</p>
<p>1.
8
98
</p>
<p>1.
8
4
4
</p>
<p>1.
8
06
</p>
<p>1.
7
56
</p>
<p>34
4
.1
30
</p>
<p>3.
2
7
6
</p>
<p>2
.8
8
3
</p>
<p>2
.6
50
</p>
<p>2
.4
94
</p>
<p>2
.3
8
0
</p>
<p>2
.2
94
</p>
<p>2
.2
2
5
</p>
<p>2
.1
7
0
</p>
<p>2
.1
2
3
</p>
<p>2
.0
50
</p>
<p>1.
97
2
</p>
<p>1.
8
8
8
</p>
<p>1.
8
33
</p>
<p>1.
7
95
</p>
<p>1.
7
4
5
</p>
<p>35
4
.1
2
1
</p>
<p>3.
2
67
</p>
<p>2
.8
7
4
</p>
<p>2
.6
4
1
</p>
<p>2
.4
8
5
</p>
<p>2
.3
7
2
</p>
<p>2
.2
8
5
</p>
<p>2
.2
17
</p>
<p>2
.1
61
</p>
<p>2
.1
14
</p>
<p>2
.0
4
1
</p>
<p>1.
96
3
</p>
<p>1.
8
7
8
</p>
<p>1.
8
2
4
</p>
<p>1.
7
8
6
</p>
<p>1.
7
35
</p>
<p>36
4
.1
13
</p>
<p>3.
2
59
</p>
<p>2
.8
66
</p>
<p>2
.6
34
</p>
<p>2
.4
7
7
</p>
<p>2
.3
64
</p>
<p>2
.2
7
7
</p>
<p>2
.2
09
</p>
<p>2
.1
53
</p>
<p>2
.1
06
</p>
<p>2
.0
33
</p>
<p>1.
95
4
</p>
<p>1.
8
7
0
</p>
<p>1.
8
15
</p>
<p>1.
7
7
6
</p>
<p>1.
7
2
6
</p>
<p>37
4
.1
05
</p>
<p>3.
2
52
</p>
<p>2
.8
59
</p>
<p>2
.6
2
6
</p>
<p>2
.4
7
0
</p>
<p>2
.3
56
</p>
<p>2
.2
7
0
</p>
<p>2
.2
01
</p>
<p>2
.1
4
5
</p>
<p>2
.0
98
</p>
<p>2
.0
2
5
</p>
<p>1.
94
6
</p>
<p>1.
8
61
</p>
<p>1.
8
06
</p>
<p>1.
7
68
</p>
<p>1.
7
17
</p>
<p>38
4
.0
98
</p>
<p>3.
2
4
5
</p>
<p>2
.8
52
</p>
<p>2
.6
19
</p>
<p>2
.4
63
</p>
<p>2
.3
4
9
</p>
<p>2
.2
62
</p>
<p>2
.1
94
</p>
<p>2
.1
38
</p>
<p>2
.0
91
</p>
<p>2
.0
17
</p>
<p>1.
93
9
</p>
<p>1.
8
53
</p>
<p>1.
7
98
</p>
<p>1.
7
60
</p>
<p>1.
7
08
</p>
<p>39
4
.0
91
</p>
<p>3.
2
38
</p>
<p>2
.8
4
5
</p>
<p>2
.6
12
</p>
<p>2
.4
56
</p>
<p>2
.3
4
2
</p>
<p>2
.2
55
</p>
<p>2
.1
8
7
</p>
<p>2
.1
31
</p>
<p>2
.0
8
4
</p>
<p>2
.0
10
</p>
<p>1.
93
1
</p>
<p>1.
8
4
6
</p>
<p>1.
7
91
</p>
<p>1.
7
52
</p>
<p>1.
7
00
</p>
<p>4
0
</p>
<p>4
.0
8
5
</p>
<p>3.
2
32
</p>
<p>2
.8
39
</p>
<p>2
.6
06
</p>
<p>2
.4
4
9
</p>
<p>2
.3
36
</p>
<p>2
.2
4
9
</p>
<p>2
.1
8
0
</p>
<p>2
.1
2
4
</p>
<p>2
.0
7
7
</p>
<p>2
.0
03
</p>
<p>1.
92
4
</p>
<p>1.
8
39
</p>
<p>1.
7
8
3
</p>
<p>1.
7
4
4
</p>
<p>1.
69
3
</p>
<p>S
o
u
rc
e
:
T
h
e
S
A
S
</p>
<p>r &copy;
fu
n
ct
io
n
F
IN
</p>
<p>V
w
a
s
u
se
d
to
</p>
<p>g
en
</p>
<p>er
a
te
</p>
<p>th
is
</p>
<p>ta
b
le
.
v
1
=
</p>
<p>n
u
m
er
a
to
r
d
eg
re
es
</p>
<p>of
fr
ee
d
om
</p>
<p>v
2
=
</p>
<p>d
en
</p>
<p>om
in
a
to
r
d
eg
re
es
</p>
<p>of
fr
ee
d
om</p>
<p/>
</div>
<div class="page"><p/>
<p>400 Appendix
</p>
<p>T
a
b
le
</p>
<p>D
R
ig
h
t-
T
a
il
C
ri
ti
ca
l
V
a
lu
es
</p>
<p>fo
r
th
e
F
-D
</p>
<p>is
tr
ib
u
ti
o
n
:
U
p
p
er
</p>
<p>1
%
</p>
<p>P
o
in
ts
</p>
<p>v
2
/v
</p>
<p>1
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>1
0
</p>
<p>1
2
</p>
<p>1
5
</p>
<p>2
0
</p>
<p>2
5
</p>
<p>3
0
</p>
<p>4
0
</p>
<p>1
4
05
2
.1
8
1
</p>
<p>4
99
9.
50
0
</p>
<p>54
03
.3
52
</p>
<p>56
2
4
.5
8
3
</p>
<p>57
63
.6
50
</p>
<p>58
58
.9
8
6
</p>
<p>59
2
8
.3
56
</p>
<p>59
8
1.
07
0
</p>
<p>60
2
2
.4
7
3
</p>
<p>60
55
.8
4
7
</p>
<p>61
06
.3
2
1
</p>
<p>61
57
.2
8
5
</p>
<p>62
08
.7
30
</p>
<p>62
39
.8
2
5
</p>
<p>62
60
.6
4
9
</p>
<p>62
8
6.
7
8
2
</p>
<p>2
98
.5
03
</p>
<p>99
.0
00
</p>
<p>99
.1
66
</p>
<p>99
.2
4
9
</p>
<p>99
.2
99
</p>
<p>99
.3
33
</p>
<p>99
.3
56
</p>
<p>99
.3
7
4
</p>
<p>99
.3
8
8
</p>
<p>99
.3
99
</p>
<p>99
.4
16
</p>
<p>99
.4
33
</p>
<p>99
.4
4
9
</p>
<p>99
.4
59
</p>
<p>99
.4
66
</p>
<p>99
.4
7
4
</p>
<p>3
34
.1
16
</p>
<p>30
.8
17
</p>
<p>2
9.
4
57
</p>
<p>2
8
.7
10
</p>
<p>2
8
.2
37
</p>
<p>2
7
.9
11
</p>
<p>2
7
.6
7
2
</p>
<p>2
7
.4
8
9
</p>
<p>2
7
.3
4
5
</p>
<p>2
7
.2
2
9
</p>
<p>2
7
.0
52
</p>
<p>2
6.
8
7
2
</p>
<p>2
6.
69
0
</p>
<p>2
6.
57
9
</p>
<p>2
6.
50
5
</p>
<p>2
6.
4
11
</p>
<p>4
2
1.
19
8
</p>
<p>18
.0
00
</p>
<p>16
.6
94
</p>
<p>15
.9
7
7
</p>
<p>15
.5
2
2
</p>
<p>15
.2
07
</p>
<p>14
.9
7
6
</p>
<p>14
.7
99
</p>
<p>14
.6
59
</p>
<p>14
.5
4
6
</p>
<p>14
.3
7
4
</p>
<p>14
.1
98
</p>
<p>14
.0
2
0
</p>
<p>13
.9
11
</p>
<p>13
.8
38
</p>
<p>13
.7
4
5
</p>
<p>5
16
.2
58
</p>
<p>13
.2
7
4
</p>
<p>12
.0
60
</p>
<p>11
.3
92
</p>
<p>10
.9
67
</p>
<p>10
.6
7
2
</p>
<p>10
.4
56
</p>
<p>10
.2
8
9
</p>
<p>10
.1
58
</p>
<p>10
.0
51
</p>
<p>9.
8
8
8
</p>
<p>9.
7
2
2
</p>
<p>9.
55
3
</p>
<p>9.
4
4
9
</p>
<p>9.
37
9
</p>
<p>9.
2
91
</p>
<p>6
13
.7
4
5
</p>
<p>10
.9
2
5
</p>
<p>9.
7
8
0
</p>
<p>9.
14
8
</p>
<p>8
.7
4
6
</p>
<p>8
.4
66
</p>
<p>8
.2
60
</p>
<p>8
.1
02
</p>
<p>7
.9
7
6
</p>
<p>7
.8
7
4
</p>
<p>7
.7
18
</p>
<p>7
.5
59
</p>
<p>7
.3
96
</p>
<p>7
.2
96
</p>
<p>7
.2
2
9
</p>
<p>7
.1
4
3
</p>
<p>7
12
.2
4
6
</p>
<p>9.
54
7
</p>
<p>8
.4
51
</p>
<p>7
.8
4
7
</p>
<p>7
.4
60
</p>
<p>7
.1
91
</p>
<p>6.
99
3
</p>
<p>6.
8
4
0
</p>
<p>6.
7
19
</p>
<p>6.
62
0
</p>
<p>6.
4
69
</p>
<p>6.
31
4
</p>
<p>6.
15
5
</p>
<p>6.
05
8
</p>
<p>5.
99
2
</p>
<p>5.
90
8
</p>
<p>8
11
.2
59
</p>
<p>8
.6
4
9
</p>
<p>7
.5
91
</p>
<p>7
.0
06
</p>
<p>6.
63
2
</p>
<p>6.
37
1
</p>
<p>6.
17
8
</p>
<p>6.
02
9
</p>
<p>5.
91
1
</p>
<p>5.
8
14
</p>
<p>5.
66
7
</p>
<p>5.
51
5
</p>
<p>5.
35
9
</p>
<p>5.
2
63
</p>
<p>5.
19
8
</p>
<p>5.
11
6
</p>
<p>9
10
.5
61
</p>
<p>8
.0
2
2
</p>
<p>6.
99
2
</p>
<p>6.
4
2
2
</p>
<p>6.
05
7
</p>
<p>5.
8
02
</p>
<p>5.
61
3
</p>
<p>5.
4
67
</p>
<p>5.
35
1
</p>
<p>5.
2
57
</p>
<p>5.
11
1
</p>
<p>4
.9
62
</p>
<p>4
.8
08
</p>
<p>4
.7
13
</p>
<p>4
.6
4
9
</p>
<p>4
.5
67
</p>
<p>10
10
.0
4
4
</p>
<p>7
.5
59
</p>
<p>6.
55
2
</p>
<p>5.
99
4
</p>
<p>5.
63
6
</p>
<p>5.
38
6
</p>
<p>5.
2
00
</p>
<p>5.
05
7
</p>
<p>4
.9
4
2
</p>
<p>4
.8
4
9
</p>
<p>4
.7
06
</p>
<p>4
.5
58
</p>
<p>4
.4
05
</p>
<p>4
.3
11
</p>
<p>4
.2
4
7
</p>
<p>4
.1
65
</p>
<p>11
9.
64
6
</p>
<p>7
.2
06
</p>
<p>6.
2
17
</p>
<p>5.
66
8
</p>
<p>5.
31
6
</p>
<p>5.
06
9
</p>
<p>4
.8
8
6
</p>
<p>4
.7
4
4
</p>
<p>4
.6
32
</p>
<p>4
.5
39
</p>
<p>4
.3
97
</p>
<p>4
.2
51
</p>
<p>4
.0
99
</p>
<p>4
.0
05
</p>
<p>3.
94
1
</p>
<p>3.
8
60
</p>
<p>12
9.
33
0
</p>
<p>6.
92
7
</p>
<p>5.
95
3
</p>
<p>5.
4
12
</p>
<p>5.
06
4
</p>
<p>4
.8
2
1
</p>
<p>4
.6
4
0
</p>
<p>4
.4
99
</p>
<p>4
.3
8
8
</p>
<p>4
.2
96
</p>
<p>4
.1
55
</p>
<p>4
.0
10
</p>
<p>3.
8
58
</p>
<p>3.
7
65
</p>
<p>3.
7
01
</p>
<p>3.
61
9
</p>
<p>13
9.
07
4
</p>
<p>6.
7
01
</p>
<p>5.
7
39
</p>
<p>5.
2
05
</p>
<p>4
.8
62
</p>
<p>4
.6
2
0
</p>
<p>4
.4
4
1
</p>
<p>4
.3
02
</p>
<p>4
.1
91
</p>
<p>4
.1
00
</p>
<p>3.
96
0
</p>
<p>3.
8
15
</p>
<p>3.
66
5
</p>
<p>3.
57
1
</p>
<p>3.
50
7
</p>
<p>3.
4
2
5
</p>
<p>14
8
.8
62
</p>
<p>6.
51
5
</p>
<p>5.
56
4
</p>
<p>5.
03
5
</p>
<p>4
.6
95
</p>
<p>4
.4
56
</p>
<p>4
.2
7
8
</p>
<p>4
.1
4
0
</p>
<p>4
.0
30
</p>
<p>3.
93
9
</p>
<p>3.
8
00
</p>
<p>3.
65
6
</p>
<p>3.
50
5
</p>
<p>3.
4
12
</p>
<p>3.
34
8
</p>
<p>3.
2
66
</p>
<p>15
8
.6
8
3
</p>
<p>6.
35
9
</p>
<p>5.
4
17
</p>
<p>4
.8
93
</p>
<p>4
.5
56
</p>
<p>4
.3
18
</p>
<p>4
.1
4
2
</p>
<p>4
.0
04
</p>
<p>3.
8
95
</p>
<p>3.
8
05
</p>
<p>3.
66
6
</p>
<p>3.
52
2
</p>
<p>3.
37
2
</p>
<p>3.
2
7
8
</p>
<p>3.
2
14
</p>
<p>3.
13
2
</p>
<p>16
8
.5
31
</p>
<p>6.
2
2
6
</p>
<p>5.
2
92
</p>
<p>4
.7
7
3
</p>
<p>4
.4
37
</p>
<p>4
.2
02
</p>
<p>4
.0
2
6
</p>
<p>3.
8
90
</p>
<p>3.
7
8
0
</p>
<p>3.
69
1
</p>
<p>3.
55
3
</p>
<p>3.
4
09
</p>
<p>3.
2
59
</p>
<p>3.
16
5
</p>
<p>3.
10
1
</p>
<p>3.
01
8
</p>
<p>17
8
.4
00
</p>
<p>6.
11
2
</p>
<p>5.
18
5
</p>
<p>4
.6
69
</p>
<p>4
.3
36
</p>
<p>4
.1
02
</p>
<p>3.
92
7
</p>
<p>3.
7
91
</p>
<p>3.
68
2
</p>
<p>3.
59
3
</p>
<p>3.
4
55
</p>
<p>3.
31
2
</p>
<p>3.
16
2
</p>
<p>3.
06
8
</p>
<p>3.
00
3
</p>
<p>2
.9
2
0
</p>
<p>18
8
.2
8
5
</p>
<p>6.
01
3
</p>
<p>5.
09
2
</p>
<p>4
.5
7
9
</p>
<p>4
.2
4
8
</p>
<p>4
.0
15
</p>
<p>3.
8
4
1
</p>
<p>3.
7
05
</p>
<p>3.
59
7
</p>
<p>3.
50
8
</p>
<p>3.
37
1
</p>
<p>3.
2
2
7
</p>
<p>3.
07
7
</p>
<p>2
.9
8
3
</p>
<p>2
.9
19
</p>
<p>2
.8
35
</p>
<p>19
8
.1
8
5
</p>
<p>5.
92
6
</p>
<p>5.
01
0
</p>
<p>4
.5
00
</p>
<p>4
.1
7
1
</p>
<p>3.
93
9
</p>
<p>3.
7
65
</p>
<p>3.
63
1
</p>
<p>3.
52
3
</p>
<p>3.
4
34
</p>
<p>3.
2
97
</p>
<p>3.
15
3
</p>
<p>3.
00
3
</p>
<p>2
.9
09
</p>
<p>2
.8
4
4
</p>
<p>2
.7
61
</p>
<p>2
0
</p>
<p>8
.0
96
</p>
<p>5.
8
4
9
</p>
<p>4
.9
38
</p>
<p>4
.4
31
</p>
<p>4
.1
03
</p>
<p>3.
8
7
1
</p>
<p>3.
69
9
</p>
<p>3.
56
4
</p>
<p>3.
4
57
</p>
<p>3.
36
8
</p>
<p>3.
2
31
</p>
<p>3.
08
8
</p>
<p>2
.9
38
</p>
<p>2
.8
4
3
</p>
<p>2
.7
7
8
</p>
<p>2
.6
95
</p>
<p>2
1
</p>
<p>8
.0
17
</p>
<p>5.
7
8
0
</p>
<p>4
.8
7
4
</p>
<p>4
.3
69
</p>
<p>4
.0
4
2
</p>
<p>3.
8
12
</p>
<p>3.
64
0
</p>
<p>3.
50
6
</p>
<p>3.
39
8
</p>
<p>3.
31
0
</p>
<p>3.
17
3
</p>
<p>3.
03
0
</p>
<p>2
.8
8
0
</p>
<p>2
.7
8
5
</p>
<p>2
.7
2
0
</p>
<p>2
.6
36
</p>
<p>2
2
</p>
<p>7
.9
4
5
</p>
<p>5.
7
19
</p>
<p>4
.8
17
</p>
<p>4
.3
13
</p>
<p>3.
98
8
</p>
<p>3.
7
58
</p>
<p>3.
58
7
</p>
<p>3.
4
53
</p>
<p>3.
34
6
</p>
<p>3.
2
58
</p>
<p>3.
12
1
</p>
<p>2
.9
7
8
</p>
<p>2
.8
2
7
</p>
<p>2
.7
33
</p>
<p>2
.6
67
</p>
<p>2
.5
8
3
</p>
<p>2
3
</p>
<p>7
.8
8
1
</p>
<p>5.
66
4
</p>
<p>4
.7
65
</p>
<p>4
.2
64
</p>
<p>3.
93
9
</p>
<p>3.
7
10
</p>
<p>3.
53
9
</p>
<p>3.
4
06
</p>
<p>3.
2
99
</p>
<p>3.
2
11
</p>
<p>3.
07
4
</p>
<p>2
.9
31
</p>
<p>2
.7
8
1
</p>
<p>2
.6
8
6
</p>
<p>2
.6
2
0
</p>
<p>2
.5
35
</p>
<p>2
4
</p>
<p>7
.8
2
3
</p>
<p>5.
61
4
</p>
<p>4
.7
18
</p>
<p>4
.2
18
</p>
<p>3.
8
95
</p>
<p>3.
66
7
</p>
<p>3.
4
96
</p>
<p>3.
36
3
</p>
<p>3.
2
56
</p>
<p>3.
16
8
</p>
<p>3.
03
2
</p>
<p>2
.8
8
9
</p>
<p>2
.7
38
</p>
<p>2
.6
4
3
</p>
<p>2
.5
7
7
</p>
<p>2
.4
92
</p>
<p>2
5
</p>
<p>7
.7
7
0
</p>
<p>5.
56
8
</p>
<p>4
.6
7
5
</p>
<p>4
.1
7
7
</p>
<p>3.
8
55
</p>
<p>3.
62
7
</p>
<p>3.
4
57
</p>
<p>3.
32
4
</p>
<p>3.
2
17
</p>
<p>3.
12
9
</p>
<p>2
.9
93
</p>
<p>2
.8
50
</p>
<p>2
.6
99
</p>
<p>2
.6
04
</p>
<p>2
.5
38
</p>
<p>2
.4
53
</p>
<p>2
6
</p>
<p>7
.7
2
1
</p>
<p>5.
52
6
</p>
<p>4
.6
37
</p>
<p>4
.1
4
0
</p>
<p>3.
8
18
</p>
<p>3.
59
1
</p>
<p>3.
4
2
1
</p>
<p>3.
2
8
8
</p>
<p>3.
18
2
</p>
<p>3.
09
4
</p>
<p>2
.9
58
</p>
<p>2
.8
15
</p>
<p>2
.6
64
</p>
<p>2
.5
69
</p>
<p>2
.5
03
</p>
<p>2
.4
17
</p>
<p>2
7
</p>
<p>7
.6
7
7
</p>
<p>5.
4
8
8
</p>
<p>4
.6
01
</p>
<p>4
.1
06
</p>
<p>3.
7
8
5
</p>
<p>3.
55
8
</p>
<p>3.
38
8
</p>
<p>3.
2
56
</p>
<p>3.
14
9
</p>
<p>3.
06
2
</p>
<p>2
.9
2
6
</p>
<p>2
.7
8
3
</p>
<p>2
.6
32
</p>
<p>2
.5
36
</p>
<p>2
.4
7
0
</p>
<p>2
.3
8
4
</p>
<p>2
8
</p>
<p>7
.6
36
</p>
<p>5.
4
53
</p>
<p>4
.5
68
</p>
<p>4
.0
7
4
</p>
<p>3.
7
54
</p>
<p>3.
52
8
</p>
<p>3.
35
8
</p>
<p>3.
2
2
6
</p>
<p>3.
12
0
</p>
<p>3.
03
2
</p>
<p>2
.8
96
</p>
<p>2
.7
53
</p>
<p>2
.6
02
</p>
<p>2
.5
06
</p>
<p>2
.4
4
0
</p>
<p>2
.3
54
</p>
<p>2
9
</p>
<p>7
.5
98
</p>
<p>5.
4
2
0
</p>
<p>4
.5
38
</p>
<p>4
.0
4
5
</p>
<p>3.
7
2
5
</p>
<p>3.
4
99
</p>
<p>3.
33
0
</p>
<p>3.
19
8
</p>
<p>3.
09
2
</p>
<p>3.
00
5
</p>
<p>2
.8
68
</p>
<p>2
.7
2
6
</p>
<p>2
.5
7
4
</p>
<p>2
.4
7
8
</p>
<p>2
.4
12
</p>
<p>2
.3
2
5
</p>
<p>30
7
.5
62
</p>
<p>5.
39
0
</p>
<p>4
.5
10
</p>
<p>4
.0
18
</p>
<p>3.
69
9
</p>
<p>3.
4
7
3
</p>
<p>3.
30
4
</p>
<p>3.
17
3
</p>
<p>3.
06
7
</p>
<p>2
.9
7
9
</p>
<p>2
.8
4
3
</p>
<p>2
.7
00
</p>
<p>2
.5
4
9
</p>
<p>2
.4
53
</p>
<p>2
.3
8
6
</p>
<p>2
.2
99
</p>
<p>31
7
.5
30
</p>
<p>5.
36
2
</p>
<p>4
.4
8
4
</p>
<p>3.
99
3
</p>
<p>3.
67
5
</p>
<p>3.
4
4
9
</p>
<p>3.
2
8
1
</p>
<p>3.
14
9
</p>
<p>3.
04
3
</p>
<p>2
.9
55
</p>
<p>2
.8
2
0
</p>
<p>2
.6
7
7
</p>
<p>2
.5
2
5
</p>
<p>2
.4
2
9
</p>
<p>2
.3
62
</p>
<p>2
.2
7
5
</p>
<p>32
7
.4
99
</p>
<p>5.
33
6
</p>
<p>4
.4
59
</p>
<p>3.
96
9
</p>
<p>3.
65
2
</p>
<p>3.
4
2
7
</p>
<p>3.
2
58
</p>
<p>3.
12
7
</p>
<p>3.
02
1
</p>
<p>2
.9
34
</p>
<p>2
.7
98
</p>
<p>2
.6
55
</p>
<p>2
.5
03
</p>
<p>2
.4
06
</p>
<p>2
.3
4
0
</p>
<p>2
.2
52
</p>
<p>33
7
.4
7
1
</p>
<p>5.
31
2
</p>
<p>4
.4
37
</p>
<p>3.
94
8
</p>
<p>3.
63
0
</p>
<p>3.
4
06
</p>
<p>3.
2
38
</p>
<p>3.
10
6
</p>
<p>3.
00
0
</p>
<p>2
.9
13
</p>
<p>2
.7
7
7
</p>
<p>2
.6
34
</p>
<p>2
.4
8
2
</p>
<p>2
.3
8
6
</p>
<p>2
.3
19
</p>
<p>2
.2
31
</p>
<p>34
7
.4
4
4
</p>
<p>5.
2
8
9
</p>
<p>4
.4
16
</p>
<p>3.
92
7
</p>
<p>3.
61
1
</p>
<p>3.
38
6
</p>
<p>3.
2
18
</p>
<p>3.
08
7
</p>
<p>2
.9
8
1
</p>
<p>2
.8
94
</p>
<p>2
.7
58
</p>
<p>2
.6
15
</p>
<p>2
.4
63
</p>
<p>2
.3
66
</p>
<p>2
.2
99
</p>
<p>2
.2
11
</p>
<p>35
7
.4
19
</p>
<p>5.
2
68
</p>
<p>4
.3
96
</p>
<p>3.
90
8
</p>
<p>3.
59
2
</p>
<p>3.
36
8
</p>
<p>3.
2
00
</p>
<p>3.
06
9
</p>
<p>2
.9
63
</p>
<p>2
.8
7
6
</p>
<p>2
.7
4
0
</p>
<p>2
.5
97
</p>
<p>2
.4
4
5
</p>
<p>2
.3
4
8
</p>
<p>2
.2
8
1
</p>
<p>2
.1
93
</p>
<p>36
7
.3
96
</p>
<p>5.
2
4
8
</p>
<p>4
.3
7
7
</p>
<p>3.
8
90
</p>
<p>3.
57
4
</p>
<p>3.
35
1
</p>
<p>3.
18
3
</p>
<p>3.
05
2
</p>
<p>2
.9
4
6
</p>
<p>2
.8
59
</p>
<p>2
.7
2
3
</p>
<p>2
.5
8
0
</p>
<p>2
.4
2
8
</p>
<p>2
.3
31
</p>
<p>2
.2
63
</p>
<p>2
.1
7
5
</p>
<p>37
7
.3
7
3
</p>
<p>5.
2
2
9
</p>
<p>4
.3
60
</p>
<p>3.
8
7
3
</p>
<p>3.
55
8
</p>
<p>3.
33
4
</p>
<p>3.
16
7
</p>
<p>3.
03
6
</p>
<p>2
.9
30
</p>
<p>2
.8
4
3
</p>
<p>2
.7
07
</p>
<p>2
.5
64
</p>
<p>2
.4
12
</p>
<p>2
.3
15
</p>
<p>2
.2
4
7
</p>
<p>2
.1
59
</p>
<p>38
7
.3
53
</p>
<p>5.
2
11
</p>
<p>4
.3
4
3
</p>
<p>3.
8
58
</p>
<p>3.
54
2
</p>
<p>3.
31
9
</p>
<p>3.
15
2
</p>
<p>3.
02
1
</p>
<p>2
.9
15
</p>
<p>2
.8
2
8
</p>
<p>2
.6
92
</p>
<p>2
.5
4
9
</p>
<p>2
.3
97
</p>
<p>2
.2
99
</p>
<p>2
.2
32
</p>
<p>2
.1
4
3
</p>
<p>39
7
.3
33
</p>
<p>5.
19
4
</p>
<p>4
.3
2
7
</p>
<p>3.
8
4
3
</p>
<p>3.
52
8
</p>
<p>3.
30
5
</p>
<p>3.
13
7
</p>
<p>3.
00
6
</p>
<p>2
.9
01
</p>
<p>2
.8
14
</p>
<p>2
.6
7
8
</p>
<p>2
.5
35
</p>
<p>2
.3
8
2
</p>
<p>2
.2
8
5
</p>
<p>2
.2
17
</p>
<p>2
.1
2
8
</p>
<p>4
0
</p>
<p>7
.3
14
</p>
<p>5.
17
9
</p>
<p>4
.3
13
</p>
<p>3.
8
2
8
</p>
<p>3.
51
4
</p>
<p>3.
2
91
</p>
<p>3.
12
4
</p>
<p>2
.9
93
</p>
<p>2
.8
8
8
</p>
<p>2
.8
01
</p>
<p>2
.6
65
</p>
<p>2
.5
2
2
</p>
<p>2
.3
69
</p>
<p>2
.2
7
1
</p>
<p>2
.2
03
</p>
<p>2
.1
14
</p>
<p>S
o
u
rc
e
:
T
h
e
S
A
S
</p>
<p>r &copy;
fu
n
ct
io
n
F
IN
</p>
<p>V
w
a
s
u
se
d
to
</p>
<p>g
en
</p>
<p>er
a
te
</p>
<p>th
is
</p>
<p>ta
b
le
.
v
1
=
</p>
<p>n
u
m
er
a
to
r
d
eg
re
es
</p>
<p>of
fr
ee
d
om
</p>
<p>v
2
=
</p>
<p>d
en
</p>
<p>om
in
a
to
r
d
eg
re
es
</p>
<p>of
fr
ee
d
om</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 401
</p>
<p>T
a
b
le
</p>
<p>E
R
ig
h
t-
T
a
il
C
ri
ti
ca
l
V
a
lu
es
</p>
<p>fo
r
th
e
C
h
i-
S
q
u
a
re
</p>
<p>D
is
tr
ib
u
ti
o
n
</p>
<p>P
r[
χ
2 5
&gt;
</p>
<p>1
1
.0
7
0
5
]
=
</p>
<p>0
.0
5
</p>
<p>v
.9
9
5
</p>
<p>.9
9
0
</p>
<p>.9
7
5
</p>
<p>.9
5
0
</p>
<p>.9
0
</p>
<p>.5
0
</p>
<p>.1
0
</p>
<p>.0
5
</p>
<p>.0
2
5
</p>
<p>.0
1
</p>
<p>.0
0
5
</p>
<p>1
0.
00
00
4
</p>
<p>0.
00
01
6
</p>
<p>0.
00
09
8
</p>
<p>0.
00
39
3
</p>
<p>0.
01
57
9
</p>
<p>0.
4
54
94
</p>
<p>2
.7
05
54
</p>
<p>3.
8
4
14
6
</p>
<p>5.
02
38
9
</p>
<p>6.
63
4
90
</p>
<p>7
.8
7
94
4
</p>
<p>2
0.
01
00
3
</p>
<p>0.
02
01
0
</p>
<p>0.
05
06
4
</p>
<p>0.
10
2
59
</p>
<p>0.
2
10
7
2
</p>
<p>1.
38
62
9
</p>
<p>4
.6
05
17
</p>
<p>5.
99
14
6
</p>
<p>7
.3
7
7
7
6
</p>
<p>9.
2
10
34
</p>
<p>10
.5
96
6
</p>
<p>3
0.
07
17
2
</p>
<p>0.
11
4
8
3
</p>
<p>0.
2
15
8
0
</p>
<p>0.
35
18
5
</p>
<p>0.
58
4
37
</p>
<p>2
.3
65
97
</p>
<p>6.
2
51
39
</p>
<p>7
.8
14
7
3
</p>
<p>9.
34
8
4
0
</p>
<p>11
.3
4
4
9
</p>
<p>12
.8
38
2
</p>
<p>4
0.
2
06
99
</p>
<p>0.
2
97
11
</p>
<p>0.
4
8
4
4
2
</p>
<p>0.
7
10
7
2
</p>
<p>1.
06
36
2
</p>
<p>3.
35
66
9
</p>
<p>7
.7
7
94
4
</p>
<p>9.
4
8
7
7
3
</p>
<p>11
.1
4
33
</p>
<p>13
.2
7
67
</p>
<p>14
.8
60
3
</p>
<p>5
0.
4
11
7
4
</p>
<p>0.
55
4
30
</p>
<p>0.
8
31
2
1
</p>
<p>1.
14
54
8
</p>
<p>1.
61
03
1
</p>
<p>4
.3
51
4
6
</p>
<p>9.
2
36
36
</p>
<p>11
.0
7
05
</p>
<p>12
.8
32
5
</p>
<p>15
.0
8
63
</p>
<p>16
.7
4
96
</p>
<p>6
0.
67
57
3
</p>
<p>0.
8
7
2
09
</p>
<p>1.
2
37
34
</p>
<p>1.
63
53
8
</p>
<p>2
.2
04
13
</p>
<p>5.
34
8
12
</p>
<p>10
.6
4
4
6
</p>
<p>12
.5
91
6
</p>
<p>14
.4
4
94
</p>
<p>16
.8
11
9
</p>
<p>18
.5
4
7
6
</p>
<p>7
0.
98
92
6
</p>
<p>1.
2
39
04
</p>
<p>1.
68
98
7
</p>
<p>2
.1
67
35
</p>
<p>2
.8
33
11
</p>
<p>6.
34
58
1
</p>
<p>12
.0
17
0
</p>
<p>14
.0
67
1
</p>
<p>16
.0
12
8
</p>
<p>18
.4
7
53
</p>
<p>2
0.
2
7
7
7
</p>
<p>8
1.
34
4
4
1
</p>
<p>1.
64
65
0
</p>
<p>2
.1
7
97
3
</p>
<p>2
.7
32
64
</p>
<p>3.
4
8
95
4
</p>
<p>7
.3
4
4
12
</p>
<p>13
.3
61
6
</p>
<p>15
.5
07
3
</p>
<p>17
.5
34
5
</p>
<p>2
0.
09
02
</p>
<p>2
1.
95
50
</p>
<p>9
1.
7
34
93
</p>
<p>2
.0
8
7
90
</p>
<p>2
.7
00
39
</p>
<p>3.
32
51
1
</p>
<p>4
.1
68
16
</p>
<p>8
.3
4
2
8
3
</p>
<p>14
.6
8
37
</p>
<p>16
.9
19
0
</p>
<p>19
.0
2
2
8
</p>
<p>2
1.
66
60
</p>
<p>2
3.
58
94
</p>
<p>10
2
.1
55
8
6
</p>
<p>2
.5
58
2
1
</p>
<p>3.
2
4
69
7
</p>
<p>3.
94
03
0
</p>
<p>4
.8
65
18
</p>
<p>9.
34
18
2
</p>
<p>15
.9
8
7
2
</p>
<p>18
.3
07
0
</p>
<p>2
0.
4
8
32
</p>
<p>2
3.
2
09
3
</p>
<p>2
5.
18
8
2
</p>
<p>11
2
.6
03
2
2
</p>
<p>3.
05
34
8
</p>
<p>3.
8
15
7
5
</p>
<p>4
.5
7
4
8
1
</p>
<p>5.
57
7
7
8
</p>
<p>10
.3
4
10
</p>
<p>17
.2
7
50
</p>
<p>19
.6
7
51
</p>
<p>2
1.
92
00
</p>
<p>2
4
.7
2
50
</p>
<p>2
6.
7
56
8
</p>
<p>12
3.
07
38
2
</p>
<p>3.
57
05
7
</p>
<p>4
.4
03
7
9
</p>
<p>5.
2
2
60
3
</p>
<p>6.
30
38
0
</p>
<p>11
.3
4
03
</p>
<p>18
.5
4
93
</p>
<p>2
1.
02
61
</p>
<p>2
3.
33
67
</p>
<p>2
6.
2
17
0
</p>
<p>2
8
.2
99
5
</p>
<p>13
3.
56
50
3
</p>
<p>4
.1
06
92
</p>
<p>5.
00
8
7
5
</p>
<p>5.
8
91
8
6
</p>
<p>7
.0
4
15
0
</p>
<p>12
.3
39
8
</p>
<p>19
.8
11
9
</p>
<p>2
2
.3
62
0
</p>
<p>2
4
.7
35
6
</p>
<p>2
7
.6
8
8
2
</p>
<p>2
9.
8
19
5
</p>
<p>14
4
.0
7
4
67
</p>
<p>4
.6
60
4
3
</p>
<p>5.
62
8
7
3
</p>
<p>6.
57
06
3
</p>
<p>7
.7
8
95
3
</p>
<p>13
.3
39
3
</p>
<p>2
1.
06
4
1
</p>
<p>2
3.
68
4
8
</p>
<p>2
6.
11
8
9
</p>
<p>2
9.
14
12
</p>
<p>31
.3
19
3
</p>
<p>15
4
.6
00
92
</p>
<p>5.
2
2
93
5
</p>
<p>6.
2
62
14
</p>
<p>7
.2
60
94
</p>
<p>8
.5
4
67
6
</p>
<p>14
.3
38
9
</p>
<p>2
2
.3
07
1
</p>
<p>2
4
.9
95
8
</p>
<p>2
7
.4
8
8
4
</p>
<p>30
.5
7
7
9
</p>
<p>32
.8
01
3
</p>
<p>16
5.
14
2
2
1
</p>
<p>5.
8
12
2
1
</p>
<p>6.
90
7
66
</p>
<p>7
.9
61
65
</p>
<p>9.
31
2
2
4
</p>
<p>15
.3
38
5
</p>
<p>2
3.
54
18
</p>
<p>2
6.
2
96
2
</p>
<p>2
8
.8
4
54
</p>
<p>31
.9
99
9
</p>
<p>34
.2
67
2
</p>
<p>17
5.
69
7
2
2
</p>
<p>6.
4
07
7
6
</p>
<p>7
.5
64
19
</p>
<p>8
.6
7
17
6
</p>
<p>10
.0
8
52
</p>
<p>16
.3
38
2
</p>
<p>2
4
.7
69
0
</p>
<p>2
7
.5
8
7
1
</p>
<p>30
.1
91
0
</p>
<p>33
.4
08
7
</p>
<p>35
.7
18
5
</p>
<p>18
6.
2
64
8
0
</p>
<p>7
.0
14
91
</p>
<p>8
.2
30
7
5
</p>
<p>9.
39
04
6
</p>
<p>10
.8
64
9
</p>
<p>17
.3
37
9
</p>
<p>2
5.
98
94
</p>
<p>2
8
.8
69
3
</p>
<p>31
.5
2
64
</p>
<p>34
.8
05
3
</p>
<p>37
.1
56
5
</p>
<p>19
6.
8
4
39
7
</p>
<p>7
.6
32
7
3
</p>
<p>8
.9
06
52
</p>
<p>10
.1
17
0
</p>
<p>11
.6
50
9
</p>
<p>18
.3
37
7
</p>
<p>2
7
.2
03
6
</p>
<p>30
.1
4
35
</p>
<p>32
.8
52
3
</p>
<p>36
.1
90
9
</p>
<p>38
.5
8
2
3
</p>
<p>2
0
</p>
<p>7
.4
33
8
4
</p>
<p>8
.2
60
4
0
</p>
<p>9.
59
07
8
</p>
<p>10
.8
50
8
</p>
<p>12
.4
4
2
6
</p>
<p>19
.3
37
4
</p>
<p>2
8
.4
12
0
</p>
<p>31
.4
10
4
</p>
<p>34
.1
69
6
</p>
<p>37
.5
66
2
</p>
<p>39
.9
96
8
</p>
<p>2
1
</p>
<p>8
.0
33
65
</p>
<p>8
.8
97
2
0
</p>
<p>10
.2
8
2
9
</p>
<p>11
.5
91
3
</p>
<p>13
.2
39
6
</p>
<p>2
0.
33
7
2
</p>
<p>2
9.
61
51
</p>
<p>32
.6
7
06
</p>
<p>35
.4
7
8
9
</p>
<p>38
.9
32
2
</p>
<p>4
1.
4
01
1
</p>
<p>2
2
</p>
<p>8
.6
4
2
7
2
</p>
<p>9.
54
2
4
9
</p>
<p>10
.9
8
2
3
</p>
<p>12
.3
38
0
</p>
<p>14
.0
4
15
</p>
<p>2
1.
33
7
0
</p>
<p>30
.8
13
3
</p>
<p>33
.9
2
4
4
</p>
<p>36
.7
8
07
</p>
<p>4
0.
2
8
94
</p>
<p>4
2
.7
95
7
</p>
<p>2
3
</p>
<p>9.
2
60
4
2
</p>
<p>10
.1
95
7
</p>
<p>11
.6
8
8
6
</p>
<p>13
.0
90
5
</p>
<p>14
.8
4
8
0
</p>
<p>2
2
.3
36
9
</p>
<p>32
.0
06
9
</p>
<p>35
.1
7
2
5
</p>
<p>38
.0
7
56
</p>
<p>4
1.
63
8
4
</p>
<p>4
4
.1
8
13
</p>
<p>2
4
</p>
<p>9.
8
8
62
3
</p>
<p>10
.8
56
4
</p>
<p>12
.4
01
2
</p>
<p>13
.8
4
8
4
</p>
<p>15
.6
58
7
</p>
<p>2
3.
33
67
</p>
<p>33
.1
96
2
</p>
<p>36
.4
15
0
</p>
<p>39
.3
64
1
</p>
<p>4
2
.9
7
98
</p>
<p>4
5.
55
8
5
</p>
<p>2
5
</p>
<p>10
.5
19
7
</p>
<p>11
.5
2
4
0
</p>
<p>13
.1
19
7
</p>
<p>14
.6
11
4
</p>
<p>16
.4
7
34
</p>
<p>2
4
.3
36
6
</p>
<p>34
.3
8
16
</p>
<p>37
.6
52
5
</p>
<p>4
0.
64
65
</p>
<p>4
4
.3
14
1
</p>
<p>4
6.
92
7
9
</p>
<p>2
6
</p>
<p>11
.1
60
2
</p>
<p>12
.1
98
1
</p>
<p>13
.8
4
39
</p>
<p>15
.3
7
92
</p>
<p>17
.2
91
9
</p>
<p>2
5.
33
65
</p>
<p>35
.5
63
2
</p>
<p>38
.8
8
51
</p>
<p>4
1.
92
32
</p>
<p>4
5.
64
17
</p>
<p>4
8
.2
8
99
</p>
<p>2
7
</p>
<p>11
.8
07
6
</p>
<p>12
.8
7
8
5
</p>
<p>14
.5
7
34
</p>
<p>16
.1
51
4
</p>
<p>18
.1
13
9
</p>
<p>2
6.
33
63
</p>
<p>36
.7
4
12
</p>
<p>4
0.
11
33
</p>
<p>4
3.
19
4
5
</p>
<p>4
6.
96
2
9
</p>
<p>4
9.
64
4
9
</p>
<p>2
8
</p>
<p>12
.4
61
3
</p>
<p>13
.5
64
7
</p>
<p>15
.3
07
9
</p>
<p>16
.9
2
7
9
</p>
<p>18
.9
39
2
</p>
<p>2
7
.3
36
2
</p>
<p>37
.9
15
9
</p>
<p>4
1.
33
7
1
</p>
<p>4
4
.4
60
8
</p>
<p>4
8
.2
7
8
2
</p>
<p>50
.9
93
4
</p>
<p>2
9
</p>
<p>13
.1
2
11
</p>
<p>14
.2
56
5
</p>
<p>16
.0
4
7
1
</p>
<p>17
.7
08
4
</p>
<p>19
.7
67
7
</p>
<p>2
8
.3
36
1
</p>
<p>39
.0
8
7
5
</p>
<p>4
2
.5
57
0
</p>
<p>4
5.
7
2
2
3
</p>
<p>4
9.
58
7
9
</p>
<p>52
.3
35
6
</p>
<p>30
13
.7
8
67
</p>
<p>14
.9
53
5
</p>
<p>16
.7
90
8
</p>
<p>18
.4
92
7
</p>
<p>2
0.
59
92
</p>
<p>2
9.
33
60
</p>
<p>4
0.
2
56
0
</p>
<p>4
3.
7
7
30
</p>
<p>4
6.
97
92
</p>
<p>50
.8
92
2
</p>
<p>53
.6
7
2
0
</p>
<p>31
14
.4
57
8
</p>
<p>15
.6
55
5
</p>
<p>17
.5
38
7
</p>
<p>19
.2
8
06
</p>
<p>2
1.
4
33
6
</p>
<p>30
.3
35
9
</p>
<p>4
1.
4
2
17
</p>
<p>4
4
.9
8
53
</p>
<p>4
8
.2
31
9
</p>
<p>52
.1
91
4
</p>
<p>55
.0
02
7
</p>
<p>32
15
.1
34
0
</p>
<p>16
.3
62
2
</p>
<p>18
.2
90
8
</p>
<p>2
0.
07
19
</p>
<p>2
2
.2
7
06
</p>
<p>31
.3
35
9
</p>
<p>4
2
.5
8
4
7
</p>
<p>4
6.
19
4
3
</p>
<p>4
9.
4
8
04
</p>
<p>53
.4
8
58
</p>
<p>56
.3
2
8
1
</p>
<p>33
15
.8
15
3
</p>
<p>17
.0
7
35
</p>
<p>19
.0
4
67
</p>
<p>2
0.
8
66
5
</p>
<p>2
3.
11
02
</p>
<p>32
.3
35
8
</p>
<p>4
3.
7
4
52
</p>
<p>4
7
.3
99
9
</p>
<p>50
.7
2
51
</p>
<p>54
.7
7
55
</p>
<p>57
.6
4
8
4
</p>
<p>34
16
.5
01
3
</p>
<p>17
.7
8
91
</p>
<p>19
.8
06
3
</p>
<p>2
1.
66
4
3
</p>
<p>2
3.
95
2
3
</p>
<p>33
.3
35
7
</p>
<p>4
4
.9
03
2
</p>
<p>4
8
.6
02
4
</p>
<p>51
.9
66
0
</p>
<p>56
.0
60
9
</p>
<p>58
.9
63
9
</p>
<p>35
17
.1
91
8
</p>
<p>18
.5
08
9
</p>
<p>2
0.
56
94
</p>
<p>2
2
.4
65
0
</p>
<p>2
4
.7
96
7
</p>
<p>34
.3
35
6
</p>
<p>4
6.
05
8
8
</p>
<p>4
9.
8
01
8
</p>
<p>53
.2
03
3
</p>
<p>57
.3
4
2
1
</p>
<p>60
.2
7
4
8
</p>
<p>36
17
.8
8
67
</p>
<p>19
.2
32
7
</p>
<p>2
1.
33
59
</p>
<p>2
3.
2
68
6
</p>
<p>2
5.
64
33
</p>
<p>35
.3
35
6
</p>
<p>4
7
.2
12
2
</p>
<p>50
.9
98
5
</p>
<p>54
.4
37
3
</p>
<p>58
.6
19
2
</p>
<p>61
.5
8
12
</p>
<p>37
18
.5
8
58
</p>
<p>19
.9
60
2
</p>
<p>2
2
.1
05
6
</p>
<p>2
4
.0
7
4
9
</p>
<p>2
6.
4
92
1
</p>
<p>36
.3
35
5
</p>
<p>4
8
.3
63
4
</p>
<p>52
.1
92
3
</p>
<p>55
.6
68
0
</p>
<p>59
.8
92
5
</p>
<p>62
.8
8
33
</p>
<p>38
19
.2
8
8
9
</p>
<p>2
0.
69
14
</p>
<p>2
2
.8
7
8
5
</p>
<p>2
4
.8
8
39
</p>
<p>2
7
.3
4
30
</p>
<p>37
.3
35
5
</p>
<p>4
9.
51
2
6
</p>
<p>53
.3
8
35
</p>
<p>56
.8
95
5
</p>
<p>61
.1
62
1
</p>
<p>64
.1
8
14
</p>
<p>39
19
.9
95
9
</p>
<p>2
1.
4
2
62
</p>
<p>2
3.
65
4
3
</p>
<p>2
5.
69
54
</p>
<p>2
8
.1
95
8
</p>
<p>38
.3
35
4
</p>
<p>50
.6
59
8
</p>
<p>54
.5
7
2
2
</p>
<p>58
.1
2
01
</p>
<p>62
.4
2
8
1
</p>
<p>65
.4
7
56
</p>
<p>4
0
</p>
<p>2
0.
7
06
5
</p>
<p>2
2
.1
64
3
</p>
<p>2
4
.4
33
0
</p>
<p>2
6.
50
93
</p>
<p>2
9.
05
05
</p>
<p>39
.3
35
3
</p>
<p>51
.8
05
1
</p>
<p>55
.7
58
5
</p>
<p>59
.3
4
17
</p>
<p>63
.6
90
7
</p>
<p>66
.7
66
0
</p>
<p>S
o
u
rc
e
:
T
h
e
S
A
S
</p>
<p>r &copy;
fu
n
ct
io
n
C
IN
</p>
<p>V
w
a
s
u
se
d
to
</p>
<p>g
en
</p>
<p>er
a
te
</p>
<p>th
is
ta
b
le
.
v
d
en
</p>
<p>ot
es
</p>
<p>th
e
d
eg
re
es
</p>
<p>of
fr
ee
d
om
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>List of Figures
</p>
<p>2.1 Efficiency Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
</p>
<p>2.2 Bias Versus Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
</p>
<p>2.3 Type I and II Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
</p>
<p>2.4 Critical Region for Testing μ0 = 2 against μ1 = 4 for n = 4 . . . . . . . . . . . . . . 24
</p>
<p>2.5 Critical Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
</p>
<p>2.6 Wald Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
</p>
<p>2.7 LM Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
</p>
<p>2.8 Log (Wage) Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
</p>
<p>2.9 Weeks Worked Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
</p>
<p>2.10 Years of Education Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>2.11 Years of Experience Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>2.12 Log (Wage) Versus Experience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>2.13 Log (Wage) Versus Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>2.14 Log (Wage) Versus Weeks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>2.15 Poisson Probability Distribution, Mean = 15 . . . . . . . . . . . . . . . . . . . . . . 46
</p>
<p>2.16 Poisson Probability Distribution, Mean = 1.5 . . . . . . . . . . . . . . . . . . . . . . 46
</p>
<p>3.1 &lsquo;True&rsquo; Consumption Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
</p>
<p>3.2 Estimated Consumption Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
</p>
<p>3.3 Consumption Function with Cov(X,u) &gt; 0 . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>3.4 Random Disturbances Around the Regression . . . . . . . . . . . . . . . . . . . . . . 54
</p>
<p>3.5 95% Confidence Bands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
</p>
<p>3.6 Positively Correlated Residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
</p>
<p>3.7 Residual Variation Growing with X . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
</p>
<p>3.8 Residual Plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
</p>
<p>3.9 Residuals Versus LNP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
</p>
<p>3.10 95% Confidence Band for Predicted Values . . . . . . . . . . . . . . . . . . . . . . . 67
</p>
<p>5.1 Plots of Residuals Versus Log Y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
</p>
<p>5.2 Normality Test (Jarque-Bera) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>5.3 Residual Plot: Consumption Regression . . . . . . . . . . . . . . . . . . . . . . . . . 118
</p>
<p>6.1 Linear Distributed Lag . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
</p>
<p>6.2 A Polynomial Lag with End Point Constraints . . . . . . . . . . . . . . . . . . . . . 134
</p>
<p>7.1 The Orthogonal Decomposition of y . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
</p>
<p>8.1 CUSUM Critical Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
</p>
<p>8.2 CUSUM Plot of the Consumption Regression . . . . . . . . . . . . . . . . . . . . . . 196
</p>
<p>8.3 The Rainbow Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
</p>
<p>13.1 Linear Probability Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
</p>
<p>13.2 Truncated Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
</p>
<p>13.3 Truncated Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360</p>
<p/>
</div>
<div class="page"><p/>
<p>404 List of Figures
</p>
<p>14.1 U.S. Consumption and Income, 1959&ndash;2007 . . . . . . . . . . . . . . . . . . . . . . . 373
14.2 Correlogram of Consumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374
14.3 AR(1) Process, ρ = 0.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
14.4 Correlogram of AR(1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
14.5 MA(1) Process, θ = 0.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
14.6 Correlogram of MA(1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
14.7 Correlogram of First Difference of Consumption . . . . . . . . . . . . . . . . . . . . 378
14.8 Random Walk Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
14.9 Correlogram of a Random Walk Process . . . . . . . . . . . . . . . . . . . . . . . . . 381</p>
<p/>
</div>
<div class="page"><p/>
<p>List of Tables
</p>
<p>2.1 Descriptive Statistics for the Earnings Data . . . . . . . . . . . . . . . . . . . . . . . 31
2.2 Test for the Difference in Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.3 Correlation Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
</p>
<p>3.1 Simple Regression Computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.2 Cigarette Consumption Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.3 Cigarette Consumption Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.4 Energy Data for 20 countries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
</p>
<p>4.1 Earnings Regression for 1982 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
4.2 U.S. Gasoline Data: 1950&ndash;1987 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
</p>
<p>5.1 White Heteroskedasticity Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.2 White Heteroskedasticity-Consistent Standard Errors . . . . . . . . . . . . . . . . . 109
5.3 U.S. Consumption Data, 1959&ndash;2007 . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.4 Breusch-Godfrey LM Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
5.5 Cochrane-Orcutt AR(1) Regression &ndash; Twostep . . . . . . . . . . . . . . . . . . . . . 119
5.6 The Iterative Prais-Winsten AR(1) Regression . . . . . . . . . . . . . . . . . . . . . 119
5.7 The Newey-West HAC Standard Errors . . . . . . . . . . . . . . . . . . . . . . . . . 120
</p>
<p>6.1 Regression with Arithmetic Lag Restriction . . . . . . . . . . . . . . . . . . . . . . . 135
6.2 Almon Polynomial, r = 2, s = 5 and Near End-Point Constraint . . . . . . . . . . . 135
6.3 Almon Polynomial, r = 2, s = 5 and Far End-Point Constraint . . . . . . . . . . . . 136
</p>
<p>8.1 Cigarette Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
8.2 Diagnostic Statistics for the Cigarettes Example . . . . . . . . . . . . . . . . . . . . 188
8.3 Regression of Real Per-Capita Consumption of Cigarettes . . . . . . . . . . . . . . . 189
8.4 Chow Forecast Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
8.5 Recursive Residuals for the Consumption Regression . . . . . . . . . . . . . . . . . . 195
8.6 Non-nested Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
8.7 Ramsey RESET Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
8.8 Consumption Regression 1971&ndash;1995 . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
8.9 Artificial Regression to compute the PSW Differencing Test . . . . . . . . . . . . . . 205
8.10 Non-nested J and JA Tests for the Consumption Regression . . . . . . . . . . . . . 206
</p>
<p>10.1 Growth and Inequality: SUR Estimates . . . . . . . . . . . . . . . . . . . . . . . . . 250
</p>
<p>11.1 Two-Stage Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
11.2 Least Squares Estimates: Crime in North Carolina . . . . . . . . . . . . . . . . . . . 278
11.3 Instrumental Variables (2SLS) Regression: Crime in North Carolina . . . . . . . . . 279
11.4 Hausman&rsquo;s Test: Crime in North Carolina . . . . . . . . . . . . . . . . . . . . . . . . 280
11.5 Robust Variance Covariance (2SLS) Regression: Crime in North Carolina . . . . . . 282
11.6 First Stage Regression: Probability of Arrest . . . . . . . . . . . . . . . . . . . . . . 283
11.7 First Stage Regression: Police per Capita . . . . . . . . . . . . . . . . . . . . . . . . 284
11.8 Weak IV Diagnostics: The Crime Example . . . . . . . . . . . . . . . . . . . . . . . 285</p>
<p/>
</div>
<div class="page"><p/>
<p>406 List of Tables
</p>
<p>11.9 Growth and Inequality: 3SLS Estimates . . . . . . . . . . . . . . . . . . . . . . . . . 286
</p>
<p>12.1 Fixed Effects Estimator &ndash; Gasoline Demand Data . . . . . . . . . . . . . . . . . . . 314
12.2 Between Estimator &ndash; Gasoline Demand Data . . . . . . . . . . . . . . . . . . . . . . 314
12.3 Random Effects Estimator &ndash; Gasoline Demand Data . . . . . . . . . . . . . . . . . . 314
12.4 Gasoline Demand Data. One-way Error Component Results . . . . . . . . . . . . . . 315
12.5 Gasoline Demand Data. Wallace and Hussain (1969) Estimator . . . . . . . . . . . . 315
12.6 Gasoline Demand Data. Wansbeek and Kapteyn (1989) Estimator . . . . . . . . . . 316
12.7 Gasoline Demand Data. Random Effects Maximum Likelihood Estimator . . . . . . 316
12.8 Dynamic Demand for Cigarettes: 1963&ndash;92 . . . . . . . . . . . . . . . . . . . . . . . . 326
</p>
<p>13.1 Grouped Logit, Beer Tax and Motor Vehicle Fatality . . . . . . . . . . . . . . . . . 338
13.2 Logit Quasi-MLE of Participation Rates in 401(K) Plan . . . . . . . . . . . . . . . . 340
13.3 Comparison of the Linear Probability, Logit and Probit Models:
</p>
<p>Union Participation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
13.4 Probit Estimates: Union Participation . . . . . . . . . . . . . . . . . . . . . . . . . . 347
13.5 Actual Versus Predicted: Union Participation . . . . . . . . . . . . . . . . . . . . . . 347
13.6 Probit Estimates: Employment and Problem Drinking . . . . . . . . . . . . . . . . . 349
13.7 Marginal Effects: Employment and Problem Drinking . . . . . . . . . . . . . . . . . 350
13.8 Average Marginal Effects: Employment and Problem Drinking . . . . . . . . . . . . 351
13.9 Actual vs Predicted: Employment and Problem Drinking . . . . . . . . . . . . . . . 351
13.10 Marginal Effects: Fertility and Same Sex of Previous Children . . . . . . . . . . . . 352
13.11 Corporate Bond Rating . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
13.12 Ordered Logit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
13.13 Predicted Bond Rating . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
13.14 Multinomial Logit Results: Problem Drinking . . . . . . . . . . . . . . . . . . . . . . 357
</p>
<p>14.1 Dickey-Fuller Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
14.2 Johansen Cointegration Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
14.3 GARCH (1,1) model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
</p>
<p>Area under the Standard Normal Distribution . . . . . . . . . . . . . . . . . . . . . 397
Right-Tail Critical Values for the t-Distribution . . . . . . . . . . . . . . . . . . . . . 398
Right-Tail Critical Values for the F-Distribution: Upper 5% Points . . . . . . . . . . 399
Right-Tail Critical Values for the F-Distribution: Upper 1% Points . . . . . . . . . . 400
Right-Tail Critical Values for the Chi-Square Distribution . . . . . . . . . . . . . . . 401
</p>
<p>A
B
C
D
E</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>Aggregation, 101
Almon lag, 133, 136, 137, 144, 147
AR(1) process, 110, 115, 121&ndash;123, 127, 128,
</p>
<p>225, 234, 375, 379, 381, 385
ARCH, 8, 373, 388, 389
ARIMA, 375, 377
Asymptotically unbiased, 18, 19, 57, 328
Autocorrelation, 110, 112, 114, 115, 122,
</p>
<p>124&ndash;127, 129, 146, 223, 230, 231,
236, 238, 321, 374&ndash;379, 381, 390,
391, 394
</p>
<p>Autoregressive Distributed Lag, 143
</p>
<p>Bartlett&rsquo;s test, 104, 127
Bernoulli distribution, 13, 15, 16, 28, 36, 43,
</p>
<p>45, 46, 339&ndash;341
Best Linear Unbiased (BLUE), 56, 60, 69,
</p>
<p>74, 78, 80, 87, 95, 98&ndash;102, 112, 113,
121, 124, 131, 153, 154, 158&ndash;160,
167, 223, 224, 227, 237, 241, 242,
306, 308, 312, 320
</p>
<p>Best Linear Unbiased Predictor (BLUP), 60,
159, 227, 233, 313
</p>
<p>Best Quadratic Unbiased (BQU), 310
Beta distribution, 13, 39
Between estimator, 311, 313, 314, 329, 330
Binary Response Model Regression, 342, 343
Binomial distribution, 13, 22, 29, 36, 37, 40,
</p>
<p>44, 192, 337
Box-Cox model, 213, 214, 219, 220
Box-Jenkins, 373, 375, 377, 378, 381
Breusch-Godfrey, 115, 118, 125, 126, 140,
</p>
<p>145
Breusch-Pagan, 105, 106, 125, 126, 129, 248,
</p>
<p>319, 320, 329
</p>
<p>Censored Normal Distribution, 364, 370
Censored regression model, 356, 362
Central Limit Theorem, 42, 44&ndash;46, 74, 98,
</p>
<p>392
Change of variable, 44, 158
Characteristic roots, 174, 175, 232, 312
Characteristic vectors, 174, 175
Chebyshev&rsquo;s inequality, 19, 36
</p>
<p>Chow, 84, 90, 91, 136, 144, 164, 165, 169,
172, 181, 183, 191, 193, 197, 274,
308, 317&ndash;319, 348, 365
</p>
<p>Classical assumptions, 50, 53, 54, 62, 73, 82,
87, 95, 99, 131, 152, 153
</p>
<p>Cointegration, 8, 384&ndash;387, 394, 395
</p>
<p>Concentrated log-likelihood, 231, 312
</p>
<p>Confidence intervals, 13, 31, 58, 60, 156, 160
</p>
<p>Consistency, 19, 55, 83, 112, 114, 238, 244,
260, 263, 265, 277, 305, 321, 355
</p>
<p>Constant returns to scale, 80, 87, 160, 299
</p>
<p>Cook&rsquo;s statistic, 187, 216
</p>
<p>Cramér-Rao lower bound, 16&ndash;18, 20, 37, 38,
57, 78, 157, 158, 312
</p>
<p>CUSUM, 193, 194, 196, 217
</p>
<p>CUSUMSQ, 193
</p>
<p>Descriptive statistics, 31, 35, 41, 69, 70, 108,
179
</p>
<p>Deterministic Time Trend model, 392
</p>
<p>Diagnostics, 71, 129, 179, 221, 324
</p>
<p>Dickey-Fuller, 380&ndash;383, 386, 391
</p>
<p>augmented, 381, 382
</p>
<p>Differencing test, 197, 198, 204, 218
</p>
<p>Distributed lags, 77, 131, 132, 138, 139, 142,
143
</p>
<p>arithmetic lag, 132, 134, 136, 144
</p>
<p>polynomial lags see Almon lag 146
</p>
<p>Distribution Function method, 44, 334, 365
</p>
<p>Double Length Regression (DLR), 214
</p>
<p>Dummy variables, 33, 81, 83, 84, 91, 155,
159, 165, 168, 181, 216, 278, 280,
306&ndash;308, 325, 327, 333, 335, 338&ndash;
341, 348, 356
</p>
<p>Durbin&rsquo;s h-test, 140, 141, 145
</p>
<p>Durbin&rsquo;s Method, 114
</p>
<p>Durbin-Watson test, 115, 116, 122, 124, 125,
127, 129, 145, 147, 231, 270, 377
</p>
<p>Dynamic models, 131, 139, 142, 144, 145
</p>
<p>Econometrics, 3, 4, 7, 8, 10
</p>
<p>critiques, 7, 8
</p>
<p>history, 5&ndash;7</p>
<p/>
</div>
<div class="page"><p/>
<p>408 Index
</p>
<p>Efficiency, 4, 16&ndash;18, 69, 71, 100, 103, 106,
116, 121, 122, 124, 175, 179, 233,
234, 237, 242, 244, 245, 251&ndash;253,
255, 267, 289, 324, 329, 330, 384
</p>
<p>Elasticity, 6, 66, 69, 71, 90, 171, 279&ndash;281,
314, 325, 378
</p>
<p>Endogeneity, 7, 139, 257&ndash;264, 267&ndash;269, 271,
275, 279, 280, 290&ndash;294, 299&ndash;302,
324, 325
</p>
<p>Equicorrelated case, 218, 330
Error components models, 226, 305, 306,
</p>
<p>312, 315, 318&ndash;322, 325, 328
Error-Correction Model (ECM), 143, 385,
</p>
<p>386
Errors in measurement, 97, 294
Exponential distribution, 13, 15, 38, 40, 200
</p>
<p>Forecasting, 3, 8, 159, 183, 234, 378, 379,
394, 395
</p>
<p>standard errors, 159
Frisch-Waugh-Lovell Theorem, 154&ndash;156,
</p>
<p>167&ndash;170, 181, 211, 212, 327, 383
</p>
<p>Gamma distribution, 13, 39, 40, 144, 147
Gauss-Markov Theorem, 55, 57, 60, 153,
</p>
<p>159, 167, 175, 224, 227
Gauss-Newton Regression, 163, 206, 210,
</p>
<p>214, 219, 274, 342
Generalized inverse, 174, 198, 287
Generalized Least Squares (GLS), 124, 223,
</p>
<p>266, 272, 287, 294, 295, 309, 311&ndash;
313, 315, 320, 321, 323, 327&ndash;330
</p>
<p>Geometric distribution, 13, 38, 40, 144
Goodness of fit measures, 344
Granger causality, 379, 390, 391
Granger Representation Theorem, 385
Group heteroskedasticity, 101
Grouped data, 336, 339
</p>
<p>Hausman test, 197, 198, 201, 254, 256, 275&ndash;
277, 280, 281, 285, 289, 290, 305,
321, 329
</p>
<p>Hessian, 342, 352, 356
Heterogeneity, 305
Heteroskedasticity, 98&ndash;108, 112, 120, 121,
</p>
<p>123&ndash;129, 179, 187, 203, 223&ndash;225,
228, 234, 235, 237, 238, 268, 281,
</p>
<p>285, 321, 329, 333, 334, 337, 342,
343, 362, 387, 394, 408
</p>
<p>Heteroskedasticity test
Breusch-Pagan test, 105, 106, 125, 126,
</p>
<p>129, 248, 319, 320, 329
Glejser&rsquo;s test, 104, 106, 107, 123
Goldfeld-Quandt test, 192
Harvey&rsquo;s test, 105, 108, 123
Spearman&rsquo;s Rank Correlation test, 104,
</p>
<p>105, 107, 123
White&rsquo;s test, 100, 105, 106, 108, 109,
</p>
<p>112, 123, 125, 126, 129, 202, 221
Heteroskedasticity, 321
Homoskedasticity see heteroskedasticity 96,
</p>
<p>99, 100, 104&ndash;108, 111, 174, 191, 225,
232, 388, 389
</p>
<p>Identification problem, 257, 259, 260, 298
order condition, 385
</p>
<p>Indirect Least Squares, 270, 292
Infinite distributed lag, 137&ndash;139, 142
Influential observations, 61, 62, 66, 179, 183,
</p>
<p>184, 187, 220
Information matrix, 27, 42, 157, 158, 168,
</p>
<p>171, 201, 202, 219&ndash;221
Instrumental variable estimator, 266, 267,
</p>
<p>293
Inverse matrix
</p>
<p>partitioned, 155, 167, 168, 170, 175, 218,
232, 252
</p>
<p>Inverse Mills ratio, 362
</p>
<p>JA test, 200, 205, 206
Jacobian, 57, 156, 226, 230
Jarque-Bera test, 31, 98, 108, 110, 126, 203
Just-identified, 261, 265, 266, 269, 273, 274,
</p>
<p>281, 287, 288, 299, 302, 303
</p>
<p>Koyck lag, 137, 142
</p>
<p>Lagged dependent variable model, 97, 98,
138&ndash;142, 145, 199
</p>
<p>Lagrange-Multiplier test, 27&ndash;29, 37, 38, 42,
67, 100, 101, 118, 163, 165, 170, 171,
211, 212, 214, 223, 228, 233, 253,
319, 342, 377, 389
</p>
<p>standardized, 320</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 409
</p>
<p>Law of iterated expectations, 47, 53
</p>
<p>Least squares, 257, 261, 263, 264, 291, 294,
307&ndash;309, 330, 333, 343, 359, 360,
362, 365
</p>
<p>numerical properties, 50, 58, 59, 63, 67
</p>
<p>Likelihood function, 14, 23, 26, 27, 37, 42,
57, 102, 103, 114, 156, 161, 163, 171,
174, 226, 228, 235, 236, 254, 269,
291, 312, 328
</p>
<p>Likelihood Ratio test, 25, 26, 31, 37, 38, 42,
104, 162, 170, 172, 227, 228, 237,
246, 248, 253, 273, 319, 343, 365,
379, 391
</p>
<p>Limited dependent variables, 84, 333, 345
</p>
<p>Linear probability model, 333&ndash;335, 344&ndash;346,
348, 362, 363, 365
</p>
<p>Linear restrictions, 78, 79, 147, 165, 167
</p>
<p>Ljung-Box statistic, 377, 391
</p>
<p>Logit models, 200, 201, 341&ndash;346, 348, 354,
355, 365
</p>
<p>Matrix algebra, 36, 75, 83, 121, 158, 177,
223, 275
</p>
<p>Matrix properties, 151, 173, 176
</p>
<p>Maximum likelihood estimation, 14, 15, 20,
27, 37, 39, 57, 63, 67, 74, 78, 96,
102, 114, 120, 121, 127, 129, 146,
147, 156, 157, 165, 168, 221, 223,
226, 228&ndash;230, 238, 244, 247, 254,
255, 305, 375, 386, 389
</p>
<p>Mean Square Error, 20, 21, 66, 69, 76, 85,
104, 134, 158, 159, 161, 163, 168,
170, 180, 185, 233, 247, 310
</p>
<p>Measurement error, 49, 97
</p>
<p>Method of moments, 13, 16, 20, 37&ndash;39, 152,
265, 266, 323
</p>
<p>Methods of estimation, 13, 15, 238
</p>
<p>Moment Generating Function, 40, 43&ndash;45
</p>
<p>Moving Average
</p>
<p>MA(1), 322, 323
</p>
<p>Moving Average, MA(1), 111, 115, 128, 138,
139, 142, 145, 146, 226, 237, 375&ndash;
377, 390, 391, 394
</p>
<p>Multicollinearity, 74&ndash;76, 81, 82, 88, 131, 173,
245, 262, 307, 308
</p>
<p>Multinomial choice models, 350
</p>
<p>Multiple regression model, 73, 75, 78, 86&ndash;88,
91&ndash;93, 100, 154, 167, 175, 244, 252
</p>
<p>Multiplicative heteroskedasticity, 103, 105,
108, 123, 128, 235, 237
</p>
<p>Newton-Raphson interactive procedure,
342, 359
</p>
<p>Neyman-Pearson lemma, 23&ndash;25
</p>
<p>Nonlinear restrictions, 165, 167, 172, 173
</p>
<p>Nonstochastic regressors, 52, 123
</p>
<p>Normal equations, 50, 57, 60, 73, 74, 99,
103, 152, 155, 168, 174, 207, 261&ndash;
263, 292, 341
</p>
<p>Order condition, 260&ndash;263, 266, 292, 299&ndash;
302, 312, 341, 366, 385
</p>
<p>Over-identification, 257, 261, 268, 269, 273&ndash;
275, 281, 289, 292, 293, 303, 324,
325
</p>
<p>Panel data, 8, 84, 91, 95, 167, 225, 278, 305,
321, 322, 324, 327, 329, 330, 337
</p>
<p>National Longitudinal Survey (NLS),
207&ndash;211, 213, 305
</p>
<p>Panel Study of Income Dynamics
(PSID), 31, 41, 84, 305, 345
</p>
<p>Partial autocorrelation, 375
</p>
<p>Partial correlation, 93, 374&ndash;378, 381
</p>
<p>Partitioned regression, 154, 170, 172, 175
</p>
<p>Perfect collinearity, 35, 74, 75, 81, 82, 173,
196
</p>
<p>Poisson distribution, 13, 37, 40&ndash;42, 45, 46,
156
</p>
<p>Prais-Winsten, 113, 114, 117, 119, 121, 124,
125, 141, 226, 235
</p>
<p>Prediction, 4, 40, 41, 60, 61, 66, 68, 71, 91,
159, 165, 173, 223, 227, 234, 237,
238, 305, 313, 333, 334, 344&ndash;346,
365, 366
</p>
<p>Probability limit, 73, 144, 154, 232, 320, 373,
392
</p>
<p>Probit models, 200, 201, 341&ndash;348, 352, 363
</p>
<p>Projection matrix, 152, 153, 155, 174, 264,
306
</p>
<p>Quadratic form, 158, 169, 176, 318, 320</p>
<p/>
</div>
<div class="page"><p/>
<p>410 Index
</p>
<p>Random effects model, 305, 308, 309, 312&ndash;
315, 319
</p>
<p>Random number generator, 30, 38, 45, 54
</p>
<p>Random sample, 13&ndash;16, 18&ndash;21, 23, 25, 27,
28, 30, 36&ndash;41, 49, 52, 110
</p>
<p>Random walk, 373, 379&ndash;381, 383&ndash;385, 391,
393&ndash;395
</p>
<p>Rank condition, 261, 264, 292, 293, 298, 302
</p>
<p>Rational expectations, 7
</p>
<p>Recursive residuals, 187, 190&ndash;193, 216, 217
</p>
<p>Recursive systems, 290
</p>
<p>Reduced form, 257, 259, 270, 271, 288, 290,
293, 299, 301&ndash;303, 385
</p>
<p>Regression stability, 164
</p>
<p>Repeated observations, 95, 101, 102, 104
</p>
<p>Residual analysis, 60, 221
</p>
<p>Residual interpretation, 86
</p>
<p>Restricted least squares, 100, 161, 169, 212,
232, 253
</p>
<p>Restricted maximum likelihood, 27, 37, 165,
228, 229
</p>
<p>Sample autocorrelation function, 374, 391
</p>
<p>Sample correlogram, 374, 375, 377, 381, 382,
391
</p>
<p>Sample selectivity, 360, 362
</p>
<p>Score test, 27, 163, 167, 236
</p>
<p>Seasonal adjustment, 83, 85, 155, 173, 377,
378, 382, 395
</p>
<p>Seemingly Unrelated Regressions (SUR),
176, 225, 241&ndash;246, 248, 249, 251&ndash;
256, 272, 273, 379
</p>
<p>unequal observations, 246, 252, 255, 256
</p>
<p>Simultaneous bias, 97, 257, 259, 260, 262,
275
</p>
<p>Simultaneous equations model, 6, 10, 97, 98,
197, 225, 257, 260, 262, 264, 269,
272, 290, 293, 294, 298, 377, 378
</p>
<p>Single equation estimation, 268, 269, 272
</p>
<p>Spatial correlation, 223, 230, 231
</p>
<p>Spearman&rsquo;s Rank Correlation test, 104, 105,
107, 123
</p>
<p>Specification analysis
</p>
<p>overspecification, 77
</p>
<p>underspecification, 77
</p>
<p>Specification error
</p>
<p>Differencing test, 197, 198, 204, 205,
217&ndash;220
</p>
<p>Specification error tests, 192, 196, 219, 221
Spectral decomposition, 175, 309, 310
Spurious regression, 373, 384, 386, 387, 395,
</p>
<p>396
Stationarity, 106, 233, 373&ndash;375, 377, 379&ndash;
</p>
<p>386, 390, 391, 393&ndash;395
covariance stationary, 374, 379
difference stationary, 373, 380, 383
trend stationary, 373, 383, 384, 391, 394
</p>
<p>Stationary process, 234, 374, 380, 385
Stochastic explanatory variables, 96, 97
Studentized residuals, 180, 183&ndash;187, 216
Sufficient statistic, 20, 37, 39, 57, 158
Superconsistent, 386, 392, 394
</p>
<p>Tobit model, 356, 358, 359, 362
Truncated regression model, 359, 360
Truncated uniform density, 363
Two-stage least squares, 129, 142, 261, 263,
</p>
<p>264, 270
</p>
<p>Uniform distribution, 13, 38, 45
Unit root, 322, 323, 373, 379&ndash;384, 386, 391,
</p>
<p>394&ndash;396
Unordered response models, 350, 354
</p>
<p>Vector Autoregression (VAR), 373, 378, 379,
386, 391
</p>
<p>Wald test, 26&ndash;29, 37, 38, 42, 162, 165&ndash;167,
170&ndash;173, 224, 229, 237, 321, 342,
363
</p>
<p>Weighted Least Squares, 100, 121, 125, 309,
334
</p>
<p>White noise, 143, 375, 377, 392
White test, 100, 105, 106, 108, 109, 112, 123,
</p>
<p>125, 126, 129, 202, 221, 238, 395
Within estimator, 307, 311&ndash;314, 320, 321,
</p>
<p>325, 327, 329, 330
</p>
<p>Zero mean assumption, 51&ndash;54, 95, 96, 98,
102, 110, 111, 122, 152, 176, 179,
190, 202, 217, 227, 234, 311, 327</p>
<p/>
</div>
<ul>	<li>Econometrics</li>
	<li>Preface</li>
	<li>Table of Contents</li>
	<li>Part I</li>
	<li>Part II</li>
	<li>Appendix</li>
	<li>List of Figures</li>
	<li>List of Tables</li>
	<li>Index</li>
</ul>
</body></html>