<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
</div>
<div class="page"><p/>
<p>Springer Texts in Statistics 
</p>
<p>Advisors: 
</p>
<p>George Casella Stephen Fienberg Ingram Olkin </p>
<p/>
</div>
<div class="page"><p/>
<p>Springer Texts in Statistics 
</p>
<p>AJfred: Elements of Statistics for the Life and Social Sciences 
Berger: An Introduction to Probability and Stochastic Processes 
Bilodeau and Brenner: Theory of Multivariate Statistics 
BIom: Probability and Statistics: Theory and Applications 
</p>
<p>Brockwell and Davis: Introduction to Times Series and Forecasting, 
</p>
<p>Second Edition 
Chow and Teicher: Probability Theory: Independence, Interchangeability, 
</p>
<p>Martingales, Third Edition 
</p>
<p>Christensen: Advanced Linear Modeling: Multivariate, Time Series, and 
Spatial Data; Nonparametric Regression and Response Surface 
</p>
<p>Maximization, Second Edition 
</p>
<p>Christensen: Log-Linear Models and Logistic Regression, Second Edition 
Christensen: Plane Answers to Complex Questions: The Theory of Linear 
</p>
<p>Models, Third Edition 
</p>
<p>Creighton: A First Course in Probability Models and Statistical Inference 
Davis: Statistical Methods for the Analysis of Repeated Measurements 
Dean and Voss: Design and Analysis of Experiments 
du Toit, Steyn, and Stumpf Graphical Exploratory Data Analysis 
Durrett: Essentials of Stochastic Processes 
Edwards: Introduction to Graphical Modelling, Second Edition 
Finkelstein and Levin: Statistics for Lawyers 
</p>
<p>Flury: A First Course in Multivariate Statistics 
</p>
<p>Jobson: Applied Multivariate Data Analysis, Volume I: Regression and 
Experimental Design 
</p>
<p>Jobson: Applied Multivariate Data Analysis, Volume II: Categorical and 
Multivariate Methods 
</p>
<p>Kalbfleisch: Probability and Statistical Inference, Volume I: Probability, 
Second Edition 
</p>
<p>Kalbfleisch: Probability and Statistical Inference, Volume II : Statistical Inference, 
Second Edition 
</p>
<p>Karr: Probability 
Keyfitz: Applied Mathematical Demography, Second Edition 
Kiefer: Introduction to Statistical Inference 
Kokoska and Nevison: Statistical Tables and Fonnulae 
Kulkarni: Modeling, Analysis, Design, and Control of Stochastic Systems 
</p>
<p>Lange: Applied Probability 
Lehmann: Elements of Large-Sample Theory 
Lehmann: Testing Statistical Hypotheses, Second Edition 
Lehmann and Casella: Theory of Point Estimation, Second Edition 
Lindman: Analysis of Variance in Experimental Design 
Lindsey: Applying Generalized Linear Models 
</p>
<p>(continued after index) </p>
<p/>
</div>
<div class="page"><p/>
<p>Larry Wassennan 
</p>
<p>All of Statistics 
</p>
<p>A Concise Course in Statistical Inference 
</p>
<p>With 95 Figures 
</p>
<p>, Springer </p>
<p/>
</div>
<div class="page"><p/>
<p>Larry W assennan 
Department of Statistics 
Carnegie Mellon University 
Baker Hali 228A 
Pittsburgh, PA 15213-3890 
USA 
larry@ stat.cmu.edu 
</p>
<p>Editorial Board 
</p>
<p>George Casella 
Department of Statistics 
University of Florida 
Gainesville, FL 32611-8545 
USA 
</p>
<p>Stephen Fienberg 
Department of Statistics 
Carnegie Mellon University 
Pittsburgh, PA 15213-3890 
USA 
</p>
<p>Library of Congress Cataloging-in-Publication Data 
</p>
<p>Wasserman, Larry A. (Larry Alan), 1959-
</p>
<p>Ingram Olkin 
Department of Statistics 
Stanford University 
Stanford, CA 94305 
USA 
</p>
<p>All of statistics: a concise course in statistica! inference 1 Larry a. W asserman. 
</p>
<p>p. cm. - (Springer texts in statistics) 
</p>
<p>Includes bibliographical references and index. 
</p>
<p>1. Mathematical statistics. 1. Title. Il. Series. 
QA276.12.W37 2003 
</p>
<p>519.5-dc21 
</p>
<p>ISBN 978-1-4419-2322-6 ISBN 978-0-387-21736-9 (eBook) 
</p>
<p>DOI 10.1007/978-0-387-21736-9 
</p>
<p>&copy; 2004 Springer Science+Business Media New York 
</p>
<p>2003062209 
</p>
<p>Originally published by Springer Science+Business Media, !ne in 2004 
</p>
<p>Softcover reprint of the hardcover 1 st edition 2004 
</p>
<p>All rights reserved. This work may not be translated or copied in whole or in part without the 
</p>
<p>written permission of the publisher (Springer Science+ Business Media, LLC ), except for brief 
</p>
<p>excerpts in connection with reviews or scholarly analysis. 
</p>
<p>Use in connection with any form of information storage and retrieval, electronic adaptation, com-
</p>
<p>puter software, or by similar or dissimilar methodology now known or hereafter developed is for-
</p>
<p>bidden. 
</p>
<p>The use in this publication of trade names, trademarks, service marks, and similar terms, even if 
</p>
<p>they are not identified as such, is not to be taken as an expression of opinion as to whether or not 
</p>
<p>they are subject to proprietary rights. 
</p>
<p>9 8 7 6 5 4 3 (Corrected second printing, 2005) 
</p>
<p>springeronline.com </p>
<p/>
</div>
<div class="page"><p/>
<p>To Isa </p>
<p/>
</div>
<div class="page"><p/>
<p>Preface 
</p>
<p>Taken literally, the title "All of Statistics" is an exaggeration. But in spirit, 
</p>
<p>the title is apt, as the book does cover a much broader range of topics than a 
</p>
<p>typical introductory book on mathematical statistics. 
</p>
<p>This book is for people who want to learn probability and statistics quickly. 
</p>
<p>It is suitable for graduate or advanced undergraduate students in computer 
</p>
<p>science, mathematics, statistics, and related disciplines. The book includes 
</p>
<p>modern topics like nonparametric curve estimation, bootstrapping, and clas-
</p>
<p>sification, topics that are usually relegated to follow-up courses. The reader is 
</p>
<p>presumed to know calculus and a little linear algebra. No previous knowledge 
</p>
<p>of probability and statistics is required. 
</p>
<p>Statistics, data mining, and machine learning are all concerned with 
</p>
<p>collecting and analyzing data. For some time, statistics research was con-
</p>
<p>ducted in statistics departments while data mining and machine learning re-
</p>
<p>search was conducted in computer science departments. Statisticians thought 
</p>
<p>that computer scientists were reinventing the wheel. Computer scientists 
</p>
<p>thought that statistical theory didn't apply to their problems. 
</p>
<p>Things are changing. Statisticians now recognize that computer scientists 
</p>
<p>are making novel contributions while computer scientists now recognize the 
</p>
<p>generality of statistical theory and methodology. Clever data mining algo-
</p>
<p>rithms are more scalable than statisticians ever thought possible. Formal sta-
</p>
<p>tistical theory is more pervasive than computer scientists had realized. 
</p>
<p>Students who analyze data, or who aspire to develop new methods for 
</p>
<p>analyzing data, should be well grounded in basic probability and mathematical 
</p>
<p>statistics. Using fancy tools like neural nets, boosting, and support vector </p>
<p/>
</div>
<div class="page"><p/>
<p>viii Preface 
</p>
<p>machines without understanding basic statistics is like doing brain surgery 
</p>
<p>before knowing how to use a band-aid. 
</p>
<p>But where can students learn basic probabili ty and statistics quickly? Nowhere. 
</p>
<p>At least, that was my conclusion when my computer science colleagues kept 
</p>
<p>asking me: "Where can I send my students to get a good understanding of 
</p>
<p>modern statistics quickly?" The typical mathematical statistics course spends 
</p>
<p>too much time on tedious and uninspiring topics (counting methods, two di-
</p>
<p>mensional integrals, etc.) at the expense of covering modern concepts (boot-
</p>
<p>strapping, curve estimation, graphical models, etc. ). So I set out to redesign 
</p>
<p>our undergraduate honors course on probability and mathematical statistics. 
</p>
<p>This book arose from that course. Here is a summary of the main features of 
</p>
<p>this book. 
</p>
<p>1. The book is suitable for graduate students in computer science and 
</p>
<p>honors undergraduates in math, statistics, and computer science. It is 
</p>
<p>also useful for students beginning graduate work in statistics who need 
</p>
<p>to fill in their background on mathematical statistics. 
</p>
<p>2. I cover advanced topics that are traditionally not taught in a first course. 
</p>
<p>For example, nonparametr ic regression, bootstrapping, density estima-
</p>
<p>tion, and graphical models. 
</p>
<p>3. I have omitted topics in probability that do not play a central role in 
</p>
<p>statistical inference. For example, counting methods are virtually ab-
</p>
<p>sent. 
</p>
<p>4. Whenever possible, I avoid tedious calculations in favor of emphasizing 
</p>
<p>concepts. 
</p>
<p>5. I cover nonparametric inference before parametric inference. 
</p>
<p>6. I abandon the usual "First Term = Probability" and "Second Term 
</p>
<p>= Statistics" approach . Some students only take the fi rst half and it 
</p>
<p>would be a crime if they did not see any statistical theory. Furthermore, 
</p>
<p>probability is more engaging when students can see it put to work in the 
</p>
<p>context of statistics. An exception is the topic of stochastic processes 
</p>
<p>which is included ill the later material. 
</p>
<p>7. T he course moves very quickly and covers much material . My colleagues 
</p>
<p>joke that I cover all of statistics in this course and hence the title. The 
</p>
<p>course is demanding but I have worked hard to make the material as 
</p>
<p>intuitive as possible so that the material is very understandable despite 
the fast pace. 
</p>
<p>8. Rigor and clarity are not synonymous. I have tried to strike a good 
</p>
<p>balance. To avoid getting bogged down in uninteresting technical details, 
</p>
<p>many results are stated without proof. The bibliographic references at 
</p>
<p>the end of each chapter point the student to appropriate sources. </p>
<p/>
</div>
<div class="page"><p/>
<p>Preface ix 
</p>
<p>Probability 
</p>
<p>Data generating process C Ob.e""d data ~ 
</p>
<p>~------------~ 
Inference and Data Mining 
</p>
<p>FIGURE L Probability and inference. 
</p>
<p>9. On my website are files with R code which students can use for doing 
</p>
<p>all the computing. The website is: 
</p>
<p>http://www.stat.cmu.eduf''-'larry/all-of-statistics 
</p>
<p>However, the book is not tied to R and any computing language can be 
</p>
<p>used. 
</p>
<p>Part I of the text is concerned with probability theory, the formal language 
</p>
<p>of uncertainty which is the basis of statistical inference. The basic problem 
</p>
<p>that we study in probability is: 
</p>
<p>Given a data generating process, what are the properties of the out-
</p>
<p>comes? 
</p>
<p>Part II is about statistical inference and its close cousins, data mining and 
</p>
<p>machine learning. The basic problem of statistical inference is the inverse of 
</p>
<p>probability: 
</p>
<p>Given the outcomes, what can we say about the process that gener-
</p>
<p>ated the data? 
</p>
<p>These ideas are illustrated in Figure 1. Prediction, classification , clustering, 
</p>
<p>and estimation are all special cases of statistical inference. Data analysis, 
</p>
<p>machine learning and data mining are various names given to the practice of 
</p>
<p>statistical inference, depending on the context. </p>
<p/>
</div>
<div class="page"><p/>
<p>x Preface 
</p>
<p>Part III applies the ideas from Part II to specific problems such as regres-
</p>
<p>sion, graphical models, causation, density est imation, smoothing, classifica~ 
</p>
<p>t ion , and simulation. Part III contains one more chaptcr on probability that 
</p>
<p>covers stochastic processes including Markov chains. 
</p>
<p>I have drawn on other books in many places. Most chapters contain a section 
</p>
<p>called Bibliographic Remarks which serves both to acknowledge my debt to 
</p>
<p>other authors and to point readers to other useful references. I would especially 
</p>
<p>like to mention the books by DeGroot and Schervish (2002) and Grimmett 
</p>
<p>and Stirzaker (1982) from which I adapted many examples and exercises. 
</p>
<p>As one develops a book ovcr several years it is easy to lose track of where pre-
</p>
<p>sentation ideas and, especially, homework problems originated. Some I made 
</p>
<p>up. Some I remembered from my education. Some I borrowed from other 
</p>
<p>books. I hope I do not offend anyone if I have used a problem from their book 
</p>
<p>and failed to give proper credit . As my colleague Mark Schervish wrote in his 
</p>
<p>book (Schcrvish (1995)), 
</p>
<p>" ... the problems at the ends of each chapter have come from many 
</p>
<p>sources .... These problems, in turn , came from various sources 
</p>
<p>unknown to me ... If I have used a problem without giving proper 
</p>
<p>credit, please take it as a compliment." 
</p>
<p>I am indebted to many people without whose help I could not have written 
</p>
<p>this book. First and foremost , the many students who used earlier versions 
</p>
<p>of this text and provided much feedback. In particular, Liz P rather and J en~ 
</p>
<p>nifer Bakal read the book carefully. Rob Reeder valiantly read through the 
</p>
<p>entire book in excruciating detail and gave me countless suggestions for im~ 
</p>
<p>provements. Chris Genovese deserves special mention. He not only provided 
</p>
<p>helpful ideas about intellectual content, but also spent many, many hours 
</p>
<p>writing ~'IEXcode for the book. The best aspects of the book's layout are due 
</p>
<p>to his hard work; any stylistic deficiencies are due to my lack of expertise. 
</p>
<p>David Hand , Sam Roweis, and David Scott read the book very carefully and 
</p>
<p>made numerous suggestions that greatly improved the book. J ohn Lafferty 
</p>
<p>and Peter Spirtes also provided helpful feedback. John Kimmel has been sup-
</p>
<p>portive and helpful throughout t he writing process. Finally, my wife Isabella 
</p>
<p>Verdinelli has been an invaluable source of love, support, and inspiration. 
</p>
<p>Larry Wasserman 
Pittsburgh, Pennsylvania 
</p>
<p>July 2003 </p>
<p/>
</div>
<div class="page"><p/>
<p>Preface Xl 
</p>
<p>Statistics /Data Mining Dictionary 
</p>
<p>Statisticians and computer scientists often use different language for the 
</p>
<p>same thing. Here is a dictionary that the reader may want to return to 
</p>
<p>throughout the course. 
</p>
<p>Statistics Computer Science Meaning 
</p>
<p>estimation learning using data to estimate 
</p>
<p>an unknown quantity 
</p>
<p>classification supervised learning predicting a discrete Y 
from X 
</p>
<p>clustering unsupervised learning putting data into groups 
</p>
<p>data training sample (Xl, Yd,&middot; .. ,(Xn , Yn ) 
covariates features the Xi'S 
</p>
<p>classifier hypothesis a map from covariates 
</p>
<p>to outcomes 
</p>
<p>hypothesis subset of a parameter 
</p>
<p>space e 
confidence interval interval that contains an 
</p>
<p>unknown quantity 
</p>
<p>with given frequency 
</p>
<p>directed acyclic graph Bayes net multivariate distribution 
</p>
<p>with given conditional 
</p>
<p>independence relations 
</p>
<p>Bayesian inference Bayesian inference statistical methods for 
</p>
<p>using data to 
</p>
<p>update beliefs 
</p>
<p>frequentist inference statistical methods 
</p>
<p>with guaranteed 
</p>
<p>frequency behavior 
</p>
<p>large deviation bounds PAC learning uniform bounds on 
</p>
<p>probability of errors </p>
<p/>
</div>
<div class="page"><p/>
<p>Contents 
</p>
<p>I Probability 
</p>
<p>1 Probability 
</p>
<p>1.1 Introduction ........ . 
</p>
<p>1.2 Sample Spaces and Events . 
</p>
<p>1.3 Probability . . . . . . . . . 
</p>
<p>1.4 Probability on Finite Sample Spaces 
</p>
<p>1.5 Independent Events .. 
</p>
<p>1.6 Conditional Probability 
</p>
<p>1.7 Bayes' Theorem ... . 
</p>
<p>1.8 Bibliographic Remarks 
</p>
<p>1.9 Appendix 
</p>
<p>1.10 Exercises ... . 
</p>
<p>2 Random Variables 
</p>
<p>2.1 Introduction ... 
</p>
<p>2.2 Distribution Functions and Probability Functions 
</p>
<p>2.3 Some Important Discrete Random Variables . . 
</p>
<p>2.4 Some Important Continuous Random Variables 
</p>
<p>2.5 Bivariate Distributions .... . 
</p>
<p>2.6 Marginal Distributions .... . 
</p>
<p>2.7 Independent Random Variables 
</p>
<p>2.8 Conditional Distributions ... 
</p>
<p>3 
</p>
<p>3 
</p>
<p>3 
</p>
<p>5 
</p>
<p>7 
</p>
<p>8 
</p>
<p>10 
</p>
<p>12 
</p>
<p>13 
</p>
<p>13 
</p>
<p>13 
</p>
<p>19 
</p>
<p>19 
</p>
<p>20 
</p>
<p>25 
</p>
<p>27 
</p>
<p>31 
</p>
<p>33 
</p>
<p>34 
</p>
<p>36 </p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Contents 
</p>
<p>2.9 Multivariate Distributions and lID Samples 
</p>
<p>2.10 Two Important Multivariate Distributions . 
</p>
<p>2.11 Transformations of Random Variables ... 
</p>
<p>2.12 Transformations of Several Random Variables 
</p>
<p>2.13 Appendix 
</p>
<p>38 
</p>
<p>39 
41 
</p>
<p>42 
</p>
<p>43 
</p>
<p>2.14 Exercises 43 
</p>
<p>3 Expectation 47 
</p>
<p>3.1 Expectation of a Random Variable 47 
</p>
<p>3.2 Properties of Expectations . . . . . 50 
</p>
<p>3.3 Variance and Covariance . . . . . . 50 
</p>
<p>3.4 Expectation and Variance of Important Random Variables. 52 
</p>
<p>3.5 Conditional Expectation . . . . 54 
</p>
<p>3.6 Moment Generating Functions 56 
</p>
<p>3.7 Appendix 58 
</p>
<p>3.8 Exercises 58 
</p>
<p>4 Inequalities 63 
</p>
<p>4.1 Probability Inequalities 63 
</p>
<p>4.2 Inequalities For Expectations 66 
</p>
<p>4.3 Bibliographic Remarks 66 
</p>
<p>4.4 Appendix 67 
</p>
<p>4.5 Exercises 
</p>
<p>5 Convergence of Random Variables 
</p>
<p>5.1 Introduction ........ . 
</p>
<p>5.2 Types of Convergence .. . 
</p>
<p>5.3 The Law of Large Numbers 
</p>
<p>5.4 The Central Limit Theorem 
</p>
<p>5.5 The Delta Method . . 
</p>
<p>5.6 Bibliographic Remarks . . . 
</p>
<p>5.7 Appendix . . . . . . . . . . 
</p>
<p>5.7.1 Almost Sure and L1 Convergence. 
</p>
<p>5.7.2 Proof of the Central Limit Theorem 
</p>
<p>5.8 Exercises ........... . ...... . 
</p>
<p>II Statistical Inference 
</p>
<p>6 Models, Statistical Inference and Learning 
</p>
<p>6.1 Introduction .. .. ........... . 
</p>
<p>6.2 Parametric and Nonparametric Models. 
</p>
<p>6.3 Fundamental Concepts in Inference . 
</p>
<p>6.3.1 Point Estimation 
</p>
<p>6.3.2 Confidence Sets .... . .. . 
</p>
<p>68 
</p>
<p>71 
</p>
<p>71 
</p>
<p>72 
</p>
<p>76 
</p>
<p>77 
</p>
<p>79 
</p>
<p>80 
</p>
<p>81 
</p>
<p>81 
</p>
<p>81 
</p>
<p>82 
</p>
<p>87 
</p>
<p>87 
</p>
<p>87 
</p>
<p>90 
</p>
<p>90 
</p>
<p>92 </p>
<p/>
</div>
<div class="page"><p/>
<p>6.3.3 Hypothesis Testing 
</p>
<p>6.4 Bibliographic Remarks 
</p>
<p>6.5 Appendix 
6.6 Exercises ....... . 
</p>
<p>7 Estimating the CDF and Statistical Functionals 
</p>
<p>7.1 The Empirical Distribution Function 
</p>
<p>7.2 Statistical F\mctionals . 
</p>
<p>7.3 Bibliographic Remarks . 
</p>
<p>7.4 Exercises . 
</p>
<p>8 The Bootstrap 
</p>
<p>8.1 Simulation. 
</p>
<p>9 
</p>
<p>8.2 Bootstrap Variance Estimation 
</p>
<p>8.3 Bootstrap Confidence Intervals 
</p>
<p>8.4 Bibliographic Remarks 
</p>
<p>8.5 Appendix ... 
</p>
<p>8.5.1 The Jackknife . 
</p>
<p>8.5 .2 Justification For The Percentile Interval 
</p>
<p>8.6 Exercises . 
</p>
<p>Parametric Inference 
</p>
<p>9.1 Parameter of Interest. 
9.2 The Method of Moments. . ..... 
9.3 Maximum Likelihood. 
9.4 Properties of Maximum Likelihood Estimators 
</p>
<p>9.5 Consistency of Maximum Likelihood Estimators . 
</p>
<p>9.6 Equivariance of the MLE 
</p>
<p>9.7 Asymptotic Normality 
</p>
<p>9.8 Optimality .... 
</p>
<p>9.9 The Delta Method 
</p>
<p>9.10 Multiparameter Models 
</p>
<p>9.11 The Parametric Bootstrap. 
</p>
<p>9.12 Checking Assumptions 
</p>
<p>9.13 Appendix .... . 
</p>
<p>9.13.1 Proofs .... . 
</p>
<p>9.13.2 Sufficiency .. . 
</p>
<p>9.13.3 Exponential Families . 
</p>
<p>9.13.4 Computing Maximum Likelihood Estimates 
</p>
<p>9.14 Exercises 
</p>
<p>10 Hypothesis Testing and p-values 
</p>
<p>10.1 The Wald Test . . 
</p>
<p>10.2 p-vaiues ..... 
</p>
<p>10.3 The X2 Distribution 
</p>
<p>Contents xv 
</p>
<p>94 
</p>
<p>95 
95 
95 
</p>
<p>97 
</p>
<p>97 
99 
</p>
<p>104 
104 
</p>
<p>107 
108 
</p>
<p>108 
110 
115 
1I5 
1I5 
1I6 
1I6 
</p>
<p>119 
120 
120 
</p>
<p>122 
124 
126 
</p>
<p>127 
128 
130 
131 
133 
134 
135 
135 
135 
137 
</p>
<p>140 
142 
146 
</p>
<p>149 
152 
156 
159 </p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Contents 
</p>
<p>10.4 Pearson's X2 Test For Multinomial Data 
</p>
<p>10.5 The Permutation Test . . . 
</p>
<p>10.6 The Likelihood Ratio Test . 
</p>
<p>10.7 Multiple Testing .... 
</p>
<p>10.8 Goodness-of-fit Tests .. 
</p>
<p>10.9 Bibliographic Remarks . 
</p>
<p>1O.lOAppendix ....... . 
</p>
<p>10.10.1 The Neyman-Pearson Lemma. 
</p>
<p>10.10.2 The t-test 
</p>
<p>10. 11 Exercises .... 
</p>
<p>11 Bayesian Inference 
</p>
<p>11.1 The Bayesian Philosophy 
</p>
<p>11.2 The Bayesian Method . 
</p>
<p>11.3 Functions of Parameters . 
</p>
<p>11.4 Simulation. . . . . . . . . 
</p>
<p>11.5 Large Sample Properties of Bayes' Procedures. 
</p>
<p>11.6 Flat Priors, Improper Priors, and "Noninformative" Priors. 
</p>
<p>11.7 Multiparameter Problems ............ . 
</p>
<p>11.8 Bayesian Testing . . . . . . . . . . . . . . . . . . 
</p>
<p>11.9 Strengths and Weaknesses of Bayesian Inference 
</p>
<p>11.10Bibliographic Remarks. 
</p>
<p>11.11Appendix 
</p>
<p>11.12Exercises ....... . 
</p>
<p>12 Statistical Decision Theory 
</p>
<p>12.1 Preliminaries ...... . 
</p>
<p>12.2 Comparing Risk Functions. 
</p>
<p>12.3 Bayes Estimators . . . . . . 
</p>
<p>12.4 Minimax Rules . . . . . . . 
</p>
<p>12.5 Maximum Likelihood, Minimax, and Bayes 
</p>
<p>12.6 Admissibility . . . . . . 
</p>
<p>12.7 Stein's Paradox . .... 
</p>
<p>12.8 Bibliographic Remarks . 
</p>
<p>12.9 Exercises ..... .. . 
</p>
<p>III Statistical Models and Methods 
</p>
<p>13 Linear and Logistic Regression 
</p>
<p>13.1 Simple Linear Regression ....... . . . 
</p>
<p>13.2 Least Squares and Maximum Likelihood . . 
</p>
<p>13.3 Properties of the Least Squares Estimators 
</p>
<p>13.4 Prediction . . . . . . 
</p>
<p>13.5 Multiple Regression . .... . ...... . 
</p>
<p>160 
</p>
<p>161 
</p>
<p>164 
</p>
<p>165 
</p>
<p>168 
</p>
<p>169 
</p>
<p>170 
</p>
<p>170 
</p>
<p>170 
</p>
<p>170 
</p>
<p>175 
</p>
<p>175 
</p>
<p>176 
</p>
<p>180 
</p>
<p>180 
</p>
<p>181 
</p>
<p>181 
</p>
<p>183 
</p>
<p>184 
</p>
<p>185 
</p>
<p>189 
</p>
<p>190 
</p>
<p>190 
</p>
<p>193 
</p>
<p>193 
</p>
<p>194 
</p>
<p>197 
</p>
<p>198 
</p>
<p>.201 
</p>
<p>&middot; 202 
</p>
<p>&middot; 204 
</p>
<p>&middot; 204 
</p>
<p>&middot; 204 
</p>
<p>209 
</p>
<p>&middot; 209 
</p>
<p>&middot; 212 
</p>
<p>&middot; 214 
</p>
<p>&middot; 215 
</p>
<p>&middot; 216 </p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Model Selection . . . . . 
</p>
<p>13.7 Logistic Regression . . . 
</p>
<p>13.8 Bibliographic Remarks . 
</p>
<p>13.9 Appendix 
</p>
<p>13.lOExercises ..... 
</p>
<p>14 Multivariate Models 
</p>
<p>14.1 Random Vectors ..... . 
</p>
<p>14.2 Estimating the Correlation 
</p>
<p>14.3 Multivariate Normal . . 
</p>
<p>14.4 Multinomial .... .. . 
</p>
<p>14.5 Bibliographic Remarks . 
</p>
<p>14.6 Appendix 
</p>
<p>14.7 Exercises ....... . 
</p>
<p>15 Inference About Independence 
</p>
<p>15.1 Two Binary Variables .. . 
</p>
<p>15.2 Two Discrete Variables .... . 
</p>
<p>15.3 Two Continuous Variables .. . 
</p>
<p>15.4 One Continuous Variable and One Discrete 
</p>
<p>15.5 Appendix 
</p>
<p>15.6 Exercises .. 
</p>
<p>16 Causal Inference 
</p>
<p>16.1 The Counterfactual Model. 
</p>
<p>16.2 Beyond Binary Treatments ..... . . 
</p>
<p>16.3 Observational Studies and Confounding 
</p>
<p>16.4 Simpson's Paradox . . 
</p>
<p>16.5 Bibliographic Remarks 
</p>
<p>16.6 Exercises .... .. . 
</p>
<p>17 Directed Graphs and Conditional Independence 
</p>
<p>17.1 Introduction ... . ... . 
</p>
<p>17.2 Conditional Independence 
</p>
<p>17.3 DAGs .......... . 
</p>
<p>17.4 Probability and DAGs . . 
</p>
<p>17.5 More Independence Relations 
</p>
<p>17.6 Estimation for DAGs . . 
</p>
<p>17.7 Bibliographic Remarks . 
</p>
<p>17.8 Appendix 
</p>
<p>17.9 Exercises .... 
</p>
<p>18 Undirected Graphs 
</p>
<p>18.1 Undirected Graphs 
</p>
<p>18.2 Probability and Graphs 
</p>
<p>Contents xvii 
</p>
<p>&middot; 218 
</p>
<p>&middot; 223 
</p>
<p>&middot; 225 
</p>
<p>&middot; 225 
</p>
<p>&middot; 226 
</p>
<p>231 
</p>
<p>232 
</p>
<p>&middot; 233 
</p>
<p>&middot; 234 
</p>
<p>&middot; 235 
</p>
<p>&middot; 237 
</p>
<p>&middot; 237 
</p>
<p>&middot; 238 
</p>
<p>239 
</p>
<p>239 
</p>
<p>&middot; 243 
</p>
<p>&middot; 244 
</p>
<p>&middot; 244 
</p>
<p>&middot; 245 
</p>
<p>&middot; 248 
</p>
<p>251 
</p>
<p>&middot; 251 
</p>
<p>255 
</p>
<p>257 
</p>
<p>259 
</p>
<p>261 
</p>
<p>&middot; 261 
</p>
<p>263 
</p>
<p>&middot; 263 
</p>
<p>&middot; 264 
</p>
<p>&middot; 264 
</p>
<p>&middot; 266 
</p>
<p>&middot; 267 
</p>
<p>&middot; 272 
.272 
</p>
<p>272 
</p>
<p>&middot; 276 
</p>
<p>281 
</p>
<p>&middot; 281 
</p>
<p>&middot; 282 </p>
<p/>
</div>
<div class="page"><p/>
<p>xviii Contents 
</p>
<p>18.3 Cliques and Potentials 285 
</p>
<p>18.4 Fitting Graphs to Data 286 
</p>
<p>18.5 Bibliographic Remarks . 286 
</p>
<p>18.6 Exercises . . .. 286 
</p>
<p>19 Log-Linear Models 291 
</p>
<p>19.1 The Log-Linear Model . . . . . . 291 
</p>
<p>19.2 Graphical Log-Linear Models . . 294 
</p>
<p>19.3 Hierarchical Log-Linear Models . 296 
</p>
<p>19.4 Model Generators . . . . . . . . . 297 
</p>
<p>19.5 Fitting Log-Linear Models to Data . 298 
</p>
<p>19.6 Bibliographic Remarks . 300 
</p>
<p>19.7 Exercises ............. . 301 
</p>
<p>20 N onparametric Curve Estimation 303 
</p>
<p>20.1 The Bias-Variance Tradeoff . 304 
</p>
<p>20.2 Histograms . . . . . . . . . . 305 
</p>
<p>20.3 Kernel Density Estimation. . 312 
</p>
<p>20.4 Nonparametric Regression . 319 
</p>
<p>20.5 Appendix . . . . . . . . 324 
</p>
<p>20.6 Bibliographic Remarks . 325 
</p>
<p>20.7 Exercises ....... . 325 
</p>
<p>21 Smoothing Using Orthogonal Functions 327 
</p>
<p>21.1 Orthogonal Functions and L2 Spaces . 327 
21.2 Density Estimation. . 331 
</p>
<p>21.3 Regression. . 335 
</p>
<p>21.4 Wavelets . . . . . . . . 340 
</p>
<p>21.5 Appendix . . . . . . . 345 
</p>
<p>21.6 Bibliographic Remarks . . 346 
</p>
<p>21. 7 Exercises ........ . 346 
</p>
<p>22 Classification 349 
</p>
<p>22.1 Introduction . . . . . . . . . . . . . . . 349 
</p>
<p>22.2 Error Rates and the Bayes Classifier. . 350 
</p>
<p>22.3 Gaussian and Linear Classifiers. . . . . 353 
</p>
<p>22.4 Linear Regression and Logistic Regression . 356 
</p>
<p>22.5 Relationship Between Logistic Regression and LDA . 358 
</p>
<p>22.6 Density Estimation and Naive Bayes . . . . . . . . . . 359 
</p>
<p>22.7 Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 360 
</p>
<p>22.8 Assessing Error Rates and Choosing a Good Classifier . . 362 
</p>
<p>22.9 Support Vector Machines . 368 
</p>
<p>22.10 Kernelization . . . . . . . 371 
</p>
<p>22.11 Other Classifiers . . . . . 375 
</p>
<p>22.12 Bibliographic Remarks . 377 </p>
<p/>
</div>
<div class="page"><p/>
<p>22.13 Exercises ................. . 
</p>
<p>23 Probability Redux: Stochastic Processes 
</p>
<p>23.1 Introduction ... . 
</p>
<p>23.2 Markov Chains ... . 
</p>
<p>23.3 Poisson Processes . . . 
</p>
<p>23.4 Bibliographic Remarks 
</p>
<p>23.5 Exercises .. ... 
</p>
<p>24 Simulation Methods 
</p>
<p>24.1 Bayesian Inference Revisited . 
</p>
<p>24.2 Basic Monte Carlo Integration 
</p>
<p>24.3 Importance Sampling. . . . . . 
</p>
<p>24.4 MCMC Part I: The Metropolis- Hastings Algorithm 
</p>
<p>24.5 MCMC Part II: Different Flavors 
</p>
<p>24.6 Bibliographic Remarks . 
</p>
<p>24.7 Exercises ............ . 
</p>
<p>Index 
</p>
<p>Contents xix 
</p>
<p>.377 
</p>
<p>381 
</p>
<p>&middot; 381 
</p>
<p>&middot; 383 
</p>
<p>&middot; 394 
</p>
<p>&middot; 397 
</p>
<p>&middot; 398 
</p>
<p>403 
</p>
<p>.403 
</p>
<p>.404 
</p>
<p>.408 
</p>
<p>&middot; 411 
</p>
<p>&middot; 415 
</p>
<p>&middot; 420 
</p>
<p>.420 
</p>
<p>434 </p>
<p/>
</div>
<div class="page"><p/>
<p>Part I 
</p>
<p>Probability </p>
<p/>
</div>
<div class="page"><p/>
<p>1 
</p>
<p>Probability 
</p>
<p>1.1 Introduction 
</p>
<p>Probability is a mathematical language for quantifying uncertainty. In this 
</p>
<p>Chapter we introduce the basic concepts underlying probability theory. We 
</p>
<p>begin with the sample space, which is the set of possible outcomes. 
</p>
<p>1.2 Sample Spaces and Events 
</p>
<p>The sample space n is the set of possible outcomes of an experiment . Points 
w in n are called sample outcomes, realizations, or elements. Subsets of 
n are called Events. 
</p>
<p>1.1 Example. If we toss a coin twice then n = {H H, HT, T H, TT}. The event 
that the first toss is heads is A = {H H, HT} .&bull; 
</p>
<p>1.2 Example. Let w be the outcome of a measurement of some physical quan-
</p>
<p>tity, for example, temperature. Then n = IR = (-00,00). One could argue that 
taking n = IR is not accurate since temperature has a lower bound. But there 
is usually no harm in taking the sample space to be larger than needed. The 
</p>
<p>event that the measurement is larger than 10 but less than or equal to 23 is 
</p>
<p>A = (10, 23] .&bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>4 1. Probability 
</p>
<p>1.3 Example. If we toss a coin forever , then the sample space is the infinite 
</p>
<p>set 
</p>
<p>n = {W = (WI,W2,W3 , ... ,): Wi E {H,T}}. 
</p>
<p>Let E be the event that the first head appears on the third toss. Then 
</p>
<p>E = { (WI, W2, W3, . . . ,) : WI = T, W2 = T, W3 = H, Wi E {H, T} for i &gt; 3}. &bull; 
</p>
<p>Given an event A, let AC = {w En: W 1:. A} denote the complement of 
</p>
<p>A. Informally, AC can be read as "not A." The complement of n is the empty 
set 0. The union of events A and B is defined 
</p>
<p>AU B = {w En: W E A or wEB or W E both} 
</p>
<p>which can be thought of as "A or B." If AI, A2 , .. . is a sequence of sets then 
</p>
<p>00 
</p>
<p>U A i = {W En: W E Ai for at least one i}. 
i=l 
</p>
<p>The intersection of A and B is 
</p>
<p>read "A and B ." Sometimes we write An Bas AB or (A, B) . If AI , A2 , .. . is 
</p>
<p>a sequence of sets then 
</p>
<p>00 
</p>
<p>n Ai = {W En: W E A i for all i}. 
i=l 
</p>
<p>The set difference is defined by A - B = {w: W E A, W 1:. B}. If every element 
</p>
<p>of A is also contained in B we write A c B or, equivalently, B J A. If A is a 
finite set, let IAI denote the number of elements in A. See the following table 
for a summary. 
</p>
<p>W 
</p>
<p>A 
AC 
</p>
<p>AUB 
AnB or AB 
A-B 
AcB 
o 
n 
</p>
<p>Summary of Terminology 
</p>
<p>sample space 
</p>
<p>outcome (point or element) 
</p>
<p>event (subset of n) 
complement of A (not A) 
union (A or B) 
</p>
<p>intersection (A and B) 
set difference (w in A but not in B) 
set inclusion 
</p>
<p>null event (always false) 
</p>
<p>true event (always true) </p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Probabili ty 5 
</p>
<p>We say that AI , A2, .. . are disjoint or are mutua lly exclusive if Ai n Aj = 
o whenever i =F j . For example, Al = [0, 1), A2 = [1, 2), A3 = [2, 3) , .. . arc 
disjoint . A partition of n i s a sequence of disjoint sets A I, A 2, .. . such t hat 
</p>
<p>U:I Ai = n. Given an event A , define the indicator function of A by 
</p>
<p>A sequence of sets At, A2, .. . is monotone increasing if A t C A2 C 
</p>
<p>and we define lirnn ..... oo A n = U: I Ai . A sequence of sets A t, A2 , ... is 
</p>
<p>monotone decreasing if A t ::l A2 ::l ... and then we define lirnn ..... oo A n = 
</p>
<p>n: 1 Ai&middot; In either case, we will write A n --+ A. 
</p>
<p>1.4 Example. Let n = IR and Jet Ai = [0, l /i) fori = 1,2, .... ThenU:l Ai = 
</p>
<p>[0, 1) and n: 1 Ai = {O}. If instead we define Ai = (0, I ii) then U:I Ai = 
(0, 1) and n: l A i = 0 . &bull; 
</p>
<p>1.3 Probability 
</p>
<p>We will assign a real number IP(A) to every event A, called the probability of 
</p>
<p>A. t We also call lP a probability distribution or a probability measure . 
</p>
<p>To qualify as a probability, IP must satisfy three axioms: 
</p>
<p>1.5 Definition. A function IP that assigns a real number IP(A) to each 
</p>
<p>event A is a probability distribution or a probability measure if it 
</p>
<p>satisfies the following three axioms: 
</p>
<p>Axiom 1: IP(A) 2: &deg; for every A 
Axiom 2: iP'(n ) = 1 
</p>
<p>Axiom 3: If A I, A2 , .. . are disjoint then 
</p>
<p>l it is not always possible to assign a probability to every event A if the sample space is large. 
such as the whole real line. Instead. we assign probabil it ies to a limited class of set called a 
</p>
<p>.,.&middot;field. See the appendi:&lt; for details . </p>
<p/>
</div>
<div class="page"><p/>
<p>6 1. Probability 
</p>
<p>There are many interpretations of IP'(A). The two common interpretations 
</p>
<p>are frequencies and degrees of beliefs. In the frequency interpretation, IP'( A) 
</p>
<p>is the long run proportion of times that A is true in repetitions. For example, 
</p>
<p>if we say that the probability of heads is 1/2, we mean that if we flip the 
</p>
<p>coin many times then the proportion of times we get heads tends to 1/2 as 
</p>
<p>the number of tosses increases. An infinitely long, unpredictable sequence of 
</p>
<p>tosses whose limiting proportion tends to a constant is an idealization, much 
</p>
<p>like the idea of a straight line in geometry. The degree-of-belief interpretation 
</p>
<p>is that IP'(A) measures an observer's strength of belief that A is true. In either 
</p>
<p>interpretation, we require that Axioms 1 to 3 hold. The difference in inter-
</p>
<p>pretation will not matter much until we deal with statistical inference. There, 
</p>
<p>the differing interpretations lead to two schools of inference: the frequentist 
</p>
<p>and the Bayesian schools. We defer discussion until Chapter 11. 
</p>
<p>One can derive many properties of IP' from the axioms, such as: 
</p>
<p>IP'( 0) 0 
</p>
<p>AcB ===? IP'(A) ~ IP'(B) 
</p>
<p>o~ IP'(A) ~1 
</p>
<p>IP'( A C) 1 -IP'(A) 
</p>
<p>An B =0 ===} IP' (A U B) = IP'(A) + IP'(B). (1.1 ) 
</p>
<p>A less obvious property is given in the following Lemma. 
</p>
<p>1.6 lemma. For any events A and B, 
</p>
<p>IP' (A U B) = IP'(A) + IP'(B) -IP'(AB). 
</p>
<p>PROOF. Write AUB = (ABC) U(AB) U(ACB) and note that these events 
</p>
<p>are disjoint. Hence, making repeated use of the fact that IP' is additive for 
</p>
<p>disjoint events, we see that 
</p>
<p>IP' (A U B) IP' ((ABC) U(AB) U(AC B)) 
= IP'(ABC) + IP'(AB) + IP'(ACB) 
</p>
<p>= IP'(ABC) + IP'(AB) + IP'(AC B) + IP'(AB) - IP'(AB) 
</p>
<p>P ((ABC) U(AB)) + IP' ((AC B) U(AB)) -IP'(AB) 
</p>
<p>IP'(A) + IP'(B) -IP'(AB). &bull; 
</p>
<p>1.7 Example. Two coin tosses. Let HI be the event that heads occurs on 
</p>
<p>toss 1 and let H2 be the event that heads occurs on toss 2. If all outcomes are </p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Probability on Finite Sample Spaces 7 
</p>
<p>&bull; 
</p>
<p>1.8 Theorem (Continuity of Probabilities). If An ~ A then 
</p>
<p>as n ~ 00. 
</p>
<p>PROOF. Suppose that An is monotone increasing so that Al C A2 C .... 
</p>
<p>Let A = limn -+oo An = U:I Ai. Define BI = AI, B2 = {w En: W E 
A2 ,w rJ. AI}, B3 = {w En: W E A3,w rJ. A2 ,w rJ. AI}, ... It can be 
</p>
<p>shown that B I , B 2 , ... are disjoint, An = U~=l Ai = U~=l Bi for each nand 
U:I Bi = U:I Ai&middot; (See exercise 1.) From Axiom 3, 
</p>
<p>and hence, using Axiom 3 again, 
</p>
<p>1.4 Probability on Finite Sample Spaces 
</p>
<p>Suppose that the sample space n = {WI, ... ,wn } is finite. For example, if we 
toss a die twice, then n has 36 elements: n = {(i,j); i,j E {I, ... 6}}. If each 
outcome is equally likely, then ]peA) = IAI/36 where IAI denotes the number 
of elements in A. The probability that the sum of the dice is 11 is 2/ 36 since 
</p>
<p>there are two outcomes that correspond to this event. 
</p>
<p>If n is finite and if each outcome is equally likely, then 
</p>
<p>IAI 
]peA) = TnT' 
</p>
<p>which is called the uniform probability distribution. To compute prob-
</p>
<p>abilities, we need to count the number of points in an event A. Methods for 
</p>
<p>counting points are called combinatorial methods. We needn't delve into these 
</p>
<p>in any great detail. We will, however, need a few facts from counting theory 
</p>
<p>that will be useful later. Given n objects, the number of ways of ordering </p>
<p/>
</div>
<div class="page"><p/>
<p>8 1. Probability 
</p>
<p>these objects is n! = n(n - 1)(n - 2)&middot;&middot;&middot;3&middot;2&middot; 1. For convenience, we define 
</p>
<p>O! = 1. We also define 
</p>
<p>( n) n! 
k - k!(n-k)!' 
</p>
<p>(1.2) 
</p>
<p>read "n choose k", which is the number of distinct ways of choosing k objects 
</p>
<p>from n. For example, if we have a class of 20 people and we want to select a 
</p>
<p>committee of 3 students, then there are 
</p>
<p>( 20) = 20! = 20 x 19 x 18 = 1140 
3 3!17! 3 x 2 x 1 
</p>
<p>possible committees. We note the following properties: 
</p>
<p>1.5 Independent Events 
</p>
<p>If we flip a fair coin twice, then the probability of two heads is ~ x ~. We 
</p>
<p>multiply the probabilities because we regard the two tosses as independent. 
</p>
<p>The formal definition of independence is as follows: 
</p>
<p>1.9 Definition. Two events A and B are independent if 
</p>
<p>JP'(AB) = JP'(A)JP'(B) (1.3) 
</p>
<p>and we write A Il B. A set of events {Ai: i E I} is independent if 
</p>
<p>for every finite subset J of I. If A and B are not independent, we write 
</p>
<p>A~B 
</p>
<p>Independence can arise in two distinct ways. Sometimes, we explicitly as-
</p>
<p>sume that two events are independent. For example, in tossing a coin twice, 
</p>
<p>we usually assume the tosses are independent which reflects the fact that the 
</p>
<p>coin has no memory of the first toss. In other instances, we derive indepen-
</p>
<p>dence by verifying that JP'(AB) = JP'(A)JP'(B) holds. For example, in tossing 
a fair die, let A = {2,4,6} and let B = {1,2,3,4}. Then, AnB = {2,4}, </p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Independent Events 9 
</p>
<p>IP'(AB) = 2/6 = IP'(A)IP'(B) = (1/2) X (2/3) and so A and B are independent. 
</p>
<p>In this case, we didn't assume that A and B are independent - it just turned 
</p>
<p>out that they were. 
</p>
<p>Suppose that A and B are disjoint events, each with positive probability. 
</p>
<p>Can they be independent? No. This follows since IP'(A)IP'(B) &gt; 0 yet IP'(AB) = 
1P'(0) = O. Except in this special case, there is no way to judge independence 
</p>
<p>by looking at the sets in a Venn diagram. 
</p>
<p>1.10 Example. Toss a fair coin 10 times. Let A ="at least one head." Let Tj 
</p>
<p>be the event that tails occurs on the jth toss. Then 
</p>
<p>IP'(A) 1 -1P'(AC) 
</p>
<p>1 - 1P'( all tails) 
</p>
<p>1 -1P'(T1T2 ... T lO ) 
</p>
<p>1 -1P'(TdlP'(T2) .. &middot;1P'(TlO ) using independence 
</p>
<p>(1) lO 1-"2 ~ .999. -
1.11 Example. Two people take turns trying to sink a basketball into a net. 
</p>
<p>Person 1 succeeds with probability 1/3 while person 2 succeeds with proba-
</p>
<p>bility 1/4. What is the probability that person 1 succeeds before person 2? 
</p>
<p>Let E denote the event of interest. Let Aj be the event that the first success 
</p>
<p>is by person 1 and that it occurs on trial number j. Note that AI, A2 , &bull;.&bull; are 
</p>
<p>disjoint and that E = Uj:1 A j . Hence, 
</p>
<p>00 
</p>
<p>IP'(E) = LIP'(Aj). 
j=l 
</p>
<p>Now, IP'(A1) = 1/3. A2 occurs if we have the sequence person 1 misses, person 
</p>
<p>2 misses, person 1 succeeds. This has probability IP'(A2) = (2/3)(3/4)(1/3) = 
</p>
<p>(1/2)(1/3). Following this logic we see that IP'(Aj) = (1/2)j-1(1/3). Hence, 
</p>
<p>00 1 (1)j-1 1 00 (1)j-1 
IP'(E) = L 3"2 = 3 L "2 
</p>
<p>j=l j=l 
</p>
<p>2 
</p>
<p>3 
</p>
<p>Here we used that fact that, if 0 &lt; r &lt; 1 then L:j:k r j = rk /(1 - r). _ </p>
<p/>
</div>
<div class="page"><p/>
<p>10 1. Probability 
</p>
<p>Summary of Independence 
</p>
<p>1. A and B are independent if and only if IP'(AB) = IP'(A)IP'(B). 
</p>
<p>2. Independence is sometimes assumed and sometimes derived. 
</p>
<p>3. Disjoint events with positive probability are not independent. 
</p>
<p>1.6 Conditional Probability 
</p>
<p>Assuming that IP'(B) &gt; 0, we define the conditional probability of A given 
</p>
<p>that B has occurred as follows: 
</p>
<p>1.12 Definition. IflP'(B) &gt; 0 then the conditional probability of A 
</p>
<p>given B is 
</p>
<p>IP'(AIB) = IP'(AB) 
IP'( B) . 
</p>
<p>(1.4) 
</p>
<p>Think of IP'(AIB) as the fraction of times A occurs among those in which 
</p>
<p>B occurs. For any fixed B such that IP'(B) &gt; 0, IP'('IB) is a probability (i.e., it 
</p>
<p>satisfies the three axioms of probability). In particular, IP'(AIB) ~ 0, 1P'(f2IB) = 
1 and if At, A2 , &bull;.. are disjoint then IP'(U:l AiIB) = L::IIP'(AiIB). But it 
</p>
<p>is in general not true that IP'(AIB U C) = IP'(AIB) + IP'(AIC) . The rules of 
probability apply to events on the left of the bar. In general it is not the case 
</p>
<p>that IP'(AIB) = IP'(BIA). People get this confused all the time. For example, 
</p>
<p>the probability of spots given you have measles is 1 but the probability that 
</p>
<p>you have measles given that you have spots is not 1. In this case, the difference 
</p>
<p>between IP'(AIB) and IP'(BIA) is obvious but there are cases where it is less 
</p>
<p>obvious. This mistake is made often enough in legal cases that it is sometimes 
</p>
<p>called the prosecutor's fallacy. 
</p>
<p>1.13 Example. A medical test for a disease D has outcomes + and -. The 
probabilities are: 
</p>
<p>+ .009 .099 
.001 .891 </p>
<p/>
</div>
<div class="page"><p/>
<p>1.6 Conditional Probability 11 
</p>
<p>From the definition of conditional probability, 
</p>
<p>JP'( +ID) = JP'( + n D) = .009 = .9 
JP'(D) .009 + .001 
</p>
<p>and 
</p>
<p>JP'( -IDC) = JP'( - n DC) = .891 ~ .9. 
JP'(Dc) .891 + .099 
</p>
<p>Apparently, the test is fairly accurate. Sick people yield a positive 90 percent 
</p>
<p>of the time and healthy people yield a negative about 90 percent of the time. 
</p>
<p>Suppose you go for a test and get a positive. What is the probability you have 
</p>
<p>the disease? Most people answer .90. The correct answer is 
</p>
<p>JP'(DI+) = JP'( + n D) = .009 ~ .08. 
JP'( + ) .009 + .099 
</p>
<p>The lesson here is that you need to compute the answer numerically. Don't 
</p>
<p>trust your intuition. _ 
</p>
<p>The results in the next lemma follow directly from the definition of condi-
</p>
<p>tional probability. 
</p>
<p>1.14 Lemma. If A and B are independent events then JP'(AIB) = JP'(A). Also, 
</p>
<p>for any pair of events A and B, 
</p>
<p>JP'(AB) = JP'(AIB)JP'(B) = JP'(BIA)JP'(A). 
</p>
<p>From the last lemma, we see that another interpretation of independence is 
</p>
<p>that knowing B doesn't change the probability of A. The formula JP'(AB) = 
JP'(A)JP'(BIA) is sometimes helpful for calculating probabilities. 
</p>
<p>1.15 Example. Draw two cards from a deck, without replacement. Let A be 
</p>
<p>the event that the first draw is the Ace of Clubs and let B be the event that 
</p>
<p>the second draw is the Queen of Diamonds. Then JP'(AB) = JP'(A)JP'(BIA) = 
(1/52) x (1/51). _ 
</p>
<p>Summary of Conditional Probability 
</p>
<p>1. If JP'(B) &gt; 0, then 
</p>
<p>JP'(AIB) = JP'(AB) 
JP'( B) . 
</p>
<p>2. JP'(&middot;IB) satisfies the axioms of probability, for fixed B. In general, 
</p>
<p>JP'(AI&middot;) does not satisfy the axioms of probability, for fixed A. 
</p>
<p>3. In general, JP'(AIB) -=I- JP'(BIA). </p>
<p/>
</div>
<div class="page"><p/>
<p>12 1. Probability 
</p>
<p>4. A and B are independent if and only if IP'(AIB) = IP'(A). 
</p>
<p>1.7 Bayes' Theorem 
</p>
<p>Bayes' theorem is the basis of "expert systems" and "Bayes' nets," which are 
</p>
<p>discussed in Chapter 17. First, we need a preliminary result. 
</p>
<p>1.16 Theorem (The Law of Total Probability). Let A l , ... , Ak be a partition 
</p>
<p>of 0.. Then, for any event B, 
</p>
<p>k 
</p>
<p>IP'(B) = LIP'(BIAi)IP'(Ai). 
</p>
<p>i=l 
</p>
<p>PROOF. Define OJ = BA j and note that 0 11 ... , Ok are disjoint and that 
</p>
<p>B = U;=l OJ. Hence, 
</p>
<p>j j j 
</p>
<p>since IP'(BAj) = IP'(BIAj)IP'(Aj) from the definition of conditional probability . 
</p>
<p>&bull; 
1.17 Theorem (Bayes' Theorem). Let A l , ... ,Ak be a partition of n such 
that IP'(Ai) &gt; 0 for each i. If IP'(B) &gt; 0 then, for each i = 1, ... ,k, 
</p>
<p>(1.5) 
</p>
<p>1.18 Remark. We call1P'(Ai) the prior probability of A and IP'(AiIB) the 
</p>
<p>posterior probability of A. 
</p>
<p>PROOF. We apply the definition of conditional probability twice, followed 
</p>
<p>by the law of total probability: 
</p>
<p>1.19 Example. I divide my email into three categories: Al = "spam," A2 = 
</p>
<p>"low priority" and A3 = "high priority." From previous experience I find that </p>
<p/>
</div>
<div class="page"><p/>
<p>1.8 Bibliographic Remarks 13 
</p>
<p>IP'(AI) = .7, IP'(A2) = .2 and IP'(A3) = .1. Of course, .7 +.2 +.1 = 1. Let B be 
the event that the email contains the word "free." From previous experience, 
</p>
<p>IP'(BIAd = .9, IP'(BIA2) = .01, IP'(BIAd = .01. (Note: .9 + .01 + .01 -=I- 1.) I 
receive an email with the word "free." What is the probability that it is spam? 
</p>
<p>Bayes' theorem yields, 
</p>
<p>( I ) .9 x .7 IP' Al B = = .995. &bull; 
(.9 x .7) + (.01 x .2) + (.01 x .1) 
</p>
<p>1.8 Bibliographic Remarks 
</p>
<p>The material in this chapter is standard. Details can be found in any number 
</p>
<p>of books. At the introductory level, there is DeGroot and Schervish (2002); 
</p>
<p>at the intermediate level, Grimmett and Stirzaker (1982) and Karr (1993); at 
</p>
<p>the advanced level there are Billingsley (1979) and Breiman (1992). I adapted 
</p>
<p>many examples and exercises from DeGroot and Schervish (2002) and Grim-
</p>
<p>mett and Stirzaker (1982). 
</p>
<p>1.9 Appendix 
</p>
<p>Generally, it is not feasible to assign probabilities to all subsets of a sample 
</p>
<p>space D. Instead, one restricts attention to a set of events called a a-algebra 
</p>
<p>or a a-field which is a class A that satisfies: 
</p>
<p>(i) 0 E A, 
</p>
<p>(ii) if AI, A2, ... , E A then U: I Ai E A and 
(iii) A E A implies that AC E A. 
</p>
<p>The sets in A are said to be measurable. We call (D, A) a measurable 
</p>
<p>space. If IP' is a probability measure defined on A, then (D, A, IP') is called a 
</p>
<p>probability space. When D is the real line, we take A to be the smallest 
</p>
<p>a-field that contains all the open subsets, which is called the Borel a-field. 
</p>
<p>1.10 Exercises 
</p>
<p>1. Fill in the details of the proof of Theorem 1.8. Also, prove the monotone 
</p>
<p>decreasing case. 
</p>
<p>2. Prove the statements in equation (1.1). </p>
<p/>
</div>
<div class="page"><p/>
<p>14 1. Probability 
</p>
<p>3. Let n be a sample space and let AI , A2, .. . , be events. Define Bn 
U:n Ai and Cn = n:n Ai&middot; 
(a) Show that Bl J B2 J ... and that C l C C2 C ... . 
</p>
<p>(b) Show that W E n:=l Bn if and only if w belongs to an infinite 
number of the events Ab A2, . . .. 
</p>
<p>(c) Show that w E U:=l Cn if and only if w belongs to all the events 
AI, A2,&middot;&middot; . except possibly a finite number of those events. 
</p>
<p>4. Let {Ai: i E I} be a collection of events where I is an arbitrary index 
</p>
<p>set. Show that 
</p>
<p>Hint: First prove this for I = {I , ... ,n}. 
</p>
<p>5. Suppose we toss a fair coin until we get exactly two heads. Describe 
</p>
<p>the sample space S. What is the probability that exactly k tosses are 
</p>
<p>required? 
</p>
<p>6. Let n = {O, 1, ... ,} . Prove that there does not exist a uniform distri-
bution on n (i.e. , if IF'(A) = IF'(B) whenever IAI = IBI, then IF' cannot 
satisfy the axioms of probability). 
</p>
<p>7. Let Ab A2, . .. be events. Show that 
</p>
<p>Hint: Define Bn = An - U:-: Ai. Then show that the Bn are disjoint 
and that U:=l An = U:=l Bn. 
</p>
<p>8. Suppose that IF'(Ai) = 1 for each i. Prove that 
</p>
<p>9. For fixed B such that IF'(B) &gt; 0, show that IF'(-IB) satisfies the axioms 
</p>
<p>of probability. 
</p>
<p>10. You have probably heard it before. Now you can solve it rigorously. 
</p>
<p>It is called the "Monty Hall Problem." A prize is placed at random </p>
<p/>
</div>
<div class="page"><p/>
<p>1.10 Exercises 15 
</p>
<p>behind one of three doors. You pick a door. To be concrete, let's suppose 
</p>
<p>you always pick door 1. Now Monty Hall chooses one of the other two 
</p>
<p>doors, opens it and shows you that it is empty. He then gives you the 
</p>
<p>opportunity to keep your door or switch to the other unopened door. 
</p>
<p>Should you stay or switch? Intuition suggests it doesn't matter. The 
</p>
<p>correct answer is that you should switch. Prove it. It will help to specify 
</p>
<p>the sample space and the relevant events carefully. Thus write n = 
{(Wl,W2): Wi E {1,2,3}} where Wl is where the prize is and W2 is the 
</p>
<p>door Monty opens. 
</p>
<p>11. Suppose that A and B are independent events. Show that AC and B C 
</p>
<p>are independent events. 
</p>
<p>12. There are three cards. The first is green on both sides, the second is red 
</p>
<p>on both sides and the third is green on one side and red on the other. We 
</p>
<p>choose a card at random and we see one side (also chosen at random) . 
</p>
<p>If the side we see is green, what is the probability that the other side is 
</p>
<p>also green? Many people intuitively answer 1/2. Show that the correct 
</p>
<p>answer is 2/3. 
</p>
<p>13. Suppose that a fair coin is tossed repeatedly until both a head and tail 
</p>
<p>have appeared at least once. 
</p>
<p>(a) Describe the sample space n. 
</p>
<p>(b) What is the probability that three tosses will be required? 
</p>
<p>14. Show that if peA) = 0 or peA) = 1 then A is independent of every other 
</p>
<p>event. Show that if A is independent of itself then peA) is either 0 or 1. 
</p>
<p>15. The probability that a child has blue eyes is 1/4. Assume independence 
</p>
<p>between children. Consider a family with 3 children. 
</p>
<p>(a) If it is known that at least one child has blue eyes, what is the 
</p>
<p>probability that at least two children have blue eyes? 
</p>
<p>(b) If it is known that the youngest child has blue eyes, what is the 
</p>
<p>probability that at least two children have blue eyes? 
</p>
<p>16. Prove Lemma 1.14. 
</p>
<p>17. Show that 
</p>
<p>JP'(ABC) = JP'(AIBC)P(BIC)P(C). </p>
<p/>
</div>
<div class="page"><p/>
<p>16 1. Probability 
</p>
<p>18. Suppose k events form a partition of the sample space n, i.e., they 
are disjoint and U7=I Ai = n. Assume that IP'(B) &gt; O. Prove that if 
IP'(AIIB) &lt; IP'(AI) then IP'(AiIB) &gt; IP'(Ai) for some i = 2, ... , k. 
</p>
<p>19. Suppose that 30 percent of computer owners use a Macintosh, 50 percent 
</p>
<p>use Windows, and 20 percent use Linux. Suppose that 65 percent of 
</p>
<p>the Mac users have succumbed to a computer virus, 82 percent of the 
</p>
<p>Windows users get the virus, and 50 percent of the Linux users get 
</p>
<p>the virus. We select a person at random and learn that her system was 
</p>
<p>infected with the virus. What is the probability that she is a Windows 
</p>
<p>user? 
</p>
<p>20. A box contains 5 coins and each has a different probability of show-
</p>
<p>ing heads. Let PI, .. . , P5 denote the probability of heads on each coin. 
</p>
<p>Suppose that 
</p>
<p>PI = 0, P2 = 1/4, P3 = 1/2, P4 = 3/4 and P5 = 1. 
</p>
<p>Let H denote "heads is obtained" and let Ci denote the event that coin 
</p>
<p>i is selected. 
</p>
<p>(a) Select a coin at random and toss it. Suppose a head is obtained. 
</p>
<p>What is the posterior probability that coin i was selected (i = 1, ... , 5)? 
</p>
<p>In other words, find IP'(CiIH) for i = 1, ... ,5. 
</p>
<p>(b) Toss the coin again. What is the probability of another head? In 
</p>
<p>other words find IP'(H2IHd where H j = "heads on toss j." 
</p>
<p>Now suppose that the experiment was carried out as follows: We select 
</p>
<p>a coin at random and toss it until a head is obtained. 
</p>
<p>(c) Find IP'(Ci IB4) where B4 = "first head is obtained on toss 4." 
</p>
<p>21. (Computer Experiment.) Suppose a coin has probability P of falling heads 
</p>
<p>up. If we flip the coin many times, we would expect the proportion of 
</p>
<p>heads to be near p. We will make this formal later. Take P = .3 and 
</p>
<p>n = 1,000 and simulate n coin flips. Plot the proportion of heads as a 
</p>
<p>function of n. Repeat for P = .03. 
</p>
<p>22. (Computer Experiment.) Suppose we flip a coin n times and let P denote 
</p>
<p>the probability of heads. Let X be the number of heads. We call X 
</p>
<p>a binomial random variable, which is discussed in the next chapter. 
</p>
<p>Intuition suggests that X will be close to n p. To see if this is true, we 
</p>
<p>can repeat this experiment many times and average the X values. Carry </p>
<p/>
</div>
<div class="page"><p/>
<p>1.10 Exercises 17 
</p>
<p>out a simulation and compare the average of the X's to n p . Try this for 
</p>
<p>p =.3 and n = 10, n = 100, and n = 1,000. 
</p>
<p>23. (Computer Experiment.) Here we will get some experience simulating 
</p>
<p>conditional probabilities. Consider tossing a fair die. Let A = {2, 4, 6} 
</p>
<p>and B = {l, 2, 3, 4}. Then, lP'(A) = 1/2, lP'(B) = 2/3 and lP'(AB) = 1/3. 
</p>
<p>Since lP'(AB) = lP'(A)lP'(B), the events A and B are independent. Simu-
late draws from the sample space and verify that P(AB) = P(A)P(B) 
</p>
<p>where P(A) is the proportion of times A occurred in the simulation and 
</p>
<p>similarly for P(AB) and P(B). Now find two events A and B that are not 
</p>
<p>independent. Compute P(A),P(B) and P(AB). Compare the calculated 
</p>
<p>values to their theoretical values. Report your results and interpret. </p>
<p/>
</div>
<div class="page"><p/>
<p>2 
</p>
<p>Random Variables 
</p>
<p>2.1 Introduction 
</p>
<p>Statistics and data mining are concerned with data. How do we link sample 
</p>
<p>spaces and events to data? The link is provided by the concept of a random 
</p>
<p>variable. 
</p>
<p>2.1 Definition. A random variable is a mapping! 
</p>
<p>that assigns a real number X(w) to each outcome w. 
</p>
<p>At a certain point in most probability courses, the sample space is rarely 
</p>
<p>mentioned anymore and we work directly with random variables. But you 
</p>
<p>should keep in mind that the sample space is really there, lurking in the 
</p>
<p>background. 
</p>
<p>2.2 Example. Flip a coin ten times. Let X(w) be the number of heads in the 
</p>
<p>sequence w. For example, if w = HHTHHTHHTT, then X(w) = 6 .&bull; 
</p>
<p>1 Technically, a random variable must be measurable. See the appendix for deta ils. </p>
<p/>
</div>
<div class="page"><p/>
<p>20 2. Random Variables 
</p>
<p>2.3 Example. Let n = { (X , y); x2 + y2 S I} be the unit disk. Consider 
</p>
<p>drawing a point at random from n. (We will make this idea more precise 
later.) A typical outcome is of the form w = (x,y) . Some examples of random 
</p>
<p>variables are X(w) = x , Y(w) = y, Z(w) = x + y, and W(w) = Jx2 + y2 . &bull; 
</p>
<p>Given a random variable X and a subset A of the real line, define X - I (A) = 
</p>
<p>{w En: X(w) E A} and let 
</p>
<p>PiX E A) ~ P(X-'(A)) ~ P&laquo;w E 11; X(w) E A)) 
</p>
<p>PiX ~ x) ~ P(X - '(x)) ~ P({w E 11; X(w) ~ x)). 
</p>
<p>Notice that X denotes the random variable a nd x denotes a particular value 
</p>
<p>of X. 
</p>
<p>2.4 Example. Flip a coin twice and let X be the number of heads. Then , 
</p>
<p>PiX ~ 0) ~ P({TT ) ) ~ 1/4, PiX ~ 1) ~ P({ HT,TH)) ~ 1/2 and 
</p>
<p>IP'( X = 2) = JP'( {HH} ) = 1/4. The random variable and its distribution 
</p>
<p>can be summarized as follows: 
</p>
<p>w P({w)) 
</p>
<p>TT 1/ 4 
</p>
<p>T H 1/4 
</p>
<p>HT 1/4 
</p>
<p>HH 1/4 
</p>
<p>X(w) 
</p>
<p>o 
1 
</p>
<p>1 
</p>
<p>2 
</p>
<p>Try generalizing this to n flips . &bull; 
</p>
<p>x P(X ~ x) 
o 1/ 4 
1 1/ 2 
</p>
<p>2 1/4 
</p>
<p>2.2 Distribution Functions and Probability Functions 
</p>
<p>Given a random variable X, we define the cumulat ive distribution function 
</p>
<p>(or distribution function) as follows. 
</p>
<p>2.5 Definition. The cumulative d istribution functio n, 01' CDF, is the 
</p>
<p>function F x : IR -+ [0, 11 defined by 
</p>
<p>Fx(x) ~ PiX ,; x). (2.1) </p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Distribution Functions and Probability Functions 21 
</p>
<p>Fx(x) 
</p>
<p>1 
</p>
<p>. 75 
</p>
<p>.50 
</p>
<p>&bull; 
&bull;&bull; -----co 
</p>
<p>.25 .... ---() 
</p>
<p>o 1 2 
FIGURE 2.1. CDF for flipping a coin twice (Example 2.6.) 
</p>
<p>x 
</p>
<p>We will see later that the CDF effectively contains all the information about 
</p>
<p>the random variable. Sometimes we write the CDF as F instead of Fx. 
</p>
<p>2.6 Example. Flip a fair coin twice and let X be the number of heads. Then 
</p>
<p>JP'(X = 0) = JP'(X = 2) = 1/4 and JP'(X = 1) = 1/2. The distribution function 
</p>
<p>is 
</p>
<p>{
</p>
<p>Ox &lt; 0 
1/40:S;x&lt;1 
</p>
<p>Fx(x) = 3/41:S;x&lt;2 
</p>
<p>1 x ~ 2. 
</p>
<p>The CDF is shown in Figure 2.1. Although this example is simple, study it 
</p>
<p>carefully. CDF's can be very confusing. Notice that the function is right contin-
</p>
<p>uous, non-decreasing, and that it is defined for all x, even though the random 
</p>
<p>variable only takes values 0,1, and 2. Do you see why Fx(1.4) = .757 &bull; 
</p>
<p>The following result shows that the CDF completely determines the distri-
</p>
<p>bution of a random variable. 
</p>
<p>2.7 Theorem. Let X have CDF F and let Y have CDF G. If F(x) = G(x) for 
</p>
<p>all x, then JP'(X E A) = JP'(Y E A) for all A. 2 
</p>
<p>2.8 Theorem. A function F mapping the real line to [0,1] is a CDF for some 
</p>
<p>probability JP' if and only if F satisfies the following three conditions: 
</p>
<p>(i) F is non-decreasing: Xl &lt; X2 implies that F(Xl) :s; F(X2)' 
</p>
<p>(ii) F is normalized: 
</p>
<p>lim F(x) = 0 
x----+-CX) 
</p>
<p>2Technically, we only have that JlD(X E A) = JlD(Y E A) for every measurable event A. </p>
<p/>
</div>
<div class="page"><p/>
<p>22 2. Random Variables 
</p>
<p>and 
</p>
<p>lim F(x) = l. 
x-+oo 
</p>
<p>(iii) F is right-continuous: F(x) = F(x+) for all x, where 
</p>
<p>F(x+) = lim F(y). 
</p>
<p>PROOF. Suppose that F is a CDF. Let us show that (iii) holds. Let x be 
</p>
<p>a real number and let YI, Y2,'" be a sequence of real numbers such that 
</p>
<p>YI &gt; Y2 &gt; ... and limiYi = x. Let Ai = (-OO,Yi] and let A = (-oo,x]. Note 
</p>
<p>that A = n: l Ai and also note that Al =:&gt; A2 =:&gt; .. '. Because the events are 
</p>
<p>monotone, limi IP'(Ad = lP'(ni Ai). Thus, 
</p>
<p>Showing (i) and (ii) is similar. Proving the other direction - namely, that if 
</p>
<p>F satisfies (i), (ii), and (iii) then it is a CDF for some random variable - uses 
</p>
<p>some deep tools in analysis. _ 
</p>
<p>2.9 Definition. X is discrete if it takes countably3 many values 
</p>
<p>{Xl, X2, .. . }. We define the probability function or probability mass 
</p>
<p>function for X by fx(x) = IP'(X = x). 
</p>
<p>Thus, f x (x) :;0. 0 for all x E lR and Li f x (Xi) = l. Sometimes we write f 
</p>
<p>instead of Ix. The CDF of X is related to fx by 
</p>
<p>Fx(x) = IP'(X ::; x) = L fX(Xi). 
X'i::;X 
</p>
<p>2.10 Example. The probability function for Example 2.6 is 
</p>
<p>See Figure 2.2. _ 
</p>
<p>{ 
</p>
<p>1/4 x = 0 
1/2 x = 1 
</p>
<p>fx(x) = 1/4 x = 2 
</p>
<p>o otherwise. 
</p>
<p>3 A set is countable if it is finite or it can be put in a one-to-one correspondence with the 
</p>
<p>integers. The even numbers, the odd numbers, and the rationals are countable; the set of real 
</p>
<p>numbers between 0 and 1 is not countable. </p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Distribution Functions and Probability Functions 23 
</p>
<p>fx(x) 
</p>
<p>1 
</p>
<p>.75 
</p>
<p>. 5 
</p>
<p>.25 
</p>
<p>&bull; 
</p>
<p>o 1 2 X 
FIGURE 2.2. Probability function for flipping a coin twice (Example 2.6). 
</p>
<p>2.11 Definition. A random variable X is continuous if there exists a 
</p>
<p>function fx such that fx(x) 20 for all x, J~oo fx(x)dx = 1 and for 
</p>
<p>every a :::; b, 
/b 
</p>
<p>W(a &lt; X &lt; b) = J
a 
</p>
<p>fx(x)dx. (2.2) 
</p>
<p>The function fx is called the probability density function (PDF). We 
</p>
<p>have that 
</p>
<p>Fx(x) = iXoo fx(t)dt 
</p>
<p>and fx(x) = F'x(x) at all points x at which Fx is differentiable. 
</p>
<p>Sometimes we write J f(x)dx or J f to mean J~oo f(x)dx. 
</p>
<p>2.12 Example. Suppose that X has PDF 
</p>
<p>{
I for 0 &lt; x &lt; 1 
</p>
<p>fx(x) = 0 other;ise.-
</p>
<p>Clearly, f x (x) 2 0 and J f x (x )dx = 1. A random variable with this density 
is said to have a Uniform (0,1) distribution. This is meant to capture the idea 
</p>
<p>of choosing a point at random between 0 and 1. The CDF is given by 
</p>
<p>Fx(x) ~ { ~ 
</p>
<p>See Figure 2.3 .&bull; 
</p>
<p>x&lt;O 
</p>
<p>O:::;x:::;1 
</p>
<p>x&gt;1. </p>
<p/>
</div>
<div class="page"><p/>
<p>24 2. Random Variables 
</p>
<p>Fx(x) 
</p>
<p>1 
</p>
<p>o 1 
FIGURE 2.3. CDF for Uniform (0,1). 
</p>
<p>2.13 Example. Suppose that X has PDF 
</p>
<p>{ 
0 
</p>
<p>f(x) = 1 
(1+x)2 
</p>
<p>for x &lt; 0 
otherwise. 
</p>
<p>Since f f(x)dx = 1, this is a well-defined PDF .&bull; 
</p>
<p>x 
</p>
<p>Warning! Continuous random variables can lead to confusion. First, note 
</p>
<p>that if X is continuous then JP'(X = x) = 0 for every x. Don't try to think 
</p>
<p>of f(x) as JP'(X = x). This only holds for discrete random variables. We get 
</p>
<p>probabilities from a PDF by integrating. A PDF can be bigger than 1 (unlike 
</p>
<p>a mass function). For example, if f(x) = 5 for x E [0,1/5] and 0 otherwise, 
</p>
<p>then f(x) ~ 0 and f f(x)dx = 1 so this is a well-defined PDF even though 
f(x) = 5 in some places. In fact, a PDF can be unbounded. For example, if 
</p>
<p>f(x) = (2/3)x- 1/ 3 for 0 &lt; x &lt; 1 and f(x) = 0 otherwise, then f f(x)dx = 1 
even though f is not bounded. 
</p>
<p>2.14 Example. Let 
</p>
<p>. { 0 j(x) = _1_ 
(1+x) 
</p>
<p>for x &lt; 0 
otherwise. 
</p>
<p>This is not a PDF since f f(x)dx = fooo dx/(l +x) = J~oo du/u = log(oo) = 00 . 
</p>
<p>&bull; 
</p>
<p>2.15 lemma. Let F be the CDF for a random variable X. Then: 
</p>
<p>1. JP'(X = x) = F(x) - F(x-) where F(x-) = limytx F(y); </p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Some Important Discrete Random Variables 25 
</p>
<p>2. lP'(x &lt; X ::; y) = F(y) - F(x); 
</p>
<p>3. lP'(X &gt; x) = 1 - F(x); 
</p>
<p>4. If X is continuous then 
</p>
<p>F(b) - F(a) lP'(a &lt; X &lt; b) = lP'(a ::; X &lt; b) 
</p>
<p>lP'(a &lt; X ::; b) = lP'(a ::; X::; b). 
</p>
<p>It is also useful to define the inverse CDF (or quantile function). 
</p>
<p>2.16 Definition. Let X be a random variable with CDF F. The inverse 
</p>
<p>CDF or quantile function is defined by4 
</p>
<p>F- 1 (q) = inf{ x: F(x) &gt; q} 
</p>
<p>for q E [0,1]. If F is strictly increasing and continuous then F- 1 (q) is the 
</p>
<p>unique real number x such that F(x) = q. 
</p>
<p>We call F- 1 (1/4) the first quartile, F- 1 (1/2) the median (or second 
</p>
<p>quartile), and F- 1 (3/4) the third quartile. 
</p>
<p>Two random variables X and Yare equal in distribution - written 
</p>
<p>X ~ Y - if Fx(x) = Fy(x) for all x. This does not mean that X and Yare 
</p>
<p>equal. Rather, it means that all probability statements about X and Y will 
</p>
<p>be the same. For example, suppose that lP'(X = 1) = lP'(X = -1) = 1/2. Let 
d 
</p>
<p>Y = -X. Then lP'(Y = 1) = lP'(Y = -1) = 1/2 and so X = Y. But X and Y 
</p>
<p>are not equal. In fact, lP'(X = Y) = O. 
</p>
<p>2.3 Smne Irnportant Discrete Randmn Variables 
</p>
<p>Warning About Notation! It is traditional to write X rv F to indicate 
</p>
<p>that X has distribution F. This is unfortunate notation since the symbol rv 
</p>
<p>is also used to denote an approximation. The notation X rv F is so pervasive 
</p>
<p>that we are stuck with it. Read X rv F as "X has distribution F" not as "X 
</p>
<p>is approximately F" . 
</p>
<p>41f you are unfamiliar with "inf", just think of it as the minimum. </p>
<p/>
</div>
<div class="page"><p/>
<p>26 2. Random Variables 
</p>
<p>THE POINT MASS DISTRIBUTION. X has a point mass distribution at a, 
</p>
<p>written X rv 6a , if J1D(X = a) = 1 in which case 
</p>
<p>F(x) = { ~ x&lt;a 
x:;:, a. 
</p>
<p>The probability mass function is f(x) = 1 for x = a and 0 otherwise. 
</p>
<p>THE DISCRETE UNIFORM DISTRIBUTION. Let k &gt; 1 be a given integer. 
</p>
<p>Suppose that X has probability mass function given by 
</p>
<p>f(x) = { ~/k for x = 1, ... , k 
otherwise. 
</p>
<p>We say that X has a uniform distribution on {I, ... , k}. 
</p>
<p>THE BERNOULLI DISTRIBUTION. Let X represent a binary coin flip. Then 
</p>
<p>J1D(X = 1) = p and J1D(X = 0) = 1 - p for some p E [0,1]. We say that X has a 
</p>
<p>Bernoulli distribution written X rv Bernoulli(p). The probability function is 
</p>
<p>f(x) = pX(l - p)l-X for x E {O, I}. 
</p>
<p>THE BINOMIAL DISTRIBUTION. Suppose we have a coin which falls heads 
</p>
<p>up with probability p for some 0 s: p s: 1. Flip the coin n times and let 
X be the number of heads. Assume that the tosses are independent. Let 
</p>
<p>f(x) = J1D(X = x) be the mass function. It can be shown that 
</p>
<p>{ 
(n) X(l )n-x 
</p>
<p>f(x)= ox P -p for x = 0, ... ,n 
otherwise. 
</p>
<p>A random variable with this mass function is called a Binomial random 
</p>
<p>variable and we write X rv Binomial(n,p). If Xl rv Binomial(nl,p) and 
</p>
<p>X 2 rv Binomial(n2,p) then Xl + X 2 rv Binomial(nl + n2,p). 
</p>
<p>Warning! Let us take this opportunity to prevent some confusion. X is a 
</p>
<p>random variable; x denotes a particular value of the random variable; nand p 
</p>
<p>are parameters, that is, fixed real numbers. The parameter p is usually un-
</p>
<p>known and must be estimated from data; that's what statistical inference is all 
</p>
<p>about. In most statistical models, there are random variables and parameters: 
</p>
<p>don't confuse them. 
</p>
<p>THE GEOMETRIC DISTRIBUTION. X has a geometric distribution with 
</p>
<p>parameter p E (0, 1), written X rv Geom (p), if 
</p>
<p>J1D(X = k) = p(l - p)k-\ k:;:, 1. </p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Some Important Continuous Random Variables 27 
</p>
<p>We have that 
</p>
<p>Think of X as the number of flips needed until the first head when flipping a 
</p>
<p>coin. 
</p>
<p>THE POISSON DISTRIBUTION. X has a Poisson distribution with parameter 
</p>
<p>A, written X rv Poisson(A) if 
</p>
<p>Note that 
00 00 AX 
L f(x) = e- A L , = e-AeA = 1. 
</p>
<p>x. 
x=o x=o 
</p>
<p>The Poisson is often used as a model for counts of rare events like radioactive 
</p>
<p>decay and traffic accidents. If Xl rv Poisson(Ad and X 2 rv Poisson(A2) then 
</p>
<p>Xl + X 2 rv PoiSSOn(Al + A2)' 
</p>
<p>Warning! We defined random variables to be mappings from a sample 
</p>
<p>space D to Jl{ but we did not mention the sample space in any of the distri-
</p>
<p>butions above. As I mentioned earlier, the sample space often "disappears" 
</p>
<p>but it is really there in the background. Let's construct a sample space ex-
</p>
<p>plicitly for a Bernoulli random variable. Let D = [0,1] and define IF' to satisfy 
</p>
<p>1F'([a, b]) = b - a for &deg; ::; a::; b::; 1. Fix p E [0,1] and define 
X(w) = { ~ w ::; p w &gt; p. 
</p>
<p>Then IF'(X = 1) = IF'(w ::; p) = 1F'([O,p]) = p and IF'(X = 0) = 1 - p. Thus, 
</p>
<p>X rv Bernoulli(p). We could do this for all the distributions defined above. In 
</p>
<p>practice, we think of a random variable like a random number but formally it 
</p>
<p>is a mapping defined on some sample space. 
</p>
<p>2.4 Smne Irnportant Continuous Randmn Variables 
</p>
<p>THE UNIFORM DISTRIBUTION. X has a Uniform(a, b) distribution, written 
</p>
<p>X rv Uniform(a, b), if 
</p>
<p>for x E [a, b] 
otherwise </p>
<p/>
</div>
<div class="page"><p/>
<p>28 2. Random Variables 
</p>
<p>where a &lt; b. The distribution function is 
</p>
<p>x&lt;a 
x E [a,b] 
x&gt; b. 
</p>
<p>NORMAL (GAUSSIAN). X has a Normal (or Gaussian) distribution with 
</p>
<p>parameters IL and (T, denoted by X rv N(fL, (T2), if 
</p>
<p>f(x) = _1_ exp { __ I (x _ fL)2} , 
(TV2]r 2(T2 
</p>
<p>xElR (2.3) 
</p>
<p>where fL E lR and (T &gt; O. The parameter fL is the "center" (or mean) of the 
</p>
<p>distribution and (T is the "spread" (or standard deviation) of the distribu-
</p>
<p>tion. (The mean and standard deviation will be formally defined in the next 
</p>
<p>chapter.) The Normal plays an important role in probability and statistics. 
</p>
<p>Many phenomena in nature have approximately Normal distributions. Later, 
</p>
<p>we shall study the Central Limit Theorem which says that the distribution of 
</p>
<p>a sum of random variables can be approximated by a Normal distribution. 
</p>
<p>We say that X has a standard Normal distribution if fL = 0 and (T = l. 
</p>
<p>Tradition dictates that a standard Normal random variable is denoted by Z. 
</p>
<p>The PDF and CDF of a standard Normal are denoted by &cent;(z) and &lt;1&gt;(z). The 
</p>
<p>PDF is plotted in Figure 2.4. There is no closed-form expression for &lt;1&gt;. Here 
</p>
<p>are some useful facts: 
</p>
<p>(i) If X rv N(fL, (T2), then Z = (X - fL)/(T rv N(O, 1). 
</p>
<p>(ii) If Z rv N(O, 1), then X = fL + (T Z rv N(fL, (T2). 
</p>
<p>(iii) If Xi rv N(ILi, (T;), i = 1, ... , n are independent, then 
</p>
<p>It follows from (i) that if X rv N(fL, (T2), then 
</p>
<p>IF' (a &lt; X &lt; b) IF'(a:IL &lt;Z&lt; b:/L) 
</p>
<p>Thus we can compute any probabilities we want as long as we can compute 
</p>
<p>the CDF &lt;1&gt;(z) of a standard Normal. All statistical computing packages will </p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Some Important Continuous Random Variables 29 
</p>
<p>-2 -1 o 1 2 
</p>
<p>FIGURE 2.4. Density of a standard Normal. 
</p>
<p>compute &lt;I&gt;(z) and &lt;I&gt;-1(q). Most statistics texts, including this one, have a 
</p>
<p>table of values of &lt;I&gt;(z). 
</p>
<p>2.17 Example. Suppose that X rv N(3, 5). Find IF'(X &gt; 1). The solution is 
</p>
<p>IF'(X&gt; 1) = 1 -IF'(X &lt; 1) = 1 -IF' (z &lt; 1 ;g3) = 1 - &lt;I&gt; ( -0.8944) = 0.81. 
Now find q = &lt;I&gt;-1(0.2). This means we have to find q such that IF'(X &lt; q) = 
</p>
<p>0.2. We solve this by writing 
</p>
<p>0.2 = IF'(X &lt; q) = IF' ( Z &lt; q: jt) = &lt;I&gt; ( q : jt) . 
</p>
<p>From the Normal table, &lt;I&gt; ( -0.8416) = 0.2. Therefore, 
</p>
<p>-0.8416 = q - jt = q - 3 
(J" y'5 
</p>
<p>and hence q = 3 - 0.8416y'5 = 1.1181. &bull; 
</p>
<p>EXPONENTIAL DISTRIBUTION. X has an Exponential distribution with 
</p>
<p>parameter (3, denoted by X rv Exp((3), if 
</p>
<p>1 
f(x) = lJe-x/(3, x&gt; 0 
</p>
<p>where (3 &gt; o. The exponential distribution is used to model the lifetimes of 
electronic components and the waiting times between rare events. 
</p>
<p>GAMMA DISTRIBUTION. For a &gt; 0, the Gamma function is defined by 
r(a) = Jooo yO'.-1e-Y dy. X has a Gamma distribution with parameters a and </p>
<p/>
</div>
<div class="page"><p/>
<p>30 2. Random Variables 
</p>
<p>fJ, denoted by X rv Gamma( 0:, fJ), if 
</p>
<p>_ 1 a-I -x/(3 
f(x) - fJaf(o:) X e , x&gt;o 
</p>
<p>where 0:, fJ &gt; O. The exponential distribution is just a Gamma(l, fJ) distribu-
</p>
<p>tion. If Xi rv Gamma(O:i, fJ) are independent, then 2:~=1 Xi rv Gamma(2:~=l O:i, fJ). 
</p>
<p>THE BETA DISTRIBUTION. X has a Beta distribution with parameters 
</p>
<p>0: &gt; 0 and fJ &gt; 0, denoted by X rv Beta( 0:, fJ), if 
</p>
<p>( ) _ f(o: + fJ) a-l( , )(3-1 f X - f(o:)r(fJ) X 1 - X ,0 &lt; :x; &lt; 1. 
</p>
<p>t AND CAUCHY DISTRIBUTION. X has a t distribution with z; degrees of 
</p>
<p>freedom - written X rv tv - if 
</p>
<p>X = f (f) 1 
f() f (':'.) ( 2) (v+l)/2' 
</p>
<p>2 1 + ";, 
</p>
<p>The t distribution is similar to a Normal but it has thicker tails. In fact, the 
</p>
<p>Normal corresponds to a t with z; = 00. The Cauchy distribution is a special 
</p>
<p>case of the t distribution corresponding to z; = 1. The density is 
</p>
<p>. 1 
j(x) = 'if(l + x2)" 
</p>
<p>To see that this is indeed a density: 
</p>
<p>I: f(x)dx .!. /00 ~ = .!. /00 dtan-1(x) 'if -00 1 + x 2 'if -00 dx 
1 [-1 -1 1 1 ['if ('if)] ;: tan (00) -tan (-00) =;: "2 - -"2 = 1. 
</p>
<p>THE X2 DISTRIBUTION. X has a X2 distribution with p degrees of freedom 
</p>
<p>- written X rv X~ - if 
</p>
<p>f(x) = / \ /2x(P/2)-le-X/2 x&gt; O. 
f(p 2 2P 
</p>
<p>If Zl,"" Zp are independent standard Normal random variables then 2:f=l Z; rv 
2 
</p>
<p>Xp' </p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Bivariate Distributions 31 
</p>
<p>2.5 Bivariate Distributions 
</p>
<p>Given a pair of discrete random variables X and Y, define the joint mass 
</p>
<p>function by f(x, y) = IP'(X = x and Y = y). From now on, we write IP'(X = 
</p>
<p>x and Y = y) as IP'(X = x, Y = y). We write f as fx,y when we want to be 
</p>
<p>more explicit. 
</p>
<p>2.18 Example. Here is a bivariate distribution for two random variables X 
</p>
<p>and Y each taking values 0 or 1: 
</p>
<p>x=o 1/9 
X=1 2/9 
</p>
<p>1/3 
</p>
<p>Y = 1 
</p>
<p>2/9 
</p>
<p>4/9 
</p>
<p>2/3 
</p>
<p>Thus, f(l, 1) = IP'(X = 1, Y = 1) = 4/9 .&bull; 
</p>
<p>1/3 
</p>
<p>2/3 
</p>
<p>1 
</p>
<p>2.19 Definition. In the continuous case, we call a function f(x, y) a PDF 
</p>
<p>for the random variables (X, Y) if 
</p>
<p>(i) f(x, y) 20 for all (x, y), 
</p>
<p>(ii) J~= J~= f(x, y)dxdy = 1 and, 
</p>
<p>(iii) for any set A c Jl{ x Jl{, IP'((X, Y) E A) = J J~ f(x, y)dxdy. 
</p>
<p>In the discrete or continuous case we define the joint CDF as Fx,Y(x, y) = 
</p>
<p>IP'(X S x, Y S y). 
</p>
<p>2.20 Example. Let (X, Y) be uniform on the unit square. Then, 
</p>
<p>f(x, y) = {I if 0 S ~ S 1, 0 S Y S 1 
o otherwIse. 
</p>
<p>Find IP'(X &lt; 1/2, Y &lt; 1/2). The event A = {X &lt; 1/2, Y &lt; 1/2} corresponds 
to a subset of the unit square. Integrating f over this subset corresponds, in 
</p>
<p>this case, to computing the area ofthe set A which is 1/4. So, IP'(X &lt; 1/2, Y &lt; 
1/2) = 1/4 .&bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>32 2. Random Variables 
</p>
<p>2.21 Example. Let (X, Y) have density 
</p>
<p>f(x ) = { x + y if 0 -&lt;; x -&lt;; 1, 0 -&lt;; Y -&lt;; 1 
, Y 0 otherwise. 
</p>
<p>Then 
</p>
<p>{I {I 
Jo Jo (x + y)dxdy 10
</p>
<p>1 [10 1 X dX] dy + 101 [10 1 Y dX] dy 
</p>
<p>{I ~dy + {I Y dy = ~ + ~ = 1 
Jo 2 Jo 2 2 
</p>
<p>which verifies that this is a PDF. 
</p>
<p>2.22 Example. If the distribution is defined over a non-rectangular region, 
</p>
<p>then the calculations are a bit more complicated. Here is an example which I 
</p>
<p>borrowed from DeGroot and Schervish (2002). Let (X, Y) have density 
</p>
<p>. { C x 2 y if x 2 -&lt;; y -&lt;; 1 
j(x, y) = 0 otherwise. 
</p>
<p>Note first that -1 -&lt;; x -&lt;; 1. Now let us find the value of c. The trick here is 
</p>
<p>to be careful about the range of integration. We pick one variable, x say, and 
</p>
<p>let it range over its values. Then, for each fixed value of x, we let y vary over 
</p>
<p>its range, which is x 2 -&lt;; Y -&lt;; 1. It may help if you look at Figure 2.5. Thus, 
</p>
<p>C x 2 Y dy dx = C x 2 ~dx = -.:.:... 11 [;,1] ;.1 1 4 4 
-1 x 2 -1 2 21 
</p>
<p>Hence, c = 21/4. Now let us compute J1D(X ~ Y). This corresponds to the set 
</p>
<p>A = {(x, y); 0 -&lt;; x -&lt;; 1, x 2 -&lt;; Y -&lt;; x}. (You can see this by drawing a diagram.) 
</p>
<p>So, 
</p>
<p>J1D(X ~ Y) - x 2 Y dy dx = - x 2 Y dy dx 21 111x 21 11 [l X ] 
4 0 x 2 4 0 x 2 
</p>
<p>- x 2 dx = - .&bull; 
21 j.l (x2 - x4) 3 
4 0 2 20 </p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Marginal Distributions 33 
</p>
<p>o 1 
x 
</p>
<p>FIGURE 2.5. The light shaded region is x 2 ::; y ::; 1. The density is positive over 
</p>
<p>this region. The hatched region is the event X 2&gt; Y intersected with x 2 ::; y ::; 1. 
</p>
<p>2.6 Marginal Distributions 
</p>
<p>2.23 Definition. If (X, Y) have joint distribution with mass function 
</p>
<p>fx,Y, then the marginal mass function for X is defined by 
</p>
<p>fx(x) = JlD(X = x) = L JlD(X = x, Y = y) = L f(x, y) (2.4) 
y y 
</p>
<p>and the marginal mass function for Y is defined by 
</p>
<p>fy(y) = JlD(Y = y) = L JlD(X = x, Y = y) = L f(x, y). (2.5) 
x x 
</p>
<p>2.24 Example. Suppose that fx,Y is given in the table that follows. The 
</p>
<p>marginal distribution for X corresponds to the row totals and the marginal 
</p>
<p>distribution for Y corresponds to the columns totals. 
</p>
<p>x=o 1/10 
X=l 3/10 
</p>
<p>4/10 
</p>
<p>Y= 1 
</p>
<p>2/10 
</p>
<p>4/10 
</p>
<p>6/10 
</p>
<p>For example, fx(O) = 3/10 and fx(1) = 7/10 .&bull; 
</p>
<p>3/10 
</p>
<p>7/10 
</p>
<p>1 </p>
<p/>
</div>
<div class="page"><p/>
<p>34 2. Random Variables 
</p>
<p>2.25 Definition. For continuous random variables, the marginal densities 
</p>
<p>are 
</p>
<p>fx(x) = J f(x, y)dy, and jy(y) = J f(x, y)dx. (2.6) 
The corresponding marginal distribution functions are denoted by Fx and 
</p>
<p>Fy. 
</p>
<p>2.26 Example. Suppose that 
</p>
<p>fx,Y(x, y) = e-(x+y) 
</p>
<p>2.27 Example. Suppose that 
</p>
<p>Then 
</p>
<p>f(x, y) = { &middot;Ox + y if 0 ::; x ::; 1, 0::; y ::; 1 
otherwise. 
</p>
<p>11 11 11 1 jy(y) = (x+y)dx= xdx+ ydx=-+y._ 
o 00 2 
</p>
<p>2.28 Example. Let (X, Y) have density 
</p>
<p>Thus, 
</p>
<p>{ 
21 x 2 y if x 2 ::; y ::; 1 
</p>
<p>f(x, y) = 04 
otherwise. 
</p>
<p>. J 21 211 21 2 4 jx(x)= f(x,y)dy=-x ydy=-x(1-x) 
4 X2 8 
</p>
<p>for -1 ::; x ::; 1 and fx(x) = 0 otherwise. _ 
</p>
<p>2.7 Independent Randorn Variables 
</p>
<p>2.29 Definition. Two random variables X and Yare independent if, 
</p>
<p>for every A and B, 
</p>
<p>W(X E A, Y E B) = W(X E A)W(Y E B) (2.7) 
</p>
<p>and we write X II Y. Otherwise we say that X and Yare dependent 
</p>
<p>and we write X IQ6OO' Y. </p>
<p/>
</div>
<div class="page"><p/>
<p>2.7 Independent Random Variables 35 
</p>
<p>In principle, to check whether X and Yare independent we need to check 
</p>
<p>equation (2.7) for all subsets A and B. Fortunately, we have the following 
</p>
<p>result which we state for continuous random variables though it is true for 
</p>
<p>discrete random variables too. 
</p>
<p>2.30 Theorem. Let X and Y have joint PDF fx,Y. Then X IT Y if and only 
</p>
<p>if fx,Y(x, y) = fx(x)Jy(y) for all values x and y. 5 
</p>
<p>2.31 Example. Let X and Y have the following distribution: 
</p>
<p>X=O 
</p>
<p>X=1 
</p>
<p>1/4 
1/4 
</p>
<p>1/2 
</p>
<p>Y = 1 
</p>
<p>1/4 
1/4 
</p>
<p>1/2 
</p>
<p>1/2 
</p>
<p>1/2 
</p>
<p>1 
</p>
<p>Then, fx(O) = fx(l) = 1/2 and Jy(O) = Jy(I) = 1/2. X and Yare inde-
</p>
<p>pendent because fx(O)Jy(O) = f(O,O), fx(O)Jy(I) = f(O, 1), fx(I)Jy(O) = 
</p>
<p>f(l, 0), fx(I)Jy(I) = f(l, 1). Suppose instead that X and Y have the follow-
</p>
<p>ing distribution: 
</p>
<p>Y=O Y = 1 
X=O 1/2 0 1/2 
</p>
<p>X=1 0 1/2 1/2 
</p>
<p>1/2 1/2 1 
</p>
<p>These are not independent because fx(O)fy(l) 
</p>
<p>f(O, 1) = O .&bull; 
</p>
<p>(1/2)(1/2) 1/4 yet 
</p>
<p>2.32 Example. Suppose that X and Yare independent and both have the 
</p>
<p>same density 
</p>
<p>. {2X if 0 &lt; x &lt; 1 
j (x) = 0 othe;'-wis;' 
</p>
<p>Let us find JP'(X + Y ~ 1). Using independence, the joint density is 
</p>
<p>f( ) = f ( )f ( ) = {4XY if 0 ~ x ~ 1, 0 ~ y ~ 1 
x, y x x Y Y 0 otherwise. 
</p>
<p>5The statement is not rigorous because the density is defined only up to sets of 
</p>
<p>measure o. </p>
<p/>
</div>
<div class="page"><p/>
<p>36 2. Random Variables 
</p>
<p>Now, 
</p>
<p>W(X + Y &lt;::: 1) J 1 f(x, y)dydx 
x+y:'Ol 
</p>
<p>411 x [11-X YdY] dx 
4 r1 x (1 x)2 dx =~ .&bull; 
</p>
<p>Jo 2 6 
</p>
<p>The following result is helpful for verifying independence. 
</p>
<p>2.33 Theorem. Suppose that the range of X and Y is a (possibly infinite) 
</p>
<p>rectangle. If f(x, y) = g(x)h(y) for some functions g and h (not necessarily 
</p>
<p>probability density functions) then X and Yare independent. 
</p>
<p>2.34 Example. Let X and Y have density 
</p>
<p>_ { 2e-(x+2y) 
f(x,y) - 0 
</p>
<p>if x &gt; 0 and y &gt; 0 
otherwise. 
</p>
<p>The range of X and Y is the rectangle (0, (0) x (0, (0). We can write f(x, y) = 
</p>
<p>g(x)h(y) where g(x) = 2e- X and h(y) = e- 2y . Thus, X II Y .&bull; 
</p>
<p>2.8 Conditional Distributions 
</p>
<p>If X and Yare discrete, then we can compute the conditional distribution of 
</p>
<p>X given that we have observed Y = y. Specifically, W(X = xlY = y) = W(X = 
</p>
<p>x, Y = y)jW(Y = y). This leads us to define the conditional probability mass 
</p>
<p>function as follows. 
</p>
<p>2.35 Definition. The conditional probability mass function is 
</p>
<p>W(X = x, Y = y) 
fXIY(xly) = W(X = xlY = y) = ( ) 
</p>
<p>WY=y 
</p>
<p>if Jy(y) &gt; O. 
</p>
<p>fx,Y(x, y) 
</p>
<p>Jy(y) 
</p>
<p>For continuous distributions we use the same definitions. 6 The interpre-
</p>
<p>tation differs: in the discrete case, fXIY(xly) is W(X = xlY = y), but in the 
</p>
<p>continuous case, we must integrate to get a probability. 
</p>
<p>6We are treading in deep water here. When we compute J1D(X E AIY = y) in the 
</p>
<p>continuous case we are conditioning on the event {Y = y} which has probability O. We </p>
<p/>
</div>
<div class="page"><p/>
<p>2.8 Conditional Distributions 37 
</p>
<p>2.36 Definition. For continuous random variables, the conditional 
</p>
<p>probability density function is 
</p>
<p>f (I) - fx,Y(x,y) 
XIY X y - jy(y) 
</p>
<p>assuming that jy(y) &gt; O. Then, 
</p>
<p>lP'(X E AIY = y) = i fXly(xly)dx. 
</p>
<p>2.37 Example. Let X and Y have a joint uniform distribution on the unit 
</p>
<p>square. Thus, fXly(xly) = 1 for 0 &lt;::: x &lt;::: 1 and 0 otherwise. Given Y = y, X 
</p>
<p>is Uniform(O, 1). We can write this as XIY = y rv Uniform(O, 1) .&bull; 
</p>
<p>From the definition of the conditional density, we see that fx,Y(x, y) 
</p>
<p>fXIY(xly)jy(y) = jylx(Ylx)fx(x). This can sometimes be useful as in exam-
</p>
<p>ple 2.39. 
</p>
<p>2.38 Example. Let 
</p>
<p>{
X + y if 0 &lt;::: x &lt;::: 1, 0 &lt;::: y &lt;::: 1 
</p>
<p>f(x,y)= 0 h' ot erWlse. 
</p>
<p>Let us find lP'(X &lt; 1/41Y 
</p>
<p>y + (1/2). Hence, 
1/3). In example 2.27 we saw that jy(y) 
</p>
<p>f (I) - fx,y(x,y) 
XIY x y - jy(y) 
</p>
<p>So, 
</p>
<p>11/4 fxlY (x I ~) dx 
</p>
<p>11/4 X + 1. ..l.. +..l.. 3 d 32 12 -1--1 X = 1 1 
o 3+2 3+2 
</p>
<p>11 
</p>
<p>80 &bull; 
</p>
<p>2.39 Example. Suppose that X rv Uniform(O, 1). After obtaining a value of 
</p>
<p>X we generate YIX = x rv Uniform(x, 1). What is the marginal distribution 
</p>
<p>avoid this problem by defining things in terms of the PDF. The fact that this leads to 
</p>
<p>a well-defined theory is proved in more advanced courses. Here, we simply take it as a 
</p>
<p>definition. </p>
<p/>
</div>
<div class="page"><p/>
<p>38 2. Random Variables 
</p>
<p>of Y? First note that, 
</p>
<p>and 
</p>
<p>So, 
</p>
<p>fx(x) = { ~ if 0 ::; x ::; 1 otherwise 
</p>
<p>ifO&lt;x&lt;y&lt;1 
</p>
<p>otherwise. 
</p>
<p>{ 
_1_ ifO&lt;x&lt;y&lt;1 
</p>
<p>fx,Y(x, y) = fylx(Ylx)fx(x) = 3- x otherwise. 
The marginal for Y is 
</p>
<p>l y lY dx J1-Y du fy(y) = fx,Y(x, y)dx = --, = - - = -log(1 - y) 
o 0 I-x 1 U 
</p>
<p>for 0 &lt; y &lt; 1. &bull; 
</p>
<p>2.40 Example. Consider the density in Example 2.28. Let's find fYlx(ylx). 
</p>
<p>When X = x, y must satisfy x 2 ::; y ::; 1. Earlier, we saw that fx(x) 
</p>
<p>(21/8)x 2 (1 - x 4 ). Hence, for x 2 ::; Y ::; 1, 
</p>
<p>2y 
</p>
<p>1- x 4 ' 
</p>
<p>Now let us compute W(Y ::;, 3/41X = 1/2). This can be done by first noting 
</p>
<p>that fylx(yll/2) = 32y/15. Thus, 
</p>
<p>W(Y ::;, 3/41 X = 1/2) = r1 f(yll/2)dy = r1 32y dy =~ .&bull; 
J3/4 J3/4 15 15 
</p>
<p>2.9 Multivariate Distributions and lID Sarnples 
</p>
<p>Let X = (Xl, ... ,Xn ) where Xl, ... ,Xn are random variables. We call X a 
</p>
<p>random vector. Let f(X1,"" x n ) denote the PDF. It is possible to define 
</p>
<p>their marginals, conditionals etc. much the same way as in the bivariate case. 
</p>
<p>We say that Xl"'" Xn are independent if, for every A 1, ... , An, 
</p>
<p>n 
</p>
<p>W(X1 E A 1, ... , Xn E An) = II W(Xi E A;). (2.8) 
i=l 
</p>
<p>It suffices to check that f(X1,"" x n ) = TI~=l fXi (Xi). </p>
<p/>
</div>
<div class="page"><p/>
<p>2.10 Two Important Multivariate Distributions 39 
</p>
<p>2.41 Definition. If Xl' ... ' Xn are independent and each has the same 
</p>
<p>marginal distribution with CDF F, we say that Xl, ... ,Xn are IID 
</p>
<p>(independent and identically distributed) and we write 
</p>
<p>If F has density f we also write Xl, ... Xn rv f. We also call Xl'&middot;&middot;&middot;' Xn 
</p>
<p>a random sample of size n from F. 
</p>
<p>Much of statistical theory and practice begins with IID observations and we 
</p>
<p>shall study this case in detail when we discuss statistics. 
</p>
<p>2.10 Two Irnportant Multivariate Distributions 
</p>
<p>MULTINOMIAL. The multivariate version of a Binomial is called a Multino-
</p>
<p>mial. Consider drawing a ball from an urn which has balls with k different 
</p>
<p>colors labeled "color 1, color 2, ... , color k." Let p = (PI, ... ,pd where 
Pj ~ 0 and L~=l Pj = 1 and suppose that Pj is the probability of drawing 
</p>
<p>a ball of color j. Draw n times (independent draws with replacement) and 
</p>
<p>let X = (Xl' ... ' X k ) where Xj is the number of times that color j appears. 
</p>
<p>Hence, n = L~=l Xj. We say that X has a Multinomial (n,p) distribution 
</p>
<p>written X rv Multinomial(n,p). The probability function is 
</p>
<p>(2.9) 
</p>
<p>where 
n! 
</p>
<p>2.42 Lemma. Suppose that X rv Multinomial(n,p) where X = (Xl, ... , Xk) 
</p>
<p>and p = (PI, ... ,Pk). The marginal distribution of Xj is Binomial (n,pj). 
</p>
<p>MULTIVARIATE NORMAL. The univariate Normal has two parameters, It 
</p>
<p>and a. In the multivariate version, fL is a vector and a is replaced by a matrix 
</p>
<p>~. To begin, let </p>
<p/>
</div>
<div class="page"><p/>
<p>40 2. Random Variables 
</p>
<p>where Zl,' .. , Zk rv N(O, 1) are independent. The density of Z is 7 
</p>
<p>f(z) k { k } g f(zi) = (27f~k/2 exp -~ f; zJ 
(27f~k/2 exp { _~zT z } . 
</p>
<p>We say that Z has a standard multivariate Normal distribution written Z rv 
</p>
<p>N(O, I) where it is understood that 0 represents a vector of k zeroes and I is 
</p>
<p>the k x k identity matrix. 
</p>
<p>More generally, a vector X has a multivariate Normal distribution, denoted 
</p>
<p>by X rv N(,t, ~), if it has density 8 
</p>
<p>f(x; fL,~) = (27f)k/2~(~)ll/2 exp {-~(x - fL)T~-l(x - fL)} (2.10) 
</p>
<p>where I~I denotes the determinant of ~, fL is a vector of length k and ~ is a 
</p>
<p>k x k symmetric, positive definite matrix. 9 Setting fL = 0 and ~ = I gives 
</p>
<p>back the standard Normal. 
</p>
<p>Since ~ is symmetric and positive definite, it can be shown that there exists 
</p>
<p>a matrix ~1/2 - called the square root of ~ - with the following properties: 
</p>
<p>(i) ~1/2 is symmetric, (ii) ~ = ~1/2~1/2 and (iii) ~1/2~-1/2 = ~-1/2~1/2 = I 
</p>
<p>where ~-1/2 = (~1/2)-1. 
</p>
<p>2.43 Theorem. If Z rv N(O,I) and X = fL + ~1/2 Z then X rv N(,t, ~). 
Conversely, if X rv N(,t, ~), then ~-1/2(X -It) rv N(O, I). 
</p>
<p>Suppose we partition a random Normal vector X as X = (Xa, X b) We can 
</p>
<p>similarly partition fL = (fLa, fLb) and 
</p>
<p>~ = (~aa ~ab). 
~ba ~bb 
</p>
<p>2.44 Theorem. Let X rv N(fL, ~). Then: 
</p>
<p>(1) The marginal distribution of Xa is Xa rv N(fLa, ~aa). 
</p>
<p>(2) The conditional distribution of Xb given Xa = Xa is 
</p>
<p>XblXa = Xa rv N ( fLb + ~ba~~;(Xa - fLa), ~bb - ~ba~~;~ab ) . 
</p>
<p>(3) Ifa is a vector then aTX rv N(aTfL,aT~a). 
</p>
<p>(4) V = (X - fL)T~-l(X -It) rv x%. 
</p>
<p>71f a and b are vectors then aTb = 2:~=1 aibi . 
8~-1 is the inverse of the matrix ~. 
</p>
<p>9A matrix ~ is positive definite if, for all nonzero vectors x, xT~x &gt; o. </p>
<p/>
</div>
<div class="page"><p/>
<p>2.11 Transformations of Random Variables 41 
</p>
<p>2.11 Transforrnations of Randorn Variables 
</p>
<p>Suppose that X is a random variable with PDF fx and CDF Fx. Let Y = r(X) 
</p>
<p>be a function of X, for example, Y = X 2 or Y = eX. We call Y = r(X) a 
</p>
<p>transformation of X. How do we compute the PDF and CDF of Y? In the 
</p>
<p>discrete case, the answer is easy. The mass function of Y is given by 
</p>
<p>fy(y) lP'(Y = y) = lP'(r(X) = y) 
</p>
<p>lP'({x; r(x) = y}) = lP'(X E r- 1 (y)). 
</p>
<p>2.45 Example. Suppose that lP'(X = -1) = lP'(X = 1) = 1/4 and lP'(X = 0) = 
</p>
<p>1/2. Let Y = X2. Then, lP'(Y = 0) = lP'(X = 0) = 1/2 and lP'(Y = 1) = lP'(X = 
</p>
<p>1) + lP'(X = -1) = 1/2. Summarizing: 
x fx(x) 
</p>
<p>-1 1/4 
</p>
<p>o 1/2 
1 1/4 
</p>
<p>y 
</p>
<p>o 
1 
</p>
<p>fy(y) 
</p>
<p>1/2 
</p>
<p>1/2 
</p>
<p>Y takes fewer values than X because the transformation is not one-to-one. _ 
</p>
<p>The continuous case is harder. There are three steps for finding fy: 
</p>
<p>Three Steps for Transformations 
</p>
<p>1. For each y, find the set Ay = {x: r(x):::; y}. 
</p>
<p>2. Find the CDF 
</p>
<p>Fy(y) 
</p>
<p>3. The PDF is fy(y) = FHy). 
</p>
<p>lP'(Y :::; y) = lP'(r(X) :::; y) 
</p>
<p>lP'({x; r(x):::; y}) 
</p>
<p>I fx(x)dx. 
A" 
</p>
<p>(2.11) 
</p>
<p>2.46 Example. Let j"x(x) = e- x for x &gt; O. Hence, Fx(x) = J~T fx(s)ds = 
</p>
<p>1- e- x . Let Y = r(X) = logX. Then, Ay = {x: x:::; eY} and 
</p>
<p>Fy(y) lP'(Y :::; y) = lP'(logX :::; y) 
</p>
<p>lP'(X :::; eY) = Fx(eY) = 1 - e-e". 
</p>
<p>Therefore, fy(y) = eYe-e Y for y E R _ </p>
<p/>
</div>
<div class="page"><p/>
<p>42 2. Random Variables 
</p>
<p>2.47 Example. Let X rv Uniform ( -1,3). Find the PDF of Y 
</p>
<p>density of X is 
</p>
<p>fx(x) = {1/4 if - 1 ~ x &lt; 3 
o otherwlse. 
</p>
<p>Y can only take values in (0,9). Consider two cases: (i) 0 &lt; y &lt; 1 and (ii) 1 &lt;::: 
</p>
<p>y &lt; 9. For case (i), Ay = [-JY, JYl and Fy(y) = fA fx(x)dx = (1/2)JY. 
For case (ii), Ay = [-1, JYl and Fy(y) = J~ fx(x)dx = (1/4)(JY + 1). 
</p>
<p>y 
</p>
<p>Differentiating F we get 
</p>
<p>{ 
</p>
<p>4~ if 0 &lt; y &lt; 1 
</p>
<p>Jy(y) = 8~ if 1 &lt; y &lt; 9 
</p>
<p>o otherwise._ 
</p>
<p>When r is strictly monotone increasing or strictly monotone decreasing then 
</p>
<p>r has an inverse s = r- 1 and in this case one can show that 
</p>
<p>I
</p>
<p>dS(Y) I Jy(y) = fx(s(y)) ----;J;Jj . (2.12) 
</p>
<p>2.12 Transformations of Several Randorn Variables 
</p>
<p>In some cases we are interested in transformations of several random variables. 
</p>
<p>For example, if X and Yare given random variables, we might want to know 
</p>
<p>the distribution of X/Y, X + Y, max{X, Y} or min {X, Y}. Let Z = r(X, Y) 
be the function of interest. The steps for finding fz are the same as before: 
</p>
<p>Three Steps for Transformations 
</p>
<p>1. For each z, find the set A z = {(x,y): r(x,y) &lt;::: z}. 
</p>
<p>2. Find the CDF 
</p>
<p>Fz(z) IP'(Z &lt;::: z) = lP'(r(X, Y) &lt;::: z) 
</p>
<p>1P'({(x,y); r(x,y) &lt;::: z}) = / t, fX,y(x,y)dxdy. 
3. Then fz(z) = F~(z). </p>
<p/>
</div>
<div class="page"><p/>
<p>2.13 Appendix 43 
</p>
<p>2.48 Example. Let X I ,X2 rv Uniform(O, 1) be independent. Find the density 
</p>
<p>of Y = Xl + X 2 . The joint density of (Xl, X 2 ) is 
</p>
<p>f( ) {
I 0 &lt; Xl &lt; 1, 0 &lt; X2 &lt; 1 
</p>
<p>Xl, X2 = 0 otherwise. 
</p>
<p>Fy(y) IF'(Y &lt;::: y) = 1F'(r-(XI ,X2 ) &lt;::: y) 
</p>
<p>1F'({(XI,X2): r-(XI,X2) &lt;::: y}) = J r f(XI,X2) dx l dx2. 
JAy 
</p>
<p>Now comes the hard part: finding Ay. First suppose that 0 &lt; y &lt;::: 1. Then Ay 
</p>
<p>is the triangle with vertices (0,0), (y, 0) and (0, y). See Figure 2.6. In this case, 
</p>
<p>J JAy f(XI, X2) dx l dx2 is the area of this triangle which is y2/2. If 1 &lt; y &lt; 2, 
then Ay is everything in the unit square except the triangle with vertices 
</p>
<p>(1, y - 1), (1, 1), (y - 1,1). This set has area 1 - (2 - y)2/2. Therefore, 
</p>
<p>Fy(y) ~ 1 
0 y&lt;O 
</p>
<p>OJ y-
O&lt;:::y&lt;l ""2 
</p>
<p>1 _ (2_y)2 1&lt;:::y&lt;2 
2 
</p>
<p>1 Y ?:. 2. 
</p>
<p>By differentiation, the PDF is 
</p>
<p>{ 
y O&lt;:::y&lt;:::l 
</p>
<p>Jy(y) = 2 - y 1 &lt;::: y &lt;::: 2 
</p>
<p>o otherwise._ 
</p>
<p>2.13 Appendix 
</p>
<p>Recall that a probability measure IF' is defined on a a-field A of a sample 
</p>
<p>space n. A random variable X is a measurable map X : n -+ R Measurable 
means that, for every X, {w: X(w) &lt;::: x} E A. 
</p>
<p>2.14 Exercises 
</p>
<p>1. Show that </p>
<p/>
</div>
<div class="page"><p/>
<p>44 2. Random Variables 
</p>
<p>(y - 1, 1) 
1 ,----------, 1 
</p>
<p>(O ,y) (1, Y - 1) 
</p>
<p>&deg; &deg; o 1 o 1 
This is the case 0 :::; y &lt; 1. This is the case 1 :::; Y :::; 2. 
</p>
<p>FIGURE 2.6. The set Ay for example 2.48. Ay consists of all points (Xl,X2) in the 
square below the line X2 = Y - Xl. 
</p>
<p>2. Let X be such that J1D(X = 2) = J1D(X = 3) = 1/10 and J1D(X = 5) = 8/10. 
</p>
<p>Plot the CDF F. Use F to find J1D(2 &lt; X :::; 4.8) and J1D(2 :::; X :::; 4.8). 
</p>
<p>3. Prove Lemma 2.15. 
</p>
<p>4. Let X have probability density function 
</p>
<p>{ 
1/4 0 &lt; x &lt; 1 
</p>
<p>fx(x) = 3/8 3 &lt; x &lt; 5 
o otherwise. 
</p>
<p>(a) Find the cumulative distribution function of X. 
</p>
<p>(b) Let Y = 1/ X. Find the probability density function jy (y) for Y. 
</p>
<p>Hint: Consider three cases: i :::; y :::; ~, ~ :::; y :::; 1, and y ::;&gt; 1. 
</p>
<p>5. Let X and Y be discrete random variables. Show that X and Yare 
</p>
<p>independent if and only if fx,Y(x,y) = fx(x)jy(y) for all x and y. 
</p>
<p>6. Let X have distribution F and density function f and let A be a subset 
of the real line. Let I A (x) be the indicator function for A: 
</p>
<p>Let Y = IA(X), Find an expression for the cumulative distribution of 
</p>
<p>Y. (Hint: first find the probability mass function for Y.) </p>
<p/>
</div>
<div class="page"><p/>
<p>2.14 Exercises 45 
</p>
<p>7. Let X and Y be independent and suppose that each has a Uniform(O, 1) 
</p>
<p>distribution. Let Z = min{X, Y}. Find the density fz(z) for Z. Hint: 
</p>
<p>It might be easier to first find J1D( Z &gt; z). 
</p>
<p>8. Let X have CDF F. Find the CDF of X+ = max{O, X}. 
</p>
<p>9. Let X rv Exp(,6). Find F(x) and F-I(q). 
</p>
<p>10. Let X and Y be independent. Show that g(X) is independent of h(Y) 
</p>
<p>where 9 and h are functions. 
</p>
<p>11. Suppose we toss a coin once and let p be the probability of heads. Let 
</p>
<p>X denote the number of heads and let Y denote the number of tails. 
</p>
<p>(a) Prove that X and Yare dependent. 
</p>
<p>(b) Let N rv Poisson(&gt;.) and suppose we toss a coin N times. Let X and 
</p>
<p>Y be the number of heads and tails. Show that X and Yare independent. 
</p>
<p>12. Prove Theorem 2.33. 
</p>
<p>13. Let X rv N(O, 1) and let Y = eX. 
</p>
<p>(a) Find the PDF for Y. Plot it. 
</p>
<p>(b) (Computer Experiment.) Generate a vector x = (Xl, ... ,XlO,OOO) con-
</p>
<p>sisting of 10,000 random standard Normals. Let Y = (YI,"" YIO,OOO) 
</p>
<p>where Yi = eX;. Draw a histogram of Y and compare it to the PDF you 
</p>
<p>found in part (a). 
</p>
<p>14. Let (X, Y) be uniformly distributed on the unit disk {(x, y) : x2 + y2 ~ 
I}. Let R = -JX2 + y2. Find the CDF and PDF of R. 
</p>
<p>15. (A universal random number generator.) Let X have a continuous, strictly 
</p>
<p>increasing CDF F. Let Y = F(X). Find the density of Y. This is called 
</p>
<p>the probability integral transform. Now let U rv Uniform(O,l) and let 
</p>
<p>X = F-I(U). Show that X rv F. Now write a program that takes 
</p>
<p>Uniform (0,1) random variables and generates random variables from 
</p>
<p>an Exponential (,6) distribution. 
</p>
<p>16. Let X rv Poisson(&gt;.) and Y rv Poisson(lL) and assume that X and Yare 
</p>
<p>independent. Show that the distribution of X given that X + Y = n is 
Binomial(n, 'if) where 'if = &gt;.j(&gt;. + IL). </p>
<p/>
</div>
<div class="page"><p/>
<p>46 2. Random Variables 
</p>
<p>Hint 1: You may use the following fact: If X rv Poisson(&gt;.) and Y rv 
</p>
<p>Poisson(/L), and X and Yare independent, then X +Y rv Poisson(tt+&gt;'). 
</p>
<p>Hint 2: Note that {X = x, X + Y = n} = {X = x, Y = n - x}. 
</p>
<p>17. Let 
</p>
<p>f . (. ) _ { c(x + y2) X.Y x,y - 0 
</p>
<p>Find P (X &lt; ~ I Y = ~). 
</p>
<p>o :::; x :::; 1 and 0 :::; y :::; 1 
otherwise. 
</p>
<p>18. Let X rv N(3, 16). Solve the following using the Normal table and using 
</p>
<p>a computer package. 
</p>
<p>(a) Find JII'(X &lt; 7). 
</p>
<p>(b) Find JII'(X &gt; -2). 
</p>
<p>(c) Find x such that JII'(X &gt; x) = .05. 
</p>
<p>(d) Find JII'(O :::; X &lt; 4). 
</p>
<p>(e) Find x such that JII'(IXI &gt; Ixl) = .05. 
</p>
<p>19. Prove formula (2.12). 
</p>
<p>20. Let X, Y rv Uniform(O, 1) be independent. Find the PDF for X - Y and 
</p>
<p>X/Yo 
</p>
<p>21. Let Xl, ... ,Xn rv Exp(,6) be IID. Let Y = max{X1, ... ,Xn }. Find the 
</p>
<p>PDF of Y. Hint: Y :::; y if and only if Xi :::; y for 'i = 1, ... , n. </p>
<p/>
</div>
<div class="page"><p/>
<p>3 
</p>
<p>Expectation 
</p>
<p>3.1 Expectation of a Randorn Variable 
</p>
<p>The mean, or expectation, of a random variable X is the average value of x. 
</p>
<p>3.1 Definition. The expected value, or mean, or first moment, of 
</p>
<p>X is defined to be 
</p>
<p>lE(X) = jXdF(X) = { Lxxf(x) 
J xf(x)dx 
</p>
<p>if X is discrete 
</p>
<p>if X is continuous 
(3.1) 
</p>
<p>assuming that the sum (or integral) is well defined. We use the following 
</p>
<p>notation to denote the expected value of x: 
</p>
<p>lE(X) = lEX = j xdF(x) = M = Mx&middot; (3.2) 
</p>
<p>The expectation is a one-number summary of the distribution. Think of 
</p>
<p>lE(X) as the average L~=l X;/n of a large number of IID draws Xl, ... ,Xn . 
</p>
<p>The fact that lE(X) ~ L~=l X;/n is actually more than a heuristic; it is a 
</p>
<p>theorem called the law of large numbers that we will discuss in Chapter 5. 
</p>
<p>The notation J x dF (x) deserves some comment. We use it merely as a 
convenient unifying notation so we don't have to write Lx xf(x) for discrete </p>
<p/>
</div>
<div class="page"><p/>
<p>48 3. Expectation 
</p>
<p>random variables and I xf(x)dx for continuous random variables, but you 
should be aware that I x dF (x) has a precise meaning that is discussed in real 
analysis courses. 
</p>
<p>To ensure that lE(X) is well defined, we say that lE(X) exists if Ix IxldFx (x) &lt; 
00. Otherwise we say that the expectation does not exist. 
</p>
<p>3.2 Example. Let X '" Bernoulli(p). Then lE(X) = L;=o xf(x) = (0 x (1 -
p)) + (1 x p) = p. _ 
</p>
<p>3.3 Example. Flip a fair coin two times. Let X be the number of heads. Then, 
</p>
<p>lE(X) = I xdFx(x) = Lx xfx(x) = (0 x f(O)) + (1 x f(l)) + (2 x f(2)) = 
(0 x (1/4)) + (1 x (1/2)) + (2 x (1/4)) = 1. _ 
</p>
<p>3.4 Example. Let x'" Uniform( -1,3). Then, lE(X) = I xdFx(x) = I xfx(x)dx = 
i I~l xdx = 1. -
</p>
<p>3.5 Example. Recall that a random variable has a Cauchy distribution if it 
</p>
<p>has density fx(x) = {7f(1 + X2)}-1. Using integration by parts, (set u = x 
and v = tan- 1 x), 
</p>
<p>J 2 ;'00 xdx 00 100 IxldF(x) = - --2 = [x tan-1(x)jo - tan- 1 xdx = 00 
7f 0 l+x 0 
</p>
<p>so the mean does not exist. If you simulate a Cauchy distribution many times 
</p>
<p>and take the average, you will see that the average never settles down. This 
</p>
<p>is because the Cauchy has thick tails and hence extreme observations are 
</p>
<p>common. _ 
</p>
<p>From now on, whenever we discuss expectations, we implicitly assume that 
</p>
<p>they exist. 
</p>
<p>Let Y = r(X). How do we compute lE(Y)? One way is to find Jy(y) and 
</p>
<p>then compute lE(Y) = I yfy(y)dy. But there is an easier way. 
</p>
<p>3.6 Theorem (The Rule of the Lazy Statistician). Let Y = r(X). Then 
</p>
<p>lE(Y) = lE(r(X)) = J r(x)dFx(x). (3.3) 
</p>
<p>This result makes intuitive sense. Think of playing a game where we draw 
</p>
<p>X at random and then I pay you Y = r(X). Your average income is r(x) times 
</p>
<p>the chance that X = x, summed (or integrated) over all values of x. Here is </p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Expectation of a Random Variable 49 
</p>
<p>a special case. Let A be an event and let r(x) = IA(x) where IA(x) = 1 if 
</p>
<p>x E A and IA(x) = 0 if x ~ A. Then 
</p>
<p>IE(IA(X)) = J IA(x)fx(x)dx = L fx(x)dx = JF'(X E A). 
In other words, probability is a special case of expectation. 
</p>
<p>3.7 Example. Let X rv Unif(O, 1). Let Y = r(X) = eX. Then, 
</p>
<p>{I 11 
IE(Y) = 10 eX f(x)dx = 0 eXdx = e - 1. 
</p>
<p>Alternatively, you could find jy(y) which turns out to be jy(y) = l/y for 
</p>
<p>1 &lt; y &lt; e. Then, IE(Y) = J~e y f(y)dy = e - 1. &bull; 
</p>
<p>3.8 Example. Take a stick of unit length and break it at random. Let Y be 
</p>
<p>the length of the longer piece. What is the mean of Y? If X is the break point 
</p>
<p>then X rv Unif(O,l) and Y = r(X) = max{X,l - X}. Thus, r(x) = 1 - x 
</p>
<p>when 0 &lt; x &lt; 1/2 and r(x) = x when 1/2 -s: x &lt; 1. Hence, 
</p>
<p>j. 11/2 ;.1 3 IE(Y) = r(:r)dF(.T) = (1 - :r)d:r + :rd:r = - .&bull; 
o 1/2 4 
</p>
<p>Functions of several variables are handled in a similar way. If Z = r(X, Y) 
</p>
<p>then 
</p>
<p>IE(Z) = IE(r(X, Y)) = J J r(x,y)dF(x,y). (3.4) 
3.9 Example. Let (X, Y) have a jointly uniform distribution on the unit 
</p>
<p>square. Let Z = r(X, Y) = X 2 + y2. Then, 
</p>
<p>IE(Z) = J J r(x, y)dF(x, y) = 11 11 (x2 + y2) dxdy 
11 2 11 2 2 x dx + y dy = - .&bull; 
</p>
<p>o 0 3 
</p>
<p>The kth moment of X is defined to be IE(Xk) assuming that IE(IXlk) &lt; 00. 
</p>
<p>3.10 Theorem. If the kth moment exists and if j &lt; k then the jth moment 
</p>
<p>exists. 
</p>
<p>PROOF. We have 
</p>
<p>&pound;: Ixl j fx(x)dx </p>
<p/>
</div>
<div class="page"><p/>
<p>50 3. Expectation 
</p>
<p>j. Ixlj fx(x)dx + j' Ixl j fx(x)dx 
Ixl9 Ixl&gt;l 
</p>
<p>&lt; 1 fx(x)dx + 1 Ixlk fx(x)dx 
Ixl9 Ixl&gt;l 
</p>
<p>&lt; 1 + lE(IXlk) &lt; 00. _ 
</p>
<p>The kth central moment is defined to be lE((X - J.L)k). 
</p>
<p>3.2 Properties of Expectations 
</p>
<p>3.11 Theorem. If Xl"'" Xn are random variables and al,"" an are con-
</p>
<p>stants, then 
</p>
<p>(3.5) 
</p>
<p>3.12 Example. Let X rv Binomial(n,p). What is the mean of X? We could 
</p>
<p>try to appeal to the definition: 
</p>
<p>but this is not an easy sum to evaluate. Instead, note that X = L~=l Xi 
</p>
<p>where Xi = 1 if the ith toss is heads and Xi = 0 otherwise. Then lE(Xi ) = 
</p>
<p>(p x 1) + ((1 - p) x 0) = p and lE(X) = lE(Li Xi) = Li lE(Xi) = np. _ 
</p>
<p>3.13 Theorem. Let Xl"'" Xn be independent random variables. Then, 
</p>
<p>(3.6) 
</p>
<p>Notice that the summation rule does not require independence but the 
</p>
<p>multiplication rule does. 
</p>
<p>3.3 Variance and Covariance 
</p>
<p>The variance measures the "spread" of a distribution. 1 
</p>
<p>lWe can't use lE(X - /1) as a measure of spread since lE(X - /1) = lE(X) - /1 = /1- /1 = O. 
We can and sometimes do use lElX - /11 as a measure of spread but more often we use the 
</p>
<p>variance. </p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Variance and Covariance 51 
</p>
<p>3.14 Definition. Let X be a random variable with mean IL. The variance 
</p>
<p>of X - denoted by (}2 or ()~ or V(X) or VX - is defined by 
</p>
<p>assuming this expectation exists. The standard deviation is 
</p>
<p>sd(X) = y'V(X) and is also denoted by () and (}x. 
</p>
<p>(3.7) 
</p>
<p>3.15 Theorem. Assuming the variance is well defined, it has the following 
</p>
<p>properties: 
</p>
<p>2. If a and b are constants then V(aX + b) = a2V(X). 
</p>
<p>3. If Xl, ... , Xn are independent and al, ... , an are constants, then 
</p>
<p>(3.8) 
</p>
<p>3.16 Example. Let X rv Binomial(n,p). We write X = Li Xi where Xi = 1 
</p>
<p>if toss i is heads and Xi = 0 otherwise. Then X = Li Xi and the random 
</p>
<p>variables are independent. Also, lP'(Xi = 1) = p and lP'(Xi = 0) = 1- p. Recall 
</p>
<p>that 
</p>
<p>Now, 
</p>
<p>JE(X;) = (PX 12) + ((l- P) X 02) =p. 
Therefore, V(Xi) = JE(Xf) - p2 = P - p2 = p(l - p). Finally, V(X) 
</p>
<p>V(LiXi) = Li V(Xi ) = LiP(1 - p) = np(l - p). Notice that V(X) = 0 
</p>
<p>if p = 1 or p = O. Make sure you see why this makes intuitive sense. _ 
</p>
<p>If Xl, ... ,Xn are random variables then we define the sample mean to be 
</p>
<p>(3.9) 
</p>
<p>and the sample variance to be 
</p>
<p>2 1 Ln ( - )2 Sn = -- Xi - X n . 
n-1 
</p>
<p>(3.10) 
</p>
<p>i=l </p>
<p/>
</div>
<div class="page"><p/>
<p>52 3. Expectation 
</p>
<p>3.17 Theorem. Let Xl"'" Xn be IID and let It = IE(Xi), (J2 = V(Xi)' Then 
</p>
<p>If X and Yare random variables, then the covariance and correlation be-
</p>
<p>tween X and Y measure how strong the linear relationship is between X and 
</p>
<p>Y. 
</p>
<p>3.18 Definition. Let X and Y be random variables with means fLx and 
</p>
<p>fLy and standard deviations (Jx and (Jy. Define the covariance between 
</p>
<p>X and Y by 
</p>
<p>Cov(X, Y) = IE ((X - fLx)(Y - fLY)) 
</p>
<p>and the correlation by 
</p>
<p>_ _ (X Y) _ Cov(X, Y) 
p - PX,y - P ,- . 
</p>
<p>(Jx(Jy 
</p>
<p>3.19 Theorem. The covariance satisfies: 
</p>
<p>Cov(X, Y) = IE(XY) - IE(X)IE(Y). 
</p>
<p>The correlation satisfies: 
</p>
<p>-1::; p(X, Y)::; 1. 
</p>
<p>(3.11) 
</p>
<p>(3.12) 
</p>
<p>If Y = aX + b for some constants a and b then p(X, Y) = 1 if a &gt; 0 and 
p(X, Y) = -1 if a &lt; O. If X and Yare independent, then Cov(X, Y) = P = O. 
The converse is not true in general. 
</p>
<p>3.20 Theorem. V(X + Y) = V(X) + V(Y) + 2Cov(X, Y) and V(X - Y) = 
V(X) + V(Y) -2Cov(X, Y). More generally, for random variables Xl,"" X n , 
</p>
<p>v (LaiXi) = La;V(Xi) +2LLaiajCov(Xi,Xj). 
I 1 1&lt;) 
</p>
<p>3.4 Expectation and Variance of Irnportant Randorn 
</p>
<p>Variables 
</p>
<p>Here we record the expectation of some important random variables: </p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Expectation and Variance of Important Random Variables 53 
</p>
<p>Distribution Mean Variance 
</p>
<p>Point mass at a a 0 
Bernoulli(p) P p(l - p) 
Binomial( 71, p) 71p 71p(l - p) 
</p>
<p>Geometric(p) l/p (1 _ p)/p2 
</p>
<p>Poisson(&gt;.) &gt;. &gt;. 
</p>
<p>Uniform(a, b) (a + b)/2 (b - a)2/12 
Normal(/L,0-2) /L 0-2 
</p>
<p>Exponential un {3 (32 
Gamma( a, (3) a{3 a{32 
</p>
<p>Beta(a, (3) a/(a + (3) a{3/((a + (3)2(a + (3 + 1)) 
tv o (if v &gt; 1) v/(v-2) (ifv&gt;2) 
X~ p 2p 
Multinomial(n,p) np see below 
Multivariate Normal(/L, ~) /L ~ 
</p>
<p>We derived lE(X) and V(X) for the Binomial in the previous section. The 
</p>
<p>calculations for some of the others are in the exercises. 
</p>
<p>The last two entries in the table are multivariate models which involve a 
</p>
<p>random vector X of the form 
</p>
<p>The mean of a random vector X is defined by 
</p>
<p>The variance-covariance matrix ~ is defined to be 
</p>
<p>V(XI) COV(XI' X 2 ) 
</p>
<p>V(X) = 
COV(X2' Xl) V(X2) 
</p>
<p>COV(XI' X k ) 
</p>
<p>CoV(X2,Xk) 
</p>
<p>If X rv Multinomial(71,p) then lE(X) = 71p = 71(PI, ... ,Pk) and 
</p>
<p>( 
</p>
<p>71pI(l-pI) 
</p>
<p>-np2PI 
V(X) = . 
</p>
<p>-npkPI 
</p>
<p>-71pIP2 
np2(1 - P2) </p>
<p/>
</div>
<div class="page"><p/>
<p>54 3. Expectation 
</p>
<p>To see this, note that the marginal distribution of anyone component of the 
</p>
<p>vector Xi rv Binomial(n,pi)' Thus, lE(Xi) = npi and V(Xi) = npi(l - Pi)' 
</p>
<p>Note also that Xi + Xj rv Binomial(n,pi + Pj). Thus, V(Xi + Xj) = n(pi + 
pj)(l - [pi + Pj]). On the other hand, using the formula for the variance 
of a sum, we have that V(Xi + Xj) = V(Xi) + V(Xj) + 2COV(Xi' Xj) = 
npi(l - Pi) + npj(l - Pj) + 2COV(Xi' Xj)' If we equate this formula with 
n(pi + pj)(l - [Pi + Pj]) and solve, we get COV(Xi' Xj) = -npiPj' 
</p>
<p>Finally, here is a lemma that can be useful for finding means and variances 
</p>
<p>of linear combinations of multivariate random vectors. 
</p>
<p>3.21 lemma. If a is a vector and X is a random vector with mean It and 
</p>
<p>variance~, thenlE(aTX) = aT,t andV(aTX) = aT~a. If A is a matrix then 
</p>
<p>lE(AX) = AIL and V(AX) = A~AT. 
</p>
<p>3.5 Conditional Expectation 
</p>
<p>Suppose that X and Yare random variables. What is the mean of X among 
</p>
<p>those times when Y = y? The answer is that we compute the mean of X as 
</p>
<p>before but we substitute fXIY(xly) for fx(x) in the definition of expectation. 
</p>
<p>3.22 Definition. The conditional expectation of X given Y = y is 
</p>
<p>{ LX fXly(xly) dx lE(XIY = y) = 
J x fXIY(xly) dx 
</p>
<p>If r(x, y) is a function of x and y then 
</p>
<p>discrete case 
</p>
<p>continuous case. 
</p>
<p>lE(r(X, Y)IY = y) = { L r(x, y) fXIY(xly) dx 
J r(x, y) fXIY(xly) dx 
</p>
<p>discrete case 
</p>
<p>continuous case. 
</p>
<p>(3.13) 
</p>
<p>(3.14) 
</p>
<p>Warning! Here is a subtle point. Whereas lE(X) is a number, lE(XIY = y) 
</p>
<p>is a function of y. Before we observe Y, we don't know the value oflE(XIY = y) 
</p>
<p>so it is a random variable which we denote lE(XIY). In other words, lE(XIY) 
</p>
<p>is the random variable whose value is lE(XIY = y) when Y = y. Similarly, 
</p>
<p>lE(r(X, Y)IY) is the random variable whose value is lE(r(X, Y)IY = y) when 
</p>
<p>Y = y. This is a very confusing point so let us look at an example. 
</p>
<p>3.23 Example. Suppose we draw X rv Unif(O, 1). After we observe X = x, 
</p>
<p>we draw YIX = x rv Unif(x, 1). Intuitively, we expect that lE(YIX = x) = </p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Conditional Expectation 55 
</p>
<p>(1 + x)/2. In fact, jylx(ylx) = 1/(1 - x) for x &lt; y &lt; 1 and 
</p>
<p>; .1 1 ;.1 1 + x IE(YIX = x) = y jylx(ylx)dy = -- ydy = --
x I-x x 2 
</p>
<p>as expected. Thus, IE(YIX) = (1 + X)/2. Notice that IE(YIX) = (1 + X)/2 is 
a random variable whose value is the number IE(YIX = x) = (1 + x)/2 once 
X = x is observed. _ 
</p>
<p>3.24 Theorem (The Rule of Iterated Expectations). For random variables X 
</p>
<p>and Y, assuming the expectations exist, we have that 
</p>
<p>IE [IE(YIX)] = IE(Y) and IE [IE(XIY)] = IE(X). (3.15) 
</p>
<p>More generally, for any function r(x, y) we have 
</p>
<p>IE [IE(r(X, Y)IX)] = IE(r(X, Y)). (3.16) 
</p>
<p>PROOF. We'll prove the first equation. Using the definition of conditional 
</p>
<p>expectation and the fact that f(x, y) = f(x)f(Ylx), 
</p>
<p>IE [IE(YIX)] J IE(YIX = x)fx(x)dx = J J yf(ylx)dyf(x)dx 
</p>
<p>J J yf(ylx)f(x)dxdy = J J yf(x, y)dxdy = IE(Y). -
</p>
<p>3.25 Example. Consider example 3.23. How can we compute IE(Y)? One 
</p>
<p>method is to find the joint density f(x, y) and then compute IE(Y) = J J yf(x, y)dxdy. 
An easier way is to do this in two steps. First, we already know that IE(YIX) = 
</p>
<p>(1 + X) /2. Thus, 
</p>
<p>IE(Y) IEIE(YIX) = IE ((1 ~ X)) 
(1 + IE(X)) = (1 + (1/2)) = 3/4. _ 
</p>
<p>2 2 
</p>
<p>3.26 Definition. The conditional variance is defined as 
</p>
<p>V(YIX = x) = J(y -/L(X))2f(ylx)dy 
</p>
<p>where fL(X) = IE(YIX = x). 
</p>
<p>3.27 Theorem. For random variables X and Y, 
</p>
<p>V(Y) = IEV(YIX) + VIE(YIX). 
</p>
<p>(3.17) </p>
<p/>
</div>
<div class="page"><p/>
<p>56 3. Expectation 
</p>
<p>3.28 Example. Draw a county at random from the United States. Then draw 
</p>
<p>17, people at random from the county. Let X be the number of those people 
</p>
<p>who have a certain disease. If Q denotes the proportion of people in that 
</p>
<p>county with the disease, then Q is also a random variable since it varies from 
</p>
<p>county to county. Given Q = q, we have that X rv Binomial(n, q). Thus, 
</p>
<p>IE(XIQ = q) = nq and V(XIQ = q) = nq(l - q). Suppose that the random 
</p>
<p>variable Q has a Uniform (0,1) distribution. A distribution that is constructed 
</p>
<p>in stages like this is called a hierarchical model and can be written as 
</p>
<p>Q 
</p>
<p>XIQ=q 
</p>
<p>Uniform(O, 1) 
</p>
<p>Binomial(n, q). 
</p>
<p>Now, IE(X) = IEIE(XIQ) = IE(nQ) = nIE(Q) = n/2. Let us compute the 
</p>
<p>variance of X. Now, V(X) = IEV(XIQ) + VJE(XIQ). Let's compute these 
two terms. First, IEV(XIQ) = IE[nQ(l - Q)] = nIE(Q(l - Q)) = n I q(l -
</p>
<p>q)f(q)dq = n I01 q(l - q)dq = n/6. Next, VIE(XIQ) = V(nQ) = n 2V(Q) 
</p>
<p>n 2 I(q - (1/2))2dq = 17,2/12. Hence, V(X) = (n/6) + (n 2/12) .&bull; 
</p>
<p>3.6 Moment Generating Functions 
</p>
<p>Now we will define the moment generating function which is used for finding 
</p>
<p>moments, for finding the distribution of sums of random variables and which 
</p>
<p>is also used in the proofs of some theorems. 
</p>
<p>3.29 Definition. The moment generating function MGF, or Laplace 
</p>
<p>transform, of X is defined by 
</p>
<p>where t varies over the real numbers. 
</p>
<p>In what follows, we assume that the MGF is well defined for all t in some 
</p>
<p>open interval around t = O. 2 
</p>
<p>When the MGF is well defined, it can be shown that we can interchange the 
</p>
<p>operations of differentiation and "taking expectation." This leads to 
</p>
<p>1j;'(O) = [.!!:...-IEetX ] = IE [.!!:...-etX ] = IE [XetXL=o = IE(X). 
dt t=O dt t=O 
</p>
<p>2 A related function is the characteristic function, defined by lEt eitX ) where i = y'=1. This 
function is always well defined for all t. </p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Moment Generating Functions 57 
</p>
<p>By taking k derivatives we conclude that 1j)k) (0) = JE(Xk). This gives us a 
</p>
<p>method for computing the moments of a distribution. 
</p>
<p>3.30 Example. Let X rv Exp(l). For any t &lt; 1, 
</p>
<p>The integral is divergent if t ~ 1. So, 1jJx(t) = 1/(1 - t) for all t &lt; 1. Now, 
</p>
<p>&cent;'(O) = 1 and 1/;//(0) = 2. Hence, JE(X) = 1 and V(X) = JE(X2) -1L2 = 2 -1 = 
</p>
<p>1.. 
</p>
<p>3.31 Lemma. Properties of the MGF. 
</p>
<p>(1) IfY = aX + b, then&cent;y(t) = ebt&cent;x(at). 
(2) If Xl"'" Xn are independent and Y = Li Xi, then 1jJy(t) = ITi 1/;i(t) 
</p>
<p>where 1jJi is the MGF of Xi' 
</p>
<p>3.32 Example. Let X rv Binomial(n,p). We know that X = L~=l Xi where 
</p>
<p>JP'(Xi = 1) = p and JP'(Xi = 0) = 1 - p. Now&cent;i(t) = JEeX;t = (p x et ) + ((1 -
p)) = pet + q where q = 1 - p. Thus, 1jJx(t) = ITi 1jJi(t) = (pet + q)n .&bull; 
</p>
<p>Recall that X and Yare equal in distribution if they have the same distri-
</p>
<p>bution function and we write X ~ Y. 
</p>
<p>3.33 Theorem. Let X and Y be random variables. If 1/;x(t) =&cent;y(t) for all t 
</p>
<p>in an open interval around 0, then X ~ Y. 
</p>
<p>3.34 Example. Let Xl rv Binomial(nl,p) and X 2 rv Binomial(n2,p) be inde-
</p>
<p>pendent. Let Y = Xl + X 2 &bull; Then, 
</p>
<p>and we recognize the latter as the MGF of a Binomial(nl + n2,p) distribu-
tion. Since the MGF characterizes the distribution (i.e., there can't be an-
</p>
<p>other random variable which has the same MGF) we conclude that Y rv 
</p>
<p>Binomial(nl + n2,p) .&bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>58 3. Expectation 
</p>
<p>Moment Generating Functions for Some Common Distributions 
</p>
<p>Distribution 
</p>
<p>Bernoulli(p) 
</p>
<p>Binomial( n, p) 
</p>
<p>Poisson(A) 
</p>
<p>Normal(fL,cr) 
</p>
<p>Gamma(a,j3) 
</p>
<p>MGF 'tjJ(t) 
</p>
<p>pet +(l-p) 
</p>
<p>(pet + (1 _ p))n 
e.\(e t -1) 
</p>
<p>exp {Id + ,,~t2 } 
</p>
<p>( I! ;3t r for t &lt; 1/ j3 
3.35 Example. Let Y1 rv Poisson(A1) and Y2 rv Poisson(A2) be independent. 
</p>
<p>The moment generating function of Y = Y1 + Y + 2 is 'ljJy (t) = VJy, (t )'ljJY2 (t) = 
e'\1(e l -1)e'\2(e l -1) = e('\1+'\2)(el -1) which is the moment generating function 
</p>
<p>of a Poisson(A1 + A2)' We have thus proved that the sum of two independent 
Poisson random variables has a Poisson distribution. _ 
</p>
<p>3.7 Appendix 
</p>
<p>EXPECTATION AS AN INTEGRAL. The integral of a measurable function r(x) 
</p>
<p>is defined as follows. First suppose that r is simple, meaning that it takes 
</p>
<p>finitely many values aI, ... , ak over a partition AI, ... , Ak. Then define 
</p>
<p>k J r(x)dF(x) = LaiJID(r(X) E Ai). 
i=l 
</p>
<p>The integral of a positive measurable function r is defined by J r(x)dF(x) = 
limi J ri(x)dF(x) where ri is a sequence of simple functions such that ri(x) ::; 
r(x) and ri(x) -+ r(x) as i -+ 00. This does not depend on the particular se-
</p>
<p>quence. The integral of a measurable function r is defined to be J r(x)dF(x) = 
J r+(x)dF(x)-J r-(x)dF(x) assuming both integrals are finite, where r+(x) = 
max{r(x),O} and r- (x) = - min{r(x), O}. 
</p>
<p>3.8 Exercises 
</p>
<p>1. Suppose we playa game where we start with c dollars. On each play of 
</p>
<p>the game you either double or halve your money, with equal probability. 
</p>
<p>What is your expected fortune after n trials? </p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Exercises 59 
</p>
<p>2. Show that V(X) 
</p>
<p>P(X = c) = 1. 
</p>
<p>o if and only if there is a constant c such that 
</p>
<p>3. Let XI, ... ,Xn rv Uniform(O,l) and let Yn = max{XI, ... ,Xn}. Find 
</p>
<p>JE(Yn). 
</p>
<p>4. A particle starts at the origin of the real line and moves along the line in 
</p>
<p>jumps of one unit. For each jump the probability is p that the particle 
</p>
<p>will jump one unit to the left and the probability is 1-p that the particle 
</p>
<p>will jump one unit to the right. Let Xn be the position of the particle 
</p>
<p>after n units. Find JE(Xn) and V(Xn). (This is known as a random 
</p>
<p>walk.) 
</p>
<p>5. A fair coin is tossed until a head is obtained. What is the expected 
</p>
<p>number of tosses that will be required? 
</p>
<p>6. Prove Theorem 3.6 for discrete random variables. 
</p>
<p>7. Let X be a continuous random variable with CDF F. Suppose that 
</p>
<p>P(X &gt; 0) = 1 and that JE(X) exists. Show that JE(X) = fOCXlJP'(X &gt; 
</p>
<p>x)dx. 
</p>
<p>Hint: Consider integrating by parts. The following fact is helpful: ifJE(X) 
</p>
<p>exists then limX --+ CXl x[l - F(x)] = O. 
</p>
<p>8. Prove Theorem 3.17. 
</p>
<p>9. (Computer Experiment.) Let Xl, X 2 , ... , Xn be N(O, 1) random variables 
</p>
<p>and let Xn = n-IL~=IXi' Plot Xn versus n for n = 1, ... ,10,000. 
</p>
<p>Repeat for Xl, X 2 , .. . , Xn rv Cauchy. Explain why there is such a dif-
</p>
<p>ference. 
</p>
<p>10. Let X rv N(O, 1) and let Y = eX. Find JE(Y) and V(Y). 
</p>
<p>11. (Computer Experiment: Simulating the Stock Market.) Let YI , Y2 , ... be 
</p>
<p>independent random variables such that P(li = 1) = P(li = -1) = 
</p>
<p>1/2. Let Xn = L~=l li. Think of li = 1 as "the stock price increased 
by one dollar", li = -1 as "the stock price decreased by one dollar", 
</p>
<p>and Xn as the value of the stock on day n. 
</p>
<p>(a) Find JE(Xn) and V(Xn)' 
</p>
<p>(b) Simulate Xn and plot Xn versus n for n = 1,2, ... ,10,000. Repeat 
</p>
<p>the whole simulation several times. Notice two things. First, it's easy 
</p>
<p>to "see" patterns in the sequence even though it is random. Second, </p>
<p/>
</div>
<div class="page"><p/>
<p>60 3. Expectation 
</p>
<p>you will find that the four runs look very different even though they 
</p>
<p>were generated the same way. How do the calculations in (a) explain 
</p>
<p>the second observation? 
</p>
<p>12. Prove the formulas given in the table at the beginning of Section 3.4 
</p>
<p>for the Bernoulli, Poisson, Uniform, Exponential, Gamma, and Beta. 
</p>
<p>Here are some hints. For the mean of the Poisson, use the fact that 
</p>
<p>ea = 2..:::=0 aX Ix!. To compute the variance, first compute lE(X(X -1)). 
For the mean of the Gamma, it will help to multiply and divide by 
</p>
<p>f(a + 1)/;600+1 and use the fact that a Gamma density integrates to l. 
</p>
<p>For the Beta, multiply and divide by r(a + l)f(;6)/f(a +;6 + 1). 
</p>
<p>13. Suppose we generate a random variable X in the following way. First 
</p>
<p>we flip a fair coin. If the coin is heads, take X to have a Unif(O,l) 
</p>
<p>distribution. If the coin is tails, take X to have a Unif(3,4) distribution. 
</p>
<p>(a) Find the mean of X. 
</p>
<p>(b) Find the standard deviation of X. 
</p>
<p>14. Let Xl, ... ,Xm and Yl , ... , Yn be random variables and let al, ... , am 
</p>
<p>and bl , ... ,bn be constants. Show that 
</p>
<p>15. Let 
</p>
<p>f ( ) { -31 (x + y) 0 &lt;_ x &lt;_ 1, 0 &lt;_ Y &lt;_ 2 X,Y x,y = 0 otherwise. 
</p>
<p>Find V(2X - 3Y + 8). 
</p>
<p>16. Let r(x) be a function of x and let s(y) be a function of y. Show that 
</p>
<p>lE(r(X)s(Y)IX) = r(X)lE(s(Y)IX). 
</p>
<p>Also, show that lE(r(X)IX) = r(X). 
</p>
<p>17. Prove that 
</p>
<p>V(Y) = lEV(Y I X) + VlE(Y I X). 
</p>
<p>Hint: Let m = lE(Y) and let b(x) = lE(YIX = x). Note that lE(b(X)) = 
</p>
<p>lElE(YIX) = lE(Y) = m. Bear in mind that b is a function of x. Now 
</p>
<p>write V(Y) = lE(Y - m)2 = lE( (Y - b(X)) + (b(X) - m))2. Expand the </p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Exercises 61 
</p>
<p>square and take the expectation. You then have to take the expectation 
</p>
<p>of three terms. In each case, use the rule of the iterated expectation: 
</p>
<p>lE(stuff) = lE(lE(stuffIX)). 
</p>
<p>18. Show that if lE(XIY = y) = c for some constant c, then X and Yare 
</p>
<p>uncorrelated. 
</p>
<p>19. This question is to help you understand the idea of a sampling dis-
</p>
<p>tribution. Let Xl, . .. ,Xn be lID with mean f.L and variance (}2. Let 
</p>
<p>Xn = n- l L~=l Xi. Then Xn is a statistic, that is, a function of the 
data. Since X n is a random variable, it has a distribution. This distri-
</p>
<p>bution is called the sampling distribution of the statistic. Recall from 
</p>
<p>Theorem 3.17 that lE(Xn) = It and V(Xn) = (}2/n . Don't confuse the 
distribution of the data fx and the distribution of the statistic fx ,,' To 
make this clear, let Xl, ... ,Xn '" Uniform(O, 1). Let fx be the density 
of the Uniform(O, 1). Plot fx. Now let Xn = n- l L~=l Xi' Find lE(Xn) 
and V(Xn). Plot them as a function of n. Interpret. Now simulate the 
</p>
<p>distribution of Xn for n = 1,5,25,100. Check that the simulated values 
</p>
<p>oflE(Xn) and V(Xn) agree with your theoretical calculations. What do 
</p>
<p>you notice about the sampling distribution of X n as n increases? 
</p>
<p>20. Prove Lemma 3.21. 
</p>
<p>21. Let X and Y be random variables. Suppose that lE(YIX) = X. Show 
</p>
<p>that Cov(X, Y) = V(X). 
</p>
<p>22. Let X '" Uniform(O, 1). Let 0 &lt; a &lt; b &lt; 1. Let 
</p>
<p>and let 
</p>
<p>Z = { ~ 
</p>
<p>O&lt;x&lt;b 
otherwise 
</p>
<p>a&lt;x&lt;l 
otherwise 
</p>
<p>(a) Are Y and Z independent? Why/Why not? 
</p>
<p>(b) Find lE(YIZ). Hint: What values z can Z take? Now find lE(YIZ = z). 
</p>
<p>23. Find the moment generating function for the Poisson, Normal, and 
</p>
<p>Gamma distributions. 
</p>
<p>24. Let Xl,"" Xn '" Exp(p). Find the moment generating function of Xi' 
</p>
<p>Prove that L~=l Xi'" Gamma(n, p). </p>
<p/>
</div>
<div class="page"><p/>
<p>4 
</p>
<p>Inequalities 
</p>
<p>4.1 Probability Inequalities 
</p>
<p>Inequalities are useful for bounding quantities that might otherwise be hard 
</p>
<p>to compute. They will also be used in the theory of convergence which is 
</p>
<p>discussed in the next chapter. Our first inequality is Markov's inequality. 
</p>
<p>4.1 Theorem (Markov's inequality). Let X be a non-negative random 
</p>
<p>variable and suppose that lE(X) exists. For any t &gt; 0, 
</p>
<p>J1D(X &gt; t) &lt;::: lE(;) . 
</p>
<p>PROOF. Since X &gt; 0, 
</p>
<p>lE(X) 100 xf(x)dx = lt xf(x)dx + 100 xf(x)dx 
&gt; 100 xf(x)dx ~ t 100 f(x)dx = tJID(X &gt; t) &bull; 
</p>
<p>( 4.1) </p>
<p/>
</div>
<div class="page"><p/>
<p>64 4. Inequalities 
</p>
<p>4.2 Theorem (Chebyshev's inequality). Let It = IE(X) and (J2 = V(X). 
</p>
<p>Then, 
1 
</p>
<p>and IP'(IZI ~ k) ::; k2 
</p>
<p>where Z = (X -It)/(J. In particular, IP'(IZI &gt; 2) ::; 1/4 and 
</p>
<p>IP'(IZI &gt; 3) ::; 1/9. 
</p>
<p>PROOF. We use Markov's inequality to conclude that 
</p>
<p>The second part follows by setting t = k(J .&bull; 
</p>
<p>(4.2) 
</p>
<p>4.3 Example. Suppose we test a prediction method, a neural net for example, 
</p>
<p>on a set of n new test cases. Let Xi = 1 if the predictor is wrong and Xi = 0 
</p>
<p>if the predictor is right. Then Xn = n- 1 L~=l Xi is the observed error rate. 
</p>
<p>Each Xi may be regarded as a Bernoulli with unknown mean p. We would 
</p>
<p>like to know the true - but unknown - error rate p. Intuitively, we expect 
</p>
<p>that X n should be close to p. How likely is X n to not be within E of p? We 
</p>
<p>have that V(Xn) = V(Xd/n = p(l - p)/n and 
</p>
<p>IP'(IX _ I &gt; ) &lt; V(Xn) = p(l - p) &lt; _I_ 
n p E - E2 nE2 - 4nE2 
</p>
<p>since p(l - p) ::; i for all p. For E = .2 and n = 100 the bound is .0625 .&bull; 
</p>
<p>Hoeffding's inequality is similar in spirit to Markov's inequality but it is a 
</p>
<p>sharper inequality. We present the result here in two parts. 
</p>
<p>4.4 Theorem (Hoeffding's Inequality). Let Y1 , ... , Yn be independent 
</p>
<p>observations such that 
</p>
<p>IE(Yi) = 0 and ai ::; Yi ::; bi . Let E &gt; O. Then, for any t &gt; 0, 
</p>
<p>n 
</p>
<p>::; e -tE II et2 (b.; -ai)2 /8. 
i=l 
</p>
<p>(4.3) </p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Probability Inequalities 65 
</p>
<p>4.5 Theorem. Let Xl' ... ' Xn rv Bernoulli(p). Then, for any E &gt; 0, 
</p>
<p>( 4.4) 
</p>
<p>4.6 Example. Let Xl, ... , Xn rv Bernoulli(p). Let n 
</p>
<p>saw that Chebyshev's inequality yielded 
</p>
<p>100 and E = .2. We 
</p>
<p>According to Hoeffding's inequality, 
</p>
<p>1P'(IXn - pi &gt; .2) ::; 2e-2(lOO)(.2)2 = .00067 
</p>
<p>which is much smaller than .0625 .&bull; 
</p>
<p>Hoeffding's inequality gives us a simple way to create a confidence inter-
</p>
<p>val for a binomial parameter p. We will discuss confidence intervals in detail 
</p>
<p>later (see Chapter 6) but here is the basic idea. Fix a &gt; 0 and let 
</p>
<p>J:.- log (3.) . 
2n a 
</p>
<p>By Hoeffding's inequality, 
</p>
<p>Let C = (Xn - En, Xn + En). Then, lP'(p ~ C) = 1P'(IXn - pi &gt; En) ::; a. Hence, 
lP'(p E C) ~ 1 - a, that is, the random interval C traps the true parameter 
</p>
<p>value p with probability 1 - a; we call Cal - a confidence interval. More on 
</p>
<p>this later. 
</p>
<p>The following inequality is useful for bounding probability statements about 
</p>
<p>Normal random variables. 
</p>
<p>4.7 Theorem (Mill's Inequality). Let Z rv N(O, 1). Then, 
</p>
<p>(2 e- t2 /2 
IP'(IZI &gt; t) ::; V ;-t-&middot; </p>
<p/>
</div>
<div class="page"><p/>
<p>66 4. Inequalities 
</p>
<p>4.2 Inequalities For Expectations 
</p>
<p>This section contains two inequalities on expected values. 
</p>
<p>4.8 Theorem (Cauchy-Schwartz inequality). If X and Y have finite 
</p>
<p>variances then 
</p>
<p>(4.5) 
</p>
<p>Recall that a function 9 is convex if for each x, y and each a E [0,1], 
</p>
<p>g(ax + (1 - a)y) &lt;::: ag(x) + (1 - a)g(y). 
</p>
<p>If 9 is twice differentiable and g" (x) :;0. 0 for all x, then 9 is convex. It can 
</p>
<p>be shown that if 9 is convex, then 9 lies above any line that touches 9 at 
</p>
<p>some point, called a tangent line. A function 9 is concave if -g is convex. 
</p>
<p>Examples of convex functions are g(x) = x 2 and g(x) = eX. Examples of 
</p>
<p>concave functions are g(x) = _x2 and g(x) = logx. 
</p>
<p>4.9 Theorem (Jensen's inequality). If 9 is convex, then 
</p>
<p>IEg(X) :;0. g(IEX). (4.6) 
</p>
<p>If 9 is concave, then 
</p>
<p>IEg(X) &lt;::: g(IEX). (4.7) 
</p>
<p>PROOF. Let L(x) = a + bx be a line, tangent to g(x) at the point IE(X). 
Since 9 is convex, it lies above the line L(x). So, 
</p>
<p>IEg(X) :;0. IEL(X) = IE(a + bX) = a + bIE(X) = L(IE(X)) = g(IEX) .&bull; 
</p>
<p>From Jensen's inequality we see that IE(X2) :;0. (IEX)2 and if X is positive, 
</p>
<p>then IE(ljX) :;0. 1jIE(X). Since log is concave, IE(logX) &lt;::: 10gIE(X). 
</p>
<p>4.3 Bibliographic Rernarks 
</p>
<p>Devroye et al. (1996) is a good reference on probability inequalities and their 
</p>
<p>use in statistics and pattern recognition. The following proof of Hoeffding's 
</p>
<p>inequality is from that text. </p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Appendix 67 
</p>
<p>4.4 Appendix 
</p>
<p>PROOF OF HOEFFDING'S INEQUALITY. We will make use of the exact form of 
</p>
<p>Taylor's theorem: if 9 is a smooth function, then there is a number e E (0, u) 
2 " 
</p>
<p>such that g(u) = g(O) + ug'(O) + '~ 9 (e). 
PROOF of Theorem 4.4. For any t &gt; 0, we have, from Markov's inequality, 
</p>
<p>that 
</p>
<p>lP' (t ~ Yi ~ tf) = lP' ( et ~:,=, Y; ~ etE ) 
&lt; e-tEIE (et~:'~l Yi) = e- tE IIIE(etY;). (4.8) 
</p>
<p>Since ai &lt;:::: Yi &lt;:::: bi , we can write Yi as a convex combination of ai and bi , 
namely, Yi = abi + (1 - a)ai where a = (Yi - ai)/(bi - ai). So, by the 
convexity of e ty we have 
</p>
<p>tY Yi - ai tb bi - Yi ta 
e' &lt; ---e '+ ---e'. 
</p>
<p>- bi - ai bi - ai 
</p>
<p>Take expectations of both sides and use the fact that IE(Yi) = 0 to get 
</p>
<p>a b ( ) 
IEetYi &lt; ---' _etb; + --' _etai = eg " 
</p>
<p>- bi - ai bi - ai 
(4.9) 
</p>
<p>where u = t(bi - ai), g(u) = -,u + log(l -, + ,e") and, = -ai/(bi - ai). 
Note that g(O) = g'(O) = o. Also, g" (u) &lt;:::: 1/4 for all u &gt; o. By Taylor's 
</p>
<p>theorem, there is a e E (0, u) such that 
, u2 fI 
</p>
<p>g(u) g(O) + ug (0) + 2"g (e) 
</p>
<p>u2 " u2 t 2(bi - ai)2 
2"g (e) &lt;:::: 8 = 8 
</p>
<p>Hence, 
</p>
<p>IEetYi &lt;:::: eg(,,) &lt;:::: et2 (b;-a;)2/8. 
</p>
<p>The result follows from (4.8) .&bull; 
</p>
<p>PROOF of Theorem 4.5. Let Yi (l/n)(Xi - p). Then IE(Yi) = 0 and 
a &lt;:::: Yi &lt;:::: b where a = -pin and b = (1 - p)/n. Also, (b - a)2 = 1/n2. 
Applying Theorem 4.4 we get 
</p>
<p>lP'(Xn - p &gt; f) = lP'( ~Yi &gt; f) &lt;:::: e- tE et2 /(8n). 
The above holds for any t &gt; o. In particular, take t = 4nf and we get lP'(Xn-
p&gt; f) &lt;:::: e-2nE2 . By a similar argument we can show that lP'(Xn - p &lt; -f) &lt;:::: 
e- 2m2 . Putting these together we get lP' (IXn - pi &gt; f) &lt;:::: 2e-2nE2 .&bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>68 4. Inequalities 
</p>
<p>4.5 Exercises 
</p>
<p>1. Let X rv Exponential(p). Find J1D(IX - /Lxi 2 k(Jx) for k &gt; 1. Compare 
this to the bound you get from Chebyshev's inequality. 
</p>
<p>2. Let X rv Poisson(,,\). Use Chebyshev's inequality to show that J1D(X 2 
</p>
<p>2,,\) -s: 1/,,\. 
</p>
<p>3. Let Xl,"" Xn rv Bernoulli(p) and Xn = 71.- 1 L~=l Xi' Bound J1D(IXn -
</p>
<p>pi &gt; E) using Chebyshev's inequality and using Hoeffding's inequality. 
</p>
<p>Show that, when 71. is large, the bound from Hoeffding's inequality is 
</p>
<p>smaller than the bound from Chebyshev's inequality. 
</p>
<p>4. Let Xl"'" Xn rv Bernoulli(p). 
</p>
<p>(a) Let Q &gt; 0 be fixed and define 
</p>
<p>~ log (3.). 
271. Q 
</p>
<p>Let fin = 71.- 1 L~=l Xi. Define Cn = (fin - En, fin + En). Use Hoeffding's 
inequality to show that 
</p>
<p>J1D(Cn contains p) 2 1 - Q. 
</p>
<p>In practice, we truncate the interval so it does not go below 0 or above 
</p>
<p>l. 
</p>
<p>(b) (Computer Experiment.) Let's examine the properties of this confi-
</p>
<p>dence interval. Let Q = 0.05 and p = 0.4. Conduct a simulation study 
</p>
<p>to see how often the interval contains p (called the coverage). Do this 
</p>
<p>for various values of 71. between 1 and 10000. Plot the coverage versus 71.. 
</p>
<p>(c) Plot the length of the interval versus 71.. Suppose we want the length 
</p>
<p>of the interval to be no more than .05. How large should 71. be? 
</p>
<p>5. Prove Mill's inequality, Theorem 4.7. Hint. Note that J1D(IZI &gt; t) = 
2J1D(Z &gt; t). Now write out what J1D(Z &gt; t) means and note that x/t &gt; 1 
</p>
<p>whenever x &gt; t. 
</p>
<p>6. Let Z rv N(O, 1). Find J1D(IZI &gt; t) and plot this as a function of t. From 
IElzl k Markov's inequality, we have the bound J1D(IZI &gt; t) -s: -tk - for any 
</p>
<p>k &gt; O. Plot these bounds for k = 1,2,3,4,5 and compare them to the 
true value of J1D(IZI &gt; t). Also, plot the bound from Mill's inequality. </p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Exercises 69 
</p>
<p>7. Let Xl"'" Xn rv N(O,l). Bound lP'(IXnl &gt; t) using Mill's inequality, 
where Xn = n- l L~=l Xi' Compare to the Chebyshev bound. </p>
<p/>
</div>
<div class="page"><p/>
<p>5 
</p>
<p>Convergence of Random Variables 
</p>
<p>5.1 Introduction 
</p>
<p>The most important aspect of probability theory concerns the behavior of 
</p>
<p>sequences of random variables. This part of probability is called large sample 
</p>
<p>theory, or limit theory, or asymptotic theory. The basic question is this: 
</p>
<p>what can we say about the limiting behavior of a sequence of random variables 
</p>
<p>X I, X 2 , X 3 , ... ? Since statistics and data mining are all about gathering data, 
</p>
<p>we will naturally be interested in what happens as we gather more and more 
</p>
<p>data. 
</p>
<p>In calculus we say that a sequence of real numbers Xn converges to a limit 
</p>
<p>x if, for every f &gt; 0, IXn - xl &lt; f for all large n. In probability, convergence is 
more subtle. Going back to calculus for a moment, suppose that Xn = x for 
</p>
<p>all n. Then, trivially, limn ---+ oo Xn = x. Consider a probabilistic version of this 
</p>
<p>example. Suppose that Xl, X 2 , ... is a sequence of random variables which 
</p>
<p>are independent and suppose each has a N(O, 1) distribution. Since these all 
</p>
<p>have the same distribution, we are tempted to say that Xn "converges" to 
</p>
<p>X rv N(O, 1). But this can't quite be right since IP'(Xn = X) = 0 for all n. 
</p>
<p>(Two continuous random variables are equal with probability zero.) 
</p>
<p>Here is another example. Consider Xl, X 2 , ... where Xi rv N(O, lin). Intu-
</p>
<p>itively, Xn is very concentrated around 0 for large n so we would like to say 
</p>
<p>that Xn converges to O. But IP'(Xn = 0) = 0 for all n. Clearly, we need to </p>
<p/>
</div>
<div class="page"><p/>
<p>72 5. Convergence of Random Variables 
</p>
<p>develop some tools for discussing convergence in a rigorous way. This chapter 
</p>
<p>develops the appropriate methods. 
</p>
<p>There are two main ideas in this chapter which we state informally here: 
</p>
<p>1. The law of large numbers says that the sample average X n = n- l L~=l Xi 
</p>
<p>converges in probability to the expectation It = JE(Xi)' This means 
</p>
<p>that Xn is close to It with high probability. 
</p>
<p>2. The central limit theorem says that fo(Xn - fL) converges in dis-
</p>
<p>tribution to a Normal distribution. This means that the sample average 
</p>
<p>has approximately a Normal distribution for large n. 
</p>
<p>5.2 Types of Convergence 
</p>
<p>The two main types of convergence are defined as follows. 
</p>
<p>5.1 Definition. Let Xl, X 2 , ... be a sequence of random variables and let 
</p>
<p>X be another random variable. Let Fn denote the CDF of Xn and let F 
</p>
<p>denote the CDF of X. 
</p>
<p>1. Xn converges to X in probability, written Xn~X, if, for every 
</p>
<p>E &gt; 0, 
(5.1 ) 
</p>
<p>as n -+ 00. 
</p>
<p>2. Xn converges to X in distribution, written Xn -vo+ X, if 
</p>
<p>lim Fn (t) = F(t) 
n---+oo 
</p>
<p>(5.2) 
</p>
<p>at all t for which F is continuous. 
</p>
<p>When the limiting random variable is a point mass, we change the notation 
</p>
<p>slightly. If JP'(X = c) = 1 and Xn ~ X then we write Xn ~ c. Similarly, if 
</p>
<p>Xn -vo+ X we write Xn -vo+ c. 
</p>
<p>There is another type of convergence which we introduce mainly because it 
</p>
<p>is useful for proving convergence in probability. </p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Types of Convergence 73 
</p>
<p>F(t) 
</p>
<p>----~~-----L-------------t ----------~-----------t 
</p>
<p>FIGURE 5.1. Example 5.3. Xn converges in distribution to X because Fn(t) con-
verges to F(t) at all points except t = o. Convergence is not required at t = 0 
because t = 0 is not a point of continuity for F. 
</p>
<p>5.2 Definition. Xn converges to X in quadratic mean (also called 
</p>
<p>convergence in L 2 ), written Xn ~ X, if 
</p>
<p>(5.3) 
</p>
<p>as 17, -+ 00. 
</p>
<p>Again, if X is a point mass at c we write Xn ~ c instead of Xn ~ X. 
</p>
<p>5.3 Example. Let Xn rv N(O, 1/17,). Intuitively, Xn is concentrating at 0 so 
</p>
<p>we would like to say that Xn converges to O. Let's see if this is true. Let F be 
</p>
<p>the distribution function for a point mass at o. Note that foXn rv N(O, 1). 
Let Z denote a standard normal random variable. For t &lt; 0, Fn(t) = IP'(Xn &lt; 
</p>
<p>t) = IP'( foXn &lt; fot) = IP'(Z &lt; fot) -+ 0 since fot -+ -00. For t &gt; 0, 
</p>
<p>Fn(t) = IP'(Xn &lt; t) = lP'(foXn &lt; fot) = IP'(Z &lt; fot) -+ 1 since fot -+ 00. 
</p>
<p>Hence, Fn(t) -+ F(t) for all t cJ 0 and so Xn ~ O. Notice that Fn(O) = 1/2 cJ 
F(1/2) = 1 so convergence fails at t = o. That doesn't matter because t = 0 
is not a continuity point of F and the definition of convergence in distribution 
</p>
<p>only requires convergence at continuity points. See Figure 5.1. Now consider 
</p>
<p>convergence in probability. For any E &gt; 0, using Markov's inequality, 
</p>
<p>&lt; 
</p>
<p>p 
as 17, -+ 00. Hence, Xn --+ o .&bull; 
</p>
<p>The next theorem gives the relationship between the types of convergence. 
</p>
<p>The results are summarized in Figure 5.2. 
</p>
<p>5.4 Theorem. The following relationships hold: 
</p>
<p>(a) Xn~X implies that Xn~X. 
</p>
<p>(b) Xn ~ X implies that Xn ~ X. 
</p>
<p>(c) If Xn ~ X and iflP'(X = c) = 1 for some real numberc, thenXn~X. </p>
<p/>
</div>
<div class="page"><p/>
<p>74 5. Convergence of Random Variables 
</p>
<p>In general, none of the reverse implications hold except the special case in 
</p>
<p>(c). 
</p>
<p>PROOF. We start by proving (a). Suppose that Xn~X. Fix E &gt; O. Then, 
</p>
<p>using Markov's inequality, 
</p>
<p>Proof of (b). This proof is a little more complicated. You may skip it if you 
</p>
<p>wish. Fix E &gt; 0 and let x be a continuity point of F. Then 
</p>
<p>Fn(x) IP'(Xn &lt;::: x) = IP'(Xn &lt;::: x, X &lt;::: x + E) + IP'(Xn &lt;::: x, X&gt; x + E) 
</p>
<p>&lt; IP'(X &lt;::: x + E) + 1P'(IXn - XI &gt; E) 
</p>
<p>F(x + E) + 1P'(IXn - XI &gt; E). 
</p>
<p>Also, 
</p>
<p>F(x - E) IP'(X &lt;::: x - E) = IP'(X &lt;::: x - E,Xn &lt;::: x) +IP'(X &lt;::: x - E,Xn &gt; x) 
</p>
<p>&lt; Fn(x) + 1P'(IXn - XI &gt; E). 
</p>
<p>Hence, 
</p>
<p>Take the limit as n -+ 00 to conclude that 
</p>
<p>F(x - E) &lt;::: liminf Fn(x) &lt;::: lim sup Fn(x) &lt;::: F(x + E). 
n--+oo n--+oo 
</p>
<p>This holds for all E &gt; O. Take the limit as E -+ 0 and use the fact that F is 
continuous at x and conclude that limn Fn(x) = F(x). 
</p>
<p>Proof of (c). Fix E &gt; O. Then, 
</p>
<p>1P'(IXn - ci &gt; E) IP'(Xn &lt; C - E) + IP'(Xn &gt; C + E) 
</p>
<p>&lt; IP'(Xn &lt;:::C-E)+IP'(Xn &gt;C+E) 
</p>
<p>Fn(c - E) + 1 - Fn(c + E) 
</p>
<p>-+ F(c-E)+l-F(c+E) 
</p>
<p>0+1 -1 = O. 
</p>
<p>Let us now show that the reverse implications do not hold. 
</p>
<p>CONVERGENCE IN PROBABILITY DOES NOT IMPLY CONVERGENCE IN QUADRATIC 
</p>
<p>MEAN. Let U rv Unif(O, 1) and let Xn = vnI(O,l/n)(U), Then 1P'(IXnl &gt; E) = </p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Types of Convergence 75 
</p>
<p>point-mass distribution ........ . . . . .. 
... .6... &bull; &bull; &bull;&bull; 
</p>
<p>quadratic mean -----)~ probability--....,)~ distribution 
</p>
<p>FIGURE 5.2. Relationship between types of convergence. 
</p>
<p>P 
IP'(Vri1(O,l/n)(U) &gt; e) = IP'(O ~ U &lt; 1/71) = 1/71 --+ o. Hence, Xn---+O. But 
lE(X~) = 71 Jolin du = 1 for all 71 so Xn does not converge in quadratic mean. 
</p>
<p>CONVERGENCE IN DISTRIBUTION DOES NOT IMPLY CONVERGENCE IN PROB-
</p>
<p>ABILITY. Let X rv N(O,l). Let Xn = -X for 71 = 1,2,3, ... ; hence Xn rv 
</p>
<p>N(O,l). Xn has the same distribution function as X for all 71 so, trivially, 
</p>
<p>limn Fn(x) = F(x) for all x. Therefore, Xn ~ X. But 1P'(IXn - XI &gt; e) = 
</p>
<p>1P'(12XI &gt; e) = IP'(IXI &gt; e/2) # O. So Xn does not converge to X in probability . 
</p>
<p>&bull; 
Warning! One might conjecture that if Xn ~ b, then lE(Xn) --+ b. This is 
</p>
<p>not l true. Let Xn be a random variable defined by IP'(Xn = 712) = 1/71 and 
</p>
<p>IP'(Xn = 0) = 1 - (1/71). Now, 1P'(IXnl &lt; to) = IP'(Xn = 0) = 1 - (1/71) --+ 1. 
Hence, Xn~ o. However, lE(Xn) = [71 2 x (1/71)] + [0 x (1- (1/71))] = n. Thus, 
lE(Xn) --+ 00. 
</p>
<p>Summary. Stare at Figure 5.2. 
</p>
<p>Some convergence properties are preserved under transformations. 
</p>
<p>5.5 Theorem. Let X n , X, Yn , Y be random variables. Let g be a continuous 
</p>
<p>function. 
P P P 
</p>
<p>(a) If Xn---+X and Yn---+ Y, then Xn + Yn---+X + Y. 
() 
</p>
<p>qm qm qm 
b If Xn--'--+X and Yn--'--+ Y, then Xn + Yn--,--+X + Y. 
</p>
<p>(c) If Xn ~ X and Yn ~ c, then Xn + Yn ~ X + c. 
P P P 
</p>
<p>(d) If Xn---+X and Yn---+ Y, then XnYn---+XY. 
</p>
<p>(e) If Xn ~ X and Yn ~ c, then XnYn ~ eX. 
</p>
<p>(f) If Xn~X, then g(Xn)~ g(X). 
</p>
<p>(g) If Xn ~ X, then g(Xn) ~ g(X). 
</p>
<p>Parts (c) and (e) are know as Slutzky's theorem. It is worth noting that 
</p>
<p>Xn ~ X and Yn ~ Y does not in general imply that Xn + Yn ~ X + Y. 
</p>
<p>lWe can conclude that lE(Xn) --+ b if Xn is uniformly integrable. See the appendix. </p>
<p/>
</div>
<div class="page"><p/>
<p>76 5. Convergence of Random Variables 
</p>
<p>5.3 The Law of Large Numbers 
</p>
<p>Now we come to a crowning achievement in probability, the law of large num-
</p>
<p>bers. This theorem says that the mean of a large sample is close to the mean 
</p>
<p>of the distribution. For example, the proportion of heads of a large number 
</p>
<p>of tosses is expected to be close to 1/2. We now make this more precise. 
</p>
<p>Let Xl, X 2 , ... be an IID sample, let fL = lE(XI) and 2 (J2 = V(X1)' Recall 
</p>
<p>that the sample mean is defined as Xn = 71,-1 L~=l Xi and that lE(Xn) = fL 
</p>
<p>and V(X n) = (J2/n . 
</p>
<p>5.6 Theorem (The Weak Law of Large Numbers (WLLN)). 3 
- p 
</p>
<p>If Xl"'" Xn are IID, then Xn--+ fL. 
</p>
<p>Interpretation of the WLLN: The distribution of X n becomes more 
</p>
<p>concentrated around fL as 71, gets large. 
</p>
<p>PROOF. Assume that (J &lt; 00. This is not necessary but it simplifies the 
</p>
<p>proof. Using Chebyshev's inequality, 
</p>
<p>which tends to 0 as 71, --+ 00 .&bull; 
</p>
<p>5.7 Example. Consider flipping a coin for which the probability of heads is 
</p>
<p>p. Let Xi denote the outcome of a single toss (0 or 1). Hence, p = P(Xi = 
</p>
<p>1) = E(Xi)' The fraction of heads after 71, tosses is X n . According to the law 
</p>
<p>of large numbers, X n converges to p in probability. This does not mean that 
</p>
<p>Xn will numerically equal p. It means that, when 71, is large, the distribution 
</p>
<p>of X n is tightly concentrated around p. Suppose that p = 1/2. How large 
</p>
<p>should 71 be so that P(.4 ::; Xn ::; .6) 2 .77 First, lE(Xn) = p = 1/2 and 
</p>
<p>V(Xn) = (J2/n = p(l - p)/n = 1/(471,). From Chebyshev's inequality, 
</p>
<p>1P'(IXn - fLl ::; .1) 
</p>
<p>1 -1P'(IXn - fLl &gt; .1) 
</p>
<p>&gt; 
1 25 
</p>
<p>1- =1--
471(.1)2 71 . 
</p>
<p>The last expression will be larger than .7 if n = 84 .&bull; 
</p>
<p>2Note that {! = lE(Xi) is the same for all i so we can define {! = lE(Xi) for any i. By 
</p>
<p>convention, we often write {! = lE( Xl). 
</p>
<p>3There is a stronger theorem in the appendix called the strong law of large numbers. </p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 The Central Limit Theorem 77 
</p>
<p>5.4 The Central Lirnit Them'ern 
</p>
<p>The law of large numbers says that the distribution of X n piles up near JL. 
</p>
<p>This isn't enough to help us approximate probability statements about X n' 
</p>
<p>For this we need the central limit theorem. 
</p>
<p>Suppose that Xl, ... ,Xn are lID with mean JL and variance 172. The central 
</p>
<p>limit theorem (CLT) says that Xn = 71.- 1 L:i Xi has a distribution which is 
</p>
<p>approximately Normal with mean JL and variance 17 2 /71.. This is remarkable 
since nothing is assumed about the distribution of Xi, except the existence of 
</p>
<p>the mean and variance. 
</p>
<p>5.8 Theorem (The Central Limit Theorem (CLT)). Let Xl, ... ,Xn be lID 
</p>
<p>with mean JL and variance 172 . Let Xn = 71.- 1 L:~=l Xi' Then 
</p>
<p>where Z rv N(O, 1). In other words, 
</p>
<p>/
z 1 2 
</p>
<p>lim IP'(Zn =:; z) = &lt;I&gt;(z) = m=e- x /2dx. 
n-+= _= v21f 
</p>
<p>Interpretation: Probability statements about Xn can be approximated 
</p>
<p>using a Normal distribution. It's the probability statements that we 
</p>
<p>are approximating, not the random variable itself. 
</p>
<p>In addition to Zn ~ N(O, 1), there are several forms of notation to denote 
</p>
<p>the fact that the distribution of Zn is converging to a Normal. They all mean 
</p>
<p>the same thing. Here they are: 
</p>
<p>vn(X n - JL) 
</p>
<p>fo(Xn - JL) 
</p>
<p>:::::: N(O,l) 
</p>
<p>:::::: N (JL' ~:) 
</p>
<p>:::::: N (0, :2) 
:::::: N(0,172) 
</p>
<p>:::::: N(O,l). 
</p>
<p>5.9 Example. Suppose that the number of errors per computer program has a 
</p>
<p>Poisson distribution with mean 5. We get 125 programs. Let Xl, . .. , X 125 be </p>
<p/>
</div>
<div class="page"><p/>
<p>78 5. Convergence of Random Variables 
</p>
<p>the number of errors in the programs. We want to approximate IP'(Xn &lt; 5.5). 
Let fL = IE(Xd = A = 5 and (J"2 = V(Xd = A = 5. Then, 
</p>
<p>TT1l (fo(X:: - fL) &lt; fo(5~ - fL)) 
IP'(Xn &lt; 5.5) lC u u 
</p>
<p>;:::::: IP'(Z &lt; 2.5) = .9938. &bull; 
</p>
<p>The central limit theorem tells us that Zn = fo( X n -IL) / (J" is approximately 
N(O,l). However, we rarely know (J". Later, we will see that we can estimate 
</p>
<p>(J"2 from Xl, ... , Xn by 
</p>
<p>This raises the following question: if we replace (J" with Sn, is the central limit 
</p>
<p>theorem still true? The answer is yes. 
</p>
<p>5.10 Theorem. Assume the same conditions as the CLT. Then, 
</p>
<p>You might wonder, how accurate the normal approximation is. The answer 
</p>
<p>is given in the Berry-Esseen theorem. 
</p>
<p>5.11 Theorem (The Berry-Esseen Inequality). Suppose thatIEIXl 13 &lt; 00. Then 
</p>
<p>(5.4) 
</p>
<p>There is also a multivariate version of the central limit theorem. 
</p>
<p>5.12 Theorem (Multivariate central limit theorem). Let Xl, ... ,Xn be lID ran-
</p>
<p>dom vectors where 
</p>
<p>with mean </p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 The Delta Method 79 
</p>
<p>and variance matrix ~. Let 
</p>
<p>yIn(X - fL) 'V'7 N(O, ~). 
</p>
<p>5.5 The Delta Method 
</p>
<p>If Yn has a limiting Normal distribution then the delta method allows us to 
</p>
<p>find the limiting distribution of g(Yn ) where 9 is any smooth function. 
</p>
<p>5.13 Theorem (The Delta Method). Suppose that 
</p>
<p>yIn(Yn - fL) 'V'7 N(O, 1) 
(J" 
</p>
<p>and that 9 is a differentiable function such that g' (fL) # 0. Then 
</p>
<p>In other words, 
</p>
<p>5.14 Example. Let Xl, ... ,Xn be lID with finite mean fL and finite variance 
</p>
<p>(J"2. By the central limit theorem, yIn(Xn - fL)l(J" 'V'7 N(O, 1). Let Wn = eX". 
</p>
<p>Thus, Wn = g(Xn) where g(s) = eS &bull; Since g'(s) = eS , the delta method 
implies that Wn :::::; N(e~, e2~(J"2 In). _ 
</p>
<p>There is also a multivariate version of the delta method. 
</p>
<p>5.15 Theorem (The Multivariate Delta Method). Suppose thatYn = (Ynl , ... , Ynd 
</p>
<p>is a sequence of random vectors such that </p>
<p/>
</div>
<div class="page"><p/>
<p>80 5. Convergence of Random Variables 
</p>
<p>Let 9 : IRk -+ IR and let 
</p>
<p>( :g" ) \1g(y) = u 
om 
</p>
<p>Let \1 JL denote \1 g(y) evaluated at y = JL and assume that the elements of \1 JL 
</p>
<p>are nonzero. Then 
</p>
<p>5.16 Example. Let 
</p>
<p>be IID random vectors with mean IL = (ILl, JL2f and variance ~. Let 
</p>
<p>_ 1 n 
</p>
<p>X 2 = - L:X2i 
n 
</p>
<p>i=l 
</p>
<p>and define Yn = X 1X 2. Thus, Yn = g(X 1,X2) where g(81,82) = 8182. By the 
</p>
<p>central limit theorem, 
</p>
<p>Now 
</p>
<p>and so 
</p>
<p>\1r~\1 JL = (!L2 !L1) ( 
</p>
<p>Therefore, 
</p>
<p>Vii ( XXI -!L1 ) ~ N(O, ~). 
2 - !L2 
</p>
<p>( 
og 
</p>
<p>) ( 82 ) \1g(8) = OSI og 81 
OS2 
</p>
<p>0"11 0"12 ) ( IL2 ) 2 2 0"12 0"22 ILl = !L20"11 + 2!L1!L20"12 + !Ll 0"22&middot; 
</p>
<p>Vii(X IX 2 - !Ll/L2) ~ N (0, IL~O"l1 + 2/Ll!L20"12 + !LI0"22). &bull; 
</p>
<p>5.6 Bibliographic Remarks 
</p>
<p>Convergence plays a central role in modern probability theory. For more de-
</p>
<p>tails, see Grimmett and Stirzaker (1982), Karr (1993), and Billingsley (1979). 
</p>
<p>Advanced convergence theory is explained in great detail in van der Vaart 
</p>
<p>and Wellner (1996) and and van der Vaart (1998). </p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 Appendix 81 
</p>
<p>5.7 Appendix 
</p>
<p>5.7.1 Almost Sure and L1 Convergence 
</p>
<p>We say that Xn converges almost surely to X, written Xn~X, if 
</p>
<p>W({s: Xn(s) -+ X(s)}) = 1. 
</p>
<p>We say that Xn converges in Ll to X, written Xn ~ X, if 
</p>
<p>as n -+ 00. 
</p>
<p>5.17 Theorem. Let Xn and X be random variables. Then: 
</p>
<p>(aj Xn~X implies that Xn~X, 
</p>
<p>(bj Xn~X implies that Xn~X, 
</p>
<p>(cj Xn~X implies that Xn~X, 
</p>
<p>The weak law of large numbers says that Xn converges to IE(Xd in proba-
</p>
<p>bility. The strong law asserts that this is also true almost surely. 
</p>
<p>5.18 Theorem (The Strong Law of Large Numbers). Let Xl, X 2, ... be lID. If 
</p>
<p>fL = IEIX11 &lt; 00 then Xn~ fl&middot; 
</p>
<p>A sequence Xn is asymptotically uniformly integrable if 
</p>
<p>lim lim sup IE (IXnII(IXnl &gt; M)) = O. 
!VI -+ (Xl n -+ (Xl 
</p>
<p>p 
5.19 Theorem. If Xn --+ band Xn is asymptotically uniformly integrable, 
</p>
<p>then IE(Xn) -+ b. 
</p>
<p>5.7.2 Proof of the Central Limit Theorem 
</p>
<p>Recall that if X is a random variable, its moment generating function (MGF) 
</p>
<p>is VJx(t) = IEetx . Assume in what follows that the MGF is finite in a neigh-
</p>
<p>borhood around t = O. 
</p>
<p>5.20 Lemma. Let Zl, Z2,'" be a sequence of random variables. Let 1/Jn be the 
</p>
<p>MGF of Zn. Let Z be another random variable and denote its MGF by 1/J. If 
</p>
<p>I/Jn(t) -+I/J(t) for all t in some open interval around 0, then Zn ~ Z. </p>
<p/>
</div>
<div class="page"><p/>
<p>82 5. Convergence of Random Variables 
</p>
<p>PROOF OF THE CENTRAL LIMIT THEOREM. Let Yi = (Xi - IL)/U. Then, 
Zn = n- l / 2 Li Yi. Let VJ(t) be the MGF of Yi. The MGF of Li Yi is (VJ(t))n 
and MGF of Zn is [1/!(t/fo)]n == en(t). NowljJ'(O) = lE(yd = 0,1jJ"(0) = 
lE(Yl) = V(Yl ) = 1. So, 
</p>
<p>VJ(t) 
t 2 t 3 
</p>
<p>VJ(O) + tv/(O) + ~1/!"(0) + ~1/!"'(O) + ... 
2. 3. 
</p>
<p>t2 t3 "'() 1 + 0 + - + -1/! 0 + ... 
2 3! 
</p>
<p>t 2 t 3 "'() 1 + - + -1/! 0 + ... 
2 3! 
</p>
<p>Now, 
</p>
<p>en(t) [1/!(Jn)r 
</p>
<p>[1 + ~ + _t
3 _1/!"'(0) + .. . ]n 
</p>
<p>2n 3!n3 / 2 
</p>
<p>[1 I ~ I b;:"'(o) I r 
--+ et2 / 2 
</p>
<p>which is the MGF of a N(O,l). The result follows from the previous Theorem. 
</p>
<p>In the last step we used the fact that if an --+ a then 
</p>
<p>( 1 + ~~ r --+ ea. &bull; 
5.8 Exercises 
</p>
<p>1. Let Xl, ... ,Xn be IID with finite mean fL = lE(Xd and finite variance 
</p>
<p>u 2 = V(Xl)' Let Xn be the sample mean and let S~ be the sample 
</p>
<p>variance. 
</p>
<p>(a) Show that lE(S~) = u 2 . 
</p>
<p>( ) 2 P 2 . . 2 _ -1 ",n 2 -2 b Show that Sn --+ U . Hmt. Show that Sn - cnn L...i=l Xi - dnXn 
</p>
<p>where Cn --+ 1 and dn --+ 1. Apply the law oflarge numbers to n- l L~=l xl 
and to X n . Then use part (e) of Theorem 5.5. 
</p>
<p>2. Let Xl, X 2 , ... be a sequence of random variables. Show that Xn ~ b 
</p>
<p>if and only if 
</p>
<p>lim lE(Xn) = b and lim V(Xn) = o. 
n---+oo n---+oo </p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Exercises 83 
</p>
<p>3. Let Xl"'" Xn be IID and let fL = lE(XI). Suppose that the variance is 
</p>
<p>finite. Show that X n ~ M. 
</p>
<p>4. Let Xl, X 2 , ... be a sequence of random variables such that 
</p>
<p>1 
and JID (Xn = n) = 2' 
</p>
<p>n 
</p>
<p>Does Xn converge in probability? Does Xn converge in quadratic mean? 
</p>
<p>5. Let Xl,"" Xn rv Bernoulli(p). Prove that 
</p>
<p>1 n 
- LX;~p and 
n 
</p>
<p>i=l 
</p>
<p>1 n 2 qm 
- LXi --=---+ p. 
n 
</p>
<p>i=l 
</p>
<p>6. Suppose that the height of men has mean 68 inches and standard de-
</p>
<p>viation 2.6 inches. We draw 100 men at random. Find (approximately) 
</p>
<p>the probability that the average height of men in our sample will be at 
</p>
<p>least 68 inches. 
</p>
<p>7. Let An = l/n for n = 1,2, .... Let Xn rv Poisson(An). 
p 
</p>
<p>(a) Show that Xn---+O. 
p 
</p>
<p>(b) Let Yn = nXn. Show that Yn---+O. 
</p>
<p>8. Suppose we have a computer program consisting of n = 100 pages of 
</p>
<p>code. Let Xi be the number of errors on the ith page of code. Suppose 
</p>
<p>that the X;$ are Poisson with mean 1 and that they are independent. 
</p>
<p>Let Y = 2::~=1 Xi be the total number of errors. Use the central limit 
</p>
<p>theorem to approximate J1D(Y &lt; 90). 
</p>
<p>9. Suppose that J1D(X = 1) = J1D(X = -1) = 1/2. Define 
</p>
<p>X = {X with probability 1 - ~ 
n en with probability ~. 
</p>
<p>Does Xn converge to X in probability? Does Xn converge to X in dis-
</p>
<p>tribution? Does lE(X - Xn)2 converge to O? 
</p>
<p>10. Let Z rv N(O, 1). Let t &gt; O. Show that, for any k &gt; 0, 
</p>
<p>Compare this to Mill's inequality in Chapter 4. </p>
<p/>
</div>
<div class="page"><p/>
<p>84 5. Convergence of Random Variables 
</p>
<p>11. Suppose that Xn rv N(O,I/n) and let X be a random variable with 
</p>
<p>distribution F(x) = 0 if X &lt; 0 and F(x) = 1 if x ~ O. Does Xn converge 
</p>
<p>to X in probability? (Prove or disprove). Does Xn converge to X in 
</p>
<p>distribution? (Prove or disprove). 
</p>
<p>12. Let X, Xl, X 2 , X 3 , ... be random variables that are positive and integer 
</p>
<p>valued. Show that Xn 'V') X if and only if 
</p>
<p>lim IP'(Xn = k) = IP'(X = k) 
n---+oo 
</p>
<p>for every integer k. 
</p>
<p>13. Let Zl, Z2, ... be lID random variables with density f. Suppose that 
</p>
<p>IP'(Zi &gt; 0) = 1 and that), = limxto f(x) &gt; O. Let 
</p>
<p>Show that Xn ~ Z where Z has an exponential distribution with mean 
</p>
<p>1/),. 
</p>
<p>14. Let Xl"'" Xn rv Uniform(O, 1). Let Yn = X~. Find the limiting distri-
bution of Yn . 
</p>
<p>15. Let 
</p>
<p>be IID random vectors with mean IL = (ILl, IL2) and variance ~. Let 
</p>
<p>_ 1 n 
Xl = - LXIi, 
</p>
<p>n 
i=l 
</p>
<p>_ 1 n 
</p>
<p>X 2 =-L X 2i n 
i=l 
</p>
<p>and define Yn = X Ii X 2. Find the limiting distribution of Yn . 
</p>
<p>16. Construct an example where Xn 'V') X and Yn 'V') Y but Xn + Yn does 
not converge in distribution to X + Y. </p>
<p/>
</div>
<div class="page"><p/>
<p>Part II 
</p>
<p>Statistical Inference </p>
<p/>
</div>
<div class="page"><p/>
<p>6 
</p>
<p>Models, Statistical Inference and 
Learning 
</p>
<p>6.1 Introduction 
</p>
<p>Statistical inference, or "learning" as it is called in computer science, is the 
</p>
<p>process of using data to infer the distribution that generated the data. A 
</p>
<p>typical statistical inference question is: 
</p>
<p>Given a sample Xl, .. . , Xn rv F, how do we infer F? 
</p>
<p>In some cases, we may want to infer only some feature of F such as its 
</p>
<p>mean. 
</p>
<p>6.2 Pararnetric and Nonparametric Models 
</p>
<p>A statistical model ~ is a set of distributions (or densities or regression 
</p>
<p>functions). A parametric model is a set ~ that can be parameterized by a 
</p>
<p>finite number of parameters. For example, if we assume that the data come 
</p>
<p>from a Normal distribution, then the model is 
</p>
<p>~ = {f(X;/L,cr) = cr~exp {- 2~2(X _/L)2}, /L E lR, cr &gt; o}. (6.1) 
This is a two-parameter model. We have written the density as f(x; IL, cr) to 
</p>
<p>show that x is a value of the random variable whereas /L and cr are parameters. </p>
<p/>
</div>
<div class="page"><p/>
<p>88 6. Models, Statistical Inference and Learning 
</p>
<p>In general, a parametric model takes the form 
</p>
<p>J = {f(X; e): e E 8 } (6.2) 
</p>
<p>where e is an unknown parameter (or vector of parameters) that can take 
values in the parameter space 8. If e is a vector but we are only interested in 
one component of e, we call the remaining parameters nuisance parameters. 
A nonparametric model is a set J that cannot be parameterized by a finite 
</p>
<p>number of parameters. For example, JALL = {all CDF'S} is nonparametric. 1 
</p>
<p>6.1 Example (One-dimensional Parametric Estimation). Let Xl, ... , Xn be in-
</p>
<p>dependent Bernoulli(p) observations. The problem is to estimate the param-
</p>
<p>eter p .&bull; 
</p>
<p>6.2 Example (Two-dimensional Parametric Estimation). Suppose that Xl, ... , 
</p>
<p>Xn rv F and we assume that the PDF f E J where J is given in (6.1). In 
this case there are two parameters, fL and (J. The goal is to estimate the 
</p>
<p>parameters from the data. If we are only interested in estimating IL, then IL is 
</p>
<p>the parameter of interest and (J is a nuisance parameter .&bull; 
</p>
<p>6.3 Example (Nonparametric estimation of the CDF). Let Xl, ... , Xn be inde-
</p>
<p>pendent observations from a CDF F. The problem is to estimate F assuming 
</p>
<p>only that F E JALL = {all CDF'S} .&bull; 
</p>
<p>6.4 Example (Nonparametric density estimation). Let Xl, ... , Xn be indepen-
</p>
<p>dent observations from a CDF F and let f = F' be the PDF. Suppose we want 
to estimate the PDF f. It is not possible to estimate f assuming only that 
FE JALL. We need to assume some smoothness on f. For example, we might 
assume that f E J = JDENS n JSOB where JDENS is the set of all probability 
density functions and 
</p>
<p>The class JSOB is called a Sobolev space; it is the set of functions that are 
</p>
<p>not "too wiggly." &bull; 
</p>
<p>6.5 Example (Nonparametric estimation of functionals). Let Xl, ... , Xn rv F. 
</p>
<p>Suppose we want to estimate IL = IE(XI) = J xdF(x) assuming only that 
</p>
<p>1 The distinction between parametric and non parametric is more subtle than this but we don't 
</p>
<p>need a rigorous definition for our purposes. </p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Parametric and Nonparametric Models 89 
</p>
<p>It exists. The mean fL may be thought of as a function of F: we can write 
</p>
<p>It = T(F) = f xdF(x). In general, any function of F is called a statis-
tical functional. Other examples of functionals are the variance T(F) 
</p>
<p>f x2dF(x) - (J xdF(x))2 and the median T(F) = F- 1 (1/2) .&bull; 
</p>
<p>6.6 Example (Regression, prediction, and classification). Suppose we observe pairs 
</p>
<p>of data (Xl, Yd, ... (Xn' Yn). Perhaps Xi is the blood pressure of subject 'i 
</p>
<p>and Yi is how long they live. X is called a predictor or regressor or fea-
ture or independent variable. Y is called the outcome or the response 
</p>
<p>variable or the dependent variable. We call r(x) = IE(YIX = x) the re-
</p>
<p>gression function. If we assume that r E J where J is finite dimensional -
</p>
<p>the set of straight lines for example - then we have a parametric regres-
</p>
<p>sion model. If we assume that r E J where J is not finite dimensional then 
</p>
<p>we have a nonparametric regression model. The goal of predicting Y for 
</p>
<p>a new patient based on their X value is called prediction. If Y is discrete 
</p>
<p>(for example, live or die) then prediction is instead called classification. If 
</p>
<p>our goal is to estimate the function r, then we call this regression or curve 
</p>
<p>estimation. Regression models are sometimes written as 
</p>
<p>Y = r(X) + f (6.3) 
</p>
<p>where IE( f) = O. We can always rewrite a regression model this way. To see 
</p>
<p>this, define f = Y - r(X) and hence Y = Y + r(X) - r(X) = r(X) + f. 
Moreover, IE(f) = IEIE(fIX) = IE(IE(Y - r(X))IX) = IE(IE(YIX) - r(X)) = 
</p>
<p>IE(r(X) - r(X)) = O .&bull; 
</p>
<p>WHAT'S NEXT? It is traditional in most introductory courses to start with 
</p>
<p>parametric inference. Instead, we will start with nonparametric inference and 
</p>
<p>then we will cover parametric inference. In some respects, nonparametric in-
</p>
<p>ference is easier to understand and is more useful than parametric inference. 
</p>
<p>FREQUENTISTS AND BAYESIANS. There are many approaches to statistical 
</p>
<p>inference. The two dominant approaches are called frequentist inference 
</p>
<p>and Bayesian inference. We'll cover both but we will start with frequentist 
</p>
<p>inference. We'll postpone a discussion of the pros and cons of these two until 
</p>
<p>later. 
</p>
<p>SOME NOTATION. If J = {j(x; e) : e E 8} is a parametric model, we write 
lP'e(X E A) = fA f(x; e)dx and IEe(r(X)) = f r(x)f(x; e)dx. The subscript e 
indicates that the probability or expectation is with respect to f(x; e); it does 
</p>
<p>not mean we are averaging over e. Similarly, we write 'life for the variance. </p>
<p/>
</div>
<div class="page"><p/>
<p>90 6. Models, Statistical Inference and Learning 
</p>
<p>6.3 Fundarnental Concepts in Inference 
</p>
<p>Many inferential problems can be identified as being one of three types: es-
</p>
<p>timation, confidence sets, or hypothesis testing. We will treat all of these 
</p>
<p>problems in detail in the rest of the book. Here, we give a brief introduction 
</p>
<p>to the ideas. 
</p>
<p>6.3.1 Point Estimation 
</p>
<p>Point estimation refers to providing a single "best guess" of some quantity 
</p>
<p>of interest. The quantity of interest could be a parameter in a parametric 
</p>
<p>model, a CDF F, a probability density function f, a regression function T, or 
a prediction for a future value Y of some random variable. 
</p>
<p>By convention, we denote a point estimate of e bye or en. Remember 
that e is a fixed, unknown quantity. The estimate e depends on the 
data so e is a random variable. 
</p>
<p>More formally, let Xl"'" Xn be n lID data points from some distribution 
F. A point estimator en of a parameter e is some function of Xl"'" Xn: 
</p>
<p>The bias of an estimator is defined by 
</p>
<p>bias(en ) = lEe (en) - e. (6.4) 
</p>
<p>We say that en is unbiased if lE(en ) = e. Unbiased ness used to receive much 
attention but these days is considered less important; many of the estimators 
</p>
<p>we will use are biased. A reasonable requirement for an estimator is that it 
</p>
<p>should converge to the true parameter value as we collect more and more 
</p>
<p>data. This requirement is quantified by the following definition: 
</p>
<p>6.7 Definition. A point estimatoT en of a pammeteT e is consistent if 
- p en -----+ e. 
</p>
<p>The distribution of en is called the sampling distribution. The standard 
deviation of en is called the standard error, denoted by se: 
</p>
<p>(6.5) 
</p>
<p>Often, the standard error depends on the unknown F. In those cases, se is 
</p>
<p>an unknown quantity but we usually can estimate it. The estimated standard 
</p>
<p>error is denoted by se. </p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Fundamental Concepts in Inference 91 
</p>
<p>6.8 Example. Let Xl' ... ' Xn rv Bernoulli(p) and let Pn = n- 1 Li Xi. Then 
</p>
<p>lE(Pn) = n- 1 Li lE(Xi) = P so Pn is unbiased. The standard error is se = 
</p>
<p>jV(Pn) = jp(l - p)/n. The estimated standard error is se = jp(l - p)/n . 
&bull; 
</p>
<p>The quality of a point estimate is sometimes assessed by the mean squared 
</p>
<p>error, or MSE defined by 
</p>
<p>~ 2 
MSE = lEg (en - e) . (6.6) 
</p>
<p>Keep in mind that lEg 0 refers to expectation with respect to the distribution 
n 
</p>
<p>f(X1, ... , Xn; e) = II f(Xi; e) 
i=l 
</p>
<p>that generated the data. It does not mean we are averaging over a distribution 
</p>
<p>for e. 
</p>
<p>6.9 Theorem. The MSE can be written as 
</p>
<p>(6.7) 
</p>
<p>PROOF. Let en = Eg(en). Then 
</p>
<p>lEg(en - e)2 lEg(Bn - en + en - e)2 
</p>
<p>lEg(Bn - en)2 + 2(en - e)lEg(en - en) + lEg (en - e)2 
</p>
<p>(en - e)2 + lEg(Bn - en)2 
</p>
<p>bias2 (Bn) + v(en ) 
</p>
<p>where we have used the fact that lEg(Bn - en) = en - en = o .&bull; 
</p>
<p>6.10 Theorem. If bias -+ 0 and se -+ 0 as n -+ IX) then en is consistent, that 
- p 
</p>
<p>is, en -----+ e. 
</p>
<p>PROOF. If bias -+ 0 and se -+ 0 then, by Theorem 6.9, MSE -+ O. It 
</p>
<p>follows that en ~ e. (Recall Definition 5.2.) The result follows from part (b) 
of Theorem 5.4 .&bull; 
</p>
<p>6.11 Example. Returning to the coin flipping example, we have that lEp(Pn) = 
</p>
<p>p so the bias = p - p = 0 and se = jp(l - p)/n -+ o. Hence, Pn~ p, that is, 
Pn is a consistent estimator .&bull; 
</p>
<p>Many of the estimators we will encounter will turn out to have, approxi-
</p>
<p>mately, a Normal distribution. </p>
<p/>
</div>
<div class="page"><p/>
<p>92 6. Models, Statistical Inference and Learning 
</p>
<p>6.12 Definition. An estimator is asymptotically Normal if 
</p>
<p>8n - 8 
-- ~N(O,I). 
</p>
<p>se 
(6.8) 
</p>
<p>6.3.2 Confidence Sets 
</p>
<p>A 1 - Q confidence interval for a parameter 8 is an interval Cn = (a, b) 
</p>
<p>where a = a(X1 , ... , Xn) and b = b(X1 , ... , Xn) are functions of the data 
</p>
<p>such that 
</p>
<p>lP'e(8 E Cn) :;:, 1 - Q, for all 8 E 8. (6.9) 
</p>
<p>In words, (0, b) traps 8 with probability 1- Q. We callI - Q the coverage of 
</p>
<p>the confidence interval. 
</p>
<p>Warning! Cn is random and 8 is fixed. 
</p>
<p>Commonly, people use 95 percent confidence intervals, which corresponds 
</p>
<p>to choosing Q = 0.05. If 8 is a vector then we use a confidence set (such as 
</p>
<p>a sphere or an ellipse) instead of an interval. 
</p>
<p>Warning! There is much confusion about how to interpret a confidence 
</p>
<p>interval. A confidence interval is not a probability statement about 8 since 
</p>
<p>8 is a fixed quantity, not a random variable. Some texts interpret confidence 
</p>
<p>intervals as follows: if I repeat the experiment over and over, the interval will 
</p>
<p>contain the parameter 95 percent of the time. This is correct but useless since 
</p>
<p>we rarely repeat the same experiment over and over. A better interpretation 
</p>
<p>is this: 
</p>
<p>On day 1, you collect data and construct a 95 percent confidence 
</p>
<p>interval for a parameter 81 . On day 2, you collect new data and con-
</p>
<p>struct a 95 percent confidence interval for an unrelated parameter 82 . 
</p>
<p>On day 3, you collect new data and construct a 95 percent confi-
</p>
<p>dence interval for an unrelated parameter 83 . You continue this way 
</p>
<p>constructing confidence intervals for a sequence of unrelated param-
</p>
<p>eters 81 ,82 , ... Then 95 percent of your intervals will trap the true 
</p>
<p>parameter value. There is no need to introduce the idea of repeating 
</p>
<p>the same experiment over and over. 
</p>
<p>6.13 Example. Every day, newspapers report opinion polls. For example, they 
</p>
<p>might say that "83 percent of the population favor arming pilots with guns." 
</p>
<p>Usually, you will see a statement like "this poll is accurate to within 4 points </p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Fundamental Concepts in Inference 93 
</p>
<p>95 percent of the time." They are saying that 83 &plusmn; 4 is a 95 percent confidence 
</p>
<p>interval for the true but unknown proportion p of people who favor arming 
</p>
<p>pilots with guns. If you form a confidence interval this way every day for the 
</p>
<p>rest of your life, 95 percent of your intervals will contain the true parameter. 
</p>
<p>This is true even though you are estimating a different quantity (a different 
</p>
<p>poll question) every day. _ 
</p>
<p>6.14 Example. The fact that a confidence interval is not a probability state-
</p>
<p>ment about e is confusing. Consider this example from Berger and Wolpert 
(1984). Let e be a fixed, known real number and let Xl, X 2 be independent 
random variables such that W(Xi = 1) = W(Xi = -1) = 1/2. Now define 
</p>
<p>Yi = e + Xi and suppose that you only observe Yl and Y 2 . Define the follow-
ing "confidence interval" which actually only contains one point: 
</p>
<p>ifYl =Y2 
</p>
<p>if Yl cJ Y2 &bull; 
</p>
<p>You can check that, no matter what e is, we have We(e E C) = 3/4 so this 
</p>
<p>is a 75 percent confidence interval. Suppose we now do the experiment and 
</p>
<p>we get Yl = 15 and Y2 = 17. Then our 75 percent confidence interval is {16}. 
</p>
<p>However, we are certain that e = 16. If you wanted to make a probability 
statement about e you would probably say that W(e E ClYl, Y2 ) = 1. There is 
</p>
<p>nothing wrong with saying that {16} is a 75 percent confidence interval. But 
</p>
<p>is it not a probability statement about e. _ 
</p>
<p>In Chapter 11 we will discuss Bayesian methods in which we treat e as if it 
were a random variable and we do make probability statements about e. In 
particular, we will make statements like "the probability that e is in Cn, given 
the data, is 95 percent." However, these Bayesian intervals refer to degree-
</p>
<p>of-belief probabilities. These Bayesian intervals will not, in general, trap the 
</p>
<p>parameter 95 percent of the time. 
</p>
<p>6.15 Example. In the coin flipping setting, let Cn = (Pn - En, Pn + En) where 
E; = log(2/a)/(2n). From Hoeffding's inequality (4.4) it follows that 
</p>
<p>for every p. Hence, Cn is a 1 - a confidence interval. _ 
</p>
<p>As mentioned earlier, point estimators often have a limiting Normal dis-
</p>
<p>tribution, meaning that equation (6.8) holds, that is, en ;:::::: N(e, SE?). In this 
case we can construct (approximate) confidence intervals as follows. </p>
<p/>
</div>
<div class="page"><p/>
<p>94 6. Models, Statistical Inference and Learning 
</p>
<p>6.16 Theorem (Normal-based Confidence Interval). Suppose that en:::::: N(e, se2 ). 
Let q:, be the CDF of a standard Normal and let ZOi/2 = q:,-1(1 - (0;/2)), that 
</p>
<p>is, JID(Z &gt; ZOI/2) = 0;/2 and JID( -ZOI/2 &lt; Z &lt; ZOI/2) = 1 - 0; where Z rv N(O, 1). 
Let 
</p>
<p>( 6.10) 
</p>
<p>Then 
</p>
<p>(6.11) 
</p>
<p>PROOF. Let Zn = (en - e)/se. By assumption Zn ~ Z where Z rv N(O, 1). 
Hence, 
</p>
<p>JID 9 (en - ZOI/2 se &lt; e &lt; en + ZOI/2 se ) 
</p>
<p>JlDg ( -ZOI/2 &lt; en ~ e &lt; ZOI/2) 
</p>
<p>--+ JID ( -ZOI/2 &lt; Z &lt; ZOI/2) 
1 - 0;. _ 
</p>
<p>For 95 percent confidence intervals, 0; = 0.05 and ZOI/2 = 1.96 :::::: 2 leading 
</p>
<p>to the approximate 95 percent confidence interval en &plusmn; 2 se. 
</p>
<p>6.17 Example. Let Xl"'" Xn rv Bernoulli(p) and let Pn = n- l L~=l Xi' 
</p>
<p>Then V(Pn) = n- 2 L~=l V(Xi ) = n- 2 L~=l p(l - p) = n- 2np(1 - p) = p(l -
</p>
<p>p)/n. Hence, se = Vp(l - p)/n and se = VPn(l - Pn)/n. By the Central 
Limit Theorem, Pn :::::: N(p, se2 ). Therefore, an approximate 1 - 0; confidence 
interval is 
</p>
<p>_ _ _! Pn (1 - Pn) 
Pn &plusmn; ZOI/2se = Pn &plusmn; ZOI/2 V n . 
</p>
<p>Compare this with the confidence interval in example 6.15. The Normal-based 
</p>
<p>interval is shorter but it only has approximately (large sample) correct cover-
</p>
<p>age. _ 
</p>
<p>6.3.3 Hypothesis Testing 
</p>
<p>In hypothesis testing, we start with some default theory - called a null 
</p>
<p>hypothesis - and we ask if the data provide sufficient evidence to reject the 
</p>
<p>theory. If not we retain the null hypothesis. 2 
</p>
<p>2The term "retaining the null hypothesis" is due to Chris Genovese. Other terminology is 
</p>
<p>"accepting the null" or "failing to reject the null." </p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Bibliographic Remarks 95 
</p>
<p>6.18 Example (Testing if a Coin is Fair). Let 
</p>
<p>Xl, ... ,Xn rv Bernoulli(p) 
</p>
<p>be n independent coin flips. Suppose we want to test if the coin is fair. Let Ho 
</p>
<p>denote the hypothesis that the coin is fair and let HI denote the hypothesis 
</p>
<p>that the coin is not fair. Ho is called the null hypothesis and HI is called 
</p>
<p>the alternative hypothesis. We can write the hypotheses as 
</p>
<p>Ho : p = 1/2 versus HI: p # 1/2. 
</p>
<p>It seems reasonable to reject Ho if T = IPn - (1/2)1 is large. When we discuss 
hypothesis testing in detail, we will be more precise about how large T should 
</p>
<p>be to reject Ho .&bull; 
</p>
<p>6.4 Bibliographic Remarks 
</p>
<p>Statistical inference is covered in many texts. Elementary texts include DeG-
</p>
<p>root and Schervish (2002) and Larsen and Marx (1986). At the intermediate 
</p>
<p>level I recommend Casella and Berger (2002), Bickel and Doksum (2000), and 
</p>
<p>Rice (1995). At the advanced level, Cox and Hinkley (2000), Lehmann and 
</p>
<p>Casella (1998), Lehmann (1986), and van der Vaart (1998). 
</p>
<p>6.5 Appendix 
</p>
<p>Our definition of confidence interval requires that lP' e (e E en) ~ 1 - a 
</p>
<p>for all e E 8. A pointwise asymptotic confidence interval requires that 
liminfn--+oolP'e(e E en) ~ I-a for all e E 8. A uniform asymptotic con-
fidence interval requires that liminfn--+ooinfeEelP'e(e E en) ~ 1- a. The 
</p>
<p>approximate Normal-based interval is a pointwise asymptotic confidence in-
</p>
<p>terval. 
</p>
<p>6.6 Exercises 
</p>
<p>1. Let Xl, ... ,Xn rv Poisson(&gt;.) and let ~ = n-lL~=lXi. Find the bias, 
</p>
<p>se, and MSE of this estimator. 
</p>
<p>2. Let Xl' ... ' Xn rv Uniform(O, e) and let e = max{Xl , ... , X n}. Find the 
bias, se, and MSE of this estimator. </p>
<p/>
</div>
<div class="page"><p/>
<p>96 6. Models, Statistical Inference and Learning 
</p>
<p>3. Let Xl"'" Xn rv Uniform(O, e) and let e = 2Xn. Find the bias, se, and 
MSE of this estimator. </p>
<p/>
</div>
<div class="page"><p/>
<p>7 
</p>
<p>Estimating the CDF and Statistical 
Functionals 
</p>
<p>The first inference problem we will consider is nonparametric estimation of the 
</p>
<p>CDF F. Then we will estimate statistical functionals, which are functions of 
</p>
<p>CDF, such as the mean, the variance, and the correlation. The nonparametric 
</p>
<p>method for estimating functionals is called the plug-in method. 
</p>
<p>7.1 The Empirical Distribution Function 
</p>
<p>Let Xl' ... ' Xn rv F be an IID sample where F is a distribution function on 
</p>
<p>the real line. We will estimate F with the empirical distribution function, 
</p>
<p>which is defined as follows. 
</p>
<p>7.1 Definition. The empirical distribution function Fn is the CDF 
</p>
<p>that puts mass lin at each data point Xi. Formally, 
</p>
<p>where 
</p>
<p>Fn(x) = L~=l J(Xi S x) 
n 
</p>
<p>if Xi S x 
if Xi &gt; x. 
</p>
<p>(7.1) </p>
<p/>
</div>
<div class="page"><p/>
<p>98 7. Estimating the COF and Statistical Functionals 
</p>
<p>o 
</p>
<p>L() 
</p>
<p>o 
</p>
<p>o 
o 
</p>
<p>0.0 0 .5 1 .0 1.5 
FIGURE 7.1. Nerve data. Each vertical line represents one data point. The solid 
</p>
<p>line is the empirical distribution function. The lines above and below the middle 
</p>
<p>line are a 95 percent confidence band. 
</p>
<p>7.2 Example (Nerve Data). Cox and Lewis (1966) reported 799 waiting times 
</p>
<p>between successive pulses along a nerve fiber. Figure 7.1 shows the empirical 
</p>
<p>CDF Fn. The data points are shown as small vertical lines at the bottom of 
</p>
<p>the plot. Suppose we want to estimate the fraction of waiting times between 
</p>
<p>.4 and .6 seconds. The estimate is Fn(.6) - Fn(.4) = .93 - .84 = .09 .&bull; 
</p>
<p>7.3 Theorem. At any fixed value of x, 
</p>
<p>IE (Fn(x)) F(x), 
</p>
<p>V (Fn(X)) 
F(x)(1 - F(x)) 
</p>
<p>n 
</p>
<p>MSE 
F(x)(1 - F(x)) 
</p>
<p>---+ 0, 
n 
</p>
<p>Fn(x) 
p 
</p>
<p>F(x). ---+ 
</p>
<p>7.4 Theorem (The Glivenko-Cantelli Theorem). Let Xl"'" Xn cv F. Then I 
</p>
<p>~ p 
</p>
<p>sup IFn(x) - F(x) 1---+ O. 
x 
</p>
<p>Now we give an inequality that will be used to construct a confidence band. 
</p>
<p>lMore precisely, sUPx IFn(x) - F(x)1 converges to 0 almost surely. </p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Statistical Functionals 99 
</p>
<p>7.5 Theorem (The Dvoretzky-Kiefer-Wolfowitz (DKW) Inequality). LetX1 , ... , 
</p>
<p>Xn rv F. Then, for any E &gt; 0, 
</p>
<p>(7.2) 
</p>
<p>From the DKW inequality, we can construct a confidence set as follows: 
</p>
<p>A Nonparametric 1 - a Confidence Band for F 
</p>
<p>Define, 
</p>
<p>L(x) max{Fn(x) - En, O} 
</p>
<p>U(X) min{Fn(x) + En, I} 
</p>
<p>where En ~ log (~). 
2n a 
</p>
<p>It follows from (7.2) that for any F, 
</p>
<p>IP'( L(x) ~ F(x) ~ U(x) for all x) ~ 1 - a. (7.3) 
</p>
<p>7.6 Example. The dashed lines in Figure 7.1 give a 95 percent confidence 
</p>
<p>band using En = V 2~ log (.55) = .048. &bull; 
</p>
<p>7.2 Statistical Functionals 
</p>
<p>A statistical functional T(F) is any function of F. Examples are the mean 
</p>
<p>fJ = J xdF(x), the variance (]"2 = J(x - fJ)2 dF(x) and the median m = 
</p>
<p>F-1 (1/2). 
</p>
<p>7.7 Definition. The plug-in estimator of8 = T(F) is defined by 
</p>
<p>In other words, just plug in Fn for the unknown F. 
</p>
<p>7.8 Definition. IfT(F) = J r(x)dF(x) for some function r(x) then T is 
</p>
<p>called a linear functional. </p>
<p/>
</div>
<div class="page"><p/>
<p>100 7. Estimating the COF and Statistical Functionals 
</p>
<p>The reason T(F) = J r(x)dF(x) is called a linear functional is because T 
</p>
<p>satisfies 
</p>
<p>T(aF + bG) = aT(F) + bT(G), 
</p>
<p>hence T is linear in its arguments. Recall that J r(x)dF(x) is defined to be 
</p>
<p>J r(x)f(x)dx in the continuous case and L: j r(xj)f(xj) in the discrete. The 
empirical cdf Fn(x) is discrete, putting mass lin at each Xi. Hence, ifT(F) = 
</p>
<p>J r(x)dF(x) is a linear functional then we have: 
</p>
<p>7.9 Theorem. The plug-in estimator for linear functional 
</p>
<p>T(F) = J r(x)dF(x) is: 
</p>
<p>(7.4) 
</p>
<p>Sometimes we can find the estimated standard error se of T(Fn) by doing 
</p>
<p>some calculations. However, in other cases it is not obvious how to estimate 
</p>
<p>the standard error. In the next chapter, we will discuss a general method for 
</p>
<p>finding se. For now, let us just assume that somehow we can find se. 
In many cases, it turns out that 
</p>
<p>(7.5) 
</p>
<p>By equation (6.11), an approximate 1- ex confidence interval for T(F) is then 
</p>
<p>(7.6) 
</p>
<p>We will call this the Normal-based interval. For a 95 percent confidence 
</p>
<p>interval, Za/2 = z.05/2 = 1.96 ;::;0 2 so the interval is 
</p>
<p>T(Fn) &plusmn; 2 se. 
</p>
<p>7.10 Example (The mean). Let J.L = T(F) = JxdF(x). The plug-in estima-
</p>
<p>tor is Ii = J xdFn(x) = X n&middot; The standard error is se = VV(Xn) = (5lfo. If 
(j denotes an estimate of (5, then the estimated standard error is (j I fo. (In 
the next example, we shall see how to estimate (5.) A Normal-based confidence 
</p>
<p>interval for It is X n &plusmn; Za/2 se .&bull; 
</p>
<p>7.11 Example (The Variance). Let (52 = T(F) = V(X) = J x 2 dF(x)-(J xdF(x)( 
</p>
<p>The plug-in estimator is </p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Statistical Functionals 101 
</p>
<p>( )
</p>
<p>2 
1 n 1 n 
- LXf- - LXi 
n n 
</p>
<p>i=l i=l 
</p>
<p>Another reasonable estimator of (J2 is the sample variance 
</p>
<p>In practice, there is little difference between (j2 and S~ and you can use either 
</p>
<p>one. Returning to the last example, we now see that the estimated standard 
</p>
<p>error of the estimate of the mean is se = (; / yin .&bull; 
</p>
<p>7.12 Example (The Skewness). Let fL and (J2 denote the mean and variance 
</p>
<p>of a random variable X. The skewness is defined to be 
</p>
<p>{J(x - fL)2dF(x)} 3/2&middot; 
</p>
<p>The skewness measures the lack of symmetry of a distribution. To find the 
</p>
<p>plug-in estimate, first recall that fi = n- 1 Li Xi and (;2 = n- 1 Li(Xi - 11)2. 
</p>
<p>The plug-in estimate of", is 
</p>
<p>~ Li(Xi - fi)3 
(;3 &bull; 
</p>
<p>7.13 Example (Correlation). Let Z = (X, Y) and let p = T(F) = JE(X -
</p>
<p>fLx )(Y -fLy )/((Jx(Jy) denote the correlation between X and Y, where F(x, y) 
</p>
<p>is bivariate. We can write 
</p>
<p>where 
</p>
<p>Tl (F) = J x dF(z), T2(F) = J y dF(z), T3(F) = J xy dF(z), 
T4(F) = J x2 dF(z), T5(F) = J y2 dF(z), 
</p>
<p>and 
t3 - tlt2 
</p>
<p>a(tl' . .. , t5) = V(t4 _ tI)(t5 - t~) 
</p>
<p>Replace F with Fn in Tl (F), ... , T5 (F), and take </p>
<p/>
</div>
<div class="page"><p/>
<p>102 7. Estimating the COF and Statistical Functionals 
</p>
<p>We get 
</p>
<p>2.: i (Xi - X n)(Yi - Y n) 
p = ----;:::c===========-'-r========= 
</p>
<p>V2.:i(Xi - X n)2V2.:i(Yi - Y n)2 
</p>
<p>which is called the sample correlation. _ 
</p>
<p>7.14 Example (Quantiles). Let F be strictly increasing with density f. For 
o &lt; p &lt; 1, the pth quantile is defined by T(F) = F-l(p). The estimate if 
T(F) is F;:l(p). We have to be a bit careful since Fn is not invertible. To 
</p>
<p>avoid ambiguity we define 
</p>
<p>F;:l(p) = inf{x: Fn(x) ~ pl. 
</p>
<p>We call T(Fn) = F;:l(p) the pth sample quantile. _ 
</p>
<p>Only in the first example did we compute a standard error or a confidence 
</p>
<p>interval. How shall we handle the other examples? When we discuss parametric 
</p>
<p>methods, we will develop formulas for standard errors and confidence intervals. 
</p>
<p>But in our nonparametric setting we need something else. In the next chapter, 
</p>
<p>we will introduce the bootstrap for getting standard errors and confidence 
</p>
<p>intervals. 
</p>
<p>7.15 Example (Plasma Cholesterol). Figure 7.2 shows histograms for plasma 
</p>
<p>cholesterol (in mg/dl) for 371 patients with chest pain (Scott et al. (1978)). 
</p>
<p>The histograms show the percentage of patients in 10 bins. The first histogram 
</p>
<p>is for 51 patients who had no evidence of heart disease while the second 
</p>
<p>histogram is for 320 patients who had narrowing of the arteries. Is the mean 
</p>
<p>cholesterol different in the two groups? Let us regard these data as samples 
</p>
<p>from two distributions Fl and F2. Let J.Ll = I xdFl (x) and J.L2 = I xdF2(x) 
denote the means of the two populations. The plug-in estimates are iiI = 
I xdFn,l(X) = Xn,l = 195.27 and li2 = I xdFn,2(X) = X n,2 = 216.19. Recall 
that the standard error of the sample mean Ii = ~ 2.:~=1 Xi is 
</p>
<p>which we estimate by 
</p>
<p>~ (~) (5 
se J.L = yin 
</p>
<p>where 
</p>
<p>(5= </p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Statistical Functionals 103 
</p>
<p>For the two groups this yields se(lld = 5.0 and se(1l2) = 2.4. Approximate 95 
</p>
<p>percent confidence intervals for ILl and IL2 are III &plusmn; 2se(lld = (185,205) and 
112 &plusmn; 2se(1l2) = (211,221). 
</p>
<p>Now, consider the functional e = T(F2) - T(Fd whose plug-in estimate is 
e = 112 - iiI = 216.19 - 195.27 = 20.92. The standard error of e is 
</p>
<p>and we estimate this by 
</p>
<p>An approximate 95 percent confidence interval for e is e&plusmn;2 se(en ) = (9.8,32.0). 
This suggests that cholesterol is higher among those with narrowed arteries. 
</p>
<p>We should not jump to the conclusion (from these data) that cholesterol causes 
</p>
<p>heart disease. The leap from statistical evidence to causation is very subtle 
</p>
<p>and is discussed in Chapter 16 .&bull; 
</p>
<p>100 150 200 250 300 350 400 
</p>
<p>plasma cholesterol for patients without heart disease 
</p>
<p>100 150 200 250 300 350 400 
</p>
<p>plasma cholesterol for patients with heart disease 
</p>
<p>FIGURE 7.2. Plasma cholesterol for 51 patients with no heart disease and 320 
</p>
<p>patients with narrowing of the arteries. </p>
<p/>
</div>
<div class="page"><p/>
<p>104 7. Estimating the COF and Statistical Functionals 
</p>
<p>7.3 Bibliographic Rernarks 
</p>
<p>The Glivenko-Cantelli theorem is the tip of the iceberg. The theory of dis-
</p>
<p>tribution functions is a special case of what are called empirical processes 
</p>
<p>which underlie much of modern statistical theory. Some references on empiri-
</p>
<p>cal processes are Shorack and Wellner (1986) and van der Vaart and Wellner 
</p>
<p>(1996). 
</p>
<p>7.4 Exercises 
</p>
<p>1. Prove Theorem 7.3. 
</p>
<p>2. Let Xl"'" Xn rv Bernoulli(p) and let YI , ... , Ym rv Bernoulli(q). Find 
</p>
<p>the plug-in estimator and estimated standard error for p. Find an ap-
</p>
<p>proximate 90 percent confidence interval for p. Find the plug-in esti-
</p>
<p>mator and estimated standard error for p - q. Find an approximate 90 
</p>
<p>percent confidence interval for p - q. 
</p>
<p>3. (Computer Experiment.) Generate 100 observations from a N(O,l) dis-
</p>
<p>tribution. Compute a 95 percent confidence band for the CDF F (as 
</p>
<p>described in the appendix). Repeat this 1000 times and see how often 
</p>
<p>the confidence band contains the true distribution function. Repeat us-
</p>
<p>ing data from a Cauchy distribution. 
</p>
<p>4. Let X I, ... , Xn rv F and let Fn (x) be the empirical distribution func-
</p>
<p>tion. For a fixed x, use the central limit theorem to find the limiting 
</p>
<p>distribution of Fn(x). 
</p>
<p>5. Let x and y be two distinct points. Find Cov(Fn(x), Fn(Y)). 
</p>
<p>6. Let Xl, ... ,Xn rv F and let F be the empirical distribution function. 
</p>
<p>Let a &lt; b be fixed numbers and define B = T(F) = F(b) - F(a). Let 
</p>
<p>e = T(Fn) = Fn(b) - Fn(a). Find the estimated standard error of e. 
Find an expression for an approximate 1 - a confidence interval for B. 
</p>
<p>7. Data on the magnitudes of earthquakes near Fiji are available on the 
</p>
<p>website for this book. Estimate the CDF F(x). Compute and plot a 95 
</p>
<p>percent confidence envelope for F (as described in the appendix). Find 
</p>
<p>an approximate 95 percent confidence interval for F(4.9) - F(4.3). </p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Exercises 105 
</p>
<p>8. Get the data on eruption times and waiting times between eruptions of 
</p>
<p>the Old Faithful geyser from the website. Estimate the mean waiting 
</p>
<p>time and give a standard error for the estimate. Also, give a 90 percent 
</p>
<p>confidence interval for the mean waiting time. Now estimate the median 
</p>
<p>waiting time. In the next chapter we will see how to get the standard 
</p>
<p>error for the median. 
</p>
<p>9. 100 people are given a standard antibiotic to treat an infection and 
</p>
<p>another 100 are given a new antibiotic. In the first group, 90 people 
</p>
<p>recover; in the second group, 85 people recover. Let PI be the probability 
</p>
<p>of recovery under the standard treatment and let P2 be the probability of 
</p>
<p>recovery under the new treatment. We are interested in estimating B = 
</p>
<p>PI - P2&middot; Provide an estimate, standard error, an 80 percent confidence 
</p>
<p>interval, and a 95 percent confidence interval for B. 
</p>
<p>10. In 1975, an experiment was conducted to see if cloud seeding produced 
</p>
<p>rainfall. 26 clouds were seeded with silver nitrate and 26 were not. The 
</p>
<p>decision to seed or not was made at random. Get the data from 
</p>
<p>http://lib.stat.cmu.edu/DASL/Stories/CloudSeeding.html 
</p>
<p>Let B be the difference in the mean precipitation from the two groups. 
</p>
<p>Estimate B. Estimate the standard error of the estimate and produce a 
</p>
<p>95 percent confidence interval. </p>
<p/>
</div>
<div class="page"><p/>
<p>8 
</p>
<p>The Bootstrap 
</p>
<p>The bootstrap is a method for estimating standard errors and computing 
</p>
<p>confidence intervals. Let Tn = g(Xl' .. . , Xn) be a statistic, that is, Tn is any 
</p>
<p>function of the data. Suppose we want to know V F(Tn ), the variance of Tn. 
</p>
<p>We have written V F to emphasize that the variance usually depends on the 
</p>
<p>unknown distribution function F. For example, if Tn = Xn then VF(Tn) = 
</p>
<p>(J2 In where (J2 = J(x - p,)2dF(x) and p, = J xdF(x). Thus the variance of Tn 
is a function of F. The bootstrap idea has two steps: 
</p>
<p>Step 2: Approximate V Fn (Tn) using simulation. 
</p>
<p>For Tn = X n , we have for Step 1 that VF~ (Tn) = 22 In where 22 = n- 1 2.:n=1(Xi -
'/I. 2 
</p>
<p>Xn). In this case, Step 1 is enough. However, in more complicated cases we 
</p>
<p>cannot write down a simple formula for V Fn (Tn) which is why we need Step 
</p>
<p>2. Before proceeding, let us discuss the idea of simulation. </p>
<p/>
</div>
<div class="page"><p/>
<p>108 8. The Bootstrap 
</p>
<p>8.1 Simulation 
</p>
<p>Suppose we draw an lID sample Y1 , ... , YB from a distribution G. By the law 
</p>
<p>of large numbers, 
</p>
<p>1 B j' 
Y n = B LYj~ ydG(y) =lE(Y) 
</p>
<p>j=l 
</p>
<p>as B ---+ 00. So if we draw a large sample from G, we can use the sample 
</p>
<p>mean Y n to approximate lE(Y). In a simulation, we can make B as large as 
</p>
<p>we like, in which case, the difference between Y nand lE(Y) is negligible. More 
</p>
<p>generally, if h is any function with finite mean then 
</p>
<p>1 B J B L h(Yj)~ h(y)dG(y) = lE(h(Y)) 
j=l 
</p>
<p>as B ---+ 00. In particular, 
</p>
<p>B 
1", -2 
B L...,(Yj - Y) 
</p>
<p>j=l 
</p>
<p>p 
</p>
<p>---+ 
</p>
<p>Hence, we can use the sample variance of the simulated values to approximate 
</p>
<p>V(Y). 
</p>
<p>8.2 Bootstrap Variance Estimation 
</p>
<p>According to what we just learned, we can approximate V F" (Tn) by simula-
</p>
<p>tio~. Now V F" (Tn) means "the variance of Tn if the distribution of the data 
</p>
<p>is Fn." How can we simulate from the distribution of Tn when the data are 
</p>
<p>assumed to have distribution Fn? The answer is to simulate Xl, ... ,X~ from 
</p>
<p>in and then compute T~ = g(XI , ... , X~). This constitutes one draw from 
the distribution of Tn. The idea is illustrated in the following diagram: 
</p>
<p>Real world F ===} Xl"'" Xn ===} Tn = g(X l , . .. , Xn) 
</p>
<p>Bootstrap world Fn ===} Xl,"" X~ ===} T;; = g(XI , ... , X~) 
</p>
<p>How do we simulate Xl, ... ,X~ from in? Notice that in puts mass l/n at 
</p>
<p>each data point Xl"'" X n . Therefore, </p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Bootstrap Variance Estimation 109 
</p>
<p>drawing an observation from Fn is equivalent to drawing 
</p>
<p>one point at random from the original data set. 
</p>
<p>Thus, to simulate Xl, ... ,X~ rv Fn it suffices to draw n observations with 
</p>
<p>replacement from Xl, ... , X n . Here is a summary: 
</p>
<p>Bootstrap Variance Estimation 
</p>
<p>1. Draw Xl,'" ,X~ rv Fn. 
</p>
<p>2. Compute T~ = g(XI , ... , X~). 
</p>
<p>3. Repeat steps 1 and 2, B times, to get T~,l"'" T~,B' 
</p>
<p>4. Let 
</p>
<p>1 B ( 1 B )2 
Vboot = B L T~,b - B LT~,r 
</p>
<p>b=l r=l 
</p>
<p>(8.1) 
</p>
<p>8.1 Example. The following pseudocode shows how to use the bootstrap to 
</p>
<p>estimate the standard error of the median. 
</p>
<p>Bootstrap for The Median 
</p>
<p>Given data X = (X(i), ... , X(n)): 
</p>
<p>T &lt;- median(X) 
</p>
<p>Tboot &lt;- vector of length B 
</p>
<p>for(i in i :B){ 
</p>
<p>Xstar &lt;- sample of size n from X (with replacement) 
</p>
<p>Tboot[i] &lt;- median(Xstar) 
</p>
<p>} 
</p>
<p>se &lt;- sqrt(variance(Tboot)) 
</p>
<p>The following schematic diagram will remind you that we are using two 
</p>
<p>approximations: 
</p>
<p>not so small small 
r-"-, r-"-, 
</p>
<p>::::0 'liFo (Tn) ::::0 Vboot&middot; 
</p>
<p>8.2 Example. Consider the nerve data. Let () = T(F) = J(x-I1)3dF(x)/a3 be 
</p>
<p>the skewness. The skewness is a measure of asymmetry. A Normal distribution, </p>
<p/>
</div>
<div class="page"><p/>
<p>110 8. The Bootstrap 
</p>
<p>for example, has skewness O. The plug-in estimate of the skewness is 
</p>
<p>To estimate the standard error with the bootstrap we follow the same steps 
</p>
<p>as with the median example except we compute the skewness from each 
</p>
<p>bootstrap sample. When applied to the nerve data, the bootstrap, based on 
</p>
<p>B = 1,000 replications, yields a standard error for the estimated skewness of 
</p>
<p>.16 .&bull; 
</p>
<p>8.3 Bootstrap Confidence Intervals 
</p>
<p>There are several ways to construct bootstrap confidence intervals. Here we 
</p>
<p>discuss three methods. 
</p>
<p>Method 1: The Normal Interval. The simplest method is the Normal interval 
</p>
<p>Tn &plusmn; Za/2 5eboot (8.2) 
</p>
<p>where 5eboot = JVboot is the bootstrap estimate of the standard error. This 
</p>
<p>interval is not accurate unless the distribution of Tn is close to Normal. 
</p>
<p>Method 2: Pivotal Intervals. Let e = T(F) and Bn = T(Fn) and define the 
---- ---- -- ........... 
</p>
<p>pivot Rn = en - e. Let e~,l' ... ,e~,B denote bootstrap replications of en. Let 
</p>
<p>H(r) denote the CDF of the pivot: 
</p>
<p>Define C~ = (a, b) where 
</p>
<p>~ -1 ( a) a = en - H 1-"2 
</p>
<p>It follows that 
</p>
<p>lP'(a:::;e:::;b) lP'(a - en :::; e - en :::; b - en) 
</p>
<p>lP'(en - b :::; en - e :::; en - a) 
</p>
<p>lP'(en - b :::; Rn :::; en - a) 
</p>
<p>H(Bn - a) - H(en - b) 
</p>
<p>H (H- 1 (1 - ~)) - H (H- 1 (~)) 
a a 
</p>
<p>1- - - - = 1- a. 
2 2 
</p>
<p>(8.3) 
</p>
<p>(8.4) </p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Bootstrap Confidence Intervals 111 
</p>
<p>Hence, C~ is an exact 1 - a confidence interval for e. Unfortunately, a and b 
depend on the unknown distribution H but we can form a bootstrap estimate 
</p>
<p>of H: 
1 B 
</p>
<p>H(r) = B L I(R~,b S r) (8.5) 
b=l 
</p>
<p>where R~,b = e~,b -en. Let rp denote the f3 sample quantile of (R~,l"'" R~,B) 
and let ep denote the f3 sample quantile of (e~,l"'" e~,B)' Note that rp = 
ep - en. It follows that an approximate 1- a confidence interval is Cn = (0:, b) 
where 
</p>
<p>~ - 1 ( a) en - H- 1- '2 
</p>
<p>b e -H -- ~-1 (a) 
n 2 
</p>
<p>= en - r~-a/2 = 2en - e~-a/2 
</p>
<p>= en - r~/2 = 2en - e~/2' 
</p>
<p>In summary, the 1 - a bootstrap pivotal confidence interval is 
</p>
<p>8.3 Theorem. Under weak conditions on T(F), 
</p>
<p>as n --+ 00, where Cn is given in (8.6). 
</p>
<p>(8.6) 
</p>
<p>Method 3: Percentile Intervals. The bootstrap percentile interval is de-
</p>
<p>fined by 
</p>
<p>The justification for this interval is given in the appendix. 
</p>
<p>8.4 Example. For estimating the skewness of the nerve data, here are the 
</p>
<p>various confidence intervals. 
</p>
<p>Method 
</p>
<p>Normal 
</p>
<p>Pivotal 
</p>
<p>Percentile 
</p>
<p>95% Interval 
</p>
<p>(1.44, 2.09) 
</p>
<p>(1.48,2.11) 
</p>
<p>(1.42, 2.03) 
</p>
<p>All these confidence intervals are approximate. The probability that T(F) 
</p>
<p>is in the interval is not exactly 1 - a. All three intervals have the same level 
</p>
<p>of accuracy. There are more accurate bootstrap confidence intervals but they 
</p>
<p>are more complicated and we will not discuss them here. </p>
<p/>
</div>
<div class="page"><p/>
<p>112 8. The Bootstrap 
</p>
<p>8.5 Example (The Plasma Cholesterol Data). Let us return to the cholesterol 
</p>
<p>data. Suppose we are interested in the difference of the medians. Pseudocode 
</p>
<p>for the bootstrap analysis is as follows: 
</p>
<p>xl &lt;- first sample 
</p>
<p>x2 &lt;- second sample 
</p>
<p>nl &lt;- length(xl) 
</p>
<p>n2 &lt;- length(x2) 
</p>
<p>th.hat &lt;- median(x2) - median(xl) 
</p>
<p>B &lt;- 1000 
</p>
<p>Tboot &lt;- vector of length B 
</p>
<p>for(i in l:B){ 
</p>
<p>xxl &lt;- sample of size nl with replacement from xl 
</p>
<p>xx2 &lt;- sample of size n2 with replacement from x2 
</p>
<p>Tboot[i] &lt;- median(xx2) - median(xxl) 
</p>
<p>} 
</p>
<p>se &lt;- sqrt(variance(Tboot&raquo; 
</p>
<p>Normal &lt;- (th.hat - 2*se, th.hat + 2*se) 
</p>
<p>percentile &lt;- (quantile(Tboot,.025), quantile(Tboot,.975&raquo; 
</p>
<p>pivotal &lt;- ( 2*th.hat-quantile(Tboot,.975), 
</p>
<p>2*th.hat-quantile(Tboot, .025) 
</p>
<p>The point estimate is 18.5, the bootstrap standard error is 7.42 and the re-
</p>
<p>sulting approximate 95 percent confidence intervals are as follows: 
</p>
<p>Method 95% Interval 
</p>
<p>Normal (3.7, 33.3) 
</p>
<p>Pivotal (5.0, 34.0) 
</p>
<p>Percentile (5.0, 33.3) 
</p>
<p>Since these intervals exclude 0, it appears that the second group has higher 
</p>
<p>cholesterol although there is considerable uncertainty about how much higher 
</p>
<p>as reflected in the width of the intervals. _ 
</p>
<p>The next two examples are based on small sample sizes. In practice, sta-
</p>
<p>tistical methods based on very small sample sizes might not be reliable. We 
</p>
<p>include the examples for their pedagogical value but we do want to sound a 
</p>
<p>note of caution about interpreting the results with some skepticism. 
</p>
<p>8.6 Example. Here is an example that was one of the first used to illustrate 
</p>
<p>the bootstrap by Bradley Efron, the inventor of the bootstrap. The data are 
</p>
<p>LSAT scores (for entrance to law school) and GPA. </p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Bootstrap Confidence Intervals 113 
</p>
<p>LSAT 576 635 558 578 666 580 555 661 
</p>
<p>651 605 653 575 545 572 594 
</p>
<p>GPA 3.39 3.30 2.81 3.03 3.44 3.07 3.00 3.43 
</p>
<p>3.36 3.13 3.12 2.74 2.76 2.88 3.96 
</p>
<p>Each data point is of the form Xi = (Yi, Zi) where Yi = LSATi and Zi = 
</p>
<p>G P Ai. The law school is interested in the correlation 
</p>
<p>B = J J(y - fLY)(Z -ILz)dF(y,z) 
</p>
<p>V J(y - fLy)2dF(y) J(z - fLz )2dF(z) 
The plug-in estimate is the sample correlation 
</p>
<p>B = Li(Yi - Y)(Zi - Z) 
</p>
<p>VLi(Yi - y)2 Li(Zi - Z)2 
</p>
<p>The estimated correlation is B = .776. The bootstrap based on B = 1000 
</p>
<p>gives se = .137. Figure 8.1 shows the data and a histogram of the bootstrap 
~ ~ 
</p>
<p>replications B1, ... , BB. This histogram is an approximation to the sampling 
distribution of B. The Normal-based 95 percent confidence interval is .78 &plusmn; 
</p>
<p>2se = (.51,1.00) while the percentile interval is (.46,.96). In large samples, the 
</p>
<p>two methods will show closer agreement. _ 
</p>
<p>8.7 Example. This example is from Efron and Tibshirani (1993). When drug 
</p>
<p>companies introduce new medications, they are sometimes required to show 
</p>
<p>bioequivalence. This means that the new drug is not substantially different 
</p>
<p>than the current treatment. Here are data on eight subjects who used medi-
</p>
<p>cal patches to infuse a hormone into the blood. Each subject received three 
</p>
<p>treatments: placebo, old-patch, new-patch. 
</p>
<p>subject placebo old new old - placebo new - old 
</p>
<p>1 9243 17649 16449 8406 -1200 
</p>
<p>2 9671 12013 14614 2342 2601 
</p>
<p>3 11792 19979 17274 8187 -2705 
</p>
<p>4 13357 21816 23798 8459 1982 
</p>
<p>5 9055 13850 12560 4795 -1290 
</p>
<p>6 6290 9806 10157 3516 351 
</p>
<p>7 12412 17208 16570 4796 -638 
</p>
<p>8 18806 29044 26325 10238 -2719 </p>
<p/>
</div>
<div class="page"><p/>
<p>114 8. The Bootstrap 
</p>
<p>..,. 
M 
</p>
<p>~ 
&laquo; 
0.. 
(!l 
</p>
<p>&lt;:&gt; 
M 
</p>
<p>00 
</p>
<p>N 
</p>
<p>560 580 600 620 640 660 
</p>
<p>LSAT 
</p>
<p>&lt;:&gt; 
</p>
<p>0 .2 0 .4 0 .6 0 .8 1 .0 
</p>
<p>Bootstrap Samples 
</p>
<p>FIGURE 8.1. Law school data. The top panel shows the raw data. The bottom panel 
</p>
<p>is a histogram of the correlations computed from each bootstrap sample. 
</p>
<p>Let Z = old - placebo and Y = new - old. The Food and Drug Adminis-
</p>
<p>tration (FDA) requirement for bioequivalence is that 181 ::; .20 where 
</p>
<p>The plug-in estimate of 8 is 
</p>
<p>- Y -452.3 
8 = = = -- = -0.0713. 
</p>
<p>Z 6342 
</p>
<p>The bootstrap standard error is se = 0.105. To answer the bioequivalence 
question, we compute a confidence interval. From B = 1000 bootstrap repli-
</p>
<p>cations we get the 95 percent interval (-0.24,0.15). This is not quite contained </p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Bibliographic Remarks 115 
</p>
<p>in (-0.20,0.20) so at the 95 percent level we have not demonstrated bioequiv-
</p>
<p>alence. Figure 8.2 shows the histogram of the bootstrap values. _ 
</p>
<p>-0,3 &middot;0.2 -0 .1 0 ,0 0 ,1 0 .2 
</p>
<p>Bootstrap Samples 
</p>
<p>FIGURE 8.2. Patch data. 
</p>
<p>8.4 Bibliographic Remarks 
</p>
<p>The bootstrap was invented by Efron (1979). There are several books on these 
</p>
<p>topics including Efron and Tibshirani (1993), Davison and Hinkley (1997), 
</p>
<p>Hall (1992) and Shao and Tu (1995). Also, see section 3.6 of van der Vaart 
</p>
<p>and Wellner (1996). 
</p>
<p>8.5 Appendix 
</p>
<p>8.5.1 The Jackknife 
</p>
<p>There is another method for computing standard errors called the jackknife, 
</p>
<p>due to Quenouille (1949). It is less computationally expensive than the boot-</p>
<p/>
</div>
<div class="page"><p/>
<p>116 8. The Bootstrap 
</p>
<p>strap but is less general. Let Tn = T(X1' ... ,Xn) be a statistic and T( -i) de-
</p>
<p>note the statistic with the ith observation removed. Let Tn = n- 1 L~=l T(-i)' 
The jackknife estimate of var(Tn) is 
</p>
<p>n-1~ - 2 
Vjack = --L)T(-i) -Tn) 
</p>
<p>n 
i=l 
</p>
<p>and the jackknife estimate of the standard error is 5ejack = ylVjack. Under 
</p>
<p>suitable conditions on T, it can be shown that Vjack consistently estimates 
</p>
<p>var(Tn) in the sense that Vjack/var(Tn)~ 1. However, unlike the bootstrap, 
</p>
<p>the jackknife does not produce consistent estimates of the standard error of 
</p>
<p>sample quantiles. 
</p>
<p>8.5.2 Justification For The Percentile Interval 
</p>
<p>Suppose there exists a monotone transformation U = m(T) such that U rv 
</p>
<p>N(&cent;, c2 ) where &cent; = m(B). We do not suppose we know the transformation, 
</p>
<p>only that one exists. Let U; = m( B~,b)' Let u~ be the f3 sample quantile of 
the U;'s. Since a monotone transformation preserves quantiles, we have that 
</p>
<p>U~/2 = m( B~/2)' Also, since U rv N( &cent;, c2), the Q/2 quantile of U is &cent; - Za/2C. 
</p>
<p>Hence u~/2 = &cent; - Za/2C. Similarly, ui-a/2 = &cent; + Za/2C, Therefore, 
</p>
<p>lP'(m(B~/2) &lt;::: m(B) &lt;::: m(Bi-a/2)) 
</p>
<p>IP'(U~/2 &lt;::: &cent; &lt;::: Ui-a/2) 
</p>
<p>IP'(U - CZa/2 &lt;::: &cent; &lt;::: U + czad 
U-&cent; 
</p>
<p>IP'( -Za/2 &lt;::: -- &lt;::: Za/2) 
C 
</p>
<p>1- Q. 
</p>
<p>An exact normalizing transformation will rarely exist but there may exist 
</p>
<p>approximate normalizing transformations. 
</p>
<p>8.6 Exercises 
</p>
<p>1. Consider the data in Example 8.6. Find the plug-in estimate of the 
</p>
<p>correlation coefficient. Estimate the standard error using the bootstrap. 
</p>
<p>Find a 95 percent confidence interval using the Normal, pivotal, and 
</p>
<p>percentile methods. </p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 Exercises 117 
</p>
<p>2. (Computer Experiment.) Conduct a simulation to compare the various 
</p>
<p>bootstrap confidence interval methods. Let n = 50 and let T(F) = 
</p>
<p>J(x - J.L)3dF(x)/(J"3 be the skewness. Draw YI , ... , Yn rv N(O,l) and 
</p>
<p>set Xi = eY ;, i = 1, ... , n. Construct the three types of bootstrap 95 
</p>
<p>percent intervals for T(F) from the data Xl"'" X n . Repeat this whole 
</p>
<p>thing many times and estimate the true coverage of the three intervals. 
</p>
<p>3. Let 
</p>
<p>where n = 25. Let B = T(F) = (q.75 - q.25)/l.34 where qp denotes the 
</p>
<p>pth quantile. Do a simulation to compare the coverage and length of the 
</p>
<p>following confidence intervals for B: (i) Normal interval with standard 
</p>
<p>error from the bootstrap, (ii) bootstrap percentile interval, and (iii) 
</p>
<p>pivotal bootstrap interval. 
</p>
<p>4. Let Xl"'" Xn be distinct observations (no ties). Show that there are 
</p>
<p>distinct bootstrap samples. 
</p>
<p>Hint: Imagine putting n balls into n buckets. 
</p>
<p>5. Let Xl, ... , Xn be distinct observations (no ties). Let Xl"'" X~ denote 
-* -1 n . -* 
</p>
<p>a bootstrap sample and let Xn = n Li=l X;:' Fmd: IE(XnIXI , ... , X n ), 
</p>
<p>V(X~IXI"'" X n ), IE(X~) and V(X~). 
</p>
<p>6. (Computer Experiment.) Let Xl, "',Xn Normal(/L, 1). Let B = eI' and let 
</p>
<p>e = eX. Create a data set (using J.L = 5) consisting of n=100 observa-
tions. 
</p>
<p>(a) Use the bootstrap to get the se and 95 percent confidence interval 
</p>
<p>for B. 
</p>
<p>(b) Plot a histogram of the bootstrap replications. This is an estimate 
</p>
<p>of the distribution of B. Compare this to the true sampling distribution 
</p>
<p>of B. 
</p>
<p>7. Let Xl, ... , Xn rv Uniform(O, B). Let e = Xmax = max{XI' ... , X n }. Gen-
erate a data set of size 50 with B = l. 
</p>
<p>(a) Find the distribution of e. Compare the true distribution of e to the 
histograms from the bootstrap. </p>
<p/>
</div>
<div class="page"><p/>
<p>118 8. The Bootstrap 
</p>
<p>(b) This is a case where the bootstrap does very poorly. In fact, we can 
</p>
<p>prove that this is the case. Show that pee = e) = 0 and yet P(B* = 
e) ~ .632. Hint: show that, p(e* = e) = 1 - (1 - (l/n))n then take the 
</p>
<p>limit as n gets large. 
</p>
<p>8. Let Tn = X~, f.L = JE(Xd, OCk = I IX-f.LlkdF(x) and ak = n-1 L:~=l IXi -
Xnl k . Show that </p>
<p/>
</div>
<div class="page"><p/>
<p>9 
</p>
<p>Parametric Inference 
</p>
<p>We now turn our attention to parametric models, that is, models of the form 
</p>
<p>.;y = {f(x; 8): 8 E 8 } (9.1) 
</p>
<p>where the 8 c ~k is the parameter space and 8 = (81 , ... , 8k ) is the param-
</p>
<p>eter. The problem of inference then reduces to the problem of estimating the 
</p>
<p>parameter 8. 
</p>
<p>Students learning statistics often ask: how would we ever know that the 
</p>
<p>distribution that generated the data is in some parametric model? This is 
</p>
<p>an excellent question. Indeed, we would rarely have such knowledge which 
</p>
<p>is why non parametric methods are preferable. Still, studying methods for 
</p>
<p>parametric models is useful for two reasons. First, there are some cases where 
</p>
<p>background knowledge suggests that a parametric model provides a reasonable 
</p>
<p>approximation. For example, counts of traffic accidents are known from prior 
</p>
<p>experience to follow approximately a Poisson model. Second, the inferential 
</p>
<p>concepts for parametric models provide background for understanding certain 
</p>
<p>nonparametric methods. 
</p>
<p>We begin with a brief discussion about parameters of interest and nuisance 
</p>
<p>parameters in the next section, then we will discuss two methods for estimat-
</p>
<p>ing 8, the method of moments and the method of maximum likelihood. </p>
<p/>
</div>
<div class="page"><p/>
<p>120 9. Parametric Inference 
</p>
<p>9.1 Pararneter of Interest 
</p>
<p>Often, we are only interested in some function T(8). For example, if X rv 
</p>
<p>N(IL, 0"2) then the parameter is 8 = (IL,O")' If our goal is to estimate IL then 
</p>
<p>IL = T(8) is called the parameter of interest and 0" is called a nuisance 
</p>
<p>parameter. The parameter of interest might be a complicated function of 8 
</p>
<p>as in the following example. 
</p>
<p>9.1 Example. Let Xl"'" Xn rv Normal(IL, 0"2). The parameter is 8 = (IL,O") 
</p>
<p>and the parameter space is e = {(IL, 0"): IL E Jl{, 0" &gt; O}. Suppose that Xi is 
the outcome of a blood test and suppose we are interested in T, the fraction 
</p>
<p>of the population whose test score is larger than 1. Let Z denote a standard 
</p>
<p>Normal random variable. Then 
</p>
<p>T J1D(X&gt; 1) = 1 _ J1D(X &lt; 1) = 1 _ JID (X; It &lt; 1: IL) 
</p>
<p>1-JID(Z&lt; l:IL) =l-&lt;I&gt;C:IL). 
</p>
<p>The parameter of interest is T = T(IL, 0") = 1 - &lt;I&gt; ( (1 - IL) /0"). _ 
</p>
<p>9.2 Example. Recall that X has a Gamma(a, p) distribution if 
</p>
<p>where a, p &gt; 0 and 
</p>
<p>is the Gamma function. The parameter is 8 = (a, p). The Gamma distri-
</p>
<p>bution is sometimes used to model lifetimes of people, animals, and elec-
</p>
<p>tronic equipment. Suppose we want to estimate the mean lifetime. Then 
</p>
<p>T(a, p) = JEg(Xd = ap. _ 
</p>
<p>9.2 The Method of Moments 
</p>
<p>The first method for generating parametric estimators that we will study 
</p>
<p>is called the method of moments. We will see that these estimators are not 
</p>
<p>optimal but they are often easy to compute. They are are also useful as starting 
</p>
<p>values for other methods that require iterative numerical routines. </p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 The Method of Moments 121 
</p>
<p>Suppose that the parameter () = (()l, ... , ()k) has k components. For 1 &lt; 
j ::.; k, define the jth moment 
</p>
<p>and the jth sample moment 
</p>
<p>~ _ 1 ~ j 
O:j - - ~Xi' 
</p>
<p>n 
i=1 
</p>
<p>(9.2) 
</p>
<p>(9.3) 
</p>
<p>9.3 Definition. The method of moments estimator ()n is defined to be 
</p>
<p>the value of () such that 
</p>
<p>0:1 (en) 0:1 
</p>
<p>0:2(en ) 0:2 
</p>
<p>(9.4) 
</p>
<p>Formula (9.4) defines a system of k equations with k unknowns. 
</p>
<p>9.4 Example. Let Xl"",Xn rv Bernoulli(p). Then 0:1 = JEp(X) = p and 
</p>
<p>a1 = n-1 L~=1 Xi' By equating these we get the estimator 
</p>
<p>9.5 Example. Let Xl"'" Xn rv Normal(IL, (J2). Then, 0:1 = JEg(XI) = IL 
</p>
<p>and 0:2 = JEg(Xf) = Vg(Xl ) + (JEg(XI)j2 = (J2 + fL2. We need to solve the 
equations1 
</p>
<p>1 n 
-LXi 
n 
</p>
<p>i=1 
</p>
<p>1 n 
-LX;. 
n 
</p>
<p>i=1 
</p>
<p>This is a system of 2 equations with 2 unknowns. The solution is 
</p>
<p>1 Recall that VeX) = JE(X2) - (JE(X))2 Hence, JE(X2) = VeX) + (JE(X))2 </p>
<p/>
</div>
<div class="page"><p/>
<p>122 9. Parametric Inference 
</p>
<p>2 1 ~ - 2 
(j - L.)Xi - Xn). &bull; 
</p>
<p>n 
i=l 
</p>
<p>9.6 Theorem. Let en denote the method of moments estimator. Under appro-
</p>
<p>priate conditions on the model, the following statements hold: 
</p>
<p>1. The estimate en exists with probability tending to 1. 
</p>
<p>2. The estimate is consistent: en ~ e. 
</p>
<p>3. The estimate is asymptotically Normal: 
</p>
<p>where 
</p>
<p>~ = gJEg(yyT)gT, 
</p>
<p>Y = (X,x 2 , ... ,Xk)T, g = (gl,'" ,gk) and gj = 8c&pound;/(e)/8e. 
</p>
<p>The last statement in the theorem above can be used to find standard errors 
</p>
<p>and confidence intervals. However, there is an easier way: the bootstrap. We 
</p>
<p>defer discussion of this until the end of the chapter. 
</p>
<p>9.3 Maxirnurn Likelihood 
</p>
<p>The most common method for estimating parameters in a parametric model is 
</p>
<p>the maximum likelihood method. Let Xl, ... , Xn be IID with PDF f(x; e). 
</p>
<p>9.7 Definition. The likelihood function is defined by 
</p>
<p>n 
</p>
<p>Ln(e) = II f(Xi ; e). (9.5) 
i=l 
</p>
<p>The log-likelihood function is defined by fl.n (e) = log Ln (e). 
</p>
<p>The likelihood function is just the joint density of the data, except that we 
</p>
<p>treat it is a function of the parameter e. Thus, Ln : 8 --+ [0, (0). The 
likelihood function is not a density function: in general, it is not true that 
</p>
<p>Ln (e) integrates to 1 (with respect to e). 
</p>
<p>9.8 Definition. The maximum likelihood estimator MLE, denoted by 
</p>
<p>en, is the value of e that maximizes Ln(e). </p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Maximum Likelihood 123 
</p>
<p>p 
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0 
</p>
<p>FIGURE 9.1. Likelihood function for Bernoulli with n = 20 and ~~=1 Xi = 12. The 
</p>
<p>MLE is Pn = 12/20 = 0.6. 
</p>
<p>The maximum of &pound;n (e) occurs at the same place as the maximum of Ln (e), 
</p>
<p>so maximizing the log-likelihood leads to the same answer as maximizing the 
</p>
<p>likelihood. Often, it is easier to work with the log-likelihood. 
</p>
<p>9.9 Remark. If we multiply Ln(e) by any positive constant c (not depending 
</p>
<p>on e) then this will not change the MLE. Hence, we shall often drop constants 
</p>
<p>in the likelihood function. 
</p>
<p>9.10 Example. Suppose that Xl' ... ' Xn rv Bernoulli(p). The probability func-
</p>
<p>tion is f(x; p) = pX (1- p)l-x for x = 0,1. The unknown parameter is p. Then, 
</p>
<p>n n 
</p>
<p>i=l i=l 
</p>
<p>where S = Li Xi. Hence, 
</p>
<p>&pound;n(P) = Slogp+ (n - S)log(l- p). 
</p>
<p>Take the derivative of &pound;n(P), set it equal to 0 to find that the MLE is fin = Sjn. 
</p>
<p>See Figure 9.1. &bull; 
</p>
<p>9.11 Example. Let Xl, ... ,Xn rv N(j1,(}2). The parameter is e = (j1,(}) and 
the likelihood function (ignoring some constants) is: </p>
<p/>
</div>
<div class="page"><p/>
<p>124 9. Parametric Inference 
</p>
<p>where X = n-l2::iXi is the sample mean and S2 = n- l 2::i(Xi - X)2. The 
</p>
<p>last equality above follows from the fact that 2:: i (Xi -It? = nS2 + n(X -It)2 
which can be verified by writing 2::i(Xi - fL)2 = 2::i(Xi - X + X - fL)2 and 
then expanding the square. The log-likelihood is 
</p>
<p>nS2 n(X -It? 
Ji.(fL, a) = -n log a - -2a-2 - ----'--::-2-a""'2--'--
</p>
<p>Solving the equations 
</p>
<p>we conclude that Ii = X and (j = S. It can be verified that these are indeed 
global maxima of the likelihood. _ 
</p>
<p>9.12 Example (A Hard Example). Here is an example that many people find 
</p>
<p>confusing. Let XI"",Xn rv Uni!(O,B). Recall that 
</p>
<p>!(x' B) = {l/B 0::'; x ~ B 
, 0 otherWIse. 
</p>
<p>Consider a fixed value of B. Suppose B &lt; Xi for some i. Then, !(Xi; B) = 0 
</p>
<p>and hence Ln(B) = IIi !(Xi; B) = O. It follows that Ln(B) = 0 if any Xi &gt; B. 
Therefore, Ln(B) = 0 if B &lt; X(n) where X(n) = max{XI"'" X n}. Now 
consider any B ~ XCn)' For every Xi we then have that !(Xi; B) = l/B so that 
</p>
<p>Ln(B) = IIi !(Xi; B) = B-n. In conclusion, 
</p>
<p>See Fig~re 9.2. Now Ln(B) is strictly decreasing over the interval [XCn)' (0). 
</p>
<p>Hence, Bn = XCn)' -
</p>
<p>The maximum likelihood estimators for the multivariate Normal and the 
</p>
<p>multinomial can be found in Theorems 14.5 and 14.3. 
</p>
<p>9.4 Properties of Maxirnurn Likelihood Estirnators 
</p>
<p>Under certain conditions on the model, the maximum likelihood estimator Bn 
</p>
<p>possesses many properties that make it an appealing choice of estimator. The 
</p>
<p>main properties of the MLE are: </p>
<p/>
</div>
<div class="page"><p/>
<p>o 
.,; 
</p>
<p>'" .,; 
</p>
<p>0 
.,; 
</p>
<p>9.4 Properties of Maximum Likelihood Estimators 125 
</p>
<p>I I I III 
</p>
<p>" I III 
</p>
<p>" 
</p>
<p>0.0 0.5 1.0 
</p>
<p>e = .75 
</p>
<p>'R ?m ? m 
" 
" '" 
" '" 
</p>
<p>" 
</p>
<p>0.0 0.5 1.0 
</p>
<p>() = 1.25 
</p>
<p>&lt;t;' 
</p>
<p>H 
'-"" 
"-, 
</p>
<p>1.5 2.0 
</p>
<p>x 
</p>
<p>~ 
~ 
</p>
<p>t..l 
</p>
<p>1.5 2.0 
</p>
<p>x 
</p>
<p>"l 
</p>
<p>co 
</p>
<p>on 
.,; 
</p>
<p>0 
.,; 
</p>
<p>"l 
</p>
<p>0 
</p>
<p>on 
d 
</p>
<p>0 
.,; 
</p>
<p>" 
" 
" 
</p>
<p>0.0 
</p>
<p>0.6 
</p>
<p>0 .5 
</p>
<p>e = 1 
</p>
<p>0.8 
</p>
<p>'" 
</p>
<p>'" 
</p>
<p>1.0 
</p>
<p>1.0 
</p>
<p>1.5 2 .0 
</p>
<p>x 
</p>
<p>1.2 I .' 
</p>
<p>e 
</p>
<p>FIGURE 9.2. Likelihood function for Uniform (0, e). The vertical lines show the 
observed data. The first three plots show f(x; e) for three different values of e. 
When e &lt; XCn) = max{X1 , ... , X n}, as in the first plot, f(XCn); e) = &deg; and 
hence Ln(e) = rr~=l f(Xi; e) = 0. Otherwise f(Xi; e) = lie for each i and hence 
Ln(e) = rr~=l f(Xi; e) = (l/e)n. The last plot shows the likelihood function. </p>
<p/>
</div>
<div class="page"><p/>
<p>126 9. Parametric Inference 
</p>
<p>1. The MLE is consistent: en ~ 8* where 8* denotes the true value of the 
parameter 8; 
</p>
<p>2. The MLE is equivariant: if en is the MLE of 8 then g(en) is the MLE of 
g( 8); 
</p>
<p>3. The MLE is asymptotically Normal: (e - 8*)/se ~ N(O, 1); also, the 
estimated standard error Se can often be computed analytically; 
</p>
<p>4. The MLE is asymptotically optimal or efficient: roughly, this means 
</p>
<p>that among all well-behaved estimators, the MLE has the smallest vari-
</p>
<p>ance, at least for large samples; 
</p>
<p>5. The MLE is approximately the Bayes estimator. (This point will be ex-
</p>
<p>plained later.) 
</p>
<p>We will spend some time explaining what these properties mean and why 
</p>
<p>they are good things. In sufficiently complicated problems, these properties 
</p>
<p>will no longer hold and the MLE will no longer be a good estimator. For now 
</p>
<p>we focus on the simpler situations where the MLE works well. The properties 
</p>
<p>we discuss only hold if the model satisfies certain regularity conditions. 
</p>
<p>These are essentially smoothness conditions on f(x; 8). Unless otherwise 
</p>
<p>stated, we shall tacitly assume that these conditions hold. 
</p>
<p>9.5 Consistency of Maxirnurn Likelihood Estirnators 
</p>
<p>Consistency means that the MLE converges in probability to the true value. 
</p>
<p>To proceed, we need a definition. If f and g are PDF'S, define the Kullback-
Leibler distance 2 between f and g to be 
</p>
<p>D(j,g) = J f(x) log (~~:D dx. (9.6) 
It can be shown that D(j, g) ~ 0 and D(j,1) = O. For any 8, VJ E 8 write 
</p>
<p>D( 8, 1/;) to mean D(j(x; 8), f( x; 1/;)). 
</p>
<p>We will say that the model ~ is identifiable if 8 # 1/; implies that D(8, 1/;) &gt; 
O. This means that different values of the parameter correspond to different 
</p>
<p>distributions. We will assume from now on the the model is identifiable. 
</p>
<p>2This is not a distance in the formal sense because D(f, g) is not symmetric. </p>
<p/>
</div>
<div class="page"><p/>
<p>9.6 Equivariance of the MLE 127 
</p>
<p>Let e* denote the true value of e. Maximizing t;n(e) is equivalent to maxi-
</p>
<p>mizing 
</p>
<p>This follows since Mn(e) = n- 1 (t;n(e) -t;n(e*)) and t;n(e*) is a constant (with 
</p>
<p>respect to e). By the law of large numbers, lvIn (e) converges to 
</p>
<p>J (f(X; e)) . log f(x; e*) j(x; e*)dx 
J (f(X; e*)) - log f(x; e) f(x; e*)dx 
</p>
<p>-D(e*, e). 
</p>
<p>Hence, Mn(e) ;:::; -D(e*, e) which is maximized at e* since -D(e*, e*) = 0 
</p>
<p>and - D( e*, e) &lt; 0 for e i=- e*. Therefore, we expect that the maximizer will 
</p>
<p>tend to e*. To prove this formally, we need more than .l'vln(e)~ - D(e*, e). 
</p>
<p>We need this convergence to be uniform over e. We also have to make sure 
that the function D( e*, e) is well behaved. Here are the formal details. 
</p>
<p>9.13 Theorem. Let e* denote the true value of e. Define 
</p>
<p>and M(e) = -D(e*, e). Suppose that 
</p>
<p>(9.7) 
</p>
<p>and that, for every E &gt; 0, 
</p>
<p>sup M(e) &lt; M(e*). (9.8) 
II:III-II.I::&gt;E 
</p>
<p>~ p 
Let en denote the MLE. Then en ---+ e*. 
</p>
<p>The proof is in the appendix. 
</p>
<p>9.6 Equivariance of the MLE 
</p>
<p>9.14 Theorem. Let T = g(e) be a function of e. Let en be the MLE of e. Then 
</p>
<p>Tn = g(en) is the MLE of T. </p>
<p/>
</div>
<div class="page"><p/>
<p>128 9. Parametric Inference 
</p>
<p>PROOF. Let h = g-l denote the inverse of g. Then en = h(Tn). For any T, 
</p>
<p>&pound;(T) = TId(Xi; h(T)) = TId(Xi; 8) = &pound;(8) where 8 = h(T). Hence, for any T, 
</p>
<p>&pound;n(T) = &pound;(8) s: &pound;(e) = &pound;n(T) .&bull; 
</p>
<p>9.15 Example. Let Xl, ... , Xn rv N(8,1). The MLEfor 8 is 8n = X n. Let 
e Th h f . - e X T = e. en, t e MLE or T IS T = e = e .&bull; 
</p>
<p>9.7 Asymptotic Normality 
</p>
<p>It turns out that the distribution of 8n is approximately Normal and we can 
</p>
<p>compute its approximate variance analytically. To explore this, we first need 
</p>
<p>a few definitions. 
</p>
<p>9.16 Definition. The score function is defined to be 
</p>
<p>(X , 8) = 8log f(X; 8) 
s, a8' (9.9) 
</p>
<p>The Fisher information is defined to be 
</p>
<p>n 
</p>
<p>L V e(s(Xi;8)). (9.10) 
i=l 
</p>
<p>For n = 1 we will sometimes write 1(8) instead of h (8). It can be shown 
</p>
<p>that lEe(s(X; 8)) = O. It then follows that Ve(s(X; 8)) = lEe(s2(X; 8)). In fact, 
</p>
<p>a further simplification of In (8) is given in the next result. 
</p>
<p>9.17 Theorem. In(8) = nI(8). Also, 
</p>
<p>1(8) = -lEe ( a2lo~;;X; 8) ) 
</p>
<p>-J ( a2lo~:2(X; 8) ) f(x; 8)dx. (9.11) </p>
<p/>
</div>
<div class="page"><p/>
<p>9.7 Asymptotic Normality 129 
</p>
<p>9.18 Theorem (Asymptotic Normality of the MLE). Let se = VV(en ). 
Under appropriate regularity conditions, the following hold: 
</p>
<p>(en - 8) ~ N(O, 1). 
se 
</p>
<p>(9.12) 
</p>
<p>2. Let se = V1/In(en). Then, 
</p>
<p>(en:: 8) ~ N(O, 1). 
se 
</p>
<p>(9.13) 
</p>
<p>The proof is in the appendix. The first statement says that en ~ N (8, se) 
where the approximate standard error of en is se = J1/In(8). The second 
statement says that this is still true even if we replace the standard error by 
</p>
<p>its estimated standard error se = V1/In(en). 
Informally, the theorem says that the distribution of the MLE can be ap-
</p>
<p>proximated with N(8,se2 ). From this fact we can construct an (asymptotic) 
</p>
<p>confidence interval. 
</p>
<p>9.19 Theorem. Let 
</p>
<p>en = (en - Za/2 se, en + Za/2 se ). 
</p>
<p>Then, lP'e(8 E en) ---+ 1- Q as n ---+ 00. 
</p>
<p>PROOF. Let Z denote a standard normal random variable. Then, 
</p>
<p>lP'e(8 E en) lP'e (en - Za/2 se ::.; 8 ::.; en + Za/2 se) 
</p>
<p>lP'e ( -Za/2 ::.; 8n ~ 8 ::.; Za/2 ) 
---+ IP'( -Za/2 &lt; Z &lt; Za/2) = 1 - Q. _ 
</p>
<p>For Q = .05, Za/2 = 1.96 ~ 2, so: 
</p>
<p>is an approximate 95 percent confidence interval. 
</p>
<p>(9.14) </p>
<p/>
</div>
<div class="page"><p/>
<p>130 9. Parametric Inference 
</p>
<p>When you read an opinion poll in the newspaper, you often see a statement 
</p>
<p>like: the poll is accurate to within one point, 95 percent of the time. They are 
</p>
<p>simply giving a 95 percent confidence interval of the form en &plusmn; 2 se. 
</p>
<p>9.20 Example. Let Xl, ... , Xn rv Bernoulli(p). The lVILE is Pn = Li Xdn 
and J(x;p) = pX(1- p)l-x, log J(x;p) = x logp + (1 - x) 10g(1 - p), 
</p>
<p>X I-X 
8(X;p) = - - --, 
</p>
<p>p I-p 
</p>
<p>and 
X I-X 
</p>
<p>-s'(X;p) = p2 + (1- p)2' 
</p>
<p>Thus, 
, p (1 - p) 1 
</p>
<p>I(p) = lEp ( -8 (X;p)) = p2 + (1 _ p)2 = p(1 _ p) 
</p>
<p>Hence, 
</p>
<p>1 = {P(1 _ p) }1/2 
JnI(Pn) n 
</p>
<p>1 
se = ----=== 
</p>
<p>J1n(Pn) 
</p>
<p>An approximate 95 percent confidence interval is 
</p>
<p>{ ~ (1 ~ )}1/2 Pn &plusmn; 2 Pn ;: Pn -
9.21 Example. Let Xl, ... , Xn rv N(e, cr 2 ) where cr2 is known. The score 
</p>
<p>function is 8(X; e) = (X - e)/cr2 and 8'(X; e) = -1/cr2 so that h(e) = l/cr2. 
</p>
<p>The lVILE is en = X n. According to Theorem 9.18, Xn ~ N(e,cr2/n). In this 
</p>
<p>case, the Normal approximation is actually exact. _ 
</p>
<p>9.22 Example. Let Xl"'" Xn rv Poisson(A). Then Xn = Xn and some cal-
</p>
<p>culations show that h (A) = 1/ A, so 
</p>
<p>se = 1 fi 
VnI(Xn) = -;;:. 
</p>
<p>Therefore, an approximate 1- ex confidence interval for A is Xn &plusmn; Za/2VXn/n. 
</p>
<p>-
9.8 Optirnality 
</p>
<p>Suppose that Xl"'" Xn rv N(e, cr2). The lVILE is en = X n. Another reason-
</p>
<p>able estimator of e is the sample median en. The lVILE satisfies </p>
<p/>
</div>
<div class="page"><p/>
<p>9.9 The Delta Method 131 
</p>
<p>It can be proved that the median satisfies 
</p>
<p>This means that the median converges to the right value but has a larger 
</p>
<p>variance than the MLE. 
</p>
<p>More generally, consider two estimators Tn and Un and suppose that 
</p>
<p>and that 
</p>
<p>We define the asymptotic relative efficiency of U to T by ARE(U, T) = t 2/u2. 
In the Normal example, ARE(en, en) = 2/7r = .63. The interpretation is that 
</p>
<p>if you use the median, you are effectively using only a fraction of the data. 
</p>
<p>9.23 Theorem. If en is the MLE and en is any other estimator then 3 
</p>
<p>Thus, the MLE has the smallest (asymptotic) variance and we say that the 
</p>
<p>MLE is efficient or asymptotically optimal. 
</p>
<p>This result is predicated upon the assumed model being correct. If the model 
</p>
<p>is wrong, the MLE may no longer be optimal. We will discuss optimality in 
</p>
<p>more generality when we discuss decision theory in Chapter 12. 
</p>
<p>9.9 The Delta Method 
</p>
<p>Let T = g( e) where g is a smooth function. The maximum likelihood esti-
</p>
<p>mator of T is T = g(e). Now we address the following question: what is the 
</p>
<p>distribution of T? 
</p>
<p>9.24 Theorem (The Delta Method). If T = g( e) where g is differentiable 
</p>
<p>and g'(e) -=I- 0 then 
</p>
<p>(9.15) 
</p>
<p>3The result is actually more subtle than this but the details are too complicated to consider 
</p>
<p>here. </p>
<p/>
</div>
<div class="page"><p/>
<p>132 9. Parametric Inference 
</p>
<p>Hence, if 
</p>
<p>Cn = (Tn - Za/2 seWn), Tn + Za/2 seWn)) 
</p>
<p>then lP'e(T E Cn) --+ 1 - a as n --+ 00. 
</p>
<p>(9.16) 
</p>
<p>(9.17) 
</p>
<p>9.25 Example. Let Xl, ... , Xn rv Bernoulli(p) and let 1jJ = g(p) = 10g(p/(I-
</p>
<p>p)). The Fisher information function is J(p) = 1/(p(1 - p)) so the estimated 
</p>
<p>standard error of the !viLE Pn is 
</p>
<p>The !viLE of 1jJ is :j; = log P / (1 - p). Since, g' (p) = 1/ (p( 1 - p)), according to 
the delta method 
</p>
<p>An approximate 95 percent confidence interval is 
</p>
<p>9.26 Example. Let Xl"'" Xn rv N(/L, er2). Suppose that fL is known, er is 
</p>
<p>unknown and that we want to estimate '1jJ = loger. The log-likelihood is &pound;(er) = 
</p>
<p>-n log er - 2!2 Li (Xi - fL)2. Differentiate and set equal to 0 and conclude that 
</p>
<p>To get the standard error we need the Fisher information. First, 
</p>
<p>(X fL)2 
log f (X; er) = - log er - -'----2er----=-2-'----
</p>
<p>with second derivative 
</p>
<p>and hence </p>
<p/>
</div>
<div class="page"><p/>
<p>9.10 Multiparameter Models 133 
</p>
<p>Therefore, se = &amp;n/J2n. Let .1jJ = g(u) = logu. Then,~n = logan. Since 
g' = l/u, 
</p>
<p>~ ~ 1 &amp;n 1 
se(~n) = =- roL = roL' 
</p>
<p>Un V 2n V 2n 
</p>
<p>and an approximate 95 percent confidence interval is ~ &plusmn; 2/ J2n .&bull; 
</p>
<p>9.10 Multiparameter Models 
</p>
<p>These ideas can directly be extended to models with several parameters. Let 
</p>
<p>e = (e1 , ... , ek) and let e = (e1 , ... , ek) be the MLE. Let fin = L~=11og f(Xi; e), 
</p>
<p>Define the Fisher Information Matrix by 
</p>
<p>r 
lEe(H11) lEe(H12) 
lEe(H2d lEe (H22) 
</p>
<p>In(e)=- . . . . . . 
lEe (Hk1 ) lEe (Hk2) 
</p>
<p>Let In(e) = I;;l(e) be the inverse of In. 
</p>
<p>lEe(Hlk) 1 
lEe (H2k ) . 
</p>
<p>lEe (Hkk ) 
</p>
<p>9.27 Theorem. Under appropriate regularity conditions, 
</p>
<p>Also, if ej is the jth component of e, then 
</p>
<p>(9.18) 
</p>
<p>(9.19) 
</p>
<p>where se; = In(j,j) is the /h diagonal element of In- The approximate co-
variance of ej and ek is Cov(ej , ed :::::: I n (j, k). 
</p>
<p>There is also a multiparameter delta method. Let T = g(e1 , ... , ek ) be a 
</p>
<p>function and let 
</p>
<p>"ilg= 
</p>
<p>be the gradient of g. 
</p>
<p>8g 
</p>
<p>8e1 
</p>
<p>8g 
</p>
<p>8ek </p>
<p/>
</div>
<div class="page"><p/>
<p>134 9. Parametric Inference 
</p>
<p>9.28 Theorem (Multiparameter delta method). Suppose that '\1g evaluated at 
</p>
<p>e is not O. Let l' = g(1i). Then 
(T - T) 
</p>
<p>~(~) ~ N(O, 1) 
se T 
</p>
<p>where 
</p>
<p>se(1') = V(Vg)T in(Vg), (9.20) 
</p>
<p>in = In(en) and Vg is '\1g evaluated at e = e. 
</p>
<p>9.29 Example. Let Xl, ... , Xn rv N(fJ, (j2). Let T = g(fJ, (j) = (j / fJ. In Excer-
</p>
<p>cise 8 you will show that 
</p>
<p>Hence, 
</p>
<p>The gradient of 9 is 
</p>
<p>'\1g= 
</p>
<p>Thus, 
</p>
<p>2~ ] . 
,,2 
</p>
<p>( (Y) 
- 1,2 
</p>
<p>~ . 
</p>
<p>1 a2 
Ji4 + 2Ji2' 
</p>
<p>9.11 The Parametric Bootstrap 
</p>
<p>&bull; 
</p>
<p>For parametric models, standard errors and confidence intervals may also be 
</p>
<p>estimated using the bootstrap. There is only one change. In the nonparametric 
</p>
<p>bootstrap, we sampled Xr, ... ,X~ from the empirical distribution Fn. In the 
parametric bootstrap we sample instead from f(x; en). Here, en could be the 
</p>
<p>MLE or the method of moments estimator. 
</p>
<p>9.30 Example. Consider example 9.29. To get the bootstrap standard er-
</p>
<p>ror, simulate XI""'X~ rv N(Ji,a2), compute Ji* = n-1LiX;* and a2* = 
n- l Li (X;* - Ji*)2. Then compute 1'* = g(Ji*, a*) = a* /Ji*. Repeating this B 
</p>
<p>times yields bootstrap replications </p>
<p/>
</div>
<div class="page"><p/>
<p>9.12 Checking Assumptions 135 
</p>
<p>and the estimated standard error is 
</p>
<p>seboot = &bull; 
</p>
<p>The bootstrap is much easier than the delta method. On the other hand, 
</p>
<p>the delta method has the advantage that it gives a closed form expression for 
</p>
<p>the standard error. 
</p>
<p>9.12 Checking Assurnptions 
</p>
<p>If we assume the data come from a parametric model, then it is a good idea to 
</p>
<p>check that assumption. One possibility is to check the assumptions informally 
</p>
<p>by inspecting plots of the data. For example, if a histogram of the data looks 
</p>
<p>very bimodal, then the assumption of Normality might be questionable. A 
</p>
<p>formal way to test a parametric model is to use a goodness-of-fit test. See 
</p>
<p>Section 10.S. 
</p>
<p>9.13 Appendix 
</p>
<p>9.13.1 Proofs 
</p>
<p>PROOF OF THEOREM 9.13. Since en maximizes Mn(e), we have Mn(en) ::;, 
</p>
<p>l'vln (e*). Hence, 
</p>
<p>M(e*) - M(en) Mn(e*) - M(en) + M(e*) - Mn(e*) 
</p>
<p>&lt; Mn(en) - M(en) + M(e*) - Mn(e*) 
</p>
<p>&lt; sup IMn(e) - M(e)1 + M(e*) - Mn(e*) 
e 
</p>
<p>~o 
</p>
<p>where the last line follows from (9.7). It follows that, for any 6 &gt; 0, 
</p>
<p>Pick any E &gt; O. By (9.S), there exists 6 &gt; 0 such that Ie - e*1 ::;, E implies that 
M(e) &lt; M(e*) - 6. Hence, 
</p>
<p>1F'(len - e*1 &gt; E) &lt;::: IF' (M(en) &lt; M(e*) - 6) --+ O .&bull; 
</p>
<p>Next we want to prove Theorem 9.1S. First we need a lemma. </p>
<p/>
</div>
<div class="page"><p/>
<p>136 9. Parametric Inference 
</p>
<p>9.31 lemma. The score function satisfies 
</p>
<p>lEe [s(X; 8)] = O. 
</p>
<p>PROOF. Note that 1 = I f(x; 8)dx. Differentiate both sides of this equation 
to conclude that 
</p>
<p>o = :8 J f(x; 8)dx = J :8f(x; 8)dx 
J 8j(x;e) J 81 f'( 8) f(~~ 8) f(x; 8)dx = og a8 X; f(x; 8)dx 
J s(x; 8)f(x; 8)dx = lEes(X; 8). &bull; 
</p>
<p>PROOF OF THEOREM 9.18. Let &pound;.(8) = log&pound;(8). Then, 
</p>
<p>0= tee) ~ t(8) + (e - 8)&pound;."(8). 
</p>
<p>Rearrange the above equation to get e - 8 = -&pound;.'(8)/&pound;."(8) or, in other words, 
~ .fiJ(8) TOP 
</p>
<p>vn(8 - 8) = -~&pound;."(8) = BOTTOM' 
</p>
<p>Let Yi = a log f (Xi; 8) / a8. Recall that IE(Yi) = 0 from the previous lemma 
</p>
<p>and also V(Yi) = 1(8). Hence, 
</p>
<p>by the central limit theorem. Let Ai = -a2 log f(Xi; 8)/a82 . Then IE(Ai) = 
1(8) and 
</p>
<p>BOTTOM = A~1(8) 
</p>
<p>by the law of large numbers. Apply Theorem 5.5 part (e), to conclude that 
</p>
<p>~ W d ( 1) 
vn(8 - 8) ~ 1(8) = N 0, 1(8) . 
</p>
<p>Assuming that 1(8) is a continuous function of 8, it follows that I (en) ~ I (8). 
</p>
<p>Now 
</p>
<p>8n - 8 
</p>
<p>se </p>
<p/>
</div>
<div class="page"><p/>
<p>9.13 Appendix 137 
</p>
<p>The first term tends in distribution to N(O,l). The second term tends in 
</p>
<p>probability to 1. The result follows from Theorem 5.5 part (e). _ 
</p>
<p>OUTLINE OF PROOF OF THEOREM 9.24. Write 
</p>
<p>Thus, 
</p>
<p>and hence 
</p>
<p>fol(ij(Tn - T) ~ Cjffl\(8)(e _ 8) 
g' (8) ~ V n1 \ U) n . 
</p>
<p>Theorem 9.18 tells us that the right-hand side tends in distribution to a N(O,l). 
</p>
<p>Hence, 
</p>
<p>or, in other words, 
</p>
<p>where 
</p>
<p>fol(ij(Tn - T) () 
g'(8) ~ !V 0,1 
</p>
<p>2(~ ) _ (g'(8))2 
se Tn - nI(8) 
</p>
<p>The result remains true if we substitute en for 8 by Theorem 5.5 part (e). _ 
</p>
<p>9.13.2 Sufficiency 
</p>
<p>A statistic is a function T(xn) of the data. A sufficient statistic is a statistic 
</p>
<p>that contains all the information in the data. To make this more formal, we 
</p>
<p>need some definitions. 
</p>
<p>9.32 Definition. Write xn ++ yn if f(xn; 8) = cf(yn; 8) for some constant 
</p>
<p>c that might depend on xn and yn but not 8. A statistic T(xn) is 
</p>
<p>sufficient ifT(xn) ++ T(yn) implies that xn ++ yn. 
</p>
<p>Notice that if xn ++ yn, then the likelihood function based on xn has the 
</p>
<p>same shape as the likelihood function based on yn. Roughly speaking, a statis-
</p>
<p>tic is sufficient if we can calculate the likelihood function knowing only T(xn). 
</p>
<p>9.33 Example. Let Xl, ... , Xn rv Bernoulli(p). Then &pound;(p) = pS (1 _ p)n-S 
</p>
<p>where S = L:i Xi, so S is sufficient. -</p>
<p/>
</div>
<div class="page"><p/>
<p>138 9. Parametric Inference 
</p>
<p>9.34 Example. Let Xl"'" Xn rv N(/L, 0-) and let T = (X, S). Then 
</p>
<p>f(xn; IL, 0-) = (o-~) n exp { _ ~~:} exp { _ n(~~/)2} 
</p>
<p>where S2 is the sample variance. The last expression depends on the data 
</p>
<p>only through T and therefore, T = (X, S) is a sufficient statistic. Note that 
</p>
<p>U = (17 X, S) is also a sufficient statistic. If I tell you the value of U then you 
</p>
<p>can easily figure out T and then compute the likelihood. Sufficient statistics 
</p>
<p>are far from unique. Consider the following statistics for the N(/L, 0- 2 ) model: 
</p>
<p>TI(xn) (Xl,'" ,Xn ) 
</p>
<p>T2(xn) (X,S) 
</p>
<p>T3(xn) X 
</p>
<p>T4(xn) (X,S,X3). 
</p>
<p>The first statistic is just the whole data set. This is sufficient. The second 
</p>
<p>is also sufficient as we proved above. The third is not sufficient: you can't 
</p>
<p>compute &pound;(/L, 0-) if I only tell you X. The fourth statistic T4 is sufficient. The 
</p>
<p>statistics TI and T4 are sufficient but they contain redundant information. 
</p>
<p>Intuitively, there is a sense in which T2 is a "more concise" sufficient statistic 
</p>
<p>than either TI or T4. We can express this formally by noting that T2 is a 
</p>
<p>function of TI and similarly, T2 is a function of T4. For example, T2 = g(T4) 
</p>
<p>where g(al' a2, a3) = (aI, a2) .&bull; 
</p>
<p>9.35 Definition. A statistic T is minimal sufficient if (i) it is 
</p>
<p>sufficient; and (ii) it is a function of every other sufficient statistic. 
</p>
<p>9.36 Theorem. T is minimal sufficient if the following is true: 
</p>
<p>A statistic induces a partition on the set of outcomes. We can think of 
</p>
<p>sufficiency in terms of these partitions. 
</p>
<p>9.37 Example. Let X I ,X2 rv Bernoulli(8). Let V = Xl, T 
</p>
<p>U = (T, Xd. Here is the set of outcomes and the statistics: 
</p>
<p>o 0 
o 1 
1 0 
</p>
<p>1 1 
</p>
<p>V T 
</p>
<p>o 0 
o 1 
1 1 
</p>
<p>1 2 
</p>
<p>U 
</p>
<p>(0,0) 
</p>
<p>(1,0) 
</p>
<p>(1,1) 
</p>
<p>(2,1) </p>
<p/>
</div>
<div class="page"><p/>
<p>9.13 Appendix 139 
</p>
<p>The partitions induced by these statistics are: 
</p>
<p>v ---+ {(O,O), (O,ln, {(I,O),(I,ln 
</p>
<p>T ---+ {(O, On, {(O, 1), (1, on, {(I, In 
</p>
<p>u ---+ {(O, on, {(O, In, {(I, on, {(I, In. 
</p>
<p>Then V is not sufficient but T and U are sufficient. T is minimal sufficient; 
</p>
<p>U is not minimal since if xn = (1,0) and yn = (0,1), then xn B yn yet 
</p>
<p>U(xn) cI u(yn). The statistic W = 17T generates the same partition as T. It 
is also minimal sufficient. _ 
</p>
<p>9.38 Example. For a N(/-L, (J2) model, T = (X, S) is a minimal sufficient 
</p>
<p>statistic. For the Bernoulli model, T = Li Xi is a minimal sufficient statistic. 
</p>
<p>For the Poisson model, T = Li Xi is a minimal sufficient statistic. Check that 
</p>
<p>T = (Li Xi, Xd is sufficient but not minimal sufficient. Check that T = Xl 
</p>
<p>is not sufficient. _ 
</p>
<p>I did not give the usual definition of sufficiency. The usual definition is this: 
</p>
<p>T is sufficient if the distribution of xn given T(xn) = t does not depend on 
</p>
<p>B. In other words, T is sufficient if f(Xl,"" xnlt; B) = h(Xl,"" Xn, t) where 
</p>
<p>h is some function that does not depend on B. 
</p>
<p>9.39 Example. Two coin flips. Let X = (Xl, X 2 ) rv Bernoulli(p). Then T = 
</p>
<p>Xl + X 2 is sufficient. To see this, we need the distribution of (Xl, X 2 ) given 
T = t. Since T can take 3 possible values, there are 3 conditional distributions 
</p>
<p>to check. They are: (i) the distribution of (Xl, X 2 ) given T = 0: 
</p>
<p>P(XI = 0, X 2 = 0lt = 0) = 1, P(XI = 0, X 2 = lit = 0) = 0, 
</p>
<p>P(XI = 1, X 2 = 0lt = 0) = 0, P(XI = 1, X 2 = lit = 0) = 0; 
</p>
<p>(ii) the distribution of (Xl ,X2 ) given T = 1: 
</p>
<p>1 
P(XI = 0,X2 = 0lt = 1) = 0, P(XI = 0,X2 = lit = 1) = 2' 
</p>
<p>1 
P(XI = I,X2 = 0lt = 1) = 2,P(XI = I,X2 = lit = 1) = 0; and 
</p>
<p>(iii) the distribution of (Xl, X 2 ) given T = 2: 
</p>
<p>P(XI = 0, X 2 = 0lt = 2) = 0, P(XI = 0, X 2 = lit = 2) = 0, 
</p>
<p>P(XI = 1, X 2 = 0lt = 2) = 0, P(XI = 1, X 2 = lit = 2) = 1. 
</p>
<p>None of these depend on the parameter p. Thus, the distribution of Xl, X 2 1T 
does not depend on B, so T is sufficient. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>140 9. Parametric Inference 
</p>
<p>9.40 Theorem (Factorization Theorem). T is sufficient if and only if there are 
</p>
<p>functions g(t, 8) and h(x) such that f(xn; 8) = g(t(xn), 8)h(xn). 
</p>
<p>9.41 Example. Return to the two coin flips. Let t = Xl + X2. Then 
</p>
<p>f(XI; 8)f(X2; 8) 
</p>
<p>8X1(1 _ 8)I-X18X2(1 _ 8)I-X2 
</p>
<p>g(t,8)h(XI,X2) 
</p>
<p>) t 2-t ) where g(t,8 = 8 (1 - 8) and h(XI, X2 = l. Therefore, T = Xl + X 2 is 
sufficient. _ 
</p>
<p>Now we discuss an implication of sufficiency in point estimation. Let 8 be 
</p>
<p>an estimator of 8. The Rao-Blackwell theorem says that an estimator should 
</p>
<p>only depend on the sufficient statistic, otherwise it can be improved. Let 
</p>
<p>R( 8, 8) = lEe (8 - e) 2 denote the !VISE of the estimator. 
</p>
<p>9.42 Theorem (Rao-Blackwell). Let e be an estimator and let T be a sufficient 
statistic. Define a new estimator by 
</p>
<p>8 = lE(eIT). 
</p>
<p>Then, for every 8, R(8,8) :S R(8, B). 
</p>
<p>9.43 Example. Consider flipping a coin twice. Let 8 = Xl. This is a well-
</p>
<p>defined (and unbiased) estimator. But it is not a function of the sufficient 
</p>
<p>statistic T = Xl + X 2 . However, note that (j = lE(XIIT) = (Xl + X 2 )/2. By 
the Rao-Blackwell Theorem, 8 has MSE at least as small as 8 = Xl. The 
</p>
<p>same applies with n coin flips. Again define 8 = Xl and T = Li Xi. Then 
(j = lE(XIIT) = n- l LiXi has improved !VISE. _ 
</p>
<p>9.13.3 Exponential Families 
</p>
<p>Most of the parametric models we have studied so far are special cases of 
</p>
<p>a general class of models called exponential families. We say that {f(x; 8) : 
</p>
<p>8 E 8} is a one-parameter exponential family if there are functions T/(8), 
</p>
<p>B(8), T(x) and h(x) such that 
</p>
<p>f(x; 8) = h(x)er)(e)T(x)-B(e). 
</p>
<p>It is easy to see that T(X) is sufficient. We call T the natural sufficient 
</p>
<p>statistic. </p>
<p/>
</div>
<div class="page"><p/>
<p>9.13 Appendix 141 
</p>
<p>9.44 Example. Let X rv Poisson(e). Then 
</p>
<p>ex -e 1 
f(x; e) = + = ,eX]oge-e 
</p>
<p>x. x. 
</p>
<p>and hence, this is an exponential family with '17 ( e) = log e, B( e) = e, T(x) = x, 
h(x) = l/x!. _ 
</p>
<p>9.45 Example. Let X rv Binomial(n, e). Then 
</p>
<p>f(x; e) = (~)ex(1- e)n-x = (~) exp {x log C ~ e) + nlog(l- e)}. 
In this case, 
</p>
<p>17(e) = log C ~ e) ,B(e) = -nlog(e) 
and 
</p>
<p>T(x) = x, h(x) = (n). 
x 
</p>
<p>-
We can rewrite an exponential family as 
</p>
<p>f(x; 71) = h(x)eT1T(x)-A(r1) 
</p>
<p>where 71 = 71(e) is called the natural parameter and 
</p>
<p>A(I7) = log J h(x)e'7T(xl dx. 
For example a Poisson can be written as f(x; 17) = e'7x- e " Ix! where the natural 
</p>
<p>parameter is '17 = log e. 
Let Xl, ... , Xn be IID from an exponential family. Then f(xn; e) is an 
</p>
<p>exponential family: 
</p>
<p>where hn(xn) = IIi h(Xi), Tn(xn) = L:i T(Xi) and Bn(e) 
implies that L:i T(Xi) is sufficient. 
</p>
<p>9.46 Example. Let Xl,"" Xn rv Uniform(O, e). Then 
</p>
<p>. 1 
j(xn; e) = en J(X(n) &lt;::: e) 
</p>
<p>nB(e). This 
</p>
<p>where J is 1 if the term inside the brackets is true and 0 otherwise, and 
</p>
<p>x(n) = max{xl, ... ,Xn}. Thus T(xn) = max{Xl, ... ,Xn} is sufficient. But 
</p>
<p>since T(xn) cJ L:i T(Xi)' this cannot be an exponential family. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>142 9. Parametric Inference 
</p>
<p>9.47 Theorem. Let X have density in an exponential family. Then, 
</p>
<p>lE(T(X)) = A'(rl), V(T(X)) = A"(f)). 
</p>
<p>If () = (()ll"" ()k) is a vector, then we say that f(x; ()) has exponential 
</p>
<p>family form if 
</p>
<p>f(x; 0) ~ h(x) exp {t ryj(B)Tj(x) - nt~) } . 
Again, T = (Tl , ... , Tk ) is sufficient. An IID sample of size n also has expo-
</p>
<p>nential form with sufficient statistic (Li Tl (Xi)' ... , Li Tk (Xi)). 
</p>
<p>9.48 Example. Consider the normal family with () = (,L,O) Now, 
</p>
<p>This is exponential with 
</p>
<p>Hence, with n lID samples, (Li Xi, Li Xl) is sufficient. -
</p>
<p>As before we can write an exponential family as 
</p>
<p>where A(f)) = log I h(x)eTTex)r]dx. It can be shown that 
</p>
<p>lE(T(X)) = A(f)) V(T(X)) = A(rl), 
</p>
<p>where the first expression is the vector of partial derivatives and the second 
</p>
<p>is the matrix of second derivatives. 
</p>
<p>9.13.4 Computin9 Maximum Likelihood Estimates 
</p>
<p>In some cases we can find the MLE () analytically. More often, we need to 
</p>
<p>find the MLE by numerical methods. We will briefly discuss two commonly </p>
<p/>
</div>
<div class="page"><p/>
<p>9.13 Appendix 143 
</p>
<p>used methods: (i) Newton-Raphson, and (ii) the EM algorithm. Both are 
</p>
<p>iterative methods that produce a sequence of values eo, e1 , ... that, under 
ideal conditions, converge to the MLE e. In each case, it is helpful to use a 
good starting value eO. Often, the method of moments estimator is a good 
starting value. 
</p>
<p>NEWTON-RAPHSON. To motivate Newton-Raphson, let's expand the deriva-
</p>
<p>tive of the log-likelihood around ej : 
</p>
<p>Solving for e gives 
~ . l (e j ) 
e';:::j eJ - r(e j )" 
</p>
<p>This suggests the following iterative scheme: 
</p>
<p>In the multiparameter case, the mle 8 = (81 , ... , 8k ) is a vector and the 
method becomes 
</p>
<p>8H1 = ej - H-1g' (e j ) 
</p>
<p>where l (e j ) is the vector of first derivatives and H is the matrix of second 
derivatives of the log-likelihood. 
</p>
<p>THE EM ALGORITHM. The letters EM stand for Expectation-Maximization. 
</p>
<p>The idea is to iterate between taking an expectation then maximizing. Sup-
</p>
<p>pose we have data Y whose density f(y; e) leads to a log-likelihood that is 
</p>
<p>hard to maximize. But suppose we can find another random variable Z such 
</p>
<p>that f(y; e) = I f(y, z; e) dz and such that the likelihood based on f(y, z; e) 
is easy to maximize. In other words, the model of interest is the marginal of a 
</p>
<p>model with a simpler likelihood. In this case, we call Y the observed data and 
</p>
<p>Z the hidden (or latent or missing) data. If we could just "fill in" the missing 
</p>
<p>data, we would have an easy problem. Conceptually, the EM algorithm works 
</p>
<p>by filling in the missing data, maximizing the log-likelihood, and iterating. 
</p>
<p>9.49 Example (Mixture of Normals). Sometimes it is reasonable to assume that 
</p>
<p>the distribution of the data is a mixture of two normals. Think of heights of 
</p>
<p>people being a mixture of men and women's heights. Let &cent;(y; IL, (J) denote 
</p>
<p>a normal density with mean IL and standard deviation (J. The density of a 
</p>
<p>mixture of two Normals is </p>
<p/>
</div>
<div class="page"><p/>
<p>144 9. Parametric Inference 
</p>
<p>The idea is that an observation is drawn from the first normal with probability 
</p>
<p>p and the second with probability I-p. However, we don't know which Normal 
</p>
<p>it was drawn from. The parameters are 8 = (fLo,uo,fLl,Ul,P). The likelihood 
</p>
<p>function is 
</p>
<p>n 
</p>
<p>&pound;(8) = II [(1 - P)&cent;(Yi; fLo, uo) + P&cent;(Yi; ILl, udl&middot; 
i=l 
</p>
<p>Maximizing this function over the five parameters is hard. Imaging that we 
</p>
<p>were given extra information telling us which of the two normals every observa-
</p>
<p>tion came from. These "complete" data are of the form (Y1 , Zd,&middot;&middot;&middot;, (Yn, Zn), 
</p>
<p>where Zi = 0 represents the first normal and Zi = 1 represents the second. 
</p>
<p>Note that JP'(Zi = 1) = p. We shall soon see that the likelihood for the com-
</p>
<p>plete data (Y1 , Zd, ... , (Yn, Zn) is much simpler than the likelihood for the 
</p>
<p>observed data Y1 , ... , Yn .&bull; 
</p>
<p>Now we describe the EM algorithm. 
</p>
<p>The EM Algorithm 
</p>
<p>(0) Pick a starting value 8&deg;. Now for j = 1,2, ... , repeat steps 1 and 2 
</p>
<p>below: 
</p>
<p>(1) (The E-step): Calculate 
</p>
<p>The expectation is over the missing data zn treating 8i and the observed 
data yn as fixed. 
</p>
<p>(2) Find 81+1 to maximize J(818 j ). 
</p>
<p>We now show that the EM algorithm always increases the likelihood, that 
</p>
<p>is, &pound;(81+1) ~ &pound;(8 j ). Note that 
</p>
<p>and hence </p>
<p/>
</div>
<div class="page"><p/>
<p>9.13 Appendix 145 
</p>
<p>J(eJ+llej ) _ IE .. (log f(znlYn;eJ+l) I yn = yn) 
BJ f(ZnlYn; eJ ) 
</p>
<p>.+1 . 
J(eJ leJ ) + K(iJ, iJ+d 
</p>
<p>where fj = f(yn; ej ) and fJ+l = f(yn; eJ+l) and K(j, g) = I f(x) log(j(x)/g(x)) dx 
is the Kullback-Leibler distance. Now, eJ+l was chosen to maximize J(ele j ). 
</p>
<p>Hence, J( eJ+llej ) 2: J( ej le j ) = O. Also, by the properties of Kullback-Leibler 
</p>
<p>divergence, K(iJ,iJ+d 2: O. Hence, &pound;(eJ+l) 2: &pound;(ej ) as claimed. 
</p>
<p>9.50 Example (Continuation of Example 9.49). Consider again the mixture of 
</p>
<p>two normals but, for simplicity assume that p = 1/2, (}l = (}2 = l. The density 
</p>
<p>is 
1 1 
</p>
<p>f(y; /.Ll, IL2) = 2"&cent;(Y; /.La, 1) + 2"&cent;(Y; ILl, 1). 
</p>
<p>Directly maximizing the likelihood is hard. Introduce latent variables Zl, ... , Zn 
</p>
<p>where Zi = 0 if Yi is from &cent;(y; /.La, 1), and Zi = 1 if Yi is from &cent;(y; /.Ll, 1), 
</p>
<p>IP'(Zi = 1) = P(Zi = 0) = 1/2, f(YilZi = 0) = &cent;(y; /.La, 1) and f(YilZi = 1) = 
</p>
<p>&cent;(y; /.Ll, 1). So f(y) = 2::;=0 f(y, z) where we have dropped the parameters 
from the density to avoid notational overload. We can write 
</p>
<p>f(z, y) = f(z)f(ylz) = ~&cent;(y; /.La, l)l-z&cent;(y; /.Ll, l)z. 
</p>
<p>Hence, the complete likelihood is 
</p>
<p>n 
</p>
<p>i=l 
</p>
<p>The complete log-likelihood is then 
</p>
<p>~ 1 n 1 n 
&pound;. = -2" 2.:(1 - Zi)(Yi - /.La) - 2" 2.: Zi(Yi - /.Ld&middot; 
</p>
<p>i=l i=l 
</p>
<p>And so 
</p>
<p>J(ele j ) = -~ t(l - IE(Zilyn, ej))(Yi - /.La) - ~ t IE(Zilyn, ej))(Yi - ILd&middot; 
i=l i=l 
</p>
<p>Since Zi is binary, IE(Zilyn,e j ) = IP'(Zi = 1Iyn,ej ) and, by Bayes' theorem, 
</p>
<p>f(ynlZi = 1; ej )IP'(Zi = 1) 
IP'(Zi = llyn, ei) = 
</p>
<p>f(ynlZi = 1; ej )IP'(Zi = 1) + f(ynlZi = 0; ej )IP'(Zi = 0) 
</p>
<p>&cent;(Yi; ILL 1)~ 
</p>
<p>&cent;(Yi;/.L{,l)~ +&cent;(Yi;/.Lb,l)~ 
</p>
<p>&cent;(Yi; /.L{, 1) 
</p>
<p>&cent;(Yi; /.L{, 1) + &cent;(Yi; /.Lb, 1) 
T( i). </p>
<p/>
</div>
<div class="page"><p/>
<p>146 9. Parametric Inference 
</p>
<p>Take the derivative of J(ele j ) with respect to /Ll and /L2, set them equal to 0 
to get 
</p>
<p>and 
~Hl L~=l (1 - Ti)Yi 
</p>
<p>/Lo = L~=l (1 - Ti) 
</p>
<p>We then recompute Ti using p,{+l and P,b+ l and iterate. _ 
</p>
<p>9.14 Exercises 
</p>
<p>1. Let Xl, ... ,Xn '" Gamma( ex, (3). Find the method of moments estimator 
</p>
<p>for ex and (3. 
</p>
<p>2. Let Xl, ... , Xn '" Uniform(a, b) where a and b are unknown parameters 
</p>
<p>and a &lt; b. 
</p>
<p>(a) Find the method of moments estimators for a and b. 
</p>
<p>(b) Find the MLE a and b. 
(c) Let T = J xdF(x). Find the MLE of T. 
</p>
<p>(d) Let T be the MLE of T. Let T be the nonparametric plug-in estimator 
</p>
<p>of T = J x dF(x). Suppose that a = 1, b = 3, and n = 10. Find the MSE 
of T by simulation. Find the MSE of T analytically. Compare. 
</p>
<p>3. Let Xl"'" Xn '" N(/L, ()2). Let T be the .95 percentile, i.e. IP'(X &lt; T) = 
.95. 
</p>
<p>(a) Find the MLE of T. 
</p>
<p>(b) Find an expression for an approximate 1 - ex confidence interval for 
</p>
<p>T. 
</p>
<p>(c) Suppose the data are: 
</p>
<p>3.23 -2.50 1. 88 -0.68 4.43 0.17 
</p>
<p>1.03 -0.07 -0.01 0.76 1. 76 3.18 
</p>
<p>0.33 -0.31 0.30 -0.61 1. 52 5.43 
</p>
<p>1. 54 2.28 0.42 2.33 -1. 03 4.00 
</p>
<p>0.39 
</p>
<p>Find the MLE T. Find the standard error using the delta method. Find 
</p>
<p>the standard error using the parametric bootstrap. </p>
<p/>
</div>
<div class="page"><p/>
<p>9.14 Exercises 147 
</p>
<p>4. Let Xl"'" Xn rv Uniform(O, e). Show that the !VILE is consistent. Hint: 
</p>
<p>Let Y = max{XI , ... , X n }. For any c, J1D(Y &lt; c) = J1D(XI &lt; C, X 2 &lt; 
</p>
<p>C, ... , Xn &lt; c) = J1D(XI &lt; C)JID(X2 &lt; c) ... JID(Xn &lt; c). 
</p>
<p>5. Let Xl, ... , Xn rv Poisson(.\). Find the method of moments estimator, 
</p>
<p>the maximum likelihood estimator and the Fisher information 1(,\). 
</p>
<p>6. Let Xl, "',Xn rv N(e, 1). Define 
</p>
<p>{ 
1 if Xi&gt; 0 
</p>
<p>Yi= 0 if Xi ::.; O. 
</p>
<p>Let 7/J = J1D(YI = 1). 
</p>
<p>(a) Find the maximum likelihood estimator :J; of .1jJ. 
</p>
<p>(b) Find an approximate 95 percent confidence interval for 7/J. 
</p>
<p>(c) Define 7/J = (lin) L:i Yi. Show that 0 is a consistent estimator ofljJ. 
(d) Compute the asymptotic relative efficiency of 7/J toJ;. Hint: Use the 
</p>
<p>delta method to get the standard error of the !VILE. Then compute the 
</p>
<p>standard error (i.e. the standard deviation) of 7/J. 
</p>
<p>(e) Suppose that the data are not really normal. Show that 7/J is not 
</p>
<p>consistent. What, if anything, doesljJ converge to? 
</p>
<p>7. (Comparing two treatments.) nl people are given treatment 1 and n2 
</p>
<p>people are given treatment 2. Let Xl be the number of people on treat-
</p>
<p>ment 1 who respond favorably to the treatment and let X 2 be the 
</p>
<p>number of people on treatment 2 who respond favorably. Assume that 
</p>
<p>Xl rv Binomial(nl,pI) X 2 rv Binomial(n2,P2)' LetljJ = PI - P2. 
</p>
<p>(a) Find the !VILE :J; for VJ. 
</p>
<p>(b) Find the Fisher information matrix I(PI,P2). 
</p>
<p>(c) Use the multiparameter delta method to find the asymptotic stan-
</p>
<p>dard error of 7/J. 
</p>
<p>(d) Suppose that nl = n2 = 200, Xl = 160 and X 2 = 148. Find:J;. Find 
</p>
<p>an approximate 90 percent confidence interval for VJ using (i) the delta 
method and (ii) the parametric bootstrap. 
</p>
<p>8. Find the Fisher information matrix for Example 9.29. 
</p>
<p>9. Let Xl, ... , Xn rv Normal(/L, 1). Let e = eM and let (j = eX be the !VILE. 
Create a data set (using fL = 5) consisting of n=100 observations. </p>
<p/>
</div>
<div class="page"><p/>
<p>148 9. Parametric Inference 
</p>
<p>(a) Use the delta method to get se and a 95 percent confidence interval 
for B. Use the parametric bootstrap to get se and 95 percent confidence 
interval for B. Use the nonparametric bootstrap to get se and 95 percent 
confidence interval for B. Compare your answers. 
</p>
<p>(b) Plot a histogram of the bootstrap replications for the parametric 
</p>
<p>and nonparametric bootstraps. These are estimates of the distribution 
</p>
<p>of B. The delta method also gives an approximation to this distribution 
</p>
<p>namely, Normal(e,se2 ). Compare these to the true sampling distribu-
</p>
<p>tion of e (which you can get by simulation). Which approximation -
parametric bootstrap, bootstrap, or delta method - is closer to the true 
</p>
<p>distri bution? 
</p>
<p>10. Let Xl, ... , Xn rv Uniform(O, B). The MLE is e = XCn) = max{Xl' ... , X n }. 
Generate a dataset of size 50 with B = l. 
</p>
<p>(a) Find the distribution of e analytically. Compare the true distribu-
tion of B to the histograms from the parametric and nonparametric 
</p>
<p>bootstraps. 
</p>
<p>(b) This is a case where the nonparametric bootstrap does very poorly. 
</p>
<p>Show that for the parametric bootstrap lP'(e* = e) = 0, but for the 
</p>
<p>non parametric bootstrap lP'(e* = e) RO .632. Hint: show that, lP'(e* = 
</p>
<p>e) = 1 - (1 - (l/n))n then take the limit as n gets large. What is the 
implication of this? </p>
<p/>
</div>
<div class="page"><p/>
<p>10 
</p>
<p>Hypothesis Testing and p-values 
</p>
<p>Suppose we want to know if exposure to asbestos is associated with lung 
</p>
<p>disease. We take some rats and randomly divide them into two groups. We 
</p>
<p>expose one group to asbestos and leave the second group unexposed. Then 
</p>
<p>we compare the disease rate in the two groups. Consider the following two 
</p>
<p>hypotheses: 
</p>
<p>The Null Hypothesis: The disease rate is the same in the two groups. 
</p>
<p>The Alternative Hypothesis: The disease rate is not the same in the two 
</p>
<p>groups. 
</p>
<p>If the exposed group has a much higher rate of disease than the unexposed 
</p>
<p>group then we will reject the null hypothesis and conclude that the evidence 
</p>
<p>favors the alternative hypothesis. This is an example of hypothesis testing. 
</p>
<p>More formally, suppose that we partition the parameter space 8 into two 
</p>
<p>disjoint sets 8 0 and 8 1 and that we wish to test 
</p>
<p>Ho : e E 8 0 versus HI: e E 8 1. (10.1 ) 
</p>
<p>We call Ho the null hypothesis and HI the alternative hypothesis. 
</p>
<p>Let X be a random variable and let X be the range of X. We test a hypoth-
</p>
<p>esis by finding an appropriate subset of outcomes ReX called the rejection </p>
<p/>
</div>
<div class="page"><p/>
<p>150 10. Hypothesis Testing and p-values 
</p>
<p>Retain Null Reject Null 
</p>
<p>Ho true J type I error 
HI true type II error 
</p>
<p>TABLE 10.1. Summary of outcomes of hypothesis testing. 
</p>
<p>region. If X E R we reject the null hypothesis, otherwise, we do not reject 
</p>
<p>the null hypothesis: 
</p>
<p>X E R ===? reject Ho 
</p>
<p>X tJ. R ===? retain (do not reject) Ho 
</p>
<p>Usually, the rejection region R is of the form 
</p>
<p>R={X: T(X&raquo;C} (10.2) 
</p>
<p>where T is a test statistic and C is a critical value. The problem in hy-
</p>
<p>pothesis testing is to find an appropriate test statistic T and an appropriate 
</p>
<p>critical value c. 
</p>
<p>Warning! There is a tendency to use hypothesis testing methods even 
</p>
<p>when they are not appropriate. Often, estimation and confidence intervals are 
</p>
<p>better tools. Use hypothesis testing only when you want to test a well-defined 
</p>
<p>hypothesis. 
</p>
<p>Hypothesis testing is like a legal trial. We assume someone is innocent 
</p>
<p>unless the evidence strongly suggests that he is guilty. Similarly, we retain Ho 
</p>
<p>unless there is strong evidence to reject Ho. There are two types of errors we 
</p>
<p>can make. Rejecting Ho when Ho is true is called a type I error. Retaining 
</p>
<p>Ho when HI is true is called a type II error. The possible outcomes for 
</p>
<p>hypothesis testing are summarized in Tab. 10.1. 
</p>
<p>10.1 Definition. The power function of a test with rejection region R is 
</p>
<p>defined by 
</p>
<p>(3(e) = lP'e(X E R). (10.3) 
</p>
<p>The size of a test is defined to be 
</p>
<p>Q = sup (3(e). (10.4) 
eEeo 
</p>
<p>A test is said to have level Q if its size is less than or equal to Q. </p>
<p/>
</div>
<div class="page"><p/>
<p>10. Hypothesis Testing and p-values 151 
</p>
<p>A hypothesis of the form e = eo is called a simple hypothesis. A hypoth-
</p>
<p>esis of the form e &gt; eo or e &lt; eo is called a composite hypothesis. A test 
</p>
<p>of the form 
</p>
<p>Ho : e = eo versus H1 : e i=- eo 
</p>
<p>is called a two-sided test. A test of the form 
</p>
<p>Ho : e ~ eo versus H1 : e &gt; eo 
</p>
<p>or 
</p>
<p>Ho : e ~ eo versus H1 : e &lt; eo 
</p>
<p>is called a one-sided test. The most common tests are two-sided. 
</p>
<p>10.2 Example. Let Xl"'" Xn rv N(IL, 0-) where 0- is known. We want to test 
</p>
<p>Ho : fL ~ 0 versus H1 : fL &gt; O. Hence, 8 0 = (-00,0] and 8 1 = (0, (0). 
Consider the test: 
</p>
<p>reject Ho if T &gt; e 
</p>
<p>where T = X. The rejection region is 
</p>
<p>R = {(X1, ... ,Xn ): T(X1, ... ,Xn ) &gt; e}. 
Let Z denote a standard Normal random variable. The power function is 
</p>
<p>{3(fL) lP'fL (X&gt; e) 
</p>
<p>lP' (foCX-fL&raquo;fo(e- fL )) 
fL 0- 0-
</p>
<p>lP'(z&gt; fo(:-fL)) 
</p>
<p>1 _ &lt;!? ( fo(: - IL)) . 
</p>
<p>This function is increasing in IL. See Figure 10.1. Hence 
</p>
<p>size = sup {3(IL) = {3(0) = 1 - &lt;!? - . ( foe) 
1,:&lt;::;0 0-
</p>
<p>For a size ex test, we set this equal to ex and solve for e to get 
</p>
<p>0-&lt;!?-1(1- ex) 
e= fo . 
</p>
<p>We reject when X &gt; 0-&lt;!?-1(1- ex)/fo. Equivalently, we reject when 
</p>
<p>fo(X - 0) 
&gt; Za' 
</p>
<p>where Za = &lt;!?-1(1 - ex). _ </p>
<p/>
</div>
<div class="page"><p/>
<p>152 10. Hypothesis Testing and p-values 
</p>
<p>Ho 
</p>
<p>FIGURE 10.1. The power function for Example 10.2. The size of the test is the 
</p>
<p>largest probability of rejecting Ho when Ho is true. This occurs at Jl = 0 hence the 
size is ;3(0). We choose the critical value c so that ;3(0) = a. 
</p>
<p>It would be desirable to find the test with highest power under HI, among 
</p>
<p>all size Q tests. Such a test, if it exists, is called most powerful. Finding 
</p>
<p>most powerful tests is hard and, in many cases, most powerful tests don't 
</p>
<p>even exist. Instead of going into detail about when most powerful tests exist, 
</p>
<p>we'll just consider four widely used tests: the Wald test,l the X2 test, the 
</p>
<p>permutation test, and the likelihood ratio test. 
</p>
<p>10.1 The Wald Test 
</p>
<p>Let e be a scalar parameter, let e be an estimate of e and let se be the 
estimated standard error of e. 
</p>
<p>lThe test is named after Abraham Wald (1902-1950), who was a very influential mathe-
</p>
<p>matical statistician. Wald died in a plane crash in India in 1950. </p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 The Wald Test 153 
</p>
<p>10.3 Definition. The Wald Test 
</p>
<p>Consider testing 
</p>
<p>Ho : 8 = 80 versus HI: 8 cJ 80 . 
</p>
<p>Assume that 8 is asymptotically Normal: 
</p>
<p>The size ex Wald test is: reject Ho when IWI &gt; Za/2 where 
</p>
<p>8 - 80 
W=-~-. 
</p>
<p>se 
(10.5) 
</p>
<p>10.4 Theorem. Asymptotically, the Wald test has size ex, that is, 
</p>
<p>as n --+ 00. 
</p>
<p>PROOF. Under 8 = 80 , (e - 80)/se -v--+ N(O,l). Hence, the probability of 
rejecting when the null 8 = 80 is true is 
</p>
<p>lP'en (IWI &gt; Za/2) lP'en (Ie ~801 &gt; za/2) 
</p>
<p>--+ IP' (IZI &gt; Za/2) 
</p>
<p>where Z rv N(O, 1) .&bull; 
</p>
<p>10.5 Remark. An alternative version of the Wald test statistic is W = (e -
80 )/seo where seo is the standard error computed at 8 = 80 . Both versions of 
</p>
<p>the test are valid. 
</p>
<p>Let us consider the power of the Wald test when the null hypothesis is false. 
</p>
<p>10.6 Theorem. Suppose the true value of8 is 8* cJ 80 , The power (3(8*) - the 
probability of correctly rejecting the null hypothesis - is given (approximately) 
</p>
<p>by 
</p>
<p>( 8
0 - 8* ) (80 - 8* ) 
</p>
<p>1 - &lt;[&gt; se + Za/2 + &lt;[&gt; ~ - Za/2 . (10.6) </p>
<p/>
</div>
<div class="page"><p/>
<p>154 10. Hypothesis Testing and p-values 
</p>
<p>Recall that se tends to 0 as the sample size increases. Inspecting (10.6) 
closely we note that: (i) the power is large if e* is far from eo, and (ii) the 
power is large if the sample size is large. 
</p>
<p>10.7 Example (Comparing Two Prediction Algorithms). We test a prediction 
</p>
<p>algorithm on a test set of size m and we test a second prediction algorithm on 
</p>
<p>a second test set of size n. Let X be the number of incorrect predictions for 
</p>
<p>algorithm 1 and let Y be the number of incorrect predictions for algorithm 
</p>
<p>2. Then X rv Binomial(m,pd and Y rv Binomial(n,P2)' To test the null 
</p>
<p>hypothesis that PI = P2 write 
</p>
<p>Ho : r5 = 0 versus HI: r5 -=I- 0 
</p>
<p>where r5 = PI - P2. The MLE is 5 = PI - P2 with estimated standard error 
</p>
<p>The size a Wald test is to reject Ho when IWI &gt; Za/2 where 
</p>
<p>The power of this test will be largest when PI is far from P2 and when the 
</p>
<p>sample sizes are large. 
</p>
<p>What if we used the same test set to test both algorithms? The two samples 
</p>
<p>are no longer independent. Instead we use the following strategy. Let Xi = 1 
</p>
<p>if algorithm 1 is correct on test case i and Xi = 0 otherwise. Let Yi = 1 if 
</p>
<p>algorithm 2 is correct on test case 'i, and Yi = 0 otherwise. Define Di = Xi - Yi. 
</p>
<p>A typical dataset will look something like this: 
</p>
<p>Test Case Xi Yi Di = Xi - Yi 
1 1 0 1 
</p>
<p>2 1 1 0 
</p>
<p>3 1 1 0 
</p>
<p>4 0 1 -1 
</p>
<p>5 0 0 0 
</p>
<p>n 0 1 -1 
</p>
<p>Let 
</p>
<p>r5 = lE(D;) = lE(Xi) -lE(Yi) = IP'(Xi = 1) -1P'(Yi = 1). 
</p>
<p>The nonparametric plug-in estimate of r5 is 5 = D = n- I L~=I Di and se(5) = 
5/vn, where 52 = n- I L~=I(Di - D)2. To test Ho : r5 = 0 versus HI: r5 -=I- 0 </p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 The Wald Test 155 
</p>
<p>we use W = lise and reject Ho if IWI &gt; Za/2' This is called a paired 
comparison. _ 
</p>
<p>10.8 Example (Comparing Two Means). Let Xl"'" Xm and Y1, ... , Yn be 
</p>
<p>two independent samples from populations with means M1 and IL2, respec-
</p>
<p>tively. Let's test the null hypothesis that M1 = M2. Write this as Ho : 0 = 0 
</p>
<p>versus H1 : 0 i= 0 where 0 = ILl - M2. Recall that the nonparametric plug-in 
estimate of 0 is l = X - Y with estimated standard error 
</p>
<p>~ vMi s~ se= -+-
m, n 
</p>
<p>where si and s~ are the sample variances. The size Q Wald test rejects Ho 
when IWI &gt; Za/2 where 
</p>
<p>0-0 X-Y 
W = -~- = ----r=~===::= 
</p>
<p>se / il. + s~ 
Ym n 
</p>
<p>-
10.9 Example (Comparing Two Medians). Consider the previous example again 
</p>
<p>but let us test whether the medians of the two distributions are the same. 
</p>
<p>Thus, Ho : 0 = 0 versus H1 : 0 i= 0 where 0 = V1 - V2 where V1 and V2 are 
the medians. The non parametric plug-in estimate of 0 is l = V1 - V2 where V1 
and lh are the sample medians. The estimated standard error se of l can be 
obtained from the bootstrap. The Wald test statistic is W = lise. _ 
</p>
<p>There is a relationship between the Wald test and the 1 - Q asymptotic 
</p>
<p>confidence interval e &plusmn; se Za/2' 
10.10 Theorem. The size Q Wald test rejects Ho : e = eo versus H1 : e i= eo 
if and only if eo ~ C where 
</p>
<p>C = (e - seza /2' e + seza /2)' 
Thus, testing the hypothesis is equivalent to checking whether the null value 
</p>
<p>is in the confidence interval. 
</p>
<p>Warning! When we reject Ho we often say that the result is statistically 
</p>
<p>significant. A result might be statistically significant and yet the size of the 
</p>
<p>effect might be small. In such a case we have a result that is statistically sig-
</p>
<p>nificant but not scientifically or practically significant. The difference between 
</p>
<p>statistical significance and scientific significance is easy to understand in light 
</p>
<p>of Theorem 10.10. Any confidence interval that excludes eo corresponds to re-
</p>
<p>jecting Ho. But the values in the interval could be close to eo (not scientifically 
</p>
<p>significant) or far from eo (scientifically significant). See Figure 10.2. </p>
<p/>
</div>
<div class="page"><p/>
<p>156 10. Hypothesis Testing and p-values 
</p>
<p>( &bull;&bull; ) e 
</p>
<p>( &bull; ) e 
</p>
<p>FIGURE 10.2. Scientific significance versus statistical significance. A level a test 
</p>
<p>rejects Ho : () = ()o if and only if the 1 - a confidence interval does not include 
</p>
<p>()o. Here are two different confidence intervals. Both exclude ()o so in both cases the 
</p>
<p>test would reject Ho. But in the first case, the estimated value of () is close to ()o so 
</p>
<p>the finding is probably of little scientific or practical value. In the second case, the 
</p>
<p>estimated value of () is far from ()o so the finding is of scientific value. This shows 
</p>
<p>two things. First, statistical significance does not imply that a finding is of scientific 
</p>
<p>importance. Second, confidence intervals are often more informative than tests. 
</p>
<p>10.2 p-values 
</p>
<p>Reporting "reject Ha" or "retain Ha" is not very informative. Instead, we 
</p>
<p>could ask, for every a, whether the test rejects at that level. Generally, if the 
</p>
<p>test rejects at level a it will also reject at level a' &gt; a. Hence, there is a 
smallest a at which the test rejects and we call this number the p-value. See 
</p>
<p>Figure 10.3. 
</p>
<p>10.11 Definition. Suppose that for every a E (0,1) we have a size a test 
</p>
<p>with rejection region Ra. Then, 
</p>
<p>p-value = inf{ a: T(xn) ERa }. 
</p>
<p>That is, the p-value is the smallest level at which we can reject Ha. 
</p>
<p>Informally, the p-value is a measure of the evidence against Ha: the smaller 
</p>
<p>the p-value, the stronger the evidence against Ha. Typically, researchers use 
</p>
<p>the following evidence scale: </p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 p-values 157 
</p>
<p>Yes 
</p>
<p>Reject? 
</p>
<p>No 
</p>
<p>o t 1 a 
p-value 
</p>
<p>FIGURE 10.3. p-values explained. For each a we can ask: does our test reject Ho 
</p>
<p>at level a? The p-value is the smallest a at which we do reject Ho. If the evidence 
</p>
<p>against Ho is strong, the p-value will be small. 
</p>
<p>p-value 
</p>
<p>&lt; .01 
.01 - .05 
</p>
<p>.05 - .10 
</p>
<p>&gt; .1 
</p>
<p>evidence 
</p>
<p>very strong evidence against Ho 
strong evidence against Ho 
weak evidence against Ho 
little or no evidence against Ho 
</p>
<p>Warning! A large p-value is not strong evidence in favor of Ho. A large 
</p>
<p>p-value can occur for two reasons: (i) Ho is true or (ii) Ho is false but the test 
</p>
<p>has low power. 
</p>
<p>Warning! Do not confuse the p-value with lP'(HoIData). 2 The p-value is 
</p>
<p>not the probability that the null hypothesis is true. 
</p>
<p>The following result explains how to compute the p-value. 
</p>
<p>2We discuss quantities like lP(HoIData) in the chapter on Bayesian inference. </p>
<p/>
</div>
<div class="page"><p/>
<p>158 10. Hypothesis Testing and p-values 
</p>
<p>10.12 Theorem. Suppose that the size Q test is of the form 
</p>
<p>Then, 
</p>
<p>reject Ho if and only if T(xn) :::: Ca. 
</p>
<p>p-value = sup lP'e(T(Xn) :::: T(xn)) 
eEeo 
</p>
<p>where xn is the observed value of xn. If 8 0 = {eo} then 
</p>
<p>We can express Theorem 10.12 as follows: 
</p>
<p>The p-value is the probability (under Ho) of observing a value of the 
</p>
<p>test statistic the same as or more extreme than what was actually 
</p>
<p>observed. 
</p>
<p>10.13 Theorem. Let w = (e - eo) lie denote the observed value of the 
Wald statistic W. The p-value is given by 
</p>
<p>p - value = lP'eo (IWI &gt; Iwl) ~ IP'(IZI &gt; Iwl) = 2&lt;1&gt;( -Iwl) 
</p>
<p>where Z rv N(O, 1). 
</p>
<p>To understand this last theorem, look at Figure 10.4. 
</p>
<p>Here is an important property of p-values. 
</p>
<p>(10.7) 
</p>
<p>10.14 Theorem. If the test statistic has a continuous distribution, then under 
</p>
<p>Ho : e = eo, the p-value has a Uniform (0,1) distribution. Therefore, if we 
</p>
<p>reject Ho when the p-value is less than Q, the probability of a type I error is 
</p>
<p>Q. 
</p>
<p>In other words, if Ho is true, the p-value is like a random draw from a 
</p>
<p>Unif(O,l) distribution. If HI is true, the distribution of the p-value will tend 
</p>
<p>to concentrate closer to 0. 
</p>
<p>10.15 Example. Recall the cholesterol data from Example 7.15. To test if the 
</p>
<p>means are different we compute 
</p>
<p>6-0 X-Y 
W = -~- = ----r=::===;;= 
</p>
<p>se . / si + s~ 
Vm n 
</p>
<p>216.2 - 195.3 
----;~==:=:=~ = 3.78. 
)52 + 2.42 </p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 The X2 Distribution 159 
</p>
<p>-Iwl Iwl 
</p>
<p>FIGURE 10.4. The p-value is the smallest a at which you would reject Ho. To 
</p>
<p>find the p-value for the Wald test, we find a such that Iwl and -Iwl are just at the 
boundary of the rejection region. Here, w is the observed value of the Wald statistic: 
</p>
<p>w = (e - Bo)/se. This implies that the p-value is the tail area IP'(IZI &gt; Iwl) where 
Z ~ N(O, 1). 
</p>
<p>To compute the p-value, let Z rv N(O, 1) denote a standard Normal random 
</p>
<p>variable. Then, 
</p>
<p>p-value = J1D(IZI &gt; 3.78) = 2J1D(Z &lt; -3.78) = .0002 
</p>
<p>which is very strong evidence against the null hypothesis. To test if the me-
</p>
<p>dians are different, let VI and V2 denote the sample medians. Then, 
</p>
<p>W VI - V2 212.5 - 194 - - - 24 - se - 7.7 -. 
where the standard error 7.7 was found using the bootstrap. The p-value is 
</p>
<p>p-value = J1D(IZI &gt; 2.4) = 2J1D(Z &lt; -2.4) = .02 
</p>
<p>which is strong evidence against the null hypothesis. _ 
</p>
<p>10.3 The X2 Distribution 
</p>
<p>Before proceeding we need to discuss the X2 distribution. Let ZI,"" Zk be 
</p>
<p>independent, standard Normals. Let V = L~=I zf. Then we say that V has 
a X2 distribution with k degrees of freedom, written V rv Xk' The probability 
</p>
<p>density of V is 
VCk/2)-le-v/2 
</p>
<p>f(v) = 2k / 2 f(k/2) 
</p>
<p>for v &gt; O. It can be shown that lE(V) = k and V(V) = 2k. We define the upper 
a quantile Xk,a = p- I (l-a) where P is the CDF. That is, J1D(Xk &gt; Xk,a) = a. </p>
<p/>
</div>
<div class="page"><p/>
<p>160 10. Hypothesis Testing and p-values 
</p>
<p>t 
</p>
<p>FIGURE 10.5. The p-value is the smallest a at which we would reject Ho. To find 
</p>
<p>the p-value for the xLi test, we find a such that the observed value t of the test 
statistic is just at the boundary of the rejection region. This implies that the p-value 
</p>
<p>is the tail area IP'(X~-i &gt; t). 
</p>
<p>10.4 Pearson's X2 Test For Multinomial Data 
</p>
<p>Pearson's x2 test is used for multinomial data. Recall that if X = (Xl' ... ' X k ) 
has a multinomial (n,p) distribution, then the !vILE of pis P= (PI, ... ,Pk) = 
(Xdn, ... , Xk/n). 
</p>
<p>Let Po = (Po 1, ... ,POk) be some fixed vector and suppose we want to test 
</p>
<p>Ho : P = Po versus HI : P # Po&middot; 
</p>
<p>10.16 Definition. Pearson's X2 statistic is 
</p>
<p>k ( )2 k ( )2 
T =" Xj - npOj =" Xj - E j 
</p>
<p>~ npo&middot; ~ E 
j=l J j=l J 
</p>
<p>where E j = lE(Xj) = npOj is the expected value of Xj under Ho. 
</p>
<p>10.17 Theorem. Under H o, T ~ xLI. Hence the test: reject Ho if T &gt; 
XL1.a has asymptotic level Q. The p-value is J1D(xLl &gt; t) where t is the 
observed value of the test statistic. 
</p>
<p>Theorem 10.17 is illustrated in Figure 10.5. </p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 The Permutation Test 161 
</p>
<p>10.18 Example (Mendel's peas). Mendel bred peas with round yellow seeds 
</p>
<p>and wrinkled green seeds. There are four types of progeny: round yellow, 
</p>
<p>wrinkled yellow, round green, and wrinkled green. The number of each type 
</p>
<p>is multinomial with probability P = (Pl,P2,P3,P4). His theory of inheritance 
</p>
<p>predicts that P is equal to 
</p>
<p>( 9 3 3 1) 
Po == 16' 16' 16' 16 . 
</p>
<p>In n = 556 trials he observed X = (315,101,108,32). We will test Ho : P = Po 
</p>
<p>versus HI : P # Po. Since, npOl = 312.75, np02 = np03 = 104.25, and np04 = 
34.75, the test statistic is 
</p>
<p>(315 - 312.75)2 (101 - 104.25)2 
</p>
<p>312.75 + 104.25 
(108 - 104.25)2 (32 - 34.75)2 
</p>
<p>+ + = 0.47. 
104.25 34.75 
</p>
<p>The ex = .05 value for a x&sect; is 7.815. Since 0.47 is not larger than 7.815 we do 
</p>
<p>not reject the null. The p-value is 
</p>
<p>p-value = W(x&sect; &gt; .47) = .93 
</p>
<p>which is not evidence against Ho. Hence, the data do not contradict Mendel's 
</p>
<p>theory.3. 
</p>
<p>In the previous example, one could argue that hypothesis testing is not the 
</p>
<p>right tool. Hypothesis testing is useful to see if there is evidence to reject Ho. 
</p>
<p>This is appropriate when Ho corresponds to the status quo. It is not useful for 
</p>
<p>proving that Ho is true. Failure to reject Ho might occur because Ho is true, 
</p>
<p>but it might occur just because the test has low power. Perhaps a confidence 
</p>
<p>set for the distance between P and Po might be more useful in this example. 
</p>
<p>10.5 The Permutation Test 
</p>
<p>The permutation test is a nonparametric method for testing whether two 
</p>
<p>distributions are the same. This test is "exact," meaning that it is not based 
</p>
<p>on large sample theory approximations. Suppose that Xl, ... , Xm rv Fx and 
</p>
<p>Yl , ... , Yn rv Fy are two independent samples and Ho is the hypothesis that 
</p>
<p>3There is some controversy about whether Mendel's results are "too good." </p>
<p/>
</div>
<div class="page"><p/>
<p>162 10. Hypothesis Testing and p-values 
</p>
<p>the two samples are identically distributed. This is the type of hypothesis we 
</p>
<p>would consider when testing whether a treatment differs from a placebo. More 
</p>
<p>precisely we are testing 
</p>
<p>Ho : Fx = Fy versus HI: Fx i=- Fy . 
</p>
<p>Let T(Xl"'" Xm, Yl, ... , Yn) be some test statistic, for example, 
</p>
<p>Let N = m + n and consider forming all N! permutations of the data Xl, ... , 
X m , Yl , ... , Yn . For each permutation, compute the test statistic T. Denote 
</p>
<p>these values by T l , ... , TN!. Under the null hypothesis, each of these values is 
</p>
<p>equally likely. 4 The distribution JlDo that puts mass 1/ N! on each T j is called 
</p>
<p>the permutation distribution of T. Let tobs be the observed value of the 
</p>
<p>test statistic. Assuming we reject when T is large, the p-value is 
</p>
<p>1 N! 
</p>
<p>p-value = JlDo(T &gt; tobs) = -I L I(Tj &gt; tobs)' 
N. 
</p>
<p>j=l 
</p>
<p>10.19 Example. Here is a toy example to make the idea clear. Suppose the 
</p>
<p>data are: (Xl, X 2 , Yl ) = (1,9,3). Let T(Xl , X 2 , Yd = IX - YI = 2. The 
permutations are: 
</p>
<p>permutation value of T 
</p>
<p>(1,9,3) 2 
</p>
<p>(9,1,3) 2 
</p>
<p>(1,3,9) 7 
</p>
<p>(3,1,9) 7 
</p>
<p>(3,9,1) 5 
</p>
<p>(9,3,1) 5 
</p>
<p>The p-value is JlD(T &gt; 2) = 4/6 .&bull; 
</p>
<p>probability 
</p>
<p>1/6 
</p>
<p>1/6 
</p>
<p>1/6 
</p>
<p>1/6 
</p>
<p>1/6 
</p>
<p>1/6 
</p>
<p>Usually, it is not practical to evaluate all N! permutations. We can approx-
</p>
<p>imate the p-value by sampling randomly from the set of permutations. The 
</p>
<p>fraction of times T j &gt; tobs among these samples approximates the p-value. 
</p>
<p>4More precisely, under the null hypothesis, given the ordered data values, 
</p>
<p>Xl, ... , X m , Yl, ... , Yn is uniformly distributed over the N! permutations of the data. </p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 The Permutation Test 163 
</p>
<p>Algorithm for Permutation Test 
</p>
<p>1. Compute the observed value of the test statistic 
</p>
<p>tabs = T(X1 , ... , X m , Y1 ,&middot;&middot;&middot;, Yn). 
</p>
<p>2. Randomly permute the data. Compute the statistic again using the 
</p>
<p>permuted data. 
</p>
<p>3. Repeat the previous step B times and let T1 , ... , TB denote the 
</p>
<p>resulting values. 
</p>
<p>4. The approximate p-value is 
</p>
<p>10.20 Example. DNA micro arrays allow researchers to measure the expres-
</p>
<p>sion levels of thousands of genes. The data are the levels of messenger RNA 
</p>
<p>(mRNA) of each gene, which is thought to provide a measure of how much 
</p>
<p>protein that gene produces. Roughly, the larger the number, the more active 
</p>
<p>the gene. The table below, reproduced from Efron et al. (2001) shows the 
</p>
<p>expression levels for genes from ten patients with two types of liver cancer 
</p>
<p>cells. There are 2,638 genes in this experiment but here we show just the first 
</p>
<p>two. The data are log-ratios of the intensity levels of two different color dyes 
</p>
<p>used on the arrays. 
</p>
<p>Type I Type II 
</p>
<p>Patient 1 2 3 4 5 6 7 8 9 10 
</p>
<p>Gene 1 230 -1,350 -1,580 -400 -760 970 110 -50 -190 -200 
</p>
<p>Gene 2 470 -850 -.8 -280 120 390 -1730 -1360 -1 -330 
</p>
<p>Let's test whether the median level of gene 1 is different between the two 
</p>
<p>groups. Let VI denote the median level of gene 1 of Type I and let V2 denote the 
</p>
<p>median level of gene 1 of Type II. The absolute difference of sample medians 
</p>
<p>is T = IVI - v21 = 710. Now we estimate the permutation distribution by 
simulation and we find that the estimated p-value is .045. Thus, if we use a 
</p>
<p>a = .05 level of significance, we would say that there is evidence to reject the 
</p>
<p>null hypothesis of no difference. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>164 10. Hypothesis Testing and p-values 
</p>
<p>In large samples, the permutation test usually gives similar results to a test 
</p>
<p>that is based on large sample theory. The permutation test is thus most useful 
</p>
<p>for small samples. 
</p>
<p>10.6 The Likelihood Ratio Test 
</p>
<p>The Wald test is useful for testing a scalar parameter. The likelihood ratio 
</p>
<p>test is more general and can be used for testing a vector-valued parameter. 
</p>
<p>10.21 Definition. Consider testing 
</p>
<p>Ho : 8 E 8 0 versus HI: 8 rt 8 0 . 
</p>
<p>The likelihood ratio statistic is 
</p>
<p>A = 2 log ( sUPeE8 &pound;( 8) ) = 2 log ( &pound;r!) ) 
sUPeE8" &pound;(8) &pound;(80 ) 
</p>
<p>where 8 is the MLE and 80 is the MLE when 8 is restricted to lie in 8 0 . 
</p>
<p>You might have expected to see the maximum of the likelihood over 88 
</p>
<p>instead of 8 in the numerator. In practice, replacing 88 with 8 has little 
</p>
<p>effect on the test statistic. Moreover, the theoretical properties of A are much 
</p>
<p>simpler if the test statistic is defined this way. 
</p>
<p>The likelihood ratio test is most useful when 8 0 consists of all parameter 
</p>
<p>values 8 such that some coordinates of 8 are fixed at particular values. 
</p>
<p>10.22 Theorem. Suppose that 8 = (81 , ... , 8q , 8q+1 , ... , 8r ). Let 
</p>
<p>Let A be the likelihood ratio test statistic. Under Ho : 8 E 8 0 ! 
</p>
<p>\( n) -v--+ 2 
/\ X Xr-q,a 
</p>
<p>where r - q is the dimension of 8 minus the dimension of 8 0 . The p-value 
</p>
<p>for the test is IP'(X~_q &gt; A). 
</p>
<p>For example, if 8 = (81,82,83,84,85) and we want to test the null hypothesis 
</p>
<p>that 84 = 85 = 0 then the limiting distribution has 5 - 3 = 2 degrees of 
</p>
<p>freedom. </p>
<p/>
</div>
<div class="page"><p/>
<p>10.7 Multiple Testing 165 
</p>
<p>10.23 Example (Mendel's Peas Revisited). Consider example 10.18 again. The 
</p>
<p>likelihood ratio test statistic for Ho : P = Po versus HI : P i=- Po is 
</p>
<p>( &pound;(p) ) 
210g &pound;(Po) 
</p>
<p>2 t Xj log ( pj) 
j=l Po] 
</p>
<p>( (
315) ( 101 ) 
</p>
<p>2 31510g 5i~66 + 101 log 51~66 
</p>
<p>+ 108 log ( ~1~6~ ) + 32 log ( 5~~~6 ) ) 
0.48. 
</p>
<p>Under HI there are four parameters. However, the parameters must sum to 
</p>
<p>one so the dimension of the parameter space is three. Under Ho there are no 
</p>
<p>free parameters so the dimension of the restricted parameter space is zero. The 
</p>
<p>difference of these two dimensions is three. Therefore, the limiting distribution 
</p>
<p>of A under Ho is x&sect; and the p-value is 
</p>
<p>2 
p-value = IP'(X3 &gt; .48) = .92. 
</p>
<p>The conclusion is the same as with the X2 test. _ 
</p>
<p>When the likelihood ratio test and the X2 test are both applicable, as in the 
</p>
<p>last example, they usually lead to similar results as long as the sample size is 
</p>
<p>large. 
</p>
<p>10.7 Multiple Testing 
</p>
<p>In some situations we may conduct many hypothesis tests. In example 10.20, 
</p>
<p>there were actually 2,638 genes. If we tested for a difference for each gene, 
</p>
<p>we would be conducting 2,638 separate hypothesis tests. Suppose each test 
</p>
<p>is conducted at level cx. For anyone test, the chance of a false rejection of 
</p>
<p>the null is cx. But the chance of at least one false rejection is much higher. 
</p>
<p>This is the multiple testing problem. The problem comes up in many data 
</p>
<p>mining situations where one may end up testing thousands or even millions of 
</p>
<p>hypotheses. There are many ways to deal with this problem. Here we discuss 
</p>
<p>two methods. </p>
<p/>
</div>
<div class="page"><p/>
<p>166 10. Hypothesis Testing and p-values 
</p>
<p>Consider rn hypothesis tests: 
</p>
<p>HOi versus H 1i , i = 1, ... , rn 
</p>
<p>and let PI, ... , Pm denote the rn p-values for these tests. 
</p>
<p>The Bonferroni Method 
</p>
<p>Given p-values PI, .. . , Pm, reject null hypothesis HOi if 
</p>
<p>10.24 Theorem. Using the Bonferroni method, the probability of falsely re-
</p>
<p>jecting any null hypotheses is less than or equal to ex. 
</p>
<p>PROOF. Let R be the event that at least one null hypothesis is falsely 
</p>
<p>rejected. Let Ri be the event that the ith null hypothesis is falsely rejected. 
</p>
<p>Recall that if AI, ... ,Ak are events then J1D(U7=1 Ai) ::; 2::7=1 J1D(Ai). Hence, 
</p>
<p>(
m ) m m 
</p>
<p>J1D(R) = JID ild Ri ::; 8 J1D(Ri) = 8 : = ex 
from Theorem 10.14. _ 
</p>
<p>10.25 Example. In the gene example, using ex = .05, we have that .05/2,638 = 
</p>
<p>.00001895375. Hence, for any gene with p-value less than .00001895375, we 
</p>
<p>declare that there is a significant difference. _ 
</p>
<p>The Bonferroni method is very conservative because it is trying to make 
</p>
<p>it unlikely that you would make even one false rejection. Sometimes, a more 
</p>
<p>reasonable idea is to control the false discovery rate (FDR) which is de-
</p>
<p>fined as the mean of the number of false rejections divided by the number of 
</p>
<p>rejections. 
</p>
<p>Suppose we reject all null hypotheses whose p-values fall below some thresh-
</p>
<p>old. Let rna be the number of null hypotheses that are true and let rnl 
</p>
<p>rn - mo. The tests can be categorized in a 2 x 2 as in Table 10.2. 
</p>
<p>Define the False Discovery Proportion (FDP) 
</p>
<p>FDP={ ~/R if R&gt; 0 
if R = o. 
</p>
<p>The FDP is the proportion of rejections that are incorrect. Next define FDR = 
</p>
<p>lE(FDP). </p>
<p/>
</div>
<div class="page"><p/>
<p>10.7 Multiple Testing 167 
</p>
<p>Ho Not Rejected Ho Rejected Total 
Ho True U V m&middot;o 
Ho False T S m1 
Total m-R R rn 
</p>
<p>TABLE 10.2. Types of outcomes in multiple testing. 
</p>
<p>The Benjamini-Hochberg (BH) Method 
</p>
<p>1. Let p(l) &lt; ... &lt; PCrn) denote the ordered p-values. 
</p>
<p>2. Define 
</p>
<p>ia {} &euro;i = Crnm ' and R = max i: PCi ) &lt; &euro;i (10.8) 
</p>
<p>where Crn is defined to be 1 if the p-values are independent and 
</p>
<p>Cm = 2..:7:1 (Iii) otherwise. 
</p>
<p>3. Let T = PCR); we call T the BH rejection threshold. 
</p>
<p>4. Reject all null hypotheses HOi for which Pi :.:; T. 
</p>
<p>10.26 Theorem (Benjamini and Hochberg). If the procedure above is applied, 
</p>
<p>then regardless of how many nulls are true and regardless of the distribution 
</p>
<p>of the p-values when the null hypothesis is false, 
</p>
<p>mo 
FDR = IE(FDP) :.:; -a :.:; a. 
</p>
<p>m 
</p>
<p>10.27 Example. Figure 10.6 shows six ordered p-values plotted as vertical 
</p>
<p>lines. If we tested at level a without doing any correction for multiple testing, 
</p>
<p>we would reject all hypotheses whose p-values are less than a. In this case, 
</p>
<p>the four hypotheses corresponding to the four smallest p-values are rejected. 
</p>
<p>The Bonferroni method rejects all hypotheses whose p-values are less than 
</p>
<p>aim. In this case, this leads to no rejections. The BH threshold corresponds 
</p>
<p>to the last p-value that falls under the line with slope a. This leads to two 
</p>
<p>hypotheses being rejected in this case. _ 
</p>
<p>10.28 Example. Suppose that 10 independent hypothesis tests are carried 
</p>
<p>leading to the following ordered p-values: 
</p>
<p>0.00017 0.00448 0.00671 0.00907 0.01220 
</p>
<p>0.33626 0.39341 0.53882 0.58125 0.98617 </p>
<p/>
</div>
<div class="page"><p/>
<p>168 10. Hypothesis Testing and p-values 
</p>
<p>&bull; 
</p>
<p>T 
aim I-:to' --:--~--!--~-""""!: 
</p>
<p>rej ct don't reject 
</p>
<p>Threshold 
</p>
<p>FIGURE 10.6. The Benjamini-Hochberg (BH) procedure. For uncorrected testing 
</p>
<p>we reject when Pi &lt; a. For Bonferroni testing we reject when Pi &lt; aim. The BH 
procedure rejects when Pi -s: T. The BH threshold T corresponds to the rightmost 
undercrossing of the upward sloping line. 
</p>
<p>With a = 0.05, the Bonferroni test rejects any hypothesis whose p-value is 
</p>
<p>less than 0./10 = 0.005. Thus, only the first two hypotheses are rejected. For 
</p>
<p>the BH test, we find the largest i such that P(i) &lt; ia/m, which in this case is 
</p>
<p>i = 5. Thus we reject the first five hypotheses .&bull; 
</p>
<p>10.8 Goodness-of-fit Tests 
</p>
<p>There is another situation where testing arises, namely, when we want to check 
</p>
<p>whether the data come from an assumed parametric model. There are many 
</p>
<p>such tests; here is one. 
</p>
<p>Let J = {f(x; 8): 8 E 8} be a parametric model. Suppose the data take 
</p>
<p>values on the real line. Divide the line into k disjoint intervals h, ... , I k . For 
</p>
<p>j = 1, ... , k, let 
</p>
<p>pj(8) = / f(x; 8) dx 
JI] 
</p>
<p>be the probability that an observation falls into interval I j under the assumed 
</p>
<p>model. Here, 8 = (81 , ... , 8 s) are the parameters in the assumed model. Let 
</p>
<p>N j be the number of observations that fall into I j . The likelihood for 8 based </p>
<p/>
</div>
<div class="page"><p/>
<p>10.9 Bibliographic Remarks 169 
</p>
<p>on the counts N 1 , ... , Nk is the multinomial likelihood 
</p>
<p>k 
</p>
<p>Q(e) = IIpi(e)Nj &bull; 
j=1 
</p>
<p>~ ~ ~ 
</p>
<p>Maximizing Q(e) yields estimates e = (e l , ... , es ) of e. Now define the test 
</p>
<p>statistic 
</p>
<p>(10.9) 
</p>
<p>10.29 Theorem. Let Ho be the null hypothesis that the data are lID draws from 
</p>
<p>the model J = {f(x; e): e E 8}. Under H - 0, the statistic Q defined in 
equation (10.9) converges in distribution to a Xk-l-s random variable. Thus, 
</p>
<p>the (approximate) p-value for the test is lP'(xLI-s &gt; q) where q denotes the 
observed value of Q. 
</p>
<p>It is tempting to replace (j in (10.9) with the MLE e. However, this will not 
result in a statistic whose limiting distribution is a Xk-l-s' However, it can 
</p>
<p>be shown - due to a theorem of Herman Chernoff and Erich Lehmann from 
</p>
<p>1954 - that the p-value is bounded approximately by the p-values obtained 
</p>
<p>using a xLl-s and a xLI' 
Goodness-of-fit testing has some serious limitations. If reject Ho then we 
</p>
<p>conclude we should not use the model. But if we do not reject Ho we can-
</p>
<p>not conclude that the model is correct. We may have failed to reject simply 
</p>
<p>because the test did not have enough power. This is why it is better to use 
</p>
<p>nonparametric methods whenever possible rather than relying on parametric 
</p>
<p>assumptions. 
</p>
<p>10.9 Bibliographic Remarks 
</p>
<p>The most complete book on testing is Lehmann (1986). See also Chapter 8 of 
</p>
<p>Casella and Berger (2002) and Chapter 9 of Rice (1995). The FDR method is 
</p>
<p>due to Benjamini and Hochberg (1995). Some of the exercises are from Rice 
</p>
<p>(1995). </p>
<p/>
</div>
<div class="page"><p/>
<p>170 10. Hypothesis Testing and p-values 
</p>
<p>10.10 Appendix 
</p>
<p>10.10.1 The Neyman-Pearson Lemma 
</p>
<p>In the special case of a simple null Ho : 8 = 80 and a simple alternative 
</p>
<p>HI : 8 = 81 we can say precisely what the most powerful test is. 
</p>
<p>10.30 Theorem (Neyman-Pearson). Suppose we test Ho : 8 = 80 versus HI : 
</p>
<p>8 = 81 . Let 
</p>
<p>T _ &pound;(8d _ TI~=l f(xi; 81 ) 
- &pound;(80 ) - TI~=l f(xi; 80 )" 
</p>
<p>Suppose we reject Ho when T &gt; k. If we choose k so that lP'eo(T &gt; k) = a 
</p>
<p>then this test is the most powerful, size a test. That is, among all tests with 
</p>
<p>size a, this test maximizes the power (3(8I). 
</p>
<p>10.10.2 The t-test 
</p>
<p>To test Ho : fL = fLo where fL = IE(Xi) is the mean, we can use the Wald test. 
</p>
<p>When the data are assumed to be Normal and the sample size is small, it is 
</p>
<p>common instead to use the t-test. A random variable T has at-distribution 
</p>
<p>with k degrees of freedom if it has density 
</p>
<p>r (k+l) 
f(t) = -2 
</p>
<p>Yhr (~) (1 + ~)(k+l)/2' 
</p>
<p>When the degrees of freedom k -+ 00, this tends to a Normal distribution. 
</p>
<p>When k = 1 it reduces to a Cauchy. 
</p>
<p>Let Xl"'" Xn rv N(fL, (]"2) where 8 = (fL, (]"2) are both unknown. Suppose 
</p>
<p>we want to test IL = fLo versus IL of. ILo. Let 
</p>
<p>T = fo(Xn - fLo) 
Sn 
</p>
<p>where S?;, is the sample variance. For large samples T ~ N(O,l) under Ho. 
The exact distribution of T under Ho is tn-I. Hence if we reject when ITI &gt; 
tn - l ,a/2 then we get a size a test. However, when n is moderately large, the 
</p>
<p>t-test is essentially identical to the Wald test. 
</p>
<p>10.11 Exercises 
</p>
<p>1. Prove Theorem 10.6. </p>
<p/>
</div>
<div class="page"><p/>
<p>10.11 Exercises 171 
</p>
<p>2. Prove Theorem 10.14. 
</p>
<p>3. Prove Theorem 10.10. 
</p>
<p>4. Prove Theorem 10.12. 
</p>
<p>5. Let XI"",Xn rv Uniform(O,e) and let Y = max{XI"",Xn}' We want 
</p>
<p>to test 
</p>
<p>Ho : e = 1/2 versus HI : e &gt; 1/2. 
The Wald test is not appropriate since Y does not converge to a Normal. 
</p>
<p>Suppose we decide to test this hypothesis by rejecting Ho when Y &gt; c. 
</p>
<p>( a) Find the power function. 
</p>
<p>(b) What choice of c will make the size of the test .05? 
</p>
<p>( c) In a sam pIe of size n = 20 with Y =0.48 what is the p-val ue? What 
</p>
<p>conclusion about Ho would you make? 
</p>
<p>(d) In a sample of size n = 20 with Y =0.52 what is the p-value? What 
</p>
<p>conclusion about Ho would you make? 
</p>
<p>6. There is a theory that people can postpone their death until after an 
</p>
<p>important event. To test the theory, Phillips and King (1988) collected 
</p>
<p>data on deaths around the Jewish holiday Passover. Of 1919 deaths, 922 
</p>
<p>died the week before the holiday and 997 died the week after. Think of 
</p>
<p>this as a binomial and test the null hypothesis that e = 1/2. Report and 
interpret the p-value. Also construct a confidence interval for e. 
</p>
<p>7. In 1861, 10 essays appeared in the New Orleans Daily Crescent. They 
</p>
<p>were signed "Quintus Curtius Snodgrass" and some people suspected 
</p>
<p>they were actually written by Mark Twain. To investigate this, we will 
</p>
<p>consider the proportion of three letter words found in an author's work. 
</p>
<p>From eight Twain essays we have: 
</p>
<p>.225 .262 .217 .240 .230 .229 .235 .217 
</p>
<p>From 10 Snodgrass essays we have: 
</p>
<p>.209 .205 .196 .210 .202 .207 .224 .223 .220 .201 
</p>
<p>(a) Perform a Wald test for equality of the means. Use the nonparamet-
</p>
<p>ric plug-in estimator. Report the p-value and a 95 per cent confidence 
</p>
<p>interval for the difference of means. What do you conclude? 
</p>
<p>(b) Now use a permutation test to avoid the use oflarge sample methods. 
</p>
<p>What is your conclusion? (Brinegar (1963)). </p>
<p/>
</div>
<div class="page"><p/>
<p>172 10. Hypothesis Testing and p-values 
</p>
<p>8. Let Xl, ... ,Xn '" N (e, 1). Consider testing 
</p>
<p>Ho : e = 0 versus e = 1. 
</p>
<p>Let the rejection region be R = {xn: T(xn) &gt; c} where T(xn) 
-l,\,n X 
</p>
<p>n L...i=l i&middot; 
</p>
<p>(a) Find c so that the test has size Q. 
</p>
<p>(b) Find the power under HI, that is, find ,6 (1). 
</p>
<p>(c) Show that ,6(1) --+ 1 as n --+ 00. 
</p>
<p>9. Let abe the MLE of a parameter e and let se = {n1(a)}-1/2 where 1(e) 
is the Fisher information. Consider testing 
</p>
<p>Ho : e = eo versus e i=- eo&middot; 
</p>
<p>Consider the Wald test with rejection region R = {xn: IZI &gt; Za/2} 
where Z = (e - eo)/se. Let e1 &gt; eo be some alternative. Show that 
,6(e1 ) --+ 1. 
</p>
<p>10. Here are the number of elderly Jewish and Chinese women who died 
</p>
<p>just before and after the Chinese Harvest Moon Festival. 
</p>
<p>Week Chinese Jewish 
</p>
<p>-2 55 141 
</p>
<p>-1 33 145 
</p>
<p>1 70 139 
</p>
<p>2 49 161 
</p>
<p>Compare the two mortality patterns. (Phillips and Smith (1990)). 
</p>
<p>11. A randomized, double-blind experiment was conducted to assess the 
</p>
<p>effectiveness of several drugs for reducing postoperative nausea. The 
</p>
<p>data are as follows. 
</p>
<p>Number of Patients Incidence of Nausea 
</p>
<p>Placebo 80 45 
</p>
<p>Chlorpromazine 75 26 
</p>
<p>Dimenhydrinate 85 52 
</p>
<p>Pentobarbital (100 mg) 67 35 
</p>
<p>Pentobarbital (150 mg) 85 37 </p>
<p/>
</div>
<div class="page"><p/>
<p>10.11 Exercises 173 
</p>
<p>(a) Test each drug versus the placebo at the 5 per cent level. Also, report 
</p>
<p>the estimated odds-ratios. Summarize your findings. 
</p>
<p>(b) Use the Bonferroni and the FDR method to adjust for multiple 
</p>
<p>testing. (Beecher (1959)). 
</p>
<p>12. Let Xl, ... , Xn rv Poisson(A). 
</p>
<p>(a) Let AO &gt; O. Find the size a Wald test for 
</p>
<p>Ho : A = AO versus HI: A i=- AO&middot; 
</p>
<p>(b) (Computer Experiment.) Let AO = 1, n = 20 and a = .05. Simulate 
</p>
<p>Xl, . .. , Xn rv Poisson(Ao) and perform the Wald test. Repeat many 
</p>
<p>times and count how often you reject the null. How close is the type I 
</p>
<p>error rate to .057 
</p>
<p>13. Let Xl,"" Xn rv N(,L, (T2). Construct the likelihood ratio test for 
</p>
<p>Ho : fL = fLo versus HI: fL i=- fLo&middot; 
</p>
<p>Compare to the Wald test. 
</p>
<p>14. Let Xl"'" Xn rv N(,L, (T2). Construct the likelihood ratio test for 
</p>
<p>Ho : (T = (To versus HI: (T i=- (To&middot; 
</p>
<p>Compare to the Wald test. 
</p>
<p>15. Let X rv Binomial(n,p). Construct the likelihood ratio test for 
</p>
<p>Ho : p = Po versus HI: p i=- Po&middot; 
</p>
<p>Compare to the Wald test. 
</p>
<p>16. Let 8 be a scalar parameter and suppose we test 
</p>
<p>Ho : 8 = 80 versus HI: 8 i=- 80 . 
</p>
<p>Let W be the Wald test statistic and let A be the likelihood ratio test 
</p>
<p>statistic. Show that these tests are equivalent in the sense that 
</p>
<p>W 2 p 
---+1 
</p>
<p>A 
</p>
<p>as n ---+ 00. Hint: Use a Taylor expansion of the log-likelihood &pound;(8) to 
</p>
<p>show that </p>
<p/>
</div>
<div class="page"><p/>
<p>11 
</p>
<p>Bayesian Inference 
</p>
<p>11.1 The Bayesian Philosophy 
</p>
<p>The statistical methods that we have discussed so far are known as frequen-
</p>
<p>tist (or classical) methods. The frequentist point of view is based on the 
</p>
<p>following postulates: 
</p>
<p>Fl Probability refers to limiting relative frequencies. Probabilities are ob-
</p>
<p>jective properties of the real world. 
</p>
<p>F2 Parameters are fixed, unknown constants. Because they are not fluctu-
</p>
<p>ating, no useful probability statements can be made about parameters. 
</p>
<p>F3 Statistical procedures should be designed to have well-defined long run 
</p>
<p>frequency properties. For example, a 95 percent confidence interval should 
</p>
<p>trap the true value of the parameter with limiting frequency at least 95 
</p>
<p>percent. 
</p>
<p>There is another approach to inference called Bayesian inference. The 
</p>
<p>Bayesian approach is based on the following postulates: </p>
<p/>
</div>
<div class="page"><p/>
<p>176 11. Bayesian Inference 
</p>
<p>B1 Probability describes degree of belief, not limiting frequency. As such, 
</p>
<p>we can make probability statements about lots of things, not just data 
</p>
<p>which are subject to random variation. For example, I might say that 
</p>
<p>"the probability that Albert Einstein drank a cup of tea on August 1, 
</p>
<p>1948" is .35. This does not refer to any limiting frequency. It reflects my 
</p>
<p>strength of belief that the proposition is true. 
</p>
<p>B2 We can make probability statements about parameters, even though 
</p>
<p>they are fixed constants. 
</p>
<p>B3 We make inferences about a parameter B by producing a probability 
</p>
<p>distribution for B. Inferences, such as point estimates and interval esti-
</p>
<p>mates, may then be extracted from this distribution. 
</p>
<p>Bayesian inference is a controversial approach because it inherently em-
</p>
<p>braces a subjective notion of probability. In general, Bayesian methods pro-
</p>
<p>vide no guarantees on long run performance. The field of statistics puts more 
</p>
<p>emphasis on frequentist methods although Bayesian methods certainly have 
</p>
<p>a presence. Certain data mining and machine learning communities seem to 
</p>
<p>embrace Bayesian methods very strongly. Let's put aside philosophical ar-
</p>
<p>guments for now and see how Bayesian inference is done. We'll conclude this 
</p>
<p>chapter with some discussion on the strengths and weaknesses of the Bayesian 
</p>
<p>approach. 
</p>
<p>11.2 The Bayesian Method 
</p>
<p>Bayesian inference is usually carried out in the following way. 
</p>
<p>1. We choose a probability density f( B) - called the prior distribution 
</p>
<p>- that expresses our beliefs about a parameter B before we see any 
</p>
<p>data. 
</p>
<p>2. We choose a statistical model f(xIB) that reflects our beliefs about x 
</p>
<p>given B. Notice that we now write this as f(xIB) instead of f(x; B). 
</p>
<p>3. After observing data X1, ... ,Xn , we update our beliefs and calculate 
</p>
<p>the posterior distribution f(BIX1 , ... , Xn). 
</p>
<p>To see how the third step is carried out, first suppose that B is discrete and 
</p>
<p>that there is a single, discrete observation X. We should use a capital letter </p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 The Bayesian Method 177 
</p>
<p>now to denote the parameter since we are treating it like a random variable, 
</p>
<p>so let 8 denote the parameter. Now, in this discrete setting, 
</p>
<p>J1D(8 = BIX = x) 
J1D(X = x, 8 = B) 
</p>
<p>J1D(X=x) 
</p>
<p>J1D(X = xl8 = B)JID(8 = B) 
</p>
<p>L&amp; J1D(X = xl8 = B)JID(8 = B) 
</p>
<p>which you may recognize from Chapter 1 as Bayes' theorem. The version 
</p>
<p>for continuous variables is obtained by using density functions: 
</p>
<p>. f(xIB)f(B) 
j(Blx) = I f(xIB)f(B)dB' (11.1) 
</p>
<p>If we have n IID observations Xl, ... , X n, we replace f (x I B) with 
</p>
<p>n 
</p>
<p>f(X1,"" xnlB) = II f(XiI B) = Ln(B). 
i=l 
</p>
<p>NOTATION. We will write xn to mean (Xl, ... , Xn) and xn to mean (Xl"'" xn). 
</p>
<p>Now, 
</p>
<p>(11.2) 
</p>
<p>where 
</p>
<p>(11.3) 
</p>
<p>is called the normalizing constant. Note that Cn does not depend on B. We 
</p>
<p>can summarize by writing: 
</p>
<p>Posterior is proportional to Likelihood times Prior 
</p>
<p>or, in symbols, 
</p>
<p>You might wonder, doesn't it cause a problem to throwaway the constant 
</p>
<p>cn ? The answer is that we can always recover the constant later if we need to. 
</p>
<p>What do we do with the posterior distribution? First, we can get a point 
</p>
<p>estimate by summarizing the center of the posterior. Typically, we use the 
</p>
<p>mean or mode of the posterior. The posterior mean is 
</p>
<p>(11.4) </p>
<p/>
</div>
<div class="page"><p/>
<p>178 11. Bayesian Inference 
</p>
<p>We can also obtain a Bayesian interval estimate. We find a and b such that 
</p>
<p>I~oo f(Blxn)dB = Iboo f(Blxn)dB = a/2. Let C = (a, b). Then 
</p>
<p>IP'(B E Clxn) = lb f(Blx n ) dB = 1 - a 
so C is a 1 - a posterior interval. 
</p>
<p>11.1 Example. Let Xl' ... ' Xn rv Bernoulli(p). Suppose we take the uniform 
</p>
<p>distribution f(p) = 1 as a prior. By Bayes' theorem, the posterior has the 
</p>
<p>form 
</p>
<p>where s = L~=l Xi is the number of successes. Recall that a random variable 
</p>
<p>has a Beta distribution with parameters a and (3 if its density is 
</p>
<p>( . ) _ f(a + (3) a-l( )13- 1 f p, a, (3 - r(a)f((3)P 1 - p . 
</p>
<p>We see that the posterior for p is a Beta distribution with parameters 8 + 1 
and n - 8 + 1. That is, 
</p>
<p>f(plxn) = f(n + 2) p(S+I)-l(l _ p)(n-s+1)-I. 
f(s+1)f(n-s+1) 
</p>
<p>We write this as 
</p>
<p>plxn rv Beta(s + 1, n - s + 1). 
</p>
<p>Notice that we have figured out the normalizing constant without actually 
</p>
<p>doing the integral I.L.n(p)f(p)dp. The mean of a Beta(a, (3) distribution is 
</p>
<p>a/(a + (3) so the Bayes estimator is 
</p>
<p>8+1 
'[5=--. 
</p>
<p>n+2 
</p>
<p>It is instructive to rewrite the estimator as 
</p>
<p>(11.5) 
</p>
<p>(11.6) 
</p>
<p>where p = sin is the MLE, is = 1/2 is the prior mean and .An = n/(n + 2) :::::; 1. 
A 95 percent posterior interval can be obtained by numerically finding a and 
</p>
<p>b such that J: f(plxn) dp = .95. 
Suppose that instead of a uniform prior, we use the prior p rv Beta(a, (3). 
</p>
<p>If you repeat the calculations above, you will see that plxn rv Beta( a + s, (3 + </p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 The Bayesian Method 179 
</p>
<p>n - s). The flat prior is just the special case with a = fJ = 1. The posterior 
mean is 
</p>
<p>p= a+s = ( n )p+ ( a+fJ )po 
a+fJ+n a+fJ+n a+fJ+n 
</p>
<p>where Po = a / (a + fJ) is the prior mean. _ 
</p>
<p>In the previous example, the prior was a Beta distribution and the posterior 
</p>
<p>was a Beta distribution. When the prior and the posterior are in the same 
</p>
<p>family, we say that the prior is conjugate with respect to the model. 
</p>
<p>11.2 Example. Let Xl"'" Xn rv N(B, (}2). For simplicity, let us assume that 
</p>
<p>() is known. Suppose we take as a prior B rv N(a, b2 ). In problem 1 in the 
</p>
<p>exercises it is shown that the posterior for B is 
</p>
<p>where 
</p>
<p>7J = wX + (1 - w)a, 
I 
</p>
<p>V! = --o-=se,--2--o-
I I ' 
</p>
<p>se2 + b2 
</p>
<p>(11.7) 
</p>
<p>and se = () / Vii is the standard error of the MLE X. This is another example 
of a conjugate prior. Note that 10 -+ 1 and T /se -+ 1 as n -+ 00. So, for large 
</p>
<p>n, the posterior is approximately NCe, se2 ). The same is true if n is fixed but 
b -+ 00, which corresponds to letting the prior become very flat. 
</p>
<p>Continuing with this example, let us find C = (c, d) such that IP'(B E 
</p>
<p>Clxn) = .95. We can do this by choosing c and d such that IP'(B &lt; clxn) = 
.025 and IP'( B &gt; dlxn) = .025. So, we want to find c such that 
</p>
<p>IP' ( B ~ 7J &lt; c ~ 7J I xn ) 
</p>
<p>( 
C - 7J) 
</p>
<p>IP' Z &lt; ~ = .025. 
</p>
<p>We know that IP'(Z &lt; -1.96) = .025. So, 
</p>
<p>c-7J 
-- = -1.96 
</p>
<p>T 
</p>
<p>implying that c = 7J-1.96T. By similar arguments, d = 7J+ 1.96. So a 95 percent 
Bayesian interval is 7J&plusmn;1.96T. Since 7J ~ Band T ~ se, the 95 percent Bayesian 
interval is approximated by B &plusmn; 1.96se which is the frequentist confidence 
interval. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>180 11. Bayesian Inference 
</p>
<p>11.3 Functions of Pararneters 
</p>
<p>How do we make inferences about a function T = g(e)7 Remember in Chapter 
</p>
<p>3 we solved the following problem: given the density fx for X, find the density 
for Y = g(X). We now simply apply the same reasoning. The posterior CDF 
</p>
<p>for T is 
</p>
<p>H(Tlxn) = lP'(g(e) ::; Tlxn) = i f(elxn)de 
where A = {e: g(e)::; T}. The posterior density is h(Tlxn) = H'(Tlxn). 
</p>
<p>11.3 Example. Let Xl"'" Xn rv Bernoulli(p) and f(p) = 1 so that plXn rv 
</p>
<p>Beta(s + 1, n - s + 1) with s = ~~=l Xi' Let 1jJ = 10g(p/(1 - p)). Then 
</p>
<p>H(1jJlxn) = IP'(W::; 1/)lxn) = IP' (lOg C ~ P ) ::; 1/) I xn) 
</p>
<p>and 
</p>
<p>for 1jJ E R &bull; 
</p>
<p>IP' (p &lt; ~ I xn) 
- 1 + eW 
</p>
<p>foe'!" /(1+e
4
') f(plxn) dp 
</p>
<p>r(n + 2) l e '!' /(1+e'!') 
pS(1- p)n-s dp 
</p>
<p>r(s+l)f(n-s+l) 0 
</p>
<p>11.4 Sirnulation 
</p>
<p>The posterior can often be approximated by simulation. Suppose we draw 
</p>
<p>el , ... , eE rv p(elxn). Then a histogram of el , ... , eE approximates the poste-
rior density p(elxn). An approximation to the posterior mean en = JE(elxn) is </p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Large Sample Properties of Bayes' Procedures 181 
</p>
<p>B-1 L:=l ej . The posterior I-a interval can be approximated by (ea / 2, e1- a / 2) 
where ea/ 2 is the a/2 sample quantile of e1, ... , eE . 
</p>
<p>Once we have a sample e1, ... , eE from J(elxn ), let Ti = g(ei ). Then 
</p>
<p>T1, ... , TE is a sample from J(Tlxn ). This avoids the need to do any analytical 
</p>
<p>calculations. Simulation is discussed in more detail in Chapter 24. 
</p>
<p>11.4 Example. Consider again Example 11.3. We can approximate the pos-
</p>
<p>terior for .1jJ without doing any calculus. Here are the steps: 
</p>
<p>1. Draw P1 , ... , PE rv Beta(s + 1, n - s + 1). 
</p>
<p>2. Let ~i = log(Pi/(1 - Pi)) for i = 1, ... , B. 
</p>
<p>Now ~l, ... ,VJE are IID draws from h(vJlxn). A histogram of these values 
</p>
<p>provides an estimate of h(~lxn) .&bull; 
</p>
<p>11.5 Large Sample Properties of Bayes' Procedures 
</p>
<p>In the Bernoulli and Normal examples we saw that the posterior mean was 
</p>
<p>close to the MLE. This is true in greater generality. 
</p>
<p>11.5 Theorem. LetEin be the MLE and letse = I/VnI(en). Under appropriate 
</p>
<p>regularity conditions, the posterior is approximately Normal with mean en and 
</p>
<p>standard deviationse. Hence, en:::::: en. Also, iJCn = (en-Za/2se,en+Za/2se) 
</p>
<p>is the asymptotic Jrequentist 1 - a confidence interval, then Cn is also an 
</p>
<p>approximate 1 - a Bayesian posterior interval: 
</p>
<p>There is also a Bayesian delta method. Let T = g( e). Then 
</p>
<p>Tlx n :::::: N(T, Se2 ) 
</p>
<p>where T = g(e) and Se = se 19' (e) I. 
</p>
<p>11.6 Flat Priors, Irnproper Priors, and 
</p>
<p>"N oninforrnative" Priors 
</p>
<p>An important question in Bayesian inference is: where does one get the prior 
</p>
<p>J(e)7 One school of thought, called subjectivism says that the prior should </p>
<p/>
</div>
<div class="page"><p/>
<p>182 11. Bayesian Inference 
</p>
<p>refiect our subjective opinion about e (before the data are collected). This may 
be possible in some cases but is impractical in complicated problems especially 
</p>
<p>if there are many parameters. Moreover, injecting subjective opinion into the 
</p>
<p>analysis is contrary to the goal of making scientific inference as objective 
</p>
<p>as possible. An alternative is to try to define some sort of "noninformative 
</p>
<p>prior." An obvious candidate for a noninformative prior is to use a fiat prior 
</p>
<p>f(e) ex constant. 
</p>
<p>In the Bernoulli example, taking f(p) = 1 leads to plXn rv Beta(05 + 1, n-
8 + 1) as we saw earlier, which seemed very reasonable. But unfettered use of 
fiat priors raises some questions. 
</p>
<p>IMPROPER PRIORS. Let X rv N(e, (J"2) with (J" known. Suppose we adopt 
</p>
<p>a fiat prior f( e) ex c where c &gt; 0 is a constant. Note that J f( e)de = 00 so 
this is not a probability density in the usual sense. We call such a prior an 
</p>
<p>improper prior. Nonetheless, we can still formally carry out Bayes' theorem 
</p>
<p>and compute the posterior density by multiplying the prior and the likelihood: 
</p>
<p>f(e) ex Ln(e)f(e) ex Ln(e). This gives elxn rv N(X, (J"2 In) and the resulting 
</p>
<p>point and interval estimators agree exactly with their frequentist counterparts. 
</p>
<p>In general, improper priors are not a problem as long as the resulting posterior 
</p>
<p>is a well-defined probability distribution. 
</p>
<p>FLAT PRIORS ARE NOT INVARIANT. Let X rv Bernoulli(p) and suppose we 
</p>
<p>use the fiat prior f(p) = 1. This fiat prior presumably represents our lack of 
</p>
<p>information about p before the experiment. Now let .1jJ = 10g(pI(1 - p)). This 
</p>
<p>is a transformation of p and we can compute the resulting distribution for VJ, 
namely, 
</p>
<p>which is not fiat. But if we are ignorant about p then we are also ignorant 
</p>
<p>about 1/J so we should use a fiat prior forljJ. This is a contradiction. In short, 
</p>
<p>the notion of a fiat prior is not well defined because a fiat prior on a parameter 
</p>
<p>does not imply a fiat prior on a transformed version of the parameter. Flat 
</p>
<p>priors are not transformation invariant. 
</p>
<p>JEFFREYS' PRIOR. Jeffreys came up with a rule for creating priors. The 
</p>
<p>rule is: take 
</p>
<p>f(e) ex J(e)1/2 
</p>
<p>where J( e) is the Fisher information function. This rule turns out to be trans-
</p>
<p>formation invariant. There are various reasons for thinking that this prior 
</p>
<p>might be a useful prior but we will not go into details here. </p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Multiparameter Problems 183 
</p>
<p>11.6 Example. Consider the Bernoulli (p) model. Recall that 
</p>
<p>1 
J(p) = p(l _ p) 
</p>
<p>Jeffreys' rule says to use the prior 
</p>
<p>This is a Beta (1/2,1/2) density. This is very close to a uniform density. _ 
</p>
<p>In a multiparameter problem, the Jeffreys' prior is defined to be f(8) ex 
</p>
<p>y'[I(B)I where IAI denotes the determinant of a matrix A and J(8) is the 
Fisher information matrix. 
</p>
<p>11.7 Multiparameter Problems 
</p>
<p>Suppose that 8 = (81 , ... , 8p ). The posterior density is still given by 
</p>
<p>(11.8) 
</p>
<p>The question now arises of how to extract inferences about one parameter. 
</p>
<p>The key is to find the marginal posterior density for the parameter of interest. 
</p>
<p>Suppose we want to make inferences about 81 . The marginal posterior for 81 
</p>
<p>is 
</p>
<p>(11.9) 
</p>
<p>In practice, it might not be feasible to do this integral. Simulation can help. 
</p>
<p>Draw randomly from the posterior: 
</p>
<p>where the superscripts index the different draws. Each 8j is a vector 8j 
</p>
<p>(8i, ... , 8~). Now collect together the first component of each draw: 
</p>
<p>These are a sample from f( 81 1xn) and we have avoided doing any integrals. 
</p>
<p>11.7 Example (Comparing Two Binomials). Suppose we have n1 control pa-
</p>
<p>tients and n2 treatment patients and that Xl control patients survive while 
</p>
<p>X 2 treatment patients survive. We want to estimate T = g(P1,P2) = P2 - Pl. 
</p>
<p>Then, </p>
<p/>
</div>
<div class="page"><p/>
<p>184 11. Bayesian Inference 
</p>
<p>If f (PI, P2) = 1, the posterior is 
</p>
<p>Notice that (Pl,P2) live on a rectangle (a square, actually) and that 
</p>
<p>where 
</p>
<p>which implies that PI and P2 are independent under the posterior. Also, 
</p>
<p>Pllxl rv Beta(xl + 1,711 - Xl + 1) and P21x2 rv Beta(:c2 + 1,712 - X2 + 1). 
If we simulate Pl,l,"" Pl,E rv Beta(xl + 1,711 - Xl + 1) and P2,1,"" P2,E rv 
Beta(x2 + 1, 712 - X2 + 1), then Tb = P2,b - Pl,b, b = 1, ... , B, is a sample from 
f(Tl x l,X2) .&bull; 
</p>
<p>11.8 Bayesian Testing 
</p>
<p>Hypothesis testing from a Bayesian point of view is a complex topic. We 
</p>
<p>will only give a brief sketch of the main idea here. The Bayesian approach 
</p>
<p>to testing involves putting a prior on Ho and on the parameter 8 and then 
</p>
<p>computing IP'(Holxn). Consider the case where 8 is scalar and we are testing 
</p>
<p>Ho : 8 = 80 versus HI: 8 -=I- 80 . 
</p>
<p>It is usually reasonable to use the prior IP'(Ho) = IP'(Hd = 1/2 (although this 
</p>
<p>is not essential in what follows). Under HI we need a prior for 8. Denote this 
</p>
<p>prior density by f( 8). From Bayes' theorem 
</p>
<p>f(xnIHo)lP'(Ho) + f(xnIHdlP'(Hl) 
~f(xn I 80 ) 
</p>
<p>V(x n I 80 ) + V(xn I Hd 
f(x n I 80 ) 
</p>
<p>f(x n I 80 ) + J f(x n I 8)f(8)d8 
&pound;(80 ) 
</p>
<p>&pound;(80 ) + J &pound;(8)f(8)d8' 
</p>
<p>We saw that, in estimation problems, the prior was not very influential and 
</p>
<p>that the frequentist and Bayesian methods gave similar answers. This is not </p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 Strengths and Weaknesses of Bayesian Inference 185 
</p>
<p>the case in hypothesis testing. Also, one can't use improper priors in testing 
</p>
<p>because this leads to an undefined constant in the denominator of the expres-
</p>
<p>sion above. Thus, if you use Bayesian testing you must choose the prior j (8) 
</p>
<p>very carefully. It is possible to get a prior-free bound on IP'(HoIXn = xn). 
</p>
<p>Notice that 0 &lt;::: I &pound;(8)j(8)d8 &lt;::: &pound;(e). Hence, 
</p>
<p>&pound;(80 ) ~ &lt;::: IP'(Holxn = xn) &lt; l. 
&pound;(80 ) + &pound;(8) 
</p>
<p>The upper bound is not very interesting, but the lower bound is non-trivial. 
</p>
<p>11.9 Strengths and Weaknesses of Bayesian Inference 
</p>
<p>Bayesian inference is appealing when prior information is available since Bayes' 
</p>
<p>theorem is a natural way to combine prior information with data. Some peo-
</p>
<p>ple find Bayesian inference psychologically appealing because it allows us to 
</p>
<p>make probability statements about parameters. In contrast, frequentist infer-
</p>
<p>ence provides confidence sets en which trap the parameter 95 percent of the 
</p>
<p>time, but we cannot say that IP'( 8 E en Ixn) is .95. In the frequentist approach 
</p>
<p>we can make probability statements about en, not 8. However, psychological 
</p>
<p>appeal is not a compelling scientific argument for using one type of inference 
</p>
<p>over another. 
</p>
<p>In parametric models, with large samples, Bayesian and frequentist methods 
</p>
<p>give approximately the same inferences. In general, they need not agree. 
</p>
<p>Here are three examples that illustrate the strengths and weakness of Bayesian 
</p>
<p>inference. The first example is Example 6.14 revisited. This example shows 
</p>
<p>the psychological appeal of Bayesian inference. The second and third show 
</p>
<p>that Bayesian methods can fail. 
</p>
<p>11.8 Example (Example 6.14 revisited). We begin by reviewing the example. 
</p>
<p>Let 8 be a fixed, known real number and let Xl, X 2 be independent random 
</p>
<p>variables such that IP'(Xi = 1) = IP'(Xi = -1) = 1/2. Now define Y; = 8 + Xi 
and suppose that you only observe Yl and Y2 . Let 
</p>
<p>ifYl =Y2 
</p>
<p>if Yl -I Y2 . 
</p>
<p>This is a 75 percent confidence set since, no matter what 8 is, lP'e(8 E e) = 3/4. 
</p>
<p>Suppose we observe Yl = 15 and Y2 = 17. Then our 75 percent confidence 
</p>
<p>interval is {16}. However, we are certain, in this case, that 8 = 16. So calling </p>
<p/>
</div>
<div class="page"><p/>
<p>186 11. Bayesian Inference 
</p>
<p>this a 75 percent confidence set, bothers many people. Nonetheless, e is a 
valid 75 percent confidence set. It will trap the true value 75 percent of the 
</p>
<p>time. 
</p>
<p>The Bayesian solution is more satisfying to many. For simplicity, assume 
</p>
<p>that e is an integer. Let f (e) be a prior mass function such that f (e) &gt; 0 for 
every integer e. When Y = (Y1 , Y2 ) = (15,17), the likelihood function is 
</p>
<p>&pound;(e) = { ~/4 e = 16 
otherwise. 
</p>
<p>Applying Bayes' theorem we see that 
</p>
<p>J1D(8 = elY = (15,17)) = { ~ e = 16 
otherwise. 
</p>
<p>Hence, J1D(e E elY = (15,17)) = 1. There is nothing wrong with saying that 
{16} is a 75 percent confidence interval. But is it not a probability statement 
</p>
<p>about e .&bull; 
</p>
<p>11.9 Example. This is a simplified version of the example in Robins and Ritov 
</p>
<p>(1997). The data consist of n IID triples 
</p>
<p>Let B be a finite but very large number, like B = 100100. Any realistic sample 
</p>
<p>size n will be small compared to B. Let 
</p>
<p>be a vector of unknown parameters such that 0 ::.; ej ::.; 1 for 1 ::.; j ::.; B. Let 
</p>
<p>be a vector of known numbers such that 
</p>
<p>0&lt; 5 ::.; ~j ::.; 1 - 5 &lt; 1, 1::'; j ::.; B, 
</p>
<p>where 5 is some, small, positive number. Each data point (Xi, R i , Yi) is drawn 
</p>
<p>in the following way: 
</p>
<p>1. Draw Xi uniformly from {1, ... , B}. 
</p>
<p>2. Draw Ri rv Bernoulli(~xJ. 
</p>
<p>3. If Ri = 1, then draw Yi rv Bernoulli(ex ;). If Ri = 0, do not draw Yi. </p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 Strengths and Weaknesses of Bayesian Inference 187 
</p>
<p>The model may seem a little artificial but, in fact, it is caricature of some 
</p>
<p>real missing data problems in which some data points are not observed. In 
</p>
<p>this example, Ri = 0 can be thought of as meaning "missing." Our goal is to 
</p>
<p>estimate 
</p>
<p>VJ = J1D(li = 1). 
</p>
<p>Note that 
</p>
<p>B 
</p>
<p>'1jJ J1D(li = 1) = LJID(li = 11X = j)JID(X = j) 
j=l 
</p>
<p>so ?jJ = g(e) is a function of e. 
</p>
<p>Let us consider a Bayesian analysis first. The likelihood of a single obser-
</p>
<p>vation is 
</p>
<p>!(Xi , Ri , li) = !(Xi)!(RiIXi)!(liIXi)Ri. 
</p>
<p>The last term is raised to the power Ri since, if Ri = 0, then li is not observed 
</p>
<p>and hence that term drops out of the likelihood. Since! (Xi) = 1/ B and that 
</p>
<p>li and Ri are Bernoulli, 
</p>
<p>Thus, the likelihood function is 
</p>
<p>n 
</p>
<p>i=l 
</p>
<p>n 1 
</p>
<p>II - CRi. (1 - c .)l-Ri eYi .Ri (1 - e .)(l-Yi )Ri B &lt;,x i &lt;,X, Xi X,. 
i=l 
</p>
<p>ex eiiRi (1 - ex J(1-Yi )Ri . 
</p>
<p>We have dropped all the terms involving B and the ej'S since these are known 
constants, not parameters. The log-likelihood is 
</p>
<p>n 
</p>
<p>i=l 
</p>
<p>B B 
</p>
<p>L nj logej + L mj 10g(1 - ej ) 
j=l j=l </p>
<p/>
</div>
<div class="page"><p/>
<p>188 11. Bayesian Inference 
</p>
<p>where 
</p>
<p>Tlj #{i: Yi = 1,Ri = 1,Xi = j} 
</p>
<p>mj #{i: Yi = O,Ri = 1,Xi = j}. 
</p>
<p>Now, nj = mj = 0 for most j since B is so much larger than TI. This has 
</p>
<p>several implications. First, the MLE for most ej is not defined. Second, for 
most ej , the posterior distribution is equal to the prior distribution, since 
those ej do not appear in the likelihood. Hence, f(eIData) :::::: f(e). It follows 
that f(1j)IData) :::::: f(1jJ). In other words, the data provide little information 
</p>
<p>about 1jJ in a Bayesian analysis. 
</p>
<p>Now we consider a frequentist solution. Define 
</p>
<p>~ l~R~ 
tjJ=_~_2_'. 
</p>
<p>n i=l ex, 
(11.10) 
</p>
<p>We will now show that this estimator is unbiased and has small mean-squared 
</p>
<p>error. It can be shown (see Exercise 7) that 
</p>
<p>and 
~ 1 
</p>
<p>V( 1jJ) ~ ---:c2. 
Tlu 
</p>
<p>(11.11) 
</p>
<p>Therefore, the MSE is of order l/n which goes to 0 fairly quickly as we collect 
</p>
<p>more data, no matter how large B is. The estimator defined in (11.10) is called 
</p>
<p>the Horwitz-Thompson estimator. It cannot be derived from a Bayesian or 
</p>
<p>likelihood point of view since it involves the terms ex,. These terms drop 
out of the log-likelihood and hence will not show up in any likelihood-based 
</p>
<p>method including Bayesian estimators. 
</p>
<p>The moral of the story is this. Bayesian methods are tied to the likeli-
</p>
<p>hood function. But in high dimensional (and nonparametric) problems, the 
</p>
<p>likelihood may not yield accurate inferences. _ 
</p>
<p>11.10 Example. Suppose that f is a probability density function and that 
</p>
<p>f(x) = cg(x) 
</p>
<p>where g(x) &gt; 0 is a known function and c is unknown. In principle we can 
compute c since J f(x) dx = 1 implies that c = 1/ J g(x) dx. But in many cases 
we can't do the integral J g(x) dx since 9 might be a complicated function and 
x could be high dimensional. Despite the fact that c is not known, it is often 
</p>
<p>possible to draw a sample Xl' ... ' Xn from f; see Chapter 24. Can we use the 
</p>
<p>sample to estimate the normalizing constant c? Here is a frequentist solution: </p>
<p/>
</div>
<div class="page"><p/>
<p>11.10 Bibliographic Remarks 189 
</p>
<p>Let in (x) be a consistent estimate of the density f. Chapter 20 explains how to 
construct such an estimate. Choose any point x and note that C = f(x)/g(x). 
</p>
<p>Hence, c= !(x)/g(x) is a consistent estimate of c. Now let us try to solve this 
problem from a Bayesian approach. Let 7f(c) be a prior such that 7f(c) &gt; 0 for 
</p>
<p>all C &gt; O. The likelihood function is 
</p>
<p>n n n 
</p>
<p>i=l i=l i=l 
</p>
<p>Hence the posterior is proportional to Cn 7f( c). The posterior does not depend 
</p>
<p>on Xl"'" X n , so we come to the startling conclusion that, from the Bayesian 
</p>
<p>point of view, there is no information in the data about c. Moreover, the 
</p>
<p>posterior mean is 
</p>
<p>1000 cn + l 7f(C) dc 
1000 Cn 7f( c) dc 
</p>
<p>which tends to infinity as n increases. _ 
</p>
<p>These last two examples illustrate an important point. Bayesians are slaves 
</p>
<p>to the likelihood function. When the likelihood goes awry, so will Bayesian 
</p>
<p>inference. 
</p>
<p>What should we conclude from all this? The important thing is to under-
</p>
<p>stand that frequentist and Bayesian methods are answering different ques-
</p>
<p>tions. To combine prior beliefs with data in a principled way, use Bayesian in-
</p>
<p>ference. To construct procedures with guaranteed long run performance, such 
</p>
<p>as confidence intervals, use frequentist methods. Generally, Bayesian methods 
</p>
<p>run into problems when the parameter space is high dimensional. In particu-
</p>
<p>lar, 95 percent posterior intervals need not contain the true value 95 percent 
</p>
<p>of the time (in the frequency sense). 
</p>
<p>11.10 Bibliographic Rernarks 
</p>
<p>Some references on Bayesian inference include Carlin and Louis (1996), Gel-
</p>
<p>man et al. (1995), Lee (1997), Robert (1994), and Schervish (1995). See Cox 
</p>
<p>(1993), Diaconis and Freedman (1986), Freedman (1999), Barron et al. (1999), 
</p>
<p>Ghosal et al. (2000), Shen and Wasserman (2001), and Zhao (2000) for discus-
</p>
<p>sions of some of the technicalities of nonparametric Bayesian inference. The 
</p>
<p>Robins-Ritov example is discussed in detail in Robins and Ritov (1997) where 
</p>
<p>it is cast more properly as a nonparametric problem. Example 11.10 is due to 
</p>
<p>Edward George (personal communication). See Berger and Delampady (1987) </p>
<p/>
</div>
<div class="page"><p/>
<p>190 11. Bayesian Inference 
</p>
<p>and Kass and Raftery (1995) for a discussion of Bayesian testing. See Kass 
</p>
<p>and Wasserman (1996) for a discussion of noninformative priors. 
</p>
<p>11.11 Appendix 
</p>
<p>Proof of Theorem 11.5. 
</p>
<p>It can be shown that the effect of the prior diminishes as n increases so 
</p>
<p>that f(8Ixn) ex: Ln(8)f(8) ::::::: Ln(8). Hence, logf(8Ixn) ::::::: &pound;(8). Now, &pound;(8) ::::::: 
</p>
<p>&pound;(e) + (8 - 8)&pound;'(e) + [(8 - 8)2/2]&pound;1/(8) = &pound;(8) + [(8 - 8)2/2]&pound;"(8) since &pound;'(8) = O. 
Exponentiating, we get approximately that 
</p>
<p>where ()~ = -1/R"(8n ). So the posterior of 8 is approximately Normal with 
</p>
<p>mean 8 and variance ()~. Let &pound;i = log f(XiI8), then 
</p>
<p>1 
</p>
<p>()~ 
</p>
<p>and hence (}n ::::::: se(8). -
</p>
<p>11.12 Exercises 
</p>
<p>1. Verify (11.7). 
</p>
<p>-&pound;"(en ) = L -&pound;;'(8n ) 
i 
</p>
<p>n (~) L -e;'(8n )::::::: nlEe [-e;'(8n )] 
2 
</p>
<p>2. Let Xl, ... , Xn rv Normal(,t, 1). 
</p>
<p>(a) Simulate a data set (using fL = 5) consisting of n=lQO observations. 
</p>
<p>(b) Take f (,t) = 1 and find the posterior density. Plot the density. 
</p>
<p>(c) Simulate 1,000 draws from the posterior. Plot a histogram of the 
</p>
<p>simulated values and compare the histogram to the answer in (b). 
</p>
<p>(d) Let 8 = elL. Find the posterior density for 8 analytically and by 
</p>
<p>simulation. 
</p>
<p>(e) Find a 95 percent posterior interval for IL. 
</p>
<p>(f) Find a 95 percent confidence interval for 8. </p>
<p/>
</div>
<div class="page"><p/>
<p>11.12 Exercises 191 
</p>
<p>3. Let Xl, ... , Xn rv Uniform(O, e). Let 1(e) ex l/e. Find the posterior 
density. 
</p>
<p>4. Suppose that 50 people are given a placebo and 50 are given a new 
</p>
<p>treatment. 30 placebo patients show improvement while 40 treated pa-
</p>
<p>tients show improvement. Let T = P2 - PI where P2 is the probability of 
</p>
<p>improving under treatment and PI is the probability of improving under 
</p>
<p>placebo. 
</p>
<p>(a) Find the MLE of T. Find the standard error and 90 percent confidence 
</p>
<p>interval using the delta method. 
</p>
<p>(b) Find the standard error and 90 percent confidence interval using the 
</p>
<p>parametric bootstrap. 
</p>
<p>( c) Use the prior 1 (PI, P2) = 1. Use simulation to find the posterior 
mean and posterior 90 percent interval for T. 
</p>
<p>(d) Let 
</p>
<p>1jJ = log ((~) -'- (~)) 
1 - PI . 1 - P2 
</p>
<p>be the log-odds ratio. Note that VJ = 0 if PI = P2. Find the MLE of 1jJ. 
</p>
<p>Use the delta method to find a 90 percent confidence interval for 1jJ. 
</p>
<p>( e) Use simulation to find the posterior mean and posterior 90 percent 
</p>
<p>interval for 1jJ. 
</p>
<p>5. Consider the Bernoulli(p) observations 
</p>
<p>0101000000 
</p>
<p>Plot the posterior for P using these priors: Beta(1/2,1/2), Beta(l,l), 
</p>
<p>Beta(lO,lO), Beta(100,100). 
</p>
<p>6. Let Xl,"" Xn rv Poisson('\). 
</p>
<p>(a) Let ,\ rv Gamma(a, p) be the prior. Show that the posterior is also 
</p>
<p>a Gamma. Find the posterior mean. 
</p>
<p>(b) Find the Jeffreys' prior. Find the posterior. 
</p>
<p>7. In Example 11.9, verify (11.11). 
</p>
<p>8. Let X rv N(JL, 1). Consider testing 
</p>
<p>Ho : JL = 0 versus HI: JL cJ O. </p>
<p/>
</div>
<div class="page"><p/>
<p>192 11. Bayesian Inference 
</p>
<p>Take P(Ho) = P(HI) = 1/2. Let the prior for /L under HI be It rv 
</p>
<p>N(O, b2 ). Find an expression for P(HoIX = x). Compare P(HoIX = x) 
</p>
<p>to the p-value of the Wald test. Do the comparison numerically for a 
</p>
<p>variety of values of x and b. Now repeat the problem using a sample of 
</p>
<p>size n. You will see that the posterior probability of Ho can be large even 
</p>
<p>when the p-value is small, especially when n is large. This disagreement 
</p>
<p>between Bayesian and frequentist testing is called the Jeffreys-Lindley 
</p>
<p>paradox. </p>
<p/>
</div>
<div class="page"><p/>
<p>12 
</p>
<p>Statistical Decision Theory 
</p>
<p>12.1 Prelirninaries 
</p>
<p>We have considered several point estimators such as the maximum likelihood 
</p>
<p>estimator, the method of moments estimator, and the posterior mean. In fact, 
</p>
<p>there are many other ways to generate estimators. How do we choose among 
</p>
<p>them? The answer is found in decision theory which is a formal theory for 
</p>
<p>comparing statistical procedures. 
</p>
<p>Consider a parameter 8 which lives in a parameter space 8. Let 8 be an 
</p>
<p>estimator of 8. In the language of decision theory, an estimator is sometimes 
</p>
<p>called a decision rule and the possible values of the decision rule are called 
</p>
<p>actions. 
</p>
<p>We shall measure the discrepancy between 8 and 8 using a loss function 
</p>
<p>L(8,8). Formally, L maps 8 x 8 into R Here are some examples of loss 
</p>
<p>functions: 
</p>
<p>L(8, 8) = (8 - 8)2 
</p>
<p>L(8,8)=18-81 
</p>
<p>L(8, 8) = 18 - 81 P 
</p>
<p>L(8, 8) = 0 if 8 = 8 or 1 if 8 i- 8 
L(8, 8) = flog (f((X;~))) f(x; 8)dx 
</p>
<p>f X,e 
</p>
<p>squared error loss, 
</p>
<p>absolute error loss, 
</p>
<p>Lp loss, 
</p>
<p>zero-one loss, 
</p>
<p>Kullback-Leibler loss. </p>
<p/>
</div>
<div class="page"><p/>
<p>194 12. Statistical Decision Theory 
</p>
<p>Bear in mind in what follows that an estimator 8 is a function of the data. 
</p>
<p>To emphasize this point, sometimes we will write B as B(X). To assess an 
</p>
<p>estimator, we evaluate the average loss or risk. 
</p>
<p>12.1 Definition. The risk of an estimator 8 is 
</p>
<p>R(8, B) = lEe (L(8, B) ) = / L(8, B(x))f(x; 8)dx. 
</p>
<p>When the loss function is squared error, the risk is just the MSE (mean 
</p>
<p>squared error): 
</p>
<p>In the rest of the chapter, if we do not state what loss function we are using, 
</p>
<p>assume the loss function is squared error. 
</p>
<p>12.2 Comparing Risk Functions 
</p>
<p>To compare two estimators we can compare their risk functions. However, this 
</p>
<p>does not provide a clear answer as to which estimator is better. Consider the 
</p>
<p>following examples. 
</p>
<p>12.2 Example. Let X rv N(8,1) and assume we are using squared error 
- -
</p>
<p>loss. Consider two estimators: 81 = X and 82 = 3. The risk functions are 
</p>
<p>R(8, BI) = lEe (X - 8)2 = 1 and R(8, ( 2 ) = lEe(3 - 8)2 = (3 - 8)2. If 2 &lt; 8 &lt; 4 
then R(8,~) &lt; R(8, ~), otherwise, R(8, BI) &lt; R(8, (2)' Neither estimator 
uniformly dominates the other; see Figure 12.1. &bull; 
</p>
<p>12.3 Example. Let Xl, ... , Xn rv Bernoulli(p). Consider squared error loss 
</p>
<p>and let PI = X. Since this has 0 bias, we have that 
</p>
<p>R(p,pI) = V(X) = p(l - p). 
n 
</p>
<p>Another estimator is 
_ Y +a 
P2 = a + fJ + n 
</p>
<p>where Y = L~=l Xi and a and fJ are positive constants. This is the posterior 
mean using a Beta (a, fJ) prior. Now, </p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Comparing Risk Functions 195 
</p>
<p>3 
</p>
<p>2 
</p>
<p>1 
</p>
<p>o 
o 1 2 3 4 5 
</p>
<p>FIGURE 12.1. Comparing two risk functions. Neither risk function dominates the 
</p>
<p>other at all values of B. 
</p>
<p>v p (a: ; : n) + ( Ep (a: ; : n) _ p) 2 
2 
</p>
<p>np(l-p) (np+a ) -,-------'------'--,:- + - p 
(a+;B+n)2 a+;B+n 
</p>
<p>Let a = ;B = foJ4. (In Example 12.12 we will explain this choice.) The 
resulting estimator is 
</p>
<p>~ Y+f0J4 
P2 = 
</p>
<p>n+vn 
and the risk function is 
</p>
<p>R(p,fh) = 4(n +n vn)2. 
</p>
<p>The risk functions are plotted in figure 12.2. As we can see, neither estimator 
</p>
<p>uniformly dominates the other. 
</p>
<p>These examples highlight the need to be able to compare risk functions. 
</p>
<p>To do so, we need a one-number summary of the risk function. Two such 
</p>
<p>summaries are the maximum risk and the Bayes risk. 
</p>
<p>12.4 Definition. The maximum risk is 
</p>
<p>R(B) = sup R( (), B) (12.1) 
() 
</p>
<p>and the Bayes risk is 
</p>
<p>r(j, B) = / R((), B)f(())d() (12.2) 
</p>
<p>where f (()) is a prior for (). </p>
<p/>
</div>
<div class="page"><p/>
<p>196 12. Statistical Decision Theory 
</p>
<p>P 
</p>
<p>FIGURE 12.2. Risk functions for fh and fh in Example 12.3. The solid curve is 
R(fh). The dotted line is R(fh). 
</p>
<p>12.5 Example. Consider again the two estimators in Example 12.3. We have 
</p>
<p>R( ~ ) p(1- p) PI = max 
O&lt;:::p&lt;:::l n 
</p>
<p>and 
</p>
<p>1 
</p>
<p>4n 
</p>
<p>n 
</p>
<p>4(n+ fo)2&middot; 
</p>
<p>Based on maximum risk, P2 is a better estimator since R(P2) &lt; R(Pl). How-
</p>
<p>ever, when n is large, R(Pl) has smaller risk except for a small region in the 
</p>
<p>parameter space near P = 1/2. Thus, many people prefer PI to P2. This il-
lustrates that one-number summaries like maximum risk are imperfect. Now 
</p>
<p>consider the Bayes risk. For illustration, let us take j(p) = l. Then 
</p>
<p>( ~) J ( ~) J p(l - p) 1 r- j, PI = R p, PI dp = dp = -
n 6n 
</p>
<p>and 
</p>
<p>r-(j,P2) = J R(P,P2)dp = 4(n +n fo)2&middot; 
For n ~ 20, r-(j,P2) &gt; r-(j,fiI) which suggests that PI is a better estimator. 
This might seem intuitively reasonable but this answer depends on the choice 
</p>
<p>of prior. The advantage of using maximum risk, despite its problems, is that 
</p>
<p>it does not require one to choose a prior. _ 
</p>
<p>These two summaries of the risk function suggest two different methods 
</p>
<p>for devising estimators: choosing e to minimize the maximum risk leads to </p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Bayes Estimators 197 
</p>
<p>minimax estimators; choosing e to minimize the Bayes risk leads to Bayes 
estimators. 
</p>
<p>12.6 Definition. A decision rule that minimizes the Bayes risk is called a 
</p>
<p>Bayes rule. Formally, e is a Bayes rule with respect to the prior f if 
</p>
<p>r(f, e) = i12f r(f, e) (12.3) 
e 
</p>
<p>where the infimum is over all estimators e. An estimator that minimizes 
</p>
<p>the maximum risk is called a minimax rule. Formally, e is minimax if 
</p>
<p>sup R( e, e) = i12f sup R( e, e) (12.4) 
e e e 
</p>
<p>where the infimum is over all estimators e. 
</p>
<p>12.3 Bayes Estimators 
</p>
<p>Let f be a prior. From Bayes' theorem, the posterior density is 
</p>
<p>f(elx) = f(xle)f(e) = f(xle)f(e) (12.5) 
rn(x) J f(xle)f(e)de 
</p>
<p>where rn(x) = J f(x, e)de = J f(xle)f(e)de is the marginal distribution of 
X. Define the posterior risk of an estimator e( x) by 
</p>
<p>r(elx) = 1 L(e, e(x))f(elx)de. 
12.7 Theorem. The Bayes risk r(f, e) satisfies 
</p>
<p>r(f, e) = 1 r(elx)rn(x) dx. 
</p>
<p>(12.6) 
</p>
<p>Let B(x) be the value of e that minimizes r(elx). Then e is the Bayes estimator. 
</p>
<p>PROOF. We can rewrite the Bayes risk as follows: 
</p>
<p>r(f, e) = 1 R(e, e)f(e)de = 1 (1 L(e, e(x))f(X1e)dx) f(e)de 
1 1 L(e, e(x))f(x, e)dxde = 1 1 L(e, e(x))f(elx)rn(x)dxde 
1 (1 L(e, e(x))f(eIX)de) rn(x) dx = 1 r(elx)rn(x) dx. </p>
<p/>
</div>
<div class="page"><p/>
<p>198 12. Statistical Decision Theory 
</p>
<p>If we choose 8(x) to be the value of 8 that minimizes r(8Ix) then we will mini-
</p>
<p>mize the integrand at every x and thus minimize the integral J r(8Ix)m(x)dx . 
</p>
<p>&bull; 
Now we can find an explicit formula for the Bayes estimator for some specific 
</p>
<p>loss functions. 
</p>
<p>12.8 Theorem. If L(8, 8) = (8 - 8)2 then the Bayes estimator is 
</p>
<p>8(x) = J 8f(8Ix)d8 = IE(8IX = x). (12.7) 
If L( 8,8) 18 - 81 then the Bayes estimator is the median of the posterior 
f( 8Ix). If L(8, 8) is zero-one loss, then the Bayes estimator is the mode of the 
posterior f(8Ix). 
</p>
<p>PROOF. We will prove the theorem for squared error loss. The Bayes rule 
</p>
<p>8(x) minimizes r(8Ix) = J(8 - 8(X))2 f(8Ix)d8. Taking the derivative of r(8Ix) 
</p>
<p>with respect to 8(x) and setting it equal to 0 yields the equation 2 J(8 -
</p>
<p>8(x))f(8Ix)d8 = o. Solving for 8(x) we get 12.7 .&bull; 
</p>
<p>12.9 Example. Let Xl, ... , Xn rv N(,l, (}2) where (}2 is known. Suppose we 
</p>
<p>use a N(a, b2 ) prior for Il. The Bayes estimator with respect to squared error 
</p>
<p>loss is the posterior mean, which is 
</p>
<p>&bull; 
</p>
<p>12.4 Minimax Rules 
</p>
<p>Finding minimax rules is complicated and we cannot attempt a complete 
</p>
<p>coverage of that theory here but we will mention a few key results. The main 
</p>
<p>message to take away from this section is: Bayes estimators with a constant 
</p>
<p>risk function are minimax. 
</p>
<p>12.10 Theorem. Let (jJ be the Bayes rule for some prior f: 
</p>
<p>Suppose that 
</p>
<p>r(f,(ff) = illfr(f,8). 
e 
</p>
<p>R(8, 8f ) :::; r(j, 8f ) for all 8. 
</p>
<p>Then 8f is minimax and f is called a least favorable prior. 
</p>
<p>(12.8) 
</p>
<p>(12.9) </p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Minimax Rules 199 
</p>
<p>PROOF. Suppose that e! is not minimax. Then there is another rule eo such 
</p>
<p>that sUPe R(e,~) &lt; sUPe R(e, e!). Since the average of a function is always 
</p>
<p>less than or equal to its maximum, we have that r(j, eo) &lt;::: sUPe R(e, eo). 
</p>
<p>Hence, 
</p>
<p>r(j,eo) &lt;::: supR(e,eo) &lt; supR(e,e!) &lt;::: r(j,e!) 
e e 
</p>
<p>which contradicts (12.8). _ 
</p>
<p>12.11 Theorem. Suppose that e is the Bayes rule with respect to some 
prior f. Suppose further that e has constant risk: R( e, e) = c for some c. 
Then e is minimax. 
</p>
<p>PROOF. The Bayes risk is r(f, e) = J R(e, e)f(e)de = c and hence R(e, e) &lt;::: 
r(j, e) for all e. Now apply the previous theorem. _ 
</p>
<p>12.12 Example. Consider the Bernoulli model with squared error loss. In 
</p>
<p>example 12.3 we showed that the estimator 
</p>
<p>has a constant risk function. This estimator is the posterior mean, and hence 
</p>
<p>the Bayes rule, for the prior Beta(a, p) with a = I' = In/4. Hence, by the 
previous theorem, this estimator is minimax. _ 
</p>
<p>12.13 Example. Consider again the Bernoulli but with loss function 
</p>
<p>L( ~) = (p - p)2 
p,p p(l - p) 
</p>
<p>Let 
</p>
<p>The risk is 
</p>
<p>R(p,p) = E ((p - p)2) = 1 (P(l- p)) = .!. 
p(l-p) p(1-p) n n 
</p>
<p>which, as a function of p, is constant. It can be shown that, for this loss 
</p>
<p>function, p(xn) is the Bayes estimator under the prior f(p) = 1. Hence, pis 
minimax. _ 
</p>
<p>A natural question to ask is: what is the minimax estimator for a Normal 
</p>
<p>model? </p>
<p/>
</div>
<div class="page"><p/>
<p>200 12. Statistical Decision Theory 
</p>
<p>e 
-0.5 o 0.5 
</p>
<p>FIGURE 12.3. Risk function for constrained Normal with m=.S. The two short 
</p>
<p>dashed lines show the least favorable prior which puts its mass at two points. 
</p>
<p>12.14 Theorem. Let Xl' ... ' Xn '" N(e, 1) and let B = X. Then e is minimax 
with respect to any well-behaved loss function. 1 It is the only estimator with 
</p>
<p>this property. 
</p>
<p>If the parameter space is restricted, then the theorem above does not apply 
</p>
<p>as the next example shows. 
</p>
<p>12.15 Example. Suppose that X '" N(e, 1) and that e is known to lie in the 
</p>
<p>interval [-m, m] where 0 &lt; m &lt; 1. The unique, minimax estimator under 
</p>
<p>squared error loss is 
</p>
<p>B(X) = m tanh(mX) 
</p>
<p>where tanh(z) = (eZ - e-Z)/(eZ + e-Z). It can be shown that this is the Bayes 
rule with respect to the prior that puts mass 1/2 at m and mass 1/2 at -m. 
</p>
<p>Moreover, it can be shown that the risk is not constant but it does satisfy 
</p>
<p>R(e, B) ~ r(J, B) for all e; see Figure 12.3. Hence, Theorem 12.10 implies that 
e is minimax. _ 
</p>
<p>1 "Well-behaved" means that the level sets must be convex and symmetric about the origin. 
</p>
<p>The result holds up to sets of measure o. </p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Maximum Likelihood, Minimax, and Bayes 201 
</p>
<p>12.5 Maxirnurn Likelihood, Minimax, and Bayes 
</p>
<p>For parametric models that satisfy weak regularity conditions, the maximum 
</p>
<p>likelihood estimator is approximately minimax. Consider squared error loss 
</p>
<p>which is squared bias plus variance. In parametric models with large samples, 
</p>
<p>it can be shown that the variance term dominates the bias so the risk of the 
</p>
<p>MLE B roughly equals the variance:2 
</p>
<p>As we saw in Chapter 9, the variance of the MLE is approximately 
</p>
<p>~ 1 
V(e) R::: nJ(e) 
</p>
<p>where J(e) is the Fisher information. Hence, 
</p>
<p>~ 1 
nR(e, e) R::: J(e)" (12.10) 
</p>
<p>For any other estimator e', it can be shown that for large n, R(e, e') ~ R(e, B). 
</p>
<p>More precisely, 
~ 1 
</p>
<p>limlimsup sup nR(e',e) ~ -(e)' 
E-+O n-+oo le-e'I&lt;E J 
</p>
<p>(12.11) 
</p>
<p>This says that, in a local, large sample sense, the MLE is minimax. It can also 
</p>
<p>be shown that the MLE is approximately the Bayes rule. 
</p>
<p>In summary: 
</p>
<p>In most parametric models, with large samples, the MLE is approxi-
</p>
<p>mately minimax and Bayes. 
</p>
<p>There is a caveat: these results break down when the number of parameters 
</p>
<p>is large as the next example shows. 
</p>
<p>12.16 Example (Many Normal means). Let Yi rv N(e i , (J2/n ), i = 1, ... ,n. 
Let Y = (Yl , ... , Yn) denote the data and let e = (e l , ... , en) denote the 
</p>
<p>unknown parameters. Assume that 
</p>
<p>2Typically, the squared bias is order O(n-2) while the variance is of order O(n- 1 ). </p>
<p/>
</div>
<div class="page"><p/>
<p>202 12. Statistical Decision Theory 
</p>
<p>for some c &gt; O. In this model, there are as many parameters as observations. 3 
The !VILE is e = Y = (Y1, ... , Yn). Under the loss function L(8, e) = L~=l (ei -
</p>
<p>8i )2, the risk of the !VILE is R(8, e) = (J2. It can be shown that the minimax risk 
is approximately (J2 / ((J2 + c2 ) and one can find an estimator (j that achieves 
this risk. Since (J2 / ((J2 + c2 ) &lt; (J2, we see that (j has smaller risk than the !VILE. 
In practice, the difference between the risks can be substantial. This shows 
</p>
<p>that maximum likelihood is not an optimal estimator in high dimensional 
</p>
<p>problems. _ 
</p>
<p>12.6 Adrnissibility 
</p>
<p>Minimax estimators and Bayes estimators are "good estimators" in the sense 
</p>
<p>that they have small risk. It is also useful to characterize bad estimators. 
</p>
<p>12.17 Definition. An estimator 8 is inadmissible if there exists another 
</p>
<p>rule e f such that 
</p>
<p>R(8,ef) &lt; R(8,e) for all 8 and 
</p>
<p>R(8, e f) &lt; R(8, e) for at least one 8. 
</p>
<p>Otherwise, 8 is admissible. 
</p>
<p>12.18 Example. Let X rv N(8,1) and consider estimating 8 with squared 
</p>
<p>error loss. Let e(X) = 3. We will show that e is admissible. Suppose not. 
</p>
<p>Then there exists a different rule (it with smaller risk. In particular, R(3, ef ) ::; 
R(3, e) = o. Hence, 0 = R(3, efl = J(ef(x) - 3)~f(x; 3)dx. Thus, ef(x) = 3. 
So there is no rule that beats 8. Even though 8 is admissible it is clearly a 
</p>
<p>bad decision rule. _ 
</p>
<p>12.19 Theorem (Bayes Rules Are Admissible). Suppose that 8 c lR and that 
</p>
<p>R( 8, e) is a continuous function of 8 for every e. Let f be a prior density with 
</p>
<p>full support, meaning that, for every 8 and every E &gt; 0, J:~EE f(8)d8 &gt; O. Let 
</p>
<p>e f be the Bayes! rule. If the Bayes risk is finite then e f is admissible. 
</p>
<p>PROOF. Suppose ef is inadmissible. Then there exists a better rule e such 
that R(8, e) ::; R(8, e f ) for all 8 and R(80 , e) &lt; R(80 , ef ) for some 80. Let 
</p>
<p>3The many Normal means problem is more general than it looks. Many non parametric esti-
</p>
<p>mation problems are mathematically equivalent to this model. </p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Admissibility 203 
</p>
<p>V = R( eo, e f ) - R( eo, e) &gt; O. Since R is continuous, there is an E &gt; 0 such 
that R(e,(jf) - R(e,fi) &gt; v/2 for all e E (eo - E,eo + E). Now, 
</p>
<p>r(j, e f ) - r(j, e) 
</p>
<p>&gt; 
</p>
<p>/ R(e, ef)J(e)de - / R(e, e)J(e)de 
</p>
<p>/ [R(e,ef ) -R(e,e)] J(e)de 
</p>
<p>r(}O+E 
</p>
<p>}(}O-E [R( e, e f ) - R( e, e)] J( e)de 
</p>
<p>&gt; ~ l(~~:E J(e)de 
&gt; o. 
</p>
<p>Hence, r(j, e f ) &gt; r(j, fi). This implies that e f does not minimize r(j, e) which 
contradicts the fact that ef is the Bayes rule. _ 
12.20 Theorem. Let Xl,"" Xn rv N(IL, CJ2). Under squared error loss, X is 
</p>
<p>admissible. 
</p>
<p>The proof of the last theorem is quite technical and is omitted but the idea 
</p>
<p>is as follows: The posterior mean is admissible for any strictly positive prior. 
</p>
<p>Take the prior to be N(a, b2 ). When b2 is very large, the posterior mean is 
</p>
<p>approximately equal to X. 
</p>
<p>How are minimaxity and admissibility linked? In general, a rule may be one, 
</p>
<p>both, or neither. But here are some facts linking admissibility and minimaxity. 
</p>
<p>12.21 Theorem. Suppose that e has constant risk and is admissible. Then it 
</p>
<p>is minimax. 
</p>
<p>PROOF. The risk is R(e, e) 
there exists a rule e' such that 
</p>
<p>c for some c. If e were not minimax then 
</p>
<p>R(e,e') ~ supR(e,B') &lt; supR(e,e) = c. 
() () 
</p>
<p>This would imply that e is inadmissible. _ 
Now we can prove a restricted version of Theorem 12.14 for squared error 
</p>
<p>loss. 
</p>
<p>12.22 Theorem. Let Xl"'" Xn rv N(e, 1). Then, under squared error loss, 
</p>
<p>e = X is minimax. 
</p>
<p>PROOF. According to Theorem 12.20, e is admissible. The risk of e is lin 
which is constant. The result follows from Theorem 12.21. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>204 12. Statistical Decision Theory 
</p>
<p>Although minimax rules are not guaranteed to be admissible they are "close 
</p>
<p>to admissible." Say that (f is strongly inadmissible if there exists a rule (ft 
</p>
<p>and an f &gt; 0 such that R(8, (ft) &lt; R(8, 1i) - f for all 8. 
</p>
<p>12.23 Theorem. If 8 is minimax, then it is not strongly inadmissible. 
</p>
<p>12.7 Stein's Paradox 
</p>
<p>Suppose that X c-v N(8, 1) and consider estimating 8 with squared error loss. 
</p>
<p>From the previous section we know that (f(X) = X is admissible. Now consider 
</p>
<p>estimating two, unrelated quantities 8 = (81 ,82 ) and suppose that Xl rv 
~ 2 ~ 
</p>
<p>N(81 , 1) and X 2 c-v N(82, 1) independently, with loss L(8, 8) = Lj=l (8j -8j j2. 
</p>
<p>Not surprisingly, (f(X) = X is again admissible where X = (Xl, X2). Now 
</p>
<p>consider the generalization to k normal means. Let 8 = (81 , ... , 8k ), X = 
</p>
<p>(Xl, ... , Xk) with Xi c-v N(8i , 1) (independent) and loss L(8, (f) = L~=l (8j -
</p>
<p>(fj)2. Stein astounded everyone when he proved that, if k ::;:, 3, then (f(X) = X 
</p>
<p>is inadmissible. It can be shown that the James-Stein estimator (fs has 
.. ~s _ ~s ~s 
</p>
<p>smaller nsk, where 8 - (81 , ... ,8k ), 
</p>
<p>~s ( k-2)+ 
8i (X) = 1 - Li X; Xi (12.12) 
</p>
<p>and (z)+ = max{z, O}. This estimator shrinks the Xi's towards o. The message 
is that, when estimating many parameters, there is great value in shrinking the 
</p>
<p>estimates. This observation plays an important role in modern nonparametric 
</p>
<p>function estimation. 
</p>
<p>12.8 Bibliographic Remarks 
</p>
<p>Aspects of decision theory can be found in Casella and Berger (2002), Berger 
</p>
<p>(1985), Ferguson (1967), and Lehmann and Casella (1998). 
</p>
<p>12.9 Exercises 
</p>
<p>1. In each of the following models, find the Bayes risk and the Bayes esti-
</p>
<p>mator, using squared error loss. 
</p>
<p>(a) X c-v Binomial(n,p), p c-v Beta(a, ,6). </p>
<p/>
</div>
<div class="page"><p/>
<p>12.9 Exercises 205 
</p>
<p>(b) X rv Poisson(A), A rv Gamma(o:,p). 
</p>
<p>(c) X rv N(e,(J"2) where (J"2 is known and e rv N(a,b2). 
</p>
<p>2. Let Xl"'" Xn rv N(e, (J"2) and suppose we estimate e with loss function 
</p>
<p>L(e, e) = (e - e)2 j(J"2. Show that X is admissible and minimax. 
</p>
<p>3. Let 8 = {e l , ... , ek} be a finite parameter space. Prove that the poste-
</p>
<p>rior mode is the Bayes estimator under zero-one loss. 
</p>
<p>4. (Casella and Berger (2002).) Let Xl, ... ,Xn be a sample from a distri-
</p>
<p>bution with variance (J"2. Consider estimators of the form bS2 where S2 
</p>
<p>is the sample variance. Let the loss function for estimating (J"2 be 
</p>
<p>2 ~2 (J" (J" ~2 (~2) 
L((J" ,(J" ) = (J"2 - 1 -log (J"2 . 
</p>
<p>Find the optimal value of b that minimizes the risk for all (J"2. 
</p>
<p>5. (Berliner (1983).) Let X rv Binomial(n,p) and suppose the loss function 
</p>
<p>is 
~ 2 
</p>
<p>L(p,p) = (1-~) 
</p>
<p>where 0 &lt; p &lt; 1. Consider the estimator p(X) = O. This estimator falls 
</p>
<p>outside the parameter space (0,1) but we will allow this. Show that 
</p>
<p>p(X) = 0 is the unique, minimax rule. 
</p>
<p>6. (Computer Experiment.) Compare the risk of the MLE and the James-
</p>
<p>Stein estimator (12.12) by simulation. Try various values of n and vari-
</p>
<p>ous vectors e. Summarize your results. </p>
<p/>
</div>
<div class="page"><p/>
<p>Part III 
</p>
<p>Statistical Models and 
</p>
<p>Methods </p>
<p/>
</div>
<div class="page"><p/>
<p>13 
</p>
<p>Linear and Logistic Regression 
</p>
<p>Regression is a method for studying the relationship between a response 
</p>
<p>variable Y and a covariate X. The covariate is also called a predictor 
</p>
<p>variable or a feature. lOne way to summarize the relationship between X 
</p>
<p>and Y is through the regression function 
</p>
<p>r(x) = lE(YIX = x) = J y f(ylx)dy. (13.1) 
Our goal is to estimate the regression function r( x) from data of the form 
</p>
<p>In this Chapter, we take a parametric approach and assume that r is linear. 
</p>
<p>In Chapters 20 and 21 we discuss nonparametric regression. 
</p>
<p>13.1 Simple Linear Regression 
</p>
<p>The simplest version of regression is when Xi is simple (one-dimensional) and 
</p>
<p>r(x) is assumed to be linear: 
</p>
<p>r(x) = Po + PIX. 
</p>
<p>IThe term "regression" is due to Sir Francis Galton (1822-1911) who noticed that tall and 
</p>
<p>short men tend to have sons with heights closer to the mean. He called this "regression towards 
</p>
<p>the mean." </p>
<p/>
</div>
<div class="page"><p/>
<p>210 13. Linear and Logistic Regression 
</p>
<p>w 
-i 
</p>
<p>&bull; &bull; 
</p>
<p>E l() -i 
Q.l .... 
::l ... 
~ -i .... 
Q.l 
P. 
</p>
<p>S '" 
Q.l -i ...., 
Q.l 
U 
</p>
<p>'" .fl -i .... 
::l 
Ul 
</p>
<p>b.o 
-i .Q 
</p>
<p>0 
</p>
<p>-i 
</p>
<p>4.0 4.5 5.0 5.5 
</p>
<p>log light int n ity (X) 
</p>
<p>FIG URE 13.1. Data on nearby stars. The solid line is the least squares line. 
</p>
<p>This model is called the the simple linear regression model. We will make 
</p>
<p>the further simplifying assumption that V( fi IX = x) = (52 does not depend 
on x. We can thus write the linear regression model as follows. 
</p>
<p>13.1 Definition. The Simple Linear Regression Model 
</p>
<p>Yi = (30 + (31 X i + fi 
</p>
<p>where lE(fiIXi) = 0 and V(fiIXi) = (52. 
</p>
<p>(13.2) 
</p>
<p>13.2 Example. Figure 13.1 shows a plot oflog surface temperature (Y) versus 
</p>
<p>log light intensity (X) for some nearby stars. Also on the plot is an estimated 
</p>
<p>linear regression line which will be explained shortly .&bull; 
</p>
<p>The unknown parameters in the model are the intercept (30 and the slope 
~ ~ 
</p>
<p>(31 and the variance (52. Let (30 and (31 denote estimates of (30 and (31. The 
</p>
<p>fitted line is 
</p>
<p>(13.3) 
</p>
<p>The predicted values or fitted values are Yi = f(Xi) and the residuals 
are defined to be 
</p>
<p>(13.4) </p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Simple Linear Regression 211 
</p>
<p>The residual sums of squares or RSS, which measures how well the line fits 
</p>
<p>the data, is defined by RSS = L~=1 fl&middot; 
</p>
<p>13.3 Definition. The least squares estimates are the values Po and PI 
that minimize RSS = L~=1 f;. 
</p>
<p>13.4 Theorem. The least squares estimates are given by 
</p>
<p>Po 
</p>
<p>L~=I(Xi -Xn)(Y; - Y n) 
L~=1 (Xi - Xn)2 
</p>
<p>Y n - PI Xn&middot; 
</p>
<p>An unbiased estimate of (J"2 is 
</p>
<p>~2_ (_1 )~~2 
(J" - n _ 2 6 Ei &middot; 
</p>
<p>i=1 
</p>
<p>(13.5) 
</p>
<p>(13.6) 
</p>
<p>(13.7) 
</p>
<p>13.5 Example. Consider the star data from Example 13.2. The least squares 
</p>
<p>estimates are iJo = 3.58 and iJl = 0.166. The fitted line r(x) = 3.58 + 0.166x 
is shown in Figure 13.1. &bull; 
</p>
<p>13.6 Example (The 2001 Presidential Election). Figure 13.2 shows the plot of 
</p>
<p>votes for Buchanan (Y) versus votes for Bush (X) in Florida. The least squares 
</p>
<p>estimates (omitting Palm Beach County) and the standard errors are 
</p>
<p>Po 66.0991 se(iJo) = 17.2926 
</p>
<p>PI 0.0035 se(iJl) = 0.0002. 
</p>
<p>The fitted line is 
</p>
<p>Buchanan = 66.0991 + 0.0035 Bush. 
</p>
<p>(We will see later how the standard errors were computed.) Figure 13.2 also 
</p>
<p>shows the residuals. The inferences from linear regression are most accurate 
</p>
<p>when the residuals behave like random normal numbers. Based on the residual 
</p>
<p>plot, this is not the case in this example. If we repeat the analysis replacing 
</p>
<p>votes with log (votes) we get 
</p>
<p>Po -2.3298 se(iJo) = 0.3529 
</p>
<p>PI 0.730300 se(iJl) = 0.0358. </p>
<p/>
</div>
<div class="page"><p/>
<p>212 13. Linear and Logistic Regression 
</p>
<p>0,------------------------, 
o 
~ 
</p>
<p>'i;,~ ~:~ '.~ ---. -' ---.- --------'- -----------_ . 
. ', : 
</p>
<p>o 
</p>
<p>o~--------~--------~--~ 
o 
</p>
<p>~~--------1-25~OO-O--------250~OO-O~ 
</p>
<p>Bush 
</p>
<p>13 
</p>
<p>Bush 
</p>
<p>". 
&bull; &bull;&bull; I 
</p>
<p>- - - - ; ..... _ - _,_ - - - - &amp; - - - - - - -r _0,- _ -. ___ ~ _____ _ .. . .. 
</p>
<p>1~---,--,---~10,-~11--~12--~13 
</p>
<p>Bush 
</p>
<p>FIGURE 13.2. Voting Data for Election 2000. See example 13.6. 
</p>
<p>This gives the fit 
</p>
<p>log (Buchanan) = -2.3298 + 0.7303 log(Bush). 
</p>
<p>The residuals look much healthier. Later, we shall address the following ques-
</p>
<p>tion: how do we see if Palm Beach County has a statistically plausible out-
</p>
<p>come? _ 
</p>
<p>13.2 Least Squares and Maxirnurn Likelihood 
</p>
<p>Suppose we add the assumption that EilXi rv N(O, (J2), that is, </p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Least Squares and Maximum Likelihood 213 
</p>
<p>where fLi = (30 + (31Xi' The likelihood function is 
</p>
<p>n n 
</p>
<p>II f(X i , li) 
i=l i=l 
</p>
<p>n n 
</p>
<p>i=l i=l 
</p>
<p>n 
</p>
<p>&pound;2 = II iYlx(liIXi). (13.8) 
i=l 
</p>
<p>The term &pound;1 does not involve the parameters (30 and (31. We shall focus on 
</p>
<p>the second term &pound;2 which is called the conditional likelihood, given by 
</p>
<p>_ IIn -n { 1 ""' 2 } &pound;2 = &pound;((30, (31, (J) = i=l iYlx(liIXi) ex: (J exp - 2(J2 ~.)li - fLi) . 
</p>
<p>The conditional log-likelihood is 
</p>
<p>n 2 
</p>
<p>&pound;((30, (31, (J) = -n log (J - 2!2 ~ (li - ((30 + (31 X ,)) (13.9) 
</p>
<p>To find the MLE of ((30, (31) we maximize &pound;((30, (31, (J). From (13.9) we see that 
</p>
<p>maximizing the likelihood is the same as minimizing the RSS L:~=1 (li - ((30 + 
2 
</p>
<p>(31Xi)) . Therefore, we have shown the following: 
</p>
<p>13.7 Theorem. Under the assumption of Normality, the least squares estima-
</p>
<p>tor is also the maximum likelihood estimator. 
</p>
<p>We can also maximize &pound;((30, (31, (J) over (J, yielding the MLE 
</p>
<p>~2 _ 1 L~2 (J -- E. 
n . ' 
</p>
<p>(13.10) 
</p>
<p>This estimator is similar to, but not identical to, the unbiased estimator. 
</p>
<p>Common practice is to use the unbiased estimator (13.7). </p>
<p/>
</div>
<div class="page"><p/>
<p>214 13. Linear and Logistic Regression 
</p>
<p>13.3 Properties of the Least Squares Estimators 
</p>
<p>We now record the standard errors and limiting distribution of the least 
</p>
<p>squares estimator. In regression problems, we usually focus on the proper-
</p>
<p>ties of the estimators conditional on xn = (Xl," ., Xn). Thus, we state the 
</p>
<p>means and variances as conditional means and variances. 
</p>
<p>13.8 Theorem. Let ~T = (~o, ~l)T denote the least squares estimators. 
</p>
<p>Then, 
</p>
<p>IE(~lxn) ( ~~ ) 
V(~lxn) ~2 ( ~ L~=lX; 
</p>
<p>-Xn ) (13.11) ns~ -Xn 1 
2 -1 ",n - )2 where Sx = n ui=l(Xi - Xn . 
</p>
<p>The estimated standard errors of f30 and f3l are obtained by taking the 
</p>
<p>square roots of the corresponding diagonal terms of V(~lxn) and inserting 
</p>
<p>the estimate (; for ~. Thus, 
</p>
<p>(13.12) 
</p>
<p>(13.13) 
</p>
<p>We should really write these as se(~o Ixn) and se(~ Ixn) but we will use the 
</p>
<p>shorter notation se(~o) and se(~d. 
</p>
<p>13.9 Theorem. Under appropriate conditions we have: 
</p>
<p>~ p - p 
1. (Consistency): f3o--+ f30 and f3l--+ f3l. 
</p>
<p>2. (Asymptotic Normality): 
</p>
<p>f30 -~f3o ~ N(O, 1) and 
se(f3o) 
</p>
<p>3. Approximate 1 - Q confidence intervals for f30 and f3l are 
</p>
<p>(13.14) </p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Prediction 215 
</p>
<p>4. The Wald test 2 for testing Ho : PI = 0 versus HI : PI i= 0 is: reject Ho 
if IWI &gt; Za/2 where W = ih/se(fjd&middot; 
</p>
<p>13.10 Example. For the election data, on the log scale, a 95 percent confi-
</p>
<p>dence interval is .7303 &plusmn; 2(.0358) = (.66, .80). The Wald statistics for testing 
</p>
<p>Ho : PI = 0 versus HI : PI i= 0 is IWI = 1.7303 - 01/.0358 = 20.40 with a 
p-value of IP'(IZI &gt; 20.40) :::::; O. This is strong evidence that that the true slope 
is not O .&bull; 
</p>
<p>13.4 Prediction 
</p>
<p>Suppose we have estimated a regression model r(x) = Po + PIX from data 
(Xl, Y1 ), . .. , (Xn' Yn). We observe the value X = x* of the covariate for a 
</p>
<p>new subject and we want to predict their outcome Y*. An estimate of Y* is 
</p>
<p>(13.15) 
</p>
<p>Using the formula for the variance of the sum of two random variables, 
</p>
<p>Theorem 13.8 gives the formulas for all the terms in this equation. The es-
</p>
<p>timated standard error seCY*) is the square root of this variance, with (j2 in 
place of ()2. However, the confidence interval for Y* is not of the usual form 
</p>
<p>Y* &plusmn; Za/2Se. The reason for this is explained in Exercise 10. The correct form 
of the confidence interval is given in the following theorem. 
</p>
<p>13.11 Theorem (Prediction Interval). Let 
</p>
<p>(13.16) 
</p>
<p>An approximate 1 - Q prediction interval for Y* zs 
</p>
<p>(13.17) 
</p>
<p>2Recall from equation (10.5) that the Wald statistic for testing Ho : ;3 = ;30 versus HI : 
</p>
<p>;3 =f.;30 is W = ((3 - ;3o)/se((3). </p>
<p/>
</div>
<div class="page"><p/>
<p>216 13. Linear and Logistic Regression 
</p>
<p>13.12 Example (Election Data Revisited). On the log scale, our linear regres-
</p>
<p>sion gives the following prediction equation: 
</p>
<p>log (Buchanan) = -2.3298 + 0.7303log(Bush). 
</p>
<p>In Palm Beach, Bush had 152,954 votes and Buchanan had 3,467 votes. On the 
</p>
<p>log scale this is 11.93789 and 8.151045. How likely is this outcome, assuming 
</p>
<p>our regression model is appropriate? Our prediction for log Buchanan votes 
</p>
<p>-2.3298 + .7303 (11.93789)=6.388441. Now, 8.151045 is bigger than 6.388441 
but is it "significantly" bigger? Let us compute a confidence interval. We 
</p>
<p>find that ~n = .093775 and the approximate 95 percent confidence interval is 
</p>
<p>(6.200,6.578) which clearly excludes 8.151. Indeed, 8.151 is nearly 20 standard 
</p>
<p>errors from Y*. Going back to the vote scale by exponentiating, the confidence 
</p>
<p>interval is (493,717) compared to the actual number of votes which is 3,467 . 
</p>
<p>&bull; 
</p>
<p>13.5 Multiple Regression 
</p>
<p>Now suppose that the covariate is a vector of length k. The data are of the 
</p>
<p>form 
</p>
<p>where 
</p>
<p>Here, Xi is the vector of k covariate values for the 'ith observation. The linear 
</p>
<p>regression model is 
</p>
<p>k 
</p>
<p>Y; = L (3jXij + Ei 
j=l 
</p>
<p>(13.18) 
</p>
<p>for 'i = 1, ... , n, where JE( Ei IX1i , ... , Xki) = O. Usually we want to include an 
</p>
<p>intercept in the model which we can do by setting X i1 = 1 for'i = 1, ... , n. At 
</p>
<p>this point it will be more convenient to express the model in matrix notation. 
</p>
<p>The outcomes will be denoted by </p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Multiple Regression 217 
</p>
<p>and the covariates will be denoted by 
</p>
<p>Xlk ) 
X:k . 
</p>
<p>Xnk 
</p>
<p>Each row is one observation; the columns correspond to the k covariates. Thus, 
</p>
<p>X is a (n x k) matrix. Let 
</p>
<p>Then we can write (13.18) as 
</p>
<p>(13.19) 
</p>
<p>The form of the least squares estimate is given in the following theorem. 
</p>
<p>13.13 Theorem. Assuming that the (k x k) matrix XT X is invertible, 
</p>
<p>(XTX)-lXTy 
</p>
<p>(J"2(XT X)-l 
</p>
<p>N({3, (J"2(XT X)-l). 
</p>
<p>(13.20) 
</p>
<p>(13.21 ) 
</p>
<p>(13.22) 
</p>
<p>The estimate regression function is r(x) = L~=l (3jXj. An unbiased esti-
</p>
<p>mate of (J"2 is 
</p>
<p>~2= (_1 )~~2 
(J" n _ k ~E, 
</p>
<p>i=l 
</p>
<p>where E = X(3 - Y is the vector of residuals. An approximate 1- a confidence 
</p>
<p>interval for {3j is 
</p>
<p>(13.23) 
</p>
<p>13.14 Example. Crime data on 47 states in 1960 can be obtained from 
</p>
<p>http://lib.stat.cmu.edu/DASL /Stories /USCrime.html. 
</p>
<p>If we fit a linear regression of crime rate on 10 variables we get the following: </p>
<p/>
</div>
<div class="page"><p/>
<p>218 13. Linear and Logistic Regression 
</p>
<p>Covariate (3j se(j3j) t value p-value 
</p>
<p>(Intercept) -589.39 167.59 -3.51 0.001 ** 
Age 1.04 0.45 2.33 0.025 * 
Southern State 11.29 13.24 0.85 0.399 
</p>
<p>Education 1.18 0.68 1.7 0.093 
</p>
<p>Expenditures 0.96 0.25 3.86 0.000 *** 
Labor 0.11 0.15 0.69 0.493 
</p>
<p>Number of Males 0.30 0.22 1.36 0.181 
</p>
<p>Population 0.09 0.14 0.65 0.518 
</p>
<p>Unemployment (14-24) -0.68 0.48 -1.4 0.165 
</p>
<p>Unemployment (25-39) 2.15 0.95 2.26 0.030 * 
Wealth -0.08 0.09 -0.91 0.367 
</p>
<p>This table is typical of the output of a multiple regression program. The "t-
</p>
<p>value" is the Wald test statistic for testing Ho : fJj = 0 versus HI : fJj i=- O. The 
</p>
<p>asterisks denote "degree of significance" and more asterisks denote smaller 
</p>
<p>p-values. The example raises several important questions: (1) should we elim-
</p>
<p>inate some variables from this model? (2) should we interpret these relation-
</p>
<p>ships as causal? For example, should we conclude that low crime prevention 
</p>
<p>expenditures cause high crime rates? We will address question (1) in the next 
</p>
<p>section. We will not address question (2) until Chapter 16 .&bull; 
</p>
<p>13.6 Model Selection 
</p>
<p>Example 13.14 illustrates a problem that often arises in multiple regression. 
</p>
<p>We may have data on many covariates but we may not want to include all of 
</p>
<p>them in the model. A smaller model with fewer covariates has two advantages: 
</p>
<p>it might give better predictions than a big model and it is more parsimonious 
</p>
<p>(simpler). Generally, as you add more variables to a regression, the bias of the 
</p>
<p>predictions decreases and the variance increases. Too few covariates yields high 
</p>
<p>bias; this called underfitting. Too many covariates yields high variance; this 
</p>
<p>called overfitting. Good predictions result from achieving a good balance 
</p>
<p>between bias and variance. 
</p>
<p>In model selection there are two problems: (i) assigning a "score" to each 
</p>
<p>model which measures, in some sense, how good the model is, and (ii) search-
</p>
<p>ing through all the models to find the model with the best score. 
</p>
<p>Let us first discuss the problem of scoring models. Let S C {I, ... ,k} and 
</p>
<p>let Xs = {Xj : j E S} denote a subset of the covariates.~Let fJs denote the 
</p>
<p>coefficients of the corresponding set of covariates and let fJs denote the least 
</p>
<p>squares estimate of fJs. Also, let Xs denote the X matrix for this subset of </p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Model Selection 219 
</p>
<p>covariates and define rs(x) to be the estimated regression function. The pre-
</p>
<p>dicted values from model S are denoted by ~(S) = rS(Xi). The prediction 
</p>
<p>risk is defined to be 
</p>
<p>n 
</p>
<p>R(S) = 2)E(~(S) - Yi*)2 (13.24) 
i=l 
</p>
<p>where Yi* denotes the value of a future observation of Yi at covariate value 
</p>
<p>Xi. Our goal is to choose S to make R(S) small. 
</p>
<p>The training error is defined to be 
</p>
<p>n 
</p>
<p>Rtr(S) = 2)~(S) - Yi)2. 
i=l 
</p>
<p>This estimate is very biased as an estimate of R(S). 
</p>
<p>13.15 Theorem. The training error is a downward-biased estimate of the pre-
</p>
<p>diction risk: 
</p>
<p>lE(Rtr(S)) &lt; R(S). 
</p>
<p>In fact, 
</p>
<p>bias(Rtr(S)) = lE(Rtr(S)) - R(S) = -2 L Cov(~, Yi). (13.25) 
i=l 
</p>
<p>The reason for the bias is that the data are being used twice: to estimate 
</p>
<p>the parameters and to estimate the risk. When we fit a complex model with 
</p>
<p>many parameters, the covariance Cov(~, Yi) will be large and the bias of the 
</p>
<p>training error gets worse. Here are some better estimates of risk. 
</p>
<p>Mallow's Cp statistic is defined by 
</p>
<p>(13.26) 
</p>
<p>where lSI denotes the number of terms in Sand &amp;2 is the estimate of (J"2 
</p>
<p>obtained from the full model (with all covariates in the model). This is simply 
</p>
<p>the training error plus a bias correction. This estimate is named in honor of 
</p>
<p>Colin Mallows who invented it. The first term in (13.26) measures the fit of 
</p>
<p>the model while the second measure the complexity of the model. Think of 
</p>
<p>the Cp statistic as: 
</p>
<p>lack of fit + complexity penalty. 
</p>
<p>Thus, finding a good model involves trading off fit and complexity. </p>
<p/>
</div>
<div class="page"><p/>
<p>220 13. Linear and Logistic Regression 
</p>
<p>A related method for estimating risk is AIC (Akaike Information Cri-
</p>
<p>terion). The idea is to choose S to maximize 
</p>
<p>is -lSI (13.27) 
</p>
<p>where is is the log-likelihood of the model evaluated at the !viLE. 3 This can 
</p>
<p>be thought of "goodness of fit" minus "complexity." In linear regression with 
</p>
<p>Normal errors (and taking (J" equal to its estimate from the largest model), 
</p>
<p>maximizing AIC is equivalent to minimizing Mallow's Cp ; see Exercise 8. The 
</p>
<p>appendix contains more explanation about AIC. 
</p>
<p>Yet another method for estimating risk is leave-one-out cross-validation. 
</p>
<p>In this case, the risk estimator is 
</p>
<p>n 
</p>
<p>Rev(S) = 2:)Yi - YCi))2 (13.28) 
i=l 
</p>
<p>where Y(i) is the prediction for Yi obtained by fitting the model with Yi omit-
</p>
<p>ted. It can be shown that 
</p>
<p>n ( ~ )2 Rev(S) = '" Yi - Yi(S) 
~ 1- U(S) 
i=l n 
</p>
<p>(13.29) 
</p>
<p>where Uii(S) is the 'ith diagonal element of the matrix 
</p>
<p>(13.30) 
</p>
<p>Thus, one need not actually drop each observation and re-fit the model. A 
</p>
<p>generalization is k-fold cross-validation. Here we divide the data into k 
</p>
<p>groups; often people take k = 10. We omit one group of data and fit the 
</p>
<p>models to the remaining data. We use the fitted model to predict the data 
</p>
<p>in the group that was omitted. We then estimate the risk by Li(Yi _ ~)2 
</p>
<p>where the sum is over the the data points in the omitted group. This process is 
</p>
<p>repeated for each of the k groups and the resulting risk estimates are averaged. 
</p>
<p>For linear regression, Mallows Cp and cross-validation often yield essentially 
</p>
<p>the same results so one might as well use Mallows' method. In some of the 
</p>
<p>more complex problems we will discuss later, cross-validation will be more 
</p>
<p>useful. 
</p>
<p>Another scoring method is BI C (Bayesian information criterion). Here we 
</p>
<p>choose a model to maximize 
</p>
<p>lSI 
BIC(S) = is - 210gn. (13.31 ) 
</p>
<p>3Some texts use a slightly difFerent definition of Ale which involves multiplying the definition 
</p>
<p>here by 2 or -2. This has no efFect on which model is selected. </p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Model Selection 221 
</p>
<p>The BIC score has a Bayesian interpretation. Let S = {Sl, ... , Sm} denote 
</p>
<p>a set of models. Suppose we assign the prior IP'(Sj) = 11m over the models. 
</p>
<p>Also, assume we put a smooth prior on the parameters within each model. It 
</p>
<p>can be shown that the posterior probability for a model is approximately, 
</p>
<p>eB1C(Sj) 
</p>
<p>IP'(Sjldata) ;:::::: Lr eB1C(Sr)&middot; 
</p>
<p>Hence, choosing the model with highest BIC is like choosing the model with 
</p>
<p>highest posterior probability. The BIC score also has an information-theoretic 
</p>
<p>interpretation in terms of something called minimum description length. The 
</p>
<p>BIC score is identical to Mallows Cp except that it puts a more severe penalty 
</p>
<p>for complexity. It thus leads one to choose a smaller model than the other 
</p>
<p>methods. 
</p>
<p>Now let us turn to the problem of model search. If there are k covariates 
</p>
<p>then there are 2k possible models. We need to search through all these models, 
</p>
<p>assign a score to each one, and choose the model with the best score. If k is 
</p>
<p>not too large we can do a complete search over all the models. When k is large, 
</p>
<p>this is infeasible. In that case we need to search over a subset of all the models. 
</p>
<p>Two common methods are forward and backward stepwise regression. 
</p>
<p>In forward stepwise regression, we start with no covariates in the model. We 
</p>
<p>then add the one variable that leads to the best score. We continue adding 
</p>
<p>variables one at a time until the score does not improve. Backwards stepwise 
</p>
<p>regression is the same except that we start with the biggest model and drop 
</p>
<p>one variable at a time. Both are greedy searches; nether is guaranteed to 
</p>
<p>find the model with the best score. Another popular method is to do random 
</p>
<p>searching through the set of all models. However, there is no reason to expect 
</p>
<p>this to be superior to a deterministic search. 
</p>
<p>13.16 Example. We applied backwards stepwise regression to the crime data 
</p>
<p>using AIC. The following was obtained from the program R. This program 
</p>
<p>uses a slightly different definition of AIC. With their definition, we seek the 
</p>
<p>smallest (not largest) possible AIC. This is the same is minimizing Mallows 
</p>
<p>Cpo 
</p>
<p>The full model (which includes all covariates) has AIC= 310.37. In ascend-
</p>
<p>ing order, the AIC scores for deleting one variable are as follows: 
</p>
<p>variable Pop Labor South Wealth Males Ul Educ. U2 Age Expend 
</p>
<p>Ale 308 309 309 309 310 310 312 314 315 324 
</p>
<p>For example, if we dropped Pop from the model and kept the other terms, 
</p>
<p>then the AIC score would be 308. Based on this information we drop "pop-</p>
<p/>
</div>
<div class="page"><p/>
<p>222 13. Linear and Logistic Regression 
</p>
<p>ulation" from the model and the current AIC score is 308. Now we consider 
</p>
<p>dropping a variable from the current model. The AIC scores are: 
</p>
<p>variable South Labor Wealth Males U1 Education U2 Age Expend 
</p>
<p>Ale 308 308 308 309 309 310 313 313 329 
</p>
<p>We then drop "Southern" from the model. This process is continued until 
</p>
<p>there is no gain in AIC by dropping any variables. In the end, we are left with 
</p>
<p>the following model: 
</p>
<p>Crime 1.2 Age + .75 Education + .87 Expenditure 
</p>
<p>+ .34 Males - .86 U1 + 2.31 U2. 
</p>
<p>Warning! This does not yet address the question of which variables are 
</p>
<p>causes of crime. _ 
</p>
<p>There is another method for model selection that avoids having to search 
</p>
<p>through all possible models. This method, which is due to Zheng and Loh 
</p>
<p>(1995), does not seek to minimize prediction errors. Rather, it assumes some 
</p>
<p>subset of the (3j'S are exactly equal to 0 and tries to find the true model, 
</p>
<p>that is, the smallest sub-model consisting of nonzero (3j terms. The method 
</p>
<p>is carried out as follows. 
</p>
<p>Zheng-Loh Model Selection Method 4 
</p>
<p>1. Fit the full model with all k covariates and let Wj = fij / {k(/3j) denote 
</p>
<p>the Wald test statistic for Ho : (3j = 0 versus HI : (3j of. o. 
</p>
<p>2. Order the test statistics from largest to smallest in absolute value: 
</p>
<p>3. Let J be the value of j that minimizes 
</p>
<p>RSS(j) + j (;2 log n 
</p>
<p>where RSS(j) is the residual sums of squares from the model with 
</p>
<p>the j largest Wald statistics. 
</p>
<p>4. Choose, as the final model, the regression with the J terms with the 
largest absolute Wald statistics. </p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Logistic Regression 223 
</p>
<p>o x 
</p>
<p>FIGURE 13.3. The logistic function p = eX /(1 + eX). 
</p>
<p>Zheng and Loh showed that, under appropriate conditions, this method 
</p>
<p>chooses the true model with probability tending to one as the sample size 
</p>
<p>increases. 
</p>
<p>13.7 Logistic Regression 
</p>
<p>So far we have assumed that Yi is real valued. Logistic regression is a para-
metric method for regression when Yi E {O, I} is binary. For a k-dimensional 
</p>
<p>covariate X, the model is 
</p>
<p>or, equivalently, 
</p>
<p>where 
</p>
<p>k 
</p>
<p>10git(Pi) = {3o + L (3j X ij 
j=l 
</p>
<p>logit(p) = log (-p_) . 
1-p 
</p>
<p>(13.32) 
</p>
<p>(13.33) 
</p>
<p>(13.34) 
</p>
<p>The name "logistic regression" comes from the fact that eX /(1 + eX) is called 
the logistic function. A plot of the logistic for a one-dimensional covariate is 
</p>
<p>shown in Figure 13.3. 
</p>
<p>Because the Yi's are binary, the data are Bernoulli: 
</p>
<p>Yi IXi = Xi rv Bernoulli(pi). 
</p>
<p>Hence the (conditional) likelihood function is 
</p>
<p>n 
</p>
<p>&pound;({3) = II Pi ({3) Y; (1 - Pi ({3) )1-Y i &bull; (13.35) 
i=l 
</p>
<p>4This is just one version of their method. In particular, the penalty j log n is only one choice 
from a set of possible penalty functions. </p>
<p/>
</div>
<div class="page"><p/>
<p>224 13. Linear and Logistic Regression 
</p>
<p>The MLE ;3 has to be obtained by maximizing &pound;({3) numerically. There is 
</p>
<p>a fast numerical algorithm called reweighted least squares. The steps are as 
</p>
<p>follows: 
</p>
<p>Reweighted Least Squares Algorithm 
</p>
<p>Choose starting values ;30 = (;38, ... ,;3~) and compute p? using equation 
(13.32), for i = 1, ... ,n. Set s = 0 and iterate the following steps until 
</p>
<p>convergence. 
</p>
<p>1. Set 
</p>
<p>Z 1 &middot;t( S) li - pf 
i = Ogl Pi + ( ) , pf 1 - pf 
</p>
<p>i = 1, ... ,no 
</p>
<p>2. Let W be a diagonal matrix with (i, i) element equal to pHI - pf). 
</p>
<p>3. Set 
</p>
<p>This corresponds to doing a (weighted) linear regression of Z on Y. 
</p>
<p>4. Set s = s + 1 and go back to the first step. 
</p>
<p>The Fisher information matrix I can also be obtained numerically. The 
</p>
<p>estimate standard error of ;3j is the (j, j) element of J = I-I. Model selection 
is usually done using the AlC score &pound;s - 151. 
</p>
<p>13.17 Example. The Coronary Risk-Factor Study (CORIS) data involve 462 
</p>
<p>males between the ages of 15 and 64 from three rural areas in South Africa, 
</p>
<p>(Rousseauwet al. (1983)). The outcome Y is the presence (Y = 1) or absence 
</p>
<p>(Y = 0) of coronary heart disease. There are 9 covariates: systolic blood 
</p>
<p>pressure, cumulative tobacco (kg), ldl (low density lipoprotein cholesterol), 
</p>
<p>adi posity, famhist (family history of heart disease), typea (type-A behavior), 
</p>
<p>obesity, alcohol (current alcohol consumption), and age. A logistic regression 
</p>
<p>yields the following estimates and Wald statistics Wj for the coefficients: </p>
<p/>
</div>
<div class="page"><p/>
<p>13.8 Bibliographic Remarks 225 
</p>
<p>Covariate {3j se Wj p-value 
</p>
<p>Intercept -6.145 1.300 -4.738 0.000 
</p>
<p>sbp 0.007 0.006 1.138 0.255 
</p>
<p>tobacco 0.079 0.027 2.991 0.003 
</p>
<p>Idl 0.174 0.059 2.925 0.003 
</p>
<p>adiposity 0.019 0.029 0.637 0.524 
</p>
<p>famhist 0.925 0.227 4.078 0.000 
</p>
<p>typea 0.040 0.012 3.233 0.001 
</p>
<p>obesity -0.063 0.044 -1.427 0.153 
</p>
<p>alcohol 0.000 0.004 0.027 0.979 
</p>
<p>age 0.045 0.012 3.754 0.000 
</p>
<p>Are you surprised by the fact that systolic blood pressure is not significant 
</p>
<p>or by the minus sign for the obesity coefficient? If yes, then you are confusing 
</p>
<p>association and causation. This issue is discussed in Chapter 16. The fact 
</p>
<p>that blood pressure is not significant does not mean that blood pressure is 
</p>
<p>not an important cause of heart disease. It means that it is not an important 
</p>
<p>predictor of heart disease relative to the other variables in the model. _ 
</p>
<p>13.8 Bibliographic Rernarks 
</p>
<p>A succinct book on linear regression is Weisberg (1985). A data-mining view 
</p>
<p>of regression is given in Hastie et al. (2001). The Akaike Information Criterion 
</p>
<p>(AIC) is due to Akaike (1973). The Bayesian Information Criterion (BIC) is 
</p>
<p>due to Schwarz (1978). References on logistic regression include Agresti (1990) 
</p>
<p>and Dobson (2001). 
</p>
<p>13.9 Appendix 
</p>
<p>THE AKAIKE INFORMATION CRITERION (AIC). Consider a set of models 
</p>
<p>{Nh, Nh, .. . }. Let h(x) denote the estimated probability function obtained 
</p>
<p>by using the maximum likelihood estimator of model NIj . Thus, h(x) = 
</p>
<p>f(x; {ij) where {ij is the MLE of the set of parameters (3j for model NIj . We 
will use the loss function D(f, J) where 
</p>
<p>D(f,g) = Lf(x)log (~~:D 
x 
</p>
<p>is the Kullback-Leibler distance between two probability functions. The cor-
</p>
<p>responding risk function is R(f, J) = lE(D(f, J). Notice that D(f, J) = c-</p>
<p/>
</div>
<div class="page"><p/>
<p>226 13. Linear and Logistic Regression 
</p>
<p>AU, 1) where c = Lx f(x) log f(x) does not depend on 1 and 
</p>
<p>AU, J) = L f(x) log !(x). 
x 
</p>
<p>Thus, minimizing the risk is equivalent to maximizing aU, 1) == lE(AU, 1)). 
</p>
<p>It is tempting to estimate aU, J) by Lx !(x) log !(x) but, just as the train-
ing error in regression is a highly biased estimate of prediction risk, it is also 
</p>
<p>the case that Lx 1(x) log !(x) is a highly biased estimate of aU, J). In fact, 
the bias is approximately equal to 1 M j I. Thus: 
</p>
<p>13.18 Theorem. AIC(Mj) is an approximately unbiased estimate of aU, 1). 
</p>
<p>13.10 Exercises 
</p>
<p>1. Prove Theorem 13.4. 
</p>
<p>2. Prove the formulas for the standard errors in Theorem 13.8. You should 
</p>
<p>regard the X/s as fixed constants. 
</p>
<p>3. Consider the regression through the origin model: 
</p>
<p>Find the least squares estimate for /3. Find the standard error of the 
</p>
<p>estimate. Find conditions that guarantee that the estimate is consistent. 
</p>
<p>4. Prove equation (13.25). 
</p>
<p>5. In the simple linear regression model, construct a Wald test for Ho 
</p>
<p>/31 = 17/30 versus HI : /31 # 17/30&middot; 
</p>
<p>6. Get the passenger car mileage data from 
</p>
<p>http://lib.stat.cmu.ed u/DASL /Datafiles/ carmpgdat.html 
</p>
<p>(a) Fit a simple linear regression model to predict MPG (miles per 
</p>
<p>gallon) from HP (horsepower). Summarize your analysis including a 
</p>
<p>plot of the data with the fitted line. 
</p>
<p>(b) Repeat the analysis but use 10g(MPG) as the response. Compare 
</p>
<p>the analyses. </p>
<p/>
</div>
<div class="page"><p/>
<p>13.10 Exercises 227 
</p>
<p>7. Get the passenger car mileage data from 
</p>
<p>http://lib.stat.cmu.edu/DASL/Datafiles/carmpgdat.html 
</p>
<p>(a) Fit a multiple linear regression model to predict MPG (miles per 
</p>
<p>gallon) from the other variables. Summarize your analysis. 
</p>
<p>(b) Use Mallow Gp to select a best sub-model. To search through the 
</p>
<p>models try (i) forward stepwise, (ii) backward stepwise. Summarize your 
</p>
<p>findings. 
</p>
<p>(c) Use the Zheng-Loh model selection method and compare to (b). 
</p>
<p>(d) Perform all possible regressions. Compare Gp and BIC. Compare the 
</p>
<p>results. 
</p>
<p>8. Assume a linear regression model with Normal errors. Take (]" known. 
</p>
<p>Show that the model with highest AIC (equation (13.27)) is the model 
</p>
<p>with the lowest Mallows Gp statistic. 
</p>
<p>9. In this question we will take a closer look at the AIC method. Let 
</p>
<p>Xl, ... , Xn be lID observations. Consider two models Mo and MI. Un-
</p>
<p>der Mo the data are assumed to be N(O, 1) while under Ml the data 
</p>
<p>are assumed to be N (e, 1) for some unknown e E Jl{: 
</p>
<p>Mo: Xl, ... ,Xn N(O,l) 
</p>
<p>Ml: Xl, ... ,Xn N(e,l), e E Jl{. 
</p>
<p>This is just another way to view the hypothesis testing problem: Ho : 
</p>
<p>e = 0 versus HI : e # O. Let jin(e) be the log-likelihood function. 
The AIC score for a model is the log-likelihood at the MLE minus the 
</p>
<p>number of parameters. (Some people multiply this score by 2 but that 
</p>
<p>is irrelevant.) Thus, the AIC score for Mo is AIGo = jin(O) and the AIC 
</p>
<p>score for Ml is AlGI = jin(ii) - 1. Suppose we choose the model with 
</p>
<p>the highest AIC score. Let I n denote the selected model: 
</p>
<p>I n = {O if AIGo &gt; AlGI 
1 if AlGI&gt; AIGo. 
</p>
<p>(a) Suppose that Mo is the true model, i.e. e = O. Find 
</p>
<p>lim IF' (In = 0) . 
n--+= 
</p>
<p>Now compute limn--+ oo IF' (In = 0) when e # o. </p>
<p/>
</div>
<div class="page"><p/>
<p>228 13. Linear and Logistic Regression 
</p>
<p>(b) The fact that limn--+oo][l' (In = 0) i= 1 when e = 0 is why some people 
say that AIC "overfits." But this is not quite true as we shall now see. 
</p>
<p>Let cPe(x) denote a Normal density function with mean e and variance 
1. Define 
</p>
<p>In(x) = {cPo(x) if I n = 0 
cPe(x) if I n = 1. 
</p>
<p>~ p 
If e = 0, show that D(cPo, fn) -'...j 0 as n -+ 00 where 
</p>
<p>D(j,g) = J f(x) log (~~~D dx 
is the K ullback-Leibler distance. Show also that D (cPe, in) '4 0 if e i= o. 
Hence, AIC consistently estimates the true density even if it "over-
</p>
<p>shoots" the correct model. 
</p>
<p>(c) Repeat this analysis for BIC which is the log-likelihood minus (p/2) log n 
</p>
<p>where p is the number of parameters and n is sample size. 
</p>
<p>10. In this question we take a closer look at prediction intervals. Let e = ---- .......... ---- .......... 
Po + PI X* and let e = Po + fhX*. Thus, Y* = e while Y* = e + E. Now, 
e;:::::: N(e, se2 ) where 
</p>
<p>2 ~ ~ ~ 
</p>
<p>se = V(e) = V(po + pIX*). 
</p>
<p>Note that V(e) is the same as VCY*). Now, e&plusmn;2vv(if) is an approximate 
95 percent confidence interval for e = PO+PIX* using the usual argument 
for a confidence interval. But, as you shall now show, it is not a valid 
</p>
<p>confidence interval for Y*. 
</p>
<p>(a) Let s = VVCY*). Show that 
</p>
<p>][l'(Y*-2s&lt;Y*&lt;Y*+2s) ;:::::: ][l'(-2&lt;N(0,1+::) &lt;2) 
</p>
<p>i= 0.95. 
</p>
<p>(b) The problem is that the quantity of interest Y* is equal to a param-
</p>
<p>eter e plus a random variable. We can fix this by defining 
</p>
<p>In practice, we substitute &amp; for (]" and we denote the resulting quantity 
~ ~ ~ 
</p>
<p>by ~n. Now consider the interval Y* &plusmn; 2 ~n. Show that </p>
<p/>
</div>
<div class="page"><p/>
<p>13.10 Exercises 229 
</p>
<p>11. Get the Coronary Risk-Factor Study (CORIS) data from the book web 
</p>
<p>site. Use backward stepwise logistic regression based on AIC to select a 
</p>
<p>model. Summarize your results. </p>
<p/>
</div>
<div class="page"><p/>
<p>14 
</p>
<p>Multivariate Models 
</p>
<p>In this chapter we revisit the Multinomial model and the multivariate Normal. 
</p>
<p>Let us first review some notation from linear algebra. In what follows, x and 
</p>
<p>yare vectors and A is a matrix. 
</p>
<p>xTy 
</p>
<p>IAI 
AT 
A-I 
</p>
<p>I 
</p>
<p>tr(A) 
A 1/ 2 
</p>
<p>Linear Algebra Notation 
</p>
<p>inner product Lj XjYj 
</p>
<p>determinant 
</p>
<p>transpose of A 
</p>
<p>inverse of A 
</p>
<p>the identity matrix 
</p>
<p>trace of a square matrix; sum of its diagonal elements 
</p>
<p>square root matrix 
</p>
<p>The trace satisfies tr(AB) = tr(BA) and tr(A) + tr(B). Also, tr(a) = a if a 
is a scalar. A matrix is positive definite if xT'f;x &gt; 0 for all nonzero vectors 
</p>
<p>x. If a matrix A is symmetric and positive definite, its square root A 1/2 exists 
</p>
<p>and has the following properties: (1) Al/2 is symmetric; (2) A = Al/2 Al/2; 
</p>
<p>(3) A 1/ 2A- 1/ 2 = A- 1/ 2 A 1/ 2 = I where A- 1/ 2 = (Al/2)-I. </p>
<p/>
</div>
<div class="page"><p/>
<p>232 14. Multivariate Models 
</p>
<p>14.1 Randorn Vectors 
</p>
<p>Multivariate models involve a random vector X of the form 
</p>
<p>The mean of a random vector X is defined by 
</p>
<p>The covariance matrix ~, also written V(X), is defined to be 
</p>
<p>~= 
</p>
<p>V(XI ) 
</p>
<p>Cov(X2 , Xl) 
</p>
<p>Cov(XI , X 2 ) 
</p>
<p>V(X2 ) 
</p>
<p>Cov(XI , X k ) 
</p>
<p>CoV(X2 ,Xk ) 
</p>
<p>(14.1 ) 
</p>
<p>(14.2) 
</p>
<p>This is also called the variance matrix or the variance-covariance matrix. The 
</p>
<p>inverse ~-l is called the precision matrix. 
</p>
<p>14.1 Theorem. Let a be a vector of length k and let X be a random vector 
</p>
<p>of the same length with mean JL and variance ~. Then IE(aT X) = aT JL and 
</p>
<p>V(aT X) = aT~a. If A is a matrix with k columns, then IE(AX) = AIL and 
</p>
<p>V(AX) = A~AT. 
</p>
<p>Now suppose we have a random sample of n vectors: 
</p>
<p>(14.3) 
</p>
<p>The sample mean X is a vector defined by </p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Estimating the Correlation 233 
</p>
<p>where Xi = n -1 L'.7=1 X ij . The sample variance matrix, also called the co-
variance matrix or the variance-covariance matrix, is 
</p>
<p>811 812 81k 
</p>
<p>812 822 82k 
</p>
<p>5= (14.4) 
</p>
<p>8lk 82k 8kk 
</p>
<p>where 
</p>
<p>It follows that JE(X) = IL. and JE(5) = ~. 
</p>
<p>14.2 Estimating the Correlation 
</p>
<p>Consider n data points from a bivariate distribution: 
</p>
<p>( Xu ) , ( X
12 ) , ... , ( X 1n ) . 
</p>
<p>X 21 X 22 X 2n 
</p>
<p>Recall that the correlation between Xl and X 2 is 
</p>
<p>(14.5) 
</p>
<p>where a} = V(Xjd, j = 1,2. The nonparametric plug-in estimator is the 
sample correlation 1 
</p>
<p>(14.6) 
</p>
<p>where 
</p>
<p>We can construct a confidence interval for p by applying the delta method. 
</p>
<p>However, it turns out that we get a more accurate confidence interval by first 
</p>
<p>constructing a confidence interval for a function e = f(p) and then applying 
</p>
<p>1 More precisely, the plug-in estimator has n rather than n - 1 in the formula for Sj but this 
difference is small. </p>
<p/>
</div>
<div class="page"><p/>
<p>234 14. Multivariate Models 
</p>
<p>the inverse function J- 1 . The method, due to Fisher, is as follows: Define J 
and its inverse by 
</p>
<p>J(r) ~ (lOg(l + r) -log(l - r)) 
</p>
<p>e2z - 1 
</p>
<p>e2z + 1&middot; 
</p>
<p>Approximate Confidence Interval for The Correlation 
</p>
<p>1. Compute 
</p>
<p>(j = J(p) = ~ (log(l + p) - log(l - p)). 
</p>
<p>2. Compute the approximate standard error of 8 which can be shown to 
</p>
<p>be 
~ 1 
</p>
<p>se(8) = ~. 
n-3 
</p>
<p>3. An approximate 1 - a confidence interval for 8 = J (p) is 
</p>
<p>_ (~ Za/2 ~ Za/2) 
(a,b)= 8- ~,8+ ~ . 
</p>
<p>n-3 n-3 
</p>
<p>4. Apply the inverse transformation J-l(Z) to get a confidence interval 
</p>
<p>for p: 
</p>
<p>( 
e2a - 1 e2b - 1) 
e 2a + l' e2b + 1 . 
</p>
<p>Yet another method for getting a confidence interval for p is to use the 
</p>
<p>bootstrap. 
</p>
<p>14.3 Multivariate Nonnal 
</p>
<p>Recall that a vector X has a multivariate Normal distribution, denoted by 
</p>
<p>X rv N(/L, ~), if its density is 
</p>
<p>(14.7) 
</p>
<p>where fL is a vector of length k and ~ is a k x k symmetric, positive definite 
</p>
<p>matrix. Then JE(X) = fL and V(X) = ~. </p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Multinomial 235 
</p>
<p>14.2 Theorem. The following properties hold: 
</p>
<p>1. IfZrvN(O,l) andX=p,+2:,1/2Z, thenXrvN(p,,2:,). 
</p>
<p>2. If X rv N(p" 2:,), then 2:,-1/2(X -It) rv N(O, 1). 
</p>
<p>3. If X rv N(/t, 2:,) a is a vector of the same length as X, then aT X rv 
</p>
<p>N(aT p" aT 2:,a). 
</p>
<p>4. Let 
</p>
<p>Then V rv X~. 
</p>
<p>14.3 Theorem. Given a random sample of size n from a N(p" 2:,), the log-
</p>
<p>likelihood is (up to a constant not depending on It or 2:,) given by 
</p>
<p>n - T -1 - n -1 n 
&pound;(p" 2:,) = -"2(X - p,) 2:, (X - p,) - "2tr(2:, S) - "2 log 12:,1&middot; 
</p>
<p>The MLE is 
</p>
<p>Ii = X and f; = (n ~ 1 ) S. (14.8) 
</p>
<p>14.4 Multinornial 
</p>
<p>Let us now review the Multinomial distribution. The data take the form 
</p>
<p>X = (Xl, ... , Xd where each Xj is a count. Think of drawing n balls (with 
</p>
<p>replacement) from an urn which has balls with k different colors. In this case, 
</p>
<p>Xj is the number of balls of the kth color. Let p = (PI, ... ,pd where Pj ~ &deg; 
and L~=l Pj = 1 and suppose that Pj is the probability of drawing a ball of 
</p>
<p>color j. 
</p>
<p>14.4 Theorem. Let X rv Multinomial(n,p). Then the marginal distribution 
</p>
<p>of Xj is Xj rv Binomial(n,pj). The mean and variance of X are 
</p>
<p>( 
npl ) 
</p>
<p>JE(X) = 
</p>
<p>npk 
</p>
<p>and 
</p>
<p>V(X) ~ ( 
npl(l - pI) -nplP2 -nplPk 
</p>
<p>) -nplP2 np2(1 - P2) -np2Pk 
-nplPk -np2Pk npk(l - Pk) </p>
<p/>
</div>
<div class="page"><p/>
<p>236 14. Multivariate Models 
</p>
<p>PROOF. That Xj rv Binomial(n,pj) follows easily. Hence, JE(Xj) = npj and 
</p>
<p>V(Xj) = npj(l - Pj). To compute COV(Xi' Xj) we proceed as follows: Notice 
</p>
<p>that Xi+Xj rv Binomial(n,Pi+pj) and so V(Xi+Xj) = n(Pi+pj)(l-Pi-pj). 
</p>
<p>On the other hand, 
</p>
<p>V(Xi) + V(Xj) + 2COV(Xi' Xj) 
</p>
<p>npi(l - Pi) + npj(l - Pj) + 2COV(Xi' Xj). 
</p>
<p>Equating this last expression with n(Pi+pj)(l-Pi-pj) implies that COV(Xi' Xj) = 
</p>
<p>-npiPj&middot; -
</p>
<p>14.5 Theorem. The maximum likelihood estimator of P is 
</p>
<p>PROOF. The log-likelihood (ignoring a constant) is 
</p>
<p>k 
</p>
<p>R(p) = LXjlogpj. 
j=1 
</p>
<p>When we maximize R we have to be careful since we must enforce the con-
</p>
<p>straint that Lj Pj = 1. We use the method of Lagrange multipliers and instead 
</p>
<p>maximize 
k 
</p>
<p>A(p) = L Xj logpj + A (LPj - 1). 
)=1 ) 
</p>
<p>Now 
</p>
<p>8A(p) = Xj + A. 
8pj Pj 
</p>
<p>Setting aaA(p) = 0 yields Pl&middot; = - X)I A. Since L Pl&middot; = 1 we see that A = -n 
~ ) 
</p>
<p>and hence Pj = Xj/n as claimed. _ 
</p>
<p>Next we would like to know the variability of the MLE. We can either 
</p>
<p>compute the variance matrix of p directly or we can approximate the vari-
ability of the MLE by computing the Fisher information matrix. These two 
</p>
<p>approaches give the same answer in this case. The direct approach is easy: 
</p>
<p>V(p) = V(X/n) = n- 2V(X), and so 
</p>
<p>V(p) = .!.~ 
n </p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Bibliographic Remarks 237 
</p>
<p>where 
</p>
<p>(
</p>
<p>P1(1_ PI) 
</p>
<p>-P1P2 
~= . 
</p>
<p>-P1Pk 
</p>
<p>-P1Pk ) 
-P2Pk 
</p>
<p>Pk(l ~ Pk) . 
For large n, p has approximately a multivariate Normal distribution. 
</p>
<p>14.6 Theorem. As n --+ 00, 
</p>
<p>Fn(p - p) ~ N(O, ~). 
</p>
<p>14.5 Bibliographic Remarks 
</p>
<p>Some references on multivariate analysis are Johnson and Wichern (1982) and 
</p>
<p>Anderson (1984). The method for constructing the confidence interval for the 
</p>
<p>correlation described in this chapter is due to Fisher (1921). 
</p>
<p>14.6 Appendix 
</p>
<p>PROOF of Theorem 14.3. Denote the ith random vector by Xi. The log-
</p>
<p>likelihood is 
</p>
<p>n 
</p>
<p>i=l 
</p>
<p>- k2n 10g(21T) - ~ log I~I - ~ i)xi - 11y~-1(Xi - J.1). 
i=l 
</p>
<p>Now, 
</p>
<p>n 
</p>
<p>2)Xi - IL?~-l(Xi - J.1) 
</p>
<p>i=l 
</p>
<p>n 
</p>
<p>i=l 
</p>
<p>n 
</p>
<p>L [(Xi - X?~-l(Xi - X)] + n(X - J.1?~-l(X - J.1) 
i=l 
</p>
<p>since L~=l (Xi - X)~-l(X - J.1) = O. Also, notice that (Xi - IL)T~-l(Xi - IL) 
</p>
<p>is a scalar, so 
</p>
<p>n n 
</p>
<p>L(Xi - J.1?~-l(Xi - J.1) Ltr [(Xi - IL?~-l(Xi - J.1)] 
</p>
<p>i=l i=l </p>
<p/>
</div>
<div class="page"><p/>
<p>238 14. Multivariate Models 
</p>
<p>and the conclusion follows .&bull; 
</p>
<p>14.7 Exercises 
</p>
<p>1. Prove Theorem 14.1. 
</p>
<p>n 
</p>
<p>i=l 
</p>
<p>tr [~-l ~(Xi - fL)(X i - fLf 1 
</p>
<p>n tr [~-lSl 
</p>
<p>2. Find the Fisher information matrix for the MLE of a Multinomial. 
</p>
<p>3. (Computer Experiment.) Write a function to generate nsim observations 
</p>
<p>from a Multinomial(n,p) distribution. 
</p>
<p>4. (Computer Experiment.) Write a function to generate nsim observations 
</p>
<p>from a Multivariate normal with given mean fL and covariance matrix 
</p>
<p>~. 
</p>
<p>5. (Computer Experiment.) Generate 100 random vectors from a N(fL,~) 
</p>
<p>distribution where 
</p>
<p>Plot the simulation as a scatterplot. Estimate the mean and covariance 
</p>
<p>matrix ~. Find the correlation p between Xl and X 2 . Compare this 
</p>
<p>with the sample correlations from your simulation. Find a 95 percent 
</p>
<p>confidence interval for p. Use two methods: the bootstrap and Fisher's 
</p>
<p>method. Compare. 
</p>
<p>6. (Com puter Experi ment.) Repeat the previous exercise 1000 times. Com-
</p>
<p>pare the coverage of the two confidence intervals for p. </p>
<p/>
</div>
<div class="page"><p/>
<p>15 
</p>
<p>Inference About Independence 
</p>
<p>In this chapter we address the following questions: 
</p>
<p>(1) How do we test if two random variables are independent? 
</p>
<p>(2) How do we estimate the strength of dependence between two 
</p>
<p>random variables? 
</p>
<p>When Y and Z are not independent, we say that they are dependent or 
</p>
<p>associated or related. If Y and Z are associated, it does not imply that Y 
</p>
<p>causes Z or that Z causes Y. Causation is discussed in Chapter 16. 
</p>
<p>Recall that we write Y IT Z to mean that Y and Z are independent and we 
</p>
<p>write Y ro606' Z to mean that Y and Z are dependent. 
</p>
<p>15.1 Two Binary Variables 
</p>
<p>Suppose that Y and Z are both binary and consider data (YI , Zl), ... , (Yn , Zn). 
</p>
<p>We can represent the data as a two-by-two table: 
</p>
<p>Y=Q Y = 1 
</p>
<p>Z=Q Xoo X OI Xo. 
Z=l XlO X 11 Xl. 
</p>
<p>Xo Xl n=X .. </p>
<p/>
</div>
<div class="page"><p/>
<p>240 15. Inference About Independence 
</p>
<p>where 
</p>
<p>X ij = number of observations for which Y = i and Z = j. 
</p>
<p>The dotted subscripts denote sums. Thus, 
</p>
<p>j i,j 
</p>
<p>This is a convention we use throughout the remainder of the book. Denote 
</p>
<p>the corresponding probabilities by: 
</p>
<p>Z=O 
Z=1 
</p>
<p>Y=o Y=1 
</p>
<p>POO 
</p>
<p>PIO 
</p>
<p>p&middot;O 
</p>
<p>POl 
</p>
<p>Pu 
</p>
<p>P&middot;l 
</p>
<p>po&middot; 
</p>
<p>Pl&middot; 
1 
</p>
<p>where Pij = W(Z = i, Y = j). Let X = (Xoo, X Ol , X IO , Xu) denote the vector 
</p>
<p>of counts. Then X rv Multinomial(n,p) where P = (pOO,POl,PlO,PU). It is now 
</p>
<p>convenient to introduce two new parameters. 
</p>
<p>15.1 Definition. The odds ratio is defined to be 
</p>
<p>1jJ = POOPll . 
</p>
<p>POlPlO 
(15.1) 
</p>
<p>The log odds ratio is defined to be 
</p>
<p>'I = log(IjJ). (15.2) 
</p>
<p>15.2 Theorem. The following statements are equivalent: 
</p>
<p>1. YIIZ. 
2. 1jJ = 1. 
3. 'I = o. 
4. For i, j E {O, I}, Pij = Pi&middot;P-j. 
</p>
<p>N ow consider testing 
</p>
<p>Ho: Y II Z versus HI: Y IQOOO' Z. (15.3) 
</p>
<p>First we consider the likelihood ratio test. Under HI, X rv Multinomial(n,p) 
</p>
<p>and the MLE is the vector if = X/no Under Ho, we again have that X rv 
Multinomial( n, p) but the restricted MLE is computed under the constraint 
</p>
<p>Pij = Pi&middot;P&middot;j This leads to the following test: </p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Two Binary Variables 241 
</p>
<p>15.3 Theorem. The likelihood ratio test statistic for (15.3) is 
</p>
<p>(15.4) 
</p>
<p>Under Ho, T ~ xi. Thus, an approximate level ex test is obtained by 
rejecting Ho when T &gt; xL .. 
</p>
<p>Another popular test for independence is Pearson's X2 test. 
</p>
<p>15.4 Theorem. Pearson's X2 test statistic for independence is 
</p>
<p>where 
</p>
<p>1 1 ( )2 
U = "'""'"'""' X ij - Eij 
</p>
<p>~~ E 
i=O j=o 'J 
</p>
<p>E .. _ Xi.X j 
'J -
</p>
<p>n 
</p>
<p>(15.5) 
</p>
<p>Under Ho, U ~ xi. Thus, an approximate level ex test is obtained by 
rejecting Ho when U &gt; xL &bull;. 
</p>
<p>Here is the intuition for the Pearson test. Under H o, Pij = Pi.P.j, so the 
</p>
<p>maximum likelihood estimator of Pij under Ho is 
</p>
<p>~ ~ ~ Xi. Xj 
Pij = Pi&middot; p. j = ----;;: ----;;:. 
</p>
<p>Thus, the expected number of observations in the (i,j) cell is 
</p>
<p>~ Xi.X j 
Eij = npij = ---. 
</p>
<p>n 
</p>
<p>The statistic U compares the observed and expected counts. 
</p>
<p>15.5 Example. The following data from Johnson and Johnson (1972) relate 
</p>
<p>tonsillectomy and Hodgkins disease. 1 
</p>
<p>Tonsillectomy 
</p>
<p>No Tonsillectomy 
</p>
<p>Total 
</p>
<p>Hodgkins Disease 
</p>
<p>90 
</p>
<p>84 
</p>
<p>174 
</p>
<p>No Disease 
</p>
<p>165 
</p>
<p>307 
</p>
<p>472 
</p>
<p>255 
</p>
<p>391 
</p>
<p>646 
</p>
<p>1 The data are actually from a case-control study; see the appendix for an explanation of 
</p>
<p>case-control studies. </p>
<p/>
</div>
<div class="page"><p/>
<p>242 15. Inference About Independence 
</p>
<p>We would like to know if tonsillectomy is related to Hodgkins disease. The 
</p>
<p>likelihood ratio statistic is T = 14.75 and the p-value is J1D(xi &gt; 14.75) = .0001. 
The X2 statistic is U = 14.96 and the p-value is J1D(xi &gt; 14.96) = .0001. We re-
ject the null hypothesis of independence and conclude that tonsillectomy is as-
</p>
<p>sociated with Hodgkins disease. This does not mean that tonsillectomies cause 
</p>
<p>Hodgkins disease. Suppose, for example, that doctors gave tonsillectomies to 
</p>
<p>the most seriously ill patients. Then the association between tonsillectomies 
</p>
<p>and Hodgkins disease may be due to the fact that those with tonsillectomies 
</p>
<p>were the most ill patients and hence more likely to have a serious disease. _ 
</p>
<p>We can also estimate the strength of dependence by estimating the odds 
</p>
<p>ratio ?jJ and the log-odds ratio f. 
</p>
<p>15.6 Theorem. The MLE's of?jJ and I are 
</p>
<p>.J; = XOOX11 9 = log~. 
X01XlO ' 
</p>
<p>(15.6) 
</p>
<p>The asymptotic standard errors (computed using the delta method) are 
</p>
<p>se(9) 
</p>
<p>se(J;) 
</p>
<p>VIllI -+-+-+-
Xoo X01 XlO X 11 
</p>
<p>J; se(9)&middot; 
</p>
<p>(15.7) 
</p>
<p>(15.8) 
</p>
<p>15.7 Remark. For small sample sizes, .J; and 9 can have a very large variance. 
In this case, we often use the modified estimator 
</p>
<p>~ = (X 00 + ~) (X 11 + ~) 
(XOl + ~) (XlO + ~). 
</p>
<p>(15.9) 
</p>
<p>Another test for independence is the Wald test for I = 0 given by W = 
</p>
<p>(9 - 0) / se(9)&middot; A 1 - Q confidence interval for I is 9 &plusmn; za/2se(9)&middot; 
A 1 - Q confidence interval for&cent; can be obtained in two ways. First, we 
</p>
<p>could use ~ &plusmn; Za/2se(~). Second, since ?jJ = e' we could use 
</p>
<p>This second method is usually more accurate. 
</p>
<p>15.8 Example. In the previous example, 
</p>
<p>and 
</p>
<p>J; = 90 x 307 = 1.99 
165 x 84 
</p>
<p>9 = log(1.99) = .69. 
</p>
<p>(15.10) </p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Two Discrete Variables 243 
</p>
<p>So tonsillectomy patients were twice as likely to have Hodgkins disease. The 
</p>
<p>standard error of '1 is 
</p>
<p>1 1 1 1 
90 + 84 + 165 + 307 = .18. 
</p>
<p>The Wald statistic is W = .69;'18 = 3.84 whose p-value is IP'(IZI &gt; 3.84) = 
</p>
<p>.0001, the same as the other tests. A 95 per cent confidence interval for r is 
'1&plusmn;2( .18) = (.33,1.05). A 95 per cent confidence interval for .1jJ is (e&middot; 33 , e1.05) = 
</p>
<p>(1.39,2.86) .&bull; 
</p>
<p>15.2 Two Discrete Variables 
</p>
<p>Now suppose that Y E {I, ... , I} and Z E {I, ... , J} are two discrete vari-
</p>
<p>ables. The data can be represented as an J x J table of counts: 
</p>
<p>Y= 1 Y=2 Y=j Y=J 
Z = 1 Xu X 12 X lj Xu Xl. 
</p>
<p>Z =i XiI X i2 X ij XiJ Xi. 
</p>
<p>Z=J X Il X I2 X lj XIJ XI. 
</p>
<p>Xl X 2 Xj XJ n 
</p>
<p>where 
</p>
<p>X ij = number of observations for which Z = i and Y = j. 
</p>
<p>Consider testing 
</p>
<p>Ho : Y II Z versus HI: Y '15000' Z. 
</p>
<p>15.9 Theorem. The likelihood ratio test statistic for (15.11) is 
</p>
<p>I J 
</p>
<p>T = 2 L L X ij log (X,j X.) . 
X,. X J 
</p>
<p>i=l j=l 
</p>
<p>(15.11) 
</p>
<p>(15.12) 
</p>
<p>The limiting distribution of T under the null hypothesis of independence 
</p>
<p>is X~ where v = (J - I)(J - 1). Pearson's X2 test statistic is 
</p>
<p>I J ( )2 
U = "" X ij - Eij 
</p>
<p>~~ E 
i=l j=l 'J 
</p>
<p>(15.13) </p>
<p/>
</div>
<div class="page"><p/>
<p>244 15. Inference About Independence 
</p>
<p>Asymptotically, under Ha, U has a X~ distribution where 
</p>
<p>v = (I - 1)(J - 1). 
</p>
<p>15.10 Example. These data are from Dunsmore et al. (1987). Patients with 
</p>
<p>Hodgkins disease are classified by their response to treatment and by histo-
</p>
<p>logical type. 
</p>
<p>Type Positive Response Partial Response No Response 
</p>
<p>LP 74 18 12 104 
</p>
<p>NS 68 16 12 96 
</p>
<p>Me 154 54 58 266 
LD 18 10 44 72 
</p>
<p>The X2 test statistic is 75.89 with 2 x 3 = 6 degrees of freedom. The p-value 
</p>
<p>is IP'(X~ &gt; 75.89) :::::; o. The likelihood ratio test statistic is 68.30 with 2 x 3 = 6 
degrees of freedom. The p-value is IP'(X~ &gt; 68.30) :::::; o. Thus there is strong 
evidence that response to treatment and histological type are associated. _ 
</p>
<p>15.3 Two Continuous Variables 
</p>
<p>Now suppose that Y and Z are both continuous. If we assume that the joint 
</p>
<p>distribution of Y and Z is bivariate Normal, then we measure the dependence 
</p>
<p>between Y and Z by means of the correlation coefficient p. Tests, estimates, 
</p>
<p>and confidence intervals for p in the Normal case are given in the previous 
</p>
<p>chapter in Section 14.2. If we do not assume Normality then we can still use the 
</p>
<p>methods in Section 14.2 to draw inferences about the correlation p. However, 
</p>
<p>if we conclude that p is 0, we cannot conclude that Y and Z are independent, 
</p>
<p>only that they are uncorrelated. Fortunately, the reverse direction is valid: 
</p>
<p>if we conclude that Y and Z are correlated than we can conclude they are 
</p>
<p>dependent. 
</p>
<p>15.4 One Continuous Variable and One Discrete 
</p>
<p>Suppose that Y E {I, ... , I} is discrete and Z is continuous. Let Fi(Z) 
</p>
<p>IP'(Z &lt;::: zlY =i) denote the CDF of Z conditional on Y =i. </p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Appendix 245 
</p>
<p>15.11 Theorem. When Y E {I, ... , I} is discrete and Z is continuous, then 
</p>
<p>Y II Z if and only if FI = ... = F J . 
</p>
<p>It follows from the previous theorem that to test for independence, we need 
</p>
<p>to test 
</p>
<p>Ho : FI = ... = F J versus HI: not Ho&middot; 
</p>
<p>For simplicity, we consider the case where I = 2. To test the null hypothesis 
</p>
<p>that FI = F2 we will use the two sample Kolmogorov-Smirnov test. Let 
</p>
<p>nl denote the number of observations for which Yi = 1 and let n2 denote the 
</p>
<p>number of observations for which Y; = 2. Let 
</p>
<p>and 
</p>
<p>denote the empirical distribution function of Z given Y 
</p>
<p>respectively. Define the test statistic 
</p>
<p>D = sup IFI(X) - F2 (x)l. 
x 
</p>
<p>15.12 Theorem. Let 
</p>
<p>00 
</p>
<p>H(t) = 1 - 22) _IF-Ie-2j2t2. 
j=l 
</p>
<p>Under the null hypothesis that FI = F2 , 
</p>
<p>lim JID ( 
n--+oo 
</p>
<p>1 and Y 2 
</p>
<p>(15.14) 
</p>
<p>It follows from the theorem that an approximate level a test is obtained by 
</p>
<p>rejecting Ho when 
</p>
<p>15.5 Appendix 
</p>
<p>INTERPRETING THE ODDS RATIOS. Suppose event A as probability J1D(A). 
</p>
<p>The odds of A are defined as odds(A) = J1D(A)j(l - J1D(A)). It follows that </p>
<p/>
</div>
<div class="page"><p/>
<p>246 15. Inference About Independence 
</p>
<p>f'(A) = odds(A)/(l + odds(A)). Let E be the event that someone is exposed 
to something (smoking, radiation, etc) and let D be the event that they get 
</p>
<p>a disease. The odds of getting the disease given that you are exposed are: 
</p>
<p>f'(DIE) 
odds(DIE) = 1 _ f'(DIE) 
</p>
<p>and the odds of getting the disease given that you are not exposed are: 
</p>
<p>The odds ratio is defined to be 
</p>
<p>.1jJ = odds(DIE) 
</p>
<p>odds(DIEc) . 
</p>
<p>If 1jJ = 1 then disease probability is the same for exposed and unexposed. This 
</p>
<p>implies that these events are independent. Recall that the log-odds ratio is 
</p>
<p>defined as I = log( 1jJ). Independence corresponds to I = o. 
Consider this table of probabilities and corresponding table of data: 
</p>
<p>E C 
</p>
<p>E 
</p>
<p>Now 
</p>
<p>and so 
</p>
<p>DC D 
</p>
<p>POO POI Po&middot; 
</p>
<p>PlO Pu Pl&middot; 
</p>
<p>p&middot;O P&middot;l 1 
</p>
<p>f'(DIE) = Pl1 
PlO + Pl1 
</p>
<p>odds(DIE) = Pl1 
PIO 
</p>
<p>DC D 
</p>
<p>E C Xoo XOl Xo. 
</p>
<p>E XlO Xu Xl. 
</p>
<p>Xo Xl X. 
</p>
<p>and f'(DIEC) = POl , 
POO + POI 
</p>
<p>POI 
and odds(DIEC) = -, 
</p>
<p>POO 
</p>
<p>and therefore, 
</p>
<p>IjJ = Pl1POO. 
</p>
<p>POlPIO 
</p>
<p>To estimate the parameters, we have to first consider how the data were 
</p>
<p>collected. There are three methods. 
</p>
<p>MULTINOMIAL SAMPLING. We draw a sample from the population and, 
</p>
<p>for each person, record their exposure and disease status. In this case, X = 
</p>
<p>(XOO,XOl,XlO,Xl1) rv Multinomial(n,p). We then estimate the probabilities 
</p>
<p>in the table by Pij = X ij / nand 
</p>
<p>.J; = ~l1~OO = Xl1 X OO. 
POIPIO XOIXlO </p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Appendix 247 
</p>
<p>PROSPECTIVE SAMPLING. (COHORT SAMPLING). We get some exposed and 
</p>
<p>unexposed people and count the number with disease in each group. Thus, 
</p>
<p>X OI Binomial(Xo., IP'(DIEC)) 
</p>
<p>XU Binomial(Xl ., IP'(DIE)). 
</p>
<p>We should really write Xo. and Xl. instead of Xo. and Xl. since in this case, 
</p>
<p>these are fixed not random, but for notational simplicity I'll keep using capital 
</p>
<p>letters. We can estimate IP'(DIE) and IP'(DIEC) but we cannot estimate all the 
</p>
<p>probabilities in the table. Still, we can estimate 1/J since 1/J is a function of 
</p>
<p>IP'(DIE) and IP'(DIEC). Now 
</p>
<p>and 
</p>
<p>Thus, 
</p>
<p>just as before. 
</p>
<p>CASE-CONTROL (RETROSPECTIVE) SAMPLING. Here we get some diseased 
</p>
<p>and non-diseased people and we observe how many are exposed. This is much 
</p>
<p>more efficient if the disease is rare. Hence, 
</p>
<p>XlO Binomial(Xo, IP'(EIDC)) 
</p>
<p>XU Binomial(Xl,IP'(EID)). 
</p>
<p>From these data we can estimate IP'(EID) and IP'(EIDC). Surprisingly, we can 
</p>
<p>also still estimate 1/J. To understand why, note that 
</p>
<p>IP'(EID) = Pu , 
POI + Pu 
</p>
<p>By a similar argument, 
</p>
<p>Hence, 
</p>
<p>1 - IP'(EID) = POI ,odds(EID) = Pl1 . 
POI + Pl1 POI 
</p>
<p>odds(EIDC) = PlO . 
POO 
</p>
<p>odds(EID) = PuPOO = 1/J 
odds(EIDc) POIPIO . 
</p>
<p>From the data, we form the following estimates: 
</p>
<p>Therefore, 
</p>
<p>.j; = XOOXl1 . 
XOIXlO </p>
<p/>
</div>
<div class="page"><p/>
<p>248 15. Inference About Independence 
</p>
<p>So in all three data collection methods, the estimate of 4) turns out to be the 
same. 
</p>
<p>It is tempting to try to estimate IP'(DIE) -IP'(DIEC). In a case-control design, 
</p>
<p>this quantity is not estimable. To see this, we apply Bayes' theorem to get 
</p>
<p>Because of the way we obtained the data, IP'( D) is not estimable from the data. 
</p>
<p>However, we can estimate ~ = IP'(DIE)/IP'(DIEC), which is called the relative 
</p>
<p>risk, under the rare disease assumption. 
</p>
<p>15.13 Theorem. Let ~ = IP'(DIE)/IP'(DIEC). Then 
</p>
<p>! --+ 1 
~ 
</p>
<p>as IP'(D) --+ O. 
</p>
<p>Thus, under the rare disease assumption, the relative risk is approximately 
</p>
<p>the same as the odds ratio and, as we have seen, we can estimate the odds 
</p>
<p>ratio. 
</p>
<p>15.6 Exercises 
</p>
<p>1. Prove Theorem 15.2. 
</p>
<p>2. Prove Theorem 15.3. 
</p>
<p>3. Prove Theorem 15.6. 
</p>
<p>4. The New York Times (January 8,2003, page A12) reported the following 
</p>
<p>data on death sentencing and race, from a study in Maryland: 2 
</p>
<p>Black Victim 
</p>
<p>White Victim 
</p>
<p>Death Sentence 
</p>
<p>14 
</p>
<p>62 
</p>
<p>No Death Sentence 
</p>
<p>641 
</p>
<p>594 
</p>
<p>Analyze the data using the tools from this chapter. Interpret the results. 
</p>
<p>Explain why, based only on this information, you can't make causal 
</p>
<p>conclusions. (The authors of the study did use much more information 
</p>
<p>in their full report.) 
</p>
<p>2The data here are an approximate re-creation using the information in the article. </p>
<p/>
</div>
<div class="page"><p/>
<p>15.6 Exercises 249 
</p>
<p>5. Analyze the data on the variables Age and Financial Status from: 
</p>
<p>http://lib.stat.cmu.edu/DASL/Datafiles/montanadat.html 
</p>
<p>6. Estimate the correlation between temperature and latitude using the 
</p>
<p>data from 
</p>
<p>http://lib.stat.cmu.edu/DASL/Datafiles/USTemperatures.html 
</p>
<p>Use the correlation coefficient. Provide estimates, tests, and confidence 
</p>
<p>intervals. 
</p>
<p>7. Test whether calcium intake and drop in blood pressure are associated. 
</p>
<p>Use the data in 
</p>
<p>http://lib.stat.cmu.edu/DASL/Datafiles/Calcium.html </p>
<p/>
</div>
<div class="page"><p/>
<p>16 
</p>
<p>Causal Inference 
</p>
<p>Roughly speaking, the statement "X causes Y" means that changing the 
</p>
<p>value of X will change the distribution of Y. When X causes Y, X and Y 
</p>
<p>will be associated but the reverse is not, in general, true. Association does not 
</p>
<p>necessarily imply causation. We will consider two frameworks for discussing 
</p>
<p>causation. The first uses counterfactual random variables. The second, pre-
</p>
<p>sented in the next chapter, uses directed acyclic graphs. 
</p>
<p>16.1 The Counterfactual Model 
</p>
<p>Suppose that X is a binary treatment variable where X = 1 means "treated" 
</p>
<p>and X = 0 means "not treated." We are using the word "treatment" in a 
</p>
<p>very broad sense. Treatment might refer to a medication or something like 
</p>
<p>smoking. An alternative to "treated/not treated" is "exposed/not exposed" 
</p>
<p>but we shall use the former. 
</p>
<p>Let Y be some outcome variable such as presence or absence of disease. 
</p>
<p>To distinguish the statement "X is associated Y" from the statement "X 
</p>
<p>causes Y" we need to enrich our probabilistic vocabulary. Specifically, we will 
</p>
<p>decompose the response Y into a more fine-grained object. 
</p>
<p>We introduce two new random variables (Co, Cd, called potential out-
</p>
<p>comes with the following interpretation: Co is the outcome if the subject is </p>
<p/>
</div>
<div class="page"><p/>
<p>252 16. Causal Inference 
</p>
<p>not treated (X = 0) and C I is the outcome if the subject is treated (X = 1). 
</p>
<p>Hence, 
</p>
<p>if X = 0 
</p>
<p>if X = 1. 
</p>
<p>We can express the relationship between Y and (Co, Cd more succinctly by 
</p>
<p>Y=Cx . (16.1) 
</p>
<p>Equation (16.1) is called the consistency relationship. 
</p>
<p>Here is a toy dataset to make the idea clear: 
</p>
<p>X Y Co C I 
</p>
<p>0 4 4 * 
0 7 7 * 
0 2 2 * 
0 8 8 * 
1 3 * 3 
1 5 * 5 
1 8 * 8 
1 9 * 9 
</p>
<p>The asterisks denote unobserved values. When X = 0 we don't observe C I , 
</p>
<p>in which case we say that C I is a counterfactual since it is the outcome 
</p>
<p>you would have had if, counter to the fact, you had been treated (X = 1). 
</p>
<p>Similarly, when X = 1 we don't observe Co, and we say that Co is counter-
</p>
<p>factual. There are four types of subjects: 
</p>
<p>Type Co C I 
</p>
<p>Survivors 1 1 
</p>
<p>Responders 0 1 
</p>
<p>Anti-responders 1 0 
</p>
<p>Doomed 0 0 
</p>
<p>Think of the potential outcomes (Co, Cd as hidden variables that contain all 
</p>
<p>the relevant information about the subject. 
</p>
<p>Define the average causal effect or average treatment effect to be 
</p>
<p>e = JE(Cd - JE(Co). (16.2) 
</p>
<p>The parameter e has the following interpretation: e is the mean if everyone 
were treated (X = 1) minus the mean if everyone were not treated (X = 0). 
</p>
<p>There are other ways of measuring the causal effect. For example, if Co and 
</p>
<p>C I are binary, we define the causal odds ratio 
</p>
<p>W(CI = 1) . W(Co = 1) 
</p>
<p>W( C I = 0) -;- W( Co = 0) </p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 The Counterfactual Model 253 
</p>
<p>and the causal relative risk 
</p>
<p>JP'(C1 = 1) 
</p>
<p>JP'(Co = 1)' 
</p>
<p>The main ideas will be the same whatever causal effect we use. For simplicity, 
</p>
<p>we shall work with the average causal effect e. 
Define the association to be 
</p>
<p>00 = lE(YIX = 1) -lE(YIX = 0). (16.3) 
</p>
<p>Again, we could use odds ratios or other summaries if we wish. 
</p>
<p>16.1 Theorem (Association Is Not Causation). In general, e # 00. 
</p>
<p>16.2 Example. Suppose the whole population is as follows: 
</p>
<p>X Y Co C1 
</p>
<p>0 0 0 0* 
</p>
<p>0 0 0 0* 
</p>
<p>0 0 0 0* 
</p>
<p>0 0 0 0* 
</p>
<p>1 1 1* 1 
</p>
<p>1 1 1* 1 
</p>
<p>1 1 1* 1 
</p>
<p>1 1 1* 1 
</p>
<p>Again, the asterisks denote unobserved values. Notice that Co = C 1 for every 
</p>
<p>subject, thus, this treatment has no effect. Indeed, 
</p>
<p>e 
1 8 1 8 
</p>
<p>lE(CI) -lE(Co) = "8 L C1i - "8 L COi 
i=l i=l 
</p>
<p>0+0+0+0+1+1+1+1 
</p>
<p>8 
o. 
</p>
<p>0+0+0+0+1+1+1+1 
</p>
<p>8 
</p>
<p>Thus, the average causal effect is O. The observed data are only the X's and 
</p>
<p>Y's, from which we can estimate the association: 
</p>
<p>Hence, e # 00. 
</p>
<p>00 lE(YIX = 1) -lE(YIX = 0) 
</p>
<p>1+1+1+1 0+0+0+0 
4 --4--=1. 
</p>
<p>To add some intuition to this example, imagine that the outcome variable 
</p>
<p>is 1 if "healthy" and 0 if "sick". Suppose that X = 0 means that the subject </p>
<p/>
</div>
<div class="page"><p/>
<p>254 16. Causal Inference 
</p>
<p>does not take vitamin C and that X = 1 means that the subject does take 
</p>
<p>vitamin C. Vitamin C has no causal effect since Co = C 1 for each subject. In 
</p>
<p>this example there are two types of people: healthy people (Co, Cd = (1,1) 
and unhealthy people (Co, C1 ) = (0,0). Healthy people tend to take vitamin 
</p>
<p>C while unhealthy people don't. It is this association between (Co, C1 ) and 
</p>
<p>X that creates an association between X and Y. If we only had data on X 
</p>
<p>and Y we would conclude that X and Yare associated. Suppose we wrongly 
</p>
<p>interpret this causally and conclude that vitamin C prevents illness. Next we 
</p>
<p>might encourage everyone to take vitamin C. If most people comply with our 
</p>
<p>advice, the population will look something like this: 
</p>
<p>X Y Co C1 
</p>
<p>0 0 0 0* 
</p>
<p>1 0 0 0* 
</p>
<p>1 0 0 0* 
</p>
<p>1 0 0 0* 
</p>
<p>1 1 1* 1 
</p>
<p>1 1 1* 1 
</p>
<p>1 1 1* 1 
</p>
<p>1 1 1* 1 
</p>
<p>Now a = (4/7) - (0/1) = 4/7. We see that a went down from 1 to 4/7. 
</p>
<p>Of course, the causal effect never changed but the naive observer who does 
</p>
<p>not distinguish association and causation will be confused because his advice 
</p>
<p>seems to have made things worse instead of better .&bull; 
</p>
<p>In the last example, 8 = 0 and a = 1. It is not hard to create examples in 
</p>
<p>which a &gt; 0 and yet 8 &lt; o. The fact that the association and causal effects 
can have different signs is very confusing to many people. 
</p>
<p>The example makes it clear that, in general, we cannot use the association 
</p>
<p>to estimate the causal effect 8. The reason that 8 -I- a is that (Co, C1 ) was 
not independent of X. That is, treatment assignment was not independent of 
</p>
<p>person type. 
</p>
<p>Can we ever estimate the causal effect? The answer is: sometimes. In par-
</p>
<p>ticular, random assignment to treatment makes it possible to estimate 8. 
</p>
<p>16.3 Theorem. Suppose we randomly assign subjects to treatment and that 
</p>
<p>JP'(X = 0) &gt; 0 and JP'(X = 1) &gt; o. Then a = 8. Hence, any consistent estima-
tor of a is a consistent estimator of 8. In particular, a consistent estimator 
</p>
<p>zs 
</p>
<p>8 E(YIX = 1) - E(YIX = 0) </p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Beyond Binary Treatments 255 
</p>
<p>is a consistent estimator of e, where 
</p>
<p>_ 1 n 
</p>
<p>Yo = - LY'i(I- Xi), 
no 
</p>
<p>i=l 
</p>
<p>PROOF. Since X is randomly assigned, X is independent of (Co, C 1 ). Hence, 
</p>
<p>e IE(Cd - IE(Co) 
IE(C1 IX = 1) - IE(CoIX = 0) 
</p>
<p>IE(YIX = 1) - IE(YIX = 0) 
</p>
<p>ex. 
</p>
<p>since X II (Co, Cd 
</p>
<p>since Y = Cx 
</p>
<p>The consistency follows from the law of large numbers. _ 
</p>
<p>If Z is a covariate, we define the conditional causal effect by 
</p>
<p>For example, if Z denotes gender with values Z = 0 (women) and Z = 1 
</p>
<p>(men), then eo is the causal effect among women and e1 is the causal effect 
among men. In a randomized experiment, ez = IE(YIX = I, Z = z)-IE(YIX = 
0, Z = z) and we can estimate the conditional causal effect using appropriate 
</p>
<p>sample averages. 
</p>
<p>Summary of the Counterfactual Model 
</p>
<p>Random variables: (Co, C1 , X, Y). 
</p>
<p>Consistency relationship: Y = Cx. 
</p>
<p>Causal Effect: e = IE(Cd - IE(Co). 
Association: ex = IE(YIX = 1) - IE(YIX = 0). 
</p>
<p>Random assignment ===} ( Co, C 1) II X ===} e = ex. 
</p>
<p>16.2 Beyond Binary Treatrnents 
</p>
<p>Let us now generalize beyond the binary case. Suppose that X E X. For 
</p>
<p>example, X could be the dose of a drug in which case X E Ji{. The counterfac-
</p>
<p>tual vector (Co, Cd now becomes the counterfactual function C(x) where </p>
<p/>
</div>
<div class="page"><p/>
<p>256 16. Causal Inference 
</p>
<p>7 
</p>
<p>6 
</p>
<p>5 
</p>
<p>Y = C(X)4 
</p>
<p>3 
</p>
<p>2 
</p>
<p>1 
</p>
<p>0 
</p>
<p>0 1 2 3 4 5 6 7 8 9 10 11 
X 
</p>
<p>x 
</p>
<p>FIGURE 16.1. A counterfactual function C(x). The outcome Y is the value of the 
curve C(x) evaluated at the observed dose X. 
</p>
<p>C(x) is the outcome a subject would have if he received dose x. The observed 
</p>
<p>response is given by the consistency relation 
</p>
<p>Y == C(X). (16.4) 
</p>
<p>See Figure 16.1. The causal regression function is 
</p>
<p>8(x) = lE(C(x)). (16.5) 
</p>
<p>The regression function, which measures association, is r(x) = lE(YIX = x). 
</p>
<p>16.4 Theorem. In general, 8(x) # r(x). However, when X is randomly as-
signed, 8(x) = r(x). 
</p>
<p>16.5 Example. An example in which 8(x) is constant but r(x) is not constant 
</p>
<p>is shown in Figure 16.2. The figure shows the counterfactual functions for 
</p>
<p>four subjects. The dots represent their X values Xl, X 2 , X 3 , X4. Since Ci(x) 
</p>
<p>is constant over x for all i, there is no causal effect and hence 
</p>
<p>8(x) = Cl(x) + C2 (x) + C3 (x) + C4 (x) 
4 </p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Observational Studies and Confounding 257 
</p>
<p>is constant. Changing the dose x will not change anyone's outcome. The four 
</p>
<p>dots in the lower plot represent the observed data points Y1 = C1(Xd, Y2 = 
</p>
<p>C2 (X2 ), Y3 = C3 (X3 ), Y4 = C4 (X4 ). The dotted line represents the regression 
</p>
<p>r(x) = IE(YIX = x). Although there is no causal effect, there is an association 
</p>
<p>since the regression curve r(x) is not constant. _ 
</p>
<p>16.3 Observational Studies and Confounding 
</p>
<p>A study in which treatment (or exposure) is not randomly assigned is called an 
</p>
<p>observational study. In these studies, subjects select their own value of the 
</p>
<p>exposure X. Many of the health studies you read about in the newspaper are 
</p>
<p>like this. As we saw, association and causation could in general be quite differ-
</p>
<p>ent. This discrepancy occurs in non-randomized studies because the potential 
</p>
<p>outcome C is not independent of treatment X. However, suppose we could 
</p>
<p>find groupings of subjects such that, within groups, X and {C(x): x E X} 
</p>
<p>are independent. This would happen if the subjects are very similar within 
</p>
<p>groups. For example, suppose we find people who are very similar in age, gen-
</p>
<p>der, educational background, and ethnic background. Among these people we 
</p>
<p>might feel it is reasonable to assume that the choice of X is essentially ran-
</p>
<p>dom. These other variables are called confounding variables. 1 If we denote 
</p>
<p>these other variables collectively as Z, then we can express this idea by saying 
</p>
<p>that 
</p>
<p>{C(x) : x E X} II XIZ. (16.6) 
</p>
<p>Equation (16.6) means that, within groups of Z, the choice of treatment X 
</p>
<p>does not depend on type, as represented by {C(x): x E X}. If (16.6) holds 
</p>
<p>and we observe Z then we say that there is no unmeasured confounding. 
</p>
<p>16.6 Theorem. Suppose that (16.6) holds. Then, 
</p>
<p>8(x) = J IE(YIX = x, Z = z)dFz(z)dz. (16.7) 
If i(x, z) is a consistent estimate of the regression function IE(YIX = x, Z = 
</p>
<p>z), then a consistent estimate of 8(x) is 
</p>
<p>1 A more precise definition of confounding is given in the next chapter. </p>
<p/>
</div>
<div class="page"><p/>
<p>258 16. Causal Inference 
</p>
<p>&bull; C1(x) 
</p>
<p>3 &bull; C2 (x) 
</p>
<p>2 &bull; C3 (x) 
</p>
<p>1 &bull; C4 (x) 
</p>
<p>0 x 
</p>
<p>0 1 2 3 4 5 
</p>
<p>&bull; Y1 
</p>
<p>3 &bull; Y2 
e(x) 
</p>
<p>2 Y3 &bull; 
</p>
<p>1 Y4 &bull; 
</p>
<p>0 X 
</p>
<p>0 1 2 3 4 5 
</p>
<p>FIGURE 16.2. The top plot shows the counterfactual function C(x) for four sub-
jects. The dots represent their X values. Since Ci (x) is constant over x for all i, there 
is no causal effect. Changing the dose will not change anyone's outcome. The lower 
</p>
<p>plot shows the causal regression function e( x) = (C1 (x) + C2 (x) + C3 (x) + C4 (x)) / 4. 
The four dots represent the observed data points Y1 = C 1 (X1 ), Y2 = C2 (X2 ), 
</p>
<p>Y3 C3 (X3 ), Y4 C4(X4). The dotted line represents the regression 
r(x) = lE(YIX = x). There is no causal effect since Ci(x) is constant for all i. 
But there is an association since the regression curve r(x) is not constant. </p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 Simpson's Paradox 259 
</p>
<p>In particular, if r(x, z) = Po + PIX + P2Z is linear, then a consistent estimate 
ofB(x) is 
</p>
<p>(16.8) 
</p>
<p>where (/30, /31, /32) are the least squares estimators. 
</p>
<p>16.7 Remark. It is useful to compare equation (16.7) to IE(YIX = x) which 
</p>
<p>can be written as IE(YIX = x) = JIE(YIX = x, Z = z)dFz1x(zlx). 
</p>
<p>Epidemiologists call (16.7) the adjusted treatment effect. The process of 
</p>
<p>computing adjusted treatment effects is called adjusting (or controlling) 
</p>
<p>for confounding. The selection of what confounders Z to measure and con-
</p>
<p>trol for requires scientific insight. Even after adjusting for confounders, we 
</p>
<p>cannot be sure that there are not other confounding variables that we missed. 
</p>
<p>This is why observational studies must be treated with healthy skepticism. 
</p>
<p>Results from observational studies start to become believable when: (i) the 
</p>
<p>results are replicated in many studies, (ii) each of the studies controlled for 
</p>
<p>plausible confounding variables, (iii) there is a plausible scientific explanation 
</p>
<p>for the existence of a causal relationship. 
</p>
<p>A good example is smoking and cancer. Numerous studies have shown a 
</p>
<p>relationship between smoking and cancer even after adjusting for many con-
</p>
<p>founding variables. Moreover, in laboratory studies, smoking has been shown 
</p>
<p>to damage lung cells. Finally, a causal link between smoking and cancer has 
</p>
<p>been found in randomized animal studies. It is this collection of evidence 
</p>
<p>over many years that makes this a convincing case. One single observational 
</p>
<p>study is not, by itself, strong evidence. Remember that when you read the 
</p>
<p>newspaper. 
</p>
<p>16.4 Sirnpson's Paradox 
</p>
<p>Simpson's paradox is a puzzling phenomenon that is discussed in most statis-
</p>
<p>tics texts. Unfortunately, most explanations are confusing (and in some cases 
</p>
<p>incorrect). The reason is that it is nearly impossible to explain the paradox 
</p>
<p>without using counterfactuals (or directed acyclic graphs). 
</p>
<p>Let X be a binary treatment variable, Y a binary outcome, and Z a third 
</p>
<p>binary variable such as gender. Suppose the joint distribution of X, Y, Z is </p>
<p/>
</div>
<div class="page"><p/>
<p>260 16. Causal Inference 
</p>
<p>Y=l Y=O Y=l Y=O 
</p>
<p>X= 1 .1500 
</p>
<p>.0375 
</p>
<p>Z = 1 (men) 
</p>
<p>The marginal distribution for (X, Y) is 
</p>
<p>Y = 1 
</p>
<p>X = 1 .25 
</p>
<p>X=O .30 
</p>
<p>.55 
</p>
<p>From these tables we find that, 
</p>
<p>Y =0 
</p>
<p>.25 
</p>
<p>.20 
</p>
<p>.45 
</p>
<p>.50 
</p>
<p>.50 
</p>
<p>1 
</p>
<p>IP'(Y = 11X = 1) -IP'(Y = 11X = 0) 
</p>
<p>IP'(Y = 11X = 1, Z = 1) -IP'(Y = 11X = 0, Z = 1) 
</p>
<p>IP'(Y = 11X = 1,Z = 0) -IP'(Y = 11X = O,Z = 0) 
</p>
<p>To summarize, we seem to have the following information: 
</p>
<p>-0.1 
</p>
<p>0.1 
</p>
<p>0.1. 
</p>
<p>Mathematical Statement English Statement? 
</p>
<p>lP'(Y = 11X = 1) &lt; lP'(Y = 11X = 0) treatment is harmful 
lP'(Y = 11X = 1, Z = 1) &gt; lP'(Y = 11X = 0, Z = 1) treatment is beneficial to men 
lP'(Y = 11X = 1, Z = 0) &gt; lP'(Y = 11X = 0, Z = 0) treatment is beneficial to women 
</p>
<p>Clearly, something is amiss. There can't be a treatment which is good for 
</p>
<p>men, good for women, but bad overall. This is nonsense. The problem is with 
</p>
<p>the set of English statements in the table. Our translation from math into 
</p>
<p>English is specious. 
</p>
<p>The inequality IP'(Y = 11X = 1) &lt; IP'(Y = 11X = 0) does not 
</p>
<p>mean that treatment is harmful. 
</p>
<p>The phrase "treatment is harmful" should be written mathematically as 
</p>
<p>IP'( C1 = 1) &lt; IP'( Co = 1). The phrase "treatment is harmful for men" should 
</p>
<p>be written IP'(C1 = liZ = 1) &lt; IP'(Co = liZ = 1). The three mathematical 
statements in the table are not at all contradictory. It is only the translation 
</p>
<p>into English that is wrong. 
</p>
<p>Let us nOW show that a real Simpson's paradox cannot happen, that is, 
</p>
<p>there cannot be a treatment that is beneficial for men and women but harmful 
</p>
<p>overall. Suppose that treatment is beneficial for both sexes. Then 
</p>
<p>IP'(C1 = liZ = z) &gt; IP'(Co = liZ = z) </p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Bibliographic Remarks 261 
</p>
<p>for all z. It then follows that 
</p>
<p>LIP'(C1 = liZ = z)IP'(Z = z) 
z 
</p>
<p>&gt; LIP'(Co = liZ = z)IP'(Z = z) 
z 
</p>
<p>IP'(Co = 1). 
</p>
<p>Hence, IP'(C1 = 1) &gt; IP'(Co = 1), so treatment is beneficial overall. No paradox. 
</p>
<p>16.5 Bibliographic Remarks 
</p>
<p>The use of potential outcomes to clarify causation is due mainly to Jerzy Ney-
</p>
<p>man and Donald Rubin. Later developments are due to Jamie Robins, Paul 
</p>
<p>Rosenbaum, and others. A parallel development took place in econometrics 
</p>
<p>by various people including James Heckman and Charles Manski. Texts on 
</p>
<p>causation include Pearl (2000), Rosenbaum (2002), Spirtes et al. (2000), and 
</p>
<p>van der Laan and Robins (2003). 
</p>
<p>16.6 Exercises 
</p>
<p>l. Create an example like Example 16.2 in which Q &gt; 0 and B &lt; O. 
</p>
<p>2. Prove Theorem 16.4. 
</p>
<p>3. Suppose you are given data (Xl, Yd, ... , (Xn' Yn ) from an observational 
</p>
<p>study, where Xi E {O, 1} and Yi E {O, 1}. Although it is not possible 
to estimate the causal effect B, it is possible to put bounds on B. Find 
</p>
<p>upper and lower bounds on B that can be consistently estimated from 
</p>
<p>the data. Show that the bounds have width l. 
</p>
<p>Hint: Note that JE(Cd = JE(C1 IX = l)IP'(X = 1) + JE(C1 IX = O)IP'(X = 
0). 
</p>
<p>4. Suppose that X E Jl{ and that, for each subject i, Ci(x) = (31iX. Each 
</p>
<p>su b ject has their own slope (31 i. Construct a joint distribution on ((31, X) 
</p>
<p>such that 1P'((31 &gt; 0) = 1 but JE(YIX = x) is a decreasing function of x, 
</p>
<p>where Y = C(X). Interpret. 
</p>
<p>5. Let X E {O, 1} be a binary treatment variable and let (Co, Cd denote 
</p>
<p>the corresponding potential outcomes. Let Y = Cx denote the observed </p>
<p/>
</div>
<div class="page"><p/>
<p>262 16. Causal Inference 
</p>
<p>response. Let Fo and Fl be the cumulative distribution functions for 
</p>
<p>Co and C l . Assume that Fo and Fl are both continuous and strictly 
</p>
<p>increasing. Let e = rnl - rno where rno = FO- l (1/2) is the median of Co 
and rnl = F l- l (1/2) is the median of C l . Suppose that the treatment X 
</p>
<p>is assigned randomly. Find an expression for e involving only the joint 
distribution of X and Y. </p>
<p/>
</div>
<div class="page"><p/>
<p>17 
</p>
<p>Directed Graphs and Conditional 
Independence 
</p>
<p>17.1 Introduction 
</p>
<p>A directed graph consists of a set of nodes with arrows between some nodes. 
</p>
<p>An example is shown in Figure 17.1. 
</p>
<p>Graphs are useful for representing independence relations between variables. 
</p>
<p>They can also be used as an alternative to counterfactuals to represent causal 
</p>
<p>relationships. Some people use the phrase Bayesian network to refer to a 
</p>
<p>directed graph endowed with a probability distribution. This is a poor choice 
</p>
<p>of terminology. Statistical inference for directed graphs can be performed using 
</p>
<p>y 
</p>
<p>x 
</p>
<p>FIGURE 17.1. A directed graph with vertices V 
</p>
<p>E = {(Y, X), (Y, Z) }. 
</p>
<p>z 
</p>
<p>{X, Y, Z} and edges </p>
<p/>
</div>
<div class="page"><p/>
<p>264 17. Directed Graphs and Conditional Independence 
</p>
<p>frequentist or Bayesian methods, so it is misleading to call them Bayesian 
</p>
<p>networks. 
</p>
<p>Before getting into details about directed acyclic graphs (DAGs), we need 
</p>
<p>to discuss conditional independence. 
</p>
<p>17.2 Conditional Independence 
</p>
<p>17.1 Definition. Let X, Y and Z be random variables. X and Yare 
</p>
<p>conditionally independent given Z, written X II Y I Z, if 
</p>
<p>(17.1) 
</p>
<p>for all x, y and z. 
</p>
<p>Intuitively, this means that, once you know Z, Y provides no extra infor-
</p>
<p>mation about X. An equivalent definition is that 
</p>
<p>f(xly, z) = f(xlz). (17.2) 
</p>
<p>The conditional independence relation satisfies some basic properties. 
</p>
<p>17.2 Theorem. The following implications hold: 1 
</p>
<p>XIIYIZ ===? YIIXIZ 
</p>
<p>XIIYIZ and U = h(X) ===? UIIYIZ 
</p>
<p>XIIYIZ and U = h(X) ===? X II Y I (Z, U) 
</p>
<p>X II Y I Z and X II WI (Y, Z) ===? XII (W, Y) I Z 
</p>
<p>X II Y I Z and X II Z I Y ===? X II (Y, Z). 
</p>
<p>17.3 DAGs 
</p>
<p>A directed graph 9 consists of a set of vertices V and an edge set E of 
ordered pairs of vertices. For our purposes, each vertex will correspond to a 
</p>
<p>random variable. If (X, Y) E E then there is an arrow pointing from X to Y. 
</p>
<p>See Figure 17.1. 
</p>
<p>1 The last property requires the assumption that all events have positive probability; the first 
</p>
<p>four do not. </p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 DAGs 265 
</p>
<p>overweight smoking 
</p>
<p>~/~ 
heart disease cough 
</p>
<p>FIGURE 17.2. DAG for Example 17.4. 
</p>
<p>If an arrow connects two variables X and Y (in either direction) we say 
</p>
<p>that X and Yare adjacent. If there is an arrow from X to Y then X is a 
</p>
<p>parent of Y and Y is a child of X. The set of all parents of X is denoted 
</p>
<p>by Jrx or Jr(X). A directed path between two variables is a set of arrows 
</p>
<p>all pointing in the same direction linking one variable to the other such as: 
</p>
<p>X &bull; &bull; &bull; 
</p>
<p>A sequence of adjacent vertices staring with X and ending with Y but 
</p>
<p>ignoring the direction of the arrows is called an undirected path. The se-
</p>
<p>quence {X, Y, Z} in Figure 17.1 is an undirected path. X is an ancestor of 
</p>
<p>Y if there is a directed path from X to Y (or X = Y). We also say that Y is 
</p>
<p>a descendant of X. 
</p>
<p>A configuration of the form: 
</p>
<p>X ----~~~ Y ~E~----- Z 
</p>
<p>is called a collider at Y. A configuration not of that form is called a non-
</p>
<p>collider, for example, 
</p>
<p>X --~~~ Y </p>
<p/>
</div>
<div class="page"><p/>
<p>266 17. Directed Graphs and Conditional Independence 
</p>
<p>or 
</p>
<p>X .... II(E---- Y .... II(E--- Z 
</p>
<p>The collider property is path dependent. In Figure 17.7, Y is a collider on 
</p>
<p>the path {X, Y, Z} but it is a non-collider on the path {X, Y, W}. When the 
</p>
<p>variables pointing into the collider are not adjacent, we say that the collider 
</p>
<p>is unshielded. A directed path that starts and ends at the same variable is 
</p>
<p>called a cycle. A directed graph is acyclic if it has no cycles. In this case we 
</p>
<p>say that the graph is a directed acyclic graph or DAG. From now on, we 
</p>
<p>only deal with acyclic graphs. 
</p>
<p>17.4 Probability and DAGs 
</p>
<p>Let Q be a DAG with vertices V = (Xl' ... ' Xk). 
</p>
<p>17.3 Definition. If rl' is a distribution for V with probability function f, 
</p>
<p>we say that rl' is Markov to Q, or that Q represents rl', if 
</p>
<p>k 
</p>
<p>f(v) = II f(Xi I 'ifi) (17.3) 
i=l 
</p>
<p>where'ifi are the parents of Xi. The set of distributions represented by Q 
</p>
<p>is denoted by M (Q) . 
</p>
<p>17.4 Example. Figure 17.2 shows a DAG with four variables. The probability 
</p>
<p>function for this example factors as 
</p>
<p>f (overweight , smoking, heart disease, cough) 
</p>
<p>f (overweight) x f (smoking) 
</p>
<p>x f(heart disease I overweight, smoking) 
</p>
<p>x f(cough I smoking). -
</p>
<p>17.5 Example. For the DAG in Figure 17.3, rl' E M(Q) if and only if its 
</p>
<p>probability function f has the form 
</p>
<p>f(x, y, z, w) = f(x)f(y)f(z I x, y)f(w I z). -</p>
<p/>
</div>
<div class="page"><p/>
<p>17.5 More Independence Relations 267 
</p>
<p>z ~W 
</p>
<p>Y 
</p>
<p>FIGURE 17.3. Another DAG. 
</p>
<p>The following theorem says that lP' E NI(Q) if and only if the Markov 
</p>
<p>Condition holds. Roughly speaking, the Markov Condition meanS that every 
</p>
<p>variable W is independent of the "past" given its parents. 
</p>
<p>17.6 Theorem. A distribution lP' E M(Q) if and only if the following Markov 
</p>
<p>Condition holds: for every variable W, 
</p>
<p>WII W l7rw (17.4) 
</p>
<p>where W denotes all the other variables except the parents and descendants 
</p>
<p>ofW. 
</p>
<p>17.7 Example. In Figure 17.3, the Markov Condition implies that 
</p>
<p>X II Y and W II {X, Y} I Z. &bull; 
</p>
<p>17.8 Example. Consider the DAG in Figure 17.4. In this case probability 
</p>
<p>function must factor like 
</p>
<p>f(a, b, c, d, e) = f(a)f(bla)f(cla)f(dlb, c)f(eld). 
</p>
<p>The Markov Condition implies the following independence relations: 
</p>
<p>DIIA I {B,C}, EII{A,B,C} I D and BIIC I A &bull; 
</p>
<p>17.5 More Independence Relations 
</p>
<p>The Markov Condition allows us to list some independence relations implied 
</p>
<p>by a DAG. These relations might imply other independence relations. Con-</p>
<p/>
</div>
<div class="page"><p/>
<p>268 17. Directed Graphs and Conditional Independence 
</p>
<p>B 
</p>
<p>/~ 
A D ~E 
</p>
<p>~/ 
C 
</p>
<p>FIGURE 17.4. Yet another DAG. 
</p>
<p>sider the DAG in Figure 17.5. The Markov Condition implies: 
</p>
<p>It turns out (but it is not obvious) that these conditions imply that 
</p>
<p>How do we find these extra independence relations? The answer is "d-
</p>
<p>separation" which means "directed separation." d-separation can be summa-
</p>
<p>rized by three rules. Consider the four DAG's in Figure 17.6 and the DAG in 
</p>
<p>Figure 17.7. The first 3 DAG's in Figure 17.6 have no colliders. The DAG in 
</p>
<p>the lower right of Figure 17.6 has a collider. The DAG in Figure 17.7 has a 
</p>
<p>collider with a descendant. </p>
<p/>
</div>
<div class="page"><p/>
<p>17.5 More Independence Relations 269 
</p>
<p>x x I( y ..... I(E--- Z 
</p>
<p>x I( y 
</p>
<p>FIGURE 17.6. The first three DAG's have no colliders. The fourth DAG in the lower 
</p>
<p>right corner has a collider at Y. 
</p>
<p>X ---3~~ Y ..... I(E--- Z 
</p>
<p>w 
</p>
<p>FIGURE 17.7. A collider with a descendant. </p>
<p/>
</div>
<div class="page"><p/>
<p>270 17. Directed Graphs and Conditional Independence 
</p>
<p>X ~ U I( V ~ W I( 
</p>
<p>! ! 
51 52 
</p>
<p>FIGURE 17.8. d-separation explained. 
</p>
<p>The Rules of d-Separation 
</p>
<p>Consider the DAGs in Figures 17.6 and 17.7. 
</p>
<p>Y 
</p>
<p>1. When Y is not a collider, X and Z are d-connected, but they are 
</p>
<p>d-separated given Y. 
</p>
<p>2. If X and Z collide at Y, then X and Z are d-separated, but they 
</p>
<p>are d-connected given Y. 
</p>
<p>3. Conditioning on the descendant of a collider has the same effect as 
</p>
<p>conditioning on the collider. Thus in Figure 17.7, X and Z are 
</p>
<p>d-separated but they are d-connected given W. 
</p>
<p>Here is a more formal definition of d-separation. Let X and Y be distinct 
</p>
<p>vertices and let W be a set of vertices not containing X or Y. Then X and 
</p>
<p>Yare d-separated given W if there exists no undirected path U between 
</p>
<p>X and Y such that (i) every collider on U has a descendant in W, and (ii) 
</p>
<p>no other vertex on U is in W. If A, B, and Ware distinct sets of vertices and 
</p>
<p>A and B are not empty, then A and Bare d-separated given W if for every 
</p>
<p>X E A and Y E B, X and Yare d-separated given W. Sets of vertices that 
</p>
<p>are not d-separated are said to be d-connected. 
</p>
<p>17.9 Example. Consider the DAG in Figure 17.8. From the d-separation rules 
</p>
<p>we conclude that: 
</p>
<p>X and Yare d-separated (given the empty set); 
</p>
<p>X and Yare d-connected given {51, 52}; 
</p>
<p>X and Yare d-separated given {51, 52, V}. 
</p>
<p>17 .10 Theorem. 2 Let A, B! and C be disjoint sets of vertices. Then AilB I C 
</p>
<p>if and only if A and Bare d-separated by C. 
</p>
<p>2We implicitly assume that lP' is faithful to Q which means that lP' has no extra independence 
</p>
<p>relations other than those logically implied by the Markov Condition. </p>
<p/>
</div>
<div class="page"><p/>
<p>17.5 More Independence Relations 271 
</p>
<p>aliens watch 
</p>
<p>late 
</p>
<p>FIGURE 17.9. Jordan's alien example (Example 17.11). Was your friend kidnapped 
</p>
<p>by aliens or did you forget to set your watch? 
</p>
<p>17.11 Example. The fact that conditioning on a collider creates dependence 
</p>
<p>might not seem intuitive. Here is a whimsical example from Jordan (2004) that 
</p>
<p>makes this idea more palatable. Your friend appears to be late for a meeting 
</p>
<p>with you. There are two explanations: she was abducted by aliens or you forgot 
</p>
<p>to set your watch ahead one hour for daylight savings time. (See Figure 17.9.) 
</p>
<p>Aliens and Watch are blocked by a collider which implies they are marginally 
</p>
<p>independent. This seems reasonable since - before we know anything about 
</p>
<p>your friend being late - we would expect these variables to be independent. 
</p>
<p>We would also expect that IP'(Aliens = yeslLate = yes) &gt; IP'(Aliens = yes); 
</p>
<p>learning that your friend is late certainly increases the probability that she 
</p>
<p>was abducted. But when we learn that you forgot to set your watch properly, 
</p>
<p>we would lower the chance that your friend was ahducted. Hence, IP'(Aliens = 
</p>
<p>yeslLate = yes) cJ IP'(Aliens = yeslLate = yes, Watch = no). Thus, Aliens and 
Watch are dependent given Late. _ 
</p>
<p>17.12 Example. Consider the DAG in Figure 17.2. In this example, over-
</p>
<p>weight and smoking are marginally independent but they are dependent given 
</p>
<p>heart disease. _ 
</p>
<p>Graphs that look different may actually imply the same independence re-
</p>
<p>lations. If Q is a DAG, we let I(Q) denote all the independence statements 
</p>
<p>implied by Q. Two DAGs Ql and Q2 for the same variables V are Markov 
</p>
<p>equivalent if I(Qd = I(Q2). Given a DAG Q, let skeleton(Q) denote the 
</p>
<p>undirected graph obtained by replacing the arrows with undirected edges. 
</p>
<p>17.13 Theorem. Two DAGs Ql and Q2 are Markov equivalent if and only if 
</p>
<p>(i) skeleton(Qd = skeleton(Q2) and (ii) Ql and Q2 have the same unshielded 
</p>
<p>colliders. 
</p>
<p>17.14 Example. The first three DAGs in Figure 17.6 are Markov equivalent. 
</p>
<p>The DAG in the lower right of the Figure is not Markov equivalent to the 
</p>
<p>others. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>272 17. Directed Graphs and Conditional Independence 
</p>
<p>17.6 Estirnation for DAGs 
</p>
<p>Two estimation questions arise in the context of DAGs. First, given a DAG 
</p>
<p>Q and data VI"'" Vn from a distribution f consistent with Q, how do we 
estimate f7 Second, given data VI,'" , Vn how do we estimate Q7 The first 
</p>
<p>question is pure estimation while the second involves model selection. These 
</p>
<p>are very involved topics and are beyond the scope of this book. We will just 
</p>
<p>briefly mention the main ideas. 
</p>
<p>Typically, one uses some parametric model f(xl7Tx; ex) for each conditional 
</p>
<p>density. The likelihood function is then 
</p>
<p>n n m 
</p>
<p>i=1 i=1 j=1 
</p>
<p>where X ij is the value of Xj for the ith data point and ej are the parameters for 
the jth conditional density. We can then estimate the parameters by maximum 
</p>
<p>likelihood. 
</p>
<p>To estimate the structure of the DAG itself, we could fit every possible DAG 
</p>
<p>using maximum likelihood and use AIC (or some other method) to choose a 
</p>
<p>DAG. However, there are many possible DAGs so you would need much data 
</p>
<p>for such a method to be reliable. Also, searching through all possible DAGs 
</p>
<p>is a serious computational challenge. Producing a valid, accurate confidence 
</p>
<p>set for the DAG structure would require astronomical sample sizes. If prior 
</p>
<p>information is available about part of the DAG structure, the computational 
</p>
<p>and statistical problems are at least partly ameliorated. 
</p>
<p>17.7 Bibliographic Remarks 
</p>
<p>There are a number of texts on DAGs including Edwards (1995) and Jordan 
</p>
<p>(2004). The first use of DAGs for representing causal relationships was by 
</p>
<p>Wright (1934). Modern treatments are contained in Spirtes et al. (2000) and 
</p>
<p>Pearl (2000). Robins et al. (2003) discuss the problems with estimating causal 
</p>
<p>structure from data. 
</p>
<p>17.8 Appendix 
</p>
<p>CAUSATION REVISITED. We discussed causation in Chapter 16 using the idea 
</p>
<p>of counterfactual random variables. A different approach to causation uses </p>
<p/>
</div>
<div class="page"><p/>
<p>17.8 Appendix 273 
</p>
<p>FIGURE 17.10. Conditioning versus intervening. 
</p>
<p>DAGs. The two approaches are mathematically equivalent though they appear 
</p>
<p>to be quite different. In the DAG approach, the extra element is the idea of 
</p>
<p>intervention. Consider the DAG in Figure 17.10. 
</p>
<p>The probability function for a distribution consistent with this DAG has 
</p>
<p>the form f(x, y, z) = f(x)f(ylx)f(zlx, y). The following is pseudocode for 
</p>
<p>generating from this distribution. 
</p>
<p>For i 1, ... ,n: 
</p>
<p>Xi &lt;- PX(Xi) 
</p>
<p>Yi &lt;- PYIX(Yilxi) 
</p>
<p>Zi &lt;- PZIX,y(Zilxi, Yi) 
</p>
<p>Suppose we repeat this code many times, yielding data (Xl, YI, zd, ... , (xn' Yn, zn). 
</p>
<p>Among all the times that we observe Y = y, how often is Z = z? The answer 
</p>
<p>to this question is given by the conditional distribution of ZIY. Specifically, 
</p>
<p>IP'(Z = zlY = y) 
f(y,z) 
</p>
<p>f(y) 
</p>
<p>IP'(Y = y,Z = z) 
</p>
<p>IP'(Y = y) 
</p>
<p>~x f(x,y, z) ~x f(x) f(Ylx) f(zlx, y) 
</p>
<p>f(y) f(y) 
</p>
<p>"f( I' ) f(ylx) f(x) = "f( I' ) f(x, y) 
~ zX,y f(y) ~ zX,y f(y) 
</p>
<p>L f(zlx, y) f(xly)&middot; 
x 
</p>
<p>Now suppose we intervene by changing the computer code. Specifically, sup-
</p>
<p>pose we fix Y at the value y. The code now looks like this: 
</p>
<p>set Y y 
</p>
<p>for i 1, ... ,n 
</p>
<p>Xi &lt;- PX(Xi) 
</p>
<p>Zi &lt;- PZIX,Y(Zilxi' y) </p>
<p/>
</div>
<div class="page"><p/>
<p>274 17. Directed Graphs and Conditional Independence 
</p>
<p>Having set Y = y, how often was Z = z? To answer, note that the inter-
</p>
<p>vention has changed the joint probability to be 
</p>
<p>j*(x, z) = f(x)f(zlx, y). 
</p>
<p>The answer to our question is given by the marginal distribution 
</p>
<p>j*(z) = L j*(x, z) = L f(x)f(zlx, y). 
x x 
</p>
<p>We shall denote this as IP'(Z = zlY := y) or f(zlY := y). We call1P'(Z 
</p>
<p>zlY = y) conditioning by observation or passive conditioning. We call 
</p>
<p>IP'(Z = zlY := y) conditioning by intervention or active conditioning. 
</p>
<p>Passive conditioning is used to answer a predictive question like: 
</p>
<p>"Given that Joe smokes, what is the probability he will get lung cancer?" 
</p>
<p>Active conditioning is used to answer a causal question like: 
</p>
<p>"If Joe quits smoking, what is the probability he will get lung cancer?" 
</p>
<p>Consider a pair (Q, IP') where 9 is a DAG and IP' is a distribution for the 
variables V of the DAG. Let P denote the probability function for IP'. Con-
</p>
<p>sider intervening and fixing a variable X to be equal to x. We represent the 
</p>
<p>intervention by doing two things: 
</p>
<p>(1) Create a new DAG g* by removing all arrows pointing into X; 
</p>
<p>(2) Create a new distribution j*(v) = IP'(V = vlX := x) by removing the 
</p>
<p>term f(xl7TX) from f(v). 
</p>
<p>The new pair (Q*, 1*) represents the intervention "set X = x." 
</p>
<p>17.15 Example. You may have noticed a correlation between rain and having 
</p>
<p>a wet lawn, that is, the variable "Rain" is not independent of the variable "Wet 
</p>
<p>Lawn" and hence PR,w(r, w) -I- PR(r)pw(w) where R denotes Rain and W 
denotes Wet Lawn. Consider the following two DAGs: 
</p>
<p>Rain --+ Wet Lawn Rain +-- Wet Lawn. 
</p>
<p>The first DAG implies that f(w, r) f(r)f(wlr) while the second implies 
</p>
<p>that f(w, r) = f(w)f(rlw) No matter what the joint distribution f(w, r) is, 
</p>
<p>both graphs are correct. Both imply that Rand Ware not independent. But, 
</p>
<p>intuitively, if we want a graph to indicate causation, the first graph is right 
</p>
<p>and the second is wrong. Throwing water on your lawn doesn't cause rain. 
</p>
<p>The reason we feel the first is correct while the second is wrong is because the 
</p>
<p>interventions implied by the first graph are correct. 
</p>
<p>Look at the first graph and form the intervention W = 1 where 1 denotes 
</p>
<p>"wet lawn." Following the rules of intervention, we break the arrows into W </p>
<p/>
</div>
<div class="page"><p/>
<p>17.8 Appendix 275 
</p>
<p>to get the modified graph: 
</p>
<p>Rain I set Wet Lawn =1 I 
</p>
<p>with distribution f*(r) = f(r). Thus IP'(R = r I W := w) = IP'(R = r) tells us 
</p>
<p>that "wet lawn" does not cause rain. 
</p>
<p>Suppose we (wrongly) assume that the second graph is the correct causal 
</p>
<p>graph and form the intervention W = 1 on the second graph. There are no 
</p>
<p>arrows into W that need to be broken so the intervention graph is the same 
</p>
<p>as the original graph. Thus f*(r) = f(rlw) which would imply that changing 
</p>
<p>"wet" changes "rain." Clearly, this is nonsense. 
</p>
<p>Both are correct probability graphs but only the first is correct causally. 
</p>
<p>We know the correct causal graph by using background knowledge. 
</p>
<p>17.16 Remark. We could try to learn the correct causal graph from data but 
</p>
<p>this is dangerous. In fact it is impossible with two variables. With more than 
</p>
<p>two variables there are methods that can find the causal graph under certain 
</p>
<p>assumptions but they are large sample methods and, furthermore, there is no 
</p>
<p>way to ever know if the sample size you have is large enough to make the 
</p>
<p>methods reliable. 
</p>
<p>We can use DAGs to represent confounding variables. If X is a treatment 
</p>
<p>and Y is an outcome, a confounding variable Z is a variable with arrows into 
</p>
<p>both X and Y; see Figure 17.11. It is easy to check, using the formalism of 
</p>
<p>interventions, that the following facts are true: 
</p>
<p>In a randomized study, the arrow between Z and X is broken. In this case, 
</p>
<p>even with Z unobserved (represented by enclosing Z in a circle), the causal 
</p>
<p>relationship between X and Y is estimable because it can be shown that 
</p>
<p>lE(YIX := x) = lE(YIX = x) which does not involve the unobserved Z. In 
</p>
<p>an observational study, with all confounders observed, we get lE(YIX := x) = 
</p>
<p>JlE(YIX = x, Z = z)dFz(z) as in formula (16.7). If Z is unobserved then we 
</p>
<p>cannot estimate the causal effect because lE(YIX:= x) = JlE(YIX = x,Z = 
</p>
<p>z)dFz(z) involves the unobserved Z. We can't just use X and Y since in this 
</p>
<p>case. IP'(Y = ylX = x) Ie IP'(Y = ylX := x) which is just another way of saying 
that causation is not association. 
</p>
<p>In fact, we can make a precise connection between DAGs and counterfac-
</p>
<p>tuals as follows. Suppose that X and Yare binary. Define the confounding </p>
<p/>
</div>
<div class="page"><p/>
<p>276 17. Directed Graphs and Conditional Independence 
</p>
<p>Z 
</p>
<p>/\ 
X-------3~ x -.Y X-------3~ 
</p>
<p>FIGURE 17.11. Randomized study; Observational study with measured con-
</p>
<p>founders; Observational study with unmeasured confounders. The circled variables 
</p>
<p>are unobserved. 
</p>
<p>variable Z by 
</p>
<p>if (Co, C 1 ) = (0,0) 
</p>
<p>if (CO,C1 ) = (0,1) 
</p>
<p>if (Co, C 1 ) = (1,0) 
</p>
<p>if (Co, C 1 ) = (1,1). 
</p>
<p>From this, you can make the correspondence between the DAG approach and 
</p>
<p>the counterfactual approach explicit. I leave this for the interested reader. 
</p>
<p>17.9 Exercises 
</p>
<p>1. Show that (17.1) and (17.2) are equivalent. 
</p>
<p>2. Prove Theorem 17.2. 
</p>
<p>3. Let X, Y and Z have the following joint distribution: 
</p>
<p>y=o Y= 1 y=o Y= 1 
X=O .405 .045 X=O .125 .125 
X= 1 .045 .005 X= 1 .125 .125 
</p>
<p>Z=O Z = 1 
</p>
<p>(a) Find the conditional distribution of X and Y given Z = 0 and the 
</p>
<p>conditional distribution of X and Y given Z = 1. 
</p>
<p>(b) Show that X II YIZ. 
</p>
<p>(c) Find the marginal distribution of X and Y. 
</p>
<p>(d) Show that X and Yare not marginally independent. 
</p>
<p>4. Consider the three DAGs in Figure 17.6 without a collider. Prove that 
</p>
<p>XII ZIY. </p>
<p/>
</div>
<div class="page"><p/>
<p>17.9 Exercises 277 
</p>
<p>FIGURE 17.12. DAG for exercise 7. 
</p>
<p>5. Consider the DAG in Figure 17.6 with a collider. Prove that X IT Z and 
</p>
<p>that X and Z are dependent given Y. 
</p>
<p>6. Let X E {O, I}, Y E {O, I}, Z E {O, 1, 2}. Suppose the distribution of 
</p>
<p>(X, Y, Z) is Markov to: 
</p>
<p>X--+Y--+Z 
</p>
<p>Create a joint distribution f(x, y, z) that is Markov to this DAG. Gen-
</p>
<p>erate 1000 random vectors from this distribution. Estimate the distribu-
</p>
<p>tion from the data using maximum likelihood. Compare the estimated 
</p>
<p>distribution to the true distribution. Let 8 = (8000,8001, ... ,8112) where 
</p>
<p>8rst = W(X = T, Y = 8, Z = t). Use the bootstrap to get standard errors 
</p>
<p>and 95 percent confidence intervals for these 12 parameters. 
</p>
<p>7. Consider the DAG in Figure 17.12. 
</p>
<p>(a) Write down the factorization of the joint density. 
</p>
<p>(b) Prove that X IT Zj. 
</p>
<p>8. Let V = (X, Y, Z) have the following joint distribution 
</p>
<p>X Bernoulli (~) </p>
<p/>
</div>
<div class="page"><p/>
<p>278 17. Directed Graphs and Conditional Independence 
</p>
<p>YIX=x 
</p>
<p>Z I X = x,Y = y 
</p>
<p>Bernoulli ( e4X~2 2) 
1 + e x-
</p>
<p>( 
e2(x+y)-2 ) 
</p>
<p>Bernoulli 2( + ) 2 . 1 + e x y -
</p>
<p>(a) Find an expression for IP'(Z = z I Y = y). In particular, find IP'(Z = 
</p>
<p>1 I Y = 1). 
</p>
<p>(b) Write a program to simulate the model. Conduct a simulation and 
</p>
<p>compute IP'( Z = 1 I Y = 1) empirically. Plot this as a function of 
</p>
<p>the simulation size N. It should converge to the theoretical value you 
</p>
<p>computed in (a). 
</p>
<p>(c) (Refers to material in the appendix.) Write down an expression for 
</p>
<p>IP'(Z = 1 I Y := y). In particular, find IP'(Z = 1 I Y := 1). 
</p>
<p>(d) (Refers to material in the appendix.) Modify your program to sim-
</p>
<p>ulate the intervention "set Y = I." Conduct a simulation and compute 
</p>
<p>IP'(Z = 1 I Y := 1) empirically. Plot this as a function of the simulation 
</p>
<p>size N. It should converge to the theoretical value you computed in (c). 
</p>
<p>9. This is a continuous, Gaussian version of the last question. Let V = 
</p>
<p>(X, Y, Z) have the following joint distribution 
</p>
<p>X 
</p>
<p>Y IX=x 
</p>
<p>Z I X = x,Y = y 
</p>
<p>Normal (0,1) 
</p>
<p>Normal (ax,l) 
</p>
<p>Normal ({3y + ,x, 1). 
</p>
<p>Here, a, (3 and, are fixed parameters. economists refer to models like 
</p>
<p>this as structural equation models. 
</p>
<p>(a) Find an explicit expression for J(z I y) and JE(Z I Y = y) = I zJ(z I 
y)dz. 
</p>
<p>(b) (Refers to material in the appendix.) Find an explicit expression 
</p>
<p>for J(z I Y := y) and then find JE(Z I Y := y) == I zJ(z I Y := y)dy. 
Compare to (b). 
</p>
<p>(c) Find the joint distribution of (Y, Z). Find the correlation p between 
</p>
<p>Y and Z. 
</p>
<p>(d) (Refers to material in the appendix.) Suppose that X is not observed 
</p>
<p>and we try to make causal conclusions from the marginal distribution of 
</p>
<p>(Y, Z). (Think of X as unobserved confounding variables.) In particular, </p>
<p/>
</div>
<div class="page"><p/>
<p>17.9 Exercises 279 
</p>
<p>suppose we declare that Y causes Z if P i= 0 and we declare that Y does 
not cause Z if P = O. Show that this will lead to erroneous conclusions. 
</p>
<p>(e) (Refers to material in the appendix.) Suppose we conduct a ran-
</p>
<p>domized experiment in which Y is randomly assigned. To be concrete, 
</p>
<p>suppose that 
</p>
<p>x 
Y 
</p>
<p>Z I X = x,Y = y 
</p>
<p>Normal(O,l) 
</p>
<p>Normal(a, 1) 
</p>
<p>Normal(iJy + IX, 1). 
</p>
<p>Show that the method in (d) now yields correct conclusions (i.e., p = 0 
</p>
<p>if and only if f(z I Y := y) does not depend on y). </p>
<p/>
</div>
<div class="page"><p/>
<p>18 
</p>
<p>Undirected Graphs 
</p>
<p>Undirected graphs are an alternative to directed graphs for representing in-
</p>
<p>dependence relations. Since both directed and undirected graphs are used in 
</p>
<p>practice, it is a good idea to be facile with both. The main difference between 
</p>
<p>the two is that the rules for reading independence relations from the graph 
</p>
<p>are different. 
</p>
<p>18.1 Undirected Graphs 
</p>
<p>An undirected graph Q = (V, E) has a finite set V of vertices (or nodes) 
</p>
<p>and a set E of edges (or arcs) consisting of pairs of vertices. The vertices 
</p>
<p>correspond to random variables X, Y, Z, ... and edges are written as unordered 
</p>
<p>pairs. For example, (X, Y) E E means that X and Yare joined by an edge. 
</p>
<p>An example of a graph is in Figure 18.I. 
</p>
<p>Two vertices are adjacent, written X rv Y, if there is an edge between 
</p>
<p>them. In Figure 18.1, X and Yare adjacent but X and Z are not adjacent. A 
</p>
<p>sequence X o, ... ,Xn is called a path if X i- 1 rv Xi for each i. In Figure 18.1, 
</p>
<p>X, Y, Z is a path. A graph is complete if there is an edge between every pair 
</p>
<p>of vertices. A subset U c V of vertices together with their edges is called a 
</p>
<p>subgraph. </p>
<p/>
</div>
<div class="page"><p/>
<p>282 18. Undirected Graphs 
</p>
<p>y 
</p>
<p>x 
</p>
<p>FIGURE 18.1. A graph with vertices V 
</p>
<p>E = {(X, Y), (Y, Z)}. 
</p>
<p>w 
</p>
<p>y 
</p>
<p>z 
</p>
<p>{X, Y, Z}. The edge set IS 
</p>
<p>x 
</p>
<p>z 
</p>
<p>FIGURE 18.2. {Y, W} and {Z} are separated by {X}. Also, Wand Z are separated 
</p>
<p>by {X, Y}. 
</p>
<p>If A, Band C are three distinct subsets of V, we say that C separates 
</p>
<p>A and B if every path from a variable in A to a variable in B intersects a 
</p>
<p>variable in C. In Figure 18.2 {Y, W} and {Z} are separated by {X}. Also, W 
</p>
<p>and Z are separated by {X, Y}. 
</p>
<p>18.2 Probability and Graphs 
</p>
<p>Let V be a set of random variables with distribution IP'. Construct a graph 
</p>
<p>with One vertex for each random variable in V. Omit the edge between a pair 
</p>
<p>of variables if they are independent given the rest of the variables: 
</p>
<p>nO edge between X and Y &cent;=} X Il Y I rest </p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Probability and Graphs 283 
</p>
<p>Y 
</p>
<p>x z 
</p>
<p>FIGURE 18.3. X II ZIY. 
</p>
<p>Y 
</p>
<p>x z 
</p>
<p>FIGURE 18.4. No implied independence relations. 
</p>
<p>where "rest" refers to all the other variables besides X and Y. The resulting 
</p>
<p>graph is called a pairwise Markov graph. Some examples are shown in 
</p>
<p>Figures 18.3, 18.4, 18.5, and 18.6. 
</p>
<p>The graph encodes a set of pairwise conditional independence relations. 
</p>
<p>These relations imply other conditional independence relations. How can we 
</p>
<p>figure out what they are? Fortunately, we can read these other conditional 
</p>
<p>independence relations directly from the graph as well, as is explained in the 
</p>
<p>next theorem. 
</p>
<p>18.1 Theorem. Let 9 = (V, E) be a pairwise Markov graph for a distribution 
</p>
<p>lP'. Let A, Band G be distinct subsets of V such that G separates A and B. 
</p>
<p>Then A II BIG. 
</p>
<p>18.2 Remark. If A and B are not connected (i.e., there is no path from A to 
</p>
<p>B) then we may regard A and B as being separated by the empty set. Then 
</p>
<p>Theorem 18.1 implies that A II B. </p>
<p/>
</div>
<div class="page"><p/>
<p>284 18. Undirected Graphs 
</p>
<p>X W 
</p>
<p>Y Z 
</p>
<p>FIGURE 18.5. X II ZI{Y, W} and Y II WI{X, Z}. 
</p>
<p>&bull; &bull; &bull; &bull; 
X Y Z W 
</p>
<p>FIGURE 18.6. Pairwise independence implies that X II ZI{Y, W}. But is X II ZIY? 
</p>
<p>The independence condition in Theorem 18.1 is called the global Markov 
</p>
<p>property. We thus see that the pairwise and global Markov properties are 
</p>
<p>equivalent. Let us state this more precisely. Given a graph 0, let 1Vlpair (Q) 
</p>
<p>be the set of distributions which satisfy the pairwise Markov property: thus 
</p>
<p>IF' E 1Vlpair (Q) if, under IF', X II Ylrest if and only if there is no edge between 
</p>
<p>X and Y. Let Mglobal (9) be the set of distributions which satisfy the global 
</p>
<p>Markov property: thus IF' E Mpair(Q) if, under IF', A II BIG if and only if G 
</p>
<p>separates A and B. 
</p>
<p>18.3 Theorem. Let 0 be a graph. Then, Mpair(Q) = Mglobal(Q). 
</p>
<p>Theorem 18.3 allows us to construct graphs using the simpler pairwise prop-
</p>
<p>erty and then we can deduce other independence relations using the global 
</p>
<p>Markov property. Think how hard this would be to do algebraically. Returning 
</p>
<p>to 18.6, we now see that X II ZIY and Y II WIZ. 
</p>
<p>18.4 Example. Figure 18.7 implies that X II Y, X II Z and X II (Y, Z) .&bull; 
</p>
<p>18.5 Example. Figure 18.8 implies that X II WI(Y, Z) and X II ZIY .&bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>18.3 Cliques and Potentials 285 
</p>
<p>Y 
</p>
<p>Xe ~z 
FIGURE 18.7. X II Y, X II Z and X II (Y, Z). 
</p>
<p>x Y 
</p>
<p>w 
</p>
<p>Z 
</p>
<p>FIGURE 18.8. X II WI (Y, Z) and X II ZIY. 
</p>
<p>18.3 Cliques and Potentials 
</p>
<p>A clique is a set of variables in a graph that are all adjacent to each other. A 
</p>
<p>set of variables is a maximal clique if it is a clique and if it is not possible 
</p>
<p>to include another variable and still be a clique. A potential is any positive 
</p>
<p>function. Under certain conditions, it can be shown that IF' is Markov 9 if and 
only if its probability function f can be written as 
</p>
<p>f(x) = ITCEC ?jJc(xc) 
Z 
</p>
<p>where C is the set of maximal cliques and 
</p>
<p>Z = L III/Jc(xc). 
x CEC 
</p>
<p>(18.1) 
</p>
<p>18.6 Example. The maximal cliques for the graph in Figure 18.1 are C1 
</p>
<p>{X, Y} and C2 = {Y, Z}. Hence, if IF' is Markov to the graph, then its proba-
</p>
<p>bility function can be written 
</p>
<p>f(x,y,z) rxI/J1(X,y)1/J2(Y,Z) 
</p>
<p>for some positive functions '1/)1 and ?jJ2' &bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>286 18. Undirected Graphs 
</p>
<p>FIGURE 18.9. The maximumly cliques of this graph are 
</p>
<p>{Xl, X 2}, {Xl, X 3}, {X2, X 4 }, {X3, X 5 }, {X2, X 5 , X6}. 
</p>
<p>18.7 Example. The maximal cliques for the graph in Figure 18.9 are 
</p>
<p>Thus we can write the probability function as 
</p>
<p>f(XI, X2, X3, X4, X5, X6) cx:tP12 (Xl, X2)tP13 (Xl, X3)1jJ24 (X2' X4) 
</p>
<p>X1'35(X3, X5)tP256(X2, X5, X6). &bull; 
</p>
<p>18.4 Fitting Graphs to Data 
</p>
<p>Given a data set, how do we find a graphical model that fits the data? As 
</p>
<p>with directed graphs, this is a big topic that we will not treat here. However, 
</p>
<p>in the discrete case, one way to fit a graph to data is to use a log-linear 
</p>
<p>model, which is the subject of the next chapter. 
</p>
<p>18.5 Bibliographic Remarks 
</p>
<p>Thorough treatments of undirected graphs can be found in Whittaker (1990) 
</p>
<p>and Lauritzen (1996). Some of the exercises below are from Whittaker (1990). 
</p>
<p>18.6 Exercises 
</p>
<p>1. Consider random variables (Xl, X 2 , X3). In each of the following cases, 
</p>
<p>draw a graph that has the given independence relations. </p>
<p/>
</div>
<div class="page"><p/>
<p>18.6 Exercises 287 
</p>
<p>FIGURE 18.10. 
</p>
<p>X3 
</p>
<p>&bull; &bull; &bull; &bull; 
</p>
<p>FIGURE 18.11. 
</p>
<p>(a) Xl II X3 I X 2 . 
</p>
<p>(b) Xl IIX2 1 X3 and Xl IIX3 1 X 2 . 
</p>
<p>(c) Xl II X 2 I X3 and Xl II X3 I X 2 and X 2 II X3 I Xl. 
</p>
<p>2. Consider random variables (Xl, X 2 , X 3, X 4 ). In each of the following 
</p>
<p>cases, draw a graph that has the given independence relations. 
</p>
<p>(a) Xl II X3 I X 2 , X 4 and Xl II X 4 I X 2 , X3 and X 2 II X 4 I Xl, X 3&middot; 
</p>
<p>(b) Xl II X 2 I X 3, X 4 and Xl II X3 I X 2 , X 4 and X 2 II X3 I Xl, X 4 . 
</p>
<p>(c) Xl IIX3 1 X 2 ,X4 and X 2 IIX4 1 X I ,X3. 
</p>
<p>3. A conditional independence between a pair of variables is minimal if it 
</p>
<p>is not possible to use the Separation Theorem to eliminate any variable 
</p>
<p>from the conditioning set, i.e. from the right hand side of the bar Whit-
</p>
<p>taker (1990). Write down the minimal conditional independencies from: 
</p>
<p>(a) Figure 18.10; (b) Figure 18.11; (c) Figure 18.12; (d) Figure 18.13. 
</p>
<p>4. Let X I ,X2 ,X3 be binary random variables. Construct the likelihood 
</p>
<p>ratio test for 
</p>
<p>5. Here are breast cancer data from Morrison et al. (1973) on diagnostic 
</p>
<p>center (XI), nuclear grade (X2 ), and survival (X3): </p>
<p/>
</div>
<div class="page"><p/>
<p>288 18. Undirected Graphs 
</p>
<p>X3 
</p>
<p>FIGURE 18.12. 
Xl 
</p>
<p>FIGURE 18.13. 
</p>
<p>X 2 malignant malignant benign benign 
X3 died survived died survived 
</p>
<p>Xl Boston 35 59 47 112 
Glamorgan 42 77 26 76 
</p>
<p>(a) Treat this as a multinomial and find the maximum likelihood esti-
</p>
<p>mator. 
</p>
<p>(b) If someOne has a tumor classified as benign at the Glamorgan clinic, 
</p>
<p>what is the estimated probability that they will die? Find the standard 
</p>
<p>error for this estimate. </p>
<p/>
</div>
<div class="page"><p/>
<p>(c) Test the following hypotheses: 
</p>
<p>X l IIX21X3 
</p>
<p>XIII X31X2 
</p>
<p>X 2 II X 31XI 
</p>
<p>versus 
</p>
<p>versus 
</p>
<p>versus 
</p>
<p>18.6 Exercises 289 
</p>
<p>Xl '0000' X21X3 
</p>
<p>Xl '0000' X 31X2 
</p>
<p>X 2 '0000' X 31XI 
</p>
<p>Use the test from question 4. Based on the results of your tests, draw 
</p>
<p>and interpret the resulting graph. </p>
<p/>
</div>
<div class="page"><p/>
<p>19 
</p>
<p>Log-Linear Models 
</p>
<p>In this chapter we study log-linear models which are useful for modeling 
</p>
<p>multivariate discrete data. There is a strong connection between log-linear 
</p>
<p>models and undirected graphs. 
</p>
<p>19.1 The Log-Linear Model 
</p>
<p>Let X = (Xl, .. . , Xm) be a discrete random vector with probability function 
</p>
<p>where x = (Xl, ... , xm). Let Tj be the number of values that Xj takes. Without 
</p>
<p>loss of generality, we can assume that Xj E {O, 1, ... ,Tj - I}. Suppose now 
</p>
<p>that we have n such random vectors. We can think of the data as a sample 
</p>
<p>from a Multinomial with N = TI X T2 X ... X Tm categories. The data can be 
</p>
<p>represented as counts in a TI x T2 X ... X T m table. Let P = (PI, ... ,p N) denote 
</p>
<p>the multinomial parameter. 
</p>
<p>Let S = {I, ... , rn}. Given a vector X = (Xl, ... , xm) and a subset A c S, 
</p>
<p>let XA = (Xj : j E A). For example, if A = {I, 3} then XA = (Xl, X3). </p>
<p/>
</div>
<div class="page"><p/>
<p>292 19. Log-Linear Models 
</p>
<p>19.1 Theorem. The joint probability function f(x) of a single random vector 
</p>
<p>X = (Xl' ... ' Xm) can be written as 
</p>
<p>log f(x) = LI/'A(X) (19.1) 
AcS 
</p>
<p>where the sum is over all subsets A of S = {I, ... , Tn} and the I/' 's satisfy the 
following conditions: 
</p>
<p>1. 1/J0 (x) is a constant; 
</p>
<p>2. For every A c S, 1/JA(X) is only a function of XA and not the rest of the 
I 
</p>
<p>xjs. 
</p>
<p>3. Ifi E A and Xi = 0, then 1/JA(X) = 0. 
</p>
<p>The formula in equation (19.1) is called the log-linear expansion of f. 
Eachl/'A(x) may depend on some unknown parameters (3A. Let (3 = ((3A : 
</p>
<p>A c S) be the set of all these parameters. We will write f(x) = f(x; (3) when 
</p>
<p>we want to emphasize the dependence on the unknown parameters (3. 
</p>
<p>In terms of the multinomial, the parameter space is 
</p>
<p>N 
</p>
<p>P={P=(P1, ... ,PN): Pj::::&gt;O, LPj=I}. 
J=l 
</p>
<p>This is an N - 1 dimensional space. In the log-linear representation, the pa-
</p>
<p>rameter space is 
</p>
<p>where (3(p) is the set of (3 values associated with p. The set 8 is a N - 1 
</p>
<p>dimensional surface in Jl{N. We can always go back and forth between the two 
</p>
<p>parameterizations we can write (3 = (3(p) and P = p((3). 
</p>
<p>19.2 Example. Let X rv Bernoulli(p) where &deg; &lt; P &lt; 1. We can write the 
probability mass function for X as 
</p>
<p>for x = 0,1, where P1 = P and P2 = 1 - p. Hence, </p>
<p/>
</div>
<div class="page"><p/>
<p>19.1 The Log-Linear Model 293 
</p>
<p>where 
</p>
<p>10g(p2) 
</p>
<p>1/J1(X) xlog (~~). 
</p>
<p>Notice thatljJr!J(x) is a constant (as a function of x) andljJ1 (x) = 0 when x = O. 
</p>
<p>Thus the three conditions of Theorem 19.1 hold. The log-linear parameters 
</p>
<p>are 
</p>
<p>f30 = 10g(P2), f31 = log (~~) . 
</p>
<p>The original, multinomial parameter space is P = {(P1,P2) : Pj ~ O,P1 +P2 = 
</p>
<p>I}. The log-linear parameter space is 
</p>
<p>8 = { (f30, (31) E JR;.2: ei30 +i31 + ei30 = 1.} 
Given (P1, P2) we can solve for (f30, f3I). Conversely, given (f30, (31) we can solve 
</p>
<p>for (P1,P2)' &bull; 
</p>
<p>19.3 Example. Let X = (Xl, X 2 ) where Xl E {O, I} and X 2 E {O, 1, 2}. The 
</p>
<p>joint distribution of n such random vectors is a multinomial with 6 categories. 
</p>
<p>The multinomial parameters can be written as a 2-by-3 table as follows: 
</p>
<p>multinomial X2 0 1 2 
</p>
<p>Xl 0 POO POl P02 
</p>
<p>1 P10 P11 P12 
</p>
<p>The n data vectors can be summarized as counts: 
</p>
<p>data X2 0 1 2 
</p>
<p>Xl 0 Coo COl CO2 
1 C lO C11 C12 
</p>
<p>For x = (Xl, X2), the log-linear expansion takes the form 
</p>
<p>where 
</p>
<p>logpoo 
</p>
<p>( P10) Xl log -
POO 
</p>
<p>I(X2 = 1) log - + I(X2 = 2) log -( POl) (P02) 
POO POO 
</p>
<p>( P11POO) (P12POO) I(X1 = 1, X2 = 1) log -- + I(X1 = 1, X2 = 2) log -- . 
P01PlO P02P10 </p>
<p/>
</div>
<div class="page"><p/>
<p>294 19. Log-Linear Models 
</p>
<p>Convince yourself that the three conditions on the 'I//S of the theorem are 
</p>
<p>satisfied. The six parameters of this model are: 
</p>
<p>(31 = logpoo (3 = log (PIO) 
2 Poo 
</p>
<p>(3 = 100" (PCll ) 
3 b Pon 
</p>
<p>&bull; 
(34 = 100" (P02 ) 
</p>
<p>b Pon 
(3 = log (PllPOO) 
</p>
<p>5 POIPIO (3 = 100" (PI2POO) . 6 b P02PIO 
</p>
<p>The next theorem gives an easy way to check for conditional independence 
</p>
<p>in a log-linear model. 
</p>
<p>19.4 Theorem. Let (Xa, Xb, Xc) be a partition of a vectors (Xl, ... , Xm). 
</p>
<p>Then Xb II XclXa if and only if all the 1/J-terms in the log-linear expansion 
</p>
<p>that have at least one coordinate in b and one coordinate in care O. 
</p>
<p>To prove this theorem, we will use the following lemma whose proof follows 
</p>
<p>easily from the definition of conditional independence. 
</p>
<p>19.5 lemma. A partition (Xa, X b, Xc) satisfies Xb II XclXa if and only if 
</p>
<p>f(x a, Xb, xc) = g(xa, xb)h(xa, xc) for some functions g and h 
</p>
<p>PROOF. (Theorem 19.4.) Suppose that 1/Jt is 0 whenever t has coordinates 
</p>
<p>in band c. Hence,l/Jt is 0 if t ~ a U b or t ~ aU c. Therefore 
</p>
<p>tCaUb tcaUc tCa 
</p>
<p>Exponentiating, we see that the joint density is of the form g(xa, xb)h(xa, xc). 
</p>
<p>By Lemma 19.5, XbllXclXa. The converse follows by reversing the argument . 
</p>
<p>&bull; 
</p>
<p>19.2 Graphical Log-Linear Models 
</p>
<p>A log-linear model is graphical if missing terms correspond only to condi-
</p>
<p>tional independence constraints. 
</p>
<p>19.6 Definition. Let logf(x) = L AcS 1/JA(X) be a log-linear model. Then 
f is graphical if all1/J-terms are nonzero except for any pair of 
</p>
<p>coordinates not in the edge set for some graph Q. In other words, 
</p>
<p>1/JA(X) = 0 if and only if {i,j} c A and (i,j) is not an edge. 
</p>
<p>Here is a way to think about the definition above: </p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Graphical Log-Linear Models 295 
</p>
<p>FIGURE 19.1. Graph for Example 19.7. 
</p>
<p>If you can add a term to the model and the graph does not change, 
</p>
<p>then the model is not gra ph ica I. 
</p>
<p>19.7 Example. Consider the graph in Figure 19.1. 
</p>
<p>The graphical log-linear model that corresponds to this graph is 
</p>
<p>log f(x) 1/)0 + 1jJl(X) + 1jJ2(X) + 1jJ3(X) + 1/)4(X) + 1/)5(X) 
</p>
<p>+ 1jJ12(X) + 1jJ23(X) + 1jJ25(X) +tP34(X) +tP35(X) +tP45(X) + 1jJ235 (x) + tP345 (x). 
</p>
<p>Let's see why this model is graphical. The edge (1,5) is missing in the graph. 
</p>
<p>Hence any term containing that pair of indices is omitted from the model. For 
</p>
<p>example, 
</p>
<p>are all omitted. Similarly, the edge (2,4) is missing and hence 
</p>
<p>are all omitted. There are other missing edges as well. You can check that the 
</p>
<p>model omits all the corresponding 1/) terms. Now consider the model 
</p>
<p>logf(x) 1jJ0(X) +tPl (x) +tP2(X) + 1jJ3(X) + 1jJ4(X) +tP5(X) 
</p>
<p>+tP12(X) +tP23(X) + 1jJ25(X) + 1jJ34(X) + 1jJ35(X) +tP45(X). 
</p>
<p>This is the same model except that the three way interactions were removed. 
</p>
<p>If we draw a graph for this model, we will get the same graph. For example, 
</p>
<p>no 1jJ terms contain (1,5) so we omit the edge between Xl and X 5. But this is 
</p>
<p>not graphical since it has extra terms omitted. The independencies and graphs </p>
<p/>
</div>
<div class="page"><p/>
<p>296 19. Log-Linear Models 
</p>
<p>&bull; &bull; &bull; 
</p>
<p>FIGURE 19.2. Graph for Example 19.10. 
</p>
<p>for the two models are the same but the latter model has other constraints 
</p>
<p>besides conditional independence constraints. This is not a bad thing. It just 
</p>
<p>means that if we are only concerned about presence or absence of conditional 
</p>
<p>independences, then we need not consider such a model. The presence of the 
</p>
<p>three-way interaction 1/J235 means that the strength of association between X 2 
and X3 varies as a function of X 5 . Its absence indicates that this is not so .&bull; 
</p>
<p>19.3 Hierarchical Log-Linear Models 
</p>
<p>There is a set of log-linear models that is larger than the set of graphical 
</p>
<p>models and that are used quite a bit. These are the hierarchical log-linear 
</p>
<p>models. 
</p>
<p>19.8 Definition. A log-linear model is hierarchical if'ljJA = 0 and A c B 
</p>
<p>implies that 'ljJB = o. 
</p>
<p>19.9 lemma. A graphical model is hierarchical but the reverse need not be 
</p>
<p>true. 
</p>
<p>19.10 Example. Let 
</p>
<p>log f (x) = 1/J0 (x) +I/Jl (x) + 1/J2 (x) + 1/J3 (x) +1/J12 (x) + 1/J13 (x). 
</p>
<p>The model is hierarchical; its graph is given in Figure 19.2. The model is 
</p>
<p>graphical because all terms involving (2,3) are omitted. It is also hierarchical. 
</p>
<p>&bull; 
</p>
<p>19.11 Example. Let 
</p>
<p>log f(x) = 1/J0 (x) + 1/Jl (x) + 1/J2 (x) +1/J3 (x) +1/J12(X) + 1/J13 (x) +1/J23 (x). </p>
<p/>
</div>
<div class="page"><p/>
<p>19.4 Model Generators 297 
</p>
<p>FIGURE 19.3. The graph is complete. The model is hierarchical but not graphical. 
</p>
<p>&bull; &bull; 
</p>
<p>FIGURE 19.4. The model for this graph is not hierarchical. 
</p>
<p>The model is hierarchical. It is not graphical. The graph corresponding to this 
</p>
<p>model is complete; see Figure 19.3. It is not graphical because tP123 (x) = 0 
</p>
<p>which does not correspond to any pairwise conditional independence. _ 
</p>
<p>19.12 Example. Let 
</p>
<p>The graph corresponding is in Figure 19.4. This model is not hierarchical since 
</p>
<p>tP2 = 0 buttP12 is not. Since it is not hierarchical, it is not graphical either. _ 
</p>
<p>19.4 Model Generators 
</p>
<p>Hierarchical models can be written succinctly using generators. This is most 
</p>
<p>easily explained by example. Suppose that X = (Xl, X 2 , X3). Then, lvI = 
</p>
<p>1.2 + 1.3 stands for </p>
<p/>
</div>
<div class="page"><p/>
<p>298 19. Log-Linear Models 
</p>
<p>The formula}\;1 = 1.2+ 1.3 says: "include Vh2 and Vh3'" We have to also include 
</p>
<p>the lower order terms or it won't be hierarchical. The generator }\;1 = 1.2.3 is 
</p>
<p>the saturated model 
</p>
<p>The saturated models corresponds to fitting an unconstrained multinomial. 
</p>
<p>Consider 1\11 = 1 + 2 + 3 which means 
</p>
<p>This is the mutual independence model. Finally, consider }\;1 = 1.2 which has 
</p>
<p>log-linear expansion 
</p>
<p>This model makes X3 1X2 = X2,Xl = Xl a uniform distribution. 
</p>
<p>19.5 Fitting Log-Linear Models to Data 
</p>
<p>Let (3 denote all the parameters in a log-linear model 1\11. The loglikelihood 
</p>
<p>for (3 is 
n 
</p>
<p>&pound;((3) = L log !(Xi ; (3) 
i=l 
</p>
<p>where !(Xi; (3) is the probability function for the ith random vector Xi = 
</p>
<p>(XiI, ... ,Xim) as give by equation (19.1). The MLE i3 generally has to be 
found numerically. The Fisher information matrix is also found numerically 
</p>
<p>and we can then get the estimated standard errors from the inverse Fisher 
</p>
<p>information matrix. 
</p>
<p>When fitting log-linear models, one has to address the following model 
</p>
<p>selection problem: which 1jJ terms should we include in the model? This is 
</p>
<p>essentially the same as the model selection problem in linear regression. 
</p>
<p>One approach is is to use AIC. Let 1\11 denote some log-linear model. Differ-
</p>
<p>ent models correspond to setting different&cent; terms to O. Now we choose the 
</p>
<p>model}\;1 which maximizes 
</p>
<p>AIC(M) = &pound;(M) - IMI (19.2) 
</p>
<p>where IMI is the number of parameters in model M and &pound;(M) is the value 
</p>
<p>of the log-likelihood evaluated at the MLE for that model. Usually the model 
</p>
<p>search is restricted to hierarchical models. This reduces the search space. Some </p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Fitting Log-Linear Models to Data 299 
</p>
<p>also claim that we should only search through the hierarchical models because 
</p>
<p>other models are less interpretable. 
</p>
<p>A different approach is based on hypothesis testing. The model that includes 
</p>
<p>all possible 'lji-terms is called the saturated model and we denote it by ]l;Isat . 
</p>
<p>N ow for each lvI we test the hypothesis 
</p>
<p>Ho : the true model is lvI versus HI: the true model is lvIsat . 
</p>
<p>The likelihood ratio test for this hypothesis is called the deviance. 
</p>
<p>19.13 Definition. For any submodel M, define the deviance dev(M) by 
</p>
<p>dev(M) = 2(lsat -lM) 
</p>
<p>where i!sat is the log-likelihood of the saturated model evaluated at the MLE 
</p>
<p>and hI is the log-likelihood of the modellvI evaluated at its MLE. 
</p>
<p>19.14 Theorem. The deviance is the likelihood ratio test statistic for 
</p>
<p>Ho : the model is lvI versus HI: the model is ]l;Isat . 
</p>
<p>Under Ho, dev(M) ~ X~ with v degrees of freedom equal to the difference in 
</p>
<p>the number of parameters between the saturated model and ]l;I. 
</p>
<p>One way to find a good model is to use the deviance to test every sub-model. 
</p>
<p>Every model that is not rejected by this test is then considered a plausible 
</p>
<p>model. However, this is not a good strategy for two reasons. First, we will end 
</p>
<p>up doing many tests which means that there is ample opportunity for making 
</p>
<p>Type I and Type II errors. Second, we will end up using models where we 
</p>
<p>failed to reject Ho. But we might fail to reject Ho due to low power. The 
</p>
<p>result is that we end up with a bad model just due to low power. 
</p>
<p>After finding a "best model" this way we can draw the corresponding graph. 
</p>
<p>19.15 Example. The following breast cancer data are from Morrison et al. 
</p>
<p>(1973). The data are on diagnostic center (XI), nuclear grade (X2 ), and sur-
</p>
<p>vival (X3 ): 
</p>
<p>X 2 malignant malignant benign benign 
</p>
<p>X3 died survived died survived 
</p>
<p>Xl Boston 35 59 47 112 
</p>
<p>Glamorgan 42 77 26 76 
</p>
<p>The saturated log-linear model is: </p>
<p/>
</div>
<div class="page"><p/>
<p>300 19. Log-Linear Models 
</p>
<p>Center Grade ------ Survival 
</p>
<p>FIGURE 19.5. The graph for Example 19.15. 
</p>
<p>Variable (3j se Wj p-value 
</p>
<p>(Intercept) 3.56 0.17 21.03 0.00 *** 
center 0.18 0.22 0.79 0.42 
</p>
<p>grade 0.29 0.22 1.32 0.18 
</p>
<p>survival 0.52 0.21 2.44 0.01 * 
center x grade -0.77 0.33 -2.31 0.02 * 
center x survival 0.08 0.28 0.29 0.76 
</p>
<p>grade x survival 0.34 0.27 1.25 0.20 
</p>
<p>center x grade x survival 0.12 0.40 0.29 0.76 
</p>
<p>The best sub-model, selected using AIC and backward searching is: 
</p>
<p>Variable (3j se Wj p-value 
</p>
<p>(Intercept) 3.52 0.13 25.62 &lt; 0.00 *** 
center 0.23 0.13 1.70 0.08 
</p>
<p>grade 0.26 0.18 1.43 0.15 
</p>
<p>survival 0.56 0.14 3.98 6.65e-05 *** 
center x grade -0.67 0.18 -3.62 0.00 *** 
grade x survival 0.37 0.19 1.90 0.05 
</p>
<p>The graph for this model JI.;[ is shown in Figure 19.5. To test the fit of this 
</p>
<p>model, we compute the deviance of JI.;[ which is 0.6. The appropriate X2 has 
</p>
<p>8 - 6 = 2 degrees of freedom. The p-value is IP'(X~ &gt; .6) = .74. So we have no 
</p>
<p>evidence to suggest that the model is a poor fit .&bull; 
</p>
<p>19.6 Bibliographic Rernarks 
</p>
<p>For this chapter, I drew heavily on Whittaker (1990) which is an excellent 
</p>
<p>text on log-linear models and graphical models. Some ofthe exercises are from 
</p>
<p>Whittaker. A classic reference on log-linear models is Bishop et al. (1975). </p>
<p/>
</div>
<div class="page"><p/>
<p>19.7 Exercises 301 
</p>
<p>19.7 Exercises 
</p>
<p>1. Solve for the P~jS in terms of the j3's in Example 19.3. 
</p>
<p>2. Prove Lemma 19.5. 
</p>
<p>3. Prove Lemma 19.9. 
</p>
<p>4. Consider random variables (Xl, X 2 , X 3 , X 4 ). Suppose the log-density is 
</p>
<p>(a) Draw the graph G for these variables. 
</p>
<p>(b) Write down all independence and conditional independence relations 
</p>
<p>implied by the graph. 
</p>
<p>(c) Is this model graphical? Is it hierarchical? 
</p>
<p>5. Suppose that parameters P(Xl' X2, X3) are proportional to the following 
</p>
<p>values: 
</p>
<p>X2 0 0 1 1 
</p>
<p>X3 0 1 0 1 
</p>
<p>Xl 0 2 8 4 16 
</p>
<p>1 16 128 32 256 
</p>
<p>Find the 1/J-terms for the log-linear expansion. Comment on the model. 
</p>
<p>6. Let Xl"'" X 4 be binary. Draw the independence graphs correspond-
</p>
<p>ing to the following log-linear models. Also, identify whether each is 
</p>
<p>graphical and/or hierarchical (or neither). 
</p>
<p>(a) logf = 7 + llXl + 2X2 + 1.5x3 + 17x4 
</p>
<p>(b) log f = 7 + llXl + 2X2 + 1.5x3 + 17x4 + 12x2x3 + 78x2X4 + 3X3X4 + 
32x2X3X4 
</p>
<p>(c) logf = 7+11xl +2X2+1.5x3+17x4+12x2X3+3x3X4+XIX4+2xIX2 
</p>
<p>(d) log f = 7 + 5055xl X2X3X4 </p>
<p/>
</div>
<div class="page"><p/>
<p>20 
</p>
<p>Nonparametric Curve Estimation 
</p>
<p>In this Chapter we discuss non parametric estimation of probability density 
</p>
<p>functions and regression functions which we refer to as curve estimation or 
</p>
<p>smoothing. 
</p>
<p>In Chapter 7 we saw that it is possible to consistently estimate a cumulative 
</p>
<p>distribution function F without making any assumptions about F. If we want 
</p>
<p>to estimate a probability density function j(x) or a regression function r-(x) = 
</p>
<p>lE(YIX = x) the situation is different. We cannot estimate these functions 
</p>
<p>consistently without making some smoothness assumptions. Correspondingly, 
</p>
<p>we need to perform some sort of smoothing operation on the data. 
</p>
<p>An example of a density estimator is a histogram, which we discuss in 
</p>
<p>detail in Section 20.2. To form a histogram estimator of a density j, we divide 
</p>
<p>the real line to disjoint sets called bins. The histogram estimator is a piecewise 
</p>
<p>constant function where the height of the function is proportional to number 
</p>
<p>of observations in each bin; see Figure 20.3. The number of bins is an example 
</p>
<p>of a smoothing parameter. If we smooth too much (large bins) we get a 
</p>
<p>highly biased estimator while if we smooth too little (small bins) we get a 
</p>
<p>highly variable estimator. Much of curve estimation is concerned with trying 
</p>
<p>to optimally balance variance and bias. </p>
<p/>
</div>
<div class="page"><p/>
<p>304 20. Nonparametric Curve Estimation 
</p>
<p>g(x) 
</p>
<p>I 
This is a function of the data This is the point at which we are 
</p>
<p>eval uating g(.) 
</p>
<p>FIGURE 20.1. A curve estimate 9 is random because it is a function of the data. 
The point x at which we evaluate 9 is not a random variable. 
</p>
<p>20.1 The Bias-Variance Tradeoff 
</p>
<p>Let 9 denote an unknown function such as a density function or a regression 
</p>
<p>function. Let gn denote an estimator of g. Bear in mind that gn(x) is a random 
</p>
<p>function evaluated at a point x. The estimator is random because it depends 
</p>
<p>on the data. See Figure 20.l. 
</p>
<p>As a loss function, we will use the integrated squared error (ISE): 1 
</p>
<p>(20.1) 
</p>
<p>The risk or mean integrated squared error (MISE) with respect to 
</p>
<p>squared error loss is 
</p>
<p>(20.2) 
</p>
<p>20.1 lemma. The risk can be written as 
</p>
<p>(20.3) 
</p>
<p>where 
</p>
<p>b(x) = IE(gn(X)) - g(x) (20.4) 
</p>
<p>is the bias ofgn(x) at a fixed x and 
</p>
<p>(20.5) 
</p>
<p>is the variance ofgn(x) at a fixed x. 
</p>
<p>lWe could use other loss functions. The results are similar but the analysis is much more 
</p>
<p>complicated. </p>
<p/>
</div>
<div class="page"><p/>
<p>. . . . . . . . . . . 
...... Less Smoothing 
</p>
<p>20.2 Histograms 305 
</p>
<p>Risk 
</p>
<p>&bull;&bull; Bias squared 
</p>
<p>.. 
. . . . . . .. . . . . 
</p>
<p>... . . . 
Optimal 
</p>
<p>Smoothing 
</p>
<p>. . . . .-....... . 
... . . ... &bull;&bull;&bull; Variance 
</p>
<p>More Smoothing &bull;&bull; ~ 
</p>
<p>FIGURE 20.2. The Bias-Variance trade-off. The bias increases and the variance de-
</p>
<p>creases with the amount of smoothing. The optimal amount of smoothing, indicated 
</p>
<p>by the vertical line, minimizes the risk = bias2 + variance. 
</p>
<p>In summary, 
</p>
<p>RISK = BIAS2 + VARIANCE. (20.6) I 
When the data are oversmoothed, the bias term is large and the variance 
</p>
<p>is small. When the data are undersmoothed the opposite is true; see Figure 
</p>
<p>20.2. This is called the bias-variance tradeoff. Minimizing risk corresponds 
</p>
<p>to balancing bias and variance. 
</p>
<p>20.2 Histograrns 
</p>
<p>Let Xl, ... ,Xn be lID on [0, 1] with density f. The restriction to [0,1] is not 
crucial; we can always rescale the data to be on this interval. Let m be an </p>
<p/>
</div>
<div class="page"><p/>
<p>306 20. Nonparametric Curve Estimation 
</p>
<p>integer and define bins 
</p>
<p>Bl = [o,~) ,B2 = [~,~), ... , Bm = [~,1]. 
rn m rn m 
</p>
<p>(20.7) 
</p>
<p>Define the binwidth h = 11m, let Vj be the number of observations in B j , 
let Pj = vjln and let Pj = IE f(u)du. 
</p>
<p>J 
</p>
<p>The histogram estimator is defined by 
</p>
<p>which we can write more succinctly as 
</p>
<p>(20.8) 
</p>
<p>To understand the motivation for this estimator, let Pj = IE f(u)du and note 
J 
</p>
<p>that, for x E B j and h small, 
</p>
<p>~ lE(p) P J~ f(u)du f(x)h 
lE(jn(x)) = --t- = ~ = J h ~ f(x) = f(x). 
</p>
<p>20.2 Example. Figure 20.3 shows three different histograms based on n = 
</p>
<p>1,266 data points from an astronomical sky survey. Each data point repre-
</p>
<p>sents the distance from us to a galaxy. The galaxies lie on a "pencilbeam" 
</p>
<p>pointing directly from the Earth out into space. Because of the finite speed of 
</p>
<p>light, looking at galaxies farther and farther away corresponds to looking back 
</p>
<p>in time. Choosing the right number of bins involves finding a good tradeoff 
</p>
<p>between bias and variance. We shall see later that the top left histogram has 
</p>
<p>too few bins resulting in oversmoothing and too much bias. The bottom left 
</p>
<p>histogram has too many bins resulting in undersmoothing and too few bins. 
</p>
<p>The top right histogram is just right. The histogram reveals the presence of 
</p>
<p>clusters of galaxies. Seeing how the size and number of galaxy clusters varies 
</p>
<p>with time, helps cosmologists understand the evolution of the universe. _ 
</p>
<p>The mean and variance of in (x) are given in the following Theorem. 
20.3 Theorem. Consider fixed x and fixed m, and let B j be the bin containing 
</p>
<p>x. Then, 
</p>
<p>(20.9) </p>
<p/>
</div>
<div class="page"><p/>
<p>r-
</p>
<p>-
</p>
<p>,---
</p>
<p>r-
</p>
<p>o S 
r--
</p>
<p>1 1 1 
</p>
<p>0 .00 0.05 0 .10 
</p>
<p>Oversmoolhed 
</p>
<p>0 .00 0.05 0 .10 
</p>
<p>Undersmoothed 
</p>
<p>-
</p>
<p>I--
</p>
<p>l 
1 1 
</p>
<p>0 . 15 0.20 
</p>
<p>~ 
. ~ 
</p>
<p>~ 
~ 
::l 
&sect; 
</p>
<p>0 . 15 0.20 
</p>
<p>o .... 
</p>
<p>o 
</p>
<p>"i' 
</p>
<p>&lt;&gt; ,. 
</p>
<p>'" , 
</p>
<p>.... 
'I 
</p>
<p>20.2 Histograms 307 
</p>
<p>Just Right 
</p>
<p>o 200 400 600 600 1000 
</p>
<p>number of bins 
</p>
<p>FIGURE 20.3. Three versions of a histogram for the astronomy data. The top left 
</p>
<p>histogram has too few bins. The bottom left histogram has too many bins. The top 
</p>
<p>right histogram is just right. The lower, right plot shows the estimated risk versus 
</p>
<p>the number of bins. </p>
<p/>
</div>
<div class="page"><p/>
<p>308 20. Nonparametric Curve Estimation 
</p>
<p>Let's take a closer look at the bias-variance tradeoff using equation (20.9). 
</p>
<p>Consider some x E B j . For any other u E B j , 
</p>
<p>f(u) R:O f(x) + (u - x)f'(x) 
</p>
<p>and so 
</p>
<p>Pj = L
j 
</p>
<p>f(u)du R:O L
j 
(f(X) + (u - x)f'(X))dU 
</p>
<p>f(X)h+hf'(X)(h(j-~) -x). 
</p>
<p>Therefore, the bias b( x) is 
</p>
<p>b(x) 
~ P 
</p>
<p>IE(Jn(x)) - f(x) = l~ - f(x) 
</p>
<p>f(x)h + hf'(x) (h (j - ~) - x) . 
R:O h - j(x) 
</p>
<p>f'(x) (h (j -~) -x). 
</p>
<p>If Xj is the center of the bin, then 
</p>
<p>Therefore, 
</p>
<p>L
j 
(J'(X))2 (h (j -~) - x r dx 
</p>
<p>R:O (J'(Xj))2 j~j (h (j - ~) - x r dx 
</p>
<p>Note that this increases as a function of h. Now consider the variance. For h 
</p>
<p>small, 1 - Pj R:O 1, so 
</p>
<p>v(x) Pj R:O 
nh2 
</p>
<p>f(x)h + hf'(x) (h (j - ~) - x) 
</p>
<p>R:O 
f(x) 
</p>
<p>nh </p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Histograms 309 
</p>
<p>where we have kept only the dominant term. So, 
</p>
<p>r1 1 
Jo v(x)dx ~ nh' 
</p>
<p>Note that this decreases with h. Putting all this together, we get: 
</p>
<p>20.4 Theorem. Suppose that J(j'(U))2du &lt; 00. Then 
</p>
<p>(20.10) 
</p>
<p>The value h* that minimizes (20.10) is 
</p>
<p>1 ( 6 ) 1/3 
h* = n1/3 J(jI(U))2du 
</p>
<p>(20.11) 
</p>
<p>With this choice of binwidth, 
</p>
<p>(20.12) 
</p>
<p>( )
</p>
<p>1/3 
</p>
<p>where C = (3/4)2/3 J(jI(U))2du 
</p>
<p>Theorem 20.4 is quite revealing. We see that with an optimally chosen bin-
</p>
<p>width, the MISE decreases to 0 at rate n -2/3. By comparison, most parametric 
</p>
<p>estimators converge at rate n -1. The slower rate of convergence is the price 
</p>
<p>we pay for being nonparametric. The formula for the optimal binwidth h* is 
</p>
<p>of theoretical interest but it is not useful in practice since it depends on the 
</p>
<p>unknown function f. 
A practical way to choose the binwidth is to estimate the risk function 
</p>
<p>and minimize over h. Recall that the loss function, which we now write as a 
</p>
<p>function of h, is 
</p>
<p>L(h) J(j~(x) - f(X))2dx 
</p>
<p>J [;(x) dx - 2 J fn(x)f(x)dx + J f2(x) dx. 
</p>
<p>The last term does not depend on the binwidth h so minimizing the risk is 
</p>
<p>equivalent to minimizing the expected value of 
</p>
<p>J(h) = J [;(x) dx - 2 J fn(x)f(x)dx. </p>
<p/>
</div>
<div class="page"><p/>
<p>310 20. Nonparametric Curve Estimation 
</p>
<p>We shall refer to lE(J(h)) as the risk, although it differs from the true risk by 
</p>
<p>the constant term I P (x) dx. 
</p>
<p>20.5 Definition. The cross-validation estimator of risk is 
</p>
<p>(20.13) 
</p>
<p>where f( -i) is the histogram estimator obtained after removing the ith 
</p>
<p>observation. We refer to J(h) as the cross-validation score or estimated 
</p>
<p>risk. 
</p>
<p>20.6 Theorem. The cross-validation estimator is nearly unbiased: 
</p>
<p>lE(J(x)) ~ lE(J(x)). 
</p>
<p>In principle, we need to recompute the histogram n times to compute 5(h). 
</p>
<p>Moreover, this has to be done for all values of h. Fortunately, there is a 
</p>
<p>shortcut formula. 
</p>
<p>20.7 Theorem. The following identity holds: 
</p>
<p>~ 2 n + 1 ~~2 
J(h) = ( )h - ( ) LPj . n-1 n-1 
</p>
<p>j=l 
</p>
<p>(20.14) 
</p>
<p>20.8 Example. We used cross-validation in the astronomy example. The cross-
</p>
<p>validation function is quite flat near its minimum. Any m in the range of 73 to 
</p>
<p>310 is an approximate minimizer but the resulting histogram does not change 
</p>
<p>much over this range. The histogram in the top right plot in Figure 20.3 was 
</p>
<p>constructed using m = 73 bins. The bottom right plot shows the estimated 
</p>
<p>risk, or more precisely, A, plotted versus the number of bins. _ 
</p>
<p>Next we want a confidence set for f. Suppose fn is a histogram with m bins 
</p>
<p>and binwidth h = 11m. We cannot realistically make confidence statements 
</p>
<p>about the fine details of the true density f. Instead, we shall make confidence 
statements about f at the resolution of the histogram. To this end, define 
</p>
<p>(20.15) 
</p>
<p>where Pj = IE f(u)du. Think of ](x) as a "histogramized" version of f&middot; 
J </p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Histograms 311 
</p>
<p>20.9 Definition. A pair of functions (fn(x), un(x)) is a 1 - a confidence 
</p>
<p>band (or confidence envelope) if 
</p>
<p>(20.16) 
</p>
<p>20.10 Theorem. Let rn = rn(n) be the number of bins in the histogram fn. 
</p>
<p>Assume that rn(n) --+ 00 and rn(n) logn/n --+ 0 as n --+ 00. Define 
</p>
<p>un(X) 
</p>
<p>where 
</p>
<p>(max{ ~ -c,O}Y 
</p>
<p>( V!n(x) + cY 
</p>
<p>c = Zcx/(2m) (in. 
2 V-:;;: 
</p>
<p>Then, (fn(x), un(x)) is an approximate 1 - a confidence band. 
</p>
<p>(20.17) 
</p>
<p>(20.18) 
</p>
<p>PROOF. Here is an outline of the proof. From the central limit theorem, Pj ~ 
N(pj,pj(1- pj)/n). By the delta method, JPj ~ N(yIPj, 1/(4n)). Moreover, 
it can be shown that the JPj's are approximately independent. Therefore, 
</p>
<p>(20.19) 
</p>
<p>where Zl, ... , Zm rv N(O, 1). Let 
</p>
<p>Then, 
</p>
<p>p (m;xl~ - ViWI &gt; c) =p (mrxlV&yen; - V&yen;I &gt; c) 
P ( mrx 2yn 1 yip; - 511 &gt; Zcx/(2m)) 
</p>
<p>~ P (mrxlZjl &gt; Zcx/(2m)) &lt;::: ~P(IZjl &gt; Zcx/(2m)) 
m 
</p>
<p>L~=a. -rn 
j=l </p>
<p/>
</div>
<div class="page"><p/>
<p>312 20. Nonparametric Curve Estimation 
</p>
<p>FIGURE 20.4. 95 percent confidence envelope for astronomy data using m = 73 
</p>
<p>bins. 
</p>
<p>20.11 Example. Figure 20.4 shows a 95 percent confidence envelope for the 
</p>
<p>astronomy data. We see that even with over 1,000 data points, there is still 
</p>
<p>substantial uncertainty. _ 
</p>
<p>20.3 Kernel Density Estirnation 
</p>
<p>Histograms are discontinuous. Kernel density estimators are smoother and 
</p>
<p>they converge faster to the true density than histograms. 
</p>
<p>Let Xl, ... , Xn denote the observed data, a sample from f. In this chap-
ter, a kernel is defined to be any smooth function K such that K(x) 2 0, 
</p>
<p>I K(x) dx = 1, I xK(x)dx = 0 and uk == I x 2 K(x)dx &gt; o. Two examples of 
kernels are the Epanechnikov kernel 
</p>
<p>K(x) = { !(1 -x 2 /5)/vf&gt; Ixl &lt; vf&gt; 
other-wise 
</p>
<p>(20.20) 
</p>
<p>and the Gaussian (Normal) kernel K(x) = (27f)-1/2 e-x 2 /2. </p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Kernel Density Estimation 313 
</p>
<p>FIGURE 20.5. A kernel density estimator 1. At each point x, f(x) is the average 
of the kernels centered over the data points Xi. The data points are indicated by 
short vertical bars. 
</p>
<p>20.12 Definition. Given a kernel K and a positive number h, called the 
</p>
<p>bandwidth, the kernel density estimator is defined to be 
</p>
<p>(20.21 ) 
</p>
<p>An example of a kernel density estimator is show in Figure 20.5. The kernel 
</p>
<p>estimator effectively puts a smoothed-out lump of mass of size lin over each 
</p>
<p>data point Xi. The bandwidth h controls the amount of smoothing. When h 
</p>
<p>is close to 0, In consists of a set of spikes, one at each data point. The height 
of the spikes tends to infinity as h -+ O. When h -+ 00, In tends to a uniform 
density. </p>
<p/>
</div>
<div class="page"><p/>
<p>314 20. Nonparametric Curve Estimation 
</p>
<p>20.13 Example. Figure 20.6 shows kernel density estimators for the astron-
</p>
<p>omy data using three different bandwidths. In each case we used a Gaussian 
</p>
<p>kernel. The properly smoothed kernel density estimator in the top right panel 
</p>
<p>shows similar structure as the histogram. However, it is easier to see the clus-
</p>
<p>ters with the kernel estimator. _ 
</p>
<p>To construct a kernel density estimator, we need to choose a kernel K and 
</p>
<p>a bandwidth h. It can be shown theoretically and empirically that the choice 
</p>
<p>of K is not crucial. 2 However, the choice of bandwidth h is very important. 
</p>
<p>As with the histogram, we can make a theoretical statement about how the 
</p>
<p>risk of the estimator depends on the bandwidth. 
</p>
<p>20.14 Theorem. Under weak assumptions on f and K, 
</p>
<p>R(j,!n) R:; ~CTkh4 /(ju (X))2 + J K~~)dX 
</p>
<p>where CTk = J x 2 K(x)dx. The optimal bandwidth is 
</p>
<p>-2/5 1/5 -1/5 
1* - c1 c2 c3 
</p>
<p>I. - 1/5 
17. 
</p>
<p>(20.22) 
</p>
<p>(20.23) 
</p>
<p>where C1 = J x 2 K(x)dx, C2 = J K(X)2dx and C3 = J(jl/(x)?dx. With this 
</p>
<p>choice of bandwidth, 
</p>
<p>for some constant C4 &gt; O. 
</p>
<p>PROOF. Write Kh(x, X) = h-1 K ((x - X)jh) and !n(x) = 17.- 1 Li Kh(x, Xi). 
</p>
<p>Thus, IE[!n(x)] = IE[Kh(X, X)] and V[!n(x)] = n- 1V[Kh(x, X)]. Now, 
</p>
<p>IE[Kh(X, X)] /.!.K (x - t) f(t) dt 
h h 
</p>
<p>/ K(u)f(x - hu) du 
</p>
<p>/ [ 
, 1 U ] 
</p>
<p>K(u) f(x) - hf (x) + -;;/ (x) +... du 
</p>
<p>f(x) + ~h2 /' (x) / u2 K(u) du&middot;&middot;&middot; 
</p>
<p>since J K(x) dx = 1 and J x K(x) dx = O. The bias is 
</p>
<p>1 2 2 U 
IE[Kh(X,X)]- f(x) R:; 2CTkh f (x). 
</p>
<p>21t can be shown that the Epanechnikov kernel is optimal in the sense of giving smallest 
</p>
<p>asymptotic mean squared error, but it is really the choice of bandwidth which is crucial. </p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Kernel Density Estimation 315 
</p>
<p>0.0 0 . 1 0 .2 0.0 0.1 0.2 
</p>
<p>0 .002 0 .004 0.006 
</p>
<p>FIGURE 20.6. Kernel density estimators and estimated risk for the astronomy data. 
</p>
<p>Top left: oversmoothed. Top right: just right (bandwidth chosen by cross-validation). 
</p>
<p>Bottom left: undersmoothed. Bottom right: cross-validation curve as a function of 
</p>
<p>bandwidth h. The bandwidth was chosen to be the value of h where the curve is a 
</p>
<p>minimum. </p>
<p/>
</div>
<div class="page"><p/>
<p>316 20. Nonparametric Curve Estimation 
</p>
<p>By a similar calculation, 
</p>
<p>The result follows from integrating the squared bias plus the variance. _ 
</p>
<p>We see that kernel estimators converge at rate n -4/5 while histograms con-
</p>
<p>verge at the slower rate n- 2/ 3 . It can be shown that, under weak assumptions, 
</p>
<p>there does not exist a non parametric estimator that converges faster than 
</p>
<p>n- 4 / 5 . 
</p>
<p>The expression for h* depends on the unknown density f which makes 
the result of little practical use. As with the histograms, we shall use cross-
</p>
<p>validation to find a bandwidth. Thus, we estimate the risk (up to a constant) 
</p>
<p>by 
</p>
<p>J(h) = J r(x)dz - ~ t 1-i(Xi ) 
i=l 
</p>
<p>(20.24) 
</p>
<p>where 1-i is the kernel density estimator after omitting the ith observation. 
</p>
<p>20.15 Theorem. For any h &gt; 0, 
</p>
<p>IE [J( h)] = IE [J (h) 1 . 
</p>
<p>Also, 
</p>
<p>(20.25) 
</p>
<p>where K*(x) = K(2) (x) - 2K(x) and K(2)(z) = I K(z - y)K(y)dy. In par-
ticular, if K is a N(O,l) Gaussian kernel then K(2)(Z) is the N(0,2) density. 
</p>
<p>We then choose the bandwidth hn that minimizes J(h).3 A justification for 
</p>
<p>this method is given by the following remarkable theorem due to Stone. 
</p>
<p>20.16 Theorem (Stone's Theorem). Suppose that f is bounded. Let 1h denote 
the kernel estimator with bandwidth h and let hn denote the bandwidth chosen 
</p>
<p>by cross-validation. Then, 
</p>
<p>I (t(x) - j",,(x)f dx p 
~ 2 --+l. 
</p>
<p>inf h I (t ( x) - !h ( x ) ) dx 
(20.26) 
</p>
<p>3For large data sets, i and (20.25) can be computed quickly using the fast Fourier transform. </p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Kernel Density Estimation 317 
</p>
<p>20.17 Example. The top right panel of Figure 20.6 is based on cross-validation. 
</p>
<p>These data are rounded which problems for cross-validation. Specifically, it 
</p>
<p>causes the minimizer to be h = o. To overcome this problem, we added a 
small amount of random Normal noise to the data. The result is that J(h) is 
</p>
<p>very smooth with a well defined minimum. _ 
</p>
<p>20.18 Remark. Do not assume that, if the estimator f is wiggly, then cross-
validation has let you down. The eye is not a good judge of risk. 
</p>
<p>To construct confidence bands, we use something similar to histograms. 
</p>
<p>Again, the confidence band is for the smoothed version, 
</p>
<p>in = lEC{;, (x)) = / ~K (x ~ u) f(u) du, 
</p>
<p>of the true density f. 4 Assume the density is on an interval (a, b). The band 
</p>
<p>is 
</p>
<p>where 
</p>
<p>l'n(x) = h(x) - q se(x), un(x) = h(x) + q se(x) 
</p>
<p>se(x) 
</p>
<p>Yi(x) 
</p>
<p>q 
</p>
<p>s(x) 
</p>
<p>fo' 
</p>
<p>1 2:n - 2 - (Yi(x) - Y n(x)) , 
71,-1 
</p>
<p>i=l 
</p>
<p>~K (x ~LXi) , 
([&gt;-1 C + (1 ; a)l/m) , 
b-a 
</p>
<p>w 
</p>
<p>(20.27) 
</p>
<p>where w is the width of the kernel. In case the kernel does not have finite 
</p>
<p>width then we take w to be the effective width, that is, the range over which 
</p>
<p>the kernel is non-negligible. In particular, we take w = 3h for the Normal 
</p>
<p>kernel. 
</p>
<p>20.19 Example. Figure 20.7 shows approximate 95 percent confidence bands 
</p>
<p>for the astronomy data. _ 
</p>
<p>4This is a modified version of the band described in Chaudhuri and Marron (1999). </p>
<p/>
</div>
<div class="page"><p/>
<p>318 20. Nonparametric Curve Estimation 
</p>
<p>FIGURE 20.7. 95 percent confidence bands for kernel density estimate for the as-
</p>
<p>tronomy data. 
</p>
<p>Suppose now that the data Xi = (XiI, ... , X id ) are d-dimensional. The ker-
</p>
<p>nel estimator can easily be generalized to d dimensions. Let h = (hI' ... ' hd ) 
</p>
<p>be a vector of bandwidths and define 
</p>
<p>_ 1 n 
</p>
<p>fn(x) = - L Kh(X - Xi) 
n 
</p>
<p>(20.28) 
</p>
<p>i=l 
</p>
<p>where 
</p>
<p>Kh(x-Xi) = 1 {rrd K(Xi-Xi j )} 
nhl ... hd j=l h j 
</p>
<p>(20.29) 
</p>
<p>where hI, ... , hd are bandwidths. For simplicity, we might take h j = 8 j h where 
</p>
<p>8j is the standard deviation of the jth variable. There is now only a single 
</p>
<p>bandwidth h to choose. Using calculations like those in the one-dimensional 
</p>
<p>case, the risk is given by 
</p>
<p>R(j, in) ~ ~(}k [t hj J fJj(x)dx + L h;h% J fjjikk dX] 
J=l J#k 
</p>
<p>U K2(X)dx)d 
+ ....:..::....---'---'------'----
</p>
<p>nh1 &middot;&middot;&middot; hd 
</p>
<p>where fjj is the second partial derivative of f. The optimal bandwidth satisfies 
</p>
<p>hi ~ Cln-1/(4+d), leading to a risk of order n- 4 /(4+d). From this fact, we see </p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Nonparametric Regression 319 
</p>
<p>that the risk increases quickly with dimension, a problem usually called the 
</p>
<p>curse of dimensionality. To get a sense of how serious this problem is, 
</p>
<p>consider the following table from Silverman (1986) which shows the sample 
</p>
<p>size required to ensure a relative mean squared error less than 0.1 at 0 when 
</p>
<p>the density is multivariate normal and the optimal bandwidth is selected: 
</p>
<p>Dimension Sample Size 
</p>
<p>1 4 
</p>
<p>2 19 
</p>
<p>3 67 
</p>
<p>4 223 
</p>
<p>5 768 
</p>
<p>6 2790 
</p>
<p>7 10,700 
</p>
<p>8 43,700 
</p>
<p>9 187,000 
</p>
<p>10 842,000 
</p>
<p>This is bad news indeed. It says that having 842,000 observations in a ten-
</p>
<p>dimensional problem is really like having 4 observations in a one-dimensional 
</p>
<p>problem. 
</p>
<p>20.4 Nonpararnetric Regression 
</p>
<p>Consider pairs of points (Xl, Yd, ... , (x n , Yn ) related by 
</p>
<p>(20.30) 
</p>
<p>where lE( Ei) = o. We have written the xi's in lower case since we will treat 
them as fixed. We can do this since, in regression, it is only the mean of Y 
</p>
<p>conditional on X that we are interested in. We want to estimate the regression 
</p>
<p>function r-(x) = lE(YIX = x). 
</p>
<p>There are many nonparametric regression estimators. Most involve esti-
</p>
<p>mating r-(x) by taking some sort of weighted average of the Y/s, giving higher 
</p>
<p>weight to those points near x. A popular version is the N ad araya-Watson 
</p>
<p>kernel estimator. 
</p>
<p>20.20 Definition. The Nadaraya-Watson kernel estimator is defined 
</p>
<p>by 
n 
</p>
<p>r(x) = L Wi(X)Y; (20.31 ) 
i=l </p>
<p/>
</div>
<div class="page"><p/>
<p>320 20. Nonparametric Curve Estimation 
</p>
<p>where K is a kernel and the weights Wi(X) are given by 
</p>
<p>K (X-Xi.) 
Wi(X) = -h-. 
</p>
<p>"n K (X-X:j ) 
L..J=1 h 
</p>
<p>(20.32) 
</p>
<p>The form of this estimator comes from first estimating the joint density 
</p>
<p>f(x, y) using kernel density estimation and then inserting the estimate into 
</p>
<p>the formula, 
</p>
<p>J . J yf(x, y)dy r(x) = lE(YIX = x) = yj(ylx)dy = J f(x, y)dy . 
20.21 Theorem. Suppose that V(Ei) = (J"2. The risk of the Nadaraya- Watson 
</p>
<p>kernel estimator is 
</p>
<p>R(pn, r) ~ ~4 (J x 2 K2(x)dx r J (rll(x) + 2r'(x) j(~} r dx 
J (J"2 J K2(x)dx + ( dx. (20.33) 
</p>
<p>nhf x) 
</p>
<p>The optimal bandwidth decreases at rate n- I / 5 and with this choice the risk 
</p>
<p>decreases at rate n -4/5. 
</p>
<p>In practice, to choose the bandwidth h we minimize the cross validation 
</p>
<p>score 
n 
</p>
<p>J(h) = 2)Yi - r_i(xi))2 (20.34) 
i=l 
</p>
<p>where r-i is the estimator we get by omitting the ith variable. Fortunately, 
</p>
<p>there is a shortcut formula for computing J. 
</p>
<p>20.22 Theorem. J can be written as 
</p>
<p>~ ~ _ 2 1 
J(h) = L.)Yi - r(xi)) 2&middot; 
</p>
<p>i=l ( K(O)) 
1 - ~T.' K(:Ti .. :Tj) 
</p>
<p>~'}=1 11 
</p>
<p>(20.35) 
</p>
<p>20.23 Example. Figures 20.8 shows cosmic microwave background (CMB) 
</p>
<p>data from BOOMERaNG (Netterfield et al. (2002)), Maxima (Lee et al. 
</p>
<p>(2001)), and DASI (Halverson et al. (2002))). The data consist of n pairs 
</p>
<p>(Xl, YI ), ... , (xn' Yn) where Xi is called the multipole moment and Y; is the </p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Nonparametric Regression 321 
</p>
<p>estimated power spectrum of the temperature fluctuations. What you are see-
</p>
<p>ing are sound waves in the cosmic microwave background radiation which is 
</p>
<p>the heat, left over from the big bang. If r(x) denotes the true power spectrum, 
</p>
<p>then 
</p>
<p>where Ei is a random error with mean O. The location and size of peaks in 
</p>
<p>r(x) provides valuable clues about the behavior of the early universe. Figure 
</p>
<p>20.8 shows the fit based on cross-validation as well as an undersmoothed and 
</p>
<p>oversmoothed fit. The cross-validation fit shows the presence of three well-
</p>
<p>defined peaks, as predicted by the physics of the big bang. _ 
</p>
<p>The procedure for finding confidence bands is similar to that for density 
</p>
<p>estimation. However, we first need to estimate (52. Suppose that the x;'s are 
</p>
<p>ordered. Assuming r(x) is smooth, we have r(xHd - r(xi) :=:::: 0 and hence 
</p>
<p>and hence 
</p>
<p>We can thus use the average of the n - 1 differences Yi+l - Yi to estimate (52. 
</p>
<p>Hence, define 
n-l 
</p>
<p>~2 1 L( )2 
(5 = ( ) Yi+l - Yi . 
</p>
<p>2n-l 
i=l 
</p>
<p>(20.36) 
</p>
<p>As with density estimate, the confidence band is for the smoothed version 
</p>
<p>rn(x) = lE(rn(x)) of the true regression function r. </p>
<p/>
</div>
<div class="page"><p/>
<p>C&gt; 
</p>
<p>:5 
</p>
<p>C&gt; 
C&gt; 
C&gt; 
&lt;0 
</p>
<p>C&gt; 
</p>
<p>:5 
U') 
</p>
<p>C&gt; 
C&gt; 
C&gt; 
</p>
<p>'" 
</p>
<p>:5 
C&gt; 
M 
</p>
<p>C&gt; 
C&gt; 
C&gt; 
N 
</p>
<p>C&gt; 
C&gt; 
</p>
<p>~ 
</p>
<p>322 20. Nonparametric Curve Estimation 
</p>
<p>C&gt; 
</p>
<p>:5 
</p>
<p>200 400 600 800 1000 
</p>
<p>Undersmoothed 
</p>
<p>U') 
</p>
<p>C&gt; 
</p>
<p>&bull; + 21l 
U') 
</p>
<p>C&gt; 
+ 
</p>
<p>'" ..... 
</p>
<p>U') 
</p>
<p>C&gt; 
~ d; 
00 .&lt;= &lt;0 
'0 
</p>
<p>'* E U') ~ C&gt; + 
'" U') 
</p>
<p>U') 
</p>
<p>C&gt; 
+ 
'" '" 
</p>
<p>~ 
+ 
</p>
<p>'" M 
</p>
<p>200 400 600 800 1000 
</p>
<p>.Just Right (Using cross-valdiation) 
</p>
<p>20 
</p>
<p>&bull; 
</p>
<p>&bull; 
&bull;&bull; 
&bull; 
</p>
<p>200 
</p>
<p>40 
</p>
<p>400 600 800 
</p>
<p>Oversmoothed 
</p>
<p>60 80 
</p>
<p>bandwidth 
</p>
<p>FIGURE 20.S. Regression analysis of the CMB data. The first fit is undersmoothed, 
</p>
<p>the second is oversmoothed, and the third is based on cross-validation. The last 
</p>
<p>panel shows the estimated risk versus the bandwidth of the smoother. The data are 
</p>
<p>from BOOMERaNG, Maxima, and DASI. 
</p>
<p>&bull;&bull; 
&bull; 
</p>
<p>&bull; 
1000 
</p>
<p>100 120 </p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Nonparametric Regression 323 
</p>
<p>Confidence Bands for Kernel Regression 
</p>
<p>An approximate 1 - a confidence band for r n (x) is 
</p>
<p>where 
</p>
<p>n 
</p>
<p>se(x) (j \ 8 wf(x), 
q &lt;1&gt;-1 C + (1 ; a)l/m) , 
</p>
<p>b-a 
m 
</p>
<p>w 
</p>
<p>(20.37) 
</p>
<p>(j is defined in (20.36) and w is the width of the kernel. In case the kernel 
</p>
<p>does not have finite width then we take w to be the effective width, that 
</p>
<p>is, the range over which the kernel is non-negligible. In particular, we take 
</p>
<p>w = 3h for the Normal kernel. 
</p>
<p>20.24 Example. Figure 20.9 shows a 95 percent confidence envelope for the 
</p>
<p>CMB data. We see that we are highly confident of the existence and position 
</p>
<p>of the first peak. We are more uncertain about the second and third peak. 
</p>
<p>At the time of this writing, more accurate data are becoming available that 
</p>
<p>apparently provide sharper estimates of the second and third peak. _ 
</p>
<p>The extension to multiple regressors X = (Xl' ... ' Xp) is straightforward. 
</p>
<p>As with kernel density estimation we just replace the kernel with a multivari-
</p>
<p>ate kernel. However, the same caveats about the curse of dimensionality apply. 
</p>
<p>In some cases, we might consider putting some restrictions on the regression 
</p>
<p>function which will then reduce the curse of dimensionality. For example, 
</p>
<p>additive regression is based on the model 
</p>
<p>p 
</p>
<p>Y = L rj(Xj ) + f. (20.38) 
j=l 
</p>
<p>Now we only need to fit p one-dimensional functions. The model can be en-
</p>
<p>riched by adding various interactions, for example, 
</p>
<p>p 
</p>
<p>Y = L rj(Xj ) + L rjk(XjXk) + f. (20.39) 
j=l j&lt;k 
</p>
<p>Additive models are usually fit by an algorithm called backfitting. </p>
<p/>
</div>
<div class="page"><p/>
<p>324 20. Nonparametric Curve Estimation 
</p>
<p>~ ~-----r-------r-------r-------r-------r~ 
</p>
<p>FIGURE 20.9. 95 percent confidence envelope for the CMB data. 
</p>
<p>Backfitti ng 
</p>
<p>2. For j = 1, ... ,p: 
</p>
<p>(a) Let fi = Yi - L:s,fj Ts(Xi). 
</p>
<p>(b) Let Tj be the function estimate obtained by regressing the f/S 
</p>
<p>on the jth covariate. 
</p>
<p>3. If converged STOP. Else, go back to step 2. 
</p>
<p>Additive models have the advantage that they avoid the curse of dimension-
</p>
<p>ality and they can be fit quickly, but they have one disadvantage: the model 
</p>
<p>is not fully nonparametric. In other words, the true regression function T(X) 
</p>
<p>may not be of the form (20.38). 
</p>
<p>20.5 Appendix 
</p>
<p>CONFIDENCE SETS AND BIAS. The confidence bands we computed are not 
</p>
<p>for the density function or regression function but rather for the smoothed </p>
<p/>
</div>
<div class="page"><p/>
<p>20.6 Bibliographic Remarks 325 
</p>
<p>function. For example, the confidence band for a kernel density estimate with 
</p>
<p>bandwidth h is a band for the function one gets by smoothing the true function 
</p>
<p>with a kernel with the same bandwidth. Getting a confidence set for the true 
</p>
<p>function is complicated for reasons we now explain. 
</p>
<p>Let In(x) denote an estimate of the function f(x). Denote the mean and 
</p>
<p>standard deviation of In(x) by fn(x) and sn(x). Then, 
</p>
<p>Typically, the first term converges to a standard Normal from which one de-
</p>
<p>rives confidence bands. The second term is the bias divided by the standard 
</p>
<p>deviation. In parametric inference, the bias is usually smaller than the stan-
</p>
<p>dard deviation of the estimator so this term goes to 0 as the sample size 
</p>
<p>increases. In nonparametric inference, optimal smoothing leads us to balance 
</p>
<p>the bias and the standard deviation. Thus the second term does not vanish 
</p>
<p>even with large sample sizes. This means that the confidence interval will not 
</p>
<p>be centered around the true function f. 
</p>
<p>20.6 Bibliographic Rernarks 
</p>
<p>Two very good books on density estimation are Scott (1992) and Silverman 
</p>
<p>(1986). The literature on nonparametric regression is very large. Two good 
</p>
<p>starting points are HardIe (1990) and Loader (1999). The latter emphasizes a 
</p>
<p>class of techniques called local likelihood methods. 
</p>
<p>20.7 Exercises 
</p>
<p>1. Let Xl, ... ,Xn rv f and let f n be the kernel density estimator using the 
boxcar kernel: 
</p>
<p>K x = 2 2 {
I _1 &lt; x &lt; 1 
</p>
<p>( ) 0 otherwise. 
</p>
<p>(a) Show that 11x +(h/2) 
lE(l(x)) = T f(y)dy 
</p>
<p>L x- (h/2) 
</p>
<p>and 
</p>
<p>[ ( ) 2] 
~ 1 x+(h/2) x+(h/2) 
</p>
<p>V(J(x)) = -h2 1 f(y)dy - 1 f(y)dy 
n x-(h/2) x-(h/2) </p>
<p/>
</div>
<div class="page"><p/>
<p>326 20. Nonparametric Curve Estimation 
</p>
<p>~ p 
</p>
<p>(b) Show that if h -+ 0 and nh -+ 00 as n -+ 00, then fn(x)--+ f(x). 
</p>
<p>2. Get the data on fragments of glass collected in forensic work from the 
</p>
<p>book website. Estimate the density of the first variable (refractive in-
</p>
<p>dex) using a histogram and use a kernel density estimator. Use cross-
</p>
<p>validation to choose the amount of smoothing. Experiment with different 
</p>
<p>binwidths and bandwidths. Comment on the similarities and differences. 
</p>
<p>Construct 95 percent confidence bands for your estimators. 
</p>
<p>3. Consider the data from question 2. Let Y be refractive index and let 
</p>
<p>x be aluminum content (the fourth variable). Perform a nonparametric 
</p>
<p>regression to fit the model Y = f (x) + E. Use cross-validation to estimate 
the bandwidth. Construct 95 percent confidence bands for your estimate. 
</p>
<p>4. Prove Lemma 20.1. 
</p>
<p>5. Prove Theorem 20.3. 
</p>
<p>6. Prove Theorem 20.7. 
</p>
<p>7. Prove Theorem 20.15. 
</p>
<p>8. Consider regression data (Xl, Yd, ... , (xn' Yn). Suppose that 0 &lt;::: Xi &lt;::: 1 
</p>
<p>for all i. Define bins B j as in equation (20.7). For X E B j define 
</p>
<p>where Y j is the mean of all the Y/s corresponding to those xi's in B j . 
</p>
<p>Find the approximate risk of this estimator. From this expression for 
</p>
<p>the risk, find the optimal bandwidth. At what rate does the risk go to 
</p>
<p>zero? 
</p>
<p>9. Show that with suitable smoothness assumptions on r(x), &amp;2 in equation 
</p>
<p>(20.36) is a consistent estimator of (J2. 
</p>
<p>10. Prove Theorem 20.22. </p>
<p/>
</div>
<div class="page"><p/>
<p>21 
</p>
<p>Smoothing Using Orthogonal Functions 
</p>
<p>In this chapter we will study an approach to nonparametric curve estima-
</p>
<p>tion based on orthogonal functions. We begin with a brief introduction to 
</p>
<p>the theory of orthogonal functions, then we turn to density estimation and 
</p>
<p>regression. 
</p>
<p>21.1 Orthogonal Functions and L2 Spaces 
</p>
<p>Let v = (VI, V2, V3) denote a three-dimensional vector, that is, a list of three 
</p>
<p>real numbers. Let V denote the set of all such vectors. If a is a scalar (a 
</p>
<p>number) and v is a vector, we define av = (avl' aV2, aV3). The sum of vectors 
</p>
<p>v and W is defined by v + W = (VI + WI, V2 + W2, v3 + W3). The inner product 
between two vectors v and W is defined by (v, W! = 2:7=1 ViWi' The norm 
(or length) of a vector v is defined by 
</p>
<p>(21.1) 
</p>
<p>Two vectors are orthogonal (or perpendicular) if (v,W! = o. A set of 
vectors are orthogonal if each pair in the set is orthogonal. A vector is normal 
</p>
<p>if Ilvll = 1. </p>
<p/>
</div>
<div class="page"><p/>
<p>328 21. Smoothing Using Orthogonal Functions 
</p>
<p>Let (h = (1,0,0), &cent;2 = (0,1,0), &cent;3 = (0,0,1). These vectors are said to be 
an orthonormal basis for V since they have the following properties: 
</p>
<p>(i) they are orthogonal; 
</p>
<p>(ii) they are normal; 
</p>
<p>(iii) they form a basis for V, which means that any v E V can be written as a 
</p>
<p>linear combination of &cent;1, &cent;2, &cent;3: 
</p>
<p>3 
</p>
<p>v=L{3j&cent;j where {3j = (&cent;j,v;. 
j=l 
</p>
<p>(21.2) 
</p>
<p>For example, if v = (12,3,4) then v = 12&cent;1 + 3&cent;2 + 4&cent;3. There are other 
orthonormal bases for V, for example, 
</p>
<p>You can check that these three vectors also form an orthonormal basis for V. 
</p>
<p>Again, if v is any vector then we can write 
</p>
<p>3 
</p>
<p>v = L {3jl/Jj where {3j = (1/Jj, v;. 
j=l 
</p>
<p>For example, if v = (12,3,4) then 
</p>
<p>v = 10.971/J1 + 6.361/J2 + 2.861/J3' 
</p>
<p>Now we make the leap from vectors to functions. Basically, we just replace 
</p>
<p>vectors with functions and sums with integrals. Let L2 (a, b) denote all func-
</p>
<p>tions defined on the interval [a, b] such that J: f(x)2dx &lt; 00: 
</p>
<p>(21.3) 
</p>
<p>We sometimes write L2 instead of L 2(a, b). The inner product between two 
</p>
<p>functions f, g E L2 is defined by J f(x)g(x)dx. The norm of f is 
</p>
<p>Ilfll = / J f(x)2dx. (21.4) 
Two functions are orthogonal if J f(x)g(x)dx = O. A function is normal if 
Ilfll = 1. 
</p>
<p>A sequence of functions &cent;1, &cent;2, &cent;3,'" is orthonormal if J &cent;;(x)dx = 1 for 
each j and J &cent;i(x)&cent;j(x)dx = 0 for i i=- j. An orthonormal sequence is com-
plete if the only function that is orthogonal to each &cent;j is the zero function. </p>
<p/>
</div>
<div class="page"><p/>
<p>21.1 Orthogonal Functions and L2 Spaces 329 
</p>
<p>In this case, the functions eP1, eP2, eP3,'" form in basis, meaning that if f E L2 
</p>
<p>then f can be written as1 
</p>
<p>00 (b 
where (3j = Ja f(x)ePj(x)dx. (21.5) f(x) = L (3jePj(x), 
</p>
<p>j=l 
</p>
<p>A useful result is Parseval's relation which says that 
</p>
<p>IIfl12 == J f2(x) dx = f{3} == 11{311 2 
j=l 
</p>
<p>(21.6) 
</p>
<p>where {3 = ({31, (32, ... ). 
</p>
<p>21.1 Example. An example of an orthonormal basis for L2(0, 1) is the cosine 
</p>
<p>basis defined as follows. Let ePo(x) = 1 and for j ~ 1 define 
</p>
<p>The first six functions are plotted in Figure 21.1. _ 
</p>
<p>21.2 Example. Let 
</p>
<p>f(x) = Jx(1 - x) sin (( 2.l7r )) 
x+ .05 
</p>
<p>(21. 7) 
</p>
<p>which is called the "doppler function." Figure 21.2 shows f (top left) and its 
approximation 
</p>
<p>J 
</p>
<p>!J(x) = L (3jePj(x) 
j=l 
</p>
<p>with J equal to 5 (top right), 20 (bottom left), and 200 (bottom right). 
</p>
<p>As J increases we see that !J(x) gets closer to f(x). The coefficients {3j 
</p>
<p>Jo1 f(x)ePj(x)dx were computed numerically. _ 
</p>
<p>21.3 Example. The Legendre polynomials on [-1,1] are defined by 
</p>
<p>1 dj 2 . 
Pj(x)=-&middot;-'I-d .(x -1)1, j=0,1,2, ... 
</p>
<p>2) J. x) 
(21.8) 
</p>
<p>It can be shown that these functions are complete and orthogonal and that 
</p>
<p>11 2 p 2(x)dx = --. 
-1 ) 2j+l 
</p>
<p>(21.9) 
</p>
<p>IThe equality in the displayed equation means that J(f(x) - fn(X))2dx --+ 0 where fn(x) = 
</p>
<p>"L.J=l (3j(/Jj(x). </p>
<p/>
</div>
<div class="page"><p/>
<p>330 21. Smoothing Using Orthogonal Functions 
</p>
<p>FIGURE 21.1. The first six functions in the cosine basis. 
</p>
<p>FIGURE 21.2. Approximating the doppler function with its expansion 
</p>
<p>in the cosine basis. The function f (top left) and its approximation 
fJ(x) = "L:=1 (3jrPj(x) with J equal to 5 (top right), 20 (bottom left), 
and 200 (bottom right). The coefficients {3j = J~l f(x)rPj(x)dx were 
computed numerically. </p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Density Estimation 331 
</p>
<p>It follows that the functions epj(x) = J(2j + 1)/2Pj (x), j = 0,1, ... form an 
orthonormal basis for L2 ( -1, 1). The first few Legendre polynomials are: 
</p>
<p>Po(X) 1, 
P1(x) x , 
</p>
<p>P2(x) ~ (3X2 - 1), and 
</p>
<p>P3 (x) ~ (5X3 - 3X). 
</p>
<p>These polynomials may be constructed explicitly using the following recursive 
</p>
<p>relation: 
P (x) = (2j + l)xPj (x) - jPj- 1(x). 
</p>
<p>]+1 j + 1 &bull; (21.10) 
</p>
<p>The coefficients {31, {32, ... are related to the smoothness of the function f. 
To see why, note that if f is smooth, then its derivatives will be finite. Thus we 
expect that, for some k, Jo1(f(k)(x))2dx &lt; 00 where f(k) is the kth derivative 
</p>
<p>of f. Now consider the cosine basis (21.7) and let f(x) = L;:o {3jepj(x). Then, 
</p>
<p>The only way that L;:l {3J(nj)2k can be finite is if the {3/s get small when 
</p>
<p>j gets large. To summarize: 
</p>
<p>If the function f is smooth, then the coefficients {3j will be small 
when j is large. 
</p>
<p>For the rest of this chapter, assume we are using the cosine basis unless 
</p>
<p>otherwise specified. 
</p>
<p>21.2 Density Estirnation 
</p>
<p>Let Xl, ... ,Xn be IID observations from a distribution on [0,1] with density 
</p>
<p>f. Assuming f E L2 we can write 
00 
</p>
<p>f(x) = L {3jepj(x) 
j=O 
</p>
<p>where ep1, ep2, ... is an orthonormal basis. Define 
</p>
<p>~ 1 n 
{3j = - L epj(Xi ). 
</p>
<p>n 
(21.11) 
</p>
<p>i=l </p>
<p/>
</div>
<div class="page"><p/>
<p>332 21. Smoothing Using Orthogonal Functions 
</p>
<p>21.4 Theorem. The mean and variance of Pj are 
</p>
<p>(21.12) 
</p>
<p>where 
</p>
<p>(21.13) 
</p>
<p>PROOF. The mean is 
</p>
<p>IE (&cent;j(X1 )) 
</p>
<p>J &cent;j(x)f(x)dx = Pj&middot; 
The calculation for the variance is similar .&bull; 
</p>
<p>Hence, Pj is an unbiased estimate of Pj. It is tempting to estimate f by 
</p>
<p>L~l (3j&cent;j(x) but this turns out to have a very high variance. Instead, consider 
</p>
<p>the estimator 
J 
</p>
<p>j(x) = L (3j&cent;j(x). (21.14) 
j=l 
</p>
<p>The number of terms J is a smoothing parameter. Increasing J will decrease 
</p>
<p>bias while increasing variance. For technical reasons, we restrict J to lie in 
</p>
<p>the range 
</p>
<p>l::';J::';p 
</p>
<p>where p = p(n) = ,[n. To emphasize the dependence of the risk function on 
</p>
<p>J, we write the risk function as R( J). 
</p>
<p>21.5 Theorem. The risk of f is 
</p>
<p>J 2 00 
</p>
<p>R(J) = L (Jj + L 13;' 
n 
</p>
<p>j=l j=J+1 
</p>
<p>(21.15) 
</p>
<p>An estimate of the risk is 
</p>
<p>J -2 P ( -2) 
R(J) = L ~ + L (3; - :~ 
</p>
<p>j=l j=J+1 + 
</p>
<p>(21.16) 
</p>
<p>where a+ = max{ a, O} and 
</p>
<p>(21.17) </p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Density Estimation 333 
</p>
<p>To motivate this estimator, note that 0'; is an unbiased estimate of o-j and 
(3; - 0'; is an unbiased estimator of fJ;. We take the positive part of the latter 
term since we know that fJ; cannot be negative. We now choose 1 s: J s: p to 
minimize R(!, 1). Here is a summary: 
</p>
<p>Summary of Orthogonal Function Density Estimation 
</p>
<p>1. Let 
</p>
<p>2. Choose J to minimize R( J) over 1 s: J s: p = fo where R is given in 
equation (21.16). 
</p>
<p>3. Let 
J 
</p>
<p>!(x) = L:(3j&lt;Pj(x). 
j=l 
</p>
<p>The estimator fn can be negative. If we are interested in exploring the 
</p>
<p>shape of f, this is not a problem. However, if we need our estimate to be a 
probability density function, we can truncate the estimate and then normalize 
</p>
<p>~ ~ &middot;1 ~ 
it. That is, we take 1* = max{fn(x), O}I 10 max{fn(u) , O}du. 
</p>
<p>Now let us construct a confidence band for f. Suppose we estimate fusing 
</p>
<p>J orthogonal functions. We are essentially estimating iJ(x) = 2:,f=l fJj&lt;Pj(x) 
</p>
<p>not the true density f(x) = 2:,;:1 fJj&lt;pj(x). Thus, the confidence band should 
</p>
<p>be regarded as a band for f J (x). 
</p>
<p>21.6 Theorem. An approximate 1- a confidence band for iJ is (l'(x),u(x)) 
</p>
<p>where 
</p>
<p>where 
</p>
<p>and 
</p>
<p>l'(x) = h(x) - c, u(x) = In(x) + c 
</p>
<p>K = max max 1 &lt;P j (x) I&middot; 
l~J~J x 
</p>
<p>For the cosine basis, K = .J2. 
</p>
<p>(21.18) 
</p>
<p>(21.19) 
</p>
<p>PROOF. Here is an outline of the proof. Let L = 2:,f=l((3j - fJj)2. By the 
</p>
<p>central limit theorem, (3j ~ N(fJj,o-jln). Hence, (3j ~ fJj + (JjEjlfo where </p>
<p/>
</div>
<div class="page"><p/>
<p>334 21. Smoothing Using Orthogonal Functions 
</p>
<p>Ej rv N(O, 1), and therefore 
</p>
<p>J 2 J 2 
</p>
<p>12::22 K2::2dK 2 LR:;- eJE&lt;::':- E'=-XJ' n JJ n J n 
j=l j=l 
</p>
<p>(21.20) 
</p>
<p>Thus we have, approximately, that 
</p>
<p>IF' L&gt; -X} a &lt;::.: IF' -X} &gt; -X} a = a. ( K2 ) (K2 K2 ) 
n' n n' 
</p>
<p>Also, 
</p>
<p>J 
</p>
<p>max 11J(x) - h(x)1 &lt; max 2:: l&cent;j(x)1 l;3j - pjl 
x x 
</p>
<p>j=l 
</p>
<p>J 
</p>
<p>&lt; K2:: l;3j - pjl 
j=l 
</p>
<p>J 
</p>
<p>&lt; VJ K 2::(;3j - pj)2 
j=l 
</p>
<p>where the third inequality is from the Cauchy-Schwartz inequality (Theorem 
</p>
<p>4.8). So, 
</p>
<p>( ~ /Jif:) IF' m;x Ih(x) - h(x)1 &gt; K2y ---:;=-n-' &lt; {/JK .II&gt; f{'t~:a ) 
W(JL&gt;f{~) 
</p>
<p>21.7 Example. Let 
</p>
<p>IF' ( L &gt; K2~},a ) 
</p>
<p>&lt; a. _ 
</p>
<p>5 1 5 
f(x) = "6&cent;(x; 0, 1) + "62:: &cent;(x; ILj, .1) 
</p>
<p>j=l 
</p>
<p>where &cent;(x; IL, eJ) denotes a Normal density with mean IL and standard deviation 
</p>
<p>eJ, and ({L1,"" {L5) = (-1, -1/2, 0, 1/2, 1). Marron and Wand (1992) call this </p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Regression 335 
</p>
<p>0 .0 0 .2 0 . 4 0.6 0.8 1.0 
</p>
<p>FIGURE 21.3. The top plot is the true density for the Bart Simpson distribution 
</p>
<p>(rescaled to have most of its mass between 0 and 1). The bottom plot is the orthog-
</p>
<p>onal function density estimate and 95 percent confidence band. 
</p>
<p>"the claw" although the "Bart Simpson" might be more appropriate. Figure 
</p>
<p>21.3 shows the true density as well as the estimated density based on n = 
</p>
<p>5, 000 observations and a 95 percent confidence band. The density has been 
</p>
<p>rescaled to have most of its mass between a and 1 using the transformation 
y = (x + 3)/6 .&bull; 
</p>
<p>21.3 Regression 
</p>
<p>Consider the regression model 
</p>
<p>(21.21 ) 
</p>
<p>where the fi are independent with mean a and variance (J"2. We will initially 
focus on the special case where Xi = 'iln. We assume that r E L2 (0, 1) and 
</p>
<p>hence we can write 
</p>
<p>CXl 
</p>
<p>r(x) = L Pj&lt;Pj(x) 
j=l 
</p>
<p>where Pj = 11 r(x)&lt;pj(x)dx 
where &lt;P1, &lt;P2,'" where is an orthonormal basis for [0,1]. 
</p>
<p>(21.22) </p>
<p/>
</div>
<div class="page"><p/>
<p>336 21. Smoothing Using Orthogonal Functions 
</p>
<p>Define 
</p>
<p>(21.23) 
</p>
<p>Since fij is an average, the central limit theorem tells us that fij will be 
</p>
<p>approximately Normally distributed. 
</p>
<p>21.8 Theorem. 
</p>
<p>(21.24) 
</p>
<p>PROOF. The mean of fij is 
</p>
<p>IE(/3y ) 
</p>
<p>where the approximate equality follows from the definition of a Riemann in-
</p>
<p>tegral: Li {j.nh(xi) --+ Jo1 h(x)dx where {j.n = lin. The variance is 
</p>
<p>since J eP;(x)dx = 1. &bull; 
Let 
</p>
<p>and let 
</p>
<p>J 
</p>
<p>r(x) = L (ijePj(x), 
j=l 
</p>
<p>R(J) = IE J (r(x) - r(x))2 dx 
be the risk of the estimator. 
</p>
<p>21.9 Theorem. The risk R(J) of the estimator rn(x) = Lf=l (ijePj(x) is 
</p>
<p>(21.25) </p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Regression 337 
</p>
<p>To estimate for (J2 = V ( Ei) we use 
</p>
<p>n 
~2 n ~ 
(J = k ~ 
</p>
<p>i=n-k+l 
</p>
<p>(21.26) 
</p>
<p>where k = n I 4. To motivate this estimator, recall that if f is smooth, then 
fJj :::::: 0 for large j. So, for j ~ k, {3j :::::: N(O, (J2 In) and thus, {3j :::::: (JZjlfo for 
</p>
<p>for j ~ k, where Zj rv N(O, 1). Therefore, 
</p>
<p>~ t (Jj::::::~ t ((Jr/jr 
i=n-k+l i=n-k+l fo 
</p>
<p>2 (J 
</p>
<p>k 
</p>
<p>since a sum of k Normals has a Xk distribution. Now IE(Xk) = k and hence 
</p>
<p>IE(0'2):::::: (J2. Also, V(xk) = 2k and hence V(0'2):::::: ((J4Ik2)(2k) = (2(J4Ik) -+ 0 
</p>
<p>as n -+ 00. Thus we expect 0'2 to be a consistent estimator of (J2. There is 
</p>
<p>nothing special about the choice k = n I 4. Any k that increases with n at an 
appropriate rate will suffice. 
</p>
<p>We estimate the risk with 
</p>
<p>~2 n ( -2) ~ (J -2 (J 
R(J) = J- + ~ fJ - - . 
</p>
<p>n ~ J n 
j=J+l + 
</p>
<p>(21.27) 
</p>
<p>21.10 Example. Figure 21.4 shows the doppler function f and n = 2,048 
observations generated from the model 
</p>
<p>Yi = r(xi) + Ei 
</p>
<p>where Xi = iln, fi rv N(O, (.1)2). The figure shows the data and the estimated 
</p>
<p>function. The estimate was based on J = 234 terms. _ 
</p>
<p>We are now ready to give a complete description of the method. 
</p>
<p>Orthogonal Series Regression Estimator 
</p>
<p>1. Let 
</p>
<p>2. Let 
</p>
<p>(21.28) </p>
<p/>
</div>
<div class="page"><p/>
<p>338 21. Smoothing Using Orthogonal Functions 
</p>
<p>FIGURE 21.4. Data from the doppler test function and the estimated function. See 
</p>
<p>Example 21.10. 
</p>
<p>where k RO n/4. 
</p>
<p>3. For 1 &lt;::: J &lt;::: n, compute the risk estimate 
</p>
<p>~2 n ( ~2) ~ U ~2 U 
R(J) = J - + '" fJ - - . n ~ J n 
</p>
<p>j=J+1 + 
</p>
<p>4. Choose j E {I, ... n} to minimize R(J). 
</p>
<p>5. Let 
J 
</p>
<p>T(x) = L ;3j&lt;pj(x). 
j=l 
</p>
<p>Finally, we turn to confidence bands. As before, these bands are not really 
</p>
<p>for the true function r(x) but rather for the smoothed version of the function 
</p>
<p>r J(x) = 2:.[=1 fJj&lt;pj(X). 
</p>
<p>21.11 Theorem. Suppose the estimate T is based on J terms and (j is defined 
</p>
<p>as in equation (21. 28}. Assume that J &lt; n - k + 1. An approximate 1 - a 
confidence band for r J is (R, u) where 
</p>
<p>R(x) = f;,(x) - c, u(x) = Tn(X) + C, (21.29) 
</p>
<p>where 
J 
</p>
<p>L &lt;p;(x), 
j=l 
</p>
<p>a(x) (j XJ,a 
C = fo ,a(x) = </p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Regression 339 
</p>
<p>and (j is given in equation (21.28). 
</p>
<p>PROOF. Let L = "Lf=l (/3j - pj)2. By the central limit theorem, Pj ~ 
</p>
<p>N(pj,CJ2In). Hence, /3j ~ Pj +CJEjIVii where Ej rv N(O, 1) and therefore 
</p>
<p>2 J 2 
CJ2:2dCJ 2 L ~ - E&middot; = -XJ. 
n J n 
</p>
<p>j=l 
</p>
<p>Thus, 
</p>
<p>ITD ( L &gt; : X},a) = ITD ( :2 X) &gt; :2 X},a) = Q. 
Also, 
</p>
<p>J 
If(x) - r J(X) I &lt; 2: l&cent;j(x)ll/3j - pjl 
</p>
<p>j=l 
</p>
<p>J J 
&lt; 2:&cent;;(x) 2:(/3j - p;) 
</p>
<p>j=l j=l 
</p>
<p>&lt; a(x) VI 
</p>
<p>by the Cauchy-Schwartz inequality (Theorem 4.8). So, 
</p>
<p>Jr max &gt;--lTll ( IJJ(x) -f(x)1 aXJ,a) 
x a(x) Vii 
</p>
<p>and the result follows. _ 
</p>
<p>21.12 Example. Figure 21.5 shows the confidence envelope for the doppler 
</p>
<p>signal. The first plot is based on J = 234 (the value of J that minimizes the 
</p>
<p>estimated risk). The second is based on J = 45 ~ Vii. Larger J yields a higher 
</p>
<p>resolution estimator at the cost of large confidence bands. Smaller J yields a 
</p>
<p>lower resolution estimator but has tighter confidence bands. _ 
</p>
<p>So far, we have assumed that the xi's are of the form {lin, 2/n, .. . ,1}. 
</p>
<p>If the xi's are on interval [a, b], then we can rescale them so that are in the 
</p>
<p>interval [0,1]. If the xi's are not equally spaced, the methods we have discussed 
</p>
<p>still apply so long as the Xi'S "fill out" the interval [0,1] in such a way so as to 
</p>
<p>not be too clumped together. If we want to treat the xi's as random instead 
</p>
<p>of fixed, then the method needs significant modifications which we shall not 
</p>
<p>deal with here. </p>
<p/>
</div>
<div class="page"><p/>
<p>340 21. Smoothing Using Orthogonal Functions 
</p>
<p>FIGURE 21.5. Estimates and confidence bands for the doppler test function using 
</p>
<p>n = 2,048 observations. First plot: J = 234 terms. Second plot: J = 45 terms. 
</p>
<p>21.4 Wavelets 
</p>
<p>Suppose there is a sharp jump in a regression function f at some point x 
but that f is otherwise very smooth. Such a function f is said to be spa-
tially inhomogeneous. The doppler function is an example of a spatially 
</p>
<p>inhomogeneous function; it is smooth for large x and unsmooth for small x. 
</p>
<p>It is hard to estimate f using the methods we have discussed so far. If we 
use a cosine basis and only keep low order terms, we will miss the peak; if 
</p>
<p>we allow higher order terms we will find the peak but we will make the rest 
</p>
<p>of the curve very wiggly. Similar comments apply to kernel regression. If we 
</p>
<p>use a large bandwidth, then we will smooth out the peak; if we use a small 
</p>
<p>bandwidth, then we will find the peak but we will make the rest of the curve 
</p>
<p>very wiggly. 
</p>
<p>One way to estimate inhomogeneous functions is to use a more carefully 
</p>
<p>chosen basis that allows us to place a "blip" in some small region without 
</p>
<p>adding wiggles elsewhere. In this section, we describe a special class of bases 
</p>
<p>called wavelets, that are aimed at fixing this problem. Statistical inference 
</p>
<p>using wavelets is a large and active area. We will just discuss a few of the 
</p>
<p>main ideas to get a flavor of this approach. 
</p>
<p>We start with a particular wavelet called the Haar wavelet. The Haar 
</p>
<p>father wavelet or Haar scaling function is defined by 
</p>
<p>&cent;(x) = { ~ if 0 ~ x &lt; 1 
otherwise. 
</p>
<p>(21.30) </p>
<p/>
</div>
<div class="page"><p/>
<p>The mother Haar wavelet is defined by 
</p>
<p>1jJ(x) = { 
</p>
<p>For any integers j and k define 
</p>
<p>-1 
</p>
<p>1 
</p>
<p>if 0 &lt; x &lt; 1, 
- - 2-
</p>
<p>if~&lt;x::;1. 
</p>
<p>21.4 Wavelets 341 
</p>
<p>(21.31 ) 
</p>
<p>(21.32) 
</p>
<p>The function 1jJj,k has the same shape as '1jJ but it has been rescaled by a factor 
</p>
<p>of 2j / 2 and shifted by a factor of k. 
</p>
<p>See Figure 21.6 for some examples of Haar wavelets. Notice that for large 
</p>
<p>j, 1jJj,k is a very localized function. This makes it possible to add a blip to a 
</p>
<p>function in one place without adding wiggles elsewhere. Increasing j is like 
</p>
<p>looking in a microscope at increasing degrees of resolution. In technical terms, 
</p>
<p>we say that wavelets provide a multiresolution analysis of L 2 (0, 1). 
</p>
<p>2 2 
</p>
<p>1 1 
</p>
<p>o o 
</p>
<p>-1 -1 
</p>
<p>-2 -2 
</p>
<p>FIGURE 21.6. Some Ham wavelets. Left: the mother wavelet 'lj;(x); Right: 'lj;2,2(X). 
</p>
<p>Let 
</p>
<p>Wj = {1jJjk' k = 0,1, ... ,2j -I} 
</p>
<p>be the set of rescaled and shifted mother wavelets at resolution j. 
</p>
<p>21.13 Theorem. The set of functions 
</p>
<p>is an orthonormal basis for L 2 (0, 1). </p>
<p/>
</div>
<div class="page"><p/>
<p>342 21. Smoothing Using Orthogonal Functions 
</p>
<p>It follows from this theorem that we can expand any function j E L 2 (0, 1) in 
</p>
<p>this basis. Because each Wj is itself a set of functions, we write the expansion 
</p>
<p>as a double sum: 
</p>
<p>00 2:j-1 
</p>
<p>j(x) = a&cent;(x) + L L (3j,k1/Jj,k(X) (21.33) 
j=O k=O 
</p>
<p>where 
</p>
<p>a = 11 j(x)&cent;(x) dx, (3j,k = 11 j(X)1/Jj,k(X) dx. 
</p>
<p>We call a the scaling coefficient and the (3j,k'S are called the detail 
</p>
<p>coefficients. We call the finite sum 
</p>
<p>J-121-1 
</p>
<p>!J(x) = a&cent;(x) + L L (3j,k1/Jj,k(X) (21.34) 
j=O k=O 
</p>
<p>the resolution J approximation to j. The total number of terms in this sum 
</p>
<p>is 
J-l 
</p>
<p>1 + L 2 j = 1 + 2J - 1 = 2J. 
j=O 
</p>
<p>21.14 Example. Figure 21. 7 shows the doppler signal, and its reconstruction 
</p>
<p>using J = 3,5 and J = 8 .&bull; 
</p>
<p>Ham wavelets are localized, meaning that they are zero outside an interval. 
</p>
<p>But they are not smooth. This raises the question of whether there exist 
</p>
<p>smooth, localized wavelets that from an orthonormal basis. In 1988, Ingrid 
</p>
<p>Daubechie showed that such wavelets do exist. These smooth wavelets are 
</p>
<p>difficult to describe. They can be constructed numerically but there is no 
</p>
<p>closed form formula for the smoother wavelets. To keep things simple, we will 
</p>
<p>continue to use Haar wavelets. 
</p>
<p>Consider the regression model Yi = r-(Xi) + (Hi where Ei rv N(O,l) and 
Xi = 'i/n. To simplify the discussion we assume that n = 2J for some J. 
</p>
<p>There is one major difference between estimation using wavelets instead of 
</p>
<p>a cosine (or polynomial) basis. With the cosine basis, we used all the terms 
</p>
<p>1 &lt;::: j &lt;::: J for some J. The number of terms J acted as a smoothing parameter. 
</p>
<p>With wavelets, we control smoothing using a method called thresholding 
</p>
<p>where we keep a term in the function approximation if its coefficient is large, </p>
<p/>
</div>
<div class="page"><p/>
<p>2L4 Wavelets 343 
</p>
<p>C&gt; 
</p>
<p>" 
C&gt; C&gt; 
</p>
<p>g 
C&gt; 
</p>
<p>~' "' 
</p>
<p>"' "' 
</p>
<p>FIGURE 2L7. The doppler signal and its reconstruction 
</p>
<p>fJ(x) = a&cent;(x) + L:~~ Lk !3j.k7j;j.k(X) based on J = 3, J = 5, and J = 8. 
</p>
<p>otherwise, we throw out that term. There are many versions of thresholding, 
</p>
<p>The simplest is called hard, universal thresholding. Let J = log2(n) and define 
</p>
<p>and (21.35) 
</p>
<p>for 0 ~ j ~ J - 1. 
</p>
<p>Haar Wavelet Regression 
</p>
<p>1. Compute a and Dj,k as in (21.35), for 0 ~ j ~ J - 1. 
</p>
<p>2. Estimate cr; see (21.37). 
</p>
<p>3. Apply universal thresholding: 
</p>
<p>~ {D (3j,k = 0 j,k if IDj,kl &gt; (jJ21~gn } 
otherwise, 
</p>
<p>(21.36) </p>
<p/>
</div>
<div class="page"><p/>
<p>344 21. Smoothing Using Orthogonal Functions 
</p>
<p>In practice, we do not compute Sk and Dj,k using (21.35). Instead, we use 
</p>
<p>the discrete wavelet transform (DWT) which is very fast. The DWT for 
</p>
<p>Haar wavelets is described in the appendix. The estimate of (J" is 
</p>
<p>_ c median (IDJ-1,kl: k = 0, ... , 2J- 1 - 1) 
(J" = yn x . 
</p>
<p>0.6745 
(21.37) 
</p>
<p>The estimate for (J" may look strange. It is similar to the estimate we used 
</p>
<p>for the cosine basis but it is designed to be insensitive to sharp peaks in the 
</p>
<p>function. 
</p>
<p>To understand the intuition behind universal thresholding, consider what 
</p>
<p>happens when there is no signal, that is, when (3j,k = 0 for all j and k. 
</p>
<p>21.15 Theorem. Suppose that (3j,k = 0 for all j and k and let (3j,k be the 
</p>
<p>universal threshold estimator. Then 
</p>
<p>1P'(,6j,k = 0 for all j, k) --+ 1 
</p>
<p>as n --+ 00. 
</p>
<p>PROOF. To simplify the proof, assume that (J" is known. Now Dj,k :::::; 
</p>
<p>N(O, (J"2 In). We will need Mill's inequality (Theorem 4.7): if Z ~ N(O,l) 
</p>
<p>then IP'(IZI &gt; t) -s: (clt)e- t2j2 where c = J2/,rr is a constant. Thus, 
</p>
<p>lP'(maxIDj,kl &gt; &gt;-) &lt; LIP'(IDj,kl &gt; &gt;-) = LIP' (Fnl~j'kl &gt; v;&gt;-) 
j,k j,k 
</p>
<p>&lt; "" C(J" {I n&gt;-2 } ~--exp ---&gt;-yIn 2 (J"2 
j,k 
</p>
<p>C 
</p>
<p>~--+O. 
y 2logn -
</p>
<p>21.16 Example. Consider Yi = r(xi) + (J"Ei where f is the doppler signal, 
(J" = .1 and n = 2,048. Figure 21.8 shows the data and the estimated function 
</p>
<p>using universal thresholding. Of course, the estimate is not smooth since Haar 
</p>
<p>wavelets are not smooth. Nonetheless, the estimate is quite accurate. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>21.5 Appendix 345 
</p>
<p>FIGURE 21.8. Estimate of the Doppler function using Ham wavelets and universal 
</p>
<p>thresholding. 
</p>
<p>21.5 Appendix 
</p>
<p>THE DWT FOR HAAR WAVELETS. Let y be the vector ofYi's (length n) and 
</p>
<p>let J = log2(n). Create a list D with elements 
</p>
<p>D[[O]J, ... , D[[J - Ill&middot; 
</p>
<p>Set: 
</p>
<p>temp +- y /,;n. 
</p>
<p>Then do: 
</p>
<p>fOT(j zn (J-1):O){ 
</p>
<p>rn +- 2j 
</p>
<p>I +- (1 : m) 
</p>
<p>D[[jll +- ~templ2 &lt;11 - templ(h I) - 1] ~ / v'2 
temp +- temp[2 * I] + temp[(2 * 1) - 1] /h 
</p>
<p>} </p>
<p/>
</div>
<div class="page"><p/>
<p>346 21. Smoothing Using Orthogonal Functions 
</p>
<p>21.6 Bibliographic Rernarks 
</p>
<p>Efromovich (1999) is a reference for orthogonal function methods. See also 
</p>
<p>Beran (2000) and Beran and Dumbgen (1998). An introduction to wavelets is 
</p>
<p>given in Ogden (1997). A more advanced treatment can be found in HardIe 
</p>
<p>et al. (1998). The theory of statistical estimation using wavelets has been 
</p>
<p>developed by many authors, especially David Donoho and Ian Johnstone. See 
</p>
<p>Donoho and Johnstone (1994), Donoho and Johnstone (1995), Donoho et al. 
</p>
<p>(1995), and Donoho and Johnstone (1998). 
</p>
<p>21.7 Exercises 
</p>
<p>1. Prove Theorem 21.5. 
</p>
<p>2. Prove Theorem 21.9. 
</p>
<p>3. Let 
</p>
<p>Show that these vectors have norm 1 and are orthogonal. 
</p>
<p>4. Prove Parseval's relation equation (21.6). 
</p>
<p>5. Plot the first five Legendre polynomials. Verify, numerically, that they 
</p>
<p>are orthonormal. 
</p>
<p>6. Expand the following functions in the cosine basis on [0, 1]. For (a) 
</p>
<p>and (b), find the coefficients (3j analytically. For (c) and (d), find the 
</p>
<p>coefficients {3j numerically, i.e. 
</p>
<p>r1 1 N 
(3j = In f(x)q;j(x) ~ N L f (~) q;j (~) 
</p>
<p>o r=l 
</p>
<p>for some large integer N. Then plot the partial sum "L-]=l (3jq;j(x) for 
increasing values of n. 
</p>
<p>(a) f(x) = V2cos(37fx). 
</p>
<p>(b) f(x) = sin(7fx). 
</p>
<p>(c) f(x) = "L-~~l hjK(x-tj ) where K(t) = (1+sign(t))/2, sign(x) = -1 
</p>
<p>if x &lt; 0, sign(x) = 0 if x = 0, sign(x) = 1 if x&gt; 0, </p>
<p/>
</div>
<div class="page"><p/>
<p>21.7 Exercises 347 
</p>
<p>(tj) = (.1, .13, .15, .23, .25, .40, .44, .65, .76, .78, .81), 
</p>
<p>(h j ) = (4, -5,3, -4,5, -4.2,2.1,4.3, -3.1,2.1, -4.2). 
</p>
<p>(d) f = jx(1 - x) sin Cx2+1~5)) . 
</p>
<p>7. Consider the glass fragments data from the book's website. Let Y be 
</p>
<p>refractive index and let X be aluminum content (the fourth variable). 
</p>
<p>(a) Do a non parametric regression to fit the model Y = f (x) + E using 
the cosine basis method. The data are not on a regular grid. Ignore this 
</p>
<p>when estimating the function. (But do sort the data first according to 
</p>
<p>x.) Provide a function estimate, an estimate of the risk, and a confidence 
</p>
<p>band. 
</p>
<p>(b) Use the wavelet method to estimate f. 
</p>
<p>8. Show that the Haar wavelets are orthonormal. 
</p>
<p>9. Consider again the doppler signal: 
</p>
<p>f(x) = jx(1 - x) sin ( 2.l7r ). 
x + 0.05 
</p>
<p>Let 17, = 1,024, (J" = 0.1, and let (Xl' ... ' X n ) = (1/17" ... ,1). Generate 
</p>
<p>data 
</p>
<p>where Ei rv N(O, 1). 
</p>
<p>(a) Fit the curve using the cosine basis method. Plot the function esti-
</p>
<p>mate and confidence band for J = 10,20, ... ,100. 
</p>
<p>(b) Use Haar wavelets to fit the curve. 
</p>
<p>10. (Haar density Estimation.) Let Xl, ... , Xn rv f for some density f on 
[0,1]. Let's consider constructing a wavelet histogram. Let &cent; and 1/J be 
</p>
<p>the Haar father and mother wavelet. Write 
</p>
<p>J-12:1-1 
</p>
<p>f(x) ~ &cent;(x) + L L pj,k1/Jj,k(X) 
j=O k=O 
</p>
<p>where J ~ log2(n). Let 
</p>
<p>_ 1 n 
</p>
<p>P k = - "" 1/J k(Xd&middot; ), n~)' 
i=l </p>
<p/>
</div>
<div class="page"><p/>
<p>348 21. Smoothing Using Orthogonal Functions 
</p>
<p>(a) Show that (ij,k is an unbiased estimate of pj,k. 
</p>
<p>(b) Define the Haar histogram 
</p>
<p>B 2j-l 
</p>
<p>J(x) = &cent;(x) + L L (ij,ktPj,k(X) 
j=O k=O 
</p>
<p>for 0 ::.; B ::.; J - 1. 
</p>
<p>(c) Find an approximate expression for the MSE as a function of B. 
</p>
<p>(d) Generate n = 1,000 observations from a Beta (15,4) density. Es-
</p>
<p>timate the density using the Haar histogram. Use leave-one-out cross 
</p>
<p>validation to choose B. 
</p>
<p>11. In this question, we will explore the motivation for equation (21.37). Let 
</p>
<p>Xl, .. . , Xn rv N(O, 0"2). Let 
</p>
<p>Ci = vIri x median (IXll,&middot;&middot;&middot;, IXnl) . 
0.6745 
</p>
<p>( a) Show that lE(Ci) = 0". 
</p>
<p>(b) Simulate n = 100 observations from a N(O,l) distribution. Compute 
</p>
<p>Ci as well as the usual estimate of 0". Repeat 1,000 times and compare 
</p>
<p>the MSE. 
</p>
<p>(c) Repeat (b) but add some outliers to the data. To do this, simulate 
</p>
<p>each observation from a N(O,l) with probability .95 and simulate each 
</p>
<p>observation from a N(0,10) with probability .95. 
</p>
<p>12. Repeat question 6 using the Haar basis. </p>
<p/>
</div>
<div class="page"><p/>
<p>22 
</p>
<p>Classification 
</p>
<p>22.1 Introduction 
</p>
<p>The problem of predicting a discrete random variable Y from another random 
</p>
<p>variable X is called classification, supervised learning, discrimination, 
</p>
<p>or pattern recognition. 
</p>
<p>Consider lID data (Xl, Yd, ... ,(Xn , Yn ) where 
</p>
<p>Xi = (XiI, ... , Xid) EX c ]Rd 
</p>
<p>is a d-dimensional vector and Yi takes values in some finite set y. A classifi-
cation rule is a function h : X --+ y. When we observe a new X, we predict 
</p>
<p>Y to be h(X). 
</p>
<p>22.1 Example. Here is a an example with fake data. Figure 22.1 shows 100 
</p>
<p>data points. The covariate X = (Xl, X 2 ) is 2-dimensional and the outcome 
</p>
<p>Y E Y = {O, I}. The Y values are indicated on the plot with the triangles 
</p>
<p>representing Y = 1 and the squares representing Y = O. Also shown is a linear 
</p>
<p>classification rule represented by the solid line. This is a rule of the form 
</p>
<p>h(x) = { ~ if a + blXI + b2X2 &gt; 0 
otherwise. 
</p>
<p>Everything above the line is classified as a 0 and everything below the line is 
</p>
<p>classified as a 1. &bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>350 22. Classification 
</p>
<p>...... ...... 
l:,. ................... 
</p>
<p>......... 
D 
</p>
<p>D D 
</p>
<p>.................. 0 0 
</p>
<p>...... ...... ...... 
</p>
<p>D 
</p>
<p>FIGURE 22.1. Two covariates and a linear decision boundary. D means Y = 1. 
D means Y = O. These two groups are perfectly separated by the linear decision 
</p>
<p>boundary; you probably won't see real data like this. 
</p>
<p>22.2 Example. Recall the the Coronary Risk-Factor Study (CORIS) data 
</p>
<p>from Example 13.17. There are 462 males between the ages of 15 and 64 from 
</p>
<p>three rural areas in South Africa. The outcome Y is the presence (Y = 1) or 
</p>
<p>absence (Y = 0) of coronary heart disease and there are 9 covariates: systolic 
</p>
<p>blood pressure, cumulative tobacco (kg), ldl (low density lipoprotein choles-
</p>
<p>terol), adiposity, famhist (family history of heart disease), typea (type-A be-
</p>
<p>havior), obesity, alcohol (current alcohol consumption), and age. I computed 
</p>
<p>a linear decision boundary using the LDA method based on two of the co-
</p>
<p>variates, systolic blood pressure and tobacco consumption. The LDA method 
</p>
<p>will be explained shortly. In this example, the groups are hard to tell apart. 
</p>
<p>In fact, 141 of the 462 subjects are misclassified using this classification rule . 
</p>
<p>&bull; 
</p>
<p>At this point, it is worth revisiting the Statistics/Data Mining dictionary: 
</p>
<p>Statistics 
</p>
<p>classification 
</p>
<p>data 
</p>
<p>covariates 
</p>
<p>classifier 
</p>
<p>estimation 
</p>
<p>Computer Science 
</p>
<p>supervised learning 
</p>
<p>training sample 
</p>
<p>features 
</p>
<p>hypothesis 
</p>
<p>learning 
</p>
<p>Meaning 
</p>
<p>predicting a discrete Y from X 
</p>
<p>(Xl, Yd,&middot;&middot;&middot;, (Xn' Yn ) 
the Xi's 
map h: X --+ y 
finding a good classifier 
</p>
<p>22.2 Error Rates and the Bayes Classifier 
</p>
<p>Our goal is to find a classification rule h that makes accurate predictions. We 
</p>
<p>start with the following definitions: </p>
<p/>
</div>
<div class="page"><p/>
<p>22.2 Error Rates and the Bayes Classifier 351 
</p>
<p>22.3 Definition. The true error ratelof a classifier h is 
</p>
<p>L(h) = J1D( {h(X) cJ Y}) 
</p>
<p>and the empirical error rate or training error rate is 
</p>
<p>_ 1 n 
</p>
<p>Ln(h) = - L I(h(Xd cJ Yi). 
n 
</p>
<p>i=l 
</p>
<p>First we consider the special case where Y = {O, I}. Let 
</p>
<p>r(x) = lE(YIX = x) = J1D(Y = 11X = x) 
</p>
<p>denote the regression function. From Bayes' theorem we have that 
</p>
<p>r(x) 
</p>
<p>where 
</p>
<p>J1D(Y = 11X = x) 
</p>
<p>f(xlY = l)JID(Y = 1) 
</p>
<p>f(xlY = l)JID(Y = 1) + f(xlY = O)JID(Y = 0) 
1fh(x) 
</p>
<p>1fh(x) + (1 -1f)fo(x) 
</p>
<p>fo(x) f(xlY = 0) 
</p>
<p>h(x) f(xlY = 1) 
</p>
<p>1f J1D(Y = 1). 
</p>
<p>22.4 Definition. The Bayes classification rule h* is 
</p>
<p>h*(x) = {I if r(x).&gt; ! 
o otherwIse. 
</p>
<p>(22.1) 
</p>
<p>(22.2) 
</p>
<p>(22.3) 
</p>
<p>(22.4) 
</p>
<p>The set D(h) = {x: J1D(Y = 11X = x) = J1D(Y = 0IX = x)} is called the 
</p>
<p>decision boundary. 
</p>
<p>Warning! The Bayes rule has nothing to do with Bayesian inference. We 
</p>
<p>could estimate the Bayes rule using either frequentist or Bayesian methods. 
</p>
<p>The Bayes rule may be written in several equivalent forms: 
</p>
<p>lOne can use other loss functions. For simplicity we will use the error rate as our loss function. </p>
<p/>
</div>
<div class="page"><p/>
<p>352 22. Classification 
</p>
<p>and 
</p>
<p>h*(x) = {I if W(Y .= ll X = x) &gt; W(Y = 0IX = x) 
o otherWIse 
</p>
<p>h*(x) = {I if 1fh(~) &gt; (1 - 1f)fo(x) 
o otherwIse. 
</p>
<p>(22.5) 
</p>
<p>(22.6) 
</p>
<p>22.5 Theorem. The Bayes rule is optimal, that is, if h is any other classifi-
</p>
<p>cation rule then L(h*) ~ L(h). 
</p>
<p>The Bayes rule depends on unknown quantities so we need to use the data 
</p>
<p>to find some approximation to the Bayes rule. At the risk of oversimplifying, 
</p>
<p>there are three main approaches: 
</p>
<p>1. Empirical Risk Minimization. Choose a set of classifiers H and find h E H 
</p>
<p>that minimizes some estimate of L(h). 
</p>
<p>2. Regression. Find an estimate r of the regression function r and define 
</p>
<p>h ( ) _ {I if r( x) &gt; ~ 
. x - 0 otherwise. 
</p>
<p>3. Density Estimation. Estimate fa from the X/s for which Yi = 0, estimate 
h from the Xi'S for which Yi = 1 and let 7r = n-1 L~=l Yi. Define 
</p>
<p>~() ~( I) 7rh(x) rx =WY=IX=x = ~ ~ 
7rh(x) + (1-7r)fo(x) 
</p>
<p>and 
</p>
<p>h(x) = {I if r(x) .&gt; ~ 
o otherwIse. 
</p>
<p>Now let us generalize to the case where Y takes on more than two values 
</p>
<p>as follows. 
</p>
<p>22.6 Theorem. Suppose that Y E Y = {I, ... , K}. The optimal rule is 
</p>
<p>where 
</p>
<p>h(x) argmaxkW(Y = klX = x) 
</p>
<p>argmaxk1fk fk(X) 
</p>
<p>W(Y = klX = x) = fdx) 1fk &bull; 
LT fT(X)1fT 
</p>
<p>(22.7) 
</p>
<p>(22.8) 
</p>
<p>(22.9) 
</p>
<p>1fr = P(Y = r), fT(X) = f(xlY = r) and argmaxk means lithe value of k 
</p>
<p>that maximizes that expression. " </p>
<p/>
</div>
<div class="page"><p/>
<p>22.3 Gaussian and Linear Classifiers 353 
</p>
<p>22.3 Gaussian and Linear Classifiers 
</p>
<p>Perhaps the simplest approach to classification is to use the density estima-
</p>
<p>tion strategy and assume a parametric model for the densities. Suppose that 
</p>
<p>y = {O, I} and that fo(x) = f(xlY = 0) and h(x) = f(xlY = 1) are both 
multivariate Gaussians: 
</p>
<p>fk(X) = (27T)d/;I~kll/2 exp { -~(x - J.Lkf~kl(X - J.Lk)} , k = 0, l. 
</p>
<p>Thus, XIY = 0 rv N(J.Lo, ~o) and XIY = 1 rv N(J.Ll' ~I). 
</p>
<p>22.7 Theorem. If XIY = 0 rv N(/LO, ~o) and XIY = 1 rv N(J.Ll' ~I), then the 
Bayes rule is 
</p>
<p>h*(x) = 1 0 7ro b 1L;,1 {
I if r2 &lt; r2 + 2 log (7rl) + 10O" (~) 
o otherwise 
</p>
<p>(22.10) 
</p>
<p>where 
</p>
<p>(22.11) 
</p>
<p>is the Manalahobis distance. An equivalent way of expressing the Bayes' 
</p>
<p>rule is 
</p>
<p>where 
</p>
<p>(22.12) 
</p>
<p>and IAI denotes the determinant of a matrix A. 
</p>
<p>The decision boundary of the above classifier is quadratic so this procedure 
</p>
<p>is called quadratic discriminant analysis (QDA). In practice, we use 
</p>
<p>sample estimates of 7T, J.Ll, J.L2, ~o, ~l in place of the true value, namely: 
</p>
<p>1 n 1 n 
</p>
<p>- 2:)1- Ii), 1Tl = - LIi 
n n 
</p>
<p>i=l i=l 
</p>
<p>J.Lo 
</p>
<p>So </p>
<p/>
</div>
<div class="page"><p/>
<p>354 22. Classification 
</p>
<p>A simplification occurs if we assume that ~o = ~o = ~. In that case, the 
</p>
<p>Bayes rule is 
</p>
<p>where now 
1 
</p>
<p>5k(x) = xT~-I/Lk - 2/Lr~-1 + 10g7fk. 
</p>
<p>The parameters are estimated as before, except that the MLE of ~ is 
</p>
<p>S = noSo + nISI. 
no + nl 
</p>
<p>The classification rule is 
</p>
<p>where 
</p>
<p>h*(x) = { ~ if 51 (x) &gt; 50 (x) 
otherwise 
</p>
<p>s: () TS-l~ 1 ~TS-l~ 1 ~ 
UjX =X ILj-2/Lj {Lj+og7fj 
</p>
<p>(22.13) 
</p>
<p>(22.14) 
</p>
<p>(22.15) 
</p>
<p>is called the discriminant function. The decision boundary {x: 50 (x) = 
</p>
<p>51 (x)} is linear so this method is called linear discrimination analysis 
</p>
<p>(LDA). 
</p>
<p>22.8 Example. Let us return to the South African heart disease data. The 
</p>
<p>decision rule in in Example 22.2 was obtained by linear discrimination. The 
</p>
<p>outcome was 
</p>
<p>y=o 
y=l 
</p>
<p>classified as 0 
</p>
<p>277 
</p>
<p>116 
</p>
<p>classified as 1 
</p>
<p>25 
</p>
<p>44 
</p>
<p>The observed misclassification rate is 141/462 = .31. Including all the covari-
</p>
<p>ates reduces the error rate to .27. The results from quadratic discrimination 
</p>
<p>are 
</p>
<p>classified as 0 classified as 1 
</p>
<p>y = 0 272 
</p>
<p>y = 1 113 
30 
</p>
<p>47 
</p>
<p>which has about the same error rate 143/462 = .31. Including all the covariates 
</p>
<p>reduces the error rate to .26. In this example, there is little advantage to QDA 
</p>
<p>over LDA .&bull; 
</p>
<p>Now we generalize to the case where Y takes on more than two values. </p>
<p/>
</div>
<div class="page"><p/>
<p>22.3 Gaussian and Linear Classifiers 355 
</p>
<p>22.9 Theorem. Suppose that Y E {I, ... , K}. If fk(x) = f(xlY = k) is 
</p>
<p>Gaussian, the Bayes rule is 
</p>
<p>where 
</p>
<p>(22.16) 
</p>
<p>If the variances of the Gaussians are equal, then 
</p>
<p>(22.17) 
</p>
<p>We estimate 5dx) by by inserting estimates of /Lk, 2'.k and Irk. There is 
</p>
<p>another version of linear discriminant analysis due to Fisher. The idea is 
</p>
<p>to first reduce the dimension of covariates to one dimension by projecting 
</p>
<p>the data onto a line. Algebraically, this means replacing the covariate X = 
</p>
<p>(Xl, ... , Xd) with a linear combination U = wT X = L~=l WjXj . The goal is 
</p>
<p>to choose the vector w = (WI, ... ,Wd) that "best separates the data." Then 
</p>
<p>we perform classification with the one-dimensional covariate Z instead of X. 
</p>
<p>We need define what we mean by separation of the groups. We would like 
</p>
<p>the two groups to have means that are far apart relative to their spread. Let 
</p>
<p>ILj denote the mean of X for Yj and let 2'. be the variance matrix of X. Then 
</p>
<p>lE(UIY = j) = lE(wT XIY = j) = WTILj and V(U) = u72'.w. 2 Define the 
</p>
<p>separation by 
</p>
<p>J(w) 
(lE(UIY = 0) -lE(UIY = 1))2 
</p>
<p>wT2'.w 
</p>
<p>(w T /Lo - wT /Ld 2 
wT2'.w 
</p>
<p>WT(/LO - /Ll)(/LO - /Llfw 
</p>
<p>wT2'.w 
</p>
<p>We estimate J as follows. Let nj = L~=l I(Yi = j) be the number of obser-
</p>
<p>vations in group j, let Xj be the sample mean vector of the X's for group j, 
</p>
<p>and let Sj be the sample covariance matrix in group j. Define 
</p>
<p>(22.18) 
</p>
<p>2The quantity J arises in physics, where it is called the Rayleigh coefficient. </p>
<p/>
</div>
<div class="page"><p/>
<p>356 22. Classification 
</p>
<p>where 
</p>
<p>Sw 
</p>
<p>22.10 Theorem. The vector 
</p>
<p>- - - - T 
(XO - Xd(Xo - Xl) 
</p>
<p>(no - l)So + (n1 - l)Sl 
(no - 1) + (n1 - 1) 
</p>
<p>-1- -
W = Sw (Xo - Xd 
</p>
<p>is a minimizer of J( w). We call 
</p>
<p>T - - T -1 
U = W X = (Xo - Xd Sw X 
</p>
<p>(22.19) 
</p>
<p>(22.20) 
</p>
<p>the Fisher linear discriminant function. The midpoint m between X 0 and 
</p>
<p>Xl is 
1 - - 1 - - T -1 - -
</p>
<p>m="2(Xo+Xd="2(Xo-Xd SE (XO+X1) 
</p>
<p>Fisher's classification rule is 
</p>
<p>h(x) = { ~ if w
T X ~ m 
</p>
<p>if wT X &lt; m. 
</p>
<p>(22.21 ) 
</p>
<p>Fisher's rule is the same as the Bayes linear classifier in equation (22.14) 
</p>
<p>when 1i' = 1/2. 
</p>
<p>22.4 Linear Regression and Logistic Regression 
</p>
<p>A more direct approach to classification is to estimate the regression function 
</p>
<p>r(x) = JE(YIX = x) without bothering to estimate the densities fk. For the 
</p>
<p>rest of this section, we will only consider the case where Y = {O, I}. Thus, 
</p>
<p>r(x) = JP'(Y = 11X = x) and once we have an estimate P, we will use the 
</p>
<p>classification rule 
</p>
<p>h (x) = {I if P( x) .&gt; ! 
o otherwIse. 
</p>
<p>The simplest regression model is the linear regression model 
</p>
<p>d 
</p>
<p>Y = r(x) + e = {30 + L (3jXj + e 
j=l 
</p>
<p>(22.22) 
</p>
<p>(22.23) 
</p>
<p>where JE(e) = O. This model can't be correct since it does not force Y = 0 or 
</p>
<p>1. Nonetheless, it can sometimes lead to a decent classifier. </p>
<p/>
</div>
<div class="page"><p/>
<p>22.4 Linear Regression and Logistic Regression 357 
</p>
<p>Recall that the least squares estimate of 13 = (130,131, ... lf3d)T minimizes 
the residual sums of squares 
</p>
<p>Let X denote the N x (d + 1) matrix of the form 
</p>
<p>X= 
</p>
<p>RSS(f3) = (Y - xf3f (Y - Xf3) 
</p>
<p>and the model can be written as 
</p>
<p>where E = (Ell ... ' En)T. From Theorem 13.13, 
</p>
<p>The predicted values are 
</p>
<p>Y = Xf3. 
</p>
<p>Now we use (22.22) to classify, where r(x) = lio + L: j lijxj. 
An alternative is to use logistic regression which was also discussed in Chap-
</p>
<p>ter 13. The model is 
</p>
<p>(22.24) 
</p>
<p>and the MLE 13 is obtained numerically. 
</p>
<p>22.11 Example. Let us return to the heart disease data. The MLE is given in 
</p>
<p>Example 13.17. The error rate, using this model for classification, is .27. The 
</p>
<p>error rate from a linear regression is .26. 
</p>
<p>We can get a better classifier by fitting a richer model. For example, we 
</p>
<p>could fit 
</p>
<p>logit JP'(Y = 11X = x) = 130 + L f3jXj + L f3jk Xj Xk. (22.25) 
j j,k </p>
<p/>
</div>
<div class="page"><p/>
<p>358 22. Classification 
</p>
<p>More generally, we could add terms of up to order T for some integer T. Large 
</p>
<p>values of T give a more complicated model which should fit the data better. 
</p>
<p>But there is a bias-variance tradeoff which we'll discuss later. 
</p>
<p>22.12 Example. If we use model (22.25) for the heart disease data with T = 2, 
</p>
<p>the error rate is reduced to .22 .&bull; 
</p>
<p>22.5 Relationship Between Logistic Regression and 
</p>
<p>LDA 
</p>
<p>LDA and logistic regression are almost the same thing. If we assume that each 
</p>
<p>group is Gaussian with the same covariance matrix, then we saw earlier that 
</p>
<p>(
IP'(Y = llX = x)) 
</p>
<p>loO" 
b IP'(Y = 0IX = x) 
</p>
<p>log ( 7To) - ~(/LO + /Llf2:,-l(/Ll -/Lo) 
7T1 2 
</p>
<p>+ xT 2:,-1 (/L1 - /Lo) 
</p>
<p>ao + aTx. 
</p>
<p>On the other hand, the logistic model is, by assumption, 
</p>
<p>(
IP'(Y = llX = x)) T 
</p>
<p>log IP'(Y = 0IX = x) = /30 + /3 x. 
</p>
<p>These are the same model since they both lead to classification rules that are 
</p>
<p>linear in x. The difference is in how we estimate the parameters. 
</p>
<p>The joint density of a single observation is f(x, y) = f(xly)f(y) = f(ylx)f(x). 
</p>
<p>In LDA we estimated the whole joint distribution by maximizing the likeli-
</p>
<p>hood 
</p>
<p>(22.26) 
</p>
<p>i i 
'"-v----'''-v--' 
</p>
<p>Gaussian Bernoulli 
</p>
<p>In logistic regression we maximized the conditional likelihood ITi f(Yi IXi) but 
we ignored the second term f(Xi): 
</p>
<p>(22.27) 
</p>
<p>logistic ignored 
</p>
<p>Since classification only requires knowing f(ylx), we don't really need to es-
</p>
<p>timate the whole joint distribution. Logistic regression leaves the marginal </p>
<p/>
</div>
<div class="page"><p/>
<p>22.6 Density Estimation and Naive Bayes 359 
</p>
<p>distribution f(x) unspecified so it is more nonparametric than LDA. This is 
</p>
<p>an advantage of the logistic regression approach over LDA. 
</p>
<p>To summarize: LDA and logistic regression both lead to a linear classi-
</p>
<p>fication rule. In LDA we estimate the entire joint distribution f(x, y) = 
</p>
<p>f(xly)f(y). In logistic regression we only estimate f(ylx) and we don't bother 
</p>
<p>estimating f(x). 
</p>
<p>22.6 Density Estirnation and Naive Bayes 
</p>
<p>The Bayes rule is h( x) = argmaxk Irk fk (x). If we can estimate Irk and fk 
</p>
<p>then we can estimate the Bayes classification rule. Estimating Irk is easy but 
</p>
<p>what about fk? We did this previously by assuming fk was Gaussian. An-
</p>
<p>other strategy is to estimate fk with some nonparametric density estimator 
</p>
<p>h such as a kernel estimator. But if x = (Xl, ... , Xd) is high-dimensional, 
non parametric density estimation is not very reliable. This problem is amelio-
</p>
<p>rated if we assume that Xl, ... ,Xd are independent, for then, ik (Xl, ... ,Xd) = 
</p>
<p>n;=l fkj (Xj). This reduces the problem to d one-dimensional density estima-
tion problems, within each of the k groups. The resulting classifier is called 
</p>
<p>the naive Bayes classifier. The assumption that the components of X are 
</p>
<p>independent is usually wrong yet the resulting classifier might still be accu-
</p>
<p>rate. Here is a summary of the steps in the naive Bayes classifier: 
</p>
<p>The Naive Bayes Classifier 
</p>
<p>1. For each group k, compute an estimate fkj of the density ikj for X j , 
</p>
<p>using the data for which Y; = k. 
</p>
<p>2. Let 
</p>
<p>3. Let 
</p>
<p>d 
</p>
<p>h(x) = h(Xl,"" Xd) = II hj(xj). 
j=l 
</p>
<p>where I(Y; = k) = 1 if Y; = k and I(Y; = k) = 0 if Y; cJ k. 
</p>
<p>4. Let </p>
<p/>
</div>
<div class="page"><p/>
<p>360 22. Classification 
</p>
<p>Age 
</p>
<p>Blood Pressure 1 
</p>
<p>o 1 
</p>
<p>FIGURE 22.2. A simple classification tree. 
</p>
<p>The naive Bayes classifier is popular when x is high-dimensional and dis-
</p>
<p>crete. In that case, hj (x j) is especially simple. 
</p>
<p>22.7 Trees 
</p>
<p>Trees are classification methods that partition the covariate space X into 
</p>
<p>disjoint pieces and then classify the observations according to which partition 
</p>
<p>element they fall in. As the name implies, the classifier can be represented as 
</p>
<p>a tree. 
</p>
<p>For illustration, suppose there are two covariates, Xl = age and X 2 = blood 
</p>
<p>pressure. Figure 22.2 shows a classification tree using these variables. 
</p>
<p>The tree is used in the following way. If a subject has Age::::' 50 then we 
</p>
<p>classify him as Y = 1. If a subject has Age &lt; 50 then we check his blood 
</p>
<p>pressure. If systolic blood pressure is &lt; 100 then we classify him as Y = 1, 
</p>
<p>otherwise we classify him as Y = O. Figure 22.3 shows the same classifier as 
</p>
<p>a partition of the covariate space. 
</p>
<p>Here is how a tree is constructed. First, suppose that y E Y = {O, I} and 
that there is only a single covariate X. We choose a split point t that divides 
</p>
<p>the real line into two sets Al = (-00, tj and A2 = (t, (0). Let Ps(j) be the 
</p>
<p>proportion of observations in As such that Yi = j: 
</p>
<p>~ ( .) L~=l I(Yi = j, Xi E As) 
Ps J = L~=l I(Xi E As) 
</p>
<p>for s = 1,2 and j = 0,1. The impurity of the split t is defined to be 
</p>
<p>2 
</p>
<p>I(t) = L /S 
8=1 
</p>
<p>(22.28) 
</p>
<p>(22.29) </p>
<p/>
</div>
<div class="page"><p/>
<p>22.7 Trees 361 
</p>
<p>1 
</p>
<p>(j) 
.... 
;:j llO 
ifl 
if] 
</p>
<p>1 (j) .... 
p..., 
</p>
<p>"d 0 
0 
0 
</p>
<p>P:i 
</p>
<p>50 
</p>
<p>Age 
</p>
<p>FIGURE 22.3. Partition representation of classification tree. 
</p>
<p>where 
1 
</p>
<p>IS = 1 - LPs(j)2. (22.30) 
j=O 
</p>
<p>This particular measure of impurity is known as the Gini index. If a partition 
</p>
<p>element As contains all O's or all l's, then IS = O. Otherwise, IS &gt; O. We 
</p>
<p>choose the split point t to minimize the impurity. (Other indices of impurity 
</p>
<p>besides can be used besides the Gini index.) 
</p>
<p>When there are several covariates, we choose whichever covariate and split 
</p>
<p>that leads to the lowest impurity. This process is continued until some stopping 
</p>
<p>criterion is met. For example, we might stop when every partition element has 
</p>
<p>fewer than no data points, where no is some fixed number. The bottom nodes 
</p>
<p>of the tree are called the leaves. Each leaf is assigned a 0 or 1 depending on 
</p>
<p>whether there are more data points with Y = 0 or Y = 1 in that partition 
</p>
<p>element. 
</p>
<p>This procedure is easily generalized to the case where Y E {I, ... ,K}. We 
</p>
<p>simply define the impurity by 
</p>
<p>k 
</p>
<p>IS = 1 - LPs(j)2 (22.31 ) 
j=l 
</p>
<p>where Pi(j) is the proportion of observations in the partition element for which 
</p>
<p>Y=j. </p>
<p/>
</div>
<div class="page"><p/>
<p>362 22. Classification 
</p>
<p>age 
</p>
<p>&lt; 31.5 231.5 
</p>
<p>I I 
tobacco age 
</p>
<p>I I I 
&lt; 0.51 20.51 &lt; 50.5 250.5 
</p>
<p>I I I I tobacco 
0 0 0 I I 
</p>
<p>&lt; 7.47 27.47 
</p>
<p>I I 
0 1 
</p>
<p>FIGURE 22.4. A classification tree for the heart disease data using two covariates. 
</p>
<p>22.13 Example. A classification tree for the heart disease data yields a mis-
</p>
<p>classification rate of .21. If we build a tree using only tobacco and age, the 
</p>
<p>misclassification rate is then .29. The tree is shown in Figure 22.4 .&bull; 
</p>
<p>Our description of how to build trees is incomplete. If we keep splitting 
</p>
<p>until there are few cases in each leaf of the tree, we are likely to over fit the 
</p>
<p>data. We should choose the complexity of the tree in such a way that the 
</p>
<p>estimated true error rate is low. In the next section, we discuss estimation of 
</p>
<p>the error rate. 
</p>
<p>22.8 Assessing Error Rates and Choosing a Good 
</p>
<p>Classifier 
</p>
<p>How do we choose a good classifier? We would like to have a classifier h with 
</p>
<p>a low true error rate L(h). Usually, we can't use the training error rate Ln(h) 
</p>
<p>as an estimate of the true error rate because it is biased downward. 
</p>
<p>22.14 Example. Consider the heart disease data again. Suppose we fit a se-
</p>
<p>quence of logistic regression models. In the first model we include one co-
</p>
<p>variate. In the second model we include two covariates, and so on. The ninth 
</p>
<p>model includes all the covariates. We can go even further. Let's also fit a tenth 
</p>
<p>model that includes all nine covariates plus the first covariate squared. Then </p>
<p/>
</div>
<div class="page"><p/>
<p>22.8 Assessing Error Rates and Choosing a Good Classifier 363 
</p>
<p>we fit an eleventh model that includes all nine covariates plus the first covari-
</p>
<p>ate squared and the second covariate squared. Continuing this way we will get 
</p>
<p>a sequence of 18 classifiers of increasing complexity. The solid line in Figure 
</p>
<p>22.5 shows the observed classification error which steadily decreases as we 
</p>
<p>make the model more complex. If we keep going, we can make a model with 
</p>
<p>zero observed classification error. The dotted line shows the lO-fold cross-
</p>
<p>validation estimate of the error rate (to be explained shortly) which is a 
</p>
<p>better estimate of the true error rate than the observed classification error. 
</p>
<p>The estimated error decreases for a while then increases. This is essentially 
</p>
<p>the bias-variance tradeoff phenomenon we have seen in Chapter 20 .&bull; 
</p>
<p>error rate 
</p>
<p>0.34 
</p>
<p>0.30 
</p>
<p>0.26 
</p>
<p>,-------, , , , , , , 
--,'\ ' -- \ , 
</p>
<p>\ , ....... -, ,. 
</p>
<p>5 number of terms in model 15 
</p>
<p>FIGURE 22.5. The solid line is the observed error rate and dashed line IS the 
</p>
<p>cross-validation estimate of true error rate. 
</p>
<p>There are many ways to estimate the error rate. We'll consider two: cross-
</p>
<p>validation and probability inequalities. 
</p>
<p>CROSS- VALIDATION. The basic idea of cross-validation, which we have al-
</p>
<p>ready encountered in curve estimation, is to leave out some of the data when 
</p>
<p>fitting a model. The simplest version of cross-validation involves randomly 
</p>
<p>splitting the data into two pieces: the training set T and the validation 
set V. Often, about 10 per cent ofthe data might be set aside as the validation 
</p>
<p>set. The classifier h is constructed from the training set. We then estimate </p>
<p/>
</div>
<div class="page"><p/>
<p>364 22. Classification 
</p>
<p>Training Data T Validation Data V 
</p>
<p>~'----------------~V~----------------~A,------~V~------~~ 
h L 
</p>
<p>FIGURE 22.6. Cross-validation. The data are divided into two groups: the training 
</p>
<p>data and J;he valid~tion data. The training data are used to produce an eSEmated 
</p>
<p>classifier h. 'l}len, h is applied to the validation data to obtain an estimate L of the 
</p>
<p>error rate of h. 
</p>
<p>the error by 
</p>
<p>~ 1", 
L(h) = - ~ I(h(Xi) i- Y1 ). 
</p>
<p>m 
XiEV 
</p>
<p>(22.32) 
</p>
<p>where m is the size of the validation set. See Figure 22.6. 
</p>
<p>Another approach to cross-validation is K-fold cross-validation which is 
</p>
<p>obtained from the following algorithm. 
</p>
<p>K-fold cross-validation. 
</p>
<p>1. Randomly divide the data into K chunks of approximately equal size. 
</p>
<p>A common choice is K = 10. 
</p>
<p>2. For k = 1 to K, do the following: 
</p>
<p>(a) Delete chunk k from the data. 
</p>
<p>(b) Compute the classifier hCk) from the rest of the data. 
~ ~ 
</p>
<p>(c) Use hCk) to the predict the data in chunk k. Let LCk) denote 
</p>
<p>the observed error rate. 
</p>
<p>3. Let 
</p>
<p>(22.33) 
</p>
<p>22.15 Example. We applied 10-fold cross-validation to the heart disease data. 
</p>
<p>The minimum cross-validation error as a function of the number of leaves 
</p>
<p>occurred at six. Figure 22.7 shows the tree with six leaves. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>22.8 Assessing Error Rates and Choosing a Good Classifier 365 
</p>
<p>I 
&lt; 31.5 
</p>
<p>I 
o 
</p>
<p>age 
</p>
<p>&lt; 50.5 
I 
</p>
<p>type A 
</p>
<p>I 1 
&lt; 68.5 ~ 68.5 
</p>
<p>I I 
o 1 
</p>
<p>~ 31.5 
</p>
<p>age 
</p>
<p>~ 50.5 
</p>
<p>I 
family history 
</p>
<p>I I 
no yes 
</p>
<p>I I tobacco r l 1 
&lt; 7.605 ~ 7.605 
</p>
<p>I I 
o 1 
</p>
<p>FIGURE 22.7. Smaller classification tree with size chosen by cross-validation. 
</p>
<p>PROBABILITY INEQUALITIES. Another approach to estimating the error rate 
</p>
<p>is to find a confidence interval for Ln (h) using probability inequalities. This 
</p>
<p>method is useful in the context of empirical risk minimization. 
</p>
<p>Let H be a set of classifiers, for example, all linear classifiers. Empirical risk 
</p>
<p>minimization means choosing the classifier h E H to minimize the training 
</p>
<p>error Ln(h), also called the empirical risk. Thus, 
</p>
<p>h = argminhE1iLn(h) = argminhE1i (~~I(h(Xi) -=l-li)) . (22.34) 
</p>
<p>Typically, Ln(h) underestimates the true error rate L(h) because h was chosen 
to make Ln (h) small. Our goal is to assess how much underestimation is taking 
</p>
<p>place. Our main tool for this analysis is Hoeffding's inequality (Theorem 
</p>
<p>4.5). Recall that if Xl"'" Xn rv Bernoulli(p), then, for any E &gt; 0, 
</p>
<p>h ' ~- -l,\,n X werep-n L...i=l i&middot; 
</p>
<p>(22.35) </p>
<p/>
</div>
<div class="page"><p/>
<p>366 22. Classification 
</p>
<p>First, suppose that 1-l = {hI! ... ' hm } consists of finitely many classifiers. 
</p>
<p>For any fixed h, Ln(h) converges in almost surely to L(h) by the law of large 
</p>
<p>numbers. We will now establish a stronger result. 
</p>
<p>22.16 Theorem (Uniform Convergence). Assume 1-l is finite and has m ele-
</p>
<p>ments. Then, 
</p>
<p>PROOF. We will use Hoeffding's inequality and we will also use the fact 
</p>
<p>that if AI! ... ' Am is a set of events then JID(U:l Ai) ~ 2..::1 JID(Ai). Now, 
</p>
<p>JID (max ILn(h) - L(h) I &gt; f) 
hOi 
</p>
<p>JID (U ILn(h) - L(h)1 &gt; f) 
hE1i 
</p>
<p>&lt; L JID (ILn(h) - L(h)1 &gt; f) 
HE1i 
</p>
<p>&lt; L 2e- 2nE2 = 2me- 2nE2 . &bull; 
HE1i 
</p>
<p>22.17 Theorem. Let 
</p>
<p>f= 2 (2m) ;: log -;- . 
Then Ln (Ft) &plusmn; f is a 1 - a confidence interval for L(h). 
</p>
<p>PROOF. This follows from the fact that 
</p>
<p>When 1-l is large the confidence interval for L(h) is large. The more functions 
there are in 1-l the more likely it is we have "overfit" which we compensate 
</p>
<p>for by having a larger confidence interval. 
</p>
<p>In practice we usually use sets 1-l that are infinite, such as the set of linear 
</p>
<p>classifiers. To extend our analysis to these cases we want to be able to say 
</p>
<p>something like 
</p>
<p>JID (sup ILn(h) - L(h)1 &gt; f) ~ something not too big. 
hOi 
</p>
<p>One way to develop such a generalization is by way of the Vapnik-Chervonenkis 
</p>
<p>or VC dimension. </p>
<p/>
</div>
<div class="page"><p/>
<p>22.8 Assessing Error Rates and Choosing a Good Classifier 367 
</p>
<p>Let A be a class of sets. Give a finite set F = {Xl, ... ,xn } let 
</p>
<p>N A (F) = # { F n A: A E A} (22.36) 
be the number of subsets of F "picked out" by .A. Here #(B) denotes the 
</p>
<p>number of elements of a set B. The shatter coefficient is defined by 
</p>
<p>(22.37) 
</p>
<p>where Fn consists of all finite sets of size n. Now let Xl"'" Xn ""' JPl and let 
</p>
<p>1 
JPln(A) = - L I(Xi E A) 
</p>
<p>n . 
2 
</p>
<p>denote the empirical probability measure. The following remarkable the-
</p>
<p>orem bounds the distance between JPl and JPl n' 
</p>
<p>22.18 Theorem (Vapnik and Chervonenkis (1971)). For any JPl, nand E &gt; 0, 
</p>
<p>JPl {sup IJPln(A) - JPl(A) I &gt; E} ::.; 8s(A, n)e- m2 /32. (22.38) 
AEA 
</p>
<p>The proof, though very elegant, is long and we omit it. If 1i is a set of 
</p>
<p>classifiers, define A to be the class of sets of the form {x: h(x) = I}. We 
</p>
<p>then define s(1i, n) = s(A, n). 
</p>
<p>22.19 Theorem. 
</p>
<p>JPl {sup ILn(h) - L(h)1 &gt; E} ::.; 8s(1i, n)e-nE2/32. 
hOi 
</p>
<p>A 1 - Q confidence interval for L(li) is Ln (li) &plusmn; En where 
</p>
<p>2_321 (8s(1i,n)) 
En - og . 
</p>
<p>n Q 
</p>
<p>These theorems are only useful if the shatter coefficients do not grow too 
</p>
<p>quickly with n. This is where VC dimension enters. 
</p>
<p>22.20 Definition. The VC (Vapnik-Chervonenkis) dimension of a class of 
</p>
<p>sets A is defined as follows. If s(A, n) = 2n for all n, set VC(A) = 00. 
</p>
<p>Otherwise, define VC(A) to be the largest k for which s(A,n) = 2k. 
</p>
<p>Thus, the VC-dimension is the size of the largest finite set F that can be 
</p>
<p>shattered by A meaning that A picks out each subset of F. If 1i is a set of 
</p>
<p>classifiers we define VC(1i) = VC(A) where A is the class of sets of the form 
</p>
<p>{x: h (x) = I} as h varies in 1i. The following theorem shows that if A has 
</p>
<p>finite VC-dimension, then the shatter coefficients grow as a polynomial in n. </p>
<p/>
</div>
<div class="page"><p/>
<p>368 22. Classification 
</p>
<p>22.21 Theorem. If A has finite VC-dimension v, then 
</p>
<p>s(A, n) -&lt;; nV + 1. 
</p>
<p>22.22 Example. Let A = {( -00, a]; a E R}. The A shatters every I-point 
set {x} but it shatters no set of the form {x, y}. Therefore, VC(A) = 1. &bull; 
</p>
<p>22.23 Example. Let A be the set of closed intervals on the real line. Then 
</p>
<p>A shatters S = {x, y} but it cannot shatter sets with 3 points. Consider 
</p>
<p>S = {x, y, z} where x &lt; y &lt; z. One cannot find an interval A such that 
</p>
<p>AnS = {x,z}. So, VC(A) = 2 .&bull; 
</p>
<p>22.24 Example. Let A be all linear half-spaces on the plane. Any 3-point 
</p>
<p>set (not all on a line) can be shattered. No 4 point set can be shattered. 
</p>
<p>Consider, for example, 4 points forming a diamond. Let T be the left and 
</p>
<p>rightmost points. This can't be picked out. Other configurations can also be 
</p>
<p>seen to be unshatterable. So VC(A) = 3. In general, halfspaces in Rd have 
</p>
<p>VC dimension d + 1 .&bull; 
</p>
<p>22.25 Example. Let A be all rectangles on the plane with sides parallel to 
</p>
<p>the axes. Any 4 point set can be shattered. Let S be a 5 point set. There is 
</p>
<p>one point that is not leftmost, rightmost, uppermost, or lowermost. Let T be 
</p>
<p>all points in S except this point. Then T can't be picked out. So VC(A) = 4 . 
</p>
<p>&bull; 
</p>
<p>22.26 Theorem. Let x have dimension d and let 1{ be th set of linear classi-
</p>
<p>fiers. The VC-dimension of 1{ is d + 1. Hence, a 1 - a confidence interval for 
the true error rate is L(i;,) &plusmn; E where 
</p>
<p>2 _ 32 (8(nd+ 1 + 1)) 
En - log . 
</p>
<p>n a 
</p>
<p>22.9 Support Vector Machines 
</p>
<p>In this section we consider a class of linear classifiers called support vector 
</p>
<p>machines. Throughout this section, we assume that Y is binary. It will be 
</p>
<p>convenient to label the outcomes as -1 and + 1 instead of 0 and 1. A linear 
classifier can then be written as 
</p>
<p>h(x) = sign ( H(X)) </p>
<p/>
</div>
<div class="page"><p/>
<p>22.9 Support Vector Machines 369 
</p>
<p>where x = (Xl' ... ' Xd), 
</p>
<p>and 
</p>
<p>d 
</p>
<p>H(x) = ao + L aiXi 
</p>
<p>eign(z) ~ { 
</p>
<p>i=l 
</p>
<p>-1 if z &lt; 0 
o ifz=O 
1 if z &gt; O. 
</p>
<p>First, suppose that the data are linearly separable, that is, there exists 
</p>
<p>a hyperplane that perfectly separates the two classes. 
</p>
<p>22.27 Lemma. The data can be separated by some hyperplane if and only if 
</p>
<p>there exists a hyperplane H(x) = ao + L~=l aixi such that 
</p>
<p>YiH(Xi) ~ l,i = 1, ... ,n. (22.39) 
</p>
<p>PROOF. Suppose the data can be separated by a hyperplane W(x) = bo + 
L~=l bixi. It follows that there exists some constant c such that Yi = 1 implies 
</p>
<p>W(X;) ~ c and Yi = -1 implies W(X;) ::.; -c. Therefore, YiW(Xi ) ~ c for 
</p>
<p>all i. Let H(x) = ao + L~=l aixi where aj = bj/c. Then YiH(Xi) ~ 1 for all 
</p>
<p>i. The reverse direction is straightforward .&bull; 
</p>
<p>In the separable case, there will be many separating hyperplanes. How 
</p>
<p>should we choose one? Intuitively, it seems reasonable to choose the hyper-
</p>
<p>plane "furthest" from the data in the sense that it separates the +ls and -Is 
</p>
<p>and maximizes the distance to the closest point. This hyperplane is called the 
</p>
<p>maximum margin hyperplane. The margin is the distance to from the 
</p>
<p>hyperplane to the nearest point. Points on the boundary of the margin are 
</p>
<p>called support vectors. See Figure 22.8. 
</p>
<p>22.28 Theorem. The hyperplane H(x) = ao + L~=1 aiXi that separates the 
data and maximizes the margin is given by minimizing (1/2) L~=1 a; subject 
</p>
<p>to (22.39). 
</p>
<p>It turns out that this problem can be recast as a quadratic programming 
</p>
<p>problem. Let (Xi, X k ) = xT X k denote the inner product of Xi and X k . 
</p>
<p>22.29 Theorem. Let H(x) = ao + L~=1 aiXi denote the optimal (largest mar-
gin) hyperplane. Then, for j = 1, ... , d, 
</p>
<p>n 
</p>
<p>aj = L aiYiXj(i) 
i=1 </p>
<p/>
</div>
<div class="page"><p/>
<p>370 22. Classification 
</p>
<p>. . . 
</p>
<p>&bull; 
</p>
<p>&middot; . 
</p>
<p>&bull; 
</p>
<p>&bull; &bull; 
</p>
<p>. . . 
</p>
<p>....... 
</p>
<p>&bull; 
</p>
<p>. . 
</p>
<p>. . . 
</p>
<p>&middot; . 
</p>
<p>&middot; . 
</p>
<p>&bull; 
</p>
<p>. . . 
</p>
<p>. . . . . . . . . . . . 
</p>
<p>'. 
</p>
<p>. . 
&bull; &bull; e. 
</p>
<p>o 
</p>
<p>o 
o 
</p>
<p>. ... ~ 
</p>
<p>.0. .&bull;...... 
</p>
<p>H(X)=ao+aTx=O~ 
</p>
<p>. . . 
</p>
<p>FIGURE 22.8. The hyperplane H(x) has the largest margin of all hyperplanes that 
separate the two classes. 
</p>
<p>where Xj (i) is the value of the covariate Xj for the ith data point, and a = 
(a1) ... ) an) is the vector that maximizes 
</p>
<p>(22.40) 
</p>
<p>subject to 
</p>
<p>and 
</p>
<p>The points Xi for which a i=- 0 are called support vectors. ao can be found 
by solving 
</p>
<p>for any support point Xi. H may be written as 
</p>
<p>n 
</p>
<p>H(x) = ao + L aiYi(X, Xi!. 
i=l 
</p>
<p>There are many software packages that will solve this problem quickly. If 
</p>
<p>there is no perfect linear classifier, then one allows overlap between the groups </p>
<p/>
</div>
<div class="page"><p/>
<p>22.10 Kernelization 371 
</p>
<p>by replacing the condition (22.39) with 
</p>
<p>(22.41 ) 
</p>
<p>The variables ~l' ... ' ~n are called slack variables. 
</p>
<p>We now maximize (22.40) subject to 
</p>
<p>o ~ ~i ~ C, 'i = 1, ... , n 
</p>
<p>and 
n 
</p>
<p>Lctili = O. 
i=l 
</p>
<p>The constant c is a tuning parameter that controls the amount of overlap. 
</p>
<p>22.10 Kernelization 
</p>
<p>There is a trick called kernelization for improving a computationally simple 
</p>
<p>classifier h. The idea is to map the covariate X - which takes values in X -
</p>
<p>into a higher dimensional space Z and apply the classifier in the bigger space 
</p>
<p>Z. This can yield a more flexible classifier while retaining computationally 
</p>
<p>simplicity. 
</p>
<p>The standard example of this idea is illustrated in Figure 22.9. The covariate 
</p>
<p>x = (Xl, X2). The lis can be separated into two groups using an ellipse. Define 
</p>
<p>a mapping &cent; by 
</p>
<p>Thus, &cent; maps X = ]R2 into Z = ]R3. In the higher-dimensional space Z, the 
</p>
<p>li's are separable by a linear decision boundary. In other words, 
</p>
<p>a linear classifier in a higher-dimensional space corresponds to a non-
</p>
<p>linear classifier in the original space. 
</p>
<p>The point is that to get a richer set of classifiers we do not need to give up the 
</p>
<p>convenience of linear classifiers. We simply map the covariates to a higher-
</p>
<p>dimensional space. This is akin to making linear regression more flexible by 
</p>
<p>using polynomials. 
</p>
<p>There is a potential drawback. If we significantly expand the dimension 
</p>
<p>of the problem, we might increase the computational burden. For example, 
</p>
<p>if X has dimension d = 256 and we wanted to use all fourth-order terms, 
</p>
<p>then z = &cent;(x) has dimension 183,181,376. We are spared this computational </p>
<p/>
</div>
<div class="page"><p/>
<p>372 22. Classification 
</p>
<p>.T2 
</p>
<p>dJ 
Z2 + 
</p>
<p>~ 
+ -------- + + + + 
</p>
<p>+ 
+ Xl + + 
</p>
<p>+ + &deg; 
+ 
</p>
<p>+ 06' 
Z, 
</p>
<p>+ + 
+ &deg; 
</p>
<p>+ + ;.: Z:l 
</p>
<p>FIGURE 22.9. Kernelization. Mapping the covariates into a higher-dimensional 
</p>
<p>space can make a complicated decision boundary into a simpler decision bound-
</p>
<p>ary. 
</p>
<p>nightmare by the following two facts. First, many classifiers do not require 
</p>
<p>that we know the values of the individual points but, rather, just the inner 
</p>
<p>product between pairs of points. Second, notice in our example that the inner 
</p>
<p>product in Z can be written 
</p>
<p>(z, Z; (&cent;(x), &cent;(x)) 
</p>
<p>xixi + 2XIXIX2X2 + x~x~ 
((X,X))2 == K(x,x). 
</p>
<p>Thus, we can compute (z, z; without ever computing Zi = &cent;(Xi). 
To summarize, kernelization involves finding a mapping &cent; : X -+ Z and a 
</p>
<p>classifier such that: 
</p>
<p>1. Z has higher dimension than X and so leads a richer set of classifiers. 
</p>
<p>2. The classifier only requires computing inner products. 
</p>
<p>3. There is a function K, called a kernel, such that (&cent;(x), &cent;(x)) = K(x, x). 
</p>
<p>4. Everywhere the term (x, x) appears in the algorithm, replace it with 
</p>
<p>K(x,x). </p>
<p/>
</div>
<div class="page"><p/>
<p>22.10 Kernelization 373 
</p>
<p>In fact, we never need to construct the mapping &cent; at all. We only need 
</p>
<p>to specify a kernel K (x, x) that corresponds to (&cent;( x), &cent;(x)) for some &cent;. This 
</p>
<p>raises an interesting question: given a function of two variables K(x, y), does 
</p>
<p>there exist a function &cent;(x) such that K(x,y) = (&cent;(x),&cent;(y))? The answer is 
</p>
<p>provided by Mercer's theorem which says, roughly, that if K is positive 
</p>
<p>definite - meaning that 
</p>
<p>J J K(x, y)f(x)f(y)dxdy ~ 0 
for square integrable functions f - then such a &cent; exists. Examples of com-
monly used kernels are: 
</p>
<p>polynomial K(x, x) 
</p>
<p>sigmoid K (x, x) 
</p>
<p>Gaussian K(x, x) 
</p>
<p>((x, x) +ar 
tanh(a(x, x) + b) 
</p>
<p>exp ( -llx - xl1 2 /(20"2)) 
</p>
<p>Let us now see how we can use this trick in LDA and in support vector 
</p>
<p>machines. 
</p>
<p>Recall that the Fisher linear discriminant method replaces X with U = 
</p>
<p>w T X where w is chosen to maximize the Rayleigh coefficient 
</p>
<p>and 
</p>
<p>J(w) = u75B w 
wT5ww' 
</p>
<p>- - - - T 
5B = (Xo - X1)(Xo - XI) 
</p>
<p>5w = ( (no - 1)50 ) + ( (n1 - 1)51 ) 
(no - 1) + (nl - 1) (no - 1) + (nl - 1) . 
</p>
<p>In the kernelized version, we replace Xi with Zi = &cent;(Xi) and we find w to 
</p>
<p>maximize 
</p>
<p>(22.42) 
</p>
<p>where 
</p>
<p>and 
</p>
<p>5w= ( 
(no-1)So ) ( (n1-1)Sl ) 
</p>
<p>(no - 1) + (n1 - 1) + (no - 1) + (n1 - 1) . 
</p>
<p>Here, 5 j is the sample of covariance of the Z/s for which Y = j. However, to 
</p>
<p>take advantage of kernelization, we need to re-express this in terms of inner 
</p>
<p>products and then replace the inner products with kernels. </p>
<p/>
</div>
<div class="page"><p/>
<p>374 22. Classification 
</p>
<p>It can be shown that the maximizing vector w is a linear combination of 
</p>
<p>the Zi'S. Hence we can write 
</p>
<p>Also, 
</p>
<p>Therefore, 
</p>
<p>n 
</p>
<p>W = LaiZi. 
i=1 
</p>
<p>_ 1 n 
Zj = - '" &cent;(Xi)1(Y; = j). 
</p>
<p>n~ 
J i=1 
</p>
<p>1 n n 
</p>
<p>-:;; L L ai1(Y8 = j)ZT &cent;(Xs) 
J i=1 8=1 
</p>
<p>1 n n 
</p>
<p>-:;; L ai L 1(Y8 = j)&cent;(Xif &cent;(X8) 
J i=1 8=1 
</p>
<p>1 n n 
</p>
<p>-:;; L ai L 1(Y8 = j)K(Xi' Xs) 
J i=1 8=1 
</p>
<p>aTMj 
</p>
<p>where JI.;[j is a vector whose ith component is 
</p>
<p>It follows that 
</p>
<p>where M = (Mo - Md(Mo - MdT. By similar calculations, we can write 
</p>
<p>where 
</p>
<p>N = Ko (I - ~1) Kif + K1 (I - ~1) Ki, 
no n1 
</p>
<p>I is the identity matrix, 1 is a matrix of all one's, and K j is the n x nj 
</p>
<p>matrix with entries (Kj ) r 8 = K (x r, X 8) with x s varying over the observations 
</p>
<p>in group j. Hence, we now find a to maximize 
</p>
<p>1(a) = aTMa. 
aTNa </p>
<p/>
</div>
<div class="page"><p/>
<p>22.11 Other Classifiers 375 
</p>
<p>All the quantities are expressed in terms of the kernel. Formally, the solution 
</p>
<p>is Q = N- 1 (J'vlo - J'vld. However, N might be non-invertible. In this case one 
</p>
<p>replaces N by N + bI, for some constant b. Finally, the projection onto the 
new subspace can be written as 
</p>
<p>n 
</p>
<p>u = w T &cent;(x) = L QiK(Xi' x). 
i=1 
</p>
<p>The support vector machine can similarly be kernelized. We simply replace 
</p>
<p>(Xi, Xj) with K(Xi' Xj). For example, instead of maximizing (22.40), we now 
</p>
<p>maximize 
n 1 n n 
</p>
<p>"Q - -"" QQkY;Y;kK(X X) ~ 2 2 ~ ~ t 2 t, J. (22.43) 
i=1 i=1 k=1 
</p>
<p>The hyperplane can be written as H(x) = 0,0 + L~=l aiIiK(X, Xi). 
</p>
<p>22.11 Other Classifiers 
</p>
<p>There are many other classifiers and space precludes a full discussion of all of 
</p>
<p>them. Let us briefly mention a few. 
</p>
<p>The k-nearest-neighbors classifier is very simple. Given a point x, find 
</p>
<p>the k data points closest to x. Classify x using the majority vote of these k 
</p>
<p>neighbors. Ties can be broken randomly. The parameter k can be chosen by 
</p>
<p>cross-validation. 
</p>
<p>Bagging is a method for reducing the variability of a classifier. It is most 
</p>
<p>helpful for highly nonlinear classifiers such as trees. We draw B bootstrap 
</p>
<p>samples from the data. The bth bootstrap sample yields a classifier hb . The 
</p>
<p>final classifier is 
</p>
<p>~ { 1 h(x) = 0 if 1I L~=1 hb(X) :::&gt; ~ 
otherwise. 
</p>
<p>Boosting is a method for starting with a simple classifier and gradually 
</p>
<p>improving it by refitting the data giving higher weight to misclassified samples. 
</p>
<p>Suppose that 1{ is a collection of classifiers, for example, trees with only 
</p>
<p>one split. Assume that Ii E {-I, I} and that each h is such that h(x) E 
</p>
<p>{ -1, I}. We usually give equal weight to all data points in the methods we 
</p>
<p>have discussed. But one can incorporate unequal weights quite easily in most 
</p>
<p>algorithms. For example, in constructing a tree, we could replace the impurity 
</p>
<p>measure with a weighted impurity measure. The original version of boosting, 
</p>
<p>called AdaBoost, is as follows. </p>
<p/>
</div>
<div class="page"><p/>
<p>376 22. Classification 
</p>
<p>1. Set the weights Wi = lin, 'i = 1, ... ,n. 
</p>
<p>2. For j = 1, ... , J, do the following steps: 
</p>
<p>(a) Constructing a classifier hj from the data using the weights WI, ... , W n . 
</p>
<p>(b) Compute the weighted error estimate: 
</p>
<p>(c) Let aj = log( (1 - Lj )/Lj ). 
</p>
<p>(d) Update the weights: 
</p>
<p>3. The final classifier is 
</p>
<p>There is now an enormous literature trying to explain and improve on 
</p>
<p>boosting. Whereas bagging is a variance reduction technique, boosting can 
</p>
<p>be thought of as a bias reduction technique. We starting with a simple -
</p>
<p>and hence highly-biased - classifier, and we gradually reduce the bias. The 
</p>
<p>disadvantage of boosting is that the final classifier is quite complicated. 
</p>
<p>Neural Networks are regression models of the form 3 
</p>
<p>p 
</p>
<p>y = Po + L pju(ao + aT X) 
j=1 
</p>
<p>where u is a smooth function, often taken to be u(v) = eV 1(1 + eV ). This 
is really nothing more than a nonlinear regression model. Neural nets were 
</p>
<p>fashionable for some time but they pose great computational difficulties. In 
</p>
<p>particular, one often encounters multiple minima when trying to find the least 
</p>
<p>squares estimates of the parameters. Also, the number of terms p is essentially 
</p>
<p>a smoothing parameter and there is the usual problem of trying to choose p 
</p>
<p>to find a good balance between bias and variance. 
</p>
<p>3This is the simplest version of a neural net. There are more complex versions of the model. </p>
<p/>
</div>
<div class="page"><p/>
<p>22.12 Bibliographic Remarks 377 
</p>
<p>22.12 Bibliographic Rernarks 
</p>
<p>The literature on classification is vast and is growing quickly. An excellent 
</p>
<p>reference is Hastie et al. (2001). For more on the theory, see Devroye et al. 
</p>
<p>(1996) and Vapnik (1998). Two recent books on kernels are Scholkopf and 
</p>
<p>Smola (2002) and Herbich (2002). 
</p>
<p>22.13 Exercises 
</p>
<p>1. Prove Theorem 22.5. 
</p>
<p>2. Prove Theorem 22.7. 
</p>
<p>3. Download the spam data from: 
</p>
<p>http://www-stat.stanford.edu/ ,,-,tibs/ElemStatLearn/index.html 
</p>
<p>The data file can also be found on the course web page. The data con-
</p>
<p>tain 57 covariates relating to email messages. Each email message was 
</p>
<p>classified as spam (Y=l) or not spam (Y=O). The outcome Y is the last 
</p>
<p>column in the file. The goal is to predict whether an email is spam or 
</p>
<p>not. 
</p>
<p>(a) Construct classification rules using (i) LDA, (ii) QDA, (iii) logistic 
</p>
<p>regression, and (iv) a classification tree. For each, report the observed 
</p>
<p>misclassification error rate and construct a 2-by-2 table of the form 
</p>
<p>h(x)=O h(x)=l 
</p>
<p>Y = O?? ?? 
</p>
<p>Y = I?? ?? 
</p>
<p>(b) Use 5-fold cross-validation to estimate the prediction accuracy of 
</p>
<p>LDA and logistic regression. 
</p>
<p>(c) Sometimes it helps to reduce the number of covariates. One strategy 
</p>
<p>is to compare Xi for the spam and email group. For each of the 57 
</p>
<p>covariates, test whether the mean of the covariate is the same or different 
</p>
<p>between the two groups. Keep the 10 covariates with the smallest p-
</p>
<p>values. Try LDA and logistic regression using only these 10 variables. </p>
<p/>
</div>
<div class="page"><p/>
<p>378 22. Classification 
</p>
<p>4. Let A be the set of two-dimensional spheres. That is, A E A if A = 
</p>
<p>{(x,y): (x-a)2+(y-b)2::.; c2} for some a,b,c. Find the VC-dimension 
</p>
<p>of A. 
</p>
<p>5. Classify the spam data using support vector machines. Free software for 
</p>
<p>the support vector machine is at http://svmlight.joachims.org/ 
</p>
<p>6. Use VC theory to get a confidence interval on the true error rate of the 
</p>
<p>LDA classifier for the iris data (from the book web site). 
</p>
<p>7. Suppose that Xi E lR and that li = 1 whenever IXil ::.; 1 and li = 0 
whenever IXil &gt; 1. Show that no linear classifier can perfectly classify 
these data. Show that the kernelized data Zi = (Xi, xl) can be linearly 
</p>
<p>separated. 
</p>
<p>8. Repeat question 5 using the kernel K(x, ?i) = (1 + xT?i)p. Choose p by 
cross-validation. 
</p>
<p>9. Apply the k nearest neighbors classifier to the "iris data." Choose k by 
</p>
<p>cross-validation. 
</p>
<p>10. (Curse of Dimensionality.) Suppose that X has a uniform distribution 
</p>
<p>on the d-dimensional cube [-1/2, 1/2]d. Let R be the distance from the 
</p>
<p>origin to the closest neighbor. Show that the median of R is 
</p>
<p>where 
d/2 
</p>
<p>Vd(r-) = r-d r((d~2) + 1) 
</p>
<p>is the volume of a sphere of radius r-. For what dimension d does the 
</p>
<p>median of R exceed the edge of the cube when n = 100, n = 1,000, 
</p>
<p>n = 10, OOO? (Hastie et al. (2001), p. 22-27.) 
</p>
<p>11. Fit a tree to the data in question 3. Now apply bagging and report your 
</p>
<p>results. 
</p>
<p>12. Fit a tree that uses only one split on one variable to the data in question 
</p>
<p>3. Now apply boosting. </p>
<p/>
</div>
<div class="page"><p/>
<p>22.13 Exercises 379 
</p>
<p>13. Let r(x) = P(Y = 11X = x) and let r(x) be an estimate of r(x). Consider 
</p>
<p>the classifier 
</p>
<p>h(x) = {I if r(x) ."21/2 
o otherwIse. 
</p>
<p>Assume that r( x) :::::; N (r( x), (]"2 (x)) for some functions r( x) and (]"2 (x). 
</p>
<p>Show that, for fixed x, 
</p>
<p>P(Y -I- h(x)) :::::; P(Y -I- h*(x)) 
</p>
<p>I I [ (
</p>
<p>Sign(r(X)-(1/2))(r(X)-(1/2)))] 
+ 2r(x) - 1 x 1 - &lt;I&gt; ( ) 
</p>
<p>(]"x 
</p>
<p>where &lt;I&gt; is the standard Normal CDF and h* is the Bayes rule. Regard 
</p>
<p>sign ((r(x) - (1/2))(r(x) - (1/2))) as a type of bias term. Explain the 
</p>
<p>implications for the bias-variance tradeoff in classification (Friedman 
</p>
<p>(1997) ). 
</p>
<p>Hint: first show that 
</p>
<p>P(Y -I- h(x)) = 12r(x) - 1IP(h(x) -I- h*(x)) + P(Y '" h*(x)). </p>
<p/>
</div>
<div class="page"><p/>
<p>23 
</p>
<p>Probability Redux: Stochastic Processes 
</p>
<p>23.1 Introduction 
</p>
<p>Most of this book has focused on IID sequences of random variables. Now we 
</p>
<p>consider sequences of dependent random variables. For example, daily tem-
</p>
<p>peratures will form a sequence of time-ordered random variables and clearly 
</p>
<p>the temperature on one day is not independent of the temperature on the 
</p>
<p>previous day. 
</p>
<p>A stochastic process {Xt : t E T} is a collection of random variables. 
</p>
<p>We shall sometimes write X(t) instead of Xt. The variables X t take values in 
</p>
<p>some set X called the state space. The set T is called the index set and 
</p>
<p>for our purposes can be thought of as time. The index set can be discrete 
</p>
<p>T = {O, 1, 2, ... } or continuous T = [0, CXJ) depending on the application. 
</p>
<p>23.1 Example (rID observations). A sequence of rID random variables can be 
</p>
<p>written as {Xt : t E T} where T = {I, 2, 3, ... ,}. Thus, a sequence of IID 
</p>
<p>random variables is an example of a stochastic process. _ 
</p>
<p>23.2 Example (The Weather). Let X = {sunny, cloudy}. A typical sequence 
</p>
<p>(depending on where you live) might be 
</p>
<p>sunny, sunny, cloudy, sunny, cloudy, cloudy"" 
</p>
<p>This process has a discrete state space and a discrete index set. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>382 23. Probability Redux: Stochastic Processes 
</p>
<p>price 
</p>
<p>time 
</p>
<p>FIGURE 23.1. Stock price over ten week period. 
</p>
<p>23.3 Example (Stock Prices). Figure 23.1 shows the price of a fictitious stock 
</p>
<p>over time. The price is monitored continuously so the index set T is continuous. 
</p>
<p>Price is discrete but for all practical purposes we can treat it as a continuous 
</p>
<p>variable. _ 
</p>
<p>23.4 Example (Empirical Distribution Function). Let Xl, ... , Xn rv F where 
</p>
<p>F is some CDF on [0,1]. Let 
</p>
<p>~ 1 n 
Fn(t) = - L I(Xi ::; t) 
</p>
<p>n 
i=l 
</p>
<p>be the empirical CDF. For any fixed value t, Fn(t) is a random variable. But 
</p>
<p>the whole empirical CDF 
</p>
<p>is a stochastic process with a continuous state space and a continuous index 
</p>
<p>set. _ 
</p>
<p>We end this section by recalling a basic fact. If Xl' ... ' Xn are random 
</p>
<p>variables, then we can write the joint density as 
</p>
<p>n 
</p>
<p>II !(xilpasti ) (23.1) 
i=l </p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Markov Chains 383 
</p>
<p>23.2 Markov Chains 
</p>
<p>A Markov chain is a stochastic process for which the distribution of X t de-
</p>
<p>pends only on X t - 1 . In this section we assume that the state space is dis-
</p>
<p>crete, either X = {I, ... , N} or X = {I, 2, ... ,} and that the index set is 
</p>
<p>T = {O, 1, 2, ... }. Typically, most authors write Xn instead of X t when dis-
</p>
<p>cussing Markov chains and I will do so as well. 
</p>
<p>23.5 Definition. The process {Xn: nET} is a Markov chain if 
</p>
<p>IP'(Xn = x I X o,&middot;&middot;&middot;, Xn-d = IP'(Xn = x I Xn-d (23.2) 
</p>
<p>for all n and for all x E X. 
</p>
<p>For a Markov chain, equation (23.1) simplifies to 
</p>
<p>A Markov chain can be represented by the following DAG: 
</p>
<p>Xo --..,)~ X 2 ----. 
</p>
<p>Each variable has a single parent, namely, the previous observation. 
</p>
<p>The theory of Markov chains is a very rich and complex. We have to get 
</p>
<p>through many definitions before we can do anything interesting. Our goal is 
</p>
<p>to answer the following questions: 
</p>
<p>1. When does a Markov chain "settle down" into some sort of equilibrium? 
</p>
<p>2. How do we estimate the parameters of a Markov chain? 
</p>
<p>3. How can we construct Markov chains that converge to a given equilib-
</p>
<p>rium distribution and why would we want to do that? 
</p>
<p>We will answer questions 1 and 2 in this chapter. We will answer question 
</p>
<p>3 in the next chapter. To understand question 1, look at the two chains in 
</p>
<p>Figure 23.2. The first chain oscillates all over the place and will continue to 
</p>
<p>do so forever. The second chain eventually settles into an equilibrium. If we 
</p>
<p>constructed a histogram of the first process, it would keep changing as we got </p>
<p/>
</div>
<div class="page"><p/>
<p>384 23. Probability Redux: Stochastic Processes 
</p>
<p>more and more observations. But a histogram from the second chain would 
</p>
<p>eventually converge to some fixed distribution. 
</p>
<p>time time 
</p>
<p>FIGURE 23.2. Two Markov chains. The first chain does not settle down into an 
</p>
<p>equilibrium. The second does. 
</p>
<p>TRANSITION PROBABILITIES. The key quantities of a Markov chain are the 
</p>
<p>probabilities of jumping from one state into another state. A Markov chain is 
</p>
<p>homogeneous if IP'(Xn+l = jlXn = i) does not change with time. Thus, for 
</p>
<p>a homogeneous Markov chain, IP'(Xn+1 = jlXn =i) = IP'(Xl = jlXo = i). We 
</p>
<p>shall only deal with homogeneous Markov chains. 
</p>
<p>23.6 Definition. We call 
</p>
<p>Pij == IP'(Xn+1 = jlXn = i) (23.3) 
</p>
<p>the transition probabilities. The matrix P whose (i, j) element is Pij 
</p>
<p>is called the transition matrix. 
</p>
<p>We will only consider homogeneous chains. Notice that P has two proper-
</p>
<p>ties: (i) Pij ~ 0 and (ii) Li Pij = 1. Each row can be regarded as a probability 
</p>
<p>mass function. 
</p>
<p>23.7 Example (Random Walk With Absorbing Barriers). Let X = {I, ... , N}. 
</p>
<p>Suppose you are standing at one of these points. Flip a coin with IP'(Heads) = P 
</p>
<p>and IP'(Tails) = q = 1 - p. If it is heads, take one step to the right. If it is 
</p>
<p>tails, take one step to the left. If you hit one of the endpoints, stay there. The </p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Markov Chains 385 
</p>
<p>transition matrix is 
</p>
<p>1 0 0 0 0 0 
</p>
<p>q 0 P 0 0 0 
</p>
<p>p= 
0 q 0 P 0 0 
</p>
<p>-
0 0 0 0 q 0 P 
0 0 0 0 0 0 1 
</p>
<p>23.8 Example. Suppose the state space is X {sunny, cloudy}. Then Xl, 
</p>
<p>X 2 , ... represents the weather for a sequence of days. The weather today 
</p>
<p>clearly depends on yesterday's weather. It might also depend on the weather 
</p>
<p>two days ago but as a first approximation we might assume that the depen-
</p>
<p>dence is only one day back. In that case the weather is a Markov chain and a 
</p>
<p>typical transition matrix might be 
</p>
<p>Sunny 
</p>
<p>Cloudy 
</p>
<p>Sunny 
</p>
<p>0.4 
</p>
<p>0.8 
</p>
<p>Cloudy 
</p>
<p>0.6 
</p>
<p>0.2 
</p>
<p>For example, if it is sunny today, there is a 60 per cent chance it will be cloudy 
</p>
<p>tomorrow. _ 
</p>
<p>Let 
</p>
<p>(23.4) 
</p>
<p>be the probability of of going from state i to state j in n steps. Let P n be the 
</p>
<p>matrix whose ('i,j) element is pij(n). These are called the n-step transition 
</p>
<p>probabilities. 
</p>
<p>23.9 Theorem (The Chapman-Kolmogorov equations). The n-step probabilities 
</p>
<p>satisfy 
</p>
<p>Pij(m + n) = LPik(m)Pkj(n). 
k 
</p>
<p>PROOF. Recall that, in general, 
</p>
<p>J1D(X = x, Y = y) = J1D(X = x)IP'(Y = ylX = x). 
</p>
<p>This fact is true in the more general form 
</p>
<p>(23.5) 
</p>
<p>J1D(X = x, Y = ylZ = z) = J1D(X = xlZ = z)IP'(Y = ylX = x, Z = z). 
</p>
<p>Also, recall the law of total probability: 
</p>
<p>J1D(X = x) = LJID(X = x, Y = y). 
y </p>
<p/>
</div>
<div class="page"><p/>
<p>386 23. Probability Redux: Stochastic Processes 
</p>
<p>Using these facts and the Markov property we have 
</p>
<p>Pij(m + 71,) IP'(Xm+n = jlXo = i) 
</p>
<p>L IP'(Xm+n = j, Xm = klXo =i) 
k 
</p>
<p>L IP'(Xm+n = jlXm = k, Xo = i)IP'(Xm = klXo =i) 
k 
</p>
<p>k 
</p>
<p>LPik(m)Pkj(n). -
k 
</p>
<p>Look closely at equation (23.5). This is nothing more than the equation for 
</p>
<p>matrix multiplication. Hence we have shown that 
</p>
<p>By definition, PI = P. Using the above theorem, P 2 
</p>
<p>PP = p2. Continuing this way, we see that 
</p>
<p>P n = pn == PxPx&middot;&middot;&middot;xp 
... ~ 
</p>
<p>v 
</p>
<p>multiply the matrix n times 
</p>
<p>Let fLn = (fLn (1), ... , fLn (N)) be a row vector where 
</p>
<p>(23.6) 
</p>
<p>(23.7) 
</p>
<p>(23.8) 
</p>
<p>is the marginal probability that the chain is in state i at time n. In particular, 
</p>
<p>fLo is called the initial distribution. To simulate a Markov chain, all you 
</p>
<p>need to know is fLo and P. The simulation would look like this: 
</p>
<p>Step 1: Draw Xo rv ILo. Thus, IP'(Xo = i) = ILo(i). 
</p>
<p>Step 2: Denote the outcome of step 1 by i. Draw Xl rv P. In other words, 
</p>
<p>IP'(XI = jlXo = i) = Pij. 
</p>
<p>Step 3: Suppose the outcome of step 2 is j. Draw X 2 rv P. In other words, 
</p>
<p>IP'(X2 = klXI = j) = Pjk. 
</p>
<p>And so on. 
</p>
<p>It might be difficult to understand the meaning of fLn. Imagine simulating 
</p>
<p>the chain many times. Collect all the outcomes at time n from all the chains. 
</p>
<p>This histogram would look approximately like ILn. A consequence of theorem 
</p>
<p>23.9 is the following: </p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Markov Chains 387 
</p>
<p>23.10 Lemma. The marginal probabilities are given by 
</p>
<p>PROOF. 
</p>
<p>IP'(Xn = j) 
</p>
<p>L IP'(Xn = j IXo =i)P(Xo = i) 
</p>
<p>Summary of Terminology 
</p>
<p>1. Transition matrix: P(i,j) = IP'(Xn+1 = jlXn = i) = Pij' 
</p>
<p>2. n-step matrix: Pn(i,j) = IP'(Xn+m = jlXm = i). 
</p>
<p>4. Marginal: ILn(i) = IP'(Xn = i). 
</p>
<p>STATES. The states of a Markov chain can be classified according to various 
</p>
<p>properties. 
</p>
<p>23.11 Definition. We say that i reaches j (or j is accessible from i) if 
</p>
<p>Pij (n) &gt; 0 for some n, and we write i --+ j. If i --+ j and j --+ i then we 
write i f-+ j and we say that i and j communicate. 
</p>
<p>23.12 Theorem. The communication relation satisfies the following proper-
</p>
<p>ties: 
</p>
<p>1. if-+i. 
</p>
<p>2. If i f-+ j then j f-+ i. 
</p>
<p>3. If i f-+ j and j f-+ k then i f-+ k. 
</p>
<p>4. The set of states X can be written as a disjoint union of classes X = 
Xl U X2 U ... where two states i and j communicate with each other if 
and only if they are in the same class. </p>
<p/>
</div>
<div class="page"><p/>
<p>388 23. Probability Redux: Stochastic Processes 
</p>
<p>If all states communicate with each other, then the chain is called irre-
</p>
<p>ducible. A set of states is closed if, once you enter that set of states you 
</p>
<p>never leave. A closed set consisting of a single state is called an absorbing 
</p>
<p>state. 
</p>
<p>23.13 Example. Let X = {1,2,3,4} and 
</p>
<p>p~ 0 ~ ~ D 
The classes are {I, 2}, {3} and {4}. State 4 is an absorbing state. _ 
</p>
<p>Suppose we start a chain in state i. Will the chain ever return to state i? 
</p>
<p>If so, that state is called persistent or recurrent. 
</p>
<p>23.14 Definition. State i is recurrent or persistent if 
</p>
<p>IP'(Xn = i for some n ~ 1 I Xo = i) = 1. 
</p>
<p>Otherwise, state i is transient. 
</p>
<p>23.15 Theorem. A state i is recurrent if and only if 
</p>
<p>n 
</p>
<p>A state i is transient if and only if 
</p>
<p>n 
</p>
<p>PROOF. Define 
if Xn = i 
if Xn # i. 
</p>
<p>(23.9) 
</p>
<p>(23.10) 
</p>
<p>The number of times that the chain is in state i is Y = L:~=o In. The mean 
</p>
<p>of Y, given that the chain starts in state i, is 
</p>
<p>00 00 00 
</p>
<p>n=O n=O n=O 
</p>
<p>Define ai = IP'(Xn = i for some n ~ 1 I Xo = i). If i is recurrent, ai = 1. Thus, 
</p>
<p>the chain will eventually return to i. Once it does return to i, we argue again </p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Markov Chains 389 
</p>
<p>that since ai = 1, the chain will return to state i again. By repeating this 
</p>
<p>argument, we conclude that JE(YIXo = i) = 00. If i is transient, then ai &lt; 1. 
</p>
<p>When the chain is in state i, there is a probability 1- ai &gt; 0 that it will never 
</p>
<p>return to state i. Thus, the probability that the chain is in state i exactly 71 
</p>
<p>times is a~-l(l - ai). This is a geometric distribution which has finite mean. 
</p>
<p>-
23.16 Theorem. Facts about recurrence. 
</p>
<p>1. If state i is recurrent and i ++ j, then j is recurrent. 
</p>
<p>2. If state i is transient and i ++ j, then j is transient. 
</p>
<p>3. A finite Markov chain must have at least one recurrent state. 
</p>
<p>4. The states of a finite, irreducible Markov chain are all recurrent. 
</p>
<p>23.17 Theorem (Decomposition Theorem). The state space X can be written 
</p>
<p>as the disjoint union 
</p>
<p>where XT are the transient states and each Xi is a closed, irreducible set of 
</p>
<p>recurrent states. 
</p>
<p>23.18 Example (Random Walk). Let X = { ... , -2, -1,0,1,2, ... ,} and sup-
</p>
<p>pose that Pi,i+1 = p, Pi,i-l = q = 1 - p. All states communicate, hence either 
</p>
<p>all the states are recurrent or all are transient. To see which, suppose we start 
</p>
<p>at Xo = O. Note that 
</p>
<p>poo(2n) = Cnn)pnqn (23.11) 
</p>
<p>since the only way to get back to 0 is to have 71 heads (steps to the right) and 
</p>
<p>71 tails (steps to the left). We can approximate this expression using Stirling's 
</p>
<p>formula which says that 
</p>
<p>Inserting this approximation into (23.11) shows that 
</p>
<p>(4pq)n 
poo(2n) rv --. vmr 
</p>
<p>It is easy to check that ~n Poo (71) &lt; 00 if and only if ~n POO (271) &lt; 00. 
Moreover, ~nPoo(2n) = 00 if and only if P = q = 1/2. By Theorem (23.15), 
</p>
<p>the chain is recurrent if P = 1/2 otherwise it is transient. _ </p>
<p/>
</div>
<div class="page"><p/>
<p>390 23. Probability Redux: Stochastic Processes 
</p>
<p>CONVERGENCE OF MARKOV CHAINS. To discuss the convergence of chains, 
</p>
<p>we need a few more definitions. Suppose that Xo = i. Define the recurrence 
</p>
<p>time 
</p>
<p>Tij = min{n &gt; 0: Xn = j} (23.12) 
</p>
<p>assuming Xn ever returns to state i, otherwise define Tij 
</p>
<p>recurrence time of a recurrent state i is 
</p>
<p>00. The mean 
</p>
<p>(23.13) 
</p>
<p>n 
</p>
<p>where 
</p>
<p>A recurrent state is null if 7ni = 00 otherwise it is called non-null or posi-
</p>
<p>tive. 
</p>
<p>23.19 Lemma. If a state is null and recurrent, then pjj --+ o. 
</p>
<p>23.20 Lemma. In a finite state Markov chain, all recurrent states are positive. 
</p>
<p>Consider a three-state chain with transition matrix 
</p>
<p>1 
</p>
<p>o 
o 
</p>
<p>Suppose we start the chain in state 1. Then we will be in state 3 at times 3, 6, 
</p>
<p>9, .... This is an example of a periodic chain. Formally, the period of state i 
</p>
<p>is d if pii(n) = 0 whenever n is not divisible by d and d is the largest integer 
</p>
<p>with this property. Thus, d = gcd {n: Pii (n) &gt; O} where gcd means "greater 
</p>
<p>common divisor." Statei is periodic if d(i) &gt; 1 and aperiodic if d(i) = l. 
</p>
<p>A state with period 1 is called aperiodic. 
</p>
<p>23.21 Lemma. If state i has period d andi B j then j has period d. 
</p>
<p>23.22 Definition. A state is ergodic if it is recurrent, non-null and 
</p>
<p>aperiodic. A chain is ergodic if all its states are ergodic. 
</p>
<p>Let Jr = (Jri: i E X) be a vector of non-negative numbers that sum to one. 
</p>
<p>Thus Jr can be thought of as a probability mass function. 
</p>
<p>23.23 Definition. We say that Jr is a stationary (or invariant) 
</p>
<p>distribution if Jr = Jr P . </p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Markov Chains 391 
</p>
<p>Here is the intuition. Draw Xo from distribution 'if and suppose that 'if is a 
</p>
<p>stationary distribution. Now draw Xl according to the transition probability 
</p>
<p>of the chain. The distribution of Xl is then fLl = fLoP = 'ifp = 'if. The 
</p>
<p>distribution of X 2 is 'ifp2 = ('ifP)P = 'ifp = 'if. Continuing this way, we see 
</p>
<p>that the distribution of Xn is 'ifpn = 'if. In other words: 
</p>
<p>If at any time the chain has distribution 'if, then it will continue to 
</p>
<p>have distribution 'if forever. 
</p>
<p>23.24 Definition. We say that a chain has limiting distribution if 
</p>
<p>for some 'if, that is, 'ifj = limn--+= Pi; exists and is independent of i. 
</p>
<p>Here is the main theorem about convergence. The theorem says that an 
</p>
<p>ergodic chain converges to its stationary distribution. Also, sample averages 
</p>
<p>converge to their theoretical expectations under the stationary distribution. 
</p>
<p>23.25 Theorem. An irreducible, ergodic Markov chain has a unique 
</p>
<p>stationary distribution 'if. The limiting distribution exists and is equal to 
</p>
<p>'if. If g is any bounded function, then, with probability 1, 
</p>
<p>(23.14) 
</p>
<p>Finally, there is another definition that will be useful later. We say that 'if 
</p>
<p>satisfies detailed balance if 
</p>
<p>(23.15) 
</p>
<p>Detailed balance guarantees that 'if is a stationary distribution. 
</p>
<p>23.26 Theorem. If'if satisfies detailed balance, then 'if is a stationary distri-
</p>
<p>bution. 
</p>
<p>PROOF. We need to show that 'ifp = 'if. The jth element of'ifP is Li 'ifiPij = 
</p>
<p>Li'ifjPji = 'ifj LiPji = 'ifj .&bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>392 23. Probability Redux: Stochastic Processes 
</p>
<p>The importance of detailed balance will become clear when we discuss 
</p>
<p>Markov chain Monte Carlo methods in Chapter 24. 
</p>
<p>Warning! Just because a chain has a stationary distribution does not mean 
</p>
<p>it converges. 
</p>
<p>23.27 Example. Let 
</p>
<p>p~ [~ ~ n 
Let Jr = (1/3,1/3,1/3). Then Jrp Jr so Jr is a stationary distribution. If 
</p>
<p>the chain is started with the distribution Jr it will stay in that distribution. 
</p>
<p>Imagine simulating many chains and checking the marginal distribution at 
</p>
<p>each time n. It will always be the uniform distribution Jr. But this chain does 
</p>
<p>not have a limit. It continues to cycle around forever. _ 
</p>
<p>EXAMPLES OF MARKOV CHAINS. 
</p>
<p>23.28 Example. Let X = {I, 2, 3, 4, 5, 6}. Let 
</p>
<p>1 1 0 0 0 0 "2 "2 
1 3 0 0 0 0 "4 "4 
1 1 1 1 0 0 
</p>
<p>p= "4 "4 "4 "4 
1 0 1 1 0 1 "4 "4 "4 "4 
</p>
<p>0 0 0 0 1 1 "2 "2 
</p>
<p>0 0 0 0 1 1 "2 "2 
</p>
<p>Then C1 = {1,2} and C2 = {5,6} are irreducible closed sets. States 3 and 
</p>
<p>4 are transient because of the path 3 --+ 4 --+ 6 and once you hit state 6 
</p>
<p>you cannot return to 3 or 4. Since Pii(l) &gt; 0, all the states are aperiodic. In 
summary, 3 and 4 are transient while 1, 2, 5, and 6 are ergodic. _ 
</p>
<p>23.29 Example (Hardy-Weinberg). Here is a famous example from genetics. 
</p>
<p>Suppose a gene can be type A or type a. There are three types of people (called 
</p>
<p>genotypes): AA, Aa, and aa. Let (p, q, T) denote the fraction of people of each 
</p>
<p>genotype. We assume that everyone contributes one of their two copies of the 
</p>
<p>gene at random to their children. We also assume that mates are selected at 
</p>
<p>random. The latter is not realistic however, it is often reasonable to assume 
</p>
<p>that you do not choose your mate based on whether they are AA, Aa, or 
</p>
<p>aa. (This would be false if the gene was for eye color and if people chose 
</p>
<p>mates based on eye color.) Imagine if we pooled everyone's genes together. 
</p>
<p>The proportion of A genes is P = p + (q/2) and the proportion of a genes is </p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Markov Chains 393 
</p>
<p>Q = r + (q/2). A child is AA with probability p2, aA with probability 2PQ, 
and aa with probability Q2. Thus, the fraction of A genes in this generation 
</p>
<p>is 
</p>
<p>However, r = 1 - p - q. Substitute this in the above equation and you get 
</p>
<p>p 2 + PQ = P. A similar calculation shows that the fraction of "a" genes is 
Q. We have shown that the proportion of type A and type a is P and Q and 
</p>
<p>this remains stable after the first generation. The proportion of people of type 
</p>
<p>AA, Aa, aa is thus (P2, 2PQ, Q2) from the second generation and on. This is 
</p>
<p>called the Hardy-Weinberg law. 
</p>
<p>Assume everyone has exactly one child. Now consider a fixed person and 
</p>
<p>let Xn be the genotype of their nth descendant. This is a Markov chain with 
</p>
<p>state space X = {AA, Aa, aa}. Some basic calculations will show you that the 
</p>
<p>transition matrix is 
</p>
<p>Q 
P+Q 
-2-
</p>
<p>P 
</p>
<p>The stationary distribution is 7r = (P2, 2PQ, Q2) .&bull; 
</p>
<p>23.30 Example (Markov chain Monte Carlo). In Chapter 24 we will present a 
</p>
<p>simulation method called Markov chain Monte Carlo (MCMC). Here is a brief 
</p>
<p>description of the idea. Let f(x) be a probability density on the real line and 
</p>
<p>suppose that f (x) = cg( x) where g( x) is a known function and c &gt; 0 is 
</p>
<p>unknown. In principle, we can compute c since J f(x)dx = 1 implies that 
c = 1/ J g(x)dx. However, it may not be feasible to perform this integral, nor 
is it necessary to know c in the following algorithm. Let X o be an arbitrary 
</p>
<p>starting value. Given Xo, ... , Xi, draw Xi+l as follows. First, draw W rv 
</p>
<p>N(Xi' b2 ) where b &gt; 0 is some fixed constant. Let 
</p>
<p>. {g(W) } 
r = mm g(Xi ) , 1 . 
</p>
<p>Draw U rv Uniform(O, 1) and set 
</p>
<p>if U &lt; r 
if U 2 r. 
</p>
<p>We will see in Chapter 24 that, under weak conditions, XO,X1 , ... , is an 
</p>
<p>ergodic Markov chain with stationary distribution f. Hence, we can regard 
the draws as a sample from f .&bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>394 23. Probability Redux: Stochastic Processes 
</p>
<p>INFERENCE FOR MARKOV CHAINS. Consider a chain with finite state space 
</p>
<p>X = {I, 2, ... , N}. Suppose we observe n observations Xl,"" Xn from this 
</p>
<p>chain. The unknown parameters of a Markov chain are the initial probabilities 
</p>
<p>/10 = (/10(1), /10(2), ... ,) and the elements of the transition matrix P. Each 
</p>
<p>row of P is a multinomial distribution. So we are essentially estimating N 
</p>
<p>distributions (plus the initial probabilities). Let nij be the observed number 
</p>
<p>of transitions from state i to state j. The likelihood function is 
</p>
<p>n N N 
</p>
<p>&pound;(/10,P) = /1o(xo) IIPx,-l,x, = /1o(xo) II IIp~ij. 
r=l i=l j=l 
</p>
<p>There is only one observation on /10 so we can't estimate that. Rather, we 
</p>
<p>focus on estimating P. The MLE is obtained by maximizing &pound;(/10, P) subject 
</p>
<p>to the constraint that the elements are non-negative and the rows sum to l. 
</p>
<p>The solution is 
~ nij 
Pij =-
</p>
<p>ni 
</p>
<p>where ni = Lf=l nij' Here we are assuming that ni &gt; O. If not, then we set 
</p>
<p>Pij = 0 by convention. 
</p>
<p>23.31 Theorem (Consistency and Asymptotic Normality of the MLE). Assume that 
</p>
<p>the chain is ergodic. Let pij(n) denote the MLE after n observations. Then 
</p>
<p>Pij(n)~ Pij' Also, 
</p>
<p>[VNi(n)(Pij - Pij)] ~ N(O,~) 
</p>
<p>where the left-hand side is a matrix, Ni(n) = L~=l J(Xr = i) and 
</p>
<p>{ 
Pij(1 - Pij) 
</p>
<p>~ij,ke = ~PijPie 
</p>
<p>23.3 Poisson Processes 
</p>
<p>(i,j) = (k,R) 
i=k,jcjR 
otherwise. 
</p>
<p>The Poisson process arises when we count occurrences of events over time, for 
</p>
<p>example, traffic accidents, radioactive decay, arrival of email messages, etc. 
</p>
<p>As the name suggests, the Poisson process is intimately related to the Poisson 
</p>
<p>distribution. Let's first review the Poisson distribution. 
</p>
<p>Recall that X has a Poisson distribution with parameter), - written X rv 
</p>
<p>Poisson(),) - if 
</p>
<p>e-.\),X 
</p>
<p>JP'(X = x) == p(x; ),) = --I -, x = 0,1,2, ... 
x. </p>
<p/>
</div>
<div class="page"><p/>
<p>23.3 Poisson Processes 395 
</p>
<p>Also recall that JE(X) = A and V(X) = A. If X rv Poisson(A), Y rv Poisson(v) 
</p>
<p>and XIIY, then X+Y rv Poisson(A+v). Finally, if N rv Poisson(A) and YIN = 
</p>
<p>n rv Binomial(n,p), then the marginal distribution of Y is Y rv Poisson(Ap). 
</p>
<p>Now we describe the Poisson process. Imagine that you are at your com-
</p>
<p>puter. Each time a new email message arrives you record the time. Let X t be 
</p>
<p>the number of messages you have received up to and including time t. Then, 
</p>
<p>{Xt : t E [0, oo)} is a stochastic process with state space X = {O, 1,2, ... }. 
</p>
<p>A process of this form is called a counting process. A Poisson process is 
</p>
<p>a counting process that satisfies certain conditions. In what follows, we will 
</p>
<p>sometimes write X(t) instead of X t . Also, we need the following notation. 
</p>
<p>Write f(h) = o(h) if f(h)/h --+ 0 as h --+ o. This means that f(h) is smaller 
than h when h is close to O. For example, h2 = o(h). 
</p>
<p>23.32 Definition. A Poisson process is a stochastic process 
</p>
<p>{Xt : t E [0, oo)} with state space X = {O, 1,2, ... } such that 
</p>
<p>1. X(O) = O. 
</p>
<p>2. For any 0 = to &lt; tl &lt; t2 &lt; ... &lt; tn, the increments 
</p>
<p>are independent. 
</p>
<p>3. There is a function A(t) such that 
</p>
<p>J1D(X(t + h) - X(t) = 1) 
</p>
<p>J1D(X(t + h) - X(t) ~ 2) 
</p>
<p>We call A(t) the intensity function. 
</p>
<p>A(t)h + o(h) 
</p>
<p>o(h). 
</p>
<p>(23.16) 
</p>
<p>(23.17) 
</p>
<p>The last condition means that the probability of an event in [t, t + h] is 
</p>
<p>approximately hA(t) while the probability of more than one event is small. 
</p>
<p>23.33 Theorem. If X t is a Poisson process with intensity function A(t), then 
</p>
<p>X(s + t) - X(s) rv Poisson(rn(s + t) - rn(s)) 
</p>
<p>where 
</p>
<p>rn(t) = lot A(S) ds. 
In particular, X(t) rv Poisson(rn(t)). Hence, JE(X(t)) = rn(t) and V(X(t)) = 
</p>
<p>rn(t) . </p>
<p/>
</div>
<div class="page"><p/>
<p>396 23. Probability Redux: Stochastic Processes 
</p>
<p>23.34 Definition. A Poisson process with intensity function A(t) == A for 
</p>
<p>some A &gt; 0 is called a homogeneous Poisson process with rate A. In 
</p>
<p>this case, 
</p>
<p>X(t) rv Poisson(At). 
</p>
<p>Let X(t) be a homogeneous Poisson process with rate A. Let Wn be the 
</p>
<p>time at which the nth event occurs and set Wo = o. The random variables 
Wo, WI' ... ' are called waiting times. Let Sn = Wn+l - Wn. Then So, Sl, ... , 
</p>
<p>are called sojourn times or interarrival times. 
</p>
<p>23.35 Theorem. The sojourn times So, Sl, ... are lID random variables. Their 
</p>
<p>distribution is exponential with mean I/A, that is, they have density 
</p>
<p>f(s) = Ae-'\s, s:;:, o. 
</p>
<p>The waiting time Wn rv Gamma(n, I/A) i.e., it has density 
</p>
<p>PROOF. First, we have 
</p>
<p>lP'(Sl &gt; t) = lP'(X(t) = 0) = e-.\t 
</p>
<p>with shows that the CDF for Sl is 1 - e-.\t. This shows the result for Sl. Now, 
</p>
<p>lP'(no events in (8,8 + tllSl = s) 
</p>
<p>lP'(no events in (8,8 + t]) (increments are independent) 
-.\t e . 
</p>
<p>Hence, S2 has an exponential distribution and is independent of Sl. The result 
</p>
<p>follows by repeating the argument. The result for Wn follows since a sum of 
</p>
<p>exponentials has a Gamma distribution. _ 
</p>
<p>23.36 Example. Figure 23.3 shows requests to a WWW server in Calgary. 1 
</p>
<p>Assuming that this is a homogeneous Poisson process, N == X(T) rv Poisson(AT). 
</p>
<p>The likelihood is 
</p>
<p>lSee http://ita.ee.lbl.gov/html/contrib/Calgary-HTTP.html for more information. </p>
<p/>
</div>
<div class="page"><p/>
<p>23.4 Bibliographic Remarks 397 
</p>
<p>1200 
</p>
<p>FIGURE 23.3. Hits on a web server. Each vertical line represents one event. 
</p>
<p>which is maximized at 
</p>
<p>- N 
A = - = 48 0077 T . 
</p>
<p>in units per minute. Let's now test the assumption that the data follow a ho-
</p>
<p>mogeneous Poisson process using a goodness-of-fit test. We divide the interval 
</p>
<p>[0, T] into 4 equal length intervals h, h, 13 , 14 . If the process is a homogeneous 
</p>
<p>Poisson process then, given the total number of events, the probability that an 
</p>
<p>event falls into any of these intervals must be equal. Let Pi be the probability 
</p>
<p>of a point being in h The null hypothesis is that PI = P2 = P3 = P4 = 1/4. 
We can test this hypothesis using either a likelihood ratio test or a X2 test. 
</p>
<p>The latter is 
</p>
<p>where Oi is the number of observations in Ii and Ei = n/4 is the expected 
</p>
<p>number under the null. This yields X2 = 252 with a p-value near O. This is 
</p>
<p>strong evidence against the null so we reject the hypothesis that the data are 
</p>
<p>from a homogeneous Poisson process. This is hardly surprising since we would 
</p>
<p>expect the intensity to vary as a function of time. _ 
</p>
<p>23.4 Bibliographic Rernarks 
</p>
<p>This is standard material and there are many good references including Grim-
</p>
<p>mett and Stirzaker (1982), Taylor and Karlin (1994), Guttorp (1995), and 
</p>
<p>Ross (2002). The following exercises are from those texts. </p>
<p/>
</div>
<div class="page"><p/>
<p>398 23. Probability Redux: Stochastic Processes 
</p>
<p>23.5 Exercises 
</p>
<p>1. Let X o, Xl, ... be a Markov chain with states {O, 1, 2} and transition 
</p>
<p>matrix 
</p>
<p>[ 
0.1 0.2 0.7] 
</p>
<p>P = 0.9 0.1 0.0 
</p>
<p>0.1 0.8 0.1 
</p>
<p>Assume that flo = (0.3,0.4,0.3). Find IP'(Xo = 0, Xl = 1, X 2 = 2) and 
</p>
<p>IP'(Xo = 0, Xl = 1, X 2 = 1). 
</p>
<p>2. Let Yl , Y2 , ... be a sequence of iid observations such that IP'(Y = 0) = 
</p>
<p>0.1, IP'(Y = 1) = 0.3, IP'(Y = 2) = 0.2, IP'(Y = 3) = 0.4. Let Xo = 0 and 
</p>
<p>let 
</p>
<p>Show that X o, Xl, ... is a Markov chain and find the transition matrix. 
</p>
<p>3. Consider a two-state Markov chain with states X = {I, 2} and transition 
</p>
<p>matrix 
</p>
<p>[ 
1 - a 
</p>
<p>p= 
b 
</p>
<p>a 
1-b 
</p>
<p>where 0 &lt; a &lt; 1 and 0 &lt; b &lt; 1. Prove that 
</p>
<p>lim pn = [ atb 
n---+oo a+b 
</p>
<p>atb ] &bull; 
a+b 
</p>
<p>4. Consider the chain from question 3 and set a = .1 and b = .3. Simulate 
</p>
<p>the chain. Let 
</p>
<p>1 n 
- LI(Xi = 1) 
n 
</p>
<p>i=l 
</p>
<p>Pn(2) 
1 n 
- LI(Xi = 2) 
n 
</p>
<p>i=l 
</p>
<p>be the proportion of times the chain is in state 1 and state 2. Plot Pn (1) 
</p>
<p>and Pn(2) versus n and verify that they converge to the values predicted 
</p>
<p>from the answer in the previous question. 
</p>
<p>5. An important Markov chain is the branching process which is used in 
</p>
<p>biology, genetics, nuclear physics, and many other fields. Suppose that 
</p>
<p>an animal has Y children. Let Pk = IP'(Y = k). Hence, Pk ~ 0 for all 
</p>
<p>k and L~=o Pk = 1. Assume each animal has the same lifespan and </p>
<p/>
</div>
<div class="page"><p/>
<p>23.5 Exercises 399 
</p>
<p>that they produce offspring according to the distribution Pk. Let Xn be 
</p>
<p>the number of animals in the nth generation. Let yI(n) , ... ,yl:) be the 
</p>
<p>offspring produced in the nth generation. Note that 
</p>
<p>x _ y(n) + ... + y(n) 
n+I - 1 X n ' 
</p>
<p>Let fL = JE(Y) and (j2 = V(Y). Assume throughout this question that 
</p>
<p>Xo = 1. Let M(n) = JE(Xn) and V(n) = V(Xn). 
</p>
<p>(a) Show that M(n + 1) = ILM(n) and V(n + 1) = (j2 M(n) + fL 2V(n). 
</p>
<p>(b) Show that M(n) = fLn and that V(n) = (j2 fLn- I (1 + fL + ... + fLn-I). 
</p>
<p>(c) What happens to the variance if fL &gt; 1? What happens to the vari-
</p>
<p>ance if fL = 1? What happens to the variance if fL &lt; 1? 
</p>
<p>(d) The population goes extinct if Xn = 0 for some n. Let us thus define 
</p>
<p>the extinction time N by 
</p>
<p>N = min {n: Xn = O}. 
</p>
<p>Let F(n) = JPl(N ::; n) be the CDP of the random variable N. Show that 
</p>
<p>(Xl 
</p>
<p>F(n) = LPk(F(n - 1))\ n = 1,2, ... 
k=O 
</p>
<p>Hint: Note that the event {N ::; n} is the same as event {X n = O}. 
</p>
<p>Thus, JPl( {N ::; n}) = JPl( {Xn = O}). Let k be the number of offspring 
</p>
<p>of the original parent. The population becomes extinct at time n if and 
</p>
<p>only if each of the k sub-populations generated from the k offspring goes 
</p>
<p>extinct in n - 1 generations. 
</p>
<p>(e) Suppose that Po = 1/4, PI = 1/2, P2 = 1/4. Use the formula from 
</p>
<p>(5d) to compute the CDF F(n). 
</p>
<p>6. Let 
</p>
<p>[ 
0.40 0.50 
</p>
<p>P = 0.05 0.70 
</p>
<p>0.05 0.50 
</p>
<p>Find the stationary distribution Jr. 
</p>
<p>0.10 1 
0.25 
</p>
<p>0.45 
</p>
<p>7. Show that if i is a recurrent state and i ++ j, then j is a recurrent state. </p>
<p/>
</div>
<div class="page"><p/>
<p>400 23. Probability Redux: Stochastic Processes 
</p>
<p>8. Let 
1 0 1 0 0 1 
</p>
<p>1 1 "3 1 0 0 0 
2 "4 4 
</p>
<p>p= 
0 0 0 0 1 0 
1 1 1 0 0 1 "4 "4 "4 "4 
0 0 1 0 0 0 
</p>
<p>0 0 0 0 0 1 
</p>
<p>Which states are transient? Which states are recurrent? 
</p>
<p>9. Let 
</p>
<p>p = [ ~ ~ ] 
Show that 'if = (1/2,1/2) is a stationary distribution. Does this chain 
</p>
<p>converge? Why/why not? 
</p>
<p>10. Let 0 &lt; p &lt; 1 and q = 1 - p. Let 
</p>
<p>p ~ r: 
p 0 0 
</p>
<p>~ 1 0 p 0 0 0 p 
l i 
</p>
<p>0 0 0 
</p>
<p>6 J 0 0 0 
Find the limiting distribution of the chain. 
</p>
<p>11. Let X(t) be an inhomogeneous Poisson process with intensity function 
</p>
<p>A(t) &gt; o. Let A(t) = J~ A(u)du. Define Y(8) = X(t) where 8 = A(t). 
Show that Y(8) is a homogeneous Poisson process with intensity A = l. 
</p>
<p>12. Let X(t) be a Poisson process with intensity A. Find the conditional 
</p>
<p>distribution of X(t) given that X(t + 8) = r1. 
</p>
<p>13. Let X(t) be a Poisson process with intensity A. Find the probability 
</p>
<p>that X(t) is odd, i.e. J1D(X(t) = 1,3,5, ... ). 
</p>
<p>14. Suppose that people logging in to the University computer system is 
</p>
<p>described by a Poisson process X(t) with intensity A. Assume that a 
</p>
<p>person stays logged in for some random time with CDF G. Assume these 
</p>
<p>times are all independent. Let Y(t) be the number of people on the 
</p>
<p>system at time t. Find the distribution of Y(t). 
</p>
<p>15. Let X(t) be a Poisson process with intensity A. Let WI, W2 , ... , be the 
</p>
<p>waiting times. Let f be an arbitrary function. Show that 
</p>
<p>(
</p>
<p>X(t) ) .t 
</p>
<p>IE 8 f(Wi ) = A 10 f(w)dw. </p>
<p/>
</div>
<div class="page"><p/>
<p>23.5 Exercises 401 
</p>
<p>16. A two-dimensional Poisson point process is a process of random points 
</p>
<p>on the plane such that (i) for any set A, the number of points falling 
</p>
<p>in A is Poisson with mean AIL(A) where IL(A) is the area of A, (ii) the 
</p>
<p>number of events in non-overlapping regions is independent. Consider 
</p>
<p>an arbitrary point Xo in the plane. Let X denote the distance from Xo 
</p>
<p>to the nearest random point. Show that 
</p>
<p>and 
1 
</p>
<p>JE(X) = /\. 
2VA </p>
<p/>
</div>
<div class="page"><p/>
<p>24 
</p>
<p>Simulation Methods 
</p>
<p>In this chapter we will show how simulation can be used to approximate inte-
</p>
<p>grals. Our leading example is the problem of computing integrals in Bayesian 
</p>
<p>inference but the techniques are widely applicable. We will look at three inte-
</p>
<p>gration methods: (i) basic Monte Carlo integration, (ii) importance sampling, 
</p>
<p>and (iii) Markov chain Monte Carlo (MCMC). 
</p>
<p>24.1 Bayesian Inference Revisited 
</p>
<p>Simulation methods are especially useful in Bayesian inference so let us briefly 
</p>
<p>review the main ideas in Bayesian inference. See Chapter 11 for more details. 
</p>
<p>Given a prior 1(8) and data xn = (Xl, ... ,Xn) the posterior density is 
</p>
<p>where &pound;(8) is the likelihood function and 
</p>
<p>c = / &pound;(8)1(8) d8 
</p>
<p>is the normalizing constant. The posterior mean is </p>
<p/>
</div>
<div class="page"><p/>
<p>404 24. Simulation Methods 
</p>
<p>If 8 = (81 , ... , 8k) is multidimensional, then we might be interested in the 
</p>
<p>posterior for one of the components, 81 , say. This marginal posterior density 
</p>
<p>is 
</p>
<p>which involves high-dimensional integration. 
</p>
<p>When 8 is high-dimensional, it may not be feasible to calculate these inte-
</p>
<p>grals analytically. Simulation methods will often be helpful. 
</p>
<p>24.2 Basic Monte Carlo Integration 
</p>
<p>Suppose we want to evaluate the integral 
</p>
<p>b 1= 1 h(x)dx 
for some function h. If h is an "easy" function like a polynomial or trigono-
</p>
<p>metric function, then we can do the integral in closed form. If h is complicated 
</p>
<p>there may be no known closed form expression for I. There are many numer-
</p>
<p>ical techniques for evaluating I such as Simpson's rule, the trapezoidal rule 
</p>
<p>and Gaussian quadrature. Monte Carlo integration is another approach for 
</p>
<p>approximating I which is notable for its simplicity, generality and scalability. 
</p>
<p>Let us begin by writing 
</p>
<p>b b 1=1 h(x)dx = 1 w(x)f(x)dx (24.1 ) 
where w(x) = h(x)(b-a) and f(x) = l/(b-a). Notice that f is the probability 
</p>
<p>density for a uniform random variable over (a, b). Hence, 
</p>
<p>where X rv Unif(a, b). If we generate Xl, ... ,XN rv Unif(a, b), then by the 
</p>
<p>law of large numbers 
</p>
<p>N 
~ 1,,", p 
1== N ~ W(Xi)--+lE(W(X)) = I. (24.2) 
</p>
<p>i=l 
</p>
<p>This is the basic Monte Carlo integration method. We can also compute 
</p>
<p>the standard error of the estimate 
</p>
<p>~ S 
se= --
</p>
<p>VN </p>
<p/>
</div>
<div class="page"><p/>
<p>24.2 Basic Monte Carlo Integration 405 
</p>
<p>where 
</p>
<p>2 ~!l (Yi - J)2 
s = ==-'---------'.-
</p>
<p>N -1 
</p>
<p>where Yi = W(Xi)' A 1- ex confidence interval for I is J &plusmn; Za/25e. We can take 
N as large as we want and hence make the length of the confidence interval 
</p>
<p>very small. 
</p>
<p>24.1 Example. Let h(x) = x 3 . Then, I = J~l x3 dx = 1/4. Based on N = 
</p>
<p>10,000 observations from a Uniform(O,l) we get J = .248 with a standard 
error of .0028 .&bull; 
</p>
<p>A generalization of the basic method is to consider integrals of the form 
</p>
<p>1= J h(x)f(x)dx (24.3) 
where f(x) is a probability density function. Taking f to be a Uniform (a,b) 
</p>
<p>gives us the special case above. Now we draw Xl"'" X N rv f and take 
</p>
<p>as before. 
</p>
<p>24.2 Example. Let 
1 2 , 
</p>
<p>f(x) = _e- x /2 
yI27T 
</p>
<p>be the standard Normal PDF. Suppose we want to compute the CDF at some 
</p>
<p>point x: 
</p>
<p>Write 
</p>
<p>where 
</p>
<p>I = L~ f(s)ds = &lt;p(x). 
</p>
<p>1= J h(s)f(s)ds 
</p>
<p>h(s) = { ~ s&lt;x 
s:;:, x. 
</p>
<p>Now we generate Xl"'" X N rv N(O, 1) and set 
</p>
<p>~ _ 1 ~ ( ) _ number of observations ::; x 
1- N ~h Xi - N 
</p>
<p>For example, with x 2, the true answer is &lt;p(2) = .9772 and the Monte 
</p>
<p>Carlo estimate with N = 10,000 yields .9751. Using N = 100,000 we get 
</p>
<p>.9771. &bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>406 24. Simulation Methods 
</p>
<p>24.3 Example (Bayesian Inference for Two Binomials). Let X rv Binomial(n,pd 
</p>
<p>and Y rv Binomial(m,P2)' We would like to estimate 5 = P2 - Pl. The MLE 
</p>
<p>is 5' = P2 - PI = (Y/m) - (X/n). We can get the standard error se using the 
delta method which yields 
</p>
<p>and then construct a 95 percent confidence interval 5' &plusmn; 2 se. Now consider a 
Bayesian analysis. Suppose we use the prior f(P1,P2) = f(pdf(p2) = 1, that 
</p>
<p>is, a flat prior on (PI, P2). The posterior is 
</p>
<p>The posterior mean of 5 is 
</p>
<p>If we want the posterior density of 5 we can first get the posterior CDF 
</p>
<p>F(cIX, Y) = P(5 &lt;::: clX, Y) = i f(PI,P2IX, Y) 
where A = {(p1,P2): P2 - PI &lt;::: c}. The density can then be obtained by 
</p>
<p>differentiating F. 
</p>
<p>To avoid all these integrals, let's use simulation. Note that f(p1,P2IX, Y) = 
</p>
<p>f(PIIX)f(P21Y) which implies that PI and P2 are independent under the pos-
</p>
<p>terior distribution. Also, we see that PI I X rv Beta( X + 1, n - X + 1) and P21 Y rv 
B . (Y Y) H .' 1 (pC1) p,(1)) (p(N) p,CN)) eta + 1, m. - + 1. ence, we can Slmu ate l' 2 , ... , 1 , 2 
from the posterior by drawing 
</p>
<p>p~i) Beta(X + 1, n - X + 1) 
</p>
<p>pJi) Beta(Y + 1, m - Y + 1) 
</p>
<p>&pound; : - 1 N NIt sCi) - p,(i) _ pCi) Th lor t - , ... , . ow e u - 2 l' en, 
</p>
<p>We can also get a 95 percent posterior interval for 5 by sorting the simulated 
</p>
<p>values, and finding the .025 and .975 quantile. The posterior density f( 51X, Y) 
</p>
<p>can be obtained by applying density estimation techniques to 5(1 ), ... ,5CN) 
</p>
<p>or, simply by plotting a histogram. For example, suppose that n = m = 10, </p>
<p/>
</div>
<div class="page"><p/>
<p>24.2 Basic Monte Carlo Integration 407 
</p>
<p>FIGURE 24.1. Posterior of &lt;5 from simulation. 
</p>
<p>x = 8 and Y = 6. From a posterior sample of size 1000 we get a 95 percent 
posterior interval of (-0.52,0.20). The posterior density can be estimated from 
</p>
<p>a histogram of the simulated values as shown in Figure 24.1. &bull; 
</p>
<p>24.4 Example (Bayesian Inference for Dose Response). Suppose we conduct an 
</p>
<p>experiment by giving rats one of ten possible doses of a drug, denoted by 
</p>
<p>Xl &lt; X2 &lt; ... &lt; XlO. For each dose level Xi we use n rats and we observe 
Yi, the number that survive. Thus we have ten independent binomials Yi rv 
</p>
<p>Binomial( n, Pi). Suppose we know from biological considerations that higher 
</p>
<p>doses should have higher probability of death. Thus, PI &lt;::: P2 &lt;::: ... &lt;::: PlO' We 
</p>
<p>want to estimate the dose at which the animals have a 50 percent chance of 
</p>
<p>dying. This is called the LD50. Formally, 6 = Xj where 
</p>
<p>j = min{i: Pi ~ .50}. 
</p>
<p>Notice that 6 is implicitly a (complicated) function of PI, ... ,PlO so we can 
</p>
<p>write 6 = 9 (PI, ... ,PlO) for some g. This just means that if we know (PI, ... ,PlO) 
</p>
<p>then we can find 6. The posterior mean of 6 is 
</p>
<p>J J ... i g(PI, ... ,Plo)f(PI,'" ,PIOiYI,"" Y lO )dpl dp2 ... dplO' 
The integral is over the region 
</p>
<p>The posterior CDF of 6 is 
</p>
<p>JP'(6 &lt;::: eIYI , ... , Y lO ) 
</p>
<p>J J"'l f(PI,'" ,PlOiYI,"" Y lO )dpIdp2 ... dplO </p>
<p/>
</div>
<div class="page"><p/>
<p>408 24. Simulation Methods 
</p>
<p>where 
</p>
<p>B=An{(Pl, ... ,PI0): g(Pl,&middot;&middot;&middot;,PlO) -s:c}. 
</p>
<p>We need to do a 10-dimensional integral over a restricted region A. Instead, 
</p>
<p>we will use simulation. Let us take a flat prior truncated over A. Except for 
</p>
<p>the truncation, each Pi has once again a Beta distribution. To draw from the 
</p>
<p>posterior we do the following steps: 
</p>
<p>(1) Draw Pi rv Beta(Yi + 1, n - Yi + 1), i = 1, ... ,10. 
(2) If PI -s: P2 -s: ... -s: P lO keep this draw. Otherwise, throw it away and 
</p>
<p>draw again until you get one you can keep. 
</p>
<p>(3) Let 5 = Xj where 
</p>
<p>j = min{i: Pi&gt; .50}. 
</p>
<p>We repeat this N times to get 5(1), ... ,5(N) and take 
</p>
<p>5 is a discrete variable. We can estimate its probability mass function by 
</p>
<p>N 
</p>
<p>lTll(' _ .IY Y ) rv ~ "'I(-(i) - .) 
Jr U - X J 1,&middot;&middot;&middot;, 10 rv N ~ 6 - J . 
</p>
<p>i=l 
</p>
<p>For example, consider the following data: 
</p>
<p>Dose 1 2 3 4 5 6 7 8 9 10 
</p>
<p>Number of animals ni 15 15 15 15 15 15 15 15 15 15 
</p>
<p>Number of survivors Yi 0 0 2 2 8 10 12 14 15 14 
</p>
<p>The posterior draws for PI, . .. , PI0 are shown in the second panel in the 
</p>
<p>figure. We find that that "5 = 4.04 with a 95 percent interval of (3,5) .&bull; 
</p>
<p>24.3 Irnportance Sarnpling 
</p>
<p>Consider again the integral I = J h(x)f(x)dx where f is a probability density. 
The basic Monte Carlo method involves sampling from f. However, there 
are cases where we may not know how to sample from f. For example, in 
Bayesian inference, the posterior density density is is obtained by multiplying 
</p>
<p>the likelihood &pound;(8) times the prior f(8). There is no guarantee that f(8Ix) 
</p>
<p>will be a known distribution like a Normal or Gamma or whatever. </p>
<p/>
</div>
<div class="page"><p/>
<p>24.3 Importance Sampling 409 
</p>
<p>Importance sampling is a generalization of basic Monte Carlo which over-
</p>
<p>comes this problem. Let 9 be a probability density that we know how to 
</p>
<p>simulate from. Then 
</p>
<p>j. j' h(x)f(x) I = h(x)f(x)dx = g(x) g(x)dx = IEg(Y) (24.4) 
</p>
<p>where Y = h(X)f(X)/g(X) and the expectation IEg(Y) is with respect to g. 
</p>
<p>We can simulate Xl, ... , X N rv 9 and estimate I by 
</p>
<p>(24.5) 
</p>
<p>This is called importance sampling. By the law of large numbers, f ~ I. 
However, there is a catch. It's possible that I might have an infinite standard 
</p>
<p>error. To see why, recall that I is the mean of w(x) = h(x)f(x)/g(x). The 
</p>
<p>second moment of this quantity is 
</p>
<p>IE ( 2(X)) = j' (h(X)f(X))2 ( )d = j' h2(x)P(x)d 
9 w ( ) 9 x x () x. 
</p>
<p>9 x 9 x 
(24.6) 
</p>
<p>If 9 has thinner tails than f, then this integral might be infinite. To avoid this, 
a basic rule in importance sampling is to sample from a density 9 with thicker 
</p>
<p>tails than f. Also, suppose that g(x) is small over some set A where f(x) is 
</p>
<p>large. Again, the ratio of fig could be large leading to a large variance. This 
</p>
<p>implies that we should choose 9 to be similar in shape to f. In summary, a 
good choice for an importance sampling density 9 should be similar to f but 
with thicker tails. In fact, we can say what the optimal choice of 9 is. 
</p>
<p>24.5 Theorem. The choice of 9 that minimizes the variance of I is 
</p>
<p>* Ih(x)lf(x) 
9 (x) = I Ih(s)lf(s)ds' 
</p>
<p>PROOF. The variance of w = fhlg is 
</p>
<p>J w2(x)g(x)dx - (J w(X)9(X)dX) 2 
J h2(X)P(X) (')d - (J h(x)f(x) (')d )2 2() 9 X X () 9 x x 
</p>
<p>9 x 9 x 
</p>
<p>J h2~1~~(X) g(x)dx - (J h(X)f(X)dX) 2 </p>
<p/>
</div>
<div class="page"><p/>
<p>410 24. Simulation Methods 
</p>
<p>The second integral does not depend on g, so we only need to minimize the 
</p>
<p>first integral. From Jensen's inequality (Theorem 4.9) we have 
</p>
<p>This establishes a lower bound on lEg (W2). However, lEg * (W2) equals this 
</p>
<p>lower bound which proves the claim. _ 
</p>
<p>This theorem is interesting but it is only of theoretical interest. If we did 
</p>
<p>not know how to sample from f then it is unlikely that we could sample from 
</p>
<p>Ih(x)lf(x)/ J Ih(s)lf(s)ds. In practice, we simply try to find a thick-tailed 
distribution g which is similar to flhl. 
</p>
<p>24.6 Example (Tail Probability). Let's estimate I = IP'(Z &gt; 3) = .0013 where 
</p>
<p>Z rv N(O,l). Write I = J h(x)f(x)dx where f(x) is the standard Normal 
density and h(x) = 1 if x &gt; 3, and 0 otherwise. The basic Monte Carlo 
</p>
<p>estimator is 1 = N- 1 2: i h(Xi ) where Xl, ... , X N rv N(O, 1). Using N = 100 
we find (from simulating many times) that lE(1) = .0015 and V(1) = .0039. 
</p>
<p>Notice that most observations are wasted in the sense that most are not near 
</p>
<p>the right tail. Now we will estimate this with importance sampling taking g 
</p>
<p>to be a Normal(4,1) density. We draw values from g and the estimate is now 
</p>
<p>1 = N- l 2:i f(Xi)h(Xi)/g(Xi ). In this case we find that lE(1) = .0011 and 
</p>
<p>V(1) = .0002. We have reduced the standard deviation by a factor of 20. _ 
</p>
<p>24.7 Example (Measurement Model With Outliers). Suppose we have measure-
</p>
<p>ments Xl, . .. , Xn of some physical quantity B. A reasonable model is 
</p>
<p>If we assume that Ei rv N (0, 1) then Xi rv N ( B i, 1). However, when taking 
</p>
<p>measurements, it is often the case that we get the occasional wild observation, 
</p>
<p>or outlier. This suggests that a Normal might be a poor model since Normals 
</p>
<p>have thin tails which implies that extreme observations are rare. One way to 
</p>
<p>improve the model is to use a density for Ei with a thicker tail, for example, 
</p>
<p>a t-distribution with v degrees of freedom which has the form 
</p>
<p>r e'!l) 1 ( X2) -(u+1)/2 
t(x) = - 1 +-
</p>
<p>r (~) VIr V 
</p>
<p>Smaller values of v correspond to thicker tails. For the sake of illustration we 
</p>
<p>will take v = 3. Suppose we observe n Xi = B + Ei, 'i = 1, ... ,n where Ei has </p>
<p/>
</div>
<div class="page"><p/>
<p>24.4 MCMC Part I: The Metropolis-Hastings Algorithm 411 
</p>
<p>a t distribution with v = 3. We will take a flat prior on e. The likelihood is 
&pound;(e) = TI~=l t(Xi - e) and the posterior mean of e is 
</p>
<p>- Ie&pound;(e)de 
e = I &pound;(e)de . 
</p>
<p>We can estimate the top and bottom integral using importance sampling. We 
</p>
<p>draw el , ... , eN rv 9 and then 
1 ",N ej&pound;(ej ) 
!Ii~j=l~ 
</p>
<p>1 ",N &pound;(e j ) &bull; 
!Ii ~j=l g(e j ) 
</p>
<p>To illustrate the idea, we drew n = 2 observations. The posterior mean (com-
</p>
<p>puted numerically) is -0.54. Using a Normal importance sampler 9 yields an 
</p>
<p>estimate of -0.74. Using a Cauchy (t-distribution with 1 degree of freedom) 
</p>
<p>importance sampler yields an estimate of -0.53 .&bull; 
</p>
<p>24.4 MCMC Part I: The Metropolis-Hastings 
</p>
<p>Algorithrn 
</p>
<p>Consider once more the problem of estimating the integral I = I h(x)f(x)dx. 
Now we introduce Markov chain Monte Carlo (MCMC) methods. The idea is 
</p>
<p>to construct a Markov chain Xl, X 2 , ... , whose stationary distribution is f. 
Under certain conditions it will then follow that 
</p>
<p>1 N 
N L h(Xi)~ IEf(h(X)) = I. 
</p>
<p>i=l 
</p>
<p>This works because there is a law of large numbers for Markov chains; see 
</p>
<p>Theorem 23.25. 
</p>
<p>The Metropolis-Hastings algorithm is a specific MCMC method that 
</p>
<p>works as follows. Let q(ylx) be an arbitrary, friendly distribution (i.e., we 
</p>
<p>know how to sample from q(ylx)). The conditional density q(Ylx) is called 
</p>
<p>the proposal distribution. The Metropolis-Hastings algorithm creates a 
</p>
<p>sequence of observations X o, Xl, ... , as follows. 
</p>
<p>Metropolis-Hastings Algorithm 
</p>
<p>Choose X o arbitrarily. Suppose we have generated XO,Xl , ... ,Xi. To 
</p>
<p>generate Xi+l do the following: 
</p>
<p>(1) Generate a proposal or candidate value Y rv q(yIXi ). </p>
<p/>
</div>
<div class="page"><p/>
<p>412 24. Simulation Methods 
</p>
<p>(2) Evaluate T == T(Xi' Y) where 
</p>
<p>(3) Set 
</p>
<p>. {f(Y) q(xIY) } 
T(X, y) = mm f(x) q(ylx)' 1 . 
</p>
<p>with probability T 
</p>
<p>with probability 1 - T. 
</p>
<p>24.8 Remark. A simple way to execute step (3) is to generate U rv (0,1). If 
</p>
<p>U &lt; T set Xi+1 = Y otherwise set Xi+1 = Xi. 
</p>
<p>24.9 Remark. A common choice for q(ylx) is N(x, b2 ) for some b &gt; O. This 
</p>
<p>means that the proposal is draw from a Normal, centered at the current 
</p>
<p>value. In this case, the proposal density q is symmetric, q(Ylx) = q(xly), and 
</p>
<p>T simplifies to 
</p>
<p>. {f(Y) } 
T=mm f(X
</p>
<p>i
) , 1 . 
</p>
<p>By construction, X o, Xl, . .. is a Markov chain. But why does this Markov 
</p>
<p>chain have f as its stationary distrihution? Before we explain why, let us first 
do an example. 
</p>
<p>24.10 Example. The Cauchy distribution has density 
</p>
<p>1 1 
f(x) = ---2&middot; 
</p>
<p>1fl+x 
</p>
<p>Our goal is to simulate a Markov chain whose stationary distribution is f. 
As suggested in the remark above, we take q(ylx) to be a N(x, b2 ). So in this 
</p>
<p>case, 
</p>
<p>T(x,y)=min{j~~~, 1}=min{~:~:, I}. 
So the algorithm is to draw Y rv N(Xi' b2 ) and set 
</p>
<p>x _ {Y with probability T(Xi' Y) 
,+1 - Xi with probability 1 - T(Xi' Y). 
</p>
<p>The simulator requires a choice of b. Figure 24.2 shows three chains of length 
</p>
<p>N = 1,000 using b = .1, b = 1 and b = 10. Setting b = .1 forces the chain 
</p>
<p>to take small steps. As a result, the chain doesn't "explore" much of the 
</p>
<p>sample space. The histogram from the sample does not approximate the true 
</p>
<p>density very well. Setting b = 10 causes the proposals to often be far in the </p>
<p/>
</div>
<div class="page"><p/>
<p>24.4 MCMC Part I: The Metropolis-Hastings Algorithm 413 
</p>
<p>FIGURE 24.2. Three Metropolis chains corresponding to b = .1, b = 1, b = 10. 
</p>
<p>tails, making r small and hence we reject the proposal and keep the chain 
</p>
<p>at its current position. The result is that the chain "gets stuck" at the same 
</p>
<p>place quite often. Again, this meanS that the histogram from the sample does 
</p>
<p>not approximate the true density very well. The middle choice avoids these 
</p>
<p>extremes and results in a Markov chain sample that better represents the 
</p>
<p>density SOOner. In summary, there are tuning parameters and the efficiency 
</p>
<p>of the chain depends On these parameters. We'll discuss this in more detail 
</p>
<p>later. _ 
</p>
<p>If the sample from the Markov chain starts to "look like" the target distri-
</p>
<p>bution f quickly, then we say that the chain is "mixing well." Constructing a 
chain that mixes well is somewhat of an art. 
</p>
<p>WHY IT WORKS. Recall from Chapter 23 that a distribution 7r satisfies 
</p>
<p>detailed balance for a Markov chain if 
</p>
<p>We showed that if 7r satisfies detailed balance, then it is a stationary distri-
</p>
<p>bution for the chain. 
</p>
<p>Because we are nOw dealing with continuous state Markov chains, we will 
</p>
<p>change notation a little and write p(x, y) for the probability of making a 
</p>
<p>transition from x to y. Also, let's use f(x) instead of 7r for a distribution. In </p>
<p/>
</div>
<div class="page"><p/>
<p>414 24. Simulation Methods 
</p>
<p>this new notation, f is a stationary distribution if f(x) = J f(y)p(y, x)dy and 
detailed balance holds for f if 
</p>
<p>f(x)p(x, y) = f(y)p(y, x). (24.7) 
</p>
<p>Detailed balance implies that f is a stationary distribution since, if detailed 
balance holds, then 
</p>
<p>/ f(y)p(y, x)dy = / f(x)p(x, y)dy = f(x) / p(x, y)dy = f(x) 
</p>
<p>which shows that f(x) = J f(y)p(y, x)dy as required. Our goal is to show that 
f satisfies detailed balance which will imply that f is a stationary distribution 
for the chain. 
</p>
<p>Consider two points x and y. Either 
</p>
<p>f(x)q(ylx) &lt; f(y)q(xly) or f(x)q(ylx) &gt; f(y)q(xly)&middot; 
</p>
<p>We will ignore ties (which occur with probability zero for continuous distribu-
</p>
<p>tions). Without loss of generality, assume that f(x)q(ylx) &gt; f(y)q(xly). This 
implies that 
</p>
<p>f(y) q(xly) 
r(x, y) = f(x) q(ylx) 
</p>
<p>and that r(y, x) = 1. Now p(x, y) is the probability of jumping from x to y. 
</p>
<p>This requires two things: (i) the proposal distribution must generate y, and 
</p>
<p>(ii) you must accept y. Thus, 
</p>
<p>f(y) q(xly) f(y) 
p(x, y) = q(ylx)r(x, y) = q(Ylx) f(x) q(ylx) = f(x) q(xly)&middot; 
</p>
<p>Therefore, 
</p>
<p>f(x)p(x, y) = f(y)q(xIY)&middot; (24.8) 
</p>
<p>On the other hand, p(y, x) is the probability of jumping from y to x. This 
</p>
<p>requires two things: (i) the proposal distribution must generate x, and (ii) you 
</p>
<p>must accept x. This occurs with probability p(y, x) = q(xly)r(y, x) = q(xly). 
</p>
<p>Hence, 
</p>
<p>f(y)p(y, x) = f(y)q(xly)&middot; (24.9) 
</p>
<p>Comparing (24.8) and (24.9), we see that we have shown that detailed balance 
</p>
<p>holds. </p>
<p/>
</div>
<div class="page"><p/>
<p>24.5 MCMC Part II: Different Flavors 415 
</p>
<p>24.5 MCMC Part II: Different Flavors 
</p>
<p>There are different types of MeMe algorithm. Here we will consider a few of 
</p>
<p>the most popular versions. 
</p>
<p>RANDOM-WALK-METROPOLIS-HASTINGS. In the previous section we con-
</p>
<p>sidered drawing a proposal Y of the form 
</p>
<p>where fi comes from some distribution with density g. In other words, q(ylx) = 
</p>
<p>g(y - x). We saw that in this case, 
</p>
<p>. { f(Y)} r(x, y) = mm 1, f(x) . 
</p>
<p>This is called a random-walk-Metropolis-Hastings method. The reason 
</p>
<p>for the name is that, if we did not do the accept-reject step, we would be 
</p>
<p>simulating a random walk. The most common choice for 9 is a N(O, b2 ). The 
</p>
<p>hard part is choosing b so that the chain mixes well. A good rule of thumb is: 
</p>
<p>choose b so that you accept the proposals about 50 percent of the time. 
</p>
<p>Warning! This method doesn't make sense unless X takes values on the 
</p>
<p>whole real line. If X is restricted to some interval then it is best to transform 
</p>
<p>X. For example, if X E (0, (0) then you might take Y = logX and then 
</p>
<p>simulate the distribution for Y instead of X. 
</p>
<p>INDEPENDENcE-METRoPoLIs-HASTINGs. This is an importance-sampling 
</p>
<p>version of MeMe. We draw the proposal from a fixed distribution g. Gen-
</p>
<p>erally, 9 is chosen to be an approximation to f. The acceptance probability 
becomes 
</p>
<p>. { f(y) g(x)} 
r(x, y) = mm 1, f(x) g(y) . 
</p>
<p>GIBBS SAMPLING. The two previous methods can be easily adapted, in 
</p>
<p>principle, to work in higher dimensions. In practice, tuning the chains to make 
</p>
<p>them mix well is hard. Gibbs sampling is a way to turn a high-dimensional 
</p>
<p>problem into several one-dimensional problems. 
</p>
<p>Here's how it works for a bivariate problem. Suppose that (X, Y) has den-
</p>
<p>sity fx,Y(x, y). First, suppose that it is possible to simulate from the condi-
</p>
<p>tional distributions fXIY(xly) and jylx(ylx). Let (Xo, Yo) be starting values. 
</p>
<p>Assume we have drawn (Xo, Yo), ... , (Xn' Yn). Then the Gibbs sampling al-
</p>
<p>gorithm for getting (Xn+l' Yn+d is: </p>
<p/>
</div>
<div class="page"><p/>
<p>416 24. Simulation Methods 
</p>
<p>Gibbs Sampling 
</p>
<p>X n +1 
</p>
<p>Yn+l 
</p>
<p>repeat 
</p>
<p>!XIY(xlYn) 
</p>
<p>jylx(yIXn+d 
</p>
<p>This generalizes in the obvious way to higher dimensions. 
</p>
<p>24.11 Example (Normal Hierarchical Model). Gibbs sampling is very useful 
</p>
<p>for a class of models called hierarchical models. Here is a simple case. 
</p>
<p>Suppose we draw a sample of k cities. From each city we draw ni people and 
</p>
<p>observe how many people Y; have a disease. Thus, Y; rv Binomial(ni, Pi). We 
</p>
<p>are allowing for different disease rates in different cities. We can also think of 
</p>
<p>the p~s as random draws from some distribution F. We can write this model 
</p>
<p>in the following way: 
</p>
<p>F 
</p>
<p>Binomial( ni, Pi). 
</p>
<p>We are interested in estimating the p~s and the overall disease rate J pdF(p). 
To proceed, it will simplify matters if we make some transformations that 
</p>
<p>allow us to use some Normal approximations. Let Pi = Yi/ni. Recall that 
</p>
<p>Pi ~ N(Pi' Si) where Si = VPi(l - Pi)/ni. Let 7/Ji = log(p;j(l - Pi)) and 
</p>
<p>define Zi == ;(j;i = 10g(iJi / (1 - Pi)). By the delta method, 
</p>
<p>~ 2 
7/Ji ~ N(7/Ji, ai) 
</p>
<p>where az = l/(npi(l- Pi)). Experience shows that the Normal approximation 
for 7/J is more accurate than the Normal approximation for P so we shall work 
</p>
<p>with 7/J. We shall treat ai as known. Furthermore, we shall take the distribution 
</p>
<p>of the 7/J~s to be Normal. The hierarchical model is now 
</p>
<p>I/Ji N(JL, T2) 
</p>
<p>Zil7/Ji N(7/Ji, aZ)&middot; 
</p>
<p>As yet another simplification we take T = 1. The unknown parameter are 
</p>
<p>8 = (JL, 7/Jl, ... ,I/Jk). The likelihood function is 
</p>
<p>&pound;(8) ex II !(l/JiIJL) II !(Zill/J) 
</p>
<p>ex II exp { -~(7/Ji - JL)2 } exp { - 2~Z (Zi - ~)i)2 } . 
t </p>
<p/>
</div>
<div class="page"><p/>
<p>24.5 MCMC Part II: Different Flavors 417 
</p>
<p>If we use the prior f (,1) ex 1 then the posterior is proportional to the likelihood. 
To use Gibbs sampling, we need to find the conditional distribution of each 
</p>
<p>parameter conditional on all the others. Let us begin by finding j(fLlrest) 
</p>
<p>where "rest" refers to all the other variables. We can throwaway any terms 
</p>
<p>that don't involve fL. Thus, 
</p>
<p>f(fLlrest) ex II exp { - ~ (1/)i - fL)2 } 
t 
</p>
<p>where 
</p>
<p>Hence we see that 111rest rv N(b, 11k). Next we will find f(1/)lrest). Again, we 
</p>
<p>can throwaway any terms not involving 1/)i leaving us with 
</p>
<p>where 
</p>
<p>f(l/'il rest) ex exp { -~(I/'i - fL)2 } exp { - 2~; (Zi -l/'i)2 } 
</p>
<p>ex exp { - 2~; (1/Ji - ei)2 } 
</p>
<p>and so 1/)ilrest rv N(ei,d;). The Gibbs sampling algorithm then involves iter-
</p>
<p>ating the following steps N times: 
</p>
<p>draw fL 
</p>
<p>draw 1/JI 
</p>
<p>draw 1/Jk 
</p>
<p>N(b,V2) 
</p>
<p>N(el' di) 
</p>
<p>It is understood that at each step, the most recently drawn version of each 
</p>
<p>variable is used. 
</p>
<p>We generated a numerical example with k = 20 cities and n = 20 people 
</p>
<p>from each city. After running the chain, we can convert each 1/Ji back into Pi 
</p>
<p>by way of Pi = eWi 1(1 + eWi ). The raw proportions are shown in Figure 24.4. 
Figure 24.3 shows "trace plots" of the Markov chain for PI and 11. Figure 
</p>
<p>24.4 shows the posterior for fL based on the simulated values. The second </p>
<p/>
</div>
<div class="page"><p/>
<p>418 24. Simulation Methods 
</p>
<p>panel of Figure 24.4 shows the raw proportions and the Bayes estimates. Note 
</p>
<p>that the Bayes estimates are "shrunk" together. The parameter T controls 
</p>
<p>the amount of shrinkage. We set T = 1 but, in practice, we should treat T as 
</p>
<p>another unknown parameter and let the data determine how much shrinkage 
</p>
<p>is needed. _ 
</p>
<p>C&gt; 
6 
</p>
<p>C&gt; 
</p>
<p>6 
</p>
<p>o 500 1000 
</p>
<p>U") 
</p>
<p>9~r-------------~~~------------~~_ 
</p>
<p>FIGURE 24.3. Posterior simulation for Example 24.11. The top panel shows simu-
</p>
<p>lated values of Pl. The top panel shows simulated values of 11. 
</p>
<p>So far we assumed that we know how to draw samples from the conditionals 
</p>
<p>fXIY(xly) and fYlx(ylx). If we don't know how, we can still use the Gibbs 
</p>
<p>sampling algorithm by drawing each observation using a Metropolis-Hastings 
</p>
<p>step. Let q be a proposal distribution for x and let qbe a proposal distribution 
</p>
<p>for y. When we do a Metropolis step for X, we treat Y as fixed. Similarly, 
</p>
<p>when we do a Metropolis step for Y, we treat X as fixed. Here are the steps: </p>
<p/>
</div>
<div class="page"><p/>
<p>24.5 MCMC Part II: Different Flavors 419 
</p>
<p>Metropolis within Gibbs 
</p>
<p>(Ia) Draw a proposal Z rv q(zIXn). 
</p>
<p>(Ib) Evaluate 
</p>
<p>(Ie) Set 
</p>
<p>with probability T 
</p>
<p>with probability 1 - T. 
</p>
<p>(2a) Draw a proposal Z rv q(zlYn). 
</p>
<p>(2b) Evaluate 
</p>
<p>(2e) Set 
</p>
<p>with probability T 
</p>
<p>with probability 1 - T. 
</p>
<p>Again, this generalizes to more than two dimensions. 
</p>
<p>-0.6 
</p>
<p>I 1ft II! \\\\\\ 
FIGURE 24.4. Example 24.11. Top panel: posterior histogram of fJ,. Lower panel: 
</p>
<p>raw proportions and the Bayes posterior estimates. The Bayes estimates have been 
</p>
<p>shrunk closer together than the raw proportions. </p>
<p/>
</div>
<div class="page"><p/>
<p>420 24. Simulation Methods 
</p>
<p>24.6 Bibliographic Rernarks 
</p>
<p>MCMC methods go back to the effort to build the atomic bomb in World War 
</p>
<p>II. They were used in various places after that, especially in spatial statistics. 
</p>
<p>There was a new surge of interest in the 1990s that still continues. My main 
</p>
<p>reference for this chapter was Robert and Casella (1999). See also Gelman 
</p>
<p>et al. (1995) and Gilks et al. (1998). 
</p>
<p>24.7 Exercises 
</p>
<p>1. Let 
</p>
<p>_ J2 e- x2 / 2 
I - r.&gt;= dx. 
</p>
<p>1 V 27T 
</p>
<p>(a) Estimate I using the basic Monte Carlo method. Use N = 100,000. 
</p>
<p>Also, find the estimated standard error. 
</p>
<p>(b) Find an (analytical) expression for the standard error of your esti-
</p>
<p>mate in (a). Compare to the estimated standard error. 
</p>
<p>(c) Estimate I using importance sampling. Take g to be N(1.5,v 2 ) with 
</p>
<p>v = .1, v = 1 and v = 10. Compute the (true) standard errors in each 
</p>
<p>case. Also, plot a histogram of the values you are averaging to see if 
</p>
<p>there are any extreme values. 
</p>
<p>(d) Find the optimal importance sampling function g*. What is the 
</p>
<p>standard error using g*? 
</p>
<p>2. Here is a way to use importance sampling to estimate a marginal density. 
</p>
<p>Let ix,y(x,y) be a bivariate density and let (X1,X2), ... ,(XN ,YN) rv 
</p>
<p>ix,y. 
</p>
<p>(a) Let w(x) be an arbitrary probability density function. Let 
</p>
<p>Show that, for each x, 
</p>
<p>lx(x) ~ ix(x). 
</p>
<p>Find an expression for the variance of this estimator. 
</p>
<p>(b) Let Y rv N(O, 1) and XIY = y rv N(y, 1 + y2). Use the method in 
(a) to estimate ix(x). </p>
<p/>
</div>
<div class="page"><p/>
<p>24.7 Exercises 421 
</p>
<p>3. Here is a method called accept-reject sampling for drawing observa-
</p>
<p>tions from a distribution. 
</p>
<p>(a) Suppose that j is some probability density function. Let 9 be any 
</p>
<p>other density and suppose that j(x) &lt;::: Mg(x) for all x, where M is a 
</p>
<p>known constant. Consider the following algorithm: 
</p>
<p>(step 1): Draw X rv 9 and U rv Unif(O, 1); 
</p>
<p>(step 2): If U &lt;::: j(X)/(Mg(X)) set Y = X, otherwise go back to step 
</p>
<p>1. (Keep repeating until you finally get an observation.) 
</p>
<p>Show that the distribution of Y is j. 
</p>
<p>(b) Let j be a standard Normal density and let g(x) = 1/(1 + x 2 ) be 
the Cauchy density. Apply the method in (a) to draw 1,000 observations 
</p>
<p>from the Normal distribution. Draw a histogram of the sample to verify 
</p>
<p>that the sample appears to be Normal. 
</p>
<p>4. A random variable Z has a inverse Gaussian distribution if it has 
</p>
<p>density 
</p>
<p>j(z) ex z-3/2 exp { -BIZ - B: + 2VB1B2 + log ( ~) }, z &gt; 0 
</p>
<p>where Bl &gt; 0 and B2 &gt; 0 are parameters. It can be shown that 
</p>
<p>fl;2 (1) #,1 1 IE(Z) = - and IE - = - + -. 
Bl Z B2 2B2 
</p>
<p>(a) Let Bl = 1.5 and B2 = 2. Draw a sample of size 1,000 using the 
</p>
<p>independence-Metropolis-Hastings method. Use a Gamma distribution 
</p>
<p>as the proposal density. To assess the accuracy, compare the mean of Z 
</p>
<p>and I/Z from the sample to the theoretical means Try different Gamma 
</p>
<p>distributions to see if you can get an accurate sample. 
</p>
<p>(b) Draw a sample of size 1,000 using the random-walk-Metropolis-
</p>
<p>Hastings method. Since z &gt; 0 we cannot just use a Normal density. 
</p>
<p>One strategy is this. Let W = log Z. Find the density of W. Use the 
</p>
<p>random-walk-Metropolis-Hastings method to get a sample WI, ... , WN 
</p>
<p>and let Zi = e Wi. Assess the accuracy of the simulation as in part (a). 
</p>
<p>5. Get the heart disease data from the book web site. Consider a Bayesian 
</p>
<p>analysis of the logistic regression model </p>
<p/>
</div>
<div class="page"><p/>
<p>422 24. Simulation Methods 
</p>
<p>Use the flat prior f((3o, ... ,(3k) ex 1. Use the Gibbs-Metropolis algorithm 
</p>
<p>to draw a sample of size 10,000 from the posterior f((3o, (31Idata). Plot 
</p>
<p>histograms of the posteriors for the (3/s. Get the posterior mean and a 
</p>
<p>95 percent posterior interval for each (3j. 
</p>
<p>(b) Compare your analysis to a frequentist approach using maximum 
</p>
<p>likelihood. </p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography 
</p>
<p>AGRESTI, A. (1990). Categorical Data Analysis. Wiley. 
</p>
<p>AKAIKE, H. (1973). Information theory and an extension of the maximum 
</p>
<p>likelihood principle. Second International Symposium on Information The-
</p>
<p>ory 267-281. 
</p>
<p>ANDERSON, T. W. (1984). An Introduction to Multivariate Statistical Anal-
</p>
<p>ysis (Second Edition). Wiley. 
</p>
<p>BARRON, A., SCHERVISH, M. J. and WASSERMAN, L. (1999). The consis-
</p>
<p>tency of posterior distributions in nonparametric problems. The Annals of 
</p>
<p>Statistics 27 536-561. 
</p>
<p>BEECHER, H. (1959). Measurement of Subjective Responses. Oxford Univer-
</p>
<p>sity Press. 
</p>
<p>BENJAMINI, Y. and HOCHBERG, Y. (1995). Controlling the false discovery 
</p>
<p>rate: A practical and powerful approach to multiple testing. Journal of the 
</p>
<p>Royal Statistical Society, Series B, Methodological 57 289-300. 
</p>
<p>BERAN, R. (2000). REACT scatterplot smoothers: Superefficiency through 
</p>
<p>basis economy. Journal of the American Statistical Association 95 155-171. 
</p>
<p>BERAN, R. and DUMBGEN, L. (1998). Modulation of estimators and confi-
</p>
<p>dence sets. The Annals of Statistics 26 1826-1856. </p>
<p/>
</div>
<div class="page"><p/>
<p>424 Bibliography 
</p>
<p>BERGER, J. and WOLPERT, R. (1984). The Likelihood Principle. Institute 
</p>
<p>of Mathematical Statistics. 
</p>
<p>BERGER, J. O. (1985). Statistical Decision Theory and Bayesian Analysis 
</p>
<p>(Second Edition). Springer-Verlag. 
</p>
<p>BERGER, J. O. and DELAMPADY, M. (1987). Testing precise hypotheses (c/r: 
</p>
<p>P335-352). Statistical Science 2 317-335. 
</p>
<p>BERLINER, L. M. (1983). Improving on inadmissible estimators in the control 
</p>
<p>problem. The Annals of Statistics 11 814-826. 
</p>
<p>BICKEL, P. J. and DOKSUM, K. A. (2000). Mathematical Statistics: Basic 
</p>
<p>Ideas and Selected Topics, Vol. I (Second Edition). Prentice Hall. 
</p>
<p>BILLINGSLEY, P. (1979). Probability and Measure. Wiley. 
</p>
<p>BISHOP, Y. M. M., FIENBERG, S. E. and HOLLAND, P. W. (1975). Discrete 
</p>
<p>Multivariate Analyses: Theory and Practice. MIT Press. 
</p>
<p>BREIMAN, L. (1992). Probability. Society for Industrial and Applied Mathe-
</p>
<p>matics. 
</p>
<p>BRINEGAR, C. S. (1963). Mark Twain and the Quintus Curtius Snodgrass 
</p>
<p>letters: A statistical test of authorship. Journal of the American Statistical 
</p>
<p>Association 58 85-96. 
</p>
<p>CARLIN, B. P. and LOUIS, T. A. (1996). Bayes and Empirical Bayes Methods 
</p>
<p>for Data Analysis. Chapman &amp; Hall. 
</p>
<p>CASELLA, G. and BERGER, R. L. (2002). Statistical Inference. Duxbury 
</p>
<p>Press. 
</p>
<p>CHAUDHURI, P. and MARRON, J. S. (1999). Sizerforexplorationofstructures 
</p>
<p>in curves. Journal of the American Statistical Association 94 807-823. 
</p>
<p>Cox, D. and LEWIS, P. (1966). The Statistical Analysis of Series of Events. 
</p>
<p>Chapman &amp; Hall. 
</p>
<p>Cox, D. D. (1993). An analysis of Bayesian inference for nonparametric 
</p>
<p>regression. The Annals of Statistics 21 903-923. 
</p>
<p>Cox, D. R. and HINKLEY, D. V. (2000). Theoretical statistics. Chapman &amp; 
</p>
<p>Hall. </p>
<p/>
</div>
<div class="page"><p/>
<p>425 
</p>
<p>DAVISON, A. C. and HINKLEY, D. V. (1997). Bootstrap Methods and Their 
</p>
<p>Application. Cambridge University Press. 
</p>
<p>DEGROOT, M. and SCHERVISH, M. (2002). Probability and Statistics (Third 
</p>
<p>Edition). Addison-Wesley. 
</p>
<p>DEVROYE, L., GYORFI, L. and LUGOSI, G. (1996). A Probabilistic Theory 
</p>
<p>of Pattern Recognition. Springer-Verlag. 
</p>
<p>DIACONIS, P. and FREEDMAN, D. (1986). On inconsistent Bayes estimates 
</p>
<p>of location. The Annals of Statistics 14 68-87. 
</p>
<p>DOBSON, A. J. (2001). An introduction to generalized linear models. Chap-
</p>
<p>man &amp; Hall. 
</p>
<p>DONOHO, D. L. and JOHNSTONE, 1. M. (1994). Ideal spatial adaptation by 
</p>
<p>wavelet shrinkage. Biometrika 81 425-455. 
</p>
<p>DONOHO, D. L. and JOHNSTONE, I. M. (1995). Adapting to unknown 
</p>
<p>smoothness via wavelet shrinkage. Journal of the American Statistical As-
</p>
<p>sociation 90 1200-1224. 
</p>
<p>DONOHO, D. L. and JOHNSTONE, 1. M. (1998). Minimax estimation via 
</p>
<p>wavelet shrinkage. The Annals of Statistics 26 879-921. 
</p>
<p>DONOHO, D. L., JOHNSTONE, 1. M., KERKYACHARIAN, G. and PICARD, D. 
</p>
<p>(1995). Wavelet shrinkage: Asymptopia? (Disc: p 337-369). Journal of the 
</p>
<p>Royal Statistical Society, Series B, Methodological 57 301-337. 
</p>
<p>DUNSMORE, 1., DALY, F. ET AL. (1987). M345 Statistical Methods, Unit 9: 
</p>
<p>Categorical Data. The Open University. 
</p>
<p>EDWARDS, D. (1995). Introduction to graphical modelling. Springer-Verlag. 
</p>
<p>EFROMOVICH, S. (1999). Nonparametric Curve Estimation: Methods, Theory 
</p>
<p>and Applications. Springer-Verlag. 
</p>
<p>EFRON, B. (1979). Bootstrap methods: Another look at the jackknife. The 
</p>
<p>Annals of Statistics 7 1-26. 
</p>
<p>EFRON, B., TIBSHIRANI, R., STOREY, J. D. and TUSHER, V. (2001). Em-
</p>
<p>pirical Bayes analysis of a microarray experiment. Journal of the American 
</p>
<p>Statistical Association 96 1151-1160. </p>
<p/>
</div>
<div class="page"><p/>
<p>426 Bibliography 
</p>
<p>EFRON, B. and TIBSHIRANI, R. J. (1993). An Introduction to the Bootstrap. 
</p>
<p>Chapman &amp; Hall. 
</p>
<p>FERGUSON, T. (1967). Mathematical Statistics." a Decision Theoretic Ap-
</p>
<p>proach. Academic Press. 
</p>
<p>FISHER, R. (1921). On the probable error of a coefficient of correlation de-
</p>
<p>duced from a small sample. Metron 1 1-32. 
</p>
<p>FREEDMAN, D. (1999). Wald lecture: On the Bernstein-von Mises theorem 
</p>
<p>with infinite-dimensional parameters. The Annals of Statistics 27 1119-
</p>
<p>1141. 
</p>
<p>FRIEDMAN, J. H. (1997). On bias, variance, 0/1-10ss, and the curse-of-
</p>
<p>dimensionality. Data Mining and Knowledge Discovery 1 55-77. 
</p>
<p>GELMAN, A., CARLIN, J. B., STERN, H. S. and RUBIN, D. B. (1995). 
</p>
<p>Bayesian Data Analysis. Chapman &amp; Hall. 
</p>
<p>GHOSAL, S., GHOSH, J. K. and VAN DER VAART, A. W. (2000). Conver-
</p>
<p>gence rates of posterior distributions. The Annals of Statistics 28 500-531. 
</p>
<p>GILKS, W. R., RICHARDSON, S. and SPIEGELHALTER, D. J. (1998). Markov 
</p>
<p>Chain Monte Carlo in Practice. Chapman &amp; Hall. 
</p>
<p>GRIMMETT, G. and STIRZAKER, D. (1982). Probability and Random Pro-
</p>
<p>cesses. Oxford University Press. 
</p>
<p>GUTTORP, P. (1995). Stochastic Modeling of Scientific Data. Chapman &amp; 
</p>
<p>Hall. 
</p>
<p>HALL, P. (1992). The Bootstrap and Edgeworth Expansion. Springer-Verlag. 
</p>
<p>HALVERSON, N., LEITCH, E., PRYKE, C., KOVAC, J., CARLSTROM, J., 
</p>
<p>HOLZAPFEL, W., DRAGOVAN, M., CARTWRIGHT, J., MASON, B., PADIN, 
</p>
<p>S., PEARSON, T., SHEPHERD, M. and READHEAD, A. (2002). DASI first 
</p>
<p>results: A measurement of the cosmic microwave background angular power 
</p>
<p>spectrum. Astrophysics Journal 568 38-45. 
</p>
<p>HARDLE, W. (1990). Applied nonparametric regression. Cambridge Univer-
</p>
<p>sity Press. 
</p>
<p>HARDLE, W., KERKYACHARIAN, G., PICARD, D. and TSYBAKOV, A. (1998). 
</p>
<p>Wavelets, Approximation, and Statistical Applications. Springer-Verlag. </p>
<p/>
</div>
<div class="page"><p/>
<p>427 
</p>
<p>HASTIE, T., TIBSHIRANI, R. and FRIEDMAN, J. H. (2001). The Elements 
</p>
<p>of Statistical Learning: Data Mining, Inference, and Prediction. Springer-
</p>
<p>Verlag. 
</p>
<p>HERBICH, R. (2002). Learning Kernel Classifiers: Theory and Algorithms. 
</p>
<p>MIT Press. 
</p>
<p>JOHNSON, R. A. and WICHERN, D. W. (1982). Applied Multivariate Statis-
</p>
<p>tical Analysis. Prentice-Hall. 
</p>
<p>JOHNSON, S. and JOHNSON, R. (1972). New England Journal of Medicine 
</p>
<p>287 1122-1125. 
</p>
<p>JORDAN, M. (2004). Graphical models. In Preparation. 
</p>
<p>KARR, A. (1993). Probability. Springer-Verlag. 
</p>
<p>KASS, R. E. and RAFTERY, A. E. (1995). Bayes factors. Journal of the 
</p>
<p>American Statistical Association 90 773-795. 
</p>
<p>KASS, R. E. and WASSERMAN, L. (1996). The selection of prior distributions 
</p>
<p>by formal rules (corr: 1998 v93 p 412). Journal of the American Statistical 
</p>
<p>Association 91 1343-1370. 
</p>
<p>LARSEN, R. J. and MARX, M. L. (1986). An Introduction to Mathematical 
</p>
<p>Statistics and Its Applications (Second Edition). Prentice Hall. 
</p>
<p>LAURITZEN, S. L. (1996). Graphical Models. Oxford University Press. 
</p>
<p>LEE, A. T. ET AL. (2001). A high spatial resolution analysis of the maxima-1 
</p>
<p>cosmic microwave background anisotropy data. Astrophys. J. 561 L1-L6. 
</p>
<p>LEE, P. M. (1997). Bayesian Statistics: An Introduction. Edward Arnold. 
</p>
<p>LEHMANN, E. L. (1986). Testing Statistical Hypotheses (Second Edition). 
</p>
<p>Wiley. 
</p>
<p>LEHMANN, E. L. and CASELLA, G. (1998). Theory of Point Estimation. 
</p>
<p>Springer-Verlag. 
</p>
<p>LOADER, C. (1999). Local regression and likelihood. Springer-Verlag. 
</p>
<p>MARRON, J. S. and WAND, M. P. (1992). Exact mean integrated squared 
</p>
<p>error. The Annals of Statistics 20 712-736. </p>
<p/>
</div>
<div class="page"><p/>
<p>428 Bibliography 
</p>
<p>MORRISON, A., BLACK, M., LOWE, C., MACMAHON, B. and YUSA, S. 
</p>
<p>(1973). Some international differences in histology and survival in breast 
</p>
<p>cancer. International Journal of Cancer 11 261-267. 
</p>
<p>NETTERFIELD, C. B. ET AL. (2002). A measurement by boomerang of mul-
</p>
<p>tiple peaks in the angular power spectrum of the cosmic microwave back-
</p>
<p>ground. Astrophys. J. 571 604-614. 
</p>
<p>OGDEN, R. T. (1997). Essential Wavelets for Statistical Applications and 
</p>
<p>Data Analysis. Birkhiiuser. 
</p>
<p>PEARL, J. (2000). Casuality: models, reasoning, and inference. Cambridge 
</p>
<p>University Press. 
</p>
<p>PHILLIPS, D. and KING, E. (1988). Death takes a holiday: Mortality sur-
</p>
<p>rounding major social occasions. Lancet 2 728-732. 
</p>
<p>PHILLIPS, D. and SMITH, D. (1990). Postponement of death until symbol-
</p>
<p>ically meaningful occasions. Journal of the American Medical Association 
</p>
<p>263 1947-1961. 
</p>
<p>QUENOUILLE, M. (1949). Approximate tests of correlation in time series. 
</p>
<p>Journal of the Royal Statistical Society B 11 18-84. 
</p>
<p>RICE, J. A. (1995). Mathematical Statistics and Data Analysis (Second Edi-
</p>
<p>tion). Duxbury Press. 
</p>
<p>ROBERT, C. P. (1994). The Bayesian Choice: A Decision-theoretic Motiva-
</p>
<p>tion. Springer-Verlag. 
</p>
<p>ROBERT, C. P. and CASELLA, G. (1999). Monte Carlo Statistical Methods. 
</p>
<p>Springer-Verlag. 
</p>
<p>ROBINS, J., SCHEINES, R., SPIRTES, P. and WASSERMAN, L. (2003). Uniform 
</p>
<p>convergence in causal inference. Biometrika (to appear). 
</p>
<p>ROBINS, J. M. and RITOV, Y. (1997). Toward a curse of dimensionality ap-
</p>
<p>propriate (CODA) asymptotic theory for semi-parametric models. Statistics 
</p>
<p>in Medicine 16 285-319. 
</p>
<p>ROSENBAUM, P. (2002). Observational Studies. Springer-Verlag. 
</p>
<p>Ross, S. (2002). Probability Models for Computer Science. Academic Press. </p>
<p/>
</div>
<div class="page"><p/>
<p>429 
</p>
<p>ROUSSEAUW, J., DU PLESSIS, J., BENADE, A., JORDAAN, P., KOTZE, J., 
</p>
<p>JOOSTE, P. and FERREIRA, J. (1983). Coronary risk factor screening in 
</p>
<p>three rural communities. South African Medical Journal 64 430-436. 
</p>
<p>SCHERVISH, M. J. (1995). Theory of Statistics. Springer-Verlag. 
</p>
<p>SCHOLKOPF, B. and SMOLA, A. (2002). Learning with Kernels: Support 
</p>
<p>Vector Machines, Regularization, Optimization, and Beyond. MIT Press. 
</p>
<p>SCHWARZ, G. (1978). Estimating the dimension of a model. The Annals of 
</p>
<p>Statistics 6 461-464. 
</p>
<p>SCOTT, D., GOTTO, A., COLE, J. and GORRY, G. (1978). Plasma lipids as 
</p>
<p>collateral risk factors in coronary artery disease: a study of 371 males with 
</p>
<p>chest pain. Journal of Chronic Diseases 31 337-345. 
</p>
<p>SCOTT, D. W. (1992). Multivariate Density Estimation: Theory, Practice, 
</p>
<p>and Visualization. Wiley. 
</p>
<p>SHAO, J. and Tu, D. (1995). The Jackknife and Bootstrap (German). 
</p>
<p>Springer-Verlag. 
</p>
<p>SHEN, X. and WASSERMAN, L. (2001). Rates of convergence of posterior 
</p>
<p>distributions. The Annals of Statistics 29 687-714. 
</p>
<p>SHORACK, G. R. and WELLNER, J. A. (1986). Empirical Processes With 
</p>
<p>Applications to Statistics. Wiley. 
</p>
<p>SILVERMAN, B. W. (1986). Density Estimation for Statistics and Data Anal-
</p>
<p>ysis. Chapman &amp; Hall. 
</p>
<p>SPIRTES, P., GLYMOUR, C. N. and SCHEINES, R. (2000). Causation, predic-
</p>
<p>tion, and search. MIT Press. 
</p>
<p>TAYLOR, H. M. and KARLIN, S. (1994). An Introduction to Stochastic Mod-
</p>
<p>eling. Academic Press. 
</p>
<p>VAN DER LAAN, M. and ROBINS, J. (2003). Unified Methods for Censored 
</p>
<p>Longitudinal Data and Causality. Springer Verlag. 
</p>
<p>VAN DER VAART, A. W. (1998). Asymptotic Statistics. Cambridge University 
</p>
<p>Press. 
</p>
<p>VAN DER VAART, A. W. and WELLNER, J. A. (1996). Weak Convergence 
</p>
<p>and Empirical Processes: With Applications to Statistics. Springer-Verlag. </p>
<p/>
</div>
<div class="page"><p/>
<p>430 Bibliography 
</p>
<p>VAPNIK, V. N. (1998). Statistical Learning Theory. Wiley. 
</p>
<p>WEISBERG, S. (1985). Applied Linear Regression. Wiley. 
</p>
<p>WHITTAKER, J. (1990). Graphical Models in Applied Multivariate Statistics. 
</p>
<p>Wiley. 
</p>
<p>WRIGHT, s. (1934). The method of path coefficients. The Annals of Mathe-
matical Statistics 5 161-215. 
</p>
<p>ZHAO, L. H. (2000). Bayesian aspects of some nonparametric problems. The 
</p>
<p>Annals of Statistics 28 532-552. 
</p>
<p>ZHENG, X. and LOH, W.-Y. (1995). Consistent variable selection in linear 
</p>
<p>models. Journal of the American Statistical Association 90 151-156. </p>
<p/>
</div>
<div class="page"><p/>
<p>lR 
</p>
<p>infxEA f(x) 
</p>
<p>SUPxEA f(x) 
</p>
<p>71! 
</p>
<p>(~) 
f(a) 
n 
</p>
<p>IP'(A) 
</p>
<p>AilB 
A '0060' B 
</p>
<p>Fx 
</p>
<p>fx 
XrvF 
</p>
<p>X rv f 
</p>
<p>X~Y 
lID 
</p>
<p>X1"",XnrvF 
&cent; 
</p>
<p>&lt;I&gt; 
</p>
<p>Zcx 
</p>
<p>IE(X) = f xdF(x) 
IE(r(X)) = f r(x)dF(x) 
V(X) 
Cov(X, Y) 
Xl, ... ,Xn 
n 
</p>
<p>List of Syrnbols 
</p>
<p>General Symbols 
</p>
<p>real numbers 
</p>
<p>infimum: the largest number y such that 
</p>
<p>y S f ( x) for all x E A 
think of this as the minimum of f 
supremum: the smallest number y such that 
</p>
<p>y ~ f ( x) for all x E A 
think of this as the maximum of f 
71 X (71 -1) x (71 - 2) x&middot;&middot;&middot; x 3 x 2 x 1 
</p>
<p>n! 
k!(n-k)! 
</p>
<p>Gamma function fooo yCX-1 e- Ydy 
</p>
<p>sample space (set of outcomes) 
</p>
<p>outcome, element, point 
</p>
<p>event (subset of n) 
</p>
<p>indicator function; 1 if W E A and 0 otherwise 
</p>
<p>number of points in set A 
</p>
<p>Probability Symbols 
</p>
<p>probability of event A 
</p>
<p>A and B are independent 
A and B are dependent 
cumulative distribution function 
</p>
<p>Fx(x) = IP'(X S x) 
probability density (or mass) function 
</p>
<p>X has distribution F 
X has density f 
</p>
<p>X and Y have the same distribution 
</p>
<p>independent and identically distributed 
</p>
<p>IID sample of size n from F 
standard Normal probability density 
</p>
<p>standard Normal distribution function 
</p>
<p>upper a quantile of N(O, 1): Zcx = &lt;I&gt;-1(1 - a) 
</p>
<p>expected value (mean) of random variable X 
</p>
<p>expected value (mean) of r(X) 
</p>
<p>variance of random variable X 
</p>
<p>covariance between X and Y 
</p>
<p>data 
</p>
<p>sample size </p>
<p/>
</div>
<div class="page"><p/>
<p>432 List of Symbols 
</p>
<p>p 
---+ 
~ 
</p>
<p>~ 
Xn :::::: N (,L, 0-;:,) 
</p>
<p>xn = o(an ) 
</p>
<p>xn = O(an ) 
</p>
<p>Xn = op(an ) 
</p>
<p>Xn = Op(an ) 
</p>
<p>Convergence Symbols 
</p>
<p>convergence in probability 
</p>
<p>convergence in distribution 
</p>
<p>convergence in quadratic mean 
</p>
<p>(Xn - IL)/On ~ N(O, 1) 
</p>
<p>xn/an --+ 0 
Ixn / an I is bounded for large n 
</p>
<p>p 
Xn/an---+O 
IXn/ an I is bounded in probability for large n 
</p>
<p>Statistical Models 
</p>
<p>statistical model; a set of distribution functions, 
</p>
<p>density functions or regression functions 
</p>
<p>parameter 
</p>
<p>estimate of parameter 
</p>
<p>statistical functional (the mean, for example) 
</p>
<p>likelihood function 
</p>
<p>Useful Math Facts 
</p>
<p>x ~CX) xk x 2 
e = L-k=O k! = 1 + x + 2T + ... 
</p>
<p>,\,00 r j = L for 0 &lt; r &lt; 1 
L-J=k l-r 
</p>
<p>limn --+ oo (1 + ~) n = ea 
</p>
<p>Stirling's approximation: n! :::::: nn e-nj27rn 
</p>
<p>THE GAMMA FUNCTION. The Gamma function is defined by 
</p>
<p>for 0; ~ O. If 0; &gt; 1 then r(o;) = (0; -l)r(o; -1). If n is a positive integer then 
r(n) = (n - I)!. Some special values are: r(l) = 1 and [(1/2) = ft. </p>
<p/>
</div>
<div class="page"><p/>
<p>T
a
b
</p>
<p>le
 o
</p>
<p>f 
D
</p>
<p>is
tr
</p>
<p>ib
u
</p>
<p>ti
o
</p>
<p>n
s 
</p>
<p>D
is
</p>
<p>tr
ib
</p>
<p>u
ti
</p>
<p>o
n
</p>
<p> 
P
</p>
<p>D
F
</p>
<p> 
o
</p>
<p>r 
p
</p>
<p>ro
b
</p>
<p>a
b
</p>
<p>il
it
</p>
<p>y
 f
</p>
<p>u
n
</p>
<p>c
ti
</p>
<p>o
n
</p>
<p> 
m
</p>
<p>e
a
n
</p>
<p> 
v
</p>
<p>a
ri
</p>
<p>a
n
</p>
<p>c
e
 
</p>
<p>M
G
</p>
<p>F
 
</p>
<p>P
o
</p>
<p>in
t 
</p>
<p>m
a
s
s
 a
</p>
<p>t 
a
</p>
<p> 
I(
</p>
<p>x
 =
</p>
<p> 
a
</p>
<p>) 
a
 
</p>
<p>0
 
</p>
<p>ea
t 
</p>
<p>B
e
rn
</p>
<p>o
u
</p>
<p>ll
i(
</p>
<p>p
) 
</p>
<p>p
X
</p>
<p>(l
 _
</p>
<p> p
)I
</p>
<p>-X
 
</p>
<p>P
 
</p>
<p>p
(l
</p>
<p> -
p
</p>
<p>) 
p
</p>
<p>e
t
+
</p>
<p>(
l-
</p>
<p>p
)
 
</p>
<p>B
in
</p>
<p>o
m
</p>
<p>ia
l(
</p>
<p> n
, 
</p>
<p>p
) 
</p>
<p>(~)px(1-
p
</p>
<p>)n
-x
</p>
<p> 
n
</p>
<p>p
 
</p>
<p>n
p
</p>
<p>(l
 -
</p>
<p>p
) 
</p>
<p>(p
et
</p>
<p> +
 (1
</p>
<p> _
 p
</p>
<p>))
n
</p>
<p> 
</p>
<p>G
e
o
</p>
<p>m
e
tr
</p>
<p>ic
 (p
</p>
<p>) 
p
</p>
<p>(l
 -
</p>
<p>p
)x
</p>
<p>-l
 I
(x
</p>
<p> ~ 
1
) 
</p>
<p>li
p
</p>
<p> 
I-
</p>
<p>p
 
</p>
<p>pe
l 
</p>
<p>( 
) 
</p>
<p>r 
1
</p>
<p>-(
I-
</p>
<p>p
)e
</p>
<p>l 
t 
</p>
<p>&lt;
 -
</p>
<p>lo
g
</p>
<p>(
l 
</p>
<p>-
p
) 
</p>
<p>P
o
</p>
<p>is
so
</p>
<p>n
(&gt;
</p>
<p>.)
 
</p>
<p>).
.;
</p>
<p>c
e
</p>
<p>-)
..
</p>
<p> 
&gt;.
</p>
<p> 
&gt;.
</p>
<p> 
e
</p>
<p>A (
e
'-
</p>
<p>I)
 
</p>
<p>-
x
!
-
</p>
<p>U
n
</p>
<p>if
o
</p>
<p>rm
(a
</p>
<p>, 
b)
</p>
<p> 
I(
</p>
<p>a
 &lt;
</p>
<p> X
 
&lt;
</p>
<p> b
)/
</p>
<p>(b
 -
</p>
<p>a
) 
</p>
<p>a+
b 
</p>
<p>(b
-a
</p>
<p>)2
 
</p>
<p>e
b
</p>
<p>t 
_
</p>
<p>e
a
</p>
<p>t 
-2
</p>
<p>-
-1
</p>
<p>-2
-
</p>
<p>(b
-a
</p>
<p>)t
 
</p>
<p>N
o
</p>
<p>rm
a
l(
</p>
<p>IL
,0
</p>
<p>-2
) 
</p>
<p>_
1
</p>
<p>_
e-
</p>
<p>(x
-I
</p>
<p>")
2
</p>
<p>/(
2
</p>
<p>0
-2
</p>
<p>) 
o
-y
</p>
<p>I2
1
f 
</p>
<p>IL 
0-
</p>
<p>2
 
</p>
<p>e
x
</p>
<p>p
 {
</p>
<p>{L
t 
+
</p>
<p> 0-~t2 
}
 
</p>
<p>E
x
</p>
<p>p
o
</p>
<p>n
e
n
</p>
<p>ti
a
l(
</p>
<p>;3
) 
</p>
<p>e-
;P
</p>
<p>/O
 
</p>
<p>;3
 
</p>
<p>;3
2 
</p>
<p>1-~Pt 
(t
</p>
<p> &lt;
 
</p>
<p>1
/;
</p>
<p>3
) 
</p>
<p>-1
3
</p>
<p>-
</p>
<p>G
a
m
</p>
<p>m
a
(a
</p>
<p>,;
3
</p>
<p>) 
x
u
</p>
<p>-1
e
-:
</p>
<p>r
/,
</p>
<p>a
 
</p>
<p>a
;3
</p>
<p> 
a;
</p>
<p>32
 
</p>
<p>(l~f3tr 
(t
</p>
<p> &lt;
 1
</p>
<p>/;
3
</p>
<p>) 
r(
</p>
<p>a)
f3
</p>
<p>" 
</p>
<p>s: (f) 
r(
</p>
<p>oo
+f
</p>
<p>3)
 
</p>
<p>0
0
-1
</p>
<p>(1
 _
</p>
<p>, 
)1
</p>
<p>3-
1 
</p>
<p>1
 
</p>
<p>2.
:0
</p>
<p>0
 
(r
</p>
<p>t-
1
 
</p>
<p>oo
+
</p>
<p>r 
) 
</p>
<p>tk
 
</p>
<p>c
+
</p>
<p> 
</p>
<p>B
e
ta
</p>
<p>(a
, 
</p>
<p>;3
) 
</p>
<p>a 
af
</p>
<p>3 
0
 
</p>
<p>r(
a)
</p>
<p>r(
f3
</p>
<p>)
x 
</p>
<p>X
 
</p>
<p>a+
f3
</p>
<p> 
(a
</p>
<p>+
f3
</p>
<p>)2
(a
</p>
<p>+
f3
</p>
<p>+
1 )
</p>
<p> 
+
</p>
<p> 
k=
</p>
<p>1 
r=
</p>
<p>O
 
</p>
<p>a+
f3
</p>
<p>+
r 
</p>
<p>kT
 
</p>
<p>,...
., 
</p>
<p>rn
 
</p>
<p>&laquo;
 
</p>
<p>r(
v
</p>
<p>t"
) 
</p>
<p>S
 
</p>
<p>tu
 
</p>
<p>1
 
</p>
<p>o
 (i
</p>
<p>f 
V
</p>
<p>&gt;
 1
</p>
<p>) 
~ 
</p>
<p>(i
f 
v
&gt;
</p>
<p> 2
) 
</p>
<p>d
o
</p>
<p>e
s 
</p>
<p>n
o
</p>
<p>t 
e
x
</p>
<p>is
t 
</p>
<p>r::
r 
</p>
<p>r(
!f
</p>
<p>) 
( 
</p>
<p>"
) 
</p>
<p>(v
+
</p>
<p>l)
/2
</p>
<p> .
 
</p>
<p>0
 
</p>
<p>1
+
</p>
<p>:1
; 
</p>
<p>u
-2
</p>
<p> 
W
</p>
<p> 
</p>
<p>( 
y
/2
</p>
<p> 
"'" 
</p>
<p>X~ 
1
</p>
<p> 
x
(p
</p>
<p>/2
)-
</p>
<p>l e
-x
</p>
<p>/2
 
</p>
<p>2
p
 
</p>
<p>1~2t 
(t
</p>
<p> &lt;
 1
</p>
<p>/2
) 
</p>
<p>w
 
</p>
<p>r(
p
</p>
<p>/2
)2
</p>
<p>P
/
</p>
<p>2
 
</p>
<p>p
 
</p>
<p>W
 </p>
<p/>
</div>
<div class="page"><p/>
<p>Index 
</p>
<p>x2 distribution, 30 
</p>
<p>accept-reject sampling, 421 
</p>
<p>accessible, 387 
</p>
<p>actions, 193 
</p>
<p>acyclic, 266 
</p>
<p>additive regression, 323 
</p>
<p>adjacent, 281 
</p>
<p>adjusted treatment effect, 259 
</p>
<p>admissibility 
</p>
<p>Bayes rules, 202 
</p>
<p>admissible, 202 
</p>
<p>AIC (Akaike Information Criterion), 
</p>
<p>220 
</p>
<p>Aliens, 271 
</p>
<p>alternative hypothesis, 95, 149 
</p>
<p>ancestor, 265 
</p>
<p>aperiodic, 390 
</p>
<p>arcs, 281 
</p>
<p>associated, 239 
</p>
<p>association, 253 
</p>
<p>association is not causation, 16.1, 
</p>
<p>253 
</p>
<p>assume, 8 
</p>
<p>asymptotic Normality, 128 
</p>
<p>asymptotic theory, 71 
</p>
<p>asymptotically Normal, 92, 126 
</p>
<p>asymptotically optimal, 126 
</p>
<p>asymptotically uniformly integrable, 
</p>
<p>81 
</p>
<p>average causal effect, 252 
</p>
<p>average treatment effect, 252 
</p>
<p>Axiom 1, 5 
</p>
<p>Axiom 2,5 
</p>
<p>Axiom 3,5 
</p>
<p>axioms of probability, 5 
</p>
<p>backfitting, 324 
</p>
<p>bagging, 375 
</p>
<p>bandwidth, 313 
</p>
<p>Bayes classification rule, 351 
</p>
<p>Bayes Estimators, 197 
</p>
<p>Bayes risk, 195 </p>
<p/>
</div>
<div class="page"><p/>
<p>Bayes rules, 197 
</p>
<p>admissibility, 202 
</p>
<p>Bayes' Theorem, 12, 1.17, 12 
</p>
<p>Bayesian inference, 89, 175 
</p>
<p>strengths and weaknesses, 185 
</p>
<p>Bayesian network, 263 
</p>
<p>Bayesian philosophy, 175 
</p>
<p>Bayesian testing, 184 
</p>
<p>Benjamini and Hochberg, 10.26, 167 
</p>
<p>Benjamini-Hochberg (BH) method, 
</p>
<p>167 
</p>
<p>Bernoulli distribution, 26, 29 
</p>
<p>Beta distribution, 30 
</p>
<p>bias-variance tradeoff, 305 
</p>
<p>Bibliographic Remarks, 13 
</p>
<p>Binomial distribution, 26 
</p>
<p>bins, 303, 306 
</p>
<p>binwidth, 306 
</p>
<p>bivariate distribution, 31 
</p>
<p>Bonferroni method, 166 
</p>
<p>boosting, 375 
</p>
<p>bootstrap, 107 
</p>
<p>parametric, 134 
</p>
<p>Bootstrap Confidence Intervals, 110 
</p>
<p>bootstrap percentile interval, 111 
</p>
<p>bootstrap pivotal confidence, 111 
</p>
<p>Bootstrap variance estimation, 109 
</p>
<p>branching process, 398 
</p>
<p>candidate, 411 
</p>
<p>Cauchy distribution, 30 
</p>
<p>Cauchy-Schwartz inequality, 4.8, 66 
</p>
<p>causal odds ratio, 252 
</p>
<p>causal regression function, 256 
</p>
<p>causal relative risk, 253 
</p>
<p>Central Limit Theorem (CLT), 5.8, 
</p>
<p>77 
</p>
<p>Chapman-Kolmogorov equations, 23.9, 
</p>
<p>385 
</p>
<p>Index 435 
</p>
<p>Chebyshev's inequality, 4.2, 64 
</p>
<p>checking assumptions, 135 
</p>
<p>child, 265 
</p>
<p>classes, 387 
</p>
<p>classification, 349 
</p>
<p>classification rule, 349 
</p>
<p>classification trees, 360 
</p>
<p>classifier 
</p>
<p>assessing error rate, 362 
</p>
<p>clique, 285 
</p>
<p>closed, 388 
</p>
<p>CLT, 77 
</p>
<p>collider, 265 
</p>
<p>comparing risk functions, 194 
</p>
<p>complete, 281, 328 
</p>
<p>composite hypothesis, 151 
</p>
<p>Computer Experiment, 16, 17 
</p>
<p>concave, 66 
</p>
<p>conditional causal effect, 255 
</p>
<p>conditional distribution, 36 
</p>
<p>conditional expectation, 54 
</p>
<p>conditional independence, 264 
</p>
<p>minimal, 287 
</p>
<p>conditional likelihood, 213 
</p>
<p>Conditional Probability, 10 
</p>
<p>conditional probability, 10, 10 
</p>
<p>conditional probability density func-
</p>
<p>tion, 37 
</p>
<p>conditional probability mass func-
</p>
<p>tion, 36 
</p>
<p>conditioning by intervention, 274 
</p>
<p>conditioning by observation, 274 
</p>
<p>confidence band, 99 
</p>
<p>confidence bands, 323 
</p>
<p>confidence interval, 65, 92 
</p>
<p>confidence set, 92 
</p>
<p>confounding variables, 257 
</p>
<p>conjugate, 179 </p>
<p/>
</div>
<div class="page"><p/>
<p>436 Index 
</p>
<p>consistency relationship, 252 
</p>
<p>consistent, 90, 126 
</p>
<p>continuity of probabilities, 1.B, 7 
</p>
<p>continuous, 23 
</p>
<p>converges in distribution, 72 
</p>
<p>converges in probability, 72 
</p>
<p>convex, 66 
</p>
<p>correlation, 52 
</p>
<p>confidence interval, 234 
</p>
<p>cosine basis, 329 
</p>
<p>counterfactual, 251, 252 
</p>
<p>counting process, 395 
</p>
<p>covariance, 52 
</p>
<p>covariance matrix, 232 
</p>
<p>covariate, 209 
</p>
<p>coverage, 92 
</p>
<p>critical value, 150 
</p>
<p>cross-validation, 363 
</p>
<p>cross-validation estimator of risk, 
</p>
<p>310 
</p>
<p>cumulative distribution function, 20 
</p>
<p>curse of dimensionality, 319 
</p>
<p>curve estimation, 89, 303 
</p>
<p>d-connected, 270 
</p>
<p>d-separated, 270 
</p>
<p>DAG, 266 
</p>
<p>data mining, vii 
</p>
<p>decision rule, 193 
</p>
<p>decision theory, 193 
</p>
<p>decomposition theorem, 23.17, 389 
</p>
<p>delta method, 5.13, 79, 131 
</p>
<p>density estimation, 312 
</p>
<p>kernel approach, 312 
</p>
<p>orthogonal function approach, 
</p>
<p>331 
</p>
<p>dependent, 34, 239 
</p>
<p>dependent variable, 89 
</p>
<p>derive, 8 
</p>
<p>descendant, 265 
</p>
<p>detail coefficients, 342 
</p>
<p>detailed balance, 391, 413 
</p>
<p>deviance, 299 
</p>
<p>directed acyclic graph, 266 
</p>
<p>directed graph, 264 
</p>
<p>directed path, 265 
</p>
<p>discrete, 22 
</p>
<p>discrete uniform distribution, 26 
</p>
<p>discrete wavelet transform (DWT), 
</p>
<p>344 
</p>
<p>discriminant function, 354 
</p>
<p>discrimination, 349 
</p>
<p>disjoint, 5 
</p>
<p>distribution 
</p>
<p>X2 , 30 
</p>
<p>Bernoulli, 26, 29 
</p>
<p>Beta, 30 
</p>
<p>Binomial, 26 
</p>
<p>Cauchy, 30 
</p>
<p>conditional, 36 
</p>
<p>discrete uniform, 26 
</p>
<p>Gaussian, 28 
</p>
<p>Geometric, 26 
</p>
<p>Multinomial, 39 
</p>
<p>multivariate Normal, 39 
</p>
<p>Normal, 28 
</p>
<p>point mass, 26 
</p>
<p>Poisson, 27 
</p>
<p>t, 30 
</p>
<p>Uniform, 27 
</p>
<p>Dvoretzky-Kiefer-Wolfowitz (DKW) 
</p>
<p>inequality, 7.5, 98 
</p>
<p>edges, 281 
</p>
<p>efficient, 126, 131 
</p>
<p>elements, 3 
</p>
<p>EM algorithm, 144 
</p>
<p>empirical distribution function, 97 </p>
<p/>
</div>
<div class="page"><p/>
<p>empirical error rate, 351 
</p>
<p>empirical probability measure, 367 
</p>
<p>empirical risk minimization, 352, 365 
</p>
<p>Epanechnikov kernel, 312 
</p>
<p>equal in distribution, 25 
</p>
<p>equivariant, 126 
</p>
<p>ergodic, 390 
</p>
<p>Events, 3 
</p>
<p>events, 3 
</p>
<p>evidence, 157 
</p>
<p>Exercises, 13 
</p>
<p>expectation, 47 
</p>
<p>conditional, 54 
</p>
<p>expected value, 47 
</p>
<p>exponential families, 140 
</p>
<p>faithful, 270 
</p>
<p>false discovery proportion, 166 
</p>
<p>false discovery rate, 166 
</p>
<p>FDP, 166 
</p>
<p>FDR, 166 
</p>
<p>feature, 89, 209 
</p>
<p>first moment, 47 
</p>
<p>first quartile, 25 
</p>
<p>Fisher information, 128 
</p>
<p>Fisher information matrix, 133 
</p>
<p>Fisher linear discriminant function, 
</p>
<p>356 
</p>
<p>fitted line, 210 
</p>
<p>fitted values, 210 
</p>
<p>frequentist (or classical), 175 
</p>
<p>frequentist inference, 89 
</p>
<p>Gamma function, 29 
</p>
<p>Gaussian classifier, 353 
</p>
<p>Gaussian distribution, 28 
</p>
<p>Geometric distribution, 26 
</p>
<p>Gibbs sampling, 416 
</p>
<p>Gini index, 361 
</p>
<p>Index 437 
</p>
<p>Glivenko-Cantelli theorem, 7.4, 98 
</p>
<p>goodness-of-fit tests, 168 
</p>
<p>graphical, 294 
</p>
<p>graphical log-linear models, 294 
</p>
<p>Haar father wavelet, 340 
</p>
<p>Haar scaling function, 340 
</p>
<p>Haar wavelet regression, 343 
</p>
<p>hierarchical log-linear model, 296 
</p>
<p>hierarchical model, 56 
</p>
<p>hierarchical models, 416 
</p>
<p>histogram, 303, 305 
</p>
<p>histogram estimator, 306 
</p>
<p>Hoeffding's inequality, 4.4, 64, 365 
homogeneous, 384 
</p>
<p>homogeneous Poisson process, 396 
</p>
<p>Horwitz-Thompson, 188 
</p>
<p>hypothesis testing, 94 
</p>
<p>identifiable, 126 
</p>
<p>importance sampling, 408 
</p>
<p>impurity, 360 
</p>
<p>inadmissible, 202 
</p>
<p>independent, 8, ~, 34 
</p>
<p>Independent Events, 8 
</p>
<p>independent random variables, 34 
</p>
<p>independent variable, 89 
</p>
<p>index set, 381 
</p>
<p>indicator function, 5 
</p>
<p>inequalities, 63 
</p>
<p>inner product, 327 
</p>
<p>integrated squared error (ISE), 304 
</p>
<p>intensity function, 395 
</p>
<p>interarrival times, 396 
</p>
<p>intervene, 273 
</p>
<p>intervention, 273 
</p>
<p>Introduction, 3 
</p>
<p>invariant, 390 
</p>
<p>inverse Gaussian distribution, 421 </p>
<p/>
</div>
<div class="page"><p/>
<p>438 Index 
</p>
<p>irreducible, 388 
</p>
<p>iterated expectations, 3.24, 55 
</p>
<p>jackknife, 115 
</p>
<p>James-Stein estimator, 204 
</p>
<p>Jeffreys-Lindley paradox, 192 
</p>
<p>Jensen's inequality, 4.9, 66 
</p>
<p>joint mass function, 31 
</p>
<p>K-fold cross-validation, 364 
</p>
<p>k-nearest-neighbors, 375 
</p>
<p>kernel, 312 
</p>
<p>kernel density estimator, 312,313 
</p>
<p>kernelization, 371 
</p>
<p>Kolmogorov-Smirnov test, 245 
</p>
<p>Kullback-Leibler distance, 126 
</p>
<p>Laplace transform, 56 
</p>
<p>large sample theory, 71 
</p>
<p>law of large numbers, 72 
</p>
<p>law of total probability, 1.16, 12 
</p>
<p>lazy, 3.6, 48 
</p>
<p>least favorable prior, 198 
</p>
<p>least squares estimates, 211 
</p>
<p>leave-one-out cross-validation, 220 
</p>
<p>leaves, 361 
</p>
<p>Legendre polynomials, 329 
</p>
<p>length, 327 
</p>
<p>level, 150 
</p>
<p>likelihood function, 122 
</p>
<p>likelihood ratio statistic, 164 
</p>
<p>likelihood ratio test, 164 
</p>
<p>limit theory, 71 
</p>
<p>limiting distribution, 391 
</p>
<p>linear algebra notation, 231 
</p>
<p>linear classifier, 353 
</p>
<p>linearly separable, 369 
</p>
<p>log odds ratio, 240 
</p>
<p>log-likelihood function, 122 
</p>
<p>log-linear expansion, 292 
</p>
<p>log-linear model, 286 
</p>
<p>log-linear models, 291 
</p>
<p>logistic regression, 223 
</p>
<p>loss function, 193 
</p>
<p>machine learning, vii 
</p>
<p>Manalahobis distance, 353 
</p>
<p>marginal Distribution, 33 
</p>
<p>marginal distribution, 197 
</p>
<p>Markov chain, 383, 383 
</p>
<p>Markov condition, 267 
</p>
<p>Markov equivalent, 271 
</p>
<p>Markov's inequality, 4.1, 63 
</p>
<p>maximal clique, 285 
</p>
<p>maximum likelihood, 122 
</p>
<p>maximum likelihood estimates 
</p>
<p>computing, 142 
</p>
<p>maximum likelihood estimator 
</p>
<p>consistent, 126 
</p>
<p>maximum risk, 195 
</p>
<p>mean, 47 
</p>
<p>mean integrated squared error (MISE), 
</p>
<p>304 
</p>
<p>mean recurrence time, 390 
</p>
<p>mean squared error, 91 
</p>
<p>measurable, 13, 43 
</p>
<p>median, 25 
</p>
<p>bootstrap, 109 
</p>
<p>Mercer's theorem, 373 
</p>
<p>method of moments estimator, 121 
</p>
<p>Metropolis within Gibbs, 419 
</p>
<p>Metropolis-Hastings algorithm, 411 
</p>
<p>Mill's inequality, 4.7, 65 
</p>
<p>minimal conditional independence, 
</p>
<p>287 
</p>
<p>minimal sufficient, 138 
</p>
<p>minimax rule, 197, 198 
</p>
<p>missing data, 187 </p>
<p/>
</div>
<div class="page"><p/>
<p>mixture of Normals, 143 
</p>
<p>model generator, 297 
</p>
<p>model selection, 218 
</p>
<p>moment generating function, 56 
</p>
<p>moments, 49 
</p>
<p>monotone decreasing, 5 
</p>
<p>monotone increasing, 5 
</p>
<p>Monte Carlo integration, 404 
</p>
<p>Monte Carlo integration method, 
</p>
<p>404 
</p>
<p>Monty Hall, 14 
</p>
<p>most powerful, 152 
</p>
<p>mother Haar wavelet, 341 
</p>
<p>MSE, 91 
</p>
<p>Multinomial, 235 
</p>
<p>Multinomial distribution, 39 
</p>
<p>multiparameter models, 133 
</p>
<p>multiple regression, 216 
</p>
<p>multiple testing, 165 
</p>
<p>multiresolution analysis, 341 
</p>
<p>Multivariate central limit theorem, 
</p>
<p>5.12,78 
</p>
<p>Multivariate Delta Method, 5.15, 
</p>
<p>79 
</p>
<p>multivariate Normal, 234 
</p>
<p>multivariate Normal distribution, 39 
</p>
<p>mutually exclusive, 5 
</p>
<p>Nadaraya-Watson kernel estimator, 
</p>
<p>319 
</p>
<p>naive Bayes classifier, 359 
</p>
<p>natural parameter, 141 
</p>
<p>natural sufficient statistic, 140 
</p>
<p>neural networks, 376 
</p>
<p>Newton-Raphson, 143 
</p>
<p>Neyman-Pearson, 10.30, 170 
</p>
<p>nodes, 281 
</p>
<p>non-collider, 265 
</p>
<p>non-null, 390 
</p>
<p>Index 439 
</p>
<p>non parametric model, 88 
</p>
<p>nonparametric regression, 319 
</p>
<p>kernel approach, 319 
</p>
<p>orthogonal function approach, 
</p>
<p>337 
</p>
<p>norm, 327 
</p>
<p>normal, 327 
</p>
<p>Normal distribution, 28 
</p>
<p>Normal-based confidence interval, 
</p>
<p>6.16,94 
</p>
<p>normalizing constant, 177, 403 
</p>
<p>not, 10 
</p>
<p>nuisance parameter, 120 
</p>
<p>nuisance parameters, 88 
</p>
<p>null, 390 
</p>
<p>null hypothesis, 94, 149 
</p>
<p>observational studies, 257 
</p>
<p>odds ratio, 240 
</p>
<p>olive statistics, i 
</p>
<p>one-parameter exponential family, 
</p>
<p>140 
</p>
<p>one-sided test, 151 
</p>
<p>optimality, 130 
</p>
<p>orthogonal, 327 
</p>
<p>orthogonal functions, 327 
</p>
<p>orthonormal, 328 
</p>
<p>orthonormal basis, 328 
</p>
<p>outcome, 89 
</p>
<p>overfitting, 218 
</p>
<p>p-value, 156, 157 
</p>
<p>pairwise Markov graph, 283 
</p>
<p>parameter of interest, 120 
</p>
<p>parameter space, 88 
</p>
<p>parameters, 26 
</p>
<p>parametric bootstrap, 134 
</p>
<p>parametric model, 87 
</p>
<p>parent, 265 </p>
<p/>
</div>
<div class="page"><p/>
<p>440 Index 
</p>
<p>Parseval's relation, 329 
</p>
<p>partition, 5 
</p>
<p>path, 281 
</p>
<p>Pearson's X2 test, 241 
</p>
<p>period, 390 
</p>
<p>periodic, 390 
</p>
<p>permutation distribution, 162 
</p>
<p>permutation test, 161 
</p>
<p>permutation test:algorithm, 163 
</p>
<p>perpendicular, 327 
</p>
<p>persistent, 388 
</p>
<p>pivot, 110 
</p>
<p>plug-in estimator, 99 
</p>
<p>point estimation, 90 
</p>
<p>point mass distribution, 26 
</p>
<p>pointwise asymptotic, 95 
</p>
<p>Poisson distribution, 27 
</p>
<p>Poisson process, 394, 395 
</p>
<p>positive definite, 231 
</p>
<p>posterior, 176 
</p>
<p>large sample properties, 181 
</p>
<p>posterior risk, 197 
</p>
<p>potential, 285 
</p>
<p>potential outcomes, 251 
</p>
<p>power function, 150 
</p>
<p>precision matrix, 232 
</p>
<p>predicted values, 210 
</p>
<p>prediction, 89, 215 
</p>
<p>prediction interval, 13.11,215 
</p>
<p>prediction risk, 219 
</p>
<p>predictor, 89 
</p>
<p>predictor variable, 209 
</p>
<p>prior distribution, 176 
</p>
<p>Probability, 5 
</p>
<p>probability, 5 
</p>
<p>probability distribution, 5, .Q. 
</p>
<p>probability function, 22 
</p>
<p>probability inequalities, 63 
</p>
<p>probability mass function, 22 
</p>
<p>probability measure, 5, .Q. 
</p>
<p>Probability on Finite Sample Spaces, 
</p>
<p>7 
</p>
<p>proposal, 411 
</p>
<p>quadratic discriminant analysis (QDA), 
</p>
<p>353 
</p>
<p>quantile function, 25 
</p>
<p>quantiles, 102 
</p>
<p>random variable, 19 
</p>
<p>independent, 34 
</p>
<p>random vector, 38, 232 
</p>
<p>random walk, 59 
</p>
<p>random-walk-Metropolis-Hastings, 
</p>
<p>415 
</p>
<p>realizations, 3 
</p>
<p>recurrence time, 390 
</p>
<p>recurrent, 388 
</p>
<p>regression, 89, 209, 335 
</p>
<p>non parametric , 319 
</p>
<p>regression function, 89, 209, 351 
</p>
<p>regression through the origin, 226 
</p>
<p>regressor, 89 
</p>
<p>rejection region, 150 
</p>
<p>relative risk, 248 
</p>
<p>represents, 266 
</p>
<p>residual sums of squares, 211 
</p>
<p>residuals, 210 
</p>
<p>response variable, 89, 209 
</p>
<p>reweighted least squares, 224 
</p>
<p>risk, 194, 304 
</p>
<p>rule of the lazy statistician, 3.6, 48 
</p>
<p>Rules of d-separation, 270 
</p>
<p>sample correlation, 102 
</p>
<p>sample mean, 51 
</p>
<p>sample outcomes, 3 </p>
<p/>
</div>
<div class="page"><p/>
<p>sample quantile, 102 
</p>
<p>sample space, 3 
</p>
<p>Sample Spaces and Events, 3 
</p>
<p>sample variance, 51 
</p>
<p>sampling distribution, 90 
</p>
<p>saturated model, 298, 299 
</p>
<p>scaling coefficient, 342 
</p>
<p>score function, 128 
</p>
<p>se,90 
</p>
<p>shatter coefficient, 367 
</p>
<p>shattered, 367 
</p>
<p>simple hypothesis, 151 
</p>
<p>simple linear regression, 210 
</p>
<p>Simpson's paradox, 259 
</p>
<p>simulation, 108, 180 
</p>
<p>size, 150 
</p>
<p>slack variables, 371 
</p>
<p>Slutzky's theorem, 75 
</p>
<p>smoothing, 303 
</p>
<p>smoothing parameter, 303 
</p>
<p>Sobolev space, 88 
</p>
<p>sojourn times, 396 
</p>
<p>spatially inhomogeneous, 340 
</p>
<p>standard deviation, 51 
</p>
<p>standard error, 90 
</p>
<p>standard Normal distribution, 28 
</p>
<p>state space, 381 
</p>
<p>stationary, 390 
</p>
<p>statistic, 61, 107, 137 
</p>
<p>statistical functional, 89, 99 
</p>
<p>statistical model, 87 
</p>
<p>Stein's paradox, 204 
</p>
<p>stochastic process, 381 
</p>
<p>Stone's theorem, 20.16, 316 
</p>
<p>strong law of large numbers, 5.18, 
</p>
<p>81 
</p>
<p>strongly inadmissible, 204 
</p>
<p>subjectivism, 181 
</p>
<p>Index 441 
</p>
<p>sufficiency, 137 
</p>
<p>sufficient statistic, 137 
</p>
<p>Summary of Terminology, 4 
</p>
<p>supervised learning, 349 
</p>
<p>support vector machines, 368 
</p>
<p>support vectors, 370 
</p>
<p>t distribution, 30 
</p>
<p>t-test, 170 
</p>
<p>test statistic, 150 
</p>
<p>third quartile, 25 
</p>
<p>thresholding, 342 
</p>
<p>training error, 219 
</p>
<p>training error rate, 351 
</p>
<p>training set, 363 
</p>
<p>transformations of random variables, 
</p>
<p>41 
</p>
<p>transient, 388 
</p>
<p>true error rate, 351 
</p>
<p>two-sided test, 151 
</p>
<p>type I error, 150 
</p>
<p>type II error, 150 
</p>
<p>types of convergence, 72 
</p>
<p>unbiased, 90 
</p>
<p>underfitting, 218 
</p>
<p>undirected graph, 281 
</p>
<p>uniform asymptotic, 95 
</p>
<p>Uniform distribution, 27 
</p>
<p>unshielded collider, 266 
</p>
<p>validation set, 363 
</p>
<p>Vapnik-Chervonenkis, 366 
</p>
<p>variance, 51 
</p>
<p>conditional, 55 
</p>
<p>variance-covariance matrix, 53 
</p>
<p>vertices, 281 
</p>
<p>waiting times, 396 
</p>
<p>Wald test, 153 </p>
<p/>
</div>
<div class="page"><p/>
<p>442 Index 
</p>
<p>wavelets, 340 
</p>
<p>weak law of large numbers (WLLN), 
</p>
<p>5.6,76 
</p>
<p>Zheng-Loh method, 222 </p>
<p/>
</div>
<div class="page"><p/>
<p>Springer Texts in Statistics (continued from page ii) 
</p>
<p>Lehmann: Testing Statistical Hypotheses, Second Edition 
</p>
<p>Lehmann and Casella: Theory of Point Estimation, Second Edition 
Lindman: Analysis of Variance in Experimental Design 
Lindsey: Applying Generalized Linear Models 
Madansky: Prescriptions for Working Statisticians 
</p>
<p>McPherson: Applying and Interpreting Statistics: A Comprehensive Guide, 
</p>
<p>Second Edition 
</p>
<p>Mueller: Basic Principles of Structural Equation Modeling: An Introduction to 
</p>
<p>LISREL and EQS 
</p>
<p>Nguyen and Rogers: Fundamentals of Mathematical Statistics: Volume I: 
</p>
<p>Probability for Statistics 
</p>
<p>Nguyen and Rogers: Fundamentals of Mathematical Statistics: Volume II: 
</p>
<p>Statistical Inference 
</p>
<p>Noether: Introduction to Statistics: The Nonparametric Way 
</p>
<p>Nolan and Speed: Stat Labs: Mathematical Statistics Through Applications 
</p>
<p>Peters: Counting for Something: Statistical Principles and Personalities 
</p>
<p>Pfeiffer: Probability for Applications 
</p>
<p>Pitman: Probability 
Rawlings, Pantula and Dickey: Applied Regression Analysis 
</p>
<p>Robert: The Bayesian Choice: From Decision-Theoretic Foundations to 
</p>
<p>Computational Implementation, Second Edition 
</p>
<p>Robert and Casella: Monte Carlo Statistical Methods, Second Edition 
</p>
<p>Rose and Smith: Mathematical Statistics with Mathematica 
</p>
<p>Ruppert: Statistics and Finance: An Introduction 
</p>
<p>Santner and Duffy: The Statistical Analysis of Discrete Data 
</p>
<p>Saville and Wood: Statistical Methods: The Geometric Approach 
</p>
<p>Sen and Srivastava: Regression Analysis: Theory, Methods, and Applications 
</p>
<p>Shao: Mathematical Statistics, Second Edition 
</p>
<p>Shorack: Probability for Statisticians 
</p>
<p>Shumway and Stoffer: Time Series Analysis and Its Applications 
</p>
<p>Simonofl: Analyzing Categorical Data 
Terrell: Mathematical Statistics: A Unified Introduction 
Timm: Applied Multivariate Analysis 
</p>
<p>Toutenburg: Statistical Analysis of Designed Experiments, Second Edition 
</p>
<p>Wasserman: All of St(!tistics: A Concise Course in Statistical Inference 
</p>
<p>Whittle: Probability via Expectation, Fourth Edition 
</p>
<p>Zacks: Introduction to Reliability Analysis: Probability Models 
</p>
<p>and Statistical Methods </p>
<p/>
</div>
<div class="page"><p/>
<p>ALSO AVAILABLE FROM SPRINGER! 
</p>
<p>AIIlhony C. Add ....... 
MuaJRIeaI 
</p>
<p>AMtacm..u 
</p>
<p>Thomas J. SantJIu 
Brian. J. Williams 
</p>
<p>William I . No .. 
</p>
<p>Exploring 
Multivariate Data 
</p>
<p>with the 
Forward Search 
</p>
<p>The Design and 
Analysis of 
Computer 
</p>
<p>Experiments 
</p>
<p>Design and 
Analysis of 
</p>
<p>DNA Mlcroarray 
Investigations 
</p>
<p>EXPLORING MULTIVARIATE 
DATA WITH THE FORWARD 
SEARCH 
ANTHONY C. ATKINSON, MARCO RIANI , and 
</p>
<p>ANDREA CERIOU 
</p>
<p>This book is about usi ng graphs to explore and 
</p>
<p>model continuous multivariate data. Such data are 
</p>
<p>often modeled usi ng the multi variate normal dis-
</p>
<p>tTibution and there is a literature of weighty sta-
</p>
<p>tistical tomes presenti ng the mathematical theory 
</p>
<p>of this activity. This book is very different. It focu -
</p>
<p>es on ways of exploring whether the data do indeed 
</p>
<p>have a normal di stribution. Outlier detection, 
</p>
<p>transformations to normal ity and the detection 
</p>
<p>of clusters and u nsuspected influential subsets 
</p>
<p>are emphasized. 
</p>
<p>2003/650 PP ./HAROCOVER/ISBN ()'387-40852-5 
</p>
<p>SPRINGER SERIES IN STATISTICS 
</p>
<p>THE DESIGN AND ANALYSIS OF 
COMPUTER EXPERIMENTS 
THOMAS J. SANTNER, BRIAN J. WILLIAMS, and 
WILUAM NOTZ 
</p>
<p>This book describes methods for designi ng and 
</p>
<p>analyzing experiments conducted using com-
</p>
<p>puter code in lieu of a physical experiment.lt dis-
</p>
<p>cusse how to select the values of the factors at 
</p>
<p>which t o run the code (the design of the computer 
</p>
<p>experi ment) in light of the research objectives of 
</p>
<p>the experimenter. It also provides techniques for 
</p>
<p>analyzing the res ult ing dala so as to achieve 
</p>
<p>these research goals, and it illustrates these meth-
</p>
<p>ods with code t hat is avai lable to the reader at the 
</p>
<p>companion web site for the book. 
</p>
<p>2003/240 PP./HARDCOVER/ ISBN 0-387-9542()'1 
</p>
<p>SPRINGER SERIES IN STATISTICS 
</p>
<p>DESIGN AND ANALYSIS OF DNA 
MICROARRAY INVESTIGATIONS 
RICHARD M. SIMON, EDWARD L. KORN, 
</p>
<p>LISA 1M . MCSHANE, MICHAEL D. RADMACHER , 
</p>
<p>GEORGE W. WRIGHT and YINGDONG ZHAO 
</p>
<p>This book is targeted to biologists with limited 
</p>
<p>statistical background and to statisticians and 
</p>
<p>computer scientists interested in being effecti ve 
</p>
<p>collaborators on multi-discipl inary DNA microar-
</p>
<p>ray projects. State-of-the-art analysis methods are 
</p>
<p>presented with minimal mathematical notation and 
</p>
<p>a focus on concepts. This book provides a sound 
</p>
<p>preparation fordesigning microarray studies that 
</p>
<p>have clear objectives, and for selecting analysis 
</p>
<p>tools and strategies that provide clear and valid 
</p>
<p>answers. The book offer an in depth under-
</p>
<p>standing of the design and analysis of experiments 
</p>
<p>uti liz in g microarrays and should benefit 
</p>
<p>scientists regardless of what software packages 
</p>
<p>they prefer. 
</p>
<p>2003/ S04 PP./HAROCOVER/ISBN 0-387-00135-2 
</p>
<p>STATISTICS FOR BIOLOGY AND HEALTH 
</p>
<p>To Order or for Infonnation: 
</p>
<p>In the Americas: CAll.: 1-BOO-SPRINGER or 
FAX: (201) 348-4505 &bull; WRITE: Springer-Verlag New 
Yotl&lt;, Inc .. Dept. S5634, PO Box 2485, Secaucus, NJ 
</p>
<p>07096-2485 &bull; VISIT: Your local technical bookstore 
</p>
<p>&bull; E-MAll: orderS@spri nger&middot;ny.com 
</p>
<p>Olltside the Americas: CAU.: +49 (0) 6221345-217/B 
&bull; FAX: + 49 (0) 6221 345-229 &bull; WRITE: Springer 
Customer Service. Haberstrasse 7, 691.26 
</p>
<p>Heidelberg , Germany . E-MAIl: orderS@sprlnger.de 
</p>
<p>PROMOTION: S5634 </p>
<p/>
</div>
<ul>	<li>Cover</li>
	<li>Title</li>
	<li>Copyright</li>
	<li>Dedication</li>
	<li>Preface</li>
	<li>Contents</li>
	<li>I Probability</li>
<ul>	<li>1 Probability</li>
<ul>	<li>1.1 Introduction</li>
	<li>1.2 Sample Spaces and Events</li>
	<li>1.3 Probability</li>
	<li>1.4 Probability on Finite Sample Spaces</li>
	<li>1.5 Independent Events</li>
	<li>1.6 Conditional Probability</li>
	<li>1.7 Bayes' Theorem</li>
	<li>1.8 Bibliographic Remarks</li>
	<li>1.9 Appendix</li>
	<li>1.10 Exercises</li>
</ul>
	<li>2 Random Variables</li>
<ul>	<li>2.1 Introduction</li>
	<li>2.2 Distribution Functions and Probability Functions</li>
	<li>2.3 Some Important Discrete Random Variables</li>
	<li>2.4 Some Important Continuous Random Variables</li>
	<li>2.5 Bivariate Distributions</li>
	<li>2.6 Marginal Distributions</li>
	<li>2.7 Independent Random Variables</li>
	<li>2.8 Conditional Distributions</li>
	<li>2.9 Multivariate Distributions and IID Samples</li>
	<li>2.10 Two Important Multivariate Distributions</li>
	<li>2.11 Transformations of Random Variables</li>
	<li>2.12 Transformations of Several Random Variables</li>
	<li>2.13 Appendix</li>
	<li>2.14 Exercises</li>
</ul>
	<li>3 Expectation</li>
<ul>	<li>3.1 Expectation of a Random Variable</li>
	<li>3.2 Properties of Expectations</li>
	<li>3.3 Variance and Covariance</li>
	<li>3.4 Expectation and Variance of Important Random Variables</li>
	<li>3.5 Conditional Expectation</li>
	<li>3.6 Moment Generating Functions</li>
	<li>3.7 Appendix</li>
	<li>3.8 Exercises</li>
</ul>
	<li>4 Inequalities</li>
<ul>	<li>4.1 Probability Inequalities</li>
	<li>4.2 Inequalities For Expectations</li>
	<li>4.3 Bibliographic Remarks</li>
	<li>4.4 Appendix</li>
	<li>4.5 Exercises</li>
</ul>
	<li>5 Convergence of Random Variables</li>
<ul>	<li>5.1 Introduction</li>
	<li>5.2 Types of Convergence</li>
	<li>5.3 The Law of Large Numbers</li>
	<li>5.4 The Central Limit Theorem</li>
	<li>5.5 The Delta Method</li>
	<li>5.6 Bibliographic Remarks</li>
	<li>5.7 Appendix</li>
<ul>	<li>5.7.1 Almost Sure and L1 Convergence</li>
	<li>5.7.2 Proof of the Central Limit Theorem</li>
</ul>
	<li>5.8 Exercises</li>
</ul>
</ul>
	<li>II Statistical Inference</li>
<ul>	<li>6 Models, Statistical Inference and Learning</li>
<ul>	<li>6.1 Introduction</li>
	<li>6.2 Parametric and Nonparametric Models</li>
	<li>6.3 Fundamental Concepts in Inference</li>
<ul>	<li>6.3.1 Point Estimation</li>
	<li>6.3.2 Confidence Sets</li>
	<li>6.3.3 Hypothesis Testing</li>
</ul>
	<li>6.4 Bibliographic Remarks</li>
	<li>6.5 Appendix</li>
	<li>6.6 Exercises</li>
</ul>
	<li>7 Estimating the CDF and Statistical Functionals</li>
<ul>	<li>7.1 The Empirical Distribution Function</li>
	<li>7.2 Statistical Functionals</li>
	<li>7.3 Bibliographic Remarks</li>
	<li>7.4 Exercises</li>
</ul>
	<li>8 The Bootstrap</li>
<ul>	<li>8.1 Simulation</li>
	<li>8.2 Bootstrap Variance Estimation</li>
	<li>8.3 Bootstrap Confidence Intervals</li>
	<li>8.4 Bibliographic Remarks</li>
	<li>8.5 Appendix</li>
<ul>	<li>8.5.1 The Jackknife</li>
	<li>8.5 .2 Justification For The Percentile Interval</li>
</ul>
	<li>8.6 Exercises</li>
</ul>
	<li>9 Parametric Inference</li>
<ul>	<li>9.1 Parameter of Interest</li>
	<li>9.2 The Method of Moments</li>
	<li>9.3 Maximum Likelihood</li>
	<li>9.4 Properties of Maximum Likelihood Estimators</li>
	<li>9.5 Consistency of Maximum Likelihood Estimators</li>
	<li>9.6 Equivariance of the MLE</li>
	<li>9.7 Asymptotic Normality</li>
	<li>9.8 Optimality</li>
	<li>9.9 The Delta Method</li>
	<li>9.10 Multiparameter Models</li>
	<li>9.11 The Parametric Bootstrap</li>
	<li>9.12 Checking Assumptions</li>
	<li>9.13 Appendix</li>
<ul>	<li>9.13.1 Proofs</li>
	<li>9.13.2 Sufficiency</li>
	<li>9.13.3 Exponential Families</li>
	<li>9.13.4 Computing Maximum Likelihood Estimates</li>
</ul>
	<li>9.14 Exercises</li>
</ul>
	<li>10 Hypothesis Testing and p-values</li>
<ul>	<li>10.1 The Wald Test</li>
	<li>10.2 p-values</li>
	<li>10.3 The X2 Distribution</li>
	<li>10.4 Pearson's X2 Test For Multinomial Data</li>
	<li>10.5 The Permutation Test</li>
	<li>10.6 The Likelihood Ratio Test</li>
	<li>10.7 Multiple Testing</li>
	<li>10.8 Goodness-of-fit Tests</li>
	<li>10.9 Bibliographic Remarks</li>
	<li>10.10 Appendix</li>
<ul>	<li>10.10.1 The Neyman-Pearson Lemma</li>
	<li>10.10.2 The t-test</li>
</ul>
	<li>10.11 Exercises</li>
</ul>
	<li>11 Bayesian Inference</li>
<ul>	<li>11.1 The Bayesian Philosophy</li>
	<li>11.2 The Bayesian Method</li>
	<li>11.3 Functions of Parameters</li>
	<li>11.4 Simulation</li>
	<li>11.5 Large Sample Properties of Bayes' Procedures</li>
	<li>11.6 Flat Priors, Improper Priors, and "Noninformative" Priors</li>
	<li>11.7 Multiparameter Problems</li>
	<li>11.8 Bayesian Testing</li>
	<li>11.9 Strengths and Weaknesses of Bayesian Inference</li>
	<li>11.10 Bibliographic Remarks</li>
	<li>11.11 Appendix</li>
	<li>11.12 Exercises</li>
</ul>
	<li>12 Statistical Decision Theory</li>
<ul>	<li>12.1 Preliminaries</li>
	<li>12.2 Comparing Risk Functions</li>
	<li>12.3 Bayes Estimators</li>
	<li>12.4 Minimax Rules</li>
	<li>12.5 Maximum Likelihood, Minimax, and Bayes</li>
	<li>12.6 Admissibility</li>
	<li>12.7 Stein's Paradox</li>
	<li>12.8 Bibliographic Remarks</li>
	<li>12.9 Exercises</li>
</ul>
</ul>
	<li>III Statistical Models and Methods</li>
<ul>	<li>13 Linear and Logistic Regression</li>
<ul>	<li>13.1 Simple Linear Regression</li>
	<li>13.2 Least Squares and Maximum Likelihood</li>
	<li>13.3 Properties of the Least Squares Estimators</li>
	<li>13.4 Prediction</li>
	<li>13.5 Multiple Regression</li>
	<li>13.6 Model Selection</li>
	<li>13.7 Logistic Regression</li>
	<li>13.8 Bibliographic Remarks</li>
	<li>13.9 Appendix</li>
	<li>13.10 Exercise</li>
</ul>
	<li>14 Multivariate Models</li>
<ul>	<li>14.1 Random Vectors</li>
	<li>14.2 Estimating the Correlation</li>
	<li>14.3 Multivariate Normal</li>
	<li>14.4 Multinomial</li>
	<li>14.5 Bibliographic Remarks</li>
	<li>14.6 Appendix</li>
	<li>14.7 Exercises</li>
</ul>
	<li>15 Inference About Independence</li>
<ul>	<li>15.1 Two Binary Variables</li>
	<li>15.2 Two Discrete Variables</li>
	<li>15.3 Two Continuous Variables</li>
	<li>15.4 One Continuous Variable and One Discrete</li>
	<li>15.5 Appendix</li>
	<li>15.6 Exercises</li>
</ul>
	<li>16 Causal Inference</li>
<ul>	<li>16.1 The Counterfactual Model</li>
	<li>16.2 Beyond Binary Treatments</li>
	<li>16.3 Observational Studies and Confounding</li>
	<li>16.4 Simpson's Paradox</li>
	<li>16.5 Bibliographic Remarks</li>
	<li>16.6 Exercises</li>
</ul>
	<li>17 Directed Graphs and Conditional Independence</li>
<ul>	<li>17.1 Introduction</li>
	<li>17.2 Conditional Independence</li>
	<li>17.3 DAGs</li>
	<li>17.4 Probability and DAGs</li>
	<li>17.5 More Independence Relations</li>
	<li>17.6 Estimation for DAGs</li>
	<li>17.7 Bibliographic Remarks</li>
	<li>17.8 Appendix</li>
	<li>17.9 Exercises</li>
</ul>
	<li>18 Undirected Graphs</li>
<ul>	<li>18.1 Undirected Graphs</li>
	<li>18.2 Probability and Graphs</li>
	<li>18.3 Cliques and Potentials</li>
	<li>18.4 Fitting Graphs to Data</li>
	<li>18.5 Bibliographic Remarks</li>
	<li>18.6 Exercises</li>
</ul>
	<li>19 Log-Linear Models</li>
<ul>	<li>19.1 The Log-Linear Model</li>
	<li>19.2 Graphical Log-Linear Models</li>
	<li>19.3 Hierarchical Log-Linear Models</li>
	<li>19.4 Model Generators</li>
	<li>19.5 Fitting Log-Linear Models to Data</li>
	<li>19.6 Bibliographic Remarks</li>
	<li>19.7 Exercises</li>
</ul>
	<li>20 Nonparametric Curve Estimation</li>
<ul>	<li>20.1 The Bias-Variance Tradeoff</li>
	<li>20.2 Histograms</li>
	<li>20.3 Kernel Density Estimation</li>
	<li>20.4 Nonparametric Regression</li>
	<li>20.5 Appendix</li>
	<li>20.6 Bibliographic Remarks</li>
	<li>20.7 Exercises</li>
</ul>
	<li>21 Smoothing Using Orthogonal Functions</li>
<ul>	<li>21.1 Orthogonal Functions and L2 Spaces</li>
	<li>21.2 Density Estimation</li>
	<li>21.3 Regression</li>
	<li>21.4 Wavelets</li>
	<li>21.5 Appendix</li>
	<li>21.6 Bibliographic Remarks</li>
	<li>21.7 Exercises</li>
</ul>
	<li>22 Classification</li>
<ul>	<li>22.1 Introduction</li>
	<li>22.2 Error Rates and the Bayes Classifier</li>
	<li>22.3 Gaussian and Linear Classifiers</li>
	<li>22.4 Linear Regression and Logistic Regression</li>
	<li>22.5 Relationship Between Logistic Regression and LDA</li>
	<li>22.6 Density Estimation and Naive Bayes</li>
	<li>22.7 Trees</li>
	<li>22.8 Assessing Error Rates and Choosing a Good Classifier</li>
	<li>22.9 Support Vector Machines</li>
	<li>22.10 Kernelization</li>
	<li>22.11 Other Classifiers</li>
	<li>22.12 Bibliographic Remarks</li>
	<li>22.13 Exercises</li>
</ul>
	<li>23 Probability Redux: Stochastic Processes</li>
<ul>	<li>23.1 Introduction</li>
	<li>23.2 Markov Chains</li>
	<li>23.3 Poisson Processes</li>
	<li>23.4 Bibliographic Remarks</li>
	<li>23.5 Exercises</li>
</ul>
	<li>24 Simulation Methods</li>
<ul>	<li>24.1 Bayesian Inference Revisited</li>
	<li>24.2 Basic Monte Carlo Integration</li>
	<li>24.3 Importance Sampling</li>
	<li>24.4 MCMC Part I: The Metropolis- Hastings Algorithm</li>
	<li>24.5 MCMC Part II: Different Flavors</li>
	<li>24.6 Bibliographic Remarks</li>
	<li>24.7 Exercises</li>
</ul>
</ul>
	<li>Bibliography</li>
	<li>Index</li>
</ul>
</body></html>