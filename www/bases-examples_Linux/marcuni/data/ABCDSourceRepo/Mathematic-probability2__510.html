<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
<p>Universitext
</p>
<p>Probability Theory 
</p>
<p>Alexandr A. Borovkov</p>
<p/>
</div>
<div class="page"><p/>
<p>Universitext</p>
<p/>
</div>
<div class="page"><p/>
<p>Universitext
</p>
<p>Series Editors:
</p>
<p>Sheldon Axler
</p>
<p>San Francisco State University, San Francisco, CA, USA
</p>
<p>Vincenzo Capasso
</p>
<p>Universit&agrave; degli Studi di Milano, Milan, Italy
</p>
<p>Carles Casacuberta
</p>
<p>Universitat de Barcelona, Barcelona, Spain
</p>
<p>Angus MacIntyre
</p>
<p>Queen Mary, University of London, London, UK
</p>
<p>Kenneth Ribet
</p>
<p>University of California, Berkeley, Berkeley, CA, USA
</p>
<p>Claude Sabbah
</p>
<p>CNRS, &Eacute;cole Polytechnique, Palaiseau, France
</p>
<p>Endre S&uuml;li
</p>
<p>University of Oxford, Oxford, UK
</p>
<p>Wojbor A. Woyczynski
</p>
<p>Case Western Reserve University, Cleveland, OH, USA
</p>
<p>Universitext is a series of textbooks that presents material from a wide variety
of mathematical disciplines at master&rsquo;s level and beyond. The books, often well
</p>
<p>class-tested by their author, may have an informal, personal, even experimental
</p>
<p>approach to their subject matter. Some of the most successful and established
</p>
<p>books in the series have evolved through several editions, always following the
</p>
<p>evolution of teaching curricula, into very polished texts.
</p>
<p>Thus as research topics trickle down into graduate-level teaching, first textbooks
</p>
<p>written for new, cutting-edge courses may make their way into Universitext.
</p>
<p>For further volumes:
</p>
<p>www.springer.com/series/223</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/223">http://www.springer.com/series/223</a></div>
</div>
<div class="page"><p/>
<p>Alexandr A. Borovkov
</p>
<p>Probability Theory
</p>
<p>Edited by K.A. Borovkov
</p>
<p>Translated by O.B. Borovkova and P.S. Ruzankin</p>
<p/>
</div>
<div class="page"><p/>
<p>Alexandr A. Borovkov
Sobolev Institute of Mathematics and
Novosibirsk State University
Novosibirsk, Russia
</p>
<p>Translation from the 5th edn. of the Russian language edition:
&lsquo;Teoriya Veroyatnostei&rsquo; by Alexandr A. Borovkov
&copy; Knizhnyi dom Librokom 2009
All Rights Reserved.
</p>
<p>1st and 2nd edn. &copy; Nauka 1976 and 1986
3rd edn. &copy; Editorial URSS and Sobolev Institute of Mathematics 1999
4th edn. &copy; Editorial URSS 2003
</p>
<p>ISSN 0172-5939 ISSN 2191-6675 (electronic)
Universitext
</p>
<p>ISBN 978-1-4471-5200-2 ISBN 978-1-4471-5201-9 (eBook)
</p>
<p>DOI 10.1007/978-1-4471-5201-9
</p>
<p>Springer London Heidelberg New York Dordrecht
</p>
<p>Library of Congress Control Number: 2013941877
</p>
<p>Mathematics Subject Classification: 60-XX, 60-01
</p>
<p>&copy; Springer-Verlag London 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied specifically for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of
this publication or parts thereof is permitted only under the provisions of the Copyright Law of the
Publisher&rsquo;s location, in its current version, and permission for use must always be obtained from Springer.
Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations
are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of pub-
lication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any
errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect
to the material contained herein.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer is part of Springer Science+Business Media (www.springer.com)</p>
<p/>
<div class="annotation"><a href="http://www.springer.com">http://www.springer.com</a></div>
<div class="annotation"><a href="http://www.springer.com/mycopy">http://www.springer.com/mycopy</a></div>
</div>
<div class="page"><p/>
<p>Foreword
</p>
<p>The present edition of the book differs substantially from the previous one. Over the
</p>
<p>period of time since the publication of the previous edition the author has accumu-
</p>
<p>lated quite a lot of ideas concerning possible improvements to some chapters of the
</p>
<p>book. In addition, some new opportunities were found for an accessible exposition
</p>
<p>of new topics that had not appeared in textbooks before but which are of certain
</p>
<p>interest for applications and reflect current trends in the development of modern
</p>
<p>probability theory. All this led to the need for one more revision of the book. As
</p>
<p>a result, many methodological changes were made and a lot of new material was
</p>
<p>added, which makes the book more logically coherent and complete. We will list
</p>
<p>here only the main changes in the order of their appearance in the text.
</p>
<p>&bull; Section 4.4 &ldquo;Expectations of Sums of a Random Number of Random Variables&rdquo;
was significantly revised. New sufficient conditions for Wald&rsquo;s identity were added.
</p>
<p>An example is given showing that, when summands are non-identically distributed,
</p>
<p>Wald&rsquo;s identity can fail to hold even in the case when its right-hand side is well-
</p>
<p>defined. Later on, Theorem 11.3.2 shows that, for identically distributed summands,
</p>
<p>Wald&rsquo;s identity is always valid whenever its right-hand side is well-defined.
</p>
<p>&bull; In Sect. 6.1 a criterion of uniform integrability of random variables is con-
structed, which simplifies the use of this notion. For example, the criterion directly
</p>
<p>implies uniform integrability of weighted sums of uniformly integrable random vari-
</p>
<p>ables.
</p>
<p>&bull; Section 7.2, which is devoted to inversion formulas, was substantially expanded
and now includes assertions useful for proving integro-local theorems in Sect. 8.7.
</p>
<p>&bull; In Chap. 8, integro-local limit theorems for sums of identically distributed ran-
dom variables were added (Sects. 8.7 and 8.8). These theorems, being substantially
</p>
<p>more precise assertions than the integral limit theorems, do not require additional
</p>
<p>conditions and play an important role in investigating large deviation probabilities
</p>
<p>in Chap. 9.
</p>
<p>v</p>
<p/>
</div>
<div class="page"><p/>
<p>vi Foreword
</p>
<p>&bull; A new chapter was written on probabilities of large deviations of sums of ran-
dom variables (Chap. 9). The chapter provides a systematic and rather complete
</p>
<p>exposition of the large deviation theory both in the case where the Cram&eacute;r condition
</p>
<p>(rapid decay of distributions at infinity) is satisfied and where it is not. Both integral
</p>
<p>and integro-local theorems are obtained. The large deviation principle is established.
</p>
<p>&bull; Assertions concerning the case of non-identically distributed random variables
were added in Chap. 10 on &ldquo;Renewal Processes&rdquo;. Among them are renewal theo-
</p>
<p>rems as well as the law of large numbers and the central limit theorem for renewal
</p>
<p>processes. A new section was written to present the theory of generalised renewal
</p>
<p>processes.
</p>
<p>&bull; An extension of the Kolmogorov strong law of large numbers to the case
of non-identically distributed random variables having the first moment only was
</p>
<p>added to Chap. 11. A new subsection on the &ldquo;Strong law of large numbers for gen-
</p>
<p>eralised renewal processes&rdquo; was written.
</p>
<p>&bull; Chapter 12 on &ldquo;Random walks and factorisation identities&rdquo; was substantially
revised. A number of new sections were added: on finding factorisation components
</p>
<p>in explicit form, on the asymptotic properties of the distribution of the suprema of
</p>
<p>cumulated sums and generalised renewal processes, and on the distribution of the
</p>
<p>first passage time.
</p>
<p>&bull; In Chap. 13, devoted to Markov chains, a section on &ldquo;The law of large numbers
and central limit theorem for sums of random variables defined on a Markov chain&rdquo;
</p>
<p>was added.
</p>
<p>&bull; Three new appendices (6, 7 and 8) were written. They present important aux-
iliary material on the following topics: &ldquo;The basic properties of regularly varying
</p>
<p>functions and subexponential distributions&rdquo;, &ldquo;Proofs of theorems on convergence to
</p>
<p>stable laws&rdquo;, and &ldquo;Upper and lower bounds for the distributions of sums and maxima
</p>
<p>of sums of independent random variables&rdquo;.
</p>
<p>As has already been noted, these are just the most significant changes; there are
</p>
<p>also many others. A lot of typos and other inaccuracies were fixed. The process of
</p>
<p>creating new typos and misprints in the course of one&rsquo;s work on a book is random
</p>
<p>and can be well described mathematically by the Poisson process (for the defini-
</p>
<p>tion of Poisson processes, see Chaps 10 and 19). An important characteristic of the
</p>
<p>quality of a book is the intensity of this process. Unfortunately, I am afraid that in
</p>
<p>the two previous editions (1999 and 2003) this intensity perhaps exceeded a certain
</p>
<p>acceptable level. Not renouncing his own responsibility, the author still admits that
</p>
<p>this may be due, to some extent, to the fact that the publication of these editions took
</p>
<p>place at the time of a certain decline of the publishing industry in Russia related to
</p>
<p>the general state of the economy at that time (in the 1972, 1976 and 1986 editions
</p>
<p>there were much fewer such defects).</p>
<p/>
</div>
<div class="page"><p/>
<p>Foreword vii
</p>
<p>Before starting to work on the new edition, I asked my colleagues from our lab-
</p>
<p>oratory at the Sobolev Institute of Mathematics and from the Chair of Probability
</p>
<p>Theory and Mathematical Statistics at Novosibirsk State University to prepare lists
</p>
<p>of any typos and other inaccuracies they had spotted in the book, as well as sug-
</p>
<p>gested improvements of exposition. I am very grateful to everyone who provided
</p>
<p>me with such information. I would like to express special thanks to I.S. Borisov,
</p>
<p>V.I. Lotov, A.A. Mogul&rsquo;sky and S.G. Foss, who also offered a number of method-
</p>
<p>ological improvements.
</p>
<p>I am also deeply grateful to T.V. Belyaeva for her invaluable assistance in type-
</p>
<p>setting the book with its numerous changes. Without that help, the work on the new
</p>
<p>edition would have been much more difficult.
</p>
<p>A.A. Borovkov</p>
<p/>
</div>
<div class="page"><p/>
<p>Foreword to the Third and Fourth Editions
</p>
<p>This book has been written on the basis of the Russian version (1986) published
</p>
<p>by &ldquo;Nauka&rdquo; Publishers in Moscow. A number of sections have been substantially
</p>
<p>revised and several new chapters have been introduced. The author has striven to
</p>
<p>provide a complete and logical exposition and simpler and more illustrative proofs.
</p>
<p>The 1986 text was preceded by two earlier editions (1972 and 1976). The first one
</p>
<p>appeared as an extended version of lecture notes of the course the author taught
</p>
<p>at the Department of Mechanics and Mathematics of Novosibirsk State University.
</p>
<p>Each new edition responded to comments by the readers and was completed with
</p>
<p>new sections which made the exposition more unified and complete.
</p>
<p>The readers are assumed to be familiar with a traditional calculus course. They
</p>
<p>would also benefit from knowing elements of measure theory and, in particular,
</p>
<p>the notion of integral with respect to a measure on an arbitrary space and its basic
</p>
<p>properties. However, provided they are prepared to use a less general version of
</p>
<p>some of the assertions, this lack of additional knowledge will not hinder the reader
</p>
<p>from successfully mastering the material. It is also possible for the reader to avoid
</p>
<p>such complications completely by reading the respective Appendices (located at the
</p>
<p>end of the book) which contain all the necessary results.
</p>
<p>The first ten chapters of the book are devoted to the basics of probability theory
</p>
<p>(including the main limit theorems for cumulative sums of random variables), and it
</p>
<p>is best to read them in succession. The remaining chapters deal with more specific
</p>
<p>parts of the theory of probability and could be divided into two blocks: random
</p>
<p>processes in discrete time (or random sequences, Chaps. 12 and 14&ndash;16) and random
</p>
<p>processes in continuous time (Chaps. 17&ndash;21).
</p>
<p>There are also chapters which remain outside the mainstream of the text as indi-
</p>
<p>cated above. These include Chap. 11 &ldquo;Factorisation Identities&rdquo;. The chapter not only
</p>
<p>contains a series of very useful probabilistic results, but also displays interesting re-
</p>
<p>lationships between problems on random walks in the presence of boundaries and
</p>
<p>boundary problems of complex analysis. Chapter 13 &ldquo;Information and Entropy&rdquo; and
</p>
<p>Chap. 19 &ldquo;Functional Limit Theorems&rdquo; also deviate from the mainstream. The for-
</p>
<p>mer deals with problems closely related to probability theory but very rarely treated
</p>
<p>in texts on the discipline. The latter presents limit theorems for the convergence
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>x Foreword to the Third and Fourth Editions
</p>
<p>of processes generated by cumulative sums of random variables to the Wiener and
</p>
<p>Poisson processes; as a consequence, the law of the iterated logarithm is established
</p>
<p>in that chapter.
</p>
<p>The book has incorporated a number of methodological improvements. Some
</p>
<p>parts of it are devoted to subjects to be covered in a textbook for the first time (for
</p>
<p>example, Chap. 16 on stochastic recursive sequences playing an important role in
</p>
<p>applications).
</p>
<p>The book can serve as a basis for third year courses for students with a rea-
</p>
<p>sonable mathematical background, and also for postgraduates. A one-semester (or
</p>
<p>two-trimester) course on probability theory might consist (there could be many vari-
</p>
<p>ants) of the following parts: Chaps. 1&ndash;2, Sects. 3.1&ndash;3.4, 4.1&ndash;4.6 (partially), 5.2 and
</p>
<p>5.4 (partially), 6.1&ndash;6.3 (partially), 7.1, 7.2, 7.4&ndash;7.6, 8.1&ndash;8.2 and 8.4 (partially), 10.1,
</p>
<p>10.3, and the main results of Chap. 12.
</p>
<p>For a more detailed exposition of some aspects of Probability Theory and the
</p>
<p>Theory of Random Processes, see for example [2, 10, 12&ndash;14, 26, 31].
</p>
<p>While working on the different versions of the book, I received advice and
</p>
<p>help from many of my colleagues and friends. I am grateful to Yu.V. Prokhorov,
</p>
<p>V.V. Petrov and B.A. Rogozin for their numerous useful comments which helped
</p>
<p>to improve the first variant of the book. I am deeply indebted to A.N. Kolmogorov
</p>
<p>whose remarks and valuable recommendations, especially of methodological char-
</p>
<p>acter, contributed to improvements in the second version of the book. In regard to
</p>
<p>the second and third versions, I am again thankful to V.V Petrov who gave me his
</p>
<p>comments, and to P. Franken, with whom I had a lot of useful discussions while the
</p>
<p>book was translated into German.
</p>
<p>In conclusion I want to express my sincere gratitude to V.V. Yurinskii, A.I. Sakha-
</p>
<p>nenko, K.A. Borovkov, and other colleagues of mine who also gave me their com-
</p>
<p>ments on the manuscript. I would also like to express my gratitude to all those who
</p>
<p>contributed, in one way or another, to the preparation and improvement of the book.
</p>
<p>A.A. Borovkov</p>
<p/>
</div>
<div class="page"><p/>
<p>For the Reader&rsquo;s Attention
</p>
<p>The numeration of formulas, lemmas, theorems and corollaries consists of three
</p>
<p>numbers, of which the first two are the numbers of the current chapter and section.
</p>
<p>For instance, Theorem 4.3.1 means Theorem 1 from Sect. 3 of Chap. 4. Section 6.2
</p>
<p>means Sect. 2 of Chap. 6.
</p>
<p>The sections marked with an asterisk may be omitted in the first reading.
</p>
<p>The symbol � at the end of a paragraph denotes the end of a proof or an important
</p>
<p>argument, when it should be pointed out that the argument has ended.
</p>
<p>The symbol :=, systematically used in the book, means that the left-hand side is
defined to be given by the right-hand side. The relation =: has the opposite meaning:
the right-hand side is defined by the left-hand side.
</p>
<p>The reader may find it useful to refer to the Index of Basic Notation and Subject
</p>
<p>index, which can be found at the end of this book.
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
<p>Introduction
</p>
<p>1. It is customary to set the origins of Probability Theory at the 17th century and
</p>
<p>relate them to combinatorial problems of games of chance. The latter can hardly be
</p>
<p>considered a serious occupation. However, it is games of chance that led to prob-
</p>
<p>lems which could not be stated and solved within the framework of the then existing
</p>
<p>mathematical models, and thereby stimulated the introduction of new concepts, ap-
</p>
<p>proaches and ideas. These new elements can already be encountered in writings by
</p>
<p>P. Fermat, D. Pascal, C. Huygens and, in a more developed form and somewhat
</p>
<p>later, in the works of J. Bernoulli, P.-S. Laplace, C.F. Gauss and others. The above-
</p>
<p>mentioned names undoubtedly decorate the genealogy of Probability Theory which,
</p>
<p>as we saw, is also related to some extent to the vices of society. Incidentally, as it
</p>
<p>soon became clear, it is precisely this last circumstance that can make Probability
</p>
<p>Theory more attractive to the reader.
</p>
<p>The first text on Probability Theory was Huygens&rsquo; treatise De Ratiociniis in Ludo
Alea (&ldquo;On Ratiocination in Dice Games&rdquo;, 1657). A bit later in 1663 the book Liber
de Ludo Aleae (&ldquo;Book on Games of Chance&rdquo;) by G. Cardano was published (in
fact it was written earlier, in the mid 16th century). The subject of these treatises
</p>
<p>was the same as in the writings of Fermat and Pascal: dice and card games (prob-
</p>
<p>lems within the framework of Sect. 1.2 of the present book). As if Huygens foresaw
</p>
<p>future events, he wrote that if the reader studied the subject closely, he would no-
</p>
<p>tice that one was not dealing just with a game here, but rather that the foundations
</p>
<p>of a very interesting and deep theory were being laid. Huygens&rsquo; treatise, which is
</p>
<p>also known as the first text introducing the concept of mathematical expectation,
</p>
<p>was later included by J. Bernoulli in his famous book Ars Conjectandi (&ldquo;The Art
of Conjecturing&rdquo;; published posthumously in 1713). To this book is related the no-
</p>
<p>tion of the so-called Bernoulli scheme (see Sect. 1.3), for which Bernoulli gave a
</p>
<p>cumbersome (cf. our Sect. 5.1) but mathematically faultless proof of the first limit
</p>
<p>theorem of Probability Theory, the Law of Large Numbers.
</p>
<p>By the end of the 19th and the beginning of the 20th centuries, the natural sci-
</p>
<p>ences led to the formulation of more serious problems which resulted in the develop-
</p>
<p>ment of a large branch of mathematics that is nowadays called Probability Theory.
</p>
<p>This subject is still going through a stage of intensive development. To a large extent,
</p>
<p>xiii</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Introduction
</p>
<p>Probability Theory owes its elegance, modern form and a multitude of achievements
</p>
<p>to the remarkable Russian mathematicians P.L. Chebyshev, A.A. Markov, A.N. Kol-
</p>
<p>mogorov and others.
</p>
<p>The fact that increasing our knowledge about nature leads to further demand for
</p>
<p>Probability Theory appears, at first glance, paradoxical. Indeed, as the reader might
</p>
<p>already know, the main object of the theory is randomness, or uncertainty, which is
</p>
<p>due, as a rule, to a lack of knowledge. This is certainly so in the classical example
</p>
<p>of coin tossing, where one cannot take into account all the factors influencing the
</p>
<p>eventual position of the tossed coin when it lands.
</p>
<p>However, this is only an apparent paradox. In fact, there are almost no exact de-
</p>
<p>terministic quantitative laws in nature. Thus, for example, the classical law relating
</p>
<p>the pressure and temperature in a volume of gas is actually a result of a probabilistic
</p>
<p>nature that relates the number of collisions of particles with the vessel walls to their
</p>
<p>velocities. The fact is, at typical temperatures and pressures, the number of particles
</p>
<p>is so large and their individual contributions are so small that, using conventional
</p>
<p>instruments, one simply cannot register the random deviations from the relationship
</p>
<p>which actually take place. This is not the case when one studies more sparse flows
</p>
<p>of particles&mdash;say, cosmic rays&mdash;although there is no qualitative difference between
</p>
<p>these two examples.
</p>
<p>We could move in a somewhat different direction and name here the uncertainty
</p>
<p>principle stating that one cannot simultaneously obtain exact measurements of any
</p>
<p>two conjugate observables (for example, the position and velocity of an object).
</p>
<p>Here randomness is not entailed by a lack of knowledge, but rather appears as a fun-
</p>
<p>damental phenomenon reflecting the nature of things. For instance, the lifetime of a
</p>
<p>radioactive nucleus is essentially random, and this randomness cannot be eliminated
</p>
<p>by increasing our knowledge.
</p>
<p>Thus, uncertainty was there at the very beginning of the cognition process, and
</p>
<p>it will always accompany us in our quest for knowledge. These are rather general
</p>
<p>comments, of course, but it appears that the answer to the question of when one
</p>
<p>should use the methods of Probability Theory and when one should not will always
</p>
<p>be determined by the relationship between the degree of precision we want to attain
</p>
<p>when studying a given phenomenon and what we know about the nature of the latter.
</p>
<p>2. In almost all areas of human activity there are situations where some exper-
</p>
<p>iments or observations can be repeated a large number of times under the same
</p>
<p>conditions. Probability Theory deals with those experiments of which the result (ex-
</p>
<p>pressed in one way or another) may vary from trial to trial. The events that refer to
</p>
<p>the experiment&rsquo;s result and which may or may not occur are usually called random
events.
</p>
<p>For example, suppose we are tossing a coin. The experiment has only two out-
</p>
<p>comes: either heads or tails show up, and before the experiment has been carried
</p>
<p>out, it is impossible to say which one will occur. As we have already noted, the rea-
</p>
<p>son for this is that we cannot take into account all the factors influencing the final
</p>
<p>position of the coin. A similar situation will prevail if you buy a ticket for each lot-
</p>
<p>tery draw and try to predict whether it will win or not, or, observing the operation of
</p>
<p>a complex machine, you try to determine in advance if it will have failed before or</p>
<p/>
</div>
<div class="page"><p/>
<p>Introduction xv
</p>
<p>Fig. 1 The plot of the
</p>
<p>relative frequencies nh/n
</p>
<p>corresponding to the outcome
</p>
<p>sequence htthtthhhthht in
</p>
<p>the coin tossing experiment
</p>
<p>after a given time. In such situations, it is very hard to find any laws when consid-
</p>
<p>ering the results of individual experiments. Therefore there is little justification for
</p>
<p>constructing any theory here.
</p>
<p>However, if one turns to a long sequence of repetitions of such an experiment,
an interesting phenomenon becomes apparent. While individual results of the ex-
</p>
<p>periments display a highly &ldquo;irregular&rdquo; behaviour, the average results demonstrate
</p>
<p>stability. Consider, say, a long series of repetitions of our coin tossing experiment
</p>
<p>and denote by nh the number of heads in the first n trials. Plot the ratio nh/n ver-
</p>
<p>sus the number n of conducted experiments (see Fig. 1; the plot corresponds to the
</p>
<p>outcome sequence htthtthhhthh, where h stands for heads and t for tails, respec-
</p>
<p>tively).
</p>
<p>We will then see that, as n increases, the polygon connecting the consecutive
</p>
<p>points (n,nh/n) very quickly approaches the straight line nh/n = 1/2. To verify
this observation, G.L. Leclerc, comte de Buffon,1 tossed a coin 4040 times. The
</p>
<p>number of heads was 2048, so that the relative frequency nh/n of heads was 0.5069.
</p>
<p>K. Pearson tossed a coin 24,000 times and got 12,012 heads, so that nh/n= 0.5005.
It turns out that this phenomenon is universal: the relative frequency of a certain
</p>
<p>outcome in a series of repetitions of an experiment under the same conditions tends
towards a certain number p &isin; [0,1] as the number of repetitions grows. It is an
objective law of nature which forms the foundation of Probability Theory.
</p>
<p>It would be natural to define the probability of an experiment outcome to be just
</p>
<p>the number p towards which the relative frequency of the outcome tends. How-
</p>
<p>ever, such a definition of probability (usually related to the name of R. von Mises)
</p>
<p>has proven to be inconvenient. First of all, in reality, each time we will be dealing
</p>
<p>not with an infinite sequence of frequencies, but rather with finitely many elements
</p>
<p>thereof. Obtaining the entire sequence is unfeasible. Hence the frequency (let it
</p>
<p>again be nh/n) of the occurrence of a certain outcome will, as a rule, be different
</p>
<p>for each new series of repetitions of the same experiment.
</p>
<p>This fact led to intense discussions and a lot of disagreement regarding how one
</p>
<p>should define the concept of probability. Fortunately, there was a class of phenomena
</p>
<p>that possessed certain &ldquo;symmetry&rdquo; (in gambling, coin tossing etc.) for which one
</p>
<p>could compute in advance, prior to the experiment, the expected numerical values
</p>
<p>1The data is borrowed from [15].</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Introduction
</p>
<p>of the probabilities. Take, for instance, a cube made of a sufficiently homogeneous
</p>
<p>material. There are no reasons for the cube to fall on any of its faces more often
</p>
<p>than on some other face. It is therefore natural to expect that, when rolling a die a
</p>
<p>large number of times, the frequency of each of its faces will be close to 1/6. Based
</p>
<p>on these considerations, Laplace believed that the concept of equiprobability is the
fundamental one for Probability Theory. The probability of an event would then be
</p>
<p>defined as the ratio of the number of &ldquo;favourable&rdquo; outcomes to the total number of
</p>
<p>possible outcomes. Thus, the probability of getting an odd number of points (e.g. 1,
</p>
<p>3 or 5) when rolling a die once was declared to be 3/6 (i.e. the number of faces with
</p>
<p>an odd number of points was divided by the total number of all faces). If the die were
</p>
<p>rolled ten times, then one would have 610 in the denominator, as this number gives
</p>
<p>the total number of equally likely outcomes and calculating probabilities reduces to
</p>
<p>counting the number of &ldquo;favourable outcomes&rdquo; (the ones resulting in the occurrence
</p>
<p>of a given event).
</p>
<p>The development of the mathematical theory of probabilities began from the in-
</p>
<p>stance when one started defining probability as the ratio of the number of favourable
</p>
<p>outcomes to the total number of equally likely outcomes, and this approach is nowa-
</p>
<p>days called &ldquo;classical&rdquo; (for more details, see Chap. 1).
</p>
<p>Later on, at the beginning of the 20th century, this approach was severely crit-
</p>
<p>icised for being too restrictive. The initiator of the critique was R. von Mises. As
</p>
<p>we have already noted, his conception was based on postulating stability of the fre-
quencies of events in a long series of experiments. That was a confusion of physical
</p>
<p>and mathematical concepts. No passage to the limit can serve as justification for
</p>
<p>introducing the notion of &ldquo;probability&rdquo;. If, for instance, the values nh/n were to
</p>
<p>converge to the limiting value 1/2 in Fig. 1 too slowly, that would mean that no-
</p>
<p>body would be able to find the value of that limit in the general (non-classical) case.
</p>
<p>So the approach is clearly vulnerable: it would mean that Probability Theory would
</p>
<p>be applicable only to those situations where frequencies have a limit. But why fre-
quencies would have a limit remained unexplained and was not even discussed.
</p>
<p>In this relation, R. von Mises&rsquo; conception has been in turn criticised by many
</p>
<p>mathematicians, including A.Ya. Khinchin, S.N. Bernstein, A.N. Kolmogorov and
</p>
<p>others. Somewhat later, another approach was suggested that proved to be fruitful
</p>
<p>for the development of the mathematical theory of probabilities. Its general features
</p>
<p>were outlined by S.N. Bernstein in 1908. In 1933 a rather short book &ldquo;Foundations
</p>
<p>of Probability Theory&rdquo; by A.N. Kolmogorov appeared that contained a complete
</p>
<p>and clear exposition of the axioms of Probability Theory. The general construction
</p>
<p>of the concept of probability based on Kolmogorov&rsquo;s axiomatics removed all the
</p>
<p>obstacles for the development of the theory and is nowadays universally accepted.
</p>
<p>The creation of an axiomatic Probability Theory provided a solution to the sixth
</p>
<p>Hilbert problem (which concerned, in particular, Probability Theory) that had been
</p>
<p>formulated by D. Hilbert at the Second International Congress of Mathematicians
</p>
<p>in Paris in 1900. The problem was on the axiomatic construction of a number of
</p>
<p>physical sciences, Probability Theory being classified as such by Hilbert at that
</p>
<p>time.
</p>
<p>An axiomatic foundation separates the mathematical aspect from the physical:
</p>
<p>one no longer needs to explain how and where the concept of probability comes</p>
<p/>
</div>
<div class="page"><p/>
<p>Introduction xvii
</p>
<p>from. The concept simply becomes a primitive one, its properties being described
</p>
<p>by axioms (which are essentially the axioms of Measure Theory). However, the
problem of how the probability thus introduced is related (and can be applied) to
</p>
<p>the real world remains open. But this problem is mostly removed by the remarkable
</p>
<p>fact that, under the axiomatic construction, the desired fundamental property that the
</p>
<p>frequencies of the occurrence of an event converge to the probability of the event
</p>
<p>does take place and is a precise mathematical result. (For more details, see Chaps. 2
</p>
<p>and 5.)2
</p>
<p>We will begin by defining probability in a somewhat simplified situation, in the
</p>
<p>so-called discrete case.
</p>
<p>2Much later, in the 1960s A.N. Kolmogorov attempted to develop a fundamentally different ap-
</p>
<p>proach to the notions of probability and randomness. In that approach, the measure of randomness,
</p>
<p>say, of a sequence 0,1,0,0,1, . . . consisting of 0s and 1s (or some other symbols) is the complex-
</p>
<p>ity of the algorithm describing this sequence. The new approach stimulated the development of a
</p>
<p>number of directions in contemporary mathematics, but, mostly due to its complexity, has not yet
</p>
<p>become widely accepted.</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Discrete Spaces of Elementary Events . . . . . . . . . . . . . . . . . 1
</p>
<p>1.1 Probability Space . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.2 The Classical Scheme . . . . . . . . . . . . . . . . . . . . . . . . 4
</p>
<p>1.3 The Bernoulli Scheme . . . . . . . . . . . . . . . . . . . . . . . . 6
</p>
<p>1.4 The Probability of the Union of Events. Examples . . . . . . . . . 9
</p>
<p>2 An Arbitrary Space of Elementary Events . . . . . . . . . . . . . . . 13
</p>
<p>2.1 The Axioms of Probability Theory. A Probability Space . . . . . . 13
</p>
<p>2.2 Properties of Probability . . . . . . . . . . . . . . . . . . . . . . . 20
</p>
<p>2.3 Conditional Probability. Independence of Events and Trials . . . . 21
</p>
<p>2.4 The Total Probability Formula. The Bayes Formula . . . . . . . . 25
</p>
<p>3 Random Variables and Distribution Functions . . . . . . . . . . . . . 31
</p>
<p>3.1 Definitions and Examples . . . . . . . . . . . . . . . . . . . . . . 31
</p>
<p>3.2 Properties of Distribution Functions. Examples . . . . . . . . . . . 33
</p>
<p>3.2.1 The Basic Properties of Distribution Functions . . . . . . . 33
</p>
<p>3.2.2 The Most Common Distributions . . . . . . . . . . . . . . 37
</p>
<p>3.2.3 The Three Distribution Types . . . . . . . . . . . . . . . . 39
</p>
<p>3.2.4 Distributions of Functions of Random Variables . . . . . . 42
</p>
<p>3.3 Multivariate Random Variables . . . . . . . . . . . . . . . . . . . 44
</p>
<p>3.4 Independence of Random Variables and Classes of Events . . . . . 48
</p>
<p>3.4.1 Independence of Random Vectors . . . . . . . . . . . . . . 48
</p>
<p>3.4.2 Independence of Classes of Events . . . . . . . . . . . . . 50
</p>
<p>3.4.3 Relations Between the Introduced Notions . . . . . . . . . 52
</p>
<p>3.5 On Infinite Sequences of Random Variables . . . . . . . . . . . . 56
</p>
<p>3.6 Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
</p>
<p>3.6.1 Integral with Respect to Measure . . . . . . . . . . . . . . 56
</p>
<p>3.6.2 The Stieltjes Integral . . . . . . . . . . . . . . . . . . . . . 57
</p>
<p>3.6.3 Integrals of Multivariate Random Variables.
</p>
<p>The Distribution of the Sum of Independent
</p>
<p>Random Variables . . . . . . . . . . . . . . . . . . . . . . 59
</p>
<p>xix</p>
<p/>
</div>
<div class="page"><p/>
<p>xx Contents
</p>
<p>4 Numerical Characteristics of Random Variables . . . . . . . . . . . 65
</p>
<p>4.1 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
</p>
<p>4.2 Conditional Distribution Functions and Conditional
</p>
<p>Expectations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
</p>
<p>4.3 Expectations of Functions of Independent Random Variables . . . 74
</p>
<p>4.4 Expectations of Sums of a Random Number of Random
</p>
<p>Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
</p>
<p>4.5 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
</p>
<p>4.6 The Correlation Coefficient and Other Numerical
</p>
<p>Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
</p>
<p>4.7 Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
</p>
<p>4.7.1 Moment Inequalities . . . . . . . . . . . . . . . . . . . . . 87
</p>
<p>4.7.2 Inequalities for Probabilities . . . . . . . . . . . . . . . . . 89
</p>
<p>4.8 Extension of the Notion of Conditional Expectation . . . . . . . . 91
</p>
<p>4.8.1 Definition of Conditional Expectation . . . . . . . . . . . . 91
</p>
<p>4.8.2 Properties of Conditional Expectations . . . . . . . . . . . 95
</p>
<p>4.9 Conditional Distributions . . . . . . . . . . . . . . . . . . . . . . 99
</p>
<p>5 Sequences of Independent Trials with Two Outcomes . . . . . . . . . 107
</p>
<p>5.1 Laws of Large Numbers . . . . . . . . . . . . . . . . . . . . . . . 107
</p>
<p>5.2 The Local Limit Theorem and Its Refinements . . . . . . . . . . . 109
</p>
<p>5.2.1 The Local Limit Theorem . . . . . . . . . . . . . . . . . . 109
</p>
<p>5.2.2 Refinements of the Local Theorem . . . . . . . . . . . . . 111
</p>
<p>5.2.3 The Local Limit Theorem for the Polynomial
</p>
<p>Distributions . . . . . . . . . . . . . . . . . . . . . . . . . 114
</p>
<p>5.3 The de Moivre&ndash;Laplace Theorem and Its Refinements . . . . . . . 114
</p>
<p>5.4 The Poisson Theorem and Its Refinements . . . . . . . . . . . . . 117
</p>
<p>5.4.1 Quantifying the Closeness of Poisson Distributions to
</p>
<p>Those of the Sums Sn . . . . . . . . . . . . . . . . . . . . 117
</p>
<p>5.4.2 The Triangular Array Scheme. The Poisson Theorem . . . 120
</p>
<p>5.5 Inequalities for Large Deviation Probabilities in the Bernoulli
</p>
<p>Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>6 On Convergence of Random Variables and Distributions . . . . . . . 129
</p>
<p>6.1 Convergence of Random Variables . . . . . . . . . . . . . . . . . 129
</p>
<p>6.1.1 Types of Convergence . . . . . . . . . . . . . . . . . . . . 129
</p>
<p>6.1.2 The Continuity Theorem . . . . . . . . . . . . . . . . . . . 134
</p>
<p>6.1.3 Uniform Integrability and Its Consequences . . . . . . . . 134
</p>
<p>6.2 Convergence of Distributions . . . . . . . . . . . . . . . . . . . . 140
</p>
<p>6.3 Conditions for Weak Convergence . . . . . . . . . . . . . . . . . 147
</p>
<p>7 Characteristic Functions . . . . . . . . . . . . . . . . . . . . . . . . . 153
</p>
<p>7.1 Definition and Properties of Characteristic Functions . . . . . . . . 153
</p>
<p>7.1.1 Properties of Characteristic Functions . . . . . . . . . . . . 154
</p>
<p>7.1.2 The Properties of Ch.F.s Related to the Structure of the
</p>
<p>Distribution of ξ . . . . . . . . . . . . . . . . . . . . . . . 159
</p>
<p>7.2 Inversion Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . 161</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xxi
</p>
<p>7.2.1 The Inversion Formula for Densities . . . . . . . . . . . . 161
</p>
<p>7.2.2 The Inversion Formula for Distributions . . . . . . . . . . 163
</p>
<p>7.2.3 The Inversion Formula in L2. The Class of Functions that
</p>
<p>Are Both Densities and Ch.F.s . . . . . . . . . . . . . . . . 164
</p>
<p>7.3 The Continuity (Convergence) Theorem . . . . . . . . . . . . . . 167
</p>
<p>7.4 The Application of Characteristic Functions in the Proof of the
</p>
<p>Poisson Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
</p>
<p>7.5 Characteristic Functions of Multivariate Distributions.
</p>
<p>The Multivariate Normal Distribution . . . . . . . . . . . . . . . . 171
</p>
<p>7.6 Other Applications of Characteristic Functions. The Properties of
</p>
<p>the Gamma Distribution . . . . . . . . . . . . . . . . . . . . . . . 175
</p>
<p>7.6.1 Stability of the Distributions �α,σ 2 and Kα,σ . . . . . . . . 175
</p>
<p>7.6.2 The Ŵ-distribution and its properties . . . . . . . . . . . . 176
</p>
<p>7.7 Generating Functions. Application to Branching Processes.
</p>
<p>A Problem on Extinction . . . . . . . . . . . . . . . . . . . . . . 180
</p>
<p>7.7.1 Generating Functions . . . . . . . . . . . . . . . . . . . . 180
</p>
<p>7.7.2 The Simplest Branching Processes . . . . . . . . . . . . . 180
</p>
<p>8 Sequences of Independent Random Variables. Limit Theorems . . . 185
</p>
<p>8.1 The Law of Large Numbers . . . . . . . . . . . . . . . . . . . . . 185
</p>
<p>8.2 The Central Limit Theorem for Identically Distributed Random
</p>
<p>Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
</p>
<p>8.3 The Law of Large Numbers for Arbitrary Independent Random
</p>
<p>Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
</p>
<p>8.4 The Central Limit Theorem for Sums of Arbitrary Independent
</p>
<p>Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>8.5 Another Approach to Proving Limit Theorems. Estimating
</p>
<p>Approximation Rates . . . . . . . . . . . . . . . . . . . . . . . . 209
</p>
<p>8.6 The Law of Large Numbers and the Central Limit Theorem in the
</p>
<p>Multivariate Case . . . . . . . . . . . . . . . . . . . . . . . . . . 214
</p>
<p>8.7 Integro-Local and Local Limit Theorems for Sums of Identically
</p>
<p>Distributed Random Variables with Finite Variance . . . . . . . . . 216
</p>
<p>8.7.1 Integro-Local Theorems . . . . . . . . . . . . . . . . . . . 216
</p>
<p>8.7.2 Local Theorems . . . . . . . . . . . . . . . . . . . . . . . 219
</p>
<p>8.7.3 The Proof of Theorem 8.7.1 in the General Case . . . . . . 222
</p>
<p>8.7.4 Uniform Versions of Theorems 8.7.1&ndash;8.7.3 for Random
</p>
<p>Variables Depending on a Parameter . . . . . . . . . . . . 225
</p>
<p>8.8 Convergence to Other Limiting Laws . . . . . . . . . . . . . . . . 227
</p>
<p>8.8.1 The Integral Theorem . . . . . . . . . . . . . . . . . . . . 230
</p>
<p>8.8.2 The Integro-Local and Local Theorems . . . . . . . . . . . 235
</p>
<p>8.8.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . 236
</p>
<p>9 Large Deviation Probabilities for Sums of Independent Random
</p>
<p>Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>9.1 Laplace&rsquo;s and Cram&eacute;r&rsquo;s Transforms. The Rate Function . . . . . . 240</p>
<p/>
</div>
<div class="page"><p/>
<p>xxii Contents
</p>
<p>9.1.1 The Cram&eacute;r Condition. Laplace&rsquo;s and Cram&eacute;r&rsquo;s
</p>
<p>Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . 240
</p>
<p>9.1.2 The Large Deviation Rate Function . . . . . . . . . . . . . 243
</p>
<p>9.2 A Relationship Between Large Deviation Probabilities for Sums
</p>
<p>of Random Variables and Those for Sums of Their Cram&eacute;r
</p>
<p>Transforms. The Probabilistic Meaning of the Rate Function . . . . 250
</p>
<p>9.2.1 A Relationship Between Large Deviation Probabilities for
</p>
<p>Sums of Random Variables and Those for Sums of Their
</p>
<p>Cram&eacute;r Transforms . . . . . . . . . . . . . . . . . . . . . 250
</p>
<p>9.2.2 The Probabilistic Meaning of the Rate Function . . . . . . 251
</p>
<p>9.2.3 The Large Deviations Principle . . . . . . . . . . . . . . . 254
</p>
<p>9.3 Integro-Local, Integral and Local Theorems on Large Deviation
</p>
<p>Probabilities in the Cram&eacute;r Range . . . . . . . . . . . . . . . . . . 256
</p>
<p>9.3.1 Integro-Local and Integral Theorems . . . . . . . . . . . . 256
</p>
<p>9.3.2 Local Theorems . . . . . . . . . . . . . . . . . . . . . . . 261
</p>
<p>9.4 Integro-Local Theorems at the Boundary of the Cram&eacute;r Range . . . 264
</p>
<p>9.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 264
</p>
<p>9.4.2 The Probabilities of Large Deviations of Sn in an
</p>
<p>o(n)-Vicinity of the Point α+n; the Case ψ &prime;&prime;(λ+) &lt;&infin; . . . 264
9.4.3 The Class of Distributions ER. The Probability of Large
</p>
<p>Deviations of Sn in an o(n)-Vicinity of the Point α+n for
Distributions F from the Class ER in Case ψ &prime;&prime;(λ+)=&infin; . . 266
</p>
<p>9.4.4 On the Large Deviation Probabilities in the Range α &gt; α+
for Distributions from the Class ER . . . . . . . . . . . . . 269
</p>
<p>9.5 Integral and Integro-Local Theorems on Large Deviation
</p>
<p>Probabilities for Sums Sn when the Cram&eacute;r Condition Is not Met . 269
</p>
<p>9.5.1 Integral Theorems . . . . . . . . . . . . . . . . . . . . . . 270
</p>
<p>9.5.2 Integro-Local Theorems . . . . . . . . . . . . . . . . . . . 271
</p>
<p>9.6 Integro-Local Theorems on the Probabilities of Large Deviations
</p>
<p>of Sn Outside the Cram&eacute;r Range (Under the Cram&eacute;r Condition) . . 274
</p>
<p>10 Renewal Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
</p>
<p>10.1 Renewal Processes. Renewal Functions . . . . . . . . . . . . . . . 277
</p>
<p>10.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 277
</p>
<p>10.1.2 The Integral Renewal Theorem for Non-identically
</p>
<p>Distributed Summands . . . . . . . . . . . . . . . . . . . . 280
</p>
<p>10.2 The Key Renewal Theorem in the Arithmetic Case . . . . . . . . . 285
</p>
<p>10.3 The Excess and Defect of a Random Walk. Their Limiting
</p>
<p>Distribution in the Arithmetic Case . . . . . . . . . . . . . . . . . 290
</p>
<p>10.4 The Renewal Theorem and the Limiting Behaviour of the Excess
</p>
<p>and Defect in the Non-arithmetic Case . . . . . . . . . . . . . . . 293
</p>
<p>10.5 The Law of Large Numbers and the Central Limit Theorem for
</p>
<p>Renewal Processes . . . . . . . . . . . . . . . . . . . . . . . . . . 298
</p>
<p>10.5.1 The Law of Large Numbers . . . . . . . . . . . . . . . . . 298
</p>
<p>10.5.2 The Central Limit Theorem . . . . . . . . . . . . . . . . . 299</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xxiii
</p>
<p>10.5.3 A Theorem on the Finiteness of the Infimum of the
</p>
<p>Cumulative Sums . . . . . . . . . . . . . . . . . . . . . . 300
</p>
<p>10.5.4 Stochastic Inequalities. The Law of Large Numbers and
</p>
<p>the Central Limit Theorem for the Maximum of Sums of
</p>
<p>Non-identically Distributed Random Variables Taking
</p>
<p>Values of Both Signs . . . . . . . . . . . . . . . . . . . . . 302
</p>
<p>10.5.5 Extension of Theorems 10.5.1 and 10.5.2 to Random
</p>
<p>Variables Assuming Values of Both Signs . . . . . . . . . . 304
</p>
<p>10.5.6 The Local Limit Theorem . . . . . . . . . . . . . . . . . . 306
</p>
<p>10.6 Generalised Renewal Processes . . . . . . . . . . . . . . . . . . . 307
</p>
<p>10.6.1 Definition and Some Properties . . . . . . . . . . . . . . . 307
</p>
<p>10.6.2 The Central Limit Theorem . . . . . . . . . . . . . . . . . 309
</p>
<p>10.6.3 The Integro-Local Theorem . . . . . . . . . . . . . . . . . 311
</p>
<p>11 Properties of the Trajectories of RandomWalks. Zero-One Laws . . 315
</p>
<p>11.1 Zero-One Laws. Upper and Lower Functions . . . . . . . . . . . . 315
</p>
<p>11.1.1 Zero-One Laws . . . . . . . . . . . . . . . . . . . . . . . 315
</p>
<p>11.1.2 Lower and Upper Functions . . . . . . . . . . . . . . . . . 318
</p>
<p>11.2 Convergence of Series of Independent Random Variables . . . . . 320
</p>
<p>11.3 The Strong Law of Large Numbers . . . . . . . . . . . . . . . . . 323
</p>
<p>11.4 The Strong Law of Large Numbers for Arbitrary Independent
</p>
<p>Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
</p>
<p>11.5 The Strong Law of Large Numbers for Generalised Renewal
</p>
<p>Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
</p>
<p>11.5.1 The Strong Law of Large Numbers for Renewal Processes . 330
</p>
<p>11.5.2 The Strong Law of Large Numbers for Generalised
</p>
<p>Renewal Processes . . . . . . . . . . . . . . . . . . . . . . 331
</p>
<p>12 RandomWalks and Factorisation Identities . . . . . . . . . . . . . . 333
</p>
<p>12.1 Factorisation Identities . . . . . . . . . . . . . . . . . . . . . . . . 333
</p>
<p>12.1.1 Factorisation . . . . . . . . . . . . . . . . . . . . . . . . . 333
</p>
<p>12.1.2 The Canonical Factorisation of the Function
</p>
<p>fz(λ)= 1 &minus; zϕ(λ) . . . . . . . . . . . . . . . . . . . . . . 335
12.1.3 The Second Factorisation Identity . . . . . . . . . . . . . . 336
</p>
<p>12.2 Some Consequences of Theorems 12.1.1&ndash;12.1.3 . . . . . . . . . . 340
</p>
<p>12.2.1 Direct Consequences . . . . . . . . . . . . . . . . . . . . 340
</p>
<p>12.2.2 A Generalisation of the Strong Law of Large Numbers . . . 343
</p>
<p>12.3 Pollaczek&ndash;Spitzer&rsquo;s Identity. An Identity for S = supk&ge;0 Sk . . . . 344
12.3.1 Pollaczek&ndash;Spitzer&rsquo;s Identity . . . . . . . . . . . . . . . . . 345
</p>
<p>12.3.2 An Identity for S = supk&ge;0 Sk . . . . . . . . . . . . . . . . 347
12.4 The Distribution of S in Insurance Problems and Queueing
</p>
<p>Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
</p>
<p>12.4.1 Random Walks in Risk Theory . . . . . . . . . . . . . . . 348
</p>
<p>12.4.2 Queueing Systems . . . . . . . . . . . . . . . . . . . . . . 349
</p>
<p>12.4.3 Stochastic Models in Continuous Time . . . . . . . . . . . 350</p>
<p/>
</div>
<div class="page"><p/>
<p>xxiv Contents
</p>
<p>12.5 Cases Where Factorisation Components Can Be Found in an
</p>
<p>Explicit Form. The Non-lattice Case . . . . . . . . . . . . . . . . 351
</p>
<p>12.5.1 Preliminary Notes on the Uniqueness of Factorisation . . . 351
</p>
<p>12.5.2 Classes of Distributions on the Positive Half-Line with
</p>
<p>Rational Ch.F.s . . . . . . . . . . . . . . . . . . . . . . . . 354
</p>
<p>12.5.3 Explicit Canonical Factorisation of the Function v(λ) in
</p>
<p>the Case when the Right Tail of the Distribution F Is an
</p>
<p>Exponential Polynomial . . . . . . . . . . . . . . . . . . . 355
</p>
<p>12.5.4 Explicit Factorisation of the Function v(λ) when the Left
</p>
<p>Tail of the Distribution F Is an Exponential Polynomial . . 361
</p>
<p>12.5.5 Explicit Canonical Factorisation for the Function v0(λ) . . 362
</p>
<p>12.6 Explicit Form of Factorisation in the Arithmetic Case . . . . . . . 364
</p>
<p>12.6.1 Preliminary Remarks on the Uniqueness of Factorisation . 365
</p>
<p>12.6.2 The Classes of Distributions on the Positive Half-Line
</p>
<p>with Rational Generating Functions . . . . . . . . . . . . . 366
</p>
<p>12.6.3 Explicit Canonical Factorisation of the Function v(z) in
</p>
<p>the Case when the Right Tail of the Distribution F Is an
</p>
<p>Exponential Polynomial . . . . . . . . . . . . . . . . . . . 367
</p>
<p>12.6.4 Explicit Canonical Factorisation of the Function v(z)
</p>
<p>when the Left Tail of the Distribution F Is an Exponential
</p>
<p>Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . 370
</p>
<p>12.6.5 Explicit Factorisation of the Function v0(z) . . . . . . . . . 371
</p>
<p>12.7 Asymptotic Properties of the Distributions of χ&plusmn; and S . . . . . . 372
12.7.1 The Asymptotics of P(χ+ &gt; x |η+ &lt;&infin;) and P(χ0&minus; &lt;&minus;x)
</p>
<p>in the Case Eξ &le; 0 . . . . . . . . . . . . . . . . . . . . . . 373
12.7.2 The Asymptotics of P(S &gt; x) . . . . . . . . . . . . . . . . 376
</p>
<p>12.7.3 The Distribution of the Maximal Values of Generalised
</p>
<p>Renewal Processes . . . . . . . . . . . . . . . . . . . . . . 380
</p>
<p>12.8 On the Distribution of the First Passage Time . . . . . . . . . . . . 381
</p>
<p>12.8.1 The Properties of the Distributions of the Times η&plusmn; . . . . 381
12.8.2 The Distribution of the First Passage Time of an Arbitrary
</p>
<p>Level x by Arithmetic Skip-Free Walks . . . . . . . . . . . 384
</p>
<p>13 Sequences of Dependent Trials. Markov Chains . . . . . . . . . . . . 389
</p>
<p>13.1 Countable Markov Chains. Definitions and Examples.
</p>
<p>Classification of States . . . . . . . . . . . . . . . . . . . . . . . . 389
</p>
<p>13.1.1 Definition and Examples . . . . . . . . . . . . . . . . . . . 389
</p>
<p>13.1.2 Classification of States . . . . . . . . . . . . . . . . . . . . 392
</p>
<p>13.2 Necessary and Sufficient Conditions for Recurrence of States.
</p>
<p>Types of States in an Irreducible Chain. The Structure of a
</p>
<p>Periodic Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
</p>
<p>13.3 Theorems on Random Walks on a Lattice . . . . . . . . . . . . . . 398
</p>
<p>13.3.1 Symmetric Random Walks in Rk , k &ge; 2 . . . . . . . . . . . 400
13.3.2 Arbitrary Symmetric Random Walks on the Line . . . . . . 401
</p>
<p>13.4 Limit Theorems for Countable Homogeneous Chains . . . . . . . 404</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xxv
</p>
<p>13.4.1 Ergodic Theorems . . . . . . . . . . . . . . . . . . . . . . 404
</p>
<p>13.4.2 The Law of Large Numbers and the Central Limit
</p>
<p>Theorem for the Number of Visits to a Given State . . . . . 412
</p>
<p>13.5 The Behaviour of Transition Probabilities for Reducible Chains . . 412
</p>
<p>13.6 Markov Chains with Arbitrary State Spaces. Ergodicity of Chains
</p>
<p>with Positive Atoms . . . . . . . . . . . . . . . . . . . . . . . . . 414
</p>
<p>13.6.1 Markov Chains with Arbitrary State Spaces . . . . . . . . . 414
</p>
<p>13.6.2 Markov Chains Having a Positive Atom . . . . . . . . . . 420
</p>
<p>13.7 Ergodicity of Harris Markov Chains . . . . . . . . . . . . . . . . . 423
</p>
<p>13.7.1 The Ergodic Theorem . . . . . . . . . . . . . . . . . . . . 423
</p>
<p>13.7.2 On Conditions (I) and (II) . . . . . . . . . . . . . . . . . . 429
</p>
<p>13.8 Laws of Large Numbers and the Central Limit Theorem for Sums
</p>
<p>of Random Variables Defined on a Markov Chain . . . . . . . . . 436
</p>
<p>13.8.1 Random Variables Defined on a Markov Chain . . . . . . . 436
</p>
<p>13.8.2 Laws of Large Numbers . . . . . . . . . . . . . . . . . . . 437
</p>
<p>13.8.3 The Central Limit Theorem . . . . . . . . . . . . . . . . . 443
</p>
<p>14 Information and Entropy . . . . . . . . . . . . . . . . . . . . . . . . 447
</p>
<p>14.1 The Definitions and Properties of Information and Entropy . . . . 447
</p>
<p>14.2 The Entropy of a Finite Markov Chain. A Theorem on the
</p>
<p>Asymptotic Behaviour of the Information Contained in a Long
</p>
<p>Message; Its Applications . . . . . . . . . . . . . . . . . . . . . . 452
</p>
<p>14.2.1 The Entropy of a Sequence of Trials Forming a Stationary
</p>
<p>Markov Chain . . . . . . . . . . . . . . . . . . . . . . . . 452
</p>
<p>14.2.2 The Law of Large Numbers for the Amount of Information
</p>
<p>Contained in a Message . . . . . . . . . . . . . . . . . . . 453
</p>
<p>14.2.3 The Asymptotic Behaviour of the Number of the Most
</p>
<p>Common Outcomes in a Sequence of Trials . . . . . . . . 454
</p>
<p>15 Martingales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
</p>
<p>15.1 Definitions, Simplest Properties, and Examples . . . . . . . . . . . 457
</p>
<p>15.2 The Martingale Property and Random Change of Time. Wald&rsquo;s
</p>
<p>Identity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
</p>
<p>15.3 Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
</p>
<p>15.3.1 Inequalities for Martingales . . . . . . . . . . . . . . . . . 477
</p>
<p>15.3.2 Inequalities for the Number of Crossings of a Strip . . . . . 481
</p>
<p>15.4 Convergence Theorems . . . . . . . . . . . . . . . . . . . . . . . 482
</p>
<p>15.5 Boundedness of the Moments of Stochastic Sequences . . . . . . . 487
</p>
<p>16 Stationary Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
</p>
<p>16.1 Basic Notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
</p>
<p>16.2 Ergodicity (Metric Transitivity), Mixing and Weak Dependence . . 497
</p>
<p>16.3 The Ergodic Theorem . . . . . . . . . . . . . . . . . . . . . . . . 502
</p>
<p>17 Stochastic Recursive Sequences . . . . . . . . . . . . . . . . . . . . . 507
</p>
<p>17.1 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507
</p>
<p>17.2 Ergodicity and Renovating Events. Boundedness Conditions . . . . 508</p>
<p/>
</div>
<div class="page"><p/>
<p>xxvi Contents
</p>
<p>17.2.1 Ergodicity of Stochastic Recursive Sequences . . . . . . . 508
</p>
<p>17.2.2 Boundedness of Random Sequences . . . . . . . . . . . . 514
</p>
<p>17.3 Ergodicity Conditions Related to the Monotonicity of f . . . . . . 516
</p>
<p>17.4 Ergodicity Conditions for Contracting in Mean Lipschitz
</p>
<p>Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 518
</p>
<p>18 Continuous Time Random Processes . . . . . . . . . . . . . . . . . . 527
</p>
<p>18.1 General Definitions . . . . . . . . . . . . . . . . . . . . . . . . . 527
</p>
<p>18.2 Criteria of Regularity of Processes . . . . . . . . . . . . . . . . . 532
</p>
<p>19 Processes with Independent Increments . . . . . . . . . . . . . . . . 539
</p>
<p>19.1 General Properties . . . . . . . . . . . . . . . . . . . . . . . . . . 539
</p>
<p>19.2 Wiener Processes. The Properties of Trajectories . . . . . . . . . . 542
</p>
<p>19.3 The Laws of the Iterated Logarithm . . . . . . . . . . . . . . . . . 545
</p>
<p>19.4 The Poisson Process . . . . . . . . . . . . . . . . . . . . . . . . . 549
</p>
<p>19.5 Description of the Class of Processes with Independent
</p>
<p>Increments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552
</p>
<p>20 Functional Limit Theorems . . . . . . . . . . . . . . . . . . . . . . . 559
</p>
<p>20.1 Convergence to the Wiener Process . . . . . . . . . . . . . . . . . 559
</p>
<p>20.2 The Law of the Iterated Logarithm . . . . . . . . . . . . . . . . . 568
</p>
<p>20.3 Convergence to the Poisson Process . . . . . . . . . . . . . . . . . 572
</p>
<p>20.3.1 Convergence of the Processes of Cumulative Sums . . . . . 572
</p>
<p>20.3.2 Convergence of Sums of Thinning Renewal Processes . . . 575
</p>
<p>21 Markov Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
</p>
<p>21.1 Definitions and General Properties . . . . . . . . . . . . . . . . . 579
</p>
<p>21.1.1 Definition and Basic Properties . . . . . . . . . . . . . . . 579
</p>
<p>21.1.2 Transition Probability . . . . . . . . . . . . . . . . . . . . 581
</p>
<p>21.2 Markov Processes with Countable State Spaces. Examples . . . . . 583
</p>
<p>21.2.1 Basic Properties of the Process . . . . . . . . . . . . . . . 583
</p>
<p>21.2.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
</p>
<p>21.3 Branching Processes . . . . . . . . . . . . . . . . . . . . . . . . . 591
</p>
<p>21.4 Semi-Markov Processes . . . . . . . . . . . . . . . . . . . . . . . 593
</p>
<p>21.4.1 Semi-Markov Processes on the States of a Chain . . . . . . 593
</p>
<p>21.4.2 The Ergodic Theorem . . . . . . . . . . . . . . . . . . . . 594
</p>
<p>21.4.3 Semi-Markov Processes on Chain Transitions . . . . . . . 597
</p>
<p>21.5 Regenerative Processes . . . . . . . . . . . . . . . . . . . . . . . 600
</p>
<p>21.5.1 Regenerative Processes. The Ergodic Theorem . . . . . . . 600
</p>
<p>21.5.2 The Laws of Large Numbers and Central Limit Theorem
</p>
<p>for Integrals of Regenerative Processes . . . . . . . . . . . 601
</p>
<p>21.6 Diffusion Processes . . . . . . . . . . . . . . . . . . . . . . . . . 603
</p>
<p>22 Processes with Finite Second Moments. Gaussian Processes . . . . . 611
</p>
<p>22.1 Processes with Finite Second Moments . . . . . . . . . . . . . . . 611
</p>
<p>22.2 Gaussian Processes . . . . . . . . . . . . . . . . . . . . . . . . . 614
</p>
<p>22.3 Prediction Problem . . . . . . . . . . . . . . . . . . . . . . . . . 616</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xxvii
</p>
<p>Appendix 1 Extension of a Probability Measure . . . . . . . . . . . . . . 619
</p>
<p>Appendix 2 Kolmogorov&rsquo;s Theorem on Consistent Distributions . . . . 625
</p>
<p>Appendix 3 Elements of Measure Theory and Integration . . . . . . . . 629
</p>
<p>3.1 Measure Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
</p>
<p>3.2 The Integral with Respect to a Probability Measure . . . . . . . . . 630
</p>
<p>3.2.1 The Integrals of a Simple Function . . . . . . . . . . . . . 630
</p>
<p>3.2.2 The Integrals of an Arbitrary Function . . . . . . . . . . . 631
</p>
<p>3.2.3 Properties of Integrals . . . . . . . . . . . . . . . . . . . . 634
</p>
<p>3.3 Further Properties of Integrals . . . . . . . . . . . . . . . . . . . . 635
</p>
<p>3.3.1 Convergence Theorems . . . . . . . . . . . . . . . . . . . 635
</p>
<p>3.3.2 Connection to Integration with Respect to a Measure on
</p>
<p>the Real Line . . . . . . . . . . . . . . . . . . . . . . . . . 636
</p>
<p>3.3.3 Product Measures and Iterated Integrals . . . . . . . . . . . 638
</p>
<p>3.4 The Integral with Respect to an Arbitrary Measure . . . . . . . . . 640
</p>
<p>3.5 The Lebesgue Decomposition Theorem and the Radon&ndash;Nikodym
</p>
<p>Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643
</p>
<p>3.6 Weak Convergence and Convergence in Total Variation of
</p>
<p>Distributions in Arbitrary Spaces . . . . . . . . . . . . . . . . . . 649
</p>
<p>3.6.1 Weak Convergence . . . . . . . . . . . . . . . . . . . . . 649
</p>
<p>3.6.2 Convergence in Total Variation . . . . . . . . . . . . . . . 652
</p>
<p>Appendix 4 The Helly and Arzel&agrave;&ndash;Ascoli Theorems . . . . . . . . . . . 655
</p>
<p>Appendix 5 The Proof of the Berry&ndash;Esseen Theorem . . . . . . . . . . . 659
</p>
<p>Appendix 6 The Basic Properties of Regularly Varying Functions and
</p>
<p>Subexponential Distributions . . . . . . . . . . . . . . . . . . . . . . 665
</p>
<p>6.1 General Properties of Regularly Varying Functions . . . . . . . . . 665
</p>
<p>6.2 The Basic Asymptotic Properties . . . . . . . . . . . . . . . . . . 668
</p>
<p>6.3 The Asymptotic Properties of the Transforms of R.V.F.s
</p>
<p>(Abel-Type Theorems) . . . . . . . . . . . . . . . . . . . . . . . . 672
</p>
<p>6.4 Subexponential Distributions and Their Properties . . . . . . . . . 674
</p>
<p>Appendix 7 The Proofs of Theorems on Convergence to Stable Laws . . 687
</p>
<p>7.1 The Integral Limit Theorem . . . . . . . . . . . . . . . . . . . . . 687
</p>
<p>7.2 The Integro-Local and Local Limit Theorems . . . . . . . . . . . . 699
</p>
<p>Appendix 8 Upper and Lower Bounds for the Distributions of the
</p>
<p>Sums and the Maxima of the Sums of Independent Random
</p>
<p>Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 703
</p>
<p>8.1 Upper Bounds Under the Cram&eacute;r Condition . . . . . . . . . . . . 703
</p>
<p>8.2 Upper Bounds when the Cram&eacute;r Condition Is Not Met . . . . . . . 704
</p>
<p>8.3 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
</p>
<p>Appendix 9 Renewal Theorems . . . . . . . . . . . . . . . . . . . . . . . 715
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723</p>
<p/>
</div>
<div class="page"><p/>
<p>xxviii Contents
</p>
<p>Index of Basic Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 725
</p>
<p>Subject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Discrete Spaces of Elementary Events
</p>
<p>Abstract Section 1.1 introduces the fundamental concept of probability space,
</p>
<p>along with some basic terminology and properties of probability when it is easy
</p>
<p>to do, i.e. in the simple case of random experiments with finitely or at most count-
</p>
<p>ably many outcomes. The classical scheme of finitely many equally likely outcomes
</p>
<p>is discussed in more detail in Sect. 1.2. Then the Bernoulli scheme is introduced and
</p>
<p>the properties of the binomial distribution are studied in Sect. 1.3. Sampling without
</p>
<p>replacement from a large population is considered, and convergence of the emerging
</p>
<p>hypergeometric distributions to the binomial one is formally proved. The inclusion-
</p>
<p>exclusion formula for the probabilities of unions of events is derived and illustrated
</p>
<p>by some applications in Sect. 1.4.
</p>
<p>1.1 Probability Space
</p>
<p>To mathematically describe experiments with random outcomes, we will first of all
</p>
<p>need the notion of the space of elementary events (or outcomes) corresponding to the
experiment under consideration. We will denote by Ω any set such that each result
</p>
<p>of the experiment we are interested in can be uniquely specified by the elements
</p>
<p>of Ω .
</p>
<p>In the simplest experiments we usually deal with finite spaces of elementary out-
comes. In the coin tossing example we considered above, Ω consists of two ele-
</p>
<p>ments, &ldquo;heads&rdquo; and &ldquo;tails&rdquo;. In the die rolling experiment, the space Ω is also finite
</p>
<p>and consists of 6 elements. However, even for tossing a coin (or rolling a die) one
</p>
<p>can arrange such experiments for which finite spaces of elementary events will not
</p>
<p>suffice. For instance, consider the following experiment: a coin is tossed until heads
</p>
<p>shows for the first time, and then the experiment is stopped. If t designates tails in
</p>
<p>a toss and h heads, then an &ldquo;elementary outcome&rdquo; of the experiment can be repre-
</p>
<p>sented by a sequence (t t . . . th). There are infinitely many such sequences, and all
</p>
<p>of them are different, so there is no way to describe unambiguously all the outcomes
</p>
<p>of the experiment by elements of a finite space.
</p>
<p>Consider finite or countably infinite spaces of elementary events Ω . These are
</p>
<p>the so-called discrete spaces. We will denote the elements of a space Ω by the letter
ω and call them elementary events (or elementary outcomes).
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_1, &copy; Springer-Verlag London 2013
</p>
<p>1</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_1">http://dx.doi.org/10.1007/978-1-4471-5201-9_1</a></div>
</div>
<div class="page"><p/>
<p>2 1 Discrete Spaces of Elementary Events
</p>
<p>The notion of the space of elementary events itself is mathematically undefinable:
</p>
<p>it is a primitive one, like the notion of a point in geometry. The specific nature of Ω
</p>
<p>will, as a rule, be of no interest to us.
</p>
<p>Any subset A &sube; Ω will be called an event (the event A occurs if any of the
elementary outcomes ω &isin;A occurs).
</p>
<p>The union or sum of two events A and B is the event A&cup;B (which may also be
denoted by A+ B) consisting of the elementary outcomes which belong to at least
one of the events A and B . The product or intersection AB (which is often denoted
by A&cap;B as well) is the event consisting of all elementary events belonging to both
A and B . The difference of the events A and B is the set A&minus;B (also often denoted
by A\B) consisting of all elements of A not belonging to B . The set Ω is called the
certain event. The empty set &empty; is called the impossible event. The set A=Ω &minus;A
is called the complementary event of A. Two events A and B are mutually exclusive
if AB =&empty;.
</p>
<p>Let, for instance, our experiment consist in rolling a die twice. Here one can take
</p>
<p>the space of elementary events to be the set consisting of 36 elements (i, j), where i
</p>
<p>and j run from 1 to 6 and denote the numbers of points that show up in the first and
</p>
<p>second roll respectively. The events A= {i + j &le; 3} and B = {j = 6} are mutually
exclusive. The product of the events A and C = {j is even} is the event (1,2). Note
that if we were interested in the events related to the first roll only, we could consider
</p>
<p>a smaller space of elementary events consisting of just 6 elements i = 1,2, . . . ,6.
One says that the probabilities of elementary events are given if a nonnegative
</p>
<p>real-valued function P is given on Ω such that
&sum;
</p>
<p>ω&isin;Ω P(ω)= 1 (one also says that
the function P specifies a probability distribution on Ω).
</p>
<p>The probability of an event A is the number
</p>
<p>P(A) :=
&sum;
</p>
<p>ω&isin;A
P(ω).
</p>
<p>This definition is consistent, for the series on the right hand side is absolutely con-
</p>
<p>vergent.
</p>
<p>We note here that specific numerical values of the function P will also be of no
</p>
<p>interest to us: this is just an issue of the practical value of the model. For instance,
</p>
<p>it is clear that, in the case of a symmetric die, for the outcomes 1,2, . . . ,6 one
</p>
<p>should put P(1)= P(2)= &middot; &middot; &middot; = P(6)= 1/6; for a symmetric coin, one has to choose
the values P(h) = P(t) = 1/2 and not any others. In the experiment of tossing a
coin until heads shows for the first time, one should put P(h)= 1/2, P(th)= 1/22,
P(t th) = 1/23, . . . . Since
</p>
<p>&sum;&infin;
n=1 2
</p>
<p>&minus;n = 1, the function P given in this way on the
outcomes of the form (t . . . th) will define a probability distribution on Ω . For ex-
</p>
<p>ample, to calculate the probability that the experiment stops on an even step (that is,
</p>
<p>the probability of the event composed of the outcomes (th), (t t th), . . . ), one should
</p>
<p>consider the sum of the corresponding probabilities which is equal to
</p>
<p>&infin;&sum;
</p>
<p>n=1
2&minus;2n = 1
</p>
<p>4
&times; 4
</p>
<p>3
= 1
</p>
<p>3
.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Probability Space 3
</p>
<p>In the experiments mentioned in the Introduction, where one had to guess when
</p>
<p>a device will break down&mdash;before a given time (the event A) or after it, quantita-
</p>
<p>tive estimates of the probability P(A) can usually only be based on the results of the
</p>
<p>experiments themselves. The methods of estimating unknown probabilities from ob-
</p>
<p>servation results are studied in Mathematical Statistics, the subject-matter of which
</p>
<p>will be exemplified somewhat later by a problem from this chapter.
</p>
<p>Note further that by no means can one construct models with discrete spaces of
</p>
<p>elementary events for all experiments. For example, suppose that one is measuring
</p>
<p>the energy of particles whose possible values fill the interval [0,V ], V &gt; 0, but the
set of points of this interval (that is, the set of elementary events) is continuous.
</p>
<p>Or suppose that the result of an experiment is a patient&rsquo;s electrocardiogram. In this
</p>
<p>case, the result of the experiment is an element of some functional space. In such
</p>
<p>cases, more general schemes are needed.
</p>
<p>From the above definitions, making use of the absolute convergence of the series&sum;
ω&isin;A P(ω), one can easily derive the following properties of probability:
</p>
<p>(1) P(&empty;)= 0, P(Ω)= 1.
(2) P(A + B) =
</p>
<p>&sum;
ω&isin;A&cup;B P(ω) =
</p>
<p>&sum;
ω&isin;A P(ω) +
</p>
<p>&sum;
ω&isin;B P(ω) &minus;
</p>
<p>&sum;
ω&isin;A&cap;B P(ω) =
</p>
<p>P(A)+ P(B)&minus; P(AB).
(3) P(A)= 1 &minus; P(A).
</p>
<p>This entails, in particular, that, for disjoint (mutually exclusive) events A and B ,
</p>
<p>P(A+B)= P(A)+ P(B).
</p>
<p>This property of the additivity of probability continues to hold for an arbitrary
number of disjoint events A1,A2, . . . : if AiAj =&empty; for i 
= j , then
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>k=1
Ak
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(Ak). (1.1.1)
</p>
<p>This follows from the equality
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>k=1
Ak
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>k=1
P(Ak)
</p>
<p>and the fact that P(
⋃&infin;
</p>
<p>k=n+1 Ak)&rarr; 0 as n &rarr; &infin;. To prove the last relation, first
enumerate the elementary events. Then we will be dealing with the sequence
</p>
<p>ω1,ω2, . . . ;
⋃
</p>
<p>ωk = Ω , P(
⋃
</p>
<p>k&gt;nωk) =
&sum;
</p>
<p>k&gt;n P(ωk) &rarr; 0 as n &rarr; &infin;. Denote by
nk the number of events Aj such that ωk &isin; Aj = Ank ; nk = 0 if ωkAj = &empty; for
all j . If nk &le; N &lt; &infin; for all k, then the events Aj with j &gt; N are empty and
the desired relation is obvious. If Ns := maxk&le;s nk &rarr;&infin; as s &rarr;&infin;, then one has⋃
</p>
<p>j&gt;nAj &sub;
⋃
</p>
<p>k&gt;s ωk for n &gt;Ns , and therefore
</p>
<p>P
</p>
<p>(⋃
</p>
<p>j&gt;n
</p>
<p>Aj
</p>
<p>)
&le; P
</p>
<p>(⋃
</p>
<p>k&gt;s
</p>
<p>ωk
</p>
<p>)
=
&sum;
</p>
<p>k&gt;s
</p>
<p>P(ωk)&rarr; 0 as s &rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Discrete Spaces of Elementary Events
</p>
<p>The required relation is proved.
</p>
<p>For arbitrary A and B , one has P(A+ B) &le; P(A)+ P(B). A similar inequality
also holds for the sum of an arbitrary number of events:
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>k=1
Ak
</p>
<p>)
&le;
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(Ak).
</p>
<p>This follows from (1.1.1) and the representation of
⋃
</p>
<p>Ak as the union
⋃
</p>
<p>AkBk of
</p>
<p>disjoint events AkBk , where Bk =
⋃
</p>
<p>j&lt;kAj . It remains to note that P(AkBk) &le;
P(Ak).
</p>
<p>Now we will consider several important special cases.
</p>
<p>1.2 The Classical Scheme
</p>
<p>Let Ω consist of n elements and all the outcomes be equally likely, that is P(ω)=
1/n for any ω &isin; Ω . In this case, the probability of any event A is defined by the
formula
</p>
<p>P(A) := 1
n
{number of elements of A}.
</p>
<p>This is the so-called classical definition of probability (the term uniform discrete
distribution is also used).
</p>
<p>Let a set {a1, a2, . . . , an} be given, which we will call the general popula-
tion. A sample of size k from the general population is an ordered sequence
(aj1 , aj2 , . . . , ajk ). One can form this sequence as follows: the first element aj1 is
</p>
<p>chosen from the whole population. The next element aj2 we choose from the general
</p>
<p>population without the element aj1 ; the element aj3 is chosen from the general pop-
</p>
<p>ulation without the elements aj1 and aj2 , and so on. Samples obtained in such a way
</p>
<p>are called samples without replacement. Clearly, one must have k &le; n in this case.
The number of such samples of size k coincides with the number of arrangements
</p>
<p>of k elements from n:
</p>
<p>(n)k := n(n&minus; 1)(n&minus; 2) &middot; &middot; &middot; (n&minus; k + 1).
</p>
<p>Indeed, according to the sampling process, in the first position we can have any
</p>
<p>element of the general population, in the second position any of the remaining
</p>
<p>(n&minus; 1) elements, and so on. We could prove this more formally by induction on k.
Assign to each of the samples without replacement the probability 1/(n)k . Such
</p>
<p>a sample will be called random. This is clearly the classical scheme.
Calculate the probability that aj1 = a1 and aj2 = a2. Since the remaining k &minus; 2
</p>
<p>positions can be occupied by any of the remaining n &minus; 2 elements of the general
population, the number of samples without replacement having elements a1 and a2</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 The Classical Scheme 5
</p>
<p>in the first two positions equals (n&minus; 2)k&minus;2. Therefore the probability of that event
is equal to
</p>
<p>(n&minus; 2)k&minus;2
(n)k
</p>
<p>= 1
n(n&minus; 1) .
</p>
<p>One can think of a sample without replacement as the result of sequential sampling
</p>
<p>from a collection of enumerated balls placed in an urn. Sampled balls are not re-
</p>
<p>turned back to the urn.
</p>
<p>However, one can form a sample in another way as well. One takes a ball out of
</p>
<p>the urn and memorises it. Then the ball is returned to the urn, and one again picks
</p>
<p>a ball from the urn; this ball is also memorised and put back to the urn, and so on.
</p>
<p>The sample obtained in this way is called a sample with replacement. At each step,
one can pick any of the n balls. There are k such steps, so that the total number of
</p>
<p>such samples will be nk . If we assign the probability of 1/nk to each sample, this
</p>
<p>will also be a classical scheme situation.
</p>
<p>Calculate, for instance, the probability that, in a sample with replacement of size
</p>
<p>k &le; n, all the elements will be different. The number of samples of elements without
repetitions is the same as the number of samples without replacement, i.e. (n)k .
</p>
<p>Therefore the desired probability is (n)k/n
k .
</p>
<p>We now return to sampling without replacement for the general population
</p>
<p>{a1, a2, . . . , an}. We will be interested in the number of samples of size k &le; n which
differ from each other in their composition only. The number of samples without
</p>
<p>replacement of size k which have the same composition and are only distinguished
</p>
<p>by the order of their elements is k! Hence the number of samples of different com-
position equals
</p>
<p>(n)k
</p>
<p>k! =
(
n
</p>
<p>k
</p>
<p>)
.
</p>
<p>This is the number of combinations of k items chosen from a total of n for 0 &le;
k &le; n.1 If the initial sample is random, we again get the classical probability scheme,
for the probability of each new sample is
</p>
<p>k!
(n)k
</p>
<p>= 1(n
k
</p>
<p>) .
</p>
<p>Let our urn contain n balls, of which n1 are black and n&minus;n1 white. We sample k
balls without replacement. What is the probability that there will be exactly k1 black
</p>
<p>balls in the sample? The total number of samples which differ in the composition
</p>
<p>is, as was shown above,
(
n
k
</p>
<p>)
. There are
</p>
<p>(
n1
k1
</p>
<p>)
ways to choose k1 black balls from the
</p>
<p>totality of n1 black balls. The remaining k &minus; k1 white balls can be chosen from the
totality of n &minus; n1 white balls in
</p>
<p>(
n&minus;n1
k&minus;k1
</p>
<p>)
ways. Note that clearly any collection of
</p>
<p>black balls can be combined with any collection of white balls. Therefore the total
</p>
<p>1In what follows, we put
(
n
k
</p>
<p>)
= 0 for k &lt; 0 and k &gt; n.</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Discrete Spaces of Elementary Events
</p>
<p>number of samples of size k which differ in composition and contain exactly k1
black balls is
</p>
<p>(
n1
k1
</p>
<p>)(
n&minus;n1
k&minus;k1
</p>
<p>)
. Thus the desired probability is equal to
</p>
<p>Pn1,n(k1, k)=
(
n1
</p>
<p>k1
</p>
<p>)(
n&minus; n1
k &minus; k1
</p>
<p>)/(
n
</p>
<p>k
</p>
<p>)
.
</p>
<p>The collection of numbers Pn1,n(0, k), Pn1,n(1, k), . . . ,Pn1,n(k, k) forms the so-
</p>
<p>called hypergeometric distribution. From the derived formula it follows, in particu-
lar, that, for any 0 &lt; n1 &lt; n,
</p>
<p>k&sum;
</p>
<p>k1=0
</p>
<p>(
n1
</p>
<p>k1
</p>
<p>)(
n&minus; n1
k&minus; k1
</p>
<p>)
=
(
n
</p>
<p>k
</p>
<p>)
.
</p>
<p>Example 1.2.1 In the 1980s, a version of a lottery called &ldquo;Sportloto 6 out of 49&rdquo;
had became rather popular in Russia. A gambler chooses six from the totality of
</p>
<p>49 sports (designated just by numbers). The prize amount is determined by how
</p>
<p>many sports he guesses correctly from another group of six sports, to be drawn at
</p>
<p>random by a mechanical device in front of the public. What is the probability that
</p>
<p>the gambler correctly guesses all six sports? A similar question could be asked about
</p>
<p>five sports, and so on.
</p>
<p>It is not difficult to see that this is nothing else but a problem on the hypergeo-
</p>
<p>metric distribution where the gambler has labelled as &ldquo;white&rdquo; six items in a general
</p>
<p>population consisting of 49 items. Therefore the probability that, of the six items
</p>
<p>chosen at random, k1 will turn out to be &ldquo;white&rdquo; (i.e. will coincide with those la-
</p>
<p>belled by the gambler) is equal to P6,49(k1, k), where the sample size k equals 6.
</p>
<p>For example, the probability of guessing all six sports correctly is
</p>
<p>P6,49(6,6)=
(
</p>
<p>49
</p>
<p>6
</p>
<p>)&minus;1
&asymp; 7.2 &times; 10&minus;8.
</p>
<p>In connection with the hypergeometric distribution, one could comment on the
</p>
<p>nature of problems in Probability Theory and Mathematical Statistics. Knowing the
</p>
<p>composition of the general population, we can use the hypergeometric distribution
</p>
<p>to find out what chances different compositions of the sample would have. This
</p>
<p>is a typical direct problem of probability theory. However, in the natural sciences
one usually has to solve inverse problems: how to determine the nature of general
populations from the composition of random samples. Generally speaking, such
</p>
<p>inverse problems form the subject matter of Mathematical Statistics.
</p>
<p>1.3 The Bernoulli Scheme
</p>
<p>Suppose one draws a sample with replacement of size r from a general population
</p>
<p>consisting of two elements {0,1}. There are 2r such samples. Let p be a number in</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 The Bernoulli Scheme 7
</p>
<p>the interval [0,1]. Define a nonnegative function P on the set Ω of all samples in the
following way: if a sample ω contains exactly k ones, then P(ω) = pk(1 &minus; p)r&minus;k .
To verify that P is a probability, one has to prove the equality
</p>
<p>P(Ω)= 1.
</p>
<p>It is easy to see that k ones can be arranged in r places in
(
r
k
</p>
<p>)
different ways. There-
</p>
<p>fore there is the same number of samples containing exactly k ones. Now we can
</p>
<p>compute the probability of Ω :
</p>
<p>P(Ω)=
r&sum;
</p>
<p>k=0
</p>
<p>(
r
</p>
<p>k
</p>
<p>)
pk(1 &minus; p)r&minus;k =
</p>
<p>(
p+ (1 &minus; p)
</p>
<p>)r = 1.
</p>
<p>The second equality here is just the binomial formula. At the same time we have
</p>
<p>found that the probability P(k, r) that the sample contains exactly k ones is:
</p>
<p>P(k, r)=
(
r
</p>
<p>k
</p>
<p>)
pk(1 &minus; p)r&minus;k.
</p>
<p>This is the so-called binomial distribution. It can be considered as the distribution
of the number of &ldquo;successes&rdquo; in a series of r trials with two possible outcomes in
</p>
<p>each trial: 1 (&ldquo;success&rdquo;) and 0 (&ldquo;failure&rdquo;). Such a series of trials with probability
</p>
<p>P(ω) defined as pk(1 &minus; p)r&minus;k , where k is the number of successes in ω, is called
the Bernoulli scheme. It turns out that the trials in the Bernoulli scheme have the
independence property which will be discussed in the next chapter.
</p>
<p>It is not difficult to verify that the probability of having 1 at a fixed place in
</p>
<p>the sample (say, at position s) equals p. Indeed, having removed the item number s
</p>
<p>from the sample, we obtain a sample from the same population, but of size r&minus;1. We
will find the desired probability if we multiply the probabilities of these truncated
</p>
<p>samples by p and sum over all &ldquo;short&rdquo; samples. Clearly, we will get p. This is why
</p>
<p>the number p in the Bernoulli scheme is often called the success probability.
</p>
<p>Arguing in the same way, we find that the probability of having 1 at k fixed
</p>
<p>positions in the sample equals pk .
</p>
<p>Now consider how the probabilities P(k, r) of various outcomes behave as k
</p>
<p>varies. Let us look at the ratio
</p>
<p>R(k, r) := P(k, r)
P (k &minus; 1, r) =
</p>
<p>p
</p>
<p>1 &minus; p
r &minus; k + 1
</p>
<p>k
= p
</p>
<p>1 &minus; p
</p>
<p>(
r + 1
k
</p>
<p>&minus; 1
)
.
</p>
<p>It clearly monotonically decreases as k increases, the value of the ratio being less
</p>
<p>than 1 for k/(r + 1) &lt; p and greater than 1 for k/(r + 1) &gt; p. This means that
the probabilities P(k, r) first increase and then, for k &gt; p(r + 1), decrease as k
increases.
</p>
<p>The above enables one to estimate, using the quantities P(k, r), the probabilities
</p>
<p>Q(k, r)=
k&sum;
</p>
<p>j=0
P(j, r)</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Discrete Spaces of Elementary Events
</p>
<p>that the number of successes in the Bernoulli scheme does not exceed k. Namely,
</p>
<p>for k &lt; p(r + 1),
</p>
<p>Q(k, r)= P(k, r)
(
</p>
<p>1 + 1
R(k, r)
</p>
<p>+ 1
R(k, r)R(k &minus; 1, r) + &middot; &middot; &middot;
</p>
<p>)
</p>
<p>&le; P(k, r) R(k, r)
R(k, r)&minus; 1 = P(k, r)
</p>
<p>(r + 1 &minus; k)p
(r + 1)p&minus; k .
</p>
<p>It is not difficult to see that this bound will be rather sharp if the numbers k and r
</p>
<p>are large and the ratio k/(pr) is not too close to 1. In that case the sum
</p>
<p>1 + 1
R(k, r)
</p>
<p>+ 1
R(k, r)R(k &minus; 1, r) + &middot; &middot; &middot;
</p>
<p>will be close to the sum of the geometric series
</p>
<p>&infin;&sum;
</p>
<p>j=0
R&minus;j (k, r)= R(k, r)
</p>
<p>R(k, r)&minus; 1 ,
</p>
<p>and we will have the approximate equality
</p>
<p>Q(k, r)&asymp; P(k, r) (r + 1 &minus; k)p
(r + 1)p&minus; k . (1.3.1)
</p>
<p>For example, for r = 30, p = 0.7 and k = 16 one has rp = 21 and P(k, r) &asymp;
0.023. Here the ratio
</p>
<p>(r+1&minus;k)p
(r&minus;1)p&minus;1 equals 15 &times; 0.7/5.7 &asymp; 1.84. Hence the right hand
</p>
<p>side of (1.3.1) estimating Q(k, r) is approximately equal to 0.023 &times; 1.84 &asymp; 0.042.
The true value of Q(k, r) for the given values of r , p and k is 0.040 (correct to three
</p>
<p>decimals).
</p>
<p>Formula (1.3.1) will be used in the example in Sect. 5.2.
</p>
<p>Now consider a general population composed of n items, of which n1 are of
</p>
<p>the first type and n2 = n&minus; n1 of the second type. Draw from it a sample without
replacement of size r .
</p>
<p>Theorem 1.3.1 Let n and n1 tend to infinity in such a way that n1/n&rarr; p, where
p is a number from the interval [0,1]. Then the following relation holds true for the
hypergeometric distribution:
</p>
<p>Pn1,n(r1, r)&rarr; P(r1, r).
</p>
<p>Proof Divide both the numerator and denominator in the formula for Pn1,n(r1, r)
(see Sect. 1.2) by nr . Putting r2 = r &minus; r1 and n2 := n&minus; n1, we get
</p>
<p>Pn1,n(r1, r)=
r!(n&minus; r)!
</p>
<p>n!
n1!
</p>
<p>r1!(n1 &minus; r1)
n2!
</p>
<p>r2!(n2 &minus; r2)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 The Probability of the Union of Events. Examples 9
</p>
<p>= r!
r1!r2!
</p>
<p>n1
n
(
n1
n
&minus; 1
</p>
<p>n
)(
</p>
<p>n1
n
&minus; 2
</p>
<p>n
) &middot; &middot; &middot; (n1
</p>
<p>n
&minus; r1&minus;1
</p>
<p>n
)
</p>
<p>n
n
(1 &minus; 1
</p>
<p>n
) &middot; &middot; &middot; (1 &minus; r&minus;1
</p>
<p>n
)
</p>
<p>&times; n2
n
</p>
<p>(
n2
</p>
<p>n
&minus; 1
</p>
<p>n
</p>
<p>)
&middot; &middot; &middot;
</p>
<p>(
n2
</p>
<p>n
&minus; r2 &minus; 1
</p>
<p>n
</p>
<p>)
</p>
<p>&rarr;
(
r
</p>
<p>r1
</p>
<p>)
pr1(1 &minus; p)r2 = P(r1, r)
</p>
<p>as n&rarr;&infin;. The theorem is proved. �
</p>
<p>For sufficiently large n, Pn1,n(r1, r) is close to P(r1, r) by the above theorem.
</p>
<p>Therefore the Bernoulli scheme can be thought of as sampling without replacement
</p>
<p>from a very large general population consisting of items of two types, the proportion
</p>
<p>of items of the first type being p.
</p>
<p>In conclusion we will consider two problems.
</p>
<p>Imagine n bins in which we place at random r enumerated particles. Each particle
</p>
<p>can be placed in any of the n bins, so that the total number of different allocations of
</p>
<p>r particles to n bins will be nr . Allocation of particles to bins can be thought of as
</p>
<p>drawing a sample with replacement of size r from a general population of n items.
</p>
<p>We will assume that we are dealing with the classical scheme, where the probability
</p>
<p>of each outcome is 1/nr .
</p>
<p>(1) What is the probability that there are exactly r1 particles in the k-th bin?
</p>
<p>The remaining r &minus; r1 particles which did not fall into bin k are allocated to the
remaining n&minus; 1 bins. There are (n&minus; 1)r&minus;r1 different ways in which these r &minus; r1
particles can be placed into n&minus; 1 bins. Of the totality of r particles, one can choose
r &minus; r1 particles which did not fall into bin k in
</p>
<p>(
r
</p>
<p>r&minus;r1
)
</p>
<p>different ways. Therefore the
</p>
<p>desired probability is
</p>
<p>(
r
</p>
<p>r &minus; r1
</p>
<p>)
(n&minus; 1)r&minus;r1
</p>
<p>nr
=
(
</p>
<p>r
</p>
<p>r &minus; r1
</p>
<p>)
1
</p>
<p>n
</p>
<p>r1
(
</p>
<p>1 &minus; 1
n
</p>
<p>)r&minus;r1
.
</p>
<p>This probability coincides with P(r1, r) in the Bernoulli scheme with p = 1/n.
(2) Now let us compute the probability that at least one bin will be empty. Denote
</p>
<p>this event by A. Let Ak mean that the k-th bin is empty, then
</p>
<p>A=
n⋃
</p>
<p>k=1
Ak.
</p>
<p>To find the probability of the event A, we will need a formula for the probability
</p>
<p>of a sum (union) of events. We cannot make use of the additivity of probability, for
</p>
<p>the events Ak are not disjoint in our case.
</p>
<p>1.4 The Probability of the Union of Events. Examples
</p>
<p>Let us return to an arbitrary discrete probability space.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Discrete Spaces of Elementary Events
</p>
<p>Theorem 1.4.1 Let A1,A2, . . . ,An be events. Then
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>i=1
Ai
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>i=1
P(Ai)&minus;
</p>
<p>&sum;
</p>
<p>i&lt;j
</p>
<p>P(AiAj )
</p>
<p>+
&sum;
</p>
<p>i&lt;j&lt;k
</p>
<p>P(AiAjAk)&minus; &middot; &middot; &middot; + (&minus;1)n&minus;1P(A1 &middot; &middot; &middot;An).
</p>
<p>Proof One has to make use of induction and the property of probability that
</p>
<p>P(A+B)= P(A)+ P(B)&minus; P(AB)
</p>
<p>which we proved in Sect. 1.1. For n= 2 the assertion of the theorem is true. Suppose
it is true for any n&minus; 1 events A1, . . . ,An&minus;1. Then, setting B =
</p>
<p>⋃n&minus;1
i=1 Ai , we get
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>i=1
Ai
</p>
<p>)
= P(B +An)= P(B)+ P(An)&minus; P(AnB).
</p>
<p>Substituting here the known values
</p>
<p>P(B)= P
(
</p>
<p>n&minus;1⋃
</p>
<p>i=1
Ai
</p>
<p>)
and P(AnB)= P
</p>
<p>(
n&minus;1⋃
</p>
<p>i=1
(AiAn)
</p>
<p>)
,
</p>
<p>we obtain the assertion of the theorem. �
</p>
<p>Now we will turn to the second problem about bins (see the end of Sect. 1.3) and
</p>
<p>find the probability of the event A that at least one bin is empty. We represented A
</p>
<p>in the form
⋃n
</p>
<p>k=1 Ak , where Ak denotes the event that all the r particles miss the
k-th bin. One has
</p>
<p>P(Ak)=
(n&minus; 1)r
</p>
<p>nr
=
(
</p>
<p>1 &minus; 1
n
</p>
<p>)r
, k &le; n.
</p>
<p>The event AkAl means that all r particles are allocated to n &minus; 2 bins with labels
differing from k and l, and therefore
</p>
<p>P(AkAl)=
(n&minus; 2)r
</p>
<p>nr
=
(
</p>
<p>1 &minus; 2
n
</p>
<p>)r
, k, l &le; n.
</p>
<p>Similarly,
</p>
<p>P(AkAlAm)=
n&minus; 3r
nr
</p>
<p>=
(
</p>
<p>1 &minus; 3
n
</p>
<p>)r
, k, l,m&le; n,
</p>
<p>and so on. The probability of the event A is equal by Theorem 1.4.1 to
</p>
<p>P(A) = n
(
</p>
<p>1 &minus; 1
n
</p>
<p>)r
&minus;
(
n
</p>
<p>2
</p>
<p>)(
1 &minus; 2
</p>
<p>n
</p>
<p>)r
+ &middot; &middot; &middot;</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 The Probability of the Union of Events. Examples 11
</p>
<p>=
n&sum;
</p>
<p>j=1
(&minus;1)j&minus;1
</p>
<p>(
n
</p>
<p>j
</p>
<p>)(
1 &minus; j
</p>
<p>n
</p>
<p>)r
.
</p>
<p>Discussion of this problem will be continued in Example 4.1.5.
</p>
<p>As an example of the use of Theorem 1.4.1 we consider one more problem having
</p>
<p>many varied applications. This is the so-called matching problem.
Suppose n items are arranged in a certain order. They are rearranged at random
</p>
<p>(all n! permutations are equally likely). What is the probability that at least one
element retains its position?
</p>
<p>There are n! different permutations. Let Ak denote the event that the k-th item
retains its position. This event is composed of (n&minus; 1)! outcomes, so its probability
equals
</p>
<p>P(Ak)=
(n&minus; 1)!
</p>
<p>n! .
</p>
<p>The event AkAl means that the k-th and l-th items retain their positions; hence
</p>
<p>P(AkAl)=
(n&minus; 2)!
</p>
<p>n! , . . . , P(A1 &middot; &middot; &middot;Ak)=
(n&minus; (n&minus; 1))!
</p>
<p>n! =
1!
n! .
</p>
<p>Now
⋃n
</p>
<p>k=1 Ak is precisely the event that at least one item retains its position. There-
fore we can make use of Theorem 1.4.1 to obtain
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>k=1
Ak
</p>
<p>)
=
(
n
</p>
<p>1
</p>
<p>)
(n&minus; 1)!
</p>
<p>n! &minus;
(
n
</p>
<p>2
</p>
<p>)
(n&minus; 2)!
</p>
<p>n! +
(
n
</p>
<p>3
</p>
<p>)
(n&minus; 3)!
</p>
<p>n! &minus; &middot; &middot; &middot; +
(&minus;1)n&minus;1
</p>
<p>n!
</p>
<p>= 1 &minus; 1
2! +
</p>
<p>1
</p>
<p>3! &minus; &middot; &middot; &middot; +
(&minus;1)n&minus;1
</p>
<p>n!
</p>
<p>= 1 &minus;
(
</p>
<p>1 &minus; 1 + 1
2! &minus;
</p>
<p>1
</p>
<p>3! + &middot; &middot; &middot; +
(&minus;1)n
n!
</p>
<p>)
.
</p>
<p>The last expression in the parentheses is the first n+ 1 terms of the expansion of
e&minus;1 into a series. Therefore, as n&rarr;&infin;,
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>k=1
Ak
</p>
<p>)
&rarr; 1 &minus; e&minus;1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>An Arbitrary Space of Elementary Events
</p>
<p>Abstract The chapter begins with the axiomatic construction of the probability
</p>
<p>space in the general case where the number of outcomes of an experiment is not
</p>
<p>necessarily countable. The concepts of algebra and sigma-algebra of sets are intro-
</p>
<p>duced and discussed in detail. Then the axioms of probability and, more generally,
</p>
<p>measure are presented and illustrated by several fundamental examples of measure
</p>
<p>spaces. The idea of extension of a measure is discussed, basing on the Carath&eacute;odory
</p>
<p>theorem (of which the proof is given in Appendix 1). Then the general elementary
</p>
<p>properties of probability are discussed in detail in Sect. 2.2. Conditional probability
</p>
<p>given an event is introduced along with the concept of independence in Sect. 2.3.
</p>
<p>The chapter concludes with Sect. 2.4 presenting the total probability formula and
</p>
<p>the Bayes formula, the former illustrated by an example leading to the introduction
</p>
<p>of the Poisson process.
</p>
<p>2.1 The Axioms of Probability Theory. A Probability Space
</p>
<p>So far we have been considering problems in which the set of outcomes had at most
</p>
<p>countably many elements. In such a case we defined the probability P(A) using the
</p>
<p>probabilities P(ω) of elementary outcomes ω. It proved to be a function defined on
</p>
<p>all the subsets A of the space Ω of elementary events having the following proper-
</p>
<p>ties:
</p>
<p>(1) P(A)&ge; 0.
(2) P(Ω)= 1.
(3) For disjoint events A1,A2, . . .
</p>
<p>P
(⋃
</p>
<p>Aj
</p>
<p>)
=
&sum;
</p>
<p>P(Aj ).
</p>
<p>However, as we have already noted, one can easily imagine a problem in which
</p>
<p>the set of all outcomes is uncountable. For example, choosing a point at random
</p>
<p>from the segment [t1, t2] (say, in an experiment involving measurement of tempera-
ture) has a continuum of outcomes, for any point of the segment could be the result
</p>
<p>of the experiment. While in experiments with finite or countable sets of outcomes
</p>
<p>any collection of outcomes was an event, this is not the case in this example. We will
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_2, &copy; Springer-Verlag London 2013
</p>
<p>13</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_2">http://dx.doi.org/10.1007/978-1-4471-5201-9_2</a></div>
</div>
<div class="page"><p/>
<p>14 2 An Arbitrary Space of Elementary Events
</p>
<p>encounter serious difficulties if we treat any subset of the segment as an event. Here
</p>
<p>one needs to select a special class of subsets which will be treated as events.
Let the space of elementary events Ω be an arbitrary set, and A be a system of
</p>
<p>subsets of Ω .
</p>
<p>Definition 2.1.1 A is called an algebra if the following conditions are met:
</p>
<p>A1. Ω &isin;A.
A2. If A &isin;A and B &isin;A, then
</p>
<p>A&cup;B &isin;A, A&cap;B &isin;A.
</p>
<p>A3. If A &isin;A then A &isin;A.
</p>
<p>It is not hard to see that in condition A2 it suffices to require that only one of the
</p>
<p>given relations holds. The second relation will be satisfied automatically since
</p>
<p>A&cap;B =A&cup;B.
</p>
<p>An algebra A is sometimes called a ring since there are two operations defined
on A (addition and multiplication) which do not lead outside of A. An algebra A is
</p>
<p>a ring with identity, for Ω &isin;A and AΩ =ΩA=A for any A &isin;A.
</p>
<p>Definition 2.1.2 A class of sets F is called a sigma-algebra (σ -algebra, or σ -ring,
or Borel field of events) if property A2 is satisfied for any sequences of sets:
</p>
<p>A2&prime;. If {An} is a sequence of sets from F, then
</p>
<p>&infin;⋃
</p>
<p>n=1
An &isin; F,
</p>
<p>&infin;⋂
</p>
<p>n=1
An &isin; F.
</p>
<p>Here, as was the case for A2, it suffices to require that only one of the two rela-
</p>
<p>tions be satisfied. The second relation will follow from the equality
</p>
<p>⋂
</p>
<p>n
</p>
<p>An =
⋃
</p>
<p>n
</p>
<p>An.
</p>
<p>Thus an algebra is a class of sets which is closed under a finite number of opera-
tions of taking complements, unions and intersections; a σ -algebra is a class of sets
</p>
<p>which is closed under a countable number of such operations.
Given a set Ω and an algebra or σ -algebra F of its subsets, one says that we are
</p>
<p>given a measurable space 〈Ω,F〉.
For the segment [0,1], all the sets consisting of a finite number of segments or
</p>
<p>intervals form an algebra, but not a σ -algebra.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 The Axioms of Probability Theory. A Probability Space 15
</p>
<p>Consider all the σ -algebras on [0,1] containing all intervals from that segment
(there is at least one such σ -algebra, for the collection of all the subsets of a given
</p>
<p>set clearly forms a σ -algebra). It is easy to see that the intersection of all such σ -
</p>
<p>algebras (i.e. the collection of all the sets which belong simultaneously to all the σ -
</p>
<p>algebras) is again a σ -algebra. It is the smallest σ -algebra containing all intervals
and is called the Borel σ -algebra. Roughly speaking, the Borel σ -algebra could be
thought of as the collection of sets obtained from intervals by taking countably many
</p>
<p>unions, intersections and complements. This is a rather rich class of sets which is
</p>
<p>certainly sufficient for any practical purposes. The elements of the Borel σ -algebra
</p>
<p>are called Borel sets. Everything we have said in this paragraph equally applies to
systems of subsets of the whole real line.
</p>
<p>Along with the intervals (a, b), the one-point sets {a} and sets of the form (a, b],
[a, b] and [a, b) (in which a and b can take infinite values) are also Borel sets. This
assertion follows, for example, from the representations of the form
</p>
<p>{a} =
&infin;⋂
</p>
<p>n=1
(a &minus; 1/n, a + 1/n), (a, b] =
</p>
<p>&infin;⋂
</p>
<p>n=1
(a, b+ 1/n).
</p>
<p>Thus all countable sets and countable unions of intervals and segments are also
</p>
<p>Borel sets.
</p>
<p>For a given class B of subsets of Ω , one can again consider the intersection of
</p>
<p>all σ -algebras containing B and obtain in this way the smallest σ -algebra contain-
ing B.
</p>
<p>Definition 2.1.3 The smallest σ -algebra containing B is called the σ -algebra gen-
erated by B and is denoted by σ(B).
</p>
<p>In this terminology, the Borel σ -algebra in the n-dimensional Euclidean space
</p>
<p>R
n is the σ -algebra generated by rectangles or balls. If Ω is countable, then the
</p>
<p>σ -algebra generated by the elements ω &isin;Ω clearly coincides with the σ -algebra of
all subsets of Ω .
</p>
<p>As an exercise, we suggest the reader to describe the algebra and the σ -algebra
</p>
<p>of sets in Ω = [0,1] generated by: (a) the intervals (0,1/3) and (1/3,1); (b) the
semi-open intervals (a,1], 0 &lt; a &lt; 1; and (c) individual points.
</p>
<p>To formalise a probabilistic problem, one has to find an appropriate measurable
</p>
<p>space 〈Ω,F〉 for the corresponding experiment. The symbol Ω denotes the set of
elementary outcomes of the experiment, while the algebra or σ -algebra F specifies a
</p>
<p>class of events. All the remaining subsets of Ω which are not elements of F are not
events. Rather often it is convenient to define the class of events F as the σ -algebra
generated by a certain algebra A.
</p>
<p>Selecting a specific algebra or σ -algebra F depends, on the one hand, on the
</p>
<p>nature of the problem in question and, on the other hand, on that of the set Ω . As
</p>
<p>we will see, one cannot always define probability in such a way that it would make
</p>
<p>sense for any subset of Ω .</p>
<p/>
</div>
<div class="page"><p/>
<p>16 2 An Arbitrary Space of Elementary Events
</p>
<p>We have already noted in Chap. 1 that, in probability theory, one uses, along
</p>
<p>with the usual set theory terminology, a somewhat different terminology related to
</p>
<p>the fact that the subsets of Ω (belonging to F) are interpreted as events. The set Ω
</p>
<p>itself is often called the certain event. By axioms A1 and A2, the empty set &empty; also
belongs to F; it is called the impossible event. The event A is called the complement
event or simply the complement of A. If A &cap; B = &empty;, then the events A and B are
called mutually exclusive or disjoint.
</p>
<p>Now it remains to introduce the notion of probability. Consider a space Ω and a
</p>
<p>system A of its subsets which forms an algebra of events.
</p>
<p>Definition 2.1.4 A probability on 〈Ω,A〉 is a real-valued function defined on the
sets from A and having the following properties:
</p>
<p>P1. P(A)&ge; 0 for any A &isin;A.
P2. P(Ω)= 1.
P3. If a sequence of events {An} is such that AiAj =&empty; for i 
= j and
</p>
<p>⋃&infin;
1 An &isin;A,
</p>
<p>then
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>n=1
An
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=1
P(An). (2.1.1)
</p>
<p>These properties can be considered as an axiomatic definition of probability.
An equivalent to axiom P3 is the requirement of additivity (2.1.1) for finite col-
</p>
<p>lections of events Aj plus the following continuity axiom.
</p>
<p>P3&prime;. Let {Bn} be a sequence of events such that Bn+1 &sub; Bn and
⋂&infin;
</p>
<p>n=1 Bn = B &isin;A.
Then P(Bn)&rarr; P(B) as n&rarr;&infin;.
</p>
<p>Proof of the equivalence Assume P3 is satisfied and let Bn+1 &sub; Bn,
⋂
</p>
<p>nBn =
B &isin; A. Then the sequence of the events B , Ck = BkBk+1, k = 1,2, . . . , consists
of disjoint events and Bn = B +
</p>
<p>⋃&infin;
k=nCk for any n. Now making use of property
</p>
<p>P3 we see that the series P(B1)= P(B)+
&sum;&infin;
</p>
<p>k=n P(Ck) is convergent, which means
that
</p>
<p>P(Bn)= P(B)+
&infin;&sum;
</p>
<p>k=n
P(Ck)&rarr; P(B)
</p>
<p>as n&rarr;&infin;. This is just the property P3&prime;.
Conversely, if An is a sequence of disjoint events, then
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>k=1
Ak
</p>
<p>)
= P
</p>
<p>(
n⋃
</p>
<p>k=1
Ak
</p>
<p>)
+ P
</p>
<p>( &infin;⋃
</p>
<p>k=n+1
Ak
</p>
<p>)
</p>
<p>and one has
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(Ak) = lim
</p>
<p>n&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>k=1
P(Ak)= lim
</p>
<p>n&rarr;&infin;
P
</p>
<p>(
n⋃
</p>
<p>k=1
Ak
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 The Axioms of Probability Theory. A Probability Space 17
</p>
<p>= lim
n&rarr;&infin;
</p>
<p>{
P
</p>
<p>( &infin;⋃
</p>
<p>k=1
Ak
</p>
<p>)
&minus; P
</p>
<p>( &infin;⋃
</p>
<p>k=n+1
Ak
</p>
<p>)}
= P
</p>
<p>( &infin;⋃
</p>
<p>k=1
Ak
</p>
<p>)
.
</p>
<p>The last equality follows from P3&prime;. �
</p>
<p>Definition 2.1.5 A triple 〈Ω,A,P〉 is called a wide-sense probability space. If an
algebra F is a σ -algebra (F= σ(F)), then condition
</p>
<p>⋃&infin;
n=1 An &isin; F in axiom P3 (for
</p>
<p>a probability on 〈Ω,F〉) will be automatically satisfied.
</p>
<p>Definition 2.1.6 A triple 〈Ω,F,P〉, where F is a σ -algebra, is called a probability
space.
</p>
<p>A probability P on 〈Ω,F〉 is also sometimes called a probability distribution on
Ω or just a distribution on Ω (on 〈Ω,F〉).
</p>
<p>Thus defining a probability space means defining a countably additive nonneg-
</p>
<p>ative measure on a measurable space such that the measure of Ω is equal to one.
</p>
<p>In this form the axiomatics of Probability Theory was formulated by A.N. Kol-
</p>
<p>mogorov. The system of axioms we introduced is incomplete and consistent.
</p>
<p>Constructing a probability space 〈Ω,F,P〉 is the basic stage in creating a math-
ematical model (formalisation) of an experiment.
</p>
<p>Discussions on what should one understand by probability have a long history
and are related to the desire to connect the definition of probability with its &ldquo;phys-
</p>
<p>ical&rdquo; nature. However, because of the complexity of the latter, such attempts have
</p>
<p>always encountered difficulties not only of mathematical, but also of philosophical
</p>
<p>character (see the Introduction). The most important stages in this discussion are re-
</p>
<p>lated to the names of Borel, von Mises, Bernstein and Kolmogorov. The emergence
</p>
<p>of Kolmogorov&rsquo;s axiomatics separated, in a sense, the mathematical aspect of the
</p>
<p>problem from all the rest. With this approach, the &ldquo;physical interpretation&rdquo; of the
</p>
<p>notion of probability appears in the form of a theorem (the strong law of large num-
</p>
<p>bers, see Chaps. 5 and 7), by virtue of which the relative frequency of the occurrence
</p>
<p>of a certain event in an increasingly long series of independent trials approaches (in
</p>
<p>a strictly defined sense) the probability of this event.
</p>
<p>We now consider examples of the most commonly used measurable and proba-
</p>
<p>bility spaces.
</p>
<p>1. Discrete measurable spaces. These are spaces 〈Ω,F〉 where Ω is a finite or
countably infinite collection of elements, and the σ -algebra F usually consists of
</p>
<p>all the subsets of Ω . Discrete probability spaces constructed on discrete measurable
spaces were studied, with concrete examples, in Chap. 1.
</p>
<p>2. The measurable space 〈R,B〉, where R is the real line(or a part of it) and B
is the σ -algebra of Borel sets. The necessity of considering such spaces arises in
</p>
<p>situations where the results of observations of interest may assume any values in R.
</p>
<p>Example 2.1.1 Consider an experiment consisting of choosing a point &ldquo;at random&rdquo;
from the interval [0,1]. By this we will understand the following. The set of elemen-
tary outcomes Ω is the interval [0,1]. The σ -algebra F will be taken to be the class</p>
<p/>
</div>
<div class="page"><p/>
<p>18 2 An Arbitrary Space of Elementary Events
</p>
<p>of subsets B for which the notion of length (Lebesgue measure) &micro;(B) is defined&mdash;
</p>
<p>for example, the σ -algebra B of Borel measurable sets. To &ldquo;conduct a trial&rdquo; means
</p>
<p>to choose a point ω &isin;Ω = [0,1], the probability of the event ω &isin; B being &micro;(B). All
the axioms are clearly satisfied for the probability space 〈[0,1],B,&micro;〉. We obtain
the so-called uniform distribution on [0,1].
</p>
<p>Why did we take the σ -algebra of Borel sets B to be our F in this example? If we
</p>
<p>considered on Ω = [0,1] the σ -algebra generated by &ldquo;individual&rdquo; points of the in-
terval, we would get the sets of which the Lebesgue measure is either 0 or 1. In other
</p>
<p>words, the obtained sets would be either very &ldquo;dense&rdquo; or very &ldquo;thin&rdquo; (countable), so
</p>
<p>that the intervals (a, b) for 0 &lt; b&minus; a &lt; 1 do not belong to this σ -algebra.
On the other hand, if we considered on Ω = [0,1] the σ -algebra of all subsets of
</p>
<p>Ω , it would be impossible to define a probability measure on it in such a way that
</p>
<p>P([a, b])= b&minus; a (i.e. to get the uniform distribution).1
Turning back to the uniform distribution P on Ω = [0,1], it is easy to see that
</p>
<p>it is impossible to define this distribution using the same approach as we used to
</p>
<p>define a probability on a discrete space of elementary events (i.e. by defining the
</p>
<p>probabilities of elementary outcomes ω). Since in this example the ωs are individual
</p>
<p>points from [0,1], we clearly have P(ω)= 0 for any ω.
3. The measurable space 〈Rn,Bn〉 is used in the cases when observations are
</p>
<p>vectors. Here Rn is the n-dimensional Euclidean space(Rn =R1 &times; &middot; &middot; &middot; &times;Rn, where
R1, . . . ,Rn are n copies of the real line), B
</p>
<p>n is the σ -algebra of Borel sets in Rn,
</p>
<p>i.e. the σ -algebra generated by the sets B = B1 &times;&middot; &middot; &middot;&times;Bn, where Bi &sub;Ri are Borel
sets on the line. Instead of Rn we could also consider some measurable part Ω &isin;Bn
(for example a cube or ball), and instead of Bn the restriction of Bn onto Ω . Thus,
</p>
<p>similarly to the last example one can construct a probability space for choosing a
</p>
<p>point at random from the cube Ω = [0,1]n. We put here P(ω &isin; B)= &micro;(B), where
&micro;(B) is the Lebesgue measure (volume) of the set B . Instead of the cube [0,1]n we
could consider any other cube, for example [a, b]n, but in this case we would have
to put
</p>
<p>P(ω &isin; B)= &micro;(B)/&micro;(Ω)= &micro;(B)/(b&minus; a)n.
</p>
<p>This is the uniform distribution on a cube.
In Probability Theory one also needs to deal with more complex probability
</p>
<p>spaces. What to do if the result of the experiment is an infinite random sequence? In
</p>
<p>this case the space 〈R&infin;,B&infin;〉 is often the most appropriate one.
4. The measurable space 〈R&infin;,B&infin;〉, where
</p>
<p>R
&infin; =
</p>
<p>&infin;&prod;
</p>
<p>j=1
Rj
</p>
<p>1See e.g. [28], p. 80.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 The Axioms of Probability Theory. A Probability Space 19
</p>
<p>is the space of all sequences (x1, x2, . . .) (the direct product of the spaces Rj ), and
</p>
<p>B&infin; the σ -algebra generated by the sets of the form
</p>
<p>(
N&prod;
</p>
<p>k=1
Bjk
</p>
<p>)
&times;
( &prod;
</p>
<p>j 
=jk
k&le;N
</p>
<p>Rj
</p>
<p>)
; Bjk &isin;Bjk ,
</p>
<p>for any N,j1, . . . , jN , where Bj is the σ -algebra of Borel sets from Rj .
</p>
<p>5. If an experiment results, say, in a continuous function on the interval [a, b]
(a trajectory of a moving particle, a cardiogram of a patient, etc.), then the probabil-
</p>
<p>ity spaces considered above turn out to be inappropriate. In such a case one should
</p>
<p>take Ω to be the space C(a, b) of all continuous functions on [a, b] or the space
R
[a,b] of all functions on [a, b]. The problem of choosing a suitable σ -algebra here
</p>
<p>becomes somewhat more complicated and we will discuss it later in Chap. 18.
</p>
<p>Now let us return to the definition of a probability space.
</p>
<p>Let a triple 〈Ω,A,P〉 be a wide-sense probability space (A is an algebra). As
we have already seen, to each algebra A there corresponds a σ -algebra F = σ(A)
generated by A. The following question is of substantial interest: does the proba-
</p>
<p>bility measure P on A define a measure on F = σ(A)? And if so, does it define
it in a unique way? In other words, to construct a probability space 〈Ω,A,P〉, is
it sufficient to define the probability just on some algebra A generating F (i.e. to
</p>
<p>construct a wide-sense probability space 〈Ω,A,P〉, where σ(A)= F)? An answer
to this important question is given by the Carath&eacute;odory theorem.
</p>
<p>The measure extension theorem Let 〈Ω,A,P〉 be a wide-sense probability space.
Then there exists a unique probability measure Q defined on F= σ(A) such that
</p>
<p>Q(A)= P(A) for all A &isin;A.
</p>
<p>Corollary 2.1.1 Any wide-sense probability space 〈Ω,A,P〉 automatically defines
a probability space 〈Ω,F,P〉 with F= σ(A).
</p>
<p>We will make extensive use of this fact in what follows. In particular, it implies
</p>
<p>that to define a probability measure on the measurable space 〈R,B〉, it suffices to
define the probability on intervals.
</p>
<p>The proof of the Carath&eacute;odory theorem is given in Appendix 1.
</p>
<p>In conclusion of this section we will make a general comment. Mathematics dif-
</p>
<p>fers qualitatively from such sciences as physics, chemistry, etc. in that it does not
</p>
<p>always base its conclusions on empirical data with the help of which a naturalist
</p>
<p>tries to answer his questions. Mathematics develops in the framework of an initial
</p>
<p>construction or system of axioms with which one describes an object under study.
</p>
<p>Thus mathematics and, in particular, Probability Theory, studies the nature of the
</p>
<p>phenomena around us in a methodologically different way: one studies not the phe-
</p>
<p>nomena themselves, but rather the models of these phenomena that have been cre-
ated based on human experience. The value of a particular model is determined by</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 An Arbitrary Space of Elementary Events
</p>
<p>the agreement of the conclusions of the theory with our observations and therefore
</p>
<p>depends on the choice of the axioms characterising the object.
</p>
<p>In this sense axioms P1, P2, and the additivity of probability look indisputable
</p>
<p>and natural (see the remarks in the Introduction on desirable properties of probabil-
</p>
<p>ity). Countable additivity of probability and the property A2&prime; of σ -algebras are more
delicate and less easy to intuit (as incidentally are a lot of other things related to the
</p>
<p>notion of infinity). Introducing the last two properties was essentially brought about
</p>
<p>by the possibility of constructing a meaningful mathematical theory. Numerous ap-
</p>
<p>plications of Probability Theory developed from the system of axioms formulated
</p>
<p>in the present section demonstrate its high efficiency and purposefulness.
</p>
<p>2.2 Properties of Probability
</p>
<p>1. P(&empty;)= 0. This follows from the equality &empty;+Ω =Ω and properties P2 and P3
of probability.
</p>
<p>2. P(A)= 1 &minus; P(A), since A+A=Ω and A&cap;A=&empty;.
3. If A&sub; B , then P(A)&le; P(B). This follows from the relation P(A)+P(AB)=
</p>
<p>P(B).
</p>
<p>4. P(A)&le; 1 (by properties 3 and P2).
5. P(A&cup;B)= P(A)+P(B)&minus;P(AB), since A&cup;B =A+ (B&minus;AB) and P(B&minus;
</p>
<p>AB)= P(B)&minus; P(AB).
6. P(A&cup;B)&le; P(A)+ P(B) follows from the previous property.
7. The formula
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>j=1
Aj
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>k=1
P(Ak)&minus;
</p>
<p>&sum;
</p>
<p>k&lt;l
</p>
<p>P(AkAl)
</p>
<p>+
&sum;
</p>
<p>k&lt;l&lt;m
</p>
<p>P(AkAlAm)&minus; &middot; &middot; &middot; + (&minus;1)n&minus;1P(A1 . . .An)
</p>
<p>has already been proved and used for discrete spaces Ω . Here the reader can prove
</p>
<p>it in exactly the same way, using induction and property 5.
</p>
<p>Denote the sums on the right hand side of the last formula by Z1, Z2, . . . ,Zn,
</p>
<p>respectively. Then statement 7 for the event Bn =
⋃n
</p>
<p>j=1 Aj can be rewritten as
</p>
<p>P(Bn)=
&sum;n
</p>
<p>j=1(&minus;1)j&minus;1Zj .
8. An important addition to property 7 is that the sequence
</p>
<p>&sum;k
j=1(&minus;1)j&minus;1Zj
</p>
<p>approximates P(Bn) by turns from above and from below as k grows, i.e.
</p>
<p>P(Bn)&minus;
2k&minus;1&sum;
</p>
<p>j=1
(&minus;1)j&minus;1Zj &le; 0,
</p>
<p>P(Bn)&minus;
2k&sum;
</p>
<p>j=1
(&minus;1)j&minus;1Zj &ge; 0, k = 1,2, . . .
</p>
<p>(2.2.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Conditional Probability. Independence of Events and Trials 21
</p>
<p>This property can also be proved by induction on n. For n = 2 this property is
ascertained in 5. Let (2.2.1) be valid for any events A1, . . . ,An&minus;1 (i.e. for any Bn&minus;1).
Then by 5 we have
</p>
<p>P(Bn)= P(Bn&minus;1 &cup;An)= P(Bn&minus;1)+ P(An)&minus; P
(
</p>
<p>k&minus;1⋃
</p>
<p>j=1
AjAn
</p>
<p>)
,
</p>
<p>where, in view of (2.2.1) for k = 1,
n=1&sum;
</p>
<p>j=1
P(Aj )&minus;
</p>
<p>n&minus;1&sum;
</p>
<p>i&lt;j
</p>
<p>P(AiAj )&le; P(Bn&minus;1)&le;
n&minus;1&sum;
</p>
<p>j=1
P(Aj ),
</p>
<p>P
</p>
<p>(
n&minus;1⋃
</p>
<p>j=1
AjAn
</p>
<p>)
&le;
</p>
<p>n&minus;1&sum;
</p>
<p>j=1
P(AjAn).
</p>
<p>Hence, for Bn = Bn&minus;1 &cup;An, we get
</p>
<p>P(Bn) &le;
n&sum;
</p>
<p>j=1
P(Aj ),
</p>
<p>P(Bn) = P(Bn&minus;1)+ P(An)&minus; P(Bn&minus;1An)
</p>
<p>&ge;
n&sum;
</p>
<p>j=1
P(Aj )&minus;
</p>
<p>n&minus;1&sum;
</p>
<p>i&lt;j
</p>
<p>P(AiAj )&minus;
n&minus;1&sum;
</p>
<p>i=1
P(AiAn)=
</p>
<p>n&sum;
</p>
<p>j=1
P(An)&minus;
</p>
<p>n&sum;
</p>
<p>i&lt;j
</p>
<p>P(AiAj ).
</p>
<p>This proves (2.2.1) for k = 1. For k = 2,3, . . . the proof is similar.
9. If An is a monotonically increasing sequence of sets (i.e. An &sub; An+1) and
</p>
<p>A=
⋃&infin;
</p>
<p>n=1 An, then
</p>
<p>P(A)= lim
n&rarr;&infin;
</p>
<p>P(An). (2.2.2)
</p>
<p>This is a different form of the continuity axiom equivalent to P3&prime;.
Indeed, introducing the sets Bn =A&minus;An, we get Bn+1 &sub; Bn and
</p>
<p>⋂&infin;
n=1 Bn =&empty;.
</p>
<p>Therefore, by the continuity axiom,
</p>
<p>P(A&minus;An)= P(A)&minus; P(An)&rarr; 0
</p>
<p>as n&rarr;&infin;. The converse assertion that (2.2.2) implies the continuity axiom can be
obtained in a similar way. �
</p>
<p>2.3 Conditional Probability. Independence of Events and Trials
</p>
<p>We will start with examples. Let an experiment consist of three tosses of a fair
</p>
<p>coin. The probability that heads shows up only once, i.e. that one of the elementary</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 An Arbitrary Space of Elementary Events
</p>
<p>events htt , tht , or t th occurs, is equal in the classical scheme to 3/8. Denote
</p>
<p>this event by A. Now assume that we know in addition that the event B =
{the number of heads is odd} has occurred.
</p>
<p>What is the probability of the event A given this additional information? The
</p>
<p>event B consists of four elementary outcomes. The event A is constituted by three
</p>
<p>outcomes from the event B . In the framework of the classical scheme, it is natural
</p>
<p>to define the new probability of the event A to be 3/4.
</p>
<p>Consider a more general example. Let a classical scheme with n outcomes be
</p>
<p>given. An event A consists of r outcomes, an event B of m outcomes, and let the
</p>
<p>event AB have k outcomes. Similarly to the previous example, it is natural to define
</p>
<p>the probability of the event A given the event B has occurred as
</p>
<p>P(A|B)= k
m
</p>
<p>= k/n
m/n
</p>
<p>.
</p>
<p>The ratio is equal to P(AB)/P(B), for
</p>
<p>P(A|B)= k
n
, P(B)= m
</p>
<p>n
.
</p>
<p>Now we can give a general definition.
</p>
<p>Definition 2.3.1 Let 〈Ω,F,P〉 be a probability space and A and B be arbitrary
events. If P(B) &gt; 0, the conditional probability of the event A given B has occurred
is denoted by P(A|B) and is defined by
</p>
<p>P(A|B) := P(AB)
P(B)
</p>
<p>.
</p>
<p>Definition 2.3.2 Events A and B are called independent if
</p>
<p>P(AB)= P(A)P(B).
</p>
<p>Below we list several properties of independent events.
</p>
<p>1. If P(B) &gt; 0, then the independence of A and B is equivalent to the equality
</p>
<p>P(A|B)= P(A).
</p>
<p>The proof is obvious.
</p>
<p>2. If A and B are independent, then A and B are also independent.
</p>
<p>Indeed,
</p>
<p>P(AB) = P(B &minus;AB)
= P(B)&minus; P(AB)= P(B)
</p>
<p>(
1 &minus; P(A)
</p>
<p>)
= P(A)P(B).
</p>
<p>3. Let the events A and B1 and the events A and B2 each be independent, and
</p>
<p>assume B1B2 =&empty;. Then the events A and B1 +B2 are independent.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Conditional Probability. Independence of Events and Trials 23
</p>
<p>Fig. 2.1 Illustration to
</p>
<p>Example 2.3.2: the dashed
rectangles represent the
events A and B
</p>
<p>The property is proved by the following chain of equalities:
</p>
<p>P
(
A(B1 +B2)
</p>
<p>)
= P(AB1 +AB2)= P(AB1)+ P(AB2)
= P(A)
</p>
<p>(
P(B1)+ P(B2)
</p>
<p>)
= P(A)P(B1 +B2).
</p>
<p>As we will see below, the requirement B1B2 =&empty; is essential here.
</p>
<p>Example 2.3.1 Let event A mean that heads shows up in the first of two tosses of a
fair coin, and event B that tails shows up in the second toss. The probability of each
</p>
<p>of these events is 1/2. The probability of the intersection AB is
</p>
<p>P(AB)= 1
4
= 1
</p>
<p>2
&middot; 1
</p>
<p>2
= P(A)P(B).
</p>
<p>Therefore the events A and B are independent.
</p>
<p>Example 2.3.2 Consider the uniform distribution on the square [0,1]2 (see Sect. 2.1).
Let A be the event that a point chosen at random is in the region on the right of an
</p>
<p>abscissa a and B the event that the point is in the region above an ordinate b.
</p>
<p>Both regions are hatched in Fig. 2.1. The event AB is squared in the figure.
</p>
<p>Clearly, P(AB)= P(A)P(B), and hence the events A and B are independent.
It is also easy to verify that if B is the event that the chosen point is inside the
</p>
<p>triangle FCD (see Fig. 2.1), then the events A and B will already be dependent.
</p>
<p>Definition 2.3.3 Events B1,B2, . . . ,Bn are jointly independent if, for any 1 &le; i1 &lt;
i2 &lt; &middot; &middot; &middot;&lt; ir &le; n, r = 2,3, . . . , n,
</p>
<p>P
</p>
<p>(
r⋂
</p>
<p>k=1
Bjk
</p>
<p>)
=
</p>
<p>r&prod;
</p>
<p>k=1
P(Bik ).
</p>
<p>Pairwise independence is not sufficient for joint independence of n events, as one
</p>
<p>can see from the following example.</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 An Arbitrary Space of Elementary Events
</p>
<p>Example 2.3.3 (Bernstein&rsquo;s example) Consider the following experiment. We roll a
symmetric tetrahedron of which three faces are painted red, blue and green respec-
</p>
<p>tively, and the fourth is painted in all three colours. Event R means that when the
</p>
<p>tetrahedron stops, the bottom face has the red colour on it, event B that it has the
</p>
<p>blue colour, and G the green. Since each of the three colours is present on two faces,
</p>
<p>P(R)= P(B)= P(G)= 1/2. For any two of the introduced events, the probability
of the intersection is 1/4, since any two colours are present on one face only. Since
1
4
= 1
</p>
<p>2
&times; 1
</p>
<p>2
, this implies the pairwise independence of all three events. However,
</p>
<p>P(RGB)= 1
4

= P(R)P(B)P(G)= 1/8. �
</p>
<p>Now it is easy to construct an example in which property 3 of independent events
</p>
<p>does not hold when B1B2 
=&empty;.
An example of a sequence of jointly independent events is given by the series of
</p>
<p>outcomes of trials in the Bernoulli scheme.
</p>
<p>If we assume that each outcome was obtained as a result of a separate trial, then
we will find that any event related to a fixed trial will be independent of any event
</p>
<p>related to other trials. In such cases one speaks of a sequence of independent trials.
To give a general definition, consider two arbitrary experiments G1 and G2 and
</p>
<p>denote by 〈Ω1,F1,P1〉 and 〈Ω2,F2,P2〉 the respective probability spaces. Consider
also the &ldquo;compound&rdquo; experiment G with the probability space 〈Ω,F,P〉, where
Ω =Ω1 &times;Ω2 is the direct product of the spaces Ω1 and Ω2, and the σ -algebra F is
generated by the direct product F1 &times; F2 (i.e. by the events B = B1 &times; B2, B1 &isin; F1,
B2 &isin; F2).
</p>
<p>Definition 2.3.4 We will say that the trials G1 and G2 are independent if, for any
B = B1 &times;B2, B1 &isin; F1, B2 &isin; F2 one has
</p>
<p>P(B)= P1(B1)P2(B2)= P(B1 &times;Ω2)P(Ω1 &times;B2).
</p>
<p>Independence of n trialsG1, . . . ,Gn is defined in a similar way, using the equal-
ity
</p>
<p>P(B)= P1(B1) &middot; &middot; &middot;Pn(Bn),
where B = B1 &times; &middot; &middot; &middot; &times;Bn, Bk &isin; Fk , and 〈Ωk,Fk,Pk〉 is the probability space corre-
sponding to the experiment Gk , k = 1, . . . , n.
</p>
<p>In the Bernoulli scheme, the probability of any sequence of outcomes consisting
</p>
<p>of r zeros and ones and containing k ones is equal to pk(1 &minus; p)r&minus;k . Therefore the
Bernoulli scheme may be considered as a result of r independent trials in each of
</p>
<p>which one has 1 (success) with probability p and 0 (failure) with probability 1&minus;p.
Thus, the probability of k successes in r independent trials equals
</p>
<p>(
r
k
</p>
<p>)
pk(1&minus;p)r&minus;k .
</p>
<p>The following assertion, which is in a sense converse to the last one, is also
</p>
<p>true: any sequence of identical independent trials with two outcomes makes up a
</p>
<p>Bernoulli scheme.
</p>
<p>In Chap. 3 several remarks will be given on the relationship between the notions
</p>
<p>of independence we introduced here and the common notion of causality.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 The Total Probability Formula. The Bayes Formula 25
</p>
<p>2.4 The Total Probability Formula. The Bayes Formula
</p>
<p>Let A be an event and B1,B2, . . . ,Bn be mutually exclusive events having positive
</p>
<p>probabilities such that
</p>
<p>A&sub;
n⋃
</p>
<p>j=1
Bj .
</p>
<p>The sequence of events B1,B2, . . . can be infinite, in which case we put n=&infin;. The
following total probability formula holds true:
</p>
<p>P(B)=
n&sum;
</p>
<p>j=1
P(Bj )P(A|Bj ).
</p>
<p>Proof It follows from the assumptions that
</p>
<p>A=
n⋃
</p>
<p>j=1
BjA.
</p>
<p>Moreover, the events AB1, AB2, . . . ,ABn are disjoint, and hence
</p>
<p>P(A)=
n&sum;
</p>
<p>j=1
P(ABj )=
</p>
<p>n&sum;
</p>
<p>j=1
P(Bj )P(A|Bj ).
</p>
<p>�
</p>
<p>Example 2.4.1 In experiments with colliding electron-positron beams, the probabil-
ity that during a time unit there will occur j collisions leading to the birth of new
</p>
<p>elementary particles is equal to
</p>
<p>pj =
e&minus;λλj
</p>
<p>j ! , j = 0,1, . . . ,
</p>
<p>where λ is a positive parameter (this is the so-called Poisson distribution, to be con-
</p>
<p>sidered in more detail in Chaps. 3, 5 and 19). In each collision, different groups of
</p>
<p>elementary particles can appear as a result of the interaction, and the probability of
</p>
<p>each group is fixed and does not depend on the outcomes of other collisions. Con-
</p>
<p>sider one such group, consisting of two &micro;-mesons, and denote by p the probability
</p>
<p>of its appearance in a collision. What is the probability of the event Ak that, during
</p>
<p>a time unit, k pairs of &micro;-mesons will be born?
</p>
<p>Assume that the event Bj that there were j collisions during the time unit has
</p>
<p>occurred. Given this condition, we will have a sequence of j independent trials, and
</p>
<p>the probability of having k pairs of &micro;-mesons will be
(
j
k
</p>
<p>)
pk(1 &minus; p)j&minus;k . Therefore
</p>
<p>by the total probability formula,
</p>
<p>P(Ak) =
&infin;&sum;
</p>
<p>j=k
P(Bj )P(Ak|Bj )=
</p>
<p>&infin;&sum;
</p>
<p>j=k
</p>
<p>e&minus;λλj
</p>
<p>j !
j !
</p>
<p>k!(j &minus; k)!p
k(1 &minus; p)j&minus;k</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 An Arbitrary Space of Elementary Events
</p>
<p>= e
&minus;λpkλk
</p>
<p>k!
</p>
<p>&infin;&sum;
</p>
<p>j=0
</p>
<p>(λ(1 &minus; p))j
j ! =
</p>
<p>e&minus;λp(λp)k
</p>
<p>k! .
</p>
<p>Thus we again obtain a Poisson distribution, but this time with parameter λp.
</p>
<p>The solution above was not formalised. A formal solution would first of all
</p>
<p>require the construction of a probability space. The space turns out to be rather
</p>
<p>complex in this example. Denote by Ωj the space of elementary outcomes in the
</p>
<p>Bernoulli scheme corresponding to j trials, and let ωj denote an element of Ωj .
</p>
<p>Then one could take Ω to be the collection of all pairs {(j,ωj )}&infin;j=0, where the
number j indicates the number of collisions, and ωj is a sequence of &ldquo;successes&rdquo;
</p>
<p>and &ldquo;failures&rdquo; of length j (&ldquo;success&rdquo; stands for the birth of two &micro;-mesons). If ωj
contains k &ldquo;successes&rdquo;, one has to put
</p>
<p>P
(
(j,ωj )
</p>
<p>)
= pjpk(1 &minus; p)j&minus;k.
</p>
<p>To get P(Ak), it remains to sum up these probabilities over all ωj containing k
</p>
<p>successes and all j &ge; k (the idea of the total probability formula is used here tacitly
when splitting Ak into the events (j,ωj )).
</p>
<p>The fact that the number of collisions is described here by a Poisson distribution
</p>
<p>could be understood from the following circumstances related to the nature of the
</p>
<p>physical process. Let Bj (t, u) be the event that there were j collisions during the
</p>
<p>time interval [t, t + u). Then it turns out that:
(a) the pairs of events Bj (v, t) and Bk(v + t, u) related to non-overlapping time
</p>
<p>intervals are independent for all v, t, u, j , and k;
</p>
<p>(b) for small ∆ the probability of a collision during the time ∆ is proportional to ∆:
</p>
<p>P
(
B1(t,∆)
</p>
<p>)
= λ∆+ o(∆),
</p>
<p>and, moreover, P(Bk(t,∆))= o(∆) for k &ge; 2.
Again using the total probability formula with the hypotheses Bj (v, t), we obtain
</p>
<p>for the probabilities pk(t)= P(Bk(v, t)) the following relations:
</p>
<p>pk(t +∆)=
k&sum;
</p>
<p>j=0
pj (t)P
</p>
<p>(
Bk(v, t +∆)
</p>
<p>∣∣ Bj (v, t)
)
</p>
<p>=
k&sum;
</p>
<p>j=0
pj (t)P
</p>
<p>(
Bk&minus;j (v+ t,∆)
</p>
<p>)
= o(∆)+ pk&minus;1(t)
</p>
<p>(
λ∆+ o(∆)
</p>
<p>)
</p>
<p>= pk(t)
(
1 &minus; λ∆&minus; o(∆)
</p>
<p>)
, k &ge; 1;
</p>
<p>p0(t +∆)= p0(t)
(
1 &minus; λ∆&minus; o(∆)
</p>
<p>)
.
</p>
<p>Transforming the last equation, we find that
</p>
<p>p0(t +∆)&minus; p0(t)
∆
</p>
<p>=&minus;λp0(t)+ o(1).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 The Total Probability Formula. The Bayes Formula 27
</p>
<p>Therefore the derivative of p0 exists and is given by
</p>
<p>p&prime;0(t)=&minus;λp0(t).
</p>
<p>In a similar way we establish the existence of
</p>
<p>p&prime;k(t)= λpk&minus;1(t)&minus; λpk(t), k &ge; 1. (2.4.1)
</p>
<p>Now note that since the functions pk(t) are continuous, one should put p0(0)= 1,
pk(0)= 0 for k &ge; 1. Hence
</p>
<p>p0(t)= e&minus;λt .
</p>
<p>Using induction and substituting into (2.4.1) the function pk&minus;1(t)= (λt)
k&minus;1e&minus;λt
</p>
<p>(k&minus;1)! , we
</p>
<p>establish (it is convenient to make the substitution pk = e&minus;λtuk , which turns (2.4.1)
into u&prime;k =
</p>
<p>λ(λt)k&minus;1
(k&minus;1)! ) that
</p>
<p>pk(t)=
(λt)ke&minus;λt
</p>
<p>k! , k = 0,1, . . .
</p>
<p>This is the Poisson distribution with parameter λt .
</p>
<p>To understand the construction of the probability space in this problem, one
</p>
<p>should consider the set Ω of all non-decreasing step-functions x(t)&ge; 0, t &ge; 0, tak-
ing values 0,1,2, . . . . Any such function can play the role of an elementary out-
</p>
<p>come: its jump points indicate the collision times, the value x(t) itself will be the
</p>
<p>number of collisions during the time interval (0, t). To avoid a tedious argument re-
</p>
<p>lated to introducing an appropriate σ -algebra, for the purposes of our computations
</p>
<p>we could treat the probability as given on the algebra A (see Sect. 2.1) generated
by the sets {x(t)= k}, t &ge; 0; k = 0,1, . . . (note that all the events considered in this
problem are just of such form). The above argument shows that one has to put
</p>
<p>P
(
x(v+ t)&minus; x(v)= k
</p>
<p>)
= (λt)
</p>
<p>ke&minus;λt
</p>
<p>k! .
</p>
<p>(See also the treatment of Poisson processes in Chap. 19.) �
</p>
<p>By these examples we would like not only to illustrate the application of the total
</p>
<p>probability formula, but also to show that the construction of probability spaces in
</p>
<p>real problems is not always a simple task.
</p>
<p>Of course, for each particular problem, such constructions are by no means nec-
</p>
<p>essary, but we would recommend to carry them out until one acquires sufficient
</p>
<p>experience.
</p>
<p>Assume that events A and B1, . . . ,Bn satisfy the conditions stated at the begin-
</p>
<p>ning of this section. If P(A) &gt; 0, then under these conditions the following Bayes&rsquo;
formula holds true:
</p>
<p>P(Bj |A)=
P(Bj )P(A|Bj )&sum;n
k=1 P(Bk)P(A|Bk)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 An Arbitrary Space of Elementary Events
</p>
<p>This formula is simply an alternative way of writing the equality
</p>
<p>P(Bj |A)=
P(BjA)
</p>
<p>P(A)
,
</p>
<p>where in the numerator one should make use of the definition of conditional prob-
</p>
<p>ability, and in the denominator, the total probability formula. In Bayes&rsquo; formula we
</p>
<p>can take n=&infin;, just as for the total probability formula.
</p>
<p>Example 2.4.2 An item is manufactured by two factories. The production volume
of the first factory is k times the production of the second one. The proportion of
</p>
<p>defective items for the first factory is P1, and for the second one P2. Now assume
</p>
<p>that the items manufactured by the factories during a certain time interval were
</p>
<p>mixed up and then sent to retailers. What is the probability that you have purchased
</p>
<p>an item produced by the second factory given the item proved to be defective?
</p>
<p>Let B1 be the event that the item you have got came from the first factory, and
</p>
<p>B2 from the second. It easy to see that
</p>
<p>P(B1)=
1
</p>
<p>1 + k , P(B2)=
k
</p>
<p>1 + k .
</p>
<p>These are the so-called prior probabilities of the events B1 and B2. Let A be the
event that the purchased item is defective. We are given conditional probabilities
</p>
<p>P(A|B1)= P1 and P(A|B2)= P2. Now, using Bayes&rsquo; formula, we can answer the
posed question:
</p>
<p>P(B2|A)=
k
</p>
<p>1+kP2
1
</p>
<p>1+kP1 +
k
</p>
<p>1+kP2
= kP2
</p>
<p>P1 + kP2
.
</p>
<p>Similarly, P(B1|A)= P1P1+kP2 . �
</p>
<p>The probabilities P(B1|A) and P(B2|A) are sometimes called posterior proba-
bilities of the events B1 and B2 respectively, after the event A has occurred.
</p>
<p>Example 2.4.3 A student is suggested to solve a numerical problem. The answer to
the problem is known to be one of the numbers 1, . . . , k. Solving the problem, the
</p>
<p>student can either find the correct way of reasoning or err. The training of the student
</p>
<p>is such that he finds a correct way of solving the problem with probability p. In
</p>
<p>that case the answer he finds coincides with the right one. With the complementary
</p>
<p>probability 1 &minus; p the student makes an error. In that case we will assume that the
student can give as an answer any of the numbers 1, . . . , k with equal probabilities
</p>
<p>1/k.
</p>
<p>We know that the student gave a correct answer. What is the probability that his
</p>
<p>solution of the problem was correct?
</p>
<p>Let B1 (B2) be the event that the student&rsquo;s solution was correct (wrong).
</p>
<p>Then, by our assumptions, the prior probabilities of these events are P(B1) = p,</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 The Total Probability Formula. The Bayes Formula 29
</p>
<p>P(B2)= 1 &minus; p. If the event A means that the student got a correct answer, then
</p>
<p>P(A|B1)= 1, P(A|B2)= 1/k.
</p>
<p>By Bayes&rsquo; formula the desired posterior probability P(B1|A) is equal to
</p>
<p>P(B1|A)=
P(B1)P(A|B1)
</p>
<p>P(B1)P(A|B1)+ P(B2)P(A|B2)
= p
</p>
<p>p+ 1&minus;p
k
</p>
<p>= 1
1 + 1&minus;p
</p>
<p>kp
</p>
<p>.
</p>
<p>Clearly, P(B1|A) &gt; P(B1)= p and P(B1|A) is close to 1 for large k.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>Random Variables and Distribution Functions
</p>
<p>Abstract Section 3.1 introduces the formal definitions of random variable and its
</p>
<p>distribution, illustrated by several examples. The main properties of distribution
</p>
<p>functions, including a characterisation theorem for them, are presented in Sect. 3.2.
</p>
<p>This is followed by listing and briefly discussing the key univariate distributions.
</p>
<p>The second half of the section is devoted to considering the three types of distri-
</p>
<p>butions on the real line and the distributions of functions of random variables. In
</p>
<p>Sect. 3.3 multivariate random variables (random vectors) and their distributions are
</p>
<p>introduced and discussed in detail, including the two key special cases: the multi-
</p>
<p>nomial and the normal (Gaussian) distributions. After that, the concepts of indepen-
</p>
<p>dence of random variables and that of classes of events are considered in Sect. 3.4,
</p>
<p>establishing criteria for independence of random variables of different types. The
</p>
<p>theorem on independence of sigma-algebras generated by independent algebras of
</p>
<p>events is proved with the help of the probability approximation theorem. Then the
</p>
<p>relationships between the introduced notions are extensively discussed. In Sect. 3.5,
</p>
<p>the problem of existence of infinite sequences of random variables is solved with
</p>
<p>the help of Kolmogorov&rsquo;s theorem on families of consistent distributions, which is
</p>
<p>proved in Appendix 2. Section 3.6 is devoted to discussing the concept of integral in
</p>
<p>the context of Probability Theory (a formal introduction to Integration Theory is pre-
</p>
<p>sented in Appendix 3). The integrals of functions of random vectors are discussed,
</p>
<p>including the derivation of the convolution formulae for sums of independent ran-
</p>
<p>dom variables.
</p>
<p>3.1 Definitions and Examples
</p>
<p>Let 〈Ω,F,P〉 be an arbitrary probability space.
</p>
<p>Definition 3.1.1 A random variable ξ is a measurable function ξ = ξ(ω) mapping
〈Ω,F〉 into 〈R,B〉, where R is the set of real numbers and B is the σ -algebra of all
Borel sets, i.e. a function for which the inverse image ξ (&minus;1)(B)= {ω : ξ(ω) &isin; B} of
any Borel set B &isin;B is a set from the σ -algebra F.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_3, &copy; Springer-Verlag London 2013
</p>
<p>31</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_3">http://dx.doi.org/10.1007/978-1-4471-5201-9_3</a></div>
</div>
<div class="page"><p/>
<p>32 3 Random Variables and Distribution Functions
</p>
<p>For example, when tossing a coin once, Ω consists of two points: heads and tails.
</p>
<p>If we put 1 in correspondence to heads and 0 to tails, we will clearly obtain a random
</p>
<p>variable.
</p>
<p>The number of points showed up on a die will also be a random variable.
</p>
<p>The distance between the origin to a point chosen at random in the square [0 &le;
x &le; 1,0 &le; y &le; 1] will also be a random variable, since the set {(x, y) : x2 + y2 &lt; t}
is measurable. The reader might have already noticed that in these examples it is
</p>
<p>very difficult to come up with a non-measurable function of ω which would be re-
</p>
<p>lated to any real problem. This is often the case, but not always. In Chap. 18, devoted
</p>
<p>to random processes, we will be interested in sets which, generally speaking, are not
</p>
<p>events and which require special modifications to be regarded as events.
</p>
<p>As we have already mentioned above, it follows from the definition of a random
</p>
<p>variable that, for any set B from the σ -algebra B of Borel sets on the real line,
</p>
<p>ξ (&minus;1)(B)=
{
ω : ξ(ω) &isin; B
</p>
<p>}
&isin; F.
</p>
<p>Hence one can define a probability Fξ (B) = P(ξ &isin; B) on the measurable space
〈R,B〉 which generates the probability space 〈R,B,Fξ 〉.
</p>
<p>Definition 3.1.2 The probability Fξ (B) is called the distribution of the random
variable ξ .
</p>
<p>Putting B = (&minus;&infin;, x) one obtains the function
Fξ (x)= Fξ (&minus;&infin;, x)= P(ξ &lt; x)
</p>
<p>defined on the whole real line which is called the distribution function1 of the ran-
dom variable ξ .
</p>
<p>We will see below that the distribution function of a random variable completely
</p>
<p>specifies its distribution and is often used to describe the latter.
</p>
<p>Where it leads to no confusion, we will write just F, F(x) instead of Fξ , Fξ (x),
</p>
<p>respectively. More generally, in what follows, as a rule, we will be using boldface
</p>
<p>letters F, G, I, �, K, �, etc. to denote distributions, and the standard font letters F ,
</p>
<p>G, I , Φ, . . . to denote the respective distribution functions.
</p>
<p>Since a random variable ξ is a mapping of Ω into R, one has P(|ξ | &lt;&infin;) = 1.
Sometimes, it is also convenient to consider along with such random variables ran-
</p>
<p>dom variables which can assume the values &plusmn;&infin; (they will be measurable map-
pings of Ω into R&cup; {&plusmn;&infin;}). If P(|ξ | =&infin;) &gt; 0, we will call such random variables
ξ(ω) improper. Each situation where such random variables appear will be explic-
itly noted.
</p>
<p>Example 3.1.1 Consider the Bernoulli scheme with success probability p and sam-
ple size k (see Sect. 3.3). As we know, the set of elementary outcomes Ω in this case
</p>
<p>1In the English language literature, the distribution function is conventionally defined as Fξ (x)=
P(ξ &le; x). The only difference is that, with the latter definition, F will be right-continuous, cf.
property F3 below.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Properties of Distribution Functions. Examples 33
</p>
<p>is the set of all k-tuples of zeros and ones. Take the σ -algebra F to be the system of
</p>
<p>all subsets of Ω . Define a random variable on Ω as follows: to each k-tuple of zeros
</p>
<p>and ones we relate the number of ones in this tuple.
</p>
<p>The probability of r successes is, as we already know,
</p>
<p>P(r, k)=
(
k
</p>
<p>r
</p>
<p>)
pr(1 &minus; p)k&minus;r .
</p>
<p>Therefore the distribution function F(x) of our random variable will be defined
</p>
<p>as
</p>
<p>F(x)=
&sum;
</p>
<p>r&lt;x
</p>
<p>P(r, k).
</p>
<p>Here the summation is over all integers r which are less than x. If x &le; 0 then
F(x)= 0, and if x &gt; k then F(x)= 1.
</p>
<p>Example 3.1.2 Suppose we choose a point at random from the segment [a, b], i.e.
the probability that the chosen point is in a subset of [a, b] is taken to be proportional
to the Lebesgue measure of this subset. Here, Ω is the segment [a, b], the σ -algebra
F is the class of Borel subsets of [a, b]. Define a random variable ξ by
</p>
<p>ξ(ω)= ω, ω &isin; [a, b],
i.e. the value of the random variable is equal to the number from [a, b] we have cho-
sen. It is a measurable function. If x &le; a, then F(x)= P(ξ &lt; x)= 0. Let x &isin; (a, b].
Then {ξ &lt; x} means that the point is in the interval [a, x). The probability of this
event is proportional to the length of the interval, hence
</p>
<p>F(x)= P(ξ &lt; x)= x &minus; a
b&minus; a .
</p>
<p>If x &gt; b, then clearly F(x)= 1. Finally, we find that
</p>
<p>F(x)=
</p>
<p>⎧
⎨
⎩
</p>
<p>0, x &lt; a,
x&minus;a
b&minus;a , a &le; x &le; b,
1, x &gt; b.
</p>
<p>(3.1.1)
</p>
<p>This distribution function defines the so-called uniform distribution on the interval
[a, b].
</p>
<p>If &micro;(B) is the Lebesgue measure on 〈R,B〉, then, as we will see in the next
section, it is not hard to show that in this case Fξ (B)= &micro;(B &cap; [a, b])/(b&minus; a).
</p>
<p>3.2 Properties of Distribution Functions. Examples
</p>
<p>3.2.1 The Basic Properties of Distribution Functions
</p>
<p>Let F(x) be the distribution function of a random variable ξ . Then F(x) has the
</p>
<p>following properties:</p>
<p/>
</div>
<div class="page"><p/>
<p>34 3 Random Variables and Distribution Functions
</p>
<p>F1. Monotonicity: if x1 &lt; x2, then F(x1)&le; F(x2).
F2. limx&rarr;&minus;&infin; F(x)= 0 and limx&rarr;&infin; F(x)= 1.
F3. Left-continuity: limx&uarr;x0 F(x)= F(x0).
</p>
<p>Proof Since for x1 &le; x2 one has {ξ &lt; x1} &sube; {ξ &lt; x2}, F1 immediately follows from
property 3 of probability (see Sect. 3.2.2).
</p>
<p>To prove F2, consider two number sequences {xn} and {yn} such that {xn} is
decreasing and xn &rarr;&minus;&infin;, while {yn} is increasing and yn &rarr;&infin;. Put An = {ξ &lt; xn}
and Bn = {ξ &lt; yn}. Since xn tends monotonically to &minus;&infin;, the sequence of sets An
decreases monotonically to
</p>
<p>⋂
An = &empty;. By the continuity axiom (see Sect. 3.2.1),
</p>
<p>P(An) &rarr; 0 as n &rarr; &infin; or, which is the same, limn&rarr;&infin; F(xn) = 0. This and the
monotonicity of F(x) imply that
</p>
<p>lim
x&rarr;&minus;&infin;
</p>
<p>F(x)= 0.
</p>
<p>Since the sequence {yn} tends monotonically to &infin;, the sequence of sets Bn in-
creases to
</p>
<p>⋃
Bn = Ω , and hence (see property 9 in Sect. 3.2.2) P(Bn) &rarr; 1. This
</p>
<p>implies, as above, that
</p>
<p>lim
n&rarr;&infin;
</p>
<p>F(yn)= 1, lim
x&rarr;&infin;
</p>
<p>F(x)= 1.
</p>
<p>Property F3 is proved in a similar way. Let {xn} be an increasing sequence with
xn &uarr; x0,
</p>
<p>A= {ξ &lt; x0}, An = {ξ &lt; xn}.
The sequence of sets An also increases, and
</p>
<p>⋃
An =A. Therefore, P(An)&rarr; P(A).
</p>
<p>This means that
</p>
<p>lim
x&uarr;x0
</p>
<p>F(x)= F(x0). �
</p>
<p>It is not hard to see that the function F would be right-continuous if we put
F(x)= P(ξ &le; x).
</p>
<p>With our definition, the function F is generally speaking not right-continuous,
</p>
<p>since by the continuity axiom
</p>
<p>F(x + 0)&minus; F(x)= lim
n&rarr;&infin;
</p>
<p>(
F
</p>
<p>(
x + 1
</p>
<p>n
</p>
<p>)
&minus; F(x)
</p>
<p>)
</p>
<p>= lim
n&rarr;&infin;
</p>
<p>P
</p>
<p>(
x &le; ξ &lt; x + 1
</p>
<p>n
</p>
<p>)
= P
</p>
<p>( &infin;⋂
</p>
<p>n=1
</p>
<p>{
ξ &isin;
</p>
<p>[
x, x + 1
</p>
<p>n
</p>
<p>)})
</p>
<p>= P(ξ = x).
This means that F(x) is continuous if and only if P(ξ = x) = 0 for any x. Exam-
ples 3.1.1 and 3.1.2 show that both continuous and discontinuous F(x) are quite
</p>
<p>common.
</p>
<p>From the above relations it also follows that
</p>
<p>P(x &le; ξ &le; y)= Fξ
(
[x, y]
</p>
<p>)
= F(y + 0)&minus; F(x).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Properties of Distribution Functions. Examples 35
</p>
<p>Theorem 3.2.1 If a function F(x) has properties F1, F2 and F3, then there exist a
probability space 〈Ω,F,P〉 and a random variable ξ such that Fξ (x)= F(x).
</p>
<p>Proof First we construct a probability space 〈Ω,F,P〉. Take Ω to be the real line R,
F the σ -algebra B of Borel sets. As we already know (see Sect. 3.2.1), to construct
</p>
<p>a probability space 〈R,B,P〉 it suffices to define a probability on the algebra A
generated, say, by the semi-intervals of the form [&middot;,&middot;) (then σ(A)=B). An arbitrary
element of the algebra A has the form of a finite union of disjoint semi-intervals:
</p>
<p>A=
n⋃
</p>
<p>i=1
[ai, bi), ai &lt; bi
</p>
<p>(the values of ai and bi can be infinite). We define
</p>
<p>P(A)=
n&sum;
</p>
<p>i=1
</p>
<p>(
F(bi)&minus; F(ai)
</p>
<p>)
.
</p>
<p>It is absolutely clear that axioms P1 and P2 are satisfied by virtue of F1 and F2. It
</p>
<p>remains to verify the countable additivity, or continuity, of P on the algebra A. Let
</p>
<p>Bn &isin; A, Bn+1 &sub; Bn,
⋂&infin;
</p>
<p>n=1 Bn = B &isin; A. One has to show that P(Bn)&rarr; P(B) as
n&rarr;&infin; or, which is the same, that P(BnB)&rarr; 0 (BnB &isin;A). To this end, it suffices
to prove that, for any fixed N , P(BnBCN )&rarr; 0, where CN = [&minus;N,N). Indeed, for
any given ε &gt; 0, by virtue of F2 we can choose an N such that P(CN ) &lt; ε. Then
</p>
<p>P(BnB CN )&le; P(CN ) &lt; ε and
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>P(BnB)&le; lim sup
n&rarr;&infin;
</p>
<p>P(BnBCN )+ ε.
</p>
<p>Since ε is arbitrary, the convergence P(BnBCN ) &rarr; 0 as n &rarr; &infin; implies the re-
quired convergence P(BnB)&rarr; 0. It follows that we can assume that the sets Bn are
bounded (Bn &sub; [&minus;N,N) for some N &lt;&infin;). Moreover, we can assume without loss
of generality that B is the empty set.
</p>
<p>By the above remarks, Bn admits the representation
</p>
<p>Bn =
kn⋃
</p>
<p>i=1
</p>
<p>[
ani , b
</p>
<p>n
i
</p>
<p>)
, kn &lt;&infin;,
</p>
<p>where ani , b
n
i are finite. Further note that, for a given ε &gt; 0 and any semi-interval
</p>
<p>[a, b), one can always find an embedded interval [a, b &minus; δ), δ &gt; 0, such that
P([a, b&minus; δ))&ge; P([a, b))&minus; ε. This follows directly from property F3: F(b&minus; δ)&rarr;
F(b) as δ &darr; 0. Hence, for a given ε &gt; 0 and set Bn, there exist δni &gt; 0, i = 1, . . . , kn,
such that
</p>
<p>B̃n =
kn⋃
</p>
<p>i=1
</p>
<p>[
ani , b
</p>
<p>n
i &minus; δni
</p>
<p>)
&sub; Bn, P(B̃n) &gt; P(Bn)&minus; ε2&minus;n.</p>
<p/>
</div>
<div class="page"><p/>
<p>36 3 Random Variables and Distribution Functions
</p>
<p>Now add the right end points of the semi-intervals to the set B̃n and consider the
</p>
<p>closed bounded set
</p>
<p>Kn =
kn⋃
</p>
<p>i=1
</p>
<p>[
ani , b
</p>
<p>n
i &minus; δni
</p>
<p>]
.
</p>
<p>Clearly,
</p>
<p>B̃n &sub;Kn &sub; Bn, K =
&infin;⋂
</p>
<p>n=1
Kn =&empty;,
</p>
<p>P(Bn &minus;Kn)= P(BnKn)&le; ε2&minus;n.
It follows from the relation K =&empty; that Kn =&empty; for all sufficiently large n. Indeed,
all the sets Kn belong to the closure [CN ] = [N,&minus;N ] which is compact. The sets
{∆n = [CN ] &minus;Kn}&infin;n=1 form an open covering of [CN ], since
</p>
<p>⋃
</p>
<p>n
</p>
<p>∆n = [CN ]
(⋃
</p>
<p>n
</p>
<p>Kn
</p>
<p>)
= [CN ]
</p>
<p>(⋂
</p>
<p>n
</p>
<p>Kn
</p>
<p>)
= [CN ].
</p>
<p>Thus, by the Heine&ndash;Borel lemma there exists a finite subcovering {∆n}n0n=1, n0 &lt;&infin;,
such that
</p>
<p>⋃n0
n=1 ∆n = [CN ] or, which is the same,
</p>
<p>⋂n0
n=1 Kn =&empty;. Therefore
</p>
<p>P(Bn0)= P
(
Bn0
</p>
<p>(
n0⋂
</p>
<p>n=1
Kn
</p>
<p>))
= P
</p>
<p>(
Bn0
</p>
<p>(
n0⋃
</p>
<p>n=1
Kn
</p>
<p>))
</p>
<p>= P
(
</p>
<p>n0⋃
</p>
<p>n=1
Bn0Kn
</p>
<p>)
&le; P
</p>
<p>(
n0⋃
</p>
<p>n=1
BnKn
</p>
<p>)
&le;
</p>
<p>n0&sum;
</p>
<p>n=1
ε2&minus;n &lt; ε.
</p>
<p>Thus, for a given ε &gt; 0 we found an n0 (depending on ε) such that P(Bn0) &lt; ε.
</p>
<p>This means that P(Bn)&rarr; 0 as n&rarr;&infin;. We proved that axiom P3 holds.
So we have constructed a probability space. It remains to take ξ to be the identity
</p>
<p>mapping of R onto itself. Then
</p>
<p>Fξ (x)= P(ξ &lt; x)= P(&minus;&infin;, x)= F(x). �
</p>
<p>The model of the sample probability space based on the assertion just proved is
often used in studies of distribution functions.
</p>
<p>Definition 3.2.1 A probability space 〈Ω,F,F〉 is called a sample space for a ran-
dom variable ξ(ω) if Ω is a subset of the real line R and ξ(ω)&equiv; ω.
</p>
<p>The probability F = Fξ is called, in accordance with Definition 3.1.1 from
Sect. 3.1, the distribution of ξ . We will write this as
</p>
<p>ξ &sub;= F. (3.2.1)
It is obvious that constructing a sample probability space is always possible. It
</p>
<p>suffices to put Ω = R, F = B, F(B) = P(ξ &isin; B). For integer-valued variables</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Properties of Distribution Functions. Examples 37
</p>
<p>ξ the space 〈Ω,F〉 can be chosen in a more &ldquo;economical&rdquo; way by taking Ω =
{. . . ,&minus;1,0, . . .}.
</p>
<p>Since by Theorem 3.2.1 the distribution function F(x) of a random variable ξ
</p>
<p>uniquely specifies the distribution F of this random variable, along with (3.2.1) we
</p>
<p>will also write ξ &sub;= F .
Now we will give examples of some of the most common distributions.
</p>
<p>3.2.2 The Most Common Distributions
</p>
<p>1. The degenerate distribution Ia . The distribution Ia is defined by
</p>
<p>Ia(B)=
{
</p>
<p>0 if a &isin; B,
1 if a /&isin; B.
</p>
<p>This distribution is concentrated at the point a: if ξ &sub;= Ia , then P(ξ = a) = 1. The
distribution function of Ia has the form
</p>
<p>F(x)=
{
</p>
<p>0 for x &le; a,
1 for x &gt; a.
</p>
<p>The next two distributions were described in Examples 3.1.1 and 3.1.2 of
</p>
<p>Sect. 3.1.
</p>
<p>2. The binomial distribution Bnp . By the definition, ξ &sub;= Bnp (n &gt; 0 is an integer,
p &isin; (0,1)) if P(ξ = k)=
</p>
<p>(
n
k
</p>
<p>)
pk(1 &minus; p)n&minus;k , 0 &le; k &le; n. The distribution B1p will be
</p>
<p>denoted by Bp .
</p>
<p>3. The uniform distribution Ua,b . If ξ &sub;=Ua,b , then
</p>
<p>P(ξ &isin; B)= &micro;(B &cap; [a, b])
&micro;([a, b]) ,
</p>
<p>where &micro; is the Lebesgue measure. We saw that this distribution has distribution
</p>
<p>function (3.1.1).
</p>
<p>The next distribution plays a special role in probability theory, and we will en-
</p>
<p>counter it many times.
</p>
<p>4. The normal distribution �α,σ 2 (the normal or Gaussian law). We will write
ξ &sub;=�α,σ 2 if
</p>
<p>P(ξ &isin; B)=�α,σ 2(B)=
1
</p>
<p>σ
&radic;
</p>
<p>2π
</p>
<p>&int;
</p>
<p>B
</p>
<p>e&minus;(u&minus;α)
2/(2σ 2) du. (3.2.2)
</p>
<p>The distribution �α,σ 2 depends on two parameters: α and σ &gt; 0. If α = 0, σ = 1, the
normal distribution is called standard. The distribution function of �0,1 is equal to
</p>
<p>Φ(x)=�0,1
(
(&minus;&infin;, x)
</p>
<p>)
= 1&radic;
</p>
<p>2π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
e&minus;u
</p>
<p>2/2 du.
</p>
<p>The distribution function of �α,σ 2 is obviously equal to Φ((x &minus; α)/σ), so that the
parameters α and σ have the meaning of the &ldquo;location&rdquo; and &ldquo;scale&rdquo; of the distribu-
</p>
<p>tion.</p>
<p/>
</div>
<div class="page"><p/>
<p>38 3 Random Variables and Distribution Functions
</p>
<p>The fact that formula (3.2.2) defines a distribution follows from Theorem 3.2.1
</p>
<p>and the observation that the function Φ(x) (or Φ((x &minus; a)/σ )) satisfies properties
F1&ndash;F3, since Φ(&minus;&infin;)= 0, Φ(&infin;)= 1, and Φ(x) is continuous and monotone. One
could also directly use the fact that the integral in (3.2.2) is a countably additive set
</p>
<p>function (see Sect. 3.6 and Appendix 3).
</p>
<p>5. The exponential distribution Ŵα . The relation ξ &sub;=Ŵα means that ξ is nonneg-
ative and
</p>
<p>P(ξ &isin; B)= Ŵα(B)= α
&int;
</p>
<p>B&cap;(0,&infin;)
e&minus;αu du.
</p>
<p>The distribution function of ξ &sub;= Ŵα clearly has the form
</p>
<p>P(ξ &lt; x)=
{
</p>
<p>1 &minus; e&minus;αx for x &ge; 0,
0 for x &lt; 0.
</p>
<p>The exponential distribution is a special case of the gamma distribution Ŵα,λ, to be
</p>
<p>considered in more detail in Sect. 7.7.
</p>
<p>6. A discrete analogue of the exponential distribution is called the geometric
distribution. It has the form
</p>
<p>P(ξ = k)= (1 &minus; p)pk, p &isin; (0,1), k = 0,1, . . .
</p>
<p>7. The Cauchy distribution Kα,σ . As was the case with the normal distribution,
this distribution depends on two parameters α and σ which are also location and
</p>
<p>scale parameters. If ξ &sub;=Kα,σ then
</p>
<p>P(ξ &isin; B)= 1
πσ
</p>
<p>&int;
</p>
<p>B
</p>
<p>du
</p>
<p>1 + ((u&minus; a)/σ )2 .
</p>
<p>The distribution function K(x) of K0,1 is
</p>
<p>K(x)= 1
π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
</p>
<p>du
</p>
<p>1 + u2 .
</p>
<p>The distribution function of Kα,σ is equal to K((x &minus; α)σ). All the remarks made
for the normal distribution continue to hold here.
</p>
<p>Example 3.2.1 Suppose that there is a source of radiation at a point (α,σ ), σ &gt; 0,
on the plane. The radiation is registered by a detector whose position coincides with
</p>
<p>the x-axis. An emitted particle moves in a random direction distributed uniformly
</p>
<p>over the circle. In other words, the angle η between this direction and the vector
</p>
<p>(0,&minus;1) has the uniform distribution U&minus;π,π on the interval [&minus;π,π]. Observation
results are the coordinates ξ1, ξ2, . . . of the points on the x-axis where the particles
</p>
<p>interacted with the detector. What is the distribution of the random variable ξ = ξ1?
To find this distribution, consider a particle emitted at the point (α,σ ) given
</p>
<p>that the particle hit the detector (i.e. given that η &isin; [&minus;π/2,π/2]). It is clear that
the conditional distribution of η given the last event (of which the probability is
</p>
<p>P(η &isin; [&minus;π/2,π/2]) = 1/2) coincides with U&minus;π/2,π/2. Since (ξ &minus; α)/σ = tanη,
one obtains that</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Properties of Distribution Functions. Examples 39
</p>
<p>P(ξ &lt; x)= P(α + σ tanη &lt; x)
= P
</p>
<p>(
η
</p>
<p>π
&lt;
</p>
<p>1
</p>
<p>π
arctan
</p>
<p>x &minus; α
σ
</p>
<p>)
= 1
</p>
<p>2
+ 1
</p>
<p>π
arctan
</p>
<p>x &minus; α
σ
</p>
<p>.
</p>
<p>Recalling that (arctanu)&prime; = 1/(1 + u2), we have
</p>
<p>arctanx =
&int; x
</p>
<p>0
</p>
<p>du
</p>
<p>1 + u2 =
&int; x
</p>
<p>&minus;&infin;
</p>
<p>du
</p>
<p>1 + u2 &minus;
π
</p>
<p>2
,
</p>
<p>P(ξ &lt; x)= 1
π
</p>
<p>&int; (x&minus;α)/σ
</p>
<p>&minus;&infin;
</p>
<p>du
</p>
<p>1 + u2 =K
(
x &minus; α
σ
</p>
<p>)
.
</p>
<p>Thus the coordinates of the traces on the x-axis of the particles emitted from the
</p>
<p>point (α,σ ) have the Cauchy distribution Kα,σ .
</p>
<p>8. The Poisson distribution �λ. We will write ξ &sub;=�λ if ξ assumes nonnegative
integer values with probabilities
</p>
<p>P(ξ =m)= λ
m
</p>
<p>m! e
&minus;λ, λ &gt; 0, m= 0,1,2, . . .
</p>
<p>The distribution function, as in Example 3.1.1, has the form of a sum:
</p>
<p>F(x)=
{&sum;
</p>
<p>m&lt;x
λm
</p>
<p>m! e
&minus;λ for x &gt; 0,
</p>
<p>0 for x &le; 0.
</p>
<p>3.2.3 The Three Distribution Types
</p>
<p>All the distributions considered in the above examples can be divided into two types.
</p>
<p>I. Discrete Distributions
</p>
<p>Definition 3.2.2 The distribution of a random variable ξ is called discrete if ξ can
assume only finitely or countably many values x1, x2, . . . so that
</p>
<p>pk = P(ξ = xk) &gt; 0,
&sum;
</p>
<p>pk = 1.
</p>
<p>A discrete distribution {pk} can obviously always be defined on a discrete prob-
ability space. It is often convenient to characterise such a distribution by a table:
</p>
<p>Values x1 x2 x3 . . .
</p>
<p>Probabilities p1 p2 p3 . . .
</p>
<p>The distributions Ia , B
n
p , �λ, and the geometric distribution are discrete. The
</p>
<p>derivative of the distribution function of such a distribution is equal to zero every-
</p>
<p>where except at the points x1, x2, . . . where F(x) is discontinuous, the jumps being
</p>
<p>F(xk + 0)&minus; F(xk)= pk.
</p>
<p>An important class of discrete distributions is formed by lattice distributions.</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 Random Variables and Distribution Functions
</p>
<p>Definition 3.2.3 We say that random variable ξ has a lattice distribution with span
h if there exist a and h such that
</p>
<p>&infin;&sum;
</p>
<p>k=&minus;&infin;
P(ξ = a + kh)= 1. (3.2.3)
</p>
<p>If h is the greatest number satisfying (3.2.3) and the number a lies in the interval
</p>
<p>[0, h) then these numbers are called the span and the shift, respectively, of the lattice.
If a = 0 and h= 1 then the distribution is called arithmetic. The same terms will
</p>
<p>be used for random variables.
</p>
<p>Obviously the greatest common divisor (g.c.d.) of all possible values of an arith-
</p>
<p>metic random variable equals 1.
</p>
<p>II. Absolutely Continuous Distributions
</p>
<p>Definition 3.2.4 The distribution F of a random variable ξ is said to be absolutely
continuous2 if, for any Borel set B ,
</p>
<p>F(B)= P(ξ &isin; B)=
&int;
</p>
<p>B
</p>
<p>f (x)dx, (3.2.4)
</p>
<p>where f (x)&ge; 0,
&int;&infin;
&minus;&infin; f (x)dx = 1.
</p>
<p>The function f (x) in (3.2.4) is called the density of the distribution.
It is not hard to derive from the proof of Theorem 3.2.1 (to be more precise, from
</p>
<p>the theorem on uniqueness of the extension of a measure) that the above definition
</p>
<p>of absolute continuity is equivalent to the representation
</p>
<p>Fξ (x)=
&int; x
</p>
<p>&minus;&infin;
f (u)du
</p>
<p>for all x &isin; R. Distribution functions with this property are also called absolutely
continuous.
</p>
<p>2The definition refers to absolute continuity with respect to the Lebesgue measure. Given a measure
&micro; on 〈R,B〉 (see Appendix 3), a distribution F is called absolutely continuous with respect to &micro;
if, for any B &isin;B, one has
</p>
<p>F(B)=
&int;
</p>
<p>B
</p>
<p>f (x)&micro;(dx).
</p>
<p>In this sense discrete distributions are also absolutely continuous, but with respect to the count-
</p>
<p>ing measure m. Indeed, if one puts f (xk) = pk , m(B) = {the number of points from the set
(x1, x2, . . .) which are in B}, then
</p>
<p>F(B)=
&sum;
</p>
<p>xk&isin;B
pk =
</p>
<p>&sum;
</p>
<p>xk&isin;B
f (xk)=
</p>
<p>&int;
</p>
<p>B
</p>
<p>f (x)m(dx)
</p>
<p>(see Appendix 3).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Properties of Distribution Functions. Examples 41
</p>
<p>Fig. 3.1 The plot shows the
</p>
<p>result of the first three steps in
</p>
<p>the construction of the Cantor
</p>
<p>function
</p>
<p>The function f (x) is determined by the above equalities up to its values on a set
</p>
<p>of Lebesgue measure 0. For this function, the relation f (x)= dF(x)
dx
</p>
<p>holds3 almost
</p>
<p>everywhere (with respect to the Lebesgue measure).
</p>
<p>The distributions Ua,b , �α,σ 2 , Kα,σ and Ŵα are absolutely continuous. The den-
</p>
<p>sity of the normal distribution with parameters αand σ is equal to
</p>
<p>φα,σ 2(x)=
1&radic;
</p>
<p>2πσ
e&minus;(x&minus;α)
</p>
<p>2/(2σ 2).
</p>
<p>From their definitions, one could easily derive the densities of the distributions Ua,b ,
</p>
<p>Kα,σ and Ŵα as well. The density of Kα,σ has a shape resembling that of the normal
</p>
<p>density, but with &ldquo;thicker tails&rdquo; (it vanishes more slowly as |x| &rarr;&infin;).
We will say that a distribution F has an atom at point x1 if F({x1}) &gt; 0. We saw
</p>
<p>that any discrete distribution consists of atoms but, for an absolutely continuous
</p>
<p>distribution, the probability of hitting a set of zero Lebesgue measure is zero. It
</p>
<p>turns out that there exists yet a third class of distributions which is characterised
by the negation of both mentioned properties of discrete and absolutely continuous
</p>
<p>distributions.
</p>
<p>III. Singular Distributions
</p>
<p>Definition 3.2.5 A distribution F is said to be singular (with respect to Lebesgue
measure) if it has no atoms and is concentrated on a set of zero Lebesgue measure.
</p>
<p>Because a singular distribution has no atoms, its distribution function is continu-
</p>
<p>ous. An example of such a distribution function is given by the famous Cantor func-
</p>
<p>tion of which the whole variation is concentrated on the interval [0,1]: F(x) = 0
for x &le; 0, F(x) = 1 for x &ge; 1. It can be constructed as follows (the construction
process is shown in Fig. 3.1).
</p>
<p>3The assertion about the &ldquo;almost everywhere&rdquo; uniqueness of the function f follows from the
</p>
<p>Radon&ndash;Nikodym theorem (see Appendix 3).</p>
<p/>
</div>
<div class="page"><p/>
<p>42 3 Random Variables and Distribution Functions
</p>
<p>Divide the segment [0,1] into three equal parts [0,1/3], [1/3,2/3], and [2/3,1].
On the inner segment put F(x) = 1/2. The remaining two segments are again di-
vided into three equal parts each, and on the inner parts one sets F(x) to be 1/4 and
</p>
<p>3/4, respectively. Each of the remaining segments is divided in turn into three parts,
</p>
<p>and F(x) is defined on the inner parts as the arithmetic mean of the two already
</p>
<p>defined neighbouring values of F(x), and so on. At the points which do not belong
</p>
<p>to such inner segments F(x) is defined by continuity. It is not hard to see that the
</p>
<p>total length of such &ldquo;inner&rdquo; segments on which F(x) is constant is equal to
</p>
<p>1
</p>
<p>3
+ 2
</p>
<p>9
+ 4
</p>
<p>27
+ &middot; &middot; &middot; = 1
</p>
<p>3
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>(
2
</p>
<p>3
</p>
<p>)k
= 1
</p>
<p>3
</p>
<p>1
</p>
<p>1 &minus; 2/3 = 1,
</p>
<p>so that the function F(x) grows on a set of measure zero but has no jumps.
</p>
<p>From the construction of the Cantor distribution we see that dF(x)/dx = 0 al-
most everywhere.
</p>
<p>It turns out that these three types of distribution exhaust all possibilities.
</p>
<p>More precisely, there is a theorem belonging to Lebesgue4 stating that any distri-
</p>
<p>bution function F(x) can be represented in a unique way as a sum of three compo-
</p>
<p>nents: discrete, absolutely continuous, and singular. Hence an arbitrary distribution
</p>
<p>function cannot have more than a countable number of jumps (which can also be
</p>
<p>observed directly: we will count all the jumps if we first enumerate all the jumps
</p>
<p>which are greater than 1/2, then the jumps greater than 1/3, then greater than 1/4,
</p>
<p>etc.). This means, in particular, that F(x) is everywhere continuous except perhaps
</p>
<p>at a countable or finite set of points.
</p>
<p>In conclusion of this section we will list several properties of distribution func-
</p>
<p>tions and densities that arise when forming new random variables.
</p>
<p>3.2.4 Distributions of Functions of Random Variables
</p>
<p>For a given function g(x), to find the distribution of g(ξ) we have to impose some
</p>
<p>measurability requirements on the function. The function g(x) is called Borel if the
inverse image
</p>
<p>g&minus;1(B)=
{
x : g(x) &isin; B
</p>
<p>}
</p>
<p>of any Borel set B is again a Borel set. For such a function g the distribution function
</p>
<p>of the random variable η= g(ξ) equals
</p>
<p>Fg(ξ)(x)= P
(
g(ξ) &lt; x
</p>
<p>)
= P
</p>
<p>(
ξ &isin; g&minus;1(&minus;&infin;, x)
</p>
<p>)
.
</p>
<p>If g(x) is continuous and strictly increasing on an interval (a, b) then, on the
</p>
<p>interval (g(a), g(b)), the inverse function y = g(&minus;1)(x) is defined as the solution to
</p>
<p>4See Sect. 3.5 in Appendix 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Properties of Distribution Functions. Examples 43
</p>
<p>the equation g(y)= x.5 Since g is a monotone mapping we have
{
g(ξ) &lt; x
</p>
<p>}
=
{
ξ &lt; g(&minus;1)(x)
</p>
<p>}
for x &isin;
</p>
<p>(
g(a), g(b)
</p>
<p>)
.
</p>
<p>Thus we get the following representation for Fg(ξ) in terms of Fξ : for x &isin;
(g(a), g(b)),
</p>
<p>Fg(ξ)(x)= P
(
ξ &lt; g&minus;1(x)
</p>
<p>)
= Fξ
</p>
<p>(
g&minus;1(x)
</p>
<p>)
. (3.2.5)
</p>
<p>Putting g = Fξ we obtain, in particular, that if Fξ is continuous and strictly increas-
ing on (a, b) and F(a)= 0, F(b)= 1 (&minus;a and b may be &infin;) then
</p>
<p>Fξ
(
g(&minus;1)(x)
</p>
<p>)
&equiv; x
</p>
<p>for x &isin; [0,1] and therefore the random variable η= Fξ (ξ) is uniformly distributed
over [0,1].
</p>
<p>Definition 3.2.6 The quantile transform F (&minus;1)(f ) of an arbitrary distribution F
with the distribution function F(x) is the &ldquo;generalised&rdquo; inverse of the function F
</p>
<p>F (&minus;1)(y) := sup
{
x : F(x) &lt; y
</p>
<p>}
for y &isin; (0,1];
</p>
<p>F (&minus;1)(0) := inf
{
x : F(x) &gt; 0
</p>
<p>}
.
</p>
<p>In mathematical statistics, the number F (&minus;1)(y) is called the quantile of order y
of the distribution F. The function F (&minus;1) has a discontinuity of size b&minus; a at a point
y if (a, b) is the interval on which F is constant and such that F(x)= y &isin; [0,1).
</p>
<p>Roughly speaking, the plot of the function F (&minus;1) can be obtained from that of the
function F(x) on the (x, y) plane in the following way: rotate the (x, y) plane in
</p>
<p>the counter clockwise direction by 90&deg;, so that the x-axis becomes the ordinate axis,
</p>
<p>but the y-axis becomes the abscissa axis directed to the left. To switch to normal
</p>
<p>coordinates, we have to reverse the direction of the new x-axis.
</p>
<p>Further, if x is a point of continuity and a point of growth of the function F (i.e.,
</p>
<p>F(x) is a point of continuity of F (&minus;1)) then F (&minus;1)(y) is the unique solution of the
equation F(x)= y and the equality F(F (&minus;1)(y))= y holds.
</p>
<p>In some cases the following statement proves to be useful.
</p>
<p>Theorem 3.2.2 Let η&sub;=U0,1. Then, for any distribution F,
f (&minus;1)(η)&sub;= F.
</p>
<p>Proof If F(x) &gt; y then F (&minus;1)(y) = sup{v : F(v) &lt; y} &lt; x, and vice versa: if
F(x) &lt; y then F (&minus;1)(y) &ge; x (recall that F(x) is left-continuous). Therefore the
following inclusions are valid for the sets in the (x, y) plane:
</p>
<p>{
y &lt; F(x)
</p>
<p>}
&sub;
{
F (&minus;1)(y) &lt; x
</p>
<p>}
&sub;
{
y &le; F(x)
</p>
<p>}
.
</p>
<p>5For an arbitrary non-decreasing function g, the inverse function g(&minus;1)(x) is defined by the equa-
tion
</p>
<p>g(&minus;1)(y) := inf
{
x : g(x)&ge; y
</p>
<p>}
= sup
</p>
<p>{
x : g(x) &lt; y
</p>
<p>}
.</p>
<p/>
</div>
<div class="page"><p/>
<p>44 3 Random Variables and Distribution Functions
</p>
<p>Substituting η &sub;= U0,1 in place of y in these relations yields that, for any x, such
inclusions hold for the respective events, and hence
</p>
<p>P
(
F (&minus;1)(η) &lt; x
</p>
<p>)
= P
</p>
<p>(
η &lt; F(x)
</p>
<p>)
= F(x).
</p>
<p>The theorem is proved. �
</p>
<p>Thus we have obtained an important method for constructing random variables
</p>
<p>with prescribed distributions from uniformly distributed random variables. For in-
</p>
<p>stance, if η&sub;=U0,1 then ξ =&minus;(1/α) lnη&sub;=Ŵα .
In another special case, when g(x)= a+ bx, b &gt; 0, from (3.2.5) we get Fg(ξ) =
</p>
<p>Fξ ((x &minus; a)/b). We have already used this relation to some extent when considering
the distributions �α,σ 2 and Kα,σ .
</p>
<p>If a function g is strictly increasing and differentiable (the inverse function g(&minus;1)
</p>
<p>is defined in this case), and ξ has a density f (x), then there exists a density for g(ξ)
</p>
<p>which is equal to
</p>
<p>fg(ξ)(y)= f
(
g(&minus;1)(y)
</p>
<p>)(
g(&minus;1)(y)
</p>
<p>)&prime; = f (x) dx
dy
</p>
<p>,
</p>
<p>where x = g(&minus;1)(y), y = g(x). A similar argument for decreasing g leads to the
general formula
</p>
<p>fg(ξ)(y)= f (x)
∣∣∣∣
dx
</p>
<p>dy
</p>
<p>∣∣∣∣.
</p>
<p>For g(x)= a + bx, b 
= 0, one obtains
</p>
<p>fa+bξ (y)=
1
</p>
<p>|b|f
(
y &minus; a
b
</p>
<p>)
.
</p>
<p>3.3 Multivariate Random Variables
</p>
<p>Let ξ1, ξ2, . . . , ξn be random variables given on a common probability space
</p>
<p>〈Ω,F,P〉. To each ω, these random variables put into correspondence an n-
dimensional vector ξ(ω)= (ξ1(ω), ξ2(ω), . . . , ξn(ω)).
</p>
<p>Definition 3.3.1 A mapping Ω &rarr; Rn given by random variables ξ1, ξ2, . . . , ξn is
called a random vector or multivariate random variable.
</p>
<p>Such a mapping Ω &rarr; Rn is a measurable mapping of the space 〈Ω,F〉 into the
space 〈Rn,Bn〉, where Bn is the σ -algebra of Borel sets in Rn. Therefore, for Borel
sets B , the function Pξ (B)= P(ξ &isin; B) is defined.
</p>
<p>Definition 3.3.2 The function Fξ (B) is called the distribution of the vector ξ .
The function
</p>
<p>Fξ1...ξn(x1, . . . , xn)= P(ξ1 &lt; x1, . . . , ξn &lt; xn)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Multivariate Random Variables 45
</p>
<p>is called the distribution function of the random vector (ξ1, . . . , ξn) or joint distri-
bution function of the random variables ξ1, . . . , ξn.
</p>
<p>The following properties of the distribution functions of random vectors, analo-
</p>
<p>gous to properties F1&ndash;F3 in Sect. 3.2, hold true.
</p>
<p>FF1. Monotonicity: &ldquo;Multiple&rdquo; differences of the values of the function Fξ1...ξn ,
which correspond to probabilities of hitting arbitrary &ldquo;open at the right&rdquo; paral-
</p>
<p>lelepipeds, are nonnegative. For instance, in the two-dimensional case this means
</p>
<p>that, for any x1 &lt; x2, y1 &lt; y2 (the points (x1, y1) and (x2, y2) being the &ldquo;extreme&rdquo;
</p>
<p>vertices of the parallelepiped),
</p>
<p>Fξ1,ξ2(x2, y2)&minus; Fξ1,ξ2(x2, y1)&minus;
(
Fξ1,ξ2(x1, y2)&minus; Fξ1,ξ2(x1, y1)
</p>
<p>)
&ge; 0.
</p>
<p>This double difference is nothing else but the probability of hitting the &ldquo;semi-open&rdquo;
</p>
<p>parallelepiped [x1, x2)&times; [y1, y2) by ξ .
In other words, the differences
</p>
<p>Fξ1,ξ2(t, y2)&minus; Fξ1,ξ2(t, y1) for y1 &lt; y2
must be monotone in t . (For this to hold, the monotonicity of the function
</p>
<p>Fξ1,ξ2(t, y1) is not sufficient.)
</p>
<p>FF2. The second property can be called consistency.
</p>
<p>lim
xn&rarr;&infin;
</p>
<p>Fξ1...ξn(x1, . . . , xn)= Fξ1...ξn&minus;1(x1, . . . , xn&minus;1),
</p>
<p>lim
xn&rarr;&minus;&infin;
</p>
<p>Fξ1...ξn(x1, . . . , xn)= 0.
</p>
<p>FF3. Left-continuity:
</p>
<p>lim
x&prime;n&uarr;&infin;
</p>
<p>Fξ1...ξn
(
x1, . . . , x
</p>
<p>&prime;
n
</p>
<p>)
= Fξ1...ξn(x1, . . . , xn).
</p>
<p>That the limits in properties FF2 and FF3 are taken in the last variable is inessential,
</p>
<p>for one can always renumber the components of the vectors.
</p>
<p>One can prove these properties in the same way as in the one-dimensional case.
</p>
<p>As above, any function F(x1, . . . , xn) possessing this collection of properties will
be the distribution function of a (multivariate) random variable.
</p>
<p>As in the one-dimensional case, when considering random vectors ξ =
(ξ1, . . . , ξn), we can make use of the simplest sample model of the probability space
</p>
<p>〈Ω,F,P〉. Namely, let Ω coincide with Rn and F=Bn be the σ -algebra of Borel
sets. We will complete the construction of the required probability space if we put
</p>
<p>F(B) = Fξ (B) = P(ξ &isin; B) for any B &isin;Bn. It remains to define the random vari-
able as the value of the elementary event itself, i.e. to put ξ(ω) = ω, where ω is a
point in Rn.
</p>
<p>It is not hard to see that the distribution function Fξ1...ξn uniquely determines the
</p>
<p>distribution Fξ (B). Indeed, Fξ1...ξn defines a probability on the σ -algebra A gener-
</p>
<p>ated by rectangles {ai &le; xi &lt; bi; i = 1, . . . , n}. For example, in the two-dimensional
case</p>
<p/>
</div>
<div class="page"><p/>
<p>46 3 Random Variables and Distribution Functions
</p>
<p>P(a1 &le; ξ1 &lt; b1, a2 &le; ξ2 &lt; b2)
= P(ξ1 &lt; b1, a2 &le; ξ2 &lt; b2)&minus; P(ξ1 &lt; a1, a2 &le; ξ2 &lt; b2)
=
[
Fξ1,ξ2(b1, b2)&minus; Fξ1,ξ2(b1, a2)
</p>
<p>]
&minus;
[
Fξ1,ξ2(a1, b2)&minus; Fξ1,ξ2(a1, a2)
</p>
<p>]
.
</p>
<p>But Bn = σ(A), and it remains to make use of the measure extension theorem (see
Sect. 3.2.1).
</p>
<p>Thus from a distribution function Fξ1...ξn = F one can always construct a sample
probability space 〈Rn,Bn,Fξ 〉 and a random variable ξ(ω) &equiv; ω on it so that the
latter will have the prescribed distribution Fξ .
</p>
<p>As in the one-dimensional case, we say that the distribution of a random vector
</p>
<p>is discrete if the random vector assumes at most a countable set of values.
</p>
<p>The distribution of a random vector will be absolutely continuous if, for any
Borel set B &sub;Rn,
</p>
<p>Fξ (B)= P(ξ &isin; B)=
&int;
</p>
<p>B
</p>
<p>f (x)dx,
</p>
<p>where clearly f (x)&ge; 0 and
&int;
Ω
f (x)dx = 1.
</p>
<p>This definition can be replaced with an equivalent one requiring that
</p>
<p>Fξ1...ξn(x1, . . . , xn)=
&int; x1
&minus;&infin;
</p>
<p>&middot; &middot; &middot;
&int; xn
&minus;&infin;
</p>
<p>f (t1, . . . , tn) dt1 &middot; &middot; &middot;dtn. (3.3.1)
</p>
<p>Indeed, if (3.3.1) holds, we define a countably additive set function
</p>
<p>Q(B)=
&int;
</p>
<p>B
</p>
<p>f (x)dx
</p>
<p>(see properties of integrals in Appendix 3), which will coincide on rectangles
</p>
<p>with Fξ . Consequently, Fξ (B)=Q(B).
The function f (x) is called the density of the distribution of ξ or density of the
</p>
<p>joint distribution of ξ1, . . . , ξn. The equality
</p>
<p>&part;n
</p>
<p>&part;x1 &middot; &middot; &middot; &part;xn
Fξ1...ξn(x1, . . . , xn)= f (x1, . . . , xn)
</p>
<p>holds for this function almost everywhere.
</p>
<p>If a random vector ξ has density f (x1, . . . , xn), then clearly any &ldquo;subvector&rdquo;
</p>
<p>(ξk1 . . . ξkn), ki &le; n, also has a density equal (let for the sake of simplicity ki = i,
i = 1, . . . , s) to
</p>
<p>f (x1, . . . , xs)=
&int;
</p>
<p>f (x1, . . . , xn) dxs+1 &middot; &middot; &middot;dxn.
</p>
<p>Let continuously differentiable functions yi = gi(x1, . . . , xn) be given in a region
A&sub;Rn. Suppose they are univalently resolvable for x1, . . . , xn: there exist functions
xi = g(&minus;1)i (y1, . . . , yn), and the Jacobian J = |&part;xi/&part;yi | 
= 0 in A. Denote by B the
image of A in the range of (y1, . . . , yn). Suppose further that a random vector ξ =
(ξ1, . . . , ξn) has a density fξ (x). Then ηi = gi(ξ1, . . . , ξn) will be random variables
with a joint density which, at a point (y1, . . . , yn) &isin; B , is equal to
</p>
<p>fn(y1, . . . , yn)= fξ (x1, . . . , xn)|J |; (3.3.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Multivariate Random Variables 47
</p>
<p>moreover
</p>
<p>P(ξ &isin;A)=
&int;
</p>
<p>A
</p>
<p>fξ (x1, . . . , xn) dx1 &middot; &middot; &middot;dxn =
&int;
</p>
<p>B
</p>
<p>fξ (x1, . . . , xn)|J |dy1 &middot; &middot; &middot;dyn
</p>
<p>=
&int;
</p>
<p>B
</p>
<p>fη(y1, . . . , yn) dy1 &middot; &middot; &middot;dyn = P(η &isin; B). (3.3.3)
</p>
<p>This is clearly an extension to the multi-dimensional case of the property of densities
</p>
<p>discussed at the end of Sect. 3.2. Formula (3.3.3) for integrals is well-known in
</p>
<p>calculus as the change of variables formula and could serve as a proof of (3.3.2).
</p>
<p>The distribution Fξ of a random vector ξ is called singular if the distribution has
no atoms (Fξ ({x})= 0 for any x &isin;Rn) and is concentrated on a set of zero Lebesgue
measure.
</p>
<p>Consider the following two important examples of multivariate distributions (we
</p>
<p>continue the list of the most common distribution from Sect. 3.2).
</p>
<p>9. The multinomial distribution Bnp . We use here the same symbol B
n
p as we used
</p>
<p>for the binomial distribution. The only difference is that now by p we understand a
</p>
<p>vector p = (p1, . . . , pr), pj &ge; 0,
&sum;r
</p>
<p>j=1 pj = 1, which could be interpreted as the
collection of probabilities of disjoint events Aj ,
</p>
<p>⋃
Aj =Ω . For an integer-valued
</p>
<p>random vector ν = (ν1, . . . , νr ), we will write ν &sub;=B if for k = (k1, . . . , kr), kj &ge; 0,&sum;r
j=1 kj = n one has
</p>
<p>P(ν = k)= n!
k1! &middot; &middot; &middot;kr !
</p>
<p>p
k1
1 &middot; &middot; &middot;p
</p>
<p>kr
r . (3.3.4)
</p>
<p>On the right-hand side we have a term from the expansion of the polynomial (p1 +
&middot; &middot; &middot; + pr)n into powers of p1, . . . , pr . This explains the name of the distribution. If
p is a number, then evidently Bnp = Bn(p,1&minus;p), so that the binomial distribution is a
multinomial distribution with r = 2.
</p>
<p>The numbers νj could be interpreted as the frequencies of the occurrence of
</p>
<p>events Aj in n independent trials, the probability of occurrence of Aj in a trial
</p>
<p>being pj . Indeed, the probability of any fixed sequence of outcomes containing
</p>
<p>k1, . . . , kr outcomes A1, . . . ,Ar , respectively, is equal to p
k1
1 &middot; &middot; &middot;p
</p>
<p>kr
r , and the number
</p>
<p>of different sequences of this kind is equal to n!/k1! &middot; &middot; &middot;kr ! (of n! permutations we
leave only those which differ by more than merely permutations of elements inside
</p>
<p>the groups of k1, . . . , kr elements). The result will be the probability (3.3.4).
</p>
<p>Example 3.3.1 The simplest model of a chess tournament with two players could
be as follows. In each game, independently of the outcomes of the past games, the
</p>
<p>1st player wins with probability p, loses with probability q , and makes a draw with
</p>
<p>probability 1 &minus; p &minus; q . In that case the probability that, in n games, the 1st player
wins i and loses j games (i + j &le; n), is
</p>
<p>p(n; i, j)= n!
i!j !(n&minus; i &minus; j)! p
</p>
<p>iqj (1 &minus; p&minus; q)n&minus;i&minus;j .
</p>
<p>Suppose that the tournament goes on until one of the players wins N games (and
</p>
<p>thereby wins the tournament). If we denote by η the duration of the tournament (the</p>
<p/>
</div>
<div class="page"><p/>
<p>48 3 Random Variables and Distribution Functions
</p>
<p>number of games played before its end) then
</p>
<p>P(η= n)=
N&minus;1&sum;
</p>
<p>i=0
p(n&minus; 1;N &minus; 1, i)p+
</p>
<p>N&minus;1&sum;
</p>
<p>i=0
p(n&minus; 1; i,N &minus; 1)q.
</p>
<p>10. The multivariate normal (or Gaussian) distribution �α,σ 2 . Let α = (α1,
. . . , αr) be a vector and σ
</p>
<p>2 = ‖σij‖, i, j = 1, . . . , r , a symmetric positive definite
matrix, and A = ‖aij‖ the matrix inverse to σ 2 = A&minus;1. We will say that a vector
ξ = (ξ1, . . . , ξr) has the normal distribution: ξ &sub;=�α,σ 2 , if it has the density
</p>
<p>ϕα,σ 2(x)=
&radic;
|A|
</p>
<p>(2π)r/2
exp
</p>
<p>{
&minus;1
</p>
<p>2
(x &minus; α)A(x &minus; α)T
</p>
<p>}
.
</p>
<p>Here T denotes transposition:
</p>
<p>xAxT =
&sum;
</p>
<p>aijxixj .
</p>
<p>It is not hard to verify that
&int;
</p>
<p>ϕα,σ 2(x) dx1 &middot; &middot; &middot;dxr = 1
</p>
<p>(see also Sect. 7.6).
</p>
<p>3.4 Independence of Random Variables and Classes of Events
</p>
<p>3.4.1 Independence of Random Vectors
</p>
<p>Definition 3.4.1 Random variables ξ1, . . . , ξn are said to be independent if
</p>
<p>P(ξ1 &isin; B1, . . . , ξn &isin; Bn)= P(ξ1 &isin; B1) &middot; &middot; &middot;P(ξn &isin; Bn) (3.4.1)
for any Borel sets B1, . . . ,Bn on the real line.
</p>
<p>One can introduce the notion of a sequence of independent random variables. The
random variables from the sequence {ξn}&infin;n=1 given on a probability space 〈Ω,F,P〉,
are independent if (3.4.1) holds for any integer n so that the independence of a
</p>
<p>sequence of random variables reduces to that of any finite collection of random
</p>
<p>variable from this sequence. As we will see below, for a sequence of independent
</p>
<p>random variables, any two events related to disjoint groups of random variables
</p>
<p>from the sequence are independent.
</p>
<p>Another possible definition of independence of random variables follows from
</p>
<p>the assertion below.
</p>
<p>Theorem 3.4.1 Random variables ξ1, . . . , ξn are independent if and only if
</p>
<p>Fξ1...ξn(x1, . . . , xn)= Fξ1(x1) &middot; &middot; &middot;Fξn(xn).
</p>
<p>The proof of the theorem is given in the third part of the present section.
</p>
<p>An important criterion of independence in the case when the distribution of ξ =
(ξ1, . . . , ξn) is absolutely continuous is given in the following theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Independence of Random Variables and Classes of Events 49
</p>
<p>Theorem 3.4.2 Let random variables ξ1, . . . , ξn have densities f1(x), . . . , fn(x),
respectively. Then for the independence of ξ1, . . . , ξn it is necessary and sufficient
that the vector ξ = (ξ1, . . . , ξn) has a density f (x1, . . . , xn) which is equal to
</p>
<p>f (x1, . . . , xn)= f1(x1) &middot; &middot; &middot;fn(xn).
</p>
<p>Thus, if it turns out that the density of ξ equals the product of densities of ξj , that
</p>
<p>will mean that the random variables ξj are independent.
</p>
<p>We leave it to the reader to verify, using this theorem, that the components of a
</p>
<p>normal vector (ξ1, . . . , ξn) are independent if and only if aij = 0, σij = 0 for i 
= j .
</p>
<p>Proof of Theorem 3.4.2 If the distribution function of the random variable ξi is given
by
</p>
<p>Fξi (xi)=
&int; xi
&minus;&infin;
</p>
<p>fi(ti) dti
</p>
<p>and ξi are independent, then the joint distribution function will be defined by the
</p>
<p>formula
</p>
<p>Fξ1...ξn(x1, . . . , xn)= Fξ1(x1) &middot; &middot; &middot;Fξn(xn)
</p>
<p>=
&int; x1
&minus;&infin;
</p>
<p>f1(t1) dt1 &middot; &middot; &middot;
&int; xn
&minus;&infin;
</p>
<p>fn(tn) dtn
</p>
<p>=
&int; x1
&minus;&infin;
</p>
<p>&middot; &middot; &middot;
&int; xn
&minus;&infin;
</p>
<p>f1(t1) &middot; &middot; &middot;fn(tn) dt1 &middot; &middot; &middot;dtn.
</p>
<p>Conversely, assuming that
</p>
<p>Fξ1...ξn(x1, . . . , xn)=
&int; x1
&minus;&infin;
</p>
<p>&middot; &middot; &middot;
&int; xn
&minus;&infin;
</p>
<p>f1(t1) &middot; &middot; &middot;fn(tn) dt1 &middot; &middot; &middot;dtn,
</p>
<p>we come to the equality
</p>
<p>Fξ1...ξn(x1, . . . , xn)= Fξ1(x1) &middot; &middot; &middot;Fξn(xn).
The theorem is proved. �
</p>
<p>Now consider the discrete case. Assume for the sake of simplicity that the com-
</p>
<p>ponents of ξ may assume only integral values. Then for the independence of ξj it is
</p>
<p>necessary and sufficient that, for all k1, . . . , kn,
</p>
<p>P(ξ1 = k1, . . . , ξn = kn)= P(ξ1 = k1) &middot; &middot; &middot;P(ξn = kn).
Verifying this assertion causes no difficulties, and we leave it to the reader.
</p>
<p>The notion of independence is very important for Probability Theory and will be
</p>
<p>used throughout the entire book. Assume that we are formalising a practical problem
</p>
<p>(constructing an appropriate probability model in which various random variables
</p>
<p>are to be present). How can one find out whether the random variables (or events)
</p>
<p>to appear in the model are independent? In such situations it is a justified rule to
consider events and random variables with no causal connection as independent.</p>
<p/>
</div>
<div class="page"><p/>
<p>50 3 Random Variables and Distribution Functions
</p>
<p>The detection of &ldquo;probabilistic&rdquo; independence in a mathematical model of a
</p>
<p>random phenomenon is often connected with a deep understanding of its physical
</p>
<p>essence.
</p>
<p>Consider some simple examples. For instance, it is known that the probability
</p>
<p>of a new-born child to be a boy (event A) has a rather stable value P(A)= 22/43.
If B denotes the condition that the child is born on the day of the conjunction of
</p>
<p>Jupiter and Mars, then, under the assumption that the position of the planets does not
</p>
<p>determine individual fates of humans, the conditional probability P(A|B) will have
the same value: P(A|B) = 22/43. That is, the actual counting of the frequency of
births of boys under these specific astrological conditions would give just the value
</p>
<p>22/43. Although such a counting might never have been carried out at a sufficiently
</p>
<p>large scale, we have no grounds to doubt its results.
</p>
<p>Nevertheless, one should not treat the connection between &ldquo;mathematical&rdquo; and
</p>
<p>causal independence as an absolute one. For instance, by Newton&rsquo;s law of gravita-
</p>
<p>tion the flight of a missile undoubtedly influences the simultaneous flight of another
</p>
<p>missile. But it is evident that in practice one can ignore this influence. This example
</p>
<p>also shows that independence of events and variables in the concrete and relative
</p>
<p>meaning of this term does not contradict the principle of the universal interdepen-
</p>
<p>dence of all events.
</p>
<p>It is also interesting to note that the formal definition of independence of events or
</p>
<p>random variables is much wider than the notion of real independence in the sense of
</p>
<p>affiliation to causally unrelated phenomena. This follows from the fact that &ldquo;math-
</p>
<p>ematical&rdquo; independence can take place in such cases when one has no reason for
</p>
<p>assuming no causal relation. We illustrate this statement by the following example.
</p>
<p>Let η be a random variable uniformly distributed over [0,1]. Then in the expansion
of η into a binary fraction
</p>
<p>η= ξ1
2
+ ξ2
</p>
<p>4
+ ξ3
</p>
<p>8
+ &middot; &middot; &middot;
</p>
<p>the random variables ξk will be independent (see Example 11.3.1), although they all
</p>
<p>have a related origin.
</p>
<p>One can see that this circumstance only enlarges the area of applicability of all
</p>
<p>the assertions we obtain below under the formal condition of independence.6
</p>
<p>The notion of independence of random variables is closely connected with that
</p>
<p>of independence of σ -algebras.
</p>
<p>3.4.2 Independence of Classes of Events
</p>
<p>Let 〈Ω,F,P〉 be a probability space and A1 and A2 classes of events from the σ -
algebra F.
</p>
<p>6For a more detailed discussion of connections between causal and probabilistic independence, see
</p>
<p>[24], from where we borrowed the above examples.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Independence of Random Variables and Classes of Events 51
</p>
<p>Definition 3.4.2 The classes of events A1 and A2 are said to be independent if, for
any events A1 and A2 such that A1 &isin;A1 and A2 &isin;A2, one has
</p>
<p>P(A1A2)= P(A1)P(A2).
</p>
<p>The following definition introduces the notion of independence of a sequence of
</p>
<p>classes of events.
</p>
<p>Definition 3.4.3 Classes of events {An}&infin;n=1 are independent if, for any collection of
integers n1, . . . , nk ,
</p>
<p>P
</p>
<p>(
k⋂
</p>
<p>j=1
Anj
</p>
<p>)
=
</p>
<p>k&prod;
</p>
<p>j=1
P(Anj )
</p>
<p>for any Anj &isin;Anj .
</p>
<p>For instance, in a sequence of independent trials, the sub-σ -algebras of events
</p>
<p>related to different trials will be independent. The independence of a sequence of
</p>
<p>algebras of events also reduces to the independence of any finite collection of alge-
</p>
<p>bras from the sequence. It is clear that subalgebras of events of independent algebras
</p>
<p>are also independent.
</p>
<p>Theorem 3.4.3 σ -algebras A1 and A2 generated, respectively, by independent al-
gebras of events A1 and A2 are independent.
</p>
<p>Before proving this assertion we will obtain an approximation theorem which
</p>
<p>will be useful for the sequel. By virtue of the theorem, any event A from the σ -
</p>
<p>algebra A generated by an algebra A can, in a sense, be approximated by events
</p>
<p>from A. To be more precise, we introduce the &ldquo;distance&rdquo; between events defined by
</p>
<p>d(A,B)= P(AB &cup;AB)= P(AB)+ P(AB)= P(A&minus;B)+ P(B &minus;A).
This distance possesses the following properties:
</p>
<p>d(A,B)= d(A,B),
d(A,C)&le; d(A,B)+ d(B,C),
</p>
<p>d(AB,CD)&le; d(A,C)+ d(B,D),∣∣P(A)&minus; P(B)
∣∣&le; d(A,B).
</p>
<p>(3.4.2)
</p>
<p>The first relation is obvious. The triangle inequality follows from the fact that
</p>
<p>d(A,C)= P(AC)+ P(AC)= P(ACB)+ P(ACB)+ P(ACB)+ P(ACB)
&le; P(CB)+ P(AB)+ P(AB)+ P(CB)= d(A,B)+ d(B,C).
</p>
<p>The third relation in (3.4.2) can be obtained in a similar way by enlarging events
</p>
<p>under the probability sign. Finally, the last inequality in (3.4.2) is a consequence of
</p>
<p>the relations
</p>
<p>P(A)= P(AB)+ P(AB)= P(B)&minus; P(BA)+ P(AB).</p>
<p/>
</div>
<div class="page"><p/>
<p>52 3 Random Variables and Distribution Functions
</p>
<p>Theorem 3.4.4 (The approximation theorem) Let 〈Ω,F,P〉 be a probability space
and A the σ -algebra generated by an algebra A of events from F. Then, for any
A &isin;A, there exists a sequence An &isin;A such that
</p>
<p>lim
n&rarr;&infin;
</p>
<p>d(A,An)= 0. (3.4.3)
</p>
<p>By the last inequality from (3.4.2), the assertion of the theorem means that
</p>
<p>P(A)= limn&rarr;&infin; P(An) and that each event A &isin;A can be represented, up to a set of
zero probability, as a limit of a sequence of events from the generating algebra A
</p>
<p>(see also Appendix 1).
</p>
<p>Proof 7 We will call an event A &isin; F approximable if there exists a sequence An &isin;A
possessing property (3.4.3), i.e. d(An,A)&rarr; 0.
</p>
<p>Since d(A,A) = 0, the class of approximable events A&lowast; contains A. Therefore
to prove the theorem it suffices to verify that A&lowast; is a σ -algebra.
</p>
<p>The fact that A&lowast; is an algebra is obvious, for the relations A &isin; A&lowast; and
B &isin; A&lowast; imply that A, A &cup; B , A &cap; B &isin; A. (For instance, if d(A,An) &rarr; 0 and
d(B,Bn) &rarr; 0, then by the third inequality in (3.4.2) one has d(AB,AnBn) &le;
d(A,An)+ d(B,Bn)&rarr; 0, so that AB &isin;A&lowast;.)
</p>
<p>Now let C =
⋂&infin;
</p>
<p>k=1 Ck where Ck &isin; A&lowast;. Since A&lowast; is an algebra, we have Dn =⋃n
k=1 Ck &isin;A&lowast;; moreover,
</p>
<p>d(Dn,C)= P(C &minus;Dn)= P(C)&minus; P(Dn)&rarr; 0.
Therefore one can choose An &isin; A so that d(Dn,An) &lt; 1/n, and consequently by
virtue of (3.4.2) we have
</p>
<p>d(C,An)&le; d(C,Dn)+ d(Dn,An)&rarr; 0.
Thus C &isin;A&lowast; and hence A&lowast; forms a σ -algebra. The theorem is proved. �
</p>
<p>Proof of Theorem 3.4.3 is now easy. If A1 &isin;A1 and A2 &isin;A2, then by Theorem 3.4.4
there exist sequences A1n &isin;A1 and A2n &isin;A2 such that d(Ai,Ain)&rarr; 0 as n&rarr;&infin;,
i = 1,2. Putting B =A1A2 and Bn =A1nA2n, we obtain that
</p>
<p>d(B,Bn)&le; d(A1,A1n)+ d(A2,A2n)&rarr; 0
as n&rarr;&infin; and
</p>
<p>P(A1A2)= lim
n&rarr;&infin;
</p>
<p>P(Bn)= lim
n&rarr;&infin;
</p>
<p>P(A1n)P(A2n)= P(A1)P(A2). �
</p>
<p>3.4.3 Relations Between the Introduced Notions
</p>
<p>We will need one more definition. Let ξ be a random variable (or vector) given on a
</p>
<p>probability space 〈Ω,F,P〉.
</p>
<p>7The theorem is also a direct consequence of the lemma from Appendix 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Independence of Random Variables and Classes of Events 53
</p>
<p>Definition 3.4.4 The class Fξ of events from F of the form A = ξ&minus;1(B) =
{ω : ξ(ω) &isin; B}, where B are Borel sets, is called the σ -algebra generated by the
random variable ξ .
</p>
<p>It is evident that Fξ is a σ -algebra since to each operation on sets A there corre-
</p>
<p>sponds the same operation on the sets B = ξ(A) forming a σ -algebra.
The σ -algebra Fξ generated by the random variable ξ will also be denoted by
</p>
<p>σ(ξ).
</p>
<p>Consider, for instance, a probability space 〈Ω,B,P〉, where Ω = R is the real
line and B is the σ -algebra of Borel sets. If
</p>
<p>ξ = ξ(ω)=
{
</p>
<p>0, ω &lt; 0,
</p>
<p>1, ω &ge; 0,
then Fξ clearly consists of four sets: R, &empty;, {ω &lt; 0} and {ω &ge; 0}. Such a random
variable ξ cannot distinguish &ldquo;finer&rdquo; sets from B. On the other hand, it is obvious
</p>
<p>that ξ will be measurable ({ξ &isin; B} &isin;B1) with respect to any other &ldquo;richer&rdquo; sub-σ -
algebra B1, such that σ(ξ)&sub;B1 &sub;B.
</p>
<p>If ξ = ξ(ω)= &lfloor;ω&rfloor; is the integral part of ω, then Fξ will be the σ -algebra of sets
composed of the events {k &le; ω &lt; k + 1}, k = . . . ,&minus;1,0,1, . . .
</p>
<p>Finally, if ξ(ω) = ϕ(ω) where ϕ is continuous and monotone, ϕ(&infin;) =&infin; and
ϕ(&minus;&infin;)=&minus;&infin;, then Fξ coincides with the σ -algebra of Borel sets B.
</p>
<p>Lemma 3.4.1 Let ξ and η be two random variables given on 〈Ω,F,P〉, the variable
ξ being measurable with respect to σ(η). Then ξ and η are functionally related, i.e.
there exists a Borel function g such that ξ = g(η).
</p>
<p>Proof By assumption,
</p>
<p>Ak,n =
{
ξ &isin;
</p>
<p>[
k
</p>
<p>2n
,
k + 1
</p>
<p>2n
</p>
<p>)}
&isin; σ(η).
</p>
<p>Denote by Bk,n = {η(ω) : ω &isin;Ak,n} the images of the sets Ak,n on the line R under
the mapping η(ω) and put gn(x)= k/2n for x &isin; Bk,n. Then gn(η)= [2nε]/2n and
because Ak,n &isin; σ(η), Bk,n &isin;B and gn is a Borel function. Since gn(x) &uarr; for any x,
the limit limn&rarr;&infin; gn(x) = g(x) exists and is also a Borel function. It remains to
observe that ε = limn&rarr;&infin; gn(η)= g(η) by the very construction. �
</p>
<p>Now we formulate an evident proposition relating independence of random vari-
</p>
<p>ables and σ -algebras.
</p>
<p>Random variables ξ1, . . . , ξn are independent if and only if the σ -algebras
σ(ξ1), . . . , σ (ξn) are independent.
</p>
<p>This is a direct consequence of the definitions of independence of random vari-
</p>
<p>ables and σ -algebras.
</p>
<p>Now we can prove Theorem 3.4.1. First note that finite unions of semi-intervals
</p>
<p>[&middot;,&middot;) (perhaps with infinite end points) form a σ -algebra generating the Borel σ -alge-
bra on the line: B= σ(A).</p>
<p/>
</div>
<div class="page"><p/>
<p>54 3 Random Variables and Distribution Functions
</p>
<p>Proof of Theorem 3.4.1 Since in one direction the assertion of the theorem is ob-
vious, it suffices to verify that the equality F(x1, . . . , xn)= Fξ1(x1) &middot; &middot; &middot;Fξn(xn) for
the joint distribution function implies the independence of σ(ξ1), . . . , σ (ξn). Put for
</p>
<p>simplicity n = 2 and denote by ∆ and Λ the semi-intervals [x1, x2) and [y1, y2),
respectively. The following equalities hold:
</p>
<p>P(ξ1 &isin;∆,ξ2 &isin;Λ)= P
(
ξ1 &isin; [x1, x2), ξ2 &isin; [y1, y2)
</p>
<p>)
</p>
<p>= F(x2, y2)F (x1, y2)&minus; F(x2, y1)+ F(x1, y1)
=
(
Fξ1(x2)&minus; Fξ1(x1)
</p>
<p>)(
Fξ2(y2)&minus; Fξ2(y1)
</p>
<p>)
</p>
<p>= P{ξ1 &isin;∆}P{ξ2 &isin;Λ}.
</p>
<p>Consequently, if ∆i , i = 1, . . . , n, and Λj , j = 1, . . . ,m, are two systems of
disjoint semi-intervals, then
</p>
<p>P
</p>
<p>(
ξ1 &isin;
</p>
<p>n⋃
</p>
<p>i=1
∆i, ξ2 &isin;
</p>
<p>m⋃
</p>
<p>j=1
Λj
</p>
<p>)
=
&sum;
</p>
<p>i,j
</p>
<p>P(ξ1 &isin;∆i, ξ2 &isin;Λj )
</p>
<p>=
&sum;
</p>
<p>i,j
</p>
<p>P(ξ1 &isin;∆i)P(ξ2 &isin;Λj )
</p>
<p>= P
(
ξ1 &isin;
</p>
<p>n⋃
</p>
<p>i=1
∆i
</p>
<p>)
P
</p>
<p>(
ξ2 &isin;
</p>
<p>m⋃
</p>
<p>j=1
λj
</p>
<p>)
. (3.4.4)
</p>
<p>But the class of events {ω : ξ(ω) &isin;A} = ξ&minus;1(A), where A &isin;A, forms, along with A,
an algebra (we will denote it by α(ξ)), and one has σ(α(ξ)) = σ(ξ). In (3.4.4)
we proved that α(ξ1) and α(ξ2) are independent. Therefore by Theorem 3.4.3 the
</p>
<p>σ -algebras σ(ξ1)= σ(α(ξ1)) and σ(ξ2)= σ(α(ξ1)) are also independent. The the-
orem is proved. �
</p>
<p>It is convenient to state the following fact as a theorem.
</p>
<p>Theorem 3.4.5 Let ϕ1 and ϕ2 be Borel functions and ξ1 and ξ2 be independent
random variables. Then η1 = ϕ1(ξ1) and η2 = ϕ2(ξ2) are also independent random
variables.
</p>
<p>Proof We have to verify that, for any Borel sets B1 and B2,
</p>
<p>P
(
ϕ1(ξ1) &isin; B1, ϕ2(ξ2) &isin; B2
</p>
<p>)
= P
</p>
<p>(
ϕ1(ξ1) &isin; B1
</p>
<p>)
P
(
ϕ2(ξ2) &isin; B2
</p>
<p>)
. (3.4.5)
</p>
<p>But the sets {x : ϕi(x) &isin; Bi} = ϕ&minus;1(Bi)= B&lowast;i , i = 1,2, are again Borel sets. There-
fore
</p>
<p>{
ω : ϕi(ξi) &isin; Bi
</p>
<p>}
=
{
ω : ξi &isin; B&lowast;i
</p>
<p>}
,
</p>
<p>and the required multiplicativity of probability (3.4.5) follows from the indepen-
</p>
<p>dence of ξi . The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Independence of Random Variables and Classes of Events 55
</p>
<p>Let {ξn}&infin;n=1 be a sequence of independent random variables. Consider the random
variables ξk, ξk+1, . . . , ξm where k &lt;m&le;&infin;. Denote by σ(ξk, . . . , ξm) (for m=&infin;
we will write σ(ξk, ξk+1, . . .)) the σ -algebra generated by the events
</p>
<p>⋂m
i=k Ai ,
</p>
<p>where Ai &isin; σ(ξi).
</p>
<p>Definition 3.4.5 The σ -algebra σ(ξk, . . . , ξm) is said to be generated by the random
variables ξk, . . . , ξm.
</p>
<p>In the sequel we will need the following proposition.
</p>
<p>Theorem 3.4.6 For any k &ge; 1, the σ -algebra σ(ξn+k) is independent of
σ(ξ1, . . . , ξn).
</p>
<p>Proof To prove the assertion, we make use of Theorem 3.4.3. To this end we have
to verify that the algebra A generated by sets of the form B =
</p>
<p>⋂n
i=1 Ai , where
</p>
<p>Ai &isin; σ(ξi), is independent of σ(ξn+k). Let A &isin; σ(ξn+k), then it follows from the
independence of the σ -algebras σ(ξ1), σ (ξ2), . . . , σ (ξn), σ (ξn+k) that
</p>
<p>P(AB)= P(A)P(A1) &middot; &middot; &middot;P(An)= P(A) &middot; P(B).
</p>
<p>In a similar way we verify that
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>i=1
AiA
</p>
<p>)
= P
</p>
<p>(
n⋃
</p>
<p>i=1
Ai
</p>
<p>)
P(A)
</p>
<p>(one just has to represent
⋃n
</p>
<p>i=1 Ai as a union of disjoint events from A). Thus the
algebra A is independent of σ(ξn+k). Hence σ(ξ1, . . . , ξn) and σ(ξn+k) are inde-
pendent. The theorem is proved. �
</p>
<p>It is not hard to see that similar conclusions can be made about vector-valued
</p>
<p>random variables ξ1, ξ2, . . . defining their independence using the relation
</p>
<p>P(ξ1 &isin; B1, . . . , ξn &isin; Bn)=
&prod;
</p>
<p>P(ξj &isin; Bj ),
</p>
<p>where Bj are Borel sets in spaces of respective dimensions.
</p>
<p>In conclusion of this section note that one can always construct a probability
</p>
<p>space 〈Ω,F,P〉 (〈Rn,Bn,Pξ 〉) on which independent random variables ξ1, . . . , ξn
with prescribed distribution functions Fξj are given whenever these distributions
</p>
<p>Fξj are known. This follows immediately from Sect. 3.3, since in our case the joint
</p>
<p>distribution function Fξ (x1, . . . , xn) of the vector ξ = (ξ1, . . . , ξn) is uniquely deter-
mined by the distribution functions Fξj (x) of the variables ξj :
</p>
<p>Fξ (x1, . . . , xn)=
n&prod;
</p>
<p>1
</p>
<p>Fξj (xj ).</p>
<p/>
</div>
<div class="page"><p/>
<p>56 3 Random Variables and Distribution Functions
</p>
<p>3.5 &lowast; On Infinite Sequences of Random Variables
</p>
<p>We have already mentioned infinite sequences of random variables. Such sequences
</p>
<p>will repeatedly be objects of our studies below. However, there arises the question
</p>
<p>of whether one can define an infinite sequence on a probability space in such a way
</p>
<p>that its components possess certain prescribed properties (for instance, that they will
</p>
<p>be independent and identically distributed).
</p>
<p>As we saw, one can always define a finite sequence of independent random vari-
ables by choosing for the &ldquo;compound&rdquo; random variable (ξ1, . . . , ξn) the sample
</p>
<p>space R1 &times;R2 &times; &middot; &middot; &middot; &times;Rn =Rn and σ -algebra B1 &times;B1 &times; &middot; &middot; &middot; &times;Bn =Bn gener-
ated by sets of the form B1 &times; B2 &times; &middot; &middot; &middot; &times; Bn &sub; Rn, Bi being Borel sets. It suffices
to define probability on the algebra of these sets. In the infinite-dimensional case,
</p>
<p>however, the situation is more complicated. Theorem 3.2.1 and its extensions to the
</p>
<p>multivariate case are insufficient here. One should define probability on an algebra
</p>
<p>of events from R&infin; =
&prod;&infin;
</p>
<p>k=1 Rk so that its closure under countably many operations
&cup; and &cap; form the σ -algebra B&infin; generated by the products
</p>
<p>⋂
Bjk , Bjk &isin;Bjk .
</p>
<p>Let N be a subset of integers. Denote by RN =
&prod;
</p>
<p>k&isin;N Rk the direct product of
the spaces Rk over k &isin;N , BN =
</p>
<p>&prod;
k&isin;N Bk . We say that distributions PN &prime; and PN &prime;&prime;
</p>
<p>on 〈RN &prime; ,BN &prime;〉 and 〈RN &prime;&prime; ,BN &prime;&prime;〉, respectively, are consistent if the measures induced
by PN &prime; and PN &prime;&prime; on the intersection R
</p>
<p>N =RN &prime; &cap;RN &prime;&prime; (here N =N &prime; &cap;N &prime;&prime;) coincide
with each other. The measures on RN are said to be the projections of PN &prime; and PN &prime;&prime; ,
respectively, on RN . An answer to the above question about the existence of an
</p>
<p>infinite sequence of random variables is given by the following theorem (the proof
</p>
<p>of which is given in Appendix 2).
</p>
<p>Theorem 3.5.1 (Kolmogorov) Specifying a family of consistent distributions PN
on finite-dimensional spaces RN defines a unique probability measure P&infin; on
〈R&infin;,B&infin;〉 such that each probability PN is the projection of P&infin; onto RN .
</p>
<p>It follows from this theorem, in particular, that one can always define on an appro-
</p>
<p>priate space an infinite sequence of arbitrary independent random variables. Indeed,
</p>
<p>direct products of measures given on R1,R2, . . . for different products R
N &prime; and RN
</p>
<p>&prime;&prime;
</p>
<p>are always consistent.
</p>
<p>3.6 Integrals
</p>
<p>3.6.1 Integral with Respect to Measure
</p>
<p>As we have already noted, defining a probability space includes specifying a finite
</p>
<p>countably additive measure. This enables one to consider integrals with respect to
</p>
<p>the measure,
&int;
</p>
<p>g
(
ξ(ω)
</p>
<p>)
P(dω) (3.6.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Integrals 57
</p>
<p>over the set Ω for a Borel function g and any random variable ξ on 〈Ω,F,P〉 (recall
that g(x) is said to be Borel if, for any t , {x : g(x) &lt; t} is a Borel set on the real
line).
</p>
<p>The definition, construction and basic properties of the integral with respect to a
</p>
<p>measure are assumed to be familiar to the reader. If the reader feels his or her back-
</p>
<p>ground is insufficient in this aspect, we recommend Appendix 3 which contains all
</p>
<p>the necessary information. However, the reader could skip this material if he/she is
</p>
<p>willing to restrict him/herself to considering only discrete or absolutely continuous
</p>
<p>distributions for which integrals with respect to a measure become sums or conven-
</p>
<p>tional Riemann integrals. It would also be useful for the sequel to know the Stieltjes
</p>
<p>integral; see the comments in the next subsection.
</p>
<p>We already know that a random variable ξ(ω) induces a measure Fξ on the real
</p>
<p>line which is specified by the equality
</p>
<p>Fξ
(
[x, y)
</p>
<p>)
= P(x &le; ξ &le; y)= Fξ (y)&minus; Fξ (x).
</p>
<p>Using this measure, one can write the integral (3.6.1) as
&int;
</p>
<p>g
(
ξ(ω)
</p>
<p>)
P(dω)=
</p>
<p>&int;
g(x)Fξ (dx).
</p>
<p>This is just the result of the substitution x = ξ(ω). It can be proved simply by
writing down the definitions of both integrals. The integral on the right hand side
</p>
<p>is called the Lebesgue&ndash;Stieltjes integral of the function g(x) with respect to the
measure Pξ and can also be written as&int;
</p>
<p>g(x)dFξ (x). (3.6.2)
</p>
<p>3.6.2 The Stieltjes Integral
</p>
<p>The integral (3.6.2) is often just called the Stieltjes integral, or the Riemann&ndash;Stieltjes
</p>
<p>integral which is defined in a somewhat different way and for a narrower class of
</p>
<p>functions.
</p>
<p>If g(x) is a continuous function, then the Lebesgue&ndash;Stieltjes integral coincides
with the Riemann&ndash;Stieltjes integral which is equal by definition to
</p>
<p>&int;
g(x)dF (x)= lim
</p>
<p>b&rarr;&infin;
a&rarr;&minus;&infin;
</p>
<p>lim
N&rarr;&infin;
</p>
<p>N&sum;
</p>
<p>k=0
g(̃xk)
</p>
<p>[
F(xk+1)&minus; F(xk)
</p>
<p>]
, (3.6.3)
</p>
<p>where the limit on the right-hand side does not depend on the choice of parti-
</p>
<p>tions x0, x1, . . . , xN of the semi-intervals [a, b) and points x̃k &isin; ∆k = [xk, xk+1).
Partitions x0, x1, . . . , xN are different for different N &rsquo;s and have the property that
</p>
<p>maxk(xk+1 &minus; xk)&rarr; 0 as N &rarr;&infin;.
Indeed, as we know (see Appendix 3), the Lebesgue&ndash;Stieltjes integral is
</p>
<p>&int;
g(x)dF (x)= lim
</p>
<p>b&rarr;&infin;
a&rarr;&minus;&infin;
</p>
<p>lim
N&rarr;&infin;
</p>
<p>&int; b
</p>
<p>a
</p>
<p>gN (x)Fξ (dx), (3.6.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>58 3 Random Variables and Distribution Functions
</p>
<p>where gN is any sequence of simple functions (assuming finitely many values) con-
</p>
<p>verging monotonically to g(x). We see from these definitions that it suffices to show
</p>
<p>that the integrals
&int; b
a
</p>
<p>with finite integration limits coincide. Since the Lebesgue&ndash;
</p>
<p>Stieltjes integral
&int; b
a
g dF of a continuous function g always exists, we could obtain
</p>
<p>its value by taking the sequence gN to be any of the two sequences of simple func-
</p>
<p>tions g&lowast;N and g
&lowast;&lowast;
N which are constant on the semi-intervals ∆k and equal on them to
</p>
<p>g&lowast;N (xk)= sup
x&isin;∆k
</p>
<p>g(x) and g&lowast;&lowast;N (xk)= inf
x&isin;∆k
</p>
<p>g(x),
</p>
<p>respectively. Both sequences in (3.6.4) constructed from g&lowast;N and g
&lowast;&lowast;
N will clearly
</p>
<p>converge monotonically from different sides to the same limit equal to the
</p>
<p>Lebesgue&ndash;Stieltjes integral
</p>
<p>&int; b
</p>
<p>a
</p>
<p>g(x)dF (x).
</p>
<p>But for any x̃k &isin;∆k , one has
</p>
<p>g&lowast;&lowast;N (xk)&le; g(̃xk)&le; g&lowast;N (xk),
</p>
<p>and therefore the integral sum in (3.6.3) will be between the bounds
</p>
<p>&int; b
</p>
<p>a
</p>
<p>g&lowast;&lowast;N dF(x)&le;
N&sum;
</p>
<p>k=0
g(X̃k)
</p>
<p>[
F(xk+1)&minus; F(xk)
</p>
<p>]
&le;
&int; b
</p>
<p>a
</p>
<p>g&lowast;N dF(x).
</p>
<p>These inequalities prove the required assertion about the coincidence of the inte-
</p>
<p>grals.
</p>
<p>It is not hard to verify that (3.6.3) and (3.6.4) will also coincide when F(x) is
</p>
<p>continuous and g(x) is a function of bounded variation. In that case,
</p>
<p>&int; b
</p>
<p>a
</p>
<p>g(x)dF (x)= g(x)F (x)|ba &minus;
&int; b
</p>
<p>a
</p>
<p>F(x)dg(x).
</p>
<p>Making use of this fact, we can extend the definition of the Riemann&ndash;Stieltjes in-
</p>
<p>tegral to the case when g(x) is a function of bounded variation and F(x) is an
</p>
<p>arbitrary distribution function. Indeed, let F(x) = Fc(x)+ Fd(x) be a representa-
tion of F(x) as a sum of its continuous and discrete components, and y1, y2, . . . be
</p>
<p>the jump points of Fd(x):
</p>
<p>pk = Fd(yk + 0)&minus; Fd(yk) &gt; 0.
</p>
<p>Then one has to put by definition
&int;
</p>
<p>g(x)dF (x)=
&sum;
</p>
<p>pkg(yk)+
&int;
</p>
<p>g(x)dFc(x),
</p>
<p>where the Riemann&ndash;Stieltjes integral
&int;
g dFc(x) can be understood, as we have
</p>
<p>already noted, in the sense of definition (3.6.3).
</p>
<p>We will say, as is generally accepted, that
&int;
g dF exists if the integral
</p>
<p>&int;
|g|dF
</p>
<p>is finite. It is easy to see from the definition of the Stieltjes integral that, for step</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Integrals 59
</p>
<p>functions F(x) (the distribution is discrete), the integral becomes the sum
&int;
</p>
<p>g(x)dF (x)=
&sum;
</p>
<p>k
</p>
<p>g(xk)
(
F(xk + 0)&minus; F(xk)
</p>
<p>)
=
&sum;
</p>
<p>k
</p>
<p>g(xk)P(ξ = xk),
</p>
<p>where x1, x2, . . . are jump points of F(x). If
</p>
<p>F(x)=
&int; x
</p>
<p>&minus;&infin;
p(x)dx
</p>
<p>is absolutely continuous and p(x) and g(x) are Riemann integrable, then the Stielt-
</p>
<p>jes integral
&int;
</p>
<p>g(x)dF (x)=
&int;
</p>
<p>g(x)p(x)dx
</p>
<p>becomes a conventional Riemann integral.
</p>
<p>We again note that for a reader who is not familiar with Stieltjes integral tech-
niques and integration with respect to measures, it is possible to continue reading
the book keeping in mind only the last two interpretations of the integral. This would
be quite sufficient for an understanding of the exposition. Moreover, most of the
</p>
<p>distributions which are important from the practical point of view are just of one of
</p>
<p>these types: either discrete or absolutely continuous.
</p>
<p>We recall some other properties of the Stieltjes integral (following immediately
</p>
<p>from definitions (3.6.4) or (3.6.3) and (3.6.5)):
</p>
<p>&int; b
</p>
<p>a
</p>
<p>dF = F(b)&minus; F(a);
&int; b
</p>
<p>a
</p>
<p>g dF =
&int; c
</p>
<p>a
</p>
<p>g dF +
&int; b
</p>
<p>c
</p>
<p>g dF if g or F is continuous at the point c;
&int;
(g1 + g2) dF =
</p>
<p>&int;
g1 dF +
</p>
<p>&int;
g2 dF ;
</p>
<p>&int;
cg dF = c
</p>
<p>&int;
g dF for c= const;
</p>
<p>&int; b
</p>
<p>a
</p>
<p>g dF = gF |ba &minus;
&int; b
</p>
<p>a
</p>
<p>F dg
</p>
<p>if g is a function of bounded variation.
</p>
<p>3.6.3 Integrals of Multivariate Random Variables.
</p>
<p>The Distribution of the Sum of Independent
</p>
<p>Random Variables
</p>
<p>Integrals with respect to measure (3.6.1) make sense for multivariate variables
</p>
<p>ξ(ω) = (ξ1(ω), . . . , ξn(ω)) as well (one cannot say the same about Riemann&ndash;</p>
<p/>
</div>
<div class="page"><p/>
<p>60 3 Random Variables and Distribution Functions
</p>
<p>Stieltjes integrals (3.6.3)). We mean here the integral
&int;
</p>
<p>Ω
</p>
<p>g
(
ξ1(ω), . . . , ξn(ω)
</p>
<p>)
P(dω), (3.6.5)
</p>
<p>where g is a measurable function mapping Rn into R, so that g(ξ1(ω), . . . , ξn(ω))
</p>
<p>is a measurable mapping of Ω into R.
</p>
<p>If 〈Rn,Bn,Fξ 〉 is a sample probability space for ξ , then the integral (3.6.5) can
be written as &int;
</p>
<p>Rn
</p>
<p>g(x)Fξ (dx), x = (x1, . . . , xn) &isin;Rn.
</p>
<p>Now turn to the case when the components ξ1, . . . , ξn of the vector ξ are independent
</p>
<p>and assume first that n= 2. For sets
B = B1 &times;B2 =
</p>
<p>{
(x1, x2) : x1 &isin; B1, x2 &isin; B2
</p>
<p>}
&sub;R2,
</p>
<p>where B1 and B2 are measurable subsets of R, one has the equality
</p>
<p>P(ξ &isin; B)= P(ξ1 &isin; B1, ξ2 &isin; B2)= P(ξ1 &isin; B1)P(ξ2 &isin; B2). (3.6.6)
In that case one says that the measure Fξ1,ξ2(dx1, dx2) = P(ξ1 &isin; dx1, ξ2 &isin; dx2)
on R2, corresponding to (ξ1, ξ2), is a direct product of the measures
</p>
<p>Fξ1(dx1)= P(ξ1 &isin; dx1) and Fξ2(dx2)= P(ξ2 &isin; dx2).
As we already know, equality (3.6.6) uniquely specifies a measure on 〈R2,B2〉
</p>
<p>from the given distributions of ξ1 and ξ2 on 〈R,B〉. It turns out that the integral&int;
g(x1, x2)Fξ1ξ2(dx1, dx2) (3.6.7)
</p>
<p>with respect to the measure Fξ1,ξ2 can be expressed in terms of integrals with respect
</p>
<p>to the measures Fξ1 and Fξ2 . Namely, Fubini&rsquo;s theorem holds true (for the proof see
</p>
<p>Appendix 3 or property 5A in Sect. 4.8).
</p>
<p>Theorem 3.6.1 (Theorem on iterated integration) For a Borel function g(x, y)&ge; 0
and independent ξ1 and ξ2,
</p>
<p>&int;
g(x1, x2)Fξ1ξ2(dx1, dx2)=
</p>
<p>&int; [&int;
g(x1, x2)Fξ2(dx2)
</p>
<p>]
Fξ1(dx1). (3.6.8)
</p>
<p>If g(x, y) can assume values of different signs, then the existence of the integral
on the left-hand side of (3.6.8) is required for the equality (3.6.8). The order of
integration on the right-hand side of (3.6.8) may be changed.
</p>
<p>It is shown in Appendix 3 that the measurability of g(x, y) implies that of the
</p>
<p>integrands on the right-hand side of (3.6.8).
</p>
<p>Corollary 3.6.1 Let g(x1, x2)= g1(x1)g2(x2). Then, if at least one of the following
three conditions is met:
</p>
<p>(1) g1 &ge; 0, g2 &ge; 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Integrals 61
</p>
<p>(2)
&int;
g1(x1)g2(x2)Fξ1ξ2(dx1, dx2) exists,
</p>
<p>(3)
&int;
gj (xj )Fξj (dxj ), j = 1,2, exist,
</p>
<p>then
&int;
</p>
<p>g1(x1)g2(x2)Fξ1ξ2(dx1, dx2)=
&int;
</p>
<p>g1(x1)Fξ1(dx1)
</p>
<p>&int;
g2(x2)Fξ2(dx2). (3.6.9)
</p>
<p>To avoid trivial complications, we assume that P(gj (ξj )= 0) 
= 1, j = 1,2.
</p>
<p>Proof Under any of the first two conditions, the assertion of the corollary follows
immediately from Fubini&rsquo;s theorem. For arbitrary g1, g2, put gj = g+j &minus;g
</p>
<p>&minus;
j , g
</p>
<p>&plusmn;
j &ge; 0,
</p>
<p>j = 1,2. If
&int;
g&plusmn;j dFξ &lt; &infin; (we will use here the abridged notation for integrals),
</p>
<p>then
&int;
</p>
<p>g1g2 dFξ1 dFξ2 =
&int;
</p>
<p>g+1 g
+
2 dFξ1 dFξ2 &minus;
</p>
<p>&int;
g+1 g
</p>
<p>&minus;
2 dFξ1 dFξ2
</p>
<p>&minus;
&int;
</p>
<p>g&minus;1 g
+
2 dFξ1 dFξ2 +
</p>
<p>&int;
g&minus;1 g
</p>
<p>&minus;
2 dFξ1 dFξ2
</p>
<p>=
&int;
</p>
<p>g+1 dFξ1
</p>
<p>&int;
g+2 dFξ2 &minus;
</p>
<p>&int;
g+1 dFξ1
</p>
<p>&int;
g+2 dFξ2
</p>
<p>&minus;
&int;
</p>
<p>g&minus;1 dFξ1
</p>
<p>&int;
g+2 dFξ2 +
</p>
<p>&int;
g&minus;1 dFξ1
</p>
<p>&int;
g&minus;2 dFξ2
</p>
<p>=
&int;
</p>
<p>g1 dFξ1
</p>
<p>&int;
g2 dFξ2 . �
</p>
<p>Corollary 3.6.2 In the special case when g(x1, x2) = IB(x1, x2) is the indicator
of a set B &isin;B2, we obtain the formula for sequential computation of the measure
of B:
</p>
<p>P
(
(ξ1, ξ2) &isin; B
</p>
<p>)
=
&int;
</p>
<p>P
(
(x1, ξ2) &isin; B
</p>
<p>)
Fξ1(dx1).
</p>
<p>The probability of the event {(x1, ξ2) &isin; B} could also be written as P(ξ2 &isin; Bx1)=
Pξ2(Bx1) where Bx1 = {x2 : (x1, x2) &isin; B} is the &ldquo;section&rdquo; of the set B at the point x1.
</p>
<p>If B = {(x1, x2) : x1 + x2 &lt; x}, we get
</p>
<p>P
(
(ξ1, ξ2) &isin; B
</p>
<p>)
= P(ξ1 + ξ2 &lt; x)&equiv; Fξ1+ξ2(x)
</p>
<p>=
&int;
</p>
<p>P(x1 + ξ2 &lt; x)Fξ1(dx1)
</p>
<p>=
&int;
</p>
<p>Fξ2(x &minus; x1) dFξ1(x1). (3.6.10)
</p>
<p>We have obtained a formula for the distribution function of the sum of independent
</p>
<p>random variables expressing Fξ1+ξ2 in terms of Fξ1 and Fξ2 . The integral on the
right-hand side of (3.6.10) is called the convolution of the distribution functions</p>
<p/>
</div>
<div class="page"><p/>
<p>62 3 Random Variables and Distribution Functions
</p>
<p>Fξ1(x) and Fξ2(x) and is denoted by Fξ1 &lowast; Fξ2(x). In the same way one can obtain
the equality
</p>
<p>P(ξ1 + ξ2 &lt; x)=
&int; &infin;
</p>
<p>&minus;&infin;
Fξ1(x &minus; t) dFξ2(t).
</p>
<p>Observe that the right-hand side here could also be considered as a result of inte-
</p>
<p>grating
&int;
</p>
<p>dFξ1(t)Fξ2(x &minus; t)
</p>
<p>by parts.
</p>
<p>If at least one of the distribution functions has a density, the convolution also
has a density. This follows immediately from the formulas for convolution. Let, for
instance,
</p>
<p>Fξ2(x)=
&int; x
</p>
<p>&minus;&infin;
fξ2(u) du.
</p>
<p>Then
</p>
<p>Fξ1+ξ2(x)=
&int; &infin;
</p>
<p>&minus;&infin;
Fξ1(dt)
</p>
<p>&int; x
</p>
<p>&minus;&infin;
fξ2(u&minus; t) du
</p>
<p>=
&int; x
</p>
<p>&minus;&infin;
</p>
<p>(&int; &infin;
</p>
<p>&minus;&infin;
Fξ1(dt)fξ2(u&minus; t)
</p>
<p>)
du,
</p>
<p>so that the density of the sum ξ1 + ξ2 equals
</p>
<p>fξ1+ξ2(x)=
&int; &infin;
</p>
<p>&minus;&infin;
Fξ1(dt)fξ2(x &minus; t)=
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
fξ2(x &minus; t) dFξ1(t).
</p>
<p>Example 3.6.1 Let ξ1, ξ2, . . . be independent random variables uniformly dis-
tributed over [0,1], i.e. ξ1, ξ2, . . . have the same distribution function with density
</p>
<p>f (x)=
{
</p>
<p>1, x &isin; [0,1],
0, x /&isin; [0,1].
</p>
<p>(3.6.11)
</p>
<p>Then the density of the sum ξ1 + ξ2 is
</p>
<p>fξ1+ξ2(x)=
&int; 1
</p>
<p>0
</p>
<p>f (x &minus; t) dt =
</p>
<p>⎧
⎨
⎩
</p>
<p>0, x /&isin; [0,2],
x, x &isin; [0,1],
2 &minus; x, x &isin; [1,2].
</p>
<p>(3.6.12)
</p>
<p>The integral present here is clearly the length of the intersection of the segments
</p>
<p>[0,1] and [x &minus; 1, x]. The graph of the density of the sum ξ1 + ξ2 + ξ3 will consist
of three pieces of parabolas:
</p>
<p>fξ1+ξ2+ξ3(x)=
&int; 1
</p>
<p>0
</p>
<p>fξ1+ξ2(x &minus; t) dt =
</p>
<p>⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
</p>
<p>0, x /&isin; [0,3],
x2
</p>
<p>2
, x &isin; [0,1],
</p>
<p>1 &minus; (2&minus;x)2
2
</p>
<p>&minus; (x&minus;1)2
2
</p>
<p>, x &isin; [1,2],
(3&minus;x)2
</p>
<p>2
, x &isin; [2,3].</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Integrals 63
</p>
<p>Fig. 3.2 Illustration to Example 3.6.1. The upper row visualizes the computation of the convolu-
tion integral for the density of ξ1 + ξ2 + ξ3. The lower row displays the densities of ξ1, ξ1 + ξ2,
and ξ1 + ξ2 + ξ3, respectively
</p>
<p>The computation of this integral is visualised in Fig. 3.2, where the shaded areas
</p>
<p>correspond to the values of fξ1+ξ2+ξ3(x) for different x. The shape of the densities
of ξ1, ξ1 + ξ2 and ξ1 + ξ2 + ξ3 is shown in Fig. 3.2b. The graph of the density of the
sum ξ1 + ξ2 + ξ3 + ξ4 will consist of four pieces of cubic parabolas and so on. If
we shift the origin to the point n/2, then, as n increases, the shape (up to a scaling
</p>
<p>transformation) of the density of the sum ξ1 + &middot; &middot; &middot; + ξn will be closer and closer to
that of the function e&minus;x
</p>
<p>2
. We will see below that this is not due to chance.
</p>
<p>In connection with this example we could note that if ξ and η are two independent
</p>
<p>random variables, ξ having the distribution function F(x) and η being uniformly
</p>
<p>distributed over [0,1], then the density of the sum ξ + η at the point x is equal to
</p>
<p>fξ+η(x)=
&int;
</p>
<p>dF(t)fη(x &minus; t)=
&int; x
</p>
<p>x&minus;1
dF(t)= F(x)&minus; F(x &minus; 1).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>Numerical Characteristics of Random Variables
</p>
<p>Abstract This chapter opens with Sect. 4.1 introducing the concept of the expec-
</p>
<p>tation of random variable as the respective Lebesgue integral and deriving its key
</p>
<p>properties, illustrated by a number of examples. Then the concepts of conditional
</p>
<p>distribution functions and conditional expectations given an event are presented and
</p>
<p>discussed in detail in Sect. 4.2, one of the illustrations introducing the ruin problem
</p>
<p>for the simple random walk. In the Sects. 4.3 and 4.4, expectations of independent
</p>
<p>random variables and those of sums of random numbers of random variables are
</p>
<p>considered. In Sect. 4.5, Kolmogorov&ndash;Prokhorov&rsquo;s theorem is proved for the case
</p>
<p>when the number of random terms in the sum is independent of the future, fol-
</p>
<p>lowed by the derivation of Wald&rsquo;s identity. After that, moments of higher orders
</p>
<p>are introduced and discussed, starting with the variance in Sect. 4.5 and proceeding
</p>
<p>to covariance and correlation coefficient and their key properties in Sect. 4.6. Sec-
</p>
<p>tion 4.7 is devoted to the fundamental moment inequalities: Cauchy&ndash;Bunjakovsky&rsquo;s
</p>
<p>inequality (a.k.a. Cauchy&ndash;Schwarz inequality), H&ouml;lder&rsquo;s and Jensen&rsquo;s inequalities,
</p>
<p>followed by inequalities for probabilities (Markov&rsquo;s and Chebyshev&rsquo;s inequalities).
</p>
<p>Section 4.8 extends the concept of conditional expectation (given a random variable
</p>
<p>or sigma-algebra), starting with the discrete case, then turning to square-integrable
</p>
<p>random variables and using projections, and finally considering the general case
</p>
<p>basing on the Radon&ndash;Nykodim theorem (proved in Appendix 3). The properties of
</p>
<p>the conditional expectation are studied, following by introducing the concept of con-
</p>
<p>ditional distribution given a random variable and illustrating it by several examples
</p>
<p>in Sect. 4.9.
</p>
<p>4.1 Expectation
</p>
<p>Definition 4.1.1 The (mathematical) expectation, or mean value, of a random vari-
able ξ given on a probability space 〈Ω,F,P〉 is defined as the quantity
</p>
<p>Eξ =
&int;
</p>
<p>Ω
</p>
<p>ξ(ω)P(dω).
</p>
<p>Let ξ&plusmn; = max(0,&plusmn;ξ). The values Eξ&plusmn; &ge; 0 are always well defined (see Ap-
pendix 3). We will say that Eξ exists if max(Eξ+,Eξ&minus;) &lt;&infin;.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_4, &copy; Springer-Verlag London 2013
</p>
<p>65</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_4">http://dx.doi.org/10.1007/978-1-4471-5201-9_4</a></div>
</div>
<div class="page"><p/>
<p>66 4 Numerical Characteristics of Random Variables
</p>
<p>We will say that Eξ is well defined if min(Eξ+,Eξ&minus;) &lt; &infin;. In this case the
difference Eξ+ &minus;Eξ&minus; is always well defined, but Eξ = Eξ+ &minus;Eξ&minus; may be &plusmn;&infin;.
</p>
<p>By virtue of the above remarks (see Sect. 3.6) one can also define Eξ as
</p>
<p>Eξ :=
&int;
</p>
<p>xFξ (dx)=
&int;
</p>
<p>x dF(x), (4.1.1)
</p>
<p>where F(x) is the distribution function of ξ . It follows from the definition that Eξ
</p>
<p>exists if E|ξ | &lt; &infin;. It is not hard to see that Eξ does not exist if, for instance,
1 &minus; F(x) &gt; 1/x for all sufficiently large x.
</p>
<p>We already know that if F(x) is a step function then the Stieltjes integral (4.1.1)
</p>
<p>becomes the sum
</p>
<p>Eξ :=
&sum;
</p>
<p>k
</p>
<p>xkP(ξ = xk).
</p>
<p>If F(x) has a density f (x), then
</p>
<p>Eξ :=
&int;
</p>
<p>xf (x)dx,
</p>
<p>so that Eξ is the point of the &ldquo;centre of gravity&rdquo; of the distribution F of the unit
</p>
<p>mass on the real line and corresponds to the natural interpretation of the mean value
</p>
<p>of the distribution.
</p>
<p>If g(x) is a Borel function, then η= g(ξ) is again a random variable and
</p>
<p>Eg(ξ)=
&int;
</p>
<p>g
(
ξ(ω)
</p>
<p>)
P(dω)=
</p>
<p>&int;
g(x)dF (x)=
</p>
<p>&int;
x dFg(ξ)(x).
</p>
<p>The last equality follows from definition (4.1.1).
</p>
<p>The basic properties of expectations coincide with those of the integral:
</p>
<p>E1. If a and b are constants, then E(a + bξ)= a + bEξ .
E2. E(ξ1 + ξ2) = E(ξ1) + E(ξ2), if any two of the expectations appearing in the
</p>
<p>formula exist.
E3. If a &le; ξ &le; b, then a &le; Eξ &le; b. The inequality Eξ &le; E|ξ | always holds.
E4. If ξ &ge; 0 and Eξ = 0, then ξ = 0 with probability 1.
E5. The probability of an event A can be expressed in terms of expectations as
</p>
<p>P(A)= EI(A),
where I(A) is the random variable equal to the indicator of the event A:
I(A)= 1 if ω &isin;A and I(A)= 0 otherwise.
</p>
<p>For further properties of expectations, see Appendix 3.
</p>
<p>We consider several examples.
</p>
<p>Example 4.1.1 Expectations related to the Bernoulli scheme. Let ξ &sub;= Bp , i.e. ξ
assumes two values: 0 with probability q and 1 with probability p, where p+q = 1.
Then
</p>
<p>Eξ = 0 &times; P(ξ = 0)+ 1 &times; P(ξ = 1)= p.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Expectation 67
</p>
<p>Now consider a sequence of trials in the Bernoulli scheme until the time of
</p>
<p>the first &ldquo;success&rdquo;. In other words, consider a sequence of independent variables
</p>
<p>ξ1, ξ2, . . . distributed as ξ until the time
</p>
<p>η := min{k &ge; 1 : ξk = 1}.
It is evident that η is a random variable,
</p>
<p>P(η= k)= qk&minus;1p, k &ge; 1,
so that η&minus; 1 has the geometric distribution. Consequently,
</p>
<p>Eη=
&infin;&sum;
</p>
<p>k=1
kqk&minus;1p = p
</p>
<p>(1 &minus; q)2 =
1
</p>
<p>p
.
</p>
<p>If we put Sn :=
&sum;n
</p>
<p>1 ξk , then clearly ESn = np. Now define, for an integer N &ge; 1,
the random variable η= min{k &ge; 1 : Sk =N} as the &ldquo;first passage time&rdquo; of level N
by the sequence Sn. One has
</p>
<p>P(η= k)= P(Sk&minus;1 =N &minus; 1)p,
</p>
<p>Eη= p
&infin;&sum;
</p>
<p>N
</p>
<p>k
</p>
<p>(
k &minus; 1
N &minus; 1
</p>
<p>)
pN&minus;1qk&minus;N = p
</p>
<p>N
</p>
<p>(N &minus; 1)!
</p>
<p>&infin;&sum;
</p>
<p>k=N
k(k &minus; 1) &middot; &middot; &middot; (k &minus;N + 1)qk&minus;N .
</p>
<p>The sum here is equal to the N -th derivative of the function ψ(z) =
&sum;&infin;
</p>
<p>0 z
k =
</p>
<p>1/(1 &minus; z) at the point z = q , i.e. it equals N !/pN+1. Thus Eη = N/p. As we will
see below, this equality could be obtained as an obvious consequence of the results
</p>
<p>of Sect. 4.4.
</p>
<p>Example 4.1.2 If ξ &sub;=�a,σ 2 then
</p>
<p>Eξ =
&int;
</p>
<p>tφa,σ 2(t) dt =
&int;
</p>
<p>t
1
</p>
<p>σ
&radic;
</p>
<p>2π
e
&minus; (t&minus;a)
</p>
<p>2
</p>
<p>2σ2 dt
</p>
<p>= 1
σ
&radic;
</p>
<p>2π
</p>
<p>&int;
(t &minus; a)e&minus;
</p>
<p>(t&minus;a)2
2σ2 dt + a
</p>
<p>σ
&radic;
</p>
<p>2π
</p>
<p>&int;
e
&minus; (t&minus;a)
</p>
<p>2
</p>
<p>2σ2 dt
</p>
<p>= 1
σ
&radic;
</p>
<p>2π
</p>
<p>&int;
ze
</p>
<p>&minus; z2
2σ2 dz+ a = a.
</p>
<p>Thus the parameter a of the normal law is equal to the expectation of the latter.
</p>
<p>Example 4.1.3 If ξ &sub;=�&micro;, then Eξ = &micro;. Indeed,
</p>
<p>Eξ =
&infin;&sum;
</p>
<p>k=0
k
&micro;k
</p>
<p>k! e
&minus;&micro; = &micro;e&minus;&micro;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>&micro;k&minus;1
</p>
<p>(k &minus; 1)! = &micro;.
</p>
<p>Example 4.1.4 If ξ &sub;=U0,1, then
</p>
<p>Eξ =
&int; 1
</p>
<p>0
</p>
<p>x dx = 1
2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>68 4 Numerical Characteristics of Random Variables
</p>
<p>It follows from property E1 that, for ξ &sub;=Ua,b , one has
</p>
<p>Eξ = a + b&minus; a
2
</p>
<p>= a + b
2
</p>
<p>.
</p>
<p>If ξ &sub;= K0,1 then the expectation Eξ does not exist. That follows from the fact
that the integral
</p>
<p>&int;
x dx
</p>
<p>1+x2 diverges.
</p>
<p>Example 4.1.5 We now consider an example that is, in a sense, close to Exam-
ple 4.1.1 on the computation of Eη, but which is more complex and corresponds
</p>
<p>to computing the mean value of the duration of a chess tournament in a real-life
</p>
<p>situation. In Sect. 3.4 we described a simple probabilistic model of a chess tourna-
</p>
<p>ment. The first player wins in a given game, independently of the outcomes of the
</p>
<p>previous games, with probability p, loses with probability q , p+ q &lt; 1, and makes
a tie with probability 1&minus;p&minus; q . Of course, this is a rather rough first approximation
since in a real-life tournament there is apparently no independence. On the other
</p>
<p>hand, it is rather unlikely that, for balanced high level players, the above probabili-
</p>
<p>ties would substantially vary from game to game or depend on the outcomes of their
</p>
<p>previous results. A more complex model incorporating dependence of p and q of
</p>
<p>the outcomes of previous games will be considered in Example 13.4.2.
</p>
<p>Assume that the tournament continues until one of the two participants wins N
</p>
<p>games (then this player will be declared the winner). For instance, the 1984 individ-
</p>
<p>ual World Championship match between A. Karpov and G. Kasparov was organised
</p>
<p>just according to this scheme with N = 6. What can one say about the expectation
Eη of the duration η of the tournament?
</p>
<p>As was shown in Example 3.3.1,
</p>
<p>P(η= n)= p
N&minus;1&sum;
</p>
<p>i=0
p(n&minus; 1;N &minus; 1, i)+ q
</p>
<p>N&minus;1&sum;
</p>
<p>i=0
p(n&minus; 1; i,N &minus; 1),
</p>
<p>where
</p>
<p>p(n; i, j)= n!
i!j !(n&minus; i &minus; j)!p
</p>
<p>iqj (1 &minus; p&minus; q)n&minus;i&minus;j .
</p>
<p>Therefore, under obvious conventions on the summation indices,
</p>
<p>Eη= 1
(N &minus; 1)!
</p>
<p>N&minus;1&sum;
</p>
<p>i=0
</p>
<p>pNq i + piqN
i!
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
n(n&minus; 1)&times; &middot; &middot; &middot;
</p>
<p>&times; (n&minus; i &minus;N + 1)(1 &minus; p&minus; q)n&minus;i&minus;N .
The sum over n was calculated in Example 4.1.1 to be (N + i)!/(p + q)N+i+1.
Consequently,
</p>
<p>Eη= N
p+ q
</p>
<p>N&minus;1&sum;
</p>
<p>i=0
</p>
<p>(pNq i + piqN )(N + i)!
i!N !(p+ q)i+N
</p>
<p>= N
p+ q
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
</p>
<p>(
N + i
</p>
<p>i
</p>
<p>)[
rN (1 &minus; r)i + r i(1 &minus; r)N
</p>
<p>]
,
</p>
<p>where r = p/(p+ q).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Expectation 69
</p>
<p>In his interview of 3 March 1985 to the newspaper &ldquo;Izvestija&rdquo;, Karpov said
</p>
<p>that in qualifying tournaments he would lose, on average, 1 game out of 20, and
</p>
<p>that Kasparov&rsquo;s results were similar. If we put in our simple model p = q = 1/20
(strictly speaking, one cannot make such a conclusion from the given data; the rela-
</p>
<p>tion p = q = 1/20 should be considered rather as one of many possible hypotheses)
then, for N = 6, direct calculations show that (r = 1/2)
</p>
<p>Eη= 15
8
</p>
<p>[
1 + 21
</p>
<p>(
1 + 5
</p>
<p>8
+ 11
</p>
<p>16
</p>
<p>)]
&asymp; 93.
</p>
<p>Thus, provided that our simplest model is adequate, the expected duration of
</p>
<p>a tournament turns out to be very large. The fact that the match between Karpov
</p>
<p>and Kasparov was interrupted by the decision of the chairman of the World Chess
</p>
<p>Federation after 48 games because the match had dragged on, might serve as a
</p>
<p>confirmation of the correctness of the assumptions we made.
</p>
<p>Taking into account the results of the match and consequent games between Kar-
</p>
<p>pov and Kasparov could lead to estimates (approximate values) for the quantities p
</p>
<p>and q that would differ from 1/20.
</p>
<p>For our model, one also has the following simple inequality:
</p>
<p>N
</p>
<p>p+ q &lt; Eη &lt;
2N &minus; 1
p+ q .
</p>
<p>It follows from the relation ηN &le; η &le; η2N&minus;1, where ηN is the number of games until
the time when the total of the points gained by both players reaches N . By virtue of
</p>
<p>Example 4.1.1, EηN =N/(p+ q).
</p>
<p>Example 4.1.6 In the problem on cells in Sects. 1.3 and 1.4, we considered the
probability that at least one of the n cells in which r particles are placed at random
</p>
<p>is empty. Find the expectation of the number Sn,r of empty cells after r particles
</p>
<p>have been placed. If Ak denotes the event that the k-th cell is empty and I(Ak) is the
</p>
<p>indicator of this event then
</p>
<p>Sn,r =
n&sum;
</p>
<p>1
</p>
<p>I(Ak), ESn,r =
n&sum;
</p>
<p>1
</p>
<p>P(Ak)= n
(
</p>
<p>1 &minus; 1
n
</p>
<p>)r
.
</p>
<p>Note now that ESn,r is close to 0 if (1 &minus; 1/n)r is small compared with 1/n, i.e.
when &minus;r ln(1 &minus; 1/n)&minus; lnn is large. For large n,
</p>
<p>ln
</p>
<p>(
1 &minus; 1
</p>
<p>n
</p>
<p>)
=&minus;1
</p>
<p>n
+O
</p>
<p>(
1
</p>
<p>n2
</p>
<p>)
,
</p>
<p>and the required relation will hold if (r &minus; n lnn)/n is large. In our case (cf. prop-
erty E4), the smallness of ESn,r will clearly imply that of P(A)= P(Sn,r &gt; 0).</p>
<p/>
</div>
<div class="page"><p/>
<p>70 4 Numerical Characteristics of Random Variables
</p>
<p>4.2 Conditional Distribution Functions and Conditional
</p>
<p>Expectations
</p>
<p>Let 〈Ω,F,P〉 be a probability space and B &isin; F be an event with P(B) &gt; 0. Form a
new probability space 〈Ω,F,PB〉, where PB is defined for A &isin; F by the equality
</p>
<p>PB(A) := P(A|B).
</p>
<p>It is easy to verify that the probability properties P1, P2 and P3 hold for PB . Let
</p>
<p>ξ be a random variable on 〈Ω,F,P〉. It is clearly a random variable on the space
〈Ω,F,PB〉 as well.
</p>
<p>Definition 4.2.1 The expectation of ξ in the space 〈Ω,F,PB〉 is called the condi-
tional expectation of ξ given B and is denoted by E(ξ |B):
</p>
<p>E(ξ |B)=
&int;
</p>
<p>Ω
</p>
<p>ξ(ω)PB(dω).
</p>
<p>By the definition of the measure PB ,
</p>
<p>E(ξ |B)=
&int;
</p>
<p>Ω
</p>
<p>ξ(ω)P(dω|B)= 1
P(B)
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>ξ(ω)P(dω &cap;B)= 1
P(B)
</p>
<p>&int;
</p>
<p>B
</p>
<p>ξ(ω)P(dω).
</p>
<p>The last integral differs from Eξ in that the integration in it is carried over the set B
</p>
<p>only. We will denote this integral by
</p>
<p>E(ξ ;B) :=
&int;
</p>
<p>B
</p>
<p>ξ(ω)P(dω),
</p>
<p>so that
</p>
<p>E(ξ |B)= 1
P(B)
</p>
<p>E(ξ ;B).
</p>
<p>It is not hard to see that the function
</p>
<p>F(x|B) := PB(ξ &lt; x)= P(ξ &lt; x|B)
</p>
<p>is the distribution function of the random variable ξ on 〈Ω,F,PB〉.
</p>
<p>Definition 4.2.2 The function F(x|B) is called the conditional distribution function
of ξ (in the &ldquo;conditional&rdquo; space 〈Ω,F,PB〉) given B .
</p>
<p>The quantity E(ξ |B) can evidently be rewritten as
&int;
</p>
<p>x dF(x|B).
</p>
<p>If the σ -algebra σ(ξ) generated by the random variable ξ does not depend on the
</p>
<p>event B , then PB(A)= P(A) for any A &isin; σ(ξ). Therefore, in that case
</p>
<p>F(x|B)= F(x), E(ξ |B)= Eξ, E(ξ ;B)= P(B)Eξ. (4.2.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Conditional Distribution Functions and Conditional Expectations 71
</p>
<p>Let {Bn} be a (possibly finite) sequence of disjoint events such that
⋃
</p>
<p>Bn =Ω and
P(Bn) &gt; 0 for any n. Then
</p>
<p>Eξ =
&int;
</p>
<p>Ω
</p>
<p>ξ(ω)P(dω)=
&sum;
</p>
<p>n
</p>
<p>&int;
</p>
<p>Bn
</p>
<p>ξ(ω)P(dω)
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p>E(ξ ;Bn)=
&sum;
</p>
<p>n
</p>
<p>P(Bn)E(ξ |Bn). (4.2.2)
</p>
<p>We have obtained the total probability formula for expectations. This formula can
be rather useful.
</p>
<p>Example 4.2.1 Let the lifetime of a device be a random variable ξ with a distribution
function F(x). We know that the device has already worked for a units of time.
</p>
<p>What is the distribution of the residual lifetime? What is the expectation of the
</p>
<p>latter?
</p>
<p>Clearly, in this problem we have to find P(ξ &minus; a &ge; x|ξ &ge; a) and E(ξ &minus; a|ξ &ge; a).
Of course, it is assumed that
</p>
<p>P(a) := P(ξ &ge; a) &gt; 0.
By the above formulae,
</p>
<p>P(ξ &minus; a &ge; x|ξ &ge; a)= P(x + a)
P (a)
</p>
<p>, E(ξ &minus; a|ξ &ge; a)= 1
P(a)
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>x dF(x + a).
</p>
<p>It is interesting to note the following. In many applied problems, especially when
</p>
<p>one deals with the operation of complex devices consisting of a large number of
</p>
<p>reliable parts, the distribution of ξ can be assumed to be exponential:
</p>
<p>P(x)= P(ξ &ge; x)= e&minus;&micro;x, &micro; &gt; 0.
(The reason for this will become clear later, when considering the Poisson theorem
</p>
<p>and Poisson process. Computers could serve as examples of such devices.) But, for
</p>
<p>the exponential distribution, it turns out that the residual lifetime distribution
</p>
<p>P(ξ &minus; a &ge; x|ξ &ge; a)= P(x + a)
P (a)
</p>
<p>= e&minus;&micro;x = P(x) (4.2.3)
</p>
<p>coincides with the lifetime distribution of a new device. In other words, a new de-
</p>
<p>vice and a device which has already worked without malfunction for some time
</p>
<p>a are, from the viewpoint of their future failure-free operation time distributions,
</p>
<p>equivalent.
</p>
<p>It is not hard to understand that the exponential distribution (along with its dis-
</p>
<p>crete analogue P(ξ = k) = qk(1 &minus; q), k = 0,1, . . .) is the only distribution pos-
sessing the above remarkable property. One can see that, from equality (4.2.3), we
</p>
<p>necessarily have
</p>
<p>P(x + a)= P(x)P (a).
</p>
<p>Example 4.2.2 Assume that n machines are positioned so that the distance between
the i-th and j -th machines is ai,j , 1 &le; i, j &le; n. Each machine requires service from</p>
<p/>
</div>
<div class="page"><p/>
<p>72 4 Numerical Characteristics of Random Variables
</p>
<p>time to time (tuning, repair, etc.). Assume that the service is to be done by a single
</p>
<p>worker and that the probability that a given new call for service comes from the
</p>
<p>j -th machine is pj (
&sum;n
</p>
<p>j=1 pj = 1). If, for instance, the worker has just completed
servicing the i-th machine, then with probability pj (not depending on i) the next
</p>
<p>machine to be served will be the j -th machine; the worker will then need to go to it
</p>
<p>and cover a distance of aij units. What is the mean length of such a passage?
</p>
<p>Let Bi denote the event that the i-th machine was serviced immediately before a
</p>
<p>given passage. Then P(Bi)= pi , and the probability that the worker will move from
the i-th machine to the j -th machine, j = 1, . . . , n, is equal to pj . The length ξ of
the passage is aij . Hence
</p>
<p>E(ξ |Bi)=
n&sum;
</p>
<p>j=1
pjai,j ,
</p>
<p>and by the total probability formula
</p>
<p>Eξ =
n&sum;
</p>
<p>i=1
P(Bi)E(ξ |Bi)=
</p>
<p>n&sum;
</p>
<p>i,j=1
pjpiaij .
</p>
<p>The obtained expression enables one to compare different variants of positioning
</p>
<p>machines from the point of view of minimisation of the quantity Eξ under given
</p>
<p>restrictions on aij . For instance, if aij &ge; 1 and all the machines are of the same type
(pj = 1/n) then, provided they are positioned along a straight line (with unit steps
between them), one gets aij = |j &minus; i| and1
</p>
<p>Eξ = 1
n2
</p>
<p>n&sum;
</p>
<p>i,j=1
|j &minus; i| = 2
</p>
<p>n2
</p>
<p>n&minus;1&sum;
</p>
<p>k=1
k(n&minus; k)= n&minus; 1
</p>
<p>3
</p>
<p>(
1 + 1
</p>
<p>n
</p>
<p>)
,
</p>
<p>so that, for large n, the value of Eξ is close to n/3. Thus, if there are s calls a day
</p>
<p>then the average total distance covered daily by the worker is approximately sn/3.
</p>
<p>It is easy to show that positioning machines around a circle would be better but still
</p>
<p>not optimal.
</p>
<p>Example 4.2.3 As was already noticed, not all random variables (distributions) have
expectations. The respective examples are by no means pathological: for instance,
</p>
<p>the Cauchy distribution Kα,σ has this property. Now we will consider a problem on
</p>
<p>random walks in which there also arise random variables having no expectations.
</p>
<p>This is the problem on the so-called fair game. Two gamblers take part in the game.
The initial capital of the first gambler is z units. This gambler wins or loses each
</p>
<p>1To compute the sum, it suffices to note that
</p>
<p>n&minus;1&sum;
</p>
<p>k=1
k(k &minus; 1)= 1
</p>
<p>3
(n&minus; 2)(n&minus; 1)n
</p>
<p>(compare the initial values and increments of the both sides).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Conditional Distribution Functions and Conditional Expectations 73
</p>
<p>play of the game with probability 1/2 independently of the outcomes of the previous
</p>
<p>plays, his capital increasing or decreasing by one unit, respectively. Let z + Sk be
the capital of the first gambler after the k-th play, η(z) is the number of steps until
</p>
<p>his ruin in the game versus an infinitely rich adversary, i.e.
</p>
<p>η(z)= min{k : z+ Sk = 0}, η(0)= 0.
If infk Sk &gt;&minus;z (i.e. the first gambler is never ruined), we put η(z)=&infin;.
First we show that η(z) is a proper random variable, i.e. a random variable
</p>
<p>assuming finite values with probability 1. For the first gambler, this will mean that
</p>
<p>he goes bankrupt with probability 1 whatever his initial capital z is. Here one could
</p>
<p>take Ω to be the &ldquo;sample&rdquo; space consisting of all possible sequences made up of
</p>
<p>1 and &minus;1. Each such sequence ω would describe a &ldquo;trajectory&rdquo; of the game. (For
example, &minus;1 in the k-th place means that the first gambler lost the k-th play.) We
leave it to the reader as an exercise to complete the construction of the probability
</p>
<p>space 〈Ω,F,P〉. Clearly, one has to do this so that the probability of any first n
outcomes of the game (the first n components of ω are fixed) is equal to 2&minus;n.
</p>
<p>Put
</p>
<p>u(z) := P
(
η(z) &lt;&infin;
</p>
<p>)
, u(0) := 1,
</p>
<p>and denote by B1 the event that the first component of ω is 1 (the gambler won in
</p>
<p>the first play) and B2 that this component is &minus;1 (the gambler lost). Noticing that
P(η(z) &lt;&infin;|B1)= u(z+ 1) (if the first play is won, the capital becomes z+ 1), we
obtain by the total probability formula that, for z&ge; 1,
</p>
<p>u(z)= P(B1)P
(
η(z) &lt;&infin;|B1
</p>
<p>)
+ P(B2)P
</p>
<p>(
η(z) &lt;&infin;|B2
</p>
<p>)
</p>
<p>= 1
2
u(z+ 1)+ 1
</p>
<p>2
u(z&minus; 1).
</p>
<p>Putting δ(z) := u(z + 1) &minus; u(z), z &ge; 0, we conclude from here that δ(z) &minus;
δ(z&minus; 1)= 0, and hence δ(z)= δ = const. Since
</p>
<p>u(z+ 1)= u(0)+
z&sum;
</p>
<p>k=1
δ(k)= u(0)+ zδ,
</p>
<p>it is evident that δ can be nothing but 0, so that u(z)&equiv; 1 for all z.
Thus, in a game against an infinitely rich adversary, the gambler will be ruined
</p>
<p>with probability 1. This explains, to some extent, the fact that all reckless gamblers
</p>
<p>(not stopping &ldquo;at the right time&rdquo;; choosing this &ldquo;right time&rdquo; is a separate problem)
</p>
<p>go bankrupt sooner or later. Even if the game is fair.
</p>
<p>We show now that although η(z) is a proper random variable, Eη(z) =&infin;. As-
sume the contrary:
</p>
<p>v(z) := Eη(z) &lt;&infin;.
Similarly to the previous argument, we notice that E(η(z)|B1)= 1 + v(z+ 1) (the
capital became z + 1, one play has already been played). Therefore by the total
probability formula we find for z&ge; 1 that
</p>
<p>v(z)= 1
2
</p>
<p>(
1 + v(z+ 1)
</p>
<p>)
+ 1
</p>
<p>2
</p>
<p>(
1 + v(z&minus; 1)
</p>
<p>)
, v(0)= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>74 4 Numerical Characteristics of Random Variables
</p>
<p>It can be seen from this formula that if v(z) &lt; &infin;, then v(k) &lt; &infin; for all k. Set
∆(z)= v(z+ 1)&minus; v(z). Then the last equality can be written down for z&ge; 1 as
</p>
<p>&minus;1 = 1
2
∆(z)&minus; 1
</p>
<p>2
∆(z&minus; 1),
</p>
<p>or
</p>
<p>∆(z)=∆(z&minus; 1)&minus; 2.
From this equality we find that ∆(z)=∆(0)&minus; 2z. Therefore
</p>
<p>v(z)=
z&minus;1&sum;
</p>
<p>k=0
∆(k)= z∆(0)&minus; z(z&minus; 1)= zv(1)&minus; z(z&minus; 1).
</p>
<p>It follows that Eη(z) &lt; 0 for sufficiently large z. But η(z) is a positive random
</p>
<p>variable and hence Eη(z) &ge; 0. The contradiction shows that the assumption on the
finiteness of the expectation of η(z) is wrong.
</p>
<p>4.3 Expectations of Functions of Independent Random Variables
</p>
<p>Theorem 4.3.1
</p>
<p>1. Let ξ and η be independent random variables and g(x, y) be a Borel function.
Then if g &ge; 0 or Eg(ξ, η) is finite, then
</p>
<p>Eg(ξ, η)= E
[
Eg(x, η)|x=ξ
</p>
<p>]
. (4.3.1)
</p>
<p>2. Let g(x, y) = g1(x)g2(y). If g1(ξ) &ge; 0 and g2(η) &ge; 0, or both Eg1(ξ) and
Eg2(η) exist, then
</p>
<p>Eg(ξ, η)= Eg1(ξ)Eg2(η). (4.3.2)
The expectation Eg(ξ, η) exists if and only if both Eg1(ξ) and Eg2(η) exist. (We
exclude here the trivial cases P(g1(ξ) = 0) = 1 and P(g2(η) = 0) = 1 to avoid
trivial complications.)
</p>
<p>Proof The first assertion of the theorem is a paraphrase of Fubini&rsquo;s theorem in terms
of expectations. The first part of the second assertion follows immediately from
</p>
<p>Corollary 3.6.1 of Fubini&rsquo;s theorem. Since |g1(ξ)| &ge; 0 and |g2(η)| &ge; 0 and these
random variables are independent, one has
</p>
<p>E
∣∣g1(ξ)g2(η)
</p>
<p>∣∣= E
∣∣g1(ξ)
</p>
<p>∣∣E
∣∣g2(η)
</p>
<p>∣∣.
Now the last assertion of the theorem follows immediately, for one clearly has
</p>
<p>E|g1(ξ)| 
= 0, E|g2(η)| 
= 0. �
</p>
<p>Remark 4.3.1 Formula (4.3.1) could be considered as the total probability formula
for computing the expectation Eg(ξ, η). Assertion (4.3.2) could be written down
</p>
<p>without loss of generality in the form
</p>
<p>Eξη= EξEη. (4.3.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Expectations of Sums of a Random Number of Random Variables 75
</p>
<p>To get (4.3.2) from this, one has to take g1(ξ) instead of ξ and g2(η) instead of
</p>
<p>η&mdash;these will again be independent random variables.
</p>
<p>Examples of the use of Theorem 4.3.1 were given in Sect. 3.6 and will be appear-
</p>
<p>ing in the sequel.
</p>
<p>The converse to (4.3.2) or (4.3.3) does not hold. There exist dependent random
</p>
<p>variables ξ and η such that
</p>
<p>Eξη= EξEη.
Let, for instance, ζ and ξ be independent and Eξ = Eζ = 0. Put η= ξζ . Then ξ and
η are clearly dependent (excluding some trivial cases when, say, ξ = const), but
</p>
<p>Eξη= Eξ2ζ = Eξ2Eζ = 0 = EξEη.
</p>
<p>4.4 Expectations of Sums of a Random Number of Random
</p>
<p>Variables
</p>
<p>Assume that a sequence {ξn}&infin;n=1 of independent random variables (or random vec-
tors) and an integer-valued random variable ν &ge; 0 are given on a probability space
〈Ω,F,P〉.
</p>
<p>Property E2 of expectations implies that, for sums Sn =
&sum;n
</p>
<p>i=1 ξi , the following
equality holds:
</p>
<p>ESn =
n&sum;
</p>
<p>i=1
Eξi .
</p>
<p>In particular, if ak = Eξk = a do not depend on k then ESn = an.
What can be said about the expectation of the sum sν of the random number ν
</p>
<p>of random variables ξ1, ξ2, . . .? To answer this question we need to introduce some
</p>
<p>new notions.
</p>
<p>Let Fk,n := σ(ξk, . . . , ξn) be the σ -algebra generated by the n&minus; k + 1 random
variables ξk, . . . , ξn.
</p>
<p>Definition 4.4.1 A random variable ν is said to be independent of the future if the
event {ν &le; n} does not depend on Fn+1,&infin;.
</p>
<p>Let, further, a family of embedded σ -algebras Fn : Fn &sub; Fn+1 be given, such that
F1,n = σ(ξ1, . . . , ξn)&sub; Fn.
</p>
<p>Definition 4.4.2 A random variable ν is said to be a Markov (or stopping) time with
respect to the family {Fn}, if {ν &le; n} &isin; Fn.
</p>
<p>Often Fn is taken to be F1,n = σ(ξ1, . . . , ξn). We will call a stopping time with re-
spect to F1,n simply a stopping (or Markov) time without indicating the correspond-
ing family of σ -algebras. In this case, knowing the values of ξ1, . . . , ξn enables us
</p>
<p>to say whether the event {ν &le; n} has occurred or not.
If the ξn are independent (the σ -algebras F1,n and Fn+1,&infin; are independent) then
</p>
<p>the requirement of independence of the future is wider than the Markov property,</p>
<p/>
</div>
<div class="page"><p/>
<p>76 4 Numerical Characteristics of Random Variables
</p>
<p>because if ν is a stopping time with respect to {F1,n} then, evidently, the random
variable ν does not depend on the future.
</p>
<p>As for a converse statement, one can only assert the following. If ν does not
</p>
<p>depend on the future and the ξk are independent then one can construct a family of
</p>
<p>embedded σ -algebras {Fn}, Fn &sup; F1,n, such that ν is a stopping time with respect
to Fn ({ν &le; n} &sub; Fn) and Fn does not depend on Fn+1,&infin;. As Fn, we can take the σ -
algebra generated by F1,n and the events {ν = k} for k &le; n. For instance, a random
variable ν independent of {ξi} clearly does not depend on the future, but is not a
stopping time. Such ν will be a stopping time only with respect to the family {Fn}
constructed above.
</p>
<p>It should be noted that, formally, any random variable can be made a stopping
</p>
<p>time using the above construction (but, generally speaking, there will be no inde-
</p>
<p>pendence of Fn and Fn1,&infin;). However, such a construction is unsubstantial and not
particularly useful. In all the examples below the variables ν not depending on the
</p>
<p>future are stopping times defined in a rather natural way.
</p>
<p>Example 4.4.1 Let ν be the number of the first random variable in the sequence
{ξn}&infin;n=1 which is greater than or equal to N , i.e. ν = inf{k : ξk &ge;N}. Clearly, ν is a
stopping time, since
</p>
<p>{ν &le; n} =
n⋃
</p>
<p>k=1
{ξk &ge;N} &isin; F1,n.
</p>
<p>If ξk are independent, then evidently ν is independent of the future.
</p>
<p>The same can be said about the random variable
</p>
<p>η(t) := min{k : Sk &ge;N}, Sk =
k&sum;
</p>
<p>j=1
ξj .
</p>
<p>Note that the random variables ν and η(t) may be improper (e.g., η(t) is not defined
</p>
<p>on the event {S := supSk &lt;N}). The random variable θ := min{k : Sk = S} is not a
stopping time and depends on the future.
</p>
<p>The term &ldquo;Markov&rdquo; random variable (or Markov time) will become clearer after
</p>
<p>introducing the notion of Markovity in Chap. 13. The term &ldquo;stopping time&rdquo; is related
</p>
<p>to the nature of a large number of applied problems in which such random variables
</p>
<p>arise. As a typical example, the following procedure could be considered. Let ξk be
</p>
<p>the number of defective items in the k-th lot produced by a factory. Statistical quality
</p>
<p>control is carried out as follows. The whole production is rejected if, in sequential
</p>
<p>testing of the lots, it turns out that, for some n, the value of the sum
</p>
<p>Sn =
n&sum;
</p>
<p>k=1
ξk
</p>
<p>exceeds a given admissible level a + bn. The lot number ν for which this happens,
</p>
<p>ν := min{n : Sn &ge; a + bn},</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Expectations of Sums of a Random Number of Random Variables 77
</p>
<p>is a stopping time for the whole testing procedure. To avoid a lengthy testing, one
</p>
<p>also introduces a (literal) stopping time
</p>
<p>ν&lowast; := min{n : Sn &le;&minus;A+ bn},
where A&gt; 0 is chosen so large as to guarantee, with a high probability, a sufficient
</p>
<p>quality level for the whole production (assuming, say, that the ξk are identically dis-
</p>
<p>tributed). It is clear that ν and ν&lowast; both satisfy the definition of a Markov or stopping
time.
</p>
<p>Consider the sum Sν = ξ1 + &middot; &middot; &middot; + ξν of a random number of random variables.
This sum is also called a stopped sum in the case when ν is a stopping time.
</p>
<p>Theorem 4.4.1 (Kolmogorov&ndash;Prokhorov) Let an integer-valued random variable ν
be independent of the future. If
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(ν &ge; k)E|ξk|&lt;&infin; (4.4.1)
</p>
<p>then
</p>
<p>ESν =
&infin;&sum;
</p>
<p>k=1
P(ν &ge; k)Eξk. (4.4.2)
</p>
<p>If ξk &ge; 0 then condition (4.4.1) is superfluous.
</p>
<p>Proof The summand ξk is present in the sum Sν if and only if the event {ν &ge; k}
occurs. Thus the following representation holds for the sum Sν :
</p>
<p>Sν =
&infin;&sum;
</p>
<p>k=1
ξkI(ν &ge; k),
</p>
<p>where I(B) is the indicator of the event B . Put Sν,n :=
&sum;n
</p>
<p>k=1 ξkI(ν &ge; k). If ξk &ge; n
then Sν,n &uarr; Sν for each ω as n &rarr; &infin;, and hence, by the monotone convergence
theorem (see Theorem A3.3.1 in Appendix 3),
</p>
<p>ESν = lim
n&rarr;&infin;
</p>
<p>ESν,n = lim
n&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>k=1
EξkI(ν &ge; k).
</p>
<p>But the event {ν &ge; k} complements the event {ν &le; k &minus; 1} and therefore does not
depend on σ(ξk, ξk+1, . . .) and, in particular, on σ(ξk). Hence, putting ak := Eξk we
get EξkI(ν &ge; k)= akP(ν &ge; k), and
</p>
<p>ESν = lim
k&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>k=1
akP(νk &ge; k)=
</p>
<p>&infin;&sum;
</p>
<p>k=1
akP(ν &ge; k). (4.4.3)
</p>
<p>This proves (4.4.2) for ξk &ge; 0.
Now assume ξk can take values of both signs. Put
</p>
<p>ξ&lowast;k := |ξk|, a&lowast;k := Eξ&lowast;k , Zn :=
n&sum;
</p>
<p>k=1
ξ&lowast;k , Zν,n :=
</p>
<p>n&sum;
</p>
<p>k=1
ξ&lowast;k I(ν &ge; k).</p>
<p/>
</div>
<div class="page"><p/>
<p>78 4 Numerical Characteristics of Random Variables
</p>
<p>Applying (4.4.3), we obtain by virtue of (4.4.1) that
</p>
<p>EZν =
&infin;&sum;
</p>
<p>k=1
a&lowast;kP(ν &ge; k) &lt;&infin;.
</p>
<p>Since |Sν,n| &le; Zν,n &le; Zν , by the monotone convergence theorem (see Corol-
lary 6.1.3 or (the Fatou&ndash;Lebesgue) Theorem A3.3.2 in Appendix 3) we have
</p>
<p>ESν = lim
n&rarr;&infin;
</p>
<p>ESν,n =
&sum;
</p>
<p>akP(ν &ge; k),
</p>
<p>where the series on the right-hand side absolutely converges by virtue of (4.4.1).
</p>
<p>The theorem is proved. �
</p>
<p>Put
</p>
<p>a&lowast; := maxak, a&lowast; := minak,
where, as above, ak = Eξk .
</p>
<p>Theorem 4.4.2 Let supk E|ξk| &lt; &infin; and ν be a random variable which does not
depend on the future. Then the following assertions hold true.
</p>
<p>(a) If Eν &lt;&infin; (or EZν &lt;&infin;, where Zn =
&sum;n
</p>
<p>k=1 |ξk|) then ESν exists and
a&lowast;Eν &le; ESν &le; a&lowast;Eν. (4.4.4)
</p>
<p>(b) If ESν is well defined (and may be &plusmn;&infin;), a&lowast; &gt; 0 and, for any N &ge; 1,
E(SN &minus; a&lowast;N;ν &gt; N)&le; c,
</p>
<p>where c does not depend on N , then (4.4.4) holds true.
(c) If ξk &ge; 0 then (4.4.4) is always valid.
</p>
<p>If Sν &ge; const a.s. then condition (c) clearly implies (b).
</p>
<p>The case a&lowast; &lt; 0 in assertions (b)&ndash;(c) can be treated in exactly the same way.
If ν does not depend on {ξk}, a&lowast; = a&lowast; = a &gt; 0, then E(SN ;ν &gt; N)= aNP(ν &gt;
</p>
<p>N) and the condition in (b) holds. But the assumption a&lowast; = a&lowast; is inessential here,
and, for independent ν and {ξ}, (4.4.4) is always true, since in this case
</p>
<p>ESν =
&sum;
</p>
<p>P(ν = k)ESk &le; a&lowast;
&sum;
</p>
<p>kP(ν = k)= a&lowast;Eν.
</p>
<p>The reverse inequality ESν &ge; a&lowast;Eν is verified in the same way.
</p>
<p>Proof of Theorem 4.4.2
(a) First note that
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(ν &ge; k)=
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>&infin;&sum;
</p>
<p>i=k
P(ν = i)=
</p>
<p>&infin;&sum;
</p>
<p>i=1
iP(ν = i)= Eν.
</p>
<p>Note also that, for E|ξk| &le; c &lt;&infin;, the condition Eν &lt;&infin; (or EZν &lt;&infin;) turns into
condition (4.4.1), and assertion (4.4.4) follows from (4.4.2). Therefore, if Eν &lt;&infin;</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Expectations of Sums of a Random Number of Random Variables 79
</p>
<p>then Theorem 4.4.2 is a direct consequence of Theorem 4.4.1. The same is true in
</p>
<p>case (d).
</p>
<p>Consider now assertions (b) and (c).
</p>
<p>For a fixed N &gt; 0, introduce the random variable
</p>
<p>νN := min(ν,N),
which, together with ν, does not depend on the future. Indeed, if n &le; N then the
event {νN &le; n} = {ν &le; n} does not depend on Fn+1,&infin;. If n &gt; N then the event
{νN &le;N} is certain and hence it too does not depend on Fn+1,&infin;.
</p>
<p>(b) If Eν &lt;&infin; then (4.4.4) is proved. Now let Eν =&infin;. We have to prove that
ESν =&infin;. Since EνN &lt;&infin;, the relations
ESνN = E(Sν;ν &le;N)+E(SN ;ν &gt; N)&ge; a&lowast;
</p>
<p>(
E(ν;ν &le;N)+NP(ν &gt; N)
</p>
<p>)
(4.4.5)
</p>
<p>are valid by (a). Together with the conditions in (b) this implies that
</p>
<p>E(Sν;ν &le;N)&ge; a&lowast;E(ν;ν &le;N)&minus; c&rarr;&infin;
as N &rarr;&infin;. Since Sν is well defined, we have
</p>
<p>E(Sν;ν &le;N)&rarr; ESν
as N &rarr;&infin; (see Corollary A3.2.1 in Appendix 3). Therefore necessarily ESν =&infin;.
</p>
<p>(c) Here it is again sufficient to show that ESν =&infin; in the case when Eν =&infin;. It
follows from (4.4.5) that
</p>
<p>ESν = E(Sν;ν &le;N)+E(Sν;ν &gt; N)
&ge; E
</p>
<p>[
Sν &minus; (SN &minus; a&lowast;N);ν &gt; N
</p>
<p>]
+ a&lowast;E(ν;ν &le;N)&ge; a&lowast;E(ν;ν &le;N)&minus; c&rarr;&infin;
</p>
<p>as N &rarr;&infin;, and thus ESν =&infin;.
The theorem is proved. �
</p>
<p>Theorem 4.4.2 implies the following famous result.
</p>
<p>Theorem 4.4.3 (Wald&rsquo;s identity) Assume a = Eξk does not depend on k,
supk E|ξk|&lt;&infin;, and a random variable ν is independent of the future. Then, under
at least one of the conditions (a)&ndash;(d) of Theorem 4.4.2 (with a&lowast; replaced by a),
</p>
<p>ESν = aEν. (4.4.6)
</p>
<p>If a = 0 and Eν = &infin; then identity (4.4.6) can hold, since there would be an
ambiguity of type 0 &middot;&infin; on the right-hand side of (4.4.6).
</p>
<p>Remark 4.4.1 If there is no independence of the future then equality (4.4.6) is, gen-
erally speaking, untrue. Let, for instance, a = Eξk &lt; 0, θ := min{k : Sk = S} and
S := supk Sk (see Example 4.4.1; see Chaps. 10&ndash;12 for conditions of finiteness of
ES and Eθ ). Then Sθ = S &gt; 0 and ES &gt; 0, while aEθ &lt; 0. Hence, (4.4.6) cannot
hold true for ν = θ .
</p>
<p>We saw that if there is no assumption on the finiteness of Eν then, even in the case
</p>
<p>a &gt; 0, in order for (4.4.6) to hold, additional conditions are needed, e.g., conditions</p>
<p/>
</div>
<div class="page"><p/>
<p>80 4 Numerical Characteristics of Random Variables
</p>
<p>(b)&ndash;(d). Without these conditions identity (4.4.6) is, generally speaking, not valid,
</p>
<p>as shown by the following example.
</p>
<p>Example 4.4.2 Let the random variables ζk be independent and identically dis-
tributed, and
</p>
<p>Eζk = 0, Eζ 2k = 1, E|ζk|3 = &micro;&lt;&infin;,
ξk := 1 +
</p>
<p>&radic;
2kζk, ν := min{k : Sk &lt; 0}.
</p>
<p>We will show below in Example 20.2.1 that ν is a proper random variable, i.e.
</p>
<p>P(ν &lt;&infin;)= P
( &infin;⋃
</p>
<p>n=1
{Sn &lt; 0}
</p>
<p>)
= 1.
</p>
<p>It is also clear that ν is a Markov time independent of the future and Eξk = a = 1.
But one has ESν &lt; 0, while aEν &gt; 0, and hence equality (4.4.6) cannot be valid.
</p>
<p>(Here necessarily Eν = &infin;, since otherwise condition (a) would be satisfied and
(4.4.6) would hold.)
</p>
<p>However, if the ξk are independent and identically distributed and ν is a stop-
ping time then statement (4.4.6) is always valid whenever its right-hand side is well
defined. We will show this below in Theorem 11.3.2 by virtue of the laws of large
</p>
<p>numbers.
</p>
<p>Conditions (b) and (c) in Theorem 4.4.2 were used in the case Eν =&infin;. However,
in some problems these conditions can be used to prove the finiteness of Eν. The
</p>
<p>following example confirms this observation.
</p>
<p>Example 4.4.3 Let ξ1, ξ2, . . . be independent and identically distributed and a =
Eξ1 &gt; 0. For a fixed t &ge; 0, consider, as a stopping time (and a variable independent
of the future), the random variable
</p>
<p>ν = η(t) := min{k : Sk &ge; t}.
Clearly, SN &lt; t on the set η(t) &gt; N and Sη(t) &ge; t . Therefore conditions (b) and (c)
are satisfied, and hence
</p>
<p>ESη(t) = aEη(t).
We now show that Eη(t) &lt;&infin;. In order to do this, we consider the &ldquo;trimmed&rdquo;
</p>
<p>random variables ξ
(N)
k := min(N, ξk) and choose N large enough for the inequality
</p>
<p>a(N) := Eξ (N) &gt; 0 to hold true. Let S(N)K and η(N)(t) be defined similarly to Sk and
η(t), but for the sequence {ξ (N)j }. Then evidently S
</p>
<p>(N)
</p>
<p>η(N)(t)
&le; t +N , η(t)&le; η(N)(t),
</p>
<p>a(N)Eη(N)(t)&le; t +N, Eη(t)&le; t +N
a(N)
</p>
<p>&lt;&infin;.
</p>
<p>If a = 0 then Eη(t)=&infin;. This can be seen from the fair game example (ξk =&plusmn;1
with probability 1/2; see Example 4.2.3). In the general case, this will be shown be-
</p>
<p>low in Chap. 12. As was noted above, in this case the right-hand side of (4.4.6) turns</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Expectations of Sums of a Random Number of Random Variables 81
</p>
<p>into the indeterminacy 0 &middot;&infin;, but the left-hand side may equal any finite number, as
in the case of the fair game where Sη(t) = t .
</p>
<p>If we take ν to be the Markov time
</p>
<p>ν = &micro;(t) := min
{
k : |Sk| &ge; t
</p>
<p>}
,
</p>
<p>where ξk may assume values of both signs, then, to prove (4.4.6), it is apparently
</p>
<p>easier to verify the condition of assertion (a) in Theorem 4.4.2. Let us show that
</p>
<p>E&micro;(t) &lt;&infin;. It is clear that, for any given t &gt; 0, there exists an N such that
</p>
<p>q := min
[
P(SN &gt; 2t),P(SN &lt;&minus;2t)
</p>
<p>]
&gt; 0.
</p>
<p>(N = 1 if the ξk are bounded from below.) For such N ,
</p>
<p>inf
|v|&le;t
</p>
<p>P
(
|v+ SN |&gt; t
</p>
<p>)
&gt; 2q.
</p>
<p>Hence, in each N steps, the random walk {Sk} has a chance to leave the strip |v| &le; t
with probability greater than 2q , whatever point v, |v| &le; t , it starts from. Therefore,
</p>
<p>P
(
&micro;(t) &gt; kN
</p>
<p>)
= P
</p>
<p>(
max
j&le;kN
</p>
<p>|Sj |&lt; t
)
&lt; P
</p>
<p>(
k⋂
</p>
<p>j=1
</p>
<p>{
|SjN |&lt; t
</p>
<p>}
)
&lt; (1 &minus; 2q)k.
</p>
<p>This implies that P(&micro;(t) &gt; kN) decreases exponentially as k grows and that E&micro;(t)
</p>
<p>is finite.
</p>
<p>Example 4.4.4 A chain reaction scheme. Suppose we have a single initial particle
which either disappears with probability q or turns into m similar particles with
</p>
<p>probability p = 1 &minus; q . Each particle from the new generation behaves in the same
way independently of the fortunes of the other particles. What is the expectation of
</p>
<p>the number ζn of particles in the n-th generation?
</p>
<p>Consider the &ldquo;double sequence&rdquo; {ξ (n)k }&infin;k=1,&infin;n=1 of independent identically dis-
tributed random variables assuming the values m and 0 with probabilities p and q ,
</p>
<p>respectively. The sequences {ξ (1)k }&infin;k=1, {ξ
(2)
k }&infin;k=1, . . . will clearly be mutually inde-
</p>
<p>pendent. Using these sequences, one can represent the variables ζn (ζ0 = 1) as
</p>
<p>ζ1 = ξ (1)ζ0 = ξ
(1)
1 ,
</p>
<p>ζ2 = ξ (2)1 + &middot; &middot; &middot; + ξ
(2)
ζ1
</p>
<p>&middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot;
ζn = ξ (n)1 + &middot; &middot; &middot; + ξ
</p>
<p>(n)
ζn&minus;1
</p>
<p>,
</p>
<p>where the number of summands in the equation for ζn is ζn&minus;1, the number of &ldquo;parent
particles&rdquo;. Since the sequence {ξ (n)k } is independent of ζn&minus;1,ζ
</p>
<p>(n)
k &ge; 0, and Eξ
</p>
<p>(n)
k =
</p>
<p>pm, by virtue of Wald&rsquo;s identity we have
</p>
<p>Eζn = Eξ (n)1 Eζn&minus;1 = pmEζn&minus;1 = (pm)
n.</p>
<p/>
</div>
<div class="page"><p/>
<p>82 4 Numerical Characteristics of Random Variables
</p>
<p>Example 4.4.5 We return to the fair game of two gamblers described in Exam-
ple 4.2.3, but now assume that the respective capitals z1 &gt; 0 and z2 &gt; 0 of the
</p>
<p>gamblers are finite. Introduce random variables ξk representing the gains of the first
</p>
<p>gambler in the respective (k-th) play. The variables ξk are obviously independent,
</p>
<p>and
</p>
<p>ξk =
{
</p>
<p>1 with probability 1/2,
</p>
<p>&minus;1 with probability 1/2.
</p>
<p>The quantity z1 + Sk = z1 +
&sum;k
</p>
<p>j=1 ξj will be the capital of the first gambler and
z2 &minus; Sk the capital of the second gambler after k plays. The quantity
</p>
<p>η := min{k : z1 + Sk = 0 or z2 &minus; Sk = 0}
is the time until the end of the game, i.e. until the ruin of one of the gamblers. The
</p>
<p>question is what is the probability Pi that the i-th gambler wins (for i = 1,2)?
Clearly, η is a Markov time, Sη = &minus;z1 with probability P2 and Sη = z2 with
</p>
<p>probability P1 = 1 &minus; P2. Therefore,
ESη = P1z2 &minus; P2z1.
</p>
<p>If Eη &lt;&infin;, then by Wald&rsquo;s identity we have
P1z2 &minus; P2z1 = EηEξ1 = 0.
</p>
<p>From this we find that Pi = zi/(z1 + z2).
It remains to verify that Eη is finite. Let, for the sake of simplicity, z1 + z2 =
</p>
<p>2z be even. With probability 2&minus;min(z1,z2) &ge; 2&minus;z, the game can be completed in
min(z1, z2) &le; z plays. Since the total capital of both players remains unchanged
during the game,
</p>
<p>P(η &gt; z)&le; 1 &minus; 2&minus;z, . . . , P(η &gt; Nz)&le;
(
1 &minus; 2&minus;z
</p>
<p>)N
.
</p>
<p>This evidently implies the finiteness of
</p>
<p>Eη=
&infin;&sum;
</p>
<p>k=0
P(η &gt; k).
</p>
<p>We will now give a less trivial example of a random variable ν which is indepen-
</p>
<p>dent of the future, but is not a stopping time.
</p>
<p>Example 4.4.6 Consider two mutually independent sequences of independent posi-
tive random variables ξ1, ξ2, . . . and ζ1, ζ2, . . ., such that ξj &sub;=F and ζj &sub;=G. Further,
consider a system consisting of two devices. After starting the system, the first de-
</p>
<p>vice operates for a random time ξ1 after which it breaks down. Then the second
</p>
<p>device replaces the first one and works for ξ2 time units (over the time interval
</p>
<p>(ξ1, ξ1 + ξ2)). Immediately after the first device&rsquo;s breakdown, one starts repairing it,
and the repair time is ζ2. If ζ2 &gt; ξ2, then at the time ξ1 + ξ2 of the second device&rsquo;s
failure both devices are faulty and the system fails. If ζ2 &le; ξ2, then at the time ξ1+ξ2
the first device starts working again and works for ξ3 time units, while the second</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Variance 83
</p>
<p>device will be under repair for ζ3 time units. If ζ3 &gt; ξ3, the system fails. If ζ3 &le; ξ3,
the second device will start working, etc. What is the expectation of the failure-free
</p>
<p>operation time τ of the system?
</p>
<p>Let ν := min{k &ge; 2 : ζk &gt; ξk}. Then clearly τ = ξ1 + &middot; &middot; &middot; + ξν , where the ξj
are independent and identically distributed and {ν &le; n} &isin; σ(ξ1, . . . , ξn; ζ1, . . . , ζν).
This means that ν is independent of the future. At the same time, if ζj 
= const, then
{ν &le; n} /&isin; F1,n = σ(ξ1, . . . , ξn) and ν is not a Markov time with respect to F1,n.
Since ξk &ge; 0, by Wald&rsquo;s identity Eτ = EνEξ1. Since
</p>
<p>{ν = k} =
k&minus;1⋂
</p>
<p>j=2
{ηj &le; ζj } &cap; {ηk &gt; ζk}, k &ge; 2,
</p>
<p>one has P(ν = k)= qk&minus;2(1 &minus; q), k &ge; 2, where
</p>
<p>q = P(ηk &le; ζk)=
&int;
</p>
<p>dF(t)G(t + 0).
</p>
<p>Consequently,
</p>
<p>Eν =
&infin;&sum;
</p>
<p>k=2
kqk&minus;2(1 &minus; q)= 1 +
</p>
<p>&infin;&sum;
</p>
<p>k=1
kqk&minus;1(1 &minus; q)= 1 + 1
</p>
<p>1 &minus; q ,
</p>
<p>Eτ = Eξ1
2 &minus; q
1 &minus; q .
</p>
<p>Wald&rsquo;s identity has a number of extensions (we will discuss these in more detail
</p>
<p>in Sects. 10.3 and 15.2).
</p>
<p>4.5 Variance
</p>
<p>We introduce one more numerical characteristic for random variables.
</p>
<p>Definition 4.5.1 The variance Var(ξ) of a random variable ξ is the quantity
</p>
<p>Var(ξ) := E(ξ &minus;Eξ)2.
</p>
<p>It is a measure of the &ldquo;dispersion&rdquo; or &ldquo;spread&rdquo; of the distribution of ξ . The vari-
</p>
<p>ance is equal to the inertia moment of the distribution of unit mass along the line.
</p>
<p>We have
</p>
<p>Var(ξ)= Eξ2 &minus; 2EξEξ + (Eξ)2 = Eξ2 &minus; (Eξ)2. (4.5.1)
The variance could also be defined as mina E(ξ &minus; a)2. Indeed, by that definition
</p>
<p>Var(ξ)= Eξ2 + min
a
</p>
<p>(
a2 &minus; 2aEξ
</p>
<p>)
= Eξ2 &minus; (Eξ)2,
</p>
<p>since the minimum of a2 &minus;2aEξ is attained at the point a = Eξ . This remark shows
that the quantity a = Eξ is the best mean square estimate (approximation) of the
random variable ξ .
</p>
<p>The quantity
&radic;
</p>
<p>Var(ξ) is called the standard deviation of ξ .</p>
<p/>
</div>
<div class="page"><p/>
<p>84 4 Numerical Characteristics of Random Variables
</p>
<p>Example 4.5.1 Let ξ &sub;=�a,σ 2 . As we saw in Example 4.1.2, a = Eξ . Therefore,
</p>
<p>Var(ξ)=
&int;
(x &minus; a)2 1
</p>
<p>σ
&radic;
</p>
<p>2π
e&minus;(x&minus;a)
</p>
<p>2/2σ 2 dx = σ
2
</p>
<p>&radic;
2π
</p>
<p>&int;
t2e&minus;t
</p>
<p>2/2 dt.
</p>
<p>The last equality was obtained by the variable change (x&minus; a)/σ = t . Integrating by
parts, one gets
</p>
<p>Var(ξ)=&minus; σ
2
</p>
<p>&radic;
2π
</p>
<p>te&minus;t
2/2
</p>
<p>∣∣∣∣
&infin;
</p>
<p>&minus;&infin;
+ σ
</p>
<p>2
</p>
<p>&radic;
2π
</p>
<p>&int;
e&minus;t
</p>
<p>2/2 dt = σ 2.
</p>
<p>Example 4.5.2 Let ξ&sub;=�&micro;. In Example 4.1.3 we computed the expectation Eξ = &micro;.
Hence
</p>
<p>Var(ξ)= Eξ2 &minus; (Eξ)2 =
&infin;&sum;
</p>
<p>k=0
k2
</p>
<p>&micro;ke&minus;&micro;
</p>
<p>k! &minus;&micro;
2
</p>
<p>=
&infin;&sum;
</p>
<p>k=2
</p>
<p>k(k &minus; 1)&micro;k
k! e
</p>
<p>&minus;&micro; +
&infin;&sum;
</p>
<p>k=0
</p>
<p>k&micro;k
</p>
<p>k! e
&minus;&micro; &minus;&micro;2 = &micro;2 +&micro;&minus;&micro;2 = &micro;.
</p>
<p>Example 4.5.3 For ξ &sub;=U0,1, one has
</p>
<p>Eξ2 =
&int; 1
</p>
<p>0
</p>
<p>x2 dx = 1
3
, Eξ = 1
</p>
<p>2
.
</p>
<p>By (4.5.1) we obtain Var(ξ)= 1
12
</p>
<p>.
</p>
<p>Example 4.5.4 For ξ &sub;=Bp , by virtue of the relations ξ2 = ξ and Eξ2 = Eξ = p we
obtain Var(ξ)= p&minus; p2 = p(1 &minus; p).
</p>
<p>Consider now some properties of the variance.
</p>
<p>D1. Var(ξ)&ge; 0, with Var(ξ)= 0 if and only if P(ξ = c)= 1, where c is a constant
(not depending on ω).
</p>
<p>The first assertion is obvious, for Var(ξ) = E(ξ &minus; Eξ)2 &ge; 0. Let
P(ξ = c)= 1, then (Eξ)2 = Eξ2 = c2 and hence
</p>
<p>Var(ξ)= c2 &minus; c2 = 0.
</p>
<p>If Var(ξ)= E(ξ &minus;Eξ)2 = 0 then (since (ξ &minus;Eξ)2 &ge; 0) P(ξ &minus;Eξ = 0)= 1, or
P(ξ = Eξ)= 1 (see property E4).
</p>
<p>D2. If a and b are constants then
</p>
<p>Var(a + bξ)= b2 Var(ξ).
</p>
<p>This property follows immediately from the definition of Var(ξ).
</p>
<p>D3. If random variables ξ and η are independent then
</p>
<p>Var(ξ + η)= Var(ξ)+ Var(η).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 The Correlation Coefficient and Other Numerical Characteristics 85
</p>
<p>Indeed,
</p>
<p>Var(ξ + η)= E(ξ + η)2 &minus; (Eξ +Eη)2
</p>
<p>= Eξ2 + 2EξEη+Eη2 &minus; (Eξ)2 &minus; (Eη)2 &minus; 2EξEη
= Eξ2 &minus; (Eξ)2 +Eη2 &minus; (Eη)2 = Var(ξ)+ Var(η).
</p>
<p>It is seen from the computations that the variance will be additive not only for inde-
</p>
<p>pendent ξ and η, but also whenever
</p>
<p>Eξη= EξEη.
</p>
<p>Example 4.5.5 Let ν &ge; 0 be an integer-valued random variable independent of a
sequence {ξj } of independent identically distributed random variables, Eν &lt;&infin; and
Eξj = a. Then, as we know, ESν = aEν. What is the variance of Sν?
</p>
<p>By the total probability formula,
</p>
<p>Var(Sν)= E(Sν &minus;ESν)2 =
&sum;
</p>
<p>P(ν = k)E(Sk &minus;ESν)2
</p>
<p>=
&sum;
</p>
<p>P(ν = k)
[
E(Sk &minus; ak)2 + (ak &minus; aEν)2
</p>
<p>]
</p>
<p>=
&sum;
</p>
<p>P(ν = k)kVar(ξ1)+ a2E(ν &minus;Eν)2 = Var(ξ1)Eν + a2 Var(ν).
</p>
<p>This equality is equivalent to the relation
</p>
<p>E(Sν &minus; νa)2 = Eν &middot; Var(ξ1).
In this form, the relation remains valid for any stopping time ν (see Chap. 15).
</p>
<p>Making use of it, one can find in Example 4.4.5 the expectation of the time η until
</p>
<p>the end of the fair game, when the initial capitals z1 and z2 of the players are finite.
</p>
<p>Indeed, in that case a = 0, Var(ξ1)= 1 and
ES2η = Var(ξ1) Eη= z21P2 + z22P1.
</p>
<p>We find from this that Eη= z1z2.
</p>
<p>4.6 The Correlation Coefficient and Other Numerical
</p>
<p>Characteristics
</p>
<p>Two random variables ξ and η could be functionally (deterministically) dependent:
</p>
<p>ξ = g(η); they could be dependent, but not in a deterministic way; finally, they could
be independent. The correlation coefficient of random variables is a quantity which
</p>
<p>can be used to quantify the degree of dependence of the variables on each other.
</p>
<p>All the random variables to appear in the present section are assumed to have
</p>
<p>finite non-zero variances.
</p>
<p>A random variable ξ is said to be standardised if Eξ = 0 and Var(ξ) = 1. Any
random variable ξ can be reduced by a linear transformation to a standardised one
</p>
<p>by putting ξ1 := (ξ &minus; Eξ)/
&radic;
</p>
<p>Var(ξ). Let ξ and η be two random variables and ξ1
and η1 the respective standardised random variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>86 4 Numerical Characteristics of Random Variables
</p>
<p>Definition 4.6.1 The correlation coefficient of the random variables ξ and η is the
quantity ρ(ξ, η)= Eξ1η1.
</p>
<p>Properties of the correlation coefficient.
</p>
<p>1. |ρ(ξ, η)| &le; 1.
</p>
<p>Proof Indeed,
</p>
<p>0 &le; Var(ξ1 &plusmn; η1)= E(ξ1 &plusmn; η1)2 = 2 &plusmn; 2ρ(ξ, η).
It follows that |ρ(ξ, η)| &le; 1. �
</p>
<p>2. If ξ and η are independent then ρ(ξ, η)= 0.
This follows from the fact that ξ1 and η1 are also independent in this case. �
</p>
<p>The converse assertion is not true, of course. In Sect. 4.3 we gave an example of
</p>
<p>dependent random variables ξ and η such that Eξ = 0, Eη = 0 and Eξη = 0. The
correlation coefficient of these variables is equal to 0, yet they are dependent. How-
</p>
<p>ever, as we will see in Chap. 7, for a normally distributed vector (ξ, η) the equality
</p>
<p>ρ(ξ, η)= 0 is necessary and sufficient for the independence of its components.
Another example where the non-correlation of random variables implies their
</p>
<p>independence is given by the Bernoulli scheme. Let P(ξ = 1) = p, P(ξ = 0) =
1 &minus; p, P(η= 1)= q and P(η= 0)= 1 &minus; q . Then
</p>
<p>Eξ = p, Eη= p, Var(ξ)= p(1 &minus; p), Var(η)= q(1 &minus; q),
</p>
<p>ρ(ξ, η)= E(ξ &minus; p)(η&minus; q)&radic;
pq(1 &minus; p)(1 &minus; q)
</p>
<p>.
</p>
<p>The equality ρ(ξ, η)= 0 means that Eξη= pq , or, which is the same,
</p>
<p>P(ξ = 1, η= 1)= P(ξ = 1)P(η= 1),
P(ξ = 1, η= 0)= P(ξ = 1)&minus; P(ξ = 1, η= 1)= p&minus; pq = P(ξ = 1)P(η= 0),
</p>
<p>and so on.
</p>
<p>One can easily obtain from this that, in the general case, ξ and η are independent
</p>
<p>if
</p>
<p>ρ
(
f (ξ), g(η)
</p>
<p>)
= 0
</p>
<p>for any bounded measurable functions f and g. It suffices to take f = I(&minus;&infin;,x),
g = I(&minus;&infin;,y), then derive that P(ξ &lt; x,η &lt; y) = P(ξ &lt; x)P(η &lt; y), and make use
of the results of the previous chapter.
</p>
<p>3. |ρ(ξ, η)| = 1 if and only if there exist numbers a and b 
= 0 such that P(η =
a + bξ)= 1.
</p>
<p>Proof Let P(η= a + bξ)= 1. Set Eξ = α and
&radic;
</p>
<p>Var(ξ)= σ ; then
</p>
<p>ρ(ξ, η)= Eξ &minus; α
σ
</p>
<p>&middot; a + bξ &minus; a &minus; bα|b|σ = signb.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Inequalities 87
</p>
<p>Assume now that |ρ(ξ, η)| = 1. Let, for instance, ρ(ξ, η)= 1. Then
Var(ξ1 &minus; η1)= 2
</p>
<p>(
1 &minus; ρ(ξ, η)
</p>
<p>)
= 0.
</p>
<p>By property D1 of the variance, this can be the case if and only if
</p>
<p>P(ξ1 &minus; η1 = c)= 1.
If ρ(ξ, η)=&minus;1 then we get Var(ξ1 + η1)= 0, and hence
</p>
<p>P(ξ1 + η1 = c)= 1. �
</p>
<p>If ρ &gt; 0 then the random variables ξ and η are said to be positively correlated; if
ρ &lt; 0 then ξ and η are said to be negatively correlated.
</p>
<p>Example 4.6.1 Consider a transmitting device. A random variable ξ denotes the
magnitude of the transmitted signal. Because of interference, a receiver gets the
</p>
<p>variable η= αξ +∆ (α is the amplification coefficient, ∆ is the noise). Assume that
the random variables ∆ and ξ are independent. Let Eξ = a, Var(ξ) = 1, E∆ = 0
and Var(∆)= σ 2. Compute the correlation coefficient of ξ and η:
</p>
<p>ρ(ξ, η)= E
(
(ξ &minus; a)αξ +∆&minus; aα&radic;
</p>
<p>α2 + σ 2
</p>
<p>)
= α&radic;
</p>
<p>α2 + σ 2
.
</p>
<p>If σ is a large number compared to the amplification α, then ρ is close to 0 and η
</p>
<p>essentially does not depend on ξ . If σ is small compared to α, then ρ is close to 1,
</p>
<p>and one can easily reconstruct ξ from η.
</p>
<p>We consider some further numerical characteristics of random variables. One
</p>
<p>often uses the so-called higher order moments.
</p>
<p>Definition 4.6.2 The k-th order moment of a random variable ξ is the quantity Eξ k .
The quantity E(ξ &minus;Eξ)k is called the k-th order central moment, so the variance is
the second central moment of ξ .
</p>
<p>Given a random vector (ξ1, . . . , ξn), the quantity Eξ
k1
1 &middot; &middot; &middot; ξ
</p>
<p>kn
n is called the mixed
</p>
<p>moment of order k1 + &middot; &middot; &middot; + kn. Similarly, E(ξ1 &minus; Eξ1)k1 &middot; &middot; &middot; (ξn &minus; Eξn)kn is said to
be the central mixed moment of the same order.
</p>
<p>For independent random variables, mixed moments are evidently equal to the
</p>
<p>products of the respective usual moments.
</p>
<p>4.7 Inequalities
</p>
<p>4.7.1 Moment Inequalities
</p>
<p>Theorem 4.7.1 (Cauchy&ndash;Bunjakovsky&rsquo;s inequality) If ξ1 and ξ2 are arbitrary ran-
dom variables, then
</p>
<p>E|ξ1ξ2| &le;
[
Eξ21Eξ
</p>
<p>2
2
</p>
<p>]1/2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>88 4 Numerical Characteristics of Random Variables
</p>
<p>This inequality is also sometimes called the Schwarz inequality.
</p>
<p>Proof The required relation follows from the inequality 2|ab| &le; a2 + b2 if one puts
a2 = ξ21 /Eξ21 , b2 = ξ22 /Eξ22 and takes the expectations of the both sides. �
</p>
<p>The Cauchy&ndash;Bunjakovsky inequality is a special case of more general inequali-
</p>
<p>ties.
</p>
<p>Theorem 4.7.2 For r &gt; 1, 1
r
+ 1
</p>
<p>s
= 1, one has H&ouml;lder&rsquo;s inequality:
</p>
<p>E|ξ1ξ2| &le;
[
E|ξ1|r
</p>
<p>]1/r[
E|ξ2|s
</p>
<p>]1/s
,
</p>
<p>and Minkowski&rsquo;s inequality:
[
E|ξ1 + ξ2|r
</p>
<p>]1/r &le;
[
E|ξ1|r
</p>
<p>]1/r +
[
E|ξ2|r
</p>
<p>]1/r
.
</p>
<p>Proof Since xr is, for r &gt; 1, a convex function in the domain x &gt; 0, which at the
point x = 1 is equal to 1 and has derivative equal to r , one has r(x &minus; 1) &le; xr &minus; 1
for all x &gt; 0. Putting x = (a/b)1/r (a &gt; 0, b &gt; 0), we obtain
</p>
<p>a1/rb1&minus;1/r &minus; b &le; a
r
&minus; b
</p>
<p>r
,
</p>
<p>or, which is the same, a1/rb1/s &le; a/r + b/r . If one puts
</p>
<p>a := |ξ1|
r
</p>
<p>E|ξ1|r
, b := |ξ2|
</p>
<p>s
</p>
<p>E|ξ2|s
</p>
<p>and takes the expectations, one gets H&ouml;lder&rsquo;s inequality.
</p>
<p>To prove Minkowski&rsquo;s inequality, note that, by the inequality |ξ1 + ξ2| &le; |ξ1| +
|ξ2|, one has
</p>
<p>E|ξ1 + ξ2|r &le; E|ξ1||ξ1 + ξ2|r&minus;1 +E|ξ2||ξ1 + ξ2|r&minus;1.
</p>
<p>Applying H&ouml;lder&rsquo;s inequality to the terms on the right-hand side, we obtain
</p>
<p>E|ξ1 + ξ2|r &le;
{[
E|ξ1|r
</p>
<p>]1/r +
[
E|ξ2|r
</p>
<p>]1/r}[
E|ξ1 + ξ2|(r&minus;1)s
</p>
<p>]1/s
.
</p>
<p>Since (r &minus; 1)s = r , 1 &minus; 1/s = 1/r , and Minkowski&rsquo;s inequality follows. �
</p>
<p>It is obvious that, for r = s = 2, H&ouml;lder&rsquo;s inequality becomes the Schwarz in-
equality.
</p>
<p>Theorem 4.7.3 (Jensen&rsquo;s inequality) If Eξ exists and g(x) is a convex function,
then g(Eξ)&le; Eg(ξ).
</p>
<p>Proof If g(x) is convex then for any y there exists a number g1(y) such that, for
all x,
</p>
<p>g(x)&ge; g(y)+ (x &minus; y)g1(y).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Inequalities 89
</p>
<p>Putting x = ξ , y = Eξ , and taking the expectations of the both sides of this inequal-
ity, we obtain
</p>
<p>Eg(ξ)&ge; g(Eξ). �
</p>
<p>The following corollary is also often useful.
</p>
<p>Corollary 4.7.1 For any 0 &lt; v &lt; u,
[
E|ξ |v
</p>
<p>]1/v &le;
[
E|ξ |u
</p>
<p>]1/u
. (4.7.1)
</p>
<p>This inequality shows, in particular, that if the u-th order moment exists, then the
</p>
<p>moments of any order v &lt; u also exist.
</p>
<p>Inequality (4.7.1) follows from H&ouml;lder&rsquo;s inequality, if one puts ξ1 := |ξ |v ,
ξ2 := 1, r := u/v, or from Jensen&rsquo;s inequality with g(x)= |x|u/v and |ξ |v in place
of ξ .
</p>
<p>4.7.2 Inequalities for Probabilities
</p>
<p>Theorem 4.7.4 Let ξ &ge; 0 with probability 1. Then, for any x &gt; 0,
</p>
<p>P(ξ &ge; x)&le; E(ξ ; ξ &ge; x)
x
</p>
<p>&le; Eξ
x
</p>
<p>. (4.7.2)
</p>
<p>If Eξ &lt;&infin; then P(ξ &ge; x)= o(1/x) as x &rarr;&infin;.
</p>
<p>Proof The inequality is proved by the following relations:
</p>
<p>Eξ &ge; E(ξ ; ξ &ge; x)&ge; xE(1; ξ &ge; x)= xP(ξ &ge; x).
If Eξ &lt;&infin; then E(ξ ; ξ &ge; x)&rarr; 0 as x &rarr;&infin;. This proves the second statement
</p>
<p>of the theorem. �
</p>
<p>If a function g(x) &ge; 0 is monotonically increasing, then clearly {ξ : g(ξ) &ge;
g(ε)} = {ξ : ξ &ge; ε} and, applying Theorem 4.7.4 to the random variable η = g(ξ),
one gets
</p>
<p>Corollary 4.7.2 If g(x) &uarr;, g(x)&ge; 0, then
</p>
<p>P(ξ &ge; x)&le; E(g(ξ); ξ &ge; x)
g(x)
</p>
<p>&le; Eg(ξ)
g(x)
</p>
<p>.
</p>
<p>In particular, for g(x)= eλx ,
P(ξ &ge; x)&le; e&minus;λxEeλξ , λ &gt; 0.
</p>
<p>Corollary 4.7.3 (Chebyshev&rsquo;s inequality) For an arbitrary random variable ξ with
a finite variance,
</p>
<p>P
(
|ξ &minus;Eξ | &ge; x
</p>
<p>)
&le; Var(ξ)
</p>
<p>x2
. (4.7.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>90 4 Numerical Characteristics of Random Variables
</p>
<p>To prove (4.7.3), it suffices to apply Theorem 4.7.4 to the random variable η =
(ξ &minus;Eξ)2 &ge; 0. �
</p>
<p>The assertions of Theorem 4.7.4 and Corollary 4.7.2 are also often called Cheby-
</p>
<p>shev&rsquo;s inequalities (or Chebyshev type inequalities), since in regard to their proofs,
they are unessential generalisations of (4.7.3).
</p>
<p>Using Chebyshev&rsquo;s inequality, we can bound probabilities of various deviations
</p>
<p>of ξ knowing only Eξ and Var(ξ). As one of the first applications of this inequality,
</p>
<p>we will derive the so-called law of large numbers in Chebyshev&rsquo;s form (the law of
large numbers in a more general form will be obtained in Chap. 8).
</p>
<p>Theorem 4.7.5 Let ξ1, ξ2, . . . be independent identically distributed random vari-
ables with expectation Eξj = a and finite variance σ 2 and let Sn =
</p>
<p>&sum;n
j=1 ξj . Then,
</p>
<p>for any ε &gt; 0,
</p>
<p>P
</p>
<p>(∣∣∣∣
Sn
</p>
<p>n
&minus; a
</p>
<p>∣∣∣∣&gt; ε
)
&le; σ
</p>
<p>2
</p>
<p>nε2
&rarr; 0
</p>
<p>as n&rarr;&infin;.
</p>
<p>We will discuss this assertion in Chaps. 5, 6 and 8.
</p>
<p>Proof of Theorem 4.7.5 follows from Chebyshev&rsquo;s inequality, for
</p>
<p>E
Sn
</p>
<p>n
= a, Var
</p>
<p>(
Sn
</p>
<p>n
</p>
<p>)
= nσ
</p>
<p>2
</p>
<p>n2
= σ
</p>
<p>2
</p>
<p>n
. �
</p>
<p>Now we will give a computational example of the use of Chebyshev&rsquo;s inequality.
</p>
<p>Example 4.7.1 Assume we decided to measure the diameter of the lunar disk us-
ing photographs made with a telescope. Due to atmospheric interference, measure-
</p>
<p>ments of pictures made at different times give different results. Let ξ &minus; a denote
the deviation of the result of a measurement from the true value a, Eξ = a and
σ =
</p>
<p>&radic;
Var(ξ)= 1 on a certain scale. Carry out a series of n independent measure-
</p>
<p>ments and put ζn := 1n (ξ1 + &middot; &middot; &middot; + ξn). Then, as we saw, Eζn = a, Var(ζn)= σ 2/n.
Since the variance of the average of the measurements decreases as the number of
</p>
<p>observations increases, it is natural to estimate the quantity a by ζn.
</p>
<p>How many observations should be made to ensure |ζn &minus; a| &le; 0.1 with a proba-
bility greater than 0.99? That is, we must have P(|ζn &minus; a| &le; 0.1) &gt; 0.99, or P(|ζn &minus;
a| &gt; 0.1) &le; 0.01. By Chebyshev&rsquo;s inequality, P(|ζn &minus; a| &gt; 0.1) &le; σ 2/(n &middot; 0.01).
Therefore, if n is chosen so that σ 2/(n &middot; 0.01) &le; 0.01 then the required inequality
will be satisfied. Hence we get n&ge; 104.
</p>
<p>The above example illustrates the possibility of using Chebyshev&rsquo;s inequality to
</p>
<p>bound the probabilities of the deviations of random variables. However, this exam-
</p>
<p>ple is an even better illustration of how crude Chebyshev&rsquo;s inequality is for practical
</p>
<p>purposes. If the reader returns to Example 4.7.1 after meeting with the central limit</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Extension of the Notion of Conditional Expectation 91
</p>
<p>theorem in Chap. 8, he/she will easily calculate that, to achieve the required accu-
</p>
<p>racy, one actually needs to conduct not 104, but only 670 observations.
</p>
<p>4.8 Extension of the Notion of Conditional Expectation
</p>
<p>In conclusion to the present chapter, we will introduce a notion which, along with
</p>
<p>those we have already discussed, is a useful and important tool in probability theory.
</p>
<p>Giving the reader the option to skip this section in the first reading of the book, we
</p>
<p>avoid direct use of this notion until Chaps. 13 and 15.
</p>
<p>4.8.1 Definition of Conditional Expectation
</p>
<p>In Sect. 4.2 we introduced the notion of conditional expectation given an arbitrary
</p>
<p>event B with P(B) &gt; 0 that was defined by the equality
</p>
<p>E(ξ |B) := E(ξ ;B)
P(B)
</p>
<p>, (4.8.1)
</p>
<p>where
</p>
<p>E(ξ ;B)=
&int;
</p>
<p>B
</p>
<p>ξ dP= Eξ IB ,
</p>
<p>IB = IB(ω) being the indicator of the set B . We have already seen and will see many
times in what follows that this is a very useful notion. Definition 4.8.1 introducing
</p>
<p>this notion has, however, the deficiency that it makes no sense when P(B)= 0. How
could one overcome this deficiency?
</p>
<p>The fact that the condition P(B) &gt; 0 should not play any substantial role could be
</p>
<p>illustrated by the following considerations. Assume that ξ and η are independent,
</p>
<p>B = {η = x} and P(B) &gt; 0. Then, for any measurable function ϕ(x, y), one has
according to (4.8.1) that
</p>
<p>E
[
ϕ(ξ, η)
</p>
<p>∣∣η= x
]
= Eϕ(ξ, η)I{η=x}
</p>
<p>P(η= x) =
Eϕ(ξ, x)I{η=x}
</p>
<p>P(η= x) = Eϕ(ξ, x). (4.8.2)
</p>
<p>The last equality holds because the random variables ϕ(ξ, x) and I{η=x} are inde-
pendent, being functions of ξ and η respectively, and consequently
</p>
<p>Eϕ(ξ, η)I{η=x} = Eϕ(ξ, x)P(η= x).
Relations (4.8.2) show that the notion of conditional expectation could also retain
</p>
<p>its meaning in the case when the probability of the condition is 0, for the equality
</p>
<p>E
[
ϕ(ξ, η)
</p>
<p>∣∣η= x
]
= Eϕ(ξ, x)
</p>
<p>itself looks quite natural for independent ξ and η and is by no means related to the
</p>
<p>assumption that P(η= x) &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 4 Numerical Characteristics of Random Variables
</p>
<p>Fig. 4.1 Conditional
</p>
<p>expectation as the projection
</p>
<p>of ξ onto HA
</p>
<p>Let A be a sub-σ -algebra of F. We will now define the notion of the conditional
</p>
<p>expectation of a random variable ξ given A, which will be denoted by E(ξ |A). First
we will give the definition in the &ldquo;discrete&rdquo; case, but in such a way that it can easily
</p>
<p>be extended.
</p>
<p>Recall that we call discrete the case when the σ -algebra A is formed (gener-
</p>
<p>ated) by an at most countable sequence of disjoint events A1,A2, . . . ,
⋃
</p>
<p>j Aj =Ω ,
P(Aj ) &gt; 0. We will write this as A= σ(A1,A2, . . .), which means that the elements
of A are all possible unions of the sets A1,A2, . . . .
</p>
<p>Let L2 be the collection of all random variables (all the measurable func-
</p>
<p>tions ξ(ω) defined on 〈Ω,F,P〉) for which Eξ2 &lt; &infin;. In the linear space L2 one
can introduce the inner product (ξ, η) = E(ξη) (whereby L2 becomes a Hilbert
space with the norm ‖ξ‖ = (Eξ2)1/2; we identify two random variables ξ1 and ξ2 if
‖ξ1 &minus; ξ2‖ = 0, see also Remark 6.1.1).
</p>
<p>Now consider the linear space HA of all functions of the form
</p>
<p>ξ(ω)=
&sum;
</p>
<p>k
</p>
<p>ckIAk (ω),
</p>
<p>where IAk (ω) are indicators of the sets Ak . The space HA is clearly the space of
</p>
<p>all A-measurable functions, and one could think of it as the space spanned by the
</p>
<p>orthogonal system {IAk (ω)} in L2.
We now turn to the definition of conditional expectation. We know that the con-
</p>
<p>ventional expectation a = Eξ of ξ &isin; L2 can be defined as the unique point at which
the minimum value of the function ϕ(a)= E(ξ&minus;a)2 is attained (see Sect. 4.5). Con-
sider now the problem of minimising the functional ϕ(a)= E(ξ &minus; a(ω))2, ξ &isin; L2,
over all A-measurable functions a(ω) from HA.
</p>
<p>Definition 4.8.1 Let ξ &isin; L2. The A-measurable function a(ω) on which the mini-
mum mina&isin;HA ϕ(a) is attained is said to be the conditional expectation of ξ given
A and is denoted by E(ξ |A).
</p>
<p>Thus, unlike the conventional expectations, the conditional expectation E(ξ |A) is
a random variable. Let us consider it in more detail. It is evident that the minimum
of ϕ(a) is attained when a(ω) is the projection ξ̂ of the element ξ in the space L2
onto HA, i.e. the element ξ̂ &isin;HA for which ξ &minus; ξ̂ &perp;HA (see Fig. 4.1). In that case,
for any a &isin;HA,</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Extension of the Notion of Conditional Expectation 93
</p>
<p>ξ̂ &minus; a &isin;HA, ξ &minus; ξ̂ &perp; ξ̂ &minus; a,
ϕ(a)= E(ξ &minus; ξ̂ + ξ̂ &minus; a)2 = E(ξ &minus; ξ̂ )2 +E(̂ξ &minus; a)2,
</p>
<p>ϕ(a)&ge; ϕ(̂ξ),
</p>
<p>and ϕ(a)= ϕ(̂ξ) if a = ξ̂ a.s.
Thus, in L2 the conditional expectation operation is just an orthoprojector onto
</p>
<p>HA (ξ̂ = E(ξ |A) is the projection of ξ onto HA).
Since, for a discrete σ -algebra A, the element ξ̂ , being an element of HA, has the
</p>
<p>form ξ̂ =
&sum;
</p>
<p>ckIAk , the orthogonality condition ξ &minus; ξ̂ &perp;HA (or, which is the same,
E(ξ &minus; ξ̂ ) IAk = 0) determines uniquely the coefficients ck :
</p>
<p>E(ξ IAk )= ckP(Ak), ck =
E(ξ ;Ak)
P(Ak)
</p>
<p>= E(ξ |Ak),
</p>
<p>so that
</p>
<p>E(ξ |A)= ξ̂ =
&sum;
</p>
<p>k
</p>
<p>E(ξ |Ak)IAk .
</p>
<p>Thus the random variable E(ξ |A) is constant on Ak and, on these sets, is equal
to the average value of ξ on Ak .
</p>
<p>If ξ and A are independent (i.e. P(ξ &isin; B; Ak) = P(ξ &isin; B)P(Ak)) then clearly
E(ξ ;Ak)= Eξ P(Ak) and ξ̂ = Eξ . If A= F then F is also discrete, ξ is constant on
the sets Ak and hence ξ̂ = ξ .
</p>
<p>Now note the following basic properties of conditional expectation which allow
</p>
<p>one to get rid of the two special assumptions (that ξ &isin; L2 and A is discrete), which
were introduced at first to gain a better understanding of the nature of conditional
</p>
<p>expectation:
</p>
<p>(1) ξ̂ is A-measurable.
(2) For any event A &isin;A,
</p>
<p>E(̂ξ ;A)= E(ξ ;A).
</p>
<p>The former property is obvious. The latter follows from the fact that any event
</p>
<p>A &isin;A can be represented as A &isin;
⋃
</p>
<p>k Ajk , and hence
</p>
<p>E(̂ξ ;A)=
&sum;
</p>
<p>k
</p>
<p>E(̂ξ ;Ajk )=
&sum;
</p>
<p>k
</p>
<p>cjkP(Ajk )=
&sum;
</p>
<p>k
</p>
<p>E(ξ ;Ajk )= E(ξ ;A).
</p>
<p>The meaning of this property is rather clear: averaging the variable ξ over the set A
</p>
<p>gives the same result as averaging the variable ξ̂ which has already been averaged
</p>
<p>over Ajk .
</p>
<p>Lemma 4.8.1 Properties (1) and (2) uniquely determine the conditional expecta-
tion and are equivalent to Definition 4.8.1.
</p>
<p>Proof In one direction the assertion of the lemma has already been proved. Assume
now that conditions (1) and (2) hold. A-measurability of ξ̂ means that ξ̂ is constant</p>
<p/>
</div>
<div class="page"><p/>
<p>94 4 Numerical Characteristics of Random Variables
</p>
<p>on each set Ak . Denote by ck the value of ξ̂ on Ak . Since Ak &isin; A, it follows from
property (2) that
</p>
<p>E(̂ξ ;Ak)= ckP(Ak)= E(ξ ;Ak),
and hence, for ω &isin;Ak ,
</p>
<p>ξ̂ = ck =
E(ξ ;Ak)
P(Ak)
</p>
<p>.
</p>
<p>The lemma is proved. �
</p>
<p>Now we can give the general definition of conditional expectation.
</p>
<p>Definition 4.8.2 Let ξ be a random variable on a probability space 〈Ω,F,P〉 and
A&sub; F an arbitrary sub-σ -algebra of F. The conditional expectation of ξ given A is
a random variable ξ̂ which is denoted by E(ξ |A) and has the following two proper-
ties:
</p>
<p>(1) ξ̂ is A-measurable.
(2) For any A &isin;A, one has E(ξ̂ ;A)= E(ξ ;A).
</p>
<p>In this definition, the random variable ξ can be both scalar and vector-valued.
</p>
<p>There immediately arises the question of whether such a random variable exists
</p>
<p>and is unique. In the discrete case we saw that the answer to this question is positive.
</p>
<p>In the general case, the following assertion holds true.
</p>
<p>Theorem 4.8.1 If E|ξ | is finite, then the function ξ̂ = E(ξ |A) in Definition 4.8.2
always exists and is unique up to its values on a set of probability 0.
</p>
<p>Proof First assume that ξ is scalar and ξ &ge; 0. Then the set function
</p>
<p>Q(A)=
&int;
</p>
<p>A
</p>
<p>ξ dP= E(ξ ;A), A &isin;A
</p>
<p>will be a measure on 〈Ω,A〉 which is absolutely continuous with respect to P, for
P(A)= 0 implies Q(A)= 0. Therefore, by the Radon&ndash;Nykodim theorem (see Ap-
pendix 3), there exists an A-measurable function ξ̂ = E(ξ |A) which is unique up to
its values on a set of measure zero and such that
</p>
<p>Q(A)=
&int;
</p>
<p>A
</p>
<p>ξ̂ dP= E(̂ξ ;A).
</p>
<p>In the general case we put ξ = ξ+ &minus; ξ&minus;, where ξ+ := max(0, ξ) &ge; 0, ξ&minus; :=
max(0,&minus;ξ) &ge; 0, ξ̂ := ξ̂+ &minus; ξ̂&minus; and ξ̂&plusmn; are conditional expectations of ξ&plusmn;. This
proves the existence of the conditional expectation, since ξ̂ satisfies conditions (1)
</p>
<p>and (2) of Definition 4.8.2. This will also imply uniqueness, for the assumption
</p>
<p>on non-uniqueness of ξ̂ would imply non-uniqueness of ξ̂+ or ξ̂&minus;. The proof for
vector-valued ξ reduces to the one-dimensional case, since the components of ξ̂ will
</p>
<p>possess properties (1) and (2) and, for the components, the existence and uniqueness
</p>
<p>have already been proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Extension of the Notion of Conditional Expectation 95
</p>
<p>The essence of the above proof is quite transparent: by condition (2), for any
</p>
<p>A &isin;A we are given the value
</p>
<p>E(̂ξ ;A)=
&int;
</p>
<p>A
</p>
<p>ξ̂ dP,
</p>
<p>i.e. the values of the integrals of ξ̂ over all sets A &isin;A are given. This clearly should
define an A-measurable function ξ̂ uniquely up to its values on a set of measure
</p>
<p>zero.
</p>
<p>The meaning of E(ξ |A) remains the same: roughly speaking, this is the result of
averaging of ξ over &ldquo;indivisible&rdquo; elements of A.
</p>
<p>If A = F then evidently ξ̂ = ξ satisfies properties (1) and (2) and therefore
E(ξ |F)= ξ .
</p>
<p>Definition 4.8.3 Let ξ and η be random variables on 〈Ω,F,P〉 and A = σ(η) be
the σ -algebra generated by the random variable η. Then E(ξ |A) is also called the
conditional expectation of ξ given η.
</p>
<p>To simplify the notation, we will sometimes write E(ξ |η) instead of E(ξ |σ(η)).
This does not lead to confusion.
</p>
<p>Since E(ξ |η) is, by definition, a σ(η)-measurable random variable, this means
(see Sect. 3.5) that there exists a measurable function g(x) for which E(ξ |η) =
g(η). By analogy with the discrete case, one can interpret the quantity g(x) as the
</p>
<p>result of averaging ξ over the set {η = x}. (Recall that in the discrete case g(x) =
E(ξ |η= x).)
</p>
<p>Definition 4.8.4 If ξ = IC is the indicator of a set C &isin; F, then E(IC |A) is called the
conditional probability P(C|A) of the event C given A. If A = σ(η), we speak of
the conditional probability P(C|η) of the event C given η.
</p>
<p>4.8.2 Properties of Conditional Expectations
</p>
<p>1. Conditional expectations have the properties of conventional expectations, the
only difference being that they hold almost surely (with probability 1):
</p>
<p>(a) E(a + bξ |A)= a + bE(ξ |A).
(b) E(ξ1 + ξ2|A)= E(ξ1|A)+E(ξ2|A).
(c) If ξ1 &le; ξ2 a.s., then E(ξ1|A)&le; E(ξ2|A) a.s.
</p>
<p>To prove, for instance, property (a), one needs to verify, according to Defini-
</p>
<p>tion 4.8.2, that
</p>
<p>(1) a + bE(ξ |A) is an A-measurable function;
(2) E(a + bξ ;A)= E(a + bE(ξ |A);A) for any A &isin;A.</p>
<p/>
</div>
<div class="page"><p/>
<p>96 4 Numerical Characteristics of Random Variables
</p>
<p>Here (1) is evident; (2) follows from the linearity of conventional expectation (or
</p>
<p>integral).
</p>
<p>Property (b) is proved in the same way.
</p>
<p>To prove (c), put, for brevity, ξ̂i := E(ξi |A). Then, for any A &isin;A,&int;
</p>
<p>A
</p>
<p>ξ̂1 dP= E(̂ξ1;A)= E(ξ1;A)&le; E(ξ2;A)=
&int;
</p>
<p>A
</p>
<p>ξ̂2 dP,
</p>
<p>&int;
</p>
<p>A
</p>
<p>(̂ξ2 &minus; ξ̂1) dP&ge; 0.
</p>
<p>This implies that ξ̂2 &minus; ξ̂1 &ge; 0 a.s.
2. Chebyshev&rsquo;s inequality. If ξ &ge; 0, x &gt; 0, then P(ξ &ge; x|A)&le; E(ξ |A)/x.
This property follows from 1(c), since P(ξ &ge; x|A) = E(I{ξ&ge;x}|A), where IA is
</p>
<p>the indicator of the event A, and one has the inequality I{ξ&ge;x} &le; ξ/x.
3. If A and σ(η) are independent, then E(ξ |A) = Eξ . Since ξ̂ = Eξ is an A-
</p>
<p>measurable function, it remains to verify the second condition from Definition 4.8.2:
</p>
<p>for any A &isin;A,
E(ξ̂ ;A)= E(ξ ;A).
</p>
<p>This equality follows from the independence of the random variables IA and ξ and
</p>
<p>the relations E(ξ ;A)= E(ξ IA)= EξEIA = E(̂ξ ;A).
It follows, in particular, that if ξ and η are independent, then E(ξ |η)= Eξ . If the
</p>
<p>σ -algebra A is trivial, then clearly one also has E(ξ |A)= Eξ .
4. Convergence theorems that are true for conventional expectations hold for
</p>
<p>conditional expectations as well. For instance, the following assertion is true.
</p>
<p>Theorem 4.8.2 (Monotone convergence theorem) If 0 &le; ξn &uarr; ξ a.s. then
E(ξn|A) &uarr; E(ξ |A) a.s.
</p>
<p>Indeed, it follows from ξn+1 &ge; ξn a.s. that ξ̂n+1 &ge; ξ̂n a.s., where ξ̂n = E(ξn|A).
Therefore there exists an A-measurable random variable ξ̂ such that ξ̂n &uarr; ξ̂ a.s. By
the conventional monotone convergence theorem, for any A &isin;A,
</p>
<p>&int;
</p>
<p>A
</p>
<p>ξ̂n dP&rarr;
&int;
</p>
<p>A
</p>
<p>ξ̂ dP,
</p>
<p>&int;
</p>
<p>A
</p>
<p>ξn dP&rarr;
&int;
</p>
<p>A
</p>
<p>ξ dP.
</p>
<p>Since the left-hand sides of these relations coincide, the same holds for the right-
</p>
<p>hand sides. This means that ξ̂ = E(ξ |A).
5. If η is an A-measurable scalar random variable, E|ξ |&lt;&infin;, and E|ξη|&lt;&infin;,
</p>
<p>then
</p>
<p>E(ηξ |A)= ηE(ξ |A). (4.8.3)
If ξ &ge; 0 and η &ge; 0 then the moment conditions are superfluous.
</p>
<p>In other words, in regard to the conditional expectation operation, A-measurable
</p>
<p>random variables behave as constants in conventional expectations (cf. prop-
</p>
<p>erty 1(a)).
</p>
<p>In order to prove (4.8.3), note that if η = IB (the indicator of a set B &isin; A) then
the assertion holds since, for any A &isin;A,
&int;
</p>
<p>A
</p>
<p>E(IBξ |A) dP=
&int;
</p>
<p>A
</p>
<p>IBξ dP=
&int;
</p>
<p>AB
</p>
<p>ξ dP=
&int;
</p>
<p>AB
</p>
<p>E(ξ |A) dP=
&int;
</p>
<p>A
</p>
<p>IBE(ξ |A) dP.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Extension of the Notion of Conditional Expectation 97
</p>
<p>This together with the linearity of conditional expectations implies that the assertion
</p>
<p>holds for all simple functions η.
</p>
<p>If ξ &ge; 0 and η &ge; 0 then, taking a sequence of simple functions 0 &le; ηn &uarr; η and
applying the monotone convergence theorem to the equality
</p>
<p>E(ηnξ |A)= ηnE(ξ |A),
we obtain (4.8.3). Transition to the case of arbitrary ξ and η is carried out in the
</p>
<p>standard way&mdash;by considering positive and negative parts of the random variables
</p>
<p>ξ and η. In addition, to ensure that the arising differences and sums make sense, we
</p>
<p>require the existence of the expectations E|ξ | and E|ξη|.
6. All the basic inequalities for conventional expectations remain true for condi-
</p>
<p>tional expectations as well, in particular, Cauchy&ndash;Bunjakovsky&rsquo;s inequality
</p>
<p>E
(
|ξ1ξ2|
</p>
<p>∣∣A
)
&le;
[
E
(
ξ21 |A
</p>
<p>)
E
(
ξ22 |A
</p>
<p>)]1/2
</p>
<p>and Jensen&rsquo;s inequality: if E|ξ |&lt;&infin; then, for any convex function g,
g
(
E(ξ |A)
</p>
<p>)
&le; E
</p>
<p>(
g(ξ)|A
</p>
<p>)
. (4.8.4)
</p>
<p>Cauchy&ndash;Bunjakovsky&rsquo;s inequality can be proved in exactly the same way as for
</p>
<p>conventional expectations, for its proof requires no properties of expectations other
</p>
<p>than linearity.
</p>
<p>Jensen&rsquo;s inequality is a consequence of the following relation. By convexity of
</p>
<p>g(x), for any y, there exists a number g&lowast;(y) such that g(x)&ge; g(y)+ (x &minus; y)g&lowast;(y)
(g&lowast;(y)= g&prime;(y) if g is differentiable at the point y). Put x = ξ , y = ξ̂ = E(ξ |A), and
take conditional expectations of the both sides of the inequality. Then, assuming for
</p>
<p>the moment that
</p>
<p>E
(∣∣(ξ &minus; ξ̂ )g&lowast;(̂ξ )
</p>
<p>∣∣)&lt;&infin; (4.8.5)
(this can be proved if E|g(ξ)|&lt;&infin;), we get
</p>
<p>E
[
(ξ &minus; ξ̂ )g&lowast;(̂ξ )
</p>
<p>∣∣A)
]
= g&lowast;(̂ξ )E(ξ &minus; ξ̂ |A)= 0
</p>
<p>by virtue of property 5. Thus we obtain (4.8.4). In the general case note that the
</p>
<p>function g&lowast;(y) is nondecreasing. Let (y&minus;N , yN ) be the maximal interval on which
|g&lowast;(y)|&lt;N . Put
</p>
<p>gN (y) :=
{
g(y) if y &isin; [y&minus;N , yN ],
g(y&plusmn;N )&plusmn; (y &minus; y&plusmn;N )N if y ≷ y&plusmn;N .
</p>
<p>(y&plusmn;N can take infinite values if &plusmn;g&lowast;(y) are bounded as y &rarr;&infin;. Note that the values
of g&lowast;(y) are always bounded from below as y &rarr;&infin; and from above as y &rarr;&minus;&infin;,
hence g&lowast;(y&plusmn;N )≷ 0 for N large enough.) The support function g&lowast;N (y) corresponding
to gN (y) has the form
</p>
<p>g&lowast;N (y)= max
[
&minus;N,min
</p>
<p>(
N,g&lowast;(y)
</p>
<p>)]
</p>
<p>and, consequently, is bounded for each N . Therefore, condition (4.8.5) is satisfied
</p>
<p>for g&lowast;N (y) (recall that E|ξ |&lt;&infin;) and hence
gN (̂ξ )&le; E
</p>
<p>(
gN (ξ)
</p>
<p>∣∣A
)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>98 4 Numerical Characteristics of Random Variables
</p>
<p>Further, we have gN (y) &uarr; g(y) as N &rarr;&infin; for all y. Therefore the left-hand side
of this inequality converges everywhere to g(̂ξ) as N &rarr;&infin;, but the right-hand side
converges to E(g(ξ)|A) by Theorem 4.8.2. Property 6 is proved. �
</p>
<p>7. The total probability formula
</p>
<p>Eξ = EE(ξ |A)
follows immediately from property 2 of Definition 4.8.2 with A=Ω .
</p>
<p>8. Iterated averaging (an extension of property 7): if A&sub;A1 &sub; F then
E(ξ |A)= E
</p>
<p>[
E(ξ |A1)
</p>
<p>∣∣A
]
.
</p>
<p>Indeed, for any A &isin;A, since A &isin;A1 one has&int;
</p>
<p>A
</p>
<p>E
[
E(ξ |A1)
</p>
<p>∣∣A
]
dP=
</p>
<p>&int;
</p>
<p>A
</p>
<p>E(ξ |A1) dP=
&int;
</p>
<p>A
</p>
<p>ξ dP=
&int;
</p>
<p>A
</p>
<p>E(ξ |A) dP.
</p>
<p>The properties 1, 3&ndash;5, 7 and 8 clearly hold for both scalar- and vector-valued
</p>
<p>random variables ξ . The next property we will single out.
</p>
<p>9. For ξ &isin; L2, the minimum of E(ξ &minus; a(ω))2 over all A-measurable functions
a(ω) is attained at a(ω)= E(ξ |A).
</p>
<p>Indeed, E(ξ &minus; a(ω))2 = EE((ξ &minus; a(ω))2|A), but a(ω) behaves as a constant in
what concerns the operation E(&middot;|A) (see property 5), so that
</p>
<p>E
((
ξ &minus; a(ω)
</p>
<p>)2∣∣A
)
= E
</p>
<p>((
ξ &minus;E(ξ |A)
</p>
<p>)2∣∣A
)
+
(
E(ξ |A)&minus; a(ω)
</p>
<p>)2
</p>
<p>and the minimum of this expression is attained at a(ω)= E(ξ |A).
This property proves the equivalence of Definitions 4.8.1 and 4.8.2 in the case
</p>
<p>when ξ &isin; L2 (in both definitions, conditional expectation is defined up to its values
on a set of measure 0). In this connection note once again that, in L2, the operation
</p>
<p>of taking conditional expectations is the projection onto HA (see our comments to
</p>
<p>Definition 4.8.1).
</p>
<p>Property 9 can be extended to the multivariate case in the following form: for any
nonnegative definite matrix V , the minimum min(ξ &minus; a(ω))V (ξ &minus; a(ω))T over all
A-measurable functions a(ω) is attained at a(ω)= E(ξ |A).
</p>
<p>The assertions proved above in the case where ξ &isin; L2 and the σ -algebra A is
countably generated will surely hold true for an arbitrary σ -algebra A, but the sub-
</p>
<p>stantiation of this fact requires additional work.
</p>
<p>In conclusion we note that property 5 admits, under wide assumptions, the fol-
</p>
<p>lowing generalisation:
</p>
<p>5A. If η is A-measurable and g(ω,η) is a measurable function of its arguments
ω &isin;Ω and η &isin;Rk such that E|g(ω,η)|A)|&lt;&infin;, then
</p>
<p>E
(
g(ω,η)
</p>
<p>∣∣A
)
= E
</p>
<p>(
g(ω,y)
</p>
<p>∣∣A
)∣∣
y=η. (4.8.6)
</p>
<p>This implies the double expectation (or total probability) formula.
</p>
<p>Eg(ω,η)= E
[
E
(
g(ω,y)
</p>
<p>∣∣A
)∣∣
y=η
</p>
<p>]
,</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Conditional Distributions 99
</p>
<p>which can be considered as an extension of Fubini&rsquo;s theorem (see Sects. 4.6 and 3.6).
</p>
<p>Indeed, if g(ω,y) is independent of A, then
</p>
<p>E
(
g(ω,y)
</p>
<p>∣∣A
)
= Eg(ω,y), E
</p>
<p>(
g(ω,η)
</p>
<p>∣∣A
)
= Eg(ω,y)
</p>
<p>∣∣
y=η,
</p>
<p>Eg(ω,η)= E
[
Eg(ω,y)
</p>
<p>∣∣
y=η
</p>
<p>]
.
</p>
<p>In regard to its form, this is Fubini&rsquo;s theorem, but here η is a vector-valued ran-
</p>
<p>dom variable, while ω can be of an arbitrary nature.
</p>
<p>We will prove property 5A under the simplifying assumption that there exists
</p>
<p>a sequence of simple functions ηn such that g(ω,ηn) &uarr; g(ω,η) and h(ω,ηn) &uarr;
h(ω,η) a.s., where h(ω,y) = E(g(ω,y)|A)). Indeed, let ηn = yk for ω &isin; Ak &sub; A.
Then
</p>
<p>g(ω,ηn)=
&sum;
</p>
<p>g(ω,yk)IAk .
</p>
<p>By property 5 it follows that (4.8.6) holds for the functions ηn. It remains to
</p>
<p>make use of the monotone convergence theorem (property 4) in the equality
</p>
<p>E(g(ω,ηn)|A))= h(ω,ηn).
</p>
<p>4.9 Conditional Distributions
</p>
<p>Along with conditional expectations, one can consider conditional distributions
given sub-σ -algebras and random variables. In the present section, we turn our at-
</p>
<p>tention to the latter.
</p>
<p>Let ξ and η be two random variables on 〈Ω,F,P〉 taking values in Rs and Rk ,
respectively, and let Bs be the σ -algebra of Borel sets in Rs .
</p>
<p>Definition 4.9.1 A function F(B|y) of two variables y &isin; Rk and B &isin;Bs is called
the conditional distribution of ξ given η= y if:
</p>
<p>1. For any B , F(B|η) is the conditional probability P(ξ &isin; B|η) of the event
{ξ &isin; B} given η, i.e. F(B|y) is a Borel function of y such that, for any A &isin;Bk ,
</p>
<p>E
(
F(B|η);η &isin;A
</p>
<p>)
&equiv;
&int;
</p>
<p>A
</p>
<p>F(B|y)P(η &isin; dy)= P(ξ &isin; B, η &isin;A).
</p>
<p>2. For any y, F(B|y) is a probability distribution in B .
</p>
<p>Sometimes we will write the function F(B|y) in a more &ldquo;deciphered&rdquo; form as
F(B|y)= P(ξ &isin; B|η= y).
</p>
<p>We know that, for each B &isin; Bs , there exists a Borel function gB(y) such that
gB(η) = P(ξ &isin; B|η). Thus, putting P(B|y) := gB(y), we will satisfy condition 1
of Definition 4.9.1. Condition 2, however, does not follow from the properties of
</p>
<p>conditional expectations and by no means needs to hold: indeed, since conditional
</p>
<p>probability P(ξ &isin; B|η) is defined for each B up to its values on a set NB of zero</p>
<p/>
</div>
<div class="page"><p/>
<p>100 4 Numerical Characteristics of Random Variables
</p>
<p>measure (so that there exist many variants of conditional expectation), and this set
</p>
<p>can be different for each B . Therefore, if the union
</p>
<p>N =
⋃
</p>
<p>B&isin;Bs
NB
</p>
<p>has a non-zero probability, it could turn out that, for instance, the equalities
</p>
<p>P(ξ &isin; B1 &cup;B2|η)= P(ξ &isin; B1|η)+ P(ξ &isin; B2|η)
(additivity of probability) for all disjoint B1 and B2 from B
</p>
<p>s hold for no ω &isin;N , i.e.
on an ω-set N of positive probability, the function gB(y) will not be a distribution
</p>
<p>as a function of B .
</p>
<p>However, in the case when ξ is a random variable taking values in Rs with the
σ -algebra Bs of Borel sets, one can always choose gB(η)= P(ξ &isin; B|η) such that
gB(y) will be a conditional distribution.2
</p>
<p>As one might expect, conditional probabilities possess the natural property that
</p>
<p>conditional expectations can be expressed as integrals with respect to conditional
</p>
<p>distributions.
</p>
<p>Theorem 4.9.1 For any measurable function g(x) mapping Rs into R such that
E|g(ξ)|&lt;&infin;, one has
</p>
<p>E
(
g(ξ)
</p>
<p>∣∣η
)
=
&int;
</p>
<p>g(x)F(dx|η). (4.9.1)
</p>
<p>Proof It suffices to consider the case g(x) &ge; 0. If g(x) = IA(x) is the indicator of
a set A, then formula (4.9.1) clearly holds. Therefore it holds for any simple (i.e.
</p>
<p>assuming only finitely many values) function gn(x). It remains to take a sequence
</p>
<p>gn &uarr; g and make use of the monotonicity of both sides of (4.9.1) and property 4
from Sect. 4.8. �
</p>
<p>In real-life problems, to compute conditional distributions one can often use the
</p>
<p>following simple rule which we will write in the form
</p>
<p>P(ξ &isin; B|η= y)= P(ξ &isin; B,η &isin; dy)
P(η &isin; dy) . (4.9.2)
</p>
<p>Both conditions of Definition 4.9.1 will clearly be formally satisfied.
</p>
<p>If ξ and η have a joint density, this equality will have a precise meaning.
</p>
<p>Definition 4.9.2 Assume that, for each y, the conditional distribution F(B|y) is
absolutely continuous with respect to some measure &micro; in Rs :
</p>
<p>P(ξ &isin; B|η= y)=
&int;
</p>
<p>B
</p>
<p>f (x|y)&micro;(dx).
</p>
<p>Then the density f (x|y) is said to be the conditional density of ξ (with respect to
the measure &micro;) given η= y.
</p>
<p>2For more details, see e.g. [12, 14, 26].</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Conditional Distributions 101
</p>
<p>In other words, a measurable function f (x|y) of two variables x and y is the
conditional density of ξ given η= y if:
(1) For any Borel sets A&sub;Rk and B &sub;Rs ,
</p>
<p>&int;
</p>
<p>y&isin;A
</p>
<p>&int;
</p>
<p>x&isin;B
f (x|y)&micro;(dx)P(η &isin; dy)= P(ξ &isin; B,η &isin;A). (4.9.3)
</p>
<p>(2) For any y, the function f (x|y) is a probability density.
It follows from Theorem 4.9.1 that if there exists a conditional density, then
</p>
<p>E
(
g(ξ)
</p>
<p>∣∣η
)
=
&int;
</p>
<p>g(x)f (x|η)&micro;(dx).
</p>
<p>If we additionally assume that the distribution of η has a density q(y) with re-
</p>
<p>spect to some measure λ in Rk , then we can re-write (4.9.3) in the form
&int;
</p>
<p>y&isin;A
</p>
<p>&int;
</p>
<p>x&isin;B
f (x|y)q(y)&micro;(dx)λ(dy)= P(ξ &isin; B,η &isin;A). (4.9.4)
</p>
<p>Consider now the direct product of spaces Rs and Rk and the direct product of
</p>
<p>measures &micro;&times;λ on it (if C = B&times;A, B &sub;Rs , A&sub;Rk then &micro;&times;λ(C)= &micro;(B)λ(A)).
In the product space, relation (4.9.4) evidently means that the joint distribution of ξ
</p>
<p>and η in Rs &times;Rk has a density with respect to &micro;&times; λ which is equal to
f (x, y)= f (x|y)q(y).
</p>
<p>The converse assertion is also true.
</p>
<p>Theorem 4.9.2 If the joint distribution of ξ and η in Rs &times;Rk has a density f (x, y)
with respect to &micro;&times; λ, then the function
</p>
<p>f (x|y)= f (x, y)
q(y)
</p>
<p>, where q(y)=
&int;
</p>
<p>f (x, y)&micro;(dx),
</p>
<p>is the conditional density of ξ given η= y, and the function q(y) is the density of η
with respect to the measure λ.
</p>
<p>Proof The assertion on q(y) is obvious, since
&int;
</p>
<p>A
</p>
<p>q(y)λ(dy)= P(η &isin;A).
</p>
<p>It remains to observe that f (x|y) = f (x, y)/q(y) satisfies all the conditions from
Definition 4.9.2 of conditional density (equality (4.9.4), which is equivalent to
</p>
<p>(4.9.3), clearly holds here). �
</p>
<p>Theorem 4.9.2 gives a precise meaning to (4.9.2) when ξ and η have densities.
</p>
<p>Example 4.9.1 Let ξ1 and ξ2 be independent random variables, ξ1 &sub;=�λ1 , ξ2 &sub;=�λ2 .
What is the distribution of ξ1 given ξ1 + ξ2 = n? We could easily compute the de-
sired conditional probability P(ξ1 = k|ξ1 + ξ2 = n), k &le; n, without using Theo-
rem 4.9.2, for ξ1 + ξ2 &sub;=�λ1+λ2 and the probability of the event {ξ1 + ξ2 = n} is</p>
<p/>
</div>
<div class="page"><p/>
<p>102 4 Numerical Characteristics of Random Variables
</p>
<p>positive. Retaining this possibility for comparison, we will still make formal use of
</p>
<p>Theorem 4.9.2. Here ξ1 and η = ξ1 + ξ2 have densities (equal to the corresponding
probabilities) with respect to the counting measure, so that
</p>
<p>f (k,n)= P(ξ1 = k, η= n)= P(ξ1 = k, ξ2 = n&minus; k)= e&minus;λ1&minus;λ2
λk1λ
</p>
<p>n&minus;k
2
</p>
<p>k!(n&minus; k)! ,
</p>
<p>q(n)= P(η= n)= e&minus;λ1&minus;λ2 (λ1 + λ2)
n
</p>
<p>n! .
</p>
<p>Therefore the required density (probability) is equal to
</p>
<p>f (k|n)= P(ξ1 = k|η= n)=
f (k,n)
</p>
<p>q(n)
= n!
</p>
<p>k!(n&minus; k)!p
k(1 &minus; p)n&minus;k,
</p>
<p>where p = λ/(λ1 + λ2). Thus the conditional distribution of ξ1 given the fixed sum
ξ1 + ξ2 = n is a binomial distribution. In particular, if ξ1, . . . , ξr are independent,
ξi &sub;=�λ, then the conditional distribution of ξ1 given the fixed sum ξ1 +&middot; &middot; &middot;+ ξr = n
will be Bn1/r , which does not depend on λ.
</p>
<p>The next example answers the same question as in Example 4.9.1 but for nor-
</p>
<p>mally distributed random variables.
</p>
<p>Example 4.9.2 Let �a,σ 2 be the two-dimensional joint normal distribution of ran-
dom variables ξ1 and ξ2, where a = (a1, a2), ai = Eξi , and σ 2 = ‖σi,j‖ is the co-
variance matrix, σij = E(ξi &minus; ai)(ξj &minus; aj ), i, j = 1,2. The determinant of σ 2 is
</p>
<p>∣∣σ 2
∣∣= σ11σ22 &minus; σ 212 = σ11σ22
</p>
<p>(
1 &minus; ρ2
</p>
<p>)
,
</p>
<p>where ρ is the correlation coefficient of ξ1 and ξ2. Thus, if |ρ| 
= 1 then the covari-
ance matrix is non-degenerate and has the inverse
</p>
<p>A=
(
σ 2
</p>
<p>)&minus;1 = 1|σ 2|
</p>
<p>∥∥∥∥
σ22 &minus;σ12
&minus;σ12 σ11
</p>
<p>∥∥∥∥=
1
</p>
<p>1 &minus; ρ2
</p>
<p>∥∥∥∥∥
</p>
<p>1
σ11
</p>
<p>&minus; ρ&radic;
σ11σ22
</p>
<p>&minus; ρ&radic;
σ11σ12
</p>
<p>1
σ22
</p>
<p>∥∥∥∥∥ .
</p>
<p>Therefore the joint density of ξ1 and ξ2 (with respect to Lebesgue measure) is (see
</p>
<p>Sect. 3.3)
</p>
<p>f (x, y)= 1
2π
</p>
<p>&radic;
σ11σ22(1 &minus; ρ2)
</p>
<p>&times; exp
{
&minus; 1
</p>
<p>2(1 &minus; ρ2)
</p>
<p>[
(x &minus; a1)2
</p>
<p>σ11
&minus; 2ρ(x &minus; a1)(y &minus; a2)&radic;
</p>
<p>σ11σ22
+ (y &minus; a2)
</p>
<p>2
</p>
<p>σ22
</p>
<p>]}
.
</p>
<p>(4.9.5)
</p>
<p>The one-dimensional densities of ξ1 and ξ2 are, respectively,
</p>
<p>f (x)= 1&radic;
2πσ11
</p>
<p>e&minus;(x&minus;a1)
2/(2σ11) , q(y)= 1&radic;
</p>
<p>2πσ22
e&minus;(y&minus;a2)
</p>
<p>2/(2σ22). (4.9.6)
</p>
<p>Hence the conditional density of ξ1 given ξ2 = y is</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Conditional Distributions 103
</p>
<p>Fig. 4.2 Illustration to
</p>
<p>Example 4.9.4. Positions of
</p>
<p>the target&rsquo;s centre, the first
</p>
<p>aimpoint, and the first hit
</p>
<p>f (x|y)= f (x, y)
q(y)
</p>
<p>= 1&radic;
2πσ11(1 &minus; ρ2)
</p>
<p>exp
</p>
<p>{
&minus; 1
</p>
<p>2σ11(1 &minus; ρ2)
</p>
<p>(
x &minus; a1 &minus; ρ
</p>
<p>&radic;
σ11
</p>
<p>σ22
(y &minus; a2)
</p>
<p>)2}
,
</p>
<p>which is the density of the normal distribution with mean a1 + ρ
&radic;
</p>
<p>σ11
σ22
</p>
<p>(y &minus; a2) and
variance σ11(1 &minus; ρ2).
</p>
<p>This implies that f (x|y) coincides with the unconditional density of f (x) in
the case ρ = 0 (and hence ξ1 and ξ2 are independent), and that the conditional
expectation of ξ1 given ξ2 is
</p>
<p>E(ξ1|ξ2)= a1 + ρ
&radic;
σ11/σ22(ξ2 &minus; a2).
</p>
<p>The straight line x = a1 + ρ
&radic;
σ11/σ22(y &minus; a2) is called the regression line of ξ1
</p>
<p>on ξ2. It gives the best mean-square approximation for ξ1 given ξ2 = y.
</p>
<p>Example 4.9.3 Consider the problem of computing the density of the random vari-
able ξ = ϕ(ζ, η) when ζ and η are independent. It follows from formula (4.9.3)
with A = Rk that the density of the distribution of ξ can be expressed in terms of
the conditional density f (x|y) as
</p>
<p>f (x)=
&int;
</p>
<p>f (x|y)P(η &isin; dy).
</p>
<p>In our problem, by f (x|y) one should understand the density of the random variable
ϕ(ζ, y), since P(ξ &isin; B|η= y)= P(ϕ(ζ, y) &isin; B).
</p>
<p>Example 4.9.4 Target shooting with adjustment. A gun fires at a target of a known
geometric form. Introduce the polar system of coordinates, of which the origin is
</p>
<p>the position of the gun. The distance r (see Fig. 4.2) from the gun to a certain point
</p>
<p>which is assumed to be the centre of the target is precisely known to the crew of the
</p>
<p>gun, while the azimuth is not. However, there is a spotter who communicates to the
</p>
<p>crew after the first trial shoot what the azimuth deviation of the hitting point from
</p>
<p>the centre of the target is.
</p>
<p>Suppose the scatter of the shells fired by the gun (the deviation (ξ, η) of the hit-
</p>
<p>ting point from the aimpoint) is described, in the polar system of coordinates, by the
</p>
<p>two-dimensional normal distribution with density (4.9.5) with α = 0. In Sect. 8.4 we
will see why the deviation is normally distributed. Here we will neglect the circum-
</p>
<p>stance that the azimuth deviation ξ cannot exceed π while the distance deviation ξ</p>
<p/>
</div>
<div class="page"><p/>
<p>104 4 Numerical Characteristics of Random Variables
</p>
<p>cannot assume values in (&minus;&infin;,&minus;r). (The standard deviations σ1 and σ2 are usually
very small in comparison with r and π , so this fact is insignificant.) If the azimuth β
</p>
<p>of the centre of the target were also exactly known along with the distance r , then
</p>
<p>the probability of hitting the target would be equal to
</p>
<p>&int;
</p>
<p>B(r,β)
</p>
<p>&int;
f (x, y) dx dy,
</p>
<p>where B(r,β) = {(x, y) : (r + x,β + y) &isin; B} and the set B represents the target.
However, the azimuth is communicated to the crew of the gun by the spotter based
</p>
<p>on the result of the trial shot, i.e. the spotter reports it with an error δ distributed
</p>
<p>according to the normal law with the density q(y) (see (4.9.6)). What is the proba-
</p>
<p>bility of the event A that, in these circumstances, the gun will hit the target from the
</p>
<p>second shot? If δ = z, then the azimuth is communicated with the error z and
</p>
<p>P(A|δ = z)=
&int;
</p>
<p>B(r,β)
</p>
<p>&int;
f (x, y &minus; z) dx dy =: ϕ(z).
</p>
<p>Therefore,
</p>
<p>P(A)= E
[
P(A| δ)
</p>
<p>]
= Eϕ(δ)= 1
</p>
<p>σ2
&radic;
</p>
<p>2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;z
</p>
<p>2/(2σ 22 )ϕ(z) dz.
</p>
<p>Example 4.9.5 The segment [0,1] is broken &ldquo;at random&rdquo; (i.e. with the uniform
distribution of the breaking point) into two parts. Then the larger part is also broken
</p>
<p>&ldquo;at random&rdquo; into two parts. What is the probability that one can form a triangle from
</p>
<p>the three fragments?
</p>
<p>The triangle can be formed if there occurs the event B that all the three fragments
</p>
<p>have lengths smaller than 1/2. Let ω1 and ω2 be the distances from the points of the
</p>
<p>first and second breaks to the origin. Use the complete probability formula
</p>
<p>P(B)= EP(B|ω1).
</p>
<p>Since ω1 is distributed uniformly over [0,1], one only has to calculate the con-
ditional probability P(B|ω1). If ω1 &lt; 1/2 then ω2 is distributed uniformly over
[ω1,1]. One can construct a triangle provided that 1/2 &lt;ω2 &lt; 1/2+ω1. Therefore
P(B|ω1)= ω1/(1 &minus; ω1) on the set {ω1 &lt; 1/2}. We easily find from symmetry that,
for ω1 &gt; 1/2,
</p>
<p>P(B|ω1)=
1 &minus;ω1
ω1
</p>
<p>.
</p>
<p>Hence
</p>
<p>P(B)= 2
&int; 1/2
</p>
<p>0
</p>
<p>x
</p>
<p>1 &minus; x dx =&minus;1 + 2
&int; 1/2
</p>
<p>0
</p>
<p>dx
</p>
<p>1 &minus; x dx =&minus;1 + 2 ln 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Conditional Distributions 105
</p>
<p>One could also solve this problem using a direct &ldquo;geometric&rdquo; method. The den-
</p>
<p>sity f (x, y) of the joint distribution of (ω1,ω2) is
</p>
<p>f (x, y)=
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>1
1&minus;x if x &lt; 1/2, y &isin; [x,1],
1
x
</p>
<p>if x &ge; 1/2, y &isin; [0, x],
0 otherwise.
</p>
<p>It remains to compute the integral of this function over the domain corresponding
</p>
<p>to B .
</p>
<p>All the above examples were on conditional expectations given random variables
(not σ -algebras).
</p>
<p>The need for conditional expectations given σ -algebras arises where it is diffi-
</p>
<p>cult to manage working just with conditional expectations given random variables.
</p>
<p>Assume, for instance, that a certain process is described by a sequence of random
</p>
<p>variables {ξj }&infin;j=&minus;&infin; which are not independent. Then the most convenient way to
describe the distribution of ξ1 given the whole &ldquo;history&rdquo; (i.e. the values ξ0, ξ&minus;1,
ξ&minus;2, . . .) is to take the conditional distribution of ξ1 given σ(ξ0, ξ&minus;1, . . .). It would
be difficult to confine oneself here to conditional distributions given random vari-
</p>
<p>ables only. Respective examples are given in Chaps. 13, 15&ndash;22.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>Sequences of Independent Trials
with Two Outcomes
</p>
<p>Abstract The weak and strong laws of large numbers are established for the
</p>
<p>Bernoulli scheme in Sect. 5.1. Then the local limit theorem on approximation of
</p>
<p>the binomial probabilities is proved in Sect. 5.2 using Stirling&rsquo;s formula (covering
</p>
<p>both the normal approximation zone and the large deviations zone). The same sec-
</p>
<p>tion also contains a refinement of that result, including a bound for the relative error
</p>
<p>of the approximation, and an extension of the local limit theorem to polynomial dis-
</p>
<p>tributions. This is followed by the derivation of the de Moivre&ndash;Laplace theorem and
</p>
<p>its refinements in Sect. 5.3. In Sect. 5.4, the coupling method is used to prove the
</p>
<p>Poisson theorem for sums of non-identically distributed independent random indica-
</p>
<p>tors, together with sharp approximation error bounds for the total variation distance.
</p>
<p>The chapter ends with derivation of large deviation inequalities for the Bernoulli
</p>
<p>scheme in Sect. 5.5.
</p>
<p>5.1 Laws of Large Numbers
</p>
<p>Suppose we have a sequence of trials in each of which a certain event A can oc-
</p>
<p>cur with probability p independently of the outcomes of other trials. Form a se-
</p>
<p>quence of random variables as follows. Put ξk = 1 if the event A has occurred in
the k-th trial, and ξk = 0 otherwise. Then (ξk)&infin;k=1 will be a sequence of indepen-
dent random variables which are identically distributed according to the Bernoulli
</p>
<p>law: P(ξk = 1) = p, P(ξk = 0) = q = 1 &minus; p, Eξk = p, Var(ξk) = pq . The sum
Sn = ξ1 + &middot; &middot; &middot; + ξn &sub;= Bnp is simply the number of occurrences of the event A in the
first n trials. Clearly ESn = np and Var(Sn)= npq .
</p>
<p>The following assertion is called the law of large numbers for the Bernoulli
scheme.
</p>
<p>Theorem 5.1.1 For any ε &gt; 0
</p>
<p>P
</p>
<p>(∣∣∣∣
Sn
</p>
<p>n
&minus; p
</p>
<p>∣∣∣∣&gt; ε
)
&rarr; 0 as n&rarr;&infin;.
</p>
<p>This assertion is a direct consequence of Theorem 4.7.5. One can also obtain the
</p>
<p>following stronger result:
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_5, &copy; Springer-Verlag London 2013
</p>
<p>107</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_5">http://dx.doi.org/10.1007/978-1-4471-5201-9_5</a></div>
</div>
<div class="page"><p/>
<p>108 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>Theorem 5.1.2 (The Strong Law of Large Numbers for the Bernoulli Scheme) For
any ε &gt; 0, as n&rarr;&infin;,
</p>
<p>P
</p>
<p>(
sup
k&ge;n
</p>
<p>∣∣∣∣
Sk
</p>
<p>k
&minus; p
</p>
<p>∣∣∣∣&gt; ε
)
&rarr; 0.
</p>
<p>The interpretation of this result is that the notion of probability which we intro-
</p>
<p>duced in Chaps. 1 and 2 corresponds to the intuitive interpretation of probability
</p>
<p>as the limiting value of the relative frequency of the occurrence of the event. In-
</p>
<p>deed, Sn/n could be considered as the relative frequency of the event A for which
</p>
<p>P(A)= p. It turned out that, in a certain sense, Sn/n converges to p.
</p>
<p>Proof of Theorem 5.1.2 One has
</p>
<p>P
</p>
<p>(
sup
k&ge;n
</p>
<p>∣∣∣∣
Sk
</p>
<p>k
&minus; p
</p>
<p>∣∣∣∣&gt; ε
)
= P
</p>
<p>( &infin;⋃
</p>
<p>k=n
</p>
<p>{∣∣∣∣
Sk
</p>
<p>k
&minus; p
</p>
<p>∣∣∣∣&gt; ε
})
</p>
<p>&le;
&infin;&sum;
</p>
<p>k=n
P
</p>
<p>(∣∣∣∣
Sk
</p>
<p>k
&minus; p
</p>
<p>∣∣∣∣&gt; ε
)
&le;
</p>
<p>&infin;&sum;
</p>
<p>k=n
</p>
<p>E(Sk &minus; kp)4
k4ε4
</p>
<p>.
</p>
<p>(5.1.1)
</p>
<p>Here we again made use of Chebyshev&rsquo;s inequality but this time for the fourth mo-
</p>
<p>ments. Expanding we find that
</p>
<p>E(Sk &minus; kp)4 = E
(
</p>
<p>k&sum;
</p>
<p>j=1
(ξj &minus; p)
</p>
<p>)4
=
</p>
<p>k&sum;
</p>
<p>j=1
E(ξj &minus; p)4 + 6
</p>
<p>&sum;
</p>
<p>i&lt;j
</p>
<p>(ξi &minus; p)2(ξj &minus; p)2
</p>
<p>= k
(
pq4 + qp4
</p>
<p>)
+ 3k(k &minus; 1)(pq)2 &le; k + k(k &minus; 1)= k2. (5.1.2)
</p>
<p>Thus the probability we want to estimate does not exceed the sum
</p>
<p>ε&minus;4
&infin;&sum;
</p>
<p>k=n
k&minus;2 &rarr; 0 as n&rarr;&infin;. �
</p>
<p>It is not hard to see that we would not have found the required bound if we used
</p>
<p>Chebyshev&rsquo;s inequality with second moments in (5.1.1).
</p>
<p>We could also note that one actually has much stronger bounds for
</p>
<p>P(|Sk &minus; kp| &gt; εk) than those that we made use of above. These will be derived
in Sect. 5.5.
</p>
<p>Corollary 5.1.1 If f (x) is a continuous function on [0,1] then, as n&rarr;&infin;,
</p>
<p>Ef
</p>
<p>(
Sn
</p>
<p>n
</p>
<p>)
&rarr; f (p) (5.1.3)
</p>
<p>uniformly in p.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 The Local Limit Theorem and Its Refinements 109
</p>
<p>Proof For any ε &gt; 0,
</p>
<p>E
</p>
<p>∣∣∣∣f
(
Sn
</p>
<p>n
</p>
<p>)
&minus; f (p)
</p>
<p>∣∣∣∣&le; E
(∣∣∣∣f
</p>
<p>(
Sn
</p>
<p>n
</p>
<p>)
&minus; f (p)
</p>
<p>∣∣∣∣;
∣∣∣∣
Sn
</p>
<p>n
&minus; p
</p>
<p>∣∣∣∣&le; ε
)
</p>
<p>+E
(∣∣∣∣f
</p>
<p>(
Sn
</p>
<p>n
</p>
<p>)
&minus; f (p)
</p>
<p>∣∣∣∣;
∣∣∣∣
Sn
</p>
<p>n
&minus; p
</p>
<p>∣∣∣∣&gt; ε
)
</p>
<p>&le; sup
|x|&le;ε
</p>
<p>∣∣f (p+ x)&minus; f (p)
∣∣+ δn(ε),
</p>
<p>where the quantity δ(ε) is independent of p by virtue of (5.1.1), (5.1.2), and since
</p>
<p>δn(ε)&rarr; 0 as n&rarr;&infin;. �
</p>
<p>Corollary 5.1.2 If f (x) is continuous on [0,1], then, as n&rarr;&infin;,
n&sum;
</p>
<p>k=0
f
</p>
<p>(
k
</p>
<p>n
</p>
<p>)(
n
</p>
<p>k
</p>
<p>)
xk(1 &minus; x)n&minus;k &rarr; f (x)
</p>
<p>uniformly in x &isin; [0,1].
</p>
<p>This relation is just a different form of (5.1.3) since
</p>
<p>P(Sn = k)=
(
n
</p>
<p>k
</p>
<p>)
pk(1 &minus; p)n&minus;k
</p>
<p>(see Chap. 1). This relation implies the well-known Weierstrass theorem on approxi-
mation of continuous functions by polynomials. Moreover, the required polynomials
</p>
<p>are given here explicitly&mdash;they are Bernstein polynomials.
</p>
<p>5.2 The Local Limit Theorem and Its Refinements
</p>
<p>5.2.1 The Local Limit Theorem
</p>
<p>We know that P(Sn = k)=
(
n
k
</p>
<p>)
pkqn&minus;k , q = 1 &minus; p. However, this formula becomes
</p>
<p>very inconvenient for computations with large n and k, which raises the question
</p>
<p>about the asymptotic behaviour of the probability P(Sn = k) as n&rarr;&infin;.
In the sequel, we will write an &sim; bnfor two number sequences {an} and {bn} if
</p>
<p>an/bn &rarr; 1 as n&rarr;&infin;. Such sequences {an} and {bn} will be said to be equivalent.
Set
</p>
<p>H(x)= x ln x
p
+ (1 &minus; x) ln 1 &minus; x
</p>
<p>1 &minus; p , p
&lowast; = k
</p>
<p>n
. (5.2.1)
</p>
<p>Theorem 5.2.1 As k&rarr;&infin; and n&minus; k&rarr;&infin;,
</p>
<p>P(Sn = k)= P
(
Sn
</p>
<p>n
= p&lowast;
</p>
<p>)
&sim; 1&radic;
</p>
<p>2πnp&lowast;(1 &minus; p&lowast;)
exp
</p>
<p>{
&minus;nH
</p>
<p>(
p&lowast;
</p>
<p>)}
. (5.2.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>110 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>Proof We will make use of Stirling&rsquo;s formula according to which n! &sim;
&radic;
</p>
<p>2πnnne&minus;n
</p>
<p>as n&rarr;&infin;. One has
</p>
<p>P(Sn = k)=
(
n
</p>
<p>k
</p>
<p>)
pkqn&minus;k &sim;
</p>
<p>&radic;
n
</p>
<p>2πk(n&minus; k)
nn
</p>
<p>kk(n&minus; k)n&minus;k p
k(1 &minus; p)n&minus;k
</p>
<p>= 1&radic;
2πnp&lowast;(1 &minus; p&lowast;)
</p>
<p>&times; exp
{
&minus;k ln k
</p>
<p>n
&minus; (n&minus; k) ln n&minus; k
</p>
<p>n
+ k lnp+ (n&minus; k) ln (1 &minus; p)
</p>
<p>}
</p>
<p>= 1&radic;
2πnp&lowast;(1 &minus; p&lowast;)
</p>
<p>exp
{
&minus;n
</p>
<p>[
p&lowast; lnp&lowast; +
</p>
<p>(
1 &minus; p&lowast;
</p>
<p>)
ln
(
1 &minus; p&lowast;
</p>
<p>)
</p>
<p>&minus; p&lowast; lnp&minus;
(
1 &minus; p&lowast;
</p>
<p>)
ln(1 &minus; p)
</p>
<p>]}
</p>
<p>= 1&radic;
2πnp&lowast;(1 &minus; p&lowast;)
</p>
<p>exp
{
nH
</p>
<p>(
p&lowast;
</p>
<p>)}
. �
</p>
<p>If p&lowast; = k/n is close to p, then one can find another form for the right-hand side
of (5.2.2) which is of significant interest. Note that the function H(x) is analytic on
</p>
<p>the interval (0,1). Since
</p>
<p>H &prime;(x)= ln x
p
&minus; ln 1 &minus; x
</p>
<p>1 &minus; p , H
&prime;&prime;(x)= 1
</p>
<p>p
+ 1
</p>
<p>1 &minus; x , (5.2.3)
</p>
<p>one has H(p)=H &prime;(p)= 0 and, as p&lowast; &minus; p&rarr; 0,1
</p>
<p>H
(
p&lowast;
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
1
</p>
<p>p
+ 1
</p>
<p>q
</p>
<p>)(
p&lowast; &minus; p
</p>
<p>)2 +O
(∣∣p&lowast; &minus; p
</p>
<p>∣∣3).
</p>
<p>Therefore if p&lowast; &sim; p and n(p&lowast; &minus; p)3 &rarr; 0 then
</p>
<p>P(Sn = k)&sim;
1&radic;
</p>
<p>2πpq
exp
</p>
<p>{
&minus; n
</p>
<p>2pq
</p>
<p>(
p&lowast; &minus; p
</p>
<p>)2
}
.
</p>
<p>Putting
</p>
<p>∆= 1&radic;
npq
</p>
<p>, ϕ(x)= 1&radic;
2π
</p>
<p>e&minus;x
2/2,
</p>
<p>one obtains the following assertion.
</p>
<p>Corollary 5.2.1 If z= n(p&lowast; &minus; p)= k&minus; np = o(n2/3) then
</p>
<p>P(Sn = k)= P(Sn &minus; np = z)&sim; ϕ(z∆)∆, (5.2.4)
</p>
<p>where ϕ = ϕ0,1(x) is evidently the density of the normal distribution with parame-
ters (0,1).
</p>
<p>1According to standard conventions, we will write a(z) = o(b(z)) as z &rarr; z0 if b(z) &gt; 0 and
limz&rarr;z0
</p>
<p>a(z)
b(z)
</p>
<p>= 0, and a(z)=O(b(z)) as z&rarr; z0 if b(z) &gt; 0 and lim supz&rarr;z0
|a(z)|
b(z)
</p>
<p>&lt;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 The Local Limit Theorem and Its Refinements 111
</p>
<p>This formula also enables one to estimate the probabilities of the events of the
</p>
<p>form {Sn &lt; k}.
If p&lowast; differs substantially from p, then one could estimate the probabilities of
</p>
<p>such events using the results of Sect. 1.3.
</p>
<p>Example 5.2.1 In a jury consisting of an odd number n= 2m+ 1 of persons, each
member makes a correct decision with probability p = 0.7 independently of the
other members. What is the minimum number of members for which the verdict
</p>
<p>rendered by the majority of jury members will be correct with a probability of at
</p>
<p>least 0.99?
</p>
<p>Put ξk = 1 if the k-th jury member made a correct decision and ξk = 0 otherwise.
We are looking for odd numbers n for which P(Sn &le; m) &le; 0.01. It is evident that
such a trustworthy decision can be achieved only for large values of n. In that case,
</p>
<p>as we established in Sect. 1.3, the probability P(Sn &le;m) is approximately equal to
(n+ 1 &minus;m)p
(n+ 1)p&minus;mP(Sn =m)&asymp;
</p>
<p>p
</p>
<p>2p&minus; 1P(Sn =m).
</p>
<p>Using Theorem 5.2.1 and the fact that in our problem
</p>
<p>p&lowast; &asymp; 1
2
, H
</p>
<p>(
1
</p>
<p>2
</p>
<p>)
=&minus;1
</p>
<p>2
ln 4p(1 &minus; p), H &prime;
</p>
<p>(
1
</p>
<p>2
</p>
<p>)
= ln
</p>
<p>(
1 &minus; p
p
</p>
<p>)
,
</p>
<p>we get
</p>
<p>P(Sn &le;m)&asymp;
p
</p>
<p>2p&minus; 1
</p>
<p>&radic;
2
</p>
<p>πn
exp
</p>
<p>{
&minus;nH
</p>
<p>(
1
</p>
<p>2
&minus; 1
</p>
<p>2n
</p>
<p>)}
</p>
<p>&asymp; p
2p&minus; 1
</p>
<p>&radic;
2
</p>
<p>πn
exp
</p>
<p>{
&minus;nH
</p>
<p>(
1
</p>
<p>2
</p>
<p>)
+ 1
</p>
<p>2
H &prime;
</p>
<p>(
1
</p>
<p>2
</p>
<p>)}
</p>
<p>&asymp;
&radic;
</p>
<p>2π(1 &minus; p)
(2p&minus; 1)&radic;πn
</p>
<p>(&radic;
4p(1 &minus; p)
</p>
<p>)n &asymp; 0.915 1&radic;
n
(0.84)n/2.
</p>
<p>On the right-hand side there is a monotonically decreasing function a(n). Solving
</p>
<p>the equation a(n)= 0.01 we get the answer n= 33. The same result will be obtained
if one makes use of the explicit formulae.
</p>
<p>5.2.2 Refinements of the Local Theorem
</p>
<p>It is not hard to bound the error of approximation (5.2.2). If, in Stirling&rsquo;s formula
</p>
<p>n! =
&radic;
</p>
<p>2πnnne&minus;n+θ(n), we make use of the well-known inequalities2
</p>
<p>1
</p>
<p>12n+ 1 &lt; θ(n) &lt;
1
</p>
<p>12n
,
</p>
<p>then the same argument will give the following refinement of Theorem 5.2.1.
</p>
<p>2See, e.g., [12], Sect. 2.9.</p>
<p/>
</div>
<div class="page"><p/>
<p>112 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>Theorem 5.2.2
</p>
<p>P(Sn = k)=
1&radic;
</p>
<p>2πnp&lowast;(1 &minus; p&lowast;)
exp
</p>
<p>{
nH
</p>
<p>(
p&lowast;
</p>
<p>)
+ θ(k,n)
</p>
<p>}
, (5.2.5)
</p>
<p>where
∣∣θ(k,n)
</p>
<p>∣∣=
∣∣θ(n)&minus; θ(k)θ(n&minus; k)
</p>
<p>∣∣&lt; 1
12k
</p>
<p>+ 1
12(n&minus; k) =
</p>
<p>1
</p>
<p>12np&lowast;(1 &minus; p&lowast;) .
(5.2.6)
</p>
<p>Relation (5.2.4) could also be refined as follows.
</p>
<p>Theorem 5.2.3 For all k such that |p&lowast; &minus; p| &le; 1
2
</p>
<p>min(p, q) one has
</p>
<p>P(Sn = k)= ϕ(z∆)∆
(
1 + ε(k,n)
</p>
<p>)
,
</p>
<p>where
</p>
<p>1 + ε(k,n)= exp
{
ϑ
</p>
<p>( |z|3∆4
3
</p>
<p>+
(
|z| + 1
</p>
<p>6
</p>
<p>)
∆2
</p>
<p>)}
, |ϑ |&lt; 1.
</p>
<p>As one can easily see from the properties of the Taylor expansion of the func-
</p>
<p>tion ex , the order of magnitude of the term ε(k,n) in the above formulae coin-
</p>
<p>cides with that of the argument of the exponential. Hence it follows from Theo-
</p>
<p>rem 5.2.3 that for z = k &minus; np = o(∆&minus;4/3) or, which is the same, z = o(n2/3), one
still has (5.2.4).
</p>
<p>Proof We will make use of Theorem 5.2.2. In addition to formulae (5.2.3) one can
write:
</p>
<p>H (k) = (&minus;1)
k(k &minus; 2)!
xk&minus;1
</p>
<p>+ (k &minus; 2)!
(1 &minus; x)k&minus;1 , k &ge; 2,
</p>
<p>H
(
p&lowast;
</p>
<p>)
= 1
</p>
<p>2pq
</p>
<p>(
p&lowast; &minus; p
</p>
<p>)2 +R1,
</p>
<p>where we can estimate the residual R1 =
&sum;&infin;
</p>
<p>k=3
H (k)(p)
</p>
<p>k! (p
&lowast;&minus;p). Taking into account
</p>
<p>that
</p>
<p>∣∣H (k)(p)
∣∣&le; (k &minus; 2)!
</p>
<p>(
1
</p>
<p>pk&minus;1
+ 1
</p>
<p>qk&minus;1
</p>
<p>)
, k &ge; 2,
</p>
<p>and letting for brevity |p&lowast; &minus; p| = δ, we get for δ &le; 1
2
</p>
<p>min(p, q) the bounds
</p>
<p>|R1| &le;
&infin;&sum;
</p>
<p>k=3
</p>
<p>(k &minus; 2)!
k!
</p>
<p>(
1
</p>
<p>pk&minus;1
+ 1
</p>
<p>qk&minus;1
</p>
<p>)
&le; δ
</p>
<p>3
</p>
<p>6
</p>
<p>(
1
</p>
<p>p2
</p>
<p>1
</p>
<p>1 &minus; δ
p
</p>
<p>+ 1
q2
</p>
<p>1
</p>
<p>1 &minus; δ
q
</p>
<p>)
</p>
<p>&le; δ
6
</p>
<p>(
2
</p>
<p>p2
+ 2
</p>
<p>q2
</p>
<p>)
&lt;
</p>
<p>δ3
</p>
<p>3(pq)2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 The Local Limit Theorem and Its Refinements 113
</p>
<p>From this it follows that
</p>
<p>&minus;nH
(
p&lowast;
</p>
<p>)
=&minus; (k &minus; np)
</p>
<p>2
</p>
<p>2npq
+ ϑ1|k&minus; np|
</p>
<p>3
</p>
<p>3(npq)2
=&minus;z
</p>
<p>2∆2
</p>
<p>2
+ ϑ1|z|
</p>
<p>3∆4
</p>
<p>3
, |ϑ1|&lt; 1.
</p>
<p>(5.2.7)
</p>
<p>We now turn to the other factors in equality (5.2.5) and consider the product
</p>
<p>p&lowast;(1 &minus; p&lowast;). Since &minus;p &le; 1 &minus; p&minus; p&lowast; &le; 1 &minus; p, we have
∣∣p&lowast;
</p>
<p>(
1 &minus; p&lowast;
</p>
<p>)
&minus; p(1 &minus; p)
</p>
<p>∣∣=
∣∣(p&minus; p&lowast;
</p>
<p>)(
1 &minus; p&minus; p&lowast;
</p>
<p>)∣∣&le;
∣∣p&lowast; &minus; p
</p>
<p>∣∣max(p, q).
</p>
<p>This implies in particular that, for |p&lowast; &minus; p|&lt; 1
2
</p>
<p>min(p, q), one has
</p>
<p>∣∣p&lowast;
(
1 &minus; p&lowast;
</p>
<p>)
&minus; pq
</p>
<p>∣∣&lt; 1
2
pq, p&lowast;
</p>
<p>(
1 &minus; p&lowast;
</p>
<p>)
&gt;
</p>
<p>1
</p>
<p>2
pq.
</p>
<p>Therefore one can write along with (5.2.6) that, for the values of k indicated in
</p>
<p>Theorem 5.2.3,
</p>
<p>∣∣θ(k,n)
∣∣&lt; 1
</p>
<p>6npq
= ∆
</p>
<p>2
</p>
<p>6
. (5.2.8)
</p>
<p>It remains to consider the factor [p&lowast;(1 &minus; p&lowast;)]&minus;1/2. Since for |γ |&lt; 1/2
</p>
<p>∣∣ln(1 + γ )
∣∣=
</p>
<p>∣∣∣∣
&int; 1+γ
</p>
<p>1
</p>
<p>1
</p>
<p>x
dx
</p>
<p>∣∣∣∣&lt; 2|γ |,
</p>
<p>one has for δ = |p&lowast; &minus; p|&lt; (1/2)min(p, q) the relations
</p>
<p>ln
(
p&lowast;
</p>
<p>(
1 &minus; p&lowast;
</p>
<p>))
= lnpq + ln
</p>
<p>(
1 + p
</p>
<p>&lowast;(1 &minus; p&lowast;)&minus; pq
pq
</p>
<p>)
</p>
<p>= ln(pq)+ ln
(
</p>
<p>1 &minus; ϑ
&lowast;δ
</p>
<p>pq
</p>
<p>)
,
</p>
<p>∣∣ϑ&lowast;
∣∣&lt; max(p, q);
</p>
<p>ln
</p>
<p>(
1 &minus; ϑ
</p>
<p>&lowast;δ
</p>
<p>pq
</p>
<p>)
= &minus; 2ϑ2δ
</p>
<p>pq
, |ϑ2|&lt; max(p, q),
</p>
<p>[
p&lowast;
</p>
<p>(
1 &minus; p&lowast;
</p>
<p>)]&minus;1/2 = [pq]&minus;1/2 exp
{
ϑ2δ
</p>
<p>pq
</p>
<p>}
.
</p>
<p>(5.2.9)
</p>
<p>Using representations (5.2.7)&ndash;(5.2.9) and the assertion of Theorem 5.2.2 com-
</p>
<p>pletes the proof. �
</p>
<p>One can see from the above estimates that the bounds for ϑ in the statement
</p>
<p>of Theorem 5.2.3 can be narrowed if we consider smaller deviations |p&lowast; &minus; p|&mdash;if
they, say, do not exceed the value αmin(p, q) where α &lt; 1/2.
</p>
<p>The relations for P(Sn = k) that we found are the so-called local limit theorems
for the Bernoulli scheme and their refinements.</p>
<p/>
</div>
<div class="page"><p/>
<p>114 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>5.2.3 The Local Limit Theorem for the Polynomial Distributions
</p>
<p>The basic asymptotic formula given in Theorem 5.2.1 admits a natural extension
</p>
<p>to the polynomial distribution Bnp , p = (p1, . . . , pr), when, in a sequence of inde-
pendent trials, in each of the trials one has not two but r &ge; 2 possible outcomes
A1, . . . ,Ar of which the probabilities are equal to p1, . . . , pr , respectively. Let S
</p>
<p>(j)
n
</p>
<p>be the number of occurrences of the event Aj in n trials,
</p>
<p>Sn =
(
S(1)n , . . . , S
</p>
<p>(r)
n
</p>
<p>)
, k = (k1, . . . , kr), p&lowast; =
</p>
<p>k
</p>
<p>n
,
</p>
<p>and put H(x)=
&sum;
</p>
<p>xi ln (xi/pi), x = (x1, . . . , xr). Clearly, Sn &sub;=Bnp . The following
assertion is a direct extension of Theorem 5.2.1.
</p>
<p>Theorem 5.2.4 If each of the r variables k1, . . . , kr is either zero or tends to &infin; as
n&rarr;&infin; then
</p>
<p>P(Sn = k)&sim; (2πn)(1&minus;r0)/2
(
</p>
<p>r&prod;
</p>
<p>j=1
p&lowast;j 
=0
</p>
<p>p&lowast;j
</p>
<p>)&minus;1/2
exp
</p>
<p>{
&minus;nH
</p>
<p>(
p&lowast;
</p>
<p>)}
,
</p>
<p>where r0 is the number of variables k1, . . . , kr which are not equal to zero.
</p>
<p>Proof As in the proof of Theorem 5.2.1, we will use Stirling&rsquo;s formula
</p>
<p>n! &sim;
&radic;
</p>
<p>2πne&minus;nnn
</p>
<p>as n&rarr;&infin;. Assuming without loss of generality that all kj &rarr;&infin;, j = 1, . . . , r , we
get
</p>
<p>P(Sn = k)&sim; (2π)(1&minus;r)/2
(
</p>
<p>n&prod;r
j=1 kj
</p>
<p>)1/2 r&prod;
</p>
<p>j=1
</p>
<p>(
npj
</p>
<p>kj
</p>
<p>)kj
</p>
<p>= (2πn)(1&minus;r)/2
(
</p>
<p>r&prod;
</p>
<p>j=1
p&lowast;j
</p>
<p>)&minus;1/2
exp
</p>
<p>{
n
</p>
<p>r&sum;
</p>
<p>j=1
</p>
<p>kj
</p>
<p>n
ln
</p>
<p>pjn
</p>
<p>kj
</p>
<p>}
.
</p>
<p>�
</p>
<p>5.3 The de Moivre&ndash;Laplace Theorem and Its Refinements
</p>
<p>Let a and b be two fixed numbers and ζn = (Sn &minus; np)/
&radic;
npq . Then
</p>
<p>P(a &lt; ζn &lt; b)=
&sum;
</p>
<p>a
&radic;
npq&lt;z&lt;b
</p>
<p>&radic;
npq
</p>
<p>P(Sn &minus; np = z).
</p>
<p>If, instead of P(Sn &minus; np = z), we substitute here the values ϕ(z∆)∆ (see Corol-
lary 5.2.1), we will get an integral sum
</p>
<p>&sum;
a&lt;z∆&lt;b ϕ(z∆)∆ corresponding to the
</p>
<p>integral
&int; b
a
ϕ(x)dx.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 The de Moivre&ndash;Laplace Theorem and Its Refinements 115
</p>
<p>Thus relations (5.2.4) make the equality
</p>
<p>lim
n&rarr;&infin;
</p>
<p>P(a &lt; ζn &lt; b)=
&int; b
</p>
<p>a
</p>
<p>ϕ(x)dx =Φ(b)&minus;Φ(a) (5.3.1)
</p>
<p>plausible, where Φ(x) is the normal distribution function with parameters (0,1):
</p>
<p>Φ(x)= 1&radic;
2π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
e&minus;t
</p>
<p>2/2 dt.
</p>
<p>This is the de Moivre&ndash;Laplace theorem, which is one of the so-called integral limit
theorems that describe probabilities of the form P(Sn &lt; x). In Chap. 8 we will derive
more general integral theorems from which (5.3.1) will follow as a special case.
</p>
<p>Theorem 5.2.3 makes it possible to obtain (5.3.1) together with an error bound
</p>
<p>or, in other words, with a bound for the convergence rate.
Let A and B be integers,
</p>
<p>a = A&minus; np&radic;
npq
</p>
<p>, b= B &minus; np&radic;
npq
</p>
<p>. (5.3.2)
</p>
<p>Theorem 5.3.1 Let b &gt; a, c= max(|a|, |b|), and
</p>
<p>ρ = c
3 + 3c
</p>
<p>3
∆+ ∆
</p>
<p>2
</p>
<p>6
.
</p>
<p>If ∆= 1/&radic;npq &le; 1/2 and ρ &le; 1/2 then
</p>
<p>P(A&le; Sn &lt;B)= P(a &le; ζn &lt; b)=
&int; b
</p>
<p>a
</p>
<p>ϕ(t) dt (1 + ϑ1∆c)(1 + 2ϑ2ρ), (5.3.3)
</p>
<p>where |ϑi | &le; 1, i = 1,2.
</p>
<p>This theorem shows that the left-hand side in (5.3.3) can be equivalent to Φ(b)&minus;
Φ(a) for growing a and b as well. In that case, Φ(b)&minus;Φ(a) can converge to 0, and
knowing the relative error in (5.3.1) is more convenient since its smallness enables
one to establish that of the absolute error as well, but not vice versa.
</p>
<p>Proof First we note that, for all k such that |z| = |k &minus; np| &lt; c&radic;npq , the con-
ditions of Theorem 5.2.3 will hold. Indeed, to have the inequality |p&lowast; &minus; p| &lt;
(1/2)min(p, q) it suffices that |k &minus; np| &lt; npq/2 = 1/(2∆2). This inequality will
hold if c &lt; 1/(2∆). But since ρ &le; 1/2, one has
</p>
<p>c(c2 + 3)∆
3
</p>
<p>&lt; 1/2, c∆ &lt; 1/2.
</p>
<p>Thus, for each k such that a
&radic;
npq &le; z &lt; b&radic;npq , we can make use of Theorem 5.2.3
</p>
<p>to conclude that</p>
<p/>
</div>
<div class="page"><p/>
<p>116 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>P(A&le; Sn &lt;B)
=
</p>
<p>&sum;
</p>
<p>a
&radic;
npq&le;z&lt;b&radic;npq
</p>
<p>P(Sn = k)
</p>
<p>=
&sum;
</p>
<p>a&le;z∆&lt;b
ϕ(z∆)∆
</p>
<p>[
1 +
</p>
<p>(
exp
</p>
<p>{
ϑ
</p>
<p>( |z|3∆4
3
</p>
<p>+
(
|z| + 1
</p>
<p>6
</p>
<p>)
∆2
</p>
<p>)}
&minus; 1
</p>
<p>)]
,
</p>
<p>(5.3.4)
</p>
<p>where |ϑ |&lt; 1. Since, for ρ &le; 1,
∣∣∣∣
ep &minus; 1
</p>
<p>ρ
</p>
<p>∣∣∣∣&lt; e&minus; 1 &lt; 2,
</p>
<p>the absolute value of the correction term in (5.3.4) does not exceed (substituting
</p>
<p>there z∆= c)
∣∣∣∣exp
</p>
<p>{
ϑ
</p>
<p>(
c3∆
</p>
<p>3
+ c∆+ ∆
</p>
<p>2
</p>
<p>6
</p>
<p>)}
&minus; 1
</p>
<p>∣∣∣∣&le; 2ϑ
(
c3∆
</p>
<p>3
+ c∆+ ∆
</p>
<p>2
</p>
<p>6
</p>
<p>)
= 2ϑp.
</p>
<p>Therefore
</p>
<p>P(A&le; Sn &lt;B)=
&sum;
</p>
<p>a&le;z∆&lt;b
ϕ(z∆)∆[1 + 2ϑ1ρ], (5.3.5)
</p>
<p>where |ϑ1|&lt; 1.
Now we transform the sum on the right-hand side of the last equality. To this end,
</p>
<p>note that, for any smooth function ϕ(x),
∣∣∣∣∆ϕ(x)&minus;
</p>
<p>&int; x+∆
</p>
<p>x
</p>
<p>ϕ(t) dt
</p>
<p>∣∣∣∣=
∆2
</p>
<p>2
max
</p>
<p>x&le;t&le;x+∆
</p>
<p>∣∣ϕ&prime;(t)
∣∣. (5.3.6)
</p>
<p>But for the function ϕ(x)= (2π)&minus;1/2e&minus;x2/2 one has ϕ&prime;(x)=&minus;xϕ(x) and the max-
imum value of ϕ(t) on the segment [x, x +∆], |x| &le; c, differs from the minimum
value by not more than the factor exp{c∆+∆2/2}. Therefore, for |x| &le; c, one has
by virtue of (5.3.6)
</p>
<p>∣∣∣∣∆ϕ(x)&minus;
&int; x+∆
</p>
<p>x
</p>
<p>ϕ(t) dt
</p>
<p>∣∣∣∣
</p>
<p>&le; ∆
2c
</p>
<p>2
ec∆+∆
</p>
<p>2/2 min
x&le;t&le;x+∆
</p>
<p>ϕ(t)&le; ∆c
2
ec∆+∆
</p>
<p>2/2
</p>
<p>&int; x+∆
</p>
<p>x
</p>
<p>ϕ(t) dt.
</p>
<p>Since c∆+∆2/2 &lt; 1/2 + 1/8, ec∆+∆2/2 &le; 2, we have the representation
</p>
<p>∆ϕ(x)=
&int; x+∆
</p>
<p>x
</p>
<p>ϕ(t) dt (1 + ϑ1∆c), |ϑ1|&lt; 1.
</p>
<p>Substituting this into (5.3.5) we obtain the assertion of the theorem. �
</p>
<p>Thus by Theorem 5.3.1 the difference
∣∣P(x &le; ζn &lt; y)&minus;
</p>
<p>(
Φ(y)&minus;Φ(x)
</p>
<p>)∣∣ (5.3.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 The Poisson Theorem and Its Refinements 117
</p>
<p>can be effectively, yet rather roughly, bounded from above by a quantity of the order
</p>
<p>1/
&radic;
npq if x = a, y = b (assuming that a and b are values which can be represented
</p>
<p>in the form (k &minus; np)∆, see (5.3.2)). If x and y do not belong to the mentioned
lattice with the span ∆ then the error (5.3.7) will still be of the same order since,
</p>
<p>for instance, when y varies, P(x &le; ζn &lt; y) remains constant on the semi-intervals
of the form (a + k∆,a + (k + 1)∆], while the function Φ(y) &minus; Φ(x) increases
monotonically with a bounded derivative. A similar argument holds for the left end
</p>
<p>point x. It is important to note that the error order 1/
&radic;
npq cannot be improved, for
</p>
<p>the jumps of the distribution function of ζn are just of this order of magnitude by
</p>
<p>Theorem 5.2.2.
</p>
<p>Theorem 5.3.1 enables one to use the normal approximation for P(x &le; ζn &lt; y)
in the so-called large deviations range as well, when both x and y grow in absolute
value and are of the same sign. In that case, both Φ(y)&minus;Φ(x) and the probability
to be approximated tend to zero. Therefore the approximation can be considered
</p>
<p>satisfactory only if
</p>
<p>P(x &le; ζn &lt; y)
(Φ(y)&minus;Φ(x)) &rarr; 1. (5.3.8)
</p>
<p>As Theorem 5.3.1 shows, this convergence will take place if
</p>
<p>c= max
(
|x|, |y|
</p>
<p>)
= o
</p>
<p>(
∆&minus;1/3
</p>
<p>)
</p>
<p>or, which is the same, c= o(n1/6). For more details about large deviation probabil-
ities, see Chap. 9.
</p>
<p>For larger values of c, as one could verify using Theorem 5.2.1, relation (5.3.8)
</p>
<p>will, generally speaking, not hold.
</p>
<p>In conclusion we note that since
</p>
<p>P
(
|ζn|&gt; b
</p>
<p>)
&rarr; 0
</p>
<p>as b&rarr;&infin;, it follows immediately from Theorem 5.3.1 that, for any fixed y,
lim
n&rarr;&infin;
</p>
<p>P(ζn &lt; y)=Φ(y).
</p>
<p>Later we will show that this assertion remains true under much wider assumptions,
</p>
<p>when ζn is a scaled sum of arbitrary distributed random variables having finite vari-
</p>
<p>ances.
</p>
<p>5.4 The Poisson Theorem and Its Refinements
</p>
<p>5.4.1 Quantifying the Closeness of Poisson Distributions to Those
</p>
<p>of the Sums Sn
</p>
<p>As we saw from the bounds in the last section, the de Moivre&ndash;Laplace theorem
</p>
<p>gives a good approximation to the probabilities of interest if the number npq (the
</p>
<p>variance of Sn) is large. This number will grow together with n if p and q are fixed</p>
<p/>
</div>
<div class="page"><p/>
<p>118 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>positive numbers. But what will happen in a problem where, say, p = 0.001 and
n= 1000 so that np = 1? Although n is large here, applying the de Moivre&ndash;Laplace
theorem in such a problem would be meaningless. It turns out that in this case the
</p>
<p>distribution P(Sn = k) can be well approximated by the Poisson distribution �&micro;
with an appropriate parameter value &micro; (see Sect. 5.4.2). Recall that
</p>
<p>�&micro;(B)=
&sum;
</p>
<p>0&le;k&isin;B
e&minus;&micro;
</p>
<p>&micro;k
</p>
<p>k! .
</p>
<p>Put np = &micro;.
</p>
<p>Theorem 5.4.1 For all sets B ,
</p>
<p>∣∣P(Sn &isin; B)&minus;�&micro;(B)
∣∣&le; &micro;
</p>
<p>2
</p>
<p>n
.
</p>
<p>We could prove this assertion in the same way as the local theorem, making use
</p>
<p>of the explicit formula for P(Sn = k). However, we can prove it in a simpler and
nicer way which could be called the common probability space method, or coupling
method. The method is often used in research in probability theory and consists,
in our case, of constructing on a common probability space random variables Sn
and S&lowast;n , the latter being as close to Sn as possible and distributed according to the
Poisson distribution.
</p>
<p>It is also important that the common probability space method admits, without
</p>
<p>any complications, extension to the case of non-identically distributed random vari-
ables, when the probability of getting 1 in a particular trial depends on the number of
</p>
<p>the trial. Thus we will now prove a more general assertion of which Theorem 5.4.1
</p>
<p>is a special case.
</p>
<p>Assume that we are given a sequence of independent random variables ξ1, . . . , ξn,
</p>
<p>such that ξj &sub;= Bpj . Put, as above, Sn =
&sum;n
</p>
<p>j=1 ξj . The theorem we state below is
intended for approximating the probability P(Sn = k) when pj are small and the
number &micro;=
</p>
<p>&sum;n
j=1 pj is &ldquo;comparable&rdquo; with 1.
</p>
<p>Theorem 5.4.2 For all sets B ,
</p>
<p>∣∣P(Sn &isin; B)&minus;�&micro;(B)
∣∣&le;
</p>
<p>n&sum;
</p>
<p>j=1
p2j .
</p>
<p>To prove this theorem we will need an important &ldquo;stability&rdquo; property of the Pois-
</p>
<p>son distribution.
</p>
<p>Lemma 5.4.1 If η1 and η2 are independent, η2 &sub;=�&micro;1 and η2 &sub;=�&micro;2 , then3
</p>
<p>η1 + η2 &sub;=�&micro;1+&micro;2 .
</p>
<p>3This fact will also easily follow from the properties of characteristic functions dealt with in
</p>
<p>Chap. 7.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 The Poisson Theorem and Its Refinements 119
</p>
<p>Proof By the total probability formula,
</p>
<p>P(η1 + η2 = k)=
k&sum;
</p>
<p>j=0
P(η1 = j)P(η2 = k&minus; j)
</p>
<p>=
k&sum;
</p>
<p>j=0
</p>
<p>&micro;
j
</p>
<p>1e
&minus;&micro;1
</p>
<p>j ! &middot;
&micro;
k&minus;j
2 e
</p>
<p>&minus;&micro;2
</p>
<p>(k &minus; j)! =
1
</p>
<p>k!e
&minus;(&micro;1+&micro;2)
</p>
<p>k&sum;
</p>
<p>j=0
</p>
<p>(
k
</p>
<p>j
</p>
<p>)
&micro;
j
</p>
<p>1&micro;
k&minus;j
2
</p>
<p>= (&micro;1 +&micro;2)
ke&minus;(&micro;1+&micro;2)
</p>
<p>k! . �
</p>
<p>Proof of Theorem 5.4.2 Let ω1, . . . ,ωn be independent random variables, each be-
ing the identity function (ξ(ωk) = ωk) on the unit interval with the uniform dis-
tribution. We can assume that the vector ω = (ω1, . . . ,ωn) is given as the identity
function on the unit n-dimensional cube Ω with the uniform distribution.
</p>
<p>Now construct the random variables ξj and ξ
&lowast;
j on Ω as follows:
</p>
<p>ξj (ω)=
{
</p>
<p>0 if ωj &lt; 1 &minus; pj ,
1 if ωj &ge; 1 &minus; pj ,
</p>
<p>ξ&lowast;j (ω)=
{
</p>
<p>0 if ωj &lt; e
&minus;pj ,
</p>
<p>k &ge; 1 if ωj &isin; [πk&minus;1,πk),
</p>
<p>where πk =
&sum;
</p>
<p>m&le;k e
&minus;pj (pj )
</p>
<p>m
</p>
<p>m! , k = 0,1, . . . .
It is evident that the ξj (ω) are independent and ξj (ω) &sub;= Bpj ; ξ&lowast;j (ω) are also
</p>
<p>jointly independent with ξ&lowast;j (ω)&sub;=�pj . Now note that since 1 &minus; pj &le; e&minus;pj one has
ξj (ω) 
= ξ&lowast;j (ω) only if ωj &isin; [1 &minus; pj , e&minus;pj ) or ωj &isin; [e&minus;pj + pje&minus;pj ,1]. Hence
</p>
<p>P
(
ξj 
= ξ jj
</p>
<p>)
=
(
e&minus;pj &minus; 1 + pj
</p>
<p>)
+
(
1 &minus; e&minus;pj &minus; pj e&minus;pj
</p>
<p>)
= pj
</p>
<p>(
1 &minus; e&minus;pj
</p>
<p>)
&le; p2j
</p>
<p>and
</p>
<p>P
(
Sn 
= S&lowast;n
</p>
<p>)
&le; P
</p>
<p>(⋃
</p>
<p>j
</p>
<p>∣∣ξj 
= ξ&lowast;j
∣∣
)
&le;
&sum;
</p>
<p>p2j ,
</p>
<p>where S&lowast;n =
&sum;n
</p>
<p>j=1 ξ
&lowast;
j &sub;=�&micro;.
</p>
<p>Now we can write
</p>
<p>P(Sn &isin; B)= P
(
Sn &isin; B,Sn = S&lowast;n
</p>
<p>)
+ P
</p>
<p>(
Sn &isin; B,Sn 
= S&lowast;n
</p>
<p>)
</p>
<p>= P
(
S&lowast;n &isin; B
</p>
<p>)
&minus; P
</p>
<p>(
S&lowast;n &isin; B,Sn 
= S&lowast;n
</p>
<p>)
+ P
</p>
<p>(
Sn &isin; B,Sn 
= S&lowast;n
</p>
<p>)
,
</p>
<p>so that
∣∣P(Sn &isin; B)&minus; P
</p>
<p>(
S&lowast;n &isin; B
</p>
<p>)∣∣
&le;
∣∣P
(
S&lowast;n &isin; B,Sn 
= S&lowast;n
</p>
<p>)
&minus; P
</p>
<p>(
Sn &isin; B,Sn 
= S&lowast;n
</p>
<p>)∣∣&le; P
(
Sn 
= S&lowast;n
</p>
<p>)
. (5.4.1)
</p>
<p>The assertion of the theorem follows from this in an obvious way. �
</p>
<p>Remark 5.4.1 One can give other common probability space constructions as well.
One of them will be used now to show that there exists a better Poisson approxima-
</p>
<p>tion to the distribution of Sn.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>Namely, let ξ&lowast;j (ω) be independent random variables distributed according to the
Poisson laws with parameters rj =&minus; ln(1 &minus; pj )&ge; pj , so that P(ξ&lowast;j = 0)= e&minus;rj =
1 &minus; pj . Then ξj (ω)= min{1, ξ&lowast;j (ω)} &sub;=Bpj and, moreover,
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>j=1
</p>
<p>{
ξj (ω) 
= ξ&lowast;j (ω)
</p>
<p>}
)
&le;
</p>
<p>n&sum;
</p>
<p>j=1
P
(
ξ&lowast;j (ω)&ge; 2
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>(
1 &minus; e&minus;rj &minus; rj e&minus;rj
</p>
<p>)
.
</p>
<p>But for r =&minus; ln(1 &minus; p) one has the inequality
</p>
<p>1 &minus; e&minus;r &minus; re&minus;r = p+ (1 &minus; p) ln(1 &minus; p)&le; p+ (1 &minus; p)
(
&minus;p&minus; p
</p>
<p>2
</p>
<p>2
</p>
<p>)
</p>
<p>= p
2
</p>
<p>2
(1 + p).
</p>
<p>Hence for the new Poisson approximation we have
</p>
<p>P
(
S&lowast;n 
= Sn
</p>
<p>)
&le; 1
</p>
<p>2
</p>
<p>n&sum;
</p>
<p>j=1
p2j (1 + pj ).
</p>
<p>Putting λ=&minus;
&sum;n
</p>
<p>j=1 ln(1 &minus; pj )&ge;
&sum;n
</p>
<p>j=1 pj , the same argument as above will lead
to the bound
</p>
<p>sup
B
</p>
<p>∣∣P(Sn &isin; B)&minus;�λ(B)
∣∣&le; 1
</p>
<p>2
</p>
<p>n&sum;
</p>
<p>j=1
p2j (1 + pj ).
</p>
<p>This bound of the rate of approximation given by the Poisson distribution with a
</p>
<p>&ldquo;slightly shifted&rdquo; parameter is better than that obtained in Theorem 5.4.2. Moreover,
</p>
<p>one could note that, in the new construction, ξj &le; ξ&lowast;j , Sn &le; S&lowast;n , and consequently
</p>
<p>P(Sn &ge; k)&le; P
(
S&lowast;n &ge; k
</p>
<p>)
=�λ
</p>
<p>(
[k,&infin;)
</p>
<p>)
.
</p>
<p>5.4.2 The Triangular Array Scheme. The Poisson Theorem
</p>
<p>Now we will return back to the case of identically distributed ξk . To obtain from
</p>
<p>Theorem 5.4.2 a limit theorem of the type similar to that of the de Moivre&ndash;Laplace
</p>
<p>theorem (see (5.3.1)), one needs a somewhat different setup. In fact, to ensure
</p>
<p>that np remains bounded as n increases, p = P(ξk = 1) needs to converge to zero
which cannot be the case when we consider a fixed sequence of random variables
</p>
<p>ξ1, ξ2, . . . .
</p>
<p>We introduce a sequence of rows (of growing length) of random variables:
</p>
<p>ξ
(1)
1 ;
ξ
(2)
1 , ξ
</p>
<p>(2)
2 ;
</p>
<p>ξ
(3)
1 , ξ
</p>
<p>(3)
2 , ξ
</p>
<p>(1)
1 ;
</p>
<p>. . . . . . . . . . . . . . .
</p>
<p>ξ
(n)
1 , ξ
</p>
<p>(n)
2 , ξ
</p>
<p>(n)
3 , . . . , ξ
</p>
<p>(n)
n .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 The Poisson Theorem and Its Refinements 121
</p>
<p>This is the so-called triangular array scheme. The superscript denotes the row num-
ber, while the subscript denotes the number of the variable in the row.
</p>
<p>Assume that the variables ξ
(n)
k in the n-th row are independent and ξ
</p>
<p>(n)
k &sub;= Bpn ,
</p>
<p>k = 1, . . . , n.
</p>
<p>Corollary 5.4.1 (The Poisson theorem) If npn &rarr; &micro;&gt; 0 as n&rarr;&infin; then, for each
fixed k,
</p>
<p>P(Sn = k)&rarr;�&micro;
(
{k}
</p>
<p>)
, (5.4.2)
</p>
<p>where Sn = ξ (n)1 + &middot; &middot; &middot; + ξ
(n)
n .
</p>
<p>Proof This assertion is an immediate corollary of Theorem 5.4.1. It can also be
obtained directly, by noting that it follows from the equality
</p>
<p>P(Sn = k)=
(
n
</p>
<p>k
</p>
<p>)
pk(1 &minus; p)n&minus;k
</p>
<p>that
</p>
<p>P(Sn = 0)= en ln(1&minus;p) &sim; e&minus;&micro;,
P(Sn = k + 1)
P(Sn = k)
</p>
<p>= n&minus; k
k + 1
</p>
<p>p
</p>
<p>1 &minus; p &sim;
&micro;
</p>
<p>k + 1 . �
</p>
<p>Theorem 5.4.2 implies an analogue of the Poisson theorem in a more general
</p>
<p>case as well, when the ξ
(n)
j are not necessarily identically distributed
</p>
<p>4 and can take
</p>
<p>values different from 0 and 1.
</p>
<p>Corollary 5.4.2 Assume that pjn = P(ξ (n)j = 1) depend on n and j so that
</p>
<p>max
j
</p>
<p>pjn &rarr; 0,
n&sum;
</p>
<p>j=1
pjn &rarr; &micro;&gt; 0, P
</p>
<p>(
ξ
(n)
j = 0
</p>
<p>)
= 1 &minus; pjn + o(pjn).
</p>
<p>Then (5.4.2) holds.
</p>
<p>Proof To prove the corollary, one has to use Theorem 5.4.2 and the fact that
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>j=1
</p>
<p>{
ξ
(n)
j 
= 0, ξ
</p>
<p>(n)
j 
= 1
</p>
<p>}
)
&le;
</p>
<p>n&sum;
</p>
<p>j=1
o(pjn)= o(1),
</p>
<p>which means that, with probability tending to 1, all the variables ξ
(n)
j assume the
</p>
<p>values 0 and 1 only. �
</p>
<p>One can clearly obtain from Theorems 5.4.1 and 5.4.2 somewhat stronger asser-
</p>
<p>tions than the above. In particular,
</p>
<p>sup
B
</p>
<p>∣∣P(Sn &isin; B)&minus;�&micro;(B)
∣∣&rarr; 0 as n&rarr;&infin;.
</p>
<p>4An extension of the de Moivre&ndash;Laplace theorem to the case of non-identically distributed random
</p>
<p>variables is contained in the central limit theorem from Sect. 8.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>122 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>Note that under the assumptions of Theorem 5.4.1 this convergence will also
</p>
<p>take place in the case where np &rarr;&infin; but only if np2 &rarr; 0. At the same time, the
refinement of the de Moivre&ndash;Laplace theorem from Sect. 5.3 shows that the normal
</p>
<p>approximation for the distribution of Sn holds if np&rarr;&infin; (for simplicity we assume
that p &le; q so that npq &ge; 1
</p>
<p>2
np&rarr;&infin;).
</p>
<p>Thus there exist sequences p &isin; {p : np &rarr; &infin;, np2 &rarr; 0} such that both the
normal and the Poisson approximations are valid. In other words, the domains of
applicability of the normal and Poisson approximations overlap.
</p>
<p>We see further from Theorem 5.4.1 that the convergence rate in Corollary 5.4.1
</p>
<p>is determined by a quantity of the order of n&minus;1. Since, as n&rarr;&infin;,
</p>
<p>P(Sn = 0)&minus;�&micro;
(
{0}
</p>
<p>)
= en ln(1&minus;p) &minus; e&minus;&micro; &sim; &micro;
</p>
<p>2
</p>
<p>2π
e&minus;&micro;,
</p>
<p>this estimate cannot be substantially improved. However, for large k (in the large
</p>
<p>deviations range, say) such an estimate for the difference
</p>
<p>P(Sn = k)&minus;�&micro;
(
{k}
</p>
<p>)
</p>
<p>becomes rough. (This is because, in (5.4.1), we neglected not only the different signs
</p>
<p>of the correction terms but also the rare events {Sn = k} and {S&lowast;n = k} that appear in
the arguments of the probabilities.) Hence we see, as in Sect. 5.4, the necessity for
</p>
<p>having approximations of which both absolute and relative errors are small.
Now we will show that the asymptotic equivalence relations
</p>
<p>P(Sn = k)&sim;�&micro;
(
{k}
</p>
<p>)
</p>
<p>remain valid when k and &micro; grow (along with n) in such a way that
</p>
<p>k = o
(
n2/3
</p>
<p>)
, &micro;= o
</p>
<p>(
n2/3
</p>
<p>)
, |k &minus;&micro;| = o(
</p>
<p>&radic;
n ).
</p>
<p>Proof Indeed,
</p>
<p>P(Sn = k)=
(
n
</p>
<p>k
</p>
<p>)
pk(1 &minus; p)n&minus;k = n(n&minus; 1) &middot; &middot; &middot; (n&minus; k+ 1)
</p>
<p>k! p
k(1 &minus; p)n&minus;k
</p>
<p>= (nk)
k
</p>
<p>k! e
&minus;pn
</p>
<p>(
1 &minus; 1
</p>
<p>n
</p>
<p>)
&middot; &middot; &middot;
</p>
<p>(
1 &minus; k &minus; 1
</p>
<p>n
</p>
<p>)
(1 &minus; p)n&minus;kepn
</p>
<p>=�&micro;
(
{k}
</p>
<p>)
eε(k,n).
</p>
<p>Thus we have to prove that, for values of k and &micro; from the indicated range,
</p>
<p>ε(k,n) := ln
[(
</p>
<p>1 &minus; 1
n
</p>
<p>)
&middot; &middot; &middot;
</p>
<p>(
1 &minus; k&minus; 1
</p>
<p>n
</p>
<p>)
(1 &minus; p)n&minus;kepn
</p>
<p>]
= o(1). (5.4.3)
</p>
<p>We will obtain this relation together with the form of the correction term. Namely,
</p>
<p>we will show that
</p>
<p>ε(k,n)= k &minus; (k &minus;&micro;)
2
</p>
<p>2n
+O
</p>
<p>(
k3 +&micro;3
</p>
<p>n2
</p>
<p>)
, (5.4.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 The Poisson Theorem and Its Refinements 123
</p>
<p>and hence
</p>
<p>P(Sn = k)=
(
</p>
<p>1 + k &minus; (k &minus;&micro;)
2
</p>
<p>2n
+O
</p>
<p>(
k3 +&micro;3
</p>
<p>n2
</p>
<p>))
�&micro;
</p>
<p>(
{k}
</p>
<p>)
.
</p>
<p>We make use of the fact that, as α&rarr; 0,
</p>
<p>ln(1 &minus; α)=&minus;α &minus; α
2
</p>
<p>2
+O
</p>
<p>(
α3
</p>
<p>)
.
</p>
<p>Then relations (5.4.3) and (5.4.4) will follow from the equalities
</p>
<p>k&minus;1&sum;
</p>
<p>j=1
ln
</p>
<p>(
1 &minus; j
</p>
<p>n
</p>
<p>)
=&minus;
</p>
<p>k&minus;1&sum;
</p>
<p>j=1
</p>
<p>j
</p>
<p>n
+O
</p>
<p>(
k3
</p>
<p>n2
</p>
<p>)
=&minus;k(k &minus; 1)
</p>
<p>2n
+O
</p>
<p>(
k3
</p>
<p>n2
</p>
<p>)
,
</p>
<p>(n&minus; k) ln(1 &minus; p)+ pn= (n&minus; k)
(
&minus;p&minus; p
</p>
<p>2
</p>
<p>2
+O
</p>
<p>(
p3
</p>
<p>))
+ pn
</p>
<p>=&minus;&micro;
2
</p>
<p>2n
+ k&micro;
</p>
<p>n
+O
</p>
<p>(
&micro;3
</p>
<p>n2
</p>
<p>)
. �
</p>
<p>In conclusion we note that the approximate Poisson formula
</p>
<p>P(Sn = k)&asymp;
&micro;k
</p>
<p>k! e
&minus;&micro;
</p>
<p>is widely used in various applications and has, as experience and the above estimates
</p>
<p>show, a rather high accuracy even for moderate values of n.
</p>
<p>Now we consider several examples of the use of the de Moivre&ndash;Laplace and
</p>
<p>Poisson theorems for approximate computations.
</p>
<p>Example 5.4.1 Suppose we are given 104 packets of grain. It is known that there are
5000 tagged grains in the packets. What is the probability that, in a particular fixed
</p>
<p>packet, there is at least one tagged grain? We can assume that the tagged grains are
</p>
<p>distributed to packets at random. Then the probability that a particular tagged grain
</p>
<p>will be in the chosen packet is p = 10&minus;4. Since there are 5000 such grains, this
will be the number of trials, i.e. n= 5000. Define a random variable ξk as follows:
ξk = 1 if the k-th grain is in the chosen packet, and ξk = 0 otherwise. Then
</p>
<p>S5000 =
5000&sum;
</p>
<p>k=1
ξk
</p>
<p>will be the number of tagged grains in our packet. By Theorem 5.4.1, P(S5000 =
0) &asymp; e&minus;np = e&minus;0.5 so that the desired probability is approximately equal to 1 &minus;
e&minus;0.5. The accuracy of this relation turns out to be rather high (by Theorem 5.4.1,
the error does not exceed 2&minus;1 &times; 10&minus;4). If we used the Poisson theorem instead of
Theorem 5.4.1, we would have to imagine a triangular array of Bernoulli random
</p>
<p>variables, our ξk constituting the 5000-th row of the array. Moreover, we would
</p>
<p>assume that, for the n-th row, one has npn = 0.5. Thus the conditions of the Poisson
theorem would be met and we could make use of the limit theorem to find the
</p>
<p>approximate equality we have already obtained.</p>
<p/>
</div>
<div class="page"><p/>
<p>124 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>Example 5.4.2 A similar argument can be used in the following problem. There are
n dangerous bacteria in a reservoir of capacity V from which we take a sample of
</p>
<p>volume v ≪ V . What is the probability that we will find the bacteria in the test
sample?
</p>
<p>One usually assumes that the probability p that any given bacterium will be in the
</p>
<p>test sample is equal to the ratio v/V . Moreover, it is also assumed that the presence
</p>
<p>of a given bacterium in the sample does not depend on whether the remaining n&minus; 1
bacteria are in the test sample or not. In other words, one usually postulates that the
</p>
<p>mechanism of bacterial transfer into the test sample is equivalent to a sequence of n
</p>
<p>independent trials with &ldquo;success&rdquo; probability equal to p = v/V in each trial.
Introducing random variables ξk as above, we obtain a description of the number
</p>
<p>of bacteria in the test sample by the sum Sn =
&sum;n
</p>
<p>k=1 ξk in the Bernoulli scheme.
If nv is comparable in magnitude with V then by the Poisson theorem the desired
</p>
<p>probability will be equal to
</p>
<p>P(Sn &gt; 0)&asymp; 1 &minus; e&minus;nv/V .
</p>
<p>Similar models are also used to describe the number of visible stars in a certain
</p>
<p>part of the sky far away from the Milky Way. Namely, it is assumed that if there are
</p>
<p>n visible stars in a region R then the probability that there are k visible stars in a
</p>
<p>subregion r &sub;R is
(
n
</p>
<p>k
</p>
<p>)
pk(1 &minus; p)k,
</p>
<p>where p is equal to the ratio S(r)/S(R) of the areas of the regions r and R respec-
</p>
<p>tively.
</p>
<p>Example 5.4.3 Suppose that the probability that a newborn baby is a boy is constant
and equals 0.512 (see Sect. 3.4.1).
</p>
<p>Consider a group of 104 newborn babies and assume that it corresponds to a
</p>
<p>series of 104 independent trials of which the outcomes are the events that either a
</p>
<p>boy or girl is born. What is the probability that the number of boys among these
</p>
<p>newborn babies will be greater than the number of girls by at least 200?
</p>
<p>Define random variables as follows: ξk = 1 if the k-th baby is a boy and ξk = 0
otherwise. Then Sn =
</p>
<p>&sum;104
k=1 ξk is the number of boys in the group. The quantity
</p>
<p>npq &sim; 2.5 &times; 103 is rather large here, hence applying the integral limit (de Moivre&ndash;
Laplace) theorem we obtain for the desired probability the value
</p>
<p>P(Sn &ge; 5100)= 1 &minus; P
(
Sn &minus; np&radic;
</p>
<p>npq
&lt;
</p>
<p>5100 &minus; 5120&radic;
2500
</p>
<p>)
</p>
<p>&asymp; 1 &minus;Φ(&minus;20/50)= 1 &minus;Φ(&minus;0.4)&asymp; 0.66.
</p>
<p>To find the numerical values of Φ(x) one usually makes use of suitable statistical
</p>
<p>computer packages or calculators.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Inequalities for Large Deviation Probabilities in the Bernoulli Scheme 125
</p>
<p>In our example, ∆= 1/&radic;npq &asymp; 1/50, and a satisfactory approximation by the de
Moivre&ndash;Laplace formula will certainly be ensured (see Theorem 5.3.1) for c &le; 2.5.
</p>
<p>If, however, we have to estimate the probability that the proportion of boys ex-
</p>
<p>ceeds 0.55, we will be dealing with large deviation probabilities when to estimate
</p>
<p>P(Sn &gt; 5500) one would rather use the approximate relation obtained in Sect. 1.3
</p>
<p>by virtue of which (k = 0.45n, q = 0.488) one has
</p>
<p>P(Sn &gt; 5500)&asymp;
(n+ 1 &minus; k)q
(n+ 1)q &minus; kP(Sn = 5500).
</p>
<p>Applying Theorem 5.2.1 we find that
</p>
<p>P(Sn &gt; 5500)&asymp;
0.55q
</p>
<p>q &minus; 0.45
1&radic;
</p>
<p>2πn0.25
e&minus;nH(0.55) &le; 1
</p>
<p>5
e&minus;25 &lt; 10&minus;11.
</p>
<p>Thus if we assume for a moment that 100 million babies are born on this planet
</p>
<p>each year and group them into batches of 10 thousand, then, to observe a group in
</p>
<p>which the proportion of boys exceeds the mean value by just 3.8 % we will have to
</p>
<p>wait, on average, 10 million years (see Example 4.1.1 in Sect. 4.1).
</p>
<p>It is clear that the normal approximation can be used for numerical evaluation of
</p>
<p>probabilities for the problems from Example 5.4.3 provided that the values of np
</p>
<p>are large.
</p>
<p>5.5 Inequalities for Large Deviation Probabilities in the
</p>
<p>Bernoulli Scheme
</p>
<p>In conclusion of the present chapter we will derive several useful inequalities for the
</p>
<p>Bernoulli scheme. In Sect. 5.2 we introduced the function
</p>
<p>H(x)= x ln x
p
+ (1 &minus; x) ln 1 &minus; x
</p>
<p>1 &minus; p ,
</p>
<p>which plays an important role in Theorems 5.2.1 and 5.2.2 on the asymptotic be-
</p>
<p>haviour of the probability P(Sn = k). We also considered there the basic properties
of this function.
</p>
<p>Theorem 5.5.1 For z&ge; 0,
</p>
<p>P(Sn &minus; np &ge; z)&le; exp
{
&minus;nH(p+ z/n)
</p>
<p>}
,
</p>
<p>P(Sn &minus; np &le;&minus;z)&le; exp
{
&minus;nH(p&minus; z/n)
</p>
<p>}
.
</p>
<p>(5.5.1)
</p>
<p>Moreover, for all p,
</p>
<p>H(p+ x)&ge; 2x2, (5.5.2)
</p>
<p>so that each of the probabilities in (5.5.1) does not exceed exp{&minus;2z2/n} for any p.</p>
<p/>
</div>
<div class="page"><p/>
<p>126 5 Sequences of Independent Trials with Two Outcomes
</p>
<p>To compare it with assertion (5.2.2) of Theorem 5.2.1, the first inequality from
</p>
<p>Theorem 5.5.1 can be re-written in the form
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>n
&ge; p&lowast;
</p>
<p>)
&le; exp
</p>
<p>{
&minus;nH
</p>
<p>(
p&lowast;
</p>
<p>)}
.
</p>
<p>The inequalities (5.5.1) are close, to some extent, to the de Moivre&ndash;Laplace theorem
</p>
<p>since, for z= o(n2/3),
</p>
<p>&minus;nH
(
p+ z
</p>
<p>n
</p>
<p>)
=&minus; z
</p>
<p>2
</p>
<p>2npq
+ o(1).
</p>
<p>The last assertion, together with (5.5.2), can be interpreted as follows: deviating by
</p>
<p>z or more from the mean value np has the maximum probability when p = 1/2.
If z/
</p>
<p>&radic;
n&rarr;&infin;, then both probabilities in (5.5.1) converge to zero as n&rarr;&infin; for
</p>
<p>they correspond to large deviations of the sum Sn from the mean np. As we have
</p>
<p>already said, they are called large deviation probabilities.
</p>
<p>Proof of Theorem 5.5.1 In Corollary 4.7.2 of the previous chapter we established
the inequality
</p>
<p>P(ξ &ge; x)&le; e&minus;λxEeλξ .
Applying it to the sum Sn we get
</p>
<p>P(Sn &ge; np+ z)&le; e&minus;λ(np+z)EeλSn .
Since EeλSn =
</p>
<p>&prod;n
k=1 Ee
</p>
<p>λξk and the random variables eλξk are independent,
</p>
<p>EeλSn =
n&prod;
</p>
<p>k=1
Eeλξk =
</p>
<p>(
peλ + q
</p>
<p>)n =
(
1 + p
</p>
<p>(
eλ &minus; 1
</p>
<p>))n
,
</p>
<p>P(Sn &ge; np+ z)&le;
[(
</p>
<p>1 + p
(
eλ &minus; 1
</p>
<p>))
e&minus;λ(p+α)
</p>
<p>]n
, α = z/n.
</p>
<p>The expression in brackets is equal to
</p>
<p>Ee&minus;λ[ξk&minus;(p+α)] = peλ(1&minus;p&minus;α) + (1 &minus; p)e&minus;λ(p+α).
Therefore, being the sum of two convex functions, it is a convex function of λ. The
</p>
<p>equation for the minimum point λ(α) of the function has the form
</p>
<p>&minus;(p&minus; α)
(
1 + p
</p>
<p>(
eλ &minus; 1
</p>
<p>))
+ peλ = 0,
</p>
<p>from which we find that
</p>
<p>eλ(α) = (p+ α)q
p(q &minus; α),
</p>
<p>(
1 + p
</p>
<p>(
eλ(α) &minus; 1
</p>
<p>))
e&minus;λ(α)(p+α) = q
</p>
<p>q &minus; α
</p>
<p>[
p(q &minus; α)
(p+ α)q
</p>
<p>]p+α
</p>
<p>= p
p+αqq&minus;α
</p>
<p>(p+ α)p+α(q &minus; α)q&minus;α
</p>
<p>= exp
{
&minus;(p+ α) ln p+ α
</p>
<p>p
&minus; (q &minus; α) ln q &minus; α
</p>
<p>q
</p>
<p>}
</p>
<p>= exp
{
&minus;H(p+ α)
</p>
<p>}
.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Inequalities for Large Deviation Probabilities in the Bernoulli Scheme 127
</p>
<p>The first of the inequalities (5.5.1) is proved. The second inequality follows from
</p>
<p>the first if we consider the latter as the inequality for the number of zeros.
</p>
<p>It follows further from (5.2.1) that H(p)=H &prime;(p)= 0 and H &prime;&prime;(x)= 1/x(1 &minus; x).
Since the function x(1 &minus; x) attains its maximum value on the interval [0,1] at the
point x = 1/2, one has H &prime;&prime;(x)&ge; 4 and hence
</p>
<p>H(p+ α)&ge; α
2
</p>
<p>2
&middot; 4 = 2α2. �
</p>
<p>For analogues of Theorem 5.5.1 for sums of arbitrary random variables, see
</p>
<p>Chap. 9 and Appendix 8. Example 9.1.2 shows that the function H(α) is the so-
</p>
<p>called deviation function for the Bernoulli scheme. This function is important in
describing large deviation probabilities.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>On Convergence of Random Variables
and Distributions
</p>
<p>Abstract In this chapter, several different types of convergence used in Probability
</p>
<p>Theory are defined and relationships between them are elucidated. Section 6.1 deals
</p>
<p>with convergence in probability and convergence with probability one (the almost
</p>
<p>sure convergence), presenting some criteria for them and, in particular, discussing
</p>
<p>the concept of Cauchy sequences (in probability and almost surely). Then the conti-
</p>
<p>nuity theorem is established (convergence of functions of random variables) and the
</p>
<p>concept of uniform integrability is introduced and discussed, together with its con-
</p>
<p>sequences (in particular, for convergence in mean of suitable orders). Section 6.2
</p>
<p>contains an extensive discussion of weak convergence of distributions. The chap-
</p>
<p>ter ends with Sect. 6.3 presenting criteria for weak convergence of distributions,
</p>
<p>including the concept of distribution determining classes of functions and that of
</p>
<p>tightness.
</p>
<p>6.1 Convergence of Random Variables
</p>
<p>In previous chapters we have already encountered several assertions which dealt
</p>
<p>with convergence, in some sense, of the distributions of random variables or of the
</p>
<p>random variables themselves. Now we will give definitions of different types of
</p>
<p>convergence and elucidate the relationships between them.
</p>
<p>6.1.1 Types of Convergence
</p>
<p>Let a sequence of random variables {ξn} and a random variable ξ be given on a prob-
ability space 〈Ω,F,P〉.
</p>
<p>Definition 6.1.1 The sequence {ξn} converges in probability1 to ξ if, for any ε &gt; 0,
</p>
<p>P
(
|ξn &minus; ξ |&gt; ε
</p>
<p>)
&rarr; 0 as n&rarr;&infin;.
</p>
<p>1In the set-theoretic terminology, convergence in probability means convergence in measure.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_6, &copy; Springer-Verlag London 2013
</p>
<p>129</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_6">http://dx.doi.org/10.1007/978-1-4471-5201-9_6</a></div>
</div>
<div class="page"><p/>
<p>130 6 On Convergence of Random Variables and Distributions
</p>
<p>One writes this as
</p>
<p>Sn
p&rarr; ξ as n&rarr;&infin;.
</p>
<p>In this notation, the assertion of the law of large numbers for the Bernoulli
</p>
<p>scheme could be written as
</p>
<p>Sn
</p>
<p>n
</p>
<p>p&rarr; p,
</p>
<p>since Sn/n can be considered as a sequence of random variables given on a common
</p>
<p>probability space.
</p>
<p>Definition 6.1.2 We will say that the sequence ξn converges to ξ with probability 1
</p>
<p>(or almost surely: ξn &rarr; ξ a.s., ξn
a.s.&minus;&rarr; ξ ), if ξn(ω)&rarr; ξ(ω) as n&rarr;&infin; for all ω &isin;Ω
</p>
<p>except for ω from a set N &sub;Ω of null probability: P(N)= 0. This convergence can
also be called convergence almost everywhere (a.e.) with respect to the measure P.
</p>
<p>Convergence ξn
a.s.&minus;&rarr; ξ implies convergence ξn
</p>
<p>p&rarr; ξ . Indeed, if we assume that
the convergence in probability does not take place then there exist ε &gt; 0, δ &gt; 0,
</p>
<p>and a sequence nk such that, for the sequence of events Ak = {|ξnk &minus; ξ | &gt; ε},
we have P(Ak) &ge; δ for all k. Let B consist of all elementary events belonging to
infinitely many Ak , i.e. B =
</p>
<p>⋂&infin;
m=1
</p>
<p>⋃&infin;
k=mAk . Then, clearly for ω &isin; B , the con-
</p>
<p>vergence ξn(ω)&rarr; ξ(ω) is impossible. But B =
⋂&infin;
</p>
<p>m=1 Bm, where Bm =
⋃
</p>
<p>k&ge;mAk
are decreasing events (Bm+1 &sub; Bm), P(Bm) &ge; P(Anm) &ge; δ and, by the continuity
axiom, P(Bm) &rarr; P(B) as m &rarr; &infin;. Therefore P(B) &ge; δ and a.s. convergence is
impossible. The obtained contradiction proves the desired statement. �
</p>
<p>The converse assertion, that convergence in probability implies a.s. convergence,
</p>
<p>is, generally speaking, not true, as we will see below. However in one important
</p>
<p>special case such a converse holds true.
</p>
<p>Theorem 6.1.1 If ξn is monotonically increasing or decreasing then convergence
</p>
<p>ξn
p&rarr; ξ implies that ξn
</p>
<p>a.s.&minus;&rarr; ξ .
</p>
<p>Proof Assume, without loss of generality, that ξ &equiv; 0, ξn &ge; 0, ξn &darr; and ξn
p&rarr; ξ . If
</p>
<p>convergence ξn
a.s.&minus;&rarr; ξ did not hold, there would exist an ε &gt; 0 and a set A with
</p>
<p>P(A) &gt; δ &gt; 0 such that supk&ge;n ξk &gt; ε for ω &isin; A and all n. But supk&ge;n ξk = ξn and
hence we have
</p>
<p>P(ξn &gt; ε)&ge; P(A) &gt; δ &gt; 0
</p>
<p>for all n, which contradicts the assumed convergence ξn
p&rarr; 0. �
</p>
<p>Thus convergence in probability is determined by the behaviour of the numerical
</p>
<p>sequence P(|ξn &minus; ξ |&gt; ε). Is it possible to characterise convergence with probabil-
ity 1 in a similar way? Set ζn := supk&ge;n |ξn &minus; ξ |.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Convergence of Random Variables 131
</p>
<p>Corollary 6.1.1 ξn
a.s.&minus;&rarr; ξ if and only if ζn
</p>
<p>p&rarr; 0, or, which is the same, when, for
any ε &gt; 0,
</p>
<p>P
(
</p>
<p>sup
k&ge;n
</p>
<p>|ξk &minus; ξ |&gt; ε
)
&rarr; 0 as n&rarr;&infin;. (6.1.1)
</p>
<p>Proof Clearly ξn &rarr; ξ a.s. if and only if ζn &rarr; 0 a.s. But the sequence ζn decreases
monotonically and it remains to make use of Theorem 6.1.1, which implies that
</p>
<p>ζn
p&rarr; 0 if and only if ζn
</p>
<p>a.s.&minus;&rarr; 0. The corollary is proved. �
</p>
<p>In the above argument, the random variables ξn and ξ could be improper, where
</p>
<p>the random variables ξn and ξ are only defined on a set B and P(B) &isin; (0,1). (These
random variables can take infinite values on Ω \B .) In this case, all the considera-
tions concerning convergence are carried out on the set B &sub;Ω only.
</p>
<p>In the introduced terminology, the assertion of the strong law of large numbers
</p>
<p>for the Bernoulli scheme (Theorem 5.1.2) can be stated, by virtue of (6.1.1), as
</p>
<p>convergence Sn/n&rarr; p with probability 1.
We have already noted that convergence almost surely implies convergence in
</p>
<p>probability. Now we will give an example showing that the converse assertion is,
</p>
<p>generally speaking, not true. Let 〈Ω,F,P〉 be the unit circle with the σ -algebra of
Borel sets and uniform distribution. Put ξ(ω)&equiv; 1, ξn(ω)= 2 on the arc [r(n), r(n)+
1/n] and ξn(ω)= 1 outside the arc. Here r(n)=
</p>
<p>&sum;n
k=1
</p>
<p>1
k
</p>
<p>. It is obvious that ξn
p&rarr; ξ .
</p>
<p>At the same time, r(n)&rarr;&infin; as n&rarr;&infin;, and the set on which ξn converges to ξ is
empty (we can find no ω for which ξn(ω)&rarr; ξ(ω)).
</p>
<p>However, if P(|ξn &minus; ξ | &gt; ε) decreases as n&rarr;&infin; sufficiently fast, then conver-
gence in probability will also become a.s. convergence. In particular, relation (6.1.1)
</p>
<p>gives the following sufficient condition for convergence with probability 1.
</p>
<p>Theorem 6.1.2 If the series
&sum;&infin;
</p>
<p>k=1 P(|ξn &minus; ξ | &gt; ε) converges for any ε &gt; 0, then
ξn &rarr; ξ a.s.
</p>
<p>Proof This assertion is obvious, for
</p>
<p>P
</p>
<p>(⋃
</p>
<p>k&ge;n
</p>
<p>{
|ξk &minus; ξ |&gt; ε
</p>
<p>})
&le;
</p>
<p>&infin;&sum;
</p>
<p>k=n
P
(
|ξn &minus; ξ |&gt; ε
</p>
<p>)
.
</p>
<p>�
</p>
<p>It is this criterion that has actually been used in proving the strong law of large
</p>
<p>numbers for the Bernoulli scheme.
</p>
<p>One cannot deduce a converse assertion about the convergence rate to zero of
</p>
<p>the probability P(|ξn &minus; ξ | &gt; ε) from the a.s. convergence. The reader can easily
construct an example where ξn &rarr; ξ a.s., while P(|ξn &minus; ξ | &gt; ε) converges to zero
arbitrarily slowly.
</p>
<p>Theorem 6.1.2 implies the following result.
</p>
<p>Corollary 6.1.2 If ξn
p&rarr; ξ , then there exists a subsequence {nk} such that ξnk &rarr; ξ
</p>
<p>a.s. as k&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>132 6 On Convergence of Random Variables and Distributions
</p>
<p>Proof This assertion is also obvious since it suffices to take nk such that
P(|ξnk &minus; ξ |&gt; ε)&le; 1/k2 and then make use of Theorem 6.1.2. �
</p>
<p>There is one more important special case where convergence in probability
</p>
<p>ξn
p&rarr; ξ implies convergence ξn &rarr; ξ a.s. This is the case when the ξn are sums
</p>
<p>of independent random variables. Namely, the following assertion is true. If ξn =&sum;n
k=1 ηk , ηk are independent, then convergence of ξn in probability implies conver-
</p>
<p>gence with probability 1. This assertion will be proved in Sect. 11.2.
Finally we consider a third type of convergence of random variables.
</p>
<p>Definition 6.1.3 We will say that ξn converges to ξ in the r-th order mean (in mean
if r = 1; in mean square if r = 2) if, as n&rarr;&infin;,
</p>
<p>E|ξn &minus; ξ |r &rarr; 0.
</p>
<p>This convergence will be denoted by ξn
(r)&minus;&rarr; ξ .
</p>
<p>Clearly, by Chebyshev&rsquo;s inequality ξn
(r)&minus;&rarr; ξ implies that ξn
</p>
<p>p&rarr; ξ . On the other
hand, convergence
</p>
<p>(r)&minus;&rarr; does not follow from a.s. convergence (and all the more
from convergence in probability). Thus convergence in probability is the weakest of
</p>
<p>the three types of convergence we have introduced.
</p>
<p>Note that, under additional conditions, convergence ξn
p&rarr; ξ can imply that
</p>
<p>ξn
(r)&minus;&rarr; ξ (see Theorem 6.1.7 below). For example, it will be shown in Corol-
</p>
<p>lary 6.1.4 that if ξn
p&rarr; ξ and E|ξn|r+α &lt; c for some α &gt; 0, c &lt;&infin; and all n, then
</p>
<p>ξn
(r)&minus;&rarr; ξ .
</p>
<p>Definition 6.1.4 A sequence ξn is said to be a Cauchy sequence in probability (a.s.,
in mean) if, for any ε &gt; 0,
</p>
<p>P
(
|ξn &minus; ξm|&gt; ε
</p>
<p>)
&rarr; 0
</p>
<p>(
P
(
</p>
<p>sup
n&ge;m
</p>
<p>|ξn &minus; ξm|&gt; ε
)
&rarr; 0, E|ξn &minus; ξm|r &rarr; 0
</p>
<p>)
</p>
<p>as n&rarr;&infin; and m&rarr;&infin;.
</p>
<p>Theorem 6.1.3 (Cauchy convergence test) ξn &rarr; ξ in one of the senses
p&rarr;, a.s.&minus;&rarr; or
</p>
<p>(r)&minus;&rarr; if and only if ξn is a Cauchy sequence in the respective sense.
</p>
<p>Proof That ξn is a Cauchy sequence follows from convergence by virtue of the
inequalities
</p>
<p>|ξn &minus; ξm| &le; |ξn &minus; ξ | + |ξm &minus; ξ |,
sup
n&ge;m
</p>
<p>|ξn &minus; ξm| &le; sup
n&ge;m
</p>
<p>|ξn &minus; ξ | + |ξm &minus; ξ | &le; 2 sup
n&ge;m
</p>
<p>|ξn &minus; ξ |,
</p>
<p>|ξn &minus; ξm|r &le; Cr
(
|ξn &minus; ξ |r + |ξm &minus; ξ |r
</p>
<p>)
</p>
<p>for some Cr .</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Convergence of Random Variables 133
</p>
<p>Now assume that ξn is a Cauchy sequence in probability. Choose a sequence {nk}
such that
</p>
<p>P
(
|ξn &minus; ξm|&gt; 2&minus;k
</p>
<p>)
&lt; 2&minus;k
</p>
<p>for n&ge; nk , m&ge; nk . Put
</p>
<p>ξ &prime;k := ξnk , Ak :=
{
|ξ &prime;k &minus; ξ &prime;k+1|&gt; 2&minus;k
</p>
<p>}
, η=
</p>
<p>&infin;&sum;
</p>
<p>k=1
I (Ak).
</p>
<p>Then P(Ak) &le; 2&minus;k and Eη =
&sum;&infin;
</p>
<p>k=1 P(Ak) &le; 1. This means, of course, that the
number of occurrences of the events Ak is a proper random variable: P(η &lt;&infin;)= 1,
and hence with probability 1 finitely many events Ak occur. This means that, for any
</p>
<p>ω for which η(ω) &lt;&infin;, there exists a k0(ω) such that |ξ &prime;k(ω) &minus; ξ &prime;k+1(ω)| &le; 2&minus;k
for all k &ge; k0(ω). Therefore one has the inequality |ξ &prime;k(ω)&minus; ξ &prime;l(ω)| &le; 2&minus;k+1 for all
k &ge; k0(ω) and l &ge; k0(ω), which means that ξ &prime;n(ω) is a numerical Cauchy sequence
and hence there exists a value ξ(ω) such that |ξ &prime;k(ω)&minus; ξ(ω)| &rarr; 0 as k &rarr;&infin;. This
means, in turn, that ξ &prime;k
</p>
<p>a.s.&minus;&rarr; ξ and hence
</p>
<p>P
(
|ξn &minus; ξ | &ge; ε
</p>
<p>)
&le; P
</p>
<p>(
|ξn &minus; ξnk | &ge;
</p>
<p>ε
</p>
<p>2
</p>
<p>)
+ P
</p>
<p>(
|ξnk &minus; ξ |&gt;
</p>
<p>ε
</p>
<p>2
</p>
<p>)
&rarr; 0
</p>
<p>as n&rarr;&infin; and k&rarr;&infin;.
Now assume that ξn is a Cauchy sequence in mean. Then, by Chebyshev&rsquo;s in-
</p>
<p>equality, it will be a Cauchy sequence in probability and hence, by Corollary 6.1.2,
</p>
<p>there will exist a random variable ξ and a subsequence {nk} such that ξnk
a.s.&minus;&rarr; ξ .
</p>
<p>Now we will show that E|ξn &minus; ξ |r &rarr; 0. For a given ε &gt; 0, choose an n such that
E|ξk &minus; ξl |r &lt; ε for k &ge; n and l &ge; n. Then, by Fatou&rsquo;s lemma (see Appendix 3),
</p>
<p>E|ξn &minus; ξ |r = E lim
nk&rarr;&infin;
</p>
<p>|ξn &minus; ξnk |r
</p>
<p>= E lim inf
nk&rarr;&infin;
</p>
<p>|ξn &minus; ξnk |r &le; lim infnk&rarr;&infin; E|ξn &minus; ξnk |
r &le; ε.
</p>
<p>This means that E|ξn &minus; ξ |r &rarr; 0 as n&rarr;&infin;.
It remains to verify the assertion of the theorem related to a.s. convergence. We
</p>
<p>already know that if ξn is a Cauchy sequence in probability (or a.s.) then there exist a
</p>
<p>ξ and a subsequence ξnk such that ξnk
a.s.&minus;&rarr; ξ . Therefore, if we put nk(n) := min{nk :
</p>
<p>nk &ge; n}, then
</p>
<p>P
(
</p>
<p>sup
k&ge;n
</p>
<p>|ξk &minus; ξ | &ge; ε
)
&le; P
</p>
<p>(
sup
k&ge;n
</p>
<p>|ξk &minus; ξnk(n) | &ge; ε/2
)
+ P
</p>
<p>(
|ξnk(n) &minus; ξ | &ge; ε/2
</p>
<p>)
&rarr; 0
</p>
<p>as n&rarr;&infin;. The theorem is proved. �
</p>
<p>Remark 6.1.1 If we introduce the space Lr of all random variables ξ on 〈Ω,F,P〉
for which E|ξ |r &lt; &infin; and the norm ‖ξ‖ = (E|ξ |r)1/r on it (the triangle inequal-
ity ‖ξ1 + ξ2‖ &le; ‖ξ1‖ + ‖ξ2‖ is then nothing else but Minkowski&rsquo;s inequality, see
Theorem 4.7.2), then the assertion of Theorem 6.1.3 on convergence
</p>
<p>(r)&minus;&rarr; (which</p>
<p/>
</div>
<div class="page"><p/>
<p>134 6 On Convergence of Random Variables and Distributions
</p>
<p>is convergence in the norm of Lr , for we identify random variables ξ1 and ξ2 if
</p>
<p>‖ξ1 &minus; ξ2‖ = 0) means that Lr is complete and hence is a Banach space.
The space of all random variables on 〈Ω,F,P〉 can be metrised so that conver-
</p>
<p>gence in the metric will be equivalent to convergence in probability. For instance,
</p>
<p>one could put
</p>
<p>ρ(ξ1, ξ2) := E
|ξ1 &minus; ξ2|
</p>
<p>1 + |ξ1 &minus; ξ2|
.
</p>
<p>Since
</p>
<p>|x + y|
1 + |x + y| &le;
</p>
<p>|x|
1 + |x| +
</p>
<p>|y|
1 + |y|
</p>
<p>always holds, ρ(ξ1, ξ2) satisfies all the axioms of a metric. It is not difficult to see
</p>
<p>that relations ρ(ξ1, ξ2) &rarr; 0 and ξn
p&rarr; 0 are equivalent. The assertion of Theo-
</p>
<p>rem 6.1.3 related to convergence
p&rarr; means that the metric space we introduced
</p>
<p>is complete.
</p>
<p>6.1.2 The Continuity Theorem
</p>
<p>Now we will derive the following &ldquo;continuity theorem&rdquo;.
</p>
<p>Theorem 6.1.4 Let ξn
a.s.&minus;&rarr; ξ (ξn
</p>
<p>p&rarr; ξ) and H(s) be a function continuous every-
where with respect to the distribution of the random variable ξ (i.e. H(s) is contin-
uous at each point of a set S such that P(ξ &isin; S)= 1). Then
</p>
<p>H(ξn)
a.s.&minus;&rarr;H(ξ)
</p>
<p>(
H(ξn)
</p>
<p>p&rarr;H(ξ)
)
.
</p>
<p>Proof Let ξn
a.s.&minus;&rarr; ξ . Since the sets A= {ω : ξn(ω)&rarr; ξ(ω)} and B = {ω : ξ(ω) &isin; S}
</p>
<p>are both of probability 1, P(AB) = P(A) + P(B) &minus; P(A &cup; B) = 1. But one has
H(ξn)&rarr;H(ξ) on the set AB . Convergence with probability 1 is proved.
</p>
<p>Now let ξn
p&rarr; ξ . If we assume that convergence H(ξn)
</p>
<p>p&rarr; H(ξ) does not take
place then there will exist ε &gt; 0, δ &gt; 0 and a subsequence {n&prime;} such that
</p>
<p>P
(∣∣H(ξn&prime;)&minus;H(ξ)
</p>
<p>∣∣&gt; ε
)
&gt; δ.
</p>
<p>But ξn&prime;
p&rarr; ξ and hence there exists a subsequence {n&prime;&prime;} such that ξn&prime;&prime;
</p>
<p>a.s.&minus;&rarr; ξ and
H(ξn&prime;&prime;)
</p>
<p>a.s.&minus;&rarr;H(ξ). This contradicts the assumption we made, for the latter implies
that
</p>
<p>P
(∣∣H(ξn&prime;&prime;)&minus;H(ξ)
</p>
<p>∣∣&gt; ε
)
&gt; δ.
</p>
<p>The theorem is proved. �
</p>
<p>6.1.3 Uniform Integrability and Its Consequences
</p>
<p>Now we will consider this question: in what cases does convergence in probability
</p>
<p>imply convergence in mean?</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Convergence of Random Variables 135
</p>
<p>The main condition that ensures the transition from convergence in probability
</p>
<p>to convergence in mean is associated with the notion of uniform integrability.
</p>
<p>Definition 6.1.5 A sequence {ξn} is said to be uniformly integrable if
sup
n
</p>
<p>E
(
|ξn|; |ξn|&gt;N
</p>
<p>)
&rarr; 0 as N &rarr;&infin;.
</p>
<p>A sequence of independent identically distributed random variables with finite
</p>
<p>mean is, clearly, uniformly integrable.
</p>
<p>If {ξn} is uniformly integrable then so are {cξn} and {ξn + c}, where c= const.
Let us present some further, less evident, properties of uniform integrability.
</p>
<p>U1. If the sequences {ξ &prime;n} and {ξ &prime;&prime;n } are uniformly integrable then the sequences
defined by ζn = max(|ξ &prime;n|, |ξ &prime;&prime;n |) and ζn = ξ &prime;n+ ξ &prime;&prime;n are also uniformly integrable.
</p>
<p>Proof Indeed, for ζn = max(|ξ &prime;n|, |ξ &prime;&prime;n |) we have
</p>
<p>E(ζn; ζn &gt;N)= E
(
ζn; ζn &gt;N,
</p>
<p>∣∣ξ &prime;n
∣∣&gt;
</p>
<p>∣∣ξ &prime;&prime;n
∣∣)+E
</p>
<p>(
ζn; ζn &gt;N,
</p>
<p>∣∣ξ &prime;n
∣∣&le;
</p>
<p>∣∣ξ &prime;&prime;n
∣∣)
</p>
<p>&le; E
(∣∣ξ &prime;n
</p>
<p>∣∣;
∣∣ξ &prime;n
</p>
<p>∣∣&gt;N
)
+E
</p>
<p>(∣∣ξ &prime;&prime;n
∣∣;
∣∣ξ &prime;&prime;n
</p>
<p>∣∣&ge;N
)
&rarr; 0
</p>
<p>as N &rarr;&infin;.
Since
</p>
<p>∣∣ξ &prime;n + ξ &prime;&prime;n
∣∣&le;
</p>
<p>∣∣ξ &prime;n
∣∣+
</p>
<p>∣∣ξ &prime;&prime;n
∣∣&le; 2 max
</p>
<p>(∣∣ξ &prime;n
∣∣,
∣∣ξ &prime;&prime;n
</p>
<p>∣∣),
from the above it follows that the sequence defined by the sum ζn = ξ &prime;n + ξ &prime;&prime;n is also
uniformly integrable. �
</p>
<p>U2. If {ξn} is uniformly integrable then supnE|ξn| &le; c &lt;&infin;.
</p>
<p>Proof Indeed, choose N so that
</p>
<p>sup
n
</p>
<p>E
(
|ξn|; |ξn|&gt;N
</p>
<p>)
&le; 1.
</p>
<p>Then
</p>
<p>sup
n
</p>
<p>E|ξn| = sup
n
</p>
<p>[
E
(
|ξn|; |ξn| &le;N
</p>
<p>)
+E
</p>
<p>(
|ξn|; |ξn|&gt;N
</p>
<p>)]
&le;N + 1. �
</p>
<p>The converse assertion is not true. For example, for a sequence
</p>
<p>ξn : P(ξn = n)= 1/n= 1 &minus; P(ξn = 0)
one has E|ξn| = 1, but the sequence is not uniformly integrable.
</p>
<p>If we somewhat strengthen the above statement U2, it becomes &ldquo;characteristic&rdquo;
</p>
<p>for uniform integrability.
</p>
<p>Theorem 6.1.5 For a sequence {ξn} to be uniformly integrable, it is necessary and
sufficient that there exists a function ψ(x) such that
</p>
<p>ψ(x)
</p>
<p>x
&uarr;&infin; as x &uarr;&infin;, sup
</p>
<p>n
Eψ
</p>
<p>(
|ξn|
</p>
<p>)
&lt; c &lt;&infin;. (6.1.2)
</p>
<p>In the necessity assertion one can choose a convex function ψ .</p>
<p/>
</div>
<div class="page"><p/>
<p>136 6 On Convergence of Random Variables and Distributions
</p>
<p>Proof Without loss of generality we can assume that ξi &ge; 0.
The sufficiency is evident, since, putting v(x) := ψ(x)
</p>
<p>x
, we get
</p>
<p>E(ξn; ξn &ge;N)&le;
1
</p>
<p>v(N)
E
(
ξnv(ξn); ξn &ge;N
</p>
<p>)
&le; c
</p>
<p>v(N)
.
</p>
<p>To prove the necessity, put
</p>
<p>ε(N) := sup
n
</p>
<p>E(ξn; ξn &ge;N).
</p>
<p>Then, by virtue of uniform integrability, ε(N) &darr; 0 as N &uarr; &infin;. Choose a sequence
Nk &uarr;&infin; as k &uarr;&infin; such that
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>&radic;
ε(Nk) &lt; c1 &lt;&infin;,
</p>
<p>and put
</p>
<p>g(x)= x
(
ε(Nk)
</p>
<p>)&minus;1/2
for x &isin; [Nk,Nk+1).
</p>
<p>Since
</p>
<p>g(Nk &minus; 0)
Nk
</p>
<p>=
(
ε(Nk&minus;1)
</p>
<p>)&minus;1/2 &le;
(
ε(Nk)
</p>
<p>)&minus;1/2 = g(Nk)
Nk
</p>
<p>,
</p>
<p>we have
g(x)
x
</p>
<p>&uarr;&infin; as x &rarr;&infin;. Further,
</p>
<p>Eg(ξn)=
&sum;
</p>
<p>k
</p>
<p>E
[
g(ξn); ξn &isin; [Nk,Nk+1)
</p>
<p>]
</p>
<p>=
&sum;
</p>
<p>k
</p>
<p>E
[
ξn
(
ε(Nk)
</p>
<p>)&minus;1/2; ξn &isin; [Nk,Nk+1)
]
</p>
<p>&le;
&sum;
</p>
<p>k
</p>
<p>(
ε(Nk)
</p>
<p>)&minus;1/2
ε(Nk)=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>&radic;
ε(Nk) &lt; c1,
</p>
<p>where the right-hand side does not depend on n. Therefore, to prove the theorem it
</p>
<p>is sufficient to construct a function ψ &le; g which is convex and such that ψ(x)
x
</p>
<p>&uarr;&infin;
as x &uarr;&infin;.
</p>
<p>Define the function ψ(x) as the continuous polygon with nodes (Nk, g(Nk &minus;0)).
Since
</p>
<p>g(Nk &minus; 0)
Nk
</p>
<p>= ε(Nk&minus;1)&minus;1/2
</p>
<p>monotonically increases as k grows, ψ is a lower envelope curve for the discontinu-
</p>
<p>ous function g(x)&ge; ψ(x). The monotonicity of ψ(x)
x
</p>
<p>follows from the fact that, on
</p>
<p>the interval [Nk,Nk+1), this function can be represented as
ψ(x)
</p>
<p>x
= ak,ψ &minus;
</p>
<p>bk
</p>
<p>x
,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Convergence of Random Variables 137
</p>
<p>where bk &gt; 0, because the values ψ(Nk+1 &minus;0) and g(Nk+1 &minus;0) coincide, while the
angular incline ak,ψ of the function ψ on the interval [Nk,Nk+1) is greater than the
&ldquo;radial&rdquo; incline ak,g of the function g:
</p>
<p>ak,g =
g(Nk+1 &minus; 0)&minus; g(Nk)
</p>
<p>Nk+1 &minus;Nk
&lt;
</p>
<p>g(Nk+1 &minus; 0)&minus; g(Nk &minus; 0)
Nk+1 &minus;Nk
</p>
<p>= ak,ψ .
</p>
<p>It is clear that
ψ(x)
x
</p>
<p>increases unboundedly, for
</p>
<p>ψ(Nk)
</p>
<p>Nk
= g(Nk &minus; 0)
</p>
<p>Nk
= ε(Nk&minus;1)&minus;1/2 &uarr;&infin;
</p>
<p>as k&rarr;&infin;. The theorem is proved. �
</p>
<p>In studying the mean values of sums of random variables, the following theorem
</p>
<p>on uniform integrability of average values, following from Theorem 6.1.5, plays an
important role.
</p>
<p>Theorem 6.1.6 Let ξ1, ξ2, . . . be an arbitrary uniformly integrable sequence of ran-
dom variables,
</p>
<p>pi,n &ge; 0,
n&sum;
</p>
<p>i=1
pi,n = 1, ζn =
</p>
<p>n&sum;
</p>
<p>k=1
|ξi |pi,n.
</p>
<p>Then the sequence {ζn} is uniformly integrable as well.
</p>
<p>Proof Let ψ(x) be the convex function from Theorem 6.1.5 satisfying proper-
ties (6.1.2). Then, by that theorem,
</p>
<p>Eψ(ζn)= Eψ
(
</p>
<p>n&sum;
</p>
<p>i=1
pi,n|ξi |
</p>
<p>)
&le; E
</p>
<p>n&sum;
</p>
<p>i=1
pi,nψ
</p>
<p>(
|ξi |
</p>
<p>)
&le; c.
</p>
<p>It remains to make use of Theorem 6.1.5 again. �
</p>
<p>Now we will show that convergence in probability together with uniform inte-
</p>
<p>grability imply convergence in mean.
</p>
<p>Theorem 6.1.7 Let ξn
p&rarr; ξ and {ξn} be uniformly integrable. Then E|ξ | exists and,
</p>
<p>as n&rarr;&infin;,
E|ξn &minus; ξ |&rarr; 0.
</p>
<p>If, moreover, {|ξ rn |} is uniformly integrable then ξn
(r)&minus;&rarr; ξ .
</p>
<p>Conversely, if, for an r &ge; 1, ξn
(r)&minus;&rarr; ξ and E|ξ |r &lt;&infin;, then {|ξn|r} is uniformly
</p>
<p>integrable.
</p>
<p>In the law of large numbers for the Bernoulli scheme (see Theorem 5.1.1) we
</p>
<p>proved that the normed sum Sn/n converges to p in probability. Since 0 &le; Sn/n&le; 1,</p>
<p/>
</div>
<div class="page"><p/>
<p>138 6 On Convergence of Random Variables and Distributions
</p>
<p>Sn/n is clearly uniformly integrable and the convergence in mean
</p>
<p>E |Sn/n &minus; p|r &rarr; 0 holds for any r . This fact can also be established directly.
For a more substantiative example of application of Theorems 6.1.6 and 6.1.7, see
</p>
<p>Sect. 8.1.
</p>
<p>Proof We show that Eξ exists. By the properties of integrals (see Lemma A3.2.3 in
Appendix 3), if E|ζ |&lt;&infin; then E(ζ ;An)&rarr; 0 as P(An)&rarr; 0. Since Eξn &lt;&infin;, for
any N and ε one has
</p>
<p>Emin
(
|ξ |,N
</p>
<p>)
= lim
</p>
<p>n&rarr;&infin;
</p>
<p>[
Emin
</p>
<p>(
|ξ |,N
</p>
<p>)
; |ξn &minus; ξ |&lt; ε
</p>
<p>]
</p>
<p>&le; lim
n&rarr;&infin;
</p>
<p>Emin
(
|ξ | + ε,N
</p>
<p>)
&le; c+ ε.
</p>
<p>It follows that E|ξ | &le; c.
Further, for brevity, put ηn = |ξn &minus; ξ |. Then ηn
</p>
<p>p&rarr; 0 and ηn are uniformly inte-
grable together with ξn. For any N and ε, one has
</p>
<p>Eηn = E(ηn; ηn &le; ε)+E(ηn; N &ge; ηn &gt; ε)+E(ηn; ηn &ge;N)
&le; ε+NP(ηn &ge; ε)+E(ηn; ηn &gt;N). (6.1.3)
</p>
<p>Choose N so that supnE(ηn; ηn &gt;N)&le; ε. Then, for such an N ,
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>Eηn &le; 2ε.
</p>
<p>Since ε is arbitrary, Eηn &rarr; 0 as n&rarr;&infin;.
The relation E|ξn &minus; ξ |r &rarr; 0 can be proved in the same way as (6.1.3), since
</p>
<p>ηrn = |ξn &minus; ξ |r
p&rarr; 0 and ηrn are uniformly integrable together with |ξn|r .
</p>
<p>Now we will prove the converse assertion. Let, for simplicity, r = 1. One has
</p>
<p>E
(
|ξn|; |ξn|&gt;N
</p>
<p>)
&le; E
</p>
<p>(
|ξn &minus; ξ |; |ξn|&gt;N
</p>
<p>)
+E
</p>
<p>(
|ξ |; |ξn|&gt;N
</p>
<p>)
</p>
<p>&le; E|ξn &minus; ξ | +E
(
|ξ |; |ξn|&gt;N
</p>
<p>)
</p>
<p>&le; E|ξn &minus; ξ | +E
(
|ξ |; |ξn &minus; ξ |&gt; 1
</p>
<p>)
+E
</p>
<p>(
|ξ |; |ξ |&gt;N &minus; 1
</p>
<p>)
.
</p>
<p>The first term on the right-hand side tends to zero by the assumption, and the second
</p>
<p>term, by Lemma A3.2.3 from Appendix 3, which we have just mentioned, and the
</p>
<p>fact that P(|ξn &minus; ξ |&gt; 1)&rarr; 0. The last term does not depend on n and can be made
arbitrarily small by choosing N . Theorem 6.1.7 is proved. �
</p>
<p>Now we can derive yet another continuity theorem which has the following form.
</p>
<p>Theorem 6.1.8 If ξn
p&rarr; ξ , H(s) satisfies the conditions of Theorem 6.1.4, and
</p>
<p>H(ξn) is uniformly integrable, then, as n&rarr;&infin;,
</p>
<p>E
∣∣H(ξn)&minus;H(ξ)
</p>
<p>∣∣&rarr; 0
</p>
<p>and, in particular, EH(ξn)&rarr; EH(ξ).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Convergence of Random Variables 139
</p>
<p>This assertion follows from Theorems 6.1.4 and 6.1.7, for H(ξn)
p&rarr; H(ξ) by
</p>
<p>Theorem 6.1.4.
</p>
<p>Sometimes it is convenient to distinguish between left and right uniform integra-
bility. We will say that a sequence {ξn} is right (left) uniformly integrable if
</p>
<p>supE(ξn; ξn &ge;N)&rarr; 0
(
supE
</p>
<p>(
|ξn|; ξn &le;&minus;N
</p>
<p>)
&rarr; 0
</p>
<p>)
</p>
<p>as N &rarr;&infin;. It is evident that a sequence {ξn} is uniformly integrable if and only if
it is both right and left uniformly integrable.
</p>
<p>Lemma 6.1.1 A sequence {ξn} is right uniform integrable if at least one of the
following conditions is met:
</p>
<p>1. For any sequence N(n)&rarr;&infin; as n&rarr;&infin;, one has
</p>
<p>E
(
ξn; ξn &gt;N(n)
</p>
<p>)
&rarr; 0.
</p>
<p>(This condition is clearly also necessary for uniform integrability.)
2. ξn &le; η, where Eη &lt;&infin;.
3. E(ξ+n )
</p>
<p>1+α &lt; c &lt;&infin; for some α &gt; 0 (here x+ = max(0, x)).
4. ξn is left uniformly integrable, ξn
</p>
<p>p&rarr; ξ , and Eξn &rarr; Eξ &lt;&infin;.
</p>
<p>Proof
</p>
<p>1. If the sequence {ξn} were not right uniformly integrable, there would exist
an ε &gt; 0 and subsequences n&prime; &rarr; &infin; and N &prime; = N &prime;(n&prime;) &rarr; &infin; such that E(ξn&prime;;
ξn&prime; &gt;N
</p>
<p>&prime;) &gt; ε. But this contradicts condition 1.
2. E(ξn; ξn &gt;N)&le; E(η; η &gt;N)&rarr; 0 as N &rarr;&infin;.
3. E(ξn; ξn &gt;N)&le; E(ξ1+αn N&minus;α; ξn &gt;N)&le;N&minus;αc&rarr; 0 as N &rarr;&infin;.
4. Without loss of generality, put ξ := 0. Then
</p>
<p>E(ξn; ξn &gt;N)= Eξn &minus;E(ξn; ξn &lt;&minus;N)&minus;E
(
ξn; |ξn| &le;N
</p>
<p>)
.
</p>
<p>The first two terms on the right-hand side vanish as n &rarr; &infin; for any N =
N(n)&rarr;&infin;. For the last term, for any ε &gt; 0, one has
</p>
<p>∣∣E
(
ξn; |ξn| &le;N
</p>
<p>)∣∣&le;
∣∣E
(
ξn; |ξn| &le; ε
</p>
<p>)∣∣+
∣∣E
(
ξn; ε &lt; |ξn| &le;N
</p>
<p>)∣∣
&le; ε+NP
</p>
<p>(
|ξn|&gt; ε
</p>
<p>)
.
</p>
<p>For any given ε &gt; 0, choose an n(ε) such that, for all n &ge; n(ε), we would have
P(|ξn| &gt; ε) &lt; ε, and put N(ε) := &lfloor;1/
</p>
<p>&radic;
ε&rfloor;. This will mean that, for all n &ge; n(ε)
</p>
<p>and N &le;N(ε), one has E(ξn; |ξn| &le;N) &lt; ε+
&radic;
ε, and therefore condition 1 of the
</p>
<p>lemma holds for E(ξn; ξn &gt;N). The lemma is proved. �
</p>
<p>Now, based on the above, we can state three useful corollaries.
</p>
<p>Corollary 6.1.3 (The dominated convergence theorem) If ξn
p&rarr; ξ , |ξn| &lt; η, and
</p>
<p>Eη &lt;&infin; then Eξ exists and Eξn &rarr; Eξ .</p>
<p/>
</div>
<div class="page"><p/>
<p>140 6 On Convergence of Random Variables and Distributions
</p>
<p>Corollary 6.1.4 If ξn
p&rarr; ξ and E|ξn|r+α &lt; c &lt;&infin; for some α &gt; 0 then ξn
</p>
<p>(r)&minus;&rarr; ξ .
</p>
<p>Corollary 6.1.5 If ξn
p&rarr; ξ and H(x) is a continuous bounded function, then
</p>
<p>E|H(ξn)&minus;EH(ξ)| &rarr; 0 as n&rarr;&infin;.
</p>
<p>In conclusion of the present section, we will derive one more auxiliary proposi-
</p>
<p>tion that can be useful.
</p>
<p>Lemma 6.1.2 (On integrals over sets of small probability) If {ξn} is a uniformly in-
tegrable sequence and {An} is an arbitrary sequence of events such that P(An)&rarr; 0,
then E(|ξn|; An)&rarr; 0 as n&rarr;&infin;.
</p>
<p>Proof Put Bn := {|ξn| &le;N}. Then
</p>
<p>E
(
|ξn|; An
</p>
<p>)
= E
</p>
<p>(
|ξn|; AnBn
</p>
<p>)
+E
</p>
<p>(
|ξn|; AnBn
</p>
<p>)
</p>
<p>&le;NP(An)+E
(
|ξn|; |ξn|&gt;N
</p>
<p>)
.
</p>
<p>For a given ε &gt; 0, first choose N so that the second summand on the right-hand side
</p>
<p>does not exceed ε/2 and then an n such that the first summand does not exceed ε/2.
</p>
<p>We obtain that, by choosing n large enough, we can make E(|ξn|; An) less than ε.
The lemma is proved. �
</p>
<p>6.2 Convergence of Distributions
</p>
<p>In Sect. 6.1 we introduced three types of convergence which can be used to charac-
</p>
<p>terise the closeness of random variables given on a common probability space. But
</p>
<p>what can one do if random variables are given on different probability spaces (or if
</p>
<p>it is not known where they are given) which nevertheless have similar distributions?
</p>
<p>(Recall, for instance, the Poisson or de Moivre&ndash;Laplace theorems.) In such cases
</p>
<p>one should be able to characterise the closeness of the distributions themselves.
</p>
<p>Having found an apt definition for such a closeness, in many problems we will be
</p>
<p>able to approximate the required but hard to come by distributions by known and,
</p>
<p>as a rule, simpler distributions.
</p>
<p>Now what distributions should be considered as close? We are clearly looking
</p>
<p>for a definition of convergence of a sequence of distribution functions Fn(x) to a
</p>
<p>distribution function F(x). It would be natural, for instance, that the distributions
</p>
<p>of the variables ξn = ξ + 1/n should converge to that of ξ as n &rarr; &infin;. Therefore
requiring in the definition of convergence that supx |Fn(x)&minus; F(x)| is small would
be unreasonable since this condition is not satisfied for the distributions of ξ + 1/n
and ξ if F(x)= P(ξ &lt; x) has at least one point of discontinuity.
</p>
<p>We will define the convergence of Fn to F as that which arises when one consid-
</p>
<p>ers convergence in probability.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Convergence of Distributions 141
</p>
<p>Definition 6.2.1 We will say that distribution functions Fn converge weakly to a
distribution function F as n&rarr;&infin;, and denote this by Fn &rArr; F if, for any continuous
bounded function f (x),
</p>
<p>&int;
f (x)dFn(x)&rarr;
</p>
<p>&int;
f (x)dF (x). (6.2.1)
</p>
<p>Considering the distributions Fn(B) and F(B) (B are Borel sets) corresponding to
</p>
<p>Fn and F , we say that Fn converges weakly to F and write Fn &rArr; F. One can clearly
re-write (6.2.1) as
</p>
<p>&int;
f (x)Fn(dx)&rarr;
</p>
<p>&int;
f (x)F(dx) or Ef (ξn)&rarr; Ef (ξ) (6.2.2)
</p>
<p>(cf. Corollary 6.1.5), where ξn &sub;= Fn and ξ &sub;= F.
</p>
<p>Another possible definition of weak convergence follows from the next assertion.
</p>
<p>Theorem 6.2.1 2 Fn &rArr; F if and only if Fn(x)&rarr; F(x) at each point of continuity
x of F .
</p>
<p>Proof Let (6.2.1) hold. Consider an ε &gt; 0 and a continuous function fε(t) which is
equal to 1 for t &lt; x and to 0 for t &ge; x + ε, and varies linearly on [x, x + ε]. Since
</p>
<p>Fn(x)=
&int; x
</p>
<p>&minus;&infin;
fε(t) dFn(t)&le;
</p>
<p>&int;
fε(t) dFn(t),
</p>
<p>by virtue of (6.2.1) one has
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>Fn(x)&le;
&int;
</p>
<p>fε(t) dF (t)&le; F(x + ε).
</p>
<p>If x is a point of continuity of F then
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>Fn(x)&le; F(x)
</p>
<p>since ε is arbitrary.
</p>
<p>In the same way, using the function f &lowast;ε (t)= fε(t + ε), we obtain the inequality
lim inf
n&rarr;&infin;
</p>
<p>Fn(x)&ge; F(x).
</p>
<p>We now prove the converse assertion. Let &minus;M and N be points of continuity
of F such that F(&minus;M) &lt; ε/5 and 1 &minus; F(N) &lt; ε/5. Then Fn(&minus;M) &lt; ε/4 and
1&minus;Fn(N) &lt; ε/4 for all sufficiently large n. Therefore, assuming for simplicity that
|f | &le; 1, we obtain that
</p>
<p>&int;
f dFn and
</p>
<p>&int;
f dF (6.2.3)
</p>
<p>2In many texts on probability theory the condition of the theorem is given as the definition of weak
</p>
<p>convergence. However, the definition in terms of the relation (6.2.2) is apparently more appropriate
</p>
<p>for it continues to remain valid for distributions on arbitrary topological spaces (see, e.g. [1, 25]).</p>
<p/>
</div>
<div class="page"><p/>
<p>142 6 On Convergence of Random Variables and Distributions
</p>
<p>will differ from
&int; N
</p>
<p>&minus;M
f dFn and
</p>
<p>&int; N
</p>
<p>&minus;M
f dF,
</p>
<p>respectively, by less than ε/2. Construct on the semi-interval (&minus;M,N ] a step func-
tion fε with jumps at the points of continuity of F which differs from f by less than
</p>
<p>ε/2. Outside (&minus;M,N ] we set fε := 0. We can put, for instance,
</p>
<p>fε(x) :=
k&sum;
</p>
<p>j=1
f (xj )δj (x),
</p>
<p>where x0 = &minus;M &lt; x1 &lt; &middot; &middot; &middot; &lt; xk = N are appropriately chosen points of continu-
ity of F , and δj (x) is the indicator function of the semi-interval (xj&minus;1, xj ]. Then&int;
fε dFn and
</p>
<p>&int;
fε dF will differ from the respective integrals in (6.2.3), for suffi-
</p>
<p>ciently large n, by less than ε. At the same time,
</p>
<p>&int;
fε dFn =
</p>
<p>k&sum;
</p>
<p>j=1
f (xj )
</p>
<p>[
Fn(xj )&minus; Fn(xj&minus;1)
</p>
<p>]
&rarr;
</p>
<p>&int;
fε dF.
</p>
<p>Since ε &gt; 0 is arbitrary, the last relation implies (6.2.1). (Indeed, one just has to
</p>
<p>make use of the inequality
</p>
<p>lim sup
</p>
<p>&int;
f dFn &le; ε+ lim sup
</p>
<p>&int;
fε dFn = ε+
</p>
<p>&int;
fε dF &le; 2ε+
</p>
<p>&int;
f dF
</p>
<p>and a similar inequality for lim inf
&int;
f dFn.) The theorem is proved. �
</p>
<p>For remarks on different and, in a certain sense, simpler proofs of the second
</p>
<p>assertion of Theorem 6.2.1, see the end of Sect. 6.3 and Sect. 7.4.
</p>
<p>Remark 6.2.1 Repeating with obvious modifications the above-presented proof, we
can get a somewhat different equivalent of convergence (4): convergence of differ-
ences Fn(y)&minus; Fn(x)&rarr; F(y)&minus; F(x) for any points of continuity x and y of F .
</p>
<p>Remark 6.2.2 If F(x) is continuous then convergence Fn &rArr; F is equivalent to the
uniform convergence supx |Fn(x)&minus; F(x)| &rarr; 0.
</p>
<p>We leave the proof of the last assertion to the reader. It follows from the fact
</p>
<p>that convergence Fn(x)&rarr; F(x) at any x implies, by virtue of the continuity of F ,
uniform convergence on any finite interval. The uniform smallness of Fn(x)&minus;F(x)
on the &ldquo;tails&rdquo; is ensured by the smallness of F(x) and 1 &minus; F(x).
</p>
<p>Remark 6.2.3 If distributions Fn and F are discrete and have jumps at the same
points x1, x2, . . . then Fn &rArr; F will clearly be equivalent to the convergence of the
probabilities of the values x1, x2, . . . (Fn(xk + 0)&minus; Fn(xk)&rarr; F(xk + 0)&minus; F(xk)).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Convergence of Distributions 143
</p>
<p>We introduce some notation which will be convenient for the sequel. Let ξn and
</p>
<p>ξ be some random variables (given, generally speaking, on different probability
</p>
<p>spaces) such that ξn &sub;= Fn and ξ &sub;= F.
</p>
<p>Definition 6.2.2 If Fn &rArr; F we will say that ξn converges to ξ in distribution and
write ξn &rArr; ξ .
</p>
<p>We used here the same symbol &rArr; as for the weak convergence, but this leads to
no confusion.
</p>
<p>It is clear that ξn
p&rarr; ξ implies ξn &rArr; ξ , but not vice versa.
</p>
<p>At the same time the following assertion holds true.
</p>
<p>Lemma 6.2.1 If ξn &rArr; ξ (Fn &rArr; F) then one can construct random variables ξ &prime;n
and ξ &prime; on a common probability space so that P(ξ &prime;n &lt; x) = P(ξn &lt; x) = Fn(x),
P(ξ &prime; &lt; x)= P(ξ &lt; x)= F(x), and
</p>
<p>ξ &prime;n
a.s.&minus;&rarr; ξ &prime;.
</p>
<p>Proof Define the quantile transforms (see Definition 3.2.6) by
</p>
<p>F&minus;1n (t) := sup
{
x : Fn(x)&le; t
</p>
<p>}
, F&minus;1(t) := sup
</p>
<p>{
x : F(x)&le; t
</p>
<p>}
.
</p>
<p>(If F(x) is continuous and strictly increasing then F&minus;1(t) coincides with the solu-
tion to the equation F(v)= t .) Let η&sub;=U0,1. Put
</p>
<p>ξ &prime;n := F&minus;1n (η)&sub;= Fn, ξ &prime; := F&minus;1(η)&sub;= F
</p>
<p>(cf. Theorem 3.2.2), and show that ξ &prime;n
a.s.&minus;&rarr; ξ &prime;. In order to do that, it suffices to prove
</p>
<p>that F&minus;1n (y)&rarr; F&minus;1(y) for almost all y &isin; [0,1].
The functions F and F&minus;1 are monotone and hence each of them has at most
</p>
<p>a countable set of discontinuity points. This means that, for all y &isin; [0,1] with the
possible exclusion of the points from a countable set T , the function F&minus;1(y) will
be continuous.
</p>
<p>So let y be a point of continuity of F (&minus;1), and F (&minus;1)(y)= x.
For t &le; y, choose a continuous strictly increasing function G(&minus;1)(t) such that
</p>
<p>G(&minus;1)(y)= F (&minus;1)(y), G(&minus;1)(t)&le; F (&minus;1)(t) for t &le; y.
Denote by G(v), v &le; x, the function inverse to G(&minus;1)(t). Clearly, G(v) domi-
</p>
<p>nates the function F(v) in the domain v &le; x. By virtue of the continuity and strict
monotonicity of the functions G(&minus;1) and G (in the domain under consideration), for
ε &gt; 0 we have
</p>
<p>G(x &minus; ε)= y &minus; δ(ε),
where δ(ε) &gt; 0, δ(ε) &rarr; 0 as ε &rarr; 0. Choose an ε such that x &minus; ε is a point of
continuity of F . Then, for all n large enough,
</p>
<p>Fn(x &minus; ε)&le; F(x &minus; ε)+
δ(ε)
</p>
<p>2
&le;G(x &minus; ε)+ δ(ε)
</p>
<p>2
= y &minus; δ(ε)
</p>
<p>2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>144 6 On Convergence of Random Variables and Distributions
</p>
<p>The opposite inequality can be proved in a similar way. Since ε can be arbitrarily
</p>
<p>small, we obtain that, for almost all y,
</p>
<p>F&minus;1n (y)&rarr; F (&minus;1)(y) as n&rarr;&infin;.
</p>
<p>Hence F
(&minus;1)
n (η)&rarr; F (&minus;1)(η) with probability 1 with respect to the distribution of η.
</p>
<p>The lemma is proved. �
</p>
<p>Lemma 6.2.1 remains true for vector-valued random variables as well.
</p>
<p>Sometimes it is also convenient to have a simple symbol for the relation &ldquo;the
</p>
<p>distribution of ξn converges weakly to F&rdquo;. We will write this relation as
</p>
<p>ξn &sub;&rArr; F, (6.2.4)
</p>
<p>so that the symbol &sub;&rArr;expresses the same fact as &rArr; but relates objects of a different
nature in the same way as the symbol &sub;= in the relation ξ &sub;= P (on the left-hand
side in (6.2.4) we have random variables, while on the right hand side there is a
</p>
<p>distribution).
</p>
<p>In these terms, the assertion of the Poisson theorem could be written as Sn&sub;&rArr;�&micro;,
while the statement of the law of large numbers for the Bernoulli scheme takes the
</p>
<p>form Sn/n&sub;&rArr; Ip .
The coincidence of the distributions of ξ and η will be denoted by ξ
</p>
<p>d= η.
</p>
<p>Lemma 6.2.2 If ξn &rArr; ξ and εn
p&rarr; 0 then ξn + εn &rArr; ξ .
</p>
<p>If ξn &rArr; ξ and γn
p&rarr; 1 then ξnγn &rArr; ξ .
</p>
<p>Proof Let us prove the first assertion. For any t and δ &gt; 0 such that t and t &plusmn; δ are
points of continuity of P(ξ &lt; t), one has
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>P(ξn + εn &lt; t)= lim sup
n&rarr;&infin;
</p>
<p>P(ξn + εn &lt; t, εn &gt;&minus;δ)
</p>
<p>&le; lim sup
n&rarr;&infin;
</p>
<p>P(ξn &lt; t + δ)= P(ξ &lt; t + δ).
</p>
<p>Similarly,
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>P(ξn + εn &lt; t)&ge; P(ξ &lt; t &minus; δ).
</p>
<p>Since P(ξ &lt; t &plusmn; δ) can be chosen arbitrary close to P(ξ &lt; t) by taking a sufficiently
small δ, the required convergence follows.
</p>
<p>The second assertion can be proved in the same way. The lemma is proved. �
</p>
<p>Now we will give analogues of Theorems 6.1.4 and 6.1.7 in terms of distribu-
</p>
<p>tions.
</p>
<p>Theorem 6.2.2 If ξn &rArr; ξ and a function H(s) satisfies the conditions of Theo-
rem 6.1.4 then H(ξn)&rArr;H(ξ).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Convergence of Distributions 145
</p>
<p>Theorem 6.2.3 If ξn &rArr; ξ and the sequence {ξn} is uniformly integrable then Eξ
exists and Eξn &rarr; Eξ .
</p>
<p>Proof There are two ways of proving these theorems. One of them consists of re-
ducing them to Theorems 6.1.4 and 6.1.7. To this end, one has to construct random
</p>
<p>variables ξ &prime;n = F (&minus;1)n (η) and ξ &prime; = F (&minus;1)(η), where η&sub;=U0,1 and F (&minus;1)n and F (&minus;1)
</p>
<p>are the quantile transforms of Fn and F , respectively, and prove that ξ
&prime;
n
</p>
<p>p&rarr; ξ &prime; (we
already know that F (&minus;1)(η) &sub;= F; if F is discontinuous or not strictly increasing,
then F (&minus;1) should be defined as in Lemma 6.2.1).
</p>
<p>Another approach is to prove the theorems anew using the language of distri-
</p>
<p>butions. Under inessential additional assumptions, such proofs are sometimes even
</p>
<p>simpler. To illustrate this, assume, for instance, in Theorem 6.2.3 that the function
</p>
<p>H is continuous. One has to prove that Eg(H(ξn))&rarr; Eg(H(ξ)) for any continuous
bounded function g. But this is an immediate consequence of (6.2.1) and (6.2.2), for
</p>
<p>f = g ◦H (f is the composition of the functions g and H ).
In Theorem 6.2.3 assume that ξn &ge; 0 (this does not restrict the generality). Then,
</p>
<p>integrating by parts, we get
</p>
<p>Eξn =&minus;
&int; &infin;
</p>
<p>0
</p>
<p>x dP(ξn &ge; x)=
&int; &infin;
</p>
<p>0
</p>
<p>P(ξn &ge; x)dx. (6.2.5)
</p>
<p>Since by virtue of uniform integrability
</p>
<p>sup
n
</p>
<p>&int; &infin;
</p>
<p>N
</p>
<p>P(ξn &ge; x)dx &le; sup
n
</p>
<p>E(ξn; ξn &ge;N)&rarr; 0
</p>
<p>as N &rarr;&infin;, the integral in (6.2.5) is uniformly convergent. Moreover, P(ξn &ge; x)&rarr;
P(ξ &ge; x) a.s., and therefore
</p>
<p>lim
n&rarr;&infin;
</p>
<p>Eξn = lim
n&rarr;&infin;
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P(ξn &ge; x)dx =
&int; &infin;
</p>
<p>0
</p>
<p>P(ξ &ge; x)dx = Eξ. �
</p>
<p>Conditions ensuring uniform integrability are contained in Lemma 6.1.1. Now
</p>
<p>we will give a modification of assertion 4 of this lemma for the case of weak con-
</p>
<p>vergence.
</p>
<p>Lemma 6.2.3 If {ξn} is left uniformly integrable, ξn &rArr; ξ and Eξn &rarr; Eξ then {ξn}
is uniformly integrable.
</p>
<p>We suggest to the reader to construct examples showing that all three conditions
</p>
<p>of the lemma are essential.
</p>
<p>Lemma 6.2.3 implies, in particular, that if ξn &ge; 0, ξn &rArr; ξ and Eξn &rarr; Eξ then
{ξn} is uniformly integrable.
</p>
<p>As for Theorems 6.2.2 and 6.2.3, two alternative ways to prove the result are
</p>
<p>possible here. One of them consists of using Lemma 6.1.1. We will present here a
</p>
<p>different, somewhat simpler, proof.</p>
<p/>
</div>
<div class="page"><p/>
<p>146 6 On Convergence of Random Variables and Distributions
</p>
<p>Proof of Lemma 6.2.3 For simplicity assume that ξn &ge; 0. Suppose that the lemma
is not valid. Then there exist an ε &gt; 0 and subsequences n&prime; &rarr;&infin; and N(n&prime;)&rarr;&infin;
such that
</p>
<p>E
(
ξn&prime;; ξn&prime; &gt;N
</p>
<p>(
n&prime;
))
</p>
<p>&gt; ε.
</p>
<p>Since
</p>
<p>Eξn&prime; = E(ξn&prime;; ξn&prime; &le;N)+E(ξn&prime;; ξn&prime; &gt;N),
</p>
<p>for any N that is a point of continuity of the distribution of ξ , one has
</p>
<p>Eξ = lim
n&rarr;&infin;
</p>
<p>ξn&prime; &ge; E(ξ ; ξ &le;N)+ ε.
</p>
<p>Choose an N such that the first summand on the right-hand side exceeds Eξ &minus; ε/2.
Then we obtain the contradiction Eξ &ge; Eξ + ε/2, which proves the lemma.
</p>
<p>We leave it to the reader to extend the proof to the case of arbitrary left uniformly
</p>
<p>integrable {ξn}. �
</p>
<p>The following theorem can also be useful.
</p>
<p>Theorem 6.2.4 Suppose that ξn &rArr; ξ , H(s) is differentiable at a point a, and
bn &rarr; 0 as n&rarr;&infin;. Then
</p>
<p>1
</p>
<p>bn
</p>
<p>(
H(a + bnξn)&minus;H(a)
</p>
<p>)
&rArr; ξH &prime;(a).
</p>
<p>If H &prime;(a)= 0 and H &prime;&prime;(a) exists then
</p>
<p>1
</p>
<p>b2n
</p>
<p>(
H(a + bnξn)&minus;H(a)
</p>
<p>)
&rArr; ξ
</p>
<p>2
</p>
<p>2
H &prime;&prime;(a).
</p>
<p>Proof Consider the function
</p>
<p>h(x)=
{
</p>
<p>H(a+x)&minus;H(a)
x
</p>
<p>if x 
= 0,
H &prime;(a) if x = 0,
</p>
<p>which is continuous at the point x = 0. Since bnξn &rArr; 0, by Theorem 6.2.2 one has
h(bnξn)&rArr; h(0) = H &prime;(a). Using the theorem again (this time for two-dimensional
distributions), we get
</p>
<p>H(a + bnξn)&minus;H(a)
bn
</p>
<p>= h(bnξn)ξn &rArr;H &prime;(a)ξ.
</p>
<p>The second assertion is proved in the same way. �
</p>
<p>A multivariate analogue of this theorem will look somewhat more complicated.
</p>
<p>The reader could obtain it himself, following the lines of the argument proving The-
</p>
<p>orem 6.2.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Conditions for Weak Convergence 147
</p>
<p>6.3 Conditions for Weak Convergence
</p>
<p>Now we will return to the concept of weak convergence. We have two criteria for this
</p>
<p>convergence: relation (6.2.1) and Theorem 6.2.1. However, from the point of view
</p>
<p>of their possible applications (their verification in concrete problems) both these
</p>
<p>criteria are inconvenient. For instance, proving, say, convergence Ef (ξn)&rarr; Ef (ξ)
not for all continuous bounded functions f but just for elements f of a certain rather
</p>
<p>narrow class of functions that has a simple and clear nature would be much easier.
</p>
<p>It is obvious, however, that such a class cannot be very narrow.
</p>
<p>Before stating the basic assertions, we will introduce a few concepts.
</p>
<p>Extend the class F of all distribution functions to the class G of all functions
</p>
<p>G satisfying conditions F1 and F2 from Sect. 3.2 and conditions G(&minus;&infin;) &ge; 0,
G(&infin;)&le; 1. Functions G from G could be called generalised distribution functions.
One can think of them as distribution functions of improper random variables as-
</p>
<p>suming infinite values with positive probabilities, so that G(&minus;&infin;) = P(ξ = &minus;&infin;)
and 1 &minus; G(&infin;) = P(ξ = &infin;). We will write Gn &rArr; G for Gn &isin; G and G &isin; G if
Gn(x)&rarr;G(x) at all points of continuity of G(x).
</p>
<p>Theorem 6.3.1 (Helly) The class G is compact with respect to convergence &rArr;,
i.e. from any sequence {Gn}, Gn &isin; G, one can choose a convergent subsequence
Gnk &rArr;G &isin; G.
</p>
<p>For the proof of Theorem 6.3.1, see Appendix 4.
</p>
<p>Corollary 6.3.1 If each convergent subsequence {Gnk } of {Gn} with Gn &isin; G con-
verges to G then Gn &rArr;G.
</p>
<p>Proof If Gn �G then there exists a point of continuity x0 of G such that Gn(x0) 
&rarr;
G(x0). Since Gn(x0) &isin; [0,1], there exists a convergent subsequence Gnk such
that Gnk (x0) &rarr; g 
= G(x0). This, however, is impossible by our assumption, for
Gnk (x0)&rarr;G(x0). �
</p>
<p>The reason for extending the class F of all distribution functions is that it is not
</p>
<p>compact (in the sense of Theorem 6.3.1) and convergence Fn &rArr; G, Fn &isin; F, does
not imply that G &isin; F. For example, the sequence
</p>
<p>Fn(x)=
</p>
<p>⎧
⎨
⎩
</p>
<p>0 if x &le;&minus;n,
1/2 if &minus; n &lt; x &le; n,
1 if x &gt; n
</p>
<p>(6.3.1)
</p>
<p>converges everywhere to the function G(x)&equiv; 1/2 /&isin; F corresponding to an improper
random variable taking the values &plusmn;&infin; with probabilities 1/2.
</p>
<p>However, dealing with the class G is also not very convenient. The fact is that
</p>
<p>convergence at points of continuity Gn &rArr; G in the class G is not equivalent to
convergence
</p>
<p>&int;
f dGn &rarr;
</p>
<p>&int;
f dG</p>
<p/>
</div>
<div class="page"><p/>
<p>148 6 On Convergence of Random Variables and Distributions
</p>
<p>(see example (6.3.1) for f &equiv; 1), and the integrals
&int;
f dG do not specify G uniquely
</p>
<p>(they specify the increments of G, but not the values G(&minus;&infin;) and G(&infin;)). Now we
will introduce two concepts that will help to avoid the above-mentioned inconve-
</p>
<p>nience.
</p>
<p>Definition 6.3.1 A sequence of distributions {Fn} (or distribution functions {Fn})
is said to be tight if, for any ε &gt; 0, there exists an N such that
</p>
<p>inf
n
Fn
</p>
<p>(
[&minus;N,N ]
</p>
<p>)
&gt; 1 &minus; ε. (6.3.2)
</p>
<p>Definition 6.3.2 A class L of continuous bounded functions is said to be distribu-
tion determining if the equality
</p>
<p>&int;
f (x)dF (x)=
</p>
<p>&int;
f (x)dG(x), F &isin; F, G &isin; G,
</p>
<p>for all f &isin; L implies that F = G (or, which is the same, if the relation Ef (ξ) =
Ef (η) for all f &isin; L, where one of the random variables ξ and η is proper, implies
that ξ
</p>
<p>d= η).
</p>
<p>The next theorem is the main result of the present section.
</p>
<p>Theorem 6.3.2 Let L be a distribution determining class and {Fn} a sequence of
distributions. For the existence of a distribution F &isin; F such that Fn &rArr; F it is nec-
essary and sufficient that:3
</p>
<p>(1) the sequence {Fn} is tight; and
(2) limn&rarr;&infin;
</p>
<p>&int;
f dFn exists for all f &isin;L.
</p>
<p>Proof The necessary part is obvious.
Sufficiency. By Theorem 6.3.1 there exists a subsequence Fnk &rArr; F &isin; G. But
</p>
<p>by condition (1) one has F &isin; F. Indeed, if x &ge; N is a point of continuity of F
then, by Definition 6.3.1, F(x)= limFnk (x)&ge; 1 &minus; ε. In a similar way we establish
that for x &le; &minus;N one has F(x) &lt; ε. Since ε is arbitrary, we have F(&minus;&infin;)= 0 and
F(&infin;)= 1.
</p>
<p>Further, take another convergent subsequence Fn&prime;k
&rArr; G &isin; F. Then, for any
</p>
<p>f &isin;L, one has
</p>
<p>lim
</p>
<p>&int;
f dFnk =
</p>
<p>&int;
f dF, lim
</p>
<p>&int;
f dFn&prime;k
</p>
<p>=
&int;
</p>
<p>f dG. (6.3.3)
</p>
<p>But, by condition (2),
&int;
</p>
<p>f dF =
&int;
</p>
<p>f dG, (6.3.4)
</p>
<p>and hence F =G. The theorem is proved by virtue of Corollary 6.3.1. �
</p>
<p>3In this form the theorem persists for spaces of a more general nature. The role of the segments
</p>
<p>[&minus;N,N] in (6.3.2) is played in that case by compact sets (cf. [1, 14, 25, 31]).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Conditions for Weak Convergence 149
</p>
<p>Fig. 6.1 The plot of the
</p>
<p>function fa,ε(x) from
</p>
<p>Example 6.3.1
</p>
<p>If one needs to prove convergence to a &ldquo;known&rdquo; distribution F &isin; F, the tightness
condition in Theorem 6.3.2 becomes redundant.
</p>
<p>Corollary 6.3.2 Let L be a distribution determining class and
&int;
</p>
<p>f dFn &rarr;
&int;
</p>
<p>f dF, F &isin; G, (6.3.5)
</p>
<p>for any f &isin;L. Moreover, assume that at least one of the following three conditions
is met:
</p>
<p>(1) the sequence {Fn} is tight;
(2) F &isin; F;
(3) f &equiv; 1 &isin;L (i.e. (6.3.5) holds for f &equiv; 1).
</p>
<p>Then F &isin; F and Fn &rArr; F .
</p>
<p>The proof of the corollary is almost next to obvious. Under condition (1) the as-
sertion follows immediately from Theorem 6.3.2. Condition (3) and convergence
</p>
<p>(6.3.5) imply condition (2). If (2) holds, then F &isin; F in relations (6.3.3) and (6.3.4),
and therefore G= F . �
</p>
<p>Since, as a rule, at least one of conditions (1)&ndash;(3) is satisfied (as we will see
</p>
<p>below), the basic task is to verify convergence (6.3.5) for the class L.
</p>
<p>Note also that, in the case where one proves convergence to a distribution F &isin; F
&ldquo;known&rdquo; in advance, the whole arrangement of the argument can be different and
simpler. One such alternative approach is presented in Sect. 7.4.
</p>
<p>Now we will give several examples of distributions determining classes L.
</p>
<p>Example 6.3.1 The class L0 of functions having the form
</p>
<p>fa,ε(x)=
{
</p>
<p>1 if x &le; a,
0 if x &ge; a + ε.
</p>
<p>On the segment [a, a+ ε] the functions fa,ε are defined to be linear and continuous
(a plot of fa,ε(x) is given in Fig. 6.1). It is a two-parameter family of functions.
</p>
<p>We show that L0 is a distribution determining class. Let&int;
f dF =
</p>
<p>&int;
f dG
</p>
<p>for all f &isin;L0. Then
</p>
<p>F(a)&le;
&int;
</p>
<p>fa,ε dF =
&int;
</p>
<p>fa,ε dG&le;G(a + ε),</p>
<p/>
</div>
<div class="page"><p/>
<p>150 6 On Convergence of Random Variables and Distributions
</p>
<p>and, conversely,
</p>
<p>G(a)&le; F(a + ε)
for any ε &gt; 0. Taking a to be a point of continuity of both F and G, we obtain that
</p>
<p>F(a)=G(a).
Since this is valid for all points of continuity, we get F =G.
</p>
<p>One can easily verify in a similar way that the class L̂0 of &ldquo;trapezium-shaped&rdquo;
</p>
<p>functions f (x)= min(fa,ε,1 &minus; fb,ε), a &lt; b, is also distribution determining.
</p>
<p>Example 6.3.2 The class L1 of continuous bounded functions such that, for each
f &isin; L0 (or f &isin; L̂0), there exists a sequence fn &isin; L1, supx |f (x)| &lt; M &lt; &infin;, for
which limn&rarr;&infin; fn(x)= f (x) for each x &isin;R.
</p>
<p>Let &int;
f dF =
</p>
<p>&int;
f dG
</p>
<p>for all f &isin;L1. By the dominated convergence theorem,
</p>
<p>lim
</p>
<p>&int;
fn dF =
</p>
<p>&int;
f dF, lim
</p>
<p>&int;
fn dG=
</p>
<p>&int;
f dG, f &isin;L0.
</p>
<p>Therefore &int;
f dF =
</p>
<p>&int;
f dG, f &isin;L0, F = G
</p>
<p>and hence L1 is a distribution determining class.
</p>
<p>Example 6.3.3 The class Ck of all bounded functions f (x) having bounded uni-
formly continuous k-th derivatives f (k)(x) (supx |f (k)(x)|&lt;&infin;), k &ge; 1.
</p>
<p>It is evident that Ck is a distribution determining class for it is a special case of
</p>
<p>an L1 class.
</p>
<p>In the same way one can see that the subclass C0k &sub; Ck of functions having fi-
nite support (vanishing outside a finite interval) is also distribution determining.
</p>
<p>This follows from the fact that C0k is an L1-class with respect to the class L̂0 of
</p>
<p>trapezium-shaped (and therefore having compact support) functions.
</p>
<p>It is clear that the class Ck satisfies condition (3) from Corollary 6.3.2 (f &equiv;
1 &isin; Ck). Therefore, to prove convergence Fn &rArr; F &isin; F it suffices to verify conver-
gence (6.3.5) for f &isin; Ck only.
</p>
<p>If one takes L to be the class C0k of differentiable functions with finite sup-
</p>
<p>port then relation (6.3.5) together with condition (2) of Corollary 6.3.2 could be
</p>
<p>re-written as &int;
Fnf
</p>
<p>&prime; dx &rarr;
&int;
</p>
<p>Ff &prime; dx, F &isin; F. (6.3.6)
</p>
<p>(One has to integrate (6.3.5) by parts and use the fact that f &prime; also has a finite sup-
port.) The convergence criterion (6.3.6) is sometimes useful. It can be used to show,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Conditions for Weak Convergence 151
</p>
<p>for example, that (6.3.5) follows from convergence Fn(x)&rarr; F(x) at all points of
continuity of F (i.e. almost everywhere), since that convergence and the dominated
</p>
<p>convergence theorem imply (6.3.6) which is equivalent to (6.3.5).
</p>
<p>Example 6.3.4 One of the most important distribution determining classes is the
one-parameter family of complex-valued functions {eitx}, t &isin;R.
</p>
<p>The next chapter will be devoted to studying the properties of
&int;
eitxdF(x).
</p>
<p>After obvious changes, all the material in the present chapter can be extended to
</p>
<p>the multivariate case.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Characteristic Functions
</p>
<p>Abstract Section 7.1 begins with formal definitions and contains an extensive dis-
</p>
<p>cussion of the basic properties of characteristic functions, including those related to
</p>
<p>the nature of the underlying distributions. Section 7.2 presents the proofs of the in-
</p>
<p>version formulas for both densities and distribution functions, and also in the space
</p>
<p>of square integrable functions. Then the fundamental continuity theorem relating
</p>
<p>pointwise convergence of characteristic functions to weak convergence of the re-
</p>
<p>spective distributions is proved in Sect. 7.3. The result is illustrated by proving the
</p>
<p>Poisson theorem, with a bound for the convergence rate, in Sect. 7.4. After that,
</p>
<p>the previously presented theory is extended in Sect. 7.5 to the multivariate case.
</p>
<p>Some applications of characteristic functions are discussed in Sect. 7.6, including
</p>
<p>the stability properties of the normal and Cauchy distributions and an in-depth dis-
</p>
<p>cussion of the gamma distribution and its properties. Section 7.7 introduces the
</p>
<p>concept of generating functions and uses it to analyse the asymptotic behaviour
</p>
<p>of a simple Markov discrete time branching process. The obtained results include
</p>
<p>the formula for the eventual extinction probability, the asymptotic behaviour of the
</p>
<p>non-extinction probabilities in the critical case, and convergence in that case of the
</p>
<p>conditional distributions of the scaled population size given non-extinction to the
</p>
<p>exponential law.
</p>
<p>7.1 Definition and Properties of Characteristic Functions
</p>
<p>As a preliminary remark, note that together with real-valued random variables ξ(ω)
</p>
<p>we could also consider complex-valued random variables, by which we mean func-
</p>
<p>tions of the form ξ1(ω) + iξ2(ω), (ξ1, ξ2) being a random vector. It is natural to
put E(ξ1 + iξ2)= Eξ1 + iEξ2. Complex-valued random variables ξ = ξ1 + iξ2 and
η = η1 + iη2 are independent if the σ -algebras σ(ξ1, ξ2) and σ(η1, η2) generated
by the vectors (ξ1, ξ2) and (η1, η2), respectively, are independent. It is not hard to
</p>
<p>verify that, for such random variables,
</p>
<p>Eξη= EξEη.
</p>
<p>Definition 7.1.1 The characteristic function (ch.f.) of a real-valued random variable
ξ is the complex-valued function
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_7, &copy; Springer-Verlag London 2013
</p>
<p>153</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_7">http://dx.doi.org/10.1007/978-1-4471-5201-9_7</a></div>
</div>
<div class="page"><p/>
<p>154 7 Characteristic Functions
</p>
<p>ϕξ (t) := Eeitξ =
&int;
</p>
<p>eitx dF(x),
</p>
<p>where t is real.
</p>
<p>If the distribution function F(x) has a density f (x) then the ch.f. is equal to
</p>
<p>ϕξ (t)=
&int;
</p>
<p>eitxf (x)dx
</p>
<p>and is just the Fourier transform of the function f (x).1 In the general case, the ch.f.
</p>
<p>is the Fourier&ndash;Stieltjes transform of the function F(x).
The ch.f. exists for any random variable ξ . This follows immediately from the
</p>
<p>relation
</p>
<p>∣∣ϕξ (t)
∣∣&le;
</p>
<p>&int; ∣∣eitx
∣∣dF(x)&le;
</p>
<p>&int;
1dF(x)= 1.
</p>
<p>Ch.f.s are a powerful tool for studying properties of the sums of independent random
</p>
<p>variables.
</p>
<p>7.1.1 Properties of Characteristic Functions
</p>
<p>1. For any random variable ξ ,
</p>
<p>ϕξ (0)= 1 and
∣∣ϕξ (t)
</p>
<p>∣∣&le; 1 for all t.
This property is obvious.
</p>
<p>2. For any random variable ξ ,
</p>
<p>ϕaξ+b(t)= eitbϕξ (ta).
Indeed,
</p>
<p>ϕaξ+b(t)= Eeit (aξ+b) = eitbEeiatξ = eitbϕξ (ta). �
</p>
<p>1More precisely, in classical mathematical analysis, the Fourier transform ϕ(t) of a function f (t)
</p>
<p>from the space L1 of integrable functions is defined by the equation
</p>
<p>ϕ(t)= 1&radic;
2π
</p>
<p>&int;
eitxf (t) dt
</p>
<p>(the difference from ch.f. consists in the factor 1/
&radic;
</p>
<p>2π ). Under this definition the inversion formula
</p>
<p>has a &ldquo;symmetric&rdquo; form: if ϕ &isin; L1 then
</p>
<p>f (x)= 1&radic;
2π
</p>
<p>&int;
e&minus;itxϕ(t) dt.
</p>
<p>This representation is more symmetric than the inversion formula for ch.f. (7.2.1) in Sect. 7.2
</p>
<p>below.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Definition and Properties of Characteristic Functions 155
</p>
<p>3. If ξ1, . . . , ξn are independent random variables then the ch.f. of the sum Sn =
ξ1 + &middot; &middot; &middot; + ξn is equal to
</p>
<p>ϕSn(t)= ϕξ1(t) &middot; &middot; &middot;ϕξn(t).
</p>
<p>Proof This follows from the properties of the expectation of the product of inde-
pendent random variables. Indeed,
</p>
<p>ϕSn(t)= Eeit (ξ1+&middot;&middot;&middot;+ξn) = Eeitξ1eitξ2 &middot; &middot; &middot; eitξn
</p>
<p>= Eeitξ1Eeitξ2 &middot; &middot; &middot;Eeitξn = ϕξ1(t)ϕξ2(t) &middot; &middot; &middot;ϕξn(t). �
</p>
<p>Thus to the convolution Fξ1 &lowast; Fξ2 there corresponds the product ϕξ1ϕξ2 .
</p>
<p>4. The ch.f. ϕξ (t) is a uniformly continuous function.
Indeed, as h&rarr; 0,
</p>
<p>∣∣ϕ(t + h)&minus; ϕ(t)
∣∣=
</p>
<p>∣∣E
(
ei(t+h)ξ &minus; eitξ
</p>
<p>)∣∣&le; E
∣∣eihξ &minus; 1
</p>
<p>∣∣&rarr; 0
</p>
<p>by the dominated convergence theorem (see Corollary 6.1.2) since |eihξ &minus; 1| p&minus;&rarr; 0
as h&rarr; 0, and |eihξ &minus; 1| &le; 2. �
</p>
<p>5. If the k-th moment exists: E|ξ |k &lt;&infin;, k &ge; 1, then there exists a continuous k-th
derivative of the function ϕξ (t), and ϕ(k)(0)= ikEξ k .
</p>
<p>Proof Indeed, since
∣∣∣∣
&int;
</p>
<p>ixeitx dF(x)
</p>
<p>∣∣∣∣&le;
&int;
</p>
<p>|x|dF(x)= E|ξ |&lt;&infin;,
</p>
<p>the integral
&int;
ixeitx dF(x) converges uniformly in t . Therefore one can differentiate
</p>
<p>under the integral sign:
</p>
<p>ϕ&prime;(t)= i
&int;
</p>
<p>xeitx dF(x), ϕ&prime;(0)= iEξ.
</p>
<p>Further, one can argue by induction. If, for l &lt; k,
</p>
<p>ϕ(l)(t)= il
&int;
</p>
<p>xleitx dF(x),
</p>
<p>then
</p>
<p>ϕ(l+1)(t)= il+1
&int;
</p>
<p>xl+1eitx dF(x)
</p>
<p>by the uniform convergence of the integral on the right-hand side. Therefore
</p>
<p>ϕ(l+1)(0)= il+1Eξ l+1. �</p>
<p/>
</div>
<div class="page"><p/>
<p>156 7 Characteristic Functions
</p>
<p>Property 5 implies that if E|ξ |k &lt;&infin; then, in a neighbourhood of the point t = 0,
one has the expansion
</p>
<p>ϕ(t)= 1 +
k&sum;
</p>
<p>j=1
</p>
<p>(it)j
</p>
<p>j ! Eξ
j + o
</p>
<p>(∣∣tk
∣∣). (7.1.1)
</p>
<p>The converse assertion is only partially true:
</p>
<p>If a derivative of an even order ϕ(2k) exists then
</p>
<p>E|ξ |2k &lt;&infin;, ϕ(2k)(0)= (&minus;1)kEξ2k.
</p>
<p>We will prove the property for k = 1 (for k &gt; 1 one can employ induction). It
suffices to verify that E|ξ |2 is finite. One has
</p>
<p>&minus;2ϕ(0)&minus; ϕ(2h)&minus; ϕ(&minus;2h)
4h2
</p>
<p>= E
(
eihξ &minus; e&minus;ihξ
</p>
<p>2h
</p>
<p>)2
= E sin
</p>
<p>2 hξ
</p>
<p>h2
.
</p>
<p>Since h&minus;2 sin2 hξ &rarr; ξ2 as h&rarr; 0, by Fatou&rsquo;s lemma
</p>
<p>&minus;ϕ&prime;&prime;(0)= lim
h&rarr;0
</p>
<p>(
2ϕ(0)&minus; ϕ(2h)&minus; ϕ(&minus;2h)
</p>
<p>4h2
</p>
<p>)
= lim
</p>
<p>h&rarr;0
E
</p>
<p>sin2 hξ
</p>
<p>h2
</p>
<p>&ge; E lim
h&rarr;0
</p>
<p>sin2 hξ
</p>
<p>h2
= Eξ2. �
</p>
<p>6. If ξ &ge; 0 then ϕξ (λ) is defined in the complex plane for Imλ &ge; 0. Moreover,
|ϕξ (λ)| &le; 1 for such λ, and in the domain Imλ &gt; 0, ϕξ (λ) is analytic and con-
tinuous including on the boundary Imλ= 0.
</p>
<p>Proof That ϕ(λ) is analytic follows from the fact that, for Imλ &gt; 0, one can differ-
entiate under the integral sign the right-hand side of
</p>
<p>ϕξ (λ)=
&int; &infin;
</p>
<p>0
</p>
<p>eiλx dF(x).
</p>
<p>(For Imλ &gt; 0 the integrand decreases exponentially fast as x &rarr;&infin;.) �
</p>
<p>Continuity is proved in the same way as in property 4. This means that for non-
</p>
<p>negative ξ the ch.f. ϕξ (λ) uniquely determines the function
</p>
<p>ψ(s)= ϕξ (is)= Ee&minus;sξ
</p>
<p>of real variable s &ge; 0, which is called the Laplace (or Laplace&ndash;Stieltjes) transform
of the distribution of ξ .
</p>
<p>The converse assertion also follows from properties of analytic functions: the
</p>
<p>Laplace transform ψ(s) on the half-line s &ge; 0 uniquely determines the ch.f. ϕξ (λ).
</p>
<p>7. ϕξ (t)= ϕξ (&minus;t)= ϕ&minus;ξ (t), where the bar denotes the complex conjugate.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Definition and Properties of Characteristic Functions 157
</p>
<p>Proof The relations follow from the equalities
</p>
<p>ϕξ (t)= Eeitξ = Eeitξ = Ee&minus;itξ . �
</p>
<p>This implies the following property.
</p>
<p>7A. If ξ is symmetric (has the same distribution as&minus;ξ ) then its ch.f. is real (ϕξ (t)=
ϕξ (&minus;t)).
</p>
<p>One can show that the converse is also true; to this end one has to make use of
</p>
<p>the uniqueness theorem to be discussed below.
</p>
<p>Now we will find the ch.f.s of the basic probability laws.
</p>
<p>Example 7.1.1 If ξ = a with probability 1, i.e. ξ &sub;= Ia , then ϕξ (t)= eita .
</p>
<p>Example 7.1.2 If ξ &sub;=Bp then ϕξ (t)= peit + (1 &minus; p)= 1 + p(eit &minus; 1).
</p>
<p>Example 7.1.3 If ξ &sub;=�0,1 then ϕξ (t)= e&minus;t
2/2.
</p>
<p>Indeed,
</p>
<p>ϕ(t)= ϕξ (t)=
1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
eitx&minus;x
</p>
<p>2/2 dx.
</p>
<p>Differentiating with respect to t and integrating by parts (xe&minus;x
2/2 dx =&minus;de&minus;x2/2),
</p>
<p>we get
</p>
<p>ϕ&prime;(t)= 1&radic;
2π
</p>
<p>&int;
ixeitx&minus;x
</p>
<p>2/2 dx =&minus; 1&radic;
2π
</p>
<p>&int;
teitx&minus;x
</p>
<p>2/2 dx =&minus;tϕ(t),
</p>
<p>(
lnϕ(t)
</p>
<p>)&prime; =&minus;t, lnϕ(t)=&minus; t
2
</p>
<p>2
+ c.
</p>
<p>Since ϕ(0)= 1, one has c= 0 and ϕ(t)= e&minus;t2/2. �
</p>
<p>Now let η be a normal random variable with parameters (a, σ ). Then it can be
</p>
<p>represented as η = σξ + a, where ξ is normally distributed with parameters (0,1).
The ch.f. of η can be found using Property 2:
</p>
<p>ϕη(t)= eitae&minus;(tσ )
2/2 = eita&minus;t2σ 2/2.
</p>
<p>Differentiating ϕη(t) for η &sub;= �0,σ 2 , we will obtain that Eηk = 0 for odd k, and
Eηk = σ k(k &minus; 1)(k &minus; 3) &middot; &middot; &middot;1 for k = 2,4, . . . .
</p>
<p>Example 7.1.4 If ξ &sub;=�&micro; then
</p>
<p>ϕξ (t)= Eeitξ =
&sum;
</p>
<p>k
</p>
<p>eitk
&micro;k
</p>
<p>k! e
&minus;&micro; = e&minus;&micro;
</p>
<p>&sum;
</p>
<p>k
</p>
<p>(&micro;eit )k
</p>
<p>k! = e
&minus;&micro;e&micro;e
</p>
<p>it = exp
[
&micro;
(
eit &minus; 1
</p>
<p>)]
.</p>
<p/>
</div>
<div class="page"><p/>
<p>158 7 Characteristic Functions
</p>
<p>Example 7.1.5 If ξ has the exponential distribution Ŵα with density αe&minus;αx for
x &ge; 0, then
</p>
<p>ϕξ (t)= α
&int; &infin;
</p>
<p>0
</p>
<p>eitx&minus;αx dx = α
α &minus; it .
</p>
<p>Therefore, if ξ has the &ldquo;double&rdquo; exponential distribution with density 1
2
e&minus;|x|, &minus;&infin;&lt;
</p>
<p>x &lt;&infin;, then
</p>
<p>ϕξ (t)=
1
</p>
<p>2
</p>
<p>(
1
</p>
<p>1 &minus; it +
1
</p>
<p>1 + it
</p>
<p>)
= 1
</p>
<p>1 + t2 .
</p>
<p>If ξ has the geometric distribution P(ξ = k)= (1 &minus; p)pk , k = 0,1, . . . , then
</p>
<p>ϕξ (t)=
1 &minus; p
</p>
<p>1 &minus; peit .
</p>
<p>Example 7.1.6 If ξ &sub;= K0,1 (has the density [π(1 + x2)]&minus;1) then ϕξ (t)= e&minus;|t |. The
reader will easily be able to prove this somewhat later, using the inversion formula
</p>
<p>and Example 7.1.5.
</p>
<p>Example 7.1.7 If ξ &sub;=U0,1, then
</p>
<p>ϕξ (t)=
&int; 1
</p>
<p>0
</p>
<p>eitx dx = e
it &minus; 1
it
</p>
<p>.
</p>
<p>By virtue of Property 3, the ch.f.s of the sums ξ1 + ξ2, ξ1 + ξ2 + ξ3, . . . that we
considered in Example 3.6.1 will be equal to
</p>
<p>ϕξ1+ξ2(t)=&minus;
(eit &minus; 1)2
</p>
<p>t2
, ϕξ1+ξ2+ξ3(t)=&minus;
</p>
<p>(eit &minus; 1)3
t3
</p>
<p>, . . . .
</p>
<p>We return to the general case. How can one verify whether one or another func-
</p>
<p>tion ϕ is characteristic or not? Sometimes one can do this using the above properties.
</p>
<p>We suggest the reader to determine whether the functions (1+ t)&minus;1, 1+ t , sin t , cos t
are characteristic, and if so, to which distributions they correspond.
</p>
<p>In the general case the posed question is a difficult one. We state without proof
</p>
<p>one of the known results.
</p>
<p>Bochner&ndash;Khinchin&rsquo;s Theorem A necessary and sufficient condition for a con-
tinuous function ϕ(t) with ϕ(0) = 1 to be characteristic is that it is nonnegatively
defined, i.e., for any real t1, . . . , tn and complex λ1, . . . , λn, one has
</p>
<p>n&sum;
</p>
<p>k,j=1
ϕ(tk &minus; tj )λkλj &ge; 0
</p>
<p>(λ is the complex conjugate of λ).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Definition and Properties of Characteristic Functions 159
</p>
<p>Note that the necessity of this condition is almost obvious, for if ϕ(t) = Eeitξ
then
</p>
<p>n&sum;
</p>
<p>k,j=1
ϕ(tk &minus; tj )λkλj = E
</p>
<p>n&sum;
</p>
<p>k,j=1
ei(tk&minus;tj )ξλkλj = E
</p>
<p>∣∣∣∣∣
</p>
<p>n&sum;
</p>
<p>k=1
λke
</p>
<p>itkξ
</p>
<p>∣∣∣∣∣
</p>
<p>2
</p>
<p>&ge; 0.
</p>
<p>7.1.2 The Properties of Ch.F.s Related to the Structure of the
</p>
<p>Distribution of ξ
</p>
<p>8. If the distribution of ξ has a density then ϕξ (t)&rarr; 0 as |t | &rarr;&infin;.
This is a direct consequence of the Lebesgue theorem on Fourier transforms. The
</p>
<p>converse assertion is false.
</p>
<p>In general, the smoother F(x) is the faster ϕξ (t) vanishes as |t | &rarr;&infin;. The for-
mulas in Example 7.1.7 are typical in this respect. If the density f (x) has an inte-
</p>
<p>grable k-th derivative then, by integrating by parts, we get
</p>
<p>ϕξ (t)=
&int;
</p>
<p>eitxf (x)dx = 1
it
</p>
<p>&int;
eitxf &prime;(x) dx = &middot; &middot; &middot; = 1
</p>
<p>(it)k
</p>
<p>&int;
eitxf (k)(x) dx,
</p>
<p>which implies that
</p>
<p>ϕξ (t)&le;
c
</p>
<p>|t |k .
</p>
<p>8A. If the distribution of ξ has a density of bounded variation then
∣∣ϕξ (t)
</p>
<p>∣∣&le; c|t | .
</p>
<p>This property is also validated by integration by parts:
</p>
<p>∣∣ϕξ (t)
∣∣=
</p>
<p>∣∣∣∣
1
</p>
<p>it
</p>
<p>&int;
eitx df (x)
</p>
<p>∣∣∣∣&le;
1
</p>
<p>|t |
</p>
<p>&int; ∣∣df (x)
∣∣.
</p>
<p>9. A random variable ξ has a lattice distribution with span h &gt; 0 (see Defini-
tion 3.2.3) if and only if
</p>
<p>∣∣∣∣ϕξ
(
</p>
<p>2π
</p>
<p>h
</p>
<p>)∣∣∣∣= 1,
∣∣∣∣ϕξ
</p>
<p>(
v
</p>
<p>h
</p>
<p>)∣∣∣∣&lt; 1 (7.1.2)
</p>
<p>if v is not a multiple of 2π .
Clearly, without loss of generality we can assume h= 1. Moreover, since
</p>
<p>∣∣ϕξ&minus;a(t)
∣∣=
</p>
<p>∣∣e&minus;itaϕξ (t)
∣∣=
</p>
<p>∣∣ϕξ (t)
∣∣,
</p>
<p>the properties (7.1.2) are invariant with respect to the shift by a. Thus we can as-
</p>
<p>sume the shift a is equal to zero and thus change the lattice distribution condition
</p>
<p>in Property 9 to the arithmeticity condition (see Definition 3.2.3). Since ϕξ (t) is a
</p>
<p>periodic function, Property 9 can be rewritten in the following equivalent form:</p>
<p/>
</div>
<div class="page"><p/>
<p>160 7 Characteristic Functions
</p>
<p>The distribution of a random variable ξ is arithmetic if and only if
</p>
<p>ϕξ (2π)= 1,
∣∣ϕξ (t)
</p>
<p>∣∣&lt; 1 for all t &isin; (0,2π). (7.1.3)
</p>
<p>Proof If ξ has an arithmetic distribution then
</p>
<p>ϕξ (t)=
&sum;
</p>
<p>k
</p>
<p>P(ξ = k)eitk = 1
</p>
<p>for t = 2π . Now let us prove the second relation in (7.1.3). Assume the contrary:
for some v &isin; (0,2π), we have |ϕξ (v)| = 1 or, which is the same,
</p>
<p>ϕξ (v)= eibv
</p>
<p>for some real b. The last relation implies that
</p>
<p>ϕξ&minus;b(v)= 1 = E cosv(ξ &minus; b)+ iE sinv(ξ &minus; b), E
[
1 &minus; cosv(ξ &minus; b)
</p>
<p>]
= 0.
</p>
<p>Hence, by Property E4 in Sect. 4.1, cosv(ξ &minus; b)= 1 and v(ξ &minus; b)= 2πk(ω) with
probability 1, where k(ω) is an integer. Thus ξ &minus; b is a multiple of 2π/v &gt; 1.
This contradicts the assumption that the span of the lattice equals 1, and hence
</p>
<p>proves (7.1.3).
</p>
<p>Conversely, let (7.1.3) hold. As we saw, the first relation in (7.1.3) implies that
</p>
<p>ξ takes only integer values. If we assume that the lattice span equals h &gt; 1 then,
</p>
<p>by the first part of the proof and the first relation in (7.1.2), we get |ϕ(2π/h)| = 1,
which contradicts the first relation in (7.1.3). Property 9 is proved. �
</p>
<p>The next definition looks like a tautology.
</p>
<p>Definition 7.1.2 The distribution of ξ is called non-lattice if it is not a lattice distri-
bution.
</p>
<p>10. If the distribution of ξ is non-lattice then
∣∣ϕξ (t)
</p>
<p>∣∣&lt; 1 for all t 
= 0.
</p>
<p>Proof Indeed, if we assume the contrary, i.e. that |ϕ(u)| = 1 for some u 
= 0, then,
by Property 9, we conclude that the distribution of ξ is a lattice with span h= 2π/u
or with a lesser span. �
</p>
<p>11. If the distribution of ξ has an absolutely continuous component of a positive
mass p &gt; 0, then it is clearly non-lattice and, moreover,
</p>
<p>lim sup
|t |&rarr;&infin;
</p>
<p>∣∣ϕξ (t)
∣∣&le; 1 &minus; p.
</p>
<p>This assertion follows from Property 8.
</p>
<p>Arithmetic distributions occupy an important place in the class of lattice distri-
</p>
<p>butions.
</p>
<p>For arithmetic distributions, the ch.f. ϕξ (t) is a function of the variable z = eit
and is periodic in t with period 2π . Hence, in this case it is sufficient to know the</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Inversion Formulas 161
</p>
<p>behaviour of the ch.f. on the interval [&minus;π,π] or, which is the same, to know the
behaviour of the function
</p>
<p>pξ (z) := Ezξ =
&sum;
</p>
<p>zkP(ξ = k)
</p>
<p>on the unit circle |z| = 1.
</p>
<p>Definition 7.1.3 The function pξ (z) is called the generating function of the random
variable ξ (or of the distribution of ξ ).
</p>
<p>Since pξ (e
it )= ϕξ (t) is a ch.f., all the properties of ch.f.s remain valid for gener-
</p>
<p>ating functions, with the only changes corresponding to the change of variable. For
</p>
<p>more on applications of generating functions, see Sect. 7.7.
</p>
<p>7.2 Inversion Formulas
</p>
<p>Thus for any random variable there exists a corresponding ch.f. We will now show
</p>
<p>that the set L of functions eitx is a distribution determining class, i.e. that the dis-
</p>
<p>tribution can be uniquely reconstructed from its ch.f. This is proved using inversion
</p>
<p>formulas.
</p>
<p>7.2.1 The Inversion Formula for Densities
</p>
<p>Theorem 7.2.1 If the ch.f. ϕ(t) of a random variable ξ is integrable then the distri-
bution of ξ has the bounded density
</p>
<p>f (x)= 1
2π
</p>
<p>&int;
e&minus;itxϕ(t) dt. (7.2.1)
</p>
<p>This fact is known from classical Fourier analysis, but we shall give a proof of a
</p>
<p>probabilistic character.
</p>
<p>Proof First we will establish the following (Parseval&rsquo;s) identity: for any fixed ε &gt; 0,
</p>
<p>pε(t) :=
1
</p>
<p>2π
</p>
<p>&int;
e&minus;ituϕ(u)e&minus;ε
</p>
<p>2u2/2 du
</p>
<p>&equiv; 1&radic;
2πε
</p>
<p>&int;
exp
</p>
<p>{
&minus; (u&minus; t)
</p>
<p>2
</p>
<p>2ε2
</p>
<p>}
F(du), (7.2.2)
</p>
<p>where F is the distribution of ξ . We begin with the equality
</p>
<p>1&radic;
2π
</p>
<p>&int;
exp
</p>
<p>{
ix
</p>
<p>ξ &minus; t
ε
</p>
<p>&minus; x
2
</p>
<p>2
</p>
<p>}
dx = exp
</p>
<p>{
&minus; (ξ &minus; t)
</p>
<p>2
</p>
<p>2ε2
</p>
<p>}
, (7.2.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>162 7 Characteristic Functions
</p>
<p>both sides of which being the value of the ch.f. of the normal distribution with
</p>
<p>parameters (0,1) at the point (ξ &minus; t)/ε. After changing the variable x = εu, the
left-hand side of this equality can be rewritten as
</p>
<p>ε&radic;
2π
</p>
<p>&int;
exp
</p>
<p>{
iu(ξ &minus; t)&minus; ε
</p>
<p>2u2
</p>
<p>2
</p>
<p>}
du.
</p>
<p>If we take expectations of both sides of (7.2.3), we obtain
</p>
<p>ε&radic;
2π
</p>
<p>&int;
e&minus;iutϕ(u)e&minus;
</p>
<p>ε2u2
</p>
<p>2 du=
&int;
</p>
<p>exp
</p>
<p>{
&minus; (u&minus; t)
</p>
<p>2
</p>
<p>2ε2
</p>
<p>}
F(du).
</p>
<p>This proves (7.2.2).
</p>
<p>To prove the theorem first consider the left-hand side of the equality (7.2.2). Since
</p>
<p>e&minus;ε
2u2/2 &rarr; 1 as ε&rarr; 0, |e&minus; ε
</p>
<p>2u2
</p>
<p>2 | &le; 1 and ϕ(u) is integrable, as ε&rarr; 0 one has
</p>
<p>pε(t)&rarr;
1
</p>
<p>2π
</p>
<p>&int;
e&minus;ituϕ(u)du= p0(t) (7.2.4)
</p>
<p>uniformly in t , because the integral on the left-hand side of (7.2.2) is uniformly
</p>
<p>continuous in t . This implies, in particular, that
</p>
<p>&int; b
</p>
<p>a
</p>
<p>pε(t) dt &rarr;
&int; b
</p>
<p>a
</p>
<p>p0(t). (7.2.5)
</p>
<p>Now consider the right-hand side of (7.2.2). It represents the density of the sum
</p>
<p>ξ + εη, where ξ and η are independent and η&sub;=�0,1. Therefore
&int; b
</p>
<p>a
</p>
<p>pε(t) dt = P(a &lt; ξ + εη &le; b). (7.2.6)
</p>
<p>Since ξ + εη p&rarr; ξ as ε&rarr; 0 and the limit
&int; b
a
pε(t) dt exists for any fixed a and b by
</p>
<p>virtue of (7.2.5), this limit (see (7.2.6)) cannot be anything other than F([a, b)).
Thus, from (7.2.5) and (7.2.6) we get
</p>
<p>&int; b
</p>
<p>a
</p>
<p>p0(t) dt = F
(
[a, b)
</p>
<p>)
.
</p>
<p>This means that the distribution F has the density p0(t), which is defined by re-
</p>
<p>lation (7.2.4). The boundedness of p0(t) evidently follows from the integrability
</p>
<p>of ϕ:
</p>
<p>p0(t)&le;
1
</p>
<p>2π
</p>
<p>&int; ∣∣ϕ(t)
∣∣dt &lt;&infin;.
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Inversion Formulas 163
</p>
<p>7.2.2 The Inversion Formula for Distributions
</p>
<p>Theorem 7.2.2 If F(x) is the distribution function of a random variable ξ and ϕ(t)
is its ch.f., then, for any points of continuity x and y of the function F(x),2
</p>
<p>F(y)&minus; F(x)= 1
2π
</p>
<p>lim
σ&rarr;0
</p>
<p>&int;
e&minus;itx &minus; e&minus;ity
</p>
<p>it
ϕ(t)e&minus;t
</p>
<p>2σ 2 dt. (7.2.7)
</p>
<p>If the function ϕ(t)/t is integrable at infinity then the passage to the limit under the
integral sign is justified and one can write
</p>
<p>F(y)&minus; F(x)= 1
2π
</p>
<p>&int;
e&minus;itx &minus; e&minus;ity
</p>
<p>it
ϕ(t) dt. (7.2.8)
</p>
<p>Proof Suppose first that the ch.f. ϕ(t) is integrable. Then F(x) has a density f (x)
and the assertion of the theorem in the form (7.2.8) follows if we integrate both sides
</p>
<p>of Eq. (7.2.1) over the interval with the end points x and y and change the order of
</p>
<p>integration (which is valid because of the absolute convergence).3
</p>
<p>Now let ϕ(t) be the characteristic function of a random variable ξ with an ar-
</p>
<p>bitrary distribution F. On a common probability space with ξ , consider a random
</p>
<p>variable η which is independent of ξ and has the normal distribution with parame-
</p>
<p>ters (0,2σ 2). As we have already pointed out, the ch.f. of η is e&minus;t
2σ 2 .
</p>
<p>This means that the ch.f. of ξ+η, being equal to ϕ(t)e&minus;t2σ 2 , is integrable. There-
fore by (7.2.8) one will have
</p>
<p>Fξ+η(y)&minus; Fξ+η(x)=
1
</p>
<p>2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>e&minus;itx &minus; e&minus;ity
it
</p>
<p>ϕ(t)e&minus;t
2σ 2 dt. (7.2.9)
</p>
<p>Since η
p&minus;&rarr; 0 as σ &rarr; 0, we have Fξ+η &rArr; F (see Chap. 6). Therefore, if x and y are
</p>
<p>points of continuity of F, then F(y)&minus; F(x)= limσ&rarr;0(Fξ+η(y)&minus; Fξ+η(x)). This,
together with (7.2.9), proves the assertion of the theorem. �
</p>
<p>In the proof of Theorem 7.2.2 we used a method which might be called the
</p>
<p>&ldquo;smoothing&rdquo; of distributions. It is often employed to overcome technical difficul-
</p>
<p>ties related to the inversion formula.
</p>
<p>Corollary 7.2.1 (Uniqueness Theorem) The ch.f. of a random variable uniquely
determines its distribution function.
</p>
<p>2In the literature, the inversion formula is often given in the form
</p>
<p>F(y)&minus; F(x)= 1
2π
</p>
<p>lim
A&rarr;&infin;
</p>
<p>&int; A
</p>
<p>&minus;A
</p>
<p>e&minus;itx &minus; e&minus;ity
it
</p>
<p>ϕ(t) dt
</p>
<p>which is equivalent to (7.2.7).
</p>
<p>3Formula (7.2.8) can also be obtained from (7.2.1) without integration by noting that
</p>
<p>(F (x) &minus; F(y))/(y &minus; x) is the value at zero of the convolution of two densities: f (x) and the
uniform density over the interval [&minus;y,&minus;x] (see also the remark at the end of Sect. 3.6). The ch.f.
of the convolution is equal to e
</p>
<p>&minus;itx&minus;e&minus;ity
(y&minus;x)it ϕ(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>164 7 Characteristic Functions
</p>
<p>The proof follows from the inversion formula and the fact that F is uniquely
</p>
<p>determined by the differences F(y)&minus; F(x).
For lattice random variables the inversion formula becomes simpler. Let, for the
</p>
<p>sake of simplicity, ξ be an integer-valued random variable.
</p>
<p>Theorem 7.2.3 If pξ (z) := Ezξ is the generating function of an arithmetic random
variable then
</p>
<p>P(ξ = k)= 1
2πi
</p>
<p>&int;
</p>
<p>|z|=1
pξ (z)z
</p>
<p>&minus;k&minus;1 dz. (7.2.10)
</p>
<p>Proof Turning to the ch.f. ϕξ (t)=
&sum;
</p>
<p>j e
itjP(ξ = j) and changing the variables z=
</p>
<p>it in (7.2.10) we see that the right-hand side of (7.2.10) equals
</p>
<p>1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
e&minus;itkϕξ (t) dt =
</p>
<p>1
</p>
<p>2π
</p>
<p>&sum;
</p>
<p>j
</p>
<p>P(ξ = j)
&int; π
</p>
<p>&minus;π
eit (j&minus;k) dt.
</p>
<p>Here all the integrals on the right-hand side are equal to zero, except for the integral
</p>
<p>with j = k which is equal to 2π . Thus the right-hand side itself equals P(ξ = k).
The theorem is proved. �
</p>
<p>Formula (7.2.10) is nothing else but the formula for Fourier coefficients and has
</p>
<p>a simple geometric interpretation. The functions {ek = eitk} form an orthonormal
basis in the Hilbert space L2(&minus;π,π) of square integrable complex-valued functions
with the inner product
</p>
<p>(f, g)= 1
2π
</p>
<p>&int; π
</p>
<p>&minus;π
f (t)g(t) dt
</p>
<p>(g is the complex conjugate of g). If ϕξ =
&sum;
</p>
<p>ekP(ξ = k) then it immediately follows
from the equality ϕξ =
</p>
<p>&sum;
ek(ξ , ek) that
</p>
<p>P(ξ = k)= (ϕξ , ek)=
1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
e&minus;itkϕξ (t) dt.
</p>
<p>7.2.3 The Inversion Formula in L2. The Class of Functions that
</p>
<p>Are Both Densities and Ch.F.s
</p>
<p>First consider some properties of ch.f.s related to the inversion formula. As a prelim-
</p>
<p>inary, note that, in classical Fourier analysis, one also considers the Fourier trans-
</p>
<p>forms of functions f from the space L2 of square-integrable functions. Since in this
</p>
<p>case a function f is not necessarily integrable, the Fourier transform is defined as</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Inversion Formulas 165
</p>
<p>the integral in the principal value sense:4
</p>
<p>ϕ(t) := lim
N&rarr;&infin;
</p>
<p>ϕ(N)(t), ϕ(N)(t) :=
&int; N
</p>
<p>&minus;N
eitxf (x)dx, (7.2.11)
</p>
<p>where the limit is taken in the sense of convergence in L2:
&int; ∣∣ϕ(t)&minus; ϕ(N)(t)
</p>
<p>∣∣2 dx &rarr; 0 as N &rarr;&infin;.
</p>
<p>Since by Parseval&rsquo;s equality
</p>
<p>‖f ‖L2 =
1
</p>
<p>2π
‖ϕ‖L2 , where ‖g‖L2 =
</p>
<p>[&int;
|g|2(t) dt
</p>
<p>]1/2
,
</p>
<p>the Fourier transform maps the space L2 into itself (there is no such isometricity
</p>
<p>for Fourier transforms in L1). Here the inversion formula (7.2.1) holds true but the
integral in (7.2.1) is understood in the principal value sense.
</p>
<p>Denote by F and H the class of all densities and the class of all ch.f.s, respec-
</p>
<p>tively, and by H1,+ &sub; L1 the class of nonnegative real-valued integrable ch.f.s,
so that the elements of H1,+ are in F up to the normalising factors. Further, let
(H1,+)(&minus;1) be the inverse image of the class H1,+ in F for the mapping f &rarr; ϕ,
i.e. the class of densities whose ch.f.s lie in H1,+. It is clear that functions f
from (H1,+)(&minus;1) and ϕ from H1,+ are necessarily symmetric (see Property 7A in
Sect. 7.1) and that f (0) &isin; (0,&infin;). The last relation follows from the fact that, by the
inversion formula for ϕ &isin;H1,+, we have
</p>
<p>‖ϕ‖ := ‖ϕ‖L1 =
&int;
</p>
<p>ϕ(t) dt = 2πf (0).
</p>
<p>Further, denote by (H1,+)‖&middot;‖ the class of normalised functions
ϕ
‖ϕ‖ , ϕ &isin;H1,+, so
</p>
<p>that (H1,+)‖&middot;‖ &sub; F, and denote by F(2,&lowast;) the class of convolutions of symmetric
densities from L2:
</p>
<p>F(2,&lowast;) :=
{
f (2)&lowast;(x) : f &isin; L2, f is symmetric
</p>
<p>}
,
</p>
<p>where
</p>
<p>f (2)&lowast;(x)=
&int;
</p>
<p>f (t)f (x &minus; t) dt.
</p>
<p>Theorem 7.2.4 The following relations hold true:
</p>
<p>(H1,+)
(&minus;1) = (H1,+)‖&middot;‖, F(2,&lowast;) &sub; (H1,+)‖&middot;‖.
</p>
<p>The class (H1,+)‖&middot;‖ may be called the class of densities conjugate to f &isin;
(H1,+)(&minus;1). It turns out that this class coincides with the inverse image (H1,+)(&minus;1).
The second statement of the theorem shows that this inverse image is a very rich
</p>
<p>4Here we again omit the factor 1&radic;
2π
</p>
<p>(cf. the footnote on page 154).</p>
<p/>
</div>
<div class="page"><p/>
<p>166 7 Characteristic Functions
</p>
<p>class and provides sufficient conditions for the density f to have a conjugate. We
</p>
<p>will need these conditions in Sect. 8.7.
</p>
<p>Proof of Theorem 7.2.4 Let f &isin; (H1,+)(&minus;1). Then the corresponding ch.f. ϕ is in
H1+ and the inversion formula (7.2.1) is applicable. Multiplying its right-hand side
by 2π‖ϕ‖ , we obtain an expression for the ch.f. (at the point &minus;t) of the density
</p>
<p>ϕ
‖ϕ‖
</p>
<p>(recall that ϕ &ge; 0 is symmetric if ϕ &isin; H1,+). This means that 2πf‖ϕ‖ is a ch.f. and,
moreover, that f &isin; (H1,+)‖&middot;‖.
</p>
<p>Conversely, suppose that f &lowast; := ϕ‖ϕ‖ &isin; (H1,+)‖&middot;‖. Then f &lowast; &isin; F is symmetric, and
the inversion formula can be applied to ϕ:
</p>
<p>f (x)= 1
2π
</p>
<p>&int;
e&minus;itxϕ(t) dt = 1
</p>
<p>2π
</p>
<p>&int;
eitxϕ(t) dt,
</p>
<p>2πf (t)
</p>
<p>‖ϕ‖ =
&int;
</p>
<p>eitxf &lowast;(x) dx.
</p>
<p>Since the ch.f. ϕ&lowast;(t) := 2πf (t)‖ϕ‖ belongs to H1,+, one has f &lowast; &isin; (H1,+)(&minus;1).
We now prove the second assertion. Suppose that f &isin; L2. Then ϕ &isin; L2 and
</p>
<p>ϕ2 &isin; L1. Moreover, by virtue of the symmetry of f and Property 7A in Sect. 7.1,
the function ϕ is real-valued, so ϕ2 &ge; 0. This implies that ϕ2 &isin;H1,+. Since ϕ2 is
the ch.f. of the density f (2)&lowast;, we have f (2)&lowast; &isin; (H1,+)(&minus;1). The theorem is proved. �
</p>
<p>Note that any bounded density f belongs to L2. Indeed, since the Lebesgue mea-
sure of {x : f (x)&ge; 1} is always less than 1, for f (&middot;)&le;N we have
</p>
<p>‖f ‖2L2 =
&int;
</p>
<p>f 2(x) dx &le;
&int;
</p>
<p>f (x)&lt;1
</p>
<p>f (x)dx +N2
&int;
</p>
<p>f (x)&ge;1
dx &le; 1 +N2. �
</p>
<p>Thus we have obtained the following result.
</p>
<p>Corollary 7.2.2 For any bounded symmetric density f , the convolution f (2)&lowast; is, up
to a constant factor, the ch.f. of a random variable.
</p>
<p>Example 7.2.1 The &ldquo;triangle&rdquo; density
</p>
<p>g(x)=
{
</p>
<p>1 &minus; |x| if |x| &le; 1,
0 if |x|&gt; 1,
</p>
<p>being the convolution of the two uniform distributions on [&minus;1/2,1/2] (cf. Exam-
ple 3.6.1) is also a ch.f. We suggest the reader to verify that the preimage of this
</p>
<p>ch.f. is the density
</p>
<p>f (x)= 1
2π
</p>
<p>sin2 x/2
</p>
<p>x2
</p>
<p>(the density conjugate to g). Conversely, the density g is conjugate to f , and the
</p>
<p>functions 8πf (t) and g(t) will be ch.f.s for g and f , respectively.
</p>
<p>These assertions will be useful in Sect. 8.7.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 The Continuity (Convergence) Theorem 167
</p>
<p>7.3 The Continuity (Convergence) Theorem
</p>
<p>Let {ϕn(t)}&infin;n=1 be a sequence of ch.f.s and {Fn}&infin;n=1 the sequence of the respective
distribution functions. Recall that the symbol &rArr; denotes the weak convergence of
distributions introduced in Chap. 6.
</p>
<p>Theorem 7.3.1 (The Continuity Theorem) A necessary and sufficient condition for
the convergence Fn &rArr; F as n&rarr;&infin; is that ϕn(t)&rarr; ϕ(t) for any t , ϕ(t) being the
ch.f. corresponding to F .
</p>
<p>The theorem follows in an obvious way from Corollary 6.3.2 (here two of the
</p>
<p>three sufficient conditions from Corollary 6.3.2 are satisfied: conditions (2) and (3)).
</p>
<p>The proof of the theorem can be obtained in a simpler way as well. This way is
</p>
<p>presented in Sect. 7.4 of the previous editions of this book.
</p>
<p>In Sect. 7.1, for nonnegative random variables ξ we introduced the notion of
</p>
<p>the Laplace transform ψ(s) := Ee&minus;sξ . Let ψn(s) and ψ(s) be Laplace transforms
corresponding to Fn and F . The following analogue of Theorem 7.3.1 holds for
</p>
<p>Laplace transforms:
</p>
<p>In order that Fn &rArr; F as n&rarr;&infin; it is necessary and sufficient that ψn(s)&rarr;ψ(s)
for each s &ge; 0.
</p>
<p>Just as in Theorem 7.3.1, this assertion follows from Corollary 6.3.2, since the
</p>
<p>class {f (x)= e&minus;sx, s &ge; 0} is (like {eitx}) a distribution determining class (see Prop-
erty 6 in Sect. 7.1) and, moreover, the sufficient conditions (2) and (3) of Corol-
</p>
<p>lary 6.3.2 are satisfied.
</p>
<p>Theorem 7.3.1 has a deficiency: one needs to know in advance that the func-
</p>
<p>tion ϕ(t) to which the ch.f.s converge is a ch.f. itself. However, one could have no
</p>
<p>such prior information (see e.g. Sect. 8.8). In this connection there arises a natural
</p>
<p>question under what conditions the limiting function ϕ(t) will be characteristic.
</p>
<p>The answer to this question is given by the following theorem.
</p>
<p>Theorem 7.3.2 Let
</p>
<p>ϕn(t)=
&int;
</p>
<p>eitx dFn(x)
</p>
<p>be a sequence of ch.f.s and ϕn(t)&rarr; ϕ(t) as n&rarr;&infin; for any t .
Then the following three conditions are equivalent:
</p>
<p>(a) ϕ(t) is a ch.f.;
(b) ϕ(t) is continuous at t = 0;
(c) the sequence {Fn} is tight.
</p>
<p>Thus if we establish that ϕn(t)&rarr; ϕ(t) and one of the above three conditions is
met, then we can assert that there exists a distribution F such that ϕ is the ch.f. of
</p>
<p>F and Fn &rArr; F .</p>
<p/>
</div>
<div class="page"><p/>
<p>168 7 Characteristic Functions
</p>
<p>Proof The equivalence of conditions (a) and (c) follows from Theorem 6.3.2. That
(a) implies (b) is known. It remains to establish that (c) follows from (b). First we
</p>
<p>will show that the following lemma is true. �
</p>
<p>Lemma 7.3.1 If ϕ is the ch.f. of ξ then, for any u &gt; 0,
</p>
<p>P
</p>
<p>(
|ξ |&gt; 2
</p>
<p>u
</p>
<p>)
&le; 1
</p>
<p>u
</p>
<p>&int; u
</p>
<p>&minus;u
</p>
<p>[
1 &minus; ϕ(t)
</p>
<p>]
dt.
</p>
<p>Proof The right-hand side of this inequality is equal to
</p>
<p>1
</p>
<p>u
</p>
<p>&int; u
</p>
<p>&minus;u
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
1 &minus; e&minus;itx
</p>
<p>)
dF(x)dt,
</p>
<p>where F is the distribution function of ξ . Changing the order of integration and
</p>
<p>noting that
</p>
<p>&int; u
</p>
<p>&minus;u
</p>
<p>(
1 &minus; e&minus;itx
</p>
<p>)
dt =
</p>
<p>(
t + e
</p>
<p>&minus;itx
</p>
<p>ix
</p>
<p>)∣∣∣∣
u
</p>
<p>&minus;u
= 2u
</p>
<p>(
1 &minus; sinux
</p>
<p>ux
</p>
<p>)
,
</p>
<p>we obtain that
</p>
<p>1
</p>
<p>u
</p>
<p>&int; u
</p>
<p>&minus;u
</p>
<p>[
1 &minus; ϕ(t)
</p>
<p>]
dt = 2
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
1 &minus; sinux
</p>
<p>ux
</p>
<p>)
dF(x)
</p>
<p>&ge; 2
&int;
</p>
<p>|x|&gt;2/u
</p>
<p>(
1 &minus;
</p>
<p>∣∣∣∣
sinux
</p>
<p>ux
</p>
<p>∣∣∣∣
)
dF(x)
</p>
<p>&ge; 2
&int;
</p>
<p>|x|&gt;2/u
</p>
<p>(
1 &minus; 1|ux|
</p>
<p>)
dF(x)&ge;
</p>
<p>&int;
</p>
<p>|x|&gt;2/u
dF(x).
</p>
<p>The lemma is proved. �
</p>
<p>Now suppose that condition (b) is met. By Lemma 7.3.1
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>&int;
</p>
<p>|x|&gt;2/u
dFn(x)&le; lim sup
</p>
<p>n&rarr;&infin;
</p>
<p>1
</p>
<p>u
</p>
<p>&int; u
</p>
<p>&minus;u
</p>
<p>[
1 &minus; ϕn(t)
</p>
<p>]
dt = 1
</p>
<p>u
</p>
<p>&int; u
</p>
<p>&minus;u
</p>
<p>[
1 &minus; ϕ(t)
</p>
<p>]
dt.
</p>
<p>Since ϕ(t) is continuous at 0 and ϕ(0)= 1, the mean value on the right-hand side can
clearly be made arbitrarily small by choosing sufficiently small u. This obviously
</p>
<p>means that condition (c) is satisfied. The theorem is proved. �
</p>
<p>Using ch.f.s one can not only establish convergence of distribution functions but
</p>
<p>also estimate the rate of this convergence in the cases when one can estimate how
</p>
<p>fast ϕn &minus; ϕ vanishes. We will encounter respective examples in Sect. 7.5.
We will mostly use the machinery of ch.f.s in Chaps. 8, 12 and 17. In the present
</p>
<p>chapter we will also touch upon some applications of ch.f.s, but they will only serve
</p>
<p>as illustrations.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 The Application of Characteristic Functions 169
</p>
<p>7.4 The Application of Characteristic Functions in the Proof
</p>
<p>of the Poisson Theorem
</p>
<p>Let ξ1, . . . , ξn be independent integer-valued random variables,
</p>
<p>Sn =
k&sum;
</p>
<p>1
</p>
<p>ξk, P(ξk = 1)= pk, P(ξk = 0)= 1 &minus; pk &minus; qk.
</p>
<p>The theorem below is a generalisation of the theorems established in Sect. 5.4.5
</p>
<p>Theorem 7.4.1 One has
</p>
<p>∣∣P(Sn = k)&minus;�&micro;
(
{k}
</p>
<p>)∣∣&le;
n&sum;
</p>
<p>k=1
p2k + 2
</p>
<p>n&sum;
</p>
<p>k=1
qk, where &micro;=
</p>
<p>n&sum;
</p>
<p>k=1
pk.
</p>
<p>Thus, if one is given a triangle array ξ1n, ξ2n, . . . , ξnn, n= 1,2, . . . , of indepen-
dent integer-valued random variables,
</p>
<p>Sn =
n&sum;
</p>
<p>k=1
ξkn, P(ξkn = 1)= pkn, P(ξkn = 0)= 1 &minus; pkn &minus; qkn,
</p>
<p>&micro;=
n&sum;
</p>
<p>k=1
pkn,
</p>
<p>then a sufficient condition for convergence of the difference P(Sn = k)&minus;�&micro;({k})
to zero is that
</p>
<p>n&sum;
</p>
<p>k=1
qkn &rarr; 0,
</p>
<p>n&sum;
</p>
<p>k=1
p2kn &rarr; 0.
</p>
<p>Since
n&sum;
</p>
<p>k=1
p2kn &le; &micro;max
</p>
<p>k&le;n
pkn,
</p>
<p>the last condition is always met if
</p>
<p>max
k&le;n
</p>
<p>pkn &rarr; 0, &micro;&le; &micro;0 = const.
</p>
<p>5This extension is not really substantial since close results could be established using Theo-
</p>
<p>rem 5.2.2 in which ξk can only take the values 0 and 1. It suffices to observe that the probability of
</p>
<p>the event A=
⋃
</p>
<p>k{ξk 
= 0, ξk 
= 1} is bounded by the sum
&sum;
</p>
<p>qk and therefore
</p>
<p>P(Sn = k)= θ1
&sum;
</p>
<p>qk +
(
</p>
<p>1 &minus; θ2
&sum;
</p>
<p>qk
</p>
<p>)
P(Sn = k|A), θi &le; 1, i = 1,2,
</p>
<p>where P(Sn = k|A)= P(S&lowast;n = k) and S&lowast;n are sums of independent random variables ξ&lowast;k with
</p>
<p>P
(
ξ&lowast;k = 1
</p>
<p>)
= p&lowast;k =
</p>
<p>pk
</p>
<p>1 &minus; qk
, P
</p>
<p>(
ξ&lowast;k = 0
</p>
<p>)
= 1 &minus; p&lowast;k .</p>
<p/>
</div>
<div class="page"><p/>
<p>170 7 Characteristic Functions
</p>
<p>To prove the theorem we will need two auxiliary assertions.
</p>
<p>Lemma 7.4.1 If Reβ &le; 0 then
∣∣eβ &minus; 1
</p>
<p>∣∣&le; |β|,
∣∣eβ &minus; 1 &minus; β
</p>
<p>∣∣&le; |β|2/2,
∣∣eβ &minus; 1 &minus; β &minus; β2/2
</p>
<p>∣∣&le; |β|3/6.
</p>
<p>Proof The first two inequalities follow from the relations (we use here the change
of variables t = βv and the fact that |es | &le; 1 for Re s &le; 0)
</p>
<p>∣∣eβ &minus; 1
∣∣=
</p>
<p>∣∣∣∣
&int; β
</p>
<p>0
</p>
<p>et dt
</p>
<p>∣∣∣∣=
∣∣∣∣β
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>eβv dv
</p>
<p>∣∣∣∣&le; |β|,
</p>
<p>∣∣eβ &minus; 1 &minus; β
∣∣=
</p>
<p>∣∣∣∣
&int; β
</p>
<p>0
</p>
<p>(
et &minus; 1
</p>
<p>)
dt
</p>
<p>∣∣∣∣=
∣∣∣∣β
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>(
eβv &minus; 1
</p>
<p>)
dv
</p>
<p>∣∣∣∣&le; |β|
2
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>v dv =
∣∣β2
</p>
<p>∣∣/2.
</p>
<p>The last inequality is proved in the same way. �
</p>
<p>Lemma 7.4.2 If |ak| &le; 1, |bk| &le; 1, k = 1, . . . , n, then
∣∣∣∣∣
</p>
<p>n&prod;
</p>
<p>k=1
ak &minus;
</p>
<p>n&prod;
</p>
<p>k=1
bk
</p>
<p>∣∣∣∣∣&le;
n&sum;
</p>
<p>k=1
|ak &minus; bk|.
</p>
<p>Thus if ϕk(t) and θk(t) are ch.f.s then, for any t ,
∣∣∣∣∣
</p>
<p>n&prod;
</p>
<p>k=1
ϕk(t)&minus;
</p>
<p>n&prod;
</p>
<p>k=1
θk(t)
</p>
<p>∣∣∣∣∣&le;
n&sum;
</p>
<p>k=1
</p>
<p>∣∣ϕk(t)&minus; θk(t)
∣∣.
</p>
<p>Proof Put An =
&prod;n
</p>
<p>k=1 ak and Bn =
&prod;n
</p>
<p>k=1 bk . Then |An| &le; 1, |Bn| &le; 1, and
</p>
<p>|An &minus;Bn| = |An&minus;1an &minus;Bn&minus;1bn|
=
∣∣(An&minus;1 &minus;Bn&minus;1)an + (an &minus; bn)Bn&minus;1
</p>
<p>∣∣&le; |An&minus;1 &minus;Bn&minus;1| + |an &minus; bn|.
Applying this inequality n times, we obtain the required relation. �
</p>
<p>Proof of Theorem 7.4.1 One has
</p>
<p>ϕk(t) := Eeitξk = 1 + pk
(
eit &minus; 1
</p>
<p>)
+ qk
</p>
<p>(
γk(t)&minus; 1
</p>
<p>)
,
</p>
<p>where γk(t) is the ch.f. of some integer-valued random variable. By independence
</p>
<p>of the random variables ξk ,
</p>
<p>ϕSn(t)=
n&prod;
</p>
<p>k=1
ϕk(t).
</p>
<p>Let further ζ &sub;=�&micro;. Then
</p>
<p>ϕζ (t)= Eeitζ = e&micro;(e
it&minus;1) =
</p>
<p>n&prod;
</p>
<p>k=1
θk(t),</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Characteristic Functions of Multivariate Distributions 171
</p>
<p>where θk(t)= epk(e
it&minus;1). Therefore the difference between the ch.f.s ϕSn and ϕζ can
</p>
<p>be bounded by Lemma 7.4.2 as follows:
</p>
<p>∣∣ϕSn(t)&minus; ϕζ (t)
∣∣=
</p>
<p>∣∣∣∣∣
</p>
<p>n&prod;
</p>
<p>k=1
ϕk &minus;
</p>
<p>n&prod;
</p>
<p>k=1
θk
</p>
<p>∣∣∣∣∣&le;
n&sum;
</p>
<p>k=1
|ϕk &minus; θk|,
</p>
<p>where by Lemma 7.4.1 (note that Re(eit &minus; 1)&le; 0)
∣∣θk(t)&minus; 1 &minus; pk
</p>
<p>(
eit &minus; 1
</p>
<p>)∣∣&le; p
2
k |eit &minus; 1|2
</p>
<p>2
=
</p>
<p>p2k
</p>
<p>2
</p>
<p>(
sin2 t + (1 &minus; cos t)2
</p>
<p>)
</p>
<p>= p2k
(
</p>
<p>sin2 t
</p>
<p>2
+ 2 sin4 t
</p>
<p>2
</p>
<p>)
, (7.4.1)
</p>
<p>n&sum;
</p>
<p>k=1
|ϕk &minus; θk| &le; 2
</p>
<p>n&sum;
</p>
<p>k=1
qk +
</p>
<p>n&sum;
</p>
<p>k=1
p2k
</p>
<p>(
sin2 t
</p>
<p>2
+ 2 sin4 t
</p>
<p>2
</p>
<p>)
.
</p>
<p>It remains to make use of the inversion formula (7.2.10):
</p>
<p>∣∣P(Sn = k)&minus;�&micro;
(
{k}
</p>
<p>)∣∣&le;
∣∣∣∣
</p>
<p>1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
e&minus;ikt
</p>
<p>(
ϕSn(t)&minus; ϕζ (t)
</p>
<p>)
dt
</p>
<p>∣∣∣∣
</p>
<p>&le; 1
π
</p>
<p>&int; π
</p>
<p>0
</p>
<p>[
2
</p>
<p>n&sum;
</p>
<p>k=1
qk +
</p>
<p>n&sum;
</p>
<p>k=1
p2k
</p>
<p>(
sin2 t
</p>
<p>2
+ 2 sin4 t
</p>
<p>2
</p>
<p>)]
dt
</p>
<p>= 2
n&sum;
</p>
<p>k=1
qk +
</p>
<p>n&sum;
</p>
<p>k=1
p2k ,
</p>
<p>for
</p>
<p>1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>0
</p>
<p>sin2 t dt = 1
4
,
</p>
<p>2
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
</p>
<p>sin4
t
</p>
<p>2
dt = 3
</p>
<p>4
.
</p>
<p>The theorem is proved. �
</p>
<p>If one makes use of the inequality |eit &minus; 1| &le; 2 in (7.4.1), the computations will
be simplified, there will be no need to calculate the last two integrals, but the bounds
</p>
<p>will be somewhat worse:
&sum;
</p>
<p>|ϕk &minus; θk| &le; 2
(&sum;
</p>
<p>qk +
&sum;
</p>
<p>p2k
</p>
<p>)
,
</p>
<p>∣∣P(Sn = k)&minus;�&micro;
(
{k}
</p>
<p>)∣∣&le; 2
(&sum;
</p>
<p>qk +
&sum;
</p>
<p>p2k
</p>
<p>)
.
</p>
<p>7.5 Characteristic Functions of Multivariate Distributions.
</p>
<p>The Multivariate Normal Distribution
</p>
<p>Definition 7.5.1 Given a random vector ξ = (ξ1, ξ2, . . . , ξd), its ch.f. (the ch.f. of
its distribution) is defined as the function of the vector variable t = (t1, . . . , td) equal
to</p>
<p/>
</div>
<div class="page"><p/>
<p>172 7 Characteristic Functions
</p>
<p>ϕξ (t) := Eeitξ
T = Eei(t,ξ) = E exp
</p>
<p>{
i
</p>
<p>d&sum;
</p>
<p>k=1
tkξk
</p>
<p>}
</p>
<p>=
&int;
</p>
<p>exp
</p>
<p>{
i
</p>
<p>d&sum;
</p>
<p>k=1
tkxk
</p>
<p>}
Fξ1,...,ξd (dx1, . . . , dxd),
</p>
<p>where ξT is the transpose of ξ (a column vector), and (t, ξ) is the inner product.
</p>
<p>The ch.f.s of multivariate distributions possess all the properties (with obvious
</p>
<p>amendments of their statements) listed in Sects. 7.1&ndash;7.3.
</p>
<p>It is clear that ϕξ (0) = 1 and that |ϕξ (t)| &le; 1 and ϕξ (&minus;t) = ϕξ (t) always hold.
Further, ϕξ (t) is everywhere continuous. If there exists a mixed moment Eξ
</p>
<p>k1
1 &middot; &middot; &middot; ξ
</p>
<p>kd
d
</p>
<p>then ϕξ has the respective derivative of order k1 + &middot; &middot; &middot; + kd :
</p>
<p>&part;ϕ
k1+&middot;&middot;&middot;+kd
ξ (t)
</p>
<p>&part;t
k1
1 . . . &part;t
</p>
<p>kd
d
</p>
<p>∣∣∣∣
t=0
</p>
<p>= ik1+&middot;&middot;&middot;+kdEξ k11 &middot; &middot; &middot; ξ
kd
d .
</p>
<p>If all the moments of some order exist, then an expansion of the function ϕξ (t)
</p>
<p>similar to (7.1.1) is valid in a neighbourhood of the point t = 0.
If ϕξ (t) is known, then the ch.f. of any subcollection of the random variables
</p>
<p>(ξk1 , . . . , ξkj ) can obviously be obtained by setting all tk except tk1 , . . . , tkj to be
</p>
<p>equal to 0.
</p>
<p>The following theorems are simple extensions of their univariate analogues.
</p>
<p>Theorem 7.5.1 (The Inversion Formula) If ∆ is a parallelepiped defined by the
inequalities ak &lt; x &lt; bk , k = 1, . . . , d , and the probability P(ξ &isin;∆) is continuous
on the faces of the parallelepiped, then
</p>
<p>P(ξ &isin;∆)= lim
σ&rarr;0
</p>
<p>1
</p>
<p>(2π)d
</p>
<p>&int;
&middot; &middot; &middot;
</p>
<p>&int; ( d&prod;
</p>
<p>k=1
</p>
<p>e&minus;itkak &minus; e&minus;itkbk
itk
</p>
<p>e&minus;t
2
k σ
</p>
<p>2
</p>
<p>)
ϕξ (t) dt1 &middot; &middot; &middot;dtd .
</p>
<p>If the random vector ξ has a density f (x) and its ch.f. ϕξ (t) is integrable, then
</p>
<p>the inversion formula can be written in the form
</p>
<p>f (x)= 1
(2π)d
</p>
<p>&int;
e&minus;i(t,x)ϕξ (t) dt.
</p>
<p>If a function g(x) is such that its Fourier transform
</p>
<p>g̃(t)=
&int;
</p>
<p>ei(t,x)g(x)dx
</p>
<p>is integrable (and this is always the case for sufficiently smooth g(x)) then the Par-
</p>
<p>seval equality holds:
</p>
<p>Eg(ξ)= E 1
(2π)d
</p>
<p>&int;
e&minus;i(t,ξ)g̃(t) dt = 1
</p>
<p>(2π)d
</p>
<p>&int;
ϕξ (&minus;t)g̃(t) dt.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Characteristic Functions of Multivariate Distributions 173
</p>
<p>As before, the inversion formula implies the theorem on one-to-one correspon-
</p>
<p>dence between ch.f.s and distribution functions and together with it the fact that
</p>
<p>{ei(t,x)} is a distribution determining class (cf. Definition 6.3.2).
The weak convergence of distributions Fn(B) in the d-dimensional space to a
</p>
<p>distribution F(B) is defined in the same way as in the univariate case: F(n) &rArr; F if&int;
f (x)dF(n)(dx)&rarr;
</p>
<p>&int;
f (x)dF(dx)
</p>
<p>for any continuous and bounded function f (x).
</p>
<p>Denote by ϕn(t) and ϕ(t) the ch.f.s of distributions Fn and F, respectively.
</p>
<p>Theorem 7.5.2 (Continuity Theorem) A necessary and sufficient condition for the
weak convergence F(n) &rArr; F is that, for any t , ϕn(t)&rarr; ϕ(t) as n&rarr;&infin;.
</p>
<p>In the case where one can establish convergence of ϕn(t) to some function ϕ(t),
</p>
<p>there arises the question of whether ϕ(t) will be the ch.f. of some distribution, or,
</p>
<p>which is the same, whether the sequence F(n) will converge weakly to some distri-
</p>
<p>bution F. Answers to these questions are given by the following assertion. Let ∆N
be the cube defined by the inequality maxk |xk|&lt;N .
</p>
<p>Theorem 7.5.3 (Continuity Theorem) Suppose a sequence ϕn(t) of ch.f.s converges
as n &rarr; &infin; to a function ϕ(t) for each t . Then the following three conditions are
equivalent:
</p>
<p>(a) ϕ(t) is a ch.f.;
(b) ϕ(t) is continuous at the point t = 0;
(c) lim supn&rarr;&infin;
</p>
<p>&int;
x /&isin;∆N F(n)(dx)&rarr; 0 as N &rarr;&infin;.
</p>
<p>All three theorems from this section can be proved in the same way as in the
</p>
<p>univariate case.
</p>
<p>Example 7.5.1 The multivariate normal distribution is defined as a distribution with
density (see Sect. 3.3)
</p>
<p>fξ (x)=
|A|1/2
(2π)d/2
</p>
<p>e&minus;Q(x)/2,
</p>
<p>where
</p>
<p>Q(x)= xAxT =
d&sum;
</p>
<p>i,j=1
aijxixj ,
</p>
<p>and |A| is the determinant of a positive definite matrix A= ‖aij‖.
This is a centred normal distribution for which Eξ = 0. The distribution of the
</p>
<p>vector ξ + a for any constant vector a is also called normal.
Find the ch.f. of ξ . Show that
</p>
<p>ϕξ (t)= exp
{
&minus; tσ
</p>
<p>2tT
</p>
<p>2
</p>
<p>}
, (7.5.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>174 7 Characteristic Functions
</p>
<p>where σ 2 = A&minus;1 is the matrix inverse to A and coinciding with the covariance
matrix ‖σij‖ of ξ :
</p>
<p>σij = Eξiξj .
</p>
<p>Indeed,
</p>
<p>ϕξ (t)=
&radic;
|A|
</p>
<p>(2π)d/2
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
&middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp
</p>
<p>{
itxT &minus; xAx
</p>
<p>T
</p>
<p>2
</p>
<p>}
dx1 &middot; &middot; &middot;dxd . (7.5.2)
</p>
<p>Choose an orthogonal matrix C such that CACT = D is a diagonal matrix, and
denote by &micro;1, . . . ,&micro;n the values of its diagonal elements. Change the variables by
</p>
<p>putting x = yC and t = vC. Then
</p>
<p>|A| = |D| =
d&prod;
</p>
<p>k=1
&micro;k,
</p>
<p>itxT &minus; 1
2
xAxT = ivyT &minus; 1
</p>
<p>2
yDyT = i
</p>
<p>d&sum;
</p>
<p>k=1
vkyk &minus;
</p>
<p>1
</p>
<p>2
</p>
<p>n&sum;
</p>
<p>k=1
&micro;ky
</p>
<p>2
k ,
</p>
<p>and, by Property 2 of ch.f.s of the univariate normal distributions,
</p>
<p>ϕξ (t)=
&radic;
|A|
</p>
<p>(2π)d/2
</p>
<p>d&prod;
</p>
<p>k=1
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp
</p>
<p>{
ivkyk &minus;
</p>
<p>&micro;ky
2
k
</p>
<p>2
</p>
<p>}
dyk =
</p>
<p>&radic;
|A|
</p>
<p>d&prod;
</p>
<p>k=1
</p>
<p>1
&radic;
&micro;k
</p>
<p>exp
</p>
<p>{
&minus;
</p>
<p>v2k
</p>
<p>2&micro;k
</p>
<p>}
</p>
<p>= exp
{
&minus;vD
</p>
<p>&minus;1vT
</p>
<p>2
</p>
<p>}
= exp
</p>
<p>{
&minus; tC
</p>
<p>TD&minus;1CtT
</p>
<p>2
</p>
<p>}
= exp
</p>
<p>{
&minus; tA
</p>
<p>&minus;1tT
</p>
<p>2
</p>
<p>}
.
</p>
<p>On the other hand, since all the moments of ξ exist, in a neighbourhood of the point
</p>
<p>t = 0 one has
</p>
<p>ϕξ (t)= 1 &minus;
1
</p>
<p>2
tA&minus;1tT + o
</p>
<p>(&sum;
t2k
</p>
<p>)
= 1 + itEξT + 1
</p>
<p>2
tσ 2tT + o
</p>
<p>(&sum;
t2k
</p>
<p>)
.
</p>
<p>From this it follows that Eξ = 0, A&minus;1 = σ 2.
</p>
<p>Formula (7.5.1) that we have just proved implies the following property of nor-
</p>
<p>mal distributions: the components of the vector (ξ1, . . . , ξd) are independent if and
only if the correlation coefficients ρ(ξi, ξj ) are zero for all i 
= j . Indeed, if σ 2 is a
diagonal matrix, then A= σ&minus;2 is also diagonal and fξ (x) is equal to the product of
densities. Conversely, if (ξ1, . . . , ξd) are independent, then A is a diagonal matrix,
</p>
<p>and hence σ 2 is also diagonal.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Other Applications of Characteristic Functions 175
</p>
<p>7.6 Other Applications of Characteristic Functions.
</p>
<p>The Properties of the Gamma Distribution
</p>
<p>7.6.1 Stability of the Distributions �α,σ 2 and Kα,σ
</p>
<p>The stability property means, roughly speaking, that the distribution type is pre-
</p>
<p>served under summation of random variables (this description of stability is not
</p>
<p>exact, for more detail see Sect. 8.8).
</p>
<p>The sum of independent normally distributed random variables is also normally
</p>
<p>distributed. Indeed, let ξ1 and ξ2 be independent and normally distributed with pa-
</p>
<p>rameters (a1, σ
2
1 ) and (a2, σ
</p>
<p>2
2 ), respectively. Then the ch.f. of ξ1 + ξ2 is equal to
</p>
<p>ϕξ1+ξ2(t)= ϕξ1(t)ϕξ2(t)= exp
{
ita1 &minus;
</p>
<p>t2σ 21
</p>
<p>2
</p>
<p>}
exp
</p>
<p>{
ita2 &minus;
</p>
<p>t2σ 22
</p>
<p>2
</p>
<p>}
</p>
<p>= exp
{
it (a1 + a2)&minus;
</p>
<p>t2
</p>
<p>2
</p>
<p>(
σ 21 + σ 22
</p>
<p>)}
.
</p>
<p>Thus the sum ξ1 + ξ2 is again a normal random variable, with parameters (a1 +
a2, σ
</p>
<p>2
1 + σ 22 ).
</p>
<p>Normality is also preserved when taking sums of dependent random variables
(components of an arbitrary normally distributed random vector). This immediately
</p>
<p>follows from the form of the ch.f. of the multivariate normal law found in Sect. 7.5.
</p>
<p>One just has to note that to get the ch.f. of the sum ξ1 + &middot; &middot; &middot; + ξn it suffices to put
t1 = &middot; &middot; &middot; = tn = t in the expression
</p>
<p>ϕ(ξ1,...,ξn)(t1, . . . , tn)= E exp{it1ξ1 + &middot; &middot; &middot; + itnξn}.
</p>
<p>The sum of independent random variables distributed according to the Poisson
</p>
<p>law also has a Poisson distribution. Indeed, consider two independent random vari-
</p>
<p>ables ξ1 &sub;=�λ1 and ξ2 &sub;=�λ2 . The ch.f. of their sum is equal to
</p>
<p>ϕξ1+ξ2(t)= exp
{
λ1
(
eit &minus; 1
</p>
<p>)}
exp
</p>
<p>{
λ2
(
eit &minus; 1
</p>
<p>)}
= exp
</p>
<p>{
(λ1 + λ2)
</p>
<p>(
eit &minus; 1
</p>
<p>)}
.
</p>
<p>Therefore ξ1 + ξ2 &sub;= �λ1+λ2 .
The sum of independent random variables distributed according to the Cauchy
</p>
<p>law also has a Cauchy distribution. Indeed, if ξ1 &sub;=Kα1,σ1 and ξ2 &sub;=Kα2,σ2 , then
</p>
<p>ϕξ1+ξ2(t)= exp
{
iα1t &minus; σ1|t |
</p>
<p>}
exp
</p>
<p>{
iα2t &minus; σ2|t |
</p>
<p>}
</p>
<p>= exp
{
i(α1 + α2)t &minus; (σ1 + σ2)|t |
</p>
<p>}
;
</p>
<p>ξ1 + ξ2 &sub;=Kα1+α2,σ1+σ2 .
</p>
<p>The above assertions are closely related to the fact that the normal and Poisson
</p>
<p>laws are, as we saw, limiting laws for sums of independent random variables (the
</p>
<p>Cauchy distribution has the same property, see Sect. 8.8). Indeed, if S2n/
&radic;
</p>
<p>2n con-
</p>
<p>verges in distribution to a normal law (where Sk =
&sum;k
</p>
<p>j=1 ξj , ξj are independent
and identically distributed) then it is clear that Sn/
</p>
<p>&radic;
n and (S2n &minus; Sn)/
</p>
<p>&radic;
n will also</p>
<p/>
</div>
<div class="page"><p/>
<p>176 7 Characteristic Functions
</p>
<p>converge to a normal law so that the sum of two asymptotically normal random
</p>
<p>variables also has to be asymptotically normal.
</p>
<p>Note, however, that due to its arithmetic structure the random variable ξ &sub;=�λ
(as opposed to ξ &sub;=�a,σ 2 or ξ &sub;=Kα,σ ) cannot be transformed by any normalisation
(linear transformation) into a random variable again having the Poisson distribution
</p>
<p>but with another parameter. For this reason the Poisson distribution cannot be stable
</p>
<p>in the sense of Definition 8.8.2.
</p>
<p>It is not hard to see that the other distributions we have met do not possess the
</p>
<p>above-mentioned property of preservation of the distribution type under summa-
</p>
<p>tion of random variables. If, for instance, ξ1 and ξ2 are uniformly distributed over
</p>
<p>[0,1] and independent then Fξ1 and Fξ1+ξ2 are substantially different functions (see
Example 3.6.1).
</p>
<p>7.6.2 The Ŵ-distribution and its properties
</p>
<p>In this subsection we will consider one more rather wide-spread type of distribution
</p>
<p>closely related to the normal distribution and frequently used in applications. This
</p>
<p>is the so-called Pearson gamma distribution Ŵα,λ. We will write ξ &sub;= Ŵα,λ if ξ has
density
</p>
<p>f (x;α,λ)=
{
</p>
<p>αλ
</p>
<p>Γ (λ)
xλ&minus;1e&minus;αx, x &ge; 0,
</p>
<p>0, x &lt; 0,
</p>
<p>depending on two parameters α &gt; 0 and λ &gt; 0, where Γ (λ) is the gamma function
</p>
<p>Γ (λ)=
&int; &infin;
</p>
<p>0
</p>
<p>xλ&minus;1e&minus;x dx, λ &gt; 0.
</p>
<p>It follows from this equality that
&int;
f (x;α,λ)dx = 1 (one needs to make the variable
</p>
<p>change αx = y). If one differentiates the ch.f.
</p>
<p>ϕ(t)= ϕ(t;α,λ)= α
λ
</p>
<p>Γ (λ)
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>xλ&minus;1eitx&minus;αx dx
</p>
<p>with respect to t and then integrates by parts, the result will be
</p>
<p>ϕ&prime;(t)= α
λ
</p>
<p>Γ (λ)
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>ixλeitx&minus;αx dx = α
λ
</p>
<p>Γ (λ)
</p>
<p>iλ
</p>
<p>α&minus; it
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>xλ&minus;1eitx&minus;αx dx
</p>
<p>= iλ
α &minus; it ϕ(t);(
lnϕ(t)
</p>
<p>)&prime; =
(
&minus;λ ln(α &minus; it)
</p>
<p>)&prime;
, ϕ(t)= c(α &minus; it)&minus;λ.
</p>
<p>Since ϕ(0)= 1 one has c= αλ and ϕ(t)= (1 &minus; it/α)&minus;λ.
It follows from the form of the ch.f. that the subfamily of distributions Ŵα,λ for
</p>
<p>a fixed α also has a certain stability property: if ξ1 &sub;= Ŵα,λ1 and ξ2 &sub;= Ŵα,λ2 are
independent, then ξ1 + ξ2 &sub;= Ŵα,λ1+λ2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Other Applications of Characteristic Functions 177
</p>
<p>An example of a particular gamma distribution is given, for instance, by the dis-
</p>
<p>tribution of the random variable
</p>
<p>χ2n =
n&sum;
</p>
<p>i=1
ξ2i ,
</p>
<p>where ξi are independent and normally distributed with parameters (0,1). This is the
</p>
<p>so-called chi-squared distribution with n degrees of freedom playing an important
role in statistics.
</p>
<p>To find the distribution of χ2n it suffices to note that, by virtue of the equality
</p>
<p>P
(
χ21 &lt; x
</p>
<p>)
= P
</p>
<p>(
|ξ1|&lt;
</p>
<p>&radic;
x
)
= 2&radic;
</p>
<p>2π
</p>
<p>&int; &radic;x
</p>
<p>0
</p>
<p>e&minus;u
2/2 du,
</p>
<p>the density of χ21 is equal to
</p>
<p>1&radic;
2π
</p>
<p>e&minus;x/2x&minus;1/2 = f (x;1/2,1/2), χ21 &sub;= Ŵ1/2,1/2.
</p>
<p>This means that the ch.f. of χ2n is
</p>
<p>ϕn(t;1/2,1/2)= (1 &minus; 2it)&minus;n/2 = ϕ(t;1/2, n/2)
and corresponds to the density f (t;1/2, n/2).
</p>
<p>Another special case of the gamma distribution is the exponential distribution
Ŵα = Ŵα,1 with density
</p>
<p>f (x;α,1)= αe&minus;αx, x &ge; 0,
and characteristic function
</p>
<p>ϕ(x;α,1)=
(
</p>
<p>1 &minus; it
α
</p>
<p>)&minus;1
.
</p>
<p>We leave it to the reader to verify with the help of ch.f.s that if ξj &sub;= Ŵαj and are
independent, αj 
= αl for j 
= l, then
</p>
<p>P
</p>
<p>(
n&sum;
</p>
<p>j=1
ξj &gt; x
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>j=1
e&minus;αj x
</p>
<p>n&prod;
</p>
<p>l=1
l 
=j
</p>
<p>(
1 &minus; αj
</p>
<p>αl
</p>
<p>)&minus;1
.
</p>
<p>In various applications (in particular, in queueing theory, cf. Sect. 12.4), the so-
</p>
<p>called Erlang distribution is also of importance. This is a distribution with density
f (x;α,λ) for integer λ. The Erlang distribution is clearly a λ-fold convolution of
the exponential distribution with itself.
</p>
<p>We find the expectation and variance of a random variable ξ that has the gamma
</p>
<p>distribution with parameters (α,λ):
</p>
<p>Eξ =&minus;iϕ&prime;(0;α,λ)= λ
α
, Eξ2 =&minus;iϕ&prime;&prime;(0;α,λ)= λ(λ+ 1)
</p>
<p>α2
,
</p>
<p>Var(ξ)= λ(λ+ 1)
α2
</p>
<p>&minus;
(
λ
</p>
<p>α
</p>
<p>)2
= λ
</p>
<p>α2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>178 7 Characteristic Functions
</p>
<p>Distributions from the gamma family, and especially the exponential ones, are
</p>
<p>often (and justifiably) used to approximate distributions in various applied problems.
</p>
<p>We will present three relevant examples.
</p>
<p>Example 7.6.1 Consider a complex device. The failure of at least one of n parts
comprising the device means the breakdown of the whole device. The lifetime dis-
</p>
<p>tribution of any of the parts is usually well described by the exponential law. (The
</p>
<p>reasons for this could be understood with the help of the Poisson theorem on rare
</p>
<p>events. See also Example 2.4.1 and Chap. 19.)
</p>
<p>Thus if the lifetimes ξj of the parts are independent, and for the part number j
</p>
<p>one has
</p>
<p>P(ξj &gt; x)= e&minus;αj x, x &gt; 0,
then the lifetime of the whole device will be equal to ηn = min(ξ1, . . . , ξn) and we
will get
</p>
<p>P(ηn &gt; x)= P
(
</p>
<p>n⋂
</p>
<p>j=1
{ξj &gt; x}
</p>
<p>)
=
</p>
<p>n&prod;
</p>
<p>j=1
P(ξj &gt; x)= exp
</p>
<p>{
&minus;x
</p>
<p>n&sum;
</p>
<p>i=1
αi
</p>
<p>}
.
</p>
<p>This means that ηn will also have the exponential distribution, and since
</p>
<p>Eξj = 1/αj ,
the mean failure-free operation time of the device will be equal to
</p>
<p>Eηn =
(
</p>
<p>n&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>Eξi
</p>
<p>)&minus;1
.
</p>
<p>Example 7.6.2 Now turn to the distribution of ζn = max(ξ1, . . . , ξn), where ξi are
independent and all have the Ŵ-distribution with parameters (α,λ). We could con-
</p>
<p>sider, for instance, a queueing system with n channels. (That could be, say, a mul-
</p>
<p>tiprocessor computer solving a problem using the complete enumeration algorithm,
</p>
<p>each of the processors of the machine checking a separate variant.) Channel number
</p>
<p>i is busy for a random time ξi . After what time will the whole system be free? This
</p>
<p>random time will clearly have the same distribution as ζn.
</p>
<p>Since the ξi are independent, we have
</p>
<p>P(ζn &lt; x)= P
(
</p>
<p>n⋂
</p>
<p>j=1
{ξj &lt; x}
</p>
<p>)
=
[
P(ξ1 &lt; x)
</p>
<p>]n
. (7.6.1)
</p>
<p>If n is large, then for approximate calculations we could find the limiting distri-
</p>
<p>bution of ζn as n&rarr;&infin;. Note that, for any fixed x, P(ζn &lt; x)&rarr; 0 as n&rarr;&infin;.
Assuming for simplicity that α = 1 (the general case can be reduced to this one
</p>
<p>by changing the scale), we apply L&rsquo;Hospital&rsquo;s rule to see that, as x &rarr;&infin;,
</p>
<p>P(ξj &lt; x)=
&int; &infin;
</p>
<p>x
</p>
<p>1
</p>
<p>Γ (λ)
yλ&minus;1e&minus;y dy &sim; x
</p>
<p>λ&minus;1
</p>
<p>Γ (λ)
e&minus;x .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Other Applications of Characteristic Functions 179
</p>
<p>Letting n&rarr;&infin; and
</p>
<p>x = x(n)= ln
[
n(lnn)λ&minus;1/Γ (λ)
</p>
<p>]
+ u, u= const,
</p>
<p>we get
</p>
<p>P(ξj &gt; x)&sim;
(lnn)λ&minus;1
</p>
<p>Γ (λ)
</p>
<p>Γ (λ)
</p>
<p>n(lnn)λ&minus;1
e&minus;u = e
</p>
<p>&minus;u
</p>
<p>n
.
</p>
<p>Therefore for such x and n&rarr;&infin; we obtain by (7.6.1) that
</p>
<p>P(ζn &lt; x)=
(
</p>
<p>1 &minus; e
&minus;u
</p>
<p>n
</p>
<p>(
1 + o(1)
</p>
<p>))n
&rarr; e&minus;e&minus;u .
</p>
<p>Thus we have established the existence of the limit
</p>
<p>lim
n&rarr;&infin;
</p>
<p>P
</p>
<p>(
ζn &minus; ln
</p>
<p>[
n(lnn)λ&minus;1
</p>
<p>Γ (λ)
</p>
<p>]
&lt; u
</p>
<p>)
= e&minus;e&minus;u ,
</p>
<p>or, which is the same, that
</p>
<p>ζn &minus; ln
[
n(lnn)λ&minus;1
</p>
<p>Γ (λ)
</p>
<p>]
&sub;&rArr; F0, F0(u)= e&minus;e
</p>
<p>&minus;u
.
</p>
<p>In other words, for large n the variable ζn admits the representation
</p>
<p>ζn &asymp; ln
[
n(lnn)λ&minus;1
</p>
<p>Γ (λ)
</p>
<p>]
+ ζ 0, where ζ 0 &sub;= F0.
</p>
<p>Example 7.6.3 Let ξ1 and ξ2 be independent with ξ1 &sub;=Ŵα,λ1 and ξ2 &sub;=Ŵα,λ2 . What
is the distribution of ξ1/(ξ1 + ξ2)? We will make use of Theorem 4.9.2. Since the
joint density f (x, y) of ξ1 and η= ξ1 + ξ2 is equal to
</p>
<p>f (x, y)= f (x;α,λ1)f (y &minus; x;α,λ2),
</p>
<p>the density of η is
</p>
<p>q(y)= f (y;α,λ1 + λ2),
</p>
<p>and the conditional density f (x | y) of ξ1 given η= y is equal to
</p>
<p>f (x | y)= f (x, y)
q(y)
</p>
<p>= Γ (λ1 + λ2)
Γ (λ1)Γ (λ2)
</p>
<p>xλ1&minus;1(y &minus; x)λ2&minus;1
yλ1+λ2&minus;1
</p>
<p>, x &isin; [0, y].
</p>
<p>By the formulas from Sect. 3.2 the conditional density of ξ1/y = ξ1/(ξ1+ξ2) (given
the same condition ξ1 + ξ2 = y) is equal to
</p>
<p>yf (yx | y)= Γ (λ1 + λ2)
Γ (λ1)Γ (λ2)
</p>
<p>xλ1&minus;1(1 &minus; x)λ2&minus;1, x &isin; [0,1].
</p>
<p>This distribution does not depend on y (nor on α). Hence the conditional density
</p>
<p>of ξ1/(ξ1 + ξ2) will have the same property, too. We obtain the so-called beta distri-
bution Bλ1,λ2 with parameters λ1 and λ2 defined on the interval [0,1]. In particular,
for λ1 = λ2 = 1, the distribution is uniform: B1,1 =U0,1.</p>
<p/>
</div>
<div class="page"><p/>
<p>180 7 Characteristic Functions
</p>
<p>7.7 Generating Functions. Application to Branching Processes.
</p>
<p>A Problem on Extinction
</p>
<p>7.7.1 Generating Functions
</p>
<p>We already know that if a random variable ξ is integer-valued, i.e.
</p>
<p>P
</p>
<p>(⋃
</p>
<p>k
</p>
<p>{ξ = k}
)
= 1,
</p>
<p>then the ch.f. ϕξ (t) will actually be a function of z = eit , and, along with its ch.f.,
the distribution of ξ can be specified by its generating function
</p>
<p>pξ (z) := Ezξ =
&sum;
</p>
<p>k
</p>
<p>zkP(ξ = k).
</p>
<p>The inversion formula can be written here as
</p>
<p>P(ξ = k)= 1
2π
</p>
<p>&int; π
</p>
<p>&minus;π
e&minus;itkϕξ (t) dt =
</p>
<p>1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>|z|=1
z&minus;k&minus;1pξ (z) dz. (7.7.1)
</p>
<p>As was already noted (see Sect. 7.2), relation (7.7.1) is simply the formula for
</p>
<p>Fourier coefficients (since eitk = cos tk+ i sin tk).
If ξ and η are independent random variables, then the distribution of ξ + η will
</p>
<p>be given by the convolution of the sequences P(ξ = k) and P(η= k):
</p>
<p>P(ξ + η= n)=
&infin;&sum;
</p>
<p>k=&minus;&infin;
P(ξ = k)P(η= n&minus; k)
</p>
<p>(the total probability formula). To this convolution there corresponds the product of
</p>
<p>the generating functions:
</p>
<p>pξ+η(z)= pξ (z)pη(z).
It is clear from the examples considered in Sect. 7.1 that the generating functions of
</p>
<p>random variables distributed according to the Bernoulli and Poisson laws are
</p>
<p>pξ (z)= 1 + p(z&minus; 1), pξ (z)= exp
{
&micro;(z&minus; 1)
</p>
<p>}
,
</p>
<p>respectively.
</p>
<p>One can see from the definition of the generating function that, for a nonnegative
</p>
<p>random variable ξ &ge; 0, the function pξ (z) is defined for |z| &le; 1 and is analytic in
the domain |z|&lt; 1.
</p>
<p>7.7.2 The Simplest Branching Processes
</p>
<p>Now we turn to sequences of random variables which describe the so-called branch-
ing processes. We have already encountered a simple example of such a process</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 Generating Functions. Application to Branching Processes 181
</p>
<p>when describing a chain reaction scheme in Example 4.4.4. Consider a more general
</p>
<p>scheme of a branching process. Imagine particles that can produce other particles
</p>
<p>of the same type; these could be neutrons in chain reactions, bacteria reproducing
</p>
<p>according to certain laws etc. Assume that initially there is a single particle (the
</p>
<p>&ldquo;null generation&rdquo;) that, as a result of a &ldquo;division&rdquo; act, transforms with probabilities
</p>
<p>fk , k = 0,1,2, . . . , into k particles of the same type,
&infin;&sum;
</p>
<p>k=0
fk = 1.
</p>
<p>The new particles form the &ldquo;first generation&rdquo;. Each of the particles from that gen-
</p>
<p>eration behaves itself in the same way as the initial particle, independently of what
</p>
<p>happened before and of the other particles from that generation. Thus we obtain the
</p>
<p>&ldquo;second generation&rdquo;, and so on. Denote by ζn the number of particles in the n-th
</p>
<p>generation. To describe the sequence ζn, introduce, as we did in Example 4.4.4,
</p>
<p>independent sequences of independent identically distributed random variables
</p>
<p>{
ξ
(1)
j
</p>
<p>}&infin;
j=1,
</p>
<p>{
ξ
(2)
j
</p>
<p>}&infin;
j=1, . . . ,
</p>
<p>where ξ
(n)
j have the distribution
</p>
<p>P
(
ξ
(n)
j = k
</p>
<p>)
= fk, k = 0,1, . . . .
</p>
<p>Then the sequence ζn can be represented as
</p>
<p>ζ0 = 1,
ζ1 = ξ (1)1 ,
ζ2 = ξ (2)1 + &middot; &middot; &middot; + ξ
</p>
<p>(2)
ζ1
</p>
<p>,
</p>
<p>&middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot;
ζn = ξ (n)1 + &middot; &middot; &middot; + ξ
</p>
<p>(n)
ζn&minus;1
</p>
<p>.
</p>
<p>These are sums of random numbers of random variables. Since ξ
(n)
1 , ξ
</p>
<p>(n)
2 , . . . do not
</p>
<p>depend on ζn&minus;1, for the generating function f(n)(z) = Ezζn we obtain by the total
probability formula that
</p>
<p>f(n)(z)=
&infin;&sum;
</p>
<p>k=0
P(ζn&minus;1 = k)Ezξ
</p>
<p>(n)
1 +&middot;&middot;&middot;+ξ
</p>
<p>(n)
k
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
P(ζn&minus;1 = k)f k(z)= f(n&minus;1)
</p>
<p>(
f (z)
</p>
<p>)
, (7.7.2)
</p>
<p>where
</p>
<p>f (z) := f(1)(z)= Ezξ
(n)
1 =
</p>
<p>&infin;&sum;
</p>
<p>k=0
fkz
</p>
<p>k.</p>
<p/>
</div>
<div class="page"><p/>
<p>182 7 Characteristic Functions
</p>
<p>Fig. 7.1 Finding the
</p>
<p>extinction probability of a
</p>
<p>branching process: it is given
</p>
<p>by the smaller of the two
</p>
<p>solutions to the equation
</p>
<p>z= f (z)
</p>
<p>Denote by fn(z) the n-th iterate of the function f (z), i.e. f1(z) = f (z), f2(z) =
f (f (z)), f3(z)= f (f2(z)) and so on. Then we conclude from (7.7.2) by induction
that the generating function of ζn is equal to the n-th iterate of f (z):
</p>
<p>Ezζn = f(n)(z).
</p>
<p>From this one can easily obtain, by differentiating at the point z= 1, recursive rela-
tions for the moments of ζn.
</p>
<p>How can one find the extinction probability of the process? By extinction we will
</p>
<p>understand the event that all ζn starting from some n will be equal to 0. (If ζn = 0
then clearly ζn+1 = ζn+2 = &middot; &middot; &middot; = 0, because P(ζn+1 = 0| ζn = 0) = 1. ) Set Ak =
{ζk = 0}. Then extinction is the event
</p>
<p>⋃&infin;
k=1 Ak . Since An &sub; An+1, the extinction
</p>
<p>probability q is equal to q = limn&rarr;&infin; P(An).
</p>
<p>Theorem 7.7.1 The extinction probability q is equal to the smallest nonnegative
solution of the equation q = f (q).
</p>
<p>Proof One has P(An)= fn(0)&le; 1, and this sequence is non-increasing. Passing in
the equality
</p>
<p>fn+1(0)= f
(
fn(0)
</p>
<p>)
(7.7.3)
</p>
<p>to the limit as n&rarr;&infin;, we obtain
</p>
<p>q = f (q), q &le; 1.
</p>
<p>This is an equation for the extinction probability. Let us analyse its solutions. The
</p>
<p>function f (z) is convex (as f &prime;&prime;(z) &ge; 0) and non-decreasing in the domain z &ge; 0
and f &prime;(1) = m is the mean number of offspring of a single particle. First assume
that P(ξ
</p>
<p>(1)
1 = 1) &lt; 1. If m &le; 1 then f (z) &gt; z for z &lt; 1 and hence q = 1. If m &gt; 1
</p>
<p>then by convexity of f the equation q = f (q) has exactly two solutions on the
interval [0,1]: q1 &lt; 1 and q2 = 1 (see Fig. 7.1). Assume that q = q2 = 1. Then the
sequence δn = 1&minus; fn(0) will monotonically converge to 0, and f (1&minus; δn) &lt; 1&minus; δn
for sufficiently large n. Therefore, for such n,
</p>
<p>δn+1 = 1 &minus; f (1 &minus; δn) &gt; δn,</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 Generating Functions. Application to Branching Processes 183
</p>
<p>which is a contradiction as δn is a decreasing sequence. This means that q = q1 &lt; 1.
Finally, in the case P(ξ
</p>
<p>(1)
1 = 1)= f1 = 1 one clearly has f (z) &equiv; z and q = 0. The
</p>
<p>theorem is proved. �
</p>
<p>Now consider in more detail the case m = 1, which is called critical. We know
that in this case the extinction probability q equals 1. Let qn = P(An) = fn(0) be
the probability of extinction by time n. How fast does qn converge to 1? By (7.7.3)
</p>
<p>one has qn+1 = f (qn). Therefore the probability pn = 1 &minus; qn of non-extinction of
the process by time n satisfies the relation
</p>
<p>pn+1 = g(pn), g(x)= 1 &minus; f (1 &minus; x).
</p>
<p>It is also clear that γn = pn &minus; pn+1 is the probability that extinction will occur
on step n.
</p>
<p>Theorem 7.7.2 If m = f &prime;(1) = 1 and 0 &lt; b := f &prime;&prime;(1) &lt; &infin; then γn &sim; 2bn2 and
pn &sim; 2bn as n&rarr;&infin;.
</p>
<p>Proof If the second moment of the number of offspring of a single particle is finite
(b &lt; &infin;) then the derivative g&prime;&prime;(0) = &minus;b exists and therefore, since g(0) = 0 and
g&prime;(0)= f &prime;(1)= 1, one has
</p>
<p>g(x)= x &minus; b
2
x2 + o
</p>
<p>(
x2
)
, x &rarr;&infin;.
</p>
<p>Putting x = pn &rarr; 0, we find for the sequence an = 1/pn that
</p>
<p>an+1 &minus; an =
pn &minus; pn+1
pnpn+1
</p>
<p>= bp
2
n(1 + o(1))
</p>
<p>2p2n(1 &minus; bpn/2 + o(pn))
&rarr; b
</p>
<p>2
,
</p>
<p>an = a1 +
n&minus;1&sum;
</p>
<p>k=1
(ak+1 &minus; ak)&sim;
</p>
<p>bn
</p>
<p>2
, pn &sim;
</p>
<p>2
</p>
<p>bn
.
</p>
<p>The theorem is proved. �
</p>
<p>Now consider the problem on the distribution of the number ζn of particles given
</p>
<p>ζn &gt; 0.
</p>
<p>Theorem 7.7.3 Under the assumptions of Theorem 7.7.2, the conditional distribu-
tion of pnξn (or 2ζn/(bn)) given ζn &gt; 0 converges as n &rarr; &infin; to the exponential
distribution:
</p>
<p>P(pnζn &gt; x|ζn &gt; 0)&rarr; e&minus;x, x &gt; 0.
</p>
<p>The above statement means, in particular, that given ζn &gt; 0, the number of parti-
</p>
<p>cles ζn is of order n as n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>184 7 Characteristic Functions
</p>
<p>Proof Consider the Laplace transform (see Property 6 in Sect. 7.1) of the condi-
tional distribution of pnζn (given ζn &gt; 0):
</p>
<p>E
(
e&minus;spnζn |ζn &gt; 0
</p>
<p>)
= 1
</p>
<p>pn
</p>
<p>&infin;&sum;
</p>
<p>k=1
e&minus;skpnP(ζn = k). (7.7.4)
</p>
<p>We will make use of the fact that, if we could find an N such that e&minus;spn = 1 &minus; pN ,
which is the probability of extinction by time N , then the right-hand side of (7.7.4)
</p>
<p>will give, by the total probability formula, the conditional probability of the extinc-
</p>
<p>tion of the process by time n+N given its non-extinction at time n. We can evaluate
this probability using Theorem 7.7.2.
</p>
<p>Since pn &rarr; 0, for any fixed s &gt; 0 one has
</p>
<p>e&minus;spn &minus; 1 &sim;&minus;spn &sim;&minus;
2s
</p>
<p>bn
.
</p>
<p>Clearly, one can always choose N &sim; n/s, sn &sim; s, sn &darr; s such that e&minus;snpn&minus;1 =&minus;pN .
Therefore e&minus;snpnk = (1 &minus; pN )k and the right-hand side of (7.7.4) can be rewritten
for s = sn as
</p>
<p>1
</p>
<p>pn
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(ζn = k)(1 &minus; pN )k =
</p>
<p>1
</p>
<p>pn
P(ζn &gt; 0, ζn+N = 0)
</p>
<p>= pn&minus;pn+N
pn
</p>
<p>= 1&minus;pn+N
pn
</p>
<p>&sim; 1&minus; n
n+N =
</p>
<p>N
</p>
<p>n+N &rarr;
1
</p>
<p>1 + s .
</p>
<p>Now note that
</p>
<p>E
(
e&minus;spnζn
</p>
<p>∣∣ζn &gt; 0
)
&minus;E
</p>
<p>(
e&minus;snpnζn
</p>
<p>∣∣ζn &gt; 0
)
= E
</p>
<p>[
e&minus;spnζn
</p>
<p>(
1 &minus; e&minus;(sn&minus;s)pnζn
</p>
<p>∣∣ζn &gt; 0
)]
.
</p>
<p>Since e&minus;α &le; 1 and 1 &minus; e&minus;α &le; α for α &ge; 0, and Eζn = 1, E(ζn|ζn &gt; 0)= 1/pn, it is
easily seen that the positive (since sn &gt; s) difference of the expectations in the last
</p>
<p>formula does not exceed
</p>
<p>(sn &minus; s)pnE(ζn|ζn &gt; 0)= sn &minus; s &rarr; 0.
Therefore the Laplace transform (7.7.4) converges, as n &rarr; &infin;, to 1/(1 + s).
</p>
<p>Since 1/(1 + s) is the Laplace transform of the exponential distribution:
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;sx&minus;x dx = 1
1 + s ,
</p>
<p>we conclude by the continuity theorem (see the remark after Theorem 7.3.1 in
</p>
<p>Sect. 7.3) that the conditional distribution of interest converges to the exponential
</p>
<p>law.6
</p>
<p>In Sect. 15.4 (Example 15.4.1) we will obtain, as consequences of martingale
</p>
<p>convergence theorems, assertions about the behaviour of ζn as n&rarr;&infin; for branching
processes in the case &micro;&gt; 1 (the so-called supercritical processes). �
</p>
<p>6The simple proof of Theorem 7.7.3 that we presented here is due to K.A. Borovkov.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Sequences of Independent Random Variables.
Limit Theorems
</p>
<p>Abstract The chapter opens with proofs of Khintchin&rsquo;s (weak) Law of Large Num-
</p>
<p>bers (Sect. 8.1) and the Central Limit Theorem (Sect. 8.2) the case of independent
</p>
<p>identically distributed summands, both using the apparatus of characteristic func-
</p>
<p>tions. Section 8.3 establishes general conditions for the Weak Law of Large Num-
</p>
<p>bers for general sequences of independent random variables and also conditions for
</p>
<p>the respective convergence in mean. Section 8.4 presents the Central Limit Theo-
</p>
<p>rem in the triangular array scheme (the Lindeberg&ndash;Feller theorem) and its corollar-
</p>
<p>ies, illustrated by several insightful examples. After that, in Sect. 8.5 an alternative
</p>
<p>method of compositions is introduced and used to prove the Central Limit Theo-
</p>
<p>rem in the same situation, establishing an upper bound for the convergence rate for
</p>
<p>the uniform distance between the distribution functions in the case of finite third
</p>
<p>moments. This is followed by an extension of the above results to the multivariate
</p>
<p>case in Sect. 8.6. Section 8.7 presents important material not to be found in other
</p>
<p>textbooks: the so-called integro-local limit theorems on convergence to the normal
</p>
<p>distribution (the Stone&ndash;Shepp and Gnedenko theorems), including versions for sums
</p>
<p>of random variables depending on a parameter. These results will be of crucial im-
</p>
<p>portance in Chap. 9, when proving theorems on exact asymptotic behaviour of large
</p>
<p>deviation probabilities. The chapter concludes with Sect. 8.8 establishing integral,
</p>
<p>integro-local and local theorems on convergence of the distributions of scaled sums
</p>
<p>on independent identically distributed random variables to non-normal stable laws.
</p>
<p>8.1 The Law of Large Numbers
</p>
<p>Theorem 8.1.1 (Khintchin&rsquo;s Law of Large Numbers) Let {ξn}&infin;n=1 be a sequence
of independent identically distributed random variables having a finite expectation
Eξn = a and let Sn := ξ1 + &middot; &middot; &middot; + ξn. Then
</p>
<p>Sn
</p>
<p>n
</p>
<p>p&rarr; a as n&rarr;&infin;.
</p>
<p>The above assertion together with Theorems 6.1.6 and 6.1.7 imply the following.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_8, &copy; Springer-Verlag London 2013
</p>
<p>185</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_8">http://dx.doi.org/10.1007/978-1-4471-5201-9_8</a></div>
</div>
<div class="page"><p/>
<p>186 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Corollary 8.1.1 Under the conditions of Theorem 8.1.1, as well as convergence of
Sn/n in probability, convergence in mean also takes place:
</p>
<p>E
</p>
<p>∣∣∣∣
Sn
</p>
<p>n
&minus; a
</p>
<p>∣∣∣∣&rarr; 0 as n&rarr;&infin;.
</p>
<p>Note that the condition of independence of ξk and the very assertion of the the-
</p>
<p>orem assume that all the random variables ξk are given on a common probability
</p>
<p>space.
</p>
<p>From the physical point of view, the stated law of large numbers is the sim-
</p>
<p>plest ergodic theorem which means, roughly speaking, that for random variables
</p>
<p>their &ldquo;time averages&rdquo; and &ldquo;space averages&rdquo; coincide. This applies to an even greater
</p>
<p>extent to the strong law of large numbers, by virtue of which Sn/n&rarr; a with prob-
ability 1.
</p>
<p>Under more strict assumptions (existence of variance) Theorem 8.1.1 was ob-
</p>
<p>tained in Sect. 4.7 as a consequence of Chebyshev&rsquo;s inequality.
</p>
<p>Proof of Theorem 8.1.1 We have to prove that, for any ε &gt; 0,
</p>
<p>P
</p>
<p>(∣∣∣∣
Sn
</p>
<p>n
&minus; a
</p>
<p>∣∣∣∣&gt; ε
)
&rarr; 0
</p>
<p>as n&rarr;&infin;. The above relation is equivalent to the weak convergence of distributions
Sn/n&sub;=&rArr; Ia . Therefore, by the continuity theorem and Example 7.1.1 it suffices to
show that, for any fixed t ,
</p>
<p>ϕSn/n(t)&rarr; eiat .
</p>
<p>The ch.f. ϕ(t) of the random variable ξk has, in a certain neighbourhood of 0, the
</p>
<p>property |ϕ(t)&minus; 1| &lt; 1/2. Therefore for such t one can define the function l(t) =
lnϕ(t) (we take the principal value of the logarithm). Since ξn has finite expectation,
</p>
<p>the derivative
</p>
<p>l&prime;(0)= ϕ
&prime;(0)
</p>
<p>ϕ(0)
= ia
</p>
<p>exists. For each fixed t and sufficiently large n, the value of l(t/n) is defined and
</p>
<p>ϕSn/n(t)= ϕn(t/n)= el(t/n)n.
</p>
<p>Since l(0)= 0, one has
</p>
<p>el(t/n)n = exp
{
t
l(t/n)&minus; l(0)
</p>
<p>t/n
</p>
<p>}
&rarr; el&prime;(0)t = eiat
</p>
<p>as n&rarr;&infin;. The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 The Central Limit Theorem for Identically Distributed Random Variables 187
</p>
<p>8.2 The Central Limit Theorem for Identically Distributed
</p>
<p>Random Variables
</p>
<p>Let, as before, {ξn} be a sequence of independent identically distributed random
variables. But now we assume, along with the expectation Eξn = a, the existence
of the variance Var ξn = σ 2. We retain the notation Sn = ξ1 + &middot; &middot; &middot; + ξn for sums of
our random variables and Φ(x) for the normal distribution function with parameters
</p>
<p>(0,1). Introduce the sequence of random variables
</p>
<p>ζn =
Sn &minus; an
σ
&radic;
n
</p>
<p>.
</p>
<p>Theorem 8.2.1 If 0 &lt; σ 2 &lt; &infin;, then P(ζn &lt; x) &rarr; Φ(x) uniformly in x (&minus;&infin; &lt;
x &lt;&infin;) as n&rarr;&infin;.
</p>
<p>In such a case, the sequence {ζn} is said to be asymptotically normal.
It follows from ζn &rArr; ζ &sub;=�0,1, ζ 2n &ge; 0, Eζ 2n = Eζ 2 = 1 and from Lemma 6.2.3
</p>
<p>that the sequence {ζ 2n } is uniformly integrable. Therefore, as well as the weak
convergence ζn &rArr; ζ , ζ &sub;= �0,1 (Ef (ζn) &rarr; Ef (ζ ) for any bounded continuous
f ), one also has convergence Ef (ζn) &rarr; Ef (ζ ) for any continuous f such that
|f (x)|&lt; c(1 + x2) (see Theorem 6.2.3).
</p>
<p>Proof of Theorem 8.2.1 The uniform convergence is a consequence of the weak
convergence and continuity of Φ(x). Further, we may assume without loss of gen-
</p>
<p>erality that a = 0, for otherwise we could consider the sequence {ξ &prime;n = ξn &minus; a}&infin;n=1
without changing the sequence {ζn}. Therefore, to prove the required convergence,
it suffices to show that ϕζn(t)&rarr; e&minus;t
</p>
<p>2/2 when a = 0. We have
</p>
<p>ϕζn(t)= ϕn
(
</p>
<p>t
</p>
<p>σ
&radic;
n
</p>
<p>)
, where ϕ(t)= ϕξk (t).
</p>
<p>Since Eξ2n exists, ϕ
&prime;&prime;(t) also exists and, as t &rarr; 0, one has
</p>
<p>ϕ(t)= ϕ(0)+ tϕ&prime;(0)+ t
2
</p>
<p>2
ϕ&prime;&prime;(0)+ o
</p>
<p>(
t2
)
= 1 &minus; t
</p>
<p>2σ 2
</p>
<p>2
+ o
</p>
<p>(
t2
)
. (8.2.1)
</p>
<p>Therefore, as n&rarr;&infin;,
</p>
<p>lnϕζn(t) = n ln
[
</p>
<p>1 &minus; σ
2
</p>
<p>2
</p>
<p>(
t
</p>
<p>σ
&radic;
n
</p>
<p>)2
+ o
</p>
<p>(
t2
</p>
<p>n
</p>
<p>)]
</p>
<p>= n
[
&minus; t
</p>
<p>2
</p>
<p>2n
+ o
</p>
<p>(
t2
</p>
<p>n
</p>
<p>)]
=&minus; t
</p>
<p>2
</p>
<p>2
+ o(1)&rarr;&minus; t
</p>
<p>2
</p>
<p>2
.
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>188 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>8.3 The Law of Large Numbers for Arbitrary Independent
</p>
<p>Random Variables
</p>
<p>Now we proceed to elucidating conditions under which the law of large numbers and
</p>
<p>the central limit theorem will hold in the case when ξk are independent but not nec-
</p>
<p>essarily identically distributed. The problem will not become more complicated if,
</p>
<p>from the very beginning, we consider a more general situation where one is given an
</p>
<p>arbitrary series ξ1,n, . . . , ξn,n, n= 1,2, . . . of independent random variables, where
the distributions of ξk,n may depend on n. This is the so-called triangular array
scheme.
</p>
<p>Put
</p>
<p>ζn :=
n&sum;
</p>
<p>k=1
ξk,n.
</p>
<p>From the viewpoint of the results to follow, we can assume without loss of generality
</p>
<p>that
</p>
<p>Eξk,n = 0. (8.3.1)
Assume that the following condition is met: as n&rarr;&infin;,
</p>
<p>D1 :=
n&sum;
</p>
<p>k=1
Emin
</p>
<p>(
|ξk,n|, |ξk,n|2
</p>
<p>)
&rarr; 0. [D1]
</p>
<p>Theorem 8.3.1 (The Law of Large Numbers) If conditions (8.3.1) and [D1] are
satisfied, then ζn &sub;=&rArr; I0 or, which is the same, ζn
</p>
<p>p&rarr; 0 as n&rarr;&infin;.
</p>
<p>Example 8.3.1 Assume ξk = ξk,n do not depend on n, Eξk = 0 and E|ξk|s &le;ms &lt;
&infin; for 1 &lt; s &le; 2. For such s, there exists a sequence b(n) = o(n) such that n =
o(bs(n)). Since, for ξk,n = ξk/b(n),
</p>
<p>Emin
(
|ξk,n|, ξ2k,n
</p>
<p>)
= E
</p>
<p>[∣∣∣∣
ξk
</p>
<p>b(n)
</p>
<p>∣∣∣∣
2
</p>
<p>; |ξk| &le; b(n)
]
+E
</p>
<p>[ |ξk|
b(n)
</p>
<p>; |ξk|&gt; b(n)
]
</p>
<p>&le; E
[∣∣∣∣
</p>
<p>ξk
</p>
<p>b(n)
</p>
<p>∣∣∣∣
s
</p>
<p>; |ξk| &le; b(n)
]
+E
</p>
<p>[∣∣∣∣
ξk
</p>
<p>b(n)
</p>
<p>∣∣∣∣
s
</p>
<p>; |ξk|&gt; b(n)
]
</p>
<p>= msb&minus;s(n),
</p>
<p>we have
</p>
<p>D1 &le; nmsb&minus;s(n)&rarr; 0,
</p>
<p>and hence Sn/b(n)
p&rarr; 0.
</p>
<p>A more general sufficient condition (compared to ms &lt;&infin;) for the law of large
numbers is contained in Theorem 8.3.3 below. Theorem 8.1.1 is an evident corollary
</p>
<p>of that theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 The Law of Large Numbers for Arbitrary Independent Random Variables 189
</p>
<p>Now consider condition [D1] in more detail. It can clearly also be written in the
form
</p>
<p>D1 =
n&sum;
</p>
<p>k=1
E
(
|ξk,n|; |ξk,n|&gt; 1
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>k=1
E
(
|ξk,n|2; |ξk,n| &le; 1
</p>
<p>)
&rarr; 0.
</p>
<p>Next introduce the condition
</p>
<p>M1 :=
n&sum;
</p>
<p>k=1
E|ξk,n| &le; c &lt;&infin; (8.3.2)
</p>
<p>and the condition
</p>
<p>M1(τ ) :=
n&sum;
</p>
<p>k=1
E
(
|ξk,n|; |ξk,n|&gt; τ
</p>
<p>)
&rarr; 0 [M1]
</p>
<p>for any τ &gt; 0 as n&rarr;&infin;. Condition [M1] could be called a Lindeberg type condition
(the Lindeberg condition [M2] will be introduced in Sect. 8.4).
</p>
<p>The following lemma explains the relationship between the introduced condi-
</p>
<p>tions.
</p>
<p>Lemma 8.3.1 1. {[M1] &cap; (3.2)} &sub; [D1]. 2. [D1] &sub; [M1].
</p>
<p>That is, conditions [M1] and (8.3.2) imply [D1], and condition [D1] implies
[M1].
</p>
<p>It follows from Lemma 8.3.1 that under condition (8.3.2), conditions [D1] and
[M1] are equivalent.
</p>
<p>Proof of Lemma 8.3.1 1. Let conditions (8.3.2) and [M1] be met. Then, for
</p>
<p>τ &le; 1, g1(x)= min
(
|x|, |x|2
</p>
<p>)
,
</p>
<p>one has
</p>
<p>D1 =
n&sum;
</p>
<p>k=1
Eg1(ξk,n)&le;
</p>
<p>n&sum;
</p>
<p>k=1
E
(
|ξk,n|; |ξk,n|&gt; τ
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>k=1
E
(
|ξk,n|2; |ξk,n| &le; τ
</p>
<p>)
</p>
<p>&le; M1(τ )+ τ
n&sum;
</p>
<p>k=1
E
(
|ξk,n|; |ξk,n| &le; τ
</p>
<p>)
&le;M1(τ )+ τM1(0). (8.3.3)
</p>
<p>Since M1(0)=M1 &le; c and τ can be arbitrary small, we have D1 &rarr; 0 as n&rarr;&infin;.
2. Conversely, let condition [D1] be met. Then, for τ &le; 1,
</p>
<p>M1(τ ) &le;
n&sum;
</p>
<p>k=1
E
(
|ξk,n|; |ξk,n|&gt; 1
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>190 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>+ τ&minus;1
n&sum;
</p>
<p>k=1
E
(
|ξk,n|2; τ &lt; |ξk,n| &le; 1
</p>
<p>)
&le; τ&minus;1D1 &rarr; 0 (8.3.4)
</p>
<p>as n&rarr;&infin; for any τ &gt; 0. The lemma is proved. �
</p>
<p>Let us show that condition [M1] (as well as [D1]) is essential for the law of large
numbers to hold.
</p>
<p>Consider the random variables
</p>
<p>ξk,n =
{
</p>
<p>1 &minus; 1
n
</p>
<p>with probability 1
n
,
</p>
<p>&minus; 1
n
</p>
<p>with probability 1 &minus; 1
n
.
</p>
<p>For them, Eξk,n = 0, E|ξk,n| = 2(n&minus;1)n2 &sim;
2
n
</p>
<p>, M1 &le; 2, condition (8.3.2) is met, but
M1(τ )= n&minus;1n &gt;
</p>
<p>1
2
</p>
<p>for n &gt; 2, τ &lt; 1/2, and thus condition [M1] is not satisfied. Here
the number νn of positive ξk,n, 1 &le; k &le; n, converges in distribution to a random
variable ν having the Poisson distribution with parameter λ = 1. The sum of the
remaining ξk,ns is equal to &minus; (n&minus;νn)n
</p>
<p>p&minus;&rarr;&minus;1. Therefore, ζn + 1 &sub;=&rArr;�1 and the law
of large numbers does not hold.
</p>
<p>Each of the conditions [D1] and [M1] imply the uniform smallness of E|ξk,n|:
</p>
<p>max
1&le;k&le;n
</p>
<p>E|ξk,n| &rarr; 0 as n&rarr;&infin;. (8.3.5)
</p>
<p>Indeed, equation [M1] means that there exists a sufficiently slowly decreasing se-
quence τn &rarr; 0 such that M1(τn)&rarr; 0. Therefore
</p>
<p>max
k&le;n
</p>
<p>E|ξk,n| &le; max
k&le;n
</p>
<p>[
τn +E
</p>
<p>(
|ξk,n|; |ξk,n|&gt; τn
</p>
<p>)]
&le; τn +M1(τn)&rarr; 0. (8.3.6)
</p>
<p>In particular, (8.3.5) implies the negligibility of the summands ξk,n.
</p>
<p>We will say that ξk,n are negligible, or, equivalently, have property [S], if, for any
ε &gt; 0,
</p>
<p>max
k&le;n
</p>
<p>P
(
|ξk,n|&gt; ε
</p>
<p>)
&rarr; 0 as n&rarr;&infin;. [S]
</p>
<p>Property [S] could also be called uniform convergence of ξk,n in probability to
zero. Property [S] follows immediately from (8.3.5) and Chebyshev&rsquo;s inequality. It
also follows from stronger relations implied by [M1]:
</p>
<p>P
(
</p>
<p>max
k&le;n
</p>
<p>|ξk,n|&gt; ε
)
= P
</p>
<p>(⋃
</p>
<p>k&le;n
</p>
<p>{
|ξk,n|&gt; ε
</p>
<p>})
</p>
<p>&le;
&sum;
</p>
<p>k&le;n
P
(
|ξk,n|&gt; ε
</p>
<p>)
&le; ε&minus;1
</p>
<p>&sum;
</p>
<p>k&le;n
E
(
|ξk,n|; |ξk,n|&gt; ε
</p>
<p>)
&rarr; 0. [S1]
</p>
<p>We now turn to proving the law of large numbers. We will give two versions of
</p>
<p>the proof. The first one illustrates the classical method of characteristic functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 The Law of Large Numbers for Arbitrary Independent Random Variables 191
</p>
<p>The second version is based on elementary inequalities and leads to a stronger as-
</p>
<p>sertion about convergence in mean.1
</p>
<p>Here is the first version.
</p>
<p>Proof of Theorem 8.3.12 Put
</p>
<p>ϕk,n(t) := Eeitξk,n , ∆k(t) := ϕk,n(t)&minus; 1.
</p>
<p>One has to prove that, for each t ,
</p>
<p>ϕζn(t)= Eeitζn =
n&prod;
</p>
<p>k=1
ϕk,n(t)&rarr; 1,
</p>
<p>as n&rarr;&infin;. By Lemma 7.4.2
</p>
<p>∣∣ϕζn(t)&minus; 1
∣∣ =
</p>
<p>∣∣∣∣∣
</p>
<p>n&prod;
</p>
<p>k=1
ϕk,n(t)&minus;
</p>
<p>n&prod;
</p>
<p>k=1
1
</p>
<p>∣∣∣∣∣&le;
n&sum;
</p>
<p>k=1
|∆k(t)|
</p>
<p>=
n&sum;
</p>
<p>k=1
</p>
<p>∣∣Eeitξk,n &minus; 1
∣∣=
</p>
<p>n&sum;
</p>
<p>k=1
</p>
<p>∣∣E
(
eitξk,n &minus; 1 &minus; itξk,n
</p>
<p>)∣∣.
</p>
<p>By Lemma 7.4.1 we have (for g1(x)= min(|x|, x2))
∣∣eitx &minus; 1 &minus; itx
</p>
<p>∣∣&le; min
(
2|tx|, t2x2/2
</p>
<p>)
&le; 2g1(tx)&le; 2h(t)g1(t),
</p>
<p>where h(t)= max(|t |, |t |2). Therefore
</p>
<p>∣∣ϕζn(t)&minus; 1
∣∣&le; 2h(t)
</p>
<p>n&sum;
</p>
<p>k=1
Eg1(ξk,n)= 2h(t)D1 &rarr; 0.
</p>
<p>The theorem is proved. �
</p>
<p>The last inequality shows that |ϕζn(t) &minus; 1| admits a bound in terms of D1. It
turns out that E|ζn| also admits a bound in terms of D1. Now we will give the
second version of the proof that actually leads to a stronger variant of the law of
</p>
<p>large numbers.
</p>
<p>Theorem 8.3.2 Under conditions (8.3.1) and [D1] one has E|ζn| &rarr; 0 (i.e.
ζn
</p>
<p>(1)&minus;&rarr; 0).
</p>
<p>1The second version was communicated to us by A.I. Sakhanenko.
</p>
<p>2There exists an alternative &ldquo;direct&rdquo; proof of Theorem 8.3.1 using not ch.f.s but the so-called
</p>
<p>truncated random variables and estimates of their variances. However, because of what follows, it
</p>
<p>is more convenient for us to use here the machinery of ch.f.s.</p>
<p/>
</div>
<div class="page"><p/>
<p>192 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>The assertion of Theorem 8.3.2 clearly means the uniform integrability of {ζn};
it implies Theorem 8.3.1, for
</p>
<p>P
(
|ζn|&gt; ε
</p>
<p>)
&le; E|ζn|/ε&rarr; 0 as n&rarr;&infin;.
</p>
<p>Proof of Theorem 8.3.2 Put
</p>
<p>ξ &prime;k,n :=
{
ξk,n if |ξk,n| &le; 1,
0 otherwise,
</p>
<p>and ξ &prime;&prime;k,n := ξk,n &minus; ξ &prime;k,n. Then ξk,n = ξ &prime;k,n + ξ &prime;&prime;k,n and ζn = ζ &prime;n + ζ &prime;&prime;n with an obvious
convention for the notations ζ &prime;n, ζ
</p>
<p>&prime;&prime;
n . By the Cauchy&ndash;Bunjakovsky inequality,
</p>
<p>E|ζn| &le; E
∣∣ζ &prime;n &minus;Eζ &prime;n
</p>
<p>∣∣+E
∣∣ζ &prime;&prime;n &minus;Eζ &prime;&prime;n
</p>
<p>∣∣&le;
&radic;
E
(
ζ &prime;n &minus;Eζ &prime;n
</p>
<p>)2 +E
∣∣ζ &prime;&prime;n
</p>
<p>∣∣+
∣∣Eζ &prime;&prime;n
</p>
<p>∣∣
</p>
<p>&le;
&radic;&sum;
</p>
<p>Var
(
ξ &prime;k,n
</p>
<p>)
+ 2
</p>
<p>&sum;
E
∣∣ξ &prime;&prime;k,n
</p>
<p>∣∣&le;
&radic;&sum;
</p>
<p>E
(
ξ &prime;k,n
</p>
<p>)2 + 2
&sum;
</p>
<p>E
∣∣ξ &prime;&prime;k,n
</p>
<p>∣∣
</p>
<p>=
[&sum;
</p>
<p>E
(
ξ2k,n; |ξk,n| &le; 1
</p>
<p>)]1/2
</p>
<p>+ 2
&sum;
</p>
<p>E
(
|ξk,n|; |ξk,n|&gt; 1
</p>
<p>)
&le;
&radic;
D1 + 2D1 &rarr; 0,
</p>
<p>if D1 &rarr; 0. The theorem is proved. �
</p>
<p>Remark 8.3.1 It can be seen from the proof of Theorem 8.3.2 that the argument will
remain valid if we replace the independence of ξk,n by the weaker condition that
</p>
<p>ξ &prime;k,n are non-correlated. It will also be valid if ξ
&prime;
k,n are only weakly correlated so that
</p>
<p>E
(
ζ &prime;n &minus;Eζ &prime;n
</p>
<p>)2 &le; c
&sum;
</p>
<p>Var
(
ξ &prime;k,n
</p>
<p>)
, c &lt;&infin;.
</p>
<p>If {ξk} is a given fixed (not dependent on n) sequence of independent random
variables, Sn =
</p>
<p>&sum;n
k=1 ξk and Eξk = ak , then one looks at the applicability of the law
</p>
<p>of large numbers to the sequences
</p>
<p>ξk,n =
ξk &minus; ak
b(n)
</p>
<p>, ζn =
&sum;
</p>
<p>ξk,n =
1
</p>
<p>b(n)
</p>
<p>(
Sn &minus;
</p>
<p>n&sum;
</p>
<p>k=1
ak
</p>
<p>)
, (8.3.7)
</p>
<p>where ξk,n satisfy (8.3.1), and b(n) is an unboundedly increasing sequence. In some
</p>
<p>cases it is natural to take b(n) =
&sum;n
</p>
<p>k=1 E|ξk| if this sum increases unboundedly.
Without loss of generality we can set ak = 0. The next assertion follows from The-
orem 8.3.2.
</p>
<p>Corollary 8.3.1 If, as n&rarr;&infin;,
</p>
<p>D1 :=
1
</p>
<p>b(n)
</p>
<p>&sum;
Emin
</p>
<p>(
|ξk|, ξ2k /b(n)
</p>
<p>)
&rarr; 0</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 The Law of Large Numbers for Arbitrary Independent Random Variables 193
</p>
<p>or, for any τ &gt; 0,
</p>
<p>M1(τ )=
1
</p>
<p>b(n)
</p>
<p>&sum;
E
(
|ξk|; |ξk|&gt; τb(n)
</p>
<p>)
&rarr; 0, b(n)=
</p>
<p>n&sum;
</p>
<p>k=1
E|ξk| &rarr;&infin;, (8.3.8)
</p>
<p>then ζn
(1)&minus;&rarr; 0.
</p>
<p>Now we will present an important sufficient condition for the law of large num-
</p>
<p>bers that is very close to condition (8.3.8) and which explains to some extent its
</p>
<p>essence. In addition, in many cases this condition is easier to check. Let bk = E|ξk|,
bn = maxk&le;n bk , and, as before,
</p>
<p>Sn =
n&sum;
</p>
<p>k=1
ξk, b(n)=
</p>
<p>n&sum;
</p>
<p>k=1
bk.
</p>
<p>The following assertion is a direct generalisation of Theorem 8.1.1 and Corol-
</p>
<p>lary 8.1.1.
</p>
<p>Theorem 8.3.3 Let Eξk = 0, the sequence of normalised random variables ξk/bk
be uniformly integrable and bn = o(b(n)) as n&rarr;&infin;. Then
</p>
<p>Sn
</p>
<p>b(n)
</p>
<p>(1)&minus;&rarr; 0.
</p>
<p>If bn &le; b &lt;&infin; then b(n)&le; bn and Snn
(1)&minus;&rarr; 0.
</p>
<p>Proof Since
</p>
<p>E
(
|ξk|; |ξk|&gt; τb(n)
</p>
<p>)
&le; bkE
</p>
<p>(∣∣∣∣
ξk
</p>
<p>bk
</p>
<p>∣∣∣∣;
∣∣∣∣
ξk
</p>
<p>bk
</p>
<p>∣∣∣∣&gt; τ
b(n)
</p>
<p>bn
</p>
<p>)
(8.3.9)
</p>
<p>and b(n)
bn
</p>
<p>&rarr; &infin;, the uniform integrability of { ξk
bk
} implies that the right-hand side
</p>
<p>of (8.3.9) is o(bk) uniformly in k (i.e. it admits a bound ε(n)bk , where ε(n)&rarr; 0 as
n&rarr;&infin; and does not depend on k). Therefore
</p>
<p>M1(τ )=
1
</p>
<p>b(n)
</p>
<p>n&sum;
</p>
<p>k=1
E
(
|ξk|; |ξk|&gt; τb(n)
</p>
<p>)
&rarr; 0
</p>
<p>as n&rarr;&infin;, and condition (8.3.8) is met. The theorem is proved. �
</p>
<p>Remark 8.3.2 If, in the context of the law of large numbers, we are interested in
convergence in probability, only then can we generalise Theorem 8.3.3. In particular,
</p>
<p>convergence
</p>
<p>Sn
</p>
<p>b(n)
</p>
<p>p&rarr; 0</p>
<p/>
</div>
<div class="page"><p/>
<p>194 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>will still hold if a finite number of the summands ξk (e.g., for k &le; l, l being fixed)
are completely arbitrary (they can even fail to have expectations) and the sequence
ξ&lowast;k = ξk+l , k &ge; 1, satisfies the conditions of Theorem 8.3.3, where b(n) is defined for
the variables ξ&lowast;k and has the property
</p>
<p>b(n&minus;1)
b(n)
</p>
<p>&rarr; 1 as n&rarr;&infin;.
</p>
<p>This assertion follows from the fact that
</p>
<p>Sn
</p>
<p>b(n)
= Sl
</p>
<p>b(n)
+ Sn &minus; Sl
</p>
<p>b(n&minus; l) &middot;
b(n&minus; l)
b(n)
</p>
<p>,
Sl
</p>
<p>b(n)
</p>
<p>p&minus;&rarr; 0, b(n&minus; l)
b(n)
</p>
<p>&rarr; 1,
</p>
<p>and by Theorem 8.3.3
</p>
<p>Sn &minus; Sl
b(n&minus; l)
</p>
<p>p&minus;&rarr; 0 as n&rarr;&infin;.
</p>
<p>Now we will show that the uniform integrability condition in Theorem 8.3.3
</p>
<p>(as well as condition M1(τ )&rarr; 0) is essential for convergence ζn
p&rarr; 0. Consider a
</p>
<p>sequence of random variables
</p>
<p>ξj =
{
</p>
<p>2s &minus; 1 with probability 2&minus;s,
&minus;1 with probability 1 &minus; 2&minus;s
</p>
<p>for j &isin; Is := (2s&minus;1,2s], s = 1,2, . . . ; ξ1 = 0. Then Eξj = 0, E|ξj | = 2(1&minus; 2&minus;s) for
j &isin; Is , and, for n= 2k , one has
</p>
<p>b(n)=
k&sum;
</p>
<p>s=1
2
(
1 &minus; 2&minus;s
</p>
<p>)
|Is |,
</p>
<p>where |Is | = 2s &minus; 2s&minus;1 = 2s&minus;1 is the number of points in Ik . Hence, as k&rarr;&infin;,
</p>
<p>b(n) &sim; 2
[(
</p>
<p>1 &minus; 2&minus;k
)
2k&minus;1 +
</p>
<p>(
1 &minus; 2&minus;k+1
</p>
<p>)
2k&minus;2 + &middot; &middot; &middot;
</p>
<p>]
</p>
<p>&sim; 2k + 2k&minus;1 + . . .&sim; 2k+1 = 2n.
</p>
<p>Observe that the uniform integrability condition is clearly not met here. The distri-
</p>
<p>bution of the number ν(s) of jumps of magnitude 2s &minus;1 on the interval Is converges,
as s &rarr;&infin;, to the Poisson distribution with parameter 1/2 = lims&rarr;&infin; 2&minus;s |Is |, while
the distribution of 2&minus;s(S2s &minus; S2s&minus;1) converges to the distribution of ν &minus; 1/2, where
ν&sub;=�1/2. Hence, assuming that n= 2k , and partitioning the segment [2, n] into the
intervals (2s&minus;1,2s], s = 1, . . . , k, we obtain that the distribution of Sn/n converges,
as k&rarr;&infin;, to the distribution of
</p>
<p>Sn
</p>
<p>n
= 2&minus;k
</p>
<p>k&sum;
</p>
<p>s=1
</p>
<p>S2s &minus; S2s&minus;1
2s
</p>
<p>2s &rArr;
&infin;&sum;
</p>
<p>l=0
(νl &minus; 1/2)2&minus;l =: ζ,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 The Law of Large Numbers for Arbitrary Independent Random Variables 195
</p>
<p>where νl , l = 0,1, . . . , are independent copies of ν. Clearly, ζ 
&equiv; 0, and so conver-
gence Sn
</p>
<p>n
</p>
<p>p&rarr; 0 fails to take place.
Let us return to arbitrary ξk,n. In order for [D1] to hold it suffices that the follow-
</p>
<p>ing condition is met: for some s, 2 &ge; s &gt; 1,
n&sum;
</p>
<p>k=1
E|ξk,n|s &rarr; 0. [Ls]
</p>
<p>This assertion is evident, since g1(x) &le; |x|s for 2 &ge; s &gt; 1. Conditions [Ls] could
be called the modified Lyapunov conditions (cf. the Lyapunov condition [Ls] in
Sect. 8.4).
</p>
<p>To prove Theorem 8.3.2, we used the so-called &ldquo;truncated versions&rdquo; ξ &prime;k,n of the
random variables ξk,n. Now we will consider yet another variant of the law of large
</p>
<p>numbers, in which conditions are expressed in terms of truncated random variables.
Denote by ξ (N) the result of truncation of the random variable ξ at level N :
</p>
<p>ξ (N) = max
[
&minus;N,min(N, ξ)
</p>
<p>]
.
</p>
<p>Theorem 8.3.4 Let the sequence of random variables {ξk} in (8.3.7) satisfy the
following condition: for any given ε &gt; 0, there exist Nk such that
</p>
<p>1
</p>
<p>b(n)
</p>
<p>n&sum;
</p>
<p>k=1
E
∣∣ξk &minus; ξ (Nk)k
</p>
<p>∣∣&lt; ε, 1
b(n)
</p>
<p>n&sum;
</p>
<p>k=1
Nk &lt;N &lt;&infin;.
</p>
<p>Then the sequence {ζn} converges to zero in mean: ζn
(1)&minus;&rarr; 0.
</p>
<p>Proof Clearly a(Nk)k := Eξ
(Nk)
k &rarr; ak as Nk &rarr; &infin; and |a
</p>
<p>(Nk)
k | &le; Nk . Further, we
</p>
<p>have
</p>
<p>E|ζn| =
1
</p>
<p>b(n)
E
</p>
<p>∣∣∣
&sum;
</p>
<p>(ξk &minus; ak)
∣∣∣ &le; 1
</p>
<p>b(n)
</p>
<p>&sum;
E
∣∣ξk &minus; ξ (Nk)k
</p>
<p>∣∣
</p>
<p>+E
∣∣∣∣
&sum; ξ (Nk)k &minus; a
</p>
<p>(Nk)
k
</p>
<p>b(n)
</p>
<p>∣∣∣∣+
1
</p>
<p>b(n)
</p>
<p>&sum;∣∣a(Nk)k &minus; ak
∣∣.
</p>
<p>Here the second term on the right-hand side converges to zero, since the sum under
</p>
<p>the expectation satisfies the conditions of Theorem 8.3.1 and is bounded. But the
</p>
<p>first and the last terms do not exceed ε. Since the left-hand side does not depend on
</p>
<p>ε, we have E|ζn| &rarr; 0 as n&rarr;&infin;. �
</p>
<p>Corollary 8.3.2 If b(n)= n and, for sufficiently large N and all k &le; n,
</p>
<p>E
∣∣ξk &minus; ξ (N)k
</p>
<p>∣∣&lt; ε,
</p>
<p>then ζn
(1)&minus;&rarr; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>196 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>The corollary follows from Theorem 8.3.4, since the conditions of the corollary
</p>
<p>clearly imply the conditions of Theorem 8.3.4.
</p>
<p>It is obvious that, for identically distributed ξk , the conditions of Corollary 8.3.2
</p>
<p>are always met, and we again obtain a generalisation of Theorem 8.1.1 and Corol-
</p>
<p>lary 8.1.1.
</p>
<p>If E|ξk|r &lt;&infin; for r &ge; 1, then we can also establish in a similar way that
</p>
<p>Sn
</p>
<p>n
</p>
<p>(r)&minus;&rarr; a.
</p>
<p>Remark 8.3.3 Condition [D1] (or [M1]) is not necessary for convergence ζn
p&rarr; 0
</p>
<p>even when (8.3.2) and (8.3.5) hold, as the following example demonstrates. Let ξk,n
assume the values &minus;n, 0, and n with probabilities 1/n2, 1 &minus; 2/n2, and 1/n2, re-
spectively. Here ζn
</p>
<p>p&rarr; 0, since P(ζn 
= 0)&le; P(
⋃
{ξk,n 
= 0})&le; 2/n&rarr; 0, E|ξk,n| =
</p>
<p>2/n &rarr; 0 and M1 =
&sum;
</p>
<p>E|ξk,n| = 2 &lt; &infin;. At the same time,
&sum;
</p>
<p>E(|ξk,n|; |ξk,n| &ge;
1)= 2 
&rarr;&infin;, so that conditions [D1] and [M1] are not satisfied.
</p>
<p>However, if we require that
</p>
<p>ξk,n &ge;&minus;εk,n, εk,n &ge; 0,
</p>
<p>max
k&le;n
</p>
<p>εk,n &rarr; 0,
n&sum;
</p>
<p>k=1
εk,n &le; c &lt;&infin;,
</p>
<p>(8.3.10)
</p>
<p>then condition [D1] will become necessary for convergence ζn
p&rarr; 0.
</p>
<p>Before proving that assertion we will establish several auxiliary relations that
</p>
<p>will be useful in the sequel. As above, put ∆k(t) := ϕk,n(t)&minus; 1.
</p>
<p>Lemma 8.3.2 One has
n&sum;
</p>
<p>k=1
</p>
<p>∣∣∆k(t)
∣∣&le; |t |M1.
</p>
<p>If condition [S] holds, then for each t , as n&rarr;&infin;,
</p>
<p>max
k&le;n
</p>
<p>∣∣∆k(t)
∣∣&rarr; 0.
</p>
<p>If a random variable ξ with Eξ = 0 is bounded from the left: ξ &gt;&minus;c, c &gt; 0, then
E|ξ | &le; 2c.
</p>
<p>Proof By Lemma 7.4.1,
</p>
<p>∣∣∆k(t)
∣∣&le; E
</p>
<p>∣∣eitξk,n &minus; 1
∣∣&le; |t |E|ξk,n|,
</p>
<p>&sum;∣∣∆k(t)
∣∣&le; |t |M1.
</p>
<p>Further,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 The Law of Large Numbers for Arbitrary Independent Random Variables 197
</p>
<p>∣∣∆k(t)
∣∣&le; E
</p>
<p>(∣∣eitξk,n &minus; 1
∣∣; |ξk,n| &le; ε
</p>
<p>)
+E
</p>
<p>(∣∣eitξk,n &minus; 1
∣∣; |ξk,n|&gt; ε
</p>
<p>)
</p>
<p>&le; |t |ε+ 2P
(
|ξk,n|&gt; ε
</p>
<p>)
.
</p>
<p>Since ε is arbitrary here, the second assertion of the lemma now follows from con-
</p>
<p>dition [S].
Put
</p>
<p>ξ+ := max(0; ξ)&ge; 0, ξ&minus; := &minus;
(
ξ &minus; ξ+
</p>
<p>)
&ge; 0.
</p>
<p>Then Eξ = Eξ+ &minus; Eξ&minus; = 0 and E|ξ | = Eξ+ + Eξ&minus; = 2Eξ&minus; &le; 2c. The lemma is
proved. �
</p>
<p>From the last assertion of the lemma it follows that (8.3.10) implies (8.3.2) and
</p>
<p>(8.3.5).
</p>
<p>Lemma 8.3.3 Let conditions [S] and (8.3.2) be satisfied. A necessary and sufficient
condition for convergence ϕζn(t)&rarr; ϕ(t) is that
</p>
<p>n&sum;
</p>
<p>k=1
∆k(t)&rarr; lnϕ(t).
</p>
<p>Proof Observe that
</p>
<p>Re∆k(t)= Re
(
ϕk,n(t)&minus; 1
</p>
<p>)
&le; 0,
</p>
<p>∣∣e∆k(t)
∣∣&le; 1,
</p>
<p>and therefore, by Lemma 7.4.2,
</p>
<p>∣∣ϕzn(t)&minus; e
&sum;
</p>
<p>∆k(t)
∣∣ =
</p>
<p>∣∣∣∣∣
</p>
<p>n&prod;
</p>
<p>k=1
ϕk,n(t)&minus;
</p>
<p>n&prod;
</p>
<p>k=1
e∆k(t)
</p>
<p>∣∣∣∣∣
</p>
<p>&le;
n&sum;
</p>
<p>k=1
</p>
<p>∣∣ϕk,n(t)&minus; e∆k(t)
∣∣=
</p>
<p>n&sum;
</p>
<p>k=1
</p>
<p>∣∣e∆k(t) &minus; 1 &minus;∆k(t)
∣∣
</p>
<p>&le; 1
2
</p>
<p>n&sum;
</p>
<p>k=1
∆2k(t)&le;
</p>
<p>1
</p>
<p>2
max
k
</p>
<p>∣∣∆k(t)
∣∣
</p>
<p>n&sum;
</p>
<p>k=1
</p>
<p>∣∣∆k(t)
∣∣.
</p>
<p>By Lemma 8.3.2 and conditions [S] and (8.3.2), the expression on the left-hand side
converges to 0 as n&rarr;&infin;. Therefore, if ϕzn(t)&rarr; ϕ(t) then exp{
</p>
<p>&sum;
∆k(t)}&rarr; ϕ(t),
</p>
<p>and vice versa. The lemma is proved. �
</p>
<p>The next assertion complements Theorem 8.3.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>198 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Theorem 8.3.5 Assume that relations (8.3.1) and (8.3.10) hold. Then condition
[D1] (or condition [M1]) is necessary for the law of large numbers.
</p>
<p>Proof If the law of large numbers holds then ϕzn(t)&rarr; 1 and, hence by Lemma 8.3.3
(recall that (8.3.10) implies (8.3.2), (8.3.5) and [S])
</p>
<p>n&sum;
</p>
<p>k=1
∆k(t)=
</p>
<p>n&sum;
</p>
<p>k=1
E
(
eitξk,n &minus; 1 &minus; itξk,n
</p>
<p>)
&rarr; 0.
</p>
<p>Moreover, by Lemma 7.4.1
</p>
<p>n&sum;
</p>
<p>k=1
E
(∣∣eitξk,n &minus; 1 &minus; itξk,n
</p>
<p>∣∣; |ξk,n| &le; εk,n
)
</p>
<p>&le; 1
2
</p>
<p>n&sum;
</p>
<p>k=1
E
(
|xik,n|2; |ξk,n| &le; εk,n
</p>
<p>)
&le;
</p>
<p>n&sum;
</p>
<p>k=1
ε2k,n &le; max
</p>
<p>k
εk,n
</p>
<p>n&sum;
</p>
<p>k=1
εk,n &rarr; 0.
</p>
<p>Therefore, if the law of large numbers holds, then by virtue of (8.3.10)
</p>
<p>n&sum;
</p>
<p>k=1
E
(
eitξk,n &minus; 1 &minus; itξk,n; ξk,n &gt; εk,n
</p>
<p>)
&rarr; 0.
</p>
<p>Consider the function α(x)= (eix &minus; 1)/ix. It is not hard to see that the inequality
|α(x)| &le; 1 proved in Lemma 7.4.1 is strict for x &gt; ε &gt; 0, and hence there exists a
δ(τ ) &gt; 0 for τ &gt; 0 such that Re(1 &minus; α(x)) &ge; δ(τ ) for x &gt; τ . This is equivalent to
Im(1 + ix &minus; eix)&ge; δ(τ )x, so that
</p>
<p>x &le; 1
δ(τ )
</p>
<p>Im
(
1 + ix &minus; eix
</p>
<p>)
for x &gt; τ.
</p>
<p>From this we find that
</p>
<p>E1(τ ) =
n&sum;
</p>
<p>k=1
E
(
|ξk,n|; |ξk,n|&gt; τ
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>k=1
E(ξk,n; ξk,n &gt; τ)
</p>
<p>&le; 1
δ(τ )
</p>
<p>Im
</p>
<p>n&sum;
</p>
<p>k=1
E
(
1 + iξk,n &minus; eiξk,n; ξk,n &gt; εk,n
</p>
<p>)
&rarr; 0.
</p>
<p>Thus condition [M1] holds. Together with relation (8.3.2), that follows from
(8.3.10), this condition implies [D1]. The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 The Central Limit Theorem 199
</p>
<p>There seem to exist some conditions that are wider than (8.3.10) and under which
</p>
<p>condition [D1] is necessary for convergence ζn
(1)&minus;&rarr; 0 in mean (condition (8.3.10) is
</p>
<p>too restrictive).
</p>
<p>8.4 The Central Limit Theorem for Sums of Arbitrary
</p>
<p>Independent Random Variables
</p>
<p>As in Sect. 8.3, we consider here a triangular array of random variables ξ1,n, . . . , ξn,n
and their sums
</p>
<p>ζn =
n&sum;
</p>
<p>k=1
ξk,n. (8.4.1)
</p>
<p>We will assume that ξk,n have finite second moments:
</p>
<p>σ 2k,n := Var(ξk,n) &lt;&infin;,
</p>
<p>and suppose, without loss of generality, that
</p>
<p>Eξk,n = 0,
n&sum;
</p>
<p>k=1
σ 2k,n = Var(ζn)= 1. (8.4.2)
</p>
<p>We introduce the following condition: for some s &gt; 2,
</p>
<p>D2 :=
n&sum;
</p>
<p>k=1
Emin
</p>
<p>(
ξ2k,n, |ξk,n|s
</p>
<p>)
&rarr; 0 as n&rarr;&infin;, [D2]
</p>
<p>which is to play an important role in what follows. Our arguments related to condi-
</p>
<p>tion [D2] and also to conditions [M2] and [Ls] to be introduced below will be quite
similar to the ones from Sect. 8.3 that were related to conditions [D1], [M1] and
[Ls].
</p>
<p>We also introduce the Lindeberg condition: for any τ &gt; 0, as n&rarr;&infin;,
</p>
<p>M2(τ ) :=
n&sum;
</p>
<p>k=1
E
(
|ξk,n|2; |ξk,n|&gt; τ
</p>
<p>)
&rarr; 0. [M2]
</p>
<p>The following assertion is an analogue of Lemma 8.3.1.
</p>
<p>Lemma 8.4.1 1. {[M2] &cap; (4.2)} &sub; [D2]. 2. [D2] &sub; [M2].
</p>
<p>That is, conditions [M2] and (8.4.2) imply [D2], and condition [D2] implies
[M2].
</p>
<p>From Lemma 8.4.1 it follows that, under condition (8.4.2), conditions [D2] and
[M2] are equivalent.</p>
<p/>
</div>
<div class="page"><p/>
<p>200 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Proof of Lemma 8.4.1 1. Let conditions [M2] and (8.4.2) be met. Put
</p>
<p>g2(x) := min
(
x2, |x|s
</p>
<p>)
, s &gt; 2.
</p>
<p>Then (cf. (8.3.3), (8.3.4); τ &le; 1)
</p>
<p>D2 =
n&sum;
</p>
<p>k=1
Eg2(ξk,n)&le;
</p>
<p>n&sum;
</p>
<p>k=1
E
(
ξ2k,n; |ξk,n|&gt; τ
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>k=1
E
(
|ξk,n|s; |ξk,n| &le; τ
</p>
<p>)
</p>
<p>&le;M2(τ )+ τ s&minus;2M2(0)=M2(τ )+ τ s&minus;2.
</p>
<p>Since τ is arbitrary, we have D2 &rarr; 0 as n&rarr;&infin;.
2. Conversely, suppose that [D2] holds. Then
</p>
<p>M2(τ )&le;
n&sum;
</p>
<p>k=1
E
(
ξ2k,n; |ξk,n|&gt; 1
</p>
<p>)
+ 1
</p>
<p>τ s&minus;2
</p>
<p>n&sum;
</p>
<p>k=1
</p>
<p>(
|ξk,n|s; τ &lt; |ξk,n| &le; 1
</p>
<p>)
&le; 1
</p>
<p>τ s&minus;2
D2 &rarr; 0
</p>
<p>for any τ &gt; 0, as n&rarr;&infin;. The lemma is proved. �
</p>
<p>Lemma 8.4.1 also implies that if (8.4.2) holds, then condition [D2] is &ldquo;invariant&rdquo;
with respect to s &gt; 2.
</p>
<p>Condition [D2] can be stated in a more general form:
n&sum;
</p>
<p>k=1
Eξ2k,nh
</p>
<p>(
|ξk,n|
</p>
<p>)
&rarr; 0,
</p>
<p>where h(x) is any function for which h(x) &gt; 0 for x &gt; 0, h(x) &uarr;, h(x) &rarr; 0 as
x &rarr; 0, and h(x)&rarr; c &lt;&infin; as x &rarr;&infin;. All the key properties of condition [D2] will
then be preserved. The Lindeberg condition clarifies the meaning of condition [D2]
from a somewhat different point of view. In Lindeberg&rsquo;s condition, h(x) = I(τ,&infin;),
τ &isin; (0,1). A similar remark may be made with regard to conditions [D1] and [M1]
in Sect. 8.3.
</p>
<p>In a way similar to what we did in Sect. 8.3 when discussing condition [M1], one
can easily verify that condition [M2] implies convergence (see (8.3.6))
</p>
<p>max
k&le;n
</p>
<p>Var(ξk,n)&rarr; 0 (8.4.3)
</p>
<p>and the negligibility of ξk,n (property [S]). Moreover, one obviously has the inequal-
ity
</p>
<p>M1(τ )&le;
1
</p>
<p>τ
M2(τ ).
</p>
<p>For a given fixed (independent of n) sequence {ξk} of independent random vari-
ables,
</p>
<p>Sn =
&infin;&sum;
</p>
<p>k=1
ξk, Eξk = ak, Var(ξk)= σ 2k , (8.4.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 The Central Limit Theorem 201
</p>
<p>one considers the asymptotic behaviour of the normed sums
</p>
<p>ζn =
1
</p>
<p>Bn
</p>
<p>(
Sn &minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
ak
</p>
<p>)
, B2n =
</p>
<p>&infin;&sum;
</p>
<p>k=1
σ 2k , (8.4.5)
</p>
<p>that are clearly also of the form (8.4.1) with ξk,n = (ξk &minus; ak)/Bn.
Conditions [D1] and [M2] for ξk will take the form
</p>
<p>D2 =
1
</p>
<p>B2n
</p>
<p>&infin;&sum;
</p>
<p>k=1
Emin
</p>
<p>(
(ξk &minus; ak)2,
</p>
<p>|ξk &minus; ak|s
</p>
<p>Bs&minus;2n
</p>
<p>)
&rarr; 0, s &gt; 2;
</p>
<p>M2(τ )=
1
</p>
<p>B2n
</p>
<p>&infin;&sum;
</p>
<p>k=1
E
(
(ξk &minus; ak)2; |ξk &minus; ak|&gt; τBn
</p>
<p>)
&rarr; 0, τ &gt; 0.
</p>
<p>(8.4.6)
</p>
<p>Theorem 8.4.1 (The Central Limit Theorem) If the sequences of random vari-
ables {ξk,n}&infin;k=1, n= 1,2, . . . , satisfy conditions (8.4.2) and [D2] (or [M2]) then, as
n&rarr;&infin;, P(ζn &lt; x)&rarr;Φ(x) uniformly in x.
</p>
<p>Proof It suffices to verify that
</p>
<p>ϕζn(t)=
&infin;&prod;
</p>
<p>k=1
ϕk,n(t)&rarr; e&minus;t
</p>
<p>2/2.
</p>
<p>By Lemma 7.4.2,
</p>
<p>∣∣ϕζn(t)&minus; e&minus;t
2/2
</p>
<p>∣∣ =
∣∣∣∣∣
</p>
<p>n&prod;
</p>
<p>k=1
ϕk,n(t)&minus;
</p>
<p>n&prod;
</p>
<p>k=1
e
&minus;t2σ 2k,n/2
</p>
<p>∣∣∣∣∣
</p>
<p>&le;
n&sum;
</p>
<p>k=1
</p>
<p>∣∣ϕk,n(t)&minus; e&minus;t
2σ 2k,n/2
</p>
<p>∣∣
</p>
<p>&le;
n&sum;
</p>
<p>k=1
</p>
<p>∣∣∣∣ϕk,n(t)&minus; 1 +
1
</p>
<p>2
t2σ 2k,n
</p>
<p>∣∣∣∣
</p>
<p>+
n&sum;
</p>
<p>k=1
</p>
<p>∣∣∣∣e
&minus;t2σ 2k,n/2 &minus; 1 + 1
</p>
<p>2
t2σ 2k,n
</p>
<p>∣∣∣∣. (8.4.7)
</p>
<p>Since by Lemma 7.4.1, for s &le; 3,
∣∣∣∣e
</p>
<p>ix &minus; 1 &minus; ix + x
2
</p>
<p>2
</p>
<p>∣∣∣∣&le; min
(
x2,
</p>
<p>|x3|
6
</p>
<p>)
&le; g2(x)</p>
<p/>
</div>
<div class="page"><p/>
<p>202 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>(see the definition of the function g2 in the beginning of the proof of Lemma 8.4.1),
</p>
<p>the first sum on the right-hand side of (8.4.7) does not exceed
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>∣∣∣∣E
(
eitξk,n &minus; 1 &minus; itξk,n +
</p>
<p>1
</p>
<p>2
t2ξ2k,n
</p>
<p>)∣∣∣∣
</p>
<p>&le;
&infin;&sum;
</p>
<p>k=1
Eg2
</p>
<p>(
|tξk,n|
</p>
<p>)
&le; h(t)
</p>
<p>&infin;&sum;
</p>
<p>k=1
Eg2
</p>
<p>(
|ξk,n|
</p>
<p>)
&le; h(t)D2 &rarr; 0,
</p>
<p>where h(t) = max(t2, |t |3). The last sum in (8.4.7) (again by Lemma 7.4.1) does
not exceed (see (8.4.2) and (8.4.3))
</p>
<p>t4
</p>
<p>8
</p>
<p>n&sum;
</p>
<p>k=1
σ 4k,n &le;
</p>
<p>t4
</p>
<p>8
max
k
</p>
<p>σ 2k,n
</p>
<p>n&sum;
</p>
<p>k=1
σ 2k,n &le;
</p>
<p>t4
</p>
<p>8
max
k
</p>
<p>σ 2k,n &rarr; 0 as n&rarr;&infin;.
</p>
<p>The theorem is proved. �
</p>
<p>If we change the second relation in (8.4.2) to Eζn &rarr; σ 2 &gt; 0, then, introducing
the new random variables ξ &prime;k,n = ξk,n/
</p>
<p>&radic;
Var ζn and using continuity theorems, it is
</p>
<p>not hard to obtain from Theorem 8.4.1 (see e.g. Lemma 6.2.2), the following asser-
</p>
<p>tion, which sometimes proves to be more useful in applications than Theorem 8.4.1.
</p>
<p>Corollary 8.4.1 Assume that Eξk,n = 0, Var(ζn)&rarr; σ 2 &gt; 0, and condition [D2] (or
[M2]) is satisfied. Then ζn &sub;=&rArr;�0,σ2 .
</p>
<p>Remark 8.4.1 A sufficient condition for [D2] and [M2] is provided by the more re-
strictive Lyapunov condition, the verification of which is sometimes easier. Assume
that (8.4.2) holds. For s &gt; 2, the quantity
</p>
<p>Ls :=
n&sum;
</p>
<p>k=1
E|ξk,n|2
</p>
<p>is called the Lyapunov fraction of the s-th order. The condition
</p>
<p>Ls &rarr; 0 as n&rarr;&infin; [Ls]
</p>
<p>is called the Lyapunov condition.
</p>
<p>The quantity Ls is called a fraction since for ξk,n = (ξk&minus;a)/Bn (where ak = Eξk ,
B2n =
</p>
<p>&sum;n
k=1 Var(ξk) and ξk do not depend on n), it has the form
</p>
<p>Ls =
1
</p>
<p>Bsn
</p>
<p>n&sum;
</p>
<p>k=1
E|ξk &minus; ak|s .</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 The Central Limit Theorem 203
</p>
<p>If the ξk are identically distributed, ak = a, Var(ξk)= σ 2, and E|ξk &minus; a|s = &micro;&lt;&infin;,
then
</p>
<p>Ls =
&micro;
</p>
<p>σ sn(s&minus;2)/2
&rarr; 0.
</p>
<p>The sufficiency of the Lyapunov condition follows from the obvious inequalities
</p>
<p>g2(x)&le; |x|s for any s, D2 &le; Ls .
In the case of (8.4.4) and (8.4.5) we can give a sufficient condition for the in-
</p>
<p>tegral limit theorem that is very close to the Lindeberg condition [M2]; the former
condition elucidates to some extent the essence of the latter (cf. Theorem 8.3.3), and
</p>
<p>in many cases it is easier to verify. Put σ n = maxk&le;n σk . Theorem 8.4.1 implies the
following assertion which is a direct extension of Theorem 8.2.1
</p>
<p>Theorem 8.4.2 Let conditions (8.4.4) and (8.4.5) be satisfied, the sequence of
normalised random variables ξ2k /σ
</p>
<p>2
k be uniformly integrable and σ n = o(Bn) as
</p>
<p>n&rarr;&infin;. Then ζn &sub;&rArr; �0,1.
</p>
<p>Proof of Theorem 8.4.2 repeats, to some extent, the proof of Theorem 8.3.3. For
simplicity assume that ak = 0. Then
</p>
<p>E
(
ξ2k ; |ξk|&gt; τBn
</p>
<p>)
&le; σ 2k E
</p>
<p>(
ξ2k
</p>
<p>σ 2k
</p>
<p>;
∣∣∣∣
ξk
</p>
<p>σk
</p>
<p>∣∣∣∣&gt; τ
Bn
</p>
<p>σ n
</p>
<p>)
, (8.4.8)
</p>
<p>where Bn/σ n &rarr;&infin;. Hence, it follows from the uniform integrability of {
ξ2k
σ 2k
} that
</p>
<p>the right-hand side of (8.4.8) is o(σ 2k ) uniformly in k. This means that
</p>
<p>M2(τ )=
1
</p>
<p>B2n
</p>
<p>n&sum;
</p>
<p>k=1
E
(
ξ2k ; |ξk|&gt; τBn
</p>
<p>)
&rarr; 0
</p>
<p>as n &rarr; &infin; and condition (8.4.6) (or condition [M2]) is satisfied. The theorem is
proved. �
</p>
<p>Remark 8.4.2 We can generalise the assertion of Theorem 8.4.2 (cf. Remark 8.3.3).
In particular, convergence ζn&sub;=&rArr;�0,1 still takes place if a finite number of summands
ξk (e.g., for k &le; l, l being fixed) are completely arbitrary, and the sequence ξ&lowast;k :=
ξk+l , k &ge; 1, satisfies the conditions of Theorem 8.4.2, in which we put σ 2k = Var(ξ&lowast;k ),
B2n =
</p>
<p>&sum;n
k=1 σ
</p>
<p>2
k , and it is also assumed that
</p>
<p>Bn&minus;1
Bn
</p>
<p>&rarr; 1 as n&rarr;&infin;.
</p>
<p>This assertion follows from the fact that
</p>
<p>Sn
</p>
<p>Bn
= Sl
</p>
<p>Bn
+ Sn &minus; Sl
</p>
<p>Bn&minus;l
&middot; Bn&minus;l
</p>
<p>Bn
,
</p>
<p>where Sl
Bn
</p>
<p>p&rarr; 0, Bn&minus;l
Bn
</p>
<p>&rarr; 1 and, by Theorem 8.4.2, Sn&minus;Sl
Bn&minus;l
</p>
<p>&sub;=&rArr;�0,1 as n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>204 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Remark 8.4.3 The uniform integrability condition that was used in Theorem 8.4.2
can be used for the triangular array scheme as well. In this more general case the
</p>
<p>uniform integrability should mean the following: the sequences η1,n, . . . , ηn,n, n=
1,2, . . . , in the triangular array scheme are uniformly integrable if there exists a
function ε(N) &darr; 0 as N &uarr;&infin; such that, for all n,
</p>
<p>max
j&le;n
</p>
<p>E
(
|ηj,n|; |ηj,n|&gt;N
</p>
<p>)
&le; ε(N).
</p>
<p>It is not hard to see that, with such an interpretation of uniform integrability,
</p>
<p>the assertion of Theorem 8.4.2 holds true for the triangular array scheme as well
</p>
<p>provided that the sequence { ξ
2
j,n
</p>
<p>σ 2j,n
} is uniformly integrable and maxj&le;n σj,n = o(1) as
</p>
<p>n&rarr;&infin;.
</p>
<p>Example 8.4.1 We will clarify the difference between the Lindeberg condition and
</p>
<p>uniform integrability of { ξ
2
k
</p>
<p>σ 2k
} in the following example. Let ηk be independent
</p>
<p>bounded identically distributed random variables, Eηk = 0, Dηk = 1 and g(k) &gt;
&radic;
</p>
<p>2
</p>
<p>be an arbitrary function. Put
</p>
<p>ξk :=
{
ηk with probability 1 &minus; 2g&minus;2(k),
&plusmn;g(k) with probability g&minus;2(k).
</p>
<p>Then clearly Eξk = 0, σ 2k := Dξk = 3 &minus; 2g&minus;2(k) &isin; (2,3) and B2n &isin; (2n,3n). The
uniform integrability of { ξ
</p>
<p>2
k
</p>
<p>σ 2k
}, or the uniform integrability of {ξ2k } which means the
</p>
<p>same in our case, excludes the case where g(k) &rarr; &infin; as k &rarr; &infin;. The Lindeberg
condition is wider and allows the growth of g(k), except for the case where g(k) &gt;
</p>
<p>c
&radic;
k. If g(k) = o(
</p>
<p>&radic;
k), then the Lindeberg condition is satisfied because, for any
</p>
<p>fixed τ &gt; 0,
</p>
<p>E
(
ξ2; |ξk|&gt; τ
</p>
<p>&radic;
k
)
= 0
</p>
<p>for all large enough k.
</p>
<p>Remark 8.4.4 Let us show that condition [M2] (or [D2]) is essential for the central
limit theorem. Consider random variables
</p>
<p>ξk,n =
{
&plusmn; 1&radic;
</p>
<p>2
with probability 1
</p>
<p>n
,
</p>
<p>0 with probability 1 &minus; 2
n
.
</p>
<p>They satisfy conditions (8.4.2), [S], but not the Lindeberg condition as M2(τ )= 1
for τ &lt; 1&radic;
</p>
<p>2
. The number νk of non-zero summands converges in distribution to
</p>
<p>a random variable ν having the Poisson distribution with parameter 2. Therefore, ζn
will clearly converge in distribution not to the normal law, but to
</p>
<p>&sum;ν
j=1 γj , where
</p>
<p>γj are independent and take values &plusmn;1 with probability 1/2.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 The Central Limit Theorem 205
</p>
<p>Note also that conditions [D2] or [M2] are not necessary for convergence of
the distributions of ζn to the normal distribution. Indeed, consider the following
</p>
<p>example: ξ1,n &sub;= �0,1, ξ2,n = &middot; &middot; &middot; = ξn,n = 0. Conditions (8.4.2) are clearly met,
P(ζn &lt; x) = Φ(x), but the variables ξk,n are not negligible and therefore do not
satisfy conditions [D2] and [M2].
</p>
<p>If, however, as well as convergence ζn &sub;=&rArr;�0,1 we require that the ξk,n are neg-
ligible, then conditions [D2] and [M2] become necessary.
</p>
<p>Theorem 8.4.3 Suppose that the sequences of independent random variables
{ξk,n}nk=1 satisfy conditions (8.4.2) and [S]. Then condition [D1] (or [M2]) is neces-
sary and sufficient for convergence ζn &sub;=&rArr;�0,1.
</p>
<p>First note that the assertions of Lemmas 8.3.2 and 8.3.3 remain true, up to some
</p>
<p>inessential modifications, if we substitute conditions (8.3.2) and [S] with (8.4.2)
and [S].
</p>
<p>Lemma 8.4.2 Let conditions (8.4.2) and [S] hold. Then (∆k(t)= ϕk,n(t)&minus; 1)
</p>
<p>max
k&le;n
</p>
<p>∣∣∆k(t)
∣∣&rarr; 0,
</p>
<p>&sum;∣∣∆k(t)
∣∣&le; t
</p>
<p>2
</p>
<p>2
,
</p>
<p>and the assertion of Lemma 8.3.3, that the convergence (8.3.10) is necessary and
sufficient for convergence ϕζn(t)&rarr; ϕ(t), remain completely true.
</p>
<p>Proof We can retain all the arguments in the proofs of Lemmas 8.3.2 and 8.3.3
except for one place where
</p>
<p>&sum;
|∆k(t)| is bounded. Under the new conditions, by
</p>
<p>Lemma 7.4.1, we have
</p>
<p>∣∣∆k(t)
∣∣=
</p>
<p>∣∣ϕk,n(t)&minus; 1 &minus; itEξk,n
∣∣&le; E
</p>
<p>∣∣eitξk,n &minus; 1 &minus; itξk,n
∣∣&le; t
</p>
<p>2
</p>
<p>2
E ξ2k,n,
</p>
<p>so that
</p>
<p>&sum;∣∣∆k(t)
∣∣&le; t
</p>
<p>2
</p>
<p>2
.
</p>
<p>No other changes in the proofs of Lemmas 8.3.2 and 8.3.3 are needed. �
</p>
<p>Proof of Theorem 8.4.3 Sufficiency is already proved. To prove necessity, we make
</p>
<p>use of Lemma 8.4.1. If ϕζn(t)&rarr; e&minus;t
2/2, then by virtue of that lemma, for ∆k(t)=
</p>
<p>ϕk,n(t)&minus; 1, one has
n&sum;
</p>
<p>k=1
∆k(t)&rarr; lnϕ(t)=&minus;
</p>
<p>t2
</p>
<p>2
.
</p>
<p>For t = 1 the above relation can be written in the form
</p>
<p>Rn :=
n&sum;
</p>
<p>k=1
E
</p>
<p>(
eiξk,n &minus; 1 &minus; iξk,n +
</p>
<p>1
</p>
<p>2
ξ2k,n
</p>
<p>)
&rarr; 0. (8.4.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>206 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Put α(x) := (eix &minus; 1 &minus; ix)/x2. It is not hard to see that the inequality |α(x)| &le; 1/2
proved in Lemma 7.4.1 is strict for x 
= 0, and
</p>
<p>sup
|x|&ge;τ
</p>
<p>∣∣α(x)
∣∣&lt; 1
</p>
<p>2
&minus; δ(τ ),
</p>
<p>where δ(τ ) &gt; 0 for τ &gt; 0. This means that, for |x| &ge; τ &gt; 0,
</p>
<p>Re
</p>
<p>[
α(x)+ 1
</p>
<p>2
</p>
<p>]
&ge; δ(τ ) &gt; 0, x2 &le; 1
</p>
<p>δ(τ )
Re
</p>
<p>(
eix &minus; 1 &minus; ix + x
</p>
<p>2
</p>
<p>2
</p>
<p>)
,
</p>
<p>E
(
ξ2k,n; |ξk,n|&gt; τ
</p>
<p>)
&le; 1
</p>
<p>δ(τ )
ReE
</p>
<p>(
eiξk,n &minus; 1 &minus; iξk,n +
</p>
<p>ξ2k,n
</p>
<p>2
</p>
<p>)
,
</p>
<p>and hence by virtue of (8.4.9), for any τ &gt; 0,
</p>
<p>M2(τ )&le;
1
</p>
<p>δ(τ )
|Rn|&rarr; 0
</p>
<p>as n&rarr;&infin;. The theorem is proved. �
</p>
<p>Corollary 8.4.2 Assume that (8.4.2) holds and
</p>
<p>max
k&le;n
</p>
<p>Var(ξk,n)&rarr; 0. (8.4.10)
</p>
<p>Then a necessary and sufficient condition for convergence ζn &sub;=&rArr;�0,1 is that
</p>
<p>ηn :=
n&sum;
</p>
<p>k=1
ξ2k,n &sub;=&rArr; I1
</p>
<p>(or that ηn
p&rarr; 1).
</p>
<p>Proof Let ηn &sub;=&rArr; I1. The random variables ξ &prime;k,n = ξ2k,n &minus; σ 2k,n satisfy, by virtue of
(8.4.10), condition (8.3.10) and satisfy the law of large numbers:
</p>
<p>n&sum;
</p>
<p>k=1
ξ &prime;k,n = ηn &minus; 1
</p>
<p>p&rarr; 0.
</p>
<p>Therefore, by Theorem 8.3.5, the ξ &prime;k,n satisfy condition [M1]: for any τ &gt; 0,
</p>
<p>n&sum;
</p>
<p>k=1
E
(∣∣ξ2k,n &minus; σ 2k,n
</p>
<p>∣∣;
∣∣ξ2k,n &minus; σ 2k,n
</p>
<p>∣∣&gt; τ
)
&rarr; 0. (8.4.11)
</p>
<p>But by (8.4.10) this condition is clearly equivalent to condition [M2] for ξk,n, and
hence ζn &sub;=&rArr;�0,1.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 The Central Limit Theorem 207
</p>
<p>Conversely, if ζn&sub;=&rArr;�0,1, then [M2] holds for ξk,n which implies (8.4.11). Since,
moreover,
</p>
<p>n&sum;
</p>
<p>k=1
E
∣∣ξ &prime;k,n
</p>
<p>∣∣&le; 2
n&sum;
</p>
<p>k=1
Var(ξk,n)= 2,
</p>
<p>relation (8.3.2) holds for ξ &prime;k,n, and by Theorem 8.3.1
</p>
<p>n&sum;
</p>
<p>k=1
ξ &prime;k,n = ηn &minus; 1
</p>
<p>p&rarr; 0.
</p>
<p>The corollary is proved. �
</p>
<p>Example 8.4.2 Let ξk , k = 1,2, . . . , be independent random variables with distribu-
tions
</p>
<p>P
(
ξk = kα
</p>
<p>)
= P
</p>
<p>(
ξk =&minus;kα
</p>
<p>)
= 1
</p>
<p>2
.
</p>
<p>Evidently, ξk can be represented as ξk = kαηk , where ηk d= η are independent,
</p>
<p>P(η= 1)= P(η=&minus;1)= 1
2
, Var(η)= 1, σ 2k = Var(ξk)= k2α.
</p>
<p>Let us show that, for all α &ge; &minus;1/2, the random variables Sn/Bn are asymptoti-
cally normal. Since
</p>
<p>ξ2k
</p>
<p>σ 2k
</p>
<p>d= η2
</p>
<p>are uniformly integrable, by Theorem 8.4.2 it suffices to verify the condition
</p>
<p>σ n = max
k&le;n
</p>
<p>σk = o(Bn).
</p>
<p>In our case σ n = max(1, n2α) and, for α &gt;&minus;1/2,
</p>
<p>B2n =
n&sum;
</p>
<p>k=1
k2α &sim;
</p>
<p>&int; n
</p>
<p>0
</p>
<p>x2αdα = n
2α+1
</p>
<p>2α + 1 .
</p>
<p>For α =&minus;1/2, one has
</p>
<p>B2n =
n&sum;
</p>
<p>k=1
k&minus;1 &sim; lnn.
</p>
<p>Clearly, in these cases σ n = o(Bn) and the asymptotical normality of Sn/n holds.
If α &lt; &minus;1/2 then the sequence Bn converges, condition σ n = 1 = o(Bn) is not
</p>
<p>satisfied and the asymptotical normality of Sn/Bn fails to take place.</p>
<p/>
</div>
<div class="page"><p/>
<p>208 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Note that, for α =&minus;1/2, the random variable
</p>
<p>Sn =
n&sum;
</p>
<p>k=1
</p>
<p>ηk&radic;
k
</p>
<p>will be &ldquo;comparable&rdquo; with
&radic;
</p>
<p>lnn with a high probability, while the sums
</p>
<p>n&sum;
</p>
<p>k=1
</p>
<p>(&minus;1)k&radic;
k
</p>
<p>converge to a constant.
</p>
<p>A rather graphical and well-known illustration of the above theorems is the scat-
</p>
<p>tering of shells when shooting at a target. The fact is that the trajectory of a shell is
</p>
<p>influenced by a large number of independent factors of which the individual effects
</p>
<p>are small. These are deviations in the amount of gun powder, in the weight and size
</p>
<p>of a shell, variations in the humidity and temperature of the air, wind direction and
</p>
<p>velocities at different altitudes and so on. As a result, the deviation of a shell from
</p>
<p>the aiming point is described by the normal law with an amazing accuracy.
</p>
<p>Similar observations could be made about errors in measurements when their
</p>
<p>accuracy is affected by many &ldquo;small&rdquo; factors. (There even exists a theory of errors
</p>
<p>of which the crucial element is the central limit theorem.)
</p>
<p>On the whole, the central limit theorem has a lot of applications in various areas.
</p>
<p>This is due to its universality and robustness under small deviations from the as-
sumptions of the theorem, and its relatively high accuracy even for moderate values
of n. The first two noted qualities mean that:
</p>
<p>(1) the theorem is applicable to variables ξk,n with any distributions so long as
the variances of ξk,n exist and are &ldquo;negligible&rdquo;;
</p>
<p>(2) the presence of a &ldquo;moderate&rdquo; dependence3 between ξk,n does not change the
</p>
<p>normality of the limiting distribution.
</p>
<p>To illustrate the accuracy of the normal approximation, consider the following
</p>
<p>example. Let Fn(x)= P(Sn/
&radic;
n &lt; x) be the distribution function of the normalised
</p>
<p>sum Sn of independent variables ξk uniformly distributed over [&minus;
&radic;
</p>
<p>3,
&radic;
</p>
<p>3], so that
Var(ξk)= 1. Then it turns out that already for n= 5 (!) the maximum of |Fn(x)&minus;
Φ(x)| over the whole axis of x-values does not exceed 0.006 (the maximum is
attained near the points x =&plusmn;0.7).
</p>
<p>And still, despite the above circumstances, one has to be careful when applying
</p>
<p>the central limit theorem. For instance, one cannot expect high accuracy from the
</p>
<p>normal approximation when estimating probabilities of rare events, say when study-
</p>
<p>ing large deviation probabilities (this issue has already been discussed in Sect. 5.3).
</p>
<p>3There exist several conditions characterising admissible dependence of ξk,n. Such considerations
</p>
<p>are beyond the scope of the present book, but can be found in the special literature. See e.g. [20].</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Another Approach to Proving Limit Theorems 209
</p>
<p>After all, the theorem only ensures the smallness of the difference
</p>
<p>∣∣Φ(x)&minus; P(ζ &lt; x)
∣∣ (8.4.12)
</p>
<p>for large n. Suppose we want to use the normal approximation to find an x0 such
</p>
<p>that the event {ζn &gt; x0} would occur on average once in 1000 trials (a problem
of this sort could be encountered by an experimenter who wants to ensure that, in
</p>
<p>a single experiment, such an event will not occur). Even if the difference (8.4.12)
</p>
<p>does not exceed 0.02 (which can be a good approximation) then, using the normal
</p>
<p>approximation, we risk making a serious error. It can turn out, say, that 1&minus;Φ(x0)=
10&minus;3 while P(ζ &lt; x) &asymp; 0.02, and then the event {ζn &gt; x0} will occur much more
often (on average, once in each 50 trials).
</p>
<p>In Chap. 9 we will consider the problem of large deviation probabilities that
</p>
<p>enables one to handle such situations. In that case one looks for a function P(n,x)
</p>
<p>such that P(ζ &lt; x)/P (n, x)&rarr; 1 as n&rarr;&infin;, x &rarr;&infin;. The function P(n,x) turns
out to be, generally speaking, different from 1&minus;Φ(x). We should note however that
using the approximation P(n,x) requires more restrictive conditions on {ξk,n}.
</p>
<p>In Sect. 8.7 we will consider the so-called integro-local and local limit theorems
that establish convergence of the density of ζn to that of the normal law and enables
</p>
<p>one to estimate probabilities of rare events of another sort&mdash;say, of the form {a &lt;
ζn &lt; b} where a and b are close to each other.
</p>
<p>8.5*Another Approach to Proving Limit Theorems. Estimating
</p>
<p>Approximation Rates
</p>
<p>The approach to proving the principal limit theorems for the distributions of sums of
</p>
<p>random variables that we considered in Sects. 8.1&ndash;8.4 was based on the use of ch.f.s.
</p>
<p>However, this is by far not the only method of proof of such assertions. Nowadays
</p>
<p>there exist several rather simple proofs of both the laws of large numbers and the
</p>
<p>central limit theorem that do not use the apparatus of ch.f.s. (This, however, does not
</p>
<p>belittle that powerful, well-developed, and rather universal tool.) Moreover, these
</p>
<p>proofs sometimes enable one to obtain more general results. As an illustration, we
</p>
<p>will give below a proof of the central limit theorem that extends, in a certain sense,
</p>
<p>Theorems 8.4.1 and 8.4.3 and provides an estimate of the convergence rate (although
</p>
<p>not the best one).
</p>
<p>Along with the random variables ξk,n in the triangular array scheme under as-
</p>
<p>sumption (8.4.2), consider mutually independent and independent of the sequence
</p>
<p>{ξk,n}nk=1 random variables ηk,n &sub;=�0,σ 2k,n , σk,n := Var(ξk,n), so that
</p>
<p>ηn :=
n&sum;
</p>
<p>k=1
ηk,n &sub;=�0,1.</p>
<p/>
</div>
<div class="page"><p/>
<p>210 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Set4
</p>
<p>&micro;k,n := E|ξk,n|3, νk,n := E|ηk,n|3 = c3σ 3k,n &le; c3&micro;k,n,
</p>
<p>&micro;0k,n :=
&int;
</p>
<p>|x|3
∣∣d
</p>
<p>(
Fk,n(x)&minus;Φk,n(x)
</p>
<p>)∣∣&le; &micro;k,n + νk,n,
</p>
<p>L3 :=
n&sum;
</p>
<p>k=1
&micro;k,n, N3 :=
</p>
<p>n&sum;
</p>
<p>k=1
νk,n, L
</p>
<p>0
3 :=
</p>
<p>n&sum;
</p>
<p>k=1
&micro;0k,n &le; L3 +N3 &le; (1 + c3)L3.
</p>
<p>Here Fk,n and Φk,n are the distribution functions of ξk,n and ηk,n, respectively. The
</p>
<p>quantities L3 and N3 are the third order Lyapunov fractions for the sequences {ξk,n}
and {ηk,n}. The quantities &micro;0k,n are called the third order pseudomoments and L0s
the Lyapunov fractions for pseudomoments. Clearly, N3 &le; c3L3 &rarr; 0, provided that
the Lyapunov condition holds. As we have already noted, for ξk,n = (ξk &minus; ak)/Bn,
where ak = Eξk , B2n =
</p>
<p>&sum;n
1 Var(ξk), and ξk do not depend on n, one has
</p>
<p>L3 =
1
</p>
<p>B3n
</p>
<p>n&sum;
</p>
<p>k=1
&micro;k, &micro;k = E|ξk &minus; ak|3.
</p>
<p>If, moreover, ξk are identically distributed, then
</p>
<p>L3 =
&micro;1
</p>
<p>σ 3
&radic;
n
.
</p>
<p>Our first task here is to estimate the closeness of Ef (ζn) to Ef (ηn) for suffi-
</p>
<p>ciently smooth f . This problem could be of independent interest. Assume that f
</p>
<p>belongs to the class C3 of all bounded functions with uniformly continuous and
</p>
<p>bounded third derivatives: supx |f (3)(x)| &le; f3.
</p>
<p>Theorem 8.5.1 If f &isin; C3 then
</p>
<p>∣∣Ef (ζn)&minus;Ef (ηn)
∣∣&le; f3L
</p>
<p>0
3
</p>
<p>6
&le; f3
</p>
<p>6
(L3 +N3). (8.5.1)
</p>
<p>Proof Put, for 1 &lt; l &le; n,
</p>
<p>Xl := ξ1,n + &middot; &middot; &middot; + ξl&minus;1,n + ηl,n + &middot; &middot; &middot; + ηn,n,
Zl := ξ1,n + &middot; &middot; &middot; + ξl&minus;1,n + ηl+1,n + &middot; &middot; &middot; + ηn,n,
X1 := ηn, Xn+1 = ζn.
</p>
<p>Then
</p>
<p>Xl+1 = Zl + ξl,n, Xl = Zl + ηl,n, (8.5.2)
</p>
<p>4If η&sub;=�0,1 then c3 = E|η|3 = 2&radic;
2π
</p>
<p>&int;&infin;
0
</p>
<p>x3e&minus;x
2/2dx = 4&radic;
</p>
<p>2π
</p>
<p>&int;&infin;
0
</p>
<p>te&minus;tdt = 4&radic;
2π
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Another Approach to Proving Limit Theorems 211
</p>
<p>f (ζn)&minus; f (ηn)=
n&sum;
</p>
<p>l=1
</p>
<p>[
f (Xl+1)&minus; f (Xl)
</p>
<p>]
. (8.5.3)
</p>
<p>Now we will make use of the following lemma.
</p>
<p>Lemma 8.5.1 Let f &isin; C3 and Z, ξ and η be independent random variables with
</p>
<p>Eξ = Eη= a, Eξ2 = Eη2 = σ 2, &micro;0 =
&int;
</p>
<p>|x3|
∣∣d
(
Fξ (x)&minus; Fη(x)
</p>
<p>)∣∣&lt;&infin;.
</p>
<p>Then
</p>
<p>∣∣Ef (Z + ξ)&minus;Ef (Z + η)
∣∣&le; f3&micro;
</p>
<p>0
</p>
<p>6
. (8.5.4)
</p>
<p>Applying this lemma to (8.5.3), we get
</p>
<p>∣∣E
[
f (Xl+1)&minus; f (X1)
</p>
<p>]∣∣&le; f3&micro;
0
</p>
<p>6
</p>
<p>which after summation gives (8.5.1). The theorem is proved. �
</p>
<p>Thus to complete the argument proving Theorem 8.5.1 it remains to prove
</p>
<p>Lemma 8.5.1.
</p>
<p>Proof of Lemma 8.5.1 Set g(x) := Ef (Z + x). It is evident that g, being the result
of the averaging of f , has all the smoothness properties of f and, in particular,
</p>
<p>|g&prime;&prime;&prime;(x)| &le; f3. By virtue of the independence of Z, ξ and η, we have
</p>
<p>Ef (Z + ξ)&minus;Ef (Z + η)=
&int;
</p>
<p>g(x)d
(
Fξ (x)&minus; Fη(x)
</p>
<p>)
. (8.5.5)
</p>
<p>For the integrand, we make use of the expansion
</p>
<p>g(x)= g(0)+ xg&prime;(0)+ x
2
</p>
<p>2
g&prime;&prime;(0)+ x
</p>
<p>3
</p>
<p>6
g&prime;&prime;&prime;(θx), θx &isin; [0, x].
</p>
<p>Since the first and second moments of ξ coincide with those of η, we obtain for the
</p>
<p>right-hand side of (8.5.5) the bound
</p>
<p>∣∣∣∣
1
</p>
<p>6
x3g&prime;&prime;&prime;(θx) d
</p>
<p>(
Fξ (x)&minus; Fη(x)
</p>
<p>)∣∣∣∣&le;
f3&micro;
</p>
<p>0
</p>
<p>6
.
</p>
<p>The lemma is proved. �
</p>
<p>Remark 8.5.1 In exactly the same way one can establish the representation
</p>
<p>∣∣Ef (ζn)&minus;Ef (ηn)
∣∣&le; g
</p>
<p>&prime;&prime;&prime;(0)
</p>
<p>6
</p>
<p>n&sum;
</p>
<p>k=1
E
(
ξ3k,n &minus; η3k,n
</p>
<p>)
+
</p>
<p>f4L
0
4
</p>
<p>24
, (8.5.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>212 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>under obvious conventions for the notations f4 and L
0
4. This bound can improve
</p>
<p>upon (8.5.1) if the differences E(ξ3k,n &minus; η3k,n) are small. If, for instance, ξk,n =
(ξk &minus; a)/(σ
</p>
<p>&radic;
n), ξk are identically distributed, and the third moments of ξk,n and
</p>
<p>ηk,n coincide, then on the right-hand side of (8.5.6) we will have a quantity of the
</p>
<p>order 1/n.
</p>
<p>Theorem 8.5.1 extends Theorem 8.4.1 in the case when s = 3. The extension
is that, to establish convergence ζn &sub;=&rArr;�0,1, one no longer needs the negligibility
of ξk,n. If, for example, ξ1,n &sub;= �0,1/2 (in that case &micro;01,n = 0) and L03 &rarr; 0, then
Ef (ζn)&rarr; Ef (η), η&sub;=�0,1, for any f from the class C3. Since C3 is a distribution
determining class (see Chap. 6), it remains to make use of Corollary 6.3.2.
</p>
<p>We can strengthen the above assertion.
</p>
<p>Theorem 8.5.2 For any x &isin;R,
∣∣P(ζn &lt; x)&minus;Φ(x)
</p>
<p>∣∣&le; c
(
L03
</p>
<p>)1/4
, (8.5.7)
</p>
<p>where c is an absolute constant.
</p>
<p>Proof Take an arbitrary function h &isin; C3, 0 &le; h &le; 1, such that h(x) = 1 for x &le; 0
and h(x)= 0 for x &ge; 1, and put h3 = supx |h&prime;&prime;&prime;(x)|. Then, for the function f (x)=
h((x &minus; t)/ε), we will have f3 = supx |f &prime;&prime;&prime;(x)| &le; h3/ε3, and by Theorem 8.5.1
</p>
<p>P(ζn &lt; t) &le; Ef (ζn)&le; Ef (η)+
f3L
</p>
<p>0
3
</p>
<p>6
</p>
<p>&le; P(η &lt; t + ε)+
h3L
</p>
<p>0
3
</p>
<p>6ε3
&le; P(η &lt; t)+ ε&radic;
</p>
<p>2π
+
</p>
<p>h3L
0
3
</p>
<p>6ε3
.
</p>
<p>The last inequality holds since the maximum of the derivative of the normal distri-
</p>
<p>bution function Φ(t)= P(η &lt; t) is equal to 1/
&radic;
</p>
<p>2π . Establishing in the same way
</p>
<p>the converse inequality and putting ε = (L03)1/4, we arrive at (8.5.7). The theorem
is proved. �
</p>
<p>The bound in Theorem 8.5.2 is, of course, not the best one. And yet inequality
</p>
<p>(8.5.7) shows that we will have a good normal approximation for P(ζn &lt; x) in the
</p>
<p>large deviations range (i.e. for |x| &rarr;&infin;) as well&mdash;at least for those x for which
(
1 &minus;Φ
</p>
<p>(
|x|
</p>
<p>))(
L03
</p>
<p>)&minus;1/4 &rarr;&infin; (8.5.8)
</p>
<p>as n&rarr;&infin;. Indeed, in that case, say, for x = |x|&gt; 0,
∣∣∣∣
P(ζn &ge; x)
1 &minus;Φ(x) &minus; 1
</p>
<p>∣∣∣∣&le;
c(L03)
</p>
<p>1/4
</p>
<p>1 &minus;Φ(x) &rarr; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Another Approach to Proving Limit Theorems 213
</p>
<p>Since by L&rsquo;Hospital&rsquo;s rule
</p>
<p>1 &minus;Φ(x)= 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>x
</p>
<p>e&minus;t
2/2 dt &sim; 1&radic;
</p>
<p>2πx
e&minus;x
</p>
<p>2/2 as x &rarr;&infin;,
</p>
<p>(8.5.8) holds for |x|&lt; c1
&radic;
&minus; lnL03 with an appropriately chosen constant c1.
</p>
<p>In Chap. 20 we will obtain an extension of Theorems 8.5.1 and 8.5.2.
</p>
<p>The problem of refinements and approximation rate bounds in the central limit
</p>
<p>theorem and other limit theorems is one of the most important in probability theory,
</p>
<p>because solving it will tell us how precise and efficient the applications of these
</p>
<p>theorems to practical problems will be. First of all, one has to find the true order of
</p>
<p>the decay of
</p>
<p>∆n = sup
x
</p>
<p>∣∣P(ζn &lt; x)&minus;Φ(x)
∣∣
</p>
<p>in n (or, say, in L3 in the case of non-identically distributed variables). There ex-
</p>
<p>ist at least two approaches to finding sharp bounds for ∆n. The first one, the so-
</p>
<p>called method of characteristic functions, is based on the unimprovable bound for
the closeness of the ch.f.s
</p>
<p>∣∣∣∣lnϕζn(t)+
t2
</p>
<p>2
</p>
<p>∣∣∣∣&lt; cL3
</p>
<p>that the reader can obtain by him/herself, using Lemma 7.4.1 and somewhat modify-
</p>
<p>ing the argument in the proof of Theorem 8.4.1. The principal technical difficulties
</p>
<p>here are in deriving, using the inversion formula, the same order of smallness for ∆n.
</p>
<p>The second approach, the so-called method of compositions, has been illustrated
in the present section in Theorem 8.5.1 (the idea of the method is expressed, to a
</p>
<p>certain extent, by relation (8.5.3)). It will be using just that method that we will
</p>
<p>prove in Appendix 5 the following general result (Cram&eacute;r&ndash;Berry&ndash;Esseen):
</p>
<p>Theorem 8.5.3 If ξk,n = (ξk &minus; ak)/Bn, where ξk do not depend on n, then
</p>
<p>sup
x
</p>
<p>∣∣P(ζn &lt; x)&minus;Φ(x)
∣∣&le; cL3,
</p>
<p>where c is an absolute constant.
</p>
<p>In the case of identically distributed ξk the right-hand side of the above inequality
</p>
<p>becomes c&micro;1/(σ
3
&radic;
n). It was established that in this case (2π)&minus;1/2 &lt; c &lt; 0.4774,
</p>
<p>while in the case of non-identically distributed summands c &lt; 0.5591.5
</p>
<p>One should keep in mind that the above theorems and the bounds for the constant
</p>
<p>c are universal and therefore hold under the most unfavourable conditions (from
</p>
<p>the point of view of the approximation). In real problems, the convergence rate is
</p>
<p>usually much better.
</p>
<p>5See [33].</p>
<p/>
</div>
<div class="page"><p/>
<p>214 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>8.6 The Law of Large Numbers and the Central Limit Theorem
</p>
<p>in the Multivariate Case
</p>
<p>In this section we assume that ξ1,n, . . . , ξn,n are random vectors in the triangular
</p>
<p>array scheme,
</p>
<p>Eξk,n = 0, ζn =
n&sum;
</p>
<p>k=1
ξk,n.
</p>
<p>The law of large numbers ζn
p&rarr; 0 follows immediately from Theorem 8.3.1, if
</p>
<p>we assume that the components of ξk,n satisfy the conditions of that theorem. Thus
</p>
<p>we can assume that Theorem 8.3.1 was formulated and proved for vectors.
</p>
<p>Dealing with the central limit theorem is somewhat more complicated. Here we
</p>
<p>will assume that E|ξk,n|2 &lt;&infin;, where |x|2 = (x, x) is square of the norm of x. Let
</p>
<p>σ 2k,n := E ξTk,nξk,n, σ 2n :=
n&sum;
</p>
<p>k=1
σ 2k,n
</p>
<p>(the superscript T denotes transposition, so that ξTk,n is a column vector).
</p>
<p>Introduce the condition
</p>
<p>n&sum;
</p>
<p>k=1
Emin
</p>
<p>(
|ξk,n|2, |ξk,n|s
</p>
<p>)
&rarr; 0, s &gt; 2, [D2]
</p>
<p>and the Lindeberg condition
</p>
<p>n&sum;
</p>
<p>k=1
E
(
|ξk,n|2; |ξk,n|&gt; τ
</p>
<p>)
&rarr; 0 [M2]
</p>
<p>as n&rarr;&infin; for any τ &gt; 0. As in the univariate case, we can easily verify that condi-
tions [D2] and [M2] are equivalent provided that trσ 2n :=
</p>
<p>&sum;d
j=1(σ
</p>
<p>2
n )jj &lt; c &lt;&infin;.
</p>
<p>Theorem 8.6.1 If σ 2n &rarr; σ 2, where σ 2 is a positive definite matrix, and condition
[D2] (or [M2]) is met, then
</p>
<p>ζn &sub;=&rArr;�0,σ 2 .
</p>
<p>Corollary 8.6.1 (&ldquo;The conventional&rdquo; central limit theorem) If ξ1, ξ2, . . . is a se-
quence of independent identically distributed random vectors, Eξk = 0, σ 2 =
E ξTk ξk and Sn =
</p>
<p>&sum;n
k=1 ξk then, as n&rarr;&infin;,
</p>
<p>Sn&radic;
n
&sub;=&rArr;�0,σ 2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 The Law of Large Numbers 215
</p>
<p>This assertion is a consequence of Theorem 8.6.1, since the random variables
</p>
<p>ξk,n = ξk/
&radic;
n satisfy its conditions.
</p>
<p>Proof of Theorem 8.6.1 Consider the characteristic functions
</p>
<p>ϕk,n(t) := Eei(t,ξk,n), ϕn(t) := Eei(t,ζn) =
n&prod;
</p>
<p>k=1
ϕk,n(t).
</p>
<p>In order to prove the theorem we have to verify that, for any t , as n&rarr;&infin;,
</p>
<p>ϕn(t)&rarr; exp
{
&minus;1
</p>
<p>2
tσ 2tT
</p>
<p>}
.
</p>
<p>We make use of Theorem 8.4.1. We can interpret ϕk,n(t) and ϕn(t) as the ch.f.s
</p>
<p>ϕθk,n(v)= E exp
(
ivξ θk,n
</p>
<p>)
, ϕθn(v)= E exp
</p>
<p>(
ivζ θn
</p>
<p>)
</p>
<p>of the random variables ξ θk,n = (ξk,n, θ), ζ θn = (ζn, θ), where θ = t/|t |, v = |t |. Let
us show that the scalar random variables ξ θk,n satisfy the conditions of Theorem 8.4.1
</p>
<p>(or Corollary 8.4.1) for the univariate case. Clearly,
</p>
<p>E ξ θk,n = 0,
n&sum;
</p>
<p>k=1
E
(
ξ θk,n
</p>
<p>)2 =
n&sum;
</p>
<p>k=1
E(ξk,n, θ)
</p>
<p>2 = θσ 2n θT &rarr; θσ 2θT &gt; 0.
</p>
<p>That condition [D2] is satisfied follows from the obvious inequalities
</p>
<p>(ξk,n, θ)
2 =
</p>
<p>∣∣ξ θk,n
∣∣2 &le; |ξk,n|2,
</p>
<p>n&sum;
</p>
<p>k=1
Eg2
</p>
<p>(
ξ θk,n
</p>
<p>)
&le;
</p>
<p>n&sum;
</p>
<p>k=1
Eg2
</p>
<p>(
|ξk,n|
</p>
<p>)
,
</p>
<p>where g2(x)= min(x2, |x|s), s &gt; 2. Thus, for any v and θ (i.e., for any t), by Corol-
lary 8.4.1 of Theorem 8.4.1
</p>
<p>ϕn(t)= E exp
{
ivζ θn
</p>
<p>}
&rarr; exp
</p>
<p>{
&minus;1
</p>
<p>2
v2θσ 2θT
</p>
<p>}
= exp
</p>
<p>{
&minus;1
</p>
<p>2
tσ 2tT
</p>
<p>}
.
</p>
<p>The theorem is proved. �
</p>
<p>Theorem 8.6.1 does not cover the case where the entries of the matrix σ 2n grow
</p>
<p>unboundedly or behave in such away that the rank of the limiting matrix σ 2 becomes
</p>
<p>less than the dimension of the vectors ξk,n. This can happen when the variances of
</p>
<p>different components of ξk,n have different orders of decay (or growth). In such a
</p>
<p>case, one should consider the transformed sums ζ &prime;n = ζnσ&minus;1n instead of ζn. Theo-
rem 8.6.1 is actually a consequence of the following more general assertion which,
</p>
<p>in turn, follows from Theorem 8.6.1.
</p>
<p>Theorem 8.6.2 If the random variables ξ &prime;k,n = ξk,nσ&minus;1n satisfy condition [D2] (or
[M2]) then ζ &prime;n &sub;=&rArr;�0,E , where E is the identity matrix.</p>
<p/>
</div>
<div class="page"><p/>
<p>216 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>8.7 Integro-Local and Local Limit Theorems for Sums of
</p>
<p>Identically Distributed Random Variables with Finite
</p>
<p>Variance
</p>
<p>Theorem 8.2.1 from Sect. 8.2 is called the integral limit theorem. To understand
the reasons for using such a name, one should compare this assertion with (more
</p>
<p>accurate) limit theorems of another type, that describe the asymptotic behaviour of
</p>
<p>the densities of the distributions of Sn (if any) or the asymptotics of the probabilities
of sums Sn hitting a fixed interval. It is natural to call the theorems for densities local
theorems. Theorems similar to Theorem 8.2.1 can be obtained from the local ones
</p>
<p>(if the densities exist) by integrating, and it is natural to call them integral theorems.
Assertions about the asymptotics of the probabilities of Sn hitting an interval are
</p>
<p>&ldquo;intermediate&rdquo; between the local and integral theorems, and it is natural to call them
</p>
<p>integro-local theorems. In the literature, such statements are often also referred to
as local, apparently because they describe the probability of the localisation of the
sum Sn in a given interval.
</p>
<p>8.7.1 Integro-Local Theorems
</p>
<p>Integro-local theorems describe the asymptotics of
</p>
<p>P
(
Sn &isin; [x, x +∆)
</p>
<p>)
</p>
<p>as n &rarr; &infin; for a fixed ∆ &gt; 0. Probabilities of this type for increasing ∆ (or for
∆ = &infin;) can clearly be obtained by summing the corresponding probabilities for
fixed ∆.
</p>
<p>We will derive integro-local and local theorems with the inversion formulas from
</p>
<p>Sect. 8.7.2.
</p>
<p>For the sake of brevity, put
</p>
<p>∆[x)= [x, x +∆)
</p>
<p>and denote by φ(x)= φ0,1(x) the density of the standard normal distribution. Below
we will restrict ourselves to the investigation of the sums Sn = ξ1 + &middot; &middot; &middot; + ξn of
independent identically distributed random variables ξk
</p>
<p>d= ξ .
</p>
<p>Theorem 8.7.1 (The Stone&ndash;Shepp integro-local theorem) Let ξ be a non-lattice
random variable, E ξ = 0 and E ξ2 = σ 2 &lt; &infin;. Then, for any fixed ∆ &gt; 0, as
n&rarr;&infin;,
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>σ
&radic;
n
φ
</p>
<p>(
x
</p>
<p>σ
&radic;
n
</p>
<p>)
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
, (8.7.1)
</p>
<p>where the remainder term o(1/
&radic;
n) is uniform in x.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Limit Theorems for Sums of Random Variables with Finite Variance 217
</p>
<p>Remark 8.7.1 Since relation (8.7.1) is valid for any fixed ∆, it will also be valid
when ∆ = ∆n &rarr; 0 slowly enough as n&rarr;&infin;. If ∆ = ∆n grows then the asymp-
totics of P(Sn &isin;∆[x)) can be obtained by summing the right-hand sides of (8.7.1)
for, say, ∆ = 1 (if ∆n &rarr;&infin; is integer-valued). Thus the integral theorem follows
from the integro-local one but not vice versa.
</p>
<p>Remark 8.7.2 By virtue of the properties of densities (see Sect. 3.2), the right-hand
side of representation (8.7.1) has the same form as if the random variable ζn =
Sn/(σ
</p>
<p>&radic;
n) had the density φ(v)+ o(1), although the existence of the density of Sn
</p>
<p>(or ζn) is not assumed in the theorem.
</p>
<p>Proof of Theorem 8.7.1 First prove the theorem under the simplifying assumption
that condition
</p>
<p>lim sup
|t |&rarr;&infin;
</p>
<p>∣∣ϕ(t)
∣∣&lt; 1 (8.7.2)
</p>
<p>is satisfied (the Cram&eacute;r condition on the ch.f.). Property 11 of ch.f.s (see Sect. 8.7.1)
</p>
<p>implies that this condition is always met if the distribution of the sum Sm, for some
</p>
<p>m&ge; 1, has a positive absolutely continuous component. The proof of Theorem 8.7.1
in its general form is more complicated and will be given at the end of this section,
</p>
<p>in Sect. 8.7.3.
</p>
<p>In order to use the inversion formula (7.2.8), we employ the &ldquo;smoothing method&rdquo;
</p>
<p>and consider, along with Sn, the sums
</p>
<p>Zn = Sn + ηδ, (8.7.3)
</p>
<p>where ηδ &sub;=U&minus;δ,0. Since the ch.f. ϕηδ (t) of the random variable ηδ , being equal to
</p>
<p>ϕηδ (t)=
1 &minus; e&minus;itδ
</p>
<p>itδ
, (8.7.4)
</p>
<p>possesses the property that the function ϕηδ (t)/t is integrable at infinity, for the
</p>
<p>increments of the distribution function Gn(x) of the random variable Zn (its ch.f.
</p>
<p>divided by t is integrable, too) we can use formula (7.2.8):
</p>
<p>Gn(x +∆)&minus;Gn(x)= P
(
Zn &isin;∆[x)
</p>
<p>)
= 1
</p>
<p>2π
</p>
<p>&int;
e&minus;itx
</p>
<p>1 &minus; e&minus;it∆
it
</p>
<p>ϕn(t)ϕηδ (t) dt
</p>
<p>= ∆
2π
</p>
<p>&int;
e&minus;itxϕn(t)ϕ̂(t) dt, (8.7.5)
</p>
<p>where ϕ̂(t)= ϕηδ (t)ϕη∆(t) (cf. (7.2.8)) is the ch.f. of the sum of independent random
variables ηδ and η∆. We obtain that the difference Gn(x +∆)&minus;Gn(x), up to the
factor ∆, is nothing else but the value of the density of the random variable Sn +
ηδ + η∆ at the point x.
</p>
<p>Split the integral on the right-hand side of (8.7.5) into the two subintegrals: one
</p>
<p>over the domain |t | &lt; γ for some γ &lt; 1, and the other&mdash;over the complementary</p>
<p/>
</div>
<div class="page"><p/>
<p>218 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>domain. Put x = v&radic;n and consider first
</p>
<p>I1 :=
&int;
</p>
<p>|t |&lt;γ
e&minus;itv
</p>
<p>&radic;
nϕn(t)ϕ̂(t) dt = 1&radic;
</p>
<p>n
</p>
<p>&int;
</p>
<p>|u|&lt;γ&radic;n
e&minus;iuvϕn
</p>
<p>(
u&radic;
n
</p>
<p>)
ϕ̂
</p>
<p>(
u&radic;
n
</p>
<p>)
du.
</p>
<p>Without loss of generality we can assume σ = 1, and by (8.2.1) obtain that
</p>
<p>1 &minus; ϕ(t) = t
2
</p>
<p>2
+ o
</p>
<p>(
t2
)
,
</p>
<p>lnϕ(t) = ln
[
1 &minus;
</p>
<p>(
1 &minus; ϕ(t)
</p>
<p>)]
=&minus; t
</p>
<p>2
</p>
<p>2
+ o
</p>
<p>(
t2
)
</p>
<p>as t &rarr; 0. (8.7.6)
</p>
<p>Hence
</p>
<p>n lnϕ
</p>
<p>(
u&radic;
n
</p>
<p>)
=&minus;u
</p>
<p>2
</p>
<p>2
+ hn(u), (8.7.7)
</p>
<p>where hn(u)&rarr; 0 for any fixed u as n&rarr;&infin;. Moreover, for γ small enough, in the
domain |u|&lt; γ&radic;n we have
</p>
<p>∣∣hn(u)
∣∣&le; u
</p>
<p>2
</p>
<p>6
,
</p>
<p>so the right-hand side of (8.7.7) does not exceed &minus;u2/3. Now we can rewrite I1 in
the form
</p>
<p>I1 =
1&radic;
n
</p>
<p>&int;
</p>
<p>|u|&lt;γ&radic;n
exp
</p>
<p>{
&minus;iuv &minus; u
</p>
<p>2
</p>
<p>2
+ hn(u)
</p>
<p>}
ϕ̂
</p>
<p>(
u&radic;
n
</p>
<p>)
du, (8.7.8)
</p>
<p>where |ϕ̂(u/&radic;n)| &le; 1 and ϕ̂(u/&radic;n )&rarr; 1 for any fixed u as n&rarr;&infin;. Therefore, by
virtue of the dominated convergence theorem,
</p>
<p>&radic;
nI1 &rarr;
</p>
<p>&int;
exp
</p>
<p>{
&minus;iuv&minus; u
</p>
<p>2
</p>
<p>2
</p>
<p>}
du (8.7.9)
</p>
<p>uniformly in v, since the integral on the right-hand side of (8.7.8) is uniformly con-
</p>
<p>tinuous in v. But the integral on the right-hand side of (8.7.9) is simply (up to the
</p>
<p>factor 1/(2π)) the result of applying the inversion formula to the ch.f. of the normal
</p>
<p>distribution, so that
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&radic;
nI1 =
</p>
<p>&radic;
2π e&minus;v
</p>
<p>2/2. (8.7.10)
</p>
<p>It remains to consider the integral
</p>
<p>I2 :=
&int;
</p>
<p>|t |&ge;γ
e&minus;itv
</p>
<p>&radic;
nϕn(t)ϕ̂(t) dt.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Limit Theorems for Sums of Random Variables with Finite Variance 219
</p>
<p>By virtue of (8.7.2) and non-latticeness of the distribution of ξ ,
</p>
<p>q := sup
|t |&ge;γ
</p>
<p>∣∣ϕ(t)
∣∣&lt; 1 (8.7.11)
</p>
<p>and therefore
</p>
<p>|I2| &le; qn
&int;
</p>
<p>|t |&ge;γ
</p>
<p>∣∣ϕ̂(t)
∣∣dt &le; qnc(∆, δ), lim
</p>
<p>n&rarr;&infin;
&radic;
nI2 = 0 (8.7.12)
</p>
<p>uniformly in v, where c(∆, δ) depends on ∆ and δ only. We have established that,
</p>
<p>for x = v&radic;n, as n&rarr;&infin;, the relations
</p>
<p>I1 + I2 =
&radic;
</p>
<p>2π
</p>
<p>n
e&minus;v
</p>
<p>2/2 + o
(
</p>
<p>1&radic;
n
</p>
<p>)
,
</p>
<p>P
(
Zn &isin;∆[x)
</p>
<p>)
= ∆&radic;
</p>
<p>2πn
e&minus;x
</p>
<p>2/(2n) + o
(
</p>
<p>1&radic;
n
</p>
<p>) (8.7.13)
</p>
<p>hold uniformly in v (see (8.7.5)). This means that representation (8.7.13) holds uni-
</p>
<p>formly for all x.
</p>
<p>Further, by (8.7.3),
</p>
<p>{
Zn &isin; [x, x +∆&minus; δ)
</p>
<p>}
&sub;
{
Sn &isin;∆[x)
</p>
<p>}
&sub;
{
Zn &isin; [x &minus; δ, x +∆)
</p>
<p>}
(8.7.14)
</p>
<p>and, so, in particular,
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
&le; ∆+ δ&radic;
</p>
<p>2πn
e&minus;(x&minus;δ)
</p>
<p>2/(2n) + o
(
</p>
<p>1&radic;
n
</p>
<p>)
= ∆+ δ&radic;
</p>
<p>2πn
e&minus;x
</p>
<p>2/(2n) + o
(
</p>
<p>1&radic;
n
</p>
<p>)
.
</p>
<p>By (8.7.14) an analogous converse inequality also holds. Since δ is arbitrary, this
</p>
<p>is possible only if
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= ∆&radic;
</p>
<p>2πn
e&minus;x
</p>
<p>2/(2n) + o
(
</p>
<p>1&radic;
n
</p>
<p>)
. (8.7.15)
</p>
<p>The theorem is proved. �
</p>
<p>8.7.2 Local Theorems
</p>
<p>If the distribution of Sn has a density than we can obtain local theorems on the
</p>
<p>asymptotics of this density.
</p>
<p>Theorem 8.7.2 Let E ξ = 0, E ξ2 = σ 2 &lt; &infin; and suppose there exists an m &ge; 1
such that at least one of the following three conditions is met:
</p>
<p>(a) the distribution of Sm has a bounded density;</p>
<p/>
</div>
<div class="page"><p/>
<p>220 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>(b) the distribution of Sm has a density from L2;
(c) the ch.f. ϕm(t) of the sum Sm is integrable.
</p>
<p>Then, for n&ge;m, the distribution of the sum Sn has density fSn(x) for which the
representation
</p>
<p>fSn(x)=
1&radic;
</p>
<p>2πnσ
exp
</p>
<p>{
&minus; x
</p>
<p>2
</p>
<p>2nσ 2
</p>
<p>}
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
(8.7.16)
</p>
<p>holds uniformly in x as n&rarr;&infin;.
Conditions (a)&ndash;(c) are equivalent to each other (possibly with different values
</p>
<p>of m).
</p>
<p>Proof We first establish the equivalence of (a)&ndash;(c). The fact that a bounded density
belongs to L2 was proved in Sect. 7.2.3. Conversely, if f &isin; L2 then
</p>
<p>∣∣f (2)&lowast;(t)
∣∣ =
</p>
<p>∣∣∣∣
&int;
</p>
<p>f (u)f (t &minus; u)du
∣∣∣∣
</p>
<p>&le;
[&int;
</p>
<p>f 2(u) du&times;
&int;
</p>
<p>f 2(t &minus; u)du
]1/2
</p>
<p>=
&int;
</p>
<p>f 2(u) du &lt;&infin;.
</p>
<p>Hence the relationship fSm &isin; L2 implies the boundedness of fS2m , and thus (a) and
(b) are equivalent.
</p>
<p>If ϕm is integrable then by Theorem 7.2.2 the density fSm exists and is bounded.
</p>
<p>Conversely, if fSm is bounded then fSm &isin; L2, ϕSm &isin; L2 and ϕS2m &isin; L1 (see
Sect. 8.7.2). This proves the equivalence of (a) and (c).
</p>
<p>We will now prove (8.7.16). By the inversion formula (7.2.1),
</p>
<p>fSn(x)=
1
</p>
<p>2π
</p>
<p>&int;
e&minus;itxϕn(t) dt.
</p>
<p>Here the integral on the right-hand side does not &ldquo;qualitatively&rdquo; differ from the
</p>
<p>integral on the right-hand side of (8.7.5), we only have to put ϕ̂(t) &equiv; 1 in the part
I1 of the integral (8.7.5) (the integral over the set |t |&lt; γ ), and, in the part I2 (over
the set |t | &ge; γ ), to replace the integrable function ϕ̂(t) with the integrable function
ϕm(t) and to replace the function ϕn(t) with ϕn&minus;m(t). After these changes the whole
argument in the proof of relation (8.7.13) remains valid, and therefore the same
</p>
<p>relation (up to the factor ∆) will hold for
</p>
<p>fSn(x)=
1&radic;
</p>
<p>2πnσ
exp
</p>
<p>{
&minus; x
</p>
<p>2
</p>
<p>2nσ 2
</p>
<p>}
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
.
</p>
<p>The theorem is proved. �
</p>
<p>Theorem 8.7.2 implies that the density fζn of the random variable ζn = Snσ&radic;n
converges to the density φ of the standard normal law:
</p>
<p>fζn(v)&rarr; φ(v)
</p>
<p>uniformly in v as n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Limit Theorems for Sums of Random Variables with Finite Variance 221
</p>
<p>For instance, the density of the uniform distribution over [&minus;1,1] satisfies the
conditions of this theorem, and hence the density of Sn at the point x = vσ
</p>
<p>&radic;
n
</p>
<p>(σ 2 = 1/3) will behave as 1
σ
&radic;
</p>
<p>2πn
e&minus;v
</p>
<p>2/(2σ 2) (cf. the remark to Example 3.6.1).
</p>
<p>In the arithmetic case, where the random variable ξ is integer-valued and the
greatest common divisor of all possible values of ξ equals 1 (see Sect. 7.1), it is the
</p>
<p>asymptotics of the probabilities P(Sn = x) for integer x that become the subject of
interest for local theorems. In this case we cannot assume without loss of generality
</p>
<p>that Eξ = 0.
</p>
<p>Theorem 8.7.3 (Gnedenko) Let E ξ = a, E ξ2 = σ 2 &lt;&infin; and ξ have an arithmetic
distribution. Then, uniformly over all integers x, as n&rarr;&infin;,
</p>
<p>P(Sn = x)=
1&radic;
</p>
<p>2πnσ
exp
</p>
<p>{
(x &minus; an)2
</p>
<p>2nσ 2
</p>
<p>}
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
. (8.7.17)
</p>
<p>Proof When proving limit theorems for arithmetic ξ , it is more convenient to use
the generating functions (see Sects. 7.1, 7.7)
</p>
<p>p(z)&equiv; pξ (z) := E zξ , |z| = 1,
</p>
<p>so that p(eit )= ϕ(t), where ϕ is the ch.f. of ξ .
In this case the inversion formulas take the following form (see (7.2.10)): for
</p>
<p>integer x,
</p>
<p>P(ξ = x) = 1
2πi
</p>
<p>&int;
</p>
<p>|z|=1
z&minus;x&minus;1p(z) dz,
</p>
<p>P(Sn = x) =
1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>|z|=1
z&minus;x&minus;1pn(z) dz= 1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
e&minus;itxϕn(t) dt.
</p>
<p>As in the proof of Theorem 8.7.1, here we split the integral on the right-hand side
</p>
<p>into two subintegrals: over the domain |t |&lt; γ and over the complementary set. The
treatment of the first subintegral
</p>
<p>I1 :=
&int;
</p>
<p>|t |&lt;γ
e&minus;itxϕn(t) dt =
</p>
<p>&int;
</p>
<p>|t |&lt;γ
e&minus;ity
</p>
<p>[
e&minus;itaϕ(t)
</p>
<p>]n
dt
</p>
<p>for y = x &minus; an differs from the considerations for I1 in Theorem 8.7.1 only in that
it is simpler and yields (see (8.7.10))
</p>
<p>I1 =
&radic;
</p>
<p>2π
</p>
<p>σ
&radic;
n
</p>
<p>exp
</p>
<p>{
&minus; y
</p>
<p>2
</p>
<p>2πσ 2
</p>
<p>}
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
.
</p>
<p>Similarly, the treatment of the second subintegral differs from that of I2 in Theo-
</p>
<p>rem 8.7.1 in that it becomes simpler, since the range of integration here is compact</p>
<p/>
</div>
<div class="page"><p/>
<p>222 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>and on that one has
∣∣ϕ(t)
</p>
<p>∣∣&le; q(γ ) &lt; 1. (8.7.18)
Therefore, as in Theorem 8.7.1,
</p>
<p>I2 = o
(
</p>
<p>1&radic;
n
</p>
<p>)
, P(Sn = x)=
</p>
<p>1&radic;
2πnσ
</p>
<p>exp
</p>
<p>{
&minus; y
</p>
<p>2
</p>
<p>2nσ 2
</p>
<p>}
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
.
</p>
<p>The theorem is proved. �
</p>
<p>Evidently, for the values of y of order
&radic;
n Theorem 8.7.3 is a generalisation of
</p>
<p>the local limit theorem for the Bernoulli scheme (see Corollary 5.2.1).
</p>
<p>8.7.3 The Proof of Theorem 8.7.1 in the General Case
</p>
<p>To prove Theorem 8.7.1 in the general case we will use the same approach as in
</p>
<p>Sect. 7.1. We will again employ the smoothing method, but now, when specifying
</p>
<p>the random variable Zn in (8.7.3), we will take θη instead of ηδ , where θ = const,
η is a random variable with the ch.f. from Example 7.2.1 (see the end of Sect. 7.2)
</p>
<p>equal to
</p>
<p>ϕη(t)=
{
</p>
<p>1 &minus; |t |, |t | &le; 1;
0, |t |&gt; 1,
</p>
<p>so that for Zn = Sn + θη, similarly to (8.7.5), we have
</p>
<p>P
(
Zn &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>2π
</p>
<p>&int;
</p>
<p>|t |&le; 1
θ
</p>
<p>e&minus;itxϕn(t)ϕη∆(t)ϕθη(t) dt, (8.7.19)
</p>
<p>where ϕθη(t) = max(0,1 &minus; θ |t |). As in Sect. 8.7.1, split the integral on the right-
hand side of (8.7.19) into two subintegrals: I1 over the domain |t |&lt; γ and I2 over
the domain γ &le; |t | &le; 1/θ . The asymptotic behaviour of these integrals is investi-
gated in almost the same way as in Sect. 8.7.1, but is somewhat simpler, since the
</p>
<p>domain of integration in I2 is compact, and so, by the non-latticeness of ξ , one has
</p>
<p>on it the upper bound
</p>
<p>q := sup
γ&le;|t |&le;1/θ
</p>
<p>∣∣ϕ(t)
∣∣&lt; 1. (8.7.20)
</p>
<p>Therefore, to bound I2 we no longer need condition (8.7.2).
</p>
<p>Thus we have established, as above, relation (8.7.13).
</p>
<p>To derive from this fact the required relation (8.7.15) we will need the following.
</p>
<p>Lemma 8.7.1 Let f (y) be a bounded uniformly continuous function, η an arbitrary
proper random variable independent of Sn and b(n)&rarr;&infin; as n&rarr;&infin;. If, for any</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Limit Theorems for Sums of Random Variables with Finite Variance 223
</p>
<p>fixed ∆&gt; 0 and θ &gt; 0, as n&rarr;&infin;, we have
</p>
<p>P
(
Sn + θη &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>b(n)
</p>
<p>[
f
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
+ o(1)
</p>
<p>]
, (8.7.21)
</p>
<p>then
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>b(n)
</p>
<p>[
f
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
+ o(1)
</p>
<p>]
. (8.7.22)
</p>
<p>In this assertion we can take Sn to be any sequence of random variables satisfying
</p>
<p>(8.7.21). In this section we will set b(n) to be equal to
&radic;
n, but later (see the proof
</p>
<p>of Theorem A7.2.1 in Appendix 7) we will need some other sequences as well.
</p>
<p>Proof Put θ := δ2∆, where δ &gt; 0 will be chosen later, ∆&plusmn;:=(1&plusmn;2δ)∆, ∆&plusmn;[x) :=
[x, x +∆&plusmn;) and f0 := maxf (y). We first obtain an upper bound for P(Sn &isin;∆[x)).
We have
</p>
<p>P
(
Zn &isin;∆+[x &minus;∆δ)
</p>
<p>)
&ge; P
</p>
<p>(
Zn &isin;∆+[x &minus;∆δ); |η|&lt; 1/δ
</p>
<p>)
.
</p>
<p>On the event |η|&lt; 1/δ one has &minus;δ∆&lt; θη &lt; δ∆, and hence on this event
{
Zn &isin;∆+[x &minus;∆δ)
</p>
<p>}
&sup;
{
Sn &isin;∆[x)
</p>
<p>}
.
</p>
<p>Thus, by independence of η and Sn,
</p>
<p>P
(
Zn &isin;∆+[x &minus;∆δ)
</p>
<p>)
&ge; P
</p>
<p>(
Sn &isin;∆[x); |η|&lt; 1/δ
</p>
<p>)
= P
</p>
<p>(
Sn &isin;∆[x)
</p>
<p>)(
1 &minus; h(δ)
</p>
<p>)
,
</p>
<p>where h(δ) := P(|η| &ge; 1/δ)&rarr; 0 as δ &rarr; 0. By condition (8.7.21) and the uniform
integrability of f we obtain
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
&le; P
</p>
<p>(
Zn &isin;∆+[x &minus;∆δ)
</p>
<p>)(
1 &minus; h(δ)
</p>
<p>)&minus;1
</p>
<p>&le;
[
</p>
<p>∆
</p>
<p>b(n)
f
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
+ 2δ∆f0
</p>
<p>b(n)
+ o
</p>
<p>(
1
</p>
<p>b(n)
</p>
<p>)](
1 &minus; h(δ)
</p>
<p>)&minus;1
.
</p>
<p>(8.7.23)
</p>
<p>If, for a given ε &gt; 0, we choose δ &gt; 0 such that
</p>
<p>(
1 &minus; h(δ)
</p>
<p>)&minus;1 &le; 1 + ε∆
3
</p>
<p>, 2δf0 &le;
ε
</p>
<p>3
,
</p>
<p>then we derive from (8.7.23) that, for all n large enough and ε small enough,
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
&le; ∆
</p>
<p>b(n)
</p>
<p>(
f
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
+ ε
</p>
<p>)
. (8.7.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>224 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>This implies, in particular, that for all x,
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
&le; ∆
</p>
<p>b(n)
(f0 + ε). (8.7.25)
</p>
<p>Now we will obtain a lower bound for P(Sn &isin;∆[x)). For the event
</p>
<p>A :=
{
Zn &isin;∆&minus;[x +∆δ)
</p>
<p>}
</p>
<p>we have
</p>
<p>P(A)= P
(
A; |η|&lt; 1/δ
</p>
<p>)
+ P
</p>
<p>(
A; |η| &ge; 1/δ
</p>
<p>)
. (8.7.26)
</p>
<p>On the event |η|&lt; 1/δ we have
{
Zn &isin;∆&minus;[x +∆δ)
</p>
<p>}
&sub;
{
Sn &isin;∆[x)
</p>
<p>}
,
</p>
<p>and hence
</p>
<p>P
(
A; |η|&lt; 1/δ
</p>
<p>)
&le; P
</p>
<p>(
Sn &isin;∆[x)
</p>
<p>)
. (8.7.27)
</p>
<p>Further, by independence of η and Sn and inequality (8.7.25),
</p>
<p>P
(
A; |η| &ge; 1/δ
</p>
<p>)
= E
</p>
<p>[
P(A | η); |η| &ge; 1/δ
</p>
<p>]
</p>
<p>= E
[
P
(
Sn &isin;∆&minus;[x + θη+∆δ) | η
</p>
<p>)
; |η| &ge; 1/δ
</p>
<p>]
</p>
<p>&le; ∆
b(n)
</p>
<p>(f0 + ε)h(δ).
</p>
<p>Therefore, combining (8.7.26), (8.7.27) and (8.7.21), we get
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
&ge; ∆
</p>
<p>b(n)
f
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
&minus; 2δ∆f0
</p>
<p>b(n)
+ o
</p>
<p>(
1
</p>
<p>b(n)
</p>
<p>)
&minus; ∆
</p>
<p>b(n)
(f0 + ε)h(δ).
</p>
<p>In addition, choosing δ such that
</p>
<p>f0h(δ) &lt;
ε
</p>
<p>3
, 2δf0 &lt;
</p>
<p>ε
</p>
<p>3
,
</p>
<p>we obtain that, for all n large enough and ε small enough,
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
&ge; ∆
</p>
<p>b(n)
</p>
<p>(
f
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
&minus; ε
</p>
<p>)
. (8.7.28)
</p>
<p>Since ε is arbitrarily small, inequalities (8.7.24) and (8.7.28) prove the required
</p>
<p>relation (8.7.22). The lemma is proved. �
</p>
<p>To prove the theorem it remains to apply Lemma 8.7.1 in the case (see (8.7.13))
</p>
<p>where f = φ and b(n)=&radic;n. Theorem 8.7.1 is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Limit Theorems for Sums of Random Variables with Finite Variance 225
</p>
<p>8.7.4 Uniform Versions of Theorems 8.7.1&ndash;8.7.3 for Random
</p>
<p>Variables Depending on a Parameter
</p>
<p>In the next chapter, we will need uniform versions of Theorems 8.7.1&ndash;8.7.3, where
</p>
<p>the summands ξk depend on a parameter λ. Denote such summands by ξ(λ)k , the
</p>
<p>corresponding distributions by F(λ), and put
</p>
<p>S(λ)n :=
n&sum;
</p>
<p>k=1
ξ(λ)k,
</p>
<p>where ξ(λ)k are independent copies of ξ(λ) &sub;= F(λ). If λ is only determined by the
number of summands n then we will be dealing with the triangular array scheme
</p>
<p>considered in Sects. 8.3&ndash;8.6 (the summands there were denoted by ξk,n). In the
</p>
<p>general case we will take the segment [0, λ1] for some λ1 &gt; 0 as the parametric set,
keeping in mind that λ &isin; [0, λ1] may depend on n (in the triangular array scheme
one can put λ= 1/n).
</p>
<p>We will be interested in what conditions must be imposed on a family of dis-
</p>
<p>tributions F(λ) for the assertions of Theorems 8.7.1&ndash;8.7.3 to hold uniformly in
</p>
<p>λ &isin; [0, λ1]. We introduce the following notation:
</p>
<p>a(λ)= Eξ(λ), σ 2(λ)= Var(ξ(λ)), ϕ(λ)(t)= Eeitξ(λ) .
</p>
<p>The next assertion is an analogue of Theorem 8.7.1.
</p>
<p>Theorem 8.7.1A Let the distributions F(λ) satisfy the following properties: 0 &lt;
σ1 &lt; σ(λ) &lt; σ2 &lt;&infin;, where σ1 and σ2 do not depend on λ:
(a) the relation
</p>
<p>ϕ(λ)(t)&minus; 1 &minus; ia(λ)t +
t2m2(λ)
</p>
<p>2
= o
</p>
<p>(
t2
)
, m2(λ) := E ξ2(λ), (8.7.29)
</p>
<p>holds uniformly in λ &isin; [0, λ1] as t &rarr; 0, i.e. there exist a t0 &gt; 0 and a function
ε(t) &rarr; 0 as t &rarr; 0, independent of λ, such that, for all |t | &le; t0, the absolute
value of the left-hand side of (8.7.29) does not exceed ε(t)t2;
</p>
<p>(b) for any fixed 0 &lt; θ1 &lt; θ2 &lt;&infin;,
</p>
<p>q(λ) := sup
θ1&le;|t |&le;θ2
</p>
<p>∣∣ϕ(λ)(t)
∣∣&le; q &lt; 1, (8.7.30)
</p>
<p>where q does not depend on λ.
</p>
<p>Then, for each fixed ∆&gt; 0,
</p>
<p>P
(
S(λ)n &minus; na(λ) &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>σ(λ)
&radic;
n
φ
</p>
<p>(
x
</p>
<p>σ(λ)
&radic;
n
</p>
<p>)
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
, (8.7.31)
</p>
<p>where the remainder term o(1/
&radic;
n) is uniform in x and λ &isin; [0, λ1].</p>
<p/>
</div>
<div class="page"><p/>
<p>226 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Proof Going through the proof of Theorem 8.7.1 in its general form (see Sect. 7.3),
we see that, to ensure the validity of all the proofs of the intermediate assertions in
</p>
<p>their uniform forms, it suffices to have uniformity in the following two places:
</p>
<p>(a) the uniformity in λ of the estimate o(t2) as t &rarr; 0 in relation (8.7.6) for the
expansion of the ch.f. of the random variable ξ = ξ(λ)&minus;a(λ)
</p>
<p>σ (λ)
;
</p>
<p>(b) the uniformity in relation (8.7.20) for the same ch.f.
</p>
<p>We verify the uniformity in (8.7.6). For ϕ(t)= E eitξ , we have by (8.7.29)
</p>
<p>lnϕ(t) = &minus; ita(λ)
σ (λ)
</p>
<p>+ lnϕ(λ)
(
</p>
<p>t
</p>
<p>σ (λ)
</p>
<p>)
</p>
<p>= &minus; t
2(m2(λ)&minus; a2(λ))
</p>
<p>2σ 2(λ)
+ o
</p>
<p>(
t2
)
=&minus; t
</p>
<p>2
</p>
<p>2
+ o
</p>
<p>(
t2
)
,
</p>
<p>where the remainder term is uniform in λ.
</p>
<p>The uniformity in relation (8.7.20) clearly follows from condition b), since σ(λ)
</p>
<p>is uniformly separated from both 0 and &infin;. The theorem is proved. �
</p>
<p>Remark 8.7.3 Conditions (a) and (b) of Theorem 8.7.1A are essential for (8.7.31)
to hold. To see this, consider random variables ξ and η with fixed distributions,
</p>
<p>E ξ = Eη = 0 and Eξ2 = Eη2 = 1. Let λ &isin; [0,1] and the random variable ξ(λ) be
defined by
</p>
<p>ξ(λ) :=
{
ξ with probability 1 &minus; λ,
η&radic;
λ
</p>
<p>with probability λ,
(8.7.32)
</p>
<p>so that E ξ(λ) = 0 and Var(ξ(λ))= 2 &minus; λ (in the case of the triangular array scheme
one can put λ= 1/n). Then, under the obvious notational conventions, for λ= t2,
t &rarr; 0, we have
</p>
<p>ϕ(λ)(t)= (1 &minus; λ)ϕξ (t)+ λϕη
(
</p>
<p>t&radic;
λ
</p>
<p>)
= 1 &minus; 3t
</p>
<p>2
</p>
<p>2
+ o
</p>
<p>(
t2
)
+ t2ϕη(1).
</p>
<p>This implies that (8.7.29) does not hold and hence condition a) is not met for the
</p>
<p>values of λ in the vicinity of zero. At the same time, the uniform versions of relation
</p>
<p>(8.7.31) and the central limit theorem will fail to hold. Indeed, putting λ= 1/n, we
obtain the triangular array scheme, in which the number νn of the summands of the
</p>
<p>form ηi/
&radic;
λ in the sum S(λ)n =
</p>
<p>&sum;n
i=1 ξ(λ)i converges in distribution to ν &sub;=�1 and
</p>
<p>1&radic;
n(2 &minus; λ)
</p>
<p>S(λ)n
d= Sn&minus;νn&radic;
</p>
<p>2n&minus; 1
+ Hνn&radic;
</p>
<p>2 &minus; 1/n
, where Hk =
</p>
<p>k&sum;
</p>
<p>i=1
ηi .
</p>
<p>The first term on the right-hand side weakly converges in distribution to ζ &sub;=�0,1/2,
while the second term converges to Hν/
</p>
<p>&radic;
2. Clearly, the sum of these independent
</p>
<p>summands is, generally speaking, not distributed normally with parameters (0,1).</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Convergence to Other Limiting Laws 227
</p>
<p>To see that condition (b) is also essential, consider an arithmetic random variable
</p>
<p>ξ with E ξ = 0 and Var(ξ) = 1, take η to be a random variable with the uniform
distribution U&minus;1,1, and put
</p>
<p>ξ(λ) :=
{
ξ with probability 1 &minus; λ,
η with probability λ.
</p>
<p>Here the random variable ξ(λ) is non-lattice (its distribution has an absolutely con-
</p>
<p>tinuous component), but
</p>
<p>ϕ(λ)(2π)= (1 &minus; λ)+ λϕη(2π), q(λ) &ge; 1 &minus; 2λ.
</p>
<p>Again putting λ= 1/n, we get the triangular array scheme for which condition (b)
is not met. Relation (8.7.31) does not hold either, since, in the previous notation, the
</p>
<p>sum S(λ)n is integer-valued with probability P(νn = 0)= e&minus;1, so that its distribution
will have atoms at integer points with probabilities comparable, by Theorem 8.7.3,
</p>
<p>with the right-hand side of (8.7.31). This clearly contradicts (8.7.31).
</p>
<p>If we put λ = 1/n2 then the sum S(λ)n will be integer-valued with probability
(1 &minus; 1/n2)n &rarr; 1, and the failure of relation (8.7.31) becomes even more evident.
</p>
<p>Uniform versions of the local Theorems 8.7.2 and 8.7.3 are established in a com-
</p>
<p>pletely analogous way.
</p>
<p>Theorem 8.7.2A Let the distributions F(λ) satisfy the conditions of Theorem 8.7.1A
with θ2 =&infin; and the conditions of Theorem 8.7.2, in which conditions (a)&ndash;(c) are
understood in the uniform sense (i.e., maxx fS(λ)m(x) or the norm of fS(λ)m in L2 or&int;
|ϕm(λ)(t)|dt are bounded uniformly in λ &isin; [0, λ1]).
Then representation (8.7.16) holds for fS(λ)n(x) uniformly in x and λ, provided
</p>
<p>that on its right-hand side we replace σ by σ(λ).
</p>
<p>Proof The conditions of Theorem 8.7.2A are such that they enable one to obtain
the proof of the uniform version without any noticeable changes in the arguments
</p>
<p>proving Theorems 8.7.1A and 8.7.2. �
</p>
<p>The following assertion is established in the same way.
</p>
<p>Theorem 8.7.3A Let the arithmetic distributions F(λ) satisfy the conditions of The-
orem 8.7.1A for θ2 = π . Then representation (8.7.17) holds uniformly in x and λ,
provided that a and σ on its right-hand side are replaced with a(λ) and σ(λ), re-
spectively.
</p>
<p>Remark 8.7.3 applies to Theorems 8.7.2A and 8.7.3A as well.
</p>
<p>8.8 Convergence to Other Limiting Laws
</p>
<p>As we saw in previous sections, the normal law occupies a special place among all
</p>
<p>distributions&mdash;it is the limiting law for normed sums of arbitrary distributed random</p>
<p/>
</div>
<div class="page"><p/>
<p>228 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>variables. There arises the natural question of whether there exist any other limiting
</p>
<p>laws for sums of independent random variables.
</p>
<p>It is clear from the proof of Theorem 8.2.1 for identically distributed random
</p>
<p>variables that the character of the limiting law is determined by the behaviour of the
</p>
<p>ch.f. of the summands in the vicinity of 0. If Eξ = 0 and Eξ2 = σ 2 =&minus;ϕ&prime;&prime;(0) exist,
then
</p>
<p>ϕ
</p>
<p>(
1&radic;
n
</p>
<p>)
= 1 + ϕ
</p>
<p>&prime;&prime;(0)t2
</p>
<p>2n
+ o
</p>
<p>(
1
</p>
<p>n
</p>
<p>)
,
</p>
<p>and this determines the asymptotic behaviour of the ch.f. of Sn/
&radic;
n, equal to
</p>
<p>ϕn(t
&radic;
n), which leads to the normal limiting law. Therefore, if one is looking for
</p>
<p>different limiting laws for the sums Sn = ξ1 + &middot; &middot; &middot; + ξn, it is necessary to renounce
the condition that the variance is finite or, which is the same, that ϕ&prime;&prime;(0) exists. In
this case, however, we will have to impose some conditions on the regular variation
</p>
<p>of the functions F+(x) = P(ξ &ge; x) and/or F&minus;(x) = P(ξ &lt; &minus;x) as x &rarr;&infin;, which
we will call the right and the left tail of the distribution of ξ , respectively. We will
need the following concepts.
</p>
<p>Definition 8.8.1 A positive (Lebesgue) measurable function L(t) is called a slowly
varying function (s.v.f.) as t &rarr;&infin;, if, for any fixed v &gt; 0,
</p>
<p>L(vt)
</p>
<p>L(t)
&rarr; 1 as t &rarr;&infin;. (8.8.1)
</p>
<p>A function V (t) is called a regularly varying function (r.v.f.) (of index &minus;β) as t &rarr;
&infin; if it can be represented as
</p>
<p>V (t)= t&minus;βL(t), (8.8.2)
</p>
<p>where L(t) is an s.v.f. as t &rarr;&infin;.
</p>
<p>One can easily see that, similarly to (8.8.1), the characteristic property of regu-
</p>
<p>larly varying functions is the convergence
</p>
<p>V (vt)
</p>
<p>V (t)
&rarr; v&minus;β as t &rarr;&infin; (8.8.3)
</p>
<p>for any fixed v &gt; 0. Thus an s.v.f. is an r.v.f. of index zero.
</p>
<p>Among typical representatives the class of s.v.f.s are the logarithmic function and
</p>
<p>its powers lnγ t , γ &isin;R, linear combinations thereof, multiple logarithms, functions
with the property that L(t) &rarr; L = const 
= 0 as t &rarr; &infin; etc. As an example of a
bounded oscillating s.v.f. we mention
</p>
<p>L0(t)= 2 + sin(ln ln t), t &gt; 1.
</p>
<p>The main properties of r.v.f.s are given in Appendix 6.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Convergence to Other Limiting Laws 229
</p>
<p>As has already been noted, for Sn/b(n) to converge to a &ldquo;nondegenerate&rdquo; limiting
</p>
<p>law under a suitable normalisation b(n), we will have to impose conditions on the
</p>
<p>regular variation of the distribution tails of ξ . More precisely, we will need a regular
</p>
<p>variation of the &ldquo;two-sided tail&rdquo;
</p>
<p>F0(t)= F&minus;(t)+ F+(t)= P
(
ξ /&isin; [&minus;t, t)
</p>
<p>)
.
</p>
<p>We will assume that the following condition is satisfied for some β &isin; (0,2],
ρ &isin; [&minus;1,1]:
</p>
<p>[Rβ,ρ] The two-sided tail F0(x) = F&minus;(x)+ F+(x) is an r.v.f. as x &rarr;&infin;, i.e. it
can be represented as
</p>
<p>F0(x)= t&minus;βLF0(x), β &isin; (0,2], (8.8.4)
</p>
<p>where LF0(x) is an s.v.f., and the following limit exists
</p>
<p>ρ+ := lim
x&rarr;&infin;
</p>
<p>F+(x)
</p>
<p>F0(x)
&isin; [0,1], ρ := 2ρ+ &minus; 1. (8.8.5)
</p>
<p>If ρ+ &gt; 0, then clearly the right tail F+(x) is an r.v.f. like F0(x), i.e. it can be
represented as
</p>
<p>F+(x)= V (x) := x&minus;βL(x), β &isin; (0,2], L(x)&sim; ρ+LF0(x).
</p>
<p>(Here, and likewise in Appendix 6, we use the symbol V to denote an r.v.f.) If
</p>
<p>ρ+ = 0, then the right tail F+(x)= o(F0(x)) is not assumed to be regularly varying.
Relation (8.8.5) implies that the following limit also exists
</p>
<p>ρ&minus; := lim
x&rarr;&infin;
</p>
<p>F&minus;(x)
</p>
<p>F0(x)
= 1 &minus; ρ+.
</p>
<p>If ρ&minus; &gt; 0, then, similarly to the case of the right tail, the left tail F&minus;(x) can be
represented as
</p>
<p>F&minus;(x)=W(x) := x&minus;βLW (x), β &isin; (0,2], LW (x)&sim; ρ&minus;LF0(x).
</p>
<p>If ρ&minus; = 0, then the left tail F&minus;(x)= o(F0(x)) is not assumed to be regularly varying.
The parameters ρ&plusmn; are related to the parameter ρ in the notation [Rβ,ρ] through
</p>
<p>the equalities
</p>
<p>ρ = ρ+ &minus; ρ&minus; = 2ρ+ &minus; 1 &isin; [&minus;1,1].
Clearly, in the case β &lt; 2 we have Eξ2 =&infin;, so that the representation
</p>
<p>ϕ(t)= 1 &minus; t
2σ 2
</p>
<p>2
+ o
</p>
<p>(
t2
)
</p>
<p>as t &rarr; 0
</p>
<p>no longer holds, and the central limit theorem is not applicable. If Eξ exists and is
</p>
<p>finite then everywhere in what follows it will be assumed without loss of generality</p>
<p/>
</div>
<div class="page"><p/>
<p>230 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>that
</p>
<p>Eξ = 0.
Since F0(x) is non-increasing, there always exists the &ldquo;generalised&rdquo; inverse function
</p>
<p>F
(&minus;1)
0 (u) understood as
</p>
<p>F
(&minus;1)
0 (u) := inf
</p>
<p>{
x : F0(x) &lt; u
</p>
<p>}
.
</p>
<p>If the function F0 is strictly monotone and continuous then b = F (&minus;1)0 (u) is the
unique solution to the equation
</p>
<p>F0(b)= u, u &isin; (0,1).
</p>
<p>Set
</p>
<p>ζn :=
Sn
</p>
<p>b(n)
,
</p>
<p>wherein the case β &gt; 2 we define the normalising factor b(n) by
</p>
<p>b(n) := F (&minus;1)0 (1/n). (8.8.6)
</p>
<p>For β = 2 put
b(n) := Y (&minus;1)(1/n), (8.8.7)
</p>
<p>where
</p>
<p>Y(x) := 2x&minus;2
&int; x
</p>
<p>0
</p>
<p>yF0(y) dy = 2x&minus;2
[&int; x
</p>
<p>0
</p>
<p>yF+(y) dy +
&int; x
</p>
<p>0
</p>
<p>yF&minus;(y) dy
</p>
<p>]
</p>
<p>= x&minus;2E
(
ξ2; &minus;x &le; ξ &lt; x
</p>
<p>)
= x&minus;2LY (x), (8.8.8)
</p>
<p>LY is an s.v.f. (see Theorem A6.2.1(iv) in Appendix 6). It follows from Theo-
</p>
<p>rem A6.2.1(v) in Appendix 6 that, under condition (8.8.4), we have
</p>
<p>b(n)= n1/βLb(n), β &le; 2,
</p>
<p>where Lb is an s.v.f.
</p>
<p>We introduce the functions
</p>
<p>VI (x)=
&int; x
</p>
<p>0
</p>
<p>V (y)dy, V I (x)=
&int; &infin;
</p>
<p>x
</p>
<p>V (y)dy.
</p>
<p>8.8.1 The Integral Theorem
</p>
<p>Theorem 8.8.1 Let condition [Rβ,ρ] be satisfied. Then the following assertions hold
true.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Convergence to Other Limiting Laws 231
</p>
<p>(i) For β &isin; (0,2), β 
= 1 and the normalising factor (8.8.6), as n&rarr;&infin;,
</p>
<p>ζn &rArr; ζ (β,ρ). (8.8.9)
</p>
<p>The distribution Fβ,ρof the random variable ζ (β,ρ) depends on parameters β
and ρ only and has a ch.f. ϕ(β,ρ)(t), given by
</p>
<p>ϕ(β,ρ)(t) := Eeitζ (β,ρ) = exp
{
|t |βB(β,ρ,ϑ)
</p>
<p>}
, (8.8.10)
</p>
<p>where ϑ = sign t ,
</p>
<p>B(β,ρ,ϑ)= Γ (1 &minus; β)
[
iρϑ sin
</p>
<p>βπ
</p>
<p>2
&minus; cos βπ
</p>
<p>2
</p>
<p>]
(8.8.11)
</p>
<p>and, for β &isin; (1,2), we put Γ (1 &minus; β)= Γ (2 &minus; β)/(1 &minus; β).
(ii) When β = 1, for the sequence ζn with the normalising factor (8.8.6) to con-
</p>
<p>verge to a limiting law, the former, generally speaking, needs to be centred.
More precisely, as n&rarr;&infin;, the following convergence takes place:
</p>
<p>ζn &minus;An &rArr; ζ (1,ρ), (8.8.12)
</p>
<p>where
</p>
<p>An =
n
</p>
<p>b(n)
</p>
<p>[
VI
</p>
<p>(
b(n)
</p>
<p>)
&minus;WI
</p>
<p>(
b(n)
</p>
<p>)]
&minus; ρ C, (8.8.13)
</p>
<p>C &asymp; 0.5772 is the Euler constant, and
</p>
<p>ϕ(1,ρ)(t)= Eeitζ (1,ρ) = exp
{
&minus;π |t |
</p>
<p>2
&minus; iρt ln |t |
</p>
<p>}
. (8.8.14)
</p>
<p>If n[VI (b(n))&minus;WI (b(n))] = o(b(n)), then ρ = 0 and we can put An = 0.
If Eξ exists and equals zero then
</p>
<p>An =
n
</p>
<p>b(n)
</p>
<p>[
W I
</p>
<p>(
b(n)
</p>
<p>)
&minus; V I
</p>
<p>(
b(n)
</p>
<p>)]
&minus; ρ C.
</p>
<p>If Eξ = 0 and ρ 
= 0 then ρAn &rarr;&minus;&infin; as n&rarr;&infin;.
(iii) For β = 2 and the normalising factor (8.8.7), as n&rarr;&infin;,
</p>
<p>ζn &rArr; ζ (2,ρ), ϕ(2,ρ)(t) := Eeitζ
(2,ρ) = e&minus;t2/2,
</p>
<p>so that ζ (2,ρ) has the standard normal distribution that is independent of ρ.
</p>
<p>The Proof of Theorem 8.8.1 is based on the same considerations as the proof of
Theorem 8.2.1, i.e. on using the asymptotic behaviour of the ch.f. ϕ(t) in the vicinity
</p>
<p>of zero. But here it will be somewhat more difficult from the technical viewpoint.
</p>
<p>This is why the proof of Theorem 8.8.1 appears in Appendix 7. �</p>
<p/>
</div>
<div class="page"><p/>
<p>232 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Remark 8.8.1 The last assertion of the theorem (for β = 2) shows that the limiting
distribution may be normal even in the case of infinite variance of ξ .
</p>
<p>Besides with the normal distribution, we also note &ldquo;extreme&rdquo; limit distributions,
</p>
<p>corresponding to the ρ =&plusmn;1 where the ch.f. ϕ(β,ρ) (or the respective Laplace trans-
form) takes a very simple form. Let, for example, ρ =&minus;1. Since eiπϑ/2 = ϑi, then,
for β 
= 1,2,
</p>
<p>B(β,&minus;1, ϑ) = &minus;Γ (1 &minus; β)
[
i sin
</p>
<p>βπϑ
</p>
<p>2
+ cos βπϑ
</p>
<p>2
</p>
<p>]
</p>
<p>= &minus;Γ (1 &minus; β)eiβπϑ/2 =&minus;Γ (1 &minus; β)(iϑ)β ,
ϕ(β,&minus;1)(t) = exp
</p>
<p>{
&minus;Γ (1 &minus; β)(it)β
</p>
<p>}
,
</p>
<p>E eλζ
(β,&minus;1) = exp
</p>
<p>{
&minus;Γ (1 &minus; β)λβ
</p>
<p>}
, Reλ&ge; 0.
</p>
<p>Similarly, for β = 1, by (8.8.14) and the equalities &minus;πϑ
2
</p>
<p>= i iπϑ
2
</p>
<p>= i ln iϑ we have
</p>
<p>lnϕ(1,&minus;1)(t) = &minus;πϑt
2
</p>
<p>+ it ln |t | = it ln iϑ + it ln |t | = it ln it,
</p>
<p>E eλζ
(1,&minus;1) = exp{λ lnλ}, Reλ&ge; 0.
</p>
<p>A similar formula is valid for ρ = 1.
</p>
<p>Remark 8.8.2 If β &lt; 2, then by virtue of the properties of s.v.f.s (see Theo-
rem A6.2.1(iv) in Appendix 6), as x &rarr;&infin;,
</p>
<p>&int; x
</p>
<p>0
</p>
<p>yF0(y) dy =
&int; x
</p>
<p>0
</p>
<p>y1&minus;βLF0(y) dy &sim;
1
</p>
<p>2 &minus; β x
2&minus;βLF0(x)=
</p>
<p>1
</p>
<p>2 &minus; β x
2F0(x).
</p>
<p>Therefore, for β &lt; 2, we have Y(x)&sim; 2(2 &minus; β)&minus;1F0(x),
</p>
<p>Y (&minus;1)(1/n)&sim; F (&minus;1)0
(
</p>
<p>2 &minus; β
2n
</p>
<p>)
&sim;
(
</p>
<p>2
</p>
<p>2 &minus; β
</p>
<p>)1/β
F
</p>
<p>(&minus;1)
0 (1/n)
</p>
<p>(cf. (8.8.6)). On the other hand, for β = 2 and σ 2 := Eξ2 &lt;&infin; one has
</p>
<p>Y(x)&sim; x&minus;2σ 2, b(n)= Y (&minus;1)(1/n)&sim;
&radic;
σn.
</p>
<p>Thus normalisation (8.8.7) is &ldquo;transitional&rdquo; from normalisation (8.8.6) (up to the
</p>
<p>constant factor (2/(2 &minus; β))1/β ) to the standard normalisation σ&radic;n in the cen-
tral limit theorem in the case where Eξ2 &lt; &infin;. This also means that normalisa-
tion (8.8.7) is &ldquo;universal&rdquo; and can be used for all β &le; 2 (as it is done in many
textbooks on probability theory). However, as we will see below, in the case β &lt; 2
</p>
<p>normalisation (8.8.6) is easier and simpler to deal with, and therefore we will use
</p>
<p>that scaling.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Convergence to Other Limiting Laws 233
</p>
<p>Recall that Fβ,ρ denotes the distribution of the random variable ζ
(β,ρ). The pa-
</p>
<p>rameter β takes values in the interval (0,2], the parameter ρ = ρ+&minus;ρ&minus; can assume
any value from [&minus;1,1]. The role of the parameters β and ρ will be clarified below.
</p>
<p>Theorem 8.8.1 implies that each of the laws Fβ,ρ , 0 &lt; β &le; 2 and &minus;1 &le; ρ &le; 1 is
limiting for the distributions of suitably normalised sums of independent identically
distributed random variables. It follows from the law of large numbers that the de-
</p>
<p>generate distribution Ia concentrated at the point a is also a limiting one. Denote the
</p>
<p>set of all such distributions by S0. Furthermore, it is not hard to see that if F is a dis-
</p>
<p>tribution from the class S0 then the law that differs from F by scaling and shifting,
</p>
<p>i.e. the distribution F{a,b} defined, for some fixed b &gt; 0 and a, by the relation
</p>
<p>F{a,b}(B) := F
(
B &minus; a
b
</p>
<p>)
, where
</p>
<p>B &minus; a
b
</p>
<p>= {u &isin;R : ub+ a &isin; B},
</p>
<p>is also limiting for the distributions of sums of random variables (Sn &minus; an)/bn as
n&rarr;&infin; for appropriate {an} and {bn}.
</p>
<p>It turns out that the class of distributionsS obtained by the above extension from
S0 exhausts all the limiting laws for sums of identically distributed independent
random variables.
</p>
<p>Another characterisation of the class of limiting laws S is also possible.
</p>
<p>Definition 8.8.2 We call a distribution F stable if, for any a1, a2, b1 &gt; 0, b2 &gt; 0,
there exist a and b &gt; 0 such that
</p>
<p>F{a1,b1} &lowast; F{a2,b2} = F{a,b}.
</p>
<p>This definition means that the convolution of a stable distribution F with itself
</p>
<p>again yields the same distribution F, up to a scaling and shift (or, which is the
</p>
<p>same, for independent random variables ξi &sub;= F we have (ξ1 + ξ2 &minus; a)/b &sub;= F for
appropriate a and b).
</p>
<p>In terms of the ch.f. ϕ, the stability property has the following form: for any
</p>
<p>b1 &gt; 0 and b2 &gt; 0, there exist a and b &gt; 0 such that
</p>
<p>ϕ(tb1)ϕ(tb2)= eitaϕ(tb), t &isin;R. (8.8.15)
</p>
<p>Denote the class of all stable laws by SS . The remarkable fact is that the class of all
limiting laws S (for (Sn &minus; an)/bn for some an and bn) and the class of all stable
laws SS coincide.
</p>
<p>If, under a suitable normalisation, as n&rarr;&infin;,
</p>
<p>ζn &rArr; ζ (β,ρ),
</p>
<p>then one says that the distribution F of the summands ξ belongs to the domain of
attraction of the stable law Fβ,ρ .
</p>
<p>Theorem 8.8.1 means that, if F satisfies condition [Rβ,ρ], then F belongs to the
domain of attraction of the stable law Fβ,ρ .</p>
<p/>
</div>
<div class="page"><p/>
<p>234 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>One can prove the converse assertion (see e.g. Chap. XVII, &sect; 5 in [30]): if F
</p>
<p>belongs to the domain of attraction of a stable law Fβ,ρ for β &lt; 2, then [Rβ,ρ] is
satisfied.
</p>
<p>As for the role of the parameters β and ρ, note the following. The parameter β
</p>
<p>characterises the rate of convergence to zero as x &rarr;&infin; for the functions
</p>
<p>Fβ,ρ,&minus;(x) := Fβ,ρ
(
(&minus;&infin;,&minus;x)
</p>
<p>)
and Fβ,ρ,+(x) := Fβ,ρ
</p>
<p>(
[x,&infin;)
</p>
<p>)
.
</p>
<p>One can prove that, for ρ+ &gt; 0, as t &rarr;&infin;,
</p>
<p>Fβ,ρ,+(t)&sim; ρ+t&minus;β , (8.8.16)
</p>
<p>and, for ρ&minus; &gt; 0, as t &rarr;&infin;,
</p>
<p>Fβ,ρ,&minus;(t)&sim; ρ&minus;t&minus;β . (8.8.17)
</p>
<p>Note that, for ξ &sub;= Fβ,ρ , the asymptotic relations in Theorem 8.8.1 turn into pre-
cise equalities provided that we replace in them b(n) with bn := n1/β . In particular,
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>bn
&ge; t
</p>
<p>)
= Fβ,ρ,+(t). (8.8.18)
</p>
<p>This follows from the fact that [ϕ(β,ρ)(t/bn)]n coincides with ϕ(β,ρ)(t) (see (8.8.10))
and hence the distribution of the normalised sum Sn/bn coincides with the distribu-
</p>
<p>tion of the random variable ξ .
</p>
<p>The parameter ρ taking values in [&minus;1,1] is the measure of asymmetry of the dis-
tribution Fβ,ρ . If, for instance, ρ = 1 (ρ&minus; = 0), then, for β &lt; 1, the distribution Fβ,1
is concentrated entirely on the positive half-line. This is evident from the fact that in
</p>
<p>this case Fβ,1 can be considered as the limiting distribution for the normalised sums
</p>
<p>of independent identically distributed random variables ξk &ge; 0 (with F&minus;(0) = 0).
Since all the prelimit distributions are concentrated on the positive half-line, so is
</p>
<p>the limiting distribution.
</p>
<p>Similarly, for ρ =&minus;1 and β &lt; 1, the distribution Fβ,&minus;1 is entirely concentrated
on the negative half-line. For ρ = 0 (ρ+ = ρ&minus; = 1/2) the ch.f. of the distribution
Fβ,0 will be real, and the distribution Fβ,0 itself is symmetric.
</p>
<p>As we saw above, the ch.f.s ϕ(β,ρ)(t) of stable laws Fβ,ρ admit closed-form rep-
</p>
<p>resentations. They are clearly integrable over R, and the same is true for the func-
</p>
<p>tions tkϕ(β,ρ)(t) for any k &ge; 1. Therefore all the stable distributions have densities
that are differentiable arbitrarily many times (see e.g. the inversion formula (7.2.1)).
</p>
<p>As for explicit forms of these densities, they are only known for a few laws. Among
</p>
<p>them are:</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Convergence to Other Limiting Laws 235
</p>
<p>1. The normal law F2,ρ (which does not depend on ρ).
</p>
<p>2. The Cauchy distribution F1,0 with density 2/(π
2 +4x2), &minus;&infin;&lt; x &lt;&infin;. Scal-
</p>
<p>ing the x-axis with a factor of π/2 transforms this density into the form 1/π(1+x2)
corresponding to K0,1.
</p>
<p>3. The L&eacute;vy distribution. This law can be obtained from the explicit form for
</p>
<p>the distribution of the maximum of the Wiener process. This will be the distribution
</p>
<p>F1/2,1 with parameters 1/2,1 and density (up to scaling; cf. (8.8.16))
</p>
<p>f (1/2,1)(x)= 1&radic;
2πx3/2
</p>
<p>e&minus;1/(2x), x &gt; 0
</p>
<p>(this density has a first hitting time of level 1 by the standard Wiener process, see
</p>
<p>Theorem 19.2.2).
</p>
<p>8.8.2 The Integro-Local and Local Theorems
</p>
<p>Under the conditions of this section we can also obtain integro-local and local the-
</p>
<p>orems in the same way as in Sect. 8.7 in the case of convergence to the normal law.
</p>
<p>As in Sect. 8.7, integro-local theorems deal here with the asymptotics of
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
, ∆[x)= [x, x +∆)
</p>
<p>as n&rarr;&infin; for a fixed ∆&gt; 0.
As we can see from Theorem 8.8.1, the ch.f. ϕ(β,ρ)(t) of the stable law Fβ,ρ is
</p>
<p>integrable, and hence, by the inversion formula, there exists a uniformly continuous
</p>
<p>density f (β,ρ)of the distribution Fβ,ρ . (As has already been noted, it is not difficult
</p>
<p>to show that f (β,ρ) is differentiable arbitrarily many times, see Sect. 7.2.)
</p>
<p>Theorem 8.8.2 (The Stone integro-local theorem) Let ξ be a non-lattice random
variable and the conditions of Theorem 8.8.1 be met. Then, for any fixed ∆&gt; 0, as
n&rarr;&infin;,
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>b(n)
f (β,ρ)
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
+ o
</p>
<p>(
1
</p>
<p>b(n)
</p>
<p>)
, (8.8.19)
</p>
<p>where the remainder term o( 1
b(n)
</p>
<p>) is uniform over x.
If β = 1 and E|ξ | does not exist then, on the right-hand side of (8.8.20), we must
</p>
<p>replace f (β,ρ)( x
b(n)
</p>
<p>) with f (β,ρ)( x
b(n)
</p>
<p>&minus;An), where An is defined in (8.8.13).
</p>
<p>All the remarks to the integro-local Theorem 8.7.1 hold true here as well, with
</p>
<p>evident changes.
</p>
<p>If the distribution of Sn has a density then we can find the asymptotics of that
</p>
<p>density.</p>
<p/>
</div>
<div class="page"><p/>
<p>236 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>Theorem 8.8.3 Let there exist an m&ge; 1 such that at least one of conditions (a)&ndash;(c)
of Theorem 8.7.2 is satisfied. Moreover, let the conditions of Theorem 8.8.1 be met.
Then for the density fSn(x) of the distribution of Sn one has the representation
</p>
<p>fSn(x)=
1
</p>
<p>b(n)
f (β,ρ)
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
+ o
</p>
<p>(
1
</p>
<p>b(n)
</p>
<p>)
(8.8.20)
</p>
<p>which holds uniformly in x as n&rarr;&infin;.
If β = 1 and E|ξ | does not exist then, on the right-hand side of (8.8.20), we must
</p>
<p>replace f (β,ρ)( x
b(n)
</p>
<p>) with f (β,ρ)( x
b(n)
</p>
<p>&minus;An), where An is defined in (8.8.13).
</p>
<p>The assertion of Theorem 8.8.3 can be rewritten for ζn = Snb(n) &minus;An as
</p>
<p>fζn(v)&rarr; f (β,ρ)(v)
</p>
<p>for any v as n&rarr;&infin;.
For integer-valued ξk the following theorem holds true.
</p>
<p>Theorem 8.8.4 Let the distribution of ξ be arithmetic and the conditions of Theo-
rem 8.8.1 be met. Then, uniformly for all integers x, as n&rarr;&infin;,
</p>
<p>P(Sn = x)=
1
</p>
<p>b(n)
f (β,ρ)
</p>
<p>(
x &minus; an
b(n)
</p>
<p>)
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
, (8.8.21)
</p>
<p>where a = E ξ if E |ξ | exists and a = 0 if E |ξ | does not exist, β 
= 1. If β = 1
and E|ξ | does not exist then, on the right-hand side of (8.8.21), we must replace
f (β,ρ)( x&minus;an
</p>
<p>b(n)
) with f (β,ρ)( x
</p>
<p>b(n)
&minus;An).
</p>
<p>The proofs of Theorems 8.8.2&ndash;8.8.4 mostly repeat those of Theorems 8.7.1&ndash;8.7.3
</p>
<p>and can be found in Appendix 7.
</p>
<p>8.8.3 An Example
</p>
<p>In conclusion we will consider an example.
</p>
<p>In Sect. 12.8 we will see that in the fair game considered in Example 4.2.3 the
</p>
<p>ruin time η(z) of a gambler with an initial capital of z units satisfies the relation
</p>
<p>P(η(z)&ge; n)&sim; z
&radic;
</p>
<p>2/πn as n&rarr;&infin;. In particular, for z= 1,
</p>
<p>P
(
η(1)&ge; n
</p>
<p>)
&sim;
&radic;
</p>
<p>2/πn. (8.8.22)
</p>
<p>It is not hard to see (for more detail, see also Chap. 12) that η(z) has the same
</p>
<p>distribution as η1 +η2 +&middot; &middot; &middot;+ηz, where ηj are independent and distributed as η(1).</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Convergence to Other Limiting Laws 237
</p>
<p>Thus for studying the distribution of η(z) when z is large, by virtue of (8.8.22), one
</p>
<p>can make use of Theorem 8.8.4 (with β = 1/2, b(n)= 2n2/π ), by which
</p>
<p>lim
z&rarr;&infin;
</p>
<p>P
</p>
<p>(
2πη(x)
</p>
<p>z2
&lt; x
</p>
<p>)
= F1/2,1(x) (8.8.23)
</p>
<p>is the L&eacute;vy stable law with parameters β = 1/2 and ρ = 1. Moreover, for integer x
and z&rarr;&infin;,
</p>
<p>P
(
η(z)= x
</p>
<p>)
= π
</p>
<p>2z2
f (1/2,1)
</p>
<p>(
xπ
</p>
<p>2z2
</p>
<p>)
+ o
</p>
<p>(
1
</p>
<p>z2
</p>
<p>)
.
</p>
<p>These assertions enable one to obtain the limiting distribution for the number of
</p>
<p>crossings of an arbitrary strip [u,v] by the trajectory S1, . . . , Sn in the case where
</p>
<p>P(ξk =&minus;1)= P(ξk =&minus;1)= 1/2.
</p>
<p>Indeed, let for simplicity u= 0. By the first positive crossing of the strip [0, v] we
will mean the Markov time
</p>
<p>η+ := min{k : Sk = v}.
</p>
<p>The first negative crossing of the strip is then defined as the time η+ + η&minus;, where
</p>
<p>η&minus; := min{k : Sη++k = 0}.
</p>
<p>The time η1 = η+ + η&minus; will also be the time of the &ldquo;double crossing&rdquo; of [0, v]. The
variables η&plusmn; are distributed as η(v) and are independent, so that η1 has the same
distribution as η(2v). The variable Hk = η1(2v)+ &middot; &middot; &middot; + ηk(2v), where ηi(2v) have
the same distribution as η(2v) and are independent, is the time of the k-th double
</p>
<p>crossing. Therefore
</p>
<p>ν(n) := max{k :Hk &le; n} = min{k :Hk &gt; n} &minus; 1
</p>
<p>is the number of double crossings of the strip [0, v] by time n. Now we can prove
the following assertion:
</p>
<p>lim
n&rarr;&infin;
</p>
<p>P
</p>
<p>(
ν(n)&radic;
</p>
<p>n
&ge; x
</p>
<p>)
= F1/2,1
</p>
<p>(
π
</p>
<p>2v2x2
</p>
<p>)
. (8.8.24)
</p>
<p>To prove it, we will make use of the following relation (which will play, in its
</p>
<p>more general form, an important role in Chap. 10):
</p>
<p>{
ν(n)&ge; k
</p>
<p>}
= {Hk &le; n},
</p>
<p>where Hk is distributed as η(2vk). If n/k
2 &rarr; s2 as n &rarr; &infin;, then by virtue of
</p>
<p>(8.8.23)
</p>
<p>P(Hk &le; n)= P
(
</p>
<p>2πHk
</p>
<p>(2vk)2
&le; 2πn
</p>
<p>(2vk)2
</p>
<p>)
&rarr; F1/2,1
</p>
<p>(
πs2
</p>
<p>2v2
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>238 8 Sequences of Independent Random Variables. Limit Theorems
</p>
<p>and therefore
</p>
<p>P
</p>
<p>(
ν(n)&radic;
</p>
<p>n
&ge; x
</p>
<p>)
= P
</p>
<p>(
ν(n)&ge; x
</p>
<p>&radic;
n
)
= P(H&lfloor;x&radic;n&rfloor; &le; n)&rarr; F1/2,1
</p>
<p>(
π
</p>
<p>2v2x2
</p>
<p>)
.
</p>
<p>(Here for k = &lfloor;x&radic;n&rfloor; one has n/k2 &rarr; s2 = 1/x2.) Relation (8.8.24) is proved. �
</p>
<p>Assertion (8.8.24) will clearly remain true for the number of crossings of the
</p>
<p>strip [u,v], u 
= 0; one just has to replace v with v &minus; u on the right-hand side of
(8.8.24). It is also clear that (8.8.24) enables one to find the limiting distribution of
</p>
<p>the number of &ldquo;simple&rdquo; (not double) crossings of [u,v] since the latter is equal to
2ν(n) or 2ν(n)+1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
</p>
<p>Large Deviation Probabilities for Sums
of Independent Random Variables
</p>
<p>Abstract The material presented in this chapter is unique to the present text. After
</p>
<p>an introductory discussion of the concept and importance of large deviation prob-
</p>
<p>abilities, Cram&eacute;r&rsquo;s condition is introduced and the main properties of the Cram&eacute;r
</p>
<p>and Laplace transforms are discussed in Sect. 9.1. A separate subsection is devoted
</p>
<p>to an in-depth analysis of the key properties of the large deviation rate function,
</p>
<p>followed by Sect. 9.2 establishing the fundamental relationship between large devi-
</p>
<p>ation probabilities for sums of random variables and those for sums of their Cram&eacute;r
</p>
<p>transforms, and discussing the probabilistic meaning of the rate function. Then the
</p>
<p>logarithmic Large Deviations Principle is established. Section 9.3 presents integro-
</p>
<p>local, integral and local theorems on the exact asymptotic behaviour of the large
</p>
<p>deviation probabilities in the so-called Cram&eacute;r range of deviations. Section 9.4 is de-
</p>
<p>voted to analysing various types of the asymptotic behaviours of the large deviation
</p>
<p>probabilities for deviations at the boundary of the Cram&eacute;r range that emerge under
</p>
<p>different assumptions on the distributions of the random summands. In Sect. 9.5,
</p>
<p>the behaviour of the large deviation probabilities is found in the case of heavy-tailed
</p>
<p>distributions, namely, when the distributions tails are regularly varying at infinity.
</p>
<p>These results are used in Sect. 9.6 to find the asymptotics of the large deviation
</p>
<p>probabilities beyond the Cram&eacute;r range of deviations, under special assumptions on
</p>
<p>the distribution tails of the summands.
</p>
<p>Let ξ, ξ1, ξ2, . . . be a sequence of independent identically distributed random vari-
</p>
<p>ables,
</p>
<p>Eξk = 0, Eξ2k = σ 2 &lt;&infin;, Sn =
n&sum;
</p>
<p>k=1
ξk.
</p>
<p>Suppose that we have to evaluate the probability P(Sn &ge; x). If x &sim; v
&radic;
n as n&rarr;&infin;,
</p>
<p>v = const, then by the integral limit theorem
</p>
<p>P(Sn &ge; x)&sim; 1 &minus;Φ
(
v
</p>
<p>σ
</p>
<p>)
(9.0.1)
</p>
<p>as n &rarr; &infin;. But if x ≫ &radic;n, then the integral limit theorem enables one only to
conclude that P(Sn &ge; x) &rarr; 0 as n &rarr; &infin;, which in fact contains no quantitative
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_9, &copy; Springer-Verlag London 2013
</p>
<p>239</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_9">http://dx.doi.org/10.1007/978-1-4471-5201-9_9</a></div>
</div>
<div class="page"><p/>
<p>240 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>information on the probability we are after. Essentially the same can happen for
</p>
<p>fixed but &ldquo;relatively&rdquo; large values of v/σ . For example, for v/σ &ge; 3 and the values
of n around 100, the relative accuracy of the approximation in (9.0.1) becomes, gen-
</p>
<p>erally speaking, bad (the true value of the left-hand side can be several times greater
</p>
<p>or smaller than that of the right-hand side). Studying the asymptotic behaviour of
</p>
<p>P(Sn &ge; x) for x ≫
&radic;
n as n &rarr; &infin;, which is not known to us yet, could fill these
</p>
<p>gaps. This problem is highly relevant since questions of just this kind arise in many
</p>
<p>problems of mathematical statistics, insurance theory, the theory of queueing sys-
</p>
<p>tems, etc. For instance, in mathematical statistics, finding small probabilities of er-
rors of the first and second kind of statistical tests when the sample size n is large
</p>
<p>leads to such problems (e.g. see [7]). In these problems, we have to find explicit
</p>
<p>functions P(n,x) such that
</p>
<p>P(Sn &ge; x)= P(n,x)
(
1 + o(1)
</p>
<p>)
(9.0.2)
</p>
<p>as n &rarr; &infin;. Thus, unlike the case of normal approximation (9.0.1), here we are
looking for approximations P(n,x) with a relatively small error rather than an ab-
solutely small error. If P(n,x)&rarr; 0 in (9.0.2) as n&rarr;&infin;, then we will speak of the
probabilities of rare events, or of the probabilities of large deviations of sums Sn.
Deviations of the order
</p>
<p>&radic;
n are called normal deviations.
</p>
<p>In order to study large deviation probabilities, we will need some notions and
</p>
<p>assertions.
</p>
<p>9.1 Laplace&rsquo;s and Cram&eacute;r&rsquo;s Transforms. The Rate Function
</p>
<p>9.1.1 The Cram&eacute;r Condition. Laplace&rsquo;s and Cram&eacute;r&rsquo;s Transforms
</p>
<p>In all the sections of this chapter, except for Sect. 9.5, the following Cram&eacute;r condi-
tion will play an important role.
</p>
<p>[C] There exists a λ 
= 0 such that
</p>
<p>Eeλξ =
&int;
</p>
<p>eλyF(dy) &lt;&infin;. (9.1.1)
</p>
<p>We will say that the right-side (left-side) Cram&eacute;r condition holds if λ &gt; 0 (λ &lt; 0)
in (9.1.1). If (9.1.1) is valid for some negative and positive λ (i.e. in a neighbour-
</p>
<p>hood of the point λ= 0), then we will say that the two-sided Cram&eacute;r&rsquo;s condition is
satisfied.
</p>
<p>The Cram&eacute;r condition can be interpreted as characterising a fast (at least expo-
</p>
<p>nentially fast) rate of decay of the tails F&plusmn;(t) of the distribution F. If, for instance,
we have (9.1.1) for λ &gt; 0, then by Chebyshev&rsquo;s inequality, for t &gt; 0,
</p>
<p>F+(t) := P(ξ &ge; t)&le; e&minus;λtEeλξ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Laplace&rsquo;s and Cram&eacute;r&rsquo;s Transforms. The Rate Function 241
</p>
<p>i.e. F+(t) decreases at least exponentially fast. Conversely, if, for some &micro;&gt; 0, one
has F+(t)&le; ce&minus;&micro;t , t &gt; 0, then, for λ &isin; (0,&micro;),
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>eλyF(dy) = &minus;
&int; &infin;
</p>
<p>0
</p>
<p>eλy dF+(y)= F+(0)+ λ
&int; &infin;
</p>
<p>0
</p>
<p>eλyF+(y) dy
</p>
<p>&le; F+(0)+ cλ
&int; &infin;
</p>
<p>0
</p>
<p>e(λ&minus;&micro;)ydy = F+(0)+
cλ
</p>
<p>&micro;&minus; λ &lt;&infin;.
</p>
<p>Since the integral
&int; 0
&minus;&infin; e
</p>
<p>λyF(dy) is finite for any λ &gt; 0, we have Eeλξ &lt; &infin; for
λ &isin; (0,&micro;).
</p>
<p>The situation is similar for the left tail F&minus;(t) := P(ξ &lt;&minus;t) provided that (9.1.1)
holds for some λ &lt; 0.
</p>
<p>Set
</p>
<p>λ+ := sup
{
λ : Eeλξ &lt;&infin;
</p>
<p>}
, λ&minus; := inf
</p>
<p>{
λ : Eeλξ &lt;&infin;
</p>
<p>}
.
</p>
<p>Condition [C] is equivalent to λ+ &gt; λ&minus;. The right-side Cram&eacute;r condition means
that λ+ &gt; 0; the two-sided condition means that λ+ &gt; 0 &gt; λ&minus;. Clearly, the ch.f.
ϕ(t)= Eeitξ is analytic in the complex plane in the strip &minus;λ+ &lt; Im t &lt;&minus;λ&minus;. This
follows from the differentiability of ϕ(t) in this region of the complex plane, since
</p>
<p>the integral
&int;
|yeity |F(dy) for the said values of Im t converges uniformly in Re t .
</p>
<p>Here and henceforth by the Laplace transform (Laplace&ndash;Stieltjes or Laplace&ndash;
Lebesgue) of the distribution F of the random variable ξ we shall mean the function
</p>
<p>ψ(λ) := Eeλξ = ϕ(&minus;iλ),
</p>
<p>which conflicts with Sect. 7.1.1 (and the terminology of mathematical analysis),
</p>
<p>according to which the term Laplace&rsquo;s transform refers to the function Ee&minus;λξ =
ϕ(iλ). The reason for such a slight inconsistency in terminology (only the sign of
</p>
<p>the argument differs, this changes almost nothing) is our reluctance to introduce new
</p>
<p>notation or to complicate the old notation. Nowhere below will it cause confusion.1
</p>
<p>As well as condition [C], we will also assume that the random variable ξ is
</p>
<p>nondegenerate, i.e. ξ 
&equiv; const or, which is the same, Var ξ &gt; 0.
</p>
<p>The main properties of Laplace&rsquo;s transform.
</p>
<p>As was already noted in Sect. 7.1.1, Laplace&rsquo;s transform, like the ch.f., uniquely
</p>
<p>characterises the distribution F. Moreover, it has the following properties, which
</p>
<p>are similar to the corresponding properties of ch.f.s (see Sect. 7.1). Under obvious
</p>
<p>conventions of notation,
</p>
<p>(Ψ 1) ψa+bξ (λ)= eλaψξ (bλ), if a and b are constant.
</p>
<p>1In the literature, the function Eeλξ is sometimes called the &ldquo;moment generating function&rdquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>242 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>(Ψ 2) If ξ1, . . . , ξn are independent and Sn =
&sum;n
</p>
<p>j=1 ξj , then
</p>
<p>ψSn(λ)=
n&prod;
</p>
<p>j=1
ψξj (λ).
</p>
<p>(Ψ 3) If E|ξ |k &lt;&infin; and the right-side Cram&eacute;r condition is satisfied then the func-
tion ψξ is k-times right differentiable at the point λ= 0,
</p>
<p>ψ
(k)
ξ (0)= Eξ k =:mk
</p>
<p>and, as λ &darr; 0,
</p>
<p>ψξ (λ)= 1 +
k&sum;
</p>
<p>j=1
</p>
<p>λj
</p>
<p>j !mj + o
(
λk
)
.
</p>
<p>This also implies that, as λ &darr; 0, the representation
</p>
<p>lnψξ (λ)=
k&sum;
</p>
<p>j=1
</p>
<p>γjλ
j
</p>
<p>j ! + o
(
λk
)
, (9.1.2)
</p>
<p>holds, where γj are the so-called semi-invariants (or cumulants) of order j of the
random variable ξ . One can easily verify that
</p>
<p>γ1 =m1, γ2 =m02 = σ 2, γ3 =m03, . . . , (9.1.3)
</p>
<p>where m0k = E(ξ &minus;m1)k is the central moment of order k.
</p>
<p>Definition 9.1.1 Let condition [C] be met. The Cram&eacute;r transform at the point λ of
the distribution F is the distribution2
</p>
<p>F(λ)(dy)=
eλyF(dy)
</p>
<p>ψ(λ)
. (9.1.4)
</p>
<p>2In some publications the transform (9.1.4) is also called the Esscher transform. However, the
systematic use of transform (9.1.4) for the study of large deviations was first done by Cram&eacute;r.
</p>
<p>If we study the probabilities of large deviations of sums of random variables using the inver-
</p>
<p>sion formula, similarly to what was done for normal deviations in Chap. 8, then we will necessarily
</p>
<p>come to employ the so-called saddle-point method, which consists of moving the contour of inte-
gration so that it passes through the so-called saddle point, at which the exponent in the integrand
function, as we move along the imaginary axis, attains its minimum (and, along the real axis, at-
</p>
<p>tains its maximum; this explains the name &ldquo;saddle point&rdquo;). Cram&eacute;r&rsquo;s transform does essentially
</p>
<p>the same, making such a translation of the contour of integration even before applying the inver-
</p>
<p>sion formula, and reduces the large deviation problem to the normal deviation problem, where the
</p>
<p>inversion formula is not needed if we use the results of Chap. 8. It is this technique that we will
</p>
<p>follow in the present chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Laplace&rsquo;s and Cram&eacute;r&rsquo;s Transforms. The Rate Function 243
</p>
<p>Clearly, the distributions F and F(λ) are mutually absolutely continuous (see
</p>
<p>Sect. 3.5 of Appendix 3) with density
</p>
<p>F(λ)(dy)
</p>
<p>F(dy)
= e
</p>
<p>λy
</p>
<p>ψ(λ)
.
</p>
<p>Denote a random variable with distribution F(λ) by ξ(λ).
</p>
<p>The Laplace transform of the distribution F(λ) is obviously equal to
</p>
<p>Ee&micro;ξ(λ) = ψ(λ+&micro;)
ψ(λ)
</p>
<p>. (9.1.5)
</p>
<p>Clearly,
</p>
<p>Eξ(λ) =
ψ &prime;(λ)
</p>
<p>ψ(λ)
=
(
lnψ(λ)
</p>
<p>)&prime;
, Eξ2(λ) =
</p>
<p>ψ &prime;&prime;(λ)
</p>
<p>ψ(λ)
,
</p>
<p>Var(ξ(λ)) =
ψ &prime;&prime;(λ)
</p>
<p>ψ(λ)
&minus;
(
ψ &prime;(λ)
</p>
<p>ψ(λ
</p>
<p>)2
=
(
lnψ(λ)
</p>
<p>)&prime;&prime;
.
</p>
<p>Since ψ &prime;&prime;(λ) &gt; 0 and Var(ξ(λ)) &gt; 0, the foregoing implies one more important prop-
erty of the Laplace transform.
</p>
<p>(Ψ 4) The functions ψ(λ) and lnψ(λ) are strictly convex, and
</p>
<p>Eξ(λ) =
ψ &prime;(λ)
</p>
<p>ψ(λ)
</p>
<p>strictly increases on (λ&minus;, λ+).
</p>
<p>The analyticity of ψ(λ) in the strip Reλ &isin; (λ&minus;, λ+) can be supplemented by
the following &ldquo;extended&rdquo; continuity property on the segment [λ&minus;, λ+] (in the strip
Reλ &isin; [λ&minus;, λ+]).
</p>
<p>(Ψ 5) The function ψ(λ) is continuous &ldquo;inside&rdquo; [λ&minus;, λ+], i.e. ψ(λ&plusmn; ∓ 0)=ψ(λ&plusmn;)
(where the cases ψ(λ&plusmn;)=&infin; are not excluded).
</p>
<p>Outside the segment [λ&minus;, λ+] such continuity, generally speaking, does not
hold as, for example, is the case when ψ(λ+) &lt; &infin; and ψ(λ+ + 0) = &infin;, which
takes place, say, for the distribution F with density f (x) = cx&minus;3e&minus;λ+x for x &ge; 1,
c= const.
</p>
<p>9.1.2 The Large Deviation Rate Function
</p>
<p>Under condition [C], the large deviation rate function will play the determining role
in the description of asymptotics of probabilities P(Sn &ge; x).</p>
<p/>
</div>
<div class="page"><p/>
<p>244 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>Definition 9.1.2 The large deviation rate function (or, for brevity, simply the rate
function) Λ of a random variable ξ is defined by
</p>
<p>Λ(α) := sup
λ
</p>
<p>(
αλ&minus; lnψ(λ)
</p>
<p>)
. (9.1.6)
</p>
<p>The meaning of the name will become clear later. In classical analysis, the right-
</p>
<p>hand side of (9.1.6) is known as the Legendre transform of the function lnψ(λ).
Consider the function A(α,λ) = αλ &minus; lnψ(λ) of the supremum appearing
</p>
<p>in (9.1.6). The function &minus; lnψ(λ) is strictly concave (see property (Ψ 4)), and hence
so is the function A(α,λ) (note also that A(α,λ) = &minus; lnψα(λ), where ψα(λ) =
e&minus;λαψ(λ) is the Laplace transform of the distribution of the random variable ξ &minus; α
and, therefore, from the &ldquo;qualitative point of view&rdquo;, A(α,λ) possesses all the prop-
</p>
<p>erties of the function &minus; lnψ(λ)). The foregoing implies that there always exists a
unique point λ= λ(α) (on the &ldquo;extended&rdquo; real line [&minus;&infin;,&infin;]) at which the supre-
mum in (9.1.6) is attained. As α grows, the value of A(α,λ) for λ &gt; 0 increases
</p>
<p>(proportionally to λ), and for λ &lt; 0 it decreases. Therefore, the graph of A(α,λ) as
</p>
<p>the function of λ will, roughly speaking, &ldquo;roll over&rdquo; to the right as α grows. This
</p>
<p>means that the maximum point λ(α) will also move to the right (or stay at the same
</p>
<p>place if λ(α)= λ+).
We now turn to more precise formulations. On the interval [λ&minus;, λ+], there exists
</p>
<p>the derivative (respectively, the right and the left derivative at the endpoints λ&plusmn;)
</p>
<p>A&prime;λ(α,λ)= α &minus;
ψ &prime;(λ)
</p>
<p>ψ(λ)
. (9.1.7)
</p>
<p>The parameters
</p>
<p>α&plusmn; =
ψ &prime;(λ&plusmn; ∓ 0)
ψ(λ&plusmn; ∓ 0)
</p>
<p>, α&minus; &lt; α+, (9.1.8)
</p>
<p>will play an important role in what follows. The value of α+ determines the angle at
which the curve lnψ(λ) &ldquo;sticks&rdquo; into the point (λ+, lnψ(λ+)). The quantity α&minus; has
a similar meaning. If α &isin; [α&minus;, α+] then the equation A&prime;λ(α,λ)=0, or (see (9.1.7))
</p>
<p>ψ &prime;(λ)
</p>
<p>ψ(λ)
= α, (9.1.9)
</p>
<p>always has a unique solution λ(α) on the segment [λ&minus;, λ+] (λ&plusmn; can be infinite).
This solution λ(α), being the inverse of an analytical and strictly increasing function
ψ &prime;(λ)
ψ(λ)
</p>
<p>on (λ&minus;, λ+) (see (9.1.9)), is also analytical and strictly increasing on (α&minus;, α+),
</p>
<p>λ(α) &uarr; λ+ as α &uarr; α+; λ(α) &darr; λ&minus; as α &darr; α&minus;. (9.1.10)
</p>
<p>The equalities
</p>
<p>Λ(α)= αλ(α)&minus; lnψ
(
λ(α)
</p>
<p>)
,
</p>
<p>ψ &prime;(λ(α))
</p>
<p>ψ(λ(α))
= α (9.1.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Laplace&rsquo;s and Cram&eacute;r&rsquo;s Transforms. The Rate Function 245
</p>
<p>yield
</p>
<p>Λ&prime;(α)= λ(α)+ αλ&prime;(α)&minus; ψ
&prime;(λ(α))
</p>
<p>ψ(λ(α))
λ&prime;(α)= λ(α).
</p>
<p>Recalling that
</p>
<p>ψ &prime;(0)
</p>
<p>ψ(0)
=m1 = Eξ, 0 &isin; [λ&minus;, λ+], m1 &isin; [α&minus;, α+],
</p>
<p>we obtain the following representation for the function Λ:
</p>
<p>(Λ1) If α0 &isin; [α&minus;, α+], α &isin; [α&minus;, α+] then
</p>
<p>Λ(α)=Λ(α0)+
&int; α
</p>
<p>α0
</p>
<p>λ(v)dv. (9.1.12)
</p>
<p>Since λ(m1)=Λ(m1)= 0 (this follows from (9.1.9) and (9.1.11)), we obtain,
in particular, for α0 =m1, that
</p>
<p>Λ(α)=
&int; α
</p>
<p>m1
</p>
<p>λ(v)dv. (9.1.13)
</p>
<p>The functions λ(α) and Λ(α) are analytic on (α&minus;, α+).
</p>
<p>Now consider what happens outside the segment [α&minus;, α+]. Assume for definite-
ness that λ+ &gt; 0. We will study the behaviour of the functions λ(α) and Λ(α) near
the point α+ and for α &gt; α+. Similar results hold true in the vicinity of the point α&minus;
in the case λ&minus; &lt; 0.
</p>
<p>First let λ+ = &infin;, i.e. the function lnψ(λ) is analytic on the whole semiaxis
λ &gt; 0, and the tail F+(t) decays as t &rarr; &infin; faster than any exponential function.
Denote by
</p>
<p>s&plusmn; =&plusmn; sup
{
t : F&plusmn;(t) &gt; 0
</p>
<p>}
</p>
<p>the boundaries of the support of F. Without loss of generality, we will assume that
</p>
<p>s+ &gt; 0, s&minus; &lt; 0. (9.1.14)
</p>
<p>This can always be achieved by shifting the random variable, similarly to our as-
</p>
<p>suming, without loss of generality, Eξ = 0 in many theorems of Chap. 8, where we
used the fact that the problem of studying the distribution of Sn is &ldquo;invariant&rdquo; with
</p>
<p>respect to a shift. (We can also note that Λξ&minus;a(α &minus; a)=Λξ (α), see property (Λ4)
below, and that (9.1.14) always holds provided that Eξ = 0.)
(Λ2) (i) If λ+ =&infin; then α+ = s+.
</p>
<p>Hence, for s+ = &infin;, we always have α+ = &infin; and so for any α &ge; α&minus; we are
dealing with the already considered &ldquo;regular&rdquo; case, where (9.1.12) and (9.1.13) hold
</p>
<p>true.</p>
<p/>
</div>
<div class="page"><p/>
<p>246 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>(ii) If s+ &lt;&infin; then λ+ =&infin;, α+ = s+,
</p>
<p>Λ(α+)=&minus; lnP(ξ = s+), Λ(α)=&infin; for α &gt; α+.
</p>
<p>Similar assertions hold true for s&minus;, α&minus;, λ&minus;.
</p>
<p>Proof (i) First let s+ &lt;&infin;. Then the asymptotics of ψ(λ) and ψ &prime;(λ) as λ&rarr;&infin; is
determined by the integrals in a neighbourhood of the point s+: for any fixed ε &gt; 0,
</p>
<p>ψ(λ)&sim; E
(
eλξ ; ξ &gt; s+ &minus; ε
</p>
<p>)
, ψ &prime;(λ)&sim; E
</p>
<p>(
ξeλξ ; ξ &gt; s+ &minus; ε
</p>
<p>)
</p>
<p>as λ&rarr;&infin;. Hence
</p>
<p>α+ = lim
λ&rarr;&infin;
</p>
<p>ψ &prime;(λ)
</p>
<p>ψ(λ)
= lim
</p>
<p>λ&rarr;&infin;
E(ξeλξ ; ξ &gt; s+ &minus; ε)
E(eλξ ; ξ &gt; s+ &minus; ε)
</p>
<p>= s+.
</p>
<p>If s+ = &infin;, then lnψ(λ) grows as λ &rarr; &infin; faster than any linear function and
therefore the derivative (lnψ(λ))&prime; increases unboundedly, α+ =&infin;.
</p>
<p>(ii) The first two assertions are obvious. Further, let p+ = P(ξ = s+) &gt; 0. Then
</p>
<p>ψ(λ)&sim; p+eλs+ ,
αλ&minus; lnψ(λ)= αλ&minus; lnp+ &minus; λs+ + o(1)= (α &minus; α+)λ&minus; lnp+ + o(1)
</p>
<p>as λ&rarr;&infin;. This and (9.1.11) imply that
</p>
<p>Λ(α)=
{
&minus; lnp+ for α = α+,
&infin; for α &gt; α+.
</p>
<p>If p+ = 0, then the relation ψ(λ)= o(eλs+) as λ&rarr;&infin; similarly implies Λ(α+)=&infin;.
Property (Λ2) is proved. �
</p>
<p>Now let 0 &lt; λ+ &lt;&infin;. If α+ &lt;&infin;, then necessarily ψ(λ+) &lt;&infin;, ψ(λ++0)=&infin;
and ψ &prime;(λ+) &lt;&infin; (here we mean the left derivative). If we assume that ψ(λ+)=&infin;,
then lnψ(λ+)=&infin;, (lnψ(λ))&prime; &rarr;&infin; as λ &uarr; λ+ and α+ =&infin;, which contradicts the
assumption α+ &lt;&infin;. Since ψ(λ)=&infin; for λ &gt; λ+, the point λ(α), having reached
the value λ+ as α grows, will stop at that point. So, for α &ge; α+, we have
</p>
<p>λ(α)= λ+, Λ(α)= αλ+ &minus; lnψ(λ+)=Λ(α+)+ λ+(α &minus; α+). (9.1.15)
</p>
<p>Thus, in this case, for α &ge; α+ the function λ(α) remains constant, while Λ(α) grows
linearly. Relations (9.1.12) and (9.1.13) remain true.
</p>
<p>If α+ =&infin;, then α &lt; α+ for all finite α &ge; α&minus;, and we again deal with the &ldquo;regu-
lar&rdquo; case that we considered earlier (see (9.1.12) and (9.1.13)). Since λ(α) does not
</p>
<p>decrease, these relations imply the convexity of Λ(α).
</p>
<p>In summary, we can formulate the following property.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Laplace&rsquo;s and Cram&eacute;r&rsquo;s Transforms. The Rate Function 247
</p>
<p>(Λ3) The functions λ(α) and Λ(α) can only be discontinuous at the points s&plusmn;
and under the condition P(ξ = s&plusmn;) &gt; 0. These points separate the domain
(s&minus;, s+) where the function Λ is finite and continuous (in the extended sense)
from the domain α /&isin; [s&minus;, s+] where Λ(α) =&infin;. In the domain [s&minus;, s+] the
function Λ is convex. (If we define convexity in the &ldquo;extended&rdquo; sense, i.e.
including infinite values as well, then Λ is convex on the entire real line.)
The function Λ is analytic in the interval (α&minus;, α+). If λ+ &lt;&infin; and α+ &lt;&infin;,
then on the half-line (α+,&infin;) the functionΛ(α) is linear with slope λ+; at the
boundary point α+ the continuity of the first derivatives persists. If λ+ =&infin;,
then Λ(α)=&infin; on (α+,&infin;). The function Λ(α) possesses a similar property
on (&minus;&infin;, α&minus;).
</p>
<p>If λ&minus; = 0, then α&minus; =m1 and λ(α)=Λ(α)= 0 for α &le;m1.
Indeed, since λ(m1) = 0 and ψ(λ) =&infin; for λ &lt; λ&minus; = 0 = λ(m1), as the value
</p>
<p>of α decreases to α&minus; = m1, the point λ(α), having reached the value 0, will stop,
and λ(α)= 0 for α &le; α&minus; =m1. This and the first identity in (9.1.11) also imply that
Λ(α)= 0 for α &le;m1.
</p>
<p>If λ&minus; = λ+ = 0 (condition [C] is not met), then λ(α)=Λ(α)&equiv; 0 for all α. This
is obvious, since the value of the function under the sup sign in (9.1.6) equals &minus;&infin;
for all λ 
= 0. In this case the limit theorems presented in the forthcoming sections
will be of little substance.
</p>
<p>We will also need the following properties of the function Λ.
</p>
<p>(Λ4) Under obvious notational conventions, for independent random variables ξ
and η, we have
</p>
<p>Λξ+η(α) = sup
λ
</p>
<p>(
αλ&minus; lnψξ (λ)&minus; lnψη(λ)
</p>
<p>)
= inf
</p>
<p>γ
</p>
<p>(
Λξ (γ )+Λη(α &minus; γ )
</p>
<p>)
,
</p>
<p>Λcξ+b(α) = sup
λ
</p>
<p>(
αλ&minus; λb&minus; lnψξ (λc)
</p>
<p>)
=Λξ
</p>
<p>(
α &minus; b
c
</p>
<p>)
.
</p>
<p>Clearly, infγ in the former relation is attained at the point γ at which λξ (γ ) =
λη(α &minus; γ ). If ξ and η are identically distributed then γ = α/2 and therefore
</p>
<p>Λξ+η(α)=Λξ
(
α
</p>
<p>2
</p>
<p>)
+Λη
</p>
<p>(
α
</p>
<p>2
</p>
<p>)
= 2Λξ
</p>
<p>(
α
</p>
<p>2
</p>
<p>)
.
</p>
<p>(Λ5) The function Λ(α) attains its minimal value 0 at the point α = Eξ =m1. For
definiteness, assume that α+ &gt; 0. If m1 = 0 and E|ξ k|&lt;&infin;, then
</p>
<p>λ(0)=Λ(0)=Λ&prime;(0)= 0, Λ&prime;&prime;(0)= 1
γ2
</p>
<p>, Λ&prime;&prime;&prime;(0)=&minus; γ3
γ 22
</p>
<p>, . . .
</p>
<p>(9.1.16)
</p>
<p>(In the case α&minus; = 0 the right derivatives are intended.) As α &darr; 0, one has the
representation</p>
<p/>
</div>
<div class="page"><p/>
<p>248 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>Λ(α)=
k&sum;
</p>
<p>j=2
</p>
<p>Λ(j)(0)
</p>
<p>j ! α
j + o
</p>
<p>(
αk
</p>
<p>)
. (9.1.17)
</p>
<p>The semi-invariants γj were defined in (9.1.2) and (9.1.3).
</p>
<p>If the two-sided Cram&eacute;r condition is satisfied then the series expansion (9.1.17)
</p>
<p>of the function Λ(α) holds for k =&infin;. This series is called the Cram&eacute;r series.
Verifying properties (Λ4) and (Λ5) is not difficult, and is left to the reader.
</p>
<p>(Λ6) The following inversion formula is valid: for λ &isin; (λ&minus;, λ+),
</p>
<p>lnψ(λ)= sup
α
</p>
<p>(
αλ&minus;Λ(α)
</p>
<p>)
. (9.1.18)
</p>
<p>This means that the rate function uniquely determines the Laplace transform ψ(λ)
</p>
<p>and hence the distribution F as well. Formula (9.1.18) also means that subsequent
</p>
<p>double applications of the Legendre transform to the convex function lnψ(λ) leads
</p>
<p>to the same original function.
</p>
<p>Proof We denote by T (λ) the right-hand side of (9.1.18) and show that T (λ) =
lnψ(λ) for λ &isin; (λ&minus;, λ+). If, in order to find the supremum in (9.1.18), we equate
to zero the derivative in α of the function under the sup sign, then we will get the
</p>
<p>equation
</p>
<p>λ=Λ&prime;(α)= λ(α). (9.1.19)
</p>
<p>Since λ(α), α &isin; (α&minus;, α+), is the function inverse to (lnψ(λ))&prime; (see (9.1.9)), for
λ &isin; (λ&minus;, λ+) Eq. (9.1.19) clearly has the solution
</p>
<p>α = a(λ) :=
(
lnψ(λ)
</p>
<p>)&prime;
. (9.1.20)
</p>
<p>Taking into account the fact that λ(a(λ))&equiv; λ, we obtain
</p>
<p>T (λ) = λa(λ)&minus;Λ
(
a(λ)
</p>
<p>)
,
</p>
<p>T &prime;(λ) = a(λ)+ λa&prime;(λ)&minus; λ
(
a(λ)
</p>
<p>)
a&prime;(λ)= a(λ).
</p>
<p>Since a(0)=m1 and T (0)=&minus;Λ(m1)= 0, we have
</p>
<p>T (λ)=
&int; λ
</p>
<p>0
</p>
<p>a(u)du= lnψ(λ). (9.1.21)
</p>
<p>The assertion is proved, and so is yet another inversion formula (the last equality
</p>
<p>in (9.1.21), which expresses lnψ(λ) as the integral of the function a(λ) inverse to
</p>
<p>λ(α)). �
</p>
<p>(Λ7) The exponential Chebyshev inequality. For α &ge;m1, we have
</p>
<p>P(Sn &ge; αn)&le; e&minus;nΛ(α).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Laplace&rsquo;s and Cram&eacute;r&rsquo;s Transforms. The Rate Function 249
</p>
<p>Proof If α &ge;m1, then λ(α)&ge; 0. For λ= λ(α)&ge; 0, we have
</p>
<p>ψn(λ) &ge; E
(
eλSn; Sn &ge; αn
</p>
<p>)
&ge; eλαnP(Sn &ge; αn);
</p>
<p>P(Sn &ge; αn) &le; e&minus;αnλ(α)+n lnψ(λ(α)) = e&minus;nΛ(α). �
</p>
<p>We now consider a few examples, where the values of λ&plusmn;, α&plusmn;, and the functions
ψ(λ), λ(α), Λ(α) can be calculated in an explicit form.
</p>
<p>Example 9.1.1 If ξ &sub;=�0,1, then
</p>
<p>ψ(λ)= eλ2/2, |λ&plusmn;| = |α&plusmn;| =&infin;, λ(α)= α, Λ(α)=
α2
</p>
<p>2
.
</p>
<p>Example 9.1.2 For the Bernoulli scheme ξ &sub;=Bp , we have
</p>
<p>ψ(λ) = peλ + q, |λ&plusmn;| =&infin;, α+ = 1, α&minus; = 0, m1 = Eξ = p,
</p>
<p>λ(α) = ln α(1 &minus; p)
p(1 &minus; α), Λ(α)= α ln
</p>
<p>α
</p>
<p>p
+ (1 &minus; α) ln 1 &minus; α
</p>
<p>1 &minus; p for α &isin; (0,1),
</p>
<p>Λ(0) = &minus; ln(1 &minus; p), Λ(1)=&minus; lnp, Λ(α)=&infin; for α /&isin; [0,1].
</p>
<p>Thus the function H(α) =Λ(α), which described large deviation probabilities for
Sn in the local Theorem 5.2.1 for the Bernoulli scheme, is nothing else but the rate
</p>
<p>function. Below, in Sect. 9.3, we will obtain generalisations of Theorem 5.2.1 for
</p>
<p>arbitrary arithmetic distributions.
</p>
<p>Example 9.1.3 For the exponential distribution Ŵβ , we have
</p>
<p>ψ(λ) = β
β &minus; λ, λ+ = β, λ&minus; =&minus;&infin;, α+ =&infin;, α&minus; = 0, m1 =
</p>
<p>1
</p>
<p>β
,
</p>
<p>λ(α) = β &minus; 1
α
, Λ(α)= αβ &minus; 1 &minus; lnαβ for α &gt; 0.
</p>
<p>Example 9.1.4 For the centred Poisson distribution with parameter β , we have
</p>
<p>ψ(λ) = exp
{
β
[
eλ &minus; 1 &minus; λ
</p>
<p>]}
, |λ&plusmn;| =&infin;, α&minus; =&minus;β, α+ =&infin;, m1 = 0,
</p>
<p>λ(α) = ln β + α
β
</p>
<p>, Λ(α)= (α + β) ln α + β
β
</p>
<p>&minus; α for α &gt;&minus;β.</p>
<p/>
</div>
<div class="page"><p/>
<p>250 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>9.2 A Relationship Between Large Deviation Probabilities for
</p>
<p>Sums of Random Variables and Those for Sums of Their
</p>
<p>Cram&eacute;r Transforms. The Probabilistic Meaning of the Rate
</p>
<p>Function
</p>
<p>9.2.1 A Relationship Between Large Deviation Probabilities for
</p>
<p>Sums of Random Variables and Those for Sums of Their
</p>
<p>Cram&eacute;r Transforms
</p>
<p>Consider the Cram&eacute;r transform of F at the point λ = λ(α) for α &isin; [α&minus;, α+] and
introduce the notation ξ (α) := ξ(λ(α)),
</p>
<p>S(α)n :=
n&sum;
</p>
<p>i=1
ξ
(α)
i ,
</p>
<p>where ξ
(α)
i are independent copies of ξ
</p>
<p>(α). The distribution F(α) := F(λ(α)) of the
random variable ξ (α) is called the Cram&eacute;r transform of F with parameter α. The
random variables ξ (α) are also called Cram&eacute;r transforms, but of the original random
</p>
<p>variable ξ . The relationship between the distributions of Sn and S
(α)
n is established
</p>
<p>in the following assertion.
</p>
<p>Theorem 9.2.1 For x = nα, α &isin; (α&minus;, α+), and any t &gt; 0, one has
</p>
<p>P
(
Sn &isin; [x, x + t)
</p>
<p>)
= e&minus;nΛ(α)
</p>
<p>&int; t
</p>
<p>0
</p>
<p>e&minus;λ(α)zP
(
S(α)n &minus; αn &isin; dz
</p>
<p>)
. (9.2.1)
</p>
<p>Proof The Laplace transform of the distribution of the sum S(α)n is clearly equal to
</p>
<p>Ee&micro;S
(α)
n =
</p>
<p>[
ψ(&micro;+ λ(α))
ψ(λ(α))
</p>
<p>]n
(9.2.2)
</p>
<p>(see (9.1.5)). On the other hand, consider the Cram&eacute;r transform (Sn)(λ(α)) of Sn at
</p>
<p>the point λ(α). Applying (9.1.5) to the distribution of Sn, we obtain
</p>
<p>Ee&micro;(Sn)(λ(α)) = ψ
n(&micro;+ λ(α))
ψn(λ(α))
</p>
<p>.
</p>
<p>Since this expression coincides with (9.2.2), the Cram&eacute;r transform of Sn at the
point λ(α) coincides in distribution with the sum S(α)n of the transforms ξ
</p>
<p>(α)
i . In
</p>
<p>other words,
</p>
<p>P(Sn &isin; dv)eλ(α)v
ψn(λ(α))
</p>
<p>= P
(
S(α)n &isin; dv
</p>
<p>)
(9.2.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Large Deviation of Sums of Random Variables and Cram&eacute;r Transforms 251
</p>
<p>or, which is the same,
</p>
<p>P(Sn &isin; dv) = e&minus;λ(α)v+n lnψ(λ(α))P
(
S(α)n &isin; dv
</p>
<p>)
= e&minus;nΛ(α)+λ(α)(nα&minus;v)P
</p>
<p>(
S(α)n &isin; dv
</p>
<p>)
.
</p>
<p>Integrating this equality in ν from x to x+ t , letting x := nα and making the change
of variables v &minus; nα = z, we get
</p>
<p>P
(
Sn &isin; [x, x + t)
</p>
<p>)
= e&minus;nΛ(α)
</p>
<p>&int; x+t
</p>
<p>x
</p>
<p>eλ(α)(nα&minus;v)P
(
S(α)n &isin; dv
</p>
<p>)
</p>
<p>= e&minus;nΛ(α)
&int; t
</p>
<p>0
</p>
<p>e&minus;λ(α)zP
(
S(α)n &minus; αn &isin; dz
</p>
<p>)
.
</p>
<p>The theorem is proved. �
</p>
<p>Since for α &isin; [α&minus;, α+] we have
</p>
<p>Eξ (α) = ψ
&prime;(λ(α))
</p>
<p>ψ(λ(α))
= α
</p>
<p>(see (9.1.11)), one has E(S
(α)
n &minus; αn)= 0 and so for t &le; c
</p>
<p>&radic;
n we have probabilities
</p>
<p>of normal deviations of S(α)n &minus;αn on the right-hand side of (9.2.1). This allows us to
reduce the problem on large deviations of Sn to the problem on normal deviations
of S(α)n . If α &gt; α+, then formula (9.2.1) is still rather useful, as will be shown in
Sects. 9.4 and 9.5.
</p>
<p>9.2.2 The Probabilistic Meaning of the Rate Function
</p>
<p>In this section we will prove the following assertion, which clarifies the probabilistic
</p>
<p>meaning of the function Λ(α).
</p>
<p>Denote by ∆[α) := [α,α + ∆) the interval of length ∆ with the left end at
the point α. The notation ∆n[α), where ∆n depends on n, will have a similar mean-
ing.
</p>
<p>Theorem 9.2.2 For each fixed α and all sequences ∆n converging to 0 as n&rarr;&infin;
slowly enough, one has
</p>
<p>Λ(α)=&minus; lim
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
. (9.2.4)
</p>
<p>This relation can also be written as
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
= e&minus;nΛ(α)+o(n).</p>
<p/>
</div>
<div class="page"><p/>
<p>252 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>Proof of Theorem 9.2.2 First let α &isin; (α&minus;, α+). Then
</p>
<p>Eξ (α) = α, Var ξ (α) =
(
lnψ(λ)
</p>
<p>)&prime;&prime;
λ=λ(α) &lt;&infin;
</p>
<p>and hence, as n &rarr; &infin; and ∆n &rarr; 0 slowly enough (e.g., for ∆n &ge; n&minus;1/3), by the
central limit theorem we have
</p>
<p>P
(
S(α)n &minus; αn &isin; [0,∆nn)
</p>
<p>)
&rarr; 1/2.
</p>
<p>Therefore, by Theorem 9.2.1 for t =∆nn, x = αn and by the mean value theorem,
</p>
<p>P
(
Sn &isin; [x, x + t)
</p>
<p>)
=
</p>
<p>(
1
</p>
<p>2
+ o(1)
</p>
<p>)
e&minus;nΛ(α)&minus;λ(α)∆nnθ , θ &isin; (0,1);
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn &isin; [x, x + t)
</p>
<p>)
= &minus;Λ(α)&minus; λ(α)θ∆n + o(1)=&minus;Λ(α)+ o(1)
</p>
<p>as n&rarr;&infin;. This proves (9.2.4) for α &isin; (α&minus;, α+).
The further proof is divided into three stages.
</p>
<p>(1) The upper bound in the general case. Now let α be arbitrary and |λ(α)|&lt;&infin;.
By Theorem 9.2.1 for t = n∆n, we have
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
&le; exp
</p>
<p>{
&minus;nΛ(α)+ max
</p>
<p>(∣∣λ(0)
∣∣,
∣∣λ(α)
</p>
<p>∣∣)n∆n
}
.
</p>
<p>If ∆n &rarr; 0 then
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
&le;&minus;Λ(α). (9.2.5)
</p>
<p>(This inequality can also be obtained from the exponential Chebyshev&rsquo;s inequal-
</p>
<p>ity (Λ7).)
</p>
<p>(2) The lower bound in the general case. Let |λ(α)| &lt;&infin; and |s&plusmn;| = &infin;. Intro-
duce &ldquo;truncated&rdquo; random variables (N)ξ with the distribution
</p>
<p>P
(
(N)ξ &isin; B
</p>
<p>)
= P(ξ &isin; B; |ξ |&lt;N)
</p>
<p>P(|ξ |&lt;N) = P
(
ξ &isin; B
</p>
<p>∣∣ |ξ |&lt;N
)
</p>
<p>and endow all the symbols that correspond to (N)ξ with the left superscript (N).
</p>
<p>Then clearly, for each λ,
</p>
<p>E
(
eλξ ; |ξ |&lt;N
</p>
<p>)
&uarr;ψ(λ), P
</p>
<p>(
|ξ |&lt;N
</p>
<p>)
&uarr; 1
</p>
<p>as N &rarr;&infin;, so that
</p>
<p>(N)ψ(λ)= E(e
λξ ; |ξ |&lt;N)
P(|ξ |&lt;N) &rarr;ψ(λ).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Large Deviation of Sums of Random Variables and Cram&eacute;r Transforms 253
</p>
<p>The functions (N)Λ(α) and Λ(α) are the upper bounds for the concave functions
</p>
<p>αλ&minus; ln (N)ψ(λ) and αλ&minus; lnψ(λ), respectively. Therefore for each α we also have
convergence (N)Λ(α)&rarr;Λ(α) as N &rarr;&infin;.
</p>
<p>Further,
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
&ge; P
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α); |ξj |&lt;N,j = 1, . . . ,N
</p>
<p>)
</p>
<p>= Pn
(
|ξ |&lt;N
</p>
<p>)
P
</p>
<p>(
(N)Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
.
</p>
<p>Since s&plusmn; = &plusmn;&infin;, one has (N)α&plusmn; = &plusmn;N and, for N large enough, we have α &isin;
((N)α&minus;, (N)α+). Hence we can apply the first part of the proof of the theorem by
virtue of which, as ∆n &rarr; 0,
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
(N)Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
= &minus;(N)Λ(α)+ o(1),
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
&ge; &minus;(N)Λ(α)+ o(1)+ lnP
</p>
<p>(
|ξ |&lt;N
</p>
<p>)
.
</p>
<p>The right-hand side of the last inequality can be made arbitrarily close to &minus;Λ(α) by
choosing a suitable N . Since the left-hand side of this inequality does not depend
</p>
<p>on N , we have
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α)
</p>
<p>)
&ge;&minus;Λ(α). (9.2.6)
</p>
<p>Together with (9.2.5), this proves (9.2.4).
</p>
<p>(3) It remains to remove the restrictions stated at the beginning of stages (1) and
</p>
<p>(2) of the proof, i.e. to consider the cases |λ(α)| = &infin; and min |s&plusmn;| &lt; &infin;. These
two relations are connected with each other since, for instance, the equality λ(α)=
λ+ = &infin; can only hold if α &ge; α+ = s+ &lt; &infin; (see property (Λ2)). For α &gt; s+,
relation (9.2.4) is evident, since P(Sn/n &isin; ∆n[α)) = 0 and Λ(α) = &infin;. For α =
α+ = s+ and p+ = P(ξ = s+), we have, for any ∆&gt; 0,
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>n
&isin;∆[α+)
</p>
<p>)
= P(Sn = nα+)= pn+. (9.2.7)
</p>
<p>Since in this case Λ(α+)=&minus; lnp+ (see (Λ2)), the equality (9.2.4) holds true.
The case λ(α)= λ&minus; =&minus;&infin; with s&minus; &gt;&minus;&infin; is considered in a similar way. How-
</p>
<p>ever, due to the asymmetry of the interval ∆[α) with respect to the point α, there
are small differences. Instead of an equality in (9.2.7) we only have the inequality
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α&minus;)
</p>
<p>)
&ge; P(Sn = nα&minus;)= pn&minus;, p&minus; = P(ξ = α&minus;). (9.2.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>254 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>Therefore we also have to use the exponential Chebyshev&rsquo;s inequality (see (Λ7))
</p>
<p>applying it to &minus;Sn for s&minus; = α&minus; &lt; 0:
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[α&minus;)
</p>
<p>)
&le; P
</p>
<p>(
Sn
</p>
<p>n
&lt; α&minus; +∆n
</p>
<p>)
&le; e&minus;nΛ(α&minus;+∆n). (9.2.9)
</p>
<p>Relations (9.2.8), (9.2.9), the equality Λ(α&minus;)=&minus; lnp&minus;, and the right continuity of
Λ(α) at the point α&minus; imply (9.2.4) for α = α&minus;. The theorem is proved. �
</p>
<p>9.2.3 The Large Deviations Principle
</p>
<p>It is not hard to derive from Theorem 9.2.2 a corollary on the asymptotics of the
</p>
<p>probabilities of Sn/n hitting an arbitrary Borel set. Denote by (B) and [B] the
interior and the closure of B , respectively ((B) is the union of all open intervals
</p>
<p>contained in B). Put
</p>
<p>Λ(B) := inf
α&isin;B
</p>
<p>Λ(α).
</p>
<p>Theorem 9.2.3 For any Borel set B , the following inequalities hold:
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin; B
</p>
<p>)
&ge; &minus;Λ
</p>
<p>(
(B)
</p>
<p>)
, (9.2.10)
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin; B
</p>
<p>)
&le; &minus;Λ
</p>
<p>(
[B]
</p>
<p>)
. (9.2.11)
</p>
<p>If Λ((B))=Λ([B]), then the following limit exists:
</p>
<p>lim
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin; B
</p>
<p>)
=&minus;Λ(B). (9.2.12)
</p>
<p>This assertion is called the large deviation principle. It is one of the so-called
&ldquo;rough&rdquo; (&ldquo;logarithmic&rdquo;) limit theorems that describe the asymptotic behaviour of
</p>
<p>lnP(Sn/n &isin; B). It is usually impossible to derive from this assertion the asymp-
totics of the probability P(Sn/n &isin; B) itself. (In the equality P(Sn/n &isin; B) =
exp{&minus;nΛ(B)+ o(n)}, the term o(n) may grow in absolute value.)
</p>
<p>Proof Without losing generality, we can assume that B &sub; [s&minus;, s+] (since Λ(α)=&infin;
outside that domain).
</p>
<p>We first prove (9.2.10). Let α(B) be such that
</p>
<p>Λ
(
(B)
</p>
<p>)
&equiv; inf
</p>
<p>α&isin;(B)
Λ(α)=Λ(α(B))
</p>
<p>(recall that Λ(α) is continuous on [s&minus;, s+]). Then there exist a sequence of points
αk and a sequence of intervals (αk &minus; δk, αk + δk), where δk &rarr; 0, lying in (B) and</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Large Deviation of Sums of Random Variables and Cram&eacute;r Transforms 255
</p>
<p>converging to the point α(B), such that
</p>
<p>Λ
(
(B)
</p>
<p>)
= inf
</p>
<p>k
Λ
(
(αk &minus; δk, αk + δk)
</p>
<p>)
.
</p>
<p>Here clearly
</p>
<p>inf
k
</p>
<p>Λ
(
(αk &minus; δk, αk + δk)
</p>
<p>)
= inf
</p>
<p>k
Λ(αk),
</p>
<p>and for a given ε &gt; 0, there exists a k = K such that Λ(αK) &lt; Λ((B)) + ε.
Since ∆n[αk)&sub; (αk &minus; δk, αk + δk) for large enough n (here ∆n[αk) is from Theo-
rem 9.2.2), we have by Theorem 9.2.2 that, as n&rarr;&infin;,
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin; B
</p>
<p>)
&ge; 1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin; (B)
</p>
<p>)
</p>
<p>&ge; 1
n
</p>
<p>lnP
</p>
<p>(
Sn
</p>
<p>n
&isin; (αK &minus; δK , αK + δK)
</p>
<p>)
</p>
<p>&ge; 1
n
</p>
<p>lnP
</p>
<p>(
Sn
</p>
<p>n
&isin;∆n[αK)
</p>
<p>)
&ge;&minus;Λ(αK)+ o(1)
</p>
<p>&ge; &minus;Λ
(
(B)
</p>
<p>)
&minus; ε+ o(1).
</p>
<p>As the left-hand side of this inequality does not depend on ε, inequality (9.2.10) is
</p>
<p>proved.
</p>
<p>We now prove inequality (9.2.11). Denote by α[B] the point at which
infα&isin;[B]Λ(α) = Λ(α[B]) is attained (this point always belongs to [B] since [B]
is closed). If Λ(α[B])= 0, then the inequality is evident. Now let Λ(α[B]) &gt; 0. By
convexity of Λ the equation Λ(α)=Λ(α[B]) can have a second solution α&prime;[B]. As-
sume it exists and, for definiteness, α&prime;[B] &lt; α[B]. The relation Λ([B]) = Λ(α[B])
means that the set [B] does not intersect with (α&prime;[B], α[B]) and
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>n
&isin; B
</p>
<p>)
&le; P
</p>
<p>(
Sn
</p>
<p>n
&isin; [B]
</p>
<p>)
&le; P
</p>
<p>(
Sn
</p>
<p>n
&le; α&prime;[B]
</p>
<p>)
+ P
</p>
<p>(
Sn
</p>
<p>n
&ge; α[B]
</p>
<p>)
. (9.2.13)
</p>
<p>Moreover, in this case m1 &isin; (α&prime;[B], α[B]) and each of the probabilities on the right-
hand side of (9.2.13) can be bounded using the exponential Chebyshev&rsquo;s inequality
</p>
<p>(see (Λ7)) by the value e&minus;nΛ(α[B]). This implies (9.2.11).
If the second solution α&prime;[B] does not exist, then one of the summands on the right-
</p>
<p>hand side of (9.2.13) equals zero, and we obtain the same result.
</p>
<p>The second assertion of the theorem (Eq. (9.2.12)) is evident.
</p>
<p>The theorem is proved. �
</p>
<p>Using Theorem 9.2.3, we can complement Theorem 9.2.2 with the following
</p>
<p>assertion.</p>
<p/>
</div>
<div class="page"><p/>
<p>256 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>Corollary 9.2.1 The following limit always exists
</p>
<p>lim
∆&rarr;0
</p>
<p>lim
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
lnP
</p>
<p>(
Sn
</p>
<p>n
&isin;∆[α)
</p>
<p>)
=&minus;Λ(α). (9.2.14)
</p>
<p>Proof Take the set B in Theorem 9.2.3 to be the interval B =∆[α). If α /&isin; [s&minus;, s+]
then the assertion is obvious (since both sides of (9.2.14) are equal to &minus;&infin;). If
α = s&plusmn; then (9.2.14) is already proved in (9.2.7), (9.2.8) and (9.2.9).
</p>
<p>It remains to consider points α &isin; (s&minus;, s+). For such α, the function Λ(α) is con-
tinuous and α+∆ is also a point of continuity of Λ for ∆ small enough, and hence
</p>
<p>Λ
(
(B)
</p>
<p>)
=Λ
</p>
<p>(
[B]
</p>
<p>)
&rarr;Λ(α)
</p>
<p>as ∆&rarr; 0. Therefore by Theorem 9.2.3 the inner limit in (9.2.14) exists and con-
verges to &minus;Λ(α) as ∆&rarr; 0.
</p>
<p>The corollary is proved. �
</p>
<p>Note that the assertions of Theorems 9.2.2 and 9.2.3 and their corollaries are
</p>
<p>&ldquo;universal&rdquo;&mdash;they contain no restrictions on the distribution F.
</p>
<p>9.3 Integro-Local, Integral and Local Theorems on Large
</p>
<p>Deviation Probabilities in the Cram&eacute;r Range
</p>
<p>9.3.1 Integro-Local and Integral Theorems
</p>
<p>In this subsection, under the assumption that the Cram&eacute;r condition λ+ &gt; 0 is met,
we will find the asymptotics of probabilities P(Sn &isin;∆[x)) for scaled deviations α =
x/n from the so-called Cram&eacute;r (or regular) range, i.e. for the range α &isin; (α&minus;, α+)
in which the rate function Λ(α) is analytic.
</p>
<p>In the non-lattice case, in addition to the condition λ+ &gt; 0, we will assume with-
out loss of generality that Eξ = 0. In this case necessarily
</p>
<p>α&minus; &le; 0, α+ =
ψ &prime;(λ+)
</p>
<p>ψ(λ+)
&gt; 0, λ(0)= 0.
</p>
<p>The length ∆ of the interval may depend on n in some cases. In such cases, we will
</p>
<p>write ∆n instead of ∆, as we did earlier. The value
</p>
<p>σ 2α =
ψ &prime;&prime;(λ(α))
</p>
<p>ψ(λ(α))
&minus; α2 (9.3.1)
</p>
<p>is clearly equal to Var(ξ (α)) (see (9.1.5) and the definition of ξ (α) in Sect. 9.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Large Deviation Probabilities in the Cram&eacute;r Range 257
</p>
<p>Theorem 9.3.1 Let λ+ &gt; 0, α &isin; [0, α+), ξ be a non-lattice random variable,
Eξ = 0 and Eξ2 &lt;&infin;. If ∆n &rarr; 0 slowly enough as n&rarr;&infin;, then
</p>
<p>P
(
Sn &isin;∆n[x)
</p>
<p>)
= ∆n
</p>
<p>σα
&radic;
</p>
<p>2πn
e&minus;nΛ(α)
</p>
<p>(
1 + o(1)
</p>
<p>)
, (9.3.2)
</p>
<p>where α = x/n, and, for each fixed α1 &isin; (0, α+), the remainder term o(1) is uniform
in α &isin; [0, α1] for any fixed α1 &isin; (0, α+).
</p>
<p>A similar assertion is valid in the case when λ&minus; &lt; 0 and α &isin; (α&minus;,0].
</p>
<p>Proof The proof is based on Theorems 9.2.1 and 8.7.1A. Since the conditions of
Theorem 9.2.1 are satisfied, we have
</p>
<p>P
(
Sn &isin;∆n[x)
</p>
<p>)
= e&minus;nΛ(α)
</p>
<p>&int; ∆n
0
</p>
<p>e&minus;λ(α)zP
(
S(α)n &minus; αn &isin; dz
</p>
<p>)
.
</p>
<p>As λ(α) &le; λ(α+ &minus; ε) &lt; &infin; and ∆n &rarr; 0, one has e&minus;λ(α)z &rarr; 1 uniformly in
z &isin;∆n[0) and hence, as n&rarr;&infin;,
</p>
<p>P
(
Sn &isin;∆n[x)
</p>
<p>)
= e&minus;nΛ(α)P
</p>
<p>(
S(α)n &minus; αn &isin;∆n[0)
</p>
<p>)(
1 + o(1)
</p>
<p>)
(9.3.3)
</p>
<p>uniformly in α &isin; [0, α+ &minus; ε].
We now show that Theorem 8.7.1A is applicable to the random variables ξ (α) =
</p>
<p>ξ(λ(α)). That σα = σ(λ(α)) is bounded away from 0 and from &infin; for α &isin; [0, α1] is
evident. (The same is true of all the theorems in this section.) Therefore, it remains
</p>
<p>to verify whether conditions (a) and (b) of Theorem 8.7.1A are met for λ= λ(α) &isin;
[0, λ1], λ1 := λ(α1) &lt; λ+ and ϕ(λ)(t)= ψ(λ+it)ψ(λ) (see (9.1.5)). We have
</p>
<p>ψ(λ+ it)=ψ(λ)+ itψ &prime;(λ)&minus; t
2
</p>
<p>2
ψ &prime;&prime;(λ)+ o
</p>
<p>(
t2
)
</p>
<p>as t &rarr; 0, where the remainder term is uniform in λ if the function ψ &prime;&prime;(λ+ iu) is
uniformly continuous in u. The required uniform continuity can easily be proved
</p>
<p>by imitating the corresponding result for ch.f.s (see property 4 in Sect. 7.1). This
</p>
<p>proves condition (a) in Theorem 8.7.1A with
</p>
<p>a(λ)= ψ
&prime;(λ)
</p>
<p>ψ(λ)
, m2(λ)=
</p>
<p>ψ &prime;&prime;(λ)
</p>
<p>ψ(λ)
.
</p>
<p>Now we will verify condition (b) in Theorem 8.7.1A. Assume the contrary: there
</p>
<p>exists a sequence λk &isin; [0, λ1] such that
</p>
<p>qλk := sup
θ1&le;|t |&le;θ2
</p>
<p>|ψ(λk + it)|
ψ(λk)
</p>
<p>&rarr; 1</p>
<p/>
</div>
<div class="page"><p/>
<p>258 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>as k &rarr; &infin;. By the uniform continuity of ψ in that domain, there exist points
tk &isin; [θ1, θ2] such that, as k&rarr;&infin;,
</p>
<p>ψ(λk + itk)
ψ(λk)
</p>
<p>&rarr; 1.
</p>
<p>Since the region λ &isin; [0, λ1], |t | &isin; [θ1, θ2] is compact, there exists a subsequence
(λk&prime; , tk&prime;) &rarr; (λ0, t0) as k&prime; &rarr; &infin;. Again using the continuity of ψ , we obtain the
equality
</p>
<p>|ψ(λ0 + it0)|
ψ(λ0)
</p>
<p>= 1, (9.3.4)
</p>
<p>which contradicts the non-latticeness of ξ(λ0). Property (b) is proved.
</p>
<p>Thus we can now apply Theorem 8.7.1A to the probability on the right-hand side
</p>
<p>of (9.3.3). Since Eξ (α) = α and E(ξ (α))2 = ψ
&prime;&prime;(λ(α))
</p>
<p>ψ(λ(α))
, this yields
</p>
<p>P
(
Sn &isin;∆n[x)
</p>
<p>)
= e&minus;nΛ(α)
</p>
<p>(
∆n
</p>
<p>σα
&radic;
n
φ(0)+ o
</p>
<p>(
1&radic;
n
</p>
<p>))
</p>
<p>= ∆n
σα
</p>
<p>&radic;
2πn
</p>
<p>e&minus;nΛ(α)
(
1 + o(1)
</p>
<p>)
(9.3.5)
</p>
<p>uniformly in α &isin; [0, α1] (or in x &isin; [0, α1n]), where the values of
</p>
<p>σ 2α = E
(
ξ (α) &minus; α
</p>
<p>)2 = ψ
&prime;&prime;(λ(α))
</p>
<p>ψ(λ(α))
&minus; α2
</p>
<p>are bounded away from 0 and from &infin;. The theorem is proved. �
</p>
<p>From Theorem 9.3.1 we can now derive integro-local theorems and integral the-
</p>
<p>orems for fixed or growing ∆. Since in the normal deviation range (when x is com-
</p>
<p>parable with
&radic;
n) we have already obtained such results, to simplify the exposition
</p>
<p>we will consider here large deviations only, when x ≫&radic;n or, which is the same,
α = x/n≫ 1/&radic;n. To be more precise, we will assume that there exists a function
N(n)&rarr;&infin;, N(n)= o(&radic;n) as n&rarr;&infin;, such that x &ge;N(n)&radic;n (α &ge;N(n)/&radic;n).
</p>
<p>Theorem 9.3.2 Let λ+ &gt; 0, α &isin; [0, α+), ξ be non-lattice, Eξ = 0 and Eξ2 &lt;&infin;.
Then, for any ∆&ge;∆0 &gt; 0, x &ge;N(n)= o(
</p>
<p>&radic;
n ), N(n)&rarr;&infin; as n&rarr;&infin;, one has
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= e
</p>
<p>&minus;nΛ(α)
</p>
<p>σαλ(α)
&radic;
</p>
<p>2πn
</p>
<p>(
1 &minus; e&minus;λ(α)∆
</p>
<p>)(
1 + o(1)
</p>
<p>)
, (9.3.6)
</p>
<p>o(1) being uniform in α = x/n &isin; [N(n)/&radic;n,α1] and ∆ &ge; ∆0 for each fixed
α1 &isin; (0, α+).
</p>
<p>In particular (for ∆=&infin;),
</p>
<p>P(Sn &ge; x)=
e&minus;nΛ(α)
</p>
<p>σαλ(α)
&radic;
</p>
<p>2πn
</p>
<p>(
1 + o(1)
</p>
<p>)
. (9.3.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Large Deviation Probabilities in the Cram&eacute;r Range 259
</p>
<p>Proof Partition the interval ∆[x) into subintervals ∆n[x + k∆n), k = 0, . . . ,
∆/∆n &minus; 1, where ∆n &rarr; 0 and, for simplicity, we assume that M = ∆/∆n is an
integer. Then, by Theorem 9.2.1, as ∆n &rarr; 0,
</p>
<p>P
(
Sn &isin;∆n[x + k∆n)
</p>
<p>)
</p>
<p>= P
(
Sn &isin;
</p>
<p>[
x, x + (k + 1)∆n
</p>
<p>))
&minus; P
</p>
<p>(
Sn &isin; [x, x + k∆n)
</p>
<p>)
</p>
<p>= e&minus;nΛ(α)
&int; (k+1)∆n
k∆n
</p>
<p>e&minus;λ(α)zP
(
S(α)n &minus; αn &isin; dz
</p>
<p>)
</p>
<p>= e&minus;nΛ(α)&minus;λ(α)k∆nP
(
S(α)n &minus; αn &isin;∆n[k∆n)
</p>
<p>)(
1 + o(1)
</p>
<p>)
(9.3.8)
</p>
<p>uniformly in α &isin; [0, α1]. Here, similarly to (9.3.5), by Theorem 8.7.1A we have
</p>
<p>P
(
S(α)n &minus; αn &isin;∆n[k∆n)
</p>
<p>)
= ∆n
</p>
<p>σα
&radic;
n
φ
</p>
<p>(
k∆n
</p>
<p>σα
&radic;
n
</p>
<p>)
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
(9.3.9)
</p>
<p>uniformly in k and α. Since
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
=
</p>
<p>M&minus;1&sum;
</p>
<p>k=0
P
(
Sn &isin;∆n[x + k∆)
</p>
<p>)
,
</p>
<p>substituting the values (9.3.8) and (9.3.9) into the right-hand side of the last equality,
</p>
<p>we obtain
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= e
</p>
<p>&minus;nΛ(α)
</p>
<p>σα
&radic;
n
</p>
<p>M&minus;1&sum;
</p>
<p>k=0
∆ne
</p>
<p>&minus;λ(α)k∆n
(
φ
</p>
<p>(
k∆n
</p>
<p>σα
&radic;
n
</p>
<p>)
+ o(1)
</p>
<p>)
</p>
<p>= e
&minus;nΛ(α)
</p>
<p>σα
&radic;
n
</p>
<p>&int; ∆&minus;∆n
0
</p>
<p>e&minus;λ(α)z
(
φ
</p>
<p>(
z
</p>
<p>σα
&radic;
n
</p>
<p>)
+ o(1)
</p>
<p>)
dz.
</p>
<p>(9.3.10)
</p>
<p>After the variable change λ(α)z= u, the right-hand side can be rewritten as
</p>
<p>e&minus;nλ(α)
</p>
<p>σαλ(α)
&radic;
n
</p>
<p>&int; (∆&minus;∆n)λ(α)
</p>
<p>0
</p>
<p>e&minus;u
(
φ
</p>
<p>(
u
</p>
<p>σαλ(α)
&radic;
n
</p>
<p>)
+ o(1)
</p>
<p>)
du, (9.3.11)
</p>
<p>where the remainder term o(1) is uniform in α &isin; [0, α1], ∆ &ge; ∆0, and u from the
integration range. Since λ(α) &sim; α/σ 2 for small α (see (9.1.12) and (9.1.16)), for
α &ge;N(n)/&radic;n we have
</p>
<p>λ(α) &gt;
N(n)
</p>
<p>σ 2
&radic;
n
</p>
<p>(
1 + o(1)
</p>
<p>)
, σαλ(α)
</p>
<p>&radic;
n &gt;
</p>
<p>σαN(n)
</p>
<p>σ 2
&rarr;&infin;.
</p>
<p>Therefore, for any fixed u, one has
</p>
<p>φ
</p>
<p>(
u
</p>
<p>σαλ(α)
&radic;
n
</p>
<p>)
&rarr; φ(0)= 1&radic;
</p>
<p>2π
.</p>
<p/>
</div>
<div class="page"><p/>
<p>260 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>Moreover, φ(v)&le; 1/
&radic;
</p>
<p>2π for all v. Hence, by (9.3.10) and (9.3.11),
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= e
</p>
<p>&minus;nΛ(α)
</p>
<p>σαλ(α)
&radic;
</p>
<p>2πn
</p>
<p>&int; λ(α)∆
</p>
<p>0
</p>
<p>e&minus;udu
(
1 + o(1)
</p>
<p>)
</p>
<p>= e
&minus;nΛ(α)
</p>
<p>σαλ(α)
&radic;
</p>
<p>2πn
</p>
<p>(
1 &minus; e&minus;λ(α)∆
</p>
<p>)(
1 + o(1)
</p>
<p>)
</p>
<p>uniformly in α &isin; [0, α1] and ∆ &ge;∆0. Relation (9.3.7) clearly follows from (9.3.6)
with ∆=&infin;. The theorem is proved. �
</p>
<p>Note that if E|ξ |k &lt;&infin; (for λ+ &gt; 0 this is a restriction on the rate of decay of the
left tails P(ξ &lt;&minus;t), t &gt; 0), then expansion (9.1.17) is valid and, for deviations x =
o(n) (α = o(1)) such that nαk = xk/nk&minus;1 &le; c= const, we can change the exponent
nΛ(α) in (9.3.6) and (9.3.7) to
</p>
<p>nΛ(α)= n
k&sum;
</p>
<p>j=2
</p>
<p>Λ(j)(0)
</p>
<p>j ! α
j + o
</p>
<p>(
nαk
</p>
<p>)
, (9.3.12)
</p>
<p>where Λ(j)(0) are found in (9.1.16). For k = 3, the foregoing implies the following.
</p>
<p>Corollary 9.3.1 Let λ+ &gt; 0, E|ξ |3 &lt; &infin;, ξ be non-lattice, Eξ = 0, Eξ2 = σ 2,
x≫&radic;n and x = o(n2/3) as n&rarr;&infin;. Then
</p>
<p>P(Sn &ge; x)&sim;
σ
&radic;
n
</p>
<p>x
&radic;
</p>
<p>2π
exp
</p>
<p>{
&minus; x
</p>
<p>2
</p>
<p>2nσ 2
</p>
<p>}
&sim;Φ
</p>
<p>(
&minus; x
σ
&radic;
n
</p>
<p>)
. (9.3.13)
</p>
<p>In the last relation we used the symmetry of the standard normal law, i.e. the
</p>
<p>equality 1 &minus;Φ(t) = Φ(&minus;t). Assertion (9.3.13) shows that in the case λ+ &gt; 0 and
E|ξ |3 &lt;&infin; the asymptotic equivalence
</p>
<p>P(Sn &ge; x)&sim;Φ
(
&minus; x
σ
&radic;
n
</p>
<p>)
</p>
<p>persists outside the range of normal deviations as well, up to the values
</p>
<p>x = o(n2/3). If Eξ3 = 0 and Eξ4 &lt;&infin;, then this equivalence holds true up to the
values x = o(n3/4). For larger x this equivalence, generally speaking, no longer
holds.
</p>
<p>Proof of Corollary 9.3.1 The first relation in (9.3.13) follows from Theorem 9.3.2
and (9.3.12). The second follows from the asymptotic equivalence
</p>
<p>&int; &infin;
</p>
<p>x
</p>
<p>e&minus;
u2
</p>
<p>2 du&sim; e
&minus;x2/2
</p>
<p>x
,
</p>
<p>which is easy to establish, using, for example, l&rsquo;Hospital&rsquo;s rule. �</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Large Deviation Probabilities in the Cram&eacute;r Range 261
</p>
<p>9.3.2 Local Theorems
</p>
<p>In this subsection we will obtain analogues of the local Theorems 8.7.2 and 8.7.3 for
</p>
<p>large deviations in the Cram&eacute;r range. To simplify the exposition, we will formulate
</p>
<p>the theorem for densities, assuming that the following condition is satisfied:
</p>
<p>[D] The distribution F has a bounded density f (x) such that
</p>
<p>f (x) = e&minus;λ+x+o(x) as x &rarr;&infin;, if λ+ &lt;&infin;; (9.3.14)
f (x) &le; ce&minus;λx for any fixed λ &gt; 0, c= c(λ), if λ+ =&infin;. (9.3.15)
</p>
<p>Since inequalities of the form (9.3.14) and (9.3.15) always hold, by the exponen-
</p>
<p>tial Chebyshev inequality, for the right tails
</p>
<p>F+(x)=
&int; &infin;
</p>
<p>x
</p>
<p>f (u)du,
</p>
<p>condition [D] is not too restrictive. It only eliminates sharp &ldquo;bursts&rdquo; of f (x) as
</p>
<p>x &rarr;&infin;.
Denote by fn(x) the density of the distribution of Sn.
</p>
<p>Theorem 9.3.3 Let
</p>
<p>Eξ = 0, Eξ2 &lt;&infin;, λ+ &gt; 0, α =
x
</p>
<p>n
&isin; [0, α+),
</p>
<p>and condition [D] be met. Then
</p>
<p>fn(x)=
e&minus;nΛ(α)
</p>
<p>σα
&radic;
</p>
<p>2π n
</p>
<p>(
1 + o(1)
</p>
<p>)
,
</p>
<p>where the remainder term o(1) is uniform in α &isin; [0, α1] for any fixed α1 &isin; (0, α+).
</p>
<p>Proof The proof is based on Theorems 9.2.1 and 8.7.2A. Denote by f (α)n (x) the
density of the distribution of S
</p>
<p>(α)
n . Relation (9.2.3) implies that, for x = αn, α &isin;
</p>
<p>[α&minus;, α+], we have
</p>
<p>fn(x)= e&minus;λ(α)xψn
(
λ(α)
</p>
<p>)
f (α)n (x)= e&minus;nΛ(α)f (α)n (x). (9.3.16)
</p>
<p>Since Eξ (α) = α, we see that E(S(α)n &minus; x) = 0 and the density value f (α)n (x)
coincides with the density of the distribution of the sum S
</p>
<p>(α)
n &minus; αn at the point 0. In
</p>
<p>order to use Theorems 8.7.1A and 8.7.2A, we have to verify conditions (a) and (b)
</p>
<p>for θ2 =&infin; in these theorems and also the uniform boundedness in α &isin; [0, α1] of
&int; ∣∣ϕ(λ(α))(t)
</p>
<p>∣∣mdt (9.3.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>262 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>for some integer m &ge; 1, where ϕ(λ(α)) is the ch.f. of ξ (α) (the uniform version of
condition (c) in Theorem 8.7.2). By condition [D] the density
</p>
<p>f (α)(v)= e
λ(α)vf (v)
</p>
<p>ψ(λ(α))
</p>
<p>in bounded uniformly in α &isin; [0, α1] (for such α one has λ(α) &isin; [0, λ1], λ1 =
λ(α1) &lt; λ+). Hence the integral
</p>
<p>&int; (
f (α)(v)
</p>
<p>)2
dv
</p>
<p>is also uniformly bounded, and so, by virtue of Parseval&rsquo;s identity (see Sect. 7.2), is
</p>
<p>the integral
&int; ∣∣ϕ(λ(α))(t)
</p>
<p>∣∣2dt.
</p>
<p>This means that the required uniform boundedness of integral (9.3.17) is proved
</p>
<p>for m= 2.
Conditions (a) and (b) for θ2 &lt;&infin; were verified in the proof of Theorem 9.3.1. It
</p>
<p>remains to extend the verification of condition (b) to the case θ2 =&infin;. This can be
done by following an argument very similar to the one used in the proof of Theo-
</p>
<p>rem 9.3.1 in the case of finite θ2. Let θ2 =&infin;. If we assume that there exist sequences
λk &isin; [0, λ+,ε] and |tk| &ge; θ1 such that
</p>
<p>|ψ(λk + itk)|
ψ(λk)
</p>
<p>&rarr; 1,
</p>
<p>then, by compactness of [0, λ+,ε], there will exist sequences λ&prime;k &rarr; λ0 &isin; [0, λ+,ε]
and t &prime;k such that
</p>
<p>|ψ(λ&prime;k + it &prime;k)|
ψ(λ0)
</p>
<p>&rarr; 1. (9.3.18)
</p>
<p>But by virtue of condition [D] the family of functions ψ(λ+ it), t &isin;R, is equicon-
tinuous in λ &isin; [0, λ+,ε]. Therefore, along with (9.3.18), we also have convergence
</p>
<p>|ψ(λ0 + it &prime;k)|
ψ(λ0)
</p>
<p>&rarr; 1, |tk| &ge; θ1 &gt; 0,
</p>
<p>which contradicts the inequality
</p>
<p>sup
|t |&ge;θ1
</p>
<p>|ψ(λ0 + it)|
ψ(λ0)
</p>
<p>&lt; 1
</p>
<p>that follows from the existence of density.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Large Deviation Probabilities in the Cram&eacute;r Range 263
</p>
<p>Thus property (b) is proved for θ2 =&infin;, and we can use Theorem 8.7.2A, which
implies that
</p>
<p>f (α)n (x)=
1
</p>
<p>σ(λ(α))
&radic;
</p>
<p>2π n
</p>
<p>(
1 + o(1)
</p>
<p>)
.
</p>
<p>This, together with (9.3.16), proves Theorem 9.3.3. �
</p>
<p>Remark 9.3.1 We can see from the proof that, in Theorem 9.3.3, as a more gen-
eral condition instead of condition [D] one could also consider the integrability of
</p>
<p>ψm(λ+ it) for any fixed λ &isin; [0, λ1], λ1 &lt; λ+, or condition [D] imposed on Sm for
some m&ge; 1.
</p>
<p>For arithmetic distributions we cannot assume without loss of generality that
</p>
<p>m1 = Eξ = 0, but that does not change much in the formulations of the assertions.
If λ+ &gt; 0, then α+ = ψ &prime;(λ+)/ψ(λ+) &gt; m1 and the scaled deviations α = x/n for
the Cram&eacute;r range must lie in the region [m1, α+).
</p>
<p>Theorem 9.3.4 Let λ+ &gt; 0, Eξ2 &lt;&infin; and the distribution of ξ be arithmetic. Then,
for integer x,
</p>
<p>P(Sn = x)=
e&minus;nΛ(α)
</p>
<p>σα
&radic;
</p>
<p>2πn
</p>
<p>(
1 + o(1)
</p>
<p>)
,
</p>
<p>where the remainder term o(1) is uniform in α = x/n &isin; [m1, α1] for any fixed α1 &isin;
(m1, α+).
</p>
<p>A similar assertion is valid in the case when λ&minus; &lt; 0 and α &isin; (α&minus;,m1].
</p>
<p>Proof The proof does not differ much from that of Theorem 9.3.1. By (9.2.3),
</p>
<p>P(Sn = x)= e&minus;λ(α)xψ&minus;n
(
λ(α)
</p>
<p>)
P
(
S(α)n = x
</p>
<p>)
= e&minus;nΛ(α)P
</p>
<p>(
S(α)n = x
</p>
<p>)
,
</p>
<p>where Eξ (α) = α for α &isin; [m1, α+). In order to compute P(S(α)n = x) we have to
use Theorem 8.7.3A. The verification of conditions (a) and (b) of Theorem 8.7.1A,
</p>
<p>which are assumed to hold in Theorem 8.7.3A, is done in the same way as in the
</p>
<p>proof of Theorem 9.3.1, the only difference being that relation (9.3.4) for t0 &isin; [θ1,π]
will contradict the arithmeticity of the distribution of ξ . Since a(λ(α))= Eξ (α) = α,
by Theorem 8.7.3A we have
</p>
<p>P
(
S(α)n = x
</p>
<p>)
= 1
</p>
<p>σα
&radic;
</p>
<p>2πn
</p>
<p>(
1 + o(1)
</p>
<p>)
</p>
<p>uniformly in α = x/n &isin; [m1, α1]. The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>264 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>9.4 Integro-Local Theorems at the Boundary of the Cram&eacute;r
</p>
<p>Range
</p>
<p>9.4.1 Introduction
</p>
<p>In this section we again assume that Cram&eacute;r&rsquo;s condition λ+ &gt; 0 is met. If α+ =&infin;
then the theorems of Sect. 9.3 describe the large deviation probabilities for any
</p>
<p>α = x/n. But if α+ &lt; &infin; then the approaches of Sect. 9.3 do not enable one to
find the asymptotics of probabilities of large deviations of Sn for scaled deviations
</p>
<p>α = x/n in the vicinity of the point α+.
In this section we consider the case α+ &lt;&infin;. If in this case λ+ =&infin;, then, by
</p>
<p>property (Λ2)(i), we have α+ = s+ = sup{t : F+(t) &gt; 0}, and therefore the ran-
dom variables ξk are bounded from above by the value α+, P(Sn &ge; x) = 0 for
α = x/n &gt; α+. We will not consider this case in what follows. Thus we will study
the case α+ &lt;&infin;, λ+ &lt;&infin;.
</p>
<p>In the present and the next sections, we will confine ourselves to considering
</p>
<p>integro-local theorems in the non-lattice case with ∆=∆n &rarr; 0 since, as we saw in
the previous section, local theorems differ from the integro-local theorems only in
</p>
<p>that they are simpler. As in Sect. 9.3, the integral theorems can be easily obtained
</p>
<p>from the integro-local theorems.
</p>
<p>9.4.2 The Probabilities of Large Deviations of Sn in an
</p>
<p>o(n)-Vicinity of the Point α+n; the Case ψ
&prime;&prime;(λ+) &lt;&infin;
</p>
<p>In this subsection we will study the asymptotics of P(Sn &isin;∆[x)), x = αn, when α
lies in the vicinity of the point α+ &lt;&infin; and, moreover, ψ &prime;&prime;(λ+) &lt;&infin;. (The case of
distributions F, for which λ+ &lt;&infin;, α+ &lt;&infin; and ψ &prime;&prime;(λ+) &lt;&infin;, will be illustrated
later, in Lemma 9.4.1.) Under the above-mentioned conditions, the Cram&eacute;r trans-
</p>
<p>form F(λ+) is well defined at the point λ+, and the random variable ξ
(α+) with the
</p>
<p>distribution F(λ+) has mean α+ and a finite variance:
</p>
<p>Eξ (α+) = ψ
&prime;(λ+)
</p>
<p>ψ(λ+)
= α+, Var
</p>
<p>(
ξ (α+)
</p>
<p>)
= σ 2α+ =
</p>
<p>ψ &prime;&prime;(λ+)
</p>
<p>ψ(λ+)
&minus; α2+ (9.4.1)
</p>
<p>(cf. (9.3.1)).
</p>
<p>Theorem 9.4.1 Let ξ be a non-lattice random variable,
</p>
<p>λ+ &isin; (0,&infin;), ψ &prime;&prime;(λ+) &lt;&infin;, y = x &minus; α+n= o(n).
</p>
<p>If ∆n &rarr; 0 slowly enough as n&rarr;&infin; then
</p>
<p>P
(
Sn &isin;∆n[x)
</p>
<p>)
= ∆n
</p>
<p>σα+
&radic;
</p>
<p>2πn
e&minus;nΛ(α+)&minus;λ+y
</p>
<p>(
exp
</p>
<p>{
&minus; y
</p>
<p>2
</p>
<p>σ 2α+n
</p>
<p>}
+ o(1)
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Integro-Local Theorems at the Boundary of the Cram&eacute;r Range 265
</p>
<p>where
</p>
<p>α = x
n
, σ 2α+ =
</p>
<p>ψ &prime;&prime;(λ+)
</p>
<p>ψ(λ+)
&minus; α2+,
</p>
<p>and the remainder term o(1) is uniform in y.
</p>
<p>Proof As in the proof of Theorem 9.3.1, we use the Cram&eacute;r transform, but now at
the fixed point λ+, so there will be no triangular array scheme when analysing the
sums S
</p>
<p>(α+)
n . In this case the following analogue of Theorem 9.2.1 holds true.
</p>
<p>Theorem 9.2.1A Let λ+ &isin; (0,&infin;), α+ &lt;&infin; and y = x &minus; nα+. Then, for x = nα
and any fixed ∆&gt; 0, the following representation is valid:
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= e&minus;nΛ(α+)&minus;λ+y
</p>
<p>&int; ∆
</p>
<p>0
</p>
<p>e&minus;λ+zP
(
S
(α+)
n &minus; αn &isin; dz
</p>
<p>)
. (9.4.2)
</p>
<p>Proof of Theorem 9.2.1A repeats that of Theorem 9.2.1 the only difference being
that, as was already noted, the Cram&eacute;r transform is now applied at the fixed point λ+
which does not depend on α = x/n. In this case, by (9.2.3),
</p>
<p>P(Sn &isin; dv)= e&minus;λ+v+n lnψ(λ+)P
(
S
(α+)
n &isin; dv
</p>
<p>)
= e&minus;nΛ(α+)+λ+(α+n&minus;v)P
</p>
<p>(
S
(α+)
n &isin; dv
</p>
<p>)
.
</p>
<p>Integrating this equality in v from x to x +∆, changing the variable v = x + z
(x = nα), and noting that α+n&minus; v =&minus;y &minus; z, we obtain (9.4.2).
</p>
<p>The theorem is proved. �
</p>
<p>Let us return to the proof of Theorem 9.4.1. Assuming that ∆ = ∆n &rarr; 0, we
obtain, by Theorem 9.2.1A, that
</p>
<p>P
(
Sn &isin;∆n[x)
</p>
<p>)
= e&minus;nΛ(α+)&minus;λ+y P
</p>
<p>(
S
(α+)
n &minus; α+n &isin;∆n[y)
</p>
<p>)(
1 + o(1)
</p>
<p>)
. (9.4.3)
</p>
<p>By virtue of (9.4.1), we can apply Theorem 8.7.1 to evaluate the probability on
</p>
<p>the right-hand side of (9.4.3). This theorem implies that, as ∆n &rarr; 0 slowly enough,
</p>
<p>P
(
S
(α+)
n &minus; α+n &isin;∆n[y)
</p>
<p>)
= ∆n
</p>
<p>σα+
&radic;
n
φ
</p>
<p>(
y
</p>
<p>σα+
&radic;
n
</p>
<p>)
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
</p>
<p>= ∆n
σα+
</p>
<p>&radic;
2πn
</p>
<p>exp
</p>
<p>{
&minus; y
</p>
<p>2
</p>
<p>σ 2α+n
</p>
<p>}
+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
</p>
<p>uniformly in y. This, together with (9.4.3), proves Theorem 9.4.1. �</p>
<p/>
</div>
<div class="page"><p/>
<p>266 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>9.4.3 The Class of Distributions ER. The Probability of Large
</p>
<p>Deviations of Sn in an o(n)-Vicinity of the Point α+n for
</p>
<p>Distributions F from the Class ER in Case ψ &prime;&prime;(λ+)=&infin;
</p>
<p>When studying the asymptotics of P(Sn &ge; αn) (or P(Sn &isin;∆[αn))) in the case where
ψ &prime;&prime;(λ+) = &infin; and α is in the vicinity of the point α+ &lt; &infin;, we have to impose
additional conditions on the distribution F similarly to what was done in Sect. 8.8
</p>
<p>when studying convergence to stable laws.
</p>
<p>To formulate these additional conditions it will be convenient to introduce certain
</p>
<p>classes of distributions. If λ+ &lt;&infin;, then it is natural to represent the right tails F+(t)
as
</p>
<p>F+(t)= e&minus;λ+tV (t), (9.4.4)
</p>
<p>where, by the exponential Chebyshev inequality, V (t)= eo(t) as t &rarr;&infin;.
</p>
<p>Definition 9.4.1 We will say that the distribution F of a random variable ξ (or the
</p>
<p>random variable ξ itself) belongs to the class R if its right tail F+(t) is a regularly
varying function, i.e. can be represented as
</p>
<p>F+(t)= t&minus;βL(t), (9.4.5)
</p>
<p>where L is a slowly varying function as t &rarr;&infin; (see also Sect. 8.8 and Appendix 6).
</p>
<p>We will say that the distribution F (or the random variable ξ ) belongs to the
</p>
<p>class ER if, in the representation (9.4.4), the function V is regularly varying (which
</p>
<p>will also be denoted as V &isin;R).
Distributions from the class R have already appeared in Sect. 8.8.
</p>
<p>The following assertion explains which distributions from ER correspond to the
</p>
<p>cases α+ =&infin;, α+ &lt;&infin;, ψ &prime;&prime;(λ+)=&infin; and ψ &prime;&prime;(λ+) &lt;&infin;.
</p>
<p>Lemma 9.4.1 Let F &isin; ER. For α+ to be finite it is necessary and sufficient that
&int; &infin;
</p>
<p>1
</p>
<p>tV (t) dt &lt;&infin;.
</p>
<p>For ψ &prime;&prime;(λ+) to be finite, it is necessary and sufficient that
</p>
<p>&int; &infin;
</p>
<p>1
</p>
<p>t2V (t) dt &lt;&infin;.
</p>
<p>The assertion of the lemma means that α+ &lt;&infin; if β &gt; 2 in the representation
V (t)= t&minus;βL(t), where L is an s.v.f. and α+ =&infin; if β &lt; 2. For β = 2, the finiteness
of α+ is equivalent to the finiteness of
</p>
<p>&int;&infin;
1 t
</p>
<p>&minus;1L(t) dt . The same is true for the
finiteness of ψ &prime;&prime;(λ+).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Integro-Local Theorems at the Boundary of the Cram&eacute;r Range 267
</p>
<p>Proof of Lemma 9.4.1 We first prove the assertion concerning α+. Since
</p>
<p>α+ =
ψ &prime;(λ+)
</p>
<p>ψ(λ+)
,
</p>
<p>we have to estimate the values of ψ &prime;(λ+) and ψ(λ+). The finiteness of ψ &prime;(λ+) is
equivalent to that of
</p>
<p>&minus;
&int; &infin;
</p>
<p>1
</p>
<p>teλ+tdF+(t)=
&int; &infin;
</p>
<p>1
</p>
<p>t
(
λ+V (t) dt &minus; dV (t)
</p>
<p>)
, (9.4.6)
</p>
<p>where, for V (t)= o(1/t),
</p>
<p>&minus;
&int; &infin;
</p>
<p>1
</p>
<p>t dV (t)= V (1)+
&int; &infin;
</p>
<p>1
</p>
<p>V (t) dt.
</p>
<p>Hence the finiteness of the integral on the left-hand side of (9.4.6) is equivalent to
</p>
<p>that of the sum
</p>
<p>λ+
</p>
<p>&int; &infin;
</p>
<p>1
</p>
<p>tV (t) dt +
&int; &infin;
</p>
<p>1
</p>
<p>V (t) dt
</p>
<p>or, which is the same, to the finiteness of the integral
&int;&infin;
</p>
<p>1 tV (t) dt . Similarly we see
</p>
<p>that the finiteness of ψ(λ+) is equivalent to that of
&int;&infin;
</p>
<p>1
V (t) dt . This implies the
</p>
<p>assertion of the lemma in the case
&int;&infin;
</p>
<p>1 V (t) dt &lt;&infin;, where one has V (t)= o(1/t).
If
</p>
<p>&int;&infin;
1 V (t) dt = &infin;, then ψ(λ+) = &infin;, lnψ(λ) &rarr; &infin; as λ &uarr; λ+ and hence α+ =
</p>
<p>limλ&uarr;λ+(lnψ(λ))
&prime; =&infin;.
</p>
<p>The assertion concerning ψ &prime;&prime;(λ+) can be proved in exactly the same way. The
lemma is proved. �
</p>
<p>The lemma implies the following:
</p>
<p>(a) If β &lt; 2 or β = 2 and
&int;&infin;
</p>
<p>1 t
&minus;1L(t)=&infin;, then α+ =&infin; and the theorems of the
</p>
<p>previous section are applicable to P(Sn &ge; x).
(b) If β &gt; 3 or β = 3 and
</p>
<p>&int;&infin;
1 t
</p>
<p>&minus;1L(t) dt &lt;&infin;, then α+ &lt;&infin;, ψ &prime;&prime;(λ+) &lt;&infin; and
we can apply Theorem 9.4.1.
</p>
<p>It remains to consider the case
</p>
<p>(c) β &isin; [2,3], where the integral
&int;&infin;
</p>
<p>1 t
&minus;1L(t) dt is finite for β = 2 and is infinite for
</p>
<p>β = 3.
</p>
<p>It is obvious that in case (c) we have α+ &lt;&infin; and ψ &prime;&prime;(λ+)=&infin;.
Put
</p>
<p>V+(t) :=
λ+tV (t)
</p>
<p>βψ(λ+)
, b(n) := V (&minus;1)+
</p>
<p>(
1
</p>
<p>n
</p>
<p>)
,
</p>
<p>where V
(&minus;1)
+ (1/n) is the value of the function inverse to V+ at the point 1/n.</p>
<p/>
</div>
<div class="page"><p/>
<p>268 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>Theorem 9.4.2 Let ξ be a non-lattice random variable, F &isin; ER and condition (c)
hold. If ∆n &rarr; 0 slowly enough as n&rarr;&infin;, then, for y = x&minus;α+n= o(n),
</p>
<p>P
(
Sn &isin;∆n[x)
</p>
<p>)
= ∆ne
</p>
<p>&minus;nΛ(α+)&minus;λ+y
</p>
<p>b(n)
</p>
<p>(
f (β&minus;1,1)
</p>
<p>(
y
</p>
<p>b(n)
</p>
<p>)
+ o(1)
</p>
<p>)
,
</p>
<p>where f (β&minus;1,1) is the density of the stable law F(β&minus;1,1) with parameters β &minus; 1,1,
and the remainder term o(1) is uniform in y.
</p>
<p>We will see from the proof of the theorem that studying the probabilities of large
</p>
<p>deviations in the case where α+ &lt; &infin; and ψ &prime;&prime;(λ+) = &infin; is basically impossible
outside the class ER, since it is impossible to find theorems on the limiting distribu-
</p>
<p>tion of Sn in the case Var(ξ)=&infin; without the conditions [Rγ,ρ] of Sect. 8.8 being
satisfied.
</p>
<p>Proof of Theorem 9.4.2 Condition (c) implies that α+ = Eξ (α+) &lt; &infin; and
Var(ξ (α+))=&infin;. We will use Theorem 9.2.1A. For ∆n &rarr; 0 slowly enough we will
obtain, as in the proof of Theorem 9.4.1, that relation (9.4.3) holds true. But now,
</p>
<p>in contrast to Theorem 9.4.1, in order to calculate the probability on the right-hand
</p>
<p>side of (9.4.3), we have to employ the integro-local Theorem 8.8.3 on convergence
</p>
<p>to a stable law. In our case, by the properties of r.v.f.s, one has
</p>
<p>P
(
ξ (α+) &ge; t
</p>
<p>)
= &minus; 1
</p>
<p>ψ(λ+)
</p>
<p>&int; &infin;
</p>
<p>t
</p>
<p>eλ+udF+(u)=
1
</p>
<p>ψ(λ+)
</p>
<p>&int; &infin;
</p>
<p>t
</p>
<p>(
λ+V (u)du&minus; dV (u)
</p>
<p>)
</p>
<p>= λ+
βψ(λ+)
</p>
<p>t&minus;β+1L+(t)&sim; V+(t), (9.4.7)
</p>
<p>where L+(t)&sim; L(t) is a slowly varying function. Moreover, the left tail of the distri-
bution F(α+) decays at least exponentially fast. By virtue of the results of Sect. 8.8,
</p>
<p>this means that, for b(n) = V (&minus;1)+ (1/n), we have convergence of the distributions
of
</p>
<p>S
(α+)
n &minus;α+n
</p>
<p>b(n)
to the stable law Fβ&minus;1,1 with parameters β &minus; 1 &isin; [1,2] and 1. It re-
</p>
<p>mains to use representation (9.4.3) and Theorem 8.8.3 which implies that, provided
</p>
<p>∆n &rarr; 0 slowly enough, one has
</p>
<p>P
(
S
(α+)
n &minus; α+n &isin;∆n[y)
</p>
<p>)
= ∆n
</p>
<p>b(n)
f (β&minus;1,1)
</p>
<p>(
y
</p>
<p>b(n)
</p>
<p>)
+ o
</p>
<p>(
1
</p>
<p>b(n)
</p>
<p>)
</p>
<p>uniformly in y. The theorem is proved. �
</p>
<p>Theorem 9.4.2 concludes the study of probabilities of large deviations of Sn/n
</p>
<p>in the vicinity of the point α+ for distributions from the class ER.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 Integral and Integro-Local Theorems on Large Deviation Probabilities 269
</p>
<p>9.4.4 On the Large Deviation Probabilities in the Range α &gt; α+
for Distributions from the Class ER
</p>
<p>Now assume that the deviations x of Sn are such that α = x/n &gt; α+, and y = x &minus;
α+n grows fast enough (faster than
</p>
<p>&radic;
n under the conditions of Theorem 9.4.1 and
</p>
<p>faster than b(n) under the conditions of Theorem 9.4.2). Then, for the probability
</p>
<p>P
(
S(α+) &minus; α+n &isin;∆n[y)
</p>
<p>)
, (9.4.8)
</p>
<p>the deviations y (see representation (9.4.3)) will belong to the zone of large devi-
</p>
<p>ations, so applying Theorems 8.7.1 and 8.8.3 to evaluate such probabilities does
</p>
<p>not make much sense. Relation (9.4.7) implies that, in the case F &isin; ER, we have
F(α+) &isin;R. Therefore, we will know the asymptotics of the probability (9.4.8) (and
hence also of the probability P(Sn &isin;∆n[x)), see (9.4.3)) if we obtain integro-local
theorems for the probabilities of large deviations of the sums Sn, in the case where
</p>
<p>the summands belong to the class R. Such theorems are also of independent inter-
</p>
<p>est in the present chapter, and the next section will be devoted to them. After that,
</p>
<p>in Sect. 9.6 we will return to the problem on large deviation probabilities in the
</p>
<p>class ER mentioned in the title of this section.
</p>
<p>9.5 Integral and Integro-Local Theorems on Large Deviation
</p>
<p>Probabilities for Sums Sn when the Cram&eacute;r Condition Is not
</p>
<p>Met
</p>
<p>If Eξ = 0 and the right-side Cram&eacute;r condition is not met (λ+ = 0), then the rate
function Λ(α) degenerates on the right semiaxis: Λ(α)= λ(α)= 0 for α &ge; 0, and
the results of Sects. 9.1&ndash;9.4 on the probabilities of large deviations of Sn are of little
</p>
<p>substance. In this case, in order to find the asymptotics of P(Sn &ge; x) and P(Sn &isin;
∆[x)), we need completely different approaches, while finding these asymptotics is
only possible under additional conditions on the behaviour of the tail F+(t) of the
distribution F, similarly to what happened in Sect. 8.8 when studying convergence
</p>
<p>to stable laws.
</p>
<p>The above-mentioned additional conditions consist of the assumption that the tail
</p>
<p>F+(t) behaves regularly enough. In this section we will assume that F+(t)= V (t) &isin;
R, where R is the class of regularly varying functions introduced in the previous
</p>
<p>section (see also Appendix 6). To make the exposition more homogeneous, we will
</p>
<p>confine ourselves to the case β &gt; 2, Var(ξ) &lt;&infin;, where &minus;β is the power exponent
in the function V &isin;R (see (9.4.5)). Studying the case β &isin; [1,2] (Var(ξ)=&infin;) does
not differ much from the exposition below, but it would significantly increase the
</p>
<p>volume of the exposition and complicate the text, and therefore is omitted. Results
</p>
<p>for the case β &isin; (0,2] can be found in [8, Chap. 3].</p>
<p/>
</div>
<div class="page"><p/>
<p>270 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>9.5.1 Integral Theorems
</p>
<p>Integral theorems for probabilities of large deviations of Sn and maxima Sn =
maxk&le;n Sk in the case Eξ = 0, Var(ξ) &lt;&infin;, F &isin;R, β &gt; 2, follow immediately from
the bounds obtained in Appendix 8. In particular, Corollaries A8.2.1 and A8.3.1 of
</p>
<p>Appendix 8 imply the following result.
</p>
<p>Theorem 9.5.1 Let Eξ = 0, Var(ξ) &lt;&infin;, F &isin;R and β &gt; 2. Then, for x ≫
&radic;
n lnn,
</p>
<p>P(Sn &ge; x)&sim; P(Sn &ge; x)&sim; nV (x). (9.5.1)
</p>
<p>Under an additional condition [D0] to be introduced below, the assertion of this
</p>
<p>theorem will also follow from the integro-local Theorem 9.5.2 (see below).
</p>
<p>Comparing Theorem 9.5.1 with the results of Sects. 9.2&ndash;9.4 shows that the nature
</p>
<p>of the large deviation probabilities is completely different here. Under the Cram&eacute;r
</p>
<p>condition and for α = x/n &isin; (0, α+), the large deviations of Sn are, roughly speak-
ing, &ldquo;equally contributed to by all the summands&rdquo; ξk , k &le; n. This is confirmed by
the fact that, for a fixed α, the limiting conditional distribution of ξk , k &le; n, given
that Sn &isin;∆[x) (or Sn &ge; x) for x = αn, ∆= 1, as n&rarr;&infin; coincides with the distri-
bution F(α) of the random variable ξ (α). The reader can verify this himself/herself
</p>
<p>using Theorem 9.3.2. In other words, the conditions {Sn &isin; ∆[x)} (or {Sn &ge; x}),
x = αn, change equally (from F to F(α)) the distributions of all the summands.
</p>
<p>However, if the Cram&eacute;r condition is not met, then under the conditions of The-
</p>
<p>orem 9.5.1 the large deviations of Sn are essentially due to one large (comparable
</p>
<p>with x) jump. This is seen from the fact that the value of nV (x) on the right-hand
</p>
<p>side of (9.5.1) is nothing else but the main term of the asymptotics for P(ξn &ge; x),
where ξn = maxk&le;n ξk . Indeed, if nV (x)&rarr; 0 then
</p>
<p>P(ξn &lt; x) =
(
1 &minus; V (x)
</p>
<p>)n = 1 &minus; nV (x)+O
((
nV (x)
</p>
<p>)2)
,
</p>
<p>P(ξn &ge; x) = nV (x)+O
((
nV (x)
</p>
<p>)2)&sim; nV (x).
</p>
<p>In other words, the probabilities of large deviations of Sn, Sn and ξn are asymp-
</p>
<p>totically the same. The fact that the probabilities of the events {ξj &ge; y} for y &sim; x
play the determining role in finding the asymptotics of P(Sn &ge; x) can easily be
discovered in the bounds from Appendix 8.
</p>
<p>Thus, while the asymptotics of P(Sn &ge; x) for x = αn≫
&radic;
n in the Cram&eacute;r case
</p>
<p>is determined by &ldquo;the whole distribution F&rdquo; (as the rate function Λ(α) depends on
</p>
<p>the &ldquo;the whole distribution F&rdquo;), these asymptotics in the case F &isin;R are determined
by the right tail F+(t) = V (t) only and do not depend on the &ldquo;remaining part&rdquo; of
the distribution F (for the fixed value of Eξ = 0).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 Integral and Integro-Local Theorems on Large Deviation Probabilities 271
</p>
<p>9.5.2 Integro-Local Theorems
</p>
<p>In this section we will study the asymptotics of P(Sn &isin;∆[x)) in the case where
</p>
<p>Eξ = 0, Var ξ2 &lt;&infin;, F &isin;R, β &gt; 2, x ≫
&radic;
n lnn. (9.5.2)
</p>
<p>These asymptotics are of independent interest and are also useful, for example, in
</p>
<p>finding the asymptotics of integrals of type E(g(Sn); Sn &ge; x) for x ≫
&radic;
n lnn for
</p>
<p>a wide class of functions g. As was already noted (see Subsection 4.4), in the next
</p>
<p>section we will use the results from the present section to obtain integro-local theo-
</p>
<p>rems under the Cram&eacute;r condition (for summands from the class ER) for deviations
</p>
<p>outside the Cram&eacute;r zone.
</p>
<p>In order to obtain integro-local theorems in this section, we will need additional
</p>
<p>conditions. Besides condition F &isin;R, we will also assume that the following holds:
</p>
<p>Condition [D0] For each fixed ∆, as t &rarr;&infin;,
</p>
<p>V (t)&minus; V (t +∆)= v(t)
(
∆+ o(1)
</p>
<p>)
, v(t)= βV (t)
</p>
<p>t
.
</p>
<p>It is clear that if the function L(t) in representation (9.4.5) (or the function V (t))
</p>
<p>is differentiable for t large enough and L&prime;(t)= o(L(t)/t) as t &rarr;&infin; (all sufficiently
smooth s.v.f.s possess this property; cf. e.g., polynomials of ln t etc.), then condi-
</p>
<p>tion [D0] will be satisfied, and the derivative &minus;V &prime;(t)&sim; v(t) will play the role of the
function v(t).
</p>
<p>Theorem 9.5.2 Let conditions (9.5.2) and [D0] be met. Then
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
=∆nv(x)
</p>
<p>(
1 + o(1)
</p>
<p>)
, v(x)= βV (x)
</p>
<p>x
,
</p>
<p>where the remainder term o(1) is uniform in x &ge; N
&radic;
n lnn and ∆ &isin; [∆1,∆2] for
</p>
<p>any fixed ∆2 &gt;∆1 &gt; 0 and any fixed sequence N&rarr;&infin;.
</p>
<p>Note that in Theorems 9.5.1 and 9.5.2 we do not assume that n&rarr;&infin;. The as-
sumption that x &rarr;&infin; is contained in (9.5.2).
</p>
<p>Proof For y &lt; x, introduce the events
</p>
<p>Gn :=
{
Sn &isin;∆[x)
</p>
<p>}
, Bj := {ξj &lt; y}, B :=
</p>
<p>n⋂
</p>
<p>j=1
Bj . (9.5.3)
</p>
<p>Then
</p>
<p>P(Gn)= P(GnB)+ P(GnB), B =
n⋃
</p>
<p>j=1
Bj , (9.5.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>272 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>where
</p>
<p>n&sum;
</p>
<p>j=1
P(GnBj )&ge; P(GnB)&ge;
</p>
<p>n&sum;
</p>
<p>j=1
P(GnBj )&minus;
</p>
<p>&sum;
</p>
<p>i&lt;j&le;n
P(GnBiBj ) (9.5.5)
</p>
<p>(see property 8 in Sect. 9.2.2).
</p>
<p>The proof is divided into three stages: the bounding of P(GnB), that of
</p>
<p>P(GnB iBj ), i 
= j , and the evaluation of P(GnBj ).
(1) A bound on P(GnB). We will make use of the rough inequality
</p>
<p>P(GnB)&le; P(Sn &ge; x;B) (9.5.6)
</p>
<p>and Theorem A8.2.1 of Appendix 8 which implies that, for x = ry with a fixed
r &gt; 2, any δ &gt; 0, and x &ge;N
</p>
<p>&radic;
n lnn, N &rarr;&infin;, we have
</p>
<p>P(Sn &ge; x;B)&le;
(
nV (y)
</p>
<p>)r&minus;δ
. (9.5.7)
</p>
<p>Here we can always choose r such that
</p>
<p>(
nV (x)
</p>
<p>)r&minus;δ ≪ n∆v(x) (9.5.8)
</p>
<p>for x ≫&radic;n. Indeed, putting n := x2 and comparing the powers of x on the right-
hand and left-hand sides of (9.5.8), we obtain that for (9.5.8) to hold it suffices to
</p>
<p>choose r such that
</p>
<p>(2 &minus; β)(r &minus; δ) &lt; 1 &minus; β,
</p>
<p>which is equivalent, for β &gt; 2, to the inequality.
</p>
<p>r &gt;
β &minus; 1
β &minus; 2 .
</p>
<p>For such r , we will have that, by (9.5.6)&ndash;(9.5.8),
</p>
<p>P(GnB)= o
(
n∆v(x)
</p>
<p>)
. (9.5.9)
</p>
<p>Since r &minus; δ &gt; 1, we see that, for n≪ x2, relations (9.5.8) and (9.5.9) will hold true
all the more.
</p>
<p>(2) A bound for P(GnB iBj ). It is sufficient to bound P(GnBn&minus;1Bn). Set
</p>
<p>δ := 1
r
&lt;
</p>
<p>1
</p>
<p>2
, Hk :=
</p>
<p>{
v : v &lt; (1 &minus; kδ)x +∆
</p>
<p>}
, k = 1,2.
</p>
<p>Then
</p>
<p>P(GnBn&minus;1Bn) =
&int;
</p>
<p>H2
</p>
<p>P(Sn&minus;2 &isin; dz)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 Integral and Integro-Local Theorems on Large Deviation Probabilities 273
</p>
<p>&times;
&int;
</p>
<p>H1
</p>
<p>P(z+ ξ &isin; dv, ξ &ge; δx)P
(
v+ ξ &isin;∆[x), ξ &ge; δx
</p>
<p>)
.
</p>
<p>(9.5.10)
</p>
<p>Since in the domain H1 we have x &minus; v &gt; δx &minus;∆, the last factor on the right-hand
side of (9.5.10) has, by condition [D0], the form ∆v(x &minus; v)(1 + o(1))&le; c∆v(x) as
x &rarr;&infin;, so the integral over H1 in (9.5.10), for x large enough, does not exceed
</p>
<p>c∆v(x)P(z+ ξ &isin;H1; ξ &ge; δx)&le; c∆v(x)V (δx).
</p>
<p>The integral over the domain H2 in (9.5.10) evidently allows a similar bound. Since
</p>
<p>nV (x)&rarr; 0, we obtain that
&sum;
</p>
<p>i&lt;j&le;n
P(GnBiBj )&le; c1∆n2v(x)V (x)= o
</p>
<p>(
∆nv(x)
</p>
<p>)
. (9.5.11)
</p>
<p>(3) The evaluation of P(GnBj ) is based on the relation
</p>
<p>P(GnBn)=
&int;
</p>
<p>H1
</p>
<p>P(Sn&minus;1 &isin; dz)P
(
ξ &isin;∆[x &minus; z), ξ &ge; δx
</p>
<p>)
</p>
<p>&le;
&int;
</p>
<p>H1
</p>
<p>P(Sn&minus;1 &isin; dz)P
(
ξ &isin;∆[x &minus; z)
</p>
<p>)
</p>
<p>=∆
&int;
</p>
<p>H1
</p>
<p>P(Sn&minus;1 &isin; dz)v(x &minus; z)
(
1 + o(1)
</p>
<p>)
, (9.5.12)
</p>
<p>which yields
</p>
<p>P(GnBn) &le; ∆E
[
v(x &minus; Sn&minus;1);Sn&minus;1 &lt; (1 &minus; δ)x +∆
</p>
<p>](
1 + o(1)
</p>
<p>)
</p>
<p>= ∆v(x)
(
1 + o(1)
</p>
<p>)
. (9.5.13)
</p>
<p>The last relation is valid for x ≫ &radic;n, since, by Chebyshev&rsquo;s inequality, E[v(x &minus;
Sn&minus;1); |Sn&minus;1| &le;M
</p>
<p>&radic;
n] &sim; v(x) as M &rarr;&infin;, M&radic;n = o(x) and, moreover, the fol-
</p>
<p>lowing evident bounds hold:
</p>
<p>E
[
v(x &minus; Sn&minus;1);Sn&minus;1 &isin;
</p>
<p>(
M
&radic;
n, (1 &minus; δ)x +∆
</p>
<p>)]
= o
</p>
<p>(
v(x)
</p>
<p>)
,
</p>
<p>E
[
v(x &minus; Sn&minus;1); Sn&minus;1 &isin; (&minus;&infin;,&minus;M
</p>
<p>&radic;
n )
</p>
<p>]
= o
</p>
<p>(
v(x)
</p>
<p>)
</p>
<p>as M &rarr;&infin;.
Similarly, by (virtue of (9.5.12)) we get
</p>
<p>P(GnBn)&ge;
&int; (1&minus;δ)x
</p>
<p>&minus;&infin;
P(Sn&minus;1 &isin; dz)P
</p>
<p>(
ξ &isin;∆[x &minus; z)
</p>
<p>)
&sim;∆v(x). (9.5.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>274 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>From (9.5.13) and (9.5.14) we obtain that
</p>
<p>P(GnBn)=∆v(x)
(
1 + o(1)
</p>
<p>)
.
</p>
<p>This, together with (9.5.4), (9.5.9) and (9.5.11), yields the representation
</p>
<p>P(Gn)=∆nv(x)
(
1 + o(1)
</p>
<p>)
.
</p>
<p>The required uniformity of the term o(1) clearly follows from the preceding argu-
</p>
<p>ment. The theorem is proved. �
</p>
<p>Theorem 9.5.2 implies the following
</p>
<p>Corollary 9.5.1 Let the conditions of Theorem 9.5.2 be satisfied. Then there exists
a fixed sequence ∆N converging to zero slowly enough as N &rarr; &infin; such that the
assertion of Theorem 9.5.2 remains true when the segment [∆1,∆2] is replaced in
it with [∆N ,∆2].
</p>
<p>9.6 Integro-Local Theorems on the Probabilities of Large
</p>
<p>Deviations of Sn Outside the Cram&eacute;r Range (Under the
</p>
<p>Cram&eacute;r Condition)
</p>
<p>We return to the case where the Cram&eacute;r condition is met. In Sects. 9.3 and 9.4
</p>
<p>we obtained integro-local theorems for deviations inside and on the boundary of
</p>
<p>the Cram&eacute;r range. It remains to study the asymptotics of P(Sn &isin; ∆[x)) outside
the Cram&eacute;r range, i.e. for α = x/n &gt; α+. Preliminary observations concerning this
problem were made in Sect. 9.4.4 where it was reduced to integro-local theorems
</p>
<p>for the sums Sn when Cram&eacute;r&rsquo;s condition is not satisfied. Recall that in that case we
</p>
<p>had to restrict ourselves to considering distributions from the class ER defined in
</p>
<p>Sect. 9.4.3 (see (9.4.4)).
</p>
<p>Theorem 9.6.1 Let F &isin; ER, β &gt; 3, α = x/n &gt; α+ and y = x &minus; α+n≫
&radic;
n. Then
</p>
<p>there exists a fixed sequence∆N converging to zero slowly enough as N &rarr;&infin;, such
that
</p>
<p>P
(
Sn &isin;∆N [x)
</p>
<p>)
= e&minus;nΛ(α+)&minus;λ+yn∆Nv+(y)
</p>
<p>(
1 + o(1)
</p>
<p>)
</p>
<p>= e&minus;nΛ(α)n∆Nv+(y)
(
1 + o(1)
</p>
<p>)
,
</p>
<p>where v+(y)= λ+V (y)/ψ(λ+), the remainder term o(1) is uniform in x and n such
that y ≫N
</p>
<p>&radic;
n lnn, N being an arbitrary fixed sequence tending to &infin;.
</p>
<p>Proof By Theorem 9.2.1A there exists a sequence ∆N converging to zero slowly
enough such that (cf. (9.4.3))
</p>
<p>P
(
Sn &isin;∆N [x)
</p>
<p>)
= e&minus;nΛ(α+)&minus;λ+y P
</p>
<p>(
S
(α+)
n &minus; α+n &isin;∆N [y)
</p>
<p>)
. (9.6.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.6 Large Deviations of Sn Outside the Cram&eacute;r Range 275
</p>
<p>Since by properties (Λ1) and (Λ2) the function Λ(α) is linear for α &gt; α+:
</p>
<p>Λ(α)=Λ(α+)+ (α &minus; α+)λ+,
</p>
<p>the exponent in (9.6.1) can be rewritten as
</p>
<p>&minus;nΛ(α+)&minus; λ+y =&minus;nΛ(α).
</p>
<p>The right tail of the distribution of ξ (α+) has the form (see (9.4.7))
</p>
<p>P
(
ξ (α+) &ge; t
</p>
<p>)
= λ+
</p>
<p>ψ(λ+)
</p>
<p>&int; &infin;
</p>
<p>t
</p>
<p>V (u)du+ V (t).
</p>
<p>By the properties of regularly varying functions (see Appendix 6),
</p>
<p>V (t)&minus; V (t &minus; u)= o(
(
V (t)
</p>
<p>)
</p>
<p>as t &rarr;&infin; for any fixed u. This implies that condition [D0] of Sect. 9.5 is satisfied
for the distribution of ξ (α+).
</p>
<p>This means that, in order to calculate the probability on the right-hand side
</p>
<p>of (9.6.1), we can use Theorem 9.5.2 and Corollary 9.5.1, by virtue of which, as
</p>
<p>∆N &rarr; 0 slowly enough,
</p>
<p>P
(
S
(α+)
n &minus; α+n &isin;∆N [y)
</p>
<p>)
= n∆Nv+(y)
</p>
<p>(
1 + o(1)
</p>
<p>)
,
</p>
<p>where the remainder term o(1) is uniform in all x and n such that y ≫ N
&radic;
n lnn,
</p>
<p>N &rarr;&infin;.
The theorem is proved. �
</p>
<p>Since P(Sn &isin; ∆N [x)) decreases exponentially fast as x (or y) grows (note the
factor e&minus;λ+y in (9.6.1)), Theorem 9.6.1 immediately implies the following integral
theorem.
</p>
<p>Corollary 9.6.1 Under the conditions of Theorem 9.6.1,
</p>
<p>P(Sn &ge; x)= e&minus;nΛ(α)
nV (y)
</p>
<p>ψ(λ+)
</p>
<p>(
1 + o(1)
</p>
<p>)
.
</p>
<p>Proof Represent the probability P(Sn &ge; x) as the sum
</p>
<p>P(Sn &ge; x) =
&infin;&sum;
</p>
<p>k=0
P
(
Sn &isin;∆N [x + k∆N )
</p>
<p>)
</p>
<p>&sim; e&minus;nΛ(α) nλ+
ψ(λ+)
</p>
<p>&infin;&sum;
</p>
<p>k=0
∆NV (y +∆Nk)e&minus;λ+∆N k.</p>
<p/>
</div>
<div class="page"><p/>
<p>276 9 Large Deviation Probabilities for Sums of Independent Random Variables
</p>
<p>Here the series on the right-hand side is asymptotically equivalent, as N &rarr;&infin;, to
the integral
</p>
<p>V (y)
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>e&minus;λ+tdt = V (y)
λ+
</p>
<p>.
</p>
<p>The corollary is proved. �
</p>
<p>Note that a similar corollary (i.e. the integral theorem) can be obtained under the
</p>
<p>conditions of Theorem 9.4.2 as well.
</p>
<p>In the range of deviations α = x
n
&gt; α+, only the case F &isin; ER, β &isin; [2,3] (recall
</p>
<p>that α+ =&infin; for β &lt; 2) has not been considered in this text. As we have already
said, it could also be considered, but that would significantly increase the length and
</p>
<p>complexity of the exposition. Results dealing with this case can be found in [8]; one
</p>
<p>can also find there a more complete study of large deviation probabilities.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
</p>
<p>Renewal Processes
</p>
<p>Abstract This is the first chapter in the book to deal with random processes in con-
</p>
<p>tinuous time, namely, with the so-called renewal processes. Section 10.1 establishes
</p>
<p>the basic terminology and proves the integral renewal theorem in the case of non-
</p>
<p>identically distributed random variables. The classical Key Renewal Theorem in the
</p>
<p>arithmetic case is proved in Sect. 10.2, including its extension to the case where
</p>
<p>random variables can assume negative values. The limiting behaviour of the excess
</p>
<p>and defect of a random walk at a growing level is established in Sect. 10.3. Then
</p>
<p>these results are extended to the non-arithmetic case in Sect. 10.4. Section 10.5 is
</p>
<p>devoted to the Law of Large Numbers and the Central Limit Theorem for renewal
</p>
<p>processes. It also contains the proofs of these laws for the maxima of sums of in-
</p>
<p>dependent non-identically distributed random variables that can take values of both
</p>
<p>signs, and a local limit theorem for the first hitting time of a growing level. The chap-
</p>
<p>ter ends with Sect. 10.6 introducing generalised (compound) renewal processes and
</p>
<p>establishing for them the Central Limit Theorem, in both integral and integro-local
</p>
<p>forms.
</p>
<p>10.1 Renewal Processes. Renewal Functions
</p>
<p>10.1.1 Introduction
</p>
<p>The sequence of sums of random variables {Sn}, considered in previous chapters, is
often called a random walk. It can be considered as the simplest random process in
discrete time n. The further study of such processes is contained in Chaps. 11, 12
and 20.
</p>
<p>In this chapter we consider the simplest processes in continuous time t that are
also entirely determined by a sequence of independent random variables and do
</p>
<p>not require, for their construction, any special structures (in the general case such
</p>
<p>constructions will be needed; see Chap. 18).
</p>
<p>Let τ1, {τj }&infin;j=2 be a sequence of independent random variables given on a prob-
ability space 〈Ω,F,P〉 (here we change our conventional notations ξj to τj for rea-
sons that will become clear in Sect. 10.6, where ξj appear again). For the random
</p>
<p>variables τ2, τ3, . . . we will usually assume some homogeneity property: proximity
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_10, &copy; Springer-Verlag London 2013
</p>
<p>277</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_10">http://dx.doi.org/10.1007/978-1-4471-5201-9_10</a></div>
</div>
<div class="page"><p/>
<p>278 10 Renewal Processes
</p>
<p>of the expectations or identical distributions. The random variable τ1 can be arbi-
</p>
<p>trary.
</p>
<p>Definition 10.1.1 A renewal process is a collection of random variables η(t) de-
pending on a parameter t and defined on 〈Ω,F,P〉 by the equality
</p>
<p>η(t) := min{k &ge; 0 : Tk &gt; t}, t &ge; 0, (10.1.1)
</p>
<p>where
</p>
<p>Tk :=
k&sum;
</p>
<p>j=1
τj , T0 := 0.
</p>
<p>The variables η(t) are not completely defined yet. We do not know what η(t) is
</p>
<p>for ω such that the level t is never reached by the sequence of sums Tk . In that case
</p>
<p>it is natural to put
</p>
<p>η(t) :=&infin; if all Tk &le; t. (10.1.2)
</p>
<p>Clearly, η(t) is a stopping time (see Sect. 4.4).
</p>
<p>Usually the random variables τ2, τ3, . . . are assumed to be identically distributed
</p>
<p>with a finite expectation. The distribution of the random variable τ1 can be arbitrary.
</p>
<p>We assume first that all the random variables τj are positive. Then definition
(10.1.1) allows us to consider η(t) as a random function that can be described
</p>
<p>as follows. If we plot the points T0 = 0, T1, T2, . . . on the real line, then one has
η(t)= 0 on the semi-axis (&minus;&infin;,0), η(t)= 1 on the semi-interval [0, T1), η(t)= 2
on the semi-interval [T1, T2) and so on.
</p>
<p>The sequence {Tk}&infin;k=0 is also often called a renewal process. Sometimes we will
call the sequence {Tk} a random walk. The quantity η(t) can also be called the first
passage time of the level t by the random walk {Tk}&infin;k=0.
</p>
<p>If, based on the sequence {Tk}, we construct a random walk T (x) in continuous
time:
</p>
<p>T (x) := Tk for x &isin; [k, k + 1), k &ge; 0,
</p>
<p>then the renewal process η(t) will be the generalised inverse of T (x):
</p>
<p>η(t)= inf
{
x &ge; 0 : T (x) &gt; t
</p>
<p>}
.
</p>
<p>The term &ldquo;renewal process&rdquo; is related to the fact that the function η(t) and the
</p>
<p>sequence {Tk} are often used to describe the operation of various physical devices
comprising replaceable components. If, say, τj is the failure-free operating time
</p>
<p>of such a component, after which the latter requires either replacement or repair
</p>
<p>(&ldquo;renewal&rdquo;, which is supposed to happen immediately), then Tk will denote the time
</p>
<p>of the k-th &ldquo;renewal&rdquo; of the component, while η(t) will be equal to the number of
</p>
<p>&ldquo;renewals&rdquo; which have occurred by the time t .</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Renewal Processes. Renewal Functions 279
</p>
<p>Remark 10.1.1 If the j -th renewal of the component does not happen immediately
but requires a time τ &prime;j &ge; 0, then, introducing the random variables
</p>
<p>τ &lowast;j := τj + τ &prime;j , T &lowast;k :=
k&sum;
</p>
<p>j=1
τ &lowast;j , η
</p>
<p>&lowast;(t) := min
{
k : T &lowast;k &gt; t
</p>
<p>}
,
</p>
<p>we get an object of the same nature as before, with nearly the same physical mean-
</p>
<p>ing. For such an object, a number of additional results can be obtained, see e.g.,
</p>
<p>Remark 10.3.1.
</p>
<p>Renewal processes are also quite often used in probabilistic research per se, and
</p>
<p>also when studying other processes for which there exist so-called &ldquo;regeneration
</p>
<p>times&rdquo; after which the evolution of the process starts anew. Below we will encounter
</p>
<p>examples of such use of renewal processes.
</p>
<p>Now we return to the general case where τj may assume both positive and nega-
</p>
<p>tive values.
</p>
<p>Definition 10.1.2 The function
</p>
<p>H(t) := Eη(t), t &ge; 0,
is called the renewal function for the sequence {Tk}&infin;k=0.
</p>
<p>In the existing literature, another definition is used more frequently.
</p>
<p>Definition 10.1.2A The renewal function for the sequence {Tk}&infin;k=0 is defined by
</p>
<p>U(t) :=
&infin;&sum;
</p>
<p>j=0
P(Tj &le; t).
</p>
<p>The values of H(u) and T (u) can be infinite.
</p>
<p>If τj &ge; 0 then the above definitions are equivalent. Indeed, for t &ge; 0, consider
the random variable
</p>
<p>ν(t) := max{k : Tk &le; t} = η(t)&minus; 1.
Then clearly
</p>
<p>&infin;&sum;
</p>
<p>j=0
I(Tj &le; t)= 1 + ν(t),
</p>
<p>where I(A) is the indicator of the event A, and
</p>
<p>U(t)= 1 +Eν(t)= Eη(t)=H(t).
The value U(t)= Eν(t)+ 1 is the mean time spent by the trajectory {Tj }&infin;j=0 in the
interval [0, t].
</p>
<p>If τj can take values of different signs then clearly ν(t) &ge; η(t) and, with a pos-
itive probability, ν(t) &gt; η(t) (the trajectory {Tj }, after crossing the level t , can re-
turn to the region (&minus;&infin;, t]). Therefore in that case U(t) &gt; H(t). Thus for τj taking</p>
<p/>
</div>
<div class="page"><p/>
<p>280 10 Renewal Processes
</p>
<p>values of different signs we have two versions of the renewal function given in Def-
initions 10.1.2 and 10.1.2A. We will call them the first and the second versions,
</p>
<p>respectively. In the present chapter we will consider the first version only (Defini-
tion 10.1.2). The second version is discussed in Appendix 9.
</p>
<p>Note that, for τj assuming values of both signs and t &lt; 0, we have H(t) = 0,
U(t) &gt; 0, so the function H(t) has a jump of magnitude 1 at the point t = 0.
</p>
<p>Note also that the functions H(t) and U(t) we defined above are right-
continuous. In the existing literature, one often considers left-continuous versions
of renewal functions defined respectively as
</p>
<p>H(t &minus; 0)= Emin{k : Sk &ge; t) and U(t &minus; 0)=
&infin;&sum;
</p>
<p>j=0
P(Sj &lt; t).
</p>
<p>If all τj are identically distributed and F
&lowast;k(t) is the k-fold convolution of the dis-
</p>
<p>tribution function F(t)= P(ξj &lt; t), then the second left-continuous version of the
renewal function can also be represented in the form
</p>
<p>&infin;&sum;
</p>
<p>k=0
F &lowast;k(t),
</p>
<p>where F &lowast;0 corresponds to the distribution degenerate at zero.
From the point of view of the exposition below, it makes no difference which
</p>
<p>version of continuity is chosen. For several reasons, in the present chapter it will be
</p>
<p>more convenient for us to deal with right-continuous renewal functions. Everything
below will equally apply to left-continuous renewal functions as well.
</p>
<p>10.1.2 The Integral Renewal Theorem for Non-identically
</p>
<p>Distributed Summands
</p>
<p>In the case where τj , j &ge; 2, are not necessarily identically distributed and do not
possess other homogeneity properties, singling out the random variable τ1 makes
</p>
<p>little sense.
</p>
<p>Theorem 10.1.1 Let τj , j &ge; 1, be uniformly integrable from the right, E|TN |&lt;&infin;
for any fixed N and ak = Eτk &rarr; a &gt; 0 as k&rarr;&infin;. Then the following limit exists
</p>
<p>lim
t&rarr;&infin;
</p>
<p>H(t)
</p>
<p>t
= 1
</p>
<p>a
. (10.1.3)
</p>
<p>Proof We will need the following definition.
</p>
<p>Definition 10.1.3 The random variable
</p>
<p>χ(t)= Tη(t) &minus; t &gt; 0
is said to be the excess of the level t (or overshoot over the level t) for the random
walk {Tj }.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Renewal Processes. Renewal Functions 281
</p>
<p>Lemma 10.1.1 If ak &isin; [a&lowast;, a&lowast;], a&lowast; &gt; 0, then
</p>
<p>Eη(t) &gt;
t
</p>
<p>a&lowast;
, lim sup
</p>
<p>t&rarr;&infin;
</p>
<p>Eη(t)
</p>
<p>t
&le; 1
</p>
<p>a&lowast;
. (10.1.4)
</p>
<p>Proof By Theorem 4.4.2 (see also Example 4.4.3)
</p>
<p>ETη(t) = t +Eχ(t)&le; a&lowast;Eη(t).
This implies the first inequality in (10.1.4). Now introduce truncated random vari-
</p>
<p>ables τ
(s)
j := min(τj , s). By virtue of the uniform integrability, one can choose an s
</p>
<p>such that, for a given ε &isin; (0, a&lowast;), we would have
</p>
<p>aj,s := Eτ (s)j &ge; a&lowast; &minus; ε.
Then, by Theorem 4.4.2,
</p>
<p>t + s &ge; ET (s)
η(s)(t)
</p>
<p>&ge; (a&lowast; &minus; ε)Eη(s),
</p>
<p>where
</p>
<p>T (s)n :=
n&sum;
</p>
<p>j=1
τ
(s)
j , η
</p>
<p>(s)(t) := min
{
k : T (s)k &gt; t
</p>
<p>}
.
</p>
<p>Since η(t)&le; η(s)(t), one has
</p>
<p>H(t)= Eη(t)&le; Eη(s)(t)&le; t + s
a&lowast; &minus; ε
</p>
<p>. (10.1.5)
</p>
<p>As ε &gt; 0 can be chosen arbitrarily, we obtain that
</p>
<p>lim sup
t&rarr;&infin;
</p>
<p>H(t)
</p>
<p>t
&le; 1
</p>
<p>a&lowast;
.
</p>
<p>The lemma is proved. �
</p>
<p>We return to the proof of Theorem 10.1.1. For a given ε &gt; 0, find an N such
</p>
<p>that ak &isin; [a &minus; ε, a + ε] for all k &gt; N and denote by HN (t) the renewal function
corresponding to the sequence {τN+k}&infin;k=1. Then
</p>
<p>H(t)= E
(
η(t);TN &gt; t
</p>
<p>)
+
&int; t
</p>
<p>&minus;&infin;
P(TN &isin; du)
</p>
<p>[
N +HN (t &minus; u)
</p>
<p>]
</p>
<p>= E
[
HN (t &minus; TN ); TN &le; t
</p>
<p>]
+ rN , (10.1.6)
</p>
<p>where
</p>
<p>rN := E
(
η(t); TN &gt; t
</p>
<p>)
+NP(TN &le; t)&le;NP(TN &gt; t)+NP(TN &le; t)=N.
</p>
<p>Relation (10.1.5) implies that there exist constants c1, c2, such that, for all t ,
</p>
<p>HN (t)&le; c1 + c2t.
Therefore, for fixed N and M ,</p>
<p/>
</div>
<div class="page"><p/>
<p>282 10 Renewal Processes
</p>
<p>RN,M := E
[
HN (t &minus; TN ); |TN | &ge;M,TN &le; t
</p>
<p>]
</p>
<p>&le; (c1 + c2t)P
(
|TN | &ge;M,TN &le; t
</p>
<p>)
+ c2E|TN |.
</p>
<p>Choose an M such that c2P(|TN | &ge;M)&lt; ε. Then
</p>
<p>lim sup
t&rarr;&infin;
</p>
<p>rN +RN,M
t
</p>
<p>&le; ε. (10.1.7)
</p>
<p>To bound H(t) in (10.1.6) it remains to consider, for the chosen N and M , the
</p>
<p>function
</p>
<p>HN,M(t) := E
[
HN (t &minus; TN ); |TN |&lt;M
</p>
<p>]
.
</p>
<p>By Lemma 10.1.1,
</p>
<p>lim sup
t&rarr;&infin;
</p>
<p>HN,M(t)
</p>
<p>t
&le; 1
</p>
<p>a &minus; ε ,
</p>
<p>lim inf
t&rarr;&infin;
</p>
<p>HN,M(t)
</p>
<p>t
&ge; P(|TN |&lt;M)
</p>
<p>a + ε &ge;
1 + ε/c1
a + ε .
</p>
<p>This together with (10.1.6) and (10.1.7) yields
</p>
<p>lim sup
t&rarr;&infin;
</p>
<p>H(t)
</p>
<p>t
&le; ε+ 1
</p>
<p>a &minus; ε , lim inft&rarr;&infin;
H(t)
</p>
<p>t
&ge; (1 &minus; ε/c2)
</p>
<p>a + ε .
</p>
<p>Since ε is arbitrary, the foregoing implies (10.1.3).
</p>
<p>The theorem is proved. �
</p>
<p>Remark 10.1.2 One can obtain the following generalisation of Theorem 10.1.1, in
which no restrictions on τ1 &ge; 0 are imposed. Let τ1 be an arbitrary nonnegative
random variable, and τ &lowast;j := τ1+j satisfy the conditions of Theorem 10.1.1. Then
(10.1.3) still holds true.
</p>
<p>This assertion follows from the relations
</p>
<p>H(t)= P(τ1 &gt; t)+
&int; t
</p>
<p>0
</p>
<p>P(τ1 &isin; dv)H &lowast;(t &minus; v), (10.1.8)
</p>
<p>where H &lowast;(t) corresponds to the sequence {τ &lowast;j } and, for each fixed N and v &le;N ,
</p>
<p>H &lowast;(t &minus; v)
t
</p>
<p>= H
&lowast;(t &minus; v)
t &minus; v &middot;
</p>
<p>t &minus; v
t
</p>
<p>&rarr; 1
a
</p>
<p>as t &rarr;&infin;. Therefore
1
</p>
<p>t
</p>
<p>&int; N
</p>
<p>0
</p>
<p>P(τ1 &isin; dv)H &lowast;(t &minus; v)&rarr;
P(τ1 &le;N)
</p>
<p>a
.
</p>
<p>For the remaining part of the integral in (10.1.8), we have
</p>
<p>lim sup
t&rarr;&infin;
</p>
<p>1
</p>
<p>t
</p>
<p>&int; t
</p>
<p>N
</p>
<p>P(τ1 &isin; dv)H &lowast;(t &minus; v)&le; lim sup
t&rarr;&infin;
</p>
<p>H &lowast;(t)
</p>
<p>t
P(τ1 &gt;N)=
</p>
<p>P(τ1 &gt;N)
</p>
<p>a
.
</p>
<p>Since the probability P(τ1 &gt;N) can be made arbitrarily small by the choice of N ,
</p>
<p>the assertion is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Renewal Processes. Renewal Functions 283
</p>
<p>It is not difficult to verify that the condition τ1 &ge; 0 can be relaxed to the condition
Emin(0, τ1) &gt;&minus;&infin;. However, if Emin(0, τ1)=&minus;&infin;, then H(t)=&infin; and relation
(10.1.3) does not hold.
</p>
<p>Obtaining an analogue of Theorem 10.1.1 for the second version U(t) of the
</p>
<p>renewal function in the case of uniformly integrable τj taking values of both signs is
accompanied by greater technical difficulties and additional conditions. For a fixed
</p>
<p>ε &gt; 0, split the series U(t)=
&sum;&infin;
</p>
<p>n=0 P(Tn &le; t) into the three parts
&sum;
</p>
<p>1
=
</p>
<p>&sum;
</p>
<p>n&lt;
t (1&minus;ε)
</p>
<p>a
</p>
<p>,
&sum;
</p>
<p>2
=
</p>
<p>&sum;
</p>
<p>|n&minus; t
a
|&le; tε
</p>
<p>a
</p>
<p>,
&sum;
</p>
<p>3
=
</p>
<p>&sum;
</p>
<p>n&gt;
t (1+ε)
</p>
<p>a
</p>
<p>.
</p>
<p>By the law of large numbers (see Corollary 8.3.2),
</p>
<p>Tn
</p>
<p>n
</p>
<p>p&rarr; a.
</p>
<p>Therefore, for n &lt; t (1&minus;ε)
a
</p>
<p>,
</p>
<p>P(Tn &le; t)&ge; P
(
Tn &le;
</p>
<p>na
</p>
<p>1 &minus; ε
</p>
<p>)
&rarr; 1
</p>
<p>and hence
</p>
<p>1
</p>
<p>t
</p>
<p>&sum;
1
&rarr; 1 &minus; ε
</p>
<p>a
.
</p>
<p>The second sum allows the trivial bound
</p>
<p>1
</p>
<p>t
</p>
<p>&sum;
2
&lt;
</p>
<p>2ε
</p>
<p>a
,
</p>
<p>where the right-hand side can be made arbitrarily small by the choice of ε.
</p>
<p>The main difficulties are related to estimating
&sum;
</p>
<p>3. To illustrate the problems
</p>
<p>arising here we confine ourselves to the case of identically distributed τj
d= τ . In
</p>
<p>this case the required estimate for
&sum;
</p>
<p>3 can only be obtained under the condition
</p>
<p>E(τ&minus;)2 &lt;&infin;, τ&minus; := max(0,&minus;τ). Assume without losing generality that Eτ 2&lt;&infin;.
(If E(τ+)2 = &infin;, τ+ := max(0, τ ), then introducing truncated random variables
τ
(s)
j = min(s, τj ), we obtain, using obvious conventions concerning notations, that
P(Tn &le; t)&le; P(T (s)n &le; t), U(t)&le;U (s)(t) and
</p>
<p>&sum;
3 &le;
</p>
<p>&sum;(s)
3 , where E(τ
</p>
<p>(s))2 &lt;&infin; and
the value of Eτ (s) can be made arbitrarily close to a by the choice of s.) In the
</p>
<p>case Eτ 2 &lt;&infin; we can use Theorem 9.5.1 by virtue of which, for a regularly vary-
ing left tail W(t) = P(τ &lt; &minus;t) = t&minus;βL(t) (L(t) is a slowly varying function) and
n &gt; t
</p>
<p>a
(1 + ε), we have
</p>
<p>P(Tn &le; t)= P
(
Tn &minus; an&le;&minus;(an&minus; t)
</p>
<p>)
&sim; nW(an&minus; t).
</p>
<p>By the properties of slowly varying functions (see Appendix 6), for the values
</p>
<p>u= n/t comparable to 1, n &gt; t
a
(1 + ε) and t &rarr;&infin;, we have
</p>
<p>W(an&minus; t)
W(εt)
</p>
<p>&sim;
(
au&minus; 1
</p>
<p>ε
</p>
<p>)&minus;β
.</p>
<p/>
</div>
<div class="page"><p/>
<p>284 10 Renewal Processes
</p>
<p>Thus for β &gt; 2, as t &rarr;&infin;,
&sum;
</p>
<p>3
=
</p>
<p>&sum;
</p>
<p>n&gt;
(1+ε)t
</p>
<p>a
</p>
<p>P(Tn &le; t)&sim;
&int;
</p>
<p>v&gt;
(1+ε)t
</p>
<p>a
</p>
<p>vW(av&minus; t) dv
</p>
<p>&sim; t2W(εt)
&int; &infin;
</p>
<p>1+ε
a
</p>
<p>u
</p>
<p>[
au&minus; 1
</p>
<p>ε
</p>
<p>]&minus;β
du&sim; c(ε)t2W(t)= o(1).
</p>
<p>Summarising, we have obtained that
</p>
<p>lim
t&rarr;&infin;
</p>
<p>U(t)
</p>
<p>t
= 1
</p>
<p>a
.
</p>
<p>Now if E(τ&minus;)2 =&infin; then U(t)=&infin; for all t . In this case, instead of U(t) one
studies the &ldquo;local&rdquo; renewal function
</p>
<p>U(t,h)=
&sum;
</p>
<p>n
</p>
<p>P
(
Tn &isin; (t, t + h]
</p>
<p>)
</p>
<p>which is always finite provided that a &gt; 0 and has all the properties of the increment
</p>
<p>H(t + h)&minus;H(t) to be studied below (see e.g. [12]).
In view of the foregoing and since the function H(t) will be of principal interest
</p>
<p>to us, in what follows we will restrict ourselves to studying the first version of the
</p>
<p>renewal function, as was noted above. We will mainly pay attention to the asymp-
</p>
<p>totic behaviour of the increments H(t + h) &minus; H(t) as t &rarr; &infin;. To this is closely
related a more general problem that often appears in applications: the problem on
</p>
<p>the asymptotic behaviour as t &rarr;&infin; of integrals (see e.g. Chap. 13)
&int; t
</p>
<p>0
</p>
<p>g(t &minus; y)dH(y) (10.1.9)
</p>
<p>for functions g(v) such that
&int; &infin;
</p>
<p>0
</p>
<p>g(v) dv &lt;&infin;.
</p>
<p>Theorems describing the asymptotic behaviour of (10.1.9) will be called the key
renewal theorems. The next sections and Appendix 9 will be devoted to these theo-
rems. Due to the technical complexity of the mentioned problems, we will confine
</p>
<p>ourselves to considering only the case where τj , j &ge; 2, are identically distributed.
Note that in some special cases the above problems can be solved in a very simple
</p>
<p>way, since the renewal function H(t) can be found there explicitly. To do this, as it
</p>
<p>follows from Wald&rsquo;s identity used above, it suffices to find Eχ(t) in explicit form.
</p>
<p>If, for instance, τj are integer-valued, P(τj = 1) &gt; 0 and P(τj &ge; 2) = 0, for all
j &ge; 1, then χ(t)&equiv; 1 and Wald&rsquo;s identity yields H(t)= (t+1)/a. Similar equalities
will hold if P(τj &gt; t)= ce&minus;γ t for t &gt; 0 and γ &gt; 0 (if τj are integer-valued, then t
takes only integer values in this formula). In that case the distribution of χ(t) will
</p>
<p>be exponential and will not depend on t (for more details, see the exposition below
</p>
<p>and also Chap. 15).</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 The Key Renewal Theorem in the Arithmetic Case 285
</p>
<p>10.2 The Key Renewal Theorem in the Arithmetic Case
</p>
<p>We will distinguish between two distribution types for τj : arithmetic in an extended
sense (when the lattice span is not necessary 1; for the definition of arithmetic distri-
butions see Sect. 7.1) and all other distributions that we will call non-arithmetic. It is
clear that, say, a random variable taking values 1 and
</p>
<p>&radic;
2 with positive probabilities
</p>
<p>cannot be arithmetic.
</p>
<p>In the present section, we will consider the arithmetic case. Without loss of gener-
</p>
<p>ality, we will assume that the lattice span is 1. Then the functions P(τj &lt; t) and H(t)
</p>
<p>will be completely determined by their values at integer points t = k, k = 0,1,2 . . . .
First we consider the case where the τj are positive, τj
</p>
<p>d= τ for j &ge; 2. In that
case, the difference
</p>
<p>h(k) :=H(k)&minus;H(k &minus; 1)=
&infin;&sum;
</p>
<p>j=0
P(Tj = k), k &ge; 1,
</p>
<p>is equal to the expectation of the number of visits of the point k by the walk {Tj }.
Put
</p>
<p>qk := P(τ1 = k), pk := P(τ = k).
</p>
<p>Definition 10.2.1 A renewal process η(t) will be called homogeneous and denoted
by η0(t) if
</p>
<p>qk =
1
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>k
</p>
<p>pj , k = 1,2, . . . , a = Eτ,
(
</p>
<p>so that
</p>
<p>&infin;&sum;
</p>
<p>k=1
qk = 1
</p>
<p>)
. (10.2.1)
</p>
<p>If we denote by p(z) the generating function
</p>
<p>p(z)= Ezτ =
&infin;&sum;
</p>
<p>k=1
pkz
</p>
<p>k,
</p>
<p>then the generating function q(z)= Ezξ1 =
&sum;&infin;
</p>
<p>k=1 qkz
k will be equal to
</p>
<p>q(z)= 1
a
</p>
<p>&infin;&sum;
</p>
<p>k=1
zk
</p>
<p>&infin;&sum;
</p>
<p>j=k
pj =
</p>
<p>z
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>j=1
pj
</p>
<p>j&minus;1&sum;
</p>
<p>k=0
zk = z
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>j=1
pj
</p>
<p>1 &minus; zj
1 &minus; z =
</p>
<p>z(1 &minus; p(z))
a(1 &minus; z) .
</p>
<p>As we will see below, the term &ldquo;homogeneous&rdquo; for the process η0(t) is quite justi-
</p>
<p>fied. One of the reasons for its use is the following exact (non-asymptotic) equality.
</p>
<p>Theorem 10.2.1 For a homogeneous renewal process η0(t), one has
</p>
<p>H0(k) := Eη0(k)= 1 +
k
</p>
<p>a
.
</p>
<p>Proof Consider the generating function r(z) for the sequence h0(k) = H0(k) &minus;
H0(k &minus; 1):</p>
<p/>
</div>
<div class="page"><p/>
<p>286 10 Renewal Processes
</p>
<p>r(z)=
&infin;&sum;
</p>
<p>1
</p>
<p>zkh0(k)=
&infin;&sum;
</p>
<p>j=1
</p>
<p>&infin;&sum;
</p>
<p>k=1
zkP(Tj = k)
</p>
<p>=
&infin;&sum;
</p>
<p>j=1
EzTj = q(z)
</p>
<p>&infin;&sum;
</p>
<p>j=0
pj (z)= q(z)
</p>
<p>1 &minus; p(z) =
z
</p>
<p>a(1 &minus; z) .
</p>
<p>This implies that h0(k) = 1/a. Since H0(0) = 1, one has H0(k) = 1 + k/a. The
theorem is proved. �
</p>
<p>Sometimes the process η0(t) is also called stationary. As we will see below,
it would be more appropriate to call it a process with stationary increments (see
Sect. 22.1).
</p>
<p>The asymptotic regular behaviour of the function h(k) as k &rarr;&infin; persists in the
case of arbitrary τ1 as well.
</p>
<p>Denote by d the greatest common divisor (g.c.d.) of the possible values of τ :
</p>
<p>d := g.c.d.{k : pk &gt; 0},
</p>
<p>and let g(k), k = 0,1, . . . , be an arbitrary sequence such that
&infin;&sum;
</p>
<p>k=0
</p>
<p>∣∣g(k)
∣∣&lt;&infin;.
</p>
<p>Theorem 10.2.2 (The key renewal theorem) If d = 1, τ1 is an arbitrary integer-
valued random variable and τj
</p>
<p>d= τ &gt; 0 for j &ge; 2, then, as k&rarr;&infin;,
</p>
<p>h(k) :=H(k)&minus;H(k &minus; 1)&rarr; 1
a
,
</p>
<p>k&sum;
</p>
<p>l=1
h(l)g(k &minus; l)&rarr; 1
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>m=0
g(m).
</p>
<p>These two relations are equivalent.
</p>
<p>The first assertion of the theorem is also called the local renewal theorem.
To prove the theorem we will need two auxiliary assertions.
</p>
<p>Lemma 10.2.1 Let all τj be identically distributed and ν &ge; 1 be a Markov
time with respect to the collection of σ -algebras {Fn}, where Fn is independent
of σ(τn+1, τn+2, . . .). Then the σ -algebra generated by the random variables ν,
τ1, . . . , τν , and the σ -algebra σ {τν+1, τν+2, . . .} are independent. The sequence
{τν+1, τν+2, . . .} has the same distribution as {τ1, τ2, . . .}.
</p>
<p>Thus, in spite of their random numbers, the elements of the sequence τν+j are
distributed as τj .
</p>
<p>Proof For given Borel sets B1,B2, . . . , C1,C2, . . . put
</p>
<p>A := {ν &isin;N, τ1 &isin; B1, . . . , τν &isin; Bν}, Dν := {τν+1 &isin; C1, . . . , τν+k &isin; Ck},</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 The Key Renewal Theorem in the Arithmetic Case 287
</p>
<p>where N is a given set of integers and k is arbitrary. Since P(Dj )= P(D0) and the
events Dj and {ν = j} are independent, the total probability formula yields
</p>
<p>P(Dν)=
&infin;&sum;
</p>
<p>j=1
P(ν = j,Dj )=
</p>
<p>&infin;&sum;
</p>
<p>j=1
P(ν = j)P(Dj )= P(D0).
</p>
<p>Therefore, by Theorem 3.4.3, in order to prove the required independence of the
</p>
<p>σ -algebras, it suffices to show that P(DνA)= P(D0)P(A).
By the total probability formula,
</p>
<p>P(DνA)=
&sum;
</p>
<p>j&isin;N
P
(
DνA{ν = j}
</p>
<p>)
=
&sum;
</p>
<p>j&isin;N
P
(
DjA{ν = j}
</p>
<p>)
.
</p>
<p>But the event A{ν = j} belongs to Fj , whereas Dj &isin; σ(τj+1, . . . , τj+k). Therefore
Dj and A{ν = j} are independent events and
</p>
<p>P
(
Dj A{ν = j}
</p>
<p>)
= P(Dj )P
</p>
<p>(
A{ν = j}
</p>
<p>)
= P(D0)P
</p>
<p>(
A{ν = j}
</p>
<p>)
, j &ge; 1.
</p>
<p>From here it clearly follows that P(DνA)= P(D0)P(A). The lemma is proved. �
</p>
<p>Lemma 10.2.2 Let ζ1, ζ2, . . . be independent arithmetic identically and symmet-
rically distributed random variables with zero expectation Eζj = 0. Put Zn :=&sum;n
</p>
<p>j=1 ζj . Then, for any integer k,
</p>
<p>νk := min{n : Zn = k}
is a proper random variable: P(νk &lt;&infin;)= 1.
</p>
<p>The proof of the lemma is given in Sect. 13.3 (see Corollary 13.3.1).
</p>
<p>Proof of Theorem 10.2.2 Consider two independent sequences of random vari-
ables (we assume that they are given on a common probability space): a sequence
</p>
<p>τ1, τ2, . . . , where τ1 has an arbitrary distribution, and a sequence τ
&prime;
1, τ
</p>
<p>&prime;
2, . . . , where
</p>
<p>P(τ &prime;1 = k)= qk (see (10.2.1)), and P(τ &prime;j = k)= P(τj = k)= pk for j &ge; 2 (so that
τ &prime;j
</p>
<p>d= τj for j &ge; 2; the process η&prime;(t) constructed from the sums T &prime;k =
&sum;k
</p>
<p>j=1 τ
&prime;
j is
</p>
<p>homogeneous (see Definition 10.2.1)).
</p>
<p>Set ν := min{n &ge; 1 : Tn = T &prime;n}. It is clearly a Markov time with respect to the
sequence {τj , τ &prime;j }. We show that P(ν &lt;&infin;)= 1. Put
</p>
<p>Zn :=
n&sum;
</p>
<p>j=2
</p>
<p>(
τj &minus; τ &prime;j
</p>
<p>)
for n&ge; 2, Z1 := 0, Z0 := τ1 &minus; τ &prime;1.
</p>
<p>Then
</p>
<p>ν = min{n&ge; 1 :Zn =&minus;Z0}.
By Lemma 10.2.2 (ζj = τj &minus; τ &prime;j have a symmetric distribution for j &ge; 2), for each
integer k the variable νk = min{n &ge; 1 : Zn = k} is proper. Since Zn for n &ge; 1 and
Z&prime;1 are independent, we have
</p>
<p>P(ν &lt;&infin;)=
&sum;
</p>
<p>k
</p>
<p>P(Z0 =&minus;k)P(νk &lt;&infin;)= 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>288 10 Renewal Processes
</p>
<p>Now we will &ldquo;glue together&rdquo; (&ldquo;couple&rdquo;) the sequences {Tn} and {T &prime;n}. Since
Tν = T &prime;ν and ν is a Markov time, by Lemma 10.2.1 one can replace τν+1, τν+2, . . .
with τ &prime;ν+1, τ
</p>
<p>&prime;
ν+2, . . . (and thereby replace Tν+1, Tν+2 with T
</p>
<p>&prime;
ν+1, T
</p>
<p>&prime;
ν+2, . . .) without
</p>
<p>changing the distribution of the sequence {Tn}.
Therefore, on the set {Tν &lt; k} one has η(t)= η&prime;(t) for t &ge; k &minus; 1 and hence
h(k)= E
</p>
<p>(
η(k)&minus; η(k &minus; 1)
</p>
<p>)
</p>
<p>= E
[
η&prime;(k)&minus; η&prime;(k &minus; 1); Tν &lt; k
</p>
<p>]
+E
</p>
<p>[
η(k)&minus; η(k&minus; 1); Tν &ge; k
</p>
<p>]
</p>
<p>= 1
a
&minus;E
</p>
<p>[
η&prime;(k)&minus; η&prime;(k &minus; 1); Tν &ge; k
</p>
<p>]
+E
</p>
<p>[
η(k)&minus; η(k &minus; 1); Tν &ge; k
</p>
<p>]
.
</p>
<p>Since |η(k)&minus; η(k &minus; 1)| &le; 1, we have∣∣∣∣h(k)&minus;
1
</p>
<p>a
</p>
<p>∣∣∣∣&le; P(Tν &ge; k)&rarr; 0
</p>
<p>as k&rarr;&infin;. The first assertion of Theorem 10.2.2 is proved.
Since h(k)&le; 1, we can make the value of
</p>
<p>∣∣∣∣∣
</p>
<p>k&minus;N&sum;
</p>
<p>l=1
h(l)g(k &minus; l)
</p>
<p>∣∣∣∣∣&le;
k&minus;1&sum;
</p>
<p>l=N+1
</p>
<p>∣∣g(l)
∣∣&le;
</p>
<p>&infin;&sum;
</p>
<p>l=N+1
</p>
<p>∣∣g(l)
∣∣
</p>
<p>arbitrarily small by choosing an appropriate N . Moreover, by virtue of the first as-
</p>
<p>sertion, for any fixed N ,
</p>
<p>k&sum;
</p>
<p>l=k&minus;N
h(l)g(k &minus; l)&rarr; 1
</p>
<p>a
</p>
<p>N&sum;
</p>
<p>l=0
g(l) as k&rarr;&infin;.
</p>
<p>This implies the second assertion of the theorem. �
</p>
<p>Remark 10.2.1 The coupling of {Tn} and {T &prime;n} in the proof of Theorem 10.2.2 could
be done earlier, at the time γ := min{n&ge; 1 : Tn &isin; T &prime;}, where T &prime; is the set of points
T &prime; = {T &prime;1, T &prime;2, . . .}.
</p>
<p>Theorem 10.2.3 The assertion of Theorem 10.2.2 remains true for arbitrary (as-
suming values of different signs) τj .
</p>
<p>Proof We will reduce the problem to the case τj &gt; 0. First let all τj be identi-
cally distributed. Consider the random variable χ1 = χ(0) that we will call the first
positive sum. We will show in Chap. 12 (see Corollary 12.2.3) that Eχ1 &lt; &infin; if
a = Eτj &lt;&infin;. According to Lemma 10.2.1, the sequence τη(0)+1, τη(0)+2, . . . will
have the same distribution as τ1, τ2, . . . . Therefore the &ldquo;second positive sum&rdquo; χ2 or,
</p>
<p>which is the same, the first positive sum of the variables τη(0)+1, τη(0)+2, . . . will
have the same distribution as χ1 and will be independent of it. The same will be true
</p>
<p>for the subsequent &ldquo;overshoots&rdquo; over the already achieved levels χ1, χ1 + χ2, . . . .
Now consider the random walk {
</p>
<p>Hk =
k&sum;
</p>
<p>i=1
χi
</p>
<p>}&infin;
</p>
<p>k=1</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 The Key Renewal Theorem in the Arithmetic Case 289
</p>
<p>and put
</p>
<p>η&lowast;(t) := min{k :Hk &gt; t}, χ&lowast;(t) :=Hη&lowast;(t) &minus; t, H &lowast;(t) := Eη&lowast;(t).
</p>
<p>Since χk &gt; 0, Theorem 10.2.2 is applicable, and therefore by Wald&rsquo;s identity
</p>
<p>H &lowast;(k)&minus;H &lowast;(k &minus; 1)= 1
Eχ1
</p>
<p>(
1 +Eχ&lowast;(k)&minus;Eχ&lowast;(k &minus; 1)
</p>
<p>)
&rarr; 1
</p>
<p>Eχ1
,
</p>
<p>Eχ&lowast;(k)&minus;Eχ&lowast;(k &minus; 1)&rarr; 0.
</p>
<p>Note now that the distributions of the random variables χ(t) (see Definition 10.1.3)
</p>
<p>and χ&lowast;(t) coincide. Therefore
</p>
<p>H(k)&minus;H(k &minus; 1)= 1
a
</p>
<p>(
1 +Eχ(k)&minus;Eχ(k &minus; 1)
</p>
<p>)
</p>
<p>= 1
a
</p>
<p>(
1 +Eχ&lowast;(k)&minus;Eχ&lowast;(k &minus; 1)
</p>
<p>)
&rarr; 1
</p>
<p>a
.
</p>
<p>Now let the distributions of τ1 and τj , j &ge; 2, be different. Then the renewal
function H1(t) for such a walk will be equal to
</p>
<p>H1(k)= 1 +
k&sum;
</p>
<p>i=&minus;&infin;
P(τ1 = i)
</p>
<p>[
H(k &minus; i)+ 1
</p>
<p>]
= 1 +
</p>
<p>k&sum;
</p>
<p>i=&minus;&infin;
P(τ1 = i)H(k &minus; i),
</p>
<p>h1(k)=H1(k)&minus;H1(k &minus; 1)=
k&sum;
</p>
<p>i=&minus;&infin;
P(τ1 = i)h(k &minus; i), k &ge; 0,
</p>
<p>where H1(&minus;1)= 0, h(0)=H(0) and the function H(t) corresponds to identically
distributed τj . If we had h(k) &lt; c &lt; &infin; for all k, that would imply convergence
h1(k)&rarr; 1/a and thus complete the proof of the theorem.
</p>
<p>The required inequality h(k) &lt; c actually follows from the following gen-
</p>
<p>eral proposition which is true for arbitrary (not necessarily lattice) random vari-
</p>
<p>ables τj . �
</p>
<p>Lemma 10.2.3 If all τj are identically distributed then, for all t and u,
</p>
<p>H(t + u)&minus;H(t)&le;H(u)&le; c1 + c2u.
</p>
<p>Proof The difference η(t + u)&minus; η(t) is the number of jumps of the trajectory {T̃k}
that started at the point t + χ(t) &gt; t until the first passage of the level t + u, where
the sequence {T̃k} has the same distribution as {Tk} and is independent of it (see
Lemma 10.2.1). In other words, η(t + u)&minus; η(u) has the same distribution as η̃(t &minus;
χ(t)) &le; η̃(t), where η̃ corresponds to {T̃k} if χ(t) &le; u and to η(t + u)&minus; η(t) = 0
if χ(t) &gt; u. Therefore H(t + u)&minus;H(t) &le;H(u). The inequality for H(u) follows
from Theorem 10.2.1. The lemma is proved. �
</p>
<p>Theorem 10.2.3 is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>290 10 Renewal Processes
</p>
<p>10.3 The Excess and Defect of a RandomWalk. Their Limiting
</p>
<p>Distribution in the Arithmetic Case
</p>
<p>Along with the excess χ(t) = Tη(t) &minus; t we introduce one more random variable
closely related to χ(t).
</p>
<p>Definition 10.3.1 The random variable
</p>
<p>γ (t) := t &minus; Tη(t)&minus;1 = t &minus; Tν(t)
is called the defect (or undershoot) of the level t in the walk {Tn}.
</p>
<p>The quantity χ(t) may be thought of as the time during which the component
</p>
<p>that was working at time t will continue working after that time, while γ (t) is the
</p>
<p>time for which the component has already been working by that time.
</p>
<p>One should not think that the sum χ(t)+ γ (t) has the same distribution as τj&mdash;
this sum is actually equal to the value of a τ with the random subscript η(t). In
particular, as we will see below, it may turn out that Eχ(t) &gt; Eτj for large t . The
</p>
<p>following apparent paradox is related to this fact. A passenger coming to a bus stop
</p>
<p>at which buses arrive with inter-arrival times τ1 &gt; 0, τ2 &gt; 0, . . . (Eτj = a), will wait
for the arrival of the next bus for a random time χ of which the mean Eχ could
</p>
<p>prove to be greater than a.
</p>
<p>One of the principal facts of renewal theory is the assertion that, under broad
</p>
<p>assumptions, the joint distribution of χ(t) and γ (t) has a limit as t &rarr;&infin;, so that
for large t the distribution of χ(t) does not depend on t any more and becomes
</p>
<p>stationary. Denote this limiting distribution of χ(t) by G and its distribution function
</p>
<p>by G:
</p>
<p>G(x)= lim
t&rarr;&infin;
</p>
<p>P
(
χ(t) &lt; x
</p>
<p>)
. (10.3.1)
</p>
<p>If we take the distribution of τ1 to be G then, for such process, by its very construc-
</p>
<p>tion the distribution of the variable χ(t) will be independent of t . Indeed, in that
</p>
<p>case we can think of the positive elements of {Tj } as the renewal times for a process
which is constructed from the sequence {τj } and of which the start is shifted to a
point &minus;N , where N is very large. Since by virtue of (10.3.1) we can assume that
the distributions of χ(N) and χ(N + t) coincide with each other, the distribution of
the variable χ(t) (which can be identified with χ(N + t)) is independent of t and
coincides with that of τ1. A formal proof of this fact is omitted, since it will not be
</p>
<p>used in what follows. However, the reader could carry it out using the explicit form
</p>
<p>of G(x) from (10.3.1) to be derived below.
</p>
<p>In the arithmetic case, the distribution G is just the law (10.2.1) used to construct
</p>
<p>the homogeneous renewal process η0(t). We will prove this in our next theorem.
</p>
<p>It follows from the fact that, for the process η0(t), the distribution of χ(t) does
</p>
<p>not depend on t and coincides with that of τ1, that the distribution of η0(t + u)&minus;
η0(t) coincides with that of η0(u) and hence is also independent of t . It is this
</p>
<p>property that establishes the stationarity of the increments of the renewal process;
</p>
<p>we called this property homogeneity. It means that the distribution of the number of</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Excess and Defect. Their Limiting Distribution in the Arithmetic Case 291
</p>
<p>renewals over a time interval of length u does not depend on when we start counting,
</p>
<p>and therefore depends on u only.
</p>
<p>Theorems on the limiting distribution of χ(t) and γ (t) are of interest not only
</p>
<p>from the point of view of their applications. We will need them for a variety of other
</p>
<p>problems. Again we consider first the case when the variables τj &gt; 0 are arithmetic.
</p>
<p>In that case the &ldquo;time&rdquo; can also be assumed discrete and we will denote it, as before,
</p>
<p>by the letters n and k. Let, as before, τj
d= τ for j &ge; 2 and pk = P(τ = k).
</p>
<p>Theorem 10.3.1 Let the random variable τ &gt; 0 be arithmetic, Eτ = a exist, τ1 be
an arbitrary integer random variable, and the g.c.d. of the possible values of τ be
equal to 1. Then the following limit exists
</p>
<p>lim
k&rarr;&infin;
</p>
<p>P
(
γ (k)= i, χ(k)= j
</p>
<p>)
= pi+j
</p>
<p>a
, i &ge; 0, j &gt; 0. (10.3.2)
</p>
<p>It follows from Theorem 10.3.1 that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>P
(
χ(k)= i
</p>
<p>)
= 1
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>j=i
pj , i &gt; 0;
</p>
<p>lim
k&rarr;&infin;
</p>
<p>P
(
γ (k)= i
</p>
<p>)
= 1
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>j=i+1
pj , j &ge; 0.
</p>
<p>(10.3.3)
</p>
<p>Proof of Theorem 10.3.1 By the renewal theorem (see Theorem 10.2.2), for k &gt; i,
</p>
<p>P
(
γ (k)= i, χ(k)= j
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>l=1
P(Tl = k &minus; i, τl+1 = i + j)
</p>
<p>=
&infin;&sum;
</p>
<p>l=1
P(Tl = k &minus; i)P(τ = i + j)= h(k &minus; i)pi+j &rarr;
</p>
<p>pi+j
a
</p>
<p>as k&rarr;&infin;. The theorem is proved. �
</p>
<p>If Eτ 2 =m2 &lt;&infin;, then Theorem 10.3.1 allows a refinement of Theorem 10.2.2
(see Theorem 10.3.2 below).
</p>
<p>Corollary 10.3.1 If m2 &lt;&infin;, then the random variables χ(k) are uniformly inte-
grable and
</p>
<p>Eχ(k)&rarr; 1
a
</p>
<p>&infin;&sum;
</p>
<p>i=0
i
</p>
<p>&infin;&sum;
</p>
<p>j=i
pj =
</p>
<p>m2 + a
2a
</p>
<p>as k&rarr;&infin;. (10.3.4)
</p>
<p>Proof The uniform integrability follows from the inequalities h(k)&le; 1,
</p>
<p>P
(
χ(k)= j
</p>
<p>)
=
</p>
<p>k&sum;
</p>
<p>i=0
h(k &minus; i)pi+j &le;
</p>
<p>&infin;&sum;
</p>
<p>i=j
pi .
</p>
<p>This implies (10.3.4) (see Sect. 6.1). �</p>
<p/>
</div>
<div class="page"><p/>
<p>292 10 Renewal Processes
</p>
<p>Now we can state a refined version of the integral theorem that implies Theo-
</p>
<p>rem 10.2.2.
</p>
<p>Theorem 10.3.2 If all τj are identically distributed and Eτ 2 =m2 &lt;&infin;, then
</p>
<p>H(k)= k
a
+ m2 + a
</p>
<p>2a2
+ o(1)
</p>
<p>as k&rarr;&infin;.
</p>
<p>The Proof immediately follows from the Wald identity
</p>
<p>H(k)= Eη(k)= k+Eχ(k)
a
</p>
<p>and Corollary 10.3.1. �
</p>
<p>Remark 10.3.1 For the process η&lowast;(t) corresponding to nonzero times τ &prime;j required
for components&rsquo; renewals (mentioned in Remark 10.1.1), the reader can easily find,
</p>
<p>similarly to Theorem 10.3.1, not only the asymptotic value pi+j/a&lowast; of the proba-
bility that at time k &rarr;&infin; the current component has already worked for time i and
will still work for time j , but also the asymptotics of the probability that the com-
</p>
<p>ponent has been &ldquo;under repair&rdquo; for time i and will stay in that state for time j , that
</p>
<p>is given by p&prime;i+j/a
&lowast;, where p&prime;i = P(τ &prime;j = i), a&lowast; = E(τj + τ &prime;j )= Eτ &lowast;j .
</p>
<p>Now consider the question of under what circumstances the distribution of the
</p>
<p>random variable τ1 for the homogeneous process (i.e. the distribution of what one
</p>
<p>could denote by χ(&infin;)) will coincide with that of τj for j &ge; 2. Such a coincidence
is equivalent to the equality
</p>
<p>pi =
1
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>j=i
pj
</p>
<p>for i = 1,2, . . . , or, which is the same, to
</p>
<p>a(pi &minus; pi&minus;1)=&minus;pi&minus;1, pi =
a &minus; 1
a
</p>
<p>pi&minus;1, pi =
1
</p>
<p>a &minus; 1
</p>
<p>(
a &minus; 1
a
</p>
<p>)i
.
</p>
<p>This means that the renewal process generated by the sequence of independent iden-
</p>
<p>tically distributed random variables τ1, τ2, . . . is homogeneous if and only if τj (or,
</p>
<p>more precisely, τj&minus;1) have the geometric distribution.
Denote by γ and χ the random variables having distribution (10.3.2). Using
</p>
<p>(10.3.1), it is not hard to show that γ and χ are independent also only in the case
</p>
<p>when τj , j &ge; 2, have the geometric distribution. When all τj , j &ge; 1, have such a
distribution, γ (n) and χ(n) are also independent, and χ(n)
</p>
<p>d= τ1. These facts can be
proved in exactly the same way as for the exponential distribution (see Sect. 10.4).
</p>
<p>We now return to the general case and recall that if Eτ 2 &lt;&infin; then (see Corol-
lary 10.3.1)
</p>
<p>Eχ = Eτ
2
</p>
<p>2a
+ 1
</p>
<p>2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 The Renewal Theorem in the Non-arithmetic Case 293
</p>
<p>This means, in particular, that if the distribution of τ is such that Eτ 2 &gt; 2a2 &minus; a,
then, for large n, the excess mean value Eχ(n) will become greater than Eτ = a.
</p>
<p>10.4 The Renewal Theorem and the Limiting Behaviour
</p>
<p>of the Excess and Defect in the Non-arithmetic Case
</p>
<p>Recall that in this chapter by the non-arithmetic case we mean that there exists no
</p>
<p>h &gt; 0 such that P(
⋃
</p>
<p>k{τ = kh}) = 1, where k runs over all integers. To state the
key renewal theorem in that case, we will need the notion of a directly integrable
function.
</p>
<p>Definition 10.4.1 A function g(u) defined on [0,&infin;) is said to be directly integrable
if:
</p>
<p>(1) the function g is Riemann integrable1 over any finite interval [0,N]; and
(2)
</p>
<p>&sum;
k g(k) &lt;&infin;, where gk = maxk&le;u&le;k+1 |g(u)|.
</p>
<p>It is evident that any monotonically decreasing function g(t) &darr; 0 having a finite
Lebesgue integral
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>g(t) dt &lt;&infin;
</p>
<p>is directly integrable. This also holds for differences of such functions.
</p>
<p>The notion of directly integrable functions introduced in [12] differs somewhat
</p>
<p>from the one just defined, although it essentially coincides with it. It will be more
</p>
<p>convenient for us to use Definition 10.4.1, since it allows us to simplify to some
</p>
<p>extent the exposition and to avoid auxiliary arguments (see Appendix 9).
</p>
<p>Theorem 10.4.1 (The key renewal theorem) Let τj
d= τ &ge; 0 for j &ge; 2 and g be a
</p>
<p>directly integrable function. If the random variable τ is non-arithmetic, there exists
Eτ = a &gt; 0, and the distribution of τ1 is arbitrary, then, as t &rarr;&infin;,&int; t
</p>
<p>0
</p>
<p>g(t &minus; u)dH(u)&rarr; 1
a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>g(u)du. (10.4.1)
</p>
<p>There is a measure H on [0,&infin;) associated with H that is defined by H((x, y]) :=
H(y)&minus;H(x). The integral
</p>
<p>&int; t
</p>
<p>0
</p>
<p>g(t &minus; u)dH(u)
</p>
<p>1That is, the sums n&minus;1
&sum;
</p>
<p>k g k
and n&minus;1
</p>
<p>&sum;
k gk have the same limits as n &rarr; &infin;, where g k =
</p>
<p>minu&isin;∆k g(u), gk = maxu&isin;∆k g(u), ∆k = [k∆, (k + 1)∆), and ∆ = N/n. The usual definition
of Riemann integrability over [0,&infin;) assumes that condition (1) of Definition 10.4.1 is satisfied
and the limit of
</p>
<p>&int; N
0 g(u)du as N &rarr; &infin; exists. This approach covers a wider class of functions
</p>
<p>than in Definition 10.4.1, allowing, for example, the existence of a sequence tk &rarr; &infin; such that
g(tk)&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>294 10 Renewal Processes
</p>
<p>in (10.4.1) can also be written as
&int; t
</p>
<p>0
</p>
<p>g(t &minus; u)H(du).
</p>
<p>It follows from (10.4.1), in particular, that, for any fixed u,
</p>
<p>H(t)&minus;H(t &minus; u)&rarr; u
a
. (10.4.2)
</p>
<p>It is not hard to see that this relation, which is called the local renewal theorem, is
equivalent to (10.4.1).
</p>
<p>The proof of Theorem 10.4.1 is technically rather difficult, so we have placed
</p>
<p>it in Appendix 9. One can also find there refinements of Theorem 10.4.1 and its
</p>
<p>analogue in the case where τ has a density.
</p>
<p>The other assertions of Sects. 10.2 and 10.3 can also be extended to the non-
</p>
<p>arithmetic case without any difficulties. Let all τj be nonnegative.
</p>
<p>Definition 10.4.2 In the non-arithmetic case, a renewal process η(t) is called ho-
mogeneous (and is denoted by η0(t)) if the distribution of the first jump has the
form
</p>
<p>P(τ1 &gt; x)=
1
</p>
<p>a
</p>
<p>&int; &infin;
</p>
<p>x
</p>
<p>P(τ &gt; t) dt.
</p>
<p>The ch.f. of τ1 equals
</p>
<p>ϕτ1(λ) := Eeiλτ1 =
1
</p>
<p>a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>eiλxP(τ &gt; x)dx.
</p>
<p>Since here we are integrating over x &gt; 0, the integral exists (as well as the func-
</p>
<p>tion ϕ(λ)= ϕτ (λ) := Eeiλτ ) for all λ with Imλ &gt; 0 (for λ= iα+ v, &minus;&infin;&lt; v &lt;&infin;,
α &ge; 0, the factor eiλx is equal to e&minus;αxeivx ; see property 6 of ch.f.s). Therefore, for
Imλ&ge; 0,
</p>
<p>ϕτ1(λ)=&minus;
1
</p>
<p>iλa
</p>
<p>[
1 +
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>eiλx dP(τ &gt; x)
</p>
<p>]
= ϕ(λ)&minus; 1
</p>
<p>iλa
.
</p>
<p>Theorem 10.4.2 For a homogeneous renewal process,
</p>
<p>H0(t)&equiv; Eη0(t)= 1 +
t
</p>
<p>a
, t &ge; 0.
</p>
<p>Proof This theorem can be proved in the same way as Theorem 10.2.1. Consider
the Fourier&ndash;Stieltjes transform of the function H0(t):
</p>
<p>r(λ) :=
&int; &infin;
</p>
<p>0
</p>
<p>eiλx dH0(x).
</p>
<p>Note that this transform exists for Imλ &gt; 0 and the uniqueness theorem established
</p>
<p>for ch.f.s remains true for it, since ϕ&lowast;(v) := r(iα+v)/r(iα), &minus;&infin;&lt; v &lt;&infin; (we put
λ= iα + v for a fixed α &gt; 0) can be considered as the ch.f. of a certain distribution
being the &ldquo;Cram&eacute;r transform&rdquo; (see Chap. 9) of H0(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 The Renewal Theorem in the Non-arithmetic Case 295
</p>
<p>Since τj &ge; 0, one has
</p>
<p>H0(x)=
&infin;&sum;
</p>
<p>j=0
P(Tj &le; x).
</p>
<p>As H0(t) has a unit jump at t = 0, we obtain
</p>
<p>r(λ)=
&int; &infin;
</p>
<p>0
</p>
<p>eiλx dH0(x)= 1 +
&infin;&sum;
</p>
<p>j=0
ϕτ1(λ)ϕ
</p>
<p>j (λ)= 1 + ϕ(λ)&minus; 1
iλa
</p>
<p>1
</p>
<p>1 &minus; ϕ(λ)
</p>
<p>= 1 &minus; 1
iλa
</p>
<p>.
</p>
<p>It is evident that this transform corresponds to the function H0(t) = 1 + t/a. The
theorem is proved. �
</p>
<p>In the non-arithmetic case, one has the same connections between the homoge-
</p>
<p>neous renewal process η0(t) and the limiting distribution of χ(t) and γ (t) as we
</p>
<p>had in the arithmetic case. In the same way as in Sect. 10.3, we can derive from the
</p>
<p>renewal theorem the following.
</p>
<p>Theorem 10.4.3 If τ &ge; 0 is non-arithmetic, Eτ = a, and the distribution of τ1 &ge; 0
is arbitrary, then the following limit exists
</p>
<p>lim
t&rarr;&infin;
</p>
<p>P
(
γ (t) &gt; u, χ(t) &gt; v
</p>
<p>)
= 1
</p>
<p>a
</p>
<p>&int; &infin;
</p>
<p>u+v
P(τ &gt; x)dx. (10.4.3)
</p>
<p>Proof For t &gt; u, by the total probability formula,
</p>
<p>P
(
γ (t) &gt; u,χ(t) &gt; v
</p>
<p>)
</p>
<p>= P(τ1 &gt; t + v)+
&infin;&sum;
</p>
<p>j=1
</p>
<p>&int; t&minus;u
</p>
<p>0
</p>
<p>P
(
η(t)= j + 1, Tj &isin; dx, γ (t) &gt; u,χ(t) &gt; v
</p>
<p>)
</p>
<p>= P(τ1 &gt; t + v)+
&infin;&sum;
</p>
<p>j=1
</p>
<p>&int; t&minus;u
</p>
<p>0
</p>
<p>P(Tj &isin; dx, τj+1 &gt; t &minus; x + v)
</p>
<p>= P(τ1 &gt; t + v)&minus; P(τ &gt; t + v)+
&int; t&minus;u
</p>
<p>0
</p>
<p>dH(x)P(τ &gt; t &minus; x + v). (10.4.4)
</p>
<p>Here the first two summands on the right-hand side converge to 0 as t &rarr; &infin;. By
the renewal theorem for g(x) = P(τ &gt; x + u + v) (see (10.4.1)), the last integral
converges to
</p>
<p>1
</p>
<p>a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P(τ &gt; x + u+ v)dx.
</p>
<p>The theorem is proved. �
</p>
<p>As was the case in the previous section (see Theorem 10.3.2), in the case
</p>
<p>Eτ 2 =m2 &lt;&infin; Theorem 10.4.3 allows us to refine the key renewal theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>296 10 Renewal Processes
</p>
<p>Theorem 10.4.4 If all τj
d= τ &ge; 0 are identically distributed and Eτ 2 =m2 &lt;&infin;,
</p>
<p>then, as t &rarr;&infin;,
</p>
<p>H(t)= t
a
+ m2
</p>
<p>2a2
+ o(1).
</p>
<p>Proof From (10.4.4) for u= 0 and Lemma 10.2.3 it follows that χ(t) are uniformly
integrable, for
</p>
<p>P
(
χ(t) &gt; v
</p>
<p>)
=
&int; t
</p>
<p>0
</p>
<p>dH(x)P(τ &gt; t &minus; x + v) &lt; (c1 + c2)
&sum;
</p>
<p>k&ge;0
P(τ &gt; k + v),
</p>
<p>(10.4.5)
</p>
<p>and therefore by (4.4.3)
</p>
<p>Eχ(t)&rarr; 1
a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>v
</p>
<p>P(τ &gt; u)dudv = m2
2a
</p>
<p>. (10.4.6)
</p>
<p>It remains to make use of Wald&rsquo;s identity. The theorem is proved. �
</p>
<p>One can add to relation (10.4.6) that, under the conditions of Theorem 10.4.4,
</p>
<p>one has
</p>
<p>Eχ2(t)= o(t) (10.4.7)
</p>
<p>as t &rarr;&infin;. Indeed, (10.4.5) and Lemma 10.2.3 imply
</p>
<p>P
(
χ(t) &gt; v
</p>
<p>)
&lt; (c1 + c2)
</p>
<p>&sum;
</p>
<p>k&le;t
P(τ &gt; k+ v) &lt; c
</p>
<p>&int; t
</p>
<p>0
</p>
<p>P(τ &gt; z+ v)dz.
</p>
<p>Further, integrating by parts, we obtain
</p>
<p>Eχ2(t)=&minus;
&int; &infin;
</p>
<p>0
</p>
<p>v2 dP
(
χ(t) &gt; v
</p>
<p>)
</p>
<p>= 2
&int; &infin;
</p>
<p>0
</p>
<p>vP
(
χ(t) &gt; v
</p>
<p>)
dv &lt; 2c
</p>
<p>&int; t
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>vP(τ &gt; z+ v)dv dz,
</p>
<p>(10.4.8)
</p>
<p>where the inner integral converges to zero as z&rarr;&infin;:
&int; &infin;
</p>
<p>0
</p>
<p>vP(τ &gt; z+ v)dv = 1
2
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>v2 dP(τ &lt; z+ v) &lt; 1
2
E
(
τ 2; τ &gt; z
</p>
<p>)
&rarr; 0.
</p>
<p>This and (10.4.8) imply (10.4.7).
</p>
<p>Note also that if only Eτ exists, then, by Theorem 10.1.1, we have Eχ(t)= o(t)
and, by Theorem 10.4.1 (or 10.4.3),
</p>
<p>P
(
χ(t) &gt; v
</p>
<p>)
&rarr; 1
</p>
<p>a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P(τ &gt; u+ v)du.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 The Renewal Theorem in the Non-arithmetic Case 297
</p>
<p>Now let, as before, γ and χ denote random variables distributed according to the
</p>
<p>limiting distribution (10.4.3). Similarly to the above, it is not hard to establish that
</p>
<p>if Eτ k &lt;&infin;, k &ge; 1, then, as t &rarr;&infin;,
Eχk&minus;1(t)&rarr; Eχk&minus;1 &lt;&infin;, Eχk(t)= o(t).
</p>
<p>Further, it is seen from Theorem 10.4.3 that each of the random variables γ and
</p>
<p>χ has density equal to a&minus;1P(τ &gt; x). The joint distribution of γ and χ may have no
density. If τ has density f (x) then there exists a joint density of γ and χ equal to
</p>
<p>a&minus;1f (x + y). It also follows from Theorem 10.4.3 that γ and χ are independent if
and only if
</p>
<p>&int; &infin;
</p>
<p>x
</p>
<p>P(τ &gt; u)du= 1
α
e&minus;αx
</p>
<p>for some α &gt; 0, i.e. independence takes place only for the exponential distribution
</p>
<p>τ &sub;=Ŵα .
Moreover, for homogeneous renewal processes the coincidence of P(τ1 &gt; x)
</p>
<p>and P(τ &gt; x) takes place only when τ &sub;= Ŵα . In other words, the renewal pro-
cess generated by a sequence of identically distributed random variables τ1, τ, . . .
</p>
<p>will be homogeneous if and only if τj &sub;= Ŵα . In that case η0(t) is called (see also
Sect. 19.4) a Poisson process. This is because for such a process, for each t , the
variable η(t)= η0(t) has the Poisson distribution with parameter t/α.
</p>
<p>The Poisson process has some other remarkable properties as well (see also
</p>
<p>Sect. 19.4). Clearly, one has χ(t)&sub;= Ŵα for such a process, and moreover, the vari-
ables γ (t) and χ(t) are independent. Indeed, by (10.4.4), taking into account that
</p>
<p>H(x) has a jump of magnitude 1 at the point x = 0, we obtain for u &lt; t that
</p>
<p>P
(
γ (t) &gt; u,χ(t) &gt; v
</p>
<p>)
= e&minus;α(t+v) + α
</p>
<p>&int; t&minus;u
</p>
<p>0
</p>
<p>e&minus;α(t&minus;x+v) dx
</p>
<p>= e&minus;α(u+v) = P
(
γ (t) &gt; u
</p>
<p>)
P
(
χ(t) &gt; v
</p>
<p>)
;
</p>
<p>P
(
γ (t)= t, χ(t) &gt; v
</p>
<p>)
= P(τ1 &gt; t + v)= e&minus;α(t+v) = P
</p>
<p>(
γ (t)= t
</p>
<p>)
P
(
χ(t) &gt; v
</p>
<p>)
;
</p>
<p>P
(
γ (t) &gt; t
</p>
<p>)
= 0.
</p>
<p>These relations also imply that the random variable τη(t) = γ (t) + χ(t) has the
same distribution as min(t, τ1)+ τ2, where τj &sub;= Ŵα , j = 1,2, are independent so
that τη(t) &sub;&rArr;Ŵα,2 as t &rarr;&infin;.
</p>
<p>The fact that γ (t) and χ(t) are independent of each other deserves attention
</p>
<p>from the point of view of its interpretation. It means the following. The residual
</p>
<p>lifetime of the component operating at a given time t has the same distribution as
</p>
<p>the lifetime of a new component (recall that τj &sub;=Ŵα) and is independent of how long
this component has already been working (which at first glance is a paradox). Since
</p>
<p>the lifetime distributions of devices consisting of large numbers of reliable elements
</p>
<p>are close to the exponential law (see Theorem 20.3.2), the above-mentioned fact is
</p>
<p>of significant practical interest.
</p>
<p>If τi can assume negative values as well, the problems related to the distributions
of γ (t) and χ(t) become much more complicated. To some extent such problems</p>
<p/>
</div>
<div class="page"><p/>
<p>298 10 Renewal Processes
</p>
<p>can be reduced to the case of nonnegative variables, since the distribution of χ(t)
</p>
<p>coincides with that of the variable χ&lowast;(t) constructed from a sequence {τ &lowast;j &ge; 0},
where τ &lowast;j have the same distribution as χ(0). The distribution of χ(0) can be found
using the methods of Chap. 12.
</p>
<p>In particular, for random variables τ1, τ2, . . . taking values of both signs, Theo-
rems 10.4.1 and 10.4.3 imply the following assertion.
</p>
<p>Corollary 10.4.1 Let τ1, τ2, . . . be non-arithmetic independent and identically dis-
tributed and Eτ1 = a. Then the following limit exists
</p>
<p>lim
t&rarr;&infin;
</p>
<p>P
(
χ(t) &gt; v
</p>
<p>)
= 1
</p>
<p>Eχ(0)
</p>
<p>&int; &infin;
</p>
<p>v
</p>
<p>P
(
χ(0) &gt; t
</p>
<p>)
dt, v &gt; 0.
</p>
<p>For arithmetic τj ,
</p>
<p>lim
k&rarr;&infin;
</p>
<p>P
(
χ(k)= i
</p>
<p>)
= 1
</p>
<p>Eχ(0)
P
(
χ(0) &gt; i
</p>
<p>)
, i &gt; 0.
</p>
<p>10.5 The Law of Large Numbers and the Central Limit
</p>
<p>Theorem for Renewal Processes
</p>
<p>In this section we return to the general case where τj are not necessarily identically
</p>
<p>distributed (cf. Sect. 10.1).
</p>
<p>10.5.1 The Law of Large Numbers
</p>
<p>First assume that τj &ge; 0 and put
</p>
<p>ak := Eτk, An :=
n&sum;
</p>
<p>k=1
ak.
</p>
<p>Theorem 10.5.1 Let τk &ge; 0 be independent, τk &minus; ak uniformly integrable, and
n&minus;1An &rarr; a &gt; 0 as n&rarr;&infin;. Then, as t &rarr;&infin;,
</p>
<p>η(t)
</p>
<p>t
</p>
<p>p&rarr; 1
a
.
</p>
<p>Proof The basic relation we shall use is the equality
{
η(t) &gt; n
</p>
<p>}
= {Tn &le; t}, (10.5.1)
</p>
<p>which implies
</p>
<p>P
</p>
<p>(
η(t)
</p>
<p>t
&minus; 1
</p>
<p>a
&gt; ε
</p>
<p>)
= P
</p>
<p>(
η(t) &gt;
</p>
<p>t
</p>
<p>a
(+ε)
</p>
<p>)
= P(Tn &le; t),</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 LLN and CLT for Renewal Processes 299
</p>
<p>where for simplicity we assume that n= t
a
(1 + ε) is an integer. Further,
</p>
<p>P(Tn &le; t)= P
(
Tn
</p>
<p>n
&le; a
</p>
<p>1 + ε
</p>
<p>)
</p>
<p>= P
(
Tn &minus;An
</p>
<p>n
&le; a
</p>
<p>1 + ε &minus;
An
</p>
<p>n
</p>
<p>)
&le; P
</p>
<p>(
Tn &minus;An
</p>
<p>n
&le;&minus;aε
</p>
<p>2
</p>
<p>)
</p>
<p>for n large enough and ε small enough. Applying the law of large numbers to the
</p>
<p>right-hand side of this relation (Theorem 8.3.3), we obtain that, for any ε &gt; 0, as
</p>
<p>t &rarr;&infin;,
</p>
<p>P
</p>
<p>(
η(t)
</p>
<p>t
&minus; 1
</p>
<p>a
&gt;
</p>
<p>ε
</p>
<p>a
</p>
<p>)
&rarr; 0.
</p>
<p>The probability P(
η(t)
t
</p>
<p>&minus; 1
a
&lt;&minus; ε
</p>
<p>a
) can be bounded in the same way. The theorem
</p>
<p>is proved. �
</p>
<p>10.5.2 The Central Limit Theorem
</p>
<p>Put
</p>
<p>σ 2k := E(τk &minus; ak)2 = Var τk, B2n :=
n&sum;
</p>
<p>k=1
σ 2k .
</p>
<p>Theorem 10.5.2 Let τk &ge; 0 and the random variables τk &minus; ak satisfy the Lindeberg
condition: for any δ &gt; 0 and n&rarr;&infin;,
</p>
<p>n&sum;
</p>
<p>k=1
E
(
|τk &minus; ak|2; |τk &minus; ak|&gt; δBn
</p>
<p>)
= o
</p>
<p>(
B2n
</p>
<p>)
.
</p>
<p>Let, moreover, there exist a &gt; 0 and σ &gt; 0 such that, as n&rarr;&infin;,
</p>
<p>An :=
n&sum;
</p>
<p>k=1
ak = an+ o(
</p>
<p>&radic;
n), B2n = σ 2n+ o(n). (10.5.2)
</p>
<p>Then
η(t)&minus; t/a
σ
&radic;
t/a3
</p>
<p>&sub;&rArr;�0,1. (10.5.3)
</p>
<p>Proof From (10.5.1) we have
</p>
<p>P
(
η(t) &gt; n
</p>
<p>)
= P(Tn &le; t)= P
</p>
<p>(
Tn &minus;An
</p>
<p>Bn
&le; t &minus;An
</p>
<p>Bn
</p>
<p>)
. (10.5.4)
</p>
<p>Let n vary as t &rarr;&infin; so that
t &minus;An
Bn
</p>
<p>&rarr; v</p>
<p/>
</div>
<div class="page"><p/>
<p>300 10 Renewal Processes
</p>
<p>for a fixed v. To find such an n, solve for n the equation
</p>
<p>t &minus; an
σ
&radic;
n
</p>
<p>= v.
</p>
<p>This is a quadratic equation in n, and its solution has the form
</p>
<p>n= t
a
&plusmn; vσ
</p>
<p>a2
</p>
<p>&radic;
at
</p>
<p>(
1 +O
</p>
<p>(
1&radic;
t
</p>
<p>))
. (10.5.5)
</p>
<p>For such n, by (10.5.2),
</p>
<p>t &minus;An
Bn
</p>
<p>=
[
∓vσ
</p>
<p>a
</p>
<p>&radic;
at + o(
</p>
<p>&radic;
t )
</p>
<p>]
(1 + o(1))
σ
&radic;
t/a
</p>
<p>=∓v+ o(1).
</p>
<p>This equality means that we have to choose the minus sign in (10.5.5). Therefore,
</p>
<p>by (10.5.4) and the central limit theorem,
</p>
<p>P
(
η(t) &gt; n
</p>
<p>)
= P
</p>
<p>(
η(t)&minus; t/a
σ
&radic;
t/a3
</p>
<p>&gt;&minus;v+ o(1)
)
&rarr;Φ(v)= 1 &minus;Φ(&minus;v).
</p>
<p>Changing &minus;v to u, by the continuity theorems (see Lemma 6.2.2) we get
</p>
<p>P
</p>
<p>(
η(t)&minus; t/a
σ
&radic;
ta&minus;3
</p>
<p>&lt; u
</p>
<p>)
&rarr;Φ(u).
</p>
<p>The theorem is proved. �
</p>
<p>Remark 10.5.1 In Theorems 10.5.1 and 10.5.2 we considered the case where An
grows asymptotically linearly as n&rarr;&infin;. Then the centring parameter t/a for η(t)
changes asymptotically linearly as well. However, nothing prevents us from consid-
</p>
<p>ering a more general case where, say, An &sim; cnα , α &gt; 0. Then the centring parameter
for η(t) will be the solution to the equation cnα = t , i.e. the function (t/c)1/α (under
the conditions of Theorem 10.5.2, in this case we have to assume that Bn = o(An)).
The asymptotics of the renewal function will have the same form.
</p>
<p>In order to extend the assertions of Theorems 10.5.1 and 10.5.2 to τj assuming
</p>
<p>values of both signs, we need some auxiliary assertions that are also of independent
</p>
<p>interest.
</p>
<p>10.5.3 A Theorem on the Finiteness of the Infimum of the
</p>
<p>Cumulative Sums
</p>
<p>In this subsection we will consider identically distributed independent random vari-
ables τ1, τ2, . . . . We first state the following simple assertion in the form of a lemma.
</p>
<p>Lemma 10.5.1 One has E|τ |&lt;&infin; if and only if
&infin;&sum;
</p>
<p>j=1
P
(
|τ |&gt; j
</p>
<p>)
&lt;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 LLN and CLT for Renewal Processes 301
</p>
<p>The Proof follows in an obvious way from the equality
</p>
<p>E|τ | =
&int; &infin;
</p>
<p>0
</p>
<p>P
(
|τ |&gt; x
</p>
<p>)
dx
</p>
<p>and the inequalities
</p>
<p>&infin;&sum;
</p>
<p>j=1
P
(
|τ |&gt; j
</p>
<p>)
&le;
&int; &infin;
</p>
<p>0
</p>
<p>P
(
|τ |&gt; x
</p>
<p>)
dx &le; 1 +
</p>
<p>&infin;&sum;
</p>
<p>j=1
P
(
|τ |&gt; j
</p>
<p>)
.
</p>
<p>�
</p>
<p>Let, as before,
</p>
<p>Tn =
n&sum;
</p>
<p>j=1
τj .
</p>
<p>Theorem 10.5.3 If τj
d= τ are identically distributed and independent and Eτ &gt; 0,
</p>
<p>then the random variable Z := infk&ge;0 Tk is proper (finite with probability 1).
</p>
<p>Proof Let η1 = η(1) be the number of the first sum Tk to exceed level 1. Consider
the sequence {τ &lowast;k = τη1+k} that, by Lemma 10.2.1, has the same distribution as {τk}
and is independent of η1, τ1, . . . , τη1 . For this sequence, denote by η2 the subscript
</p>
<p>k for which the sum T &lowast;k =
&sum;k
</p>
<p>j=1 τ
&lowast;
j first exceeds level 1. It is clear that the random
</p>
<p>variables η1 and η2 are identically distributed and independent. Next, construct for
</p>
<p>the sequence {τ &lowast;&lowast;k = τη1+η2+k} the random variable η3 following the same rule,
and so on. As a result we will obtain a sequence of Markov times η1, η2, . . . that
</p>
<p>determine the times of &ldquo;renewals&rdquo; of the original sequence {Tk}, associated with
attaining level 1.
</p>
<p>Now set
</p>
<p>Z1 := min
k&lt;η1
</p>
<p>Tk, Z2 := min
k&lt;η2
</p>
<p>T &lowast;k , . . .
</p>
<p>Clearly, the Zj are identically distributed and
</p>
<p>Z = inf{Z1, Tη1 +Z2, Tη1+η2 +Z3, . . .},
where by definition Tη1 &gt; 1, Tη1+η2 &gt; 2 and so on. Hence
</p>
<p>{Z &lt;&minus;N} =
&infin;⋃
</p>
<p>k=0
{Zk+1 + Tη1+&middot;&middot;&middot;+ηk &lt;&minus;N} &sub;
</p>
<p>&infin;⋃
</p>
<p>k=0
{Zk + k &lt;&minus;N},
</p>
<p>P(Z &lt;&minus;N)&le;
&infin;&sum;
</p>
<p>k=1
P(Zk + k &lt;&minus;N)=
</p>
<p>&infin;&sum;
</p>
<p>j=N+1
P(Z1 &lt;&minus;j).
</p>
<p>This expression tends to 0 as N &rarr;&infin; provided that E|Z1|&lt;&infin; (see Lemma 10.5.1).
It remains to verify the finiteness of EZ1, which follows from the finiteness of
</p>
<p>Eη1 = Eη(1)=H(1) &lt; c (see Example 4.4.5) and the relations
</p>
<p>E|Z1| &le; E
η1&sum;
</p>
<p>j=1
|τj | = Eη1E|τ1|&lt;&infin;
</p>
<p>(see Theorem 4.4.2). �</p>
<p/>
</div>
<div class="page"><p/>
<p>302 10 Renewal Processes
</p>
<p>10.5.4 Stochastic Inequalities. The Law of Large Numbers and the
</p>
<p>Central Limit Theorem for the Maximum of Sums of
</p>
<p>Non-identically Distributed Random Variables Taking
</p>
<p>Values of Both Signs
</p>
<p>In this subsection we extend the assertions of some theorems of Chap. 8 to maxima
</p>
<p>of sums of random variables with a positive &ldquo;mean drift&rdquo;. To do this we will have to
</p>
<p>introduce some additions restrictions that are always satisfied when the summands
</p>
<p>are identically distributed. Here we will need the notion of stochastic inequalities
</p>
<p>(or inequalities in distribution). Let ξ and ζ be given random variables.
</p>
<p>Definition 10.5.1 We will say that ζ majorises (minorises) ξ in distributionand de-
</p>
<p>note this by ξ
d
&le; ζ (ξ
</p>
<p>d
&ge; ζ ) if, for all t ,
</p>
<p>P(ξ &ge; t)&le; P(ζ &ge; t)
(
P(ξ &ge; t)&ge; P(ζ &ge; t)
</p>
<p>)
.
</p>
<p>Clearly, if ξ
d
&le; ζ then &minus;ξ
</p>
<p>d
&ge; &minus;ζ . We show that stochastic inequalities possess
</p>
<p>some other properties of ordinary inequalities.
</p>
<p>Lemma 10.5.2 If {ξk}&infin;k=1 and {ζ }&infin;k=1 are sequences of independent (in each se-
quence) random variables and ξk
</p>
<p>d
&le; ζk , then, for all n,
</p>
<p>Sn
d
&le; Zn, Sn
</p>
<p>d
&le;Zn,
</p>
<p>where
</p>
<p>Sn =
n&sum;
</p>
<p>k=1
ξk, Zn =
</p>
<p>n&sum;
</p>
<p>k=1
ζk, Sn = max
</p>
<p>k&le;n
Sk, Zn = max
</p>
<p>k&le;n
Zk.
</p>
<p>Similarly, if ξk
d
&ge; ζk , then mink&le;n Sk
</p>
<p>d
&ge; mink&le;nZk .
</p>
<p>Proof Let Fk(t) := P(ξk &lt; t) and Gk(t) := P(ζk &lt; t). Using quantile transforma-
tions F
</p>
<p>(&minus;1)
k and G
</p>
<p>(&minus;1)
k (see Definition 3.2.4) and a sequence of independent random
</p>
<p>variables {ωk}&infin;k=1, ωk &sub;=U0,1, we can construct on a common probability space the
sequences ξ&lowast;k = F
</p>
<p>(&minus;1)
k (ωk) and ζ
</p>
<p>&lowast;
k =G
</p>
<p>(&minus;1)
k (ωk) such that ξ
</p>
<p>&lowast;
k
</p>
<p>d= ξk and ζ &lowast;k
d= ζk (the
</p>
<p>distributions of ξ&lowast;k and ξk and of ζ
&lowast;
k and ζ
</p>
<p>&lowast;
k coincide). Moreover, ξ
</p>
<p>&lowast;
k &le; ζ &lowast;k , which is
</p>
<p>a direct consequence of the inequality Fk(t) &ge;Gk(t) for all t . Endowing with the
superscript &lowast; all the notations for sums and maximum of sums of random variables
with asterisks, we obviously obtain that
</p>
<p>Sn
d= S&lowast;n &le; Z&lowast;n
</p>
<p>d= Zn, Sn d= S
&lowast;
n &le;Z
</p>
<p>&lowast;
n
</p>
<p>d= Zn.
The last assertion of the lemma follows from the previous ones. The lemma is
</p>
<p>proved. �
</p>
<p>Below we will need the following corollary of Theorem 10.5.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 LLN and CLT for Renewal Processes 303
</p>
<p>Lemma 10.5.3 Let ξk be independent, ξk
d
&ge; ζ for all k and Eζ &gt; 0. Then, for all n,
</p>
<p>the random variable
</p>
<p>Dn := Sn &minus; Sn &ge; 0
</p>
<p>is majorised in distribution by the random variable &minus;Z: Dn
d
&le; &minus;Z, where Z :=
</p>
<p>infZk , Zk :=
&sum;k
</p>
<p>j=1 ζj and ζj are independent copies of ζ .
</p>
<p>Proof We have
</p>
<p>Sn = max(0, S1, . . . , Sn)= Sn + max(0,&minus;ξn,&minus;ξn &minus; ξn&minus;1, . . . ,&minus;Sn)
= Sn &minus; min(0, ξn, ξn + ξn&minus;1, . . . , Sn),
</p>
<p>where, by the last assertion of Lemma 10.5.2,
</p>
<p>&minus;Dn &equiv; min(0, ξn, ξn + ξn&minus;1, . . . , Sn)
d
&ge; min
</p>
<p>k&le;n
Zk &ge; Z, Dn
</p>
<p>d
&le;&minus;Z.
</p>
<p>The fact that Z is a proper random variable follows from Theorem 10.5.3 on the
</p>
<p>finiteness of the infimum of partial sums. The lemma is proved. �
</p>
<p>If ξk
d= ξ are identically distributed and a = Eξ &gt; 0, then we can put ξ = ζ . The
</p>
<p>above reasoning shows that in this case the limit distribution of Sn &minus; Sn as n&rarr;&infin;
exists and coincides with the distribution of the random variable Z (the random
</p>
<p>variables Sn &minus; Sn themselves do not have a limit, and, by the way, neither do the
variables Sn&minus;an&radic;
</p>
<p>n
in the central limit theorem).
</p>
<p>Lemma 10.5.3 shows that, for ξk
d
&ge; ζ and Eζ &gt; 0, the random variables Sn and
</p>
<p>Sn differ from each other by a proper random variable only. This makes the limit
</p>
<p>theorems for Sn and Sn essentially the same.
</p>
<p>We proceed to the law of large numbers and the central limit theorem for Sn.
</p>
<p>Theorem 10.5.4 Let ak = Eξk &gt; 0, An =
&sum;n
</p>
<p>k=1 ak and An &sim; an as n&rarr;&infin;, a &gt; 0.
Let, moreover, ξk &minus; ak be uniformly integrable for all k and ξk
</p>
<p>d
&ge; ζ with Eζ &gt; 0.
</p>
<p>Then, as n&rarr;&infin;,
</p>
<p>Sn
</p>
<p>n
</p>
<p>p&minus;&rarr; a.
</p>
<p>Note that the left uniform integrability of ξk &minus; ak follows from the inequalities
ξk
</p>
<p>d
&ge; ζ .
</p>
<p>Proof By Lemma 10.5.3,
</p>
<p>Sn = Sn +Dn, where Dn &ge; 0, Dn
d
&le;&minus;Z. (10.5.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>304 10 Renewal Processes
</p>
<p>Therefore,
</p>
<p>Sn
</p>
<p>n
= Sn &minus;An
</p>
<p>n
+ An
</p>
<p>n
+ Dn
</p>
<p>n
,
</p>
<p>where by Theorem 8.3.3, as n&rarr;&infin;,
Sn &minus;An
</p>
<p>n
</p>
<p>p&minus;&rarr; 0.
</p>
<p>It is also clear that
</p>
<p>An
</p>
<p>n
&rarr; a, Dn
</p>
<p>n
</p>
<p>p&minus;&rarr; 0.
</p>
<p>The theorem is proved. �
</p>
<p>In addition to the notation from Theorem 10.5.3, put
</p>
<p>σ 2k := E(ξk &minus; ak)2, B2n :=
n&sum;
</p>
<p>k=1
σ 2k .
</p>
<p>Theorem 10.5.5 Let, for some a &gt; 0 and σ &gt; 0,
</p>
<p>An = an+ o(
&radic;
n ), B2n = σ 2n+ o(n),
</p>
<p>and let the random variables ξk &minus; ak satisfy the Lindeberg condition, ξk
d
&ge; ζ with
</p>
<p>Eζ &gt; 0. Then
</p>
<p>Sn &minus; an
σ
&radic;
n
</p>
<p>&sub;&rArr;�0,1. (10.5.7)
</p>
<p>Proof By virtue of (10.5.6),
</p>
<p>Sn &minus; an
σ
&radic;
n
</p>
<p>= Sn &minus;An
Bn
</p>
<p>&middot; Bn
σ
&radic;
n
+ An &minus; an
</p>
<p>σ
&radic;
n
</p>
<p>+ Dn
σ
&radic;
n
, (10.5.8)
</p>
<p>where, by the central limit theorem,
</p>
<p>Sn &minus;An
Bn
</p>
<p>&sub;&rArr;�0,1.
</p>
<p>Moreover,
</p>
<p>Bn
</p>
<p>σ
&radic;
n
</p>
<p>p&minus;&rarr; 1, An &minus; an
σ
&radic;
n
</p>
<p>&rarr; 0, Dn
σ
&radic;
n
</p>
<p>p&minus;&rarr; 0.
</p>
<p>This and (10.5.8) imply (10.5.7). The theorem is proved. �
</p>
<p>10.5.5 Extension of Theorems 10.5.1 and 10.5.2 to Random
</p>
<p>Variables Assuming Values of Both Signs
</p>
<p>We return to renewal processes and limit theorems for them. In Theorems 10.5.1
</p>
<p>and 10.5.2 we obtained the law of large numbers and the central limit theorem for</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 LLN and CLT for Renewal Processes 305
</p>
<p>the renewal process η(t) defined in (10.1.1) with jumps τk &ge; 0. Now we drop the
last assumption and assume that τj can take values of both signs.
</p>
<p>Theorem 10.5.6 Let the conditions of Theorem 10.5.1 be met, the condition τk &ge; 0
being replaced with the condition τk
</p>
<p>d
&ge; ζ with Eζ &gt; 0. Then
</p>
<p>η(t)
</p>
<p>t
</p>
<p>p&minus;&rarr; 1
a
. (10.5.9)
</p>
<p>If τk
d= τ are identically distributed and Eτ &gt; 0, then we can put ζ = τ . There-
</p>
<p>fore Theorem 10.5.6 implies the following result.
</p>
<p>Corollary 10.5.1 If τk are independent and identically distributed and Eτ = a &gt; 0,
then (10.5.9) holds true.
</p>
<p>Proof of Theorem 10.5.6 Here instead of (10.5.1) we should use the relation
</p>
<p>{
η(t) &gt; n
</p>
<p>}
= {T n &le; t}, T n = max
</p>
<p>k&le;n
Tk, Tk =
</p>
<p>k&sum;
</p>
<p>j=1
τj . (10.5.10)
</p>
<p>Then we repeat the argument from the proof of Theorem 10.5.1, changing in it Tn
to T n and using Theorem 10.5.4, which implies that T n and Tn satisfy the law of
</p>
<p>large numbers. The theorem is proved. �
</p>
<p>Theorem 10.5.7 Let the conditions of Theorem 10.5.2 be met, the condition τk &ge; 0
being replaced with the condition τk
</p>
<p>d
&ge; ζ with Eζ &gt; 0. Then (10.5.3) holds true.
</p>
<p>Proof Here we again have to use (10.5.10), instead of (10.5.1), and then repeat the
argument proving Theorem 10.5.2 using Theorem 10.5.5, which implies that the
</p>
<p>distribution of T n&minus;an
σ
&radic;
n
</p>
<p>, as well as the distribution of Tn&minus;an
σ
&radic;
n
</p>
<p>, converges to the standard
</p>
<p>normal law �0,1. The theorem is proved. �
</p>
<p>Remark 10.5.2 (An analogue of Remarks 8.3.3, 8.4.1 and 10.1.1) The assertions of
Theorems 10.5.6 and 10.5.7 can be generalised as follows. Let τ1 be an arbitrary
random variable and random variables τ &lowast;k := τ1+k , k &ge; 1, satisfy the conditions
of Theorem 10.5.6 (Theorem 10.5.7). Then convergence (10.5.9) (10.5.3) still takes
place.
</p>
<p>Consider, for example, Theorem 10.5.7. Denote by Ax the event
</p>
<p>Ax :=
{
η(t)&minus; a/t
σ
&radic;
t/a3
</p>
<p>&lt; x
</p>
<p>}
.
</p>
<p>Then the foregoing assertion follows from the relations
</p>
<p>P(Ax)= E
[
P(Ax |τ1); |τ1| &le;N
</p>
<p>]
+ rN ,</p>
<p/>
</div>
<div class="page"><p/>
<p>306 10 Renewal Processes
</p>
<p>where rN &le; P(|τ1|&gt; N) can be made arbitrarily small by the choice of N , and by
Theorem 10.5.7
</p>
<p>P(Ax |τ1)= P
(
η&lowast;(t &minus; τ1)&minus; t&minus;τ1a
σ
&radic;
(t &minus; τ1)/a3
</p>
<p>+O
(
</p>
<p>1&radic;
t
</p>
<p>)
&lt; x
</p>
<p>)
&rarr;�(x)
</p>
<p>as t &rarr;&infin; for each fixed τ1, |τ1| &le;N . Here η&lowast;(t) is the renewal process that corre-
sponds to the sequence {τ &lowast;k }. �
</p>
<p>10.5.6 The Local Limit Theorem
</p>
<p>If we again narrow our assumptions and return to identically distributed τk
d= τ &ge; 0
</p>
<p>then we can derive local theorems more precise than Theorem 10.5.2. In this sub-
</p>
<p>section we will find an asymptotic representation for P(η(t) = n) as t &rarr; &infin;. We
know from Theorem 10.5.2 what range of values of n the bulk of the distribution
</p>
<p>of η(t) is concentrated in. Therefore we will from the start consider not arbitrary n,
</p>
<p>but the values of n that can be represented as
</p>
<p>n=
[
t
</p>
<p>a
+ vσ
</p>
<p>&radic;
t
</p>
<p>a3
</p>
<p>]
, σ 2 = Var(τ ), (10.5.11)
</p>
<p>for &ldquo;proper&rdquo; values of v ([s] in (10.5.11) is the integer part of s), so that
(t &minus; an)
σ
&radic;
n
</p>
<p>= v+O
(
</p>
<p>1&radic;
t
</p>
<p>)
(10.5.12)
</p>
<p>(see (10.5.5)). For the proof, it will be more convenient to consider the probabilities
</p>
<p>P(η(t)= n+ 1). Changing n+ 1 to n amends nothing in the argument below.
</p>
<p>Theorem 10.5.8 If τ &ge; 0 is either non-lattice or arithmetic and Var(τ )= σ 2 &lt;&infin;,
then, for the values of n defined in (10.5.11), as t &rarr;&infin;,
</p>
<p>P
(
η(t)= n+ 1
</p>
<p>)
&sim; a
</p>
<p>3/2
</p>
<p>σ
&radic;
</p>
<p>2πt
e&minus;v
</p>
<p>2/2, (10.5.13)
</p>
<p>where in the arithmetic case t is assumed to be integer.
</p>
<p>Proof First let, for simplicity, τ have a density and satisfy the conditions of the local
limit Theorem 8.7.2. Then
</p>
<p>P
(
η(t)= n+ 1
</p>
<p>)
=
&int; t
</p>
<p>0
</p>
<p>P(Tn &isin; du)P(τ &gt; t &minus; u), (10.5.14)
</p>
<p>where by Theorem 8.7.2, as n&rarr;&infin;,
</p>
<p>P
(
Tn &minus; na &isin; d(u&minus; na)
</p>
<p>)
= du
</p>
<p>σ
&radic;
</p>
<p>2πn
</p>
<p>[
exp
</p>
<p>{
&minus; (u&minus; na)
</p>
<p>2
</p>
<p>2nσ 2
</p>
<p>}
+ o(1)
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Generalised Renewal Processes 307
</p>
<p>uniformly in u. Change the variable u = t &minus; z. Since for the values of n we are
dealing with one has (10.5.12), the exponential
</p>
<p>exp
</p>
<p>{
&minus; (u&minus; na)
</p>
<p>2
</p>
<p>2nσ 2
</p>
<p>}
= exp
</p>
<p>{
&minus;1
</p>
<p>2
</p>
<p>(
v&minus; z
</p>
<p>σ
&radic;
n
</p>
<p>)2}
</p>
<p>remains &ldquo;almost constant&rdquo; and asymptotically equivalent to e&minus;v
2/2 for |z| &lt; N ,
</p>
<p>N&rarr;&infin;, N = o(&radic;n). Hence the integral in (10.5.14) is asymptotically equivalent
to
</p>
<p>1
</p>
<p>σ
&radic;
</p>
<p>2πn
e&minus;v
</p>
<p>2/2
</p>
<p>&int; N
</p>
<p>0
</p>
<p>P(τ &gt; z)dz&sim; a
σ
&radic;
</p>
<p>2πn
ev
</p>
<p>2/2.
</p>
<p>Since n&sim; t/a as t &rarr;&infin;, we obtain (10.5.13).
If τ has no density, but is non-lattice, then we should use the integro-local Theo-
</p>
<p>rem 8.7.1 for small ∆ and, in a quite similar fashion, bound the integral in (10.5.14)
</p>
<p>(with t , which is a multiple of ∆) from above and from below by the sums
</p>
<p>t/∆&minus;1&sum;
</p>
<p>k=0
P
(
Tn &isin;∆[k∆)
</p>
<p>)
P
(
τ &gt; t &minus; (k + 1)∆
</p>
<p>)
</p>
<p>and
</p>
<p>t/∆&minus;1&sum;
</p>
<p>k=0
P
(
Tn &isin;∆[k∆)
</p>
<p>)
P(τ &gt; t &minus; k∆),
</p>
<p>respectively. For small ∆ both bounds will be close to the right-hand side
</p>
<p>of (10.5.13).
</p>
<p>If τ has an arithmetic distribution then we have to replace integral (10.5.14) with
</p>
<p>the corresponding sum and, for integer u and t , make use of Theorem 8.7.3.
</p>
<p>The theorem is proved. �
</p>
<p>If examine the arguments in the proof concerning the behaviour of the correction
</p>
<p>term, then, in addition to (10.5.13), we can also obtain the representation
</p>
<p>P
(
η(t)= n
</p>
<p>)
= a
</p>
<p>3/2
</p>
<p>σ
&radic;
</p>
<p>2πt
e&minus;v
</p>
<p>2/2 + o
(
</p>
<p>1&radic;
t
</p>
<p>)
(10.5.15)
</p>
<p>uniformly in v (or in n).
</p>
<p>10.6 Generalised Renewal Processes
</p>
<p>10.6.1 Definition and Some Properties
</p>
<p>Let, instead of the sequence {τj }&infin;j=1, there be given a sequence of two-dimensional
independent vectors (τj , ξj ), τj &ge; 0, having the same distribution as (τ, ξ). Let, as
before,</p>
<p/>
</div>
<div class="page"><p/>
<p>308 10 Renewal Processes
</p>
<p>Sk =
k&sum;
</p>
<p>j=1
ξj , Tk =
</p>
<p>k&sum;
</p>
<p>j=1
τj , S0 = T0 = 0,
</p>
<p>η(t)= min{k : Tk &gt; t}, ν(t)= max{k : Tk &le; t} = η(t)&minus; 1.
</p>
<p>Definition 10.6.1 The process
</p>
<p>S(ν)(t)= qt + Sν(t)
is called a generalised renewal process with linear drift q .
</p>
<p>The process S(ν)(t), as well as ν(t), is right-continuous. Clearly, S(ν)(t)= qt for
t &lt; τ1. At time t = τ1 the first jump in the process S(ν)(t) occurs, which is of size ξ1:
</p>
<p>S(ν)(τ1 &minus; 0)= qτ1, S(ν)(τ1)= qτ1 + ξ1.
After that, on the interval [T1, T2) the value of S(ν)(t) varies linearly with slope q .
At the point T2, the second jump occurs, which is of size ξ2, and so on.
</p>
<p>Generalised renewal processes are evidently a generalisation of random walks Sk
(for τj &equiv; 1, q = 0) and renewal processes η(t)= ν(t)+ 1 (for ξj &equiv; 1, q = 0). They
are widespread in applications, as mathematical models of various physical systems.
</p>
<p>Along with the process S(ν)(t), we will consider generalised renewal processes
</p>
<p>of the form
</p>
<p>S(t)= qt + Sη(t) = S(ν)(t)+ ξη(t),
that are in a certain sense more convenient to analyse since η(t) is a Markov time
</p>
<p>with respect to Fn = σ(τ1, . . . , τn; ξ1, . . . , ξn) and has already been well studied.
The fact that the asymptotic properties of the processes S(t) and S(ν)(t), as
</p>
<p>t &rarr;&infin;, (the law of large numbers, the central limit theorem) are identical follows
from the next assertion, which shows that the difference S(t)&minus; S(ν)(t) has a proper
limiting distribution.
</p>
<p>Lemma 10.6.1 If Eτ &lt;&infin;, then the following limiting distribution exists
</p>
<p>lim
t&rarr;&infin;
</p>
<p>P(ξη(t) &lt; v)=
E(τ ; ξ &lt; v)
</p>
<p>Eτ
.
</p>
<p>The lemma implies that ξη(t)/b(t)
p&minus;&rarr; 0 for any function b(t)&rarr;&infin; as t &rarr;&infin;.
</p>
<p>Proof By virtue of the key renewal theorem,
</p>
<p>P(ξη(t) &lt; v)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>&int; t
</p>
<p>0
</p>
<p>P(Tk &isin; du)P(τ &gt; t &minus; u, ξ &lt; v)
</p>
<p>=
&int; t
</p>
<p>0
</p>
<p>dH(t)P(τ &gt; t &minus; u, ξ &lt; v)&rarr; 1
Eτ
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P(τ &gt; u, ξ &lt; v)du
</p>
<p>= E(τ ; ξ &lt; v)
Eτ
</p>
<p>.
</p>
<p>The lemma is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Generalised Renewal Processes 309
</p>
<p>As was already noted, η(t) is a stopping time with respect to
</p>
<p>Fn = σ(τ1, . . . , τn; ξ1, . . . , ξn).
Therefore, if (τj , ξj ) are identically distributed, then by the Wald identity (see The-
</p>
<p>orem 4.4.2 and Example 4.4.5)
</p>
<p>ES(t)= qt + aξEη(t)&sim; qt +
aξ t
</p>
<p>a
(10.6.1)
</p>
<p>as t &rarr;&infin;, where aξ = Eξ and a = Eτ . The second moments of S(t) will be found
in Sect. 15.2. The laws of large numbers for S(t) will be established in Sect. 11.5.
</p>
<p>10.6.2 The Central Limit Theorem
</p>
<p>In order to simplify the exposition, we first assume that the components τj and ξj of
</p>
<p>the vectors (τj , ξj )
d= (τ, ξ) are independent. Moreover, without losing generality,
</p>
<p>we assume that q = 0.
</p>
<p>Theorem 10.6.1 Let there exist σ 2 = Var τ &lt; &infin;, σ 2ξ = Var(ξ) &lt; &infin; with σ +
σξ &gt; 0. If the coordinates τ and ξ are independent then, as t &rarr;&infin;,
</p>
<p>S(t)&minus; rt
σS
</p>
<p>&radic;
t
</p>
<p>&sub;&rArr;�0,1,
</p>
<p>where r = aξ/a and σ 2S = a&minus;1(σ 2ξ + r2σ 2)= a&minus;1 Var(ξ &minus; rτ ). The same assertion
holds true for S(ν)(t) as well.
</p>
<p>Proof If one of the values of σ and σξ is zero, then the assertion of the theorem
follows from Theorems 8.2.1 and 10.5.2. Therefore we can assume that σ &gt; 0 and
</p>
<p>σξ &gt; 0. Denote by G= σ(τ1, τ2, . . .) the σ -algebra generated by the sequence {τj }
and by At &sub;G the set
</p>
<p>At =
{∣∣η(t)&minus; t/a
</p>
<p>∣∣&lt; t1/2+ε
}
, ε &isin; (0,1/2).
</p>
<p>Since by the central limit theorem P(At ) &rarr; 1 as t &rarr; &infin;, for any trajectory η(&middot;)
in At we have η(t)&rarr;&infin; as t &rarr;&infin;, and the random variables
</p>
<p>Z(t)= S(t)&minus; aξη(t)
σξ
&radic;
η(t)
</p>
<p>are asymptotically normal with parameters (0,1) by the independence of {ξj }
and {τj }. In other words, on the sets At ,
</p>
<p>E
(
eiλZ(t)
</p>
<p>∣∣G
)
&rarr; e&minus;λ2/2 as t &rarr;&infin;.
</p>
<p>Since
</p>
<p>η(t)= t
a
+ σ
</p>
<p>&radic;
t
</p>
<p>a3/2
ζt , ζt &sub;&rArr;�0,1, and η(t)&sim;
</p>
<p>t
</p>
<p>a</p>
<p/>
</div>
<div class="page"><p/>
<p>310 10 Renewal Processes
</p>
<p>on the sets At &isin;G, we also have on the sets At the relation
</p>
<p>E
</p>
<p>(
exp
</p>
<p>{
iλ(S(t)&minus; rt &minus; aξσ
</p>
<p>&radic;
t
</p>
<p>a3/2
ζt )
</p>
<p>σξ
&radic;
t/a
</p>
<p>}∣∣∣G
)
&rarr; e&minus;λ2/2.
</p>
<p>Since the random variables ζt and η(t) are measurable with respect to G, the corre-
</p>
<p>sponding factor can be taken outside of the conditional expectation, so that
</p>
<p>E
</p>
<p>(
exp
</p>
<p>{
iλ(S(t)&minus; rt)
</p>
<p>σξ
&radic;
t/a
</p>
<p>}∣∣∣G
)
&sim; exp
</p>
<p>{
&minus;λ
</p>
<p>2
</p>
<p>2
+ iλrσ
</p>
<p>σξ
ζt
</p>
<p>}
.
</p>
<p>Hence
</p>
<p>E exp
</p>
<p>{
iλ(S(t)&minus; rt)
</p>
<p>σξ
&radic;
t/a
</p>
<p>}
= o(1)+E
</p>
<p>(
exp
</p>
<p>{
&minus;λ
</p>
<p>2
</p>
<p>2
+ iλσ
</p>
<p>σξ
ζt
</p>
<p>}
; At
</p>
<p>)
</p>
<p>= o(1)+ exp
{
&minus;λ
</p>
<p>2
</p>
<p>2
</p>
<p>[
1 +
</p>
<p>(
rσ
</p>
<p>σξ
</p>
<p>)2]}
.
</p>
<p>This means that
</p>
<p>1&radic;
t
</p>
<p>(
S(t)&minus; taξ
</p>
<p>a
</p>
<p>)
&sub;&rArr;�0,σ 2S ,
</p>
<p>where
</p>
<p>σ 2S =
σ 2ξ
</p>
<p>a
</p>
<p>[
1 +
</p>
<p>(
rσ
</p>
<p>σξ
</p>
<p>)2]
= a&minus;1
</p>
<p>[
σ 2ξ + r2σ 2
</p>
<p>]
.
</p>
<p>The assertion corresponding to S(ν)(t) follows from Lemma 10.6.1. The theorem
</p>
<p>is proved. �
</p>
<p>Note that Theorems 8.2.1 and 10.5.2 are special cases of Theorem 10.6.1. If
</p>
<p>aξ = 0, then S(t) is distributed identically to S[t/a] and is independent of σ .
Now consider the general case where τ and ξ are, generally speaking, dependent.
</p>
<p>Since Tη(t) = t + χ(t), we have the representation
</p>
<p>S(t)&minus; rt = Zη(t) + rχ(t), (10.6.2)
</p>
<p>where
</p>
<p>Zn =
n&sum;
</p>
<p>j=1
ζj , ζj = ξj &minus; rτj , Eζj = 0,
</p>
<p>χ(t)&radic;
t
</p>
<p>p&minus;&rarr; 0
</p>
<p>as t &rarr;&infin; (χ(t) has a proper limiting distribution as t &rarr;&infin;). Moreover, we will
use yet another Wald identity
</p>
<p>EZ2η(t) = d2Eη(t), d2 = Eζ 2, ζ = ξ &minus; rτ, (10.6.3)
</p>
<p>that is derived below in Sect. 15.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Generalised Renewal Processes 311
</p>
<p>Theorem 10.6.2 Let (τj , ξj )
d= (τ, ξ) be independent identically distributed and
</p>
<p>such that σ 2 = Var(τ ) &lt;&infin; and σ 2ξ = Var(ξ) &lt;&infin; exist. Then
</p>
<p>S(t)&minus; rt
σS
</p>
<p>&radic;
t
</p>
<p>&sub;&rArr;�0,1,
</p>
<p>where r = aξ/a and σ 2S = a&minus;1d2. The random variables
S(ν)(t)&minus;rt
</p>
<p>σS
&radic;
t
</p>
<p>and
Zη(t)
</p>
<p>σS
&radic;
t
have
</p>
<p>the same limiting distribution.
</p>
<p>Proof It is seen from (10.6.2) that it suffices to prove that
</p>
<p>Zη(t)
</p>
<p>σS
&radic;
t
&sub;&rArr;�0,1.
</p>
<p>The main contribution to Zη(t) comes from Zm with m = [ ta &minus; 2N
&radic;
t], N &rarr;&infin;,
</p>
<p>N = o(
&radic;
t ), where
</p>
<p>&radic;
a Zm
</p>
<p>d
&radic;
t
</p>
<p>= Zm
d
&radic;
m
</p>
<p>&radic;
ma
</p>
<p>t
&sub;&rArr;�0,1.
</p>
<p>The remainder Zη(t) &minus;Zm, for each fixed
</p>
<p>Tm &isin; IN := [t &minus; 3aN
&radic;
t, t &minus; aN
</p>
<p>&radic;
t ], P(Tm &isin; IN )&rarr; 1,
</p>
<p>has the same distribution as Zη(t&minus;Tm), and its variance (see (10.6.3)) is equal to
</p>
<p>d2Eη(t &minus; Tm)&sim; d2
t &minus; Tm
</p>
<p>a
&lt; 3d2N
</p>
<p>&radic;
t = o(t).
</p>
<p>Since EZη(t&minus;Tm) = 0, we have
Zη(t&minus;Tm)&radic;
</p>
<p>t
</p>
<p>p&minus;&rarr; 0 (10.6.4)
</p>
<p>as t &rarr;&infin;. The theorem is proved. �
</p>
<p>Note that, for N &rarr; &infin; slowly enough, relation (10.6.4) can be derived using
not (10.6.3), but the law of large numbers for generalised renewal processes that
</p>
<p>was obtained in Sect. 11.5.
</p>
<p>Theorem 10.6.1 could be proved in a somewhat different way&mdash;with the help
</p>
<p>of the local Theorem 10.5.3. We will illustrate this approach by the proof of the
</p>
<p>integro-local theorem for S(t).
</p>
<p>10.6.3 The Integro-Local Theorem
</p>
<p>In this section we will obtain the integro-local theorem for S(t) in the case of non-
</p>
<p>lattice ξ . In a quite similar way we can obtain local theorems for densities (if they
</p>
<p>exist) and for the probability P(S(t)= k) for q = 0 for arithmetic ξj .</p>
<p/>
</div>
<div class="page"><p/>
<p>312 10 Renewal Processes
</p>
<p>Theorem 10.6.3 Let the conditions of Theorem 10.6.1 hold and, moreover, ξ be
non-lattice. Then, for any fixed ∆&gt; 0, as t &rarr;&infin;,
</p>
<p>P
(
S(t)&minus; rt &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>σS
&radic;
t
φ
</p>
<p>(
x
</p>
<p>σS
&radic;
t
</p>
<p>)
+ o
</p>
<p>(
1&radic;
t
</p>
<p>)
, (10.6.5)
</p>
<p>where the remainder term o(1/
&radic;
t) is uniform in x.
</p>
<p>Proof Since ξ is non-lattice, one has σξ &gt; 0. If σ = 0 then the assertion of the
theorem follows from Theorem 8.7.1. Therefore we will assume that σ &gt; 0. By the
</p>
<p>independence of {ξj } and {τj },
</p>
<p>P
(
S(t)&minus; rt &isin;∆[x)
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=1
P
(
η(t)= n
</p>
<p>)
P
(
Sn &minus; rt &isin;∆[x)
</p>
<p>)
=
</p>
<p>&sum;
</p>
<p>n&isin;Mt
+
</p>
<p>&sum;
</p>
<p>n/&isin;Mt
,
</p>
<p>where Mt = {n : |n&minus; t/a|&lt; t1/2N(t)}, N(t)&rarr;&infin;, N(t)= o(
&radic;
t) as t &rarr;&infin;. We
</p>
<p>know the asymptotics of both factors of the terms in the sum from Theorems 8.7.1
</p>
<p>and 10.5.8 (see also (10.5.15)). It remains to do the summation, which is unfortu-
</p>
<p>nately somewhat cumbersome. At the same time, it presents no substantial difficul-
</p>
<p>ties, so we will sketch this part of the proof. If we put an&minus; t =: u,
</p>
<p>P1(t) :=
∆
</p>
<p>σξ
&radic;
</p>
<p>2πn
exp
</p>
<p>{
&minus; (x &minus; ru)
</p>
<p>2
</p>
<p>2nσ 2ξ
</p>
<p>}
, P2(t) :=
</p>
<p>a3/2
</p>
<p>σ
&radic;
</p>
<p>2πt
exp
</p>
<p>{
&minus; u
</p>
<p>2
</p>
<p>2σ 2n
</p>
<p>}
,
</p>
<p>then
</p>
<p>P
(
Sn &minus; rt &isin;∆[x)
</p>
<p>)
= P1(t)+ o
</p>
<p>(
1&radic;
n
</p>
<p>)
.
</p>
<p>Furthermore,
</p>
<p>P
(
η(t)= n
</p>
<p>)
= P2(t)+ o
</p>
<p>(
1&radic;
t
</p>
<p>)
</p>
<p>for n &isin;Mt and N(t)&rarr;&infin; slowly enough as t &rarr;&infin;. Clearly,
&sum;
</p>
<p>n/&isin;Mt
= o
</p>
<p>(
1&radic;
t
</p>
<p>)
.
</p>
<p>Since the sums of P1(t) and P2(t) are bounded in n by a constant, we have
</p>
<p>&sum;
</p>
<p>n&isin;Mt
= o
</p>
<p>(
1&radic;
t
</p>
<p>)
+
</p>
<p>&sum;
</p>
<p>n&isin;Mt
P1(t)P2(t).
</p>
<p>The exponent in the product P1(t)P2(t), taken with the negative sign, is equal to
</p>
<p>1
</p>
<p>2n
</p>
<p>[
(x &minus; ru)2
</p>
<p>σ 2ξ
+ u
</p>
<p>2
</p>
<p>σ 2
</p>
<p>]
&sim; a
</p>
<p>2t
</p>
<p>[
(d2u&minus; rxσ 2)2
</p>
<p>d2σ 2σ 2ξ
+ x
</p>
<p>2
</p>
<p>d2
</p>
<p>]
,
</p>
<p>where d2 = r2σ 2 + σ 2ξ . Since, for x = o(
&radic;
t N(t)),
</p>
<p>&sum;
</p>
<p>n&isin;At
</p>
<p>a3/2d&radic;
2πtσσξ
</p>
<p>exp
</p>
<p>{
&minus;a(d
</p>
<p>2u&minus; rxσ 2)2
</p>
<p>2td2σ 2σ 2ξ
</p>
<p>}
&rarr; 1</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Generalised Renewal Processes 313
</p>
<p>as t &rarr;&infin; and this sum does not exceed 1 + o(1) for all x (this is an integral sum
that corresponds to the integral of the density of the normal law), it is easy to de-
</p>
<p>rive (10.6.5) from the foregoing. �
</p>
<p>We will continue the study of generalised renewal processes in Sect. 11.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
</p>
<p>Properties of the Trajectories of Random Walks.
Zero-One Laws
</p>
<p>Abstract The chapter begins with Sect. 11.1 establishing the Borel&ndash;Cantelli and
</p>
<p>Kolmogorov zero-one laws, and also the zero-one law for exchangeable sequences.
</p>
<p>The concepts of lower and upper functions are introduced. Section 11.2 contains
</p>
<p>the first Kolmogorov inequality and several theorems on convergence of random se-
</p>
<p>ries. Section 11.3 presents Kolmogorov&rsquo;s Strong Law of Large Numbers and Wald&rsquo;s
</p>
<p>identity for stopping times. Sections 11.4 and 11.5 are devoted to the Strong Law of
</p>
<p>Large Numbers for independent non-identically distributed random variables, and to
</p>
<p>the Strong Law of Large Numbers for generalised renewal processes, respectively.
</p>
<p>11.1 Zero-One Laws. Upper and Lower Functions
</p>
<p>Let, as before, Sn =
&sum;n
</p>
<p>j=1 ξj be the sums of independent random variables
ξ1, ξ2, . . . . In this chapter we will consider properties of the &ldquo;whole&rdquo; trajectories
of random walks {Sn}.
</p>
<p>The first limit theorem we proved for the distribution of the sums of independent
</p>
<p>identically distributed random variables was the law of large numbers: Sn/n
p&rarr; Eξ .
</p>
<p>One could ask whether the whole trajectory Sn/n,Sn+1/(n+ 1), . . . , starting from
some n, will be close to Eξ with a high probability. That is, whether, for any ε &gt; 0,
</p>
<p>we will have
</p>
<p>lim
n&rarr;&infin;
</p>
<p>P
</p>
<p>(
sup
k&ge;n
</p>
<p>∣∣∣∣
Sk
</p>
<p>k
&minus;Eξ
</p>
<p>∣∣∣∣&lt; ε
)
= 1. (11.1.1)
</p>
<p>This is clearly a problem on almost sure convergence, or convergence with probabil-
</p>
<p>ity 1. A similar question arises concerning generalised renewal processes discussed
in Sect. 10.6.
</p>
<p>Assertion (11.1.1), which is called the strong law of large numbers and is to be
proved in this chapter, is a special case of the so-called zero-one laws. As the first
such law, we will now present the Borel&ndash;Cantelli zero-one law.
</p>
<p>11.1.1 Zero-One Laws
</p>
<p>Theorem 11.1.1 Let {An}&infin;n=1 be a sequence of events on a probability space
〈Ω,F,P〉, and let A be the event that infinitely many events Ak occur, i.e.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_11, &copy; Springer-Verlag London 2013
</p>
<p>315</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_11">http://dx.doi.org/10.1007/978-1-4471-5201-9_11</a></div>
</div>
<div class="page"><p/>
<p>316 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>A=
⋂&infin;
</p>
<p>n=1
⋃&infin;
</p>
<p>k=nAk (the event A consists of those ω that belong to infinitely many
Ak).
</p>
<p>If
&sum;&infin;
</p>
<p>k=1 P(Ak) &lt;&infin;, then P(A)= 0. If
&sum;&infin;
</p>
<p>k=1 P(Ak)=&infin; and the events Ak are
independent, then P(A)= 1.
</p>
<p>Proof Assume that
&sum;&infin;
</p>
<p>k=1 P(Ak) &lt;&infin;. Denote by η =
&sum;&infin;
</p>
<p>k=1 I(Ak) the number of
occurrences of events Ak . Then Eη=
</p>
<p>&sum;&infin;
k=1 P(Ak) &lt;&infin; which certainly means that
</p>
<p>η is a proper random variable: P(η &lt;&infin;)= 1 &minus; P(A)= 1.
If Ak are independent and
</p>
<p>&sum;&infin;
k=1 P(Ak)=&infin;, then, since Ak =Ω \Ak are also
</p>
<p>independent, we have
</p>
<p>P(A)= lim
n&rarr;&infin;
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>k=n
Ak
</p>
<p>)
= lim
</p>
<p>n&rarr;&infin;
P
</p>
<p>(
Ω &minus;
</p>
<p>&infin;⋂
</p>
<p>k=n
Ak
</p>
<p>)
</p>
<p>= 1 &minus; lim
n&rarr;&infin;
</p>
<p>P
</p>
<p>( &infin;⋂
</p>
<p>k=n
Ak
</p>
<p>)
= 1 &minus; lim
</p>
<p>n&rarr;&infin;
lim
</p>
<p>m&rarr;&infin;
P
</p>
<p>(
m⋂
</p>
<p>k=n
Ak
</p>
<p>)
</p>
<p>= 1 &minus; lim
n&rarr;&infin;
</p>
<p>&infin;&prod;
</p>
<p>k=n
</p>
<p>(
1 &minus; P(Ak)
</p>
<p>)
.
</p>
<p>Using the inequality ln(1 &minus; x)&le;&minus;x we obtain that
&infin;&prod;
</p>
<p>k=n
</p>
<p>(
1 &minus; P(Ak)
</p>
<p>)
&le; exp
</p>
<p>{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=n
P(Ak)
</p>
<p>}
.
</p>
<p>Hence
</p>
<p>&infin;&prod;
</p>
<p>k=n
</p>
<p>(
1 &minus; P(Ak)
</p>
<p>)
&le; e&minus;&infin; = 0, P(A)= 1.
</p>
<p>The theorem is proved. �
</p>
<p>Remark 11.1.1 It follows from Theorem 11.1.1 that, for independent events Ak ,
the assertions that Eη &lt; &infin; and that P(η &lt; &infin;) = 1 are equivalent to each other.
Although in one direction this relationship is obvious, in the opposite direction it
</p>
<p>is quite meaningful. It implies, in particular, that if η &lt;&infin; with probability 1, but
Eη=&infin;, then Ak are necessarily dependent.
</p>
<p>Note also that the argument proving the first part of the theorem has already been
</p>
<p>used for the same purpose in the proof of Theorem 6.1.1.
</p>
<p>Assume that {ξn}&infin;n=1 is a sequence of independent random variables given on
〈Ω,F,P〉. Denote, as before, by σ(ξ1, . . . , ξn) the σ -algebra generated by the first
n random variables ξ1, . . . , ξn, and by σ(ξn, . . .) the σ -algebra generated by the
</p>
<p>random variables ξn, ξn+1, ξn+2, . . . .
</p>
<p>Definition 11.1.1 An event A is said to be a tail event if A &isin; σ(ξn, . . .) for any
n &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Zero-One Laws. Upper and Lower Functions 317
</p>
<p>For example, the event
</p>
<p>A=
&infin;⋂
</p>
<p>n=1
</p>
<p>&infin;⋃
</p>
<p>k=n
{ξk &gt;N}
</p>
<p>meaning that there occurred infinitely many events {ξk &gt;N} is clearly a tail event.
</p>
<p>Theorem 11.1.2 (Kolmogorov zero-one law) If A is a tail event, then either
P(A)= 0 or P(A)= 1.
</p>
<p>Proof Since A is a tail event, A &isin; σ(ξn+1, . . .), n &ge; 0. Therefore the event A is
independent of the σ -algebra σ(ξ1, . . . , ξn) for any n. Hence (see Theorem 3.4.3)
</p>
<p>the event A is independent of the σ -algebra σ(ξ1, . . .). Since A &isin; σ(ξ1, . . .), it is
independent of itself:
</p>
<p>P(A)= P(AA)= P(A)P(A).
But this is only possible if P(A)= 0 or 1. The theorem is proved. �
</p>
<p>Put S = sup{0, S1, S2, . . .}, where Sn =
&sum;n
</p>
<p>k=1 ξk . An example of an application
of the above theorem is given by the following
</p>
<p>Corollary 11.1.1 If ξk , k = 1,2, . . . , are independent, then either P(S =&infin;) = 1
or P(S &lt;&infin;)= 1.
</p>
<p>The Proof follows from the fact that {S =&infin;} is a tail event. Indeed, for any n
</p>
<p>{S =&infin;}=
{
sup(Sn&minus;1, Sn, . . .)=&infin;
</p>
<p>}
</p>
<p>=
{
sup(0, Sn &minus; Sn&minus;1, . . .)=&infin;
</p>
<p>}
&isin; σ(ξn, . . .). �
</p>
<p>Further examples of tail events can be obtained if we consider, for a sequence
</p>
<p>of independent variables ξ1, ξ2, . . . , the event {the series
&sum;&infin;
</p>
<p>1 ξk is convergent}.
Theorem 11.1.2 means that the probability of that event can only be 0 or 1.
</p>
<p>If we consider the power series
&sum;&infin;
</p>
<p>k=0 z
kξk where ξk are independent, we will
</p>
<p>see that the convergence radius ρ = lim supk&rarr;&infin; |ξk|&minus;1/k of this series is a random
variable measurable with respect to the σ -algebra σ(ξn, . . .) for any n ({ρ &lt; x} &isin;
σ(ξn, . . .), 0 &le; x &le; &infin;). Such random variables are also called tail random vari-
ables. Since by the foregoing one has Fρ(x) = P(ρ &lt; x) = 0 or 1, this implies
that ρ, as well as any other tail random variable, must be equal to a constant with
</p>
<p>probability 1.
</p>
<p>Under the assumption that the elements of the sequence {ξk}&infin;k=1 are not only in-
dependent but also identically distributed, Kolmogorov&rsquo;s zero-one law was extended
</p>
<p>by Hewitt and Savage to a wider class of events.
</p>
<p>Let ω = (x1, x2, . . .) be an element of the sample space 〈R&infin;,B&infin;,P〉 for the
sequence ξ = (ξ1, ξ2, . . .) (R&infin; is a countable direct product of the real lines Rk ,
k = 1,2, . . . , B&infin; = σ(ξ1, . . .) is generated by the sets
</p>
<p>&prod;N
k=1 Bk &isin; σ(ξ1, . . . , ξN ),
</p>
<p>where Bk &isin; σ(ξk) are Borel sets on the lines Rk).</p>
<p/>
</div>
<div class="page"><p/>
<p>318 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>Definition 11.1.2 An event A &isin;B&infin; is said to be exchangeable if
</p>
<p>(x1, x2, . . . , xn&minus;1, xn, xn+1 . . .) &isin;A
</p>
<p>implies that (xn, x2, . . . , xn&minus;1, x1, xn+1 . . .) &isin; A for every n &ge; 1. It is evident that
this condition of membership automatically extends to any permutations of finitely
</p>
<p>many components. Examples of exchangeable events are given by tail events.
</p>
<p>Theorem 11.1.3 (Zero-one law for exchangeable events) If ξk are independent and
identically distributed and A is an exchangeable event, then either P(A) = 0 or
P(A)= 1.
</p>
<p>Proof By the approximation theorem (Sect. 3.5), for any A &isin; B&infin; there exists a
sequence of events An &isin; σ(ξ1, . . . , ξn) such that
</p>
<p>P(AnA&cup;AAn)&rarr; 0
</p>
<p>as n&rarr;&infin;.
Introduce the transformation
</p>
<p>Tnω= Tn(x1, x2, . . .)= (xn+1, . . . , x2n, x1, . . . , xn, x2n+1 . . .)
</p>
<p>and put Bn = TnAn. If A is exchangeable, then TnA=A and, for any B &isin;B&infin;, one
has P(TnB)= P(B) since ξj are independent and identically distributed. Therefore
P(BnA) = P(TnAnA) = P(AnA), and hence Bn will also approximate A, which
obviously implies that Cn = AnBn will have the same approximation property. By
independence of An and Bn, this means that
</p>
<p>P(A)= lim
n&rarr;&infin;
</p>
<p>P(AnBn)= lim
n&rarr;&infin;
</p>
<p>P2(An)= P2(A).
</p>
<p>The theorem is proved. �
</p>
<p>11.1.2 Lower and Upper Functions
</p>
<p>Theorem 11.1.3 implies the following interesting fact, the statement of which re-
</p>
<p>quires the next definition.
</p>
<p>Definition 11.1.3 For a sequence of random variables {ηn}&infin;n=1, a numerical se-
quence {an}&infin;n=1 is said to be an upper sequence (function) if, with probability 1,
there occur only finitely many events {ηn &gt; an}. A sequence {an}&infin;n=1 is said to be a
lower sequence (function) if, with probability 1, there occur infinitely many events
{ηn &gt; an}.
</p>
<p>Corollary 11.1.2 If ξk are independent and identically distributed, then any se-
quence {an} is either upper or lower for the sequence of sums {Sn}&infin;n=1 with
Sn =
</p>
<p>&sum;n
k=1 ξk .</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Zero-One Laws. Upper and Lower Functions 319
</p>
<p>In other words, one cannot find an &ldquo;intermediate&rdquo; sequence {an} such that the
probability of the event A= {Sn &gt; an infinitely often} would be equal, say, to 1/2.
</p>
<p>Proof To prove the corollary, it suffices to notice that the event A is exchangeable,
because swapping ξ1 and ξn in the realisation (ξ1, ξ2, . . .) influences the behaviour
</p>
<p>of the first n sums S1, . . . , Sn only. �
</p>
<p>A similar fact holds, of course, for the sequence of random variables {ξn}&infin;n=1
itself, but, unlike the above corollary, that assertion can be proved more easily, since
</p>
<p>B = {ξn &gt; a infinitely often} is a tail event.
</p>
<p>Remark 11.1.2 In regard to the properties of upper and lower sequences for sums
{Sn} we also note here the following. If P(ξk = c) 
= 1, and {an} is an upper (lower)
sequence for {Sn}, then, for any fixed k &ge; 0 and v, the sequence {bn = an+k + v}&infin;n=1
is also upper (lower) for {Sn}. This is a consequence of the following relations. Let
v1 &gt; v2 be such that
</p>
<p>P(ξ &gt; v1) &gt; 0, P(ξ &lt; v2) &gt; 0.
</p>
<p>Then, for the upper sequence {an} and the event A= {Sn &gt; aninfinitely many times},
we have
</p>
<p>0 = P(A)&ge; P(ξ1 &gt; v1)P(A|ξ1 &gt; v1)
&ge; P(ξ1 &gt; v1)P(Sn &gt; an+1 &minus; v1 infinitely many times).
</p>
<p>This implies that the second factor on the right-hand side equals 0, and hence the
</p>
<p>sequence {an+1 &minus; v1} is also an upper sequence. On the other hand, if ξ &prime; d= ξ is
independent of ξ then
</p>
<p>0 = P(A)&ge; P
(
ξ &prime; + Sn &gt; ξ &prime; + an infinitely many times; ξ &prime; &lt; v2
</p>
<p>)
</p>
<p>&ge; P(ξ &lt; v2)P(Sn+1 &gt; an + v2 infinitely many times)
= P(ξ &lt; v2)P(Sn &gt; an&minus;1 + v2 infinitely many times).
</p>
<p>Here the second factor on the right-hand side equals 0, and hence the sequence
</p>
<p>{an&minus;1 + v2} is also upper. Combining these assertions as many times as necessary,
we find that the sequence {an+k + v} is upper for any given k and v. �
</p>
<p>From the above remark it follows, in particular, that the quantities lim supn&rarr;&infin; Sn
and lim infn&rarr;&infin; Sn cannot both be finite for a sequence of sums of independent
identically distributed random variables that are not zeros with probability 1. Indeed,
</p>
<p>the event B = {lim supn&rarr;&infin; Sn &isin; (a, b)} is exchangeable and therefore P(B)= 0 or
P(B)= 1 by virtue of the zero-one law. If P(B) were equal to 1, (b, b, . . .) would be
an upper sequence for {Sn}. But, by our remark, (a, a, . . .) would then be an upper
sequence as well, which would mean that
</p>
<p>P
(
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>Sn &le; a
)
= 1,</p>
<p/>
</div>
<div class="page"><p/>
<p>320 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>which contradicts the assumption P(B)= 1. �
</p>
<p>The reader can also derive from Theorem 11.1.3 that, for any sequences {an}
and {bn}, the random variables
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>Sn &minus; an
bn
</p>
<p>and lim inf
n&rarr;&infin;
</p>
<p>Sn &minus; an
bn
</p>
<p>are constant with probability 1.
</p>
<p>11.2 Convergence of Series of Independent Random Variables
</p>
<p>In the present section we will discuss in more detail convergence of series of inde-
</p>
<p>pendent random variables. We already know that such series converge with proba-
</p>
<p>bility 1 or 0. We are interested in conditions ensuring convergence.
</p>
<p>First of all we answer the following interesting question. It is well known that the
</p>
<p>series
&sum;&infin;
</p>
<p>n=1 n
&minus;α is divergent for α &le; 1, while the alternating series
</p>
<p>&sum;&infin;
n=1(&minus;1)nn&minus;α
</p>
<p>converges for any α &gt; 0 (the difference between neighbouring elements is of order
</p>
<p>αn&minus;α&minus;1). What can be said about the behaviour of the series
&sum;&infin;
</p>
<p>n=1 δnn
&minus;α , where
</p>
<p>δn are identically distributed and independent with Eδn = 0 (for instance, δn =&plusmn;1
with probabilities 1/2)?
</p>
<p>One of the main approaches to studying such problems is based on elucidat-
</p>
<p>ing the relationship between a.s. convergence and the simpler notion of conver-
</p>
<p>gence in probability. It is known that, generally speaking, convergence in prob-
</p>
<p>ability ξn
p&rarr; ξ does not imply a.s. convergence. However, in our situation when
</p>
<p>ζn = Sn :=
&sum;n
</p>
<p>k=1 ξk , ξk being independent, this is not the case. The main assertion
of the present section is the following.
</p>
<p>Theorem 11.2.1 If ξk are independent and Sn =
&sum;n
</p>
<p>k=1 ξk , then convergence of Sn
in probability implies a.s. convergence of Sn.
</p>
<p>We will prove that Sn is a Cauchy sequence. To do this, we will need the follow-
</p>
<p>ing inequality.
</p>
<p>Lemma 11.2.1 (The First Kolmogorov inequality) If ξj are independent and, for
some b &gt; 0 and all j &le; n,
</p>
<p>P
(
|Sn &minus; Sj | &ge; b
</p>
<p>)
&le; p &lt; 1,
</p>
<p>then
</p>
<p>P
(
</p>
<p>max
j&le;n
</p>
<p>|Sj | &ge; x
)
&le; 1
</p>
<p>1 &minus; pP
(
|Sn|&gt; x &minus; b
</p>
<p>)
. (11.2.1)
</p>
<p>Corollary 11.2.1 If Eξj = 0 then
</p>
<p>P
(
</p>
<p>max
j&le;n
</p>
<p>|Sj | &ge; x
)
&le; 2P
</p>
<p>(
|Sn|&gt; x &minus;
</p>
<p>&radic;
2 Var(Sn)
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Convergence of Series of Independent Random Variables 321
</p>
<p>Kolmogorov actually established this last inequality (Lemma 11.2.1 is an in-
</p>
<p>significant extension of it). It follows from (11.2.1) with p = 1/2, since by the
Chebyshev inequality
</p>
<p>P
(
|Sn &minus; Sj | &ge;
</p>
<p>&radic;
2 Var(Sn)
</p>
<p>)
&le; Var(Sn &minus; Sj )
</p>
<p>2 Var(Sn)
&le; 1
</p>
<p>2
.
</p>
<p>Proof of Lemma 11.2.1 Let
</p>
<p>η :=
{
mink &ge; 1 : |Sk| &ge; x
</p>
<p>}
.
</p>
<p>Put Aj := {η= j}, j = 1,2, . . . . Clearly, Aj are disjoint events and hence
</p>
<p>P
(
|Sn|&gt; x &minus; b
</p>
<p>)
&ge;
</p>
<p>n&sum;
</p>
<p>j=1
P
(
|Sn|&gt; x &minus; b; Aj
</p>
<p>)
&ge;
</p>
<p>n&sum;
</p>
<p>j=1
P
(
|Sn &minus; Sj |&lt; b; Aj
</p>
<p>)
.
</p>
<p>(The last inequality holds because the event {|Sn &minus; Sj | &lt; b}Aj implies {|Sn| &gt;
x&minus;b}Aj .) But Aj &isin; σ(ξ1, . . . , ξj ) and {|Sn&minus;Sj |&lt; b} &isin; σ(ξj+1, . . . , ξn). Therefore
these two events are independent and
</p>
<p>P
(
|Sn|&gt; x &minus; b
</p>
<p>)
&ge;
</p>
<p>n&sum;
</p>
<p>j=1
P(Aj )P
</p>
<p>(
|Sn &minus; Sj |&lt; b
</p>
<p>)
</p>
<p>&ge; (1 &minus; p)
n&sum;
</p>
<p>1
</p>
<p>P(Aj )= (1 &minus; p)P
(
</p>
<p>max
j&le;n
</p>
<p>|Sj | &ge; x
)
.
</p>
<p>The lemma is proved. �
</p>
<p>Proof of Theorem 11.2.1 It suffices to prove that {Sn} is a.s. a Cauchy sequence, i.e.
that, for any ε &gt; 0,
</p>
<p>P
(
</p>
<p>sup
n&ge;m
</p>
<p>|Sn &minus; Sm|&gt; 2ε
)
&rarr; 0 (11.2.2)
</p>
<p>as m&rarr;&infin;. Let
</p>
<p>Aεn,m :=
{
|Sn &minus; Sm|&gt; ε
</p>
<p>}
, Aεm :=
</p>
<p>⋃
</p>
<p>n&ge;m
Aεn,m.
</p>
<p>Then relation (11.2.2) can be written as
</p>
<p>P
(
A2εm
</p>
<p>)
&rarr; 0 (11.2.3)
</p>
<p>as m&rarr;&infin;.
Since {Sn} is a Cauchy sequence in probability, one has
</p>
<p>pm,M := sup
m&le;n&le;M
</p>
<p>P
(
Aεn,M
</p>
<p>)
&rarr; 0
</p>
<p>as m&rarr;&infin; and M &rarr;&infin;, so that pm,M &lt; 1/2 for all m and M large enough. For
such m and M we have by Lemma 11.2.1, for a = ε and x = 2ε, that</p>
<p/>
</div>
<div class="page"><p/>
<p>322 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>P
(
</p>
<p>sup
m&le;n&le;M
</p>
<p>|Sn &minus; Sm|&gt; 2ε
)
= P
</p>
<p>(
M⋃
</p>
<p>n=m+1
A2εm,n
</p>
<p>)
</p>
<p>&le; 1
1 &minus; pm,M
</p>
<p>P
(
AεM,m
</p>
<p>)
&le; 2P
</p>
<p>(
AεM,m
</p>
<p>)
.
</p>
<p>By the properties of probability,
</p>
<p>P
(
A2εm
</p>
<p>)
= lim
</p>
<p>M&rarr;&infin;
P
</p>
<p>(
M⋃
</p>
<p>n=m+1
A2εm,n
</p>
<p>)
&le; 2 lim sup
</p>
<p>M&rarr;&infin;
P
(
AεM,m
</p>
<p>)
. (11.2.4)
</p>
<p>Denote by S the limit (in probability) of the sequence Sn, and
</p>
<p>Bεn :=
{
|Sn &minus; S|&gt; ε
</p>
<p>}
.
</p>
<p>Then P(Bεn)&rarr; 0 as n&rarr;&infin;, AεM,m &sub; B
ε/2
M &cup;B
</p>
<p>ε/2
m , and by (11.2.4)
</p>
<p>P
(
A2εm
</p>
<p>)
&le; 2P
</p>
<p>(
B
ε/2
m
</p>
<p>)
&rarr; 0 as m&rarr;&infin;.
</p>
<p>Relation (11.2.3), and hence the assertion of the theorem, are proved. �
</p>
<p>Corollary 11.2.2 If Eξk = 0 and
&sum;&infin;
</p>
<p>1 Var(ξk) &lt;&infin;, then Sn converges a.s.
</p>
<p>Proof The assertion follows immediately from Theorem 11.2.1 and the fact that
{Sn} is a Cauchy sequence in mean quadratic (E(Sn&minus;Sm)2 =
</p>
<p>&sum;n
k=m+1 Var(ξk)&rarr; 0
</p>
<p>as m&rarr;&infin; and n&rarr;&infin;) and hence in probability.
It turns out that if Eξk = 0 and |ξk| &lt; c for all k, then the condition&sum;
Var(ξk) &lt;&infin; is necessary and sufficient for a.s. convergence of Sn.1
Corollary 11.2.2 also contains an answer to the question posed at the beginning
</p>
<p>of the section about convergence of
&sum;
</p>
<p>δnn
&minus;α , where δn are independent and identi-
</p>
<p>cally distributed and Eδn = 0.
</p>
<p>Corollary 11.2.3 The series
&sum;
</p>
<p>δnan converges with probability 1 if Var(δk) =
σ 2 &lt;&infin; and
</p>
<p>&sum;
a2n &lt;&infin;.
</p>
<p>Thus we obtain that the series
&sum;
</p>
<p>δnn
&minus;α , where δn =&plusmn;1 with probabilities 1/2,
</p>
<p>is convergent if and only if α &gt; 1/2.
</p>
<p>An extension of Corollary 11.2.2 is given by the following.
</p>
<p>Corollary 11.2.4 (The two series theorem) A sufficient condition for a.s. conver-
gence of the series
</p>
<p>&sum;
ξn is that the series
</p>
<p>&sum;
Eξn and
</p>
<p>&sum;
Var(ξn) are convergent.
</p>
<p>The Proof is obvious, for the sequences
&sum;n
</p>
<p>k=1 Eξk and
&sum;n
</p>
<p>k=1(ξk&minus;Eξk) converge
a.s. by Corollary 11.2.2. �
</p>
<p>1For more detail, see e.g. [31].</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 The Strong Law of Large Numbers 323
</p>
<p>11.3 The Strong Law of Large Numbers
</p>
<p>It is not hard to see that, using the terminology of Sect. 11.1, the strong law of large
</p>
<p>numbers (11.1.1) means that, for any ε &gt; 0, the sequence {εn}&infin;n=1 is an upper one
for both sequences {Sn} and {&minus;Sn} only if Eξ1 = 0.
</p>
<p>We will derive the strong law of large numbers as a corollary of Theorem 10.5.3
</p>
<p>on finiteness of the infimum of sums of random variables.
</p>
<p>Let, as before, ξ1, ξ2, . . . be independent and identically distributed, ξ
d= ξk .
</p>
<p>Theorem 11.3.1 (Kolmogorov&rsquo;s Strong Law of Large Numbers) A necessary and
</p>
<p>sufficient condition for Sn/n
a.s.&minus;&rarr; a is that there exists Eξk = a.
</p>
<p>Proof Sufficiency. Assume, without loss of generality, that Eξk = 0. Then it follows
from Theorem 10.5.3 that the random variable Z(ε) = infk&gt;0(Sk + εk) is proper
for any ε &gt; 0 (Sk + εk is a sum of random variables ξk + ε with E(ξk + ε) &gt; 0).
Therefore,
</p>
<p>P
</p>
<p>(
inf
k&ge;n
</p>
<p>Sk
</p>
<p>k
&lt;&minus;2ε
</p>
<p>)
&le; P
</p>
<p>(⋃
</p>
<p>k&ge;n
{Sk + εk &lt;&minus;εn}
</p>
<p>)
&le; P
</p>
<p>(
Z(ε) &lt;&minus;εn
</p>
<p>)
&rarr; 0
</p>
<p>as n&rarr;&infin;. In a similar way we find that
</p>
<p>P
</p>
<p>(
sup
k&ge;n
</p>
<p>Sk
</p>
<p>k
&gt; 2ε
</p>
<p>)
&rarr; 0 as n&rarr;&infin;.
</p>
<p>Since P(supk&ge;n |Sk/k| &gt; 2ε) does not exceed the sum of the above two probabili-
ties, we obtain that Sn/n
</p>
<p>a.s.&minus;&rarr; 0.
Necessity. Note that
</p>
<p>ξn
</p>
<p>n
= Sn
</p>
<p>n
&minus; n&minus; 1
</p>
<p>n
</p>
<p>Sn&minus;1
n&minus; 1
</p>
<p>a.s.&minus;&rarr; 0,
</p>
<p>so that the event {|ξn/n|&gt; 1} occurs finitely often with probability 1. By the Borel&ndash;
Cantelli zero-one law, this means that
</p>
<p>&sum;&infin;
n=1 P(|ξn/n| &gt; 1) &lt; &infin; or, which is the
</p>
<p>same,
&sum;
</p>
<p>P(|ξ |&gt; n) &lt;&infin;. Therefore, by Lemma 10.5.1, Eξ &lt;&infin; and with necessity
Eξ = a. The theorem is proved. �
</p>
<p>Thus the condition Eξ = 0 is necessary and sufficient for {εn}&infin;n=1 to be an upper
sequence for both sequences {Sn} and {&minus;Sn}. In the next chapter, we will derive
necessary and sufficient conditions for {εn} to be an upper sequence for each of
the trajectories {Sn} and {&minus;Sn} separately. Of course, such a condition, say, for the
sequence {Sn} will be broader than just Eξk = 0.
</p>
<p>We saw that the above proof of the strong law of large numbers was based on
</p>
<p>Theorem 10.5.3 on the finiteness of infSk which is based, in turn, on Wald&rsquo;s iden-
</p>
<p>tity stated as Theorem 4.4.3. There exist other approaches to the proof that are unre-
</p>
<p>lated to Theorem 4.4.3 (see below, e.g. Theorems 11.4.2 and 12.3.1). Now we will</p>
<p/>
</div>
<div class="page"><p/>
<p>324 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>show that, using the strong law of large numbers, one can prove Wald&rsquo;s identity
</p>
<p>for stopping times without any additional restrictions (see e.g. conditions (a)&ndash;(d) in
</p>
<p>Theorem 4.4.3). Furthermore, in our opinion, the proof below better elucidates the
</p>
<p>nature of the phenomenon we are dealing with.
</p>
<p>Consider stopping times ν with respect to a family of σ -algebras of a special
</p>
<p>kind. In particular, we assume that a sequence {ζj }&infin;j=1 of independent identically
distributed random vectors ζj = (ξj , τj ) is given (where τj can also be vectors) and
</p>
<p>Fn := σ(ζ1, . . . , ζn). (11.3.1)
</p>
<p>Theorem 11.3.2 (Wald&rsquo;s identity for stopping times) Let ν be a stopping time with
respect to the family of σ -algebras Fn and assume one of the following conditions
hold: (a) Eν &lt;&infin;; or (b) a := Eξj 
= 0.
</p>
<p>Then
</p>
<p>ESν = aEν. (11.3.2)
</p>
<p>The assertion of the theorem means that Wald&rsquo;s identity is true whenever the
</p>
<p>right-hand side is defined, i.e. only the indefinite case 0 &middot; &infin; is excluded. Roughly
speaking, identity (11.3.2) is valid whenever it makes sense.
</p>
<p>This identity implies that, when Eν &lt; &infin;, the condition a 
= 0 is superfluous
and that, for a 
= 0, the finiteness of ESν implies that of Eν. If a = 0 then the last
assertion is not true. The reader can easily illustrate this fact using the fair game
</p>
<p>discussed in Sect. 4.2.
</p>
<p>Proof of Theorem 11.3.2 By the strong law of large numbers, for all large k, the ratio
Sk/k lies in the vicinity of the point a. (Here and in what follows, we leave more
</p>
<p>precise formulations to the reader.) By Lemma 11.2.1, the sequence {ζν+k}&infin;k=1 has
the same distribution as the original sequence {ζk}&infin;k=1. For this &ldquo;shifted&rdquo; sequence,
consider the stopping time ν2 defined the same way as ν for the original sequence.
</p>
<p>Put ν1 := ν and consider the sequence {ζν1+ν2+k}&infin;k=1 which is again distributed
as {ζk}&infin;k=1 (for ν1 + ν2 is again a stopping time). For the new sequence, define
the stopping time ν3, and so on. Clearly, the νk are independent and identically
</p>
<p>distributed, and so are the differences
</p>
<p>SNk &minus; SNk&minus;1 , k &ge; 1, S0 = 0, where Nk :=
n&sum;
</p>
<p>j=1
νj .
</p>
<p>By virtue of the strong law of large numbers, SNk/Nk also lie in the vicinity of the
</p>
<p>point a for all large k (or Nk).
</p>
<p>If Eν &lt;&infin; then Nk/k lie in the vicinity of the point Eν as k&rarr;&infin;. Since
SNk
</p>
<p>Nk
= SNk
</p>
<p>k
&middot; k
Nk
</p>
<p>, (11.3.3)
</p>
<p>SNk/k is necessarily in a neighbourhood of the point aEν for all large k. This means
</p>
<p>that the expectation ESν = aEν exists.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 The Strong Law of Large Numbers 325
</p>
<p>If Eν =&infin; then, for a &gt; 0, the assumption that the expectation ESν exists and
is finite, together with equality (11.3.3) and the previous argument, leads to a con-
</p>
<p>tradiction, since the limit of the left-hand side of (11.3.3) equals a &gt; 0, but that of
</p>
<p>the right-hand side is zero. The contradiction vanishes only if ESν =&infin;. The case
a &lt; 0 is dealt with in the same way. The theorem is proved. �
</p>
<p>We now return to the strong law of large numbers and illustrate it by the following
</p>
<p>example.
</p>
<p>Example 11.3.1 Let ω = (ω1,ω2, . . .) be a sequence of independent random vari-
ables taking the values 1 and 0 with probabilities p and q = 1 &minus; p, respectively. To
each such sequence, we put into correspondence the number
</p>
<p>ξ = ξ(ω)=
&infin;&sum;
</p>
<p>k=1
ωk2
</p>
<p>&minus;k,
</p>
<p>so that ω is the binary expansion of ξ . It is evident that the possible values of ξ fill
</p>
<p>the interval [0,1].
We show that if p = q = 1/2 then the distribution of ξ is uniform. But if p 
= 1/2,
</p>
<p>then ξ has a singular distribution. Indeed, if x =
&sum;&infin;
</p>
<p>k=1 δk2
&minus;k , where δk assume the
</p>
<p>values 0 or 1, then
</p>
<p>{ξ &lt; x} = {ε1 &lt; δ1} &cup; {ω1 = δ1,ω2 &lt; δ2} &cup; {ω1 = δ1,ω2 = δ2,ω3 &lt; δ3} &cup; &middot; &middot; &middot; .
Since the events in this union are disjoint, for p = 1/2 we have
</p>
<p>P(ξ &lt; x)=
&infin;&sum;
</p>
<p>k=0
P(ω1 = δ1, . . . ,ωk = δk, ωk+1 &lt; δk+1)
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
2&minus;kP(ωk+1 &lt; δk+1)=
</p>
<p>&infin;&sum;
</p>
<p>k=0
2&minus;k&minus;1δk+1 = x.
</p>
<p>This means that the distribution of ξ is uniform, i.e. for any Borel set B &sub; [0,1], the
probability P(ξ &isin; B)= mesB is equal to the Lebesgue measure of B . Put
</p>
<p>Dn :=
n&sum;
</p>
<p>k=1
δk, Ωn :=
</p>
<p>n&sum;
</p>
<p>k=1
ωk.
</p>
<p>Then the set {x : limn&rarr;&infin;Dn/n= p} is Borel measurable and hence
</p>
<p>mes
</p>
<p>{
x : lim
</p>
<p>n&rarr;&infin;
Dn
</p>
<p>n
= 1
</p>
<p>2
</p>
<p>}
= P
</p>
<p>(
lim
n&rarr;&infin;
</p>
<p>Ωn
</p>
<p>n
= 1
</p>
<p>2
</p>
<p>)
.
</p>
<p>Since by the strong law of large numbers the right-hand side here is equal to one,
</p>
<p>mes
</p>
<p>{
x : lim
</p>
<p>n&rarr;&infin;
Dn
</p>
<p>n
= 1
</p>
<p>2
</p>
<p>}
= 1.
</p>
<p>In other words, for almost all x &isin; [0,1], the proportion of ones in the binary expan-
sion of x is equal to 1/2.</p>
<p/>
</div>
<div class="page"><p/>
<p>326 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>Now let p 
= 1/2. Then
</p>
<p>P
</p>
<p>(
lim
n&rarr;&infin;
</p>
<p>Ωn
</p>
<p>n
= p
</p>
<p>)
= 1,
</p>
<p>although, as we saw above,
</p>
<p>mes
</p>
<p>{
x : lim
</p>
<p>n&rarr;&infin;
Dn
</p>
<p>n
= p
</p>
<p>}
= 0,
</p>
<p>so that the probability measure is concentrated on a subset of [0,1] of Lebesgue
measure zero. On the other hand, the distribution of the random variable ξ is con-
</p>
<p>tinuous. This follows from the fact that
</p>
<p>{ξ = x} =
&infin;⋂
</p>
<p>k=1
{ωk = δk},
</p>
<p>if x is binary-irrational.
</p>
<p>If ξ is binary-rational, i.e. if, for some r &lt;&infin;, either δk = 0 for all k &ge; r or δk = 1
for all k &ge; r , the continuity follows from the inclusion
</p>
<p>{ξ = x} &sub;
&infin;⋂
</p>
<p>k=r
{ωk = 0} +
</p>
<p>&infin;⋂
</p>
<p>k=r
{ωk = 1},
</p>
<p>since the probabilities of the two events on the right-hand side are clearly equal to
</p>
<p>zero. The singularity of Fξ (x) for p 
= 1/2 is proved. �
</p>
<p>We suggest the reader to plot the distribution function of ξ .
</p>
<p>11.4 The Strong Law of Large Numbers for Arbitrary
</p>
<p>Independent Variables
</p>
<p>Finding necessary and sufficient conditions for convergence
</p>
<p>Sn/bn
a.s.&minus;&rarr; a
</p>
<p>when bn &uarr;&infin; and the summands ξ1, ξ2, . . . are not identically distributed is a diffi-
cult task. We first prove the following theorem.
</p>
<p>Theorem 11.4.1 (Kolmogorov&rsquo;s test for almost everywhere convergence) Assume
that ξk , k = 1,2, . . . , are independent, Eξk = 0, Var(ξk)= σ 2k &lt;&infin; and, moreover,
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>σ 2k
</p>
<p>b2k
</p>
<p>&lt;&infin;. (11.4.1)
</p>
<p>Then Sn/bn
a.s.&minus;&rarr; 0 as n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 The Strong Law of Large Numbers for Arbitrary Independent Variables 327
</p>
<p>Proof It follows from the conditions of Theorem 11.4.1 that (see Corollary 11.2.2)
the series
</p>
<p>&sum;&infin;
k=1 ξk/bk is convergent with probability 1. Therefore the assertion of
</p>
<p>Theorem 11.4.1 is a consequence of the following well-known lemma from calcu-
</p>
<p>lus. �
</p>
<p>Lemma 11.4.1 Let bn &uarr; &infin; and a sequence x1, x2, . . . be such that the series&sum;&infin;
k=1 xk is convergent. Then, as n&rarr;&infin;,
</p>
<p>1
</p>
<p>bn
</p>
<p>&infin;&sum;
</p>
<p>k=1
bkxk &rarr; 0.
</p>
<p>Proof Put Xn :=
&sum;&infin;
</p>
<p>k=n+1 xk so that Xn &rarr; 0 as n &rarr; &infin;, and X :=
maxn&ge;0 |Xn|&lt;&infin;. Using the Abel transform, we obtain that
</p>
<p>n&sum;
</p>
<p>k=1
bkxk =
</p>
<p>n&sum;
</p>
<p>k=1
bk(Xk&minus;1 &minus;Xk)=
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
bk+1Xk &minus;
</p>
<p>n&sum;
</p>
<p>k=1
bkXk
</p>
<p>=
n&minus;1&sum;
</p>
<p>k=1
(bk+1 &minus; bk)Xk + b1X0 &minus; bnXn,
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>1
</p>
<p>bn
</p>
<p>n&sum;
</p>
<p>k=1
bkxk &le; lim sup
</p>
<p>n&rarr;&infin;
</p>
<p>1
</p>
<p>bn
</p>
<p>n&minus;1&sum;
</p>
<p>k=1
(bk+1 &minus; bk)Xk. (11.4.2)
</p>
<p>Here, for a given ε &gt; 0, we can choose an N such that |Xk|&lt; ε for k &ge;N . Therefore
</p>
<p>n&minus;1&sum;
</p>
<p>k=1
(bk+1 &minus; bk)Xk &le;
</p>
<p>N&minus;1&sum;
</p>
<p>k=1
(bk+1 &minus; bk)X+ ε
</p>
<p>n&minus;1&sum;
</p>
<p>k=N
(bk+1 &minus; bk)
</p>
<p>=X(bN &minus; b1)+ ε(bn &minus; bN ).
</p>
<p>From here and (11.4.2) it follows that
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>1
</p>
<p>bn
</p>
<p>n&sum;
</p>
<p>k=1
bkxk &le; ε.
</p>
<p>Since a similar inequality holds for lim inf, the lemma is proved. �
</p>
<p>We could also prove Theorem 11.4.1 directly, using the Kolmogorov inequality,
</p>
<p>in a way similar to the argument in Theorem 11.2.1.
</p>
<p>Example 11.4.1 Assume that ξk , k = 1,2, . . . , are independent random variables
taking the values ξk = &plusmn;kα with probabilities 1/2. As we saw in Example 8.4.1,
for α &gt; &minus;1/2, the sums Sn of these variables are asymptotically normal with the
appropriate normalising factor n&minus;α&minus;1/2. Since Var(ξk)= σ 2k = k2α , we see that, for</p>
<p/>
</div>
<div class="page"><p/>
<p>328 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>β &gt; α+ 1/2, n&minus;βSn satisfies the strong law of large numbers because, for bk = kβ ,
the series
</p>
<p>&infin;&sum;
</p>
<p>k=1
k2αb&minus;2k =
</p>
<p>&infin;&sum;
</p>
<p>k=1
k2α&minus;2β
</p>
<p>converges. The &ldquo;usual&rdquo; strong law of large numbers (with the normalising factor
</p>
<p>n&minus;1) holds if the value β = 1 is admissible, i.e. when α &lt; 1/2.
</p>
<p>Now we will derive the &ldquo;usual&rdquo; strong law of large numbers (with scaling factor
</p>
<p>1/n) under conditions which do not assume the existence of the variances Var(ξk)
</p>
<p>and are, in a certain sense, minimal. The following generalisation of the &ldquo;sufficiency
</p>
<p>part&rdquo; of Theorem 11.1.3 is valid.
</p>
<p>Theorem 11.4.2 Let Eξk = 0 and the tails P(|ξk|&gt; t) admit a common integrable
majorant:
</p>
<p>P
(
|ξk|&gt; t
</p>
<p>)
&le; g(t),
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>g(t) dt &lt;&infin;. (11.4.3)
</p>
<p>Then, as n&rarr;&infin;,
Sn
</p>
<p>n
</p>
<p>a.s.&minus;&rarr; 0. (11.4.4)
</p>
<p>Note that condition (11.4.3) can also be rewritten as |ξk|
d
&le; ζ , Eζ &lt; &infin;. To
</p>
<p>see this, it suffices to consider a random variable ζ &ge; 0 for which P(ζ &gt; t) =
min(1, g(t)). Here, without loss of generality, we can assume that g(t) is non-
</p>
<p>increasing (we can take the minimal majorant g(t) := supk P(|ξk|&gt; t) &darr;).
Condition (11.4.3) clearly implies the uniform integrability of ξk . The latter was
</p>
<p>sufficient for the law of large numbers, but is insufficient for the strong law of large
</p>
<p>numbers. This is shown by the following example.
</p>
<p>Example 11.4.2 Let ξk be such that, for t &gt; 0 and k &gt; 1,
</p>
<p>P(ξk &ge; t)=
{
g(t)+ 1
</p>
<p>k lnk
if t &le; k,
</p>
<p>g(t) if t &gt; k,
</p>
<p>P(ξk &lt;&minus;t)&le; g(t),
where g(t) is integrable so that the ξk has a positive atom of size 1/(k ln k) at the
</p>
<p>point k. Evidently, the ξk are uniformly integrable. Now suppose that Sn/n
a.s&minus;&rarr; 0.
</p>
<p>Since
&infin;&sum;
</p>
<p>k=2
P(ξk &ge; k)&ge;
</p>
<p>&infin;&sum;
</p>
<p>k=2
</p>
<p>1
</p>
<p>k ln k
=&infin;,
</p>
<p>it follows by the Borel&ndash;Cantelli lemma that infinitely many events {ξk &ge; k} occur
with probability 1. Since, for any ε &lt; 1/2 and all k large enough, |Sk| &lt; εk with
probability 1, the events Sk+1 = Sk + ξk+1 &gt; k(1 &minus; ε) occur infinitely often. We
have obtained a contradiction. �</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 The Strong Law of Large Numbers for Arbitrary Independent Variables 329
</p>
<p>Proof of Theorem 11.4.2 Represent the random variables ξk in the form
</p>
<p>ξk = ξ&lowast;k + ξ&lowast;&lowast;k , ξ&lowast;k := ξkI
(
|ξk|&lt; k
</p>
<p>)
, ξ&lowast;&lowast;k := ξkI
</p>
<p>(
|ξk| &ge; k
</p>
<p>)
,
</p>
<p>and denote by S&lowast;n and S
&lowast;&lowast;
n the respective sums of random variables ξ
</p>
<p>&lowast;
k and ξ
</p>
<p>&lowast;&lowast;
k . Then
</p>
<p>the sum Sn can be written as
</p>
<p>Sn =
(
S&lowast;n &minus;ES&lowast;n
</p>
<p>)
+ S&lowast;&lowast;n &minus;ES&lowast;&lowast;n . (11.4.5)
</p>
<p>Now we will evaluate the three summands on the right-hand side of (11.4.5).
</p>
<p>1. Since ξk are uniformly integrable, we have
</p>
<p>Eξ&lowast;&lowast;k = o(1) as k&rarr;&infin;,
</p>
<p>ES&lowast;&lowast;n = o(n),
ES&lowast;&lowast;n
n
</p>
<p>&rarr; 0 as n&rarr;&infin;.
(11.4.6)
</p>
<p>2. Since &sum;
P
(
|ξk|&gt; k
</p>
<p>)
&le;
&sum;
</p>
<p>g(k) &lt;&infin;,
we obtain from Theorem 11.1.1 that, with probability 1, only a finite number of
</p>
<p>random variables ξ&lowast;&lowast;k are nonzero and hence, as n&rarr;&infin;,
S&lowast;&lowast;n
n
</p>
<p>a.s.&minus;&rarr; 0. (11.4.7)
</p>
<p>3. To bound the first summand on the right-hand side of (11.4.5) we make use of
</p>
<p>Theorem 11.4.1. Since
</p>
<p>Var
(
ξ&lowast;k
)
&le; E
</p>
<p>(
ξ&lowast;k
)2 = 2
</p>
<p>&int; k
</p>
<p>0
</p>
<p>uP
(
|ξ&lowast;k | &ge; u
</p>
<p>)
du&le; 2
</p>
<p>&int; k
</p>
<p>o
</p>
<p>ug(u)du,
</p>
<p>we see that the series in (11.4.1) for ξ&lowast;k &minus;Eξ&lowast;k and bk = k admits the upper bound
</p>
<p>2
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>k2
</p>
<p>&int; k
</p>
<p>0
</p>
<p>ug(u)du. (11.4.8)
</p>
<p>The last series converges if the integral
&int; &infin;
</p>
<p>1
</p>
<p>1
</p>
<p>t2
</p>
<p>(&int; t
</p>
<p>0
</p>
<p>ug(u)du
</p>
<p>)
dt
</p>
<p>converges. Integrating by parts, we obtain
</p>
<p>&minus;1
t
</p>
<p>&int; t
</p>
<p>0
</p>
<p>ug(u)du
</p>
<p>∣∣∣∣
&infin;
</p>
<p>1
</p>
<p>+
&int; &infin;
</p>
<p>1
</p>
<p>g(t) dt. (11.4.9)
</p>
<p>The last summand here is clearly finite. Since g(u) is integrable and monotone, one
</p>
<p>has
</p>
<p>ug(u)= o(1) as u&rarr;&infin;,
&int; t
</p>
<p>0
</p>
<p>ug(u)du= o(t) as t &rarr;&infin;,
</p>
<p>and hence the value of the first summand in (11.4.9) is zero at t = &infin;. We have
established that series (11.4.8) converges, and hence, by Theorem 11.4.1, as n&rarr;&infin;,
</p>
<p>S&lowast;n &minus;ES&lowast;n
n
</p>
<p>a.s.&minus;&rarr; 0. (11.4.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>330 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>Combining (11.4.5)&ndash;(11.4.7) and (11.4.10), we obtain (11.4.4). The theorem is
</p>
<p>proved. �
</p>
<p>11.5 The Strong Law of Large Numbers for Generalised
</p>
<p>Renewal Processes
</p>
<p>11.5.1 The Strong Law of Large Numbers for Renewal Processes
</p>
<p>Let {τj } be a sequence of independent identically distributed variables, Tn :=&sum;n
j=1 τj and η(t) := min{k : Tk &gt; t}.
</p>
<p>Theorem 11.5.1 If τj
d= τ and Eτ = a &gt; 0 exists then, as t &rarr;&infin;,
</p>
<p>η(t)
</p>
<p>t
</p>
<p>a.s.&minus;&rarr; 1
a
, (11.5.1)
</p>
<p>i.e., for any ε &gt; 0,
</p>
<p>P
</p>
<p>(∣∣∣∣
η(u)
</p>
<p>u
&minus; 1
</p>
<p>a
</p>
<p>∣∣∣∣&lt; ε for all u&ge; t
)
&rarr; 1 (11.5.2)
</p>
<p>as t &rarr;&infin;.
</p>
<p>Proof First let τ &ge; 0. Set
</p>
<p>An :=
{∣∣∣∣
</p>
<p>Tk
</p>
<p>k
&minus; a
</p>
<p>∣∣∣∣&lt; ε for all k &ge; n
}
.
</p>
<p>The strong law of large numbers for {Tk} means that P(An)&rarr; 1 as n&rarr;&infin;.
Consider the function T (v) := T&lfloor;v&rfloor;, where &lfloor;v&rfloor; is the integer part of v. As was
</p>
<p>noted in Sect. 10.1, η(t) is the generalised inverse function to T (v). In other words,
</p>
<p>if we plot the graph of the function T (v) as a continuous line (including &ldquo;vertical&rdquo;
</p>
<p>segments corresponding to jumps) then η(t) can be regarded as the abscissa of the
</p>
<p>point of intersection of the graph of T (v) with level t (see Fig. 11.1); for the values
</p>
<p>of t coinciding with Tk , the intersection will be a segment of length 1, and η(t) is
</p>
<p>then to be taken equal to the right end point of the segment.
</p>
<p>Therefore the event that T (v) lies within the limits v/(a &plusmn; ε) for all sufficiently
large v coincides with the event that η(t) lies within the limits t(a &plusmn; ε) for all suffi-
ciently large t . More precisely,
</p>
<p>An &sub; Bn :=
{
</p>
<p>u
</p>
<p>a &minus; ε &gt; η(u) &gt;
u
</p>
<p>a + ε for all u&ge; n(a + ε)
}
.
</p>
<p>This means that
</p>
<p>P(Bn)&rarr; 1 as n&rarr;&infin;.
This relation is clearly equivalent to (11.5.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 The Strong Law of Large Numbers for Generalised Renewal Processes 331
</p>
<p>Fig. 11.1 The relative
</p>
<p>positions of a trajectory of
</p>
<p>T (v) and the levels v(a &plusmn; ε)
(see the proof of
</p>
<p>Theorem 11.5.1)
</p>
<p>Now suppose τ can also assume negative values. Then η(t) := min{k : T k &gt; t},
where T k = maxk&le;n Tk , so that η(t) is the generalised inverse function
of T (v) := T [v]. Moreover, it is clear that, if T (v) lies within the limits v(a &plusmn; ε)
for all sufficiently large v, then the same is true for the function T (v). It remains to
</p>
<p>repeat the above argument applying it to the processes T (v) and η(t). The theorem
</p>
<p>is proved. �
</p>
<p>Remark 11.5.1 (An analogue of Remarks 8.3.3, 8.4.1, 10.1.1 and 10.5.1) Conver-
gence (11.5.1) persists if we remove all the restrictions on the random variable τ1.
</p>
<p>Namely, the following assertion generalising Theorem 11.5.1 is valid. Let τ1 be an
</p>
<p>arbitrary random variable and the variables τ &lowast;k = τk+1
d= τ , k &ge; 1, satisfy the con-
</p>
<p>ditions of Theorem 11.5.1. Then (11.5.1) holds true.
The Proof of this assertion is quite similar to the proofs of the corresponding
</p>
<p>assertions in the above mentioned remarks, and we leave it to the reader. �
</p>
<p>These assertions show that replacement of one or several terms in the consid-
</p>
<p>ered sequences of random variables with arbitrary variables changes nothing in the
</p>
<p>established convergence relations. (The exception is Theorem 11.1.1, in which the
</p>
<p>condition Emin(0, τ1) &gt; &minus;&infin; is essential.) This fact will be used in Chap. 13 de-
voted to Markov chains.
</p>
<p>11.5.2 The Strong Law of Large Numbers for Generalised
</p>
<p>Renewal Processes
</p>
<p>Now let a sequence of independent identically distributed random vectors (τj , ξj )
d=
</p>
<p>(τ, ξ) be given and Sn =
&sum;n
</p>
<p>j=1 ξj . Our goal is to obtain an analogue of Theo-
rem 11.5.1 for generalised renewal processes S(t)= Sη(t) (see Sect. 10.6).</p>
<p/>
</div>
<div class="page"><p/>
<p>332 11 Properties of the Trajectories of Random Walks. Zero-One Laws
</p>
<p>Theorem 11.5.2 If τ &gt; 0 and there exist a := Eτ and aξ := Eξ then
S(t)
</p>
<p>t
</p>
<p>a.s.&minus;&rarr; aξ
a
</p>
<p>as t &rarr;&infin;.
</p>
<p>The Proof of the theorem is almost obvious. It follows from the representation
</p>
<p>S(t)
</p>
<p>t
= Sη(t)
</p>
<p>η(t)
&middot; η(t)
</p>
<p>t
</p>
<p>and the a.s. convergence relations
</p>
<p>Sn
</p>
<p>n
</p>
<p>a.s.&minus;&rarr; aξ ,
η(t)
</p>
<p>t
</p>
<p>a.s.&minus;&rarr; 1
a
. �
</p>
<p>Note that the independence of the components τ and ξ is not assumed here.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
</p>
<p>Random Walks and Factorisation Identities
</p>
<p>Abstract In this chapter, several remarkable and rather useful relations establishing
</p>
<p>interconnections between different characteristics of random walks (the so-called
</p>
<p>boundary functionals) are derived, and the arising problems are related to the sim-
</p>
<p>plest boundary problems of Complex Analysis. Section 12.1 introduces the concept
</p>
<p>of factorisation identity and derives two fundamental identities of that kind. Some
</p>
<p>consequences of these identities, including the trichotomy theorem on the oscilla-
</p>
<p>tory behaviour of random walks and a one-sided version of the Strong Law of Large
</p>
<p>Numbers are presented in Sect. 12.2. Pollaczek&ndash;Spitzer&rsquo;s identity and an identity
</p>
<p>for the global maximum of the random walk are derived in Sect. 12.3, followed
</p>
<p>by illustrating these results by examples from the ruin theory and the theory of
</p>
<p>queueing systems in Sect. 12.4. Sections 12.5 and 12.6 are devoted to studying the
</p>
<p>cases where factorisation components can be obtained in explicit form and so closed
</p>
<p>form expressions are available for the distributions of a number of important bound-
</p>
<p>ary functionals. Sections 12.7 and 12.8 employ factorisation identities to derive the
</p>
<p>asymptotic properties of the distribution of the excess of a random walk of a high
</p>
<p>level and that of the global maximum of the walk, and also to analyse the distribution
</p>
<p>of the first passage time.
</p>
<p>In the present chapter we derive several remarkable and rather useful relations es-
</p>
<p>tablishing interconnections between different characteristics of random walks (the
</p>
<p>so-called boundary functionals) and also relate the arising problems with the sim-
</p>
<p>plest boundary problems of complex analysis.
</p>
<p>12.1 Factorisation Identities
</p>
<p>12.1.1 Factorisation
</p>
<p>On the plane of a complex variable λ, denote by Π the real axis Imλ= 0 and by Π+
(Π&minus;) the half-plane Imλ &gt; 0 (Imλ &lt; 0). Let f(λ) be a continuous function defined
on Π .
</p>
<p>Definition 12.1.1 If there exists a representation
</p>
<p>f(λ)= f+(λ)f&minus;(λ), λ &isin;Π, (12.1.1)
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_12, &copy; Springer-Verlag London 2013
</p>
<p>333</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_12">http://dx.doi.org/10.1007/978-1-4471-5201-9_12</a></div>
</div>
<div class="page"><p/>
<p>334 12 Random Walks and Factorisation Identities
</p>
<p>where f&plusmn; are analytic in the domains Π&plusmn; and continuous on Π&plusmn; &cup;Π , then we will
say that the function f allows factorisation. The functions f&plusmn; are called factorisation
components (positive and negative, respectively).
</p>
<p>Further, denote by K the class of functions f defined on Π that are continuous
</p>
<p>and such that
</p>
<p>sup
λ&isin;Π
</p>
<p>∣∣f(λ)
∣∣&lt;&infin;, inf
</p>
<p>λ&isin;Π
</p>
<p>∣∣f(λ)
∣∣&gt; 0. (12.1.2)
</p>
<p>Similarly we define the classes K&plusmn; of functions analytic in Π&plusmn; and continuous
on Π&plusmn; &cup;Π , such that
</p>
<p>sup
λ&isin;Π&plusmn;
</p>
<p>∣∣f&plusmn;(λ)
∣∣&lt;&infin;, inf
</p>
<p>λ&isin;Π&plusmn;
</p>
<p>∣∣f&plusmn;(λ)
∣∣&gt; 0. (12.1.3)
</p>
<p>Definition 12.1.2 If, for an f &isin; K, there exists a representation (12.1.1), where
f&plusmn; &isin;K&plusmn;, then we will say that the function f allows canonical factorisation.
</p>
<p>Representations of the form
</p>
<p>f(λ)= f+(λ)f&minus;(λ)f0, f(λ)=
f+(λ)f0
f&minus;(λ)
</p>
<p>, λ &isin;Π,
</p>
<p>where f0 = const and f&plusmn; &isin;K&plusmn;, are also called canonical factorisations.
</p>
<p>Lemma 12.1.1 The components f&plusmn; of a canonical factorisation of a function f &isin;K
are defined uniquely up to a constant factor.
</p>
<p>Proof Together with the canonical factorisation (12.1.1), let there exist another
canonical factorisation
</p>
<p>f(λ)= g+(λ)g&minus;(λ), λ &isin;Π.
Then
</p>
<p>f+(λ)f&minus;(λ)= g+(λ)g&minus;(λ), λ &isin;Π,
and, by (12.1.2), we can divide both sides of the inequality by g+(λ)f&minus;(λ). We get
</p>
<p>f+(λ)
</p>
<p>g+(λ)
= g&minus;(λ)
</p>
<p>f&minus;(λ)
,
</p>
<p>where, by virtue of (12.1.2), the function
f+(λ)
g+(λ)
</p>
<p>(
g&minus;(λ)
f&minus;(λ)
</p>
<p>) belongs to the class K+
</p>
<p>(K&minus;). We have obtained that the function
f+(λ)
g+(λ)
</p>
<p>, analytical in Π+, can be analyti-
</p>
<p>cally continued over the line Π onto the half-plane Π&minus; (to the function
g&minus;(λ)
f&minus;(λ)
</p>
<p>). After
</p>
<p>such a continuation, in view of (12.1.3), this function remains bounded on the whole
</p>
<p>complex plane. By Liouville&rsquo;s theorem, bounded entire functions must be constant,
</p>
<p>i.e. there exists a constant c, such that, on the whole plane
</p>
<p>f+(λ)
</p>
<p>g+(λ)
= g&minus;(λ)
</p>
<p>f&minus;(λ)
= c,
</p>
<p>holds, so f+(λ)= cg+(λ), f&minus;(λ)= c&minus;1g&minus;(λ). The lemma is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 Factorisation Identities 335
</p>
<p>The factorisation problem consists in finding conditions under which a given
function f admits a factorisation, and in finding the components of the factorisation.
</p>
<p>This problem has a number of important applications to solving integral equations
</p>
<p>and is a version of the well-known Cauchy&ndash;Riemann boundary-value problem in
</p>
<p>complex function theory. We will see later that factorisation is also an important
</p>
<p>tool for studying the so-called boundary problems in probability theory.
</p>
<p>12.1.2 The Canonical Factorisation of the Function
</p>
<p>fz(λ)= 1&minus; zϕ(λ)
</p>
<p>Let (Ω,F,P) be a probability space on which a sequence {ξk}&infin;k=1 of indepen-
dent identically distributed (ξk
</p>
<p>d= ξ ) random variables is given. Put, as before,
Sn :=
</p>
<p>&sum;n
k=1 ξk and S0 = 0. The sequence {Sk}&infin;k=0 forms a random walk.
</p>
<p>First of all, note that the function
</p>
<p>fz(λ) := 1 &minus; zϕ(λ), ϕ(λ) := Eeiλξ , λ &isin;Π,
belongs to K, for all z with |z| &lt; 1 (here z is a complex-valued parameter). This
follows from the inequalities |ϕ(λ)| &le; 1 for λ &isin;Π and |zϕ(λ)|&lt; |z|&lt; 1.
</p>
<p>Theorem 12.1.1 (The first factorisation identity) For |z| &lt; 1, the function fz(λ)
admits the canonical factorisation
</p>
<p>fz(λ)= fz+(λ)C(z)fz&minus;(λ), λ &isin;Π, (12.1.4)
where
</p>
<p>f+(λ)= exp
{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
E
(
eiλSk ; Sk &gt; 0
</p>
<p>)
}
&isin;K+,
</p>
<p>fz&minus;(λ)= exp
{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
E
(
eiλSk ; Sk &lt; 0
</p>
<p>)
}
&isin;K&minus;, (12.1.5)
</p>
<p>C(z)= exp
{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
P(Sk = 0)
</p>
<p>}
.
</p>
<p>Proof Since |z|&lt; 1, ln(1 &minus; zϕ(λ)) exists, understood in the principal value sense.
The following equalities give the desired decomposition:
</p>
<p>fz(λ)= eln(1&minus;zϕ(λ)) = exp
{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zkϕk(λ)
</p>
<p>k
</p>
<p>}
= exp
</p>
<p>{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
EeiλSk
</p>
<p>}
</p>
<p>= exp
{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
E
(
eiλSk ; Sk &gt; 0
</p>
<p>)
}
</p>
<p>exp
</p>
<p>{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
P(Sk = 0)
</p>
<p>}
</p>
<p>&times; exp
{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
E
(
eiλSk ; Sk &lt; 0
</p>
<p>)
}
.</p>
<p/>
</div>
<div class="page"><p/>
<p>336 12 Random Walks and Factorisation Identities
</p>
<p>Show that fz+(λ) &isin;K+. Indeed, the function E(eiλSk ; Sk &gt; 0), for every k and λ &isin;
Π+&cup;Π , does not exceed 1 in the absolute value, is analytic in Π+, and is continuous
on Π+ &cup; Π . Analyticity follows from the differentiability of this function at any
point λ &isin;Π+ (see also Property 6 of ch.f.s in Sect. 7.1). The function ln fz+(λ) is
a uniformly converging series of functions analytic in Π+, and hence possesses the
same properties together with the function fz+(λ). The same can be said about the
continuity on Π &cup;Π+.
</p>
<p>That fz&minus;(λ) &isin;K&minus; is established in a similar way. The theorem is proved. �
</p>
<p>12.1.3 The Second Factorisation Identity
</p>
<p>The second factorisation identity is associated with the so-called boundary function-
</p>
<p>als of the random walk {Sk}. On the main probability space (Ω,F,P) we define,
together with {ξk}, the random variable
</p>
<p>η0+ := min{k &ge; 1; Sk &ge; 0}.
</p>
<p>This is the first-passage time to zero level. For the elementary events such that all
Sk &lt; 0, k &ge; 1, we put η0+ := &infin;. Like the random variable η(0) in Sect. 10.1, the
variable η0+ is a Markov time.
</p>
<p>The random variable χ0+ := Sη0+ is called the first nonnegative sum. It is defined
on the set {η0+ &lt;&infin;} only.
</p>
<p>The first passing time of zero from the right
</p>
<p>η0&minus; := min{k &ge; 1; Sk &le; 0}
</p>
<p>possesses quite similar properties, and so does the first nonpositive sum χ0&minus; := Sη0&minus; .
Studying the properties of the introduced random variables, which are called
</p>
<p>boundary functionals of the random walk {Sk}, is of significant independent interest.
For instance, the variable η0+ is a stopping time, and understanding its nature is
essential for studying stopping times in many more complex problems (see e.g.
</p>
<p>the problems of the renewal theory in Chap. 10, the problems of statistical control
</p>
<p>described in Sect. 4.4 and so on). Moreover, the variables η0+ and χ
0
+ will be needed
</p>
<p>to describe the extrema
</p>
<p>ζ := sup(S1, S2, . . .) and γ := inf(S1, S2, . . .),
</p>
<p>which are also termed boundary functionals and play an important role in the prob-
</p>
<p>lems of mathematical statistics, queueing theory (see Sect. 12.4), etc.
</p>
<p>Put, as before, ϕ(λ) := ϕξ (λ)= Eeiλξ .
</p>
<p>Theorem 12.1.2 (The second factorisation identity) For the ch.f. of the joint distri-
butions of the introduced random variables, for |z|&lt; 1 and Imλ= 0, the canonical
factorisation</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 Factorisation Identities 337
</p>
<p>fz(λ) := 1 &minus; zϕ(λ)
=
</p>
<p>[
1 &minus;E
</p>
<p>(
eiλχ
</p>
<p>0
+zη
</p>
<p>0
+; η0+ &lt;&infin;
</p>
<p>)]
D&minus;1(z)
</p>
<p>[
1 &minus;E
</p>
<p>(
eiλχ
</p>
<p>0
&minus;zη
</p>
<p>0
&minus;; η0&minus; &lt;&infin;
</p>
<p>)]
,
</p>
<p>of fz(λ) holds true, where
</p>
<p>D(z) := 1 &minus;E
(
zη
</p>
<p>0
+; χ0+ = 0, η0+ &lt;&infin;
</p>
<p>)
= 1 &minus;E
</p>
<p>(
zη
</p>
<p>0
&minus;; χ0&minus; = 0, η0&minus; &lt;&infin;
</p>
<p>)
.
</p>
<p>Proof Set ζn := max{S1, . . . , Sn}. We have
</p>
<p>ϕn(λ)= EeiλSn =
n&sum;
</p>
<p>k=1
E
(
eiλSn; η0+ = k
</p>
<p>)
+E
</p>
<p>(
eiλSn; ζn &lt; 0
</p>
<p>)
</p>
<p>=
n&sum;
</p>
<p>k=1
E
(
eiλ(Sn&minus;Sk)eiλSk I
</p>
<p>(
η0+ = k
</p>
<p>))
+Mn, (12.1.6)
</p>
<p>where Mn = E(eiλSn; ζn &lt; 0) and I(A) is the indicator of the event A. For each
fixed k, the random variables Sn&minus; Sk and SkI(η0+ = k)= χ0+I(η0+ = k) are indepen-
dent. Hence,
</p>
<p>ϕn(λ)=
n&sum;
</p>
<p>k=1
ϕn&minus;k(λ)E
</p>
<p>(
eiλχ
</p>
<p>0
+; η0+ = k
</p>
<p>)
+Mn.
</p>
<p>Now multiply both sides by zn, n= 0,1, . . . , and then sum up over n. We will use
the convention that, for n= 0,
</p>
<p>n&sum;
</p>
<p>k=1
= 0, Mn = 1.
</p>
<p>For the convolution of two sequences cn =
&sum;n
</p>
<p>k=1 akbn&minus;k , we have
&infin;&sum;
</p>
<p>n=0
cnz
</p>
<p>n =
&infin;&sum;
</p>
<p>n=1
anz
</p>
<p>n
</p>
<p>&infin;&sum;
</p>
<p>n=0
bnz
</p>
<p>n,
</p>
<p>provided that the series in this equality converges absolutely. Since |z| &lt; 1 and
|ϕ(λ)| &le; 1 for Imλ= 0, one has
</p>
<p>&infin;&sum;
</p>
<p>n=0
znϕn(λ)= 1
</p>
<p>1 &minus; zϕ(λ) =
&infin;&sum;
</p>
<p>n=0
zn
</p>
<p>n&sum;
</p>
<p>k=1
ϕn&minus;k(λ)E
</p>
<p>(
eiλχ
</p>
<p>0
+; η0+ = k
</p>
<p>)
+
</p>
<p>&infin;&sum;
</p>
<p>n=0
znMn
</p>
<p>=
&infin;&sum;
</p>
<p>k=1
zkE
</p>
<p>(
eiλχ
</p>
<p>0
+; η0+ = k
</p>
<p>) &infin;&sum;
</p>
<p>n=0
znϕn(λ)+
</p>
<p>&infin;&sum;
</p>
<p>n=0
znMn
</p>
<p>= 1
1 &minus; zϕ(λ) E
</p>
<p>(
eiλχ
</p>
<p>0
+zη
</p>
<p>0
+; η0+ &lt;&infin;
</p>
<p>)
+
</p>
<p>&infin;&sum;
</p>
<p>n=0
znMn,
</p>
<p>or, which is the same,
</p>
<p>fz(λ)= 1 &minus; zϕ(λ)=
1 &minus;E(eiλχ0+zη0+; η0+ &lt;&infin;)&sum;&infin;
</p>
<p>n=0 z
nE(eiλSn; ζn &lt; 0)
</p>
<p>= az+(λ)
az&minus;(λ)
</p>
<p>, (12.1.7)
</p>
<p>where az&plusmn;(λ) denote the numerator and denominator of the ratio obtained for fz(λ).</p>
<p/>
</div>
<div class="page"><p/>
<p>338 12 Random Walks and Factorisation Identities
</p>
<p>It is easy to see that, if we put
</p>
<p>γn := min(S1, . . . , Sn)
</p>
<p>then, repeating the above arguments, we will arrive at the equality
</p>
<p>fz(λ)=
1 &minus;E(eiλχ0&minus;zη0&minus;; η0&minus; &lt;&infin;)&sum;&infin;
</p>
<p>n=0 z
nE(eiλSn; γn &gt; 0)
</p>
<p>= bz&minus;(λ)
bz+(λ)
</p>
<p>, (12.1.8)
</p>
<p>where, similarly to the above, bz∓(λ), respectively, denote the numerator and de-
nominator in relation (12.1.8).
</p>
<p>Now we show that az&plusmn;(λ) &isin;K and bz&plusmn;(λ) &isin;K for |z| &lt; 1. Indeed, for |z| &lt; 1
and Imλ= 0,
</p>
<p>∣∣E
(
eiλχ
</p>
<p>0
+zη
</p>
<p>0
+; η0+ &lt;&infin;
</p>
<p>)∣∣&le; E
(
|z|η0+; η0+ &lt;&infin;
</p>
<p>)
&lt; 1
</p>
<p>and therefore
</p>
<p>sup
λ&isin;Π
</p>
<p>∣∣az+(λ)
∣∣&lt;&infin;, inf
</p>
<p>λ&isin;Π
</p>
<p>∣∣az+(λ)
∣∣&gt; 0.
</p>
<p>Since fz(λ) &isin;K, this also implies that az&minus;(λ) &isin;K. In the same way we obtain that
bz∓(λ) &isin;K. By equating the right-hand sides of (12.1.7) and (12.1.8) and multiply-
ing them by az&minus;(λ)bz+(λ), we get
</p>
<p>az+(λ)bz+(λ)= az&minus;(λ)bz&minus;(λ), λ &isin;Π. (12.1.9)
</p>
<p>Further, the functions az+(λ) and bz+(λ) are bounded and analytic in Π+ for the
same reasons as the function fz+(λ) (see the proof of Theorem 12.1.1). Similarly,
az&minus;(λ) and bz&minus;(λ) are bounded and analytic in Π&minus;. We obtain that the function
az+(λ)bz+(λ) is bounded and analytic in Π+ and, by (12.1.9), has an entire bounded
analytic continuation over the boundary Π to the whole complex plane. This means
</p>
<p>that this function necessarily equals a constant c, and bz+(λ) = ca&minus;1z+(λ) &isin; K+,
az&minus;(λ)= cb&minus;1z&minus;(λ) &isin;K&minus;, so relations (12.1.7) and (12.1.8) deliver a canonical fac-
torisation of fz(λ).
</p>
<p>Further, eiλx &rarr; 0 as Im λ&rarr;&minus;&infin;, x &lt; 0, and therefore
</p>
<p>bz&minus;(&minus;i&infin;)= 1 &minus;E
(
zη
</p>
<p>0
&minus;; χ0&minus; = 0, η0&minus; &lt;&infin;
</p>
<p>)
, az&minus;(&minus;i&infin;)= 1,
</p>
<p>az&minus;(λ)bz&minus;(λ)= az&minus;(&minus;i&infin;)bz&minus;(&minus;i&infin;)= 1 &minus;E
(
zη
</p>
<p>0
&minus;; χ0&minus; = 0, η0&minus; &lt;&infin;
</p>
<p>)
=D(z).
</p>
<p>Substituting into (12.1.7) the value az&minus;(λ)=D(z)/bz&minus;(λ) derived from this equal-
ity, we obtain the assertion of the theorem. The second relation for D(z) follows
</p>
<p>from the equality D(z)= az+(i&infin;)bz+(i&infin;). The theorem is proved. �
</p>
<p>In the proof of Theorem 12.1.2 we used, in formula (12.1.6), a decomposition of
</p>
<p>EeiλSn into summands corresponding to the disjoint events
{
</p>
<p>n⋃
</p>
<p>k=1
</p>
<p>{
η0+ = k
</p>
<p>}
}
= {ζn &ge; 0} and {ζn &lt; 0}.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 Factorisation Identities 339
</p>
<p>But the scheme of the proof will still work if we consider the partition of Ω into the
</p>
<p>events {ζn &gt; 0} and {ζn &le; 0}. In order to do this, we introduce the random variables
η+ := min{k : Sk &gt; 0}
</p>
<p>(η+ =&infin; if ζ &le; 0; note that η+ = η(0) in the notation of Sect. 10.1),
</p>
<p>χ+ := Sη+ ,
η&minus; := min{k : Sk &lt; 0} (η&minus; =&infin; if γ &ge; 0),
χ&minus; := Sη&minus; .
</p>
<p>The variable η+ (η&minus;) is called the time of the first positive (negative) sum χ+
(χ&minus;). Now we can write, together with equalities (12.1.7) and (12.1.8), the relations
</p>
<p>fz(λ)= 1 &minus; zϕ(λ)=
1 &minus;E(eiλχ+zη+; η+ &lt;&infin;)&sum;&infin;
</p>
<p>n=0 z
nE(eiλSn; ζn &le; 0)
</p>
<p>= 1 &minus;E(e
iλχ&minus;zη&minus;; η&minus; &lt;&infin;)&sum;&infin;
</p>
<p>n=0 z
nE(eiλSn; γn &ge; 0)
</p>
<p>. (12.1.10)
</p>
<p>Combining these relations with (12.1.7) and (12.1.8), we will use below the same
</p>
<p>argument as above to prove the following assertion.
</p>
<p>Theorem 12.1.3
</p>
<p>1 &minus;E
(
eiλχ
</p>
<p>0
+zη
</p>
<p>0
+; η0+ &lt;&infin;
</p>
<p>)
=D(z)
</p>
<p>[
1 &minus;E
</p>
<p>(
eiλχ+zη+; η+ &lt;&infin;
</p>
<p>)]
,
</p>
<p>1 &minus;E
(
eiλχ
</p>
<p>0
&minus;zη
</p>
<p>0
&minus;; η0&minus; &lt;&infin;
</p>
<p>)
=D(z)
</p>
<p>[
1 &minus;E
</p>
<p>(
eiλχ&minus;zη&minus;; η&minus; &lt;&infin;
</p>
<p>)]
.
</p>
<p>(12.1.11)
</p>
<p>Here the function D(z) defined in Theorem 12.1.2 also satisfies the relations
</p>
<p>D&minus;1(z)=
&infin;&sum;
</p>
<p>n=0
znP(Sn = 0, ζn &le; 0)=
</p>
<p>&infin;&sum;
</p>
<p>n=0
znP(Sn = 0, γn &ge; 0). (12.1.12)
</p>
<p>Clearly, from Theorem 12.1.3 one can obtain some other versions of the factori-
</p>
<p>sation identity. For instance, one has
</p>
<p>fz(λ)=
[
1 &minus;E
</p>
<p>(
eiλχ+zη+; η+ &lt;&infin;
</p>
<p>)][
1 &minus;E
</p>
<p>(
eiλχ
</p>
<p>0
&minus;zη
</p>
<p>0
&minus;; η0&minus; &lt;&infin;
</p>
<p>)]
. (12.1.13)
</p>
<p>Representations (12.1.12) for D(z) imply, in particular, that
</p>
<p>P(Sn = 0, ζn &le; 0)= P(Sn = 0, γn &ge; 0)
and that D(z)&equiv; 1 if P(Sn = 0)= 0 for all n&ge; 1.
</p>
<p>Proof of Theorem 12.1.3 Let us derive the first relation in (12.1.11). Comparing
(12.1.8) with (12.1.10) we find, as above, that
</p>
<p>[
1 &minus;E
</p>
<p>(
eiλχ+zη+; η+ &lt;&infin;
</p>
<p>)]
bz+(λ)= const = 1, (12.1.14)
</p>
<p>since the product equals 1 for λ= i&infin;. Therefore we obtain (12.1.13) by virtue of
(12.1.8). It remains to compare (12.1.13) with the identity of Theorem 12.1.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>340 12 Random Walks and Factorisation Identities
</p>
<p>Expressions (12.1.12) for D(z) follow if we recall (see (12.1.8) and (12.1.10))
</p>
<p>that the left-hand side of (12.1.14) equals
[ &infin;&sum;
</p>
<p>n=0
znE
</p>
<p>(
eiλSn; ζn &le; 0
</p>
<p>)
]
[
1 &minus;E
</p>
<p>(
eiλχ
</p>
<p>0
&minus;zη
</p>
<p>0
&minus;; η0&minus; &lt;&infin;
</p>
<p>)]
.
</p>
<p>Since this product also equals 1, letting λ = &minus;i&infin; here and in the second identity
of (12.1.11) we get the first equality in (12.1.12). The second equality is proved in
</p>
<p>a similar way. �
</p>
<p>Remark 12.1.1 It is important to note that Theorems 12.1.2 and 12.1.3, as well as
proving the existence of the identities, also provide a means of finding the charac-
</p>
<p>teristic function of the joint distribution of χ and η. That is, if we manage some-
</p>
<p>how to get a representation for fz(λ)= 1 &minus; zϕ(λ) of the form hz+(λ)hz&minus;(λ), where
hz&plusmn;(λ) &isin;K&plusmn;, then by uniqueness of the canonical factorisation we can, for instance,
claim that, up to a constant factor, the function 1 &minus; E(eiλχ+zη+; η+) coincides
with hz+(λ). For examples of how such arguments can be used, see Sects. 12.5
and 12.6.
</p>
<p>12.2 Some Consequences of Theorems 12.1.1&ndash;12.1.3
</p>
<p>12.2.1 Direct Consequences
</p>
<p>Theorems 12.1.1&ndash;12.1.3 (and also their modifications of the form (12.1.13)) and
</p>
<p>the uniqueness of the canonical factorisation (see Lemma 12.1.1) directly imply the
</p>
<p>next result.
</p>
<p>Corollary 12.2.1 In the notation of Theorems 12.1.1 and 12.1.2 one has the follow-
ing equalities.
</p>
<p>1 &minus;E
(
eiλχ+zη+; η+ &lt;&infin;
</p>
<p>)
= fz+(λ);
</p>
<p>D(z)= C(z);
1 &minus;E
</p>
<p>(
eiλχ&minus;zη&minus;; η&minus; &lt;&infin;
</p>
<p>)
= fz&minus;(λ).
</p>
<p>Now we will obtain, as corollaries of Theorems 12.1.1&ndash;12.1.3, some further iden-
</p>
<p>tities in which the parameter z is fixed and equal to 1.
</p>
<p>Corollary 12.2.2 Letting z&rarr; 1 in (12.1.13) we obtain
</p>
<p>f1(λ) := 1 &minus; ϕ(λ)=
[
1 &minus;E
</p>
<p>(
eiλχ+; η+ &lt;&infin;
</p>
<p>)][
1 &minus;E
</p>
<p>(
eiλχ
</p>
<p>0
&minus;; η0&minus; &lt;&infin;
</p>
<p>)]
. (12.2.1)
</p>
<p>It is obvious that one can similarly write other identities of such type correspond-
</p>
<p>ing to the identities that can be derived from Theorems 12.1.1&ndash;12.1.3.
</p>
<p>Clearly, identity (12.2.1) delivers a factorisation of the function f1(λ)= 1&minus;ϕ(λ),
but this factorisation is not canonical since f1(0)= 0 and f1(λ) /&isin;K.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Some Consequences of Theorems 12.1.1&ndash;12.1.3 341
</p>
<p>Corollary 12.2.3 If there exists Eξ = a &lt; 0 then P(η0&minus; &lt;&infin;)= 1, Eχ0&minus; exists, and
P(ζ &le; 0)= a/Eχ0&minus; &gt; 0.
</p>
<p>Proof The first relation follows from the law of large numbers, because
</p>
<p>P
(
η0&minus; &gt; n
</p>
<p>)
&lt; P(Sn &gt; 0)&rarr; 0
</p>
<p>as n&rarr;&infin;. Therefore, in the case under consideration, one has
</p>
<p>E
(
eiλχ
</p>
<p>0
&minus;;η0&minus; &lt;&infin;
</p>
<p>)
= Eeiλχ0&minus; .
</p>
<p>The existence of Eχ0&minus; follows from Wald&rsquo;s identity Eχ
0
&minus; = aEη0&minus; and the theorems
</p>
<p>of Chap. 10, which imply that Eη0&minus; &le; Eη&minus; &lt; &infin;, since Eη&minus; is the value of the
corresponding renewal function at 0.
</p>
<p>Finally, dividing both sides of the identity in Corollary 12.2.2 by λ and taking
</p>
<p>the limit as λ&rarr; 0, we obtain
</p>
<p>a =
(
1 &minus; P(η+ &lt;&infin;)
</p>
<p>)
Eχ0&minus; = P(ζ &le; 0)Eχ0&minus;. �
</p>
<p>It is interesting to note that, as a consequence of this assertion, we can obtain the
</p>
<p>strong law of large numbers. Indeed, since {ζ &lt;&infin;} is a tail event and P(ζ &lt;&infin;)&ge;
P(ζ &le; 0), Corollary 12.2.3 implies that P(ζ&lt;&infin;)= 1 for a &lt; 0. This means that the
assertion of Theorem 10.5.3 holds, and it was this assertion that the strong law of
</p>
<p>large numbers was derived from.
</p>
<p>Based on factorisation identities, we will obtain below a generalisation of this
</p>
<p>law.
</p>
<p>In the remaining part of this chapter, to avoid trivial complications, we will be
</p>
<p>assuming that ξ takes, with positive probability, both positive and negative values.
</p>
<p>Corollary 12.2.4 If a = Eξ = 0 then P(η+ &lt;&infin;)= P(η0&minus; &lt;&infin;)= 1, so that
</p>
<p>1 &minus; ϕ(λ)=
(
1 &minus;Eeiλχ+
</p>
<p>)(
1 &minus;E eiλχ0&minus;
</p>
<p>)
. (12.2.2)
</p>
<p>If, moreover, Eξ2 = σ 2 &lt;&infin; then there exist Eχ+ and Eχ0&minus;, and
</p>
<p>Eχ+Eχ
0
&minus; =&minus;
</p>
<p>σ 2
</p>
<p>2
.
</p>
<p>Proof Consider the sequence ξ̃k = ξk &minus; ε, ε &gt; 0. Denoting by ζ̃ , χ̃0&minus; and ã the cor-
responding characteristics for the newly introduced sequence, we obtain by Corol-
</p>
<p>lary 12.2.3 that
</p>
<p>P(ζ &le; 0) &lt; P(̃ζ &le; 0)= ã
Eχ̃0&minus;
</p>
<p>=&minus; ε
χ̃0&minus;
</p>
<p>,
</p>
<p>where
</p>
<p>Eχ̃0&minus; &le; E(ξ̃1; ξ̃1 &le; 0)= E(ξ &minus; ε; ξ &le; ε)&le; E(ξ ; ξ &le; 0) &lt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>342 12 Random Walks and Factorisation Identities
</p>
<p>So we can make the probability P(ζ &le; 0) arbitrarily small by choosing an appropri-
ate ε, and thus P(ζ &le; 0) = P(η+ =&infin;) = 0. Similarly, we find that P(γ &ge; 0) = 0
and hence
</p>
<p>P
(
η0&minus; =&infin;
</p>
<p>)
&le; P(η&minus; =&infin;)= P(γ &ge; 0)= 0.
</p>
<p>The obtained relations and Corollary 12.2.2 yield identity (12.2.2).
</p>
<p>In order to prove the second assertion of the corollary, divide both sides of iden-
</p>
<p>tity (12.2.2) by λ2 =&minus;(iλ)2 and let λ &isin;Π tend to zero. Then the limit of the left-
hand side will be equal to σ 2/2 (see (7.1.1)), whereas that of the right-hand side will
</p>
<p>be equal to &minus;Eχ+Eχ0&minus;, where Eχ+ &gt; 0, |Eχ0&minus;|&gt; 0. The corollary is proved. �
</p>
<p>Corollary 12.2.5
</p>
<p>1. We always have
&sum; P(Sk=0)
</p>
<p>k
&lt;&infin;.
</p>
<p>2. The following three conditions are equivalent:
</p>
<p>(a) P(ζ &lt;&infin;)= 1;
(b) P(ζ &le; 0)= P(η+ =&infin;) &gt; 0;
(c)
</p>
<p>&sum;&infin;
k=1
</p>
<p>P(Sk&gt;0)
k
</p>
<p>&lt;&infin; or
&sum;&infin;
</p>
<p>k=1
P(Sk&ge;0)
</p>
<p>k
&lt;&infin;.
</p>
<p>Proof To obtain the first assertion, one should let z&rarr; 1 in the second equality in
Corollary 12.2.1 and recall that
</p>
<p>D(1)= 1 &minus; P
(
χ0+ = 0, η0+ &lt;&infin;
</p>
<p>)
&gt; P(ξ &gt; 0) &gt; 0.
</p>
<p>The equivalence of (b) and (c) follows from the equality
</p>
<p>1 &minus; P(η+ &lt;&infin;)= P(ζ &le; 0)= exp
{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>P(Sk &gt; 0
</p>
<p>k
</p>
<p>}
,
</p>
<p>which is derived by putting λ = 0 and letting z &rarr; 1 in the first identity of Corol-
lary 12.2.1.
</p>
<p>Now we will establish the equivalence of (b) and (c). If P(ζ &le; 0) &gt; 0 then P(ζ &lt;
&infin;) &gt; 0 and hence P(ζ &lt;&infin;)= 1, since {ζ &lt;&infin;} is a tail event. Conversely, let ζ
be a proper random variable. Choose an N such that P(ζ &le;N) &gt; 0, and b &gt; 0 such
that k =N/b is an integer and P(ξ &lt;&minus;b) &gt; 0. Then
</p>
<p>{ζ &le; 0} &sup;
{
ξ1 &lt;&minus;b, . . . , ξk &lt;&minus;b, sup
</p>
<p>j&ge;1
(&minus;bk+ ξk+1 + &middot; &middot; &middot; + ξk+j )&le; 0
</p>
<p>}
.
</p>
<p>Since the sequence ξk+1, ξk+2, . . . is distributed identically to ξ1, ξ2, . . . , one has
</p>
<p>P(ζ &le; 0)&ge;
[
P(ξ &lt;&minus;b)
</p>
<p>]k
P(ζ &le; bk) &gt; 0. �
</p>
<p>Corollary 12.2.6
</p>
<p>1. P(ζ &lt;&infin;, γ &gt;&minus;&infin;)= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Some Consequences of Theorems 12.1.1&ndash;12.1.3 343
</p>
<p>2. If there exists Eξ = a &lt; 0 then
</p>
<p>P(η+ &lt;&infin;) &lt; 1, P(ζ &lt;&infin;, γ =&minus;&infin;)= 1,( &infin;&sum;
</p>
<p>k=1
</p>
<p>P(Sk &gt; 0)
</p>
<p>k
&lt;&infin;,
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>P(Sk &lt; 0)
</p>
<p>k
=&infin;
</p>
<p>)
.
</p>
<p>3. If there exists Eξ = a = 0 then
</p>
<p>P(ζ =&infin;, γ =&minus;&infin;)= 1,( &infin;&sum;
</p>
<p>k=1
</p>
<p>P(Sk &gt; 0)
</p>
<p>k
=&infin;,
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>P(Sk &lt; 0)
</p>
<p>k
=&infin;
</p>
<p>)
.
</p>
<p>Here we do not consider the case a &gt; 0 since it is &ldquo;symmetric&rdquo; to the case a &lt; 0.
</p>
<p>Proof The first assertion follows from the fact that at least one of the two series&sum;&infin;
k=1
</p>
<p>P(Sk&lt;0)
k
</p>
<p>and
&sum;&infin;
</p>
<p>k=1
P(Sk&ge;0)
</p>
<p>k
diverges. Therefore, by Corollary 12.2.5 either
</p>
<p>P(γ =&minus;&infin;)= 1 or P(ζ =&infin;)= 1.
The second and third assertions follow from Corollaries 12.2.3&ndash;12.2.5 in an ob-
</p>
<p>vious way. �
</p>
<p>12.2.2 A Generalisation of the Strong Law of Large Numbers
</p>
<p>The above mentioned generalisation of the strong law of large numbers consists of
</p>
<p>the following.
</p>
<p>Theorem 12.2.1 (The one-sided law of large numbers) Convergence of the series
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>P(Sk &gt; εk)
</p>
<p>k
</p>
<p>for every ε &gt; 0 is a necessary and sufficient condition for
</p>
<p>P
</p>
<p>(
lim sup
n&rarr;&infin;
</p>
<p>Sn
</p>
<p>n
&le; 0
</p>
<p>)
= 1. (12.2.3)
</p>
<p>Proof Sufficiency. If the series converges then by Corollary 12.2.5 we have
</p>
<p>P
(
</p>
<p>sup
k
</p>
<p>{Sk &minus; εk}&lt;&infin;
)
= 1.
</p>
<p>Hence {εn} is an upper sequence for {Sn} and
</p>
<p>P
</p>
<p>(
lim sup
k&rarr;&infin;
</p>
<p>Sk
</p>
<p>k
&le; ε
</p>
<p>)
= 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>344 12 Random Walks and Factorisation Identities
</p>
<p>But since ε is arbitrary, we see that
</p>
<p>P
</p>
<p>(
lim sup
k&rarr;&infin;
</p>
<p>Sk
</p>
<p>k
&le; 0
</p>
<p>)
= 1.
</p>
<p>Necessity. Conversely, if equality (12.2.3) holds then, for any ε &gt; 0, with proba-
bility 1 we have Sn/n &lt; ε for all n large enough. This means that
</p>
<p>supk(Sk &minus; εk) &lt; &infin; with probability 1, and hence by Corollary 12.2.5 the series&sum;&infin;
k=1
</p>
<p>P(Sk&gt;εk)
k
</p>
<p>converges. The theorem is proved. �
</p>
<p>Corollary 12.2.7 With probability 1 we have
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>Sn
</p>
<p>n
= α,
</p>
<p>where
</p>
<p>α = inf
{
b :
</p>
<p>&sum; P(Sk &gt; bk)
k
</p>
<p>&lt;&infin;
}
.
</p>
<p>Proof For any b &gt; α, the series in the definition of the number α converges. Since
{lim supn&rarr;&infin; Sn/n &le; b} is a tail event and S&prime;n = Sn &minus; bn again form a sequence
of sums of independent identically distributed random variables, Theorem 12.2.1
</p>
<p>immediately implies that
</p>
<p>P
</p>
<p>(
lim sup
</p>
<p>Sn
</p>
<p>n
&le; b
</p>
<p>)
= 1,
</p>
<p>P
</p>
<p>(
lim sup
</p>
<p>Sn
</p>
<p>n
&le; α
</p>
<p>)
= P
</p>
<p>( &infin;⋂
</p>
<p>k=1
</p>
<p>{
lim sup
</p>
<p>Sn
</p>
<p>n
&le; α + 1
</p>
<p>k
</p>
<p>})
= 1.
</p>
<p>If we assume that P(lim sup Sn/n&le; α&lowast;)= 1 for α&lowast; &lt; α then, for ξ&lowast;k = ξk &minus; α&lowast; and
S&lowast;k =
</p>
<p>&sum;k
j=1 ξ
</p>
<p>&lowast;
j , we will have lim sup
</p>
<p>S&lowast;n
n
&le; 0, and
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>P(Sk &gt; (α
&lowast; + ε) k)
</p>
<p>k
&lt;&infin;
</p>
<p>for any ε &gt; 0, which contradicts the definition of α. The corollary is proved. �
</p>
<p>In order to derive the conventional law of large numbers from Theorem 12.2.1 it
</p>
<p>suffices to use Corollary 12.2.7 and assertion 2 of Corollary 12.2.6. We obtain that in
</p>
<p>the case Eξ = 0 the value of α in Corollary 12.2.7 is 0 and hence lim sup Sn/n= 0
with probability 1. One can establish in the same way that lim inf Sn/n= 0. �
</p>
<p>12.3 Pollaczek&ndash;Spitzer&rsquo;s Identity. An Identity for S = supk&ge;0 Sk
</p>
<p>It is important to note that, besides Theorems 12.1.1 and 12.1.2, there exist a number
</p>
<p>of factorisation identities that give explicit representations (in terms of factorisation</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Pollaczek&ndash;Spitzer&rsquo;s Identity. An Identity for S = supk&ge;0 Sk 345
</p>
<p>components) for ch.f.s of the so-called boundary functionals of the trajectory of
the random walk {Sk}, i.e. functionals associated with the crossing by the trajectory
of {Sk} of certain levels (not just the zero level, as in Theorems 12.1.1&ndash;12.1.3). The
functionals
</p>
<p>Sn = max
k&le;n
</p>
<p>Sk, θn = min{k : Sk = Sn}
</p>
<p>and some others are also among the boundary functionals. For instance, for the triple
</p>
<p>transform of the joint distribution of (Sn, θn), the following representation is valid.
</p>
<p>For |z|&lt; 1, |ρ|&lt; 1/|z| and Imλ&ge; 0, one has
</p>
<p>(1 &minus; z)
&infin;&sum;
</p>
<p>n=0
znE
</p>
<p>(
ρθneiλSn
</p>
<p>)
= fz+(0)
</p>
<p>fzρ+(λ)
.
</p>
<p>(For more detail on factorisation identities, see [3].)
</p>
<p>Among many consequences of this identity we will highlight two results that can
</p>
<p>also be established using the already available Theorems 12.1.1&ndash;12.1.3.
</p>
<p>12.3.1 Pollaczek&ndash;Spitzer&rsquo;s Identity
</p>
<p>So far we have obtained several factorisation identities as relations for numerators
</p>
<p>in representations (12.1.7), (12.1.8) and (12.1.9). Now we turn to the denomina-
</p>
<p>tors. We will obtain one more identity playing an important role in studying the
</p>
<p>distributions of
</p>
<p>Sn = max(0, ζn)= max(0, S1, . . . , Sn).
This is the so-called Pollaczek&ndash;Spitzer identity relating the ch.f.s of Sn, n= 1,2, . . . ,
with those of max(0, Sn), n= 1,2, . . . .
</p>
<p>Theorem 12.3.1 For |z|&lt; 1 and Imλ&ge; 0,
&infin;&sum;
</p>
<p>n=0
znEeiλSn = exp
</p>
<p>{ &infin;&sum;
</p>
<p>n=0
</p>
<p>zk
</p>
<p>k
Eeiλmax(0,Sk)
</p>
<p>}
.
</p>
<p>Using the notation of Theorem 12.1.1, one could write the right-hand side of this
</p>
<p>identity as
</p>
<p>fz+(0)
</p>
<p>(1 &minus; z)fz+(λ)
(see the last relation in the proof of the theorem).
</p>
<p>Proof Theorems 12.1.1&ndash;12.1.3 (as well as their modifications of the form (12.1.13))
and the uniqueness of the canonical factorisation imply that
</p>
<p>&infin;&sum;
</p>
<p>k=0
znE
</p>
<p>(
eiλSn; ζn &lt; 0
</p>
<p>)
=
[
1 &minus;E
</p>
<p>(
eiλχ&minus;zη&minus;; η&minus; &lt;&infin;
</p>
<p>)]&minus;1 = f&minus;1z&minus;(λ),</p>
<p/>
</div>
<div class="page"><p/>
<p>346 12 Random Walks and Factorisation Identities
</p>
<p>where we assume that E(eiλS0; ζ0 &lt; 0)= 1, so all the functions in the above relation
turn into 1 at λ=&minus;i&infin;. Set
</p>
<p>S&lowast;k := Sn&minus;k &minus; Sn, θ&lowast;n := min
{
k : S&lowast;k = S
</p>
<p>&lowast;
n := max
</p>
<p>(
0, S&lowast;1 , . . . , S
</p>
<p>&lowast;
n
</p>
<p>)}
</p>
<p>(θ&lowast;n is time of the first maximum in the sequence 0, S
&lowast;
1 , . . . , S
</p>
<p>&lowast;
n ). Then the event
</p>
<p>{Sn &isin; dx, ζn &lt; 0} can be rewritten as {S&lowast;n &isin;&minus;dx, θ&lowast;n = n}. This implies that
</p>
<p>E
(
eiλSn; ζn &lt; 0
</p>
<p>)
= E
</p>
<p>(
e&minus;iλS
</p>
<p>&lowast;
n ; θ&lowast;n = n
</p>
<p>)
,
</p>
<p>&infin;&sum;
</p>
<p>n=0
znE
</p>
<p>(
eiλS
</p>
<p>&lowast;
n ; θ&lowast;n = n
</p>
<p>)
= f&minus;1z&minus;(&minus;λ).
</p>
<p>(12.3.1)
</p>
<p>But the sequence S&lowast;1 , . . . , S
&lowast;
n is distributed identically to the sequence of sums
</p>
<p>ξ&lowast;1 , ξ
&lowast;
1 + ξ&lowast;2 , . . . , ξ&lowast;1 + &middot; &middot; &middot; + ξ&lowast;n , where ξ&lowast;k =&minus;ξk . If we put θn := min{k : Sk = Sn}
</p>
<p>then identity (12.3.1) can be equivalently rewritten as
</p>
<p>&infin;&sum;
</p>
<p>n=0
znE
</p>
<p>(
eiλSn; θn = n
</p>
<p>)
=
(
f&lowast;z&minus;(&minus;λ)
</p>
<p>)&minus;1
,
</p>
<p>where f&lowast;z&minus;(λ) is the negative factorisation component of the function 1 &minus; zϕ&lowast;(λ)=
1 &minus; zϕ(&minus;λ) corresponding to the random variable &minus;ξ . Since
</p>
<p>1 &minus; zϕ(&minus;λ)= fz+(&minus;λ)C(z)fz&minus;(&minus;λ)
and the function fz+(&minus;λ) possesses all the properties of the negative component
f&lowast;z&minus;(λ) of the factorisation of 1 &minus; zϕ&lowast;(λ), while the function fz&minus;(&minus;λ) has all
the properties of a positive component, we see that f&lowast;z&minus;(λ)= fz+(&minus;λ) and
</p>
<p>&infin;&sum;
</p>
<p>n=0
znE
</p>
<p>(
eiλSn; θn = n
</p>
<p>)
= 1
</p>
<p>fz+(λ)
.
</p>
<p>Now we note that
</p>
<p>EeiλSn =
n&sum;
</p>
<p>k=0
E
(
eiλSn; θn = k
</p>
<p>)
</p>
<p>=
n&sum;
</p>
<p>k=0
E
(
eiλSk ; θk = k, Sk+1 &minus; Sk &le; 0, . . . , Sn &minus; Sk &le; 0
</p>
<p>)
</p>
<p>=
n&sum;
</p>
<p>k=0
E
(
eiλSk ; θk = k
</p>
<p>)
P(Sn&minus;k = 0).
</p>
<p>Since the right-hand side is the convolution of two sequences, we obtain that
&infin;&sum;
</p>
<p>n=0
znE eiλSn =
</p>
<p>&infin;&sum;
</p>
<p>n=0
znP(Sn = 0)
</p>
<p>1
</p>
<p>fz+(λ)
.
</p>
<p>Putting λ= 0 we get
1
</p>
<p>1 &minus; z =
&infin;&sum;
</p>
<p>n=0
znP(Sn = 0)
</p>
<p>1
</p>
<p>fz+(0)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Pollaczek&ndash;Spitzer&rsquo;s Identity. An Identity for S = supk&ge;0 Sk 347
</p>
<p>Therefore,
</p>
<p>&infin;&sum;
</p>
<p>n=0
znE eiλSn = fz+(0)
</p>
<p>(1 &minus; z)fz+(λ)
</p>
<p>= exp
{
&minus; ln(1 &minus; z)+
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
E
(
eiλSk ; Sk &gt; 0
</p>
<p>)
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
P(Sk &gt; 0)
</p>
<p>}
</p>
<p>= exp
{ &infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
E
(
eiλSk ; Sk &gt; 0
</p>
<p>)
+
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
P(Sk &le; 0)
</p>
<p>}
</p>
<p>= exp
{ &infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
E
(
eiλmax(0,Sk)
</p>
<p>)
}
.
</p>
<p>The theorem is proved. �
</p>
<p>12.3.2 An Identity for S = supk&ge;0 Sk
</p>
<p>The second useful identity to be discussed in this subsection is associated with the
</p>
<p>distribution of the random variable S = supk&ge;0 Sk = max(0, ζ ) (of course, we deal
here with the cases when P(S &lt;&infin;) = 1). This distribution is of interest in many
applications. Two such illustrative applications will be discussed in the next subsec-
</p>
<p>tion.
</p>
<p>We will establish the relationship of the distribution of S with that of the vector
</p>
<p>(χ+, η+) and with the factorisation components of the function 1 &minus; zϕ(λ).
First of all, note that the random variable η+ is a Markov time. For such variables,
</p>
<p>one can easily see (cf. Lemma 10.2.1) that the sequence ξ&lowast;1 = ξη++1, ξ&lowast;2 = ξη++2, . . .
on the set {ω : η+ &lt; &infin;} (or given that η+ &lt; &infin;) is distributed identically to
ξ1, ξ2, . . . and does not depend on (η+, ξ1, . . . , ξη+). Indeed,
</p>
<p>P
(
ξ&lowast;1 &isin; B1, . . . , ξ&lowast;k &isin; Bk
</p>
<p>∣∣ η+ = j, ξ1 &isin;A1, . . . , ξη+ &isin;Aη+
)
</p>
<p>= P(ξj+1 &isin; B1, . . . , ξj+k &isin; Bk | ξ1 &isin;A1, . . . , ξj &isin;Aj ; η+ = j)
= P(ξ1 &isin; B1, . . . , ξk &isin; Bk).
</p>
<p>Considering the new sequence {ξ&lowast;k }&infin;k=1 we note that it will exceed level 0 (the
level χ+ for the original sequence) with probability p = P(η+ &lt;&infin;), and that the
distribution of ζ &lowast; = supk&ge;1(ξ&lowast;1 + &middot; &middot; &middot; + ξ&lowast;k ) coincides with the distribution of ζ =
supk&ge;1 Sk .
</p>
<p>Thus, with S&lowast; := max(0, ζ &lowast;), we have
</p>
<p>S = S(ω)=
{
</p>
<p>0 on {ω : η+ =&infin;},
Sη+ + S&lowast; = χ+ + S&lowast; on {ω : η+ &lt;&infin;}.
</p>
<p>Since, as has already been noted, S&lowast; does not depend on χ+ and η+, and the distri-
bution of S&lowast; coincides with that of S, we have</p>
<p/>
</div>
<div class="page"><p/>
<p>348 12 Random Walks and Factorisation Identities
</p>
<p>EeiλS = P(η+ =&infin;)+E
(
eiλ(χ++S
</p>
<p>&lowast;); η+ &lt;&infin;
)
</p>
<p>= (1 &minus; p)+EeiλSE
(
eiλχ+; η+ &lt;&infin;
</p>
<p>)
.
</p>
<p>This implies the following result.
</p>
<p>Theorem 12.3.2 If
&sum; P(Sk&gt;0)
</p>
<p>k
&lt; &infin; or, which is the same, p = P(η+ &lt;&infin;) &lt; 1,
</p>
<p>then
</p>
<p>EeiλS = 1 &minus; p
1 &minus;E(eiλχ+ , η+ &lt;&infin;)
</p>
<p>= 1 &minus; p
f1+(λ)
</p>
<p>.
</p>
<p>In exactly the same way we can obtain the relation
</p>
<p>EeiλS = 1 &minus; p0
1 &minus;E(eiλχ0+ , η0+ &lt;&infin;)
</p>
<p>, (12.3.2)
</p>
<p>where p0 = P(η0+ &lt;&infin;) &lt; 1.
In this case, one can write a factorisation identity in following form:
</p>
<p>1 &minus; ϕ(λ)= (1 &minus; p0)(1 &minus;Ee
iλχ&minus;)
</p>
<p>EeiλS
= (1 &minus; p)(1 &minus;Ee
</p>
<p>iλχ0&minus;)
</p>
<p>EeiλS
. (12.3.3)
</p>
<p>In Sects. 12.5&ndash;12.7 we will discuss the possibility of finding the explicit form
</p>
<p>and the asymptotic properties of the distribution of S.
</p>
<p>12.4 The Distribution of S in Insurance Problems and Queueing
</p>
<p>Theory
</p>
<p>In this section we show that the need to analyse the distribution of the variable S
</p>
<p>considered in Sect. 12.3 arises in insurance problems and also when studying queue-
</p>
<p>ing systems.
</p>
<p>12.4.1 Random Walks in Risk Theory
</p>
<p>Consider the following simplified model of an insurance business operation. De-
</p>
<p>note by x the initial surplus of the company and consider the daily dynamics of
</p>
<p>the surplus. During the k-th day the company receives insurance premiums at the
</p>
<p>rate ξ+k &ge; 0 and pays out claims made by insured persons at the rate ξ
&minus;
k &ge; 0 (in
</p>
<p>case of a fire, a traffic accident, and so on). The amounts ξk = ξ&minus;k &minus; ξ
+
k are ran-
</p>
<p>dom since they depend on the number of newly insured persons, the size of pre-
</p>
<p>miums, claim amounts and so on. For a foreseeable &ldquo;homogeneous&rdquo; time period,
</p>
<p>the amount ξk can be assumed to be independent and identically distributed. If we
</p>
<p>put Sn :=
&sum;n
</p>
<p>k=1 ξk then the company&rsquo;s surplus after n days will be Zn = x &minus; Sn,
provided that we allow it to be negative. But if we assume that the company ruins at</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 The Distribution of S in Insurance Problems and Queueing Theory 349
</p>
<p>the time when Zn first becomes negative, then the probability of no ruin during the
</p>
<p>first n days equals
</p>
<p>P
(
</p>
<p>min
k&le;n
</p>
<p>Zk &ge; 0
)
= P(Sn &le; x),
</p>
<p>where, as above, Sn = maxk&le;n Sk . Accordingly, the probability of ruin within n days
is equal to P(Sn &gt; x), and the probability of ruin in the long run can be identified
</p>
<p>with P(S &gt; x). It follows that, for the probability of ruin to be less than 1, it is nec-
</p>
<p>essary that Eξk &lt; 0 or, which is the same, that Eξ
&minus;
k &lt; Eξ
</p>
<p>+
k . When this condition is
</p>
<p>satisfied, in order to make the probability of ruin small enough, one has to make the
</p>
<p>initial surplus x large enough. In this connection it is of interest to find the explicit
</p>
<p>form of the distribution of S, or at least the asymptotic behaviour of P(S &gt; x) as
</p>
<p>x &rarr;&infin;. Sections 12.5&ndash;12.7 will be focused on this.
</p>
<p>12.4.2 Queueing Systems
</p>
<p>Imagine that &ldquo;customers&rdquo; who are to be served by a certain system arrive with time
</p>
<p>intervals τ1, τ2, . . . between successive arrivals. These could be phone calls, planes
</p>
<p>landing at an airport, clients in a shop, messages to be processed by a computer,
</p>
<p>etc. Assume that serving the k-th customer (the first customer arrived at time 0, the
</p>
<p>second at time τ1, and so on) requires time sk , k = 1,2, . . . If, at the time of the k-
th customer&rsquo;s arrival, the system was busy serving one of the preceding customers,
</p>
<p>the newly arrived customer joins the &ldquo;queue&rdquo; and waits for service which starts
</p>
<p>immediately after the system has finished serving all the preceding customers. The
</p>
<p>problem is to find the distribution of the waiting time wn of the n-th customer&mdash;the
</p>
<p>time spent waiting for the service.
</p>
<p>Let us find out how the quantities wn+1 and wn are related to each other. The
(n+ 1)-th customer arrived τn time units after the n-th customer, but will have to
wait for an extra sn time units during the service of the n-th customer. Therefore,
</p>
<p>wn+1 =wn &minus; τn + sn,
only if wn &minus; τn + sn &ge; 0. If wn &minus; τn + sn &lt; 0 then clearly wn+1 = 0. Thus, if we
put ξn+1 := sn &minus; τn, then
</p>
<p>wn+1 = max(0,wn + ξn+1), n&ge; 1, (12.4.1)
with the initial value of w1 &ge; 0. Let us find the solution to this recurrence equa-
tion. Let, as above, Sn =
</p>
<p>&sum;n
k=1 ξk . Denote by θ(n) the time when the trajectory of
</p>
<p>0, S1, . . . , Sn first attains its minimum:
</p>
<p>θ(n) := min{k : Sk = S n}, S n := min
0&le;j&le;n
</p>
<p>Sj .
</p>
<p>Then clearly (for w0 :=w1)
wn+1 =w1 + Sn if wθ(n) =w1 + S n &gt; 0 (12.4.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>350 12 Random Walks and Factorisation Identities
</p>
<p>(since in this case the right-hand side of (12.4.1) does not vanish and wk+1 =wk+ξk
for all k &le; n), and
</p>
<p>wn+1 = Sn &minus; Sθ(n) if w1 + S n &le; 0 (12.4.3)
(wθ(n) = 0 and wk+1 =wk + ξk for all k &ge; θ(n)). Put
</p>
<p>Sn,j :=
n&sum;
</p>
<p>k=n&minus;j+1
ξk, Sn,n := max
</p>
<p>0&le;j&le;n
Sn,j ,
</p>
<p>so that
</p>
<p>Sn,0 = 0, Sn,n = Sn.
Then
</p>
<p>Sn &minus; Sθ(n) = Sn &minus; S n = max
0&le;j&le;n
</p>
<p>(Sn &minus; Sj )= Sn,n,
</p>
<p>so that w1 + S n = w1 + Sn &minus; Sn,n and the inequality w1 + Sn &le; 0 in (12.4.3) is
equivalent to the inequality Sn,n = Sn &minus; Sθ(n) &ge; w1 + Sn. Therefore (12.4.2) and
(12.4.3) can be rewritten as
</p>
<p>wn+1 = max(Sn,n,w1 + Sn). (12.4.4)
This implies that, for each fixed x &gt; 0,
</p>
<p>P(wn+1 &gt; x)= P(Sn,n &gt; x)+ P(Sn,n &le; x, w1 + Sn &gt; x).
</p>
<p>Now assume that ξk
d= ξ are independent and identically distributed with Eξ &lt; 0.
</p>
<p>Then Sn,n
d= Sn and, as n&rarr;&infin;, we have Sn
</p>
<p>a.s.&minus;&rarr;&minus;&infin;, P(w1 + Sn &gt; x)&rarr; 0 and
P(Sn &gt; x) &uarr; P(S &gt; x). We conclude that, for any initial value w1, the following
limit exists
</p>
<p>lim
n&rarr;&infin;
</p>
<p>P(wn &gt; x)= P(S &gt; x).
</p>
<p>This distribution is called the stationary waiting time distribution. We already
know that it will be proper if Eξ = Es1 &minus;Eτ1 &lt; 0. As in the previous section, here
arises the problem of finding the distribution of S. If, on the other hand, Es1 &gt; E τ1
or Es1 = Eτ1 and s1 
&equiv; τ1 then the &ldquo;stationary&rdquo; waiting time will be infinite.
</p>
<p>12.4.3 Stochastic Models in Continuous Time
</p>
<p>In the theory of queueing systems and risk theory one can equally well employ
</p>
<p>stochastic models in continuous time, when, instead of random walks {Sn}, one uses
generalised renewal processes Z(t) as described in Sect. 10.6. For a given sequence
</p>
<p>of independent identically distributed random vectors (τj , ζj ), the process Z(t) is
</p>
<p>defined by the equality
</p>
<p>Z(t) := Zν(t),</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Explicit Form of Factorisation Components. The Non-lattice Case 351
</p>
<p>where
</p>
<p>Zn :=
n&sum;
</p>
<p>j=1
ζj , ν(t) := max{k : Tk &le; t}, Tk :=
</p>
<p>k&sum;
</p>
<p>j=1
τj .
</p>
<p>For instance, in risk theory, the capital inflow during time t that comes from
</p>
<p>regular premium payments can be described by the function qt , q &gt; 0. The insurer
</p>
<p>covers claims of sizes ζ1, ζ2, . . . with time intervals τ1, τ2, . . . between them (the
</p>
<p>first claim is covered at time τ1). Thus, if the initial surplus is x, then the surplus at
</p>
<p>time t will be
</p>
<p>x + qt &minus;Zν(t) = x + qt &minus;Z(t).
</p>
<p>The insurer ruins if inft (x + qt &minus;Z(t)) &lt; 0 or, which is the same,
</p>
<p>sup
t
</p>
<p>(
Z(t)&minus; qt
</p>
<p>)
&gt; x.
</p>
<p>It is not hard to see that
</p>
<p>sup
t
(Zν(t) &minus; qt)= sup
</p>
<p>k&ge;0
Sk =: S,
</p>
<p>where Sk =
&sum;k
</p>
<p>j=1 ξj , ξj = ζj &minus; qτj . Thus the continuous-time version of the ruin
problem for an insurance company also reduces to finding the distribution of the
</p>
<p>maximums of the cumulative sums.
</p>
<p>12.5 Cases Where Factorisation Components Can Be Found in
</p>
<p>an Explicit Form. The Non-lattice Case
</p>
<p>As was already noted, the boundary functionals of random walks that were consid-
</p>
<p>ered in Sects. 12.1&ndash;12.3 appear in many applied problems (see e.g., Sect. 12.4). This
</p>
<p>raises the question: in what cases can one find, in an explicit form, the factorisation
</p>
<p>components and hence the explicit form of the boundary functionals distributions
</p>
<p>we need? Here we will deal with factorisation of the function 1 &minus; ϕ(λ) and will be
interested in the boundary functionals χ&plusmn; and S.
</p>
<p>12.5.1 Preliminary Notes on the Uniqueness of Factorisation
</p>
<p>As was already mentioned, the factorisation of the function 1 &minus; ϕ(λ) obtained in
Corollaries 12.2.2 and 12.2.4 is not canonical since that function vanishes at λ= 0.
In this connection arises the question of whether a factorisation is unique. In other
</p>
<p>words, if, say, in the case Eξ &lt; 0, we obtained a factorisation
</p>
<p>1 &minus; ϕ(λ)= f+(λ)f&minus;(λ),</p>
<p/>
</div>
<div class="page"><p/>
<p>352 12 Random Walks and Factorisation Identities
</p>
<p>where f&plusmn; are analytic on Π&plusmn; and continuous on Π&plusmn;&cup;Π , then under what conditions
can we state that
</p>
<p>EeiλS = f+(0)
f+(λ)
</p>
<p>(cf. Theorem 12.3.2)? In order to answer this question, in contrast to the above, we
</p>
<p>will have to introduce here restrictions on the distribution of ξ .
</p>
<p>1. We will assume that Eξ exists, and in the case Eξ = 0 that Eξ2 also exists.
2. Regarding the structure of the distribution of ξ we will assume that either
</p>
<p>(a) the distribution F is non-lattice and the Cram&eacute;r condition on ch.f. holds:
</p>
<p>lim sup
|λ|&rarr;&infin;
Imλ=0
</p>
<p>∣∣ϕ(λ)
∣∣&lt; 1, (12.5.1)
</p>
<p>or
</p>
<p>(b) the distribution F is arithmetic.
</p>
<p>Condition (12.5.1) always holds once the distribution F has a nonzero absolutely
</p>
<p>continuous component. Indeed, if F= Fa + Fs + Fd is the decomposition of F into
the absolutely continuous, singular and discrete components then, by the Lebesgue
</p>
<p>theorem,
&int;
eiλxFa(dx)&rarr; 0 as |λ| &rarr;&infin; on Im λ= 0, and so
</p>
<p>lim sup
|λ|&rarr;&infin;
</p>
<p>∣∣ϕ(λ)
∣∣&le; Fs
</p>
<p>(
(&minus;&infin;,&infin;)
</p>
<p>)
+ Fd
</p>
<p>(
(&minus;&infin;,&infin;)
</p>
<p>)
&lt; 1.
</p>
<p>For lattice distributions concentrated at the points a + hk, k being an inte-
ger, condition (12.5.1) is evidently not satisfied since, for λ = 2πj/h, we have
|ϕ(λ)| = |ei2πa/h| = 1 for all integers j . The condition is also not met for any dis-
crete distribution, since any &ldquo;part&rdquo; of such a distribution, concentrated on a finite
</p>
<p>number of points, can be approximated arbitrarily well by a lattice distribution. For
</p>
<p>singular distributions, condition (12.5.1) can yet be satisfied.
</p>
<p>Since, for non-lattice distributions, |ϕ(λ)|&lt; 1 for λ 
= 0, under condition (12.5.1)
one has
</p>
<p>sup
|λ|&gt;ε
</p>
<p>∣∣ϕ(λ)
∣∣&lt; 1 (12.5.2)
</p>
<p>for any ε &gt; 0. This means that the function f(λ)= 1&minus; ϕ(λ) has no zeros on the real
line Π (completed by the points &plusmn;&infin;) except at the point λ= 0.
</p>
<p>In case (b), when the distribution of F is arithmetic, one can consider the ch.f.
</p>
<p>ϕ(λ) on the segment [0,2π] only or, which is the same, consider the generating
function p(z)= Ezξ , in which case we will be interested in the factorisation of the
function 1 &minus; p(z) on the unit circle |z| = 1.
</p>
<p>Under the aforementioned conditions, we can &ldquo;tweak&rdquo; the function 1 &minus; ϕ(λ) so
that it allows canonical factorisation.
</p>
<p>In this section we will confine ourselves to the non-lattice case. The arithmetic
</p>
<p>case will be considered in Sect. 12.6.
</p>
<p>Lemma 12.5.1 Let the distribution F be non-lattice and condition (12.5.1) hold.
Then:</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Explicit Form of Factorisation Components. The Non-lattice Case 353
</p>
<p>1. If Eξ &lt; 0 then the function
</p>
<p>v(λ) := 1 &minus; ϕ(λ)
iλ
</p>
<p>(iλ+ 1) (12.5.3)
</p>
<p>belongs to K and allows a unique canonical factorisation
</p>
<p>v(λ)= v+(λ)v&minus;(λ),
where
</p>
<p>v+(λ) := 1 &minus;E
(
eiλχ+; η+ &lt;&infin;
</p>
<p>)
= 1 &minus; p
</p>
<p>EeiλS
, (12.5.4)
</p>
<p>v&minus;(λ) :=
1 &minus;Eeiλχ0&minus;
</p>
<p>iλ
(iλ+ 1). (12.5.5)
</p>
<p>2. If Eξ = 0 and Eξ2 &lt;&infin; then the function
</p>
<p>v0(λ) := 1 &minus; ϕ(λ)
λ2
</p>
<p>(
λ2 + 1
</p>
<p>)
(12.5.6)
</p>
<p>belongs to K and allows a unique canonical factorisation
</p>
<p>v0(λ)= v0+(λ)v0&minus;(λ),
where
</p>
<p>v0+(λ) :=
1 &minus;Eeiλχ+
</p>
<p>iλ
(iλ&minus; 1),
</p>
<p>v0&minus;(λ) :=
1 &minus;Eeiλχ0&minus;
</p>
<p>iλ
(iλ+ 1)
</p>
<p>(12.5.7)
</p>
<p>(cf. Corollaries 12.2.2 and 12.2.4).
</p>
<p>Here we do not consider the case Eξ &gt; 0 since it is &ldquo;symmetric&rdquo; to the case
</p>
<p>Eξ &lt; 0 and the corresponding assertion can be derived from the assertion 1 of the
</p>
<p>lemma by applying it to the random variables &minus;ξk (or by changing λ to &minus;λ in the
identities), so that in the case Eξ &gt; 0, the function
</p>
<p>1&minus;ϕ(λ)
iλ
</p>
<p>(iλ &minus; 1) will allow a
unique canonical factorisation.
</p>
<p>The uniqueness of the canonical factorisation immediately implies the following
</p>
<p>result.
</p>
<p>Corollary 12.5.1 If, for Eξ &lt; 0, we have a canonical factorisation
</p>
<p>v(λ)=w+(λ)w&minus;(λ),
then
</p>
<p>EeiλS = w+(0)
w+(λ)
</p>
<p>. (12.5.8)
</p>
<p>Proof of Lemma 12.5.1 Let Eξ &lt; 0. Since
</p>
<p>1 &minus; ϕ(λ)
iλ
</p>
<p>&rarr;&minus;Eξ &gt; 0</p>
<p/>
</div>
<div class="page"><p/>
<p>354 12 Random Walks and Factorisation Identities
</p>
<p>as λ&rarr; 0 and (12.5.1) is satisfied, we see that v(λ) is bounded and continuous on Π
and is bounded away from zero. This means that v(λ) &isin;K.
</p>
<p>Further, by Corollary 12.2.2 (see (12.2.1))
</p>
<p>v(λ)= (1 &minus;Ee
iλχ0&minus;)(iλ+ 1)
iλ
</p>
<p>[
1 &minus;E
</p>
<p>(
eiλχ+; η+ &lt;&infin;
</p>
<p>)]
,
</p>
<p>where Eχ0&minus; &isin; (&minus;&infin;,0). Therefore, similarly to the above, we find that
</p>
<p>v&minus;(λ) :=
(1 &minus;Eeiλχ0&minus;)(iλ+ 1)
</p>
<p>iλ
&isin;K.
</p>
<p>Furthermore, v&minus;(λ) &isin; K&minus; (the factor iλ + 1 has a zero at the point λ = i &isin; Π+).
Evidently, we also have
</p>
<p>v+(λ)= 1 &minus;E
(
eiλχ+; η+ &lt;&infin;
</p>
<p>)
&isin;K&cap;K+.
</p>
<p>This proves the first assertion of the lemma. The last equality in (12.5.4) follows
</p>
<p>from Theorem 12.3.2. The uniqueness follows from Lemma 12.1.1.
</p>
<p>The second assertion is proved in a similar way using Corollary 12.2.5, which
</p>
<p>implies that Eχ+ &isin; (0,&infin;), Eχ0&minus; &isin; (&minus;&infin;,0), and
</p>
<p>v0(λ)=
[
(1 &minus;Eeiλχ+)(iλ&minus; 1)
</p>
<p>iλ
</p>
<p>][
(1 &minus;Eeiλχ0&minus;)(iλ+ 1)
</p>
<p>iλ
</p>
<p>]
, (12.5.9)
</p>
<p>where, as before, we can show that v0(λ) &isin;K and the factors on the right-hand side
of (12.5.9) belong to K&cap;K&plusmn;, correspondingly. The lemma is proved. �
</p>
<p>12.5.2 Classes of Distributions on the Positive Half-Line with
</p>
<p>Rational Ch.F.s
</p>
<p>As we saw in Example 7.1.5, the ch.f. of the exponential distribution with density
</p>
<p>βe&minus;βx on (0,&infin;) is β/(β &minus; iλ). The j -th power of this ch.f. ocrresponds to the
gamma-distribution Ŵβ,j (the j -th convolution of the exponential distribution) with
</p>
<p>density (see Sect. 7.7)
</p>
<p>βkxj&minus;1e&minus;βx
</p>
<p>(j &minus; 1)! , x &ge; 0.
</p>
<p>This means that a density of the form
</p>
<p>K&sum;
</p>
<p>k=1
</p>
<p>lk&sum;
</p>
<p>j=1
akjx
</p>
<p>j&minus;1e&minus;βkx (12.5.10)
</p>
<p>on (0,&infin;) (where all βk &gt; 0 are different) can then be considered as a mixture
of gamma-distributions and its ch.f. will be a rational function Pm(λ)/Qn(λ), where</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Explicit Form of Factorisation Components. The Non-lattice Case 355
</p>
<p>Pm and Qn are polynomials of degrees m and n, respectively (for definiteness, we
</p>
<p>can put)
</p>
<p>Qn(λ)=
K&prod;
</p>
<p>k=1
(βk &minus; iλ)lk , (12.5.11)
</p>
<p>and necessarily m&lt; n (see Property 7.1.8) with n=
&sum;K
</p>
<p>k=1 lk . Here all the zeros of
the polynomial Qn are real. But not only densities of the form (12.5.10) can have
</p>
<p>rational ch.f.s. Clearly, the Fourier transform of the function e&minus;βx cosγ x, which can
be rewritten as
</p>
<p>1
</p>
<p>2
e&minus;βx
</p>
<p>(
eiγ x + e&minus;iγ x
</p>
<p>)
, (12.5.12)
</p>
<p>will also be a rational function. Complex-valued functions of this kind will have
</p>
<p>poles that are symmetric with respect to the imaginary line (in our case, at the points
λ = &minus;iβ &plusmn; γ ). Convolutions of functions of the form (12.5.12) will have a more
complex form but will not go beyond representation (12.5.10), where βk are &ldquo;sym-
</p>
<p>metric&rdquo; complex numbers. Clearly, densities of the form (12.5.10), where βk are
</p>
<p>either real and positive or complex and symmetric, Reβk &gt; 0, exhaust all the dis-
</p>
<p>tributions with rational ch.f.s (the coefficients of the &ldquo;conjugate&rdquo; complex-valued
</p>
<p>exponentials must coincide to avoid the presence of irremovable complex terms).
</p>
<p>It is obvious that the converse is also true: rational ch.f.s Pm(λ)/Qn(λ) corre-
</p>
<p>spond to densities of the form (12.5.10). In order to show this it suffices to decom-
</p>
<p>pose Pm(λ)/Qn(λ) into partial fractions, for which the inverse Fourier transforms
</p>
<p>are known.
</p>
<p>We will call densities of the form (12.5.10) on (0,&infin;) exponential polynomials
with exponents βk . We will call the number lk the multiplicity of the exponent βk
&mdash; it corresponds to the multiplicity of the pole of the Fourier transform at the point
</p>
<p>λ=&minus;iβk (recall that Qn(λ)=
&prod;K
</p>
<p>k=1(βk &minus; iλ)lk ). One can approximate an arbitrary
distribution on (0,&infin;) by exponential polynomials (for more details, see [3]).
</p>
<p>12.5.3 Explicit Canonical Factorisation of the Function v(λ) in
</p>
<p>the Case when the Right Tail of the Distribution F Is an
</p>
<p>Exponential Polynomial
</p>
<p>Consider a distribution F on the whole real line (&minus;&infin;,&infin;) with Eξ &lt; 0 and such
that, for x &gt; 0, the distribution has a density that is an exponential polynomial
</p>
<p>(12.5.10). Denote by EP the class of all such distributions. The ch.f. of a distri-
</p>
<p>bution F &isin; EP can be represented as
ϕ(λ)= ϕ+(λ)+ ϕ&minus;(λ),
</p>
<p>where the function
</p>
<p>ϕ&minus;(λ)= E
(
eiλξ ; ξ &le; 0
</p>
<p>)
, ξ &sub;= F,</p>
<p/>
</div>
<div class="page"><p/>
<p>356 12 Random Walks and Factorisation Identities
</p>
<p>is analytic on Π&minus; and continuous on Π&minus; &cup;Π , and ϕ+(λ) is a rational function
</p>
<p>ϕ+(λ)= Pm(λ)
Qn(λ)
</p>
<p>, m &lt; n, (12.5.13)
</p>
<p>analytic on Π+. Here ϕ+(λ) is a ch.f. up to the factor P(ξ &gt; 0) &gt; 0.
It is important to note that, for real &micro;, the equality
</p>
<p>ψ+(&micro;) := ϕ+(&minus;i&micro;)= E
(
e&micro;ξ ; ξ &gt; 0
</p>
<p>)
= Pm(&minus;i&micro;)
</p>
<p>Qn(&minus;i&micro;)
(12.5.14)
</p>
<p>only makes sense for &micro; &lt; β1, where β1 is the minimal zero of the polynomial
</p>
<p>Qn(&minus;i&micro;) (i.e. the pole of ψ+(λ)). It is necessarily a simple and real root since
the function ψ+(&micro;) is real and monotonically increasing. Further, ψ+(&micro;) = &infin;
for &micro; &ge; β1. Therefore the function E(eiλξ ; ξ &gt; 0) is undefined for Re iλ &ge; β1
(Imλ&le;&minus;β1). However, the right-hand side of (12.5.14) (and hence ϕ+(λ)) can be
analytically continued onto the lower half-plane Im λ &le; &minus;β1 to a function defined
on the whole complex plane. In what follows, when we will be discussing zeros of
</p>
<p>the function 1 &minus; ϕ(λ) on Π&minus;, we will mean zeros of this analytical continuation,
i.e. of the function ϕ&minus;(λ)+ Pm(λ)/Qn(λ).
</p>
<p>Further, note that, for distributions from the class EP, the Cram&eacute;r condition
</p>
<p>(12.5.1) on ch.f.s always holds, since ϕ+(λ)&rarr; 0 as |λ| &rarr;&infin;, and
lim sup
</p>
<p>|λ|&rarr;&infin;, λ&isin;Π
</p>
<p>∣∣ϕ(λ)
∣∣= lim sup
</p>
<p>|λ|&rarr;&infin;, λ&isin;Π
</p>
<p>∣∣ϕ&minus;(λ)
∣∣&le; P(ξ &le; 0) &lt; 1.
</p>
<p>For a distribution F &isin; EP, the canonical factorisation of the functions v(λ) and
v0(λ) (see (12.5.3) and (12.5.6)) can be obtained in explicit form expressed in terms
</p>
<p>of the zeros of the function 1 &minus; ϕ(λ).
</p>
<p>Theorem 12.5.1 Let there exist Eξ &lt; 0. In order for the positive component w+(λ)
of a canonical factorisation
</p>
<p>v(λ)=w+(λ)w&minus;(λ), w&plusmn; &isin;K&plusmn; &cap;K,
to be a rational function, it is necessary and sufficient that the function
</p>
<p>ϕ+(λ)= E
(
eiλξ ; ξ &gt; 0
</p>
<p>)
</p>
<p>is rational.
If ϕ+ = Pm/Qn is an uncancellable ratio of polynomials Pm and Qn of degrees
</p>
<p>m and n, respectively, m &lt; n, then the function 1 &minus; ϕ(λ) has precisely n zeros on
Π&minus; (we denote them by &minus;i&micro;1, . . . ,&minus;i&micro;n), and
</p>
<p>w+(λ)=
&prod;n
</p>
<p>k=1(&micro;k &minus; iλ)
Qn(λ)
</p>
<p>, (12.5.15)
</p>
<p>where Qn(&minus;i&micro;k) 
= 0 (i.e. ratio (12.5.15) is uncancellable).
If all zeros &minus;i&micro;k are arranged in descending order of their imaginary parts:
</p>
<p>Re&micro;1 &le; Re&micro;2 &le; &middot; &middot; &middot; &le; Re&micro;n,
then the zero &minus;i&micro;1 will be simple and purely imaginary, &micro;1 &lt; min(Re&micro;2, β1),
where β1 is the minimal zero of Qn(&minus;i&micro;).</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Explicit Form of Factorisation Components. The Non-lattice Case 357
</p>
<p>The theorem implies that the component w&minus;(λ) can also be found in an explicit
form:
</p>
<p>w&minus;(λ)=
(1 &minus; ϕ(λ))(iλ+ 1)Qn(λ)
</p>
<p>iλ
&prod;n
</p>
<p>k=1(&micro;k &minus; iλ)
.
</p>
<p>From Corollary 12.5.1 we obtain the following assertion.
</p>
<p>Corollary 12.5.2 If Eξ &lt; 0 and ϕ = Pm/Qm then
</p>
<p>EeiλS = w+(0)
w+(λ)
</p>
<p>= Qn(λ)&prod;n
k=1(&micro;k &minus; iλ)
</p>
<p>&prod;n
k=1 &micro;k
Qn(0)
</p>
<p>.
</p>
<p>By Theorem 12.5.1 and (12.3.3) we also have
</p>
<p>Eeiλχ
0
&minus; = 1 &minus; (1 &minus; ϕ(λ))
</p>
<p>1 &minus; p
Qn(λ)&prod;&infin;
</p>
<p>k=1(&micro;k &minus; iλ)
</p>
<p>&prod;n
k=1 &micro;k
Qn(0)
</p>
<p>,
</p>
<p>E
(
eiλχ+; η+ &lt;&infin;
</p>
<p>)
= 1 &minus; (1 &minus; p)
</p>
<p>&prod;n
k=1(&micro;k &minus; iλ)Qn(0)
</p>
<p>Qn(λ)
&prod;n
</p>
<p>k=1 &micro;k
.
</p>
<p>(12.5.16)
</p>
<p>Proof of Theorem 12.5.1 The proof of sufficiency will be divided into several stages.
1. In the vicinity of the point λ= 0 on the line Π , the value of
</p>
<p>v(λ)= (1 &minus; ϕ(λ))(iλ+ 1)
iλ
</p>
<p>lies in the vicinity of the point &minus;Eξ &gt; 0. By virtue of (12.5.2), outside a neighbour-
hood of zero one has
</p>
<p>arg
(
1 &minus; ϕ(λ)
</p>
<p>)
&isin;
(
&minus;π
</p>
<p>2
,
π
</p>
<p>2
</p>
<p>)
, arg
</p>
<p>iλ+ 1
iλ
</p>
<p>&isin;
(
&minus;π
</p>
<p>2
,
π
</p>
<p>2
</p>
<p>)
, (12.5.17)
</p>
<p>where, for a complex number z= |z|eiγ , arg z denotes the exponent γ . In (12.5.17)
arg z means the principal value of the argument from (&minus;π,π]. Clearly, arg z1z2 =
arg z1 + arg z2. This implies that, when λ changes from &minus;T to T for large T , the
values of argv(λ) do not leave the interval (&minus;π,π) and do not come close to its
boundaries. Moreover, the initial and final values of v(λ) lie in the sector arg z &isin;
(&minus;π
</p>
<p>2
, π
</p>
<p>2
). This means that, for any T , the following relation is valid for the index of
</p>
<p>the function v on [&minus;T ,T ]:
</p>
<p>indT v :=
1
</p>
<p>2π
</p>
<p>&int; T
</p>
<p>&minus;T
d
(
argv(λ)
</p>
<p>)
&isin;
(
&minus;b
</p>
<p>2
,
b
</p>
<p>2
</p>
<p>)
, b &lt; 1. (12.5.18)
</p>
<p>(If the distribution F has a density on (&minus;&infin;,0] as well then ϕ(&plusmn;T ) &rarr; 0 and
indT v&rarr; 0 as T &rarr;&infin;.)
</p>
<p>2. Represent the function v as the product v(λ)= v1(λ)v2(λ), where
</p>
<p>v1(λ)=
(iλ+ 1)n
Qn(λ)
</p>
<p>,
</p>
<p>v2(λ)=
Qn(λ)(1 &minus; ϕ(λ))
iλ(iλ+ 1)n&minus;1 =
</p>
<p>Qn(λ)&minus; Pm(λ)&minus;Qn(λ)ϕ&minus;(λ)
iλ(iλ+ 1)n&minus;1 .
</p>
<p>(12.5.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>358 12 Random Walks and Factorisation Identities
</p>
<p>We show that
</p>
<p>|n+ indT v2|&lt;
1
</p>
<p>2
. (12.5.20)
</p>
<p>In order to do this, we first note that the function v1 is analytic on Π+ and has there
a zero of multiplicity n at the point λ = i. Consider a closed contour T+T consist-
ing of the segment [&minus;T ,T ] and the semicircle |λ| = T lying in Π+. According to
the argument principle in complex function theory, the number of zeros of the func-
</p>
<p>tion v1 inside TT equals the increment of the argument of v1(λ) divided by 2π when
</p>
<p>moving along the contour T+T in the positive direction, i.e.
</p>
<p>1
</p>
<p>2π
</p>
<p>&int;
</p>
<p>T
+
T
</p>
<p>d argv1(λ)= n.
</p>
<p>As, moreover, v1(λ) &rarr; (&minus;1)n = const as |λ| &rarr; &infin; (see (12.5.11) and (12.5.19)),
we see that the increment of argv1 on the semicircle tends to 0 as T &rarr; &infin;, and
hence
</p>
<p>indT v1 &rarr; indv1 :=
1
</p>
<p>2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
d argv1(λ)= n.
</p>
<p>It remains to note that indT v= indT v1 + indT v2 and make use of (12.5.18).
3. We show that 1&minus; ϕ(λ) has precisely n zeros in Π&minus;. To this end, we first show
</p>
<p>that the function v2(λ), which is analytic in Π&minus; and continuous on Π&minus; &cup;Π , has n
zeros in Π&minus;. Consider the positively oriented closed contour T
</p>
<p>&minus;
T consisting of the
</p>
<p>segment [&minus;T ,T ] (traversed in the negative direction) and the lower half of the circle
|λ| = T , and compute
</p>
<p>1
</p>
<p>2
</p>
<p>&int;
</p>
<p>T
&minus;
T
</p>
<p>d argv2(λ). (12.5.21)
</p>
<p>Since v2(λ) &sim; (&minus;1)n(1 &minus; ϕ&minus;(λ)) (see (12.5.11) and (12.5.19)), |ϕ&minus;(λ)| &lt; 1 as
|λ| &rarr; &infin;, Imλ &le; 0, for large T the part of integral (12.5.21) over the semicircle
will be less than 1/2 in absolute value. Comparing this with (12.5.20) we obtain
</p>
<p>that integral (12.5.21), being an integer, is necessarily equal to n. This means that
</p>
<p>v2(λ) has exactly n zeros in Π&minus;, which we will denote by &minus;i&micro;1, . . . ,&minus;i&micro;n. Since
Qn(&minus;i&micro;k) 
= 0 (otherwise we would have, by (12.5.19), Pm(&minus;i&micro;k) = 0, which
would mean cancellability of the fraction Pm/Qn), the function 1&minus;ϕ(λ) has in Π&minus;
the same zeros as v2(λ) (see (12.5.19)).
</p>
<p>4. It remains to put
</p>
<p>w+(λ)=
&prod;n
</p>
<p>k=1(&micro;k &minus; iλ)
Qn(λ)
</p>
<p>,
</p>
<p>w&minus;(λ)=
(Qn(λ)&minus; Pm(λ)&minus;Qn(λ)ϕ&minus;(λ))(iλ+ 1)
</p>
<p>iλ
&prod;n
</p>
<p>k=1(&micro;k &minus; iλ)
and note that w&plusmn; &isin;K&plusmn; &cap;K.
</p>
<p>The last assertion of the theorem follows from the fact that the real func-
</p>
<p>tion ψ(&micro;) = ϕ(&minus;i&micro;) for Im &micro; = 0 is convex on [0, β1), ψ &prime;(0) = Eξ &lt; 0 and</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Explicit Form of Factorisation Components. The Non-lattice Case 359
</p>
<p>ψ(&micro;) &rarr; &infin; as &micro; &rarr; β1. Therefore on [0, β1) there exists a unique real solution
to the equation ψ(&micro;)= 1. There are no complex zeros in the half-plane Re&micro;&le; &micro;1
since in this region, for Im &micro; 
= 0, one has
</p>
<p>∣∣ψ(&micro;)
∣∣&lt;ψ(Re&micro;)&le;ψ(&micro;1)= 1
</p>
<p>because of the presence of an absolutely continuous component.
</p>
<p>Necessity. Now let w+(λ) be rational. This means that
</p>
<p>w+(λ)= c1 +
&int; &infin;
</p>
<p>0
</p>
<p>eiλxg(x)dx,
</p>
<p>where c1 = w+(i&infin;) and g(x) is an exponential polynomial. It follows from the
equality (see (12.5.5))
</p>
<p>1 &minus; ϕ(λ)=w+(λ)
w&minus;(λ)iλ
</p>
<p>iλ+ 1 = c2w+(λ)
(
1 &minus;Eeiλχ0&minus;
</p>
<p>)
</p>
<p>= c2
(
c1 +
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>eiλxg(x)dx
</p>
<p>)&int; 0
</p>
<p>&minus;&infin;
eiλx dW(x),
</p>
<p>where W(x) = &minus;P(χ0&minus; &lt; x) for x &lt; 0, c2 = const, that ξ has a density for x &gt; 0
that is equal to
</p>
<p>&int; 0
</p>
<p>&minus;&infin;
dW(t) g(x &minus; t).
</p>
<p>Since the integral
</p>
<p>&int; 0
</p>
<p>&minus;&infin;
dW(t) (x &minus; t)ke&minus;β(x&minus;t) = e&minus;βx
</p>
<p>k&sum;
</p>
<p>j=0
(&minus;1)j
</p>
<p>(
k
</p>
<p>j
</p>
<p>)
xj ckj ,
</p>
<p>ckj =
&int; 0
</p>
<p>&minus;&infin;
dW(t) tk&minus;jeβt ,
</p>
<p>is an exponential polynomial, the integral
&int; 0
&minus;&infin; dW(t) g(x &minus; t) is also an exponen-
</p>
<p>tial polynomial, which implies the rationality of E(eiλξ ; ξ &gt; 0). The theorem is
proved. �
</p>
<p>Example 12.5.1 Let the distribution F be exponential on the positive half-line:
</p>
<p>P(ξ &gt; x)= qe&minus;βx, β &gt; 0, q &lt; 1.
Then ϕ+(λ) = qβ/(β &minus; iλ) and we can put m = 0, n = 1, P0(λ) = qβ , Q1(λ) =
β&minus; iλ. The equation ψ(&micro;) := Ee&micro;ξ1 = 1 has, in the half-plane Re&micro;&gt; 0, the unique
solution &micro;1,
</p>
<p>w+(λ)=
&micro;1 &minus; iλ
Q1(λ)
</p>
<p>(see (12.5.15)). By Corollary 12.5.2,
</p>
<p>EeiλS = &micro;1
Q1(0)
</p>
<p>Q1(λ)
</p>
<p>(&micro;1 &minus; iλ)
= &micro;1(β &minus; iλ)
</p>
<p>β(&micro;1 &minus; iλ)
= &micro;1
</p>
<p>β
+ β &minus;&micro;1
</p>
<p>β
</p>
<p>&micro;1
</p>
<p>&micro;1 &minus; iλ
.</p>
<p/>
</div>
<div class="page"><p/>
<p>360 12 Random Walks and Factorisation Identities
</p>
<p>This yields P(S = 0)= &micro;1/β ,
P(S &isin; dx)
</p>
<p>dx
=
(
</p>
<p>1 &minus; &micro;1
β
</p>
<p>)
&micro;1e
</p>
<p>&minus;&micro;1x for x &gt; 0,
</p>
<p>i.e. the distribution of S is exponential on (0,&infin;) with parameter &micro;1 and has a
positive atom at zero.
</p>
<p>Example 12.5.2 (A generalisation of Example 12.5.1) Let F have, on the positive
half-line, the density
</p>
<p>&sum;n
k=1 ake
</p>
<p>&minus;βkx (a sum of exponentials), where 0 &lt; β1 &lt; β2 &lt;
&middot; &middot; &middot;&lt; βn, ak &gt; 0. Then
</p>
<p>Qn(λ)=
n&prod;
</p>
<p>k=1
(βk &minus; iλ).
</p>
<p>As was already noted in Theorem 12.5.1, the equation ψ(&micro;) := ϕ(&minus;iλ)= 1 has, on
the interval (0, β1), a unique zero &micro;1. The function ψ
</p>
<p>&minus;(&micro;) := ϕ&minus;(&minus;i&micro;) is continu-
ous, positive, and bounded for &micro;&gt; 0. On each interval (βk, βk+1), k = 1, . . . , n&minus; 1,
the function
</p>
<p>ψ+(&micro;) := ϕ+(&minus;i&micro;)=
n&sum;
</p>
<p>k=1
</p>
<p>akβk
</p>
<p>(βk &minus;&micro;)
</p>
<p>is continuous and changes from &minus;&infin; to &infin;. Therefore, on each of these inter-
vals, there exists at least one root &micro;k+1 of the equation ψ(&micro;) = 1. Since by The-
orem 12.5.1 there are only n roots of this equation in Re&micro;&gt; 0, we obtain that &micro;k+1
is the unique root in (βk, βk+1) and
</p>
<p>w+(λ)=
&prod;n
</p>
<p>k=1(&micro;k &minus; iλ)
Qn(λ)
</p>
<p>, E eiλS =
n&prod;
</p>
<p>k=1
</p>
<p>(βk &minus; iλ)&micro;k
(&micro;k &minus; iλ)βk
</p>
<p>. (12.5.22)
</p>
<p>This means that 1 &minus; p := P(S = 0)=
&prod;n
</p>
<p>k=1
&micro;k
βk
</p>
<p>, and
</p>
<p>P(S &isin; dx)
dx
</p>
<p>=
n&sum;
</p>
<p>k=1
bke
</p>
<p>&minus;&micro;kx for x &gt; 0,
</p>
<p>where &micro;k &isin; (βk&minus;1, βk), k = 1, . . . , n, β0 = 0, and the coefficients bk are defined by
the decomposition of (12.5.22) into partial fractions.
</p>
<p>By (12.5.16),
</p>
<p>E
(
eiλχ+; η+ &lt;&infin;
</p>
<p>)
= 1 &minus; 1 &minus; p
</p>
<p>EeiλS
= 1 &minus;
</p>
<p>n&prod;
</p>
<p>k=1
</p>
<p>(&micro;k &minus; iλ)
(βk &minus; iλ)
</p>
<p>, (12.5.23)
</p>
<p>so the conditional distribution of χ+ given χ+ &lt;&infin; has a density which is equal to
n&sum;
</p>
<p>k=1
cke
</p>
<p>&minus;βkx, (12.5.24)
</p>
<p>where the coefficients ck , similarly to the above, are defined by the expansion
</p>
<p>of the right-hand side of (12.5.23) into partial fractions. Relation (12.5.24) means</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Explicit Form of Factorisation Components. The Non-lattice Case 361
</p>
<p>that the density of χ+ has the same &ldquo;structure&rdquo; as the density of ξ does for x &gt; 0,
but differs in coefficients of the exponentials only. By (12.5.16) this property of the
</p>
<p>density of χ+ holds in the general case as well.
</p>
<p>12.5.4 Explicit Factorisation of the Function v(λ) when the Left
</p>
<p>Tail of the Distribution F Is an Exponential Polynomial
</p>
<p>Now consider the case where the left tail of the distribution F has a density which is
an exponential polynomial (belongs to the class EP). In this case,
</p>
<p>ϕ&minus;(λ)= E
(
eiλξ ; ξ &lt; 0
</p>
<p>)
= Pm(λ)
</p>
<p>Qn(λ)
,
</p>
<p>where
</p>
<p>Qn(λ)=
K&prod;
</p>
<p>k=1
(βk &minus; iλ)lk , n=
</p>
<p>K&sum;
</p>
<p>k=1
lk, Reβk &lt; 0, m &lt; n.
</p>
<p>Theorem 12.5.2 Let there exist Eξ &lt; 0. For the positive component of the canonical
factorisation v(λ)=w+(λ)w&minus;(λ) of the function
</p>
<p>v(λ)= (1 &minus; ϕ(λ))(iλ+ 1)
iλ
</p>
<p>to be representable as
</p>
<p>w+(λ)=
(
1 &minus; ϕ(λ)
</p>
<p>)
R(λ),
</p>
<p>where R(λ) is a rational function, it is necessary and sufficient that the func-
tion ϕ&minus;(λ) is rational. If ϕ&minus;(λ) = Pm(λ)/Qn(λ) then the function 1 &minus; ϕ(λ)
has precisely n &minus; 1 zeros in the half-plane on Imλ &gt; 0 which we denote by
&minus;i&micro;1, . . . ,&minus;i&micro;n&minus;1, and
</p>
<p>R(λ)= Qn(λ)
iλ
</p>
<p>&prod;n&minus;1
k=1(&micro;k &minus; iλ)
</p>
<p>.
</p>
<p>Theorem 12.5.2, Corollary 12.5.1 and (12.3.3) imply the following assertion.
</p>
<p>Corollary 12.5.3 If Eξ &lt; 0 and ϕ&minus;(λ)= Pm(λ)/Qn(λ) then
</p>
<p>EeiλS = w+(0)
w+(λ)
</p>
<p>=&minus;EξQn(0)iλ
&prod;n&minus;1
</p>
<p>k=1(&micro;k &minus; iλ)
(1 &minus; ϕ(λ))Qn(λ)
</p>
<p>&prod;n&minus;1
k=1 &micro;k
</p>
<p>,
</p>
<p>Eeiλχ&minus; = 1 + (1 &minus; p0)EξQn(0)iλ
&prod;n&minus;1
</p>
<p>k=1(&micro;k &minus; iλ)&prod;n&minus;1
k=1 &micro;kQn(λ)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>362 12 Random Walks and Factorisation Identities
</p>
<p>Here the density of χ&minus; has the same &ldquo;structure&rdquo; as the density of ξ does for
x &lt;&infin;.
</p>
<p>Proof of Theorem 12.5.2 The proof is close to that of Theorem 12.5.1, but unfortu-
nately is not its direct consequence. We present here a brief proof of Theorem 12.5.2
</p>
<p>under the simplifying assumption that the distribution F is absolutely continuous.
</p>
<p>Using the scheme of the proof of Theorem 12.5.1, the reader can easily reconstruct
</p>
<p>the argument in the general case.
</p>
<p>Sufficiency. As in Theorem 12.5.1, we verify that the trajectory of v(λ), &minus;&infin;&lt;
λ&lt;&infin;, does not intersect the ray argv=&minus;π , so in our case there exists
</p>
<p>ind v := lim
T&rarr;&infin;
</p>
<p>indT v= 0.
</p>
<p>Put v := v1v2, where
</p>
<p>v1 :=
Qn &minus; Pm &minus;Qnϕ+
iλ(iλ&minus; 1)n&minus;1 , v2 :=
</p>
<p>(iλ+ 1)(iλ&minus; 1)n&minus;1
Qn
</p>
<p>.
</p>
<p>Clearly, v2 &isin;K&minus; &cap;K and has exactly n&minus; 1 zeros in Π&minus;. Hence, by the argument
principle, ind v2 =&minus;(n&minus; 1), and
</p>
<p>ind v1 =&minus; ind v2 = n&minus; 1.
Since v1 &isin;K+ &cap;K, again using the argument principle we obtain that v1, as well
as 1 &minus; ϕ, has exactly n&minus; 1 zeros &minus;i&micro;1, . . . ,&minus;i&micro;n&minus;1 in Π+. Putting
</p>
<p>w+ :=
(1 &minus; ϕ)Qn
</p>
<p>iλ
&prod;n&minus;1
</p>
<p>k=1(&micro;k &minus; iλ)
, w&minus; :=
</p>
<p>(iλ+ 1)
&prod;n&minus;1
</p>
<p>k=1(&micro;k &minus; iλ)
Qn
</p>
<p>,
</p>
<p>we obtain a canonical factorisation.
</p>
<p>Necessity. Similarly to the preceding arguments, the necessity follows from the
factorisation identity
</p>
<p>1 &minus; ϕ(λ)= c1
(
1 &minus;E
</p>
<p>(
eiλχ+; η+ &lt;&infin;
</p>
<p>))
w&minus;(λ)
</p>
<p>= c1
&int; &infin;
</p>
<p>0
</p>
<p>eiλx dV (x)
</p>
<p>(
c2 +
</p>
<p>&int; 0
</p>
<p>&minus;&infin;
eiλxg(x)dx
</p>
<p>)
,
</p>
<p>where V (x)= P(χ+ &gt; x; η+ &lt;&infin;) for x &gt; 0, ci = const and g(x) is an exponential
polynomial. The theorem is proved. �
</p>
<p>As in Sect. 12.5.1, we do not consider the case Eξ &gt; 0 since it reduces to apply-
</p>
<p>ing the aforementioned argument to the random variable &minus;ξ .
</p>
<p>12.5.5 Explicit Canonical Factorisation for the Function v0(λ)
</p>
<p>The goal of this subsection, as it was in Sects. 12.5.3 and 12.5.4, is to find an ex-
</p>
<p>plicit form of the components w0&plusmn;(λ) in the canonical factorisation of the function</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Explicit Form of Factorisation Components. The Non-lattice Case 363
</p>
<p>v0(λ) = 1&minus;ϕ(λ)
λ2
</p>
<p>(λ2 + 1) in (12.5.6) in terms of the zeros of the function 1 &minus; ϕ(λ)
in the case where Eξ = 0 and either ϕ+(λ) or ϕ&minus;(λ) is a rational function. When
Eξ = 0, it is sufficient to consider the case where ϕ+(λ) is rational, i.e. the distribu-
tion F has on the positive half-line a density which is an exponential polynomial, so
</p>
<p>that
</p>
<p>ϕ+(λ)= Pm(λ)
Qn(λ)
</p>
<p>, Qn(λ)=
K&prod;
</p>
<p>k=1
(βk &minus; iλ)lk , n=
</p>
<p>K&sum;
</p>
<p>k=1
lk.
</p>
<p>The case where it is the function ϕ&minus;(λ) that is rational is treated by switching to
random variable &minus;ξ .
</p>
<p>Theorem 12.5.3 Let Eξ = 0 and Eξ2 = σ 2 &lt; &infin;. For the positive compo-
nent w0+(λ) of the canonical factorisation
</p>
<p>v0(λ)=w0+(λ)w0&minus;(λ), w&plusmn; &isin;K&plusmn; &cap;K,
to be a rational function it is necessary and sufficient that the function ϕ+(λ) =
E(eiλξ ; ξ &gt; 0) is rational. If ϕ+(λ) = Pm(λ)/Qn(λ) is an uncancellable ratio
of polynomials of degrees m and n, respectively, m&lt; n, then the function 1 &minus; ϕ(λ)
has exactly n&minus; 1 zeros in Π&minus; which we denote by &minus;i&micro;1, . . . ,&minus;i&micro;n&minus;1, and
</p>
<p>w0+(λ)=
&prod;n&minus;1
</p>
<p>k=1(&micro;k &minus; iλ)(iλ&minus; 1)
Qn(λ)
</p>
<p>, w0&minus;(λ)=
(1 &minus; ϕ(λ))(iλ+ 1)Qn(λ)
</p>
<p>λ2
&prod;n&minus;1
</p>
<p>k=1(&micro;k &minus; iλ)
.
</p>
<p>(12.5.25)
</p>
<p>Relation (12.5.3) and the uniqueness of canonical factorisation imply the follow-
</p>
<p>ing representation.
</p>
<p>Corollary 12.5.4 Under the conditions of Theorem 12.5.3,
</p>
<p>Eeiλχ+ = 1 &minus;
iλEχ+Qn(0)
</p>
<p>&prod;n&minus;1
k=1(1 &minus; iλ&micro;k )
</p>
<p>Qn(λ)
.
</p>
<p>Proof The corollary follows from (12.5.7), (12.5.25), the uniqueness of canonical
factorisation and the equalities
</p>
<p>v0+(0)= Eχ+, v0+(λ)=
w0+(λ)Eχ+
w0+(0)
</p>
<p>,
</p>
<p>1 &minus;Eeiλχ+ = v
0
+(λ)iλ
</p>
<p>iλ&minus; 1 =
iλEχ+Qn(0)
</p>
<p>&prod;n&minus;1
k=1(&micro;k &minus; iλ)
</p>
<p>Qn(λ)
&prod;n&minus;1
</p>
<p>k=1 &micro;k
.
</p>
<p>Thus, here the &ldquo;structure&rdquo; of the density of χ+ again repeats the structure of the
density of ξ for x &gt; 0. �
</p>
<p>Proof of Theorem 12.5.3 The proof is similar to that of Theorem 12.5.1.
Sufficiency.
1. In the vicinity of the point λ= 0, λ &isin;Π , the value of v0(λ) lies in the vicinity
</p>
<p>of the point σ 2/2 &gt; 0 by Property 7.1.5 of ch.f.s. Outside of a neighbourhood of</p>
<p/>
</div>
<div class="page"><p/>
<p>364 12 Random Walks and Factorisation Identities
</p>
<p>zero, similarly to (12.5.17), we have
</p>
<p>arg
(
1 &minus; ϕ(λ)
</p>
<p>)
&isin;
(
&minus;π
</p>
<p>2
,
π
</p>
<p>2
</p>
<p>)
, arg
</p>
<p>λ2 + 1
λ2
</p>
<p>= 0.
</p>
<p>This, analogously to (12.5.18), implies
</p>
<p>indT v
0 := 1
</p>
<p>2π
</p>
<p>&int; T
</p>
<p>&minus;T
d
(
argv0(λ)
</p>
<p>)
&isin; (&minus;b/2, b/2), b &lt; 1.
</p>
<p>2. Represent v0 as v0 = v1v2, where
</p>
<p>v1 :=
(iλ+ 1)n
</p>
<p>Qn
,
</p>
<p>v2 :=
(1 &minus; ϕ)(1 &minus; iλ)Qn
</p>
<p>λ2(iλ+ 1)n&minus;1 =
(Qn &minus; Pm &minus;Qnϕ&minus;)(1 &minus; iλ)
</p>
<p>λ2(iλ+ 1)n&minus;1 .
(12.5.26)
</p>
<p>Then, similarly to (12.5.20), we find that
</p>
<p>indT v1 &rarr; n as T &rarr;&infin;, |n+ indT v2|&lt;
1
</p>
<p>2
.
</p>
<p>3. We show that 1 &minus; ϕ(λ) has exactly n&minus; 1 zeros in Π&minus;. To this end, note that
the function v2, which is analytic in Π&minus; and continuous on Π&minus; &cup;Π has exactly n
zeros in Π&minus;. As in the proof of Theorem 12.5.1, consider the contour T
</p>
<p>&minus;
T . In the
</p>
<p>same way as in the argument in this proof, we obtain that
</p>
<p>1
</p>
<p>2π
</p>
<p>&int;
</p>
<p>T
&minus;
T
</p>
<p>d
(
argv2(λ)
</p>
<p>)
= n,
</p>
<p>so that v2 has exactly n zeros in Π&minus;. Further, by (12.5.26) we have v2 = v3v4,
where the function v3 = (1 &minus; iλ)/(iλ+ 1) has one zero in Π&minus; at the point λ=&minus;i.
Therefore the function
</p>
<p>v4 =
(Qn &minus; Pm &minus;Qnϕ&minus;)
</p>
<p>λ2(iλ+ 1)n&minus;2 ,
</p>
<p>which is analytic in Π&minus;, has n&minus;1 zeros there. Since the zeros of 1&minus;ϕ(λ) and those
of v4(λ) in Π&minus; coincide, the assertion concerning the zeros of 1 &minus; ϕ(λ) is proved.
</p>
<p>4. It remains to put
</p>
<p>w0+(λ) :=
&prod;n&minus;1
</p>
<p>k=1(&micro;k &minus; iλ)(1 &minus; iλ)
Qn(λ)
</p>
<p>, w0&minus;(λ) :=
(1 &minus; ϕ(λ))(iλ+ 1)Qn(λ)
</p>
<p>λ2
&prod;n&minus;1
</p>
<p>k=1(&micro;k &minus; iλ)
and note that w0&plusmn; &isin;K&plusmn; &cap;K.
</p>
<p>Necessity is proved in exactly the same way as in Theorems 12.5.1 and 12.5.2.
The theorem is proved. �
</p>
<p>12.6 Explicit Form of Factorisation in the Arithmetic Case
</p>
<p>The content of this section is similar to that of Sect. 12.5 and has the same structure,
</p>
<p>but there are also some significant differences.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Explicit Form of Factorisation in the Arithmetic Case 365
</p>
<p>12.6.1 Preliminary Remarks on the Uniqueness of Factorisation
</p>
<p>As was already noted in Sect. 12.5, for arithmetic distributions defined by collec-
</p>
<p>tions of probabilities pk = P(ξ = k), we should use, instead of the ch.f.s ϕ(λ), the
generating functions
</p>
<p>p(z)= Ezξ =
&infin;&sum;
</p>
<p>k=&minus;&infin;
zkpk
</p>
<p>defined on the unit circle |z| = 1, which will be denoted by Π , as the axis Imλ= 0
was in Sect. 12.5. The symbols Π+ (Π&minus;) will denote the interior (exterior) of Π .
For arithmetic distributions we will discuss the factorisation
</p>
<p>1 &minus; p(z)= f+(z)f&minus;(z)
on the unit circle, where f&plusmn; are analytic on Π&plusmn; and continuous including the bound-
ary Π . Similarly to the non-lattice case, the classes of such functions, that, more-
</p>
<p>over, are bounded and bounded away from zero on Π&plusmn;, we will denote by K&plusmn;.
Continuous bounded functions on Π , which are also bounded away from zero, form
</p>
<p>the class K. The notion of canonical factorisation on Π is introduced in exactly the
same way as above. Factorisation components must belong to the classes K&plusmn;. The
uniqueness of factorisation components (up to a constant factor) is proved in the
</p>
<p>same way as in Lemma 12.1.1.
</p>
<p>We now show that if, similarly to the above, we &ldquo;tweak&rdquo; the function 1 &minus; p(z)
then it will admit a canonical factorisation. We will denote the tweaked function and
</p>
<p>its factorisation components by the same symbols as in Sect. 12.5. This will not lead
</p>
<p>to any confusion.
</p>
<p>Lemma 12.6.1 1. If Eξ &lt; 0 then the function
</p>
<p>v(z) := (1 &minus; p(z))z
1 &minus; z
</p>
<p>belongs to K and admits a unique canonical factorisation
</p>
<p>v(z)= v+(z)v&minus;(z),
where
</p>
<p>v+(z) := 1 &minus;E
(
zχ+; η+ &lt;&infin;
</p>
<p>)
= 1 &minus; p
</p>
<p>EzS
, p := P(η+ &lt;&infin;),
</p>
<p>v&minus;(z) :=
(1 &minus;Ezχ0&minus;)z
</p>
<p>1 &minus; z .
</p>
<p>2. If Eξ = 0 and Eξ2 &lt;&infin; then the function
</p>
<p>v0(z) := (1 &minus; p(z))z
(1 &minus; z)2
</p>
<p>belongs to K and admits a unique canonical factorisation
</p>
<p>v0(z)= v0+(z)v0&minus;(z),</p>
<p/>
</div>
<div class="page"><p/>
<p>366 12 Random Walks and Factorisation Identities
</p>
<p>where
</p>
<p>v0+ :=
1 &minus;Ezχ+
</p>
<p>1 &minus; z , v
0
&minus; :=
</p>
<p>(1 &minus;Ezχ0&minus;)z
1 &minus; z .
</p>
<p>Here we do not discuss the case Eξ &gt; 0 since it reduces to the case Eξ &lt; 0. We
</p>
<p>will also not present an analogue of Corollary 12.5.1 in view of its obviousness.
</p>
<p>Proof of Lemma 12.6.1 Let Eξ &lt; 0. Since
</p>
<p>(1 &minus; p(z))z
1 &minus; z &rarr;&minus;Eξ &gt; 0
</p>
<p>as z &rarr; 1, p(z) is continuous on the compact Π and, furthermore, |p(z)| &lt; 1 for
z 
= 1, we see that v(z) is bounded away from zero on Π and bounded, and hence
belongs to K. Further, by Corollary 12.2.2 (see (12.2.1) for iλ= z),
</p>
<p>v(z)= (1 &minus;Ez
χ0&minus;)z
</p>
<p>1 &minus; z
[
1 &minus;E
</p>
<p>(
zχ+; η+ &lt;&infin;
</p>
<p>)]
,
</p>
<p>where Eχ0&minus; &isin; (&minus;&infin;,0). Therefore, similarly to the above, we get
</p>
<p>v&minus;(z)=
(1 &minus;Ezχ0&minus;)z
</p>
<p>1 &minus; z &isin;K.
</p>
<p>Moreover, it is obvious that v&minus;(z) &isin;K&minus;. In the same way as above, we obtain that
</p>
<p>v+(z)= 1 &minus;E
(
zχ+; η+ &lt;&infin;
</p>
<p>)
&isin;K+ &cap;K.
</p>
<p>This proves the first assertion of the lemma.
</p>
<p>The second assertion is proved similarly by using Corollary 12.2.4, by which
</p>
<p>v0(z)= 1 &minus;Ez
χ+
</p>
<p>1 &minus; z &middot;
(1 &minus;Ezχ0&minus;)z
</p>
<p>1 &minus; z .
</p>
<p>Next, as before, we establish that v0 &isin;K and that the factors on the right-hand side,
denoted by v0&plusmn;(z), belong to K&plusmn; &cap;K. The lemma is proved. �
</p>
<p>12.6.2 The Classes of Distributions on the Positive Half-Line with
</p>
<p>Rational Generating Functions
</p>
<p>The content of Sect. 12.5.2 is mostly preserved here. Now by exponential polyno-
</p>
<p>mials we mean the sequences
</p>
<p>px =
K&sum;
</p>
<p>k=1
</p>
<p>lk&sum;
</p>
<p>j=1
akjx
</p>
<p>j&minus;1qxk , x = 1,2, . . . , (12.6.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Explicit Form of Factorisation in the Arithmetic Case 367
</p>
<p>where qk &lt; 1 are different (cf. (12.5.10)). To probabilities px of such type will
</p>
<p>correspond rational functions
</p>
<p>p+(z)= E
(
zξ ; ξ &gt; 0
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>x=1
zxpx =
</p>
<p>Pm(z)
</p>
<p>Qn(z)
,
</p>
<p>where 1 &le;m&lt; n, n=
&sum;K
</p>
<p>k=1 lk , and, for definiteness, we put
</p>
<p>Qn(z)=
K&prod;
</p>
<p>k=1
(1 &minus; qkz)lk . (12.6.2)
</p>
<p>Here a significant difference from the non-lattice case is that, for p+(z) to be
rational, we do not need (12.6.1) to be valid for all x &gt; 0. It is sufficient that (12.6.1)
holds for all x, starting from some r+1 &ge; 1. The first r probabilities p1, . . . , pr can
be arbitrary. In this case p+(z) will have the form
</p>
<p>p+(z)= Pm(z)
Qn(z)
</p>
<p>+ Tr(z)=
PM(z)
</p>
<p>Qn(z)
, (12.6.3)
</p>
<p>where Tr is a polynomial of degree r (for r = 0 we put T0 = 0), so that p+ is again
a rational function, but now the degree of the polynomial PM
</p>
<p>M =
{
m, if r = 0,
n+ r, if r &ge; 1
</p>
<p>(12.6.4)
</p>
<p>in the numerator can be greater than the degree n of the polynomial in the denom-
</p>
<p>inator. In what follows, we only assume that n+ r &gt; 0, so that the value n = 0 is
allowed (in this case there will be no exponential part in (12.6.1)). In that case we
will assume that Q0 = 1 and Pm = 0. The distributions corresponding to (12.6.3)
will also be called exponential polynomials.
</p>
<p>12.6.3 Explicit Canonical Factorisation of the Function v(z) in
</p>
<p>the Case when the Right Tail of the Distribution F Is an
</p>
<p>Exponential Polynomial
</p>
<p>Consider an arithmetic distribution F on the whole real line (&minus;&infin;,&infin;), Eξ &lt; 0,
which is an exponential polynomial on the half-line x &gt; 0. As before, denote the
</p>
<p>class of all such distributions by EP. The generating function p(z) of the distribution
</p>
<p>F &isin; EP can be represented as
</p>
<p>p(z)= p+(z)+ p&minus;(z),
</p>
<p>where the function
</p>
<p>p&minus;(z)= E
(
zξ ; ξ &le; 0
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>368 12 Random Walks and Factorisation Identities
</p>
<p>is analytic in Π&minus; and continuous including the boundary Π , and p+(z) is a rational
function
</p>
<p>p+(z)= E
(
zξ ; ξ &gt; 0
</p>
<p>)
= PM (z)
</p>
<p>Qn(z)
</p>
<p>analytic in Π+.
As above, in this case the canonical factorisation of the function
</p>
<p>v(z)= (1 &minus; p(z))z
1 &minus; z
</p>
<p>can be found in explicit form in terms of the zeros of the function 1 &minus; p(z).
</p>
<p>Theorem 12.6.1 Let there exist Eξ &lt; 0. For the positive component w+(z) of the
canonical factorisation
</p>
<p>v(z)=w+(z)w&minus;(z), w&plusmn; &isin;K&plusmn; &cap;K,
to be a rational function it is necessary and sufficient that p+(z)= E(zξ ; ξ &gt; 0) is
a rational function.
</p>
<p>If p+ = PM/Qn, where M is defined in (12.6.4), is an uncancellable ratio of
polynomials then the function 1&minus;p(z) has inΠ&minus; exactly n+ r zeros, which will be
denoted by z1, . . . , zn+r , and
</p>
<p>w+(z)=
&prod;n+r
</p>
<p>k=1(zk &minus; z)
Qn(z)
</p>
<p>,
</p>
<p>where Qn(zk) 
= 0.
If we arrange the zeros {zk} according to the values of |zk| in ascending order,
</p>
<p>then the point z1 &gt; 1 is a simple real zero.
</p>
<p>The theorem implies that
</p>
<p>w&minus;(z)=
(1 &minus; p(z))zQn(z)
</p>
<p>(1 &minus; z)
&prod;n+r
</p>
<p>k=1(zk &minus; z)
.
</p>
<p>By Lemma 12.6.1, from Theorem 12.6.1 we obtain the following representation.
</p>
<p>Corollary 12.6.1 If Eξ &lt; 0 and p+ = PM/Qn then
</p>
<p>EzS = w+(1)
w+(z)
</p>
<p>= Qn(z)
&prod;n+r
</p>
<p>k=1(zk &minus; 1)
Qn(1)
</p>
<p>&prod;n+r
k=1(zk &minus; z)
</p>
<p>.
</p>
<p>Similarly to (12.5.16), we can also write down the explicit form of Ezχ
0
&minus; and
</p>
<p>E(zχ+; η+ &lt;&infin;) as well.
</p>
<p>Proof of Theorem 12.6.1 The proof is similar to that of Theorem 12.5.1.
Sufficiency.
1. In the vicinity of the point z= 1 in Π the value of &minus;v(z) lies in the vicinity of
</p>
<p>the point &minus;Eξ &gt; 0. Outside a neighbourhood of the point z= 1 we have for z &isin;Π ,</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Explicit Form of Factorisation in the Arithmetic Case 369
</p>
<p>arg
(
1 &minus; p(z)
</p>
<p>)
&isin;
(
&minus;π
</p>
<p>2
,
π
</p>
<p>2
</p>
<p>)
, arg
</p>
<p>(
z
</p>
<p>z&minus; 1
</p>
<p>)
=&minus; arg
</p>
<p>(
1 &minus; 1
</p>
<p>z
</p>
<p>)
&isin;
(
&minus;π
</p>
<p>2
,
π
</p>
<p>2
</p>
<p>)
.
</p>
<p>This implies that, for z &isin;Π ,
arg
</p>
<p>(
&minus;v(z)
</p>
<p>)
&isin; (&minus;π,π),
</p>
<p>and hence the trajectory of &minus;v(z), z &isin;Π , never intersects the ray argv=&minus;π ,
</p>
<p>ind v := 1
2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>d
(
argv
</p>
<p>(
eiλ
</p>
<p>))
= 0.
</p>
<p>2. Represent the function v as v= v1v2, where
</p>
<p>v1(z) :=
zn+r
</p>
<p>Qn(z)
, v2(z) :=
</p>
<p>Qn(z)&minus; PM(z)&minus; p&minus;(z)Qn(z)
(1 &minus; z)zn+r&minus;1 .
</p>
<p>We show that
</p>
<p>ind v2 =&minus;n&minus; r. (12.6.5)
In order to do this, we first note that the function v1 is analytic in Π+ and has there
a zero of multiplicity n+ r . Hence by the argument principle ind v1 = n+ r . Since
0 = ind v= ind v1 + ind v2, we obtain the desired relation.
</p>
<p>3. We show that 1 &minus; p(z) has exactly n+ r zeros in Π&minus;. The function v2(z) is
analytic on Π&minus; and continuous including the boundary Π . The positively oriented
contour Π , which contains Π+, corresponds to the negatively oriented contour with
respect to Π&minus;. By (12.6.5) this means that v2(z) has precisely n+ r zeros on Π&minus;
while the point z=&infin; is not a zero since the numerator and the denominator of v(z)
grow as |z|n+r as |z| &rarr;&infin;.
</p>
<p>4. Denote the zeros of v2 by z1, . . . , zn+r and put
</p>
<p>w+(z) :=
&prod;n+r
</p>
<p>k=1(zk &minus; z)
Qn(z)
</p>
<p>, w&minus;(z) :=
Qn(z)(1 &minus; p(z))z
</p>
<p>(1 &minus; z)
&prod;n+r
</p>
<p>k=1(zk &minus; z)
.
</p>
<p>It is easy to see that w&plusmn; &isin;K&plusmn; &cap;K. The fact that Qn(zk) 
= 0 and z1 is a simple real
zero of 1 &minus; p(z) is proved in the same way as in Theorem 12.5.1.
</p>
<p>Necessity is also established in the same fashion as in Theorem 12.5.1. The the-
orem is proved. �
</p>
<p>Clearly, in the arithmetic case we have complete analogues of Examples 12.5.1
</p>
<p>and 12.5.2. In particular, if
</p>
<p>P(ξ = k)= cqk&minus;1, c &lt; (1 &minus; q), k = 1,2, . . . ,
then
</p>
<p>w+(z)=
z1 &minus; z
1 &minus; qz , Ez
</p>
<p>S = (1 &minus; qz)(z1 &minus; 1)
(z1 &minus; z)(1 &minus; q)
</p>
<p>,
</p>
<p>P(S = 0)=
1 &minus; z&minus;11
1 &minus; q , P(S = k)=
</p>
<p>(z&minus;11 &minus; q)(z1 &minus; 1)zk1
1 &minus; q , k &ge; 1.
</p>
<p>In contrast to Sect. 12.5, here one can give another example where the distribution
</p>
<p>of S is geometric.</p>
<p/>
</div>
<div class="page"><p/>
<p>370 12 Random Walks and Factorisation Identities
</p>
<p>Example 12.6.1 Let P(ξ = 1) = p1 &gt; 0 and P(ξ &ge; 2) = 0. In this case χ+ &equiv; 1 on
the set {η+ &lt; &infin;}, and to find the distribution of S there is no need to use Theo-
rem 12.6.1. Indeed, P(S = 0)= 1&minus;p = P(η+ =&infin;). If η+ &lt;&infin; then the trajectory
ξη++1, ξη+=2, . . . is distributed identically to ξ1, ξ2, . . . and hence
</p>
<p>S =
{
</p>
<p>0 with probability 1 &minus; p,
χ+ + S(1) with probability p,
</p>
<p>where the variable S(1) is distributed identically to S, χ+ &equiv; 1. This yields
</p>
<p>EzS = (1 &minus; p)+ pzEzS, EzS = 1 &minus; p
1 &minus; pz,
</p>
<p>P(S = k)= (1 &minus; p)pk, k = 0,1, . . .
By virtue of identity (12.3.3) (for eiλ = z) the point z1 = p&minus;1 is necessarily a zero
of the function 1 &minus; p(z).
</p>
<p>12.6.4 Explicit Canonical Factorisation of the Function v(z) when
</p>
<p>the Left Tail of the Distribution F Is an Exponential
</p>
<p>Polynomial
</p>
<p>We now consider the case where the distribution F on the negative half-line can be
represented as an exponential polynomial, up to the values of P(ξ =&minus;k) at finitely
many points 0,&minus;1,&minus;2, . . . ,&minus;r . In this case, the value of p&minus;(z) is derived similarly
to that of p+(z) in (12.6.3) by replacing z with z&minus;1:
</p>
<p>p&minus;(z)= E
(
zξ ; ξ &lt; 0
</p>
<p>)
= z
</p>
<p>n&minus;MPM (z)
</p>
<p>Qn(z)
,
</p>
<p>where Qn and PM are polynomials (which differ from (12.6.3)),
</p>
<p>M =
{
m, if r = 0,
n+ r, if r &ge; 1,
</p>
<p>Qn(z)=
K&prod;
</p>
<p>k=1
(z&minus; qk)lk ,
</p>
<p>and all qk &lt; 1 are distinct.
</p>
<p>Theorem 12.6.2 Let there exist Eξ &lt; 0. For the positive component of the canonical
factorisation
</p>
<p>v(z)=w+(z)w&minus;(z)
to be representable as
</p>
<p>w+(z)=
(
1 &minus; p(z)
</p>
<p>)
R(z),
</p>
<p>where R(z) is a rational function, it is necessary and sufficient that p&minus;(z) is a
rational function. If
</p>
<p>p&minus;(z)= z
n&minus;MPM(z)
</p>
<p>Qn(z)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Explicit Form of Factorisation in the Arithmetic Case 371
</p>
<p>where PM and Qn are defined in (12.6.2) and (12.6.3), then the function 1 &minus; p(z)
has in Π+ exactly n+ r &minus; 1 zeros that we denote by z1, . . . , zn+r&minus;1, and
</p>
<p>R(z) := Qn(z)
(1 &minus; z)
</p>
<p>&prod;n+r&minus;1
k=1 (z&minus; zk)
</p>
<p>.
</p>
<p>Proof The proof is very close to that of Theorems 12.5.2 and 12.6.1. Therefore we
will only present a brief proof of sufficiency.
</p>
<p>1. As in Theorem 12.6.1, one can verify that
</p>
<p>ind v= 0.
2. Represent v(z) as v= v1v2, where
</p>
<p>v1 :=
(Qn(z)&minus; zn&minus;MPM(z)&minus; p+(z)Qn(z))zr
</p>
<p>(1 &minus; z) , v2(z) :=
z1&minus;r
</p>
<p>Qn(z)
.
</p>
<p>The function v2 is analytic in Π&minus;, continuous including the boundary Π , and has a
zero at z=&infin; of multiplicity n+ r &minus; 1, so that
</p>
<p>ind v2 = n+ r &minus; 1.
The function v1 is analytic in Π+ and, by the argument principle, has there n+ r&minus;1
zeros z1, . . . , zn+r&minus;1. The function 1 &minus; p(z) has the same zeros.
</p>
<p>3. By putting
</p>
<p>w+(z) :=
(1 &minus; p(z))Qn(z)
</p>
<p>(1 &minus; z)
&prod;n+r&minus;1
</p>
<p>k=1 (z&minus; zk)
, w&minus;(z) :=
</p>
<p>z
&prod;n+r&minus;1
</p>
<p>k=1 (z&minus; zk)
Qn(z)
</p>
<p>we obtain w&plusmn; &isin;K&plusmn; &cap;K. The theorem is proved. �
</p>
<p>12.6.5 Explicit Factorisation of the Function v0(z)
</p>
<p>By virtue of the remarks at the beginning of Sect. 12.5.5 it is sufficient to consider
</p>
<p>factorisation of the function
</p>
<p>v0(z) := (1 &minus; p(z))z
(1 &minus; z)2
</p>
<p>for Eξ = 0 and Eξ2 &lt;&infin; just in the case when the function
</p>
<p>p+(z)= E
(
zξ ; ξ &gt; 0
</p>
<p>)
= PM (z)
</p>
<p>Qn(z)
</p>
<p>is rational, where Qn(z)=
&prod;K
</p>
<p>k=1(1 &minus; qkz)lk , n=
&sum;K
</p>
<p>k=1 lk (see (12.6.2), (12.6.3)).
</p>
<p>Theorem 12.6.3 Let Eξ = 0 and Eξ2 = σ 2 &lt; &infin;. For the positive component
w0+(z) of the canonical factorisation
</p>
<p>v0(z)=w0+(z)w0&minus;(z), w&plusmn; &isin;K&plusmn; &cap;K,</p>
<p/>
</div>
<div class="page"><p/>
<p>372 12 Random Walks and Factorisation Identities
</p>
<p>to be rational, it is necessary and sufficient that the function p+(z) is rational. If
p+(z)= PM(z)/Qn(z), whereM is defined in (12.6.4), is an uncancellable ratio of
polynomials then the function 1 &minus; p(z) has in Π&minus; exactly n+ r &minus; 1 zeros that we
denote by z1, . . . , zn+r&minus;1, and
</p>
<p>w0+(z)=
&prod;n+r&minus;1
</p>
<p>k=1 (zk &minus; z)
Qn(z)
</p>
<p>, w0&minus;(z)=
(1 &minus; p(z))zQn(z)
</p>
<p>(1 &minus; z)2
&prod;n+r&minus;1
</p>
<p>k=1 (zk &minus; z)
.
</p>
<p>Corollaries similar to Corollary 12.5.4 hold true here as well.
</p>
<p>Proof of Theorem 12.6.3 The proof is similar to those of Theorems 12.5.3, 12.6.1
and 12.6.2. Therefore, as in the previous theorem, we restrict ourselves to the key
</p>
<p>elements of the proof of sufficiency.
</p>
<p>1. In the vicinity of the point z= 1 in Π , the value of &minus;v0(z) lies in the vicinity
of the point σ 2/2 &gt; 0. Outside of a neighbourhood of the point z= 1, for z &isin;Π we
have
</p>
<p>arg
(
1 &minus; p(z)
</p>
<p>)
&isin;
(
&minus;π
</p>
<p>2
,
π
</p>
<p>2
</p>
<p>)
,
</p>
<p>arg
&minus;z
</p>
<p>(1 &minus; z)2 =&minus; arg
(
(1 &minus; z)
</p>
<p>(
1 &minus; 1
</p>
<p>z
</p>
<p>))
=&minus; arg
</p>
<p>(
2 &minus; z&minus; 1
</p>
<p>z
</p>
<p>)
= 0.
</p>
<p>Hence
</p>
<p>ind v0 := 1
2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>d
(
argv0
</p>
<p>(
eiλ
</p>
<p>))
= 0.
</p>
<p>2. Represent the function v0(z) as
</p>
<p>v0(z)= v1(z)v2(z),
where
</p>
<p>v1(z) :=
zn+r&minus;1
</p>
<p>Qn(z)
, v2(z) :=
</p>
<p>Qn &minus; PM &minus; p&minus;(z)Qn
(1 &minus; z)2zn+r&minus;2 .
</p>
<p>As before, we show that indv1 = n+ r &minus; 1 and that 1 &minus; p(z) has, on Π&minus;, exactly
n+ r &minus; 1 zeros, which are denoted by z1, . . . , zn+r&minus;1. It remains to put
</p>
<p>w0+(z)=
&prod;n+r&minus;1
</p>
<p>k=1 (zk &minus; z)
Qn(z)
</p>
<p>, w0&minus;(z)=
Qn(z)(1 &minus; p(z))z
</p>
<p>(1 &minus; z)2
&prod;n+r&minus;1
</p>
<p>k=1 (zk &minus; z)
.
</p>
<p>The theorem is proved. �
</p>
<p>12.7 Asymptotic Properties of the Distributions of χ&plusmn; and S
</p>
<p>We saw in the previous sections that one can find the distributions of the variables S
</p>
<p>and χ&plusmn; in explicit form only in some special cases. Meanwhile, in applied problems</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Asymptotic Properties of the Distributions of χ&plusmn; and S 373
</p>
<p>of, say, risk theory (see Sect. 12.4) one is interested in the values of P(S &gt; x) for
</p>
<p>large x (corresponding to small ruin probabilities). In this connection there arises
</p>
<p>the problem on the asymptotic behaviour of P(S &gt; x) as x &rarr;&infin;, as well as related
problems on the asymptotics of P(|χ&plusmn;|&gt; x). It turns out that these problems can be
solved under rather broad conditions.
</p>
<p>12.7.1 The Asymptotics of P(χ+ &gt; x |η+ &lt;&infin;) and P(χ
0
&minus; &lt;&minus;x)
</p>
<p>in the Case Eξ &le; 0
</p>
<p>We introduce some classes of functions that will be used below.
</p>
<p>Definition 12.7.1 A function G(t) is called (asymptotically) locally constant (l.c.)
if, for any fixed v,
</p>
<p>G(t + v)
G(t)
</p>
<p>&rarr; 1 as t &rarr;&infin;. (12.7.1)
</p>
<p>It is not hard to see that, say, the functions G(t)= tα[ln(1 + t)]γ , t &gt; 0, are l.c.
We denote the class of all l.c. functions by L. The properties of functions from L
</p>
<p>are studied in Appendix 6. In particular, it is established that (12.7.1) holds uni-
</p>
<p>formly in v on any fixed segment, and that G(t) = eo(t) and G(t) = o(GI (t)) as
t &rarr;&infin;, where
</p>
<p>GI (t) :=
&int; &infin;
</p>
<p>t
</p>
<p>G(u)du. (12.7.2)
</p>
<p>Denote by E the class of distributions satisfying the right-hand side Cram&eacute;r con-
</p>
<p>dition (the exponential class). The class E&lowast; &sub; E of distributions G whose &ldquo;tails&rdquo;
G(t)=G((t,&infin;)) satisfy, for any fixed v &gt; 0, the relation
</p>
<p>G(t + v)
G(t)
</p>
<p>&rarr; 0 as t &rarr;&infin;, (12.7.3)
</p>
<p>could be called the &ldquo;superexponential&rdquo; class. For example, the normal distribution
</p>
<p>belongs to E&lowast;. In the arithmetic case, one has to put v = 1 in (12.7.3) and consider
integer-valued t .
</p>
<p>In the case Eξ &le; 0 it is convenient to introduce a random variable χ with the
distribution
</p>
<p>P(χ &isin; dv)= P(χ+ &isin; dv |η+ &lt;&infin;)=
P(χ+ &isin; dv; η+ &lt;&infin;)
</p>
<p>p
, p = P(η+ &lt;&infin;).
</p>
<p>If Eξ = 0 then the distributions of χ and χ+ coincide. In the sequel we will confine
ourselves to non-lattice ξ (then χ&plusmn; will also be non-lattice). In the arithmetic case
everything will look quite similar.
</p>
<p>Denote by F+(t) the right &ldquo;tail&rdquo; of the distribution F: F+(t) := F((t,&infin;)) and
put
</p>
<p>F I+(t) :=
&int; &infin;
</p>
<p>t
</p>
<p>F+(u) du.</p>
<p/>
</div>
<div class="page"><p/>
<p>374 12 Random Walks and Factorisation Identities
</p>
<p>Theorem 12.7.1 Let there exist Eξ &le; 0 and, in the case E ξ = 0, assume Eξ2 &lt;&infin;
holds.
</p>
<p>1. If F+(t)= o(F I+(t)) as t &rarr;&infin; then, as x &rarr;&infin;,
</p>
<p>P(χ &gt; x)&sim;&minus;
F I+(x)
</p>
<p>pEχ0&minus;
. (12.7.4)
</p>
<p>2. If F+(t)= V (t)e&minus;βt , β &gt; 0, V &isin;L then
</p>
<p>P(χ &gt; x)&sim; F+(x)
p(1 &minus;Eeβχ0&minus;)
</p>
<p>. (12.7.5)
</p>
<p>3. If F+ &isin; E&lowast; then
</p>
<p>P(χ &gt; x)&sim; F+(x)
pP(χ0&minus; &lt; 0)
</p>
<p>. (12.7.6)
</p>
<p>Proof The proof is based on identity (12.2.1) of Corollary 12.2.2, which can be
rewritten as
</p>
<p>1 &minus; pEeiλχ = 1 &minus; ϕ(λ)
1 &minus; ϕ0&minus;(λ)
</p>
<p>, ϕ0&minus;(λ) := Eeiλχ
0
&minus; . (12.7.7)
</p>
<p>Introduce the renewal function H&minus;(t) corresponding to the random variable χ0&minus; &le; 0:
</p>
<p>H&minus;(t)=
&infin;&sum;
</p>
<p>k=0
P(Hk &ge; t), Hk = χ (1)&minus; + &middot; &middot; &middot; + χ
</p>
<p>(k)
&minus; ,
</p>
<p>where χ
(k)
&minus; are independent copies of χ
</p>
<p>0
&minus;, a&minus; := Eχ0&minus; &gt; &minus;&infin;. As was noted in
</p>
<p>Sect. 10.1, the function 1/(1 &minus; ϕ0&minus;(λ)) can be represented as
1
</p>
<p>1 &minus; ϕ0&minus;(λ)
=&minus;
</p>
<p>&int; 0
</p>
<p>&minus;&infin;
eiλt dH&minus;(t)
</p>
<p>(the function H&minus;(t) decreases). Therefore, for x &gt; 0 and any N &gt; 0, we obtain from
(12.7.7) that
</p>
<p>pP(χ &gt; x)=&minus;
&int; 0
</p>
<p>&minus;&infin;
dH&minus;(t)F+(x &minus; t)=&minus;
</p>
<p>&int; 0
</p>
<p>&minus;N
&minus;
&int; &minus;N
</p>
<p>&minus;&infin;
. (12.7.8)
</p>
<p>Here, by the condition of assertion 1,
</p>
<p>&minus;
&int; 0
</p>
<p>&minus;N
&le; F+(x)
</p>
<p>[
H&minus;(&minus;N)&minus;H&minus;(0)
</p>
<p>]
= o
</p>
<p>(
F I+(x)
</p>
<p>)
as x &rarr;&infin;.
</p>
<p>Evidently, this relation will still be true when N &rarr;&infin; slowly enough as x &rarr;&infin;.
Furthermore, by the local renewal theorem, as N &rarr;&infin;,
</p>
<p>&minus;
&int; &minus;N
</p>
<p>&minus;&infin;
dH&minus;(t)F+(x &minus; t)&sim;
</p>
<p>&int; &minus;N
</p>
<p>&minus;&infin;
F+(x &minus; t)
</p>
<p>dt
</p>
<p>|a&minus;|
=
</p>
<p>F I+(x +N)
|a&minus;|
</p>
<p>. (12.7.9)
</p>
<p>For a formal justification of this relation, the interval (&minus;&infin;,&minus;N ] should be divided
into small intervals (&minus;Nk+1,&minus;Nk], k = 0,1, . . . , N0 = N , Nk+1 &gt;Nk , on each of</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Asymptotic Properties of the Distributions of χ&plusmn; and S 375
</p>
<p>which we use the local renewal theorem, so that
</p>
<p>F+(x &minus;Nk)(Nk+1 &minus;Nk)
|a&minus;|
</p>
<p>(
1 + o(1)
</p>
<p>)
&le;&minus;
</p>
<p>&int; &minus;Nk
&minus;Nk+1
</p>
<p>dH&minus;(t)F+(x &minus; t)
</p>
<p>&le; F+(x &minus;Nk+1)(Nk+1 &minus;Nk)|a&minus;|
(
1 + o(1)
</p>
<p>)
.
</p>
<p>From here it is not difficult to obtain the required bounds for the left-hand side
</p>
<p>of (12.7.9) that are asymptotically equivalent to the right-hand side. Since, for N
</p>
<p>growing slowly enough,
</p>
<p>F I+(x)&minus; F I+(x +N)=
&int; x+N
</p>
<p>x
</p>
<p>F+(u) du &lt; F+(x)N = o
(
F I+(x)
</p>
<p>)
</p>
<p>one has F I+(x +N)&sim; F I+(x), and we finally obtain the relation
</p>
<p>pP(χ &gt; x)&sim;
F I+(x +N)
</p>
<p>|a&minus;|
.
</p>
<p>This proves (12.7.4).
</p>
<p>If F+(t)= V (t)e&minus;βt , V &isin;L, then we find from (12.7.8) that
</p>
<p>pP(χ &gt; x)&sim;&minus;V (x)e&minus;βx
&int; 0
</p>
<p>&minus;&infin;
dH&minus;(t) e
</p>
<p>tβ = F+(x)
1 &minus;Eeβχ0&minus;
</p>
<p>.
</p>
<p>This proves (12.7.5).
</p>
<p>Now let F+ &isin; E&lowast;. If we denote by h0 &gt; 0 the jump of the function H&minus;(t) at the
point 0 then, clearly,
</p>
<p>&minus;
&int; 0
</p>
<p>&minus;&infin;
dH&minus;(t)
</p>
<p>F+(x &minus; t)
F+(x)
</p>
<p>&rarr; h0 as x &rarr;&infin;,
</p>
<p>and hence
</p>
<p>pP(χ &gt; x)&sim; F+(x)h0.
If we put q := P(χ0&minus; = 0) then h0, being the average time spent by the random
walk {Hk} at the point 0, equals
</p>
<p>h0 =
&infin;&sum;
</p>
<p>k=0
qk = 1
</p>
<p>1 &minus; q .
</p>
<p>The theorem is proved. �
</p>
<p>Now consider the asymptotics of P(χ0&minus; &lt;&minus;x) as x &rarr;&infin;.
Put F&minus;(t) := F((&minus;&infin;,&minus;t))= P(ξ &lt;&minus;t).
</p>
<p>Theorem 12.7.2 Let Eξ &lt; 0.
</p>
<p>1. If F&minus; &isin;L then, as x &rarr;&infin;,
</p>
<p>P
(
χ0&minus; &lt;&minus;x
</p>
<p>)
&sim; F&minus;(x)
</p>
<p>1 &minus; p .</p>
<p/>
</div>
<div class="page"><p/>
<p>376 12 Random Walks and Factorisation Identities
</p>
<p>2. If F&minus;(t)= e&minus;γ tV (t), V (t) &isin;L, then
</p>
<p>P
(
χ0&minus; &lt;&minus;x
</p>
<p>)
&sim; Ee
</p>
<p>&minus;γ SF&minus;(x)
</p>
<p>1 &minus; p .
</p>
<p>3. If F&minus; &isin; E&lowast; then
</p>
<p>P
(
χ0&minus; &lt;&minus;x
</p>
<p>)
&sim; F&minus;(x)P(S = 0)
</p>
<p>1 &minus; p .
</p>
<p>Proof Making use of identity (12.3.3):
</p>
<p>1 &minus; ϕ0&minus;(λ)=
(1 &minus; ϕ(λ))E eiλS
</p>
<p>1 &minus; p , ϕ
0
&minus;(λ)= Eeiλχ
</p>
<p>0
&minus; .
</p>
<p>This implies that P(χ0&minus; &lt;&minus;x) is the weighted mean of the value F&minus;(x+ t) with the
weight function P(S &isin; dt)/(1 &minus; p):
</p>
<p>P
(
χ0&minus; &lt;&minus;x
</p>
<p>)
= 1
</p>
<p>1 &minus; p
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P(S &isin; dt)F&minus;(x + t).
</p>
<p>From here the assertions of the theorem follow in an obvious way. �
</p>
<p>If Eξ = 0 then the asymptotics of P(χ0&minus; &lt;&minus;x) will be different.
</p>
<p>12.7.2 The Asymptotics of P(S &gt; x)
</p>
<p>We will study the asymptotics of P(S &gt; x) in the two non-overlapping and mutually
</p>
<p>complementary cases where F+ &isin; E (the Cram&eacute;r condition holds) and where F I+
belongs to the class S of subexponential functions.
</p>
<p>Definition 12.7.2 A distribution G on [0,&infin;) with the tail G(t) := G([t,&infin;)) be-
longs to the class S+ of subexponential distributions on the positive half-line if
</p>
<p>G2&lowast;(t)&sim; 2G(t) as t &rarr;&infin;. (12.7.10)
A distribution G on the whole real line belongs to the class S of subexponential
distributions if the distribution G+ of the positive part ζ+ = max{0, ζ } of a random
variable ζ &sub;= G belongs to S+. A random variable is called subexponential if its
distribution is subexponential.
</p>
<p>As we will see later (see Theorem A6.4.3 in Appendix 6), the subexponentiality
</p>
<p>distribution G is in essence a property of the asymptotics of the tail of G(t) as
t &rarr;&infin;. Therefore we can also talk about subexponential functions. A nonincreasing
function G1(t) on (0,&infin;) is called subexponential if the distribution G with a tail
G(t) such that G(t) &sim; cG1(t) as t &rarr; &infin; for some c &gt; 0 is subexponential. (For
example, distributions with tails G1(t)/G1(0) or min(1,G1(t)) if G1(0) &gt; 1.)
</p>
<p>The properties of subexponential distributions are studied in Appendix 6. In par-
</p>
<p>ticular, it is established that S&sub;L, R&sub; S (R is the class of regularly varying func-
tions) and that G(t)= o(GI (t)) if GI &isin; S.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Asymptotic Properties of the Distributions of χ&plusmn; and S 377
</p>
<p>Theorem 12.7.3 If F I+(t) &isin; S and a = Eξ &lt; 0, then, as x &rarr;&infin;,
</p>
<p>P(S &gt; x)&sim; 1|a|F
I
+(x). (12.7.11)
</p>
<p>Proof Making use of the identity from Theorem 12.3.2:
</p>
<p>EeiλS = 1 &minus; p
1 &minus; pϕχ (λ)
</p>
<p>, ϕχ (λ) := Eeiλχ , (12.7.12)
</p>
<p>it follows that
</p>
<p>EeiλS = (1 &minus; p)
&infin;&sum;
</p>
<p>k=0
pkϕkχ (λ),
</p>
<p>and hence, for x &gt; 0,
</p>
<p>P(S &gt; x)= (1 &minus; p)
&infin;&sum;
</p>
<p>k=1
pkP(Hk &gt; x), Hk :=
</p>
<p>k&sum;
</p>
<p>j=1
χj , (12.7.13)
</p>
<p>where χj are independent copies of χ . By assertion 1 of Theorem 12.7.1 the distri-
</p>
<p>bution of χ is subexponential, while by Theorem A6.4.3 of Appendix 6, as x &rarr;&infin;,
for each fixed k one has
</p>
<p>P(Hk &gt; x)&sim; kP(χ &gt; x). (12.7.14)
Moreover, again by Theorem A6.4.3 of Appendix 6, for any ε &gt; 0, there exists a
</p>
<p>b= b(ε) such that, for all x and k &ge; 2,
P(Hk &gt; x)
</p>
<p>P(χ &gt; x)
&lt; b(1 + ε)k.
</p>
<p>Therefore, for (1 + ε)p &lt; 1, the series
&infin;&sum;
</p>
<p>k=1
pk
</p>
<p>P(Hk &gt; x)
</p>
<p>P(χ &gt; x)
</p>
<p>converges uniformly in x. Passing to the limit as x &rarr;&infin;, by virtue of (12.7.14) we
obtain that
</p>
<p>lim
x&rarr;&infin;
</p>
<p>P(S &gt; x)
</p>
<p>P(χ &gt; x)
= (1 &minus; p)
</p>
<p>&infin;&sum;
</p>
<p>k=1
kpk = p
</p>
<p>1 &minus; p
</p>
<p>or, which is the same, that
</p>
<p>P(S &gt; x)&sim; pP(χ &gt; x)
1 &minus; p as x &rarr;&infin;,
</p>
<p>where, by Theorem 12.7.1,
</p>
<p>P(χ &gt; x)&sim;&minus;
F I+(x)
</p>
<p>pEχ0&minus;
.</p>
<p/>
</div>
<div class="page"><p/>
<p>378 12 Random Walks and Factorisation Identities
</p>
<p>Since, by Corollary 12.2.3,
</p>
<p>(1 &minus; p)Eχ0&minus; = Eξ,
we obtain (12.7.11). The theorem is proved. �
</p>
<p>Now consider the case when F satisfies the right-hand side Cram&eacute;r condition
</p>
<p>(F+ &isin; E). For definiteness, we will again assume that the distribution F is non-
lattice. Furthermore, we will assume that there exists an &micro;1 &gt; 0 such that
</p>
<p>ψ(&micro;1) := Ee&micro;1ξ = 1, b := Eξe&micro;1ξ =ψ &prime;(&micro;1) &lt;&infin;. (12.7.15)
In this case the Cram&eacute;r transform of the distribution of F at the point &micro;1 will be of
</p>
<p>the form
</p>
<p>F(&micro;1)(dt)=
e&micro;1tF(dt)
</p>
<p>ψ(&micro;1)
= e&micro;1tF(dt). (12.7.16)
</p>
<p>A random variable ξ(&micro;1) with the distribution F(&micro;1) has, by (12.7.15), a finite expec-
</p>
<p>tation equal to b. Denote the size of the first overshoot of the level x by a random
</p>
<p>walk with jumps ξ(&micro;1) by χ(&micro;1)(x). By Corollary 10.4.1, the distribution of χ(&micro;1)(x)
</p>
<p>converges, as x &rarr;&infin;, to the limiting distribution: χ(&micro;1)(x)&rArr; χ(&micro;1), so that
</p>
<p>Ee&minus;&micro;1χ(&micro;1)(x) &rarr; Ee&minus;&micro;1χ(&micro;1) . (12.7.17)
</p>
<p>Theorem 12.7.4 Let F+ &isin; E and (12.7.15) be satisfied. Then, as x &rarr;&infin;,
P(S &gt; x)&sim; ce&minus;&micro;1x, (12.7.18)
</p>
<p>where c= Ee&minus;&micro;1χ(&micro;1) &lt; 1.
</p>
<p>There is a somewhat different interpretation of the constant c in Remark 15.2.3.
</p>
<p>Exact upper and lower bounds for e&micro;1xP(S &gt; x) are contained in Theorem 15.3.5.
</p>
<p>Note that the finiteness of Eξ &lt; 0 is not assumed in Theorem 12.7.4. In the
</p>
<p>arithmetic case, we have to consider only integer x.
</p>
<p>Proof Put η(x) := min{n&ge; 1 : Sn &gt; x}, Xn := x1 +&middot; &middot; &middot;+xn and Xn := maxk&le;nXk .
Then
</p>
<p>P(S &gt; x)= P
(
η(x) &lt;&infin;
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=1
P
(
η(x)= n
</p>
<p>)
, (12.7.19)
</p>
<p>where
</p>
<p>P
(
η(x)= n
</p>
<p>)
=
&int;
</p>
<p>. . .
</p>
<p>&int;
</p>
<p>︸ ︷︷ ︸
n
</p>
<p>F(dx1) . . .F(dxn) I(Xn&minus;1 &le; x,Xn &gt; x)
</p>
<p>=
&int;
</p>
<p>. . .
</p>
<p>&int;
</p>
<p>︸ ︷︷ ︸
n
</p>
<p>F(&micro;1)(dx1) . . .F(&micro;1)(dxn)e
&minus;&micro;1Xn I(Xn&minus;1 &le; x,Xn &gt; x)
</p>
<p>= E(&micro;1)e&minus;&micro;1Sn I
(
η(x)= n
</p>
<p>)
. (12.7.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Asymptotic Properties of the Distributions of χ&plusmn; and S 379
</p>
<p>Here E(&micro;1) denotes the expectation when taken assuming that the distribution of the
</p>
<p>summands ξi is F(&micro;1). By the convexity of the function ψ(&micro;)= Ee&micro;ξ ,
</p>
<p>E(&micro;1)ξ =
&int;
</p>
<p>xe&micro;1xF(dx)=ψ &prime;(&micro;1)= b &gt; 0,
</p>
<p>and hence
</p>
<p>P(&micro;1)
(
η(x) &lt;&infin;
</p>
<p>)
= 1.
</p>
<p>Therefore, returning to (12.7.19), we obtain
</p>
<p>P(S &gt; x)= E(&micro;1)
&infin;&sum;
</p>
<p>k=1
e&minus;&micro;1Sn I
</p>
<p>(
η(x)= n
</p>
<p>)
= E(&micro;1)e&minus;&micro;1Sη(x) , (12.7.21)
</p>
<p>where Sη(x) = x + χ(&micro;1)(x) and, by (12.7.17),
</p>
<p>e&micro;1xP(S &gt; x)&rarr; c= Ee&minus;&micro;1χ(&micro;1) &lt; 1.
</p>
<p>This proves (12.7.18). For arithmetic ξ the proof is the same. We only have to re-
</p>
<p>place F(dt) in (12.7.15) and (12.7.16) by pk = P(ξ = k), as well as integration by
summation. The theorem is proved. �
</p>
<p>Corollary 12.7.1 If, in the arithmetic case, Eξ &lt; 0, p1 = P(ξ = 1) &gt; 0, P(ξ &ge; 2)=
0 then the conditions of Theorem 12.7.4 are satisfied and one has
</p>
<p>P(S &gt; x)= e&minus;&micro;1(k+1), k &ge; 0.
</p>
<p>Proof The proof follows immediately from (12.7.21) if we note that, in the case
under consideration, χ(&micro;1)(x)&equiv; 1 and Sη(x) = x+1. This assertion repeats the result
of Example 12.6.1. �
</p>
<p>Remark 12.7.1 The asymptotics (12.7.18), obtained by a probabilistic argument,
admits a simple analytic interpretation. From (12.7.18) it follows that, as &micro; &uarr; &micro;1,
we have
</p>
<p>Ee&micro;S &sim; c&micro;1
&micro;1 &minus;&micro;
</p>
<p>.
</p>
<p>But that Ee&micro;S has precisely this form follows from identity (12.3.3):
</p>
<p>Ee&micro;S = (1 &minus; p)(1 &minus;Ee
&micro;χ0&minus;)
</p>
<p>1 &minus;ψ(&micro;) .
</p>
<p>Indeed, since, by assumption, ψ(&micro;)= Ee&micro;ξ is left-differentiable at the point &micro;1 and
</p>
<p>ψ(&micro;)= 1 &minus; b(&micro;1 &minus;&micro;)+ o
(
(&micro;1 &minus;&micro;)
</p>
<p>)
, (12.7.22)
</p>
<p>one has
</p>
<p>Ee&micro;S &sim; (1 &minus; p)(1 &minus;Ee
&micro;1χ
</p>
<p>0
&minus;)
</p>
<p>b(&micro;1 &minus;&micro;)
(12.7.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>380 12 Random Walks and Factorisation Identities
</p>
<p>as &micro; &uarr; &micro;1. This implies, in particular, yet another representation for the constant c
in (12.7.18):
</p>
<p>c= (1 &minus; p)(1 &minus;Ee
&micro;1χ
</p>
<p>0
&minus;)
</p>
<p>b
.
</p>
<p>Since
</p>
<p>Ee&micro;S = w+(0)
w+(λ)
</p>
<p>and w+(λ) has a zero at the point &micro;1, we can obtain representations similar to
(12.7.22) and (12.7.23) in terms of the values of w+(0) and w&prime;(&micro;1).
</p>
<p>We should also note that the proof of asymptotics (12.7.18) with the help of
</p>
<p>relations of the form (12.7.23) is based on certain facts from mathematical analysis
</p>
<p>and is relatively simple only under the additional condition (12.5.1).
</p>
<p>There are other ways to prove (12.7.18), but they also involve additional restric-
</p>
<p>tions. For instance, (12.3.3) implies
</p>
<p>EeiλS = (1 &minus; p)
&infin;&sum;
</p>
<p>k=0
</p>
<p>[
ϕk(λ)&minus; ϕk(λ)E eiλχ0&minus;
</p>
<p>]
,
</p>
<p>P(S &gt; x)= (1 &minus; p)
&infin;&sum;
</p>
<p>k=0
</p>
<p>[
P(Sk &gt; x)&minus; P
</p>
<p>(
Sk + χ0&minus; &gt; x
</p>
<p>)]
</p>
<p>= (1 &minus; p)
&int; &infin;
</p>
<p>0
</p>
<p>P
(
χ0&minus; &isin; dt
</p>
<p>) &infin;&sum;
</p>
<p>k=0
P
(
Sk &isin; (x, x + t]
</p>
<p>)
,
</p>
<p>and the problem now reduces to integro-local theorems for large deviations of Sk
(see Chap. 9) or to local theorems for the renewal function in the region where the
</p>
<p>function converges to zero.
</p>
<p>12.7.3 The Distribution of the Maximal Values of Generalised
</p>
<p>Renewal Processes
</p>
<p>Let {(τi, ζi)}&infin;j=1 be a sequence of independent identically distributed random vec-
tors,
</p>
<p>Z(t)= Zν(t),
where
</p>
<p>Zn :=
n&sum;
</p>
<p>j=1
ζj , ν(t) := max{k : Tk &le; t}, Tk :=
</p>
<p>k&sum;
</p>
<p>j=1
τj .
</p>
<p>In Sect. 12.4.3 we reduced the problem of finding the distribution of supt (Z(t)&minus;qt)
to that of the distribution of S := supk&ge;0 Sk , Sk :=
</p>
<p>&sum;k
j=1 ξj , ξj := ζj &minus; qτj in the
</p>
<p>case q &gt; 0, ζk &ge; 0. We show that such a reduction takes place in the general case
as well. If q &ge; 0 and the ζk can take values of both signs, then the reduction is the</p>
<p/>
</div>
<div class="page"><p/>
<p>12.8 On the Distribution of the First Passage Time 381
</p>
<p>same as in Sect. 12.4.3. Now if q &lt; 0 then
</p>
<p>sup
t
(Zν(t) &minus; qt)= sup(&minus;qT1,Z1 &minus; qT2,Z2 &minus; qT3, . . .)
</p>
<p>=&minus;qτ1 + sup
k&ge;1
</p>
<p>[
Zk&minus;1 &minus; q(Tk &minus; τ1)
</p>
<p>] d= S &minus; qτ,
</p>
<p>where the random variables τ1 and S are independent.
</p>
<p>12.8 On the Distribution of the First Passage Time
</p>
<p>12.8.1 The Properties of the Distributions of the Times η&plusmn;
</p>
<p>In this section we will establish a number of relations between the random vari-
</p>
<p>ables η&plusmn; and the time θ when the global maximum S = supSk is attained for the
first time:
</p>
<p>θ := min{k : Sk = S} (if S &lt;&infin; a.s.).
Put
</p>
<p>P(z) :=
&infin;&sum;
</p>
<p>k=0
zkP
</p>
<p>(
η0&minus; &gt; k
</p>
<p>)
, q(z) := E
</p>
<p>(
zη+
</p>
<p>∣∣η+ &lt;&infin;
)
,
</p>
<p>D+ :=
&infin;&sum;
</p>
<p>k=1
</p>
<p>P(Sk &gt; 0)
</p>
<p>k
.
</p>
<p>Further, let η be a random variable with the distribution
</p>
<p>P(η= k)= P(η+ = k | η+ &lt;&infin;)
(and the generating function q(z)), η1, η2, . . . be independent copies of η,
</p>
<p>Hk := η1 + &middot; &middot; &middot; + ηk, H0 = 0,
and ν be a random variable independent of {ηk} with the geometric distribution
P(ν = k)= (1 &minus; p)pk , k &ge; 0.
</p>
<p>Theorem 12.8.1 If p = P(η+ &lt;&infin;) &lt; 1 then
</p>
<p>1. 1 &minus; p = 1
Eη0&minus;
</p>
<p>= e&minus;D+ . (12.8.1)
</p>
<p>2. P(z)= 1
1 &minus; pq(z) =
</p>
<p>Ezθ
</p>
<p>1 &minus; p . (12.8.2)
</p>
<p>3. P
(
η0&minus; &gt; n
</p>
<p>)
= (1 &minus; p)P(Hν = n) &gt; P(η+ = n) (12.8.3)
</p>
<p>for all n&ge; 0.
</p>
<p>Recall that, for the condition p &lt; 1 to hold, it is sufficient that Eξ &lt; 0 (see
</p>
<p>Corollary 12.2.6).</p>
<p/>
</div>
<div class="page"><p/>
<p>382 12 Random Walks and Factorisation Identities
</p>
<p>The second assertion of the theorem implies that the distributions of η0&minus;, η+ and θ
uniquely determine each other, so that if at least one of them is known then, to find
</p>
<p>the other two, it is not necessary to know the original distribution F. In particular,
</p>
<p>P(θ = n)= (1 &minus; p)P(χ0&minus; &gt; n).
</p>
<p>Proof of Theorem 12.8.1 The arguments in this subsection are based on the follow-
ing identities which follow from Theorems 12.1.1&ndash;12.1.3 if we put there λ= 0 and
|z|&lt; 1:
</p>
<p>1 &minus; z=
[
1 &minus;Ezη0&minus;
</p>
<p>][
1 &minus;E
</p>
<p>(
zη+; η+ &lt;&infin;
</p>
<p>)]
, (12.8.4)
</p>
<p>1 &minus;Ezη0&minus; = exp
{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
P(Sk &le; 0)
</p>
<p>}
, (12.8.5)
</p>
<p>1 &minus;E
(
zη+; η+ &lt;&infin;
</p>
<p>)
= exp
</p>
<p>{
&minus;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>zk
</p>
<p>k
P(Sk &gt; 0)
</p>
<p>}
. (12.8.6)
</p>
<p>Since
</p>
<p>1 &minus;Ezη0&minus;
1 &minus; z = P(z), P (1)= Eη
</p>
<p>0
&minus;
</p>
<p>we obtain from (12.8.4) the first equalities in (12.8.1) and (12.8.2). The second
</p>
<p>equality in (12.8.1) follows from (12.8.6).
</p>
<p>To prove the second equality in (12.8.2), we make use of the relation
</p>
<p>θ =
{
</p>
<p>0 on {ω : η+ =&infin;},
η+ + θ&lowast; on {ω : η+ &lt;&infin;},
</p>
<p>where θ&lowast; is distributed on {η+ &lt;&infin;} identically to θ and does not depend on η+. It
follows that
</p>
<p>Ezθ = (1 &minus; p)+EzθE
(
zη+; η+ &lt;&infin;
</p>
<p>)
.
</p>
<p>This implies the second equality in (12.8.2). The last assertion of the theorem fol-
</p>
<p>lows from the first equality in (12.8.2), which implies
</p>
<p>P(z)=
&infin;&sum;
</p>
<p>k=0
pkqk(z)= (1 &minus; p)
</p>
<p>&infin;&sum;
</p>
<p>k=0
P(ν = k)
</p>
<p>&infin;&sum;
</p>
<p>n=0
P(Hk = n)zn
</p>
<p>= (1 &minus; p)
&infin;&sum;
</p>
<p>n=0
znP(Hν = n).
</p>
<p>The theorem is proved. �
</p>
<p>The second equality in (12.8.2) and identity (12.7.12) mean that the representa-
</p>
<p>tions
</p>
<p>θ = η1 + &middot; &middot; &middot; + ην and S = χ1 + &middot; &middot; &middot; + χν,
respectively, hold true, where ν has the geometric distribution P(ν = k) =
(1 &minus; p)pk , k &ge; 0, and does not depend on {ηj }, {χj }.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.8 On the Distribution of the First Passage Time 383
</p>
<p>Note that the probabilities P(Sk &gt; 0) = P(Sk &minus; ak &gt; &minus;ak) on the right-hand
sides of (12.8.5) and (12.8.6) are, for large k and a = Eξ &lt; 0, the probabilities of
large deviations that were studied in Chap. 9. The results of that chapter on the
</p>
<p>asymptotics of these probabilities together with relations (12.8.5) and (12.8.6) give
</p>
<p>us an opportunity to find the asymptotics of P(η+ = n) and P(η0&minus; = n) as n&rarr;&infin;
(see [8]).
</p>
<p>Now consider the case where the both random variables η0&minus; and η+ are proper.
That is always the case if Eξ = 0 (see Corollary 12.2.6). Here identities (12.8.4)&ndash;
(12.8.6) hold true (with P(η+ &lt;&infin;) = 1). As before, (12.8.4) implies that the dis-
tributions of η0&minus; and η+ uniquely determine each other.
</p>
<p>Let η1, η2, . . . be independent copies of η+, Hk = η1 + &middot; &middot; &middot; + ηk and H0 = 0. For
the sums Hk , define the local renewal function
</p>
<p>hn :=
&infin;&sum;
</p>
<p>n=0
P(Hk = n).
</p>
<p>Theorem 12.8.2 If P(η0&minus; &lt;&infin;)= P(η+ &lt;&infin;)= 1 then:
1. Eη0&minus; = Eη+ =&infin;.
2. P(η0&minus; &gt; n)= hn.
</p>
<p>Proof From (12.8.4) it follows that
</p>
<p>P(z)= 1 &minus;Ez
η0&minus;
</p>
<p>1 &minus; z =
1
</p>
<p>1 &minus;Ezη+ &rarr;&infin; (12.8.7)
</p>
<p>as z&rarr; 1. Since P(z)&rarr; Eη0&minus; as z&rarr; 1, we have proved that Eη0&minus; is infinite. That
Eη+ is also infinite is shown in the same way. The second assertion also follows
from (12.8.7) since the right-hand side of (12.8.7) is
</p>
<p>&sum;&infin;
n=0 z
</p>
<p>nhn. The theorem is
</p>
<p>proved. �
</p>
<p>Now we turn to the important class of symmetric distributions. We will say that
</p>
<p>the distribution of a random variable ξ is symmetric if it coincides with the distribu-
tion of &minus;ξ , and will call the distribution of ξ continuous if the distribution function
of ξ is continuous. For such random variables, Eξ = 0 (if Eξ exists), the distribu-
tions of Sn are also symmetric continuous for all n, and
</p>
<p>P(Sn &gt; 0)= P(Sn &lt; 0)=
1
</p>
<p>2
, P(Sn = 0)= 0,
</p>
<p>and hence D(z)&equiv; 1, P(χ0+ = 0)= 0, and η+ = η0+, χ+ = χ0+ with probability 1.
</p>
<p>Theorem 12.8.3 If the distribution of ξ is symmetric and continuous then
</p>
<p>P(η+ = n)= P
(
η0&minus; = n
</p>
<p>)
= (2n)!
</p>
<p>(2n&minus; 1)(n!)222n &sim;
1
</p>
<p>2
&radic;
π n3/2
</p>
<p>,
</p>
<p>P(γn &gt; 0)= P(ζn &lt; 0)&sim;
1&radic;
πn
</p>
<p>(12.8.8)
</p>
<p>as n&rarr;&infin; (γn and ζn are defined in Section 12.1.3).</p>
<p/>
</div>
<div class="page"><p/>
<p>384 12 Random Walks and Factorisation Identities
</p>
<p>Proof Since Ezη
0
&minus; = Ezη+ , by virtue of (12.8.4) one has
</p>
<p>1 &minus;Ezη+ =
&radic;
</p>
<p>1 &minus; z.
Expanding
</p>
<p>&radic;
1 &minus; z into a series, we obtain the second equality in (12.8.8). The
</p>
<p>asymptotic equivalence follows from Stirling&rsquo;s formula.
</p>
<p>The second assertion of the theorem follows from the first one and the equality
</p>
<p>P(ζn &lt; 0)=
&infin;&sum;
</p>
<p>k=n+1
P(η+ = k).
</p>
<p>The assertions concerning η0&minus; and γn follow by symmetry.
The theorem is proved. �
</p>
<p>Note that, under the conditions of Theorem 12.8.3, the distributions of the vari-
</p>
<p>ables η+, η&minus;, γn, ζn do not depend on the distribution of ξ . Also note that the
asymptotics
</p>
<p>P(η+ = n)&sim;
1
</p>
<p>2
&radic;
π n3/2
</p>
<p>persists in the case of non-symmetric distributions as well provided that Eξ = 0 and
Eξ2 &lt;&infin; (see [8]).
</p>
<p>12.8.2 The Distribution of the First Passage Time of an Arbitrary
</p>
<p>Level x by Arithmetic Skip-Free Walks
</p>
<p>The main object in this section is the time
</p>
<p>η(x)= min{k : Sk &ge; x}
of the first passage of the level x by the random walk {Sk}. Below we will consider
the class of arithmetic random walks for which χ+ &equiv; 1.
</p>
<p>By an arithmetic skip-free walk we will call a sequence {Sk}&infin;k=0, where the dis-
tribution of ξ is arithmetic and maxω ξ(ω) = 1 (i.e. p1 &gt; 0 and pk = 0 for k &ge; 2,
where pk = P(ξ = k)). The term &ldquo;skip-free walk&rdquo; appears due to the fact that the
walk {Sk}, k = 0,1, . . . , cannot skip any integer level x &gt; 0: if Sn &gt; x then neces-
sarily there is a k &lt; n such that Sk = x.
</p>
<p>As we already know from Example 12.6.1, for skip-free walks with Eξ &lt; 0 the
</p>
<p>distribution of S is geometric:
</p>
<p>P(S = k)= (1 &minus; p)pk, k = 0,1, . . . ,
where p = P(η+ &lt; &infin;) and z1 = p&minus;1 is the zero of the function 1 &minus; p(z) with
p(z)=
</p>
<p>&sum;
k pkz
</p>
<p>k .
</p>
<p>It turns out that one can find many other explicit formulas for skip-free
</p>
<p>walks. In this section we will be interested in the distribution of the maximum</p>
<p/>
</div>
<div class="page"><p/>
<p>12.8 On the Distribution of the First Passage Time 385
</p>
<p>Sn = max(0, S1, . . . , Sn); as we already noted, knowing the distribution is impor-
tant for many problems of mathematical statistics, queueing theory, etc. Note that
</p>
<p>finding the distribution of Sn is the same as finding the distribution of η(x), since
</p>
<p>{Sn &lt; x} =
{
η(x) &gt; n
</p>
<p>}
. (12.8.9)
</p>
<p>Here we put η(x) :=&infin; if S &lt; x.
The Pollaczek&ndash;Spitzer identity (see Theorem 12.3.1) provides the double trans-
</p>
<p>form of the distribution of Sn. Analysing this identity shows that the distribution of
</p>
<p>Sn (or η(x)) itself typically cannot be expressed in terms of the distribution of ξk in
</p>
<p>explicit form. However, for discrete skip-free walks one has remarkable &ldquo;duality&rdquo;
</p>
<p>relations which we will now prove with the help of Pollaczek&ndash;Spitzer&rsquo;s identity.
</p>
<p>Theorem 12.8.4 If ξ is integer-valued then P(ξk &ge; 2)= 0 is a necessary and suffi-
cient condition for
</p>
<p>nP
(
η(x)= n
</p>
<p>)
= xP(Sn = x), x &ge; 1. (12.8.10)
</p>
<p>Using the Wald identity, it is also not hard to verify that if the expectation Eξ1 =
a &gt; 0 exists then the walk {Sn} will be skip-free if and only if Eη(x)= x/a. (Note
that the definition of η(x) in this section somewhat differs from that in Chap. 10.
</p>
<p>One obtains it by changing x to x+1 on the right-hand side of the definition of η(x)
from Chap. 10.)
</p>
<p>The asymptotics of the local probabilities P(Sn = x) was studied in Chap. 9 (see
e.g., Theorem 9.3.4). This together with (12.8.10) enables us to find the asymptotics
</p>
<p>of P(η(x)= n).
</p>
<p>Proof of Theorem 12.8.4 Set
</p>
<p>rx := P
(
η(x)=&infin;
</p>
<p>)
= P(S &lt; x), qx,n := P
</p>
<p>(
η(x)= n
</p>
<p>)
,
</p>
<p>Qx,n := P
(
η(x) &gt; n
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>k=n+1
qx,k + rx .
</p>
<p>Since for each y, 0 &le; y &le; x,
{
η(x)= n
</p>
<p>}
&sub;
</p>
<p>n⋃
</p>
<p>k=0
</p>
<p>{
η(y)= k
</p>
<p>}
,
</p>
<p>using the fact that the walk is skip-free, by the total probability formula one has
</p>
<p>qx,n =
n&sum;
</p>
<p>k=0
qy,kqx&minus;y,n&minus;k,
</p>
<p>where q0,0 = 1, and qy,0 = 0 for y &gt; 0. Hence for |z| &le; 1 using convolution we have
</p>
<p>qx(z) :=
&infin;&sum;
</p>
<p>k=0
qx,nz
</p>
<p>n = E
(
zη(x); η(x) &lt;&infin;
</p>
<p>)
= qy(z)qx&minus;y(z).</p>
<p/>
</div>
<div class="page"><p/>
<p>386 12 Random Walks and Factorisation Identities
</p>
<p>Putting y = 1 and q0(z)= 1, we obtain
</p>
<p>qx(z)= q(z)qx&minus;1(z)= qx(z), x &ge; 0.
</p>
<p>From here one can find the generating function Qx(z) of the sequence Qx,n:
</p>
<p>Qx(z) :=
&infin;&sum;
</p>
<p>n=0
zn
</p>
<p>(
rx +
</p>
<p>&infin;&sum;
</p>
<p>k=n+1
qx,k
</p>
<p>)
= rx
</p>
<p>1 &minus; z +
&infin;&sum;
</p>
<p>n=1
qx,n
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
zk
</p>
<p>= rx
1 &minus; z +
</p>
<p>&infin;&sum;
</p>
<p>n=1
qx,n
</p>
<p>1 &minus; zn
1 &minus; z =
</p>
<p>rx
</p>
<p>1 &minus; z +
qx(1)&minus; qx(z)
</p>
<p>1 &minus; z =
1 &minus; qx(z)
</p>
<p>1 &minus; z .
</p>
<p>Note that here the quantity qx(1)= P(η(x) &lt;&infin;)= P(S &ge; x) can be less than 1.
Using (12.8.9) we obtain that
</p>
<p>P(Sn = x)= P
(
η(x + 1) &gt; n
</p>
<p>)
&minus; P
</p>
<p>(
η(x) &gt; n
</p>
<p>)
,
</p>
<p>&infin;&sum;
</p>
<p>n=0
znP(Sn = x)=
</p>
<p>(1 &minus; qx+1(z))&minus; (1 &minus; qx(z))
1 &minus; z =
</p>
<p>qx(z)(1 &minus; q(z))
1 &minus; z .
</p>
<p>Finally, making use of the absolute summability of the series below, we find that,
</p>
<p>for |v|&lt; 1 and |z|&lt; 1,
&infin;&sum;
</p>
<p>n=0
znEvSn =
</p>
<p>&infin;&sum;
</p>
<p>x=0
vx
</p>
<p>&infin;&sum;
</p>
<p>n=0
znP(Sn = x)=
</p>
<p>1 &minus; q(z)
(1 &minus; z)(1 &minus; vq(z)) .
</p>
<p>Turning now to the Pollaczek&ndash;Spitzer formula, we can write that
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>zn
</p>
<p>n
Evmax(0,Sn) = ln 1 &minus; q(z)
</p>
<p>1 &minus; z &minus; ln
(
1 &minus; vq(z)
</p>
<p>)
= ln 1 &minus; q(z)
</p>
<p>1 &minus; z +
&infin;&sum;
</p>
<p>x=1
</p>
<p>(vq(z))x
</p>
<p>x
.
</p>
<p>Comparing the coefficients of vx , x &ge; 1, we obtain
&infin;&sum;
</p>
<p>n=1
</p>
<p>zn
</p>
<p>n
P(Sn = x)=
</p>
<p>qx(z)
</p>
<p>x
, x &ge; 1. (12.8.11)
</p>
<p>Taking into account that qx(z)= qx(z) and comparing the coefficients of zn, n&ge; 1,
in (12.8.11) we get
</p>
<p>1
</p>
<p>n
P(Sn = x)=
</p>
<p>1
</p>
<p>x
P(ηx = n), x &ge; 1, n&ge; 1.
</p>
<p>Sufficiency is proved.
</p>
<p>The necessity of the condition P(ξ &ge; 2)= 0 follows from equality (12.8.10) for
x = n= 1:
</p>
<p>p1 = q1,1 =
&infin;&sum;
</p>
<p>k=1
pk,
</p>
<p>&infin;&sum;
</p>
<p>k=2
pk = P(ξ &ge; 2)= 0.
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>12.8 On the Distribution of the First Passage Time 387
</p>
<p>Using the obtained formulas one can, for instance, find in Example 4.2.3 the
</p>
<p>distribution of the time to ruin in a game with an infinitely rich adversary (the total
</p>
<p>capital being infinite). If the initial capital of the first player is x then, for the time
</p>
<p>η(x) of his ruin, we obtain
</p>
<p>P
(
η(x)= n
</p>
<p>)
= x
</p>
<p>n
P(Sn = x),
</p>
<p>where
</p>
<p>Sn =
n&sum;
</p>
<p>j=1
ξj ; P(ξj = 1)= q, P(ξj =&minus;1)= p
</p>
<p>(p is the probability for the first player to win in a single play). Therefore, if n and
</p>
<p>x are both either odd or even then
</p>
<p>P
(
η(x)= n
</p>
<p>)
= x
</p>
<p>n
</p>
<p>(
n
</p>
<p>(n&minus; x)/2
</p>
<p>)
q(n+x)/2p(n&minus;x)/2, (12.8.12)
</p>
<p>and P(η(x)= n)= 0 otherwise.
It is interesting to ask how fast P(η(x) &gt; n) decreases as n grows in the case
</p>
<p>when the player will be ruined with probability 1, i.e. when P(η(x) &lt;&infin;)= 1. As
we already know, this happens if and only if p &le; q . (The assertion also follows from
the results of Sect. 13.3.)
</p>
<p>Applying Stirling&rsquo;s formula, as was done when proving the local limit theorem
</p>
<p>for the Bernoulli scheme, it is not difficult to obtain from (12.8.12) that, for each
</p>
<p>fixed x, as n&rarr;&infin; (n and x having the same parity), for p &le; q ,
</p>
<p>P
(
η(x)= n
</p>
<p>)
&sim; x
</p>
<p>n3/2
</p>
<p>&radic;
2
</p>
<p>π
(4pq)n/2
</p>
<p>(
q
</p>
<p>p
</p>
<p>)x/2
;
</p>
<p>P
(
η(x)&ge; n
</p>
<p>)
&sim; x
</p>
<p>n3/2(p&minus; q)2
</p>
<p>&radic;
2
</p>
<p>π
(4pq)n/2
</p>
<p>(
q
</p>
<p>p
</p>
<p>)x/2
for p &lt; q
</p>
<p>and
</p>
<p>P
(
η(x)&ge; n
</p>
<p>)
&sim; x
</p>
<p>&radic;
2
</p>
<p>πn
for p = q.
</p>
<p>The last relation allowed us, under the conditions of Sect. 8.8, to obtain the lim-
</p>
<p>iting distribution for the number of intersections of the trajectory S1, . . . , Sn with
</p>
<p>the strip [u,v] (see (8.8.24)). Up to the normalising constants, this assertion also
remains true for arbitrary random walks such that Eξk = 0 and Eξ2k &lt;&infin;. However,
even in the case of a skip-free walk, the proof of this assertion requires additional
</p>
<p>efforts, despite the fact that, for such walks, an upward intersection of the line x = 0
by the trajectory {Sn} divides the trajectory, as in Sect. 8.8, into independent identi-
cally distributed cycles.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
</p>
<p>Sequences of Dependent Trials. Markov Chains
</p>
<p>Abstract The chapter opens with in Sect. 13.1 presenting the key definitions and
</p>
<p>first examples of countable Markov chains. The section also contains the classifica-
</p>
<p>tion of states of the chain. Section 13.2 contains necessary and sufficient conditions
</p>
<p>for recurrence of states, the Solidarity Theorem for irreducible Markov chains and
</p>
<p>a theorem on the structure of a periodic Markov chain. Key theorems on random
</p>
<p>walks on lattices are presented in Sect. 13.3, along with those for a general sym-
</p>
<p>metric random walk on the real line. The ergodic theorem for general countable
</p>
<p>homogeneous chains is established in Sect. 13.4, along with its special case for fi-
</p>
<p>nite Markov chains and the Law of Large Numbers and the Central Limit Theorem
</p>
<p>for the number of visits to a given state. This is followed by a short Sect. 13.5 de-
</p>
<p>tailing the behaviour of transition probabilities for reducible chains. The last three
</p>
<p>sections are devoted to Markov chains with arbitrary state spaces. First the ergod-
</p>
<p>icity of such chains possessing a positive atom is proved in Sect. 13.6, then the
</p>
<p>concept of Harris Markov chains is introduced and conditions of ergodicity of such
</p>
<p>chains are established in Sect. 13.7. Finally, the Laws of Large Numbers and the
</p>
<p>Central Limit Theorem for sums of random variables defined on a Markov chain are
</p>
<p>obtained in Sect. 13.8.
</p>
<p>13.1 Countable Markov Chains. Definitions and Examples.
</p>
<p>Classification of States
</p>
<p>13.1.1 Definition and Examples
</p>
<p>So far we have studied sequences of independent trials. Now we will consider the
</p>
<p>simplest variant of a sequence of dependent trials.
Let G be an experiment having a finite or countable set of outcomes {E1,E2, . . .}.
</p>
<p>Suppose we keep repeating the experiment G. Denote by Xn the number of the
</p>
<p>outcome of the n-th experiment.
</p>
<p>In general, the probabilities of different values of EXn can depend on what events
</p>
<p>occurred in the previous n&minus;1 trials. If this probability, given a fixed outcome EXn&minus;1
of the (n&minus;1)-st trial, does not depend on the outcomes of the preceding n&minus;2 trials,
then one says that this sequence of trials forms a Markov chain.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_13, &copy; Springer-Verlag London 2013
</p>
<p>389</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_13">http://dx.doi.org/10.1007/978-1-4471-5201-9_13</a></div>
</div>
<div class="page"><p/>
<p>390 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>To give a precise definition of a Markov chain, consider a sequence of integer-
</p>
<p>valued random variables {Xn}&infin;n=0. If the n-th trial resulted in outcome Ej , we set
Xn := j .
</p>
<p>Definition 13.1.1 A sequence {Xn}&infin;0 forms a Markov chain if
</p>
<p>P(Xn = j |X0 = k0,X1 = k1, . . . ,Xn&minus;2 = kn&minus;2,Xn&minus;1 = i)
</p>
<p>P(Xn = j |Xn&minus;1 = i)=: p(n)ij . (13.1.1)
</p>
<p>These are the so-called countable (or discrete) Markov chains, i.e. Markov chains
</p>
<p>with countable state spaces.
</p>
<p>Thus, a Markov chain may be thought of as a system with possible states
</p>
<p>{E1,E2, . . .}. Some &ldquo;initial&rdquo; distribution of the variable X0 is given:
</p>
<p>P(X0 = j)= p0j ,
&sum;
</p>
<p>p0j = 1.
</p>
<p>Next, at integer time epochs the system changes its state, the conditional probability
</p>
<p>of being at state Ej at time n given the previous history of the system only being
</p>
<p>dependent on the state of the system at time n&minus; 1. One can briefly characterise this
property as follows: given the present, the future and the past of the sequence Xn
are independent.
</p>
<p>For example, the branching process {ζn} described in Sect. 7.7, where ζn was the
number of particles in the n-th generation, is a Markov chain with possible states
</p>
<p>{0,1,2, . . .}.
In terms of conditional expectations or conditional probabilities (see Sect. 4.8),
</p>
<p>the Markov property (as we shall call property (13.1.1)) can also be written as
</p>
<p>P
(
Xn = j
</p>
<p>∣∣ σ(X0, . . . ,Xn&minus;1)
)
= P
</p>
<p>(
Xn
</p>
<p>∣∣ σ(Xn&minus;1)
)
,
</p>
<p>where σ(&middot;) is the σ -algebra generated by random variables appearing in the argu-
ment, or, which is the same,
</p>
<p>P(Xn = j
∣∣X0, . . . ,Xn&minus;1)= P(Xn |Xn&minus;1).
</p>
<p>This definition allows immediate extension to the case of a Markov chain with a
</p>
<p>more general state space (see Sects. 13.6 and 13.7).
</p>
<p>The problem of the existence of a sequence {Xn}&infin;0 which is a Markov chain
with given transition probabilities p
</p>
<p>(n)
ij (p
</p>
<p>(n)
ij &ge; 0,
</p>
<p>&sum;
j p
</p>
<p>(n)
ij = 1) and a given &ldquo;initial&rdquo;
</p>
<p>distribution {p0k} of the variable X0 can be solved in the same way as for independent
random variables. It suffices to apply the Kolmogorov theorem (see Appendix 2) and
</p>
<p>specify consistent joint distributions by
</p>
<p>P(X0 = k0,X1 = k1, . . . ,Xn = kn) := p0k0p
(1)
k0k1
</p>
<p>p
(2)
k1k2
</p>
<p>&middot; &middot; &middot;p(n)kn&minus;1kn ,
</p>
<p>which are easily seen to satisfy the Markov property (13.1.1).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Countable Markov Chains. Definitions and Examples 391
</p>
<p>Definition 13.1.2 A Markov chain {Xn}&infin;0 is said to be homogeneous if the proba-
bilities p
</p>
<p>(n)
ij do not depend on n.
</p>
<p>We consider several examples.
</p>
<p>Example 13.1.1 (Walks with absorption and reflection) Let a &gt; 1 be an integer.
Consider a walk of a particle over integers between 0 and a. If 0 &lt; k &lt; a, then from
</p>
<p>the point k with probabilities 1/2 the particle goes to k&minus;1 or k+1. If k is equal to 0
or a, then the particle remains at the point k with probability 1. This is the so-called
</p>
<p>walk with absorption. If Xn is a random variable which is equal to the coordinate
of the particle at time n, then the sequence {Xn} forms a Markov chain, since the
conditional expectation of the random variable Xn given X0,X1, . . . ,Xn&minus;1 depends
only on the value of Xn&minus;1. It is easy to see that this chain is homogeneous.
</p>
<p>This walk can be used to describe a fair game (see Example 4.2.3) in the case
</p>
<p>when the total capital of both gamblers equals a. Reaching the point a means the
</p>
<p>ruin of the second gambler.
</p>
<p>On the other hand, if the particle goes from the point 0 to the point 1 with prob-
</p>
<p>ability 1, and from the point a to the point a &minus; 1 with probability 1, then we have a
walk with reflection. It is clear that in this case the positions Xn of the particle also
form a homogeneous Markov chain.
</p>
<p>Example 13.1.2 Let {ξk}&infin;k=0 be a sequence of independent integer-valued random
variables and d &gt; 0 be an integer. The random variables Xn :=
</p>
<p>&sum;n
k=0 ξk (mod d)
</p>
<p>obtained by adding ξk modulo d (Xn =
&sum;n
</p>
<p>k=0 ξk &minus; jd , where j is such that 0 &le;
Xn &lt; d) form a Markov chain. Indeed, we have Xn = Xn&minus;1 + ξn (mod d), and
therefore the conditional distribution of Xn given X1,X2, . . . ,Xn&minus;1 depends only
on Xn&minus;1.
</p>
<p>If, in addition, {ξk} are identically distributed, then this chain is homogeneous.
Of course, all the aforesaid also holds when d = &infin;, i.e. for the conventional
</p>
<p>summation. The only difference is that the set of possible states of the system is in
</p>
<p>this case infinite.
</p>
<p>From the definition of a homogeneous Markov chain it follows that the probabil-
</p>
<p>ities p
(n)
ij of transition from state Ei to state Ej on the n-th step do not depend on n.
</p>
<p>Denote these probabilities by pij . They form the transition matrix P = ‖pij‖ with
the properties
</p>
<p>pij &ge; 0,
&sum;
</p>
<p>j
</p>
<p>pij = 1.
</p>
<p>The second property is a consequence of the fact that the system, upon leaving the
</p>
<p>state Ei , enters with probability 1 one of the states E1,E2, . . . .
</p>
<p>Matrices with the above properties are said to be stochastic.
The matrix P completely describes the law of change of the state of the system
</p>
<p>after one step. Now consider the change of the state of the system after k steps. We</p>
<p/>
</div>
<div class="page"><p/>
<p>392 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>introduce the notation pij (k) := P(Xk = j |X0 = i). For k &gt; 1, the total probability
formula yields
</p>
<p>pij (k)=
&sum;
</p>
<p>s
</p>
<p>P(Xk&minus;1 = s|X0 = i)psj =
&sum;
</p>
<p>s
</p>
<p>pis(k &minus; 1)psj .
</p>
<p>Summation here is carried out over all states. If we denote by P(k) := ‖pij (k)‖ the
matrix of transition probabilities pij (k), then the above equality means that P(k)=
P(k &minus; 1)P or, which is the same, that P(k) = P k . Thus the matrix P uniquely
determines transition probabilities for any number of steps. It should be added here
that, for a homogeneous chain,
</p>
<p>P(Xn+k = j |Xn = i)= P(Xk = j |X0 = i)= pij (k).
</p>
<p>We see from the aforesaid that the &ldquo;distribution&rdquo; of a chain will be completely de-
</p>
<p>termined by the matrix P and the initial distribution p0k = P(X0 = k).
We leave it to the reader as an exercise to verify that, for an arbitrary k &ge; 1 and
</p>
<p>sets B1, . . . ,Bn&minus;k ,
</p>
<p>P(Xn = j |Xn&minus;k = i;Xn&minus;k&minus;1 &isin; B1, . . . ,X0 &isin; Bn&minus;k)= pij (k).
</p>
<p>To prove this relation one can first verify it for k = 1 and then make use of induction.
It is obvious that a sequence of independent integer-valued identically distributed
</p>
<p>random variables Xn forms a Markov chain with pij = pj = P(Xn = j). Here one
has P(k)= P k = P .
</p>
<p>13.1.2 Classification of States1
</p>
<p>Definition 13.1.3
</p>
<p>K1. A state Ei is called inessential if there exist a state Ej and an integer t0 &gt; 0
such that pij (t0) &gt; 0 and pji(t)= 0 for every integer t .
</p>
<p>Otherwise the state Ei is called essential.
K2. Essential states Ei and Ej are called communicating if there exist such integers
</p>
<p>t &gt; 0 and s &gt; 0 that pij (t) &gt; 0 and pji(s) &gt; 0.
</p>
<p>Example 13.1.3 Assume a system can be in one of the four states {E1,E2,E2,E4}
and has the transition matrix
</p>
<p>P =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 1/2 1/2 0
</p>
<p>1/2 0 0 1/2
</p>
<p>0 0 1/2 1/2
</p>
<p>0 0 1/2 1/2
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>1Here and in Sect. 12.2 we shall essentially follow the paper by A.N. Kolmogorov [23].</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Countable Markov Chains. Definitions and Examples 393
</p>
<p>Fig. 13.1 Possible transitions
</p>
<p>and their probabilities in
</p>
<p>Example 13.1.3
</p>
<p>In Fig. 13.1 the states are depicted by dots, transitions from state to state by
</p>
<p>arrows, numbers being the corresponding probabilities. In this chain, the states E1
and E2 are inessential while E3 and E4 are essential and communicating.
</p>
<p>In the walk with absorption described in Example 13.1.1, the states 1,2, . . . ,
</p>
<p>a&minus;1 are inessential. The states 0 and a are essential but non-communicating, and it
is natural to call them absorbing. In the walk with reflection, all states are essential
and communicating.
</p>
<p>Let {Xn}&infin;n=0 be a homogeneous Markov chain. We distinguish the class S0 of
all inessential states. Let Ei be an essential state. Denote by SEi the class of states
</p>
<p>comprising Ei and all states communicating with it. If Ej &isin; SEi , then Ej is essential
and communicating with Ei , and Ei &isin; SEj . Hence SEi = SEj . Thus, the whole set
of essential states can be decomposed into disjoint classes of communicating states
</p>
<p>which will be denoted by S1, S2, . . .
</p>
<p>Definition 13.1.4 If the class SEi consists of the single state Ei , then this state is
</p>
<p>called absorbing.
</p>
<p>It is clear that after a system has hit an essential state Ei , it can never leave
</p>
<p>the class SEi .
</p>
<p>Definition 13.1.5 A Markov chain consisting of a single class of essential com-
</p>
<p>municating states is said to be irreducible. A Markov chain is called reducible if it
contains more than one such class.
</p>
<p>If we enumerate states so that the states from S0 come first, next come states
</p>
<p>from S1 and so on, then the matrix of transition probabilities will have the form
</p>
<p>shown in Fig. 13.2. Here the submatrices marked by zeros have only zero entries.
</p>
<p>The cross-hatched submatrices are stochastic.
</p>
<p>Each such submatrix corresponds to some irreducible chain. If, at some time, the
</p>
<p>system is at a state of such an irreducible chain, then the system will never leave this
</p>
<p>chain in the future. Hence, to study the dynamics of an arbitrary Markov chain, it
</p>
<p>is sufficient to study the dynamics of irreducible chains. Therefore one of the basic
</p>
<p>objects of study in the theory of Markov chains is irreducible Markov chains. We
will consider them now.</p>
<p/>
</div>
<div class="page"><p/>
<p>394 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Fig. 13.2 The structure of
</p>
<p>the matrix of transition
</p>
<p>probabilities of a general
</p>
<p>Markov chain. The class S0
</p>
<p>consists of all inessential
</p>
<p>states, whereas S1, S2, . . . are
</p>
<p>closed classes of
</p>
<p>communicating states
</p>
<p>We introduce the following notation:
</p>
<p>fj (n) := P(Xn = j,Xn&minus;1 
= j, . . . ,X1 
= j |X0 = j), Fj :=
&infin;&sum;
</p>
<p>n=1
fj (n);
</p>
<p>fj (n) is the probability that the system leaving the j -th state will return to it for the
</p>
<p>first time after n steps. The probability that the system leaving the j -th state will
</p>
<p>eventually return to it is equal to Fj .
</p>
<p>Definition 13.1.6
</p>
<p>K3. A state Ej is said to be recurrent (or persistent) if Fj = 1, and transient if
Fj &lt; 1.
</p>
<p>K4. A state Ej is called null if pjj (n)&rarr; 0 as n&rarr;&infin;, and positive otherwise.
K5. A state Ej is called periodic with period dj if the recurrence with this state has
</p>
<p>a positive probability only when the number of steps is a multiple of dj &gt; 1,
</p>
<p>and dj is the maximum number having such property.
</p>
<p>In other words, dj &gt; 1 is the greatest common divisor (g.c.d.) of the set of num-
</p>
<p>bers {n : fj (n) &gt; 0}. Note that one can always choose from this set a finite subset
{n1, . . . , nk} such that dj is the greatest common divisor of these numbers. It is also
clear that pjj (n)= fj (n)= 0 if n 
= 0 (mod dj ).
</p>
<p>Example 13.1.4 Consider a walk of a particle over integer points on the real line
defined as follows. The particle either takes one step to the right or remains on
</p>
<p>the spot with probabilities 1/2. Here fj (1)= 1/2, and if n &gt; 1 then fj (n)= 0 for
any point j . Therefore Fj &lt; 1 and all the states are transient. It is easily seen that
</p>
<p>pjj (n)= 1/2n &rarr; 0 as n&rarr;&infin; and hence every state is null.
On the other hand, if the particle jumps to the right with probability 1/2 and with
</p>
<p>the same probability jumps to the left, then we have a chain with period 2, since
</p>
<p>recurrence to any particular state is only possible in an even number of steps.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Recurrence of States. Irreducible Chains 395
</p>
<p>13.2 Necessary and Sufficient Conditions for Recurrence of
</p>
<p>States. Types of States in an Irreducible Chain. The
</p>
<p>Structure of a Periodic Chain
</p>
<p>Recall that the function
</p>
<p>a(z)=
&infin;&sum;
</p>
<p>n=0
anz
</p>
<p>n
</p>
<p>is called the generating function of the sequence {an}&infin;n=0. Here z is a complex vari-
able. If the sequence {an} is bounded, then this series converges for |z|&lt; 1.
</p>
<p>Theorem 13.2.1 A state Ej is recurrent if and only if Pj =
&sum;&infin;
</p>
<p>n=1 pjj (n)=&infin;. For
a transient Ej ,
</p>
<p>Fj =
Pj
</p>
<p>1 + Pj
. (13.2.1)
</p>
<p>The assertion of this theorem is a kind of expansion of the Borel&ndash;Cantelli lemma
</p>
<p>to the case of dependent events An = {Xn = j}. With probability 1 there occur
infinitely many events An if and only if
</p>
<p>&infin;&sum;
</p>
<p>n=1
P(An)= Pj =&infin;.
</p>
<p>Proof By the total probability formula we have
</p>
<p>pjj (n)= fj (1)pjj (n&minus; 1)+ fj (2)pjj (n&minus; 2)+ &middot; &middot; &middot; + fj (n&minus; 1)pjj (1)+ fj (n) &middot; 1.
</p>
<p>Introduce the generating functions of the sequences {pjj (n)}&infin;n=0 and {fj (n)}&infin;n=0:
</p>
<p>Pj (z) :=
&infin;&sum;
</p>
<p>n=1
pjj (n)z
</p>
<p>n, Fj (z) :=
&infin;&sum;
</p>
<p>n=1
fj (n)z
</p>
<p>n.
</p>
<p>Both series converge inside the unit circle and represent analytic functions. The
</p>
<p>above formula for pjj (n), after multiplying both sides by z
n and summing up over
</p>
<p>n, leads (by the rule of convolution) to the equality
</p>
<p>Pj (z)= zf1(1)
(
1 + Pj (z)
</p>
<p>)
+ z2f1(2)
</p>
<p>(
1 + Pj (z)
</p>
<p>)
+ &middot; &middot; &middot; =
</p>
<p>(
1 + Pj (z)
</p>
<p>)
Fj (z).
</p>
<p>Thus
</p>
<p>Fj (z)=
Pj (z)
</p>
<p>1 + Pj (z)
, Pj (z)=
</p>
<p>Fj (z)
</p>
<p>1 + Fj (z)
.
</p>
<p>Assume that Pj =&infin;. Then Pj (z)&rarr;&infin; as z &uarr; 1 and therefore Fj (z)&rarr; 1. Since
Fj (z) &lt; Fj for real z &lt; 1, we have Fj = 1 and hence Ej is recurrent.</p>
<p/>
</div>
<div class="page"><p/>
<p>396 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Now suppose that Fj = 1. Then Fj (z)&rarr; 1 as z &uarr; 1, and so Pj (z)&rarr;&infin;. There-
fore Pj (z)=&infin;.
</p>
<p>If Ej is transient, it follows from the above that Pj (z) &lt;&infin;, and setting z := 1
we obtain equality (13.2.1). �
</p>
<p>The quantity Pj =
&sum;&infin;
</p>
<p>n=1 pjj (n) can be interpreted as the mean number of visits
to the state Ej , provided that the initial state is also Ej . It follows from the fact that
</p>
<p>the number of visits to the state Ej can be represented as
&sum;&infin;
</p>
<p>n=1 I (Xn = j), where,
as before, I (A) is the indicator of the event A. Therefore the expectation of this
</p>
<p>number is equal to
</p>
<p>E
</p>
<p>&infin;&sum;
</p>
<p>n=1
I (Xn = j)=
</p>
<p>&infin;&sum;
</p>
<p>n=1
EI (Xn = j)=
</p>
<p>&infin;&sum;
</p>
<p>n=1
pjj (n)= Pj .
</p>
<p>Theorem 13.2.1 implies the following result.
</p>
<p>Corollary 13.2.1 A transient state is always null.
</p>
<p>This is obvious, since it immediately follows from the convergence of the series&sum;
pjj (n) &lt;&infin; that pjj (n)&rarr; 0.
Thus, based on definitions K3&ndash;K5, we could distinguish, in an irreducible chain,
</p>
<p>8 possible types of states (each of the three properties can either be present or not).
</p>
<p>But in reality there are only 6 possible types since transient states are automatically
</p>
<p>null, and positive states are recurrent. These six types are generated by:
</p>
<p>1) Classification by the asymptotic properties of the probabilities pjj (n) (tran-
</p>
<p>sient, recurrent null and positive states).
</p>
<p>2) Classification by the arithmetic properties of the probabilities pjj (n) or fj (n)
</p>
<p>(periodic or aperiodic).
</p>
<p>Theorem 13.2.2 (Solidarity Theorem) In an irreducible homogeneous Markov
chain all states are of the same type: if one is recurrent then all are recurrent, if
one is null then all are null, if one state is periodic with period d then all states are
periodic with the same period d .
</p>
<p>Proof Let Ek and Ej be two different states. There exist numbers N and M such
that
</p>
<p>pkj (N) &gt; 0, pjk(M) &gt; 0.
</p>
<p>The total probability formula
</p>
<p>pkk(N +M + n)=
&sum;
</p>
<p>l,s
</p>
<p>pkl(N)pls(n)psk(M)
</p>
<p>implies the inequality
</p>
<p>pkk(N +M + n)&ge; pkj (N)pjj (n)pjk(M)= αβpjj (n).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Recurrence of States. Irreducible Chains 397
</p>
<p>Here n &gt; 0 is an arbitrary integer, α = pjj (N) &gt; 0, and β = pjj (M) &gt; 0. In the
same way one can obtain the inequality
</p>
<p>pjj (N +M + n)&ge; αβpkk(n).
</p>
<p>Hence
</p>
<p>1
</p>
<p>αβ
pkk(N +M + n)&ge; pkk(n)&ge; αβpkk(n&minus;M &minus;N). (13.2.2)
</p>
<p>We see from these inequalities that the asymptotic properties of pkk(n) and
</p>
<p>pjj (n) are the same. If Ek is null, then pkk(n) &rarr; 0, therefore pjj (n) &rarr; 0 and
Ej is also null. If Ek is recurrent or, which is equivalent, Pk =
</p>
<p>&sum;&infin;
n=1 pkk(n)=&infin;,
</p>
<p>then
</p>
<p>&infin;&sum;
</p>
<p>n=M+N+1
pjj (n)&ge; αβ
</p>
<p>&infin;&sum;
</p>
<p>n=M+N+1
pkk(n&minus;M &minus;N)=&infin;,
</p>
<p>and Ej is also recurrent.
</p>
<p>Suppose now that Ek is a periodic state with period dk . If pkk(n) &gt; 0, then dk
divides n. We will write this as dk | n. Since pkk(M + N) &ge; αβ &gt; 0, then dk |
(M +N).
</p>
<p>We now show that the state Ej is also periodic and its period dj is equal to dk .
</p>
<p>Indeed, if pjj (n) &gt; 0 for some n, then by virtue of (13.2.2), pkk(n+M +N) &gt; 0.
Therefore dk | (n+M +N), and since dk | (M +N), dk | n and hence dk &le; dj . In
a similar way one can prove that dj &le; dk . Thus dj = dk . �
</p>
<p>If the states of an irreducible Markov chain are periodic with period d &gt; 1, then
</p>
<p>the chain is called periodic.
We will now show that the study of periodic chains can essentially be reduced to
</p>
<p>the study of aperiodic chains.
</p>
<p>Theorem 13.2.3 If a Markov chain is periodic with period d , then the set of states
can be split into d subclasses Ψ0,Ψ1, . . . ,Ψd&minus;1 such that, with probability 1, in one
step the system passes from Ψk to Ψk+1, and from Ψd&minus;1 the system passes to Ψ0.
</p>
<p>Proof Choose some state, say, E1. Based on this we will construct the subclasses
Ψ0,Ψ1, . . . ,Ψd&minus;1 in the following way: Ei &isin; Ψα , 0 &le; α &le; d &minus; 1, if there exists an
integer k &gt; 0 such that p1i(kd + α) &gt; 0.
</p>
<p>We show that no state can belong to two subclasses simultaneously. To this end
</p>
<p>it suffices to prove that if Ei &isin; Ψα and p1i(s) &gt; 0 for some s, then s = α (mod d).
Indeed, there exists a number t1 &gt; 0 such that pi1(t1) &gt; 0. So, by the definition
</p>
<p>of Ψα , we have p11(kd + α + t1) &gt; 0. Moreover, p11(s + t1) &gt; 0. Hence d | (kd +
α + t1) and d | (s + t1). This implies α = s (mod d).
</p>
<p>Since starting from the state E1 it is possible with positive probability to enter
</p>
<p>any state Ei , the union
⋃
</p>
<p>α Ψα contains all the states.</p>
<p/>
</div>
<div class="page"><p/>
<p>398 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Fig. 13.3 The structure of
</p>
<p>the matrix of transition
</p>
<p>probabilities of a periodic
</p>
<p>Markov chain: an illustration
</p>
<p>to the proof of
</p>
<p>Theorem 13.2.3
</p>
<p>We now prove that in one step the system goes from Ψα with probability 1 to
</p>
<p>Ψα+1 (here the sum α + 1 is modulo d). We have to show that, for Ei &isin; Ψα ,
&sum;
</p>
<p>Ej&isin;Ψα+1
pij = 1.
</p>
<p>To do this, it suffices to prove that pij = 0 when Ei &isin; Ψα , Ej /&isin; Ψα+1.
If we assume the opposite (pij &gt; 0) then, taking into account the inequality
</p>
<p>p1i(kd + α) &gt; 0, we have p1j (kd + α + 1) &gt; 0 and consequently Ej &isin; Ψα+1. This
contradiction completes the proof of the theorem. �
</p>
<p>We see from the theorem that the matrix of a periodic chain has the form shown
</p>
<p>in Fig. 13.3 where non-zero entries can only be in the shaded cells.
</p>
<p>From a periodic Markov chain with period d one can construct d new Markov
</p>
<p>chains. The states from the subset Ψα will be the states of the α-th chain. Transition
</p>
<p>probabilities are given by
</p>
<p>pαij := pij (d).
By virtue of Theorem 13.2.3,
</p>
<p>&sum;
Ej&isin;Ψα p
</p>
<p>α
ij = 1. The new chains, to which one can
</p>
<p>reduce in a certain sense the original one, will have no subclasses.
</p>
<p>13.3 Theorems on Random Walks on a Lattice
</p>
<p>1. A random walk on integer points on the line. Imagine a particle moving on
</p>
<p>integer points of the real line. Transitions from one point to another occur in equal
</p>
<p>time intervals. In one step, from point k the particle goes with a positive probability
</p>
<p>p to the point k + 1, and with positive probability q = 1 &minus; p it moves to the point
k &minus; 1. As was already mentioned, to this physical system there corresponds the
following Markov chain:
</p>
<p>Xn =Xn&minus;1 + ξn =X0 + Sn,
</p>
<p>where ξn takes values 1 and &minus;1 with probabilities p and q , respectively, and Sn =&sum;n
k=1 ξk . The states of the chain are integer points on the line.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Theorems on Random Walks on a Lattice 399
</p>
<p>It is easy to see that returning to a given point with a positive probability is only
</p>
<p>possible after an even number of steps, and f0(2)= 2pq &gt; 0. Therefore this chain
is periodic with period 2.
</p>
<p>We now establish conditions under which the random walk forms a recurrent
</p>
<p>chain.
</p>
<p>Theorem 13.3.1 The random walk {Xn} forms a recurrent Markov chain if and only
if p = q = 1/2.
</p>
<p>Proof Since 0 &lt;p &lt; 1, the random walk is an irreducible Markov chain. Therefore
by Theorem 13.2.2 it suffices to examine the type of any given point, for example,
</p>
<p>zero.
</p>
<p>We will make use of Theorem 13.2.1. In order to do this, we have to investigate
</p>
<p>the convergence of the series
&sum;&infin;
</p>
<p>n=1 p00(n). Since our chain is periodic with period
2, one has p00(2k + 1)= 0. So it remains to compute
</p>
<p>&sum;&infin;
1 p00(2k). The sum Sn is
</p>
<p>the coordinate of the walking particle after n steps (X0 = 0). Therefore p00(2k)=
P(S2k = 0). The equality S2k = 0 holds if k of the random variables ξj are equal
to 1 and the other k are equal to &minus;1 (k steps to the right and k steps to the left).
Therefore, by Theorem 5.2.1,
</p>
<p>P(S2k = 0)&sim;
1&radic;
πk
</p>
<p>e&minus;2kH(1/2) = 1&radic;
πk
</p>
<p>(4pq)k.
</p>
<p>We now elucidate the behaviour of the function β(p)= 4pq = 4p(1&minus;p) on the
interval [0,1]. At the point p = 1/2 the function β(p) attains its only extremum,
β(1/2) = 1. At all the other points of [0,1], β(p) &lt; 1. Therefore 4pq &lt; 1 for
p 
= 1/2, which implies convergence of the series
</p>
<p>&sum;&infin;
k=1 p00(2k) and hence the tran-
</p>
<p>sience of the Markov chain. But if p = 1/2 then p00(2k) &sim; 1/
&radic;
πk and the series&sum;&infin;
</p>
<p>k=1 p00(2k) diverges, which implies, in turn, that all the states of the chain are
recurrent. The theorem is proved. �
</p>
<p>Theorem 13.3.1 allows us to make the following remark. If p 
= 1/2, then the
mean number of recurrences to 0 is finite, as it is equal to
</p>
<p>&sum;&infin;
k=1 p00(2k). This
</p>
<p>means that, after a certain time, the particle will never return to zero. The particle
</p>
<p>will &ldquo;drift&rdquo; to the right or to the left depending on whether p is greater than 1/2 or
</p>
<p>less. This can easily be obtained from the law of large numbers.
</p>
<p>If p = 1/2, then the mean number of recurrences to 0 is infinite; the particle
has no &ldquo;drift&rdquo;. It is interesting to note that the increase in the mean number of re-
</p>
<p>currences is not proportional to the number of steps. Indeed, the mean number of
</p>
<p>recurrences over the first 2n steps is equal to
&sum;n
</p>
<p>k=1 p00(2k). From the proof of The-
orem 13.3.1 we know that p00(2k)&sim; 1/
</p>
<p>&radic;
πk. Therefore, as n&rarr;&infin;,
</p>
<p>n&sum;
</p>
<p>k=1
p00(2k)&sim;
</p>
<p>n&sum;
</p>
<p>k=1
</p>
<p>1&radic;
πk
</p>
<p>&sim; 2
&radic;
n&radic;
π
.</p>
<p/>
</div>
<div class="page"><p/>
<p>400 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Thus, in the fair game considered in Example 4.2.2, the proportion of ties rapidly
</p>
<p>decreases as the number of steps increases, and deviations are growing both in mag-
</p>
<p>nitude and duration.
</p>
<p>13.3.1 Symmetric Random Walks in Rk , k &ge; 2
</p>
<p>Consider the following random walk model in the k-dimensional Euclidean space
</p>
<p>R
k . If the walking particle is at point (m1, . . . ,mk), then it can move with prob-
</p>
<p>abilities 1/2k to any of the 2k vertices of the cube |xj &minus; mj | = 1, i.e. the points
with coordinates (m1 &plusmn; 1, . . . ,mk &plusmn; 1). It is natural to call this walk symmetric.
Denoting by Xn the position of the particle after the n-th jump, we have, as before,
</p>
<p>a sequence of k-dimensional random variables forming a homogeneous irreducible
</p>
<p>Markov chain. We shall show that all states of the walk on the plane are, as in the
</p>
<p>one-dimensional case, recurrent. In the three-dimensional space, the states will turn
</p>
<p>out to be transient. Thus we shall prove the following assertion.
</p>
<p>Theorem 13.3.2 The symmetric random walk is recurrent in spaces of one and two
dimensions and transient in spaces of three or more dimensions.
</p>
<p>In this context, W. Feller made the sharp comment that the proverb &ldquo;all roads
</p>
<p>lead to Rome&rdquo; is true only for two-dimensional surfaces. The assertion of Theo-
</p>
<p>rem 13.3.2 is adjacent to the famous theorem of P&oacute;lya on the transience of sym-
</p>
<p>metric walks in Rk for k &gt; 2 when the particle jumps to neighbouring points along
</p>
<p>the coordinate axes (so that ξj assumes 2k values with probabilities 1/2k each). We
</p>
<p>now turn to the proof of Theorem 13.3.2.
</p>
<p>Proof of Theorem 13.3.2 Let k = 2. It is not difficult to see that our walk Xn can be
represented as a sum of two independent components
</p>
<p>Xn =
(
X&minus;1n ,0
</p>
<p>)
+
(
0,X2n
</p>
<p>)
,
</p>
<p>(
X10,X
</p>
<p>2
0
</p>
<p>)
=X0,
</p>
<p>where Xin, i = 1,2, . . . , are scalar (one-dimensional) sequences describing symmet-
ric independent random walks on the respective lines (axes). This is obvious, for the
</p>
<p>two-dimensional sequence admits the representation
</p>
<p>Xn+1 =Xn + ξn, (13.3.1)
</p>
<p>where ξn assumes 4 values (&plusmn;1,0) + (0,&plusmn;1) = (&plusmn;1,&plusmn;1) with probabilities 1/4
each.
</p>
<p>With the help of representation (13.3.1) we can investigate the asymptotic be-
</p>
<p>haviour of the transition probabilities pij (n). Let X0 coincide with the origin (0,0).
</p>
<p>Then
</p>
<p>p00(2n) = P
(
X2n = (0,0)|X0 = (0,0)
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Theorems on Random Walks on a Lattice 401
</p>
<p>= P
(
X12n = 0|X10 = 0
</p>
<p>)
P
(
X22n = 0|X20 = 0
</p>
<p>)
&sim; (1/
</p>
<p>&radic;
πn)2 = 1/(πn).
</p>
<p>From this it follows that the series
&sum;&infin;
</p>
<p>n=0 p00(n) diverges and so all the states of our
chain are recurrent.
</p>
<p>The case k = 3 should be treated in a similar way. Represent the sequence Xn as
a sum of three independent components
</p>
<p>Xn =
(
X1n,0,0
</p>
<p>)
+
(
0,X2n,0
</p>
<p>)
+
(
0,0,X3n
</p>
<p>)
,
</p>
<p>where the Xin are, as before, symmetric random walks on the real line. If we set
</p>
<p>X0 = (0,0,0), then
</p>
<p>p00(2n)=
(
P
(
X12n = 0|X10 = 0
</p>
<p>))3 &sim; 1/(πn)3/2.
</p>
<p>The series
&sum;&infin;
</p>
<p>n=1 p00(n) is convergent here, and hence the states of the chain are
transient. In contrast to the straight line and plane cases, a particle leaving the origin
</p>
<p>will, with a positive probability, never come back.
</p>
<p>It is evident that a similar situation takes place for walks in k-dimensional space
</p>
<p>with k &ge; 3, since
&sum;&infin;
</p>
<p>n=1(πn)
&minus;k/2 &lt;&infin; for k &ge; 3. The theorem is proved. �
</p>
<p>13.3.2 Arbitrary Symmetric Random Walks on the Line
</p>
<p>Let, as before,
</p>
<p>Xn =X0 +
n&sum;
</p>
<p>1
</p>
<p>ξj , (13.3.2)
</p>
<p>but now ξj are arbitrary independent identically distributed integer-valued random
</p>
<p>variables. Theorem 13.3.1 may be generalised in the following way:
</p>
<p>Theorem 13.3.3 If the ξj are symmetric and the expectation Eξj exists (and hence
Eξj = 0) then the random walk Xn forms a recurrent Markov chain with null states.
</p>
<p>Proof It suffices to verify that
</p>
<p>&infin;&sum;
</p>
<p>n=1
P(Sn = 0)=&infin;,
</p>
<p>where Sn =
&sum;n
</p>
<p>1 ξj , and that P(Sn = 0)&rarr; 0 as n&rarr;&infin;. Put
</p>
<p>p(z) := Ezξ1 =
&infin;&sum;
</p>
<p>k=&minus;&infin;
zkP(ξ1 = k).</p>
<p/>
</div>
<div class="page"><p/>
<p>402 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Then the generating function of Sn will be equal to Ez
Sn = pn(z), and by the inver-
</p>
<p>sion formula (see Sect. 7.7)
</p>
<p>P(Sn = 0)=
1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>|z|=1
pnz&minus;1dz, (13.3.3)
</p>
<p>&infin;&sum;
</p>
<p>n=0
P(Sn = 0)=
</p>
<p>1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>|z|=1
</p>
<p>dz
</p>
<p>z(1 &minus; p(z)) =
1
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
</p>
<p>dt
</p>
<p>1 &minus; p(eit ) .
</p>
<p>The last equality holds since the real function p(r) is even and is obtained by sub-
</p>
<p>stituting z= eit .
Since Eξ1 = 0, one has 1 &minus; p(eit )= o(t) as t &rarr; 0 and, for sufficiently small δ
</p>
<p>and 0 &le; t &lt; δ,
</p>
<p>0 &le; 1 &minus; p
(
eit
</p>
<p>)
&lt; t
</p>
<p>(the function p(eit ) is real by virtue of the symmetry of ξ1). This implies
</p>
<p>&int; π
</p>
<p>0
</p>
<p>dt
</p>
<p>1 &minus; p(eit ) &ge;
&int; δ
</p>
<p>0
</p>
<p>dt
</p>
<p>t
=&infin;.
</p>
<p>Convergence P(Sn = 0) &rarr; 0 is a consequence of (13.3.3) since, for all z on the
circle |z| = 1, with the possible exclusion of finitely many points, one has p(z) &lt; 1
and hence pn(z)&rarr; 0 as n&rarr;&infin;. The theorem is proved. �
</p>
<p>Theorem 13.3.3 can be supplemented by the following assertion.
</p>
<p>Theorem 13.3.4 Under the conditions of Theorem 13.3.3, if the g.c.d. of the possi-
ble values of ξj equals 1 then the set of values of {Xn} constitutes a single class of
essential communicating states. This class coincides with the set of all integers.
</p>
<p>The assertion of the theorem follows from the next lemma.
</p>
<p>Lemma 13.3.1 If the g.c.d. of integers a1 &gt; 0, . . . , ar &gt; 0 is equal to 1, then there
exists a number K such that every natural k &ge;K can be represented as
</p>
<p>k = n1a1 + &middot; &middot; &middot; + nrar ,
</p>
<p>where ni &ge; 0 are some integers.
</p>
<p>Proof Consider the function L(n) = n1a1 + &middot; &middot; &middot; + nrar , where n = (n1, . . . , nr) is
a vector with integer (possibly negative) components. Let d &gt; 0 be the minimal
</p>
<p>natural number for which there exists a vector n0 such that
</p>
<p>d = L
(
n0
)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Theorems on Random Walks on a Lattice 403
</p>
<p>We show that every natural number that can be represented as L(n) is divisible by d .
</p>
<p>Suppose that this is not true. Then there exist n, k and 0 &lt; α &lt; d such that
</p>
<p>L(n)= kd + α.
</p>
<p>But since the function L(n) is linear,
</p>
<p>L
(
n&minus; kx0
</p>
<p>)
= kd + α &minus; kd = α &lt; d,
</p>
<p>which contradicts the minimality of d in the set of positive integer values of L(n).
</p>
<p>The numbers a1, . . . , ar are also the values of the function L(n), so they are
</p>
<p>divisible by d . The greatest common divisor of these numbers is by assumption
</p>
<p>equal to one, so that d = 1.
Let k be an arbitrary natural number. Denoting by θ &lt; A the remainder after
</p>
<p>dividing k by A := a1 + &middot; &middot; &middot; + ar , we can write
</p>
<p>k = m(a1 + &middot; &middot; &middot; + ar)+ θ =m(a1 + &middot; &middot; &middot; + ar)+ θL
(
n0
)
</p>
<p>= a1
(
m+ θn01
</p>
<p>)
+ a2
</p>
<p>(
m+ θn02
</p>
<p>)
+ &middot; &middot; &middot; + ar
</p>
<p>(
m+ θn0r
</p>
<p>)
,
</p>
<p>where ni :=m+ θn0i &gt; 0, i = 1, . . . , r , for sufficiently large k (or m).
The lemma is proved. �
</p>
<p>Proof of Theorem 13.3.4 Put qj := P(ξ = aj ) &gt; 0. Then, for each k &ge; K , there
exists an n such that nj &ge; 0,
</p>
<p>&sum;r
j=1 ajnj = k, and hence, for n=
</p>
<p>&sum;r
j=1 nj , we have
</p>
<p>p0k(n)&ge; qn11 &middot; &middot; &middot;q
nr
r &gt; 0.
</p>
<p>In other words, all the states k &ge; K are reachable from 0. Similarly, all the states
k &le; &minus;K are reachable from 0. The states k &isin; [&minus;K,K] are reachable from the
point &minus;2K (which is reachable from 0). The theorem is proved. �
</p>
<p>Corollary 13.3.1 If the conditions of Theorems 13.3.3 and 13.3.4 are satisfied, then
the chain (13.3.2) with an arbitrary initial state X0 visits every state k infinitely
many times with probability 1. In particular, for any X0 and k, the random variable
ν = min{n :Xn = k} will be proper.
</p>
<p>If we are interested in investigating the periodicity of the chain (13.3.2), then
</p>
<p>more detailed information on the set of possible values of ξj is needed. We leave
</p>
<p>it to the reader to verify that, for example, if this set is of the form {a + akd},
k = 1,2, . . . , d &ge; 1, g.c.d. (a1, a2, . . .)= 1, g.c.d. (a, d)= 1, then the chain will be
periodic with period d .</p>
<p/>
</div>
<div class="page"><p/>
<p>404 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>13.4 Limit Theorems for Countable Homogeneous Chains
</p>
<p>13.4.1 Ergodic Theorems
</p>
<p>Now we return to arbitrary countable homogeneous Markov chains. We will need
</p>
<p>the following conditions:
</p>
<p>(I) There exists a state E0 such that the recurrence time τ
(s) to Es (P(τ
</p>
<p>(s) = n)=
fs(n)) has finite expectation Eτ
</p>
<p>(s) &lt;&infin;.
(II) The chain is irreducible.
</p>
<p>(III) The chain is aperiodic.
</p>
<p>We introduce the so-called &ldquo;taboo probabilities&rdquo; Pi(n, j) of transition from Ei
to Ej in n steps without visiting the &ldquo;forbidden&rdquo; state Ei :
</p>
<p>Pi(n, j) := P(Xn = j ;X1 
= i, . . . ,Xn&minus;1 
= i |X0 = i).
</p>
<p>Theorem 13.4.1 (The ergodic theorem) Conditions (I)&ndash;(III) are necessary and suf-
ficient for the existence, for all i and j , of the positive limits
</p>
<p>lim
n&rarr;&infin;
</p>
<p>pij (n)= πj &gt; 0, i, j = 0,1,2, . . . . (13.4.1)
</p>
<p>The sequence of values {πj } is the unique solution of the system
{&sum;&infin;
</p>
<p>j=0 πj = 1,
πj =
</p>
<p>&sum;&infin;
k=0 πkpkj , j = 0,1,2, . . . ,
</p>
<p>(13.4.2)
</p>
<p>in the class of absolutely convergent series.
Moreover, Eτ (j) &lt; &infin; for all j , and the quantities πj = (Eτ (j))&minus;1 admit the
</p>
<p>representation
</p>
<p>πj =
(
Eτ (j)
</p>
<p>)&minus;1 =
(
Eτ (s)
</p>
<p>)&minus;1 &infin;&sum;
</p>
<p>k=1
Ps(k, j) (13.4.3)
</p>
<p>for any s.
</p>
<p>Definition 13.4.1 A chain possessing property (13.4.1) is called ergodic.
</p>
<p>The numbers πj are essentially the probabilities that the system will be in the
</p>
<p>respective states Ej after a long period of time has passed. It turns out that these
</p>
<p>probabilities lose dependence on the initial state of the system. The system &ldquo;forgets&rdquo;
</p>
<p>where it began its motion. The distribution {πj } is called stationary or invariant.
Property (13.4.2) expresses the invariance of the distribution with respect to the
</p>
<p>transition probabilities pij . In other words, if P(Xn = k)= πk , then P(Xn+1 = k)=&sum;
πjpjk is also equal to πk .</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Limit Theorems for Countable Homogeneous Chains 405
</p>
<p>Proof of Theorem 13.4.1 Sufficiency in the first assertion of the theorem. Consider
the &ldquo;trajectory&rdquo; of the Markov chain starting at a fixed state Es . Let τ1 &ge; 1, τ2 &ge; 1,
. . . be the time intervals between successive returns of the system to Es . Since after
</p>
<p>each return the evolution of the system begins anew from the same state, by the
</p>
<p>Markov property the durations τk of the cycles (as well as the cycles themselves)
</p>
<p>are independent and identically distributed, τk
d= τ (s). Moreover, it is obvious that
</p>
<p>P(τk = n)= P
(
τ (s) = n
</p>
<p>)
= fs(n).
</p>
<p>Recurrence of Es means that the τk are proper random variables. Aperiodicity
</p>
<p>of Es means that the g.c.d. of all possible values of τk is equal to 1. Since
</p>
<p>pss(n)= P
(
γ (n)= 0
</p>
<p>)
,
</p>
<p>where γ (n) is the defect of level n for the renewal process {Tk},
</p>
<p>Tk =
k&sum;
</p>
<p>i=1
τi,
</p>
<p>by Theorem 10.3.1 the following limit exists
</p>
<p>lim
n&rarr;&infin;
</p>
<p>pss(n)= lim
n&rarr;&infin;
</p>
<p>P
(
γ (n)= 0
</p>
<p>)
= 1
</p>
<p>Eτ1
&gt; 0. (13.4.4)
</p>
<p>Now prove the existence of limn&rarr;&infin; psj (n) for j 
= s. If γ (n) is the defect of level
n for the walk {Tk} then, by the total probability formula,
</p>
<p>psj (n)=
n&sum;
</p>
<p>k=1
P
(
γ (n)= k
</p>
<p>)
P
(
Xn = j |X0 = s, γ (n)= k
</p>
<p>)
. (13.4.5)
</p>
<p>Note that the second factors in the terms on the right-hand side of this formula do
</p>
<p>not depend on n by the Markov property:
</p>
<p>P
(
Xn = j |X0 = s,γ (n)= k
</p>
<p>)
</p>
<p>= P(Xn = j |X0 = s,Xn&minus;1 
= s, . . . ,Xn&minus;k+1 
= s,Xn&minus;k = s)
</p>
<p>= P(Xk = j |X0 = s,X1 
= s, . . . ,Xk&minus;1 
= s)=
Ps(k, j)
</p>
<p>P(τ1 &ge; k)
,
</p>
<p>(13.4.6)
</p>
<p>since, for a fixed X0 = s,
</p>
<p>P(Xk = j |X1 
= s, . . . ,Xk&minus;1 
= s) =
P(Xk = j,X1 
= s, . . . ,Xk&minus;1 
= s)
</p>
<p>P(X1 
= s, . . . ,Xk&minus;1 
= s)
</p>
<p>= Ps(k, j)
P(τ (s) &ge; k) .</p>
<p/>
</div>
<div class="page"><p/>
<p>406 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>For the sake of brevity, put P(τ1 &gt; k) = Pk . The first factors in (13.4.5) converge,
as n&rarr;&infin;, to Pk&minus;1/Eτ1 and, by virtue of the equality
</p>
<p>P
(
γ (n)= k
</p>
<p>)
= P
</p>
<p>(
γ (n&minus; k)= 0
</p>
<p>)
Pk&minus;1 &le; Pk&minus;1, (13.4.7)
</p>
<p>are dominated by the convergent sequence Pk&minus;1. Therefore, by the dominated con-
vergence theorem, the following limit exists
</p>
<p>lim
n&rarr;&infin;
</p>
<p>psj (n)=
&infin;&sum;
</p>
<p>k=1
</p>
<p>Pk&minus;1
Eτ1
</p>
<p>Ps(k, j)
</p>
<p>P(τ1 &ge; k)
= 1
</p>
<p>Eτ1
</p>
<p>&infin;&sum;
</p>
<p>k=1
Ps(k, j)=: πj , (13.4.8)
</p>
<p>and we have, by (13.4.5)&ndash;(13.4.7),
</p>
<p>psj (n)&le;
n&sum;
</p>
<p>k=1
Ps(k, j)&le;
</p>
<p>&infin;&sum;
</p>
<p>k=1
Ps(k, j)= πjEτ1. (13.4.9)
</p>
<p>To establish that, for any i,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>pij (n)= πj &gt; 0,
</p>
<p>we first show that the system departing from Ei will, with probability 1, eventually
</p>
<p>reach Es .
</p>
<p>In other words, if fis(n) is the probability that the system, upon leaving Ei , hits
</p>
<p>Es for the first time on the n-th step then
</p>
<p>&infin;&sum;
</p>
<p>n=1
fis(n)= 1.
</p>
<p>Indeed, both states Ei and Es are recurrent. Consider the cycles formed by sub-
</p>
<p>sequent visits of the system to the state Ei . Denote by Ak the event that the system
</p>
<p>is in the state Es at least once during the k-th cycle. By the Markov property the
</p>
<p>events Ak are independent and P(Ak) &gt; 0 does not depend on k. Therefore, by the
</p>
<p>Borel&ndash;Cantelli zero&ndash;one law (see Sect. 11.1), with probability 1 there will occur
</p>
<p>infinitely many events Ak and hence P(
⋃
</p>
<p>Ak)= 1.
By the total probability formula,
</p>
<p>pij (n)=
n&sum;
</p>
<p>k=1
fis(k)psj (n&minus; k),
</p>
<p>and the dominated convergence theorem yields
</p>
<p>lim
n&rarr;&infin;
</p>
<p>pij (n)=
&infin;&sum;
</p>
<p>n=1
fis(k)πj = πj .
</p>
<p>Representation (13.4.3) follows from (13.4.8).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Limit Theorems for Countable Homogeneous Chains 407
</p>
<p>Now we will prove the necessity in the first assertion of the theorem. That con-
ditions (II)&ndash;(III) are necessary is obvious, since pij (n) &gt; 0 for every i and j if n
</p>
<p>is large enough. The necessity of condition (I) follows from the fact that equalities
</p>
<p>(13.4.4) are valid for Es . The first part of the theorem is proved.
</p>
<p>It remains to prove the second part of the theorem. Since
</p>
<p>&sum;
psj (n)= 1,
</p>
<p>one has
&sum;
</p>
<p>j πj &le; 1. By virtue of the inequalities psj (n) &le; πjEτ1 (see (13.4.9)),
we can use the dominated convergence theorem both in the last equality and in the
</p>
<p>equality psj (n+ 1)=
&sum;&infin;
</p>
<p>k=0 psk(n)pkj which yields
</p>
<p>&sum;
πj = 1, πj =
</p>
<p>&infin;&sum;
</p>
<p>k=0
πkpkj .
</p>
<p>It remains to show that the system has a unique solution. Let the numbers {qj } also
satisfy (13.4.2) and assume the series
</p>
<p>&sum;
|qj | converges. Then, changing the order
</p>
<p>of summation, we obtain that
</p>
<p>qj =
&sum;
</p>
<p>k
</p>
<p>qkpkj =
&sum;
</p>
<p>k
</p>
<p>pkj
</p>
<p>(&sum;
</p>
<p>l
</p>
<p>plkql
</p>
<p>)
=
&sum;
</p>
<p>l
</p>
<p>ql
&sum;
</p>
<p>k
</p>
<p>plkpkj =
&sum;
</p>
<p>l
</p>
<p>qlplj (2)
</p>
<p>=
&sum;
</p>
<p>l
</p>
<p>plj (2)
</p>
<p>(&sum;
</p>
<p>m
</p>
<p>pmlqm
</p>
<p>)
=
&sum;
</p>
<p>m
</p>
<p>qmpmj (3)= &middot; &middot; &middot; =
&sum;
</p>
<p>k
</p>
<p>qkpkj (n)
</p>
<p>for any n. Since
&sum;
</p>
<p>qk = 1, passing to the limit as n&rarr;&infin; gives
</p>
<p>qj =
&sum;
</p>
<p>k
</p>
<p>qkπj = πj .
</p>
<p>The theorem is proved. �
</p>
<p>If a Markov chain is periodic with period d , then pij (t)= 0 for t 
= kd and every
pair of states Ei and Ej belonging to the same subclass (see Theorem 13.2.3). But
</p>
<p>if t = kd , then from the theorem just proved and Theorem 13.2.3 it follows that the
limit limk&rarr;&infin; pij (kd)= πj &gt; 0 exists and does not depend on i.
</p>
<p>Verifying conditions (II)&ndash;(III) of Theorem 13.4.1 usually presents no serious dif-
</p>
<p>ficulties. The main difficulties would be related to verifying condition (I). For finite
</p>
<p>Markov chains, this condition is always met.
</p>
<p>Theorem 13.4.2 Let a Markov chain have finitely many states and satisfy conditions
(II)&ndash;(III). Then there exist c &gt; 0 and q &lt; 1 such that, for the recurrence time τ to
an arbitrary fixed state, one has
</p>
<p>P(τ &gt; n) &lt; cqn, n&ge; 1. (13.4.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>408 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>These equalities clearly mean that condition (I) is always met for finite chains
and hence the ergodic theorem for them holds if and only if conditions (II)&ndash;(III) are
satisfied.
</p>
<p>Proof Consider a state Es and put
</p>
<p>rj (n) := P(Xk 
= s, k = 1,2, . . . , n|X0 = j).
</p>
<p>Then, if the chain has m states one has rj (m) &lt; 1 for any j . Indeed, rj (n) does
</p>
<p>not grow as n increases. Let N be the smallest number satisfying rj (N) &lt; 1. This
</p>
<p>means that there exists a sequence of states Ej , Ej1, . . . ,EjN such that EjN = Es
and the probability of this sequence pjj1 &middot; &middot; &middot;pjN&minus;1jN is positive. But it is easy to
see that N &le;m, since otherwise this sequence would contain at least two identical
states. Therefore the cycle contained between these states could be removed from
</p>
<p>the sequence which could only increase its probability. Thus
</p>
<p>rj (m) &lt; 1, r(m)= max
j
</p>
<p>rj (m) &lt; 1.
</p>
<p>Moreover, rj (n1 + n2)&le; rj (n1)r(n2)&le; r(n1)r(n2).
It remains to note that if τ is the recurrence time to Es , then P(τ &gt; nm) =
</p>
<p>rs(nm)&le; r(m)n. The statement of the theorem follows. �
</p>
<p>Remark 13.4.1 Condition (13.4.10) implies the exponential rate of convergence of
the differences |pij (n) &minus; πj | to zero. One can verify this by making use of the
analyticity of the function
</p>
<p>Fs(z)=
&infin;&sum;
</p>
<p>n=1
fs(n)z
</p>
<p>n
</p>
<p>in the domain |z|&lt; q&minus;1, q&minus;1 &gt; 1, and of the equality
</p>
<p>Ps(z)=
&sum;
</p>
<p>pss(n)z
n = 1
</p>
<p>1 &minus; Fs(z)
&minus; 1 (13.4.11)
</p>
<p>(see Theorem 13.2.1; we assume that the τ in condition (13.4.10) refers to the state
</p>
<p>Es , so that fs(n)= P(τ = n)). Since F &prime;s(1)= Eτ = 1/πs , one has
</p>
<p>Fs(z)= 1 +
(z&minus; 1)
πs
</p>
<p>+ &middot; &middot; &middot; ,
</p>
<p>and from (13.4.11) it follows that the function
</p>
<p>Ps(z)&minus;
zπs
</p>
<p>1 &minus; z =
&infin;&sum;
</p>
<p>n=1
</p>
<p>(
pss(n)&minus; πs
</p>
<p>)
zn
</p>
<p>is analytic in the disk |z| &le; 1 + ε, ε &gt; 0. It evidently follows from this that
</p>
<p>|pss(n)&minus; πs |&lt; c(1 + ε)&minus;n, c= const.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Limit Theorems for Countable Homogeneous Chains 409
</p>
<p>Now we will give two examples of finite Markov chains.
</p>
<p>Example 13.4.1 Suppose that the behaviour of two chess players A and B playing
in a multi-player tournament can be described as follows. Independently of the out-
</p>
<p>comes of the previous games, player A wins every new game with probability p,
</p>
<p>loses with probability q , and makes a tie with probability r = 1&minus;p&minus;q . Player B is
less balanced. He wins a game with probabilities p+ ε, p and p&minus; ε, respectively, if
he won, made a tie, or lost in the previous one. The probability that he loses behaves
</p>
<p>in a similar way: in the above three cases, it equals q &minus; ε, q and q + ε, respectively.
Which of the players A and B will score more points in a long tournament?
</p>
<p>To answer this question, we will need to compute the stationary probabilities
</p>
<p>π1, π2, π3 of the states E1, E2, E3 which represent a win, tie, and loss in a game,
</p>
<p>respectively (cf. the law of large numbers at the end of this section).
</p>
<p>For player A, the Markov chain with states E1,E2,E3 describing his perfor-
</p>
<p>mance in the tournament will have the matrix of transition probabilities
</p>
<p>PA =
</p>
<p>⎛
⎝
p r q
</p>
<p>p r q
</p>
<p>p r q
</p>
<p>⎞
⎠ .
</p>
<p>It is obvious that π1 = p, π2 = r , π3 = q here.
For player B , the matrix of transition probabilities is equal to
</p>
<p>PB =
</p>
<p>⎛
⎝
p+ ε r q &minus; ε
p r q
</p>
<p>p&minus; ε r q + ε
</p>
<p>⎞
⎠ .
</p>
<p>Equations for stationary probabilities in this case have the form
</p>
<p>π1(p+ ε)+ π2p+ π3(p&minus; ε) = π1,
π1r + π2r + π3r = π2,
</p>
<p>π1 + π2 + π3 = 1.
</p>
<p>Solving this system we find that
</p>
<p>π2 &minus; r = 0, π1 &minus; p = ε
p&minus; q
1 &minus; 2ε .
</p>
<p>Thus, the long run proportions of ties will be the same for both players, and B will
</p>
<p>have a greater proportion of wins if ε &gt; 0, p &gt; q or ε &lt; 0, p &lt; q . If p = q , then the
stationary distributions will be the same for both A and B .
</p>
<p>Example 13.4.2 Consider the summation of independent integer-valued random
variables ξ1, ξ2, . . . modulo some d &gt; 1 (see Example 13.1.2). Set X0 := 0, X1 :=
ξ1 &minus; &lfloor;ξ1/d&rfloor;d , X2 :=X1 + ξ2 &minus; &lfloor;(X1 + ξ2)/d&rfloor;d etc. (here &lfloor;x&rfloor; denotes the integral</p>
<p/>
</div>
<div class="page"><p/>
<p>410 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>part of x), so that Xn is the remainder of the division of Xn&minus;1 + ξn by d . Such sum-
mation is sometimes also called summation on a circle (points 0 and d are glued
</p>
<p>together in a single point). Without loss of generality, we can evidently suppose that
</p>
<p>ξk takes the values 0,1, . . . , d &minus; 1 only. If P(ξk = j)= pj then
</p>
<p>pij = P(Xn = j |Xn&minus;1 = i)=
{
pj&minus;i if j &ge; i,
pd+j&minus;i if j &lt; i.
</p>
<p>Assume that the set of all indices k with pk &gt; 0 has a g.c.d. equal to 1. Then it is
</p>
<p>clear that the chain {Xn} has a single class of essential states without subclasses,
and there will exist the limits
</p>
<p>lim
n&rarr;&infin;
</p>
<p>pij (n)= πj
</p>
<p>satisfying the system
&sum;
</p>
<p>i πipij = πj ,
&sum;
</p>
<p>πj = 1, j = 0, . . . , d &minus; 1. Now note that
the stochastic matrix of transition probabilities ‖pij‖ has in this case the following
property:
</p>
<p>&sum;
</p>
<p>i
</p>
<p>pij =
&sum;
</p>
<p>j
</p>
<p>pij = 1.
</p>
<p>Such matrices are called doubly stochastic. Stationary distributions for them are
always uniform, since πj = 1/d satisfy the system for final probabilities.
</p>
<p>Thus summation of arbitrary random variables on a circle leads to the uniform
limit distribution. The rate of convergence of pij (k) to the stationary distribution is
exponential.
</p>
<p>It is not difficult to see that the convolution of two uniform distributions under
</p>
<p>addition modulo d is also uniform. The uniform distribution is in this sense stable.
</p>
<p>Moreover, the convolution of an arbitrary distribution with the uniform distribution
</p>
<p>will also be uniform. Indeed, if η is uniformly distributed and independent of ξ1
then (addition and subtraction are modulo d , pj = P(ξ1 = j))
</p>
<p>P(ξ1 + η= k)=
d&minus;1&sum;
</p>
<p>j=0
pjP(η= k &minus; j)=
</p>
<p>d&minus;1&sum;
</p>
<p>j=0
pj
</p>
<p>1
</p>
<p>d
= 1
</p>
<p>d
.
</p>
<p>Thus, if one transmits a certain signal taking d possible values (for example,
</p>
<p>letters) and (uniform) &ldquo;random&rdquo; noise is superimposed on it, then the received signal
</p>
<p>will also have the uniform distribution and therefore will contain no information
</p>
<p>about the transmitted signal. This fact is widely used in cryptography.
</p>
<p>This example also deserves attention as a simple illustration of laws that appear
</p>
<p>when summing random variables taking values not in the real line but in some group
</p>
<p>(the set of numbers 0,1, . . . , d &minus; 1 with addition modulo d forms a finite Abelian
group). It turns out that the phenomenon discovered in the example&mdash;the uniformity
</p>
<p>of the limit distribution&mdash;holds for a much broader class of groups.
</p>
<p>We return to arbitrary countable chains. We have already mentioned that the main
</p>
<p>difficulties when verifying the conditions of Theorem 13.4.1 are usually related to</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Limit Theorems for Countable Homogeneous Chains 411
</p>
<p>condition (I). We consider this problem in Sect. 13.7 in more detail for a wider
</p>
<p>class of chains (see Theorems 13.7.2&ndash;13.7.3 and corollaries thereafter). Sometimes
</p>
<p>condition (I) can easily be verified using the results of Chaps. 10 and 12.
</p>
<p>Example 13.4.3 We saw in Sect. 12.5 that waiting times in the queueing system
satisfy the relationships
</p>
<p>Xn+1 = max(Xn + ξn+1,0), w1 = 0,
</p>
<p>where the ξn are independent and identically distributed. Clearly, Xn form a ho-
</p>
<p>mogeneous Markov chain with the state space {0,1, . . .}, provided that the ξk are
integer-valued. The sequence Xn may be interpreted as a walk with a delaying
screen at the point 0. If Eξk &lt; 0 then it is not hard to derive from the theorems
of Chap. 10 (see also Sect. 13.7) that the recurrence time to 0 has finite expectation.
</p>
<p>Thus, applying the ergodic theorem we can, independently of Sect. 11.4, come to
</p>
<p>the conclusion that there exists a limiting (stationary) distribution for Xn as n&rarr;&infin;
(or, taking into account what we said in Sect. 11.4, conclude that supk&ge;0 Sk is finite,
</p>
<p>where Sk =
&sum;k
</p>
<p>j=1 ξj , which is essentially the assertion of Theorem 10.2.1).
</p>
<p>Now we will make several remarks allowing us to state one more criterion for
</p>
<p>ergodicity which is related to the existence of a solution to Eq. (13.4.2).
</p>
<p>First of all, note that Theorem 13.2.2 (the solidarity theorem) can now be com-
</p>
<p>plemented as follows. A state Ej is said to be ergodic if, for any i, pij (n)&rarr; πj &gt; 0
as n&rarr;&infin;. A state Ej is said to be positive recurrent if it is recurrent and non-null
(in that case, the recurrence time τ (j) to Ej has finite expectation Eτ
</p>
<p>(j) &lt;&infin;). It
follows from Theorem 13.4.1 that, for an irreducible aperiodic chain, a state Ej is
ergodic if and only if it is positive recurrent. If at least one state is ergodic, all states
are.
</p>
<p>Theorem 13.4.3 Suppose a chain is irreducible and aperiodic (satisfies conditions
(II)&ndash;(III)). Then only one of the following two alternatives can take place: either all
the states are null or they are all ergodic. The existence of an absolutely convergent
solution to system (13.4.2) is necessary and sufficient for the chain to be ergodic.
</p>
<p>Proof The first assertion of the theorem follows from the fact that, by the local
renewal Theorem 10.2.2 for the random walk generated by the times of the chain&rsquo;s
</p>
<p>hitting the state Ej , the limit limn&rarr;&infin; pjj (n) always exists and equals (Eτ (j))&minus;1.
Therefore, to prove sufficiency in the second assertion (the necessity follows
</p>
<p>from Theorem 13.4.1) we have, in the case of the existence of an absolutely con-
</p>
<p>vergent solution {πj }, to exclude the existence of null states. Assume the contrary,
pij (n)&rarr; 0. Choose j such that πj &gt; 0. Then
</p>
<p>0 &lt; πj =
&sum;
</p>
<p>πipij (n)&rarr; 0
</p>
<p>as n&rarr;&infin; by dominated convergence. This contradiction completes the proof of the
theorem. �</p>
<p/>
</div>
<div class="page"><p/>
<p>412 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>13.4.2 The Law of Large Numbers and the Central Limit Theorem
</p>
<p>for the Number of Visits to a Given State
</p>
<p>In conclusion of this section we will give two assertions about the limiting be-
</p>
<p>haviour, as n &rarr; &infin;, of the number mj (n) of visits of the system to a fixed state
Ej by the time n. Let τ
</p>
<p>(j) be the recurrence time to the state Ej .
</p>
<p>Theorem 13.4.4 Let the chain be ergodic and, at the initial time epoch, be at an
arbitrary state Es . Then, as n&rarr;&infin;,
</p>
<p>Emj (n)
</p>
<p>n
&rarr; πj ,
</p>
<p>mj (n)
</p>
<p>n
</p>
<p>a.s.&minus;&rarr; πj .
</p>
<p>If additionally Var(τ (j))= σ 2j &lt;&infin; then
</p>
<p>P
</p>
<p>(
mj (n)&minus; nπj
σj
</p>
<p>&radic;
nπ3j
</p>
<p>&lt; x|X0 = s
)
&rarr;Φ(x)
</p>
<p>as n &rarr; &infin;, where Φ(x) is, as before, the distribution function of the normal law
with parameters (0,1).
</p>
<p>Proof Note that the sequence mj (n)+ 1 coincides with the renewal process formed
by the random variables τ1, τ2, τ3, . . . , where τ1 is the time of the first visit to the
</p>
<p>state Ej by the system which starts at Es and τk
d= τ (j) for k &ge; 2. Clearly, by the
</p>
<p>Markov property all τj are independent. Since τ1 &ge; 0 is a proper random variable,
Theorem 13.4.4 is a simple consequence of the generalisations of Theorems 10.1.1,
</p>
<p>11.5.1, and 10.5.2 that were stated in Remarks 10.1.1, 11.5.1 and 10.5.1, respec-
</p>
<p>tively.
</p>
<p>The theorem is proved. �
</p>
<p>Summarising the contents of this section, one can note that studying the se-
</p>
<p>quences of dependent trials forming homogeneous Markov chains with discrete sets
</p>
<p>of states can essentially be carried out with the help of results obtained for sequences
</p>
<p>of independent random variables. Studying other types of dependent trials requires,
</p>
<p>as a rule, other approaches.
</p>
<p>13.5* The Behaviour of Transition Probabilities for Reducible
</p>
<p>Chains
</p>
<p>Now consider a finite Markov chain of the general type. As we saw, its state space
consists of the class of inessential states S0 and several classes S1, . . . , Sl of es-
</p>
<p>sential states. To clarify the nature of the asymptotic behaviour of pij (n) for such</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 The Behaviour of Transition Probabilities for Reducible Chains 413
</p>
<p>Fig. 13.4 The structure of
</p>
<p>the matrix of transition
</p>
<p>probabilities of a periodic
</p>
<p>Markov chain with the class
</p>
<p>S0 of inessential states: an
</p>
<p>illustration to the proof of
</p>
<p>Theorem 13.2.3
</p>
<p>chains, it suffices to consider the case where essential states constitute a single class
</p>
<p>without subclasses (l = 1). Here, the matrix of transition probabilities pij (n) has
the form depicted in Fig. 13.4.
</p>
<p>By virtue of the ergodic theorem, the entries of the submatrix L have positive
</p>
<p>limits πj . Thus it remains to analyse the behaviour of the entries in the upper part
</p>
<p>of the matrix.
</p>
<p>Theorem 13.5.1 Let Ei &isin; S0. Then
</p>
<p>lim
t&rarr;&infin;
</p>
<p>pij (t)=
{
</p>
<p>0, if Ej &isin; S0,
πj &gt; 0, if Ej &isin; S1.
</p>
<p>Proof Let Ej &isin; S0. Set
</p>
<p>Aj (t) := max
Ei&isin;S0
</p>
<p>pij (t).
</p>
<p>For any essential state Er there exists an integer tr such that pir(tr ) &gt; 0. Since
</p>
<p>transition probabilities in L are all positive starting from some step, there exists an s
</p>
<p>such that pil(s) &gt; 0 for Ei &isin; S0 and all El &isin; S1. Therefore, for sufficiently large t ,
</p>
<p>pij (t)=
&sum;
</p>
<p>Ek&isin;S0
pik(s)pkj (t &minus; s)&le;Aj (t &minus; s)
</p>
<p>&sum;
</p>
<p>Ek&isin;S0
pik(s),
</p>
<p>where
</p>
<p>q(i) :=
&sum;
</p>
<p>Ek&isin;S0
pik(s)= 1 &minus;
</p>
<p>&sum;
</p>
<p>Ek&isin;S1
pik(s) &lt; 1.
</p>
<p>If we put q := maxEi&isin;S0 q(i), then the displayed inequality implies that
</p>
<p>Aj (t)&le; qAj (t &minus; s)&le; &middot; &middot; &middot; &le; q[t/s].
</p>
<p>Thus limt&rarr;&infin; pij (t)&le; limt&rarr;&infin;Aj (t)= 0.
Now let Ei &isin; S0 and Ej &isin; S1. One has
</p>
<p>pij (t + s)=
&sum;
</p>
<p>k
</p>
<p>pik(t)pkj (s)=
&sum;
</p>
<p>Ek&isin;S0
pik(t)pkj (s)+
</p>
<p>&sum;
</p>
<p>Ek&isin;S1
pik(t)pkj (s).</p>
<p/>
</div>
<div class="page"><p/>
<p>414 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Letting t and s go to infinity, we see that the first sum in the last expression is o(1).
</p>
<p>In the second sum,
</p>
<p>&sum;
</p>
<p>E&isin;S1
pik(t)= 1 + o(1); pkj (t)= πj + o(1).
</p>
<p>Therefore
</p>
<p>pij (t + s)= πj
&sum;
</p>
<p>Ek&isin;S
pik(t)+ o(t)= πj + o(1)
</p>
<p>as t , s &rarr;&infin;. The theorem is proved. �
</p>
<p>Using Theorem 13.5.1, it is not difficult to see that the existence of the limit
</p>
<p>lim
t&rarr;&infin;
</p>
<p>pij (n)= πj &ge; 0
</p>
<p>is a necessary and sufficient condition for the chain to have two classes S0 and S1,
</p>
<p>of which S1 contains no subclasses.
</p>
<p>13.6 Markov Chains with Arbitrary State Spaces. Ergodicity of
</p>
<p>Chains with Positive Atoms
</p>
<p>13.6.1 Markov Chains with Arbitrary State Spaces
</p>
<p>The Markov chains X = {Xn} considered so far have taken values in the count-
able sets {1,2, . . .} or {0,1, . . .}; such chains are called countable (denumerable)
or discrete. Now we will consider Markov chains with values in an arbitrary set of
states X endowed with a σ -algebra BX of subsets of X. The pair (X,BX) forms
</p>
<p>a (measurable) state space of the chain {Xn}. Further let (Ω,F,P) be the underly-
ing probability space. A measurable mapping Y of the space (Ω,F) into (X,BX) is
</p>
<p>called an X-valued random element. If X=R and BX is the σ -algebra of Borel sets
on the line, then Y will be a conventional random variable. The mapping Y could
</p>
<p>be the identity, in which case (Ω,F)= (X,BX) is also called a sample space.
Consider a sequence {Xn} of X-valued random elements and denote by Fk,m,
</p>
<p>m &ge; k, the σ -algebra generated by the elements Xk, . . . ,Xm (i.e. by events of
the form {Xk &isin; Bk}, . . . , {Xm &isin; Bm}, Bi &isin; BX, i = k, . . . ,m). It is evident that
Fn := F0,n form a non-decreasing sequence F0 &sub; F1 . . . &sub; Fn . . . . The conditional
expectation E(ξ |Fk,m) will sometimes also be denoted by E(ξ |Xk, . . . ,Xm).
</p>
<p>Definition 13.6.1 An X-valued Markov chain is a sequence of X-valued elements
Xn such that, for any B &isin;BX,
</p>
<p>P(Xn+1 &isin; B | Fn)= P(Xn+1 &isin; B |Xn) a.s. (13.6.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Chains with Arbitrary State Spaces. Ergodicity 415
</p>
<p>In the sequel, the words &ldquo;almost surely&rdquo; will, as a rule, be omitted.
</p>
<p>By the properties of conditional expectations, relation (13.6.1) is clearly equiva-
</p>
<p>lent to the condition: for any measurable function f :X&rarr;R, one has
</p>
<p>E
(
f (Xn+1) | Fn
</p>
<p>)
= E
</p>
<p>(
f (Xn+1) |Xn
</p>
<p>)
. (13.6.2)
</p>
<p>Definition 13.6.1 is equivalent to the following.
</p>
<p>Definition 13.6.2 A sequence X = {Xn} forms a Markov chain if, for any A &isin;
Fn+1,&infin;,
</p>
<p>P(A|Fn)= P(A|Xn) (13.6.3)
or, which is the same, for any Fn+1,&infin;-measurable function f (ω),
</p>
<p>E
(
f (ω)|Fn
</p>
<p>)
= E
</p>
<p>(
f (ω)|Xn
</p>
<p>)
. (13.6.4)
</p>
<p>Proof of equivalence We have to show that (13.6.2) implies (13.6.3). First take any
B1,B2 &isin;BX and let A := {Xn+1 &isin; B1,Xn+2 &isin; B2}. Then, by virtue of (13.6.2),
</p>
<p>P(A|Fn) = E
[
I(Xn+1 &isin; B1)P(Xn+2 &isin; B2|Fn+1)|Fn
</p>
<p>]
</p>
<p>= E
[
I(Xn+1 &isin; B1)P(Xn+2 &isin; B2|Xn+1)|Fn
</p>
<p>]
</p>
<p>= E(A|Xn).
</p>
<p>This implies inequality (13.6.3) for any A &isin; An+1,n+2, where Ak,m is the algebra
generated by sets {Xk &isin; Bk, . . . , Xm &isin; Bm}. It is clear that An+1,n+2 generates
Fn+1,n+2. Now let A &isin; Fn+1,n+2. Then, by the approximation theorem, there exist
Ak &isin; An+1,n+2 such that d(A,Ak) &rarr; 0 (see Sect. 3.4). From this it follows that
I(Ak)
</p>
<p>p&rarr; I(A) and, by the properties of conditional expectations (see Sect. 4.8.2),
</p>
<p>P
(
Ak|F&lowast;
</p>
<p>) p&rarr; P
(
A|F&lowast;
</p>
<p>)
,
</p>
<p>where F&lowast; &sub; F is some σ -algebra. Put PA = PA(ω) := P(A|Xn). We know that, for
Ak &isin;An+1,n+2,
</p>
<p>E(PAk ;B)= P(AkB) (13.6.5)
for any B &isin; Fn (this just means that PAk (ω) = P(Ak|Fn)). Again making use of
the properties of conditional expectations (the dominated convergence theorem, see
</p>
<p>Sect. 4.8.2) and passing to the limit in (13.6.5), we obtain that E(PA; B)= P(AB).
This proves (13.6.3) for A &isin; Fn+1,n+2.
</p>
<p>Repeating the above argument m times, we prove (13.6.3) for A &isin; Fn+1,m. Using
a similar scheme, we can proceed to the case of A &isin; Fn+1,&infin;. �
</p>
<p>Note that (13.6.3) can easily be extended to events A &isin; Fn,&infin;. In the above proof
of equivalence, one could work from the very beginning with A &isin; Fn,&infin; (first with
A &isin;An,n+2, and so on).
</p>
<p>We will give one more equivalent definition of the Markov property.</p>
<p/>
</div>
<div class="page"><p/>
<p>416 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Definition 13.6.3 A sequence {Xn} forms a Markov chain if, for any events A &isin; Fn
and B &isin; Fn,&infin;,
</p>
<p>P(AB|Xn)= P(A|Xn)P(B|Xn). (13.6.6)
</p>
<p>This property means that the future is conditionally independent of the past given
</p>
<p>the present (conditional independence of Fn and Fn,&infin; given Xn).
</p>
<p>Proof of the equivalence Assume that (13.6.4) holds. Then, for A &isin; Fn and B &isin;
Fn,&infin;,
</p>
<p>P(AB|Xn) = E
[
E(IAIB |Fn)|Xn
</p>
<p>]
= E
</p>
<p>[
IAE(IB |Fn)|Xn
</p>
<p>]
</p>
<p>= E
[
IAE(IB |Xn)|Xn
</p>
<p>]
= E(IB |Xn)E(IA|Xn),
</p>
<p>where IA is the indicator of the event A.
</p>
<p>Conversely, let (13.6.6) hold. Then
</p>
<p>P(AB) = EP(AB|Xn)= EP(A|Xn)P(B|Xn)
= EE[IAP(B|Xn)|Xn] = EIAP(B|Xn).
</p>
<p>(13.6.7)
</p>
<p>On the other hand,
</p>
<p>P(AB)= EIAIB = EIA P(B|Fn). (13.6.8)
</p>
<p>Since (13.6.7) and (13.6.8) hold for any A &isin; Fn, this means that
</p>
<p>P(B|Xn)= P(B|Fn). �
</p>
<p>Thus, let {Xn} be an X-valued Markov chain. Then, by the properties of condi-
tional expectations,
</p>
<p>P(Xn+1 &isin; B|Xn)= P(n)(Xn,B),
</p>
<p>where the function Pn(x,B) is, for each B &isin;BX, measurable in x with respect to
the σ -algebra BX. In what follows, we will assume that the functions P(n)(x,B)
</p>
<p>are conditional distributions (see Definition 4.9.1), i.e., for each x &isin; X, P(n)(x,B)
is a probability distribution in B . Conditional distributions P(n)(x,B) always exist
</p>
<p>if the σ -algebra BX is countably-generated, i.e. generated by a countable collec-
</p>
<p>tion of subsets of X (see [27]). This condition is always met if X = Rk and BX
is the σ -algebra of Borel sets. In our case, there is an additional problem that the
</p>
<p>&ldquo;null probability&rdquo; sets N &sub; X, on which one can arbitrarily vary P(n)(x,B), can
depend on the distribution of Xn, since the &ldquo;null probability&rdquo; is with respect to the
</p>
<p>distribution of Xn.
</p>
<p>Definition 13.6.4 A Markov chain X = {Xn} is called homogeneous if there ex-
ist conditional distributions P(n)(x,B) = P(x,B) independent of n and the initial
value X0 (or the distributions of Xn). The function P(x,B) is called the transition</p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Chains with Arbitrary State Spaces. Ergodicity 417
</p>
<p>probability (or transition function) of the homogeneous Markov chain. It can be
graphically written as
</p>
<p>P(x,B)= P(X1 &isin; B|X0 = x). (13.6.9)
</p>
<p>If the Markov chain is countable, X= {1,2, . . .}, then, in the notation of Sect. 13.1,
one has P(i, {j})= pij = pij (1).
</p>
<p>The transition probability and initial distribution (of X0) completely determine
</p>
<p>the joint distribution of X0, . . . ,Xn for any n. Indeed, by the total probability for-
</p>
<p>mula and the Markov property
</p>
<p>P(X0 &isin; B0, . . . ,Xn &isin; Bn)
</p>
<p>=
&int;
</p>
<p>y0&isin;B0
&middot; &middot; &middot;
</p>
<p>&int;
</p>
<p>yn&isin;Bn
P(X0 &isin; dy0)P (y0, dy1) &middot; &middot; &middot;P(yn&minus;1, dyn).
</p>
<p>(13.6.10)
</p>
<p>A Markov chain with the initial value X0 = x will be denoted by {Xn(x)}.
In applications, Markov chains are usually given by their conditional distribu-
</p>
<p>tions P(x,B) or&mdash;in a &ldquo;stronger form&rdquo;&mdash;by explicit formulas expressing Xn+1
in terms Xn and certain &ldquo;control&rdquo; elements (see Examples 13.4.2, 13.4.3, 13.6.1,
</p>
<p>13.6.2, 13.7.1&ndash;13.7.3) which enable one to immediately write down transition
</p>
<p>probabilities. In such cases, as we already mentioned, the joint distribution of
</p>
<p>(X0, . . . ,Xn) can be defined in terms of the initial distribution of X0 and the transi-
</p>
<p>tion function P(x,B) by formula (13.6.10). It is easily seen that the sequence {Xn}
with so defined joint distributions satisfy all the definitions of a Markov chain and
</p>
<p>has transition function P(x,B). In what follows, wherever it is needed, we will as-
</p>
<p>sume condition (13.6.10) is satisfied. It can be considered as one more definition of
</p>
<p>a Markov chain, but a stronger one than Definitions 13.6.2&ndash;13.6.4, for it explicitly
</p>
<p>gives (or uses) the transition function P(x,B).
</p>
<p>One of the main objects of study will be the asymptotic behaviour of the n step
</p>
<p>transition probability:
</p>
<p>P (x,n,B) := P
(
Xn(x) &isin; B
</p>
<p>)
= P(Xn &isin; B|X0 = x).
</p>
<p>The following recursive relation, which follows from the total probability formula
</p>
<p>(or from (13.6.10)), holds for this function:
</p>
<p>P(Xn+1 &isin; B)= EE
(
I(Xn+1 &isin; B)|Fn
</p>
<p>)
=
&int;
</p>
<p>P(Xn &isin; dy)P (y,B),
</p>
<p>P (x,n+ 1,B)=
&int;
</p>
<p>P(x,n, dy)P (y,B). (13.6.11)
</p>
<p>Now note that the Markov property (13.6.3) of homogeneous chains can also be
</p>
<p>written in the form
</p>
<p>P(Xn+k &isin; Bk|Fn)= P (Xn, k,Bk),</p>
<p/>
</div>
<div class="page"><p/>
<p>418 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>or, more generally,
</p>
<p>P(Xn+1 &isin; B1, . . . ,Xn+k &isin; Bk|Fn)= P
(
Xnew1 (Xn) &isin; B1, . . . ,Xnewk (Xn) &isin; Bk
</p>
<p>)
,
</p>
<p>(13.6.12)
</p>
<p>where {Xnewk (x)} is a Markov chain independent of {Xn} and having the same tran-
sition function as {Xn} and the initial value x. Property (13.6.12) can be extended
to a random time n. Recall the definition of a stopping time.
</p>
<p>Definition 13.6.5 A random variable ν &ge; 0 is called a Markov or stopping time with
respect to {Fn} if {ν &le; n} &isin; Fn. In other words, that the event {ν &le; n} occurred or
not is completely determined by the trajectory segment X0,X1, . . . ,Xn.
</p>
<p>Note that, in Definition 13.6.5, by Fn one often understands wider σ -algebras,
</p>
<p>the essential requirements being the relations {ν &le; n} &isin; Fn and measurability of
X0, . . . ,Xn with respect to Fn.
</p>
<p>Denote by Fν the σ -algebra of events B such that B &cap; {ν = k} &isin; Fk . In other
words, Fν can be thought of as the σ -algebra generated by the sets {ν = k}Bk ,
Bk &isin; Fk , i.e. by the trajectory of {Xn} until time ν.
</p>
<p>Lemma 13.6.1 (The Strong Markov Property) For any k &ge; 1 andB1, . . . ,Bk &isin;BX ,
</p>
<p>P(Xν+1 &isin; B1, . . . ,Xν+k &isin; Bk|Fν)= P
(
Xnew1 (Xν) &isin; B1, . . . ,Xnewk (Xν) &isin; Bk
</p>
<p>)
,
</p>
<p>where the process {Xnewk } is defined in (13.6.12).
</p>
<p>Thus, after a random stopping time ν, the trajectory Xν+1, Xν+2, . . . will evolve
according to the same laws as X1,X2, . . . , but with the initial condition Xν . This
</p>
<p>property is called the strong Markov property. It will be used below for the first
hitting times ν = τV of certain sets V &sub; X by {Xn}. We have already used this
property tacitly in Sect. 13.4, when the set V coincided with a point, which allowed
</p>
<p>us to cut the trajectory of {Xn} into independent cycles.
</p>
<p>Proof of Lemma 13.6.1 For the sake of simplicity, consider one-dimensional distri-
butions. We have to prove that
</p>
<p>P(Xν+1 &isin; B1|Fν)= P(Xν,B1).
</p>
<p>For any A &isin; Fν ,
</p>
<p>E
(
P(Xν,B1);A
</p>
<p>)
=
</p>
<p>&sum;
</p>
<p>n
</p>
<p>E
(
P(Xn,B1);A{ν = n}
</p>
<p>)
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p>EE
(
I
(
A{ν = n}{Xn+1 &isin; B1}
</p>
<p>)
|Fn
</p>
<p>)
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p>P
(
A{ν = n}{Xn+1 &isin; B1}
</p>
<p>)
= P
</p>
<p>(
A{Xν+1 &isin; B1}
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Chains with Arbitrary State Spaces. Ergodicity 419
</p>
<p>But this just means that P(Xν,B1) is the required conditional expectation. The case
</p>
<p>of multi-dimensional distributions is dealt with in the same way, and we leave it to
</p>
<p>the reader. �
</p>
<p>Now we turn to consider the asymptotic properties of distributions P(x,n,B) as
</p>
<p>n&rarr;&infin;.
</p>
<p>Definition 13.6.6 A distribution π(&middot;) on (X,BX) is called invariant if it satisfies
the equation
</p>
<p>π(B)=
&int;
</p>
<p>π(dy)P (y,B), B &isin;BX . (13.6.13)
</p>
<p>It follows from (13.6.11) that if Xn &sub;= π , then Xn+1 &sub;= π . The distribution π is
also called stationary.
</p>
<p>For Markov chains in arbitrary state spaces X, a simple and complete classifica-
</p>
<p>tion similar to the one carried out for countable chains in Sect. 13.1 is not possible,
</p>
<p>although some notions can be extended to the general case.
</p>
<p>Such natural and important notions for countable chains as, say, irreducibility of
</p>
<p>a chain, take in the general case another form.
</p>
<p>Example 13.6.1 Let Xn+1 = Xn + ξn (mod 1) (Xn+1 is the fractional part of
Xn + ξn), ξn be independent and identically distributed and take with positive prob-
abilities the two values 0 and
</p>
<p>&radic;
2. In this example, the chain &ldquo;splits&rdquo;, according
</p>
<p>to the initial state x, into a continual set of &ldquo;subchains&rdquo; with state spaces of the
</p>
<p>form Mx = {x + k
&radic;
</p>
<p>2 (mod 1), k = 0,1,2 . . .}. It is evident that if x1 &minus; x2 is not a
multiple of
</p>
<p>&radic;
2 (mod 1), then Mx1 and Mx2 are disjoint, P(Xn(x1) &isin;Mx2)= 0 and
</p>
<p>P(Xn(x2) &isin;Mx1)= 0 for all n. Thus the chain is clearly reducible. Nevertheless, it
turns out that the chain is ergodic in the following sense: for any x, Xn(x)&sub;=&rArr;U0,1
(P(x,n, [0, t])&rarr; t) as n&rarr;&infin; (see, e.g., [6], [18]). For the most commonly used
irreducibility conditions, see Sect. 13.7.
</p>
<p>Definition 13.6.7 A chain is called periodic if there exist an integer d &ge; 2 and a
set X1 &sub; X such that, for x &isin; X1, one has P(x,n,X1) = P(Xn(x) &isin; X1) = 1 for
n= kd , k = 1,2, . . . , and P(x,n,X1)= 0 for n 
= kd .
</p>
<p>Periodicity means that the whole set of states X is decomposed into subclasses
</p>
<p>X1, . . . ,Xd , such that P(X1(x) &isin; Xk+1)= 1 for x &isin; Xk , k = 1, . . . , d , Xd+1 = X1.
In the absence of such a property, the chain will be called aperiodic.
</p>
<p>A state x0 &isin;X is called an atom of the chain X if, for any x &isin;X,
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>n=1
</p>
<p>{
Xn(x)= x0
</p>
<p>}
)
= 1.
</p>
<p>Example 13.6.2 Let X0 &ge; 0 and, for n&ge; 0,
</p>
<p>Xn+1 =
{
(Xn + ξn+1)+ if Xn &gt; 0,
ηn+1 if Xn = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>420 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>where ξn and ηn &ge; 0, n= 1,2, . . . , are two sequences of independent random vari-
ables, identically distributed in each sequence. It is clear that {Xn} is a Markov chain
and, for Eξk &lt; 0, by the strong law of large numbers, this chain has an atom at the
</p>
<p>point x0 = 0:
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>n=1
</p>
<p>{
Xn(x)= 0
</p>
<p>}
)
= P
</p>
<p>(
inf
k
Sk &le;&minus;x
</p>
<p>)
= 1,
</p>
<p>where Sk =
&sum;k
</p>
<p>j=1 ξj . This chain is a generalisation of the Markov chain from Ex-
ample 13.4.3.
</p>
<p>Markov chains in an arbitrary state space X are rather difficult to study. However,
</p>
<p>if a chain has an atom, the situation may become much simpler, and the ergodic
</p>
<p>theorem on the asymptotic behaviour of P(x,n,B) as n&rarr;&infin; can be proved using
the approaches considered in the previous sections.
</p>
<p>13.6.2 Markov Chains Having a Positive Atom
</p>
<p>Let x0 be an atom of a chain {Xn}. Set
</p>
<p>τ := min
{
k &gt; 0 :Xk(x0)= x0
</p>
<p>}
.
</p>
<p>This is a proper random variable (P(τ &lt;&infin;)= 1).
</p>
<p>Definition 13.6.8 The atom x0 is said to be positive if Eτ &lt;&infin;.
</p>
<p>In the terminology of Sect. 13.4, x0 is a recurrent non-null (positive) state.
</p>
<p>To characterise convergence of distributions in arbitrary spaces, we will need the
</p>
<p>notions of the total variation distance and convergence in total variation. If P and Q
</p>
<p>are two distributions on (X,BX), then the total variation distance between them is
defined by
</p>
<p>‖P&minus;Q‖ = 2 sup
B&isin;BX
</p>
<p>∣∣P(B)&minus;Q(B)
∣∣.
</p>
<p>One says that a sequence of distributions Pn on (X,BX) converges in total variation
</p>
<p>to P (Pn
T V&minus;&rarr; P) if ‖Pn &minus; P‖&rarr; 0 as n&rarr;&infin;. For more details, see Sect. 3.6.2 of
</p>
<p>Appendix 3.
</p>
<p>As in Sect. 13.4, denote by Px0(k,B) the &ldquo;taboo probability&rdquo;
</p>
<p>Px0(k,B) := P
(
Xk(x0) &isin; B,X1(x0) 
= x0, . . . ,Xk&minus;1(x0) 
= x0
</p>
<p>)
</p>
<p>of transition from x0 into B in k steps without visiting the &ldquo;forbidden&rdquo; state x0.
</p>
<p>Theorem 13.6.1 If the chain {Xn} has a positive atom and the g.c.d. of the possible
values of τ is 1, then the chain is ergodic in the convergence in total variation sense:</p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Chains with Arbitrary State Spaces. Ergodicity 421
</p>
<p>there exists a unique invariant distribution π such that, for any x &isin;X, as n&rarr;&infin;,
∥∥P(x,n, &middot;)&minus; π(&middot;)
</p>
<p>∥∥&rarr; 0. (13.6.14)
</p>
<p>Moreover, for any B &isin;BX,
</p>
<p>π(B)= 1
Eτ
</p>
<p>&infin;&sum;
</p>
<p>k=1
Px0(k,B). (13.6.15)
</p>
<p>If we denote byXn(μ0) a Markov chain with the initial distribution μ0 (X0&sub;=μ0)
and put
</p>
<p>P(μ0, n,B) := P
(
Xn(μ0) &isin; B
</p>
<p>)
=
&int;
</p>
<p>μ0(dx)P (x,n,B),
</p>
<p>then, as well as (13.6.14), we will also have that, as n&rarr;&infin;,
∥∥P(μ0, n, &middot;)&minus; π(&middot;)
</p>
<p>∥∥&rarr; 0 (13.6.16)
</p>
<p>for any initial distribution μ0.
</p>
<p>The condition that there exists a positive atom is an analogue of conditions (I)
</p>
<p>and (II) of Theorem 13.4.1. A number of conditions sufficient for the finiteness of
</p>
<p>Eτ can be found in Sect. 13.7. The condition on the g.c.d. of possible values of τ is
</p>
<p>the aperiodicity condition.
</p>
<p>Proof We will effectively repeat the proof of Theorem 13.4.1. First let X0 = x0. As
in Theorem 13.4.1 (we keep the notation of that theorem), we find that
</p>
<p>P(x0, n,B)
</p>
<p>=
n&sum;
</p>
<p>k=1
P
(
γ (n)= k
</p>
<p>)
P(Xn &isin; B|Xn&minus;k = x0,Xn&minus;k+1 
= x0, . . . ,Xn&minus;1 
= x0)
</p>
<p>=
n&sum;
</p>
<p>k=1
</p>
<p>P(γ (n)= k)
P(τ &ge; k) P(τ &ge; k)P(Xk &isin; B|X0 = x0,X1 
= x0, . . . ,Xk&minus;1 
= x0)
</p>
<p>=
n&sum;
</p>
<p>k=1
</p>
<p>P(γ (n)= k)
P(τ &ge; k) Px0(k,B).
</p>
<p>For the measure π defined in (13.6.15) one has
</p>
<p>P(x0, n,B)&minus; π(B)
</p>
<p>=
n&sum;
</p>
<p>k=1
</p>
<p>(
P(γ (n)= k)
P(τ &ge; k) &minus;
</p>
<p>1
</p>
<p>Eτ
</p>
<p>)
Px0(k,B)&minus;
</p>
<p>1
</p>
<p>Eτ
</p>
<p>&sum;
</p>
<p>k&gt;n
</p>
<p>Px0(k,B).</p>
<p/>
</div>
<div class="page"><p/>
<p>422 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Since P(γ (n) = k) &le; P(τ &ge; k) and Px0(k,B) &le; P(τ &ge; k) (see the proof of Theo-
rem 13.4.1), one has, for any N ,
</p>
<p>sup
B
</p>
<p>∣∣P (x0, n,B)&minus; π(B)
∣∣&le;
</p>
<p>N&sum;
</p>
<p>k=1
</p>
<p>(
P(γ (n)= k)
P(τ &ge; k) &minus;
</p>
<p>1
</p>
<p>Eτ
</p>
<p>)
+ 2
</p>
<p>&sum;
</p>
<p>k&gt;N
</p>
<p>P(τ &ge; k).
</p>
<p>(13.6.17)
</p>
<p>Further, since
</p>
<p>P
(
γ (n)= k
</p>
<p>)
&rarr; P(τ &ge; k)/Eτ,
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(τ &ge; k)= Eτ &lt;&infin;,
</p>
<p>the right-hand side of (13.6.17) can be made arbitrarily small by choosing N and
</p>
<p>then n. Therefore,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>sup
B
</p>
<p>∣∣P(x0, n,B)&minus; π(B)
∣∣= 0.
</p>
<p>Now consider an arbitrary initial state x &isin;X, x 
= x0. Since x0 is an atom, for the
probabilities
</p>
<p>F(x, k, x0) := P
(
Xk(x)= x0, X1 
= x0, . . . ,Xk&minus;1 
= x0
</p>
<p>)
</p>
<p>of hitting x0 for the first time on the k-th step, one has
</p>
<p>&sum;
</p>
<p>k
</p>
<p>F(x, k, x0)= 1, P (x,n,B)=
n&sum;
</p>
<p>k=1
F(x, k, x0)P (x0, n&minus; k,B),
</p>
<p>∥∥P(x,n, &middot;)&minus; π(&middot;)
∥∥
</p>
<p>&le;
&sum;
</p>
<p>k&le;n/2
F(x, k, x0)
</p>
<p>∥∥P(x0, n&minus; k, &middot;)&minus; π(&middot;)
∥∥+ 2
</p>
<p>&sum;
</p>
<p>k&gt;n/2
</p>
<p>F(x, k, x0)&rarr; 0
</p>
<p>as n&rarr;&infin;.
Relation (13.6.16) follows from the fact that
</p>
<p>∥∥P(μ0, n, &middot;)&minus; π(&middot;)
∥∥&le;
</p>
<p>&int;
μ0(dx)
</p>
<p>∥∥P(x,n, &middot;)&minus; π(&middot;)
∥∥&rarr; 0
</p>
<p>by the dominated convergence theorem.
</p>
<p>Further, from the convergence of P(x,n, &middot;) in total variation it follows that
&int;
</p>
<p>P(x,n, dy)P (y,B)&rarr;
&int;
</p>
<p>π(dy)P (y,B).
</p>
<p>Since the left hand-side of this relation is equal to P(x,n + 1,B) by virtue of
(13.6.11) and converges to π(B), one has (13.6.13), and hence π is an invariant
</p>
<p>measure.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Ergodicity of Harris Markov Chains 423
</p>
<p>Now assume that π1 is another invariant distribution. Then
</p>
<p>π1(&middot;)= P(π1, n, &middot;)
T V&minus;&rarr; π(&middot;), π1 = π .
</p>
<p>The theorem is proved. �
</p>
<p>Returning to Example 13.6.2, we show that the conditions of Theorem 13.6.1 are
</p>
<p>met provided that Eξk &lt; 0 and Eηk &lt;&infin;. Indeed, put
</p>
<p>η(&minus;x) := min
{
k &ge; 1 : Sk =
</p>
<p>k&sum;
</p>
<p>j=1
ξj &le;&minus;x
</p>
<p>}
.
</p>
<p>By the renewal Theorem 10.1.1,
</p>
<p>H(x)= Eη(&minus;x)&sim; x|Eξ1|
as x &rarr;&infin;
</p>
<p>for Eξ1 &lt; 0, and therefore there exist constants c1 and c2 such that H(x) &lt; c1 + c2x
for all x &ge; 0. Hence, for the atom x0 = 0, we obtain that
</p>
<p>Eτ =
&int; &infin;
</p>
<p>0
</p>
<p>P(η1 &isin; dx)H(x)&le; c1 + c2
&int; &infin;
</p>
<p>0
</p>
<p>xP(η1 &isin; dx)= c1 + c2Eη1 &lt;&infin;.
</p>
<p>13.7* Ergodicity of Harris Markov Chains
</p>
<p>13.7.1 The Ergodic Theorem
</p>
<p>In this section we will consider the problem of establishing ergodicity of Markov
</p>
<p>chains in arbitrary state spaces (X,BX). A lot of research has been done on this
</p>
<p>problem, the most important advancements being associated with the names of
</p>
<p>W. D&ouml;blin, J.L. Doob, T.E. Harris and E. Omey. Until recently, this research area
</p>
<p>had been considered as a rather difficult one, and not without reason. However, the
</p>
<p>construction of an artificial atom suggested by K.B. Athreya, P.E. Ney and E. Num-
</p>
<p>melin (see, e.g. [6, 27, 29]) greatly simplified considerations and allowed the proof
</p>
<p>of ergodicity by reducing the general case to the special case discussed in the last
</p>
<p>section.
</p>
<p>In what follows, the notion of a &ldquo;Harris chain&rdquo; will play an important role. For a
</p>
<p>fixed set V &isin;BX, define the random variable
</p>
<p>τV (x)= min
{
k &ge; 1 :Xk(x) &isin; V
</p>
<p>}
,
</p>
<p>the time of the first hitting of V by the chain starting from the state x (we put
</p>
<p>τV (x)=&infin; if all Xk(x) /&isin; V ).</p>
<p/>
</div>
<div class="page"><p/>
<p>424 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Definition 13.7.1 A Markov chain X = {Xn} in (X,BX) is said to be a Harris
chain (or Harris irreducible) if there exists a set V &isin;BX, a probability measure μ
on (X,BX), and numbers n0 &ge; 1, p &isin; (0,1) such that
(I0) P(τV (x) &lt;&infin;)= 1 for all x &isin;X; and
(II) P(x,n0,B)&ge; pμ(B) for all x &isin; V , B &isin;BX.
</p>
<p>Condition (I0) plays the role of an irreducibility condition: starting from any
</p>
<p>point x &isin; X, the trajectory of Xn will sooner or later visit the set V . Condition (II)
guarantees that, after n0 steps since hitting V , the distribution of the walking particle
</p>
<p>will be minorised by a common &ldquo;distribution&rdquo; pμ(&middot;). This condition is sometimes
called a &ldquo;mixing condition&rdquo;; it ensures a &ldquo;partial loss of memory&rdquo; about the trajec-
</p>
<p>tory&rsquo;s past. This is not the case for the chain from Example 13.6.1 for which con-
</p>
<p>dition (II) does not hold for any V , μ or n0 (P (x, &middot;) form a collection of mutually
singular distributions which are singular with respect to Lebesgue measure).
</p>
<p>If a chain has an atom x0, then conditions (I0) and (II) are always satisfied for
</p>
<p>V = {x0}, n0 = 1, p = 1, and μ(&middot;)= P(x0, &middot;), so that such a chain is a Harris chain.
The set V is usually chosen to be a &ldquo;compact&rdquo; set (if X=Rk , it will be a bounded
</p>
<p>set), for otherwise one cannot, as a rule, obtain inequalities in (II). If the space X
</p>
<p>is &ldquo;compact&rdquo; itself (a finite or bounded subset of Rk), condition (II) can be met
</p>
<p>for V = X (condition (I0) then always holds). For example, if {Xn} is a finite, ir-
reducible and aperiodic chain, then by Theorem 13.4.2 there exists an n0 such that
</p>
<p>P (i, n0, j)&ge; p &gt; 0 for all i and j . Therefore condition (II) holds for V =X if one
takes μ to be a uniform distribution on X.
</p>
<p>One could interpret condition (II) as that of the presence, in all distributions
</p>
<p>P (x,n0, &middot;) for x &isin; V , of a component which is absolutely continuous with respect
to the measure μ:
</p>
<p>inf
x&isin;V
</p>
<p>P (x,n0, dy)
</p>
<p>μ(dy)
&ge; p &gt; 0.
</p>
<p>We will also need a condition of &ldquo;positivity&rdquo; (positive recurrence) of the set V
</p>
<p>(or that of &ldquo;positivity&rdquo; of the chain):
</p>
<p>(I) supx&isin;V EτV (x) &lt;&infin;,
</p>
<p>and the aperiodicity condition which will be written in the following form. Let
</p>
<p>Xk(μ) be a Markov chain with an initial value X0 &sub;= μ, where μ is from condi-
tion (II). Put
</p>
<p>τV (μ) := min
{
k &ge;:Xk(μ) &isin; V
</p>
<p>}
.
</p>
<p>It is evident that τV (μ) is, by virtue of (I0), a proper random variable. Denote by
</p>
<p>n1, n2, . . . the possible values of τV (μ), i.e. the values for which
</p>
<p>P
(
τV (μ)= nk
</p>
<p>)
&gt; 0, k = 1,2, . . . .
</p>
<p>Then the aperiodicity condition will have the following form.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Ergodicity of Harris Markov Chains 425
</p>
<p>(III) There exists a k &ge; 1 such that
</p>
<p>g.c.d.{n0 + n1, n0 + n2, . . . , n0 + nk} = 1,
</p>
<p>where n0 is from condition (II).
Condition (III) is always satisfied if (II) holds for n0 = 1 and μ(V ) &gt; 0 (then
</p>
<p>n1 = 0, n0 + n1 = 1).
Verifying condition (I) usually requires deriving bounds for EτV (x) for x /&isin; V
</p>
<p>which would automatically imply (I0) (see the examples below).
</p>
<p>Theorem 13.7.1 Suppose conditions (I0), (I), (II) and (III) are satisfied for a
Markov chain X, i.e. the chain is an aperiodic positive Harris chain. Then there
exists a unique invariant distribution π such that, for any initial distribution μ0, as
n&rarr;&infin;,
</p>
<p>∥∥P(μ0, n, &middot;)&minus; π(&middot;)
∥∥&rarr; 0. (13.7.1)
</p>
<p>The proof is based on the use of the above-mentioned construction of an &ldquo;arti-
</p>
<p>ficial atom&rdquo; and reduction of the problem to Theorem 13.6.1. This allows one to
</p>
<p>obtain, in the course of the proof, a representation for the invariant measure π simi-
</p>
<p>lar to (13.6.15) (see (13.7.5)).
</p>
<p>A remarkable fact is that the conditions of Theorem 13.7.1 are necessary for
</p>
<p>convergence (13.7.1) (for more details, see [6]).
</p>
<p>Proof of Theorem 13.7.1 For simplicity&rsquo;s sake, assume that n0 = 1. First we will
construct an &ldquo;extended&rdquo; Markov chain X&lowast; = {X&lowast;n} = {X̃n,ω(n)}, ω(n) being a se-
quence of independent identically distributed random variables with
</p>
<p>P
(
ω(n)= 1
</p>
<p>)
= p, P
</p>
<p>(
ω(n)= 0
</p>
<p>)
= 1 &minus; p.
</p>
<p>The joint distribution of (X̃(n),ω(n)) in the state space
</p>
<p>X&lowast; :=X&times; {0,1} =
{
x&lowast; = (x, δ) : x &isin;X; δ = 0,1
</p>
<p>}
</p>
<p>and the transition function P &lowast; of the chain X&lowast; are defined as follows (the notation
X&lowast;n(x
</p>
<p>&lowast;) has the same meaning as Xn(x)):
</p>
<p>P
(
X&lowast;1
</p>
<p>(
x&lowast;
</p>
<p>)
&isin; (B, δ)
</p>
<p>)
=: P &lowast;
</p>
<p>(
x&lowast;, (B, δ)
</p>
<p>)
= P(x,B) P
</p>
<p>(
ω(1)= δ
</p>
<p>)
for x /&isin; V
</p>
<p>(i.e., for X̃n /&isin; V , the components of X&lowast;n+1 are &ldquo;chosen at random&rdquo; indepen-
dently with the respective marginal distributions). But if x &isin; V , the distribution of
X&lowast;(x&lowast;,1) is given by
</p>
<p>P(X&lowast;1
(
(x,1) &isin; (B, δ)
</p>
<p>)
= P &lowast;
</p>
<p>(
(x,1), (B, δ)
</p>
<p>)
= μ(B)P
</p>
<p>(
ω(1)= δ
</p>
<p>)
,
</p>
<p>P(X&lowast;1
(
(x,0) &isin; (B, δ)
</p>
<p>)
= P &lowast;
</p>
<p>(
(x,0), (B, δ)
</p>
<p>)
=Q(x,B)P
</p>
<p>(
ω(1)= δ
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>426 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>where
</p>
<p>Q(x,B) :=
(
P(x,B)&minus; pμ(B)
</p>
<p>)
/(1 &minus; p),
</p>
<p>so that, for any B &isin;BX,
</p>
<p>pμ(B)+ (1 &minus; p)Q(x,B)= P(x,B). (13.7.2)
</p>
<p>Thus P(ω(n+ 1) = 1|X&lowast;n) = p for any values of X&lowast;n. However, when &ldquo;choosing&rdquo;
the value X̃n+1 there occurs (only when X̃n &isin; V ) a partial randomisation (or split-
ting): for X̃n &isin; V , we let P(X̃n+1 &isin; B|X&lowast;n) be equal to the value μ(B) (not depend-
ing on X̃n &isin; V !) provided that ω(n)= 1. If ω(n)= 0, then the value of the probabil-
ity is taken to be Q(X̃n,B). It is evident that, by virtue of condition (II) (for n0 = 1),
μ(B) and Q(x,B) are probability distributions, and by equality (13.7.2) the first
</p>
<p>component X̃n of the process X
&lowast;
n has the property P(X̃n+1 &isin; B| X̃n) = P(X̃n,B),
</p>
<p>and therefore the distributions of the sequences X and X̃ coincide.
</p>
<p>As we have already noted, the &ldquo;extended&rdquo; process X&lowast;(n) possesses the fol-
lowing property: the conditional distribution P(X&lowast;n+1 &isin; (B, δ)|X&lowast;n) does not de-
pend on X&lowast;(n) on the set X&lowast;n &isin; V &lowast; := (V ,1) and is there the known distribution
μ(B)P(ω(1) = δ). This just means that visits of the chain X&lowast; to the set V &lowast; divide
the trajectory of X&lowast; into independent cycles, in the same way as it happens in the
presence of a positive atom.
</p>
<p>We described above how one constructs the distribution of X&lowast; from that of X.
Now we will give obvious relations reconstructing the distribution of X from that
</p>
<p>of the chain X&lowast;:
</p>
<p>P
(
Xn(x) &isin; B
</p>
<p>)
= pP(X&lowast;n
</p>
<p>(
(x,1) &isin; B&lowast;
</p>
<p>)
+ (1 &minus; p)P
</p>
<p>(
X&lowast;n(x,0) &isin; B&lowast;
</p>
<p>)
, (13.7.3)
</p>
<p>where B&lowast; := (B,0)&cup; (B,1). Note also that, if we consider Xn = X̃n as a component
of X&lowast;n, we need to write it as a function Xn(x
</p>
<p>&lowast;) of the initial value x&lowast; &isin;X&lowast;.
Put
</p>
<p>τ &lowast; := min
{
k &ge; 1 :X&lowast;k
</p>
<p>(
x&lowast;
</p>
<p>)
&isin; V &lowast;
</p>
<p>}
, x&lowast; &isin; V &lowast; = (V ,1).
</p>
<p>It is obvious that τ &lowast; does not depend on the value x&lowast; = (x,1), since X1(x&lowast;) has
the distribution μ for any x &isin; V . This property allows one to identify the set V &lowast;
with a single point. In other words, one needs to consider one more state space X&lowast;&lowast;
</p>
<p>which is obtained from X&lowast; if we replace the set V &lowast; = (V ,1) by a point to be denoted
by x0. In the new state space, we construct a chain X
</p>
<p>&lowast;&lowast; equivalent to X&lowast; using the
obvious relations for the transition probability P &lowast;&lowast;:
</p>
<p>P &lowast;&lowast;
(
x&lowast;, (B, δ)
</p>
<p>)
:= P &lowast;
</p>
<p>(
x&lowast;, (B, δ)
</p>
<p>)
for x&lowast; 
= (V ,1)= V &lowast;, (B, d) 
= V &lowast;,
</p>
<p>P &lowast;&lowast;
(
x0, (B, δ)
</p>
<p>)
:= pμ(B), P &lowast;&lowast;
</p>
<p>(
x&lowast;, x0
</p>
<p>)
:= P &lowast;
</p>
<p>(
x&lowast;,V &lowast;
</p>
<p>)
.
</p>
<p>Thus we have constructed a chain X&lowast;&lowast; with the transition function P &lowast;&lowast;, and this
chain has atom x0. Clearly, τ
</p>
<p>&lowast; = min{k &ge; 1 :X&lowast;&lowast;k (x0)= x0}. We now prove that this
atom is positive. Put
</p>
<p>E := sup
x&isin;V
</p>
<p>EτV (x).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Ergodicity of Harris Markov Chains 427
</p>
<p>Lemma 13.7.1 Eτ &lowast; &le; 2
p
E.
</p>
<p>Proof Consider the evolution of the first component Xk(x&lowast;) of the process X&lowast;k (x
&lowast;),
</p>
<p>x&lowast; &isin; V &lowast;. Partition the time axis k &ge; 0 into intervals by hitting the set V by Xk(x&lowast;).
Let τ1 &ge; 1 be the first such hitting time (recall that X1(x&lowast;) d= X0(μ) has the dis-
tribution μ, so that τ1 = 1 if μ(V ) = 1). Prior to time τ1 (in the case τ1 &gt; 1)
transitions of Xk(x
</p>
<p>&lowast;), k &ge; 2, were governed by the transition function P(y,B),
y &isin; V c = X \ V . At time τ1, according to the definition of X&lowast;, one carries out a
Bernoulli trial independent of the past history of the process with success (which
</p>
<p>is the event ω(τ1)= 1) probability p. If ω(τ1)= 1 then τ &lowast; = τ1. If ω(τ1)= 0 then
the transition from Xτ1(x
</p>
<p>&lowast;) to Xτ1+1(x
&lowast;) is governed by the transition function
</p>
<p>Q(y,B)= (P (y,B)&minus; pμ(B))/(1 &minus; p), y &isin; V . The further evolution of the chain
is similar: if τ1 + τ2 is the time of the second visit of X(x&lowast;, k) to V (in the case
ω(τ1)= 0) then in the time interval [τ1+1, τ2] transitions of X(x&lowast;, k) occur accord-
ing to the transition function P(y,B), y &isin; V c. At time τ1 + τ2 one carries out a new
Bernoulli trial with the outcome ω(τ1 + τ2). If ω(τ1 + τ2)= 1, then τ &lowast; = τ1 + τ2.
If ω(τ1 + τ2)= 0, then the transition from X(x&lowast;, τ1 + τ2) to X(x&lowast;, τ1 + τ2 + 1) is
governed by Q(y,B), and so on.
</p>
<p>In other words, the evolution of the component Xk(x
&lowast;) of the process X&lowast;k (x
</p>
<p>&lowast;) is
as follows. Let X̃ = {X̃k}, k = 1,2, . . . , be a Markov chain with the distribution μ
at time k = 1 and transition probability Q(x,B) at times k &ge; 2,
</p>
<p>Q(x,B)=
{
(P (x,B)&minus; pμ(B))/(1 &minus; p) if x &isin; V,
P (x,B) if x &isin; V c.
</p>
<p>Define Ti as follows:
</p>
<p>T0 := 0, T1 = τ1 = min{k &ge; 1 : X̃k &isin; V },
</p>
<p>Ti := τ1 + &middot; &middot; &middot; + τi = min{k &gt; Ti&minus;1 : X̃k &isin; V }, i &ge; 2.
</p>
<p>Let, further, ν be a random variable independent of X̃ and having the geometric
</p>
<p>distribution
</p>
<p>P(ν = k)= (1 &minus; p)k&minus;1p, k &ge; 1, ν = min
{
k &ge; 1 : ω(Tk)= 1
</p>
<p>}
. (13.7.4)
</p>
<p>Then it follows from the aforesaid that the distribution of X1(x
&lowast;), . . . ,Xτ&lowast;(x&lowast;) co-
</p>
<p>incides with that of X̃1, . . . , X̃ν ; in particular, τ
&lowast; = Tν , and
</p>
<p>Eτ &lowast; =
&infin;&sum;
</p>
<p>k=1
p(1 &minus; p)k&minus;1ETk.
</p>
<p>Further, since μ(B)&le; P(x,B)/p for x &isin; V , then, for any x &isin; V ,
</p>
<p>Eτ1 = μ(V )+
&int;
</p>
<p>V c
μ(du)
</p>
<p>(
1 +EτV (u)
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>428 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>&le; 1
p
</p>
<p>[
P(x,V )+
</p>
<p>&int;
</p>
<p>V c
P(x, du)
</p>
<p>(
1 +EτV (u)
</p>
<p>)]
= EτV (x)
</p>
<p>p
&le; E
</p>
<p>p
.
</p>
<p>To bound Eτi for i &ge; 2, we note that Q(x,B) &le; (1 &minus; p)&minus;1P(x,B) for x &isin; V .
Therefore, if we denote by F(i) the σ -algebra generated by {X̃k, ω(τk)} for k &le; Ti ,
then
</p>
<p>E(τi |F(i&minus;1)) &le; sup
x&isin;V
</p>
<p>[
Q(x,V )+
</p>
<p>&int;
</p>
<p>V c
Q(x,du)
</p>
<p>(
1 +EτV (u)
</p>
<p>)]
</p>
<p>&le; 1
1 &minus; p supx&isin;V
</p>
<p>[
P(x,V )+
</p>
<p>&int;
</p>
<p>V c
P(x, du)
</p>
<p>(
1 +EτV (u)
</p>
<p>)]
</p>
<p>= (1 &minus; p)&minus;1 sup
x&isin;V
</p>
<p>EτV (x)=E(1 &minus; p)&minus;1.
</p>
<p>This implies the inequality ETk &le;E(1/p+ (k&minus;1)/(1&minus;p)), from which we obtain
that
</p>
<p>Eτ &lowast; &le;E
(
</p>
<p>1/p+ p
&infin;&sum;
</p>
<p>k=1
(k &minus; 1)(1 &minus; p)k&minus;2
</p>
<p>)
= 2E/p.
</p>
<p>The lemma is proved. �
</p>
<p>We return to the proof of the theorem. To make use of Theorem 13.6.1, we now
</p>
<p>have to show that P(τ &lowast;(x&lowast;) &lt;&infin;)= 1 for any x&lowast; &isin;X&lowast;, where
</p>
<p>τ &lowast;
(
x&lowast;
</p>
<p>)
:= min
</p>
<p>{
k &ge; 1 :X&lowast;k
</p>
<p>(
x&lowast;
</p>
<p>)
&isin; V &lowast;
</p>
<p>}
.
</p>
<p>But the chain X visits V with probability 1. After ν visits to V (ν was defined in
</p>
<p>(13.7.4)), the process X&lowast; = (X(n),ω(n)) will be in the set V &lowast;.
The aperiodicity condition for n0 = 1 will be met if μ(V ) &gt; 0. In that case we
</p>
<p>obtain by virtue of Theorem 13.6.1 that there exists a unique invariant measure π&lowast;
</p>
<p>such that, for any x&lowast; &isin;X&lowast;,
</p>
<p>∥∥P &lowast;
(
x&lowast;, n, &middot;
</p>
<p>)
&minus; π&lowast;(&middot;)
</p>
<p>∥∥&rarr; 0, π&lowast;
(
(B, δ)
</p>
<p>)
= 1
</p>
<p>Eτ &lowast;
</p>
<p>&infin;&sum;
</p>
<p>k=1
P &lowast;V &lowast;
</p>
<p>(
k, (B,d)
</p>
<p>)
,
</p>
<p>P &lowast;V &lowast;
(
k, (B, δ)
</p>
<p>)
= P
</p>
<p>(
X&lowast;k
</p>
<p>(
x&lowast;
</p>
<p>)
&isin; (B, δ),X&lowast;1
</p>
<p>(
x&lowast;
</p>
<p>)
/&isin; V &lowast;, . . . ,X&lowast;k&minus;1
</p>
<p>(
x&lowast;
</p>
<p>)
/&isin; V &lowast;
</p>
<p>)
.
</p>
<p>(13.7.5)
</p>
<p>In the last equality, we can take any point x&lowast; &isin; V &lowast;; the probability does not depend
on the choice of x&lowast; &isin; V &lowast;.
</p>
<p>From this and the &ldquo;inversion formula&rdquo; (13.7.3) we obtain assertion (13.7.1) and
</p>
<p>a representation for the invariant measure π of the process X.
</p>
<p>The proof of the convergence ‖P(μ0, n, &middot;) &minus; π(&middot;)‖ &rarr; 0 and uniqueness of the
invariant measure is exactly the same as in Theorem 13.6.1 (these facts also follow
</p>
<p>from the respective assertions for X&lowast;).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Ergodicity of Harris Markov Chains 429
</p>
<p>Verifying the conditions of Theorem 13.6.1 in the case where n0 &gt; 1 or μ(V )= 0
causes no additional difficulties and we leave it to the reader.
</p>
<p>The theorem is proved. �
</p>
<p>Note that in a way similar to that in the proof of Theorem 13.4.1, one could also
</p>
<p>establish the uniqueness of the solution to the integral equation for the invariant
</p>
<p>measure (see Definition 13.6.6) in a wider class of signed finite measures.
</p>
<p>The main and most difficult to verify conditions of Theorem 13.7.1 are undoubt-
</p>
<p>edly conditions (I) and (II). Condition (I0) is usually obtained &ldquo;automatically&rdquo;, in
</p>
<p>the course of verifying condition (I), for the latter requires bounding EτV (x) for
</p>
<p>all x. Verifying the aperiodicity condition (III) usually causes no difficulties. If, say,
</p>
<p>recurrence to the set V is possible in m1 and m2 steps and g.c.d. (m1,m2)= 1, then
the chain is aperiodic.
</p>
<p>13.7.2 On Conditions (I) and (II)
</p>
<p>Now we consider in more detail the main conditions (I) and (II). Condition (II) is
</p>
<p>expressed directly in terms of local characteristics of the chain (transition probabili-
</p>
<p>ties in one or a fixed number of steps n0 &gt; 1), and in this sense it could be treated as
</p>
<p>a &ldquo;final&rdquo; one. One only needs to &ldquo;guess&rdquo; the most appropriate set V and measure μ
</p>
<p>(of course, if there are any). For example, for multi-dimensional Markov chains in
</p>
<p>X=Rd , condition (II) will be satisfied if at least one of the following two conditions
is met.
</p>
<p>(IIa) The distribution of Xn0(x) has, for some n0 and N &gt; 0 and all x &isin; VN :=
{y : |y| &le;N}, a component which is absolutely continuous with respect to Lebesgue
measure (or to the sum of the Lebesgue measures on Rd and its &ldquo;coordinate&rdquo; sub-
spaces) and is &ldquo;uniformly&rdquo; positive on the set VM for someM &gt; 0. In this case, one
can take μ to be the uniform distribution on VM .
</p>
<p>(IIl) X = Zd is the integer lattice in Rd . In this case the chain is countable and
everything simplifies (see Sect. 13.4).
</p>
<p>We have already noted that, in the cases when a chain has a positive atom, which
</p>
<p>is the case in Example 13.6.2, no assumptions about the structure (smoothness) of
</p>
<p>the distribution of Xn0(x) are needed.
</p>
<p>The &ldquo;positivity&rdquo; condition (I) is different. It is given in terms of rather compli-
</p>
<p>cated characteristics EτV (x) requiring additional analysis and a search for condi-
</p>
<p>tions in terms of local characteristics which would ensure (I). The rest of the section
</p>
<p>will mostly be devoted to this task.
</p>
<p>First of all, we will give an &ldquo;intermediate&rdquo; assertion which will be useful for the
</p>
<p>sequel. We have already made use of such an assertion in Example 13.6.2.
</p>
<p>Theorem 13.7.2 Suppose there exists a nonnegative measurable function
g :X&rarr;R such that the following conditions (Ig) are met:
</p>
<p>(Ig)1 EτV (x)&le; c1 + c2g(x) for x &isin; V c =X \ V , c1, c2 = const.</p>
<p/>
</div>
<div class="page"><p/>
<p>430 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>(Ig)2 supx&isin;V Eg(X1(x)) &lt;&infin;.
Then conditions (I0) and (I) are satisfied.
</p>
<p>The function g from Theorem 13.7.2 is often called the test, or Lyapunov, func-
tion. For brevity&rsquo;s sake, put τV (x) := τ(x).
</p>
<p>Proof If (Ig) holds then, for x &isin; V ,
</p>
<p>Eτ(x) &le; 1 +E
[
τ
(
X1(x)
</p>
<p>)
;X1(x) &isin; V c
</p>
<p>]
</p>
<p>&le; 1 +E
(
E
[
τ
(
X1(x)
</p>
<p>)
|X1(x)
</p>
<p>]
;X1(x) &isin; V c
</p>
<p>)
</p>
<p>&le; 1 +E
(
c1 + c2g
</p>
<p>(
X1(x)
</p>
<p>)
;X1(x) &isin; V c
</p>
<p>)
</p>
<p>&le; 1 + c1 + c2 sup
x&isin;V
</p>
<p>Eg
(
X1(x)
</p>
<p>)
&lt;&infin;.
</p>
<p>The theorem is proved. �
</p>
<p>Note that condition (Ig)2, like condition (II), refers to &ldquo;local&rdquo; characteristics of
</p>
<p>the system, and in that sense it can also be treated as a &ldquo;final&rdquo; condition (up to the
</p>
<p>choice of function g).
</p>
<p>We now consider conditions ensuring (Ig)1. The processes
</p>
<p>{Xn} =
{
Xn(x)
</p>
<p>}
, X0(x)= x,
</p>
<p>to be considered below (for instance, in Theorem 13.7.3) do not need to be Marko-
</p>
<p>vian. We will only use those properties of the processes which will be stated in
</p>
<p>conditions of assertions.
</p>
<p>We will again make use of nonnegative trial functions g :X&rarr; R and consider a
set V &ldquo;induced&rdquo; by the function g and a set U which in most cases will be a bounded
</p>
<p>interval of the real line:
</p>
<p>V := g&minus;1(U)=
{
x &isin;X : g(x) &isin;U
</p>
<p>}
.
</p>
<p>The notation τ(x)= τU (x) will retain its meaning:
</p>
<p>τ(x) := min
{
k &ge; 1 : g
</p>
<p>(
Xk(x)
</p>
<p>)
&isin;U
</p>
<p>}
= min
</p>
<p>{
k &ge; 1 :Xk(x) &isin; V
</p>
<p>}
.
</p>
<p>The next assertion is an essential element of Lyapunov&rsquo;s (or the test functions)
</p>
<p>approach to the proof of positive recurrence of a Markov chain.
</p>
<p>Theorem 13.7.3 If {Xn} is a Markov chain and, for x &isin; V c ,
</p>
<p>Eg
(
X1(x)
</p>
<p>)
&minus; g(x)&le;&minus;ε, (13.7.6)
</p>
<p>then Eτ(x)&le; g(x)/ε and therefore (Ig)1 holds.
</p>
<p>To prove the theorem we need</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Ergodicity of Harris Markov Chains 431
</p>
<p>Lemma 13.7.2 If, for some ε &gt; 0, all n= 0,1,2, . . . , and any x &isin; V c,
</p>
<p>E
(
g(Xn+1)&minus; g(Xn)|τ(x) &gt; n
</p>
<p>)
&le;&minus;ε, (13.7.7)
</p>
<p>then
</p>
<p>Eτ(x)&le; g(x)
ε
</p>
<p>, x &isin; V c,
</p>
<p>and therefore (Ig)1 holds.
</p>
<p>Proof Put τ(x) := τ for brevity and set
</p>
<p>τ(N) := min(τ,N), ∆(n) := g(Xn+1)&minus; g(Xn).
</p>
<p>We have
</p>
<p>&minus;g(x)=&minus;Eg(X0)&le; E
(
g(Xτ(N))&minus; g(X0)
</p>
<p>)
</p>
<p>= E
τ(N)&minus;1&sum;
</p>
<p>n=0
∆(n)=
</p>
<p>N&sum;
</p>
<p>n=0
E∆(n)I (τ &gt; n)
</p>
<p>=
N&sum;
</p>
<p>n=0
P(τ &gt; n)E
</p>
<p>(
∆(n)|τ &gt; n
</p>
<p>)
&le;&minus;ε
</p>
<p>N&sum;
</p>
<p>n=0
P(τ &gt; n).
</p>
<p>This implies that, for any N ,
</p>
<p>N&sum;
</p>
<p>n=0
P(τ &gt; n)&le; g(x)
</p>
<p>ε
.
</p>
<p>Therefore this inequality will also hold for N =&infin;, so that Eτ &le; g(x)/ε. The lemma
is proved. �
</p>
<p>Proof of Theorem 13.7.3 The proof follows in an obvious way from the fact that, by
(13.7.6) and the homogeneity of the chain, E(g(Xn+1)&minus;g(Xn)|Xn)&le;&minus;ε holds on
{Xn &isin; V c}, and from inclusion {τ &gt; n} &sub; {Xn &isin; V c}, so that
</p>
<p>E
(
g(Xn+1)&minus;g(Xn); τ &gt; n
</p>
<p>)
= E
</p>
<p>[
E
(
g(Xn+1)&minus;g(Xn)|Xn
</p>
<p>)
; τ &gt; n
</p>
<p>]
&le;&minus;εP(τ &gt; n).
</p>
<p>The theorem is proved. �
</p>
<p>Theorem 13.7.3 is a modification of the positive recurrence criterion known as
</p>
<p>the Foster&ndash;Moustafa&ndash;Tweedy criterion (see, e.g., [6, 27]).
</p>
<p>Consider some applications of the obtained results. Let X be a Markov chain on
</p>
<p>the real half-axis R+ = [0,&infin;). For brevity&rsquo;s sake, put ξ(x) := X1(x)&minus; x. This is
the one-step increment of the chain starting at the point x; we could also define ξ(x)
</p>
<p>as a random variable with the distribution
</p>
<p>P
(
ξ(x) &isin; B
</p>
<p>)
= P(x,B &minus; x)
</p>
<p>(
B &minus; x = {y &isin;X : y + x &isin; B}
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>432 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Corollary 13.7.1 If, for some N &ge; 0 and ε &gt; 0,
</p>
<p>sup
x&le;N
</p>
<p>Eξ(x) &lt;&infin;, sup
x&gt;N
</p>
<p>Eξ(x)&le;&minus;ε, (13.7.8)
</p>
<p>then conditions (I0) and (I) hold for V = [0,N ].
</p>
<p>Proof Make use of Theorems 13.7.2, 13.7.3 and Corollary 13.3.1 with g(x) &equiv; x,
V = [0,N ]. Conditions (Ig)2 and (13.7.6) are clearly satisfied. �
</p>
<p>Thus the presence of a &ldquo;negative drift&rdquo; in the region x &gt;N guarantees positivity
</p>
<p>of the chain. However, that condition (I) is met could also be ensured when the
</p>
<p>&ldquo;drift&rdquo; Eξ(x) vanishes as x &rarr;&infin;.
</p>
<p>Corollary 13.7.2 Let supx Eξ
2(x) &lt;&infin; and
</p>
<p>Eξ2(x)&le; β, Eξ(x)&le;&minus; c
x
</p>
<p>for x &gt;N.
</p>
<p>If 2c &gt; β then conditions (I0) and (I) hold for V = [0,N].
</p>
<p>Proof We again make use of Theorems 13.7.2 and 13.7.3, but with g(x)= x2. We
have for x &gt;N :
</p>
<p>Eg
(
X1(x)
</p>
<p>)
&minus; g(x)= E
</p>
<p>(
2xξ(x)+ ξ2(x)
</p>
<p>)
&le;&minus;2c+ β &lt; 0. �
</p>
<p>Before proceeding to examples related to ergodicity we note the following. The
</p>
<p>&ldquo;larger&rdquo; the set V the easier it is to verify condition (I), and the &ldquo;smaller&rdquo; that set,
</p>
<p>the easier it is to verify condition (II). In this connection there arises the question
</p>
<p>of when one can consider two sets: a &ldquo;small&rdquo; set W and a &ldquo;large&rdquo; set V &sup;W such
that if (I) holds for V and (II) holds for W then both (I) and (II) would hold for W .
</p>
<p>Under conditions of Sect. 13.6 one can take W to be a &ldquo;one-point&rdquo; atom x0.
</p>
<p>Lemma 13.7.3 Let sets V and W be such that the condition
</p>
<p>(IV ) E := sup
x&isin;V
</p>
<p>EτV (x) &lt;&infin;
</p>
<p>holds and there exists an m such that
</p>
<p>inf
x&isin;V
</p>
<p>P
</p>
<p>(
m⋃
</p>
<p>j=1
</p>
<p>{
Xj (x) &isin;W
</p>
<p>}
)
&ge; q &gt; 0.
</p>
<p>Then the following condition is also met:
</p>
<p>(IW ) sup
x&isin;W
</p>
<p>EτW (x)&le; sup
x&isin;V
</p>
<p>EτW (x)&le;
mE
</p>
<p>q
.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Ergodicity of Harris Markov Chains 433
</p>
<p>Thus, under the assumptions of Lemma 13.7.3, if condition (I) holds for V and
</p>
<p>condition (II) holds for W , then conditions (I) and (II) hold for W .
</p>
<p>To prove Lemma 13.7.3, we will need the following assertion extending (in the
</p>
<p>form of an inequality) the well-known Wald identity.
</p>
<p>Assume we are given a sequence of nonnegative random variables τ1, τ2, . . .
</p>
<p>which are measurable with respect to σ -algebras U1 &sub; U2 &sub; &middot; &middot; &middot; , respectively, and
let Tn := τ1 + &middot; &middot; &middot; + τn. Furthermore, let ν be a given stopping time with respect to
{Un}: {ν &le; n} &isin; Un.
</p>
<p>Lemma 13.7.4 If E(τn|Un&minus;1)&le; a then ETν &le; aEν.
</p>
<p>Proof We can assume without loss of generality that Eν &lt; &infin; (otherwise the in-
equality is trivial). The proof essentially repeats that of Theorem 4.4.1. One has
</p>
<p>Eτν =
&infin;&sum;
</p>
<p>k=1
E(Tk;ν = k)=
</p>
<p>&infin;&sum;
</p>
<p>k=1
E(τk, ν &ge; k). (13.7.9)
</p>
<p>Changing the summation order here is well-justified, for the summands are nonneg-
</p>
<p>ative. Further, {ν &le; k &minus; 1} &isin; Uk&minus;1 and hence {ν &ge; k} &isin; Uk&minus;1. Therefore
</p>
<p>E(τk;ν &ge; k)= EI(ν &ge; k)E(τk|Uk&minus;1)&le; aP(ν &ge; k).
</p>
<p>Comparing this with (13.7.9) we get
</p>
<p>ETν &le; a
&infin;&sum;
</p>
<p>k=1
P(ν &ge; k)= aEnu.
</p>
<p>The lemma is proved. �
</p>
<p>Proof of Lemma 13.7.3 Suppose the chain starts at a point x &isin; V . Consider the
times T1, T2, . . . of successive visits of X to V , T0 = 0. Put Y0 := x, Yk :=XTk (x),
k = 1,2, . . . . Then, by virtue of the strong Markov property, the sequence (Yk, Tk)
will form a Markov chain. Set Uk := σ(T1, . . . , Tk;Y1, . . . , Yk), τk := Tk &minus; Tk&minus;1,
k = 1,2 . . . . Then ν := min{k : Yk &isin;W } is a stopping time with respect to {Uk}. It is
evident that E(τk|Uk&minus;1)&le;E. Bound Eν. We have
</p>
<p>pk := P(ν &ge; km)&le; P
(
</p>
<p>Tkm⋂
</p>
<p>j=1
{Xj /&isin;W }
</p>
<p>)
</p>
<p>= EI
( T(k&minus;1)m⋂
</p>
<p>j=1
{Xj /&isin;W }
</p>
<p>)
E
</p>
<p>(
I
</p>
<p>(
Tkm⋂
</p>
<p>j=T(k&minus;1)m+1
{Xj /&isin;W }
</p>
<p>)∣∣∣U(k&minus;1)m
)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>434 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Since τj &ge; 1, the last factor, by the assumptions of the lemma and the strong
Markov property, does not exceed
</p>
<p>P
</p>
<p>(
m⋂
</p>
<p>j=1
</p>
<p>{
Xnewj (XT(k&minus;1)m) /&isin;W
</p>
<p>}
)
&le; (1 &minus; q),
</p>
<p>where, as before, Xnewk (x) is a chain with the same distribution as Xk(x) but in-
</p>
<p>dependent of the latter chain. Thus pk &le; (1 &minus; q)pk&minus;1 &le; (1 &minus; q)k , Eν &le; m/q , and
by Lemma 13.7.4 we have ETν &le;Em/q . It remains to notice that τW (x)= Tν . The
lemma is proved. �
</p>
<p>Example 13.7.1 A random walk with reflection. Let ξ1, ξ2, . . . be independent iden-
tically distributed random variables,
</p>
<p>Xn+1 := |Xn + ξn+1|, n= 0,1, . . . . (13.7.10)
</p>
<p>If the ξk and hence the Xk are non-arithmetic, then the chain X has, generally
</p>
<p>speaking, no atoms. If, for instance, ξk have a density f (t) with respect to Lebesgue
</p>
<p>measure then P(Xk(x)= y)= 0 for any x, y, k &ge; 1. We will assume that a broader
condition (A) holds:
</p>
<p>(A). In the decomposition
</p>
<p>P(ξk &lt; t)= pa Fa(t)+ pc Fc(t)
</p>
<p>of the distribution of ξk into the absolutely continuous (Fa) and singular (Fc) (in-
cluding discrete) components, one has pa &gt; 0.
</p>
<p>Corollary 13.7.3 If condition (A) holds, a = Eξk &lt; 0, and E|ξk| &lt; &infin;, then the
Markov chain defined in (13.7.10) satisfies the conditions of Theorem 13.7.2 and
therefore is ergodic in the sense of convergence in total variation.
</p>
<p>Proof We first verify that the chain satisfies the conditions of Corollary 13.7.1.
Since in our case |X1(x) &minus; x| &le; |ξ1|, the first of conditions (13.7.8) is satisfied.
Further,
</p>
<p>Eξ(x)= E|x + ξ1| &minus; x = E(ξ1; ξ1 &ge;&minus;x)&minus;E(2x + ξ1; ξ1 &lt;&minus;x)&rarr; Eξ1
</p>
<p>as x &rarr;&infin;, since
</p>
<p>xP(ξ1 &lt;&minus;x)&le; E
(
|ξ1|, |ξ1|&gt; x
</p>
<p>)
&rarr; 0.
</p>
<p>Hence there exists an N such that Eξ(x) &le; a/2 &lt; 0 for x &ge; N . This proves that
conditions (I0) and (I) hold for V = [0,N ].
</p>
<p>Now verify that condition (II) holds for the set W = [0, h] with some h. Let f (t)
be the density of the distribution Fa from condition (A). There exist an f0 &gt; 0 and
</p>
<p>a segment [t1, t2], t2 &gt; t1, such that f (t) &gt; f0 for t &isin; [t1, t2]. The density of x + ξ1</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Ergodicity of Harris Markov Chains 435
</p>
<p>will clearly be greater than f0 on [x + t1, x + t2]. Put h := (t2 &minus; t1)/2. Then, for
0 &le; x &le; h, one will have [t2 &minus; h, t2] &sub; [x + t1, x + t2].
</p>
<p>Suppose first that t2 &gt; 0. The aforesaid will then mean that the density of x + ξ1
will be greater than f0 on [(t2 &minus; h)+, t2] for all x &le; h and, therefore,
</p>
<p>inf
x&le;h
</p>
<p>P
(
X1(x) &isin; B
</p>
<p>)
&ge; p1
</p>
<p>&int;
</p>
<p>B
</p>
<p>f0(t) dt,
</p>
<p>where
</p>
<p>f0(t)=
{
f0 ift &isin; [(t2 &minus; h)+, t2],
0 otherwise.
</p>
<p>This means that condition (II) is satisfied on the set W = [0, h]. The case t2 &le; 0 can
be considered in a similar way.
</p>
<p>It remains to make use of Lemma 13.7.3 which implies that condition (I) will
</p>
<p>hold for the set W . The condition of Lemma 13.7.3 is clearly satisfied (for suffi-
</p>
<p>ciently large m, the distribution of Xm(x), x &le;N , will have an absolutely continu-
ous component which is positive on W ). For the same reason, the chain X cannot be
</p>
<p>periodic. Thus all conditions of Theorem 13.7.2 are met. The corollary is proved. �
</p>
<p>Example 13.7.2 An oscillating random walk. Suppose we are given two indepen-
dent sequences ξ1, ξ2, . . . and η1, η2, . . . of independent random variables, identi-
</p>
<p>cally distributed in each of the sequences. Put
</p>
<p>Xn+1 :=
{
Xn + ξn+1 if Xn &ge; 0,
Xn + ηn+1 if Xn &lt; 0.
</p>
<p>(13.7.11)
</p>
<p>Such a random walk is called oscillating. It clearly forms a Markov chain in the
state space X= (&minus;&infin;,&infin;).
</p>
<p>Corollary 13.7.4 If at least one of the distributions of ξk or ηk satisfies condition
(A) and &minus;&infin; &lt; Eξk &lt; 0, &infin; &gt; Eηk &gt; 0, then the chain (13.7.11) will satisfy the
conditions of Theorem 13.7.2 and therefore will be ergodic.
</p>
<p>Proof The argument is quite similar to the proof of Corollary 13.7.3. One just needs
to take, in order to verify condition (I), g(x)= |x| and V = [&minus;N,N ]. After that it
remains to make use of Lemma 13.7.3 with W = [0, h] if condition (A) is satisfied
for ξk (and with W = [&minus;h,0) if it is met for ηk). �
</p>
<p>Note that condition (A) in Examples 13.7.1 and 13.7.2 can be relaxed to that of
</p>
<p>the existence of an absolutely continuous component for the distribution of the sum&sum;m
j=1 ξj (or
</p>
<p>&sum;m
j=1 ηj ) for some m. On the other hand, if the distributions of these
</p>
<p>sums are singular for all m, then convergence of distributions P(x,n, &middot;) in total vari-
ation cannot take place. If, for instance, one has P(ξk =&minus;
</p>
<p>&radic;
2)= P(ξk = 1)= 1/2 in
</p>
<p>Example 13.7.1, then Eξk &lt; 0 and condition (I) will be met, while condition (II) will</p>
<p/>
</div>
<div class="page"><p/>
<p>436 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>not. Convergence of P(x,n, &middot;) in total variation to the limiting distribution π is also
impossible. Indeed, it follows from the equation for the invariant distribution π that
</p>
<p>this distribution is necessarily continuous. On the other hand, say, the distributions
</p>
<p>P(0, n, &middot;) are concentrated on the countable set N of the numbers | &minus; k
&radic;
</p>
<p>2 + l|;
k, l = 1,2, . . . . Therefore P(0, n,N) = 1 for all n, π(N) = 0. Hence only weak
convergence of the distributions P(x,n, &middot;) to π(&middot;) may take place. And although this
convergence does not raise any doubts, we know no reasonably simple proof of this
</p>
<p>fact.
</p>
<p>Example 13.7.3 (continuation of Examples 13.4.2 and 13.6.1) Let X = [0,1],
ξ1, ξ2, . . . be independent and identically distributed, and Xn+1 := Xn + ξn+1
(mod 1) or, which is the same, Xn+1 := {Xn + ξn+1}, where {x} denotes the frac-
tional part of x. Here, condition (I) is clearly met for V =X= [0,1]. If the ξk satisfy
condition (A) then, as was the case in Example 13.7.1, condition (II) will be met for
</p>
<p>the set W = [0, h] with some h &gt; 0, which, together with Lemma 13.7.3, will mean,
as before, that the conditions of Theorem 13.7.2 are satisfied. The invariant distri-
</p>
<p>bution π will in this example be uniform on [0,1]. For simplicity&rsquo;s sake, we can
assume that the distribution of ξk has a density f (t), and without loss of generality
</p>
<p>we can suppose that ξk &isin; [0,1] (f (t)= 0 for t /&isin; [0,1]). Then the density p(x)&equiv; 1
of the invariant measure π will satisfy the equation for the invariant measure:
</p>
<p>p(x)= 1 =
&int; x
</p>
<p>0
</p>
<p>dy f (x &minus; y)+
&int; 1
</p>
<p>x
</p>
<p>dy f (x &minus; y + 1)=
&int; 1
</p>
<p>0
</p>
<p>f (y)dy.
</p>
<p>Since the stationary distribution is unique, one has π = U0,1. Moreover, by The-
orem A3.4.1 of Appendix 3, along with convergence of P(x,n, &middot;) to U0,1 in total
variation, convergence of the densities P(x,n, dt)/dt to 1 in (Lebesgue) measure
</p>
<p>will take place.
</p>
<p>The fact that the invariant distribution is uniform remains true for arbitrary
</p>
<p>non-lattice distributions of ξk . However, as we have already mentioned in Exam-
</p>
<p>ple 13.6.1, in the general case (without condition (A)) only weak convergence of
</p>
<p>the distributions P(x,n, &middot;) to the uniform distribution is possible (see [6, 18]).
</p>
<p>13.8 Laws of Large Numbers and the Central Limit Theorem for
</p>
<p>Sums of Random Variables Defined on a Markov Chain
</p>
<p>13.8.1 Random Variables Defined on a Markov Chain
</p>
<p>Let, as before, X = {Xn} be a Markov Chain in an arbitrary measurable state space
〈X ,BX 〉 defined in Sect. 13.6, and let a measurable function f : X &rarr; R be given
on 〈X ,BX 〉. The sequence of sums
</p>
<p>Sn :=
n&sum;
</p>
<p>k=1
f (Xk) (13.8.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8 Limit Theorems for Sums of Random Variables 437
</p>
<p>is a generalisation of the random walks that were studied in Chaps. 8 and 11. One
</p>
<p>can consider an even more general problem on the behaviour of sums of random
variables defined on a Markov chain. Namely, we will assume that a collection
of distributions {Fx} is given which depend on the parameter x &isin; X . If F (&minus;1)x (t)
is the quantile transform of Fx and ω &sub;= U0,1, then ξx := F (&minus;1)x (ω) will have the
distribution Fx (see Sect. 3.2.4).
</p>
<p>The mapping Fx of the space X into the set of distributions is assumed to be such
</p>
<p>that the function ξx(t)= F (&minus;1)x (t) is measurable on X &times;R with respect to BX &times;B,
where B is the σ -algebra of Borel sets on the real line. In this case, ξx(ω) will be a
</p>
<p>random variable such that the moments
</p>
<p>Eξ sx =
&int; &infin;
</p>
<p>&minus;&infin;
vsdFx(v)=
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>[
F (&minus;1)x (u)
</p>
<p>]s
du
</p>
<p>are measurable with respect to BX (and hence will be random variables themselves
</p>
<p>if we set a distribution on 〈X ,BX 〉).
</p>
<p>Definition 13.8.1 If ωi &sub;=U0,1 are independent then the sequence
</p>
<p>ξXn := F
(&minus;1)
Xn
</p>
<p>(ωn), n= 0,1, . . . ,
</p>
<p>is called a sequence of random variables defined on the Markov chain {Xn}.
The basic objects of study in this section are the asymptotic properties of the
</p>
<p>distributions of the sums
</p>
<p>Sn :=
n&sum;
</p>
<p>k=0
ξXk . (13.8.2)
</p>
<p>If the distribution Fx is degenerate and concentrated at the point f (x) then
</p>
<p>(13.8.2) turns into the sum (13.8.1). If the chain X is countable with states
</p>
<p>E0,E1, . . . and f (x) = I(Ej ) then Sn = mj (n) is the number of visits to the state
Ej by the time n considered in Theorem 13.4.4.
</p>
<p>13.8.2 Laws of Large Numbers
</p>
<p>In this and the next subsection we will confine ourselves to Markov chains satis-
</p>
<p>fying the ergodicity conditions from Sects. 13.6 and 13.7. As was already noticed,
</p>
<p>ergodicity conditions for Harris chains mean, in essence, the existence of a positive
</p>
<p>atom (possibly in the extended state space). Therefore, for the sake of simplicity, we
</p>
<p>will assume from the outset that the chain X has a positive atom at a point x0 and
</p>
<p>put, as before,
</p>
<p>τ(x) := min
{
k &ge; 0 :Xk(x)= x0
</p>
<p>}
, τ (x0)= τ.
</p>
<p>Summing up the conditions sufficient for (I0) and (I) to hold (the finiteness of τ(x)
</p>
<p>and Eτ ) studied in Sect. 13.7, we obtain the following assertion in our case.</p>
<p/>
</div>
<div class="page"><p/>
<p>438 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Corollary 13.8.1 Let there exist a set V &isin; BX such that, for the stopping time
τV (x) := min{k :Xk(x) &isin; V }, we have
</p>
<p>E := sup
x&isin;V
</p>
<p>EτV (x) &lt;&infin;. (13.8.3)
</p>
<p>Furthermore, let there exist an m&ge; 1 such that
</p>
<p>inf
x&isin;V
</p>
<p>P
</p>
<p>(
m⋃
</p>
<p>j=1
</p>
<p>{
Xj (x)= x0
</p>
<p>}
)
&ge; q &gt; 0.
</p>
<p>Then
</p>
<p>Eτ &le; mE
q
</p>
<p>.
</p>
<p>This assertion follows from Lemma 13.7.2. One can justify conditions (I0) and
</p>
<p>(13.8.3) by the following assertion.
</p>
<p>Corollary 13.8.2 Let there exist an ε &gt; 0 and a nonnegative measurable function
g :X &rarr;R such that
</p>
<p>sup
x&isin;V
</p>
<p>Eg
(
X1(x)
</p>
<p>)
&lt;&infin;
</p>
<p>and, for x &isin; V c,
Eg
</p>
<p>(
X1(x)
</p>
<p>)
&minus; g(x)&le;&minus;ε.
</p>
<p>Then conditions (I0) and (13.8.3) are met.
</p>
<p>In order to formulate and prove the law of large numbers for the sums (13.8.2), we
</p>
<p>will use the notion of the increment of the sums (13.8.2) on a cycle between conse-
</p>
<p>quent visits of the chain to the atom x0. Divide the trajectory X0,X1,X2, . . . ,Xn of
</p>
<p>the chain X on the time interval [0, n] into segments of lengths τ1 := τ(x), τ2, τ3, . . .
(τj
</p>
<p>d= τ for j &ge; 2) corresponding to the visits of the chain to the atom x0. Denote
the increment of the sum Sn on the k-th cycle (on (Tk&minus;1, Tk]) by ζk :
</p>
<p>ζ1 :=
τ1&sum;
</p>
<p>j=0
ξXj ,
</p>
<p>ζk :=
Tk&sum;
</p>
<p>j=Tk&minus;1+1
ξXj , k &ge; 2, where Tk :=
</p>
<p>k&sum;
</p>
<p>j=1
τj , k &ge; 1, T0 = 0.
</p>
<p>(13.8.4)
</p>
<p>The vectors (τk, ζk), k &ge; 2, are clearly independent and identically distributed. For
brevity, the index k will sometimes be omitted: (τk, ζk)
</p>
<p>d= (τ, ζ ) for k &ge; 2.
Now we can state the law of large numbers for the sums (13.8.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8 Limit Theorems for Sums of Random Variables 439
</p>
<p>Theorem 13.8.1 Let P(τ (x) &lt;&infin;)= 1 for all x, Eτ &lt;&infin;, E|ζ |&lt;&infin;, and the g.c.d.
of all possible values of τ equal 1. Then
</p>
<p>Sn
</p>
<p>n
= 1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>k=1
ξXk
</p>
<p>p&rarr; Eζ
Eτ
</p>
<p>as n&rarr;&infin;.
</p>
<p>Proof Put
</p>
<p>ν(n) := max{k : Tk &le; n}.
Then the sum Sn can be represented as
</p>
<p>Sn = ζ1 +Zν(n) + zn, (13.8.5)
</p>
<p>where
</p>
<p>Zk :=
k&sum;
</p>
<p>j=2
ζj , zn :=
</p>
<p>n&sum;
</p>
<p>j=Tν(n)+1
ξXj .
</p>
<p>Since τ1 and ζ1 are proper random variables, we have, as n&rarr;&infin;,
</p>
<p>ζ1
</p>
<p>n
</p>
<p>a.s.&minus;&rarr; 0. (13.8.6)
</p>
<p>The sum zn consists of γ (n) := n&minus; Tν(n) summands. Theorem 10.3.1 implies that
the distribution of γ (n) converges to a proper limiting distribution, and the same is
</p>
<p>true for zn. Hence, as n&rarr;&infin;,
zn
</p>
<p>n
</p>
<p>p&rarr; 0. (13.8.7)
</p>
<p>The sums Zν(n), being the main part of (13.8.5), are nothing else but a generalised
</p>
<p>renewal process corresponding to the vectors (τ, ζ ) (see Sect. 10.6).
</p>
<p>Since Eτ &lt;&infin;, by Theorem 11.5.2, as n&rarr;&infin;,
</p>
<p>Zν(n)
</p>
<p>n
</p>
<p>p&rarr; Eζ
Eτ
</p>
<p>. (13.8.8)
</p>
<p>Together with (13.8.6) and (13.8.7) this means that
</p>
<p>Sn
</p>
<p>n
</p>
<p>p&rarr; Eζ
Eτ
</p>
<p>. (13.8.9)
</p>
<p>The theorem is proved. �
</p>
<p>As was already noted, sufficient conditions for P(τ (x) &lt;&infin;)= 1 and Eτ &lt;&infin; to
hold are contained in Corollaries 13.8.1 and 13.8.2. It is more difficult to find con-
</p>
<p>ditions sufficient for Eζ &lt;&infin; that would be adequate for the nature of the problem.
Below we will obtain certain relations which clarify, to some extent, the con-
</p>
<p>nection between the distributions of ζ and τ and the stationary distribution of the
</p>
<p>chain X.</p>
<p/>
</div>
<div class="page"><p/>
<p>440 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Theorem 13.8.2 (A generalisation of the Wald identity) Assume Eτ &lt;&infin;, the g.c.d.
of all possible values of τ be 1, π be the stationary distribution of the chain X, and
</p>
<p>EπE|ξx | :=
&int;
</p>
<p>E|ξx |π(dx) &lt;&infin;. (13.8.10)
</p>
<p>Then
</p>
<p>Eζ = EτEπEξx . (13.8.11)
</p>
<p>The value of EπEξx is the &ldquo;doubly averaged&rdquo; value of the random variable ξx :
</p>
<p>over the distribution Fx and over the stationary distribution π .
</p>
<p>Theorem 13.8.2 implies that the condition supx E|ξx | &lt; &infin; is sufficient for the
finiteness of E|ζ |.
</p>
<p>Proof [of Theorem 13.8.2] First of all, we show that condition (13.8.10) implies
the finiteness of E|ζ |. If ξx &ge; 0 then Eζ is always well-defined. If we assume that
Eζ =&infin; then, repeating the proof of Theorem 13.8.1, we would easily obtain that,
in this case, Sn/n
</p>
<p>p&rarr;&infin;, and hence necessarily ESn/n&rarr;&infin; as n&rarr;&infin;. But
</p>
<p>ESn =
n&sum;
</p>
<p>j=0
EξXj =
</p>
<p>n&sum;
</p>
<p>j=0
</p>
<p>&int;
(Eξx)P(Xj &isin; dx),
</p>
<p>where the distribution P(Xj &isin; &middot;) converges in total variation to π(&middot;) as j &rarr;&infin;,
&int;
(Eξx)P(Xj &isin; dx)&rarr;
</p>
<p>&int;
(Eξx)π(dx),
</p>
<p>and hence
</p>
<p>1
</p>
<p>n
ESn &rarr; EπEξx &lt;&infin;. (13.8.12)
</p>
<p>This contradicts the above assumption, and therefore Eζ &lt;&infin;. Applying the above
argument to the random variables |ξx |, we conclude that condition (13.8.10) implies
E|ζ |&lt;&infin;.
</p>
<p>Let, as above, η(n) := ν(n)+ 1 = min{k : Tk &gt; n}. We will need the following.
</p>
<p>Lemma 13.8.5 If E|ζ |&lt;&infin; then
</p>
<p>Eζη(n) = o(n). (13.8.13)
</p>
<p>If Eζ 2 &lt;&infin; then
</p>
<p>Eζ 2η(n) = o(n) (13.8.14)
</p>
<p>as n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8 Limit Theorems for Sums of Random Variables 441
</p>
<p>Proof Without losing generality, assume that ξx &ge; 0 and ζ &ge; 0. Since τj &ge; 1, we
have
</p>
<p>h(k) :=
k&sum;
</p>
<p>j=0
P(Tj = k)&le; 1 for all k.
</p>
<p>Therefore,
</p>
<p>P(ζη(n) &gt; v)=
n&sum;
</p>
<p>k=0
h(k)P(ζ &gt; v, τ &gt; n&minus; k)&le;
</p>
<p>n&sum;
</p>
<p>k=0
P(ζ &gt; v, τ &gt; k).
</p>
<p>If Eζ &lt;&infin; then
</p>
<p>Eζη(n) &le;
n&sum;
</p>
<p>k=0
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P(ζ &gt; v; τ &gt; k)dv =
n&sum;
</p>
<p>k=0
E(ζ ; τ &gt; k), (13.8.15)
</p>
<p>where E(ζ ; τ &gt; k) &rarr; 0 as k &rarr; &infin;. This follows from Lemma A3.2.3 of Ap-
pendix 3. Together with (13.8.15) this proves (13.8.13).
</p>
<p>Similarly, for Eζ 2 &lt;&infin;,
</p>
<p>Eζ 2η(n) &le; 2
n&sum;
</p>
<p>k=0
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>vP(ζ &gt; v, τ &gt; k)dv =
n&sum;
</p>
<p>k=0
E
(
ζ 2, τ &gt; k
</p>
<p>)
= o(n).
</p>
<p>The lemma is proved. �
</p>
<p>Now we continue the proof of Theorem 13.8.2. Consider representation (13.8.5)
</p>
<p>for X0 = x0 and assume again that ξx &ge; 0. Then ζ1 = ξx0 ,
</p>
<p>Sn = ζ1 +Zη(n) + zn &minus; ζη(n),
</p>
<p>where by the Wald identity
</p>
<p>EZη(n) = Eη(n)Eζ &sim; n
Eζ
</p>
<p>Eτ
.
</p>
<p>Since π({x0}) = 1/Eτ &gt; 0, we have, by (13.8.10), E|ξx0 | &lt; &infin;. Moreover, for
ξx &ge; 0,
</p>
<p>|ζη(n) &minus; zn|&lt; ζη(n).
Hence, by Lemma 13.8.5,
</p>
<p>ESn = n
Eζ
</p>
<p>Eτ
+ o(n). (13.8.16)
</p>
<p>Combining this with (13.8.12), we obtain the assertion of the theorem.
</p>
<p>It remains to consider the case where ξx can take values of both signs. Introduce
</p>
<p>new random variables ξ&lowast;x on the chain X, defined by the equalities ξ
&lowast;
x := |ξx |, and</p>
<p/>
</div>
<div class="page"><p/>
<p>442 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>endow with the superscript &lowast; all already used notations that will correspond to the
new random variables. Since all ξ&lowast;x &ge; 0, by condition (13.8.10) we can apply to them
all the above assertions and, in particular, obtain that
</p>
<p>Eζ &lowast; &lt;&infin;, Eζ &lowast;η(n) = o(n). (13.8.17)
</p>
<p>Since
</p>
<p>|ζ | &le; ζ &lowast;, |ζη(n)| &le; ζ &lowast;η(n), |ζη(n) &minus; zn|&lt; ζ &lowast;η(n),
</p>
<p>it follows from (13.8.17) that
</p>
<p>E|ζ |&lt;&infin;, E|ζη(n) &minus; zn| = o(n)
</p>
<p>and relation (13.8.16) is valid along with identity (13.8.11).
</p>
<p>The theorem is proved. �
</p>
<p>Now we will prove the strong law of large numbers.
</p>
<p>Theorem 13.8.3 Let the conditions of Theorem 13.8.1 be satisfied. Then
</p>
<p>Sn
</p>
<p>n
</p>
<p>a.s.&minus;&rarr; EπEξx as n&rarr;&infin;.
</p>
<p>Proof Since in representation (13.8.5) one has ζ1/n
a.s.&minus;&rarr; 0 as n&rarr;&infin;, we can ne-
</p>
<p>glect this term in (13.8.5).
</p>
<p>The strong laws of large numbers for {Zk} and {Tk} mean that, for a given ε &gt; 0,
the trajectory of {STk } will lie within the boundaries kEζ(1 &plusmn; ε) and EζEτ Tk(1 &plusmn; 2ε)
for all k &ge; n and n large enough. (We leave a more formal formulation of this to the
reader.)
</p>
<p>We will prove the theorem if we verify that the probability of the event that, be-
</p>
<p>tween the times Tk , k &ge; n, the trajectory of Sj will cross at least once the boundaries
rj (1 &plusmn; 3ε), where r = Eζ
</p>
<p>Eτ
, tends to zero as n&rarr;&infin;. Since
</p>
<p>max
Tk&minus;1&lt;j&le;Tk
</p>
<p>|Sj &minus; STk | &le; ζ &lowast;k (13.8.18)
</p>
<p>(in the notation of the proof of Theorem 13.8.1), it is sufficient to verify that
</p>
<p>P(An)&rarr; 0 as n&rarr;&infin;, where An :=
⋃&infin;
</p>
<p>k=n{ζ &lowast;k &gt; εrTk}. But
</p>
<p>P(An)= P(AnBn)+ P(AnBn), (13.8.19)
</p>
<p>where
</p>
<p>Bn =
&infin;⋂
</p>
<p>k=n
</p>
<p>{
Tk &gt; kEτ(1 &minus; ε)
</p>
<p>}
, P(Bn)&rarr; 0 as n&rarr;&infin;,</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8 Limit Theorems for Sums of Random Variables 443
</p>
<p>so the second summand in (13.8.19) tends to zero. The first summand on the right-
</p>
<p>hand side of (13.8.19) does not exceed (for c= ε(1 &minus; ε)Eζ )
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>k=n
</p>
<p>{
ζ &lowast;k &gt; εEζk(1 &minus; ε)
</p>
<p>}
)
&le;
</p>
<p>&infin;&sum;
</p>
<p>k=n
P
(
ζ &lowast;k &gt; ck
</p>
<p>)
&rarr; 0
</p>
<p>as n&rarr;&infin;, since Eζ &lowast; &lt;&infin; (see (13.8.17)). The theorem is proved. �
</p>
<p>13.8.3 The Central Limit Theorem
</p>
<p>As in Theorem 13.8.1, first we will prove the main assertion under certain condi-
</p>
<p>tions on the moments of ζ and τ , and then we will establish a connection of these
</p>
<p>conditions to the stationary distribution of the chain X. Below we retain the notation
</p>
<p>of the previous section.
</p>
<p>Theorem 13.8.4 Let P(τ (x) &lt;&infin;)= 1 for any x, Eτ 2 &lt;&infin;, the g.c.d. of all possi-
ble values of τ is 1, and Eζ 2 &lt;&infin;. Then, as n&rarr;&infin;,
</p>
<p>Sn &minus; rn
d
&radic;
n/a
</p>
<p>&sub;=&rArr;�0,1,
</p>
<p>where r := aζ /a, aζ := Eζ , a := Eτ and d2 :=D(ζ &minus; rτ ).
</p>
<p>Proof We again make use of representation (13.8.5), where clearly
</p>
<p>ζ1&radic;
n
</p>
<p>p&rarr; 0, zn&radic;
n
</p>
<p>p&rarr; 0
</p>
<p>(see the proof of Theorem 13.8.1). This means that the problem reduces to that of
</p>
<p>finding the limiting distribution of Zν(n) = Zη(n) &minus; ζη(n), where by Lemma 10.6.1
ζη(n) has a proper limiting distribution, and so ζη(n)/
</p>
<p>&radic;
n
</p>
<p>p&rarr; 0 as n&rarr;&infin;. Further-
more, by Theorem 10.6.3,
</p>
<p>Zη(n)
</p>
<p>σS
&radic;
n
&sub;=&rArr;�0,1,
</p>
<p>where σ 2S := a&minus;1D(ζ &minus; rτ ), r =
Eζ
Eτ
</p>
<p>. The theorem is proved. �
</p>
<p>Now we will establish relations between the moment characteristics used for
</p>
<p>normalising Sn and the stationary distribution π . The answer for the number r was
</p>
<p>given in Theorem 13.8.2: r = EπEξx . For the number σ 2S we have the following
result.</p>
<p/>
</div>
<div class="page"><p/>
<p>444 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>Theorem 13.8.5 Let
</p>
<p>σ 2 :=
&int;
</p>
<p>Dξxπ(dx)+ 2
&infin;&sum;
</p>
<p>j=1
E(ξX0 &minus; r)(ξXj &minus; r)
</p>
<p>be well-defined and finite, where X0 &sub;= π . Then
</p>
<p>σ 2S := a&minus;1d2 = σ 2.
</p>
<p>Note that here the expectation under the sum sign is a &ldquo;triple averaging&rdquo;: over
</p>
<p>the distribution π(dy)P(y, j, dz) and the distributions of ξy and ξz.
</p>
<p>Proof We have
</p>
<p>E(Sn &minus; rn)2 = E
[
</p>
<p>n&sum;
</p>
<p>k=0
(ξXk &minus; r)
</p>
<p>]2
</p>
<p>=
n&sum;
</p>
<p>k=0
E(ξXk &minus; r)2 + 2
</p>
<p>&sum;
</p>
<p>k&lt;j
</p>
<p>E(ξXk &minus; r)(ξXj &minus; r), (13.8.20)
</p>
<p>where
</p>
<p>n&sum;
</p>
<p>k=0
E(ξXk &minus; r)2 =
</p>
<p>n&sum;
</p>
<p>k=0
E(ξXk &minus;EξXk )2 +
</p>
<p>n&sum;
</p>
<p>k=0
(EξXk &minus; r)2. (13.8.21)
</p>
<p>The summands in the first sum on the right-hand side of (13.8.21) converge to σ 2ξ :=&int;
Dξxπ(dx), the summands in the second sum converging to zero. Therefore, the
</p>
<p>left-hand side of (13.8.21) is asymptotically equivalent to nσ 2ξ .
</p>
<p>Further,
</p>
<p>&sum;
</p>
<p>k&lt;j
</p>
<p>E(ξXk &minus; r)(ξXj &minus; r)=
n&sum;
</p>
<p>k=0
</p>
<p>&sum;
</p>
<p>j&ge;k+1
E(ξXk &minus; r)(ξXj &minus; r), (13.8.22)
</p>
<p>where the distribution of Xk converges in total variation to the stationary distribu-
</p>
<p>tion π of the chain. Hence the inner sums on the right-hand side of (13.8.22), for
</p>
<p>large k and n&minus; k (say, for &radic;n &lt; k &lt; n&minus;&radic;n when n&rarr;&infin;), will be close to
</p>
<p>E :=
&infin;&sum;
</p>
<p>j=1
E(ξX0 &minus; r)(ξXj &minus; r),
</p>
<p>where X0 = π and the whole sum on the right-hand side of (13.8.22) is asymptoti-
cally equivalent, as n&rarr;&infin;, to nE (or will be o(n) if E = 0).
</p>
<p>Thus
</p>
<p>1
</p>
<p>n
E(Sn &minus; rn)2 &sim; σ 2ξ + 2E. (13.8.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8 Limit Theorems for Sums of Random Variables 445
</p>
<p>We now show that the existence of σ 2ξ and E implies the finiteness of d
2 = E(ζ &minus;
</p>
<p>rτ )2.
</p>
<p>Consider the truncated random variables
</p>
<p>ξ (N)x :=
</p>
<p>⎧
⎨
⎩
ξx if ξx &isin; [&minus;N,N ],
N if ξx &gt;N,
</p>
<p>&minus;N if ξx &lt;&minus;N.
</p>
<p>Since σ 2ξ &lt;&infin;, we have Eξ2x &lt;&infin; (a.e. with respect to the measure π ) and
</p>
<p>r(N) &rarr; r,
(
σ
(N)
ξ
</p>
<p>)2 &rarr; σ 2ξ , E(N) &rarr;E as N &rarr;&infin;,
</p>
<p>where the superscript (N) means that the notation corresponds to the truncated ran-
</p>
<p>dom variables. By virtue of Theorem 13.8.4,
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
E
(
S(N)n &minus; r(N)
</p>
<p>)2 &ge; a&minus;1
(
d(N)
</p>
<p>)2
.
</p>
<p>If we assume that d =&infin; then we will get that the lim inf on the left-hand side of this
relation is infinite. But this contradicts relation (13.8.23), by which the above lim inf
</p>
<p>equals (σ
(N)
ξ )
</p>
<p>2 + 2E(N) and remains bounded. We have obtained a contradiction,
which shows that d &lt;&infin;.
</p>
<p>On the other hand, for d &lt;&infin;, Eζ 2 &lt;&infin; and, for the initial value x0, by (13.8.5)
we have
</p>
<p>E(Sn &minus; rn)2 = E(Zν(n) + zn &minus; rn)2
</p>
<p>= E(Zη(n) &minus; rn)2 + 2E(Zη(n) &minus; rn)(zn &minus; ζη(n))+E(zn &minus; ζη(n))2,
(13.8.24)
</p>
<p>where n= Tη(n) &minus; χ(n). Therefore, putting Yn := Zn &minus; rTn =
&sum;n
</p>
<p>k=1(ζk &minus; rτk), we
obtain
</p>
<p>E(Zη(n) &minus; rn)2 = EY 2η(n) &minus; 2EYη(n)χ(n)+Eχ2(n).
</p>
<p>By virtue of (10.4.7), Eχ2(n)= o(n). By (10.6.4) (with a somewhat different nota-
tion),
</p>
<p>EY 2η(n) = d2Eη(n),
</p>
<p>where d2 := D(ζ &minus; rτ ), Eη(n) &sim; n/a and a = Eτ . Hence, applying the Cauchy&ndash;
Bunjakovsky inequality, we get
</p>
<p>∣∣EYη(n)χ(n)
∣∣= o(n), E(Zη(n) &minus; rn)2 &sim; nd2a&minus;1. (13.8.25)
</p>
<p>It remains to estimate the last two terms on the right-hand side of (13.8.24). But
</p>
<p>∣∣ζη(n) &minus; zn
∣∣&le; ζ &lowast;η(n),</p>
<p/>
</div>
<div class="page"><p/>
<p>446 13 Sequences of Dependent Trials. Markov Chains
</p>
<p>where ζ &lowast; corresponds to the summands ξ&lowast;Xk = |ξXk | and where, by Lemma 13.8.5
applied to ξ&lowast;x = |ξx |, we have
</p>
<p>E
(
ζ &lowast;η(n)
</p>
<p>)2 = o(n).
</p>
<p>Therefore E(ζη(n) &minus; zn) = o(n) and, by the Cauchy&ndash;Bunjakovsky inequality and
relation (13.8.25), the same relation is valid for the shifted moment in (13.8.24).
</p>
<p>Thus,
</p>
<p>E(Sn &minus; rn)2 &sim; a&minus;1d2n.
Combining this relation with (13.8.23), we obtain the assertion of the theorem. �</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
</p>
<p>Information and Entropy
</p>
<p>Abstract Section 14.1 presents the definitions and key properties of information
</p>
<p>and entropy. Section 14.2 discusses the entropy of a (stationary) finite Markov chain.
</p>
<p>The Law of Large Numbers is proved for the amount of information contained in
</p>
<p>a message that is a long sequence of successive states of a Markov chain, and the
</p>
<p>asymptotic behaviour of the number of the most common states in a sequence of
</p>
<p>successive values of the chain is established. Applications of this result to coding
</p>
<p>are discussed.
</p>
<p>14.1 The Definitions and Properties of Information and Entropy
</p>
<p>Suppose one conducts an experiment whose outcome is not predetermined. The
</p>
<p>term &ldquo;experiment&rdquo; will have a broad meaning. It may be a test of a new device, a
</p>
<p>satellite launch, a football match, a referendum and so on. If, in a football match,
</p>
<p>the first team is stronger than the second, then the occurrence of the event A that the
</p>
<p>first team won carries little significant information. On the contrary, the occurrence
</p>
<p>of the complementary event A contains a lot of information. The event B that a
</p>
<p>leading player of the first team was injured does contain information concerning the
</p>
<p>event A. But if it was the first team&rsquo;s doctor who was injured then that would hardly
</p>
<p>affect the match outcome, so such an event B carries no significant information
</p>
<p>about the event A.
</p>
<p>The following quantitative measure of information is conventionally adopted. Let
</p>
<p>A and B be events from some probability space 〈Ω,F,P〉.
</p>
<p>Definition 14.1.1 The amount of information about the event A contained in the
event (message) B is the quantity
</p>
<p>I (A|B) := log P(A|B)
P(A)
</p>
<p>.
</p>
<p>The notions of the &ldquo;amount of information&rdquo; and &ldquo;entropy&rdquo; were introduced by C.E. Shannon in
</p>
<p>1948. For some special situations the notion of amount of information had also been considered in
</p>
<p>earlier papers (e.g., by R.V.L. Hartley, 1928). The exposition in Sect. 14.2 of this chapter is
</p>
<p>substantially based on the paper of A.Ya. Khinchin [21].
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_14, &copy; Springer-Verlag London 2013
</p>
<p>447</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_14">http://dx.doi.org/10.1007/978-1-4471-5201-9_14</a></div>
</div>
<div class="page"><p/>
<p>448 14 Information and Entropy
</p>
<p>The occurrence of the event B = A may be interpreted as the message that A
took place.
</p>
<p>Definition 14.1.2 The number I (A) := I (A|A) is called the amount of information
contained in the message A:
</p>
<p>I (A) := I (A|A)=&minus; logP(A).
</p>
<p>We see from this definition that the larger the probability of the event A, the
</p>
<p>smaller I (A). As a rule, the logarithm to the base 2 is used in the definition of infor-
</p>
<p>mation. Thus, say, the message that a boy (or girl) was born in a family carries a unit
</p>
<p>of information (it is supposed that these events are equiprobable, and &minus; log2 p = 1
for p = 1/2). Throughout this chapter, we will write just logx for log2 x.
</p>
<p>If the events A and B are independent, then I (A|B) = 0. This means that the
event B does not carry any information about A, and vice versa. It is worth noting
</p>
<p>that we always have
</p>
<p>I (A|B)= I (B|A).
It is easy to see that if the events A and B are independent, then
</p>
<p>I (AB)= I (A)+ I (B). (14.1.1)
Consider an example. Let a chessman be placed at random on one of the squares
</p>
<p>of a chessboard. The information that the chessman is on square number k (the
</p>
<p>event A) is equal to I (A)= log 64 = 6. Let B1 be the event that the chessman is in
the i-th row, and B2 that the chessman is in the j -th column. The message A can be
</p>
<p>transmitted by transmitting B1 first and then B2. We have
</p>
<p>I (B1)= log 8 = 3 = I (B2).
Therefore
</p>
<p>I (B1)+ I (B2)= 6 = I (A),
so that transmitting the message A &ldquo;by parts&rdquo; requires communicating the same
</p>
<p>amount of information (which is equal to 6) as transmitting A itself. One could
</p>
<p>give other examples showing that the introduced numerical characteristics are quite
</p>
<p>natural.
</p>
<p>Let G be an experiment with outcomes E1, . . . ,EN occurring with probabilities
</p>
<p>p1, . . . , pN .
</p>
<p>The information resulting from the experiment G is a random variable
JG = JG(ω) assuming the value &minus; logpj on the set Ej , j = 1, . . . ,N .
</p>
<p>Thus, if in the probability space 〈Ω,F,P〉 corresponding to the experiment G,
Ω coincides with the set (E1, . . . ,EN ), then JG(ω)= I (ω).
</p>
<p>Definition 14.1.3 The expectation of the information obtained in the experiment G,
</p>
<p>EJG =&minus;
&sum;
</p>
<p>pj logpj , is called the entropy of the experiment. We shall denote it by
</p>
<p>Hp =H(G) := &minus;
N&sum;
</p>
<p>j=1
pj logpj ,</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 The Definitions and Properties of Information and Entropy 449
</p>
<p>Fig. 14.1 The plot of the
</p>
<p>entropy f (p) of a random
</p>
<p>experiment with two
</p>
<p>outcomes
</p>
<p>where p= (p1, . . . , pN ). For pj = 0, by continuity we set pj logpj to be equal to
zero.
</p>
<p>The entropy of an experiment is, in a sense, a measure of its uncertainty. Let,
</p>
<p>for example, our experiment have two outcomes A and B with probabilities p and
</p>
<p>q = 1 &minus; p, respectively. The entropy of the experiment is equal to
</p>
<p>Hp =&minus;p logp&minus; (1 &minus; p) log(1 &minus; p)= f (p).
</p>
<p>The graph of this function is depicted in Fig. 14.1.
</p>
<p>The only maximum of f (p) equals log 2 = 1 and is attained at the point p = 1/2.
This is the case of maximum uncertainty. If p decreases, then the uncertainty also
</p>
<p>decreases together with Hp, and Hp = 0 for p= (0,1) or (0,1).
The same properties can easily be seen in the general case as well.
</p>
<p>The properties of entropy.
</p>
<p>1. H(G)= 0 if and only if there exists a j , 1 &le; j &le;N , such that pj = P(Ej )= 1.
2. H(G) attains its maximum when pj = 1/N for all j .
</p>
<p>Proof The second derivative of the function β(x) = x logx is positive on [0,1],
so that β(x) is convex. Therefore, for any qi &ge; 0 such that
</p>
<p>&sum;N
i=1 qi = 1, and any
</p>
<p>xi &ge; 0, one has the inequality
</p>
<p>β
</p>
<p>(
N&sum;
</p>
<p>i=1
qixi
</p>
<p>)
&le;
</p>
<p>N&sum;
</p>
<p>i=1
qiβ(xi).
</p>
<p>If we take qi = 1/N , xi = pi , then
(
</p>
<p>1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>i=1
pi
</p>
<p>)
log
</p>
<p>(
1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>i=1
pi
</p>
<p>)
&le;
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>N
pi logpi .
</p>
<p>Setting u := ( 1
N
, . . . , 1
</p>
<p>N
) we obtain from this that
</p>
<p>&minus; log 1
N
</p>
<p>= logN =Hu &ge;&minus;
N&sum;
</p>
<p>i=1
pi logpi =Hp. �
</p>
<p>Note that if the entropy H(G) equals its maximum value H(G) = logN , then
JG(ω)= logN with probability 1, i.e. the information JG(ω) becomes constant.</p>
<p/>
</div>
<div class="page"><p/>
<p>450 14 Information and Entropy
</p>
<p>3. Let G1 and G2 be two independent experiments. We write down the outcomes
</p>
<p>and their probabilities in these experiments in the following way:
</p>
<p>G1 =
(
E1, . . . ,EN
p1, . . . , pN
</p>
<p>)
, G2 =
</p>
<p>(
A1, . . . ,AM
q1, . . . , qM
</p>
<p>)
.
</p>
<p>Combining the outcomes of these two experiments we obtain a new experiment
</p>
<p>G=G1 &times;G2 =
(
E1A1,E1A2, . . . ,ENAM
p1q1,p1q2, . . . , pNqM
</p>
<p>)
.
</p>
<p>The information JG obtained as a result of this experiment is a random variable
</p>
<p>taking values &minus; logpiqj with probabilities piqj , i = 1, . . . ,N ; j = 1, . . . ,M . But
the sum JG1 + JG2 of two independent random variables equal to the amounts of
information obtained in the experiments G1 and G2, respectively, clearly has the
</p>
<p>same distribution. Thus the information obtained in a sequence of independent ex-
periments is equal to the sum of the information from these experiments. Since in
that case clearly
</p>
<p>EJG = EJG1 +EJG2,
we have that for independent G1 and G2 the entropy of the experiment G is equal
to the sum of the entropies of the experiments G1 and G2:
</p>
<p>H(G)=H(G1)+H(G2).
4. If the experiments G1 and G2 are dependent, then the experiment G can be
</p>
<p>represented as
</p>
<p>G=
(
E1A1,E1A2, . . . ,ENAM
</p>
<p>q11, q12, . . . , qNM
</p>
<p>)
</p>
<p>with qij = pipij , where pij is the conditional probability of the event Aj
given Ei , so that
</p>
<p>M&sum;
</p>
<p>j=1
qij = pi = P(Ei), i = 1, . . . ,N;
</p>
<p>N&sum;
</p>
<p>j=1
qij = qj = P(Ai), j = 1, . . . ,M.
</p>
<p>In this case the equality JG = JG1 + JG2 , generally speaking, does not hold. In-
troduce a random variable J &lowast;2 which is equal to &minus; logpij on the set EiAj . Then
evidently JG = JG1 + J &lowast;2 . Since
</p>
<p>P(A|Ei)= pij ,
the quantity J &lowast;2 for a fixed i can be considered as the information from the experi-
ment G2 given the event Ei occurred. We will call the quantity
</p>
<p>E
(
J &lowast;2 |Ei
</p>
<p>)
=&minus;
</p>
<p>M&sum;
</p>
<p>j=1
pij logpij</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 The Definitions and Properties of Information and Entropy 451
</p>
<p>the conditional entropy H(G2|E1) of the experiment G2 given Ei , and the quantity
</p>
<p>EJ &lowast;2 =&minus;
&sum;
</p>
<p>i,j
</p>
<p>qij logpij =
&sum;
</p>
<p>i
</p>
<p>piH(G2|E1)
</p>
<p>the conditional entropy H(G2|G1) of the experiment G2 given G1. In this notation,
we obviously have
</p>
<p>H(G)=H(G1)+H(G2|G1).
We will prove that in this equality we always have
</p>
<p>H(G2|G1)&le;H(G2),
i.e. for two experiments G1 and G2 the entropy H(G) never exceeds the sum of the
entropies H(G1) and H(G2):
</p>
<p>H(G)=H(G1 &times;G2)&le;H(G1)+H(G2).
Equality takes place here only when qij = piqj , i.e. when G1 and G2 are indepen-
dent.
</p>
<p>Proof First note that, for any two distributions (u1, . . . , un) and (v1, . . . , vn), one
has the inequality
</p>
<p>&minus;
&sum;
</p>
<p>i
</p>
<p>ui logui &le;&minus;
&sum;
</p>
<p>i
</p>
<p>ui logvi, (14.1.2)
</p>
<p>equality being possible here only if vi = ui , i = 1, . . . , n. This follows from the
concavity of the function logx, since it implies that, for any ai &gt; 0,
</p>
<p>&sum;
</p>
<p>i
</p>
<p>ui logai &le; log
(&sum;
</p>
<p>i
</p>
<p>uiai
</p>
<p>)
,
</p>
<p>equality being possible only if a1 = a2 = &middot; &middot; &middot; = an. Putting ai = vi/ui , we obtain
relation (14.1.2).
</p>
<p>Next we have
</p>
<p>H(G1)+H(G2)=&minus;
&sum;
</p>
<p>i,j
</p>
<p>qij (logpi + logqj )=&minus;
&sum;
</p>
<p>i,j
</p>
<p>qij logpiqj ,
</p>
<p>and because {piqj } is obviously a distribution, by virtue of (14.1.2)
</p>
<p>&minus;
&sum;
</p>
<p>qij logpiqj &ge;&minus;
&sum;
</p>
<p>qij logqij =H(G)
</p>
<p>holds, and equality is possible here only if qij = piqj . �
</p>
<p>5. As we saw when considering property 3, the information obtained as a result of
</p>
<p>the experiment Gn1 consisting of n independent repetitions of the experiment G1
is equal to
</p>
<p>JGn1
=&minus;
</p>
<p>N&sum;
</p>
<p>j=1
νj logpj ,</p>
<p/>
</div>
<div class="page"><p/>
<p>452 14 Information and Entropy
</p>
<p>where νj is the number of occurrences of the outcome Ej . By the law of large
</p>
<p>numbers, νj/n
p&rarr; pj as n&rarr;&infin;, and hence
</p>
<p>1
</p>
<p>n
JGn1
</p>
<p>p&rarr;H(G1)=Hp.
</p>
<p>To conclude this section, we note that the measure of the amount of information
</p>
<p>resulting from an experiment we considered here can be derived as the only possible
one (up to a constant multiplier) if one starts with a few simple requirements that
</p>
<p>are natural to impose on such a quantity.1
</p>
<p>It is also interesting to note the connections between the above-introduced no-
</p>
<p>tions and large deviation probabilities. As one can see from Theorems 5.1.2 and
</p>
<p>5.2.4, the difference between the &ldquo;biased&rdquo; entropy &minus;
&sum;
</p>
<p>p&lowast;j lnpj and the entropy
&minus;
&sum;
</p>
<p>p&lowast;j lnp
&lowast;
j (p
</p>
<p>&lowast;
j = νj/n are the relative frequencies of the outcomes Ej ) is an
</p>
<p>analogue of the deviation function (see Sect. 8.8) in the multi-dimensional case.
</p>
<p>14.2 The Entropy of a Finite Markov Chain. A Theorem on the
</p>
<p>Asymptotic Behaviour of the Information Contained in a
</p>
<p>Long Message; Its Applications
</p>
<p>14.2.1 The Entropy of a Sequence of Trials Forming a Stationary
</p>
<p>Markov Chain
</p>
<p>Let {Xk}&infin;k=1 be a stationary finite Markov chain with one class of essential states
without subclasses, E1, . . . ,EN being its states. Stationarity of the chain means that
</p>
<p>P(X1 = j)= πj coincide with the stationary probabilities. It is clear that
</p>
<p>P(X2 = j)=
&sum;
</p>
<p>k
</p>
<p>πkpkj = πj , P(X3 = j)= πj , and so on.
</p>
<p>Let Gk be an experiment determining the value of Xk (i.e. the state the system
</p>
<p>entered on the k-th step). If Xk&minus;1 = i, then the entropy of the k-th step equals
</p>
<p>H(Gk|Xk&minus;1 = i)=&minus;
&sum;
</p>
<p>j
</p>
<p>pij logpij .
</p>
<p>By definition, the entropy of a stationary Markov chain is equal to
</p>
<p>H = EH(Gk|Xk&minus;1)=H(Gk|Gk&minus;1)=&minus;
&sum;
</p>
<p>i
</p>
<p>πi
&sum;
</p>
<p>j
</p>
<p>pij logpij .
</p>
<p>Consider the first n steps X1, . . . ,Xn of the Markov chain. By the Markov prop-
</p>
<p>erty, the entropy of this composite experiment G(n) =G1 &times; &middot; &middot; &middot; &times;Gn is equal to
</p>
<p>1See, e.g., [11].</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Entropy of a Finite Markov Chain 453
</p>
<p>H
(
G(n)
</p>
<p>)
=H(G1)+H(G2|G1)+ &middot; &middot; &middot; +H(Gn|Gn&minus;1)
=&minus;
</p>
<p>&sum;
πj logπj + (n&minus; 1)H &sim; nH
</p>
<p>as n&rarr;&infin;. If Xk were independent then, as we saw, we would have exact equality
here.
</p>
<p>14.2.2 The Law of Large Numbers for the Amount of Information
</p>
<p>Contained in a Message
</p>
<p>Now consider a finite sequence (X1, . . . ,Xn) as a message (event) Cn and denote,
</p>
<p>as before, by I (Cn) = &minus; logP(Cn) the amount of information contained in Cn.
The value of I (Cn) is a function on the space of elementary outcomes equal to
</p>
<p>the information JG(n) contained in the experiment G
(n). We now show that, with
</p>
<p>probability close to 1, this information behaves asymptotically as nH , as was the
</p>
<p>case for independent Xk . Therefore H is essentially the average information per
</p>
<p>trial in the sequence {Xk}&infin;k=1.
</p>
<p>Theorem 14.2.1 As n&rarr;&infin;,
I (Cn)
</p>
<p>n
= &minus; logP(Cn)
</p>
<p>n
</p>
<p>a.s.&minus;&rarr;H.
</p>
<p>This means that, for any δ &gt; 0, the set of all messages Cn can be decomposed into
</p>
<p>two classes. For the first class, |I (Cn)/n&minus;H |&lt; δ, and the sum of the probabilities
of the elements of the second class tends to 0 as n&rarr;&infin;.
</p>
<p>Proof Construct from the given Markov chain a new one {Yk}&infin;k=1 by setting Yk :=
(Xk,Xk+1). The states of the new chain are pairs of states (Ei,Ej ) of the chain
{Xk} with pij &gt; 0. The transition probabilities are obviously given by
</p>
<p>p(i,j)(k,l) =
{
</p>
<p>0, j 
= k,
pkl, j = k.
</p>
<p>Note that one can easily prove by induction that
</p>
<p>p(i,j)(k,l)(n)= pjk(n&minus; 1)pkl . (14.2.1)
From the definition of {Yk} it follows that the ergodic theorem holds for this chain.
This can also be seen directly from (14.2.1), the stationary probabilities being
</p>
<p>lim
n&rarr;&infin;
</p>
<p>p(i,j)(k,l)(n)= πkpkl .
</p>
<p>Now we will need the law of large numbers for the number of visits m(k,l)(n)
</p>
<p>of the chain {Yk}&infin;k=1 to state (k, l) over time n. By virtue of this law (see Theo-
rem 13.4.4),
</p>
<p>m(k,l)(n)
</p>
<p>n
</p>
<p>a.s.&minus;&rarr; πkpkl as n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>454 14 Information and Entropy
</p>
<p>Consider the random variable P(Cn):
</p>
<p>P(Cn)= P(EX1EX2 &middot; &middot; &middot;EXn)= P(EX1)P(EX2 |EX1) &middot; &middot; &middot;P(EXn |EXn&minus;1)
</p>
<p>= πX1pX1X2 &middot; &middot; &middot;pXn&minus;1Xn = πX1
&prod;
</p>
<p>(k,l)
</p>
<p>p
m(k,l)(n&minus;1)
kl .
</p>
<p>The product here is taken over all pairs (k, l). Therefore (πi = P(X1 = i))
</p>
<p>logP(Cn)= logπX1 +
&sum;
</p>
<p>k,l
</p>
<p>m(k,l)(n&minus; 1) logpkl,
</p>
<p>1
</p>
<p>n
logP(Cn)
</p>
<p>p&rarr;
&sum;
</p>
<p>k,l
</p>
<p>πkpkl logpkl =&minus;H. �
</p>
<p>14.2.3 The Asymptotic Behaviour of the Number of the Most
</p>
<p>Common Outcomes in a Sequence of Trials
</p>
<p>Theorem 14.2.1 has an important corollary. Rank all the messages (words) Cn of
</p>
<p>length n according to the values of their probabilities in descending order. Next pick
</p>
<p>the most probable words one by one until the sum of their probabilities exceeds a
</p>
<p>prescribed level α, 0 &lt; α &lt; 1. Denote the number (and also the set) of the selected
</p>
<p>words by Mα(n).
</p>
<p>Theorem 14.2.2 For each 0 &lt; α &lt; 1, there exists one and the same limit
</p>
<p>lim
n&rarr;&infin;
</p>
<p>logMα(n)
</p>
<p>n
=H.
</p>
<p>Proof Let δ &gt; 0 be a number, which can be arbitrarily small. We will say that Cn
falls into category K1 if its probability P(Cn) &gt; 2
</p>
<p>&minus;n(H&minus;δ), and into category K2 if
</p>
<p>2&minus;n(H+δ) &lt; P(Cn)&le; 2&minus;n(H&minus;δ).
</p>
<p>Finally, Cn belongs to the third category K3 if
</p>
<p>P(Cn)&le; 2&minus;n(H+δ).
</p>
<p>Since, by Theorem 14.2.1, P(Cn &isin;K1&cup;K3)&rarr; 0 as n&rarr;&infin;, the set Mα(n) contains
only the words from K1 and K2, and the last word from Mα(n) (i.e. having the
</p>
<p>smallest probability)&mdash;we denote it by Cα,n&mdash;belongs to K2. This means that
</p>
<p>Mα(n)2
&minus;n(H+δ) &lt;
</p>
<p>&sum;
</p>
<p>Cn&isin;Mα(n)
P(Cn) &lt; α + P(Cα,n) &lt; α + 2&minus;n(H&minus;δ).
</p>
<p>This implies
</p>
<p>logMα(n)
</p>
<p>n
&lt;
</p>
<p>(α + 2&minus;n(H&minus;δ))
n
</p>
<p>+H + δ.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Entropy of a Finite Markov Chain 455
</p>
<p>Since δ is arbitrary, we have
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>logMα(n)
</p>
<p>n
&le;H.
</p>
<p>On the other hand, the words from K2 belonging to Mα(n) have total probability
</p>
<p>&ge; α &minus; P(K1). If M(2)α (n) is the number of these messages then
</p>
<p>M(2)α (n)2
&minus;n(H&minus;δ) &ge; α&minus; P(K1),
</p>
<p>and, consequently,
</p>
<p>Mα(n)2
&minus;n(H&minus;δ) &ge; α &minus; P(K1).
</p>
<p>Since P(K1)&rarr; 0 as n&rarr;&infin;, for sufficiently large n one has
logMα(n)
</p>
<p>n
&ge;H &minus; δ + 1
</p>
<p>n
log
</p>
<p>α
</p>
<p>2
.
</p>
<p>It follows that
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>logMα(n)
</p>
<p>n
&ge;H.
</p>
<p>The theorem is proved. �
</p>
<p>Now one can obtain a useful interpretation of this theorem. Let N be the number
</p>
<p>of the chain states. Suppose for simplicity&rsquo;s sake that N = 2m. Then the number of
different words of length n (chains Cn) will be equal to N
</p>
<p>n = 2nm. Suppose, further,
that these words are transmitted using a binary code, so that m binary symbols
</p>
<p>are used to code every state. Thus, with such transmission method&mdash;we will call it
</p>
<p>direct coding&mdash;the length of the messages will be equal to nm. (For example, one
can use Markov chains to model the Russian language and take N = 32, m = 5.)
The assertion of Theorem 14.2.2 means that, for large n, with probability 1 &minus; ε,
ε &gt; 0, only 2nH of the totality of 2nm words will be transmitted. The probability
</p>
<p>of transmitting all the remaining words will be small if ε is small. From this it is
</p>
<p>easy to establish the existence of another more economical code requiring, with a
</p>
<p>large probability, a smaller number of digits to transmit a word. Indeed, one can
</p>
<p>enumerate the selected 2nH most likely words using, say, a binary code again, and
</p>
<p>then transmit only the number of the word. This clearly requires only nH digits.
</p>
<p>Since we always have H &le; logN =m, the length of the message will be m/H &ge; 1
times smaller.
</p>
<p>This is a special case of the so-called basic coding theorem for Markov chains:
for large n, there exists a code for which, with a high probability, the original mes-
</p>
<p>sage Cn can be transmitted by a sequence of signals which is m/H times shorter
</p>
<p>than in the case of the direct coding.
</p>
<p>The above coding method is rather an oversimplified example than a recipe for
</p>
<p>efficiently compressing the messages. It should be noted that finding a really ef-
</p>
<p>ficient coding method is a rather difficult task. For example, in Morse code it is
</p>
<p>reasonable to encode more frequent letters by shorter sequences of dots and dashes.</p>
<p/>
</div>
<div class="page"><p/>
<p>456 14 Information and Entropy
</p>
<p>However, the text reduction by m/H times would not be achieved. Certain compres-
</p>
<p>sion techniques have been used in this book as well. For example, we replaced the
</p>
<p>frequently encountered words &ldquo;characteristic function&rdquo; by &ldquo;ch.f.&rdquo; We could achieve
</p>
<p>better results if, say, shorthand was used. The structure of a code with a high com-
</p>
<p>pression coefficient will certainly be very complicated. The theorems of the present
</p>
<p>chapter give an upper bound for the results we can achieve.
</p>
<p>Since H =
&sum;
</p>
<p>1
n
</p>
<p>logN = m, for a sequence of independent equiprobable sym-
bols, such a text is incontractible. This is why the proximity of &ldquo;new&rdquo; messages
(encoded using a new alphabet) to a sequence of equiprobable symbols could serve
</p>
<p>as a criterion for constructing new codes.
</p>
<p>It should be taken into account, however, that the text &ldquo;redundancy&rdquo; we are
</p>
<p>&ldquo;fighting&rdquo; with is in many cases a useful and helpful phenomenon. Without such
</p>
<p>redundancy, it would be impossible to detect misprints or reconstruct omissions as
</p>
<p>easily as we, say, restore the letter &ldquo;r&rdquo; in the word &ldquo;info &middot; mation&rdquo;.
The reader might know how difficult it is to read a highly abridged and formalised
</p>
<p>mathematical text. While working with an ideal code no errors would be admissible
</p>
<p>(even if we could find any), since it is impossible to reconstruct an omitted or dis-
</p>
<p>torted symbol in a sequence of equiprobable digits. In this connection, there arises
</p>
<p>one of the basic problems of information theory: to find a code with the smallest
</p>
<p>&ldquo;redundancy&rdquo; which still allows one to eliminate the transmission noise.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 15
</p>
<p>Martingales
</p>
<p>Abstract The definitions, simplest properties and first examples of martingales and
</p>
<p>sub/super-martingales are given in Sect. 15.1. Stopping (Markov) times are intro-
</p>
<p>duced in Sect. 15.2, which also contains Doob&rsquo;s theorem on random change of time
</p>
<p>and Wald&rsquo;s identity together with a number of its applications to boundary crossing
</p>
<p>problems and elsewhere. This is followed by Sect. 15.3 presenting fundamental mar-
</p>
<p>tingale inequalities, including Doob&rsquo;s inequality with a number of its consequences,
</p>
<p>and an inequality for the number of strip crossings. Section 15.4 begins with Doob&rsquo;s
</p>
<p>martingale convergence theorem and also presents L&eacute;vy&rsquo;s theorem and an applica-
</p>
<p>tion to branching processes. Section 15.5 derives several important inequalities for
</p>
<p>the moments of stochastic sequences.
</p>
<p>15.1 Definitions, Simplest Properties, and Examples
</p>
<p>In Chap. 13 we considered sequences of dependent random variables X0,X1, . . .
</p>
<p>forming Markov chains. Dependence was described there in terms of transition
</p>
<p>probabilities determining the distribution of Xn+1 given Xn. That enabled us to
investigate rather completely the properties of Markov chains.
</p>
<p>In this chapter we consider another type of sequence of dependent random vari-
</p>
<p>ables. Now dependence will be characterised only by the mean value of Xn+1 given
the whole &ldquo;history&rdquo; X0, . . . ,Xn. It turns out that one can also obtain rather general
</p>
<p>results for such sequences.
</p>
<p>Let a probability space 〈Ω,F,P〉 be given together with a sequence of random
variables X0,X1, . . . defined on it and an increasing family (or flow) of σ -algebras
</p>
<p>{Fn}n&ge;0: F0 &sube; F1 &sube; &middot; &middot; &middot; &sube; Fn &sube; &middot; &middot; &middot; &sube; F.
</p>
<p>Definition 15.1.1 A sequence of pairs {Xn,Fn; n &ge; 0} is called a stochastic se-
quence if, for each n &ge; 0, Xn is Fn-measurable. A stochastic sequence is said to
be a martingale (one also says that {Xn} is a martingale with respect to the flow of
σ -algebras {Fn}) if, for every n&ge; 0,
</p>
<p>(1)
</p>
<p>E|Xn|&lt;&infin;, (15.1.1)
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_15, &copy; Springer-Verlag London 2013
</p>
<p>457</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_15">http://dx.doi.org/10.1007/978-1-4471-5201-9_15</a></div>
</div>
<div class="page"><p/>
<p>458 15 Martingales
</p>
<p>(2) Xn is measurable with respect to Fn,
</p>
<p>(3)
</p>
<p>E(Xn+1 | Fn)=Xn. (15.1.2)
</p>
<p>A stochastic sequence {Xn,Fn; n &ge; 0} is called a submartingale (supermartin-
gale) if conditions (1)&ndash;(3) hold with the sign &ldquo;=&rdquo; replaced in (15.1.2) with &ldquo;&ge;&rdquo;
(&ldquo;&le;&rdquo;, respectively).
</p>
<p>We will say that a sequence {Xn} forms a martingale (submartingale, super-
martingale) if, for Fn = σ(X0, . . . ,Xn), the pairs {Xn,Fn} form a sequence with
the same name. Submartingales and supermartingales are often called semimartin-
gales.
</p>
<p>It is evident that relation (15.1.2) persists if we replace Xn+1 on its left-hand side
with Xm for any m&gt; n. Indeed, by virtue of the properties of conditional expecta-
</p>
<p>tions,
</p>
<p>E(Xm|Fn)= E
[
E(Xm|Fm&minus;1)
</p>
<p>∣∣Fn
]
= E(Xm&minus;1|Fn)= &middot; &middot; &middot; =Xn.
</p>
<p>A similar assertion holds for semimartingales.
</p>
<p>If {Xn} is a martingale, then E(Xn+1|σ(X0, . . . ,Xn))= Xn, and, by a property
of conditional expectations,
</p>
<p>E
(
Xn+1
</p>
<p>∣∣σ(Xn)
)
= E
</p>
<p>[
E
(
Xn+1
</p>
<p>∣∣σ(X0, . . . ,Xn)
)∣∣σ(Xn)
</p>
<p>]
= E
</p>
<p>(
Xn
</p>
<p>∣∣σ(Xn)
)
=Xn.
</p>
<p>So, for martingales, as for Markov chains, we have
</p>
<p>E
(
Xn+1
</p>
<p>∣∣σ(X0, . . . ,Xn)
)
= E
</p>
<p>(
Xn+1
</p>
<p>∣∣σ(Xn)
)
.
</p>
<p>The similarity, however, is limited to this relation, because for a martingale, the
</p>
<p>equality does not hold for distributions, but the additional condition
</p>
<p>E
(
Xn+1
</p>
<p>∣∣σ(Xn)
)
=Xn
</p>
<p>is imposed.
</p>
<p>Example 15.1.1 Let ξn, n &ge; 0 be independent. Then Xn = ξ1 + &middot; &middot; &middot; + ξn form a
martingale (submartingale, supermartingale) if Eξn = 0 (Eξn &ge; 0, Eξn &le; 0). It is
obvious that Xn also form a Markov chain. The same is true of Xn =
</p>
<p>&prod;n
k=0 ξk if
</p>
<p>Eξn = 1.
</p>
<p>Example 15.1.2 Let ξn, n&ge; 0, be independent. Then
</p>
<p>Xn =
n&sum;
</p>
<p>k=1
ξk&minus;1ξk, n&ge; 1, X0 = ξ0,
</p>
<p>form a martingale if Eξn = 0, because
</p>
<p>E
(
Xn+1
</p>
<p>∣∣σ(X0, . . . ,Xn)
)
=Xn +E
</p>
<p>(
ξnξn+1
</p>
<p>∣∣σ(ξn)
)
=Xn.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Definitions, Simplest Properties, and Examples 459
</p>
<p>Clearly, {Xn} is not a Markov chain here. An example of a sequence which is a
Markov chain but not a martingale can be obtained, say, if we consider a random
</p>
<p>walk on a segment with reflection at the endpoints (see Example 13.1.1).
</p>
<p>As well as {0,1, . . .} we will use other sets of indices for Xn, for example,
{&minus;&infin; &lt; n &lt; &infin;} or {n &le; &minus;1}, and also sets of integers including infinite values
&plusmn;&infin;, say, {0 &le; n &le; &infin;}. We will denote these sets by a common symbol N and
write martingales (semimartingales) as {Xn,Fn; n &isin; N}. By F&minus;&infin; we will under-
stand the σ -algebra
</p>
<p>⋂
n&isin;N Fn, and by F&infin; the σ -algebra σ(
</p>
<p>⋃
n&isin;N Fn) generated by⋃
</p>
<p>n&isin;N Fn, so that F&minus;&infin; &sube; Fn &sube; F&infin; &sube; F for any n &isin;N.
</p>
<p>Definition 15.1.2 A stochastic sequence {Xn,Fn; n &isin; N} is called a martingale
(submartingale, supermartingale), if the conditions of Definition 15.1.1 hold for
any n &isin;N.
</p>
<p>If {Xn,F; n &isin;N} is a martingale and the left boundary n0 of N is finite (for ex-
ample, N= {0,1, . . .}), then the martingale {Xn,Fn} can be always extended &ldquo;to the
whole axis&rdquo; by setting Fn := Fn0 and Xn :=Xn0 for n &lt; n0. The same holds for the
right boundary as well. Therefore if a martingale (semimartingale) {Xn,Fn; n &isin;N}
is given, then without loss of generality we can always assume that one is actually
</p>
<p>given a martingale (semimartingale) {Xn,Fn; &minus;&infin;&le; n&le;&infin;}.
</p>
<p>Example 15.1.3 Let {Fn, &minus;&infin; &le; n &le; &infin;} be a given sequence of increasing
σ -algebras, and ξ a random variable on 〈Ω,F,P〉, E|ξ | &lt; &infin;. Then {Xn,Fn;
&minus;&infin;&le; n&le;&infin;} with Xn = E(ξ |Fn) forms a martingale.
</p>
<p>Indeed, by the property of conditional expectations, for any m&le;&infin; and m&gt; n,
E(Xm|Fn)= E
</p>
<p>[
E(ξ |Fm)
</p>
<p>∣∣Fn
]
= E(ξ |Fn)=Xn.
</p>
<p>Definition 15.1.3 The martingale of Example 15.1.3 is called a martingale gener-
ated by the random variable ξ (and the family {Fn}).
</p>
<p>Definition 15.1.4 A set N+ is called the right closure of N if:
</p>
<p>(1) N+ =N when the maximal element of N is finite;
(2) N+ =N &cup; {&infin;} if N is not bounded from the right.
</p>
<p>If N = N+ then we say that N is right closed. A martingale (semimartingale)
{Xn,F; n &isin;N} is said to be right closed if N is right closed.
</p>
<p>Lemma 15.1.1 A martingale {Xn,F; n &isin;N} is generated by a random variable if
and only if it is right closed.
</p>
<p>The Proof of the lemma is trivial. In one direction it follows from Example 15.1.3,
and in the other from the equality
</p>
<p>E(XN |Fn)=Xn, N = sup{k; k &isin;N},
which implies that {Xn,F} is generated by XN . The lemma is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>460 15 Martingales
</p>
<p>Now we consider an interesting and more concrete example of a martingale gen-
</p>
<p>erated by a random variable.
</p>
<p>Example 15.1.4 Let ξ1, ξ2, . . . be independent and identically distributed and as-
sume E|ξ1|&lt;&infin;. Set
Sn = ξ1 + &middot; &middot; &middot; + ξn, X&minus;n = Sn/n, F&minus;n = σ(Sn, Sn+1, . . .)= σ(Sn, ξn+1, . . .).
</p>
<p>Then F&minus;n &sub; F&minus;n+1 and, for any 1 &le; k &le; n, by symmetry
E(ξk|F&minus;n)= E(ξ1|F&minus;n).
</p>
<p>From this it follows that
</p>
<p>Sn = E(Sn|F&minus;n)=
n&sum;
</p>
<p>k=1
E(ξk|F&minus;n)= nE(ξ1|F&minus;n),
</p>
<p>Sn
</p>
<p>n
= E(ξ1|F&minus;n).
</p>
<p>This means that {Xn,Fn; n&le; 1} forms a martingale generated by ξ1.
</p>
<p>We will now obtain a series of auxiliary assertions giving the simplest properties
</p>
<p>of martingales and semimartingales. When considering semimartingales, we will
</p>
<p>confine ourselves to submartingales only, since the corresponding properties of su-
</p>
<p>permartingales will follow immediately if one considers the sequence Yn = &minus;Xn,
where {Xn} is a submartingale.
</p>
<p>Lemma 15.1.2
</p>
<p>(1) The property that {Xn,Fn; n &isin;N} is a martingale is equivalent to invariability
in m&ge; n of the set functions (integrals)
</p>
<p>E(Xm; A)= E(Xn;A) (15.1.3)
for any A &isin; Fn. In particular, EXm = const.
</p>
<p>(2) The property that {Xn,Fn; n &isin;N} is a submartingale is equivalent to the mono-
tone increase in m&ge; n of the set functions
</p>
<p>E(Xm;A)&ge; E(Xn;A) (15.1.4)
for every A &isin; Fn. In particular, EXm &uarr;.
</p>
<p>The Proof follows immediately from the definitions. If (15.1.3) holds then, by the
definition of conditional expectation, Xn = E(Xm|Fn), and vice versa. Now let
(15.1.4) hold. Put Yn = E(Xm|Fn). Then (15.1.4) implies that E(Yn;A)&ge; E(Xn;A)
and E(Yn &minus;Xn;A)&ge; 0 for any A &isin; Fn. From this it follows that Yn = E(Xm|Fn)&ge;
Xn with probability 1. The converse assertion can be obtained as easily as the direct
</p>
<p>one. The lemma is proved. �
</p>
<p>Lemma 15.1.3 Let {Xn,Fn; n &isin; N} be a martingale, g(x) be a convex function,
and E|g(Xn)|&lt;&infin;. Then {g(Xn),Fn; n &isin;N} is a submartingale.
</p>
<p>If, in addition, g(x) is nondecreasing, then the assertion of the theorem remains
true when {Xn,Fn; n &isin;N} is a submartingale.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Definitions, Simplest Properties, and Examples 461
</p>
<p>The Proof of both assertions follows immediately from Jensen&rsquo;s inequality
</p>
<p>E
(
g(Xn+1)
</p>
<p>∣∣Fn
)
&ge; g
</p>
<p>(
E(Xn+1|Fn)
</p>
<p>)
&ge; g
</p>
<p>(
E(Xn|Fn)
</p>
<p>)
. �
</p>
<p>Clearly, the function g(x) = |x|p for p &ge; 1 satisfies the conditions of the first
part of the lemma, and the function g(x) = eλx for λ &gt; 0 meets the conditions of
the second part of the lemma.
</p>
<p>Lemma 15.1.4 Let {Xn,Fn; n &isin; N} be a right closed submartingale. Then, for
Xn(a)= max{Xn, a} and any a, {Xn(a),Fn; n &isin;N} is a uniformly integrable sub-
martingale.
</p>
<p>If {Xn,Fn; n &isin;N} is a right closed martingale, then it is uniformly integrable.
</p>
<p>Proof Let N := sup{k : k &isin; N}. Then, by Lemma 15.1.3, {Xn(a),Fn; n &isin; N} is
a submartingale. Hence, for any c &gt; 0,
</p>
<p>cP
(
Xn(a) &gt; c
</p>
<p>)
&le; E
</p>
<p>(
Xn(a); Xn(a) &gt; c
</p>
<p>)
&le; E
</p>
<p>(
XN (a); Xn(a) &gt; c
</p>
<p>)
&le; EX+N (a)
</p>
<p>(here X+ = max(0,X)) and so
</p>
<p>P
(
Xn(a) &gt; c
</p>
<p>)
&le; 1
</p>
<p>c
E
(
X+N (a)
</p>
<p>)
&rarr; 0,
</p>
<p>uniformly in n as c&rarr;&infin;. Therefore we get the required uniform integrability:
</p>
<p>sup
n
</p>
<p>E
(
Xn(a); Xn(a) &gt; c
</p>
<p>)
&le; sup
</p>
<p>n
E
(
XN (a); Xn(a) &gt; c
</p>
<p>)
&rarr; 0,
</p>
<p>since supn P(Xn(a) &gt; c) &rarr; 0 as c &rarr; &infin; (see Lemma A3.2.3 in Appendix 3; by
truncating at the level a we avoided estimating the &ldquo;negative tails&rdquo;).
</p>
<p>If {Xn,Fn; n &isin;N} is a martingale, then its uniform integrability will follow from
the first assertion of the lemma applied to the submartingale {|Xn|,Fn; n &isin; N}.
The lemma is proved. �
</p>
<p>The nature of martingales can be clarified to some extent by the following exam-
</p>
<p>ple.
</p>
<p>Example 15.1.5 Let ξ1, ξ2, . . . be an arbitrary sequence of random variables,
E|ξk|&lt;&infin;, Fn = σ(ξ1, . . . , ξn) for n&ge; 1, F0 = (&empty;,Ω) (the trivial σ -algebra),
</p>
<p>Sn =
n&sum;
</p>
<p>k=1
ξk, Zn =
</p>
<p>n&sum;
</p>
<p>k=1
E(ξk|Fk&minus;1), Xn = Sn &minus;Zn.
</p>
<p>Then {Xn,Fn; n&ge; 1} is a martingale. This is a consequence of the fact that
</p>
<p>E(Sn+1 &minus;Zn+1|Fn)= E
(
Xn + ξn+1 &minus;E(ξn+1|Fn)
</p>
<p>∣∣Fn
)
=Xn.
</p>
<p>In other words, for an arbitrary sequence {ξn}, the sequence Sn can be &ldquo;com-
pensated&rdquo; by a so-called &ldquo;predictable&rdquo; (in the sense that its value is determined by
</p>
<p>S1, . . . , Sn&minus;1) sequence Zn so that Sn &minus;Zn will be a martingale.</p>
<p/>
</div>
<div class="page"><p/>
<p>462 15 Martingales
</p>
<p>15.2 The Martingale Property and Random Change of Time.
</p>
<p>Wald&rsquo;s Identity
</p>
<p>Throughout this section we assume that N= {n&ge; 0}. Recall the definition of a stop-
ping time.
</p>
<p>Definition 15.2.1 A random variable ν will be called a stopping time or a Markov
time (with respect to an increasing family of σ -algebras {Fn; n &ge; 0}) if, for any
n&ge; 0, {ν &le; n} &isin; Fn.
</p>
<p>It is obvious that a constant ν &equiv; m is a stopping time. If ν is a stopping time,
then, for any fixed m, ν(m) = min(ν,m), is also a stopping time, since for n &ge; m
we have
</p>
<p>ν(m)&le;m&le; n,
{
ν(m)&le; n
</p>
<p>}
=Ω &isin; Fn,
</p>
<p>and if n &lt;m then
{
ν(m)&le; n
</p>
<p>}
= {ν &le; n} &isin; Fn.
</p>
<p>If ν is a stopping time, then
</p>
<p>{ν = n} = {ν &le; n} &minus; {ν &le; n&minus; 1} &isin; Fn, {ν &ge; n} =Ω &minus; {ν &le; n&minus; 1} &isin; Fn&minus;1.
</p>
<p>Conversely, if {ν = n} &isin; Fn, then {ν &le; n} &isin; Fn and therefore ν is a stopping time.
Let a martingale {Xn,Fn; n&ge; 0} be given. A typical example of a stopping time
</p>
<p>is the time ν at which Xn first hits a given measurable set B:
</p>
<p>ν = inf{n&ge; 0 :Xn &isin; B}
</p>
<p>(ν =&infin; if all Xn /&isin; B). Indeed,
</p>
<p>{ν = n} = {X0 /&isin; B, . . . ,Xn&minus;1 /&isin; B, Xn &isin; B} &isin; Fn.
</p>
<p>If ν is a proper stopping time (P(ν &lt; &infin;) = 1), then Xν is a random variable,
since
</p>
<p>Xν =
&infin;&sum;
</p>
<p>n=0
XnI{ν=n}.
</p>
<p>By Fν we will denote the σ -algebra of sets A &isin; F such that A &cap; {ν = n} &isin; Fn,
n = 0,1, . . . This σ -algebra can be thought of as being generated by the events
{ν &le; n} &cap; Bn, n = 0,1, . . ., where Bn &isin; Fn. Clearly, ν and Xν are Fν -measurable.
If ν1 and ν2 are two stopping times, then {ν2 &ge; ν1} &isin; Fν1 and {ν2 &ge; ν1} &isin; Fν2 , since
{ν2 &ge; ν1} =
</p>
<p>⋃
n[{ν2 = n} &cap; {ν1 &le; n}].
</p>
<p>We already know that if {Xn,Fn} is a martingale then EXn is constant for all n.
Will this property remain valid for EXν if ν is a stopping time? From Wald&rsquo;s identity
</p>
<p>we know that this is the case for the martingale from Example 15.1.1. In the general
</p>
<p>case one has the following.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 The Martingale Property and Random Change of Time. Wald&rsquo;s Identity 463
</p>
<p>Theorem 15.2.1 (Doob) Let {Xn,Fn; n&ge; 0} be a martingale (submartingale) and
ν1, ν2 be stopping times such that
</p>
<p>E|Xνi |&lt;&infin;, i = 1,2, (15.2.1)
lim inf
n&rarr;&infin;
</p>
<p>E
(
|Xn|; ν2 &ge; n
</p>
<p>)
= 0. (15.2.2)
</p>
<p>Then, on the set {ν2 &ge; ν1},
E(Xν2 |Fν1)=Xν1 (&ge;Xν1). (15.2.3)
</p>
<p>This theorem extends the martingale (submartingale) property to random time.
</p>
<p>Corollary 15.2.1 If ν2 = ν &ge; 0 is an arbitrary stopping time, then putting ν1 = n
(also a stopping time) we have that, on the set ν &ge; n,
</p>
<p>E(Xν |Fn)=Xn, EXν = EX0,
or, which is the same, for any A &isin; Fn &cap; {ν &ge; n},
</p>
<p>E(Xν; A)= E(Xn; A).
</p>
<p>For submartingales substitute &ldquo;=&rdquo; by &ldquo;&ge;&rdquo;.
</p>
<p>Proof of Theorem 15.2.1 To prove (15.2.3) it suffices to show that, for any A &isin; Fν1 ,
E
(
Xν2; A&cap; {ν2 &ge; ν1}
</p>
<p>)
= E
</p>
<p>(
Xν1; A&cap; {ν2 &ge; ν1}
</p>
<p>)
. (15.2.4)
</p>
<p>Since the random variables νi are discrete, we just have to establish (15.2.4) for sets
</p>
<p>An =A&cap; {ν1 = n} &isin; Fn, n= 0,1, . . . , i.e. to establish the equality
E
(
Xν2; An &cap; {ν2 &ge; n}
</p>
<p>)
= E
</p>
<p>(
Xn; An &cap; {ν2 &ge; n}
</p>
<p>)
. (15.2.5)
</p>
<p>Thus the proof is reduced to the case ν1 = n. We have
</p>
<p>E
(
Xn; An &cap; {ν2 &ge; n}
</p>
<p>)
= E
</p>
<p>(
Xn; An &cap; {ν2 = n}
</p>
<p>)
+E
</p>
<p>(
Xn; An &cap; {ν2 &ge; n+ 1}
</p>
<p>)
</p>
<p>= E
(
Xν2; An &cap; {ν2 = n}
</p>
<p>)
+E
</p>
<p>(
Xn+1; An &cap; {ν2 &ge; n+ 1}
</p>
<p>)
.
</p>
<p>Here we used the fact that {ν2 &ge; n1} &isin; Fn and the martingale property (15.1.3).
Applying this equality m&minus; n times we obtain that
</p>
<p>E
(
Xν2; An &cap; {n&le; ν2 &lt;m}
</p>
<p>)
</p>
<p>= E
(
Xn; An &cap; {ν2 &ge; n}
</p>
<p>)
&minus;E
</p>
<p>(
Xm; An &cap; {ν2 &ge;m}
</p>
<p>)
. (15.2.6)
</p>
<p>By (15.2.2) the last expression converges to zero for some sequence m&rarr;&infin;.
Since
</p>
<p>An,m :=An &cap; {n&le; ν2 &lt;m} &uarr; Bn =An &cap; {n&le; ν2},
by the property of integrals and by virtue of (15.2.6),
</p>
<p>E
(
Xν2; An &cap; {n&le; ν2}
</p>
<p>)
= lim
</p>
<p>m&rarr;&infin;
E(Xν2; An,m)= E
</p>
<p>(
Xn; An &cap; {ν2 &ge; n}
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>464 15 Martingales
</p>
<p>Thus we proved equality (15.2.5) and hence Theorem 15.2.1 for martingales. The
</p>
<p>proof for submartingales can be obtained by simply changing the equality signs in
</p>
<p>certain places to inequalities. The theorem is proved. �
</p>
<p>The conditions of Theorem 15.2.1 are far from always being met, even in rather
</p>
<p>simple cases. Consider, for instance, a fair game (see Examples 4.2.3 and 4.4.5)
</p>
<p>versus an infinitely rich adversary, in which z+ Sn is the fortune of the first gam-
bler after n plays (given he has not been ruined yet). Here z &gt; 0, Sn =
</p>
<p>&sum;n
k=1 ξk ,
</p>
<p>P(ξk = &plusmn;1) = 1/2, η(z) = min{k : Sk = &minus;z} is obviously a Markov (stopping)
time, and the sequence {Sn; n&ge; 0}, S0 = 0, is a martingale, but Sη(z) =&minus;z. Hence
ESη(z) =&minus;z 
= ESn = 0, and equality (15.2.5) does not hold for ν1 = 0, ν2 = η(z),
z &gt; 0, n &gt; 0. In this example, this means that condition (15.2.2) is not satisfied (this
</p>
<p>is related to the fact that Eη(z)=&infin;).
Conditions (15.2.1) and (15.2.2) of Theorem 15.2.1 can, generally speaking, be
</p>
<p>rather hard to verify. Therefore the following statements are useful in applications.
</p>
<p>Put for brevity
</p>
<p>ξn :=Xn &minus;Xn&minus;1, ξ0 :=X0, Yn :=
n&sum;
</p>
<p>k=0
|ξk|, n= 0,1, . . .
</p>
<p>Lemma 15.2.1 The condition
</p>
<p>EYν &lt;&infin; (15.2.7)
is sufficient for (15.2.1) and (15.2.2) (with νi = ν).
</p>
<p>The Proof is almost evident since |Xν | &le; Yν and
E(|Xn|; ν &gt; n)&le; E(Yν; ν &gt; n).
</p>
<p>Because P(ν &gt; n)&rarr; 0 and EYν &lt;&infin;, it remains to use the property of integrals by
which E(η; An)&rarr; 0 if E|η|&lt;&infin; and P(An)&rarr; 0. �
</p>
<p>We introduce the following notation:
</p>
<p>an := E
(
|ξn|
</p>
<p>∣∣Fn&minus;1
)
, σ 2n := E
</p>
<p>(
ξ2n
</p>
<p>∣∣Fn&minus;1
)
, n= 0,1,2, . . . ,
</p>
<p>where F&minus;1 can be taken to be the trivial σ -algebra.
</p>
<p>Theorem 15.2.2 Let {Xn; n&ge; 0} be a martingale (submartingale) and ν be a stop-
ping time (with respect to {Fn = σ(X0, . . . ,Xn)}).
(1) If
</p>
<p>Eν &lt;&infin; (15.2.8)
and, for all n&ge; 0, on the set {ν &ge; n} &isin; Fn&minus;1 one has
</p>
<p>an &le; c= const, (15.2.9)
then
</p>
<p>E|Xν |&lt;&infin;, EXν = EX0 (&ge; EX0). (15.2.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 The Martingale Property and Random Change of Time. Wald&rsquo;s Identity 465
</p>
<p>(2) If, in addition, Eσ 2n = Eξ2n &lt;&infin; then
</p>
<p>EX2ν = E
ν&sum;
</p>
<p>k=1
σ 2k . (15.2.11)
</p>
<p>Proof By virtue of Theorem 15.2.1, Corollary 15.2.1 and Lemma 15.2.1, to prove
(15.2.10) it suffices to verify that conditions (15.2.8) and (15.2.9) imply (15.2.7).
</p>
<p>Quite similarly to the proof of Theorem 4.4.1, we have
</p>
<p>E|Yν | =
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
n&sum;
</p>
<p>k=0
E
(
|ξk|; ν = n
</p>
<p>)
)
=
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>&infin;&sum;
</p>
<p>n=k
E
(
|ξk|; ν = n
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>k=0
E
(
|ξk|; ν &ge; k
</p>
<p>)
.
</p>
<p>Here {ν &ge; k} =Ω \ {ν &le; k &minus; 1} &isin; Fk&minus;1. Therefore, by condition (15.2.9),
E(|ξk|; ν &ge; k)= E
</p>
<p>(
E
(
|ξk|
</p>
<p>∣∣Fk&minus;1
)
; ν &ge; k
</p>
<p>)
&le; cP(ν &ge; k).
</p>
<p>This means that
</p>
<p>EYν &le; c
&infin;&sum;
</p>
<p>k=0
P(ν &ge; k)= cEν &lt;&infin;.
</p>
<p>Now we will prove (15.2.11). Set Zn :=X2n&minus;
&sum;n
</p>
<p>0 σ
2
k . One can easily see that Zn
</p>
<p>is a martingale, since
</p>
<p>E
(
X2n+1 &minus;X2n &minus; σ 2n+1
</p>
<p>∣∣Fn
)
= E
</p>
<p>(
2Xnξn+1 + ξ2n+1 &minus; σ 2n+1
</p>
<p>∣∣Fn
)
= 0.
</p>
<p>It is also clear that E|Zn|&lt;&infin; and ν(n)= min(ν, n) is a stopping time. By virtue of
Lemma 15.2.1, conditions (15.2.1) and (15.2.2) always hold for the pair {Zk}, ν(n).
Therefore, by the first part of the theorem,
</p>
<p>EZν(n) = 0, EX2ν(n) = E
ν(n)&sum;
</p>
<p>k=1
σ 2k . (15.2.12)
</p>
<p>It remains to verify that
</p>
<p>lim
n&rarr;&infin;
</p>
<p>EX2ν(n) = EX2ν , limn&rarr;&infin;E
ν(n)&sum;
</p>
<p>k=1
σ 2k = E
</p>
<p>ν&sum;
</p>
<p>k=1
σ 2k . (15.2.13)
</p>
<p>The second equality follows from the monotone convergence theorem (ν(n) &uarr; ν,
σ 2k &ge; 0). That theorem implies the former equality as well, for X2ν(n)
</p>
<p>a.s.&minus;&rarr; X2ν and
X2ν(n)&uarr;. To verify the latter claim, note that {X2n,Fn; n &ge; 0} is a martingale, and
therefore, for any A &isin; Fn,
</p>
<p>E
(
X2ν(n); A
</p>
<p>)
= E
</p>
<p>(
X2ν; A&cap; {ν &le; n}
</p>
<p>)
+E
</p>
<p>(
X2n; A&cap; {ν &gt; n}
</p>
<p>)
</p>
<p>&le; E
(
X2ν; A&cap; {ν &le; n}
</p>
<p>)
+E
</p>
<p>(
E
(
X2n+1
</p>
<p>∣∣Fn
)
; A&cap; {ν &gt; n}
</p>
<p>)
</p>
<p>= E
(
X2ν; A&cap; {ν &lt; n+ 1}
</p>
<p>)
+E
</p>
<p>(
X2n+1; A&cap; {ν &ge; n+ 1}
</p>
<p>)
</p>
<p>= E
(
X2ν(n+1); A
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>466 15 Martingales
</p>
<p>Thus (15.2.12) and (15.2.13) imply (15.2.11), and the theorem is completely
</p>
<p>proved. �
</p>
<p>The main assertion of Theorem 15.2.2 for martingales (submartingales):
</p>
<p>EXν = EX0 (&ge; EX0) (15.2.14)
was obtained as a consequence of Theorem 15.2.1. However, we could get it directly
</p>
<p>from some rather transparent relations which, moreover, enable one to extend it to
</p>
<p>improper stopping times ν.
</p>
<p>A stopping time ν is called improper if 0 &lt; P(ν &lt; &infin;) = 1 &minus; P(ν = &infin;) &lt; 1.
To give an example of an improper stopping time, consider independent identically
</p>
<p>distributed random variables ξk , a = Eξk &lt; 0, Xn =
&sum;n
</p>
<p>k=1 ξk , and put
</p>
<p>ν = η(x) := min{k &ge; 1 :Xk &gt; x}, x &ge; 0.
Here ν is finite only for such trajectories {Xk} that supk Xk &gt; x. If the last inequality
does not hold, we put ν =&infin;. Clearly,
</p>
<p>P(ν =&infin;)= P
(
</p>
<p>sup
k
</p>
<p>Xk &le; x
)
&gt; 0.
</p>
<p>Thus, for an arbitrary (possibly improper) stopping time, we have
</p>
<p>E(Xν; ν &lt;&infin;)=
&infin;&sum;
</p>
<p>k=0
E(Xk; ν = k)=
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>[
E(Xk; ν &ge; k)&minus;E(Xk; ν &ge; k+ 1)
</p>
<p>]
.
</p>
<p>(15.2.15)
</p>
<p>Assume now that changing the order of summation is justified here. Then, by virtue
</p>
<p>of the relation {ν &ge; k+ 1} &isin; Fk , we get
</p>
<p>E(Xν; ν &lt;&infin;)= EX0 +
&infin;&sum;
</p>
<p>k=0
E(Xk+1 &minus;Xk; ν &ge; k + 1)
</p>
<p>= EX0 +
&infin;&sum;
</p>
<p>k=0
EI(ν &ge; k + 1)E(Xk+1 &minus;Xk|Fk). (15.2.16)
</p>
<p>Since for martingales (submartingales) the factors E(Xk+1 &minus;Xk|Fk)= 0 (&ge; 0), we
obtain the following.
</p>
<p>Theorem 15.2.3 If the change of the order of summation in (15.2.15) and (15.2.16)
is legitimate then, for martingales (submartingales),
</p>
<p>E(Xν; ν &lt;&infin;)= EX0 (&ge; EX0). (15.2.17)
</p>
<p>Assumptions (15.2.8) and (15.2.9) of Theorem 15.2.2 are nothing else but con-
</p>
<p>ditions ensuring the absolute convergence of the series in (15.2.15) (see the proof of
</p>
<p>Theorem 15.2.2) and (15.2.16), because the sum of the absolute values of the terms
</p>
<p>in (15.2.16) is dominated by
</p>
<p>&infin;&sum;
</p>
<p>k=1
akP(ν &ge; k+ 1)&le; aEν &lt;&infin;,</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 The Martingale Property and Random Change of Time. Wald&rsquo;s Identity 467
</p>
<p>where, as before, ak = E(|ξk| | Fk&minus;1) with ξk =Xk&minus;Xk&minus;1. This justifies the change
of the order of summation.
</p>
<p>There is still another way of proving (15.2.17) based on (15.2.15) specifying a
</p>
<p>simple condition ensuring the required justification. First note that identity (15.2.17)
</p>
<p>assumes that the expectation E(Xν; ν &lt;&infin;) exists, i.e. both values E(X&plusmn;ν ; ν &lt;&infin;)
are finite, where x&plusmn; = max(&plusmn;x,0).
</p>
<p>Theorem 15.2.4 1. Let {Xn,Fn} be a martingale. Then the condition
</p>
<p>lim
n&rarr;&infin;
</p>
<p>E(Xn; ν &gt; n)= 0 (15.2.18)
</p>
<p>is necessary and sufficient for the relation
</p>
<p>lim
n&rarr;&infin;
</p>
<p>E(Xn; ν &le; n)= EX0. (15.2.19)
</p>
<p>A necessary and sufficient condition for (15.2.17) is that (15.2.18) holds and at
least one of the values E(X&plusmn;ν ; ν &lt;&infin;) is finite.
</p>
<p>2. If {Xn,Fn} is a supermartingale and
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>E(Xn; ν &gt; n)&ge; 0, (15.2.20)
</p>
<p>then
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>E(Xn; ν &le; n)&le; EX0.
</p>
<p>If, in addition, at least one of the values E(X&plusmn;ν ; ν &lt;&infin;) is finite then
</p>
<p>E(Xν; ν &lt;&infin;)&le; EX0.
</p>
<p>3. If, in conditions (15.2.18) and (15.2.20), we replace the quantity E(Xn; ν &gt; n)
with E(Xn; ν &ge; n), the first two assertions of the theorem will remain true.
</p>
<p>The corresponding symmetric assertions hold for submartingales.
</p>
<p>Proof As we have already mentioned, for martingales, E(ξk; ν &ge; k)= 0. Therefore,
by virtue of (15.2.18)
</p>
<p>EX0 = lim
n&rarr;&infin;
</p>
<p>[
EX0 +
</p>
<p>n&sum;
</p>
<p>k=1
E(ξk; ν &ge; k)&minus;E(Xn, ν &ge; n+ 1)
</p>
<p>]
.
</p>
<p>Here
</p>
<p>n&sum;
</p>
<p>k=1
E(ξk; ν &ge; k)=
</p>
<p>n&sum;
</p>
<p>k=1
E(Xk; ν &ge; k)&minus;
</p>
<p>n&sum;
</p>
<p>k=1
E(Xk&minus;1; ν &ge; k)
</p>
<p>=
n&sum;
</p>
<p>k=1
E(Xk; ν &ge; k)&minus;
</p>
<p>n&minus;1&sum;
</p>
<p>k=1
E(Xk; ν &ge; k+ 1).
</p>
<p>Hence</p>
<p/>
</div>
<div class="page"><p/>
<p>468 15 Martingales
</p>
<p>EX0 = lim
n&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>k=0
</p>
<p>[
E(Xk; ν &ge; k)&minus;E(Xk; ν &ge; k+ 1)
</p>
<p>]
</p>
<p>= lim
n&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>k=0
E(Xk; ν = k)= lim
</p>
<p>n&rarr;&infin;
E(Xν; ν &le; n).
</p>
<p>These equalities also imply the necessity of condition (15.2.18).
</p>
<p>If at least one of the values E(X&plusmn;ν ; ν &lt;&infin;) is finite, then by the monotone con-
vergence theorem
</p>
<p>lim
n&rarr;&infin;
</p>
<p>E(Xn; ν &le; n)= lim
n&rarr;&infin;
</p>
<p>E
(
X+n ; ν &le; n
</p>
<p>)
&minus; lim
</p>
<p>n&rarr;&infin;
E
(
X&minus;n ; ν &le; n
</p>
<p>)
</p>
<p>= E
(
X+ν ; ν &lt;&infin;
</p>
<p>)
&minus;E
</p>
<p>(
X&minus;ν ; ν &lt;&infin;
</p>
<p>)
= E(Xν; ν &lt;&infin;).
</p>
<p>The third assertion of the theorem follows from the fact that the stopping time
</p>
<p>ν(n)= min(ν, n) satisfies the conditions of the first part of the theorem (or those of
Theorems 15.2.1 and 15.2.3), and therefore, for the martingale {Xn},
</p>
<p>EX0 = EXν(n) = E(Xν; ν &lt; n)+E(Xν; ν &ge; n),
so that (15.2.19) implies the convergence E(Xn; ν &ge; n)&rarr; 0 and vice versa.
</p>
<p>The proof for semimartingales is similar. The theorem is proved. �
</p>
<p>That assertions (15.2.17) and (15.2.19) are, generally speaking, not equivalent
</p>
<p>even when (15.2.18) holds (i.e., limn&rarr;&infin;E(Xν;ν &le; n) = E(Xν;ν &lt;&infin;) is not al-
ways the case), can be illustrated by the following example. Let ξk be independent
</p>
<p>random variables with
</p>
<p>P
(
ξk = 3k
</p>
<p>)
= P
</p>
<p>(
ξk =&minus;3k
</p>
<p>)
= 1/2,
</p>
<p>ν be independent of {ξk}, and P(ν = k) = 2&minus;k , k = 1,2, . . . . Then X0 = 0, Xk =
Xk&minus;1 + ξk for k &ge; 1 is a martingale,
</p>
<p>EXn = 0, P(ν &lt;&infin;)= 1, E(Xn; ν &gt; n)= EXnP(ν &gt; n)= 0
by independence, and condition (15.2.18) is satisfied. By virtue of (15.2.19), this
</p>
<p>means that limn&rarr;&infin; P(Xν; ν &le; n) = 0 (one can also verify this directly). On the
other hand, the expectation E(Xν; ν &lt; &infin;) = EXν is not defined, since EX+ν =
EX&minus;ν =&infin;. Indeed, clearly
</p>
<p>Xk&minus;1 &ge;&minus;
3k &minus; 3
</p>
<p>2
,
</p>
<p>{
ξk = 3k
</p>
<p>}
&sub;
{
Xk &ge;
</p>
<p>3k + 3
2
</p>
<p>}
, P
</p>
<p>(
Xk &ge;
</p>
<p>3k + 3
2
</p>
<p>)
&ge; 1
</p>
<p>2
,
</p>
<p>EX+k &ge;
3k + 3
</p>
<p>4
, EX+ν =
</p>
<p>&infin;&sum;
</p>
<p>k=1
2&minus;kEX+k &ge;
</p>
<p>&infin;&sum;
</p>
<p>k=1
2&minus;k&minus;23k =&infin;.
</p>
<p>By symmetry, we also have EX&minus;ν =&infin;.
</p>
<p>Corollary 15.2.2 1. If {Xn,Fn} is a nonnegative martingale, then condition
(15.2.18) is necessary and sufficient for (15.2.17).</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 The Martingale Property and Random Change of Time. Wald&rsquo;s Identity 469
</p>
<p>2. If {Xn,Fn} is a nonnegative supermartingale and ν is an arbitrary stopping
time, then
</p>
<p>E(Xν; ν &lt;&infin;)&le; EX0. (15.2.21)
</p>
<p>Proof The assertion follows in an obvious way from Theorem 15.2.4 since one has
E(X&minus;ν ; ν &lt;&infin;)= 0. �
</p>
<p>Theorem 15.2.2 implies the already known Wald&rsquo;s identity (see Theorem 4.4.3)
</p>
<p>supplemented with another useful statement.
</p>
<p>Theorem 15.2.5 (Wald&rsquo;s identity) Let ζ1, ζ2, . . . be independent identically dis-
tributed random variables, Sn = ζ1 + &middot; &middot; &middot; + ζn, S0 = 0, and assume Eζ1 = a. Let,
further, ν be a stopping time with Eν &lt;&infin;. Then
</p>
<p>ESν = aEν. (15.2.22)
If, moreover, σ 2 = Var ζk &lt;&infin;, then
</p>
<p>E[Sν &minus; νa]2 = σ 2Eν. (15.2.23)
</p>
<p>Proof It is clear that Xn = Sn &minus; na forms a martingale and conditions (15.2.8) and
(15.2.9) are met. Therefore EXν = EX0 = 0, which is equivalent to (15.2.22), and
EX2ν = Eνσ 2, which is equivalent to (15.2.23). �
</p>
<p>Example 15.2.1 Consider a generalised renewal process (see Sect. 10.6) S(t) =
Sη(t), where Sn =
</p>
<p>&sum;n
j=1 ξj (in this example we follow the notation of Chap. 10
</p>
<p>and change the meaning of the notation Sn from the above), η(t)= min{k : Tk &gt; t},
Tn =
</p>
<p>&sum;n
j=1 τj and (τj , ξj ) are independent vectors distributed as (τ, ξ), τ &gt; 0. Set
</p>
<p>aξ = Eξ , a = Eτ , σ 2ξ = Var ξ and σ 2 = Var τ . As we know from Wald&rsquo;s identity in
Sect. 4.4,
</p>
<p>Eη(t)= t +Eχ(t)
a
</p>
<p>, ES(t)= aξEη(t),
</p>
<p>where Eχ(t) = o(t) as t &rarr; &infin; (see Theorem 10.1.1) and, in the non-lattice case,
Eχ(t)&rarr; σ 2+a2
</p>
<p>2a2
if σ 2 &lt;&infin; (see Theorem 10.4.3).
</p>
<p>We now find Varη(t) and VarS(t). Omitting for brevity&rsquo;s sake the argument t ,
</p>
<p>we can write
</p>
<p>a2 Varη(t)= a2 Varη= E(aη&minus; aEη)2 = E(aη&minus; Tη + Tη &minus; aEη)2
</p>
<p>= E(Tη &minus; aη)2 +E(Tη &minus; aEη)2 &minus; 2E(Tη &minus; aη)(Tη &minus; aEη).
The first summand on the right-hand side is equal to
</p>
<p>σ 2Eη= σ
2t
</p>
<p>a
+O(1)
</p>
<p>by Theorem 15.2.3. The second summand equals, by (10.4.8) (χ(t)= Tη(t) &minus; t),
</p>
<p>E
(
t + χ(t)&minus; aEη
</p>
<p>)2 = E
(
χ(t)&minus;Eχ(t)
</p>
<p>)2 &le; Eχ2(t)= o(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>470 15 Martingales
</p>
<p>The last summand, by the Cauchy&ndash;Bunjakovsky inequality, is also o(t). Finally, we
</p>
<p>get
</p>
<p>Varη(t)= σ
2t
</p>
<p>a3
+ o(t).
</p>
<p>Consider now (with r = aξ/a; ζj = ξj &minus; rτj , Eζj = 0)
</p>
<p>VarS(t)= E(Sη &minus; aξEη)2 = E
[
Sη &minus; rTη + r(Tη &minus; aEη)
</p>
<p>]2
</p>
<p>= E
(
</p>
<p>η&sum;
</p>
<p>j=1
ζj
</p>
<p>)2
+ r2E(Tη &minus; aEη)2 + 2rE
</p>
<p>(
η&sum;
</p>
<p>j=1
ζj
</p>
<p>)
(Tη &minus; aEη).
</p>
<p>The first term on the right-hand side is equal to
</p>
<p>EηVar ζ = t Var ζ
a
</p>
<p>+O(1)
</p>
<p>by Theorem 15.2.3. The second term has already been estimated above. Therefore,
</p>
<p>as before, the sum of the last two terms is o(t). Thus
</p>
<p>VarS(t)= t
a
E(ξ &minus; rτ )2 + o(t).
</p>
<p>This corresponds to the scaling used in Theorem 10.6.2.
</p>
<p>Example 15.2.2 Examples 4.4.4 and 4.5.5 referring to the fair game situation with
P(ζk = &plusmn;1) = 1/2 and ν = min{k : Sk = z2 or Sk = &minus;z1} (z1 and z2 being the
capitals of the gamblers) can also illustrate the use of Theorem 15.2.5.
</p>
<p>Now consider the case p = P(ζk = 1) 
= 1/2. The sequence Xn = (q/p)Sn ,
n&ge; 0, q = 1 &minus; p is a martingale, since
</p>
<p>E(q/p)ζk = p(q/p)+ q(p/q)= 1.
</p>
<p>By Theorem 15.2.5 (the probabilities P1 and P2 were defined in Example 4.4.5),
</p>
<p>EXν = EX0 = 1, P1(q/p)z2 + P2(q/p)z1 = 1.
</p>
<p>From this relation and equality P1 + P2 = 1 we have
</p>
<p>P1 =
(q/p)z1 &minus; 1
</p>
<p>(q/p)z1 &minus; (q/p)z2 , P2 = 1 &minus; P1.
</p>
<p>Using Wald&rsquo;s identity again, we also obtain that
</p>
<p>Eν = ESν
Eζ1
</p>
<p>= P1z2 &minus; P2z1
p&minus; q .
</p>
<p>Note that these equalities could have been obtained by elementary methods1 but this
</p>
<p>would require lengthy calculations.
</p>
<p>1See, e.g., [12].</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 The Martingale Property and Random Change of Time. Wald&rsquo;s Identity 471
</p>
<p>In the cases when the nature of Sν is simple enough, the assertions of the type
</p>
<p>of Theorems 15.2.1&ndash;15.2.2 enable one to obtain (or estimate) the distribution of the
</p>
<p>random variable ν itself. In such situations, the following assertion is rather helpful.
</p>
<p>Suppose that the conditions of Theorem 15.2.5 are met, but, instead of conditions
</p>
<p>on the moments of ζn, the Cram&eacute;r condition (cf. Chap. 9) is assumed to be satisfied:
</p>
<p>ψ(λ) := Eeλζ &lt;&infin;
</p>
<p>for some λ 
= 0.
In other words, if
</p>
<p>λ+ := sup
(
λ :ψ(λ) &lt;&infin;
</p>
<p>)
&ge; 0, λ&minus; := inf
</p>
<p>(
λ :ψ(λ) &lt;&infin;
</p>
<p>)
&le; 0,
</p>
<p>then λ+ &minus; λ&minus; &gt; 0. Everywhere in what follows we will only consider the values
</p>
<p>λ &isin; B :=
{
ψ(λ) &lt;&infin;
</p>
<p>}
&sube; [λ&minus;, λ+]
</p>
<p>for which ψ &prime;(λ) &lt;&infin;. For such λ, the positive martingale
</p>
<p>Xn =
eλSn
</p>
<p>ψn(λ)
, X0 = 1,
</p>
<p>is well-defined so that EXn = 1.
</p>
<p>Theorem 15.2.6 Let ν be an arbitrary stopping time and λ &isin; B . Then
</p>
<p>E
</p>
<p>(
eλSν
</p>
<p>ψ(λ)ν
; ν &lt;&infin;
</p>
<p>)
&le; 1 (15.2.24)
</p>
<p>and, for any s &gt; 1 and r &gt; 1 such that 1/r + 1/s = 1,
</p>
<p>E
(
eλSν ; ν &lt;&infin;
</p>
<p>)
&le;
{
E
[
ψ rν/s(λs); ν &lt;&infin;
</p>
<p>]}1/r
. (15.2.25)
</p>
<p>A necessary and sufficient condition for
</p>
<p>E
</p>
<p>(
eλSν
</p>
<p>ψ(λ)ν
; ν &lt;&infin;
</p>
<p>)
= 1 (15.2.26)
</p>
<p>is that
</p>
<p>lim
n&rarr;&infin;
</p>
<p>E
</p>
<p>(
eλSn
</p>
<p>ψ(λ)n
; ν &gt; n
</p>
<p>)
= 0. (15.2.27)
</p>
<p>Remark 15.2.1 Relation (15.2.26) is known as the fundamental Wald identity. In the
literature it is usually considered for a.s. finite ν (when P(ν &lt;&infin;)= 1) being in that
case an extension of the obvious equality EeλSn = ψn(λ) to the case of random ν.
Originally, identity (15.2.26) was established by A. Wald in the special case where
</p>
<p>ν is the exit time of the sequence {Sn} from a finite interval (see Corollary 15.2.3),
and was accompanied by rather restrictive conditions. Later, these conditions were
</p>
<p>removed (see e.g. [13]). Below we will obtain a more general assertion for the prob-
</p>
<p>lem on the first exit of the trajectory {Sn} from a strip with curvilinear boundaries.</p>
<p/>
</div>
<div class="page"><p/>
<p>472 15 Martingales
</p>
<p>Remark 15.2.2 The fundamental Wald identity shows that, although the nature of
a stopping time could be quite general, there exists a stiff functional constraint
</p>
<p>(15.2.26) on the joint distribution of ν and Sν (the distribution of ζk is assumed
</p>
<p>to be known). In the cases where one of these variables can somehow be &ldquo;com-
</p>
<p>puted&rdquo; or &ldquo;eliminated&rdquo; (see Examples 15.2.2&ndash;15.2.4) Wald&rsquo;s identity turns into an
</p>
<p>explicit formula for the Laplace transform of the distribution of the other variable.
</p>
<p>If ν and Sν prove to be independent (which rarely happens), then (15.2.26) gives the
</p>
<p>relationship
</p>
<p>EeλSν =
[
Eψ(λ)&minus;ν
</p>
<p>]&minus;1
</p>
<p>between the Laplace transforms of the distributions of ν and Sν .
</p>
<p>Proof of Theorem 15.2.6 As we have already noted, for
</p>
<p>Xn = eλSnψ&minus;n(λ), Fn = σ(ζ1, . . . , ζn),
{Xn,Fn; n&ge; 0} is a positive martingale with X0 = 1 and EXn = 1. Corollary 15.2.2
immediately implies (15.2.24).
</p>
<p>Inequality (15.2.25) is a consequence of H&ouml;lder&rsquo;s inequality and (15.2.24):
</p>
<p>E
(
e(λ/s)Sν ;ν &lt;&infin;
</p>
<p>)
= E
</p>
<p>[(
eλSν
</p>
<p>ψν(λ)
</p>
<p>)1/s
ψν/s(λ);ν &lt;&infin;
</p>
<p>]
</p>
<p>&le;
[
E
(
ψνr/s(λ); ν &lt;&infin;
</p>
<p>)]1/r
.
</p>
<p>The last assertion of the theorem (concerning the identity (15.2.26)) follows from
</p>
<p>Theorem 15.2.4. �
</p>
<p>We now consider several important special cases. Note that ψ(λ) is a convex
</p>
<p>function (ψ &prime;&prime;(λ) &gt; 0), ψ(0) = 1, and therefore there exists a unique point λ0 at
which ψ(λ) attains its minimum value ψ(λ0)&le; 1 (see also Sect. 9.1).
</p>
<p>Corollary 15.2.3 Assume that we are given a sequence g(n) such that
</p>
<p>g+(n) := max
(
0, g(n)
</p>
<p>)
= o(n) as n&rarr;&infin;.
</p>
<p>If Sn &le; g(n) holds on the set {ν &gt; n}, then (15.2.26) holds for λ &isin; (λ0, λ+] &cap; B ,
B = {λ :ψ(λ) &lt;&infin;}.
</p>
<p>The random variable ν = νg = inf{k &ge; 1 : Sk &gt; g(k)} for g(k)= o(k) obviously
satisfies the conditions of Corollary 15.2.3. For stopping times νg one could also
</p>
<p>consider the case g(n)/n &rarr; c &ge; 0 as n &rarr; &infin;, which can be reduced to the case
g(n)= o(n) by introducing the random variables
</p>
<p>ζ &lowast;k := ζk &minus; c, S&lowast;k :=
k&sum;
</p>
<p>j=1
ζ &lowast;j ,
</p>
<p>for which νg = inf{k &ge; 1 : S&lowast;k &gt; g(k)&minus; ck}.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 The Martingale Property and Random Change of Time. Wald&rsquo;s Identity 473
</p>
<p>Proof of Corollary 15.2.3 For λ &gt; λ0, λ &isin; B , we have
</p>
<p>E
</p>
<p>(
eλSn
</p>
<p>ψn(λ)
;ν &gt; n
</p>
<p>)
&le; ψ&minus;n(λ)E
</p>
<p>(
eλSn;Sn &le; g(n)
</p>
<p>)
</p>
<p>=ψ&minus;n(λ)E
(
e(λ&minus;λ0)Sn &middot; eλ0Sn;Sn &le; g(n)
</p>
<p>)
</p>
<p>&le; ψ&minus;n(λ)e(λ&minus;λ0)g(n)E
(
eλ0Sn;Sn &le; g(n)
</p>
<p>)
</p>
<p>&le; ψ&minus;n(λ)e(λ&minus;λ0)g+(n)Eeλ0Sn =
(
ψ(λ0)
</p>
<p>ψ(λ)
</p>
<p>)n
e(λ&minus;λ0)g
</p>
<p>+(n) &rarr; 0
</p>
<p>as n&rarr;&infin;, because (λ&minus; λ0)g+(n)= o(n). It remains to use Theorem 15.2.6. The
corollary is proved. �
</p>
<p>We now return to Theorem 15.2.6 for arbitrary stopping times. It turns out that,
</p>
<p>based on the Cram&eacute;r transform introduced in Sect. 9.1, one can complement its
</p>
<p>assertions without using any martingale techniques.
</p>
<p>Together with the original distribution P of the sequence {ζk}&infin;k=1 we introduce the
family of distributions Pλ of this sequence in 〈R&infin;,B&infin;〉 (see Sect. 5.5) generated
by the finite-dimensional distributions
</p>
<p>Pλ(ζk &isin; dxk)=
eλxk
</p>
<p>ψ(λ)
P(ζk &isin; dxk),
</p>
<p>Pλ(ζk &isin; dx1, . . . , ζn &isin; dxn)=
n&prod;
</p>
<p>k=1
Pλ(ζk &isin; dxk).
</p>
<p>This is the Cram&eacute;r transform of the distribution P.
</p>
<p>Theorem 15.2.7 Let ν be an arbitrary stopping time. Then, for any λ &isin; B ,
</p>
<p>E
</p>
<p>(
eλSν
</p>
<p>ψν(λ)
;ν &lt;&infin;
</p>
<p>)
= Pλ(ν &lt;&infin;). (15.2.28)
</p>
<p>Proof Since {ν = n} &isin; σ(ζ1, . . . , ζn), there exists a Borel set Dn &sub;Rn, such that
{ν = n} =
</p>
<p>{
(ζ1, . . . , ζn) &isin;Dn
</p>
<p>}
.
</p>
<p>Further,
</p>
<p>E
</p>
<p>(
eλSν
</p>
<p>ψν(λ)
;ν &lt;&infin;
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=0
E
</p>
<p>(
eλSn
</p>
<p>ψn(λ)
;ν = n
</p>
<p>)
,
</p>
<p>where
</p>
<p>E
</p>
<p>(
eλSn
</p>
<p>ψn(λ)
;ν = n
</p>
<p>)
=
&int;
</p>
<p>(x1,...,xn)&isin;Dn
</p>
<p>eλ(x1+&middot;&middot;&middot;+xn)
</p>
<p>ψn(λ)
P(ζ1 &isin; dx1, . . . , ζn &isin; dx)
</p>
<p>=
&int;
</p>
<p>(x1,...,xn)&isin;Dn
Pλ(ζ1 &isin; dx1, . . . , ζn &isin; dxn)= Pλ(ν = n).
</p>
<p>This proves the theorem. �</p>
<p/>
</div>
<div class="page"><p/>
<p>474 15 Martingales
</p>
<p>For a given function g(n), consider now the stopping time
</p>
<p>ν = νg = inf
{
k : Sk &ge; g(k)
</p>
<p>}
</p>
<p>(cf. Corollary 15.2.3). The assertion of Theorem 15.2.7 can be obtained in that case
</p>
<p>in the following way. Denote by Eλ the expectation with respect to the distribu-
</p>
<p>tion Pλ.
</p>
<p>Corollary 15.2.4 1. If g+(n) = max(0, g(n)) = o(n) as n &rarr; &infin; and λ &isin;
(λ0, λ+] &cap;B , then one has Pλ(νg &lt;&infin;)= 1 in relation (15.2.28).
</p>
<p>2. If g(n)&ge; 0 and λ &lt; λ0, then Pλ(νg &lt;&infin;) &lt; 1.
3. For λ= λ0, the distribution Pλ0 of the variable ν can either be proper (when
</p>
<p>one has Pλ0(νg &lt; &infin;) = 1) or improper (Pλ0(νg &lt; &infin;) &lt; 1). If λ0 &isin; (λ&minus;, λ+),
g(n) &lt; (1 &minus; ε)σ (2 log logn)1/2 for all n &ge; n0, starting from some n0, and σ 2 =
Eλ0ζ
</p>
<p>2
1 , then Pλ(νg &lt;&infin;)= 1.
</p>
<p>But if λ &isin; (λ&minus;, λ+), g(n) &ge; 0, and g(n) &ge; (1 + ε)σ (2 log logn)1/2 for n &ge; n0,
then Pλ(νg &lt;&infin;) &lt; 1 (we exclude the trivial case ζk &equiv; 0).
</p>
<p>Proof Since Eλζk = ψ
&prime;(λ)
</p>
<p>ψ(λ)
, the expectation Eλζk is of the same sign as the differ-
</p>
<p>ence λ&minus; λ0, and Eλ0ζk = 0 (ψ &prime;(λ0)= 0 if λ0 &isin; (λ&minus;, λ+)). Hence the first assertion
follows from the relations
</p>
<p>Pλ(ν =&infin;)= Pλ
(
Xn &lt; g(n) for all n
</p>
<p>)
&lt; P
</p>
<p>(
Xn &lt; g
</p>
<p>+(n)
)
&rarr; 0
</p>
<p>as n&rarr;&infin; by the law of large numbers for the sums Xn =
&sum;n
</p>
<p>k=1 ζk , since Eλζk &gt; 0.
The second assertion is a consequence of the strong law of large numbers since
</p>
<p>Eλζk &lt; 0 and hence Pλ(ν =&infin;)= P(supnXn &le; 0) &gt; 0.
The last assertion of the corollary follows from the law of the iterated logarithm
</p>
<p>which we prove in Sect. 20.2. The corollary is proved. �
</p>
<p>The condition g(n)&ge; 0 of part 2 of the corollary can clearly be weakened to the
condition g(n)= o(n), P(ν &gt; n) &gt; 0 for any n &gt; 0. The same is true for part 3.
</p>
<p>An assertion similar to Corollary 15.2.4 is also true for the (stopping) time νg&minus;,g+
of the first passage of one of the two boundaries g&plusmn;(n)= o(n):
</p>
<p>νg&minus;,g+ := inf
{
k &ge; 1 : Sk &ge; g+(k) or Sk &le; g&minus;(k)
</p>
<p>}
.
</p>
<p>Corollary 15.2.5 For λ &isin; B\{λ0}, we have Pλ(νg&minus;,g+ &lt;&infin;)= 1.
If λ = λ0 &isin; (λ&minus;, λ+), then the Pλ-distribution of ν may be either proper or im-
</p>
<p>proper.
If, for some n0 &gt; 2,
</p>
<p>g&plusmn;(n)≶&plusmn;(1 &minus; ε)σ
&radic;
</p>
<p>2 ln lnn
</p>
<p>for n&ge; n0 then Pλ0(νg&minus;,g+ &lt;&infin;)= 1.
If g&plusmn;(n)≷ 0 and, additionally,
</p>
<p>g&plusmn;(n)≷&plusmn;(1 + ε)σ
&radic;
</p>
<p>2 ln lnn
</p>
<p>for n&ge; n0 then Pλ0(νg&minus;,g+ &lt;&infin;) &lt; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 The Martingale Property and Random Change of Time. Wald&rsquo;s Identity 475
</p>
<p>Proof The first assertion follows from Corollary 15.2.4 applied to the sequences
{&plusmn;Xn}. The second is a consequence of the law of the iterated logarithm from
Sect. 20.2. �
</p>
<p>We now consider several relations following from Corollaries 15.2.3, 15.2.4
</p>
<p>and 15.2.5 (from identity (15.2.26)) for the random variables ν = νg and ν = νg&minus;,g+ .
Let a &lt; 0 and ψ(λ+)&ge; 1. Since ψ &prime;(0)= a &lt; 0 and the function ψ(λ) is convex,
</p>
<p>the equation ψ(λ)= 1 will have a unique root &micro; &gt; 0 in the domain λ &gt; 0. Setting
λ= &micro; in (15.2.26) we obtain the following.
</p>
<p>Corollary 15.2.6 If a &lt; 0 and ψ(λ+) &ge; 1 then, for the stopping times ν = νg and
ν = νg&minus;,g+ , we have the equality
</p>
<p>E
(
e&micro;Sν ; ν &lt;&infin;
</p>
<p>)
= 1.
</p>
<p>Remark 15.2.3 For an x &gt; 0, put (as in Chap. 10) η(x) := inf{k : Sk &gt; 0}. Since
Sη(x) = x+χ(x), where χ(x) := Sη(x)&minus;x is the value of overshoot over the level x,
Corollary 15.2.6 implies
</p>
<p>E
(
e&micro;(x+χ(x)); η(x) &lt;&infin;
</p>
<p>)
= 1. (15.2.29)
</p>
<p>Note that P(η(x) &lt; &infin;) = P(S &gt; x), where S = supk&ge;0 Sk . Therefore, Theo-
rem 12.7.4 and (15.2.29) imply that, as x &rarr;&infin;,
</p>
<p>e&micro;xP
(
η(x) &lt;&infin;
</p>
<p>)
=
[
E
(
e&micro;χ(x)
</p>
<p>∣∣η(x) &lt;&infin;
)]&minus;1 &rarr; c. (15.2.30)
</p>
<p>The last convergence relation corresponds to the fact that the limiting condi-
</p>
<p>tional distribution (as x &rarr; &infin;) G of χ(x) exists given η(x) &lt; &infin;. If we denote
by χ a random variable with the distribution G then (15.2.30) will mean that
</p>
<p>c= [E e&micro;χ ]&minus;1 &lt; 1. This provides an interpretation of the constant c that is different
from the one in Theorem 12.7.4.
</p>
<p>In Corollary 15.2.6 we &ldquo;eliminated&rdquo; the &ldquo;component&rdquo; ψν(λ) in identity (15.2.26).
</p>
<p>&ldquo;Elimination&rdquo; of the other component eλSν is possible only in some special cases of
</p>
<p>random walks, such as the so-called skip-free walks (see Sect. 12.8) or walks with
</p>
<p>exponentially (or geometrically) distributed ζ+k = max(0, ζk) or ζ
&minus;
k =&minus;min(0, ζk).
</p>
<p>We will illustrate this with two examples.
</p>
<p>Example 15.2.3 We return to the ruin problem discussed in Example 15.2.2. In that
case, Corollary 15.2.4 gives, for g&minus;(n) := &minus;z1 and g+(n)= z2, that
</p>
<p>eλz2E
(
ψ(λ)&minus;ν; Sν = z2
</p>
<p>)
+ e&minus;λz1E
</p>
<p>(
ψ(λ)&minus;ν; Sν =&minus;z1
</p>
<p>)
= 1.
</p>
<p>In particular, for z1 = z2 = z and p = 1/2, we have by symmetry that
</p>
<p>E
(
ψ(λ)&minus;ν; Sν = z
</p>
<p>)
= 1
</p>
<p>eλz + e&minus;λz , E
(
ψ(λ)&minus;ν
</p>
<p>)
= 2
</p>
<p>eλz + e&minus;λz . (15.2.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>476 15 Martingales
</p>
<p>Let λ(s) be the unique positive solution of the equation sψ(λ)= 1, s &isin; (0,1). Since
here ψ(λ)= 1
</p>
<p>2
(eλ + e&minus;λ), solving the quadratic equation yields
</p>
<p>eλ(s) = 1 +
&radic;
</p>
<p>1 &minus; s2
s
</p>
<p>.
</p>
<p>Identity (15.2.31) now gives
</p>
<p>Esν = 2
(
eλ(s)z + e&minus;λ(s)z
</p>
<p>)
.
</p>
<p>We obtain an explicit form of the generating function of the random variable ν,
</p>
<p>which enables us to find the probabilities P(ν = n), n= 1,2, . . . by expanding ele-
mentary functions into series.
</p>
<p>Example 15.2.4 Simple explicit formulas can also be obtained from Wald&rsquo;s identity
in the problem with one boundary, where ν = νg , g(n)= z. In that case, the class of
distributions of ζk could be wider than in Example 15.2.3. Suppose that one of the
</p>
<p>two following conditions holds (cf. Sect. 12.8).
</p>
<p>1. The transform walk is arithmetic and skip-free, i.e. ζk are integers, P(ξk = 1) &gt; 0
and P(ζk &ge; 2)= 0.
</p>
<p>2. The walk is right exponential, i.e.
</p>
<p>P(ζk &gt; t)= ce&minus;αt (15.2.32)
either for all t &gt; 0 or for t = 0,1,2, . . . if the walk is integer-valued (the geo-
metric distribution).
</p>
<p>The random variable νg will be proper if and only if Eξk = ψ &prime;(0) &ge; 0 (see
Chaps. 9 and 12). For skip-free random walks, Wald&rsquo;s identity (15.2.26) yields
</p>
<p>(g(n)= z &gt; 0, Sν = z)
eλzEψ&minus;ν(λ)= 1, λ &gt; λ0. (15.2.33)
</p>
<p>For s &le; 1, the equation ψ(λ)= s&minus;1 (cf. Example 15.2.3) has in the domain λ &gt; λ0
a unique solution λ(s). Therefore identity (15.2.33) can be written as
</p>
<p>Esν = e&minus;zλ(s). (15.2.34)
This statement implies a series of results from Chaps. 9 and 12. Many properties
</p>
<p>of the distribution of ν := νz can be derived from this identity, in particular, the
asymptotics of P(νz = n) as z&rarr;&infin;, n&rarr;&infin;. We already know one of the ways to
find this asymptotics. It consists of using Theorem 12.8.4, which implies
</p>
<p>P(νz = n)=
x
</p>
<p>n
P(Sn = z), (15.2.35)
</p>
<p>and the local Theorem 9.3.4 providing the asymptotics of P(Sn = z). Using rela-
tion (15.2.34) and the inversion formula is an alternative approach to studying the
</p>
<p>asymptotics of P(νz = n). If we use the inversion formula, there will arise an integral
of the form &int;
</p>
<p>|s|=1
s&minus;ne&minus;z&micro;(s) ds, (15.2.36)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Inequalities 477
</p>
<p>where the integrand s&minus;ne&minus;z&micro;(s), after the change of variable &micro;(s)=λ (or s =
ψ(λ)&minus;1), takes the form
</p>
<p>exp&minus;
{
zλ&minus; n lnψ(λ)
</p>
<p>}
.
</p>
<p>The integrand in the inversion formula for the probability P(Sn = z) has the same
form. This probability has already been studied quite well (see Theorem 9.3.4); its
</p>
<p>exponential part has the form e&minus;nΛ(α), where α = z/n, Λ(α)= supλ(αλ&minus; lnψ(λ))
is the large deviation rate function (see Sect. 9.1 and the footnote for Defini-
</p>
<p>tion 9.1.1). A more detailed study of the inversion formula (15.2.36) allows us to
</p>
<p>obtain (15.2.35).
</p>
<p>Similar relations can be obtained for random walks with exponential right dis-
</p>
<p>tribution tails. Let, for example, (15.2.32) hold for all t &gt; 0. Then the conditional
</p>
<p>distribution P(Sν &gt; t |ν = n,Sn&minus;1 = x) coincides with the distribution
P(ζn &gt; z&minus; x + t |ζn &gt; z&minus; x)= e&minus;αt
</p>
<p>and clearly depends neither on n nor on x. This means that ν and Sν are independent,
</p>
<p>Sν = z+ γ , γ &sub;=Ŵα ,
</p>
<p>Eψ(λ)&minus;ν = 1
Ee(z+γ )λ
</p>
<p>= e&minus;λz α &minus; λ
α
</p>
<p>, λ0 &lt; λ&lt; α; Esν = e&minus;zλ(s)
α &minus; λ(s)
</p>
<p>α
,
</p>
<p>where λ(s) is, as before, the only solution to the equation ψ(λ)= s&minus;1 in the domain
λ &gt; λ0. This implies the same results as (15.2.34).
</p>
<p>If P(ζk &gt; t)= c1e&minus;αt and P(ζk &lt;&minus;t)= c2e&minus;βt , t &gt; 0, then, in the problem with
two boundaries, we obtain for ν = νg&minus;,g+ , g+(n)= z2 and g&minus;(n)=&minus;z1 in exactly
the same way from (15.2.26) that
</p>
<p>αeλz2
</p>
<p>α &minus; λE
(
ψ&minus;ν(λ); Sν &ge; z2
</p>
<p>)
+ βe
</p>
<p>&minus;λz1
</p>
<p>β + λ E
(
ψ&minus;ν(λ); Sν &le;&minus;z1
</p>
<p>)
= 1, λ &isin; (&minus;β,α).
</p>
<p>15.3 Inequalities
</p>
<p>15.3.1 Inequalities for Martingales
</p>
<p>First of all we note that the property EXn &le; 1 of the sequence Xn = eλSnψ0(λ)&minus;n
forming a supermartingale for an appropriate function ψ0(λ) remains true when we
</p>
<p>replace n with a stopping time ν (an analogue of inequality (15.2.24)) in a much
</p>
<p>more general case than that of Theorem 15.2.6. Namely, ζk may be dependent.
</p>
<p>Let, as before, {Fn} be an increasing sequence of σ -algebras, and ζn be
Fn-measurable random variables. Suppose that a.s.
</p>
<p>E
(
eλζn
</p>
<p>∣∣Fn&minus;1
)
&le;ψ0(λ). (15.3.1)
</p>
<p>This condition is always met if a.s.
</p>
<p>P(ζn &ge; x|Fn&minus;1)&le;G(x), ψ0(λ)=&minus;
&int;
</p>
<p>eλx dG(x) &lt;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>478 15 Martingales
</p>
<p>In that case the sequence Xn = eλSnψ&minus;n0 (λ) forms a supermartingale:
</p>
<p>E(Xn|Fn&minus;1)&le;Xn&minus;1, EXn &le; 1.
</p>
<p>Theorem 15.3.1 Let (15.3.1) hold and ν be a stopping time. Then inequalities
(15.2.24) and (15.2.25) will hold true with ψ replaced by ψ0.
</p>
<p>The Proof of the theorem repeats almost verbatim that of Theorem 15.2.6. �
</p>
<p>Now we will obtain inequalities for the distribution of
</p>
<p>Xn = max
k&le;n
</p>
<p>Xk and X
&lowast;
n = max
</p>
<p>k&le;n
|Xk|,
</p>
<p>Xn being an arbitrary submartingale.
</p>
<p>Theorem 15.3.2 (Doob) Let {Xn,Fn; n &ge; 0} be a nonnegative submartingale.
Then, for all x &ge; 0 and n&ge; 0,
</p>
<p>P(Xn &gt; x)&le;
1
</p>
<p>x
EXn.
</p>
<p>Proof Let
</p>
<p>ν = η(x) := inf{k &ge; 0 : Xk &gt; x}, ν(n) := min(ν, n).
</p>
<p>It is obvious that n and ν(n) are stopping times, ν(n)&le; n, and therefore, by Theo-
rem 15.2.1 (see (15.2.3) for ν2 = n, ν1 = ν(n)),
</p>
<p>EXn &ge; EXν(n).
</p>
<p>Observing that {Xn &gt; x} = {Xν(n) &gt; x}, we have from Chebyshev&rsquo;s inequality that
</p>
<p>P(Xn &gt; x)= P(Xν(n) &gt; x)&le;
1
</p>
<p>x
EXν(n) &le;
</p>
<p>1
</p>
<p>x
EXn.
</p>
<p>The theorem is proved. �
</p>
<p>Theorem 15.3.2 implies the following.
</p>
<p>Theorem 15.3.3 (The second Kolmogorov inequality) Let {Xn,Fn; n &ge; 0} be a
martingale with a finite second moment EX2n. Then {X2n,Fn; n&ge; 0} is a submartin-
gale and by Theorem 15.3.2
</p>
<p>P
(
X&lowast;n &gt; x
</p>
<p>)
&le; 1
</p>
<p>x2
EX2n.
</p>
<p>Originally A.N. Kolmogorov established this inequality for sums Xn = ξ1 +
&middot; &middot; &middot;+ξn of independent random variables ξn. Theorem 15.3.3 extends Kolmogorov&rsquo;s
proof to the case of submartingales and refines Chebyshev&rsquo;s inequality.
</p>
<p>The following generalisation of Theorem 15.3.3 is also valid.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Inequalities 479
</p>
<p>Theorem 15.3.4 If {Xn,Fn; n&ge; 0} is a martingale and E|Xn|p &lt;&infin;, p &ge; 1, then
{|Xn|p,Fn; n&ge; 0} forms a nonnegative submartingale and, for all x &gt; 0,
</p>
<p>P
(
X&lowast;n &ge; x
</p>
<p>)
&le; 1
</p>
<p>xp
E|Xn|p.
</p>
<p>If {Xn,Fn; n&ge; 0} is a submartingale, EeλXn &lt;&infin;, λ &gt; 0, then {eλXn,Fn; n&ge; 0}
also forms a nonnegative submartingale,
</p>
<p>P(Xn &ge; x)&le; e&minus;λxEeλXn .
</p>
<p>Both Theorem 15.3.4 and Theorem 15.3.3 immediately follow from Lem-
</p>
<p>ma 15.1.3 and Theorem 15.3.2.
</p>
<p>If Xn = Sn =
&sum;n
</p>
<p>k=1 ζk , where ζk are independent, identically distributed and
satisfy the Cram&eacute;r condition: λ+ = sup{λ : ψ(λ) &lt;&infin;}&gt; 0, then, with the help of
the fundamental Wald identity, one can obtain sharper inequalities for P(Xn &gt; x) in
</p>
<p>the case a = Eξk &lt; 0.
Recall that, in the case a = ψ &prime;(0) &lt; 0, the function ψ(λ)= Eeλζk decreases in a
</p>
<p>neighbourhood of λ= 0, and, provided that ψ(λ+)&ge; 1, the equation ψ(λ)= 1 has
a unique solution &micro; in the domain λ &gt; 0.
</p>
<p>Let ζ be a random variable having the same distribution as ζk . Put
</p>
<p>ψ+ := sup
t&gt;0
</p>
<p>E
(
e&micro;(ζ&minus;t)
</p>
<p>∣∣ζ &gt; t
)
, ψ&minus; := inf
</p>
<p>t&gt;0
E
(
e&micro;(ζ&minus;t)
</p>
<p>∣∣ζ &gt; t
)
.
</p>
<p>If, for instance, P(ζ &gt; t) = ce&minus;αt for t &gt; 0 (in this case necessarily α &gt; &micro; in
(15.2.32)), then
</p>
<p>P(ζ &minus; t &gt; v|ζ &gt; t)= P(ζ &gt; t + v)
P(ζ &gt; t)
</p>
<p>= e&minus;αv, ψ+ =ψ&minus; =
α
</p>
<p>α &minus;&micro;.
</p>
<p>A similar equality holds for integer-valued ξ with a geometric distribution.
</p>
<p>For other distributions, one has ψ+ &gt;ψ&minus;.
Under the above conditions, one has the following assertion which supplements
</p>
<p>Theorem 12.7.4 for the distribution of the random variable S = supk Sk .
</p>
<p>Theorem 15.3.5 If a = Eζ &lt; 0 then
</p>
<p>ψ&minus;1+ e
&minus;&micro;x &le; P(S &gt; x)&le;ψ&minus;1&minus; e&minus;&micro;x, x &gt; 0. (15.3.2)
</p>
<p>This theorem implies that, in the case of exponential right tails of the distribution
</p>
<p>of ζ (see (15.2.32)), inequalities (15.3.2) become the exact equality
</p>
<p>P(S &gt; x)= α &minus;&micro;
α
</p>
<p>e&minus;&micro;x .
</p>
<p>(The same result was obtained in Example 12.5.1.) This means that inequalities
</p>
<p>(15.3.2) are unimprovable. Since Sn = maxk&le;n Sk &le; S, relation (15.3.2) implies
that, for any n,
</p>
<p>P(Sn &gt; x)&le;ψ&minus;1&minus; e&minus;&micro;x .</p>
<p/>
</div>
<div class="page"><p/>
<p>480 15 Martingales
</p>
<p>Proof of Theorem 15.3.5 Set ν := &infin; if S = supk&ge;0 Sk &le; x, and put ν := η(x) =
min{k : Sk &gt; x} otherwise. Further, let χ(x) := Sη(x) &minus; x be the excess of the
level x. We have
</p>
<p>P
(
χ(x) &gt; v; ν &lt;&infin;
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>&int; x
</p>
<p>&minus;&infin;
P(Sk&minus;1 &le; x, Sk&minus;1 &isin; du, ζk &gt; x &minus; u+ v)
</p>
<p>=
&infin;&sum;
</p>
<p>k=1
</p>
<p>&int; x
</p>
<p>&minus;&infin;
P(Sk&minus;1 &le; x, Sk&minus;1 &isin; du, ζk &gt; x &minus; u)
</p>
<p>&times; P(ζk &gt; x &minus; u+ v|ζk &gt; x &minus; u),
</p>
<p>E
(
e&micro;χ(x); ν &lt;&infin;
</p>
<p>)
&le;
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>&int; x
</p>
<p>&minus;&infin;
P(Sk&minus;1 &le; x, Sk&minus;1 &isin; du, ζk &gt; x &minus; u)ψ+
</p>
<p>=ψ+
&infin;&sum;
</p>
<p>k=1
P(ν = k)=ψ+P(ν &lt;&infin;).
</p>
<p>Similarly,
</p>
<p>E
(
e&micro;χ(x); ν &lt;&infin;
</p>
<p>)
&ge;ψ&minus;P(ν &lt;&infin;).
</p>
<p>Next, by Corollary 15.2.6,
</p>
<p>1 = E
(
e&micro;Sν ; ν &lt;&infin;
</p>
<p>)
= e&micro;xE
</p>
<p>(
e&micro;χ(x); ν &lt;&infin;
</p>
<p>)
&le; e&micro;xψ+ P(ν &lt;&infin;).
</p>
<p>Because P(ν &lt; &infin;) = P(S &gt; x), we get from this the right inequality of The-
orem 15.3.5. The left inequality is obtained in the same way. The theorem is
</p>
<p>proved. �
</p>
<p>Remark 15.3.1 We proved Theorem 15.3.5 with the help of the fundamental Wald
identity. But there is a direct proof based on the following relations:
</p>
<p>ψn(λ)= EeλSn &ge;
n&sum;
</p>
<p>k=1
E
(
e(Sk+Sn&minus;Sk)λ; ν = k
</p>
<p>)
</p>
<p>=
n&sum;
</p>
<p>k=1
E
(
e(x+χ(x))λe(Sn&minus;Sk)λ; ν = k
</p>
<p>)
. (15.3.3)
</p>
<p>Here the random variables eλχ(x)I(ν = k) and Sn &minus; Sk are independent and, as be-
fore,
</p>
<p>E
(
eλχ(x); ν = k
</p>
<p>)
&ge;ψ&minus; P(ν = k).
</p>
<p>Therefore, for all λ such that ψ(λ)&le; 1,
</p>
<p>ψn(λ)&ge; eλxψ&minus;
n&sum;
</p>
<p>k=1
ψn&minus;k(λ)P(ν = k)&ge;ψ&minus;eλxψn(λ)P(ν &le; n).
</p>
<p>Hence we obtain
</p>
<p>P(Sn &gt; x)= P(ν &le; n)&le;ψ&minus;1&minus; e&minus;λx .</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Inequalities 481
</p>
<p>Since the right-hand side does not depend on n, the same inequality also holds for
</p>
<p>P(S &gt; x). The lower bound is obtained in a similar way. One just has to show that,
</p>
<p>in the original equality (cf. (15.3.3))
</p>
<p>ψn(λ)=
n&sum;
</p>
<p>k=1
E
(
eλSn; ν = k
</p>
<p>)
+E
</p>
<p>(
eλSn; ν &gt; n
</p>
<p>)
,
</p>
<p>one has E(eλSn; ν &gt; n)= o(1) as n&rarr;&infin; for λ= &micro;, which we did in Sect. 15.2.
</p>
<p>15.3.2 Inequalities for the Number of Crossings of a Strip
</p>
<p>We now return to arbitrary submartingales Xn and prove an inequality that will be
</p>
<p>necessary for the convergence theorems of the next section. It concerns the number
</p>
<p>of crossings of a strip by the sequence Xn. Let a &lt; b be given numbers. Set ν0 = 0,
ν1 :=min{n &gt; 0 :Xn &le; a}, ν2 :=min{n &gt; ν1 :Xn &ge; b},
</p>
<p>. . . . . . . . . . . . . . . . . . . . . .
</p>
<p>ν2k&minus;1 :=min{n &gt; ν2k&minus;2 :Xn &le; a}, ν2k :=min{n &gt; ν2k&minus;1 :Xn &ge; b}.
We put νm := &infin; if the path {Xn} for n &ge; νm&minus;1 never crosses the corresponding
level. Using this notation, one can define the number of upcrossings of the strip
</p>
<p>(interval) [a, b] by the trajectory X0, . . . ,Xn as the random variable
</p>
<p>ν(a, b;n) :=
{
</p>
<p>max{k : ν2k &le; n} if ν2 &le; n,
0 if ν2 &gt; n.
</p>
<p>Set (a)+ = max(0, a).
</p>
<p>Theorem 15.3.6 (Doob) Let {Xn,Fn; n&ge; 0} be a submartingale. Then, for all n,
</p>
<p>Eν(a, b;n)&le; E(Xn &minus; a)
+
</p>
<p>b&minus; a . (15.3.4)
</p>
<p>It is clear that inequality (15.3.4) assumes by itself that only the submartingale
</p>
<p>{Xn,Fn;0 &le; k &le; n} is given.
</p>
<p>Proof The random variable ν(a, b; n) coincides with the number of upcrossings of
the interval [0, b &minus; a] by the sequence (Xn &minus; a)+. Now {(Xn &minus; a)+,Fn; n &ge; 0}
is a nonnegative submartingale (see Example 15.1.4) and therefore, without loss of
</p>
<p>generality, one can assume that a = 0 and Xn &ge; 0, and aim to prove that
</p>
<p>Eν(0, b; n)&le; EXn
b
</p>
<p>.
</p>
<p>Let
</p>
<p>ηj :=
{
</p>
<p>1 if νk &lt; j &le; νk+1 for some odd k,
0 if νk &lt; j &le; νk+1 for some even k.</p>
<p/>
</div>
<div class="page"><p/>
<p>482 15 Martingales
</p>
<p>Fig. 15.1 Illustration to the
</p>
<p>proof of Theorem 15.3.6
</p>
<p>showing the locations of the
</p>
<p>random times ν1, ν2, and ν3
(here a = 0)
</p>
<p>In Fig. 15.1, ν1 = 2, ν2 = 5, ν3 = 8; ηj = 0 for j &le; 2, ηj = 1 for 3 &le; j &le; 5 etc.
It is not hard to see (using the Abel transform) that (with X0 = 0, η0 = 1)
</p>
<p>η0X0 +
n&sum;
</p>
<p>1
</p>
<p>ηj (Xj &minus;Xj&minus;1)=
n&minus;1&sum;
</p>
<p>0
</p>
<p>Xj (ηj &minus; ηj+1)+ ηnXn &ge; bν(0, b;n).
</p>
<p>Moreover (here N1 denotes the set of odd numbers),
</p>
<p>{ηj = 1} =
⋃
</p>
<p>k&isin;N1
{νk &lt; j &le; νk+1} =
</p>
<p>⋃
</p>
<p>k&isin;N1
</p>
<p>[
{νk &le; j &minus; 1} &minus; {νk+1 &le; j &minus; 1}
</p>
<p>]
&isin; Fj&minus;1.
</p>
<p>Therefore, by virtue of the relation E(Xj |Fj&minus;1)&minus;Xj&minus;1 &ge; 0, we obtain
</p>
<p>bEν(0, b; n)&le; E
n&sum;
</p>
<p>1
</p>
<p>ηj (Xj &minus;Xj&minus;1)=
n&sum;
</p>
<p>1
</p>
<p>E(Xj &minus;Xj&minus;1; ηj = 1)
</p>
<p>=
n&sum;
</p>
<p>1
</p>
<p>E
[
E(Xj &minus;Xj&minus;1|Fj&minus;1); ηj = 1
</p>
<p>]
=
</p>
<p>n&sum;
</p>
<p>1
</p>
<p>E
[
E(Xj |Fj&minus;1)&minus;Xj&minus;1; ηj = 1
</p>
<p>]
</p>
<p>&le;
n&sum;
</p>
<p>1
</p>
<p>E
[
E(Xj |Fj&minus;1)&minus;Xj&minus;1
</p>
<p>]
=
</p>
<p>n&sum;
</p>
<p>1
</p>
<p>E(Xj &minus;Xj&minus;1)= EXn.
</p>
<p>The theorem is proved. �
</p>
<p>15.4 Convergence Theorems
</p>
<p>Theorem 15.4.1 (Doob&rsquo;s martingale convergence theorem) Let
</p>
<p>{Xn,Fn; &minus;&infin;&lt; n&lt;&infin;}
</p>
<p>be a submartingale. Then
</p>
<p>(1) The limit X&minus;&infin; := limn&rarr;&minus;&infin;Xn exists a.s., EX+&minus;&infin; &lt; &infin;, and the process
{Xn,Fn; &minus;&infin;&le; n &lt;&infin;} is a submartingale.
</p>
<p>(2) If supnEX
+
n &lt;&infin; then X&infin; := limn&rarr;&infin;Xn exists a.s. and EX+&infin; &lt;&infin;. If, more-
</p>
<p>over, supnE|Xn|&lt;&infin; then E|X&infin;|&lt;&infin;.
(3) The random sequence {Xn,Fn; &minus;&infin; &le; n &le; &infin;} forms a submartingale if and
</p>
<p>only if the sequence {X+n } is uniformly integrable.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Convergence Theorems 483
</p>
<p>Proof (1) Since
</p>
<p>{lim supXn &gt; lim infXn} =
⋃
</p>
<p>rational
a,b
</p>
<p>{lim supXn &gt; b &gt; a &gt; lim infXn}
</p>
<p>(here the limits are taken as n&rarr;&minus;&infin;), the assumption on divergence with positive
probability
</p>
<p>P(lim supXn &gt; lim infXn) &gt; 0
</p>
<p>means that there exist rational numbers a &lt; b such that
</p>
<p>P(lim supXn &gt; b &gt; a &gt; lim infXn) &gt; 0. (15.4.1)
</p>
<p>Let ν(a, b; m) be the number of upcrossings of the interval [a, b] by the sequence
Y1 =X&minus;m, . . . , Ym =X&minus;1 and ν(a, b)= limm&rarr;&infin; ν(a, b;m). Then (15.4.1) means
that
</p>
<p>P
(
ν(a, b)=&infin;
</p>
<p>)
&gt; 0. (15.4.2)
</p>
<p>By Theorem 15.3.6 (applied to the sequence Y1, . . . , Ym),
</p>
<p>Eν(a, b; m)&le; E(X&minus;1 &minus; a)
+
</p>
<p>b&minus; a &le;
EX+&minus;1 + |a|
</p>
<p>b&minus; a , (15.4.3)
</p>
<p>Eν(a, b)&le;
EX+&minus;1 + |a|
</p>
<p>b&minus; a . (15.4.4)
</p>
<p>Inequality (15.4.4) contradicts (15.4.2) and hence proves that
</p>
<p>P(lim supXn = lim infXn)= 1.
Moreover, by the Fatou&ndash;Lebesgue theorem (X+&minus;&infin; := lim infX+n ),
</p>
<p>EX+&minus;&infin; &le; lim infX+n &le; EX+&minus;1 &lt;&infin;. (15.4.5)
Here the second inequality follows from the fact that {X+n ,Fn} is also a submartin-
gale (see Lemma 15.1.3) and therefore EX+n &uarr;.
</p>
<p>By Lemma 15.1.2, to prove that {Xn,Fn; &minus;&infin; &le; n &lt;&infin;} is a submartingale, it
suffices to verify that, for any A &isin; F&minus;&infin; &sub; F,
</p>
<p>E(X&minus;&infin;; A)&le; E(Xn; A). (15.4.6)
Set Xn(a) := max(Xn, a). By Lemma 15.1.4, {Xn(a),Fn; n &le; 0} is a uniformly
integrable submartingale. Therefore, for any &minus;&infin;&lt; k &lt; n,
</p>
<p>E
(
Xk(a); A
</p>
<p>)
&le; E
</p>
<p>(
Xn(a); A
</p>
<p>)
,
</p>
<p>E
(
X&minus;&infin;(a); A
</p>
<p>)
= lim
</p>
<p>k&rarr;&minus;&infin;
E
(
Xk(a); A
</p>
<p>)
&le; E
</p>
<p>(
Xn(a); A
</p>
<p>)
.
</p>
<p>(15.4.7)
</p>
<p>Letting a &rarr;&minus;&infin; we obtain (15.4.6) from the monotone convergence theorem.
(2) The second assertion of the theorem is proved in the same way. One just has
</p>
<p>to replace the right-hand sides of (15.4.3) and (15.4.4) with EX+n and supnEX
+
n ,
</p>
<p>respectively. Instead of (15.4.5) we get (the limits here are as n&rarr;&infin;)
EX+&infin; &le; lim infEX+n &lt;&infin;,</p>
<p/>
</div>
<div class="page"><p/>
<p>484 15 Martingales
</p>
<p>and if supnE|Xn|&lt;&infin; then
E|X&infin;| &le; lim infE|Xn|&lt;&infin;.
</p>
<p>(3) The last assertion of the theorem is proved in exactly the same way as the
</p>
<p>first one&mdash;the uniform integrability enables us to deduce along with (15.4.7) that,
</p>
<p>for any A &isin; Fn,
E
(
X&infin;(a); A
</p>
<p>)
= lim
</p>
<p>k&rarr;&infin;
E
(
Xk(a); A
</p>
<p>)
&ge; E
</p>
<p>(
Xn(a); A
</p>
<p>)
.
</p>
<p>The converse part of the third assertion of the theorem follows from Lemma 15.1.4.
</p>
<p>The theorem is proved. �
</p>
<p>Now we will obtain some consequences of Theorem 15.4.1.
</p>
<p>So far (see Sect. 4.8), while studying convergence of conditional expectations,
</p>
<p>we dealt with expectations of the form E(Xn|F). Now we can obtain from Theo-
rem 15.4.1 a useful theorem on convergence of conditional expectations of another
</p>
<p>type.
</p>
<p>Theorem 15.4.2 (L&eacute;vy) Let a nondecreasing family F1 &sube; F2 &sube; &middot; &middot; &middot; &sube; F of σ -
algebras and a random variable ξ , with E|ξ |&lt;&infin;, be given on a probability space
〈Ω,F,P〉. Let, as before, F&infin; := σ(
</p>
<p>⋃
n Fn) be the σ -algebra generated by events
</p>
<p>from F1,F2, . . . . Then, as n&rarr;&infin;,
</p>
<p>E(ξ |Fn)
a.s.&minus;&rarr; E(ξ |F&infin;). (15.4.8)
</p>
<p>Proof Set Xn := E(ξ |Fn). We already know (see Example 15.1.3) that the sequence
{Xn,Fn; 1 &lt; n &le; &infin;} is a martingale and therefore, by Theorem 15.4.1, the limit
limn&rarr;&infin;Xn =X(&infin;) exists a.s. It remains to prove that X(&infin;) = E(ξ |F&infin;) (i.e., that
X(&infin;) = X&infin;). Since {Xn,Fn; 1 &le; n &le; &infin;} is by Lemma 15.1.4 a uniformly inte-
grable martingale,
</p>
<p>E(X(&infin;); A)= lim
n&rarr;&infin;
</p>
<p>E(Xn; A)= lim
n&rarr;&infin;
</p>
<p>E
(
E(ξ |Fn); A
</p>
<p>)
= E(ξ ; A)
</p>
<p>for A &isin; Fk and any k = 1,2, . . . This means that the left- and right-hand sides of
the last relation, being finite measures, coincide on the algebra
</p>
<p>⋃&infin;
n=1 Fn. By the
</p>
<p>theorem on extension of a measure (see Appendix 1), they will coincide for all
</p>
<p>A &isin; σ(
⋃&infin;
</p>
<p>n=1 Fn)= F&infin;. Therefore, by the definition of conditional expectation,
X(&infin;) = E(ξ |F&infin;)=X&infin;.
</p>
<p>The theorem is proved. �
</p>
<p>We could also note that the uniform integrability of {Xn,Fn; 1 &le; n&le;&infin;} implies
that
</p>
<p>a.s.&minus;&rarr; in (47) can be replaced by (1)&minus;&rarr;.
Theorem 15.4.1 implies the strong law of large numbers. Indeed, turn to our Ex-
</p>
<p>ample 15.1.4. By Theorem 15.4.1, the limit X&minus;&infin; = limn&rarr;&minus;&infin;Xn = limn&rarr;&infin; n&minus;1Sn
exists a.s. and is measurable with respect to the tail (trivial) σ -algebra, and therefore
</p>
<p>it is constant with probability 1. Since EX&minus;&infin; = Eξ1, we have n&minus;1Sn
a.s.&minus;&rarr; Eξ1.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Convergence Theorems 485
</p>
<p>One can also obtain some extensions of the theorems on series convergence of
</p>
<p>Chap. 11 to the case of dependent variables. Let
</p>
<p>Xn = Sn =
n&sum;
</p>
<p>k=1
ξk
</p>
<p>and Xn form a submartingale (E(ξn+1|Fn) &ge; 0). Let, moreover, E|Xn| &lt; c for all
n and for some c &lt; &infin;. Then the limit S&infin; = limn&rarr;&infin; Sn exists a.s. (As well as
Theorem 15.4.1, this assertion is a generalisation of the monotone convergence the-
</p>
<p>orem. The crucial role is played here by the condition that E|Xn| is bounded.) In
particular, if ξk are independent, Eξk = 0, and the variances σ 2k of ξk are such that&sum;&infin;
</p>
<p>k=1 σ
2
k &lt; σ
</p>
<p>2 &lt;&infin;, then
</p>
<p>E|Xn| &le;
(
EX2n
</p>
<p>)1/2 &le;
(
</p>
<p>n&sum;
</p>
<p>k=1
σ 2k
</p>
<p>)1/2
&le; σ &lt;&infin;,
</p>
<p>and therefore Sn
a.s.&minus;&rarr; S&infin;. Thus we obtain, as a consequence, the Kolmogorov theo-
</p>
<p>rem on series convergence.
</p>
<p>Example 15.4.1 Consider a branching process {Zn} (see Sect. 7.7). We know that
Zn admits a representation
</p>
<p>Zn = ζ1 + &middot; &middot; &middot; + ζZn&minus;1 ,
where the ζk are identically distributed integer-valued random variables independent
</p>
<p>of each other and of Zn&minus;1, ζk being the number of descendants of the k-th particle
from the (n &minus; 1)-th generation. Assuming that Z0 = 1 and setting &micro; := Eζk , we
obtain
</p>
<p>E(Zn|Zn&minus;1)= &micro;Zn&minus;1, EZn = &micro;EZn&minus;1 = &micro;n.
This implies that Xn = Zn/&micro;n is a martingale, because
</p>
<p>E(Xn|Xn&minus;1)= &micro;1&minus;nZn&minus;1 =Xn&minus;1.
</p>
<p>For branching processes we have the following.
</p>
<p>Theorem 15.4.3 The sequence Xn = &micro;&minus;nZn converges almost surely to a proper
random variable X with EX &lt;&infin;. The ch.f. ϕ(λ) of the random variable X satisfies
the equation
</p>
<p>ϕ(&micro;λ)= p
(
ϕ(λ)
</p>
<p>)
,
</p>
<p>where p(v)= Evζk .
</p>
<p>Theorem 15.4.3 means that &micro;&minus;nZn has a proper limiting distribution as n&rarr;&infin;.
</p>
<p>Proof Since Xn &ge; 0 and EXn = 1, the first assertion follows immediately from
Theorem 15.4.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>486 15 Martingales
</p>
<p>Since EzZn is equal to the n-th iteration of the function f (z), for the ch.f. of Zn
we have (ϕη(λ) := Eeiλη)
</p>
<p>ϕZn(λ)= p
(
ϕZn&minus;1(λ)
</p>
<p>)
,
</p>
<p>ϕXn(λ)= ϕZn
(
&micro;&minus;nλ
</p>
<p>)
= p
</p>
<p>(
ϕZn&minus;1
</p>
<p>(
&micro;&minus;nλ
</p>
<p>))
= p
</p>
<p>(
ϕXn&minus;1
</p>
<p>(
λ
</p>
<p>&micro;
</p>
<p>))
.
</p>
<p>Because Xn &rArr;X and the function p is continuous, from this we obtain the equation
for the ch.f. of the limiting distribution X:
</p>
<p>ϕ(λ)= p
(
ϕ
</p>
<p>(
λ
</p>
<p>&micro;
</p>
<p>))
.
</p>
<p>The theorem is proved. �
</p>
<p>In Sect. 7.7 we established that in the case &micro;&le; 1 the process Zn becomes extinct
with probability 1 and therefore P(X = 0)= 1. We verify now that, for &micro; &gt; 1, the
distribution of X is nondegenerate (not concentrated at zero). It suffices to prove
</p>
<p>that {Xn,0 &le; n&le;&infin;} forms a martingale and consequently
EX = EXn 
= 0.
</p>
<p>By Theorem 15.4.1, it suffices to verify that the sequence Xn is uniformly integrable.
</p>
<p>To simplify the reasoning, we suppose that Var(ζk) = σ 2 &lt;&infin; and show that then
EX2n &lt; c &lt;&infin; (this certainly implies the required uniform integrability of Xn, see
Sect. 6.1). One can directly verify the identity
</p>
<p>Z2n &minus;&micro;2n =
n&sum;
</p>
<p>k=1
</p>
<p>[
Z2k &minus; (&micro;Zk&minus;1)2
</p>
<p>]
&micro;2n&minus;2k.
</p>
<p>Since E[Z2k &minus; (&micro;Zk&minus;1)2|Zk&minus;1] = σ 2Zk&minus;1 (recall that Var(η)= E(η2 &minus; (Eη)2)), we
have
</p>
<p>Var(Zn)= E
(
Z2n &minus;&micro;2n
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>k=1
&micro;2n&minus;2kσ 2EZk&minus;1
</p>
<p>= &micro;2nσ 2
n&sum;
</p>
<p>k=1
&micro;&minus;k&minus;1 = σ
</p>
<p>2&micro;n(&micro;n &minus; 1)
&micro;(&micro;&minus; 1) ,
</p>
<p>EX2n = &micro;&minus;2nEZ2n = 1 +
σ 2(1 &minus;&micro;&minus;n)
&micro;(&micro;&minus; 1) &le; 1 +
</p>
<p>σ 2
</p>
<p>&micro;(&micro;&minus; 1) .
</p>
<p>Thus we have proved that X is a nondegenerate random variable,
</p>
<p>EX = 1, Var(Xn)&rarr;
σ 2
</p>
<p>&micro;(&micro;&minus; 1) .
</p>
<p>From the last relation one can easily obtain that Var(X)= σ 2
&micro;(&micro;&minus;1) . To this end one
</p>
<p>can, say, prove that Xn is a Cauchy sequence in mean quadratic and hence (see
</p>
<p>Theorem 6.1.3) Xn
(2)&minus;&rarr;X.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Boundedness of the Moments of Stochastic Sequences 487
</p>
<p>15.5 Boundedness of the Moments of Stochastic Sequences
</p>
<p>When one uses convergence theorems for martingales, conditions ensuring bound-
</p>
<p>edness of the moments of stochastic sequences {Xn,Fn} are of significant interest
(recall that the boundedness of EXn is one of the crucial conditions for convergence
</p>
<p>of submartingales). The boundedness of the moments, in turn, ensures that Xn is
</p>
<p>stochastically bounded, i.e., that supn P(Xn &gt;N)&rarr; 0 as N &rarr;&infin;. The last bound-
edness is also of independent interest in the cases where one is not able to prove, for
</p>
<p>the sequence {Xn}, convergence or any other ergodic properties.
For simplicity&rsquo;s sake, we confine ourselves to considering nonnegative sequences
</p>
<p>Xn &ge; 0. Of course, if we could prove convergence of the distributions of Xn to a
limiting distribution, as was the case for Markov chains or submartingales in The-
</p>
<p>orem 15.4.1, then we would have a more detailed description of the asymptotic
</p>
<p>behaviour of Xn as n&rarr;&infin;. This convergence, however, requires that the sequence
Xn satisfies stronger constraints than will be used below.
</p>
<p>The basic and rather natural elements of the boundedness conditions to be con-
</p>
<p>sidered below are: the boundedness of the moments of ξn = Xn &minus;Xn&minus;1 of the re-
spective orders and the presence of a negative &ldquo;drift&rdquo; E(ξn|Fn&minus;1) in the domain
Xn&minus;1 &gt; N for sufficiently large N . Such a property has already been utilised for
Markov chains; see Corollary 13.7.1 (otherwise the trajectory of Xn may go to &infin;).
</p>
<p>Let us begin with exponential moments. The simplest conditions ensuring the
</p>
<p>boundedness of supnEe
λXn for some λ &gt; 0 are as follows: for all n &ge; 1 and some
</p>
<p>λ &gt; 0 and N &lt;&infin;,
</p>
<p>E
(
eλξn
</p>
<p>∣∣Fn&minus;1
)
</p>
<p>I(Xn&minus;1 &gt;N)&le; β(λ) &lt; 1, (15.5.1)
E
(
eλξn
</p>
<p>∣∣Fn&minus;1
)
</p>
<p>I(Xn&minus;1 &ge;N)&le;ψ(λ) &lt;&infin;. (15.5.2)
</p>
<p>Theorem 15.5.1 If conditions (15.5.1) and (15.5.2) hold then
</p>
<p>E
(
eλXn
</p>
<p>∣∣F0
)
&le; β(λ)eλX0 + ψ(λ) e
</p>
<p>λN
</p>
<p>1 &minus; β(λ) . (15.5.3)
</p>
<p>Proof Denote by An the left-hand side of (15.5.3). Then, by virtue of (15.5.1) and
(15.5.2), we obtain
</p>
<p>An = E
{
E
[
eλXn
</p>
<p>(
I(Xn&minus;1 &gt;N)+ I(Xn&minus;1 &le;N)
</p>
<p>)∣∣Fn&minus;1
]∣∣F0
</p>
<p>}
</p>
<p>&le; E
[
eλXn&minus;1
</p>
<p>(
β(λ) I(Xn&minus;1 &gt;N)+ψ(λ) I(Xn&minus;1 &le;N)
</p>
<p>)∣∣F0
]
</p>
<p>&le; β(λ)An&minus;1 + eλNψ(λ).
</p>
<p>This immediately implies that
</p>
<p>An &le;A0βn(λ)+ eλNψ(λ)
n&minus;1&sum;
</p>
<p>k=0
βk(λ)&le;A0βn(λ)+
</p>
<p>eλNψ(λ)
</p>
<p>1 &minus; β(λ) .
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>488 15 Martingales
</p>
<p>The conditions
</p>
<p>E(ξn|Fn&minus;1)&le;&minus;ε &lt; 0 on the ω-set {Xn&minus;1 &gt;N}, (15.5.4)
E
(
eλ|ξn|
</p>
<p>∣∣Fn&minus;1
)
&le;ψ1(λ) &lt;&infin; for some λ &gt; 0 (15.5.5)
</p>
<p>are sufficient for (15.5.1) and (15.5.2).
The first condition means that Yn := (Xn + εn) I(Xn&minus;1 &gt; N) is a supermartin-
</p>
<p>gale.
</p>
<p>We now prove sufficiency of (15.5.4) and (15.5.5). That (15.5.2) holds is clear.
</p>
<p>Further, make use of the inequality
</p>
<p>ex &le; 1 + x + x
2
</p>
<p>2
e|x|,
</p>
<p>which follows from the Taylor formula for ex with the remainder in the Cauchy
</p>
<p>form:
</p>
<p>ex = 1 + x + x
2
</p>
<p>2
eθx, θ &isin; [0,1].
</p>
<p>Then, on the set {Xn&minus;1 &gt;N}, one has
</p>
<p>E
(
eλξn
</p>
<p>∣∣Fn&minus;1
)
&le; 1 &minus; λε+ λ
</p>
<p>2
</p>
<p>2
E
(
ξ2ne
</p>
<p>λ|ξn|∣∣Fn&minus;1
)
.
</p>
<p>Since x2 &lt; eλx/2 for sufficiently large x, by the H&ouml;lder inequality it follows that,
</p>
<p>together with (15.5.5), we will have
</p>
<p>E
(
ξ2ne
</p>
<p>λ|ξn|/2∣∣Fn&minus;1
)
&le;ψ2(λ) &lt;&infin;.
</p>
<p>This implies that, for sufficiently small λ, one has on the set {Xn&minus;1 &gt; N} the in-
equality
</p>
<p>E
(
eλξn
</p>
<p>∣∣Fn&minus;1
)
&le; 1 &minus; λε+ λ
</p>
<p>2
</p>
<p>2
ψ2(λ)=: β(λ)&le; 1 &minus;
</p>
<p>λε
</p>
<p>2
&lt; 1.
</p>
<p>This proves (15.5.1). �
</p>
<p>Corollary 15.5.1 If, in addition to the conditions of Theorem 15.5.1, the distribution
of Xn converges to a limiting distribution: P(Xn &lt; t)&rArr; P(X &lt; t), then
</p>
<p>EeλX &le; e
λNψ(λ)
</p>
<p>1 &minus; β(λ) .
</p>
<p>The corollary follows from the Fatou&ndash;Lebesgue theorem (see also Lemma 6.1.1):
</p>
<p>EeλX &le; lim inf
n&rarr;&infin;
</p>
<p>EeλXn . �
</p>
<p>We now obtain bounds for &ldquo;conventional&rdquo; moments. Set
</p>
<p>M l(n) := EXln,
m(0) := 1, m(1) := sup
</p>
<p>n&ge;1
sup
</p>
<p>ω&isin;{Xn&minus;1&gt;N}
E(ξn|Fn&minus;1),
</p>
<p>m(l) := sup
n&ge;1
</p>
<p>sup
ω
</p>
<p>E
(
|ξn|l
</p>
<p>∣∣Fn&minus;1
)
, l &gt; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Boundedness of the Moments of Stochastic Sequences 489
</p>
<p>Theorem 15.5.2 Assume that EXs0 &lt;&infin; for some s &gt; 1 and there exist N &ge; 0 and
ε &gt; 0 such that
</p>
<p>m(1)&le;&minus;ε, (15.5.6)
m(s) &lt; c &lt;&infin;. (15.5.7)
</p>
<p>Then
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>Ms&minus;1(n) &lt;&infin;. (15.5.8)
</p>
<p>If, moreover,
</p>
<p>Ms(n+ 1) &gt;Ms(n)&minus; c1 (15.5.9)
for some c1 &gt; 0, then
</p>
<p>sup
n
</p>
<p>Ms&minus;1(n) &lt;&infin;. (15.5.10)
</p>
<p>Corollary 15.5.2 If conditions (15.5.6) and (15.5.7) are met and the distribution
of Xn converges weakly to a limiting distribution: P(Xn &lt; t) &rArr; P(X &lt; t), then
EXs&minus;1 &lt;&infin;.
</p>
<p>This assertion follows from the Fatou&ndash;Lebesgue theorem (see also Lemma 6.1.1),
</p>
<p>which implies
</p>
<p>EXs&minus;1 &le; lim inf
n&rarr;&infin;
</p>
<p>EXs&minus;1n . �
</p>
<p>The assertion of Corollary 15.5.2 is unimprovable. One can see this from the
</p>
<p>example of the sequence Xn = (Xn&minus;1 + ζn)+, where ζk d= ζ are independent and
identically distributed. If Eζk &lt; 0 then the limiting distribution of Xn coincides with
</p>
<p>the distribution of S = supk Sk (see Sect. 12.4). From factorisation identities one can
derive that ESs&minus;1 is finite if and only if E(ζ+)s &lt;&infin;. An outline of the proof is as
follows. Theorem 12.3.2 implies that ESk = cE(χk+; η+ &lt;&infin;), c = const &lt;&infin;. It
follows from Corollary 12.2.2 that
</p>
<p>1 &minus;E
(
eiλχ+; η+ &lt;&infin;
</p>
<p>)
=
(
1 &minus;Eeiλζ
</p>
<p>)&int; &infin;
</p>
<p>0
</p>
<p>e&minus;iλx dH(x),
</p>
<p>where H(x) is the renewal function for the random variable &minus;χ0&minus; &ge; 0. Since
a1 + b1x &le;H(x)&le; a2 + b2x
</p>
<p>(see Theorem 10.1.1 and Lemma 10.1.1; ai , bi are constants), integrating the con-
</p>
<p>volution
</p>
<p>P(χ+ &gt; x, η+ &lt;&infin;)=
&int; &infin;
</p>
<p>0
</p>
<p>P(ζ &gt; v+ x)dH(v)
</p>
<p>by parts we verify that, as x &rarr;&infin;, the left-hand side has the same order of magni-
tude as
</p>
<p>&int;&infin;
0 P(ζ &gt; v+ x)dv. Hence the required statement follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>490 15 Martingales
</p>
<p>We now return to Theorem 15.5.2. Note that in all of the most popular problems
</p>
<p>the sequence Ms&minus;1(n) behaves &ldquo;regularly&rdquo;: either it is bounded or Ms&minus;1(n)&rarr;&infin;.
Assertion (15.5.8) means that, under the conditions of Theorem 15.5.2, the sec-
</p>
<p>ond possibility is excluded. Condition (15.5.9) ensuring (15.5.10) is also rather
</p>
<p>broad.
</p>
<p>Proof of Theorem 15.5.2 Let for simplicity&rsquo;s sake s &gt; 1 be an integer. We have
</p>
<p>E
(
Xsn; Xn&minus;1 &gt;N
</p>
<p>)
=
&int; &infin;
</p>
<p>N
</p>
<p>E
(
(x + ξn)s; Xn&minus;1 &isin; dx
</p>
<p>)
</p>
<p>=
s&sum;
</p>
<p>l=0
</p>
<p>(
s
</p>
<p>l
</p>
<p>)&int; &infin;
</p>
<p>N
</p>
<p>xlE
(
ξ s&minus;ln ; Xn&minus;1 &isin; dx
</p>
<p>)
.
</p>
<p>If we replace ξ s&minus;ln for s &minus; l &ge; 2 with |ξn|s&minus;l then the right-hand side can only in-
crease. Therefore,
</p>
<p>E
(
Xsn; Xn&minus;1 &gt;N
</p>
<p>)
&le;
</p>
<p>s&sum;
</p>
<p>l=0
</p>
<p>(
s
</p>
<p>l
</p>
<p>)
m(s &minus; l)M lN (n&minus; 1),
</p>
<p>where
</p>
<p>M lN (n)= E
(
Xln; Xn &gt;N
</p>
<p>)
.
</p>
<p>The moments Ms(n)= EXsn satisfy the inequalities
</p>
<p>Ms(n)&le; E
[(
N + |ξn|
</p>
<p>)s; Xn&minus;1 &le;N
]
+
</p>
<p>s&sum;
</p>
<p>l=0
</p>
<p>(
s
</p>
<p>l
</p>
<p>)
m(s &minus; l)M lN (n&minus; 1)
</p>
<p>&le; 2s
[
N s + c
</p>
<p>]
+
</p>
<p>s&sum;
</p>
<p>l=0
</p>
<p>(
s
</p>
<p>l
</p>
<p>)
m(s &minus; l)M lN (n&minus; 1). (15.5.11)
</p>
<p>Suppose now that (15.5.8) does not hold: Ms&minus;1(n) &rarr; &infin;. Then all the more
Ms(n) &rarr; &infin; and there exists a subsequence n&prime; such that Ms(n&prime;) &gt; Ms(n&prime; &minus; 1).
Since M l(n)&le; [M l+1(n)]l/ l+1, we obtain from (15.5.6) and (15.5.11) that
</p>
<p>Ms
(
n&prime;
)
&le; const +Ms
</p>
<p>(
n&prime; &minus; 1
</p>
<p>)
+ sMs&minus;1
</p>
<p>(
n&prime; &minus; 1
</p>
<p>)
m(1)+ o
</p>
<p>(
Ms&minus;1
</p>
<p>(
n&prime; &minus; 1
</p>
<p>))
</p>
<p>&le;Ms
(
n&prime; &minus; 1
</p>
<p>)
&minus; 1
</p>
<p>2
sεMs&minus;1
</p>
<p>(
n&prime; &minus; 1
</p>
<p>)
</p>
<p>for sufficiently large n&prime;. This contradicts the assumption that Ms(n)&rarr;&infin; and hence
proves (15.5.8).
</p>
<p>We now prove (15.5.10). If this relation is not true then there exists a sequence
</p>
<p>n&prime; such that Ms&minus;1(n&prime;)&rarr;&infin; and Ms(n&prime;) &gt;Ms(n&prime;&minus; 1)&minus; c1. It remains to make use
of the above argument.
</p>
<p>We leave the proof for a non-integer s &gt; 1 to the reader (the changes are elemen-
</p>
<p>tary). The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Boundedness of the Moments of Stochastic Sequences 491
</p>
<p>Remark 15.5.1 (1) The assertions of Theorems 15.5.1 and 15.5.2 will remain valid
if one requires inequalities (15.5.4) or E(ξn + ε|Fn&minus;1) I(Xn&minus;1 &gt;N)&le; 0 to hold not
for all n, but only for n&ge; n0 for some n0 &gt; 1.
</p>
<p>(2) As in Theorem 15.5.1, condition (15.5.6) means that the sequence of random
</p>
<p>variables (Xn + εn) I(Xn&minus;1 &gt;N) forms a supermartingale.
(3) The conditions of Theorems 15.5.1 and 15.5.2 may be weakened by replac-
</p>
<p>ing them with &ldquo;averaged&rdquo; conditions. Consider, for instance, condition (15.5.1). By
</p>
<p>integrating it over the set {Xn&minus;1 &gt; x &gt;N} we obtain
</p>
<p>E
(
eλξn; Xn&minus;1 &gt; x
</p>
<p>)
&le; β(λ)P(Xn&minus;1 &gt; x)
</p>
<p>or, which is the same,
</p>
<p>E
(
eλξn
</p>
<p>∣∣Xn&minus;1 &gt; x
)
&le; β(λ). (15.5.12)
</p>
<p>The converse assertion that (15.5.12) for all x &gt;N implies relation (15.5.1) is obvi-
</p>
<p>ously false, so that condition (15.5.12) is weaker than (15.5.1). A similar remark is
</p>
<p>true for condition (15.5.4).
</p>
<p>One has the following generalisations of Theorems 15.5.1 and 15.5.2 to the case
</p>
<p>of &ldquo;averaged conditions&rdquo;.
</p>
<p>Theorem 15.5.1A Let, for some λ &gt; 0, N &gt; 0 and all x &ge;N ,
</p>
<p>E
(
eλξn
</p>
<p>∣∣Xn&minus;1 &gt; x
)
&le; β(λ) &lt; 1, E
</p>
<p>(
eλξn; Xn&minus;1 &le;N
</p>
<p>)
&le;ψ(λ) &lt;&infin;.
</p>
<p>Then
</p>
<p>EeλXn &le; βn(λ)EeλX(0) + e
λNψ(λ)
</p>
<p>1 &minus; β(λ) .
</p>
<p>Put
</p>
<p>m(1) := sup
n&ge;1
</p>
<p>sup
x&ge;N
</p>
<p>E(ξn|Xn&minus;1 &gt; x),
</p>
<p>m(l) := sup
n&ge;1
</p>
<p>sup
x&ge;N
</p>
<p>E
(
|ξn|l
</p>
<p>∣∣X(n) &gt; x
)
, l &gt; 1.
</p>
<p>Theorem 15.5.2A Let EXs0 &lt;&infin; and there exist N &ge; 0 and ε &gt; 0 such that
</p>
<p>m(1)&le;&minus;ε, m(s) &lt;&infin;, E
(
|ξn|s; Xn&minus;1 &le;N
</p>
<p>)
&lt; c &lt;&infin;.
</p>
<p>Then (15.5.8) holds true. If, in addition, (15.5.9) is valid, then (15.5.10) is true.
</p>
<p>The proofs of Theorems 15.5.1A and 15.5.2A are quite similar to those of Theo-
rems 15.5.1 and 15.5.2. The only additional element in both cases is integration by
</p>
<p>parts. We will illustrate this with the proof of Theorem 15.5.1A. Consider</p>
<p/>
</div>
<div class="page"><p/>
<p>492 15 Martingales
</p>
<p>E
(
eλXn; Xn&minus;1 &gt;N
</p>
<p>)
=
&int; &infin;
</p>
<p>N
</p>
<p>eλxE
(
eλξn; Xn&minus;1 &isin; dx
</p>
<p>)
</p>
<p>= E
(
eλ(N+ξn); Xn&minus;1 &gt;N
</p>
<p>)
+
&int; &infin;
</p>
<p>N
</p>
<p>λeλxE
(
eλξn; Xn&minus;1 &gt; x
</p>
<p>)
dx
</p>
<p>&le; E
(
eλ(N+ξn); Xn&minus;1 &gt;N + β(λ)
</p>
<p>)&int; &infin;
</p>
<p>N
</p>
<p>λeλxP(Xn&minus;1 &gt; x)dx
</p>
<p>= eλNE
(
eλξn &minus; β(λ); Xn&minus;1 &gt;N
</p>
<p>)
+ β(λ)
</p>
<p>&int; &infin;
</p>
<p>N
</p>
<p>eλxP(Xn&minus;1 &isin; dx)
</p>
<p>&le; β(λ)E
(
eλXn&minus;1; Xn&minus;1 &gt;N
</p>
<p>)
.
</p>
<p>From this we find that
</p>
<p>βn(λ) := EeλXn &le; E
(
eλ(Xn&minus;1+ξn); Xn&minus;1 &le;N
</p>
<p>)
+E
</p>
<p>(
eλXn;Xn&minus;1 &gt;N
</p>
<p>)
</p>
<p>&le; eλNψ(λ)+ β(λ)E
(
eλXn&minus;1; Xn&minus;1 &gt;N
</p>
<p>)
</p>
<p>&le; eλNψ(λ)&minus; P(Xn&minus;1 &le;N)β(λ)+ β(λ)βn(λ);
</p>
<p>βn(λ) &le; βn(λ)β0(λ)+
eλNψ(λ)
</p>
<p>1 &minus; β(λ) . �
</p>
<p>Note that Theorem 13.7.2 and Corollary 13.7.1 on &ldquo;positive recurrence&rdquo; can also
</p>
<p>be referred to as theorems on boundedness of stochastic sequences.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 16
</p>
<p>Stationary Sequences
</p>
<p>Abstract Section 16.1 contains the definitions and a discussion of the concepts
</p>
<p>of strictly stationary sequences and measure preserving transformations. It also
</p>
<p>presents Poincar&eacute;&rsquo;s theorem on the number of visits to a given set by a stationary se-
</p>
<p>quence. Section 16.2 discusses invariance, ergodicity, mixing and weak dependence.
</p>
<p>The Birkhoff&ndash;Khintchin ergodic theorem is stated and proved in Sect. 16.3.
</p>
<p>16.1 Basic Notions
</p>
<p>Let 〈Ω,F,P〉 be a probability space and ξ = (ξ0, ξ1, . . .) an infinite sequence of
random variables given on it.
</p>
<p>Definition 16.1.1 A sequence ξ is said to be strictly stationary if, for any k, the
distribution of the vector (ξn, . . . , ξn+k) does not depend on n, n&ge; 0.
</p>
<p>Along with the sequence ξ , consider the sequence (ξn, ξn+1, . . .). Since the finite-
dimensional distributions of these sequences (i.e. the distributions of the vectors
</p>
<p>(ξm, . . . , ξm+k)) coincide, the distributions of the sequences will also coincide (one
has to make use of the measure extension theorem (see Appendix 1) or the Kol-
</p>
<p>mogorov theorem (see Sect. 3.5). In other words, for a stationary sequence ξ , for
</p>
<p>any n and B &isin;B&infin; (for notation see Sect. 3.5), one has
</p>
<p>P(ξ &isin; B)= P
(
(ξn, ξn+1, . . .) &isin; B
</p>
<p>)
.
</p>
<p>The simplest example of a stationary sequence is given by a sequence of inde-
</p>
<p>pendent identically distributed random variables ζ = (ζ0, ζ1, . . .). It is evident that
the sequence ξk = α0ζk + &middot; &middot; &middot; + αsζk+s , k = 0,1,2, . . . , will also be stationary, but
the variables ξk will no longer be independent. The same holds for sequences of the
</p>
<p>form
</p>
<p>ξk =
&infin;&sum;
</p>
<p>j=0
αj ζk+j ,
</p>
<p>provided that E|ζj |&lt;&infin;,
&sum;
</p>
<p>|αj |&lt;&infin;, or if Eζk = 0, Var(ζk) &lt;&infin;,
&sum;
</p>
<p>α2j &lt;&infin; (the
latter ensures a.s. convergence of the series of random variables, see Sect. 10.2). In a
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_16, &copy; Springer-Verlag London 2013
</p>
<p>493</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_16">http://dx.doi.org/10.1007/978-1-4471-5201-9_16</a></div>
</div>
<div class="page"><p/>
<p>494 16 Stationary Sequences
</p>
<p>similar way one can consider stationary sequences ξk = g(ζk, ζk+1, . . .) &ldquo;generated&rdquo;
by ζ , where g(x) is an arbitrary measurable functional R&infin; #&rarr;R.
</p>
<p>Another example is given by stationary Markov chains. If {Xn} is a real-valued
Markov chain with invariant measure π and transition probability P(&middot;, &middot;) then the
chain {Xn} with X&sub;= π will form a stationary sequence, because the distribution
</p>
<p>P(Xn &isin; B0, . . . ,Xn+k &isin; Bk)=
&int;
</p>
<p>B0
</p>
<p>π(dx0)
</p>
<p>&int;
</p>
<p>B1
</p>
<p>P(x0, dx1) &middot; &middot; &middot;
&int;
</p>
<p>Bk
</p>
<p>P(xk&minus;1, dxk)
</p>
<p>will not depend on n.
</p>
<p>Any stationary sequence ξ = (ξ0, ξ1, . . .) can always be extended to a stationary
sequence ξ = (. . . ξ&minus;1, ξ0, ξ1, . . .) given on the &ldquo;whole axis&rdquo;.
</p>
<p>Indeed, for any n, &minus;&infin; &lt; n &lt; &infin;, and k &ge; 0 define the joint distributions of
(ξn, . . . , ξn+k) as those of (ξ0, . . . , ξk). These distributions will clearly be consistent
(see Sect. 3.5) and by the Kolmogorov theorem there will exist a unique probabil-
</p>
<p>ity distribution on R&infin;&minus;&infin; =
&prod;&infin;
</p>
<p>k=&minus;&infin;Rk with the respective σ -algebra such that any
finite-dimensional distribution is a projection of that distribution on the correspond-
</p>
<p>ing subspace. It remains to take the random element ξ to be the identity mapping
</p>
<p>of R&infin;&minus;&infin; onto itself.
In some of the subsequent sections it will be convenient for us to use stationary
</p>
<p>sequences given on the whole axis.
</p>
<p>Let ξ be such a sequence. Define a transformation θ of the space R&infin;&minus;&infin; onto itself
with the help of the relations
</p>
<p>(θx)k = (x)k+1 = xk+1, (16.1.1)
</p>
<p>where (x)k is the k-th component of the vector x &isin;R&infin;&minus;&infin;, &minus;&infin;&lt; k &lt;&infin;. The trans-
formation θ clearly has the following properties:
</p>
<p>1. It is a one-to-one mapping, θ&minus;1 is defined by
(
θ&minus;1x
</p>
<p>)
k
= xk&minus;1.
</p>
<p>2. The sequence θξ is also stationary, its distribution coinciding with that of ξ :
</p>
<p>P(θξ &isin; B)= P(ξ &isin; B).
</p>
<p>It is natural to call the last property of the transformation θ the &ldquo;measure preserv-
</p>
<p>ing&rdquo; property.
</p>
<p>The above remarks explain to some extent why historically exploring the prop-
</p>
<p>erties of stationary sequences followed the route of studying measure preserving
</p>
<p>transforms. Studies in that area constitute a substantial part of the modern analysis.
</p>
<p>In what follows, we will relate the construction of stationary sequences to measure
</p>
<p>preserving transformations, and it will be more convenient to regard the latter as
</p>
<p>&ldquo;primary&rdquo; objects.
</p>
<p>Definition 16.1.2 Let 〈Ω,F,P〉 be the basic probability space. A transformation T
of Ω into itself is said to be measure preserving if:</p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 Basic Notions 495
</p>
<p>(1) T is measurable, i.e. T &minus;1A= {ω : T ω &isin;A} &isin; F for any A &isin; F; and
(2) T preserves the measure: P(T &minus;1A)= P(A) for any A &isin; F.
</p>
<p>Let T be a measure preserving transformation, T n its n-th iteration and ξ = ξ(ω)
be a random variable. Put Uξ(ω)= ξ(T ω), so that U is a transformation of random
variables, and U kξ(ω)= ξ(T kω). Then
</p>
<p>ξ =
{
Unξ(ω)
</p>
<p>}&infin;
0
</p>
<p>=
{
ξ
(
T nω
</p>
<p>)}&infin;
0
</p>
<p>(16.1.2)
</p>
<p>is a stationary sequence of random variables.
</p>
<p>Proof Indeed, let A= {ω; ξ &isin; B}, B &isin;B&infin; and A1 = {ω : θξ &isin; B}. We have
</p>
<p>ξ =
(
ξ(ω), ξ(T ω), . . .
</p>
<p>)
, θξ =
</p>
<p>(
ξ(T ω), ξ
</p>
<p>(
T 2ω
</p>
<p>)
, . . .
</p>
<p>)
.
</p>
<p>Therefore ω &isin; A1 if and only if T ω &isin; A, i.e. when A1 = T &minus;1A. But P(T &minus;1A) =
P(A) and hence P(A1)= P(A), so that P(An)= P(A) for any n&ge; 1 as well, where
An = {ω : θnξ &isin; B}. �
</p>
<p>Stationary sequences defined by (16.1.2) will be referred to as sequences gener-
ated by the transformation T .
</p>
<p>To be able to construct stationary sequences on the whole axis, we will need mea-
</p>
<p>sure preserving transformations acting both in &ldquo;positive&rdquo; and &ldquo;negative&rdquo; directions.
</p>
<p>Definition 16.1.3 A transformation T is said to be bidirectional measure preserving
if:
</p>
<p>(1) T is a one-to-one transformation, the domain and range of T coincide with the
</p>
<p>whole Ω ;
</p>
<p>(2) the transformations T and T &minus;1 are measurable, i.e.
</p>
<p>T &minus;1A= {ω : T ω &isin;A} &isin; F, T A= {T ω : ω &isin;A} &isin; F
</p>
<p>for any A &isin; F;
(3) the transformation T preserves the measure: P(T &minus;1A) = P(A), and therefore
</p>
<p>P(A)= P(T A) for any A &isin; F.
</p>
<p>For such transformations we can, as before, construct stationary sequences ξ
</p>
<p>defined on the whole axis:
</p>
<p>ξ =
{
Unξ(ω)
</p>
<p>}&infin;
&minus;&infin; =
</p>
<p>{
ξ
(
T nω
</p>
<p>)}&infin;
&minus;&infin;.
</p>
<p>The argument before Definition 16.1.2 shows that this approach &ldquo;exhausts&rdquo; all
</p>
<p>stationary sequences given on 〈Ω,F,P〉, i.e. to any stationary sequence ξ we can
relate a measure preserving transformation T and a random variable ξ = ξ0 such
that ξk(ω) = ξ0(T kω). In this construction, we consider the &ldquo;sample probability
space&rdquo; 〈R&infin;,B&infin;,P〉 for which ξ(ω)= ω, θ = T . The transformation θ = T (that is,</p>
<p/>
</div>
<div class="page"><p/>
<p>496 16 Stationary Sequences
</p>
<p>transformation (16.1.1)) will be called the pathwise shift transformation. It always
exists and &ldquo;generates&rdquo; any stationary sequence.
</p>
<p>Now we will give some simpler examples of (bidirectional) measure preserving
</p>
<p>transformations.
</p>
<p>Example 16.1.1 Let Ω = {ω1, . . . ,ωd}, d &ge; 2, be a finite set, F be the σ -algebra of
all its subsets, T ωi = ωi+1, 1 &le; i &le; d &minus; 1 and T ωd = ω1. If P(ωi) = 1/d then T
and T &minus;1 are measure preserving transformations.
</p>
<p>Example 16.1.2 Let Ω = [0,1), F be the σ -algebra of Borel sets, P the Lebesgue
measure and s a fixed number. Then T ω= ω+ s (mod 1) is a bidirectional measure
preserving transformation.
</p>
<p>In these examples, the spaces Ω are rather small, which allows one to construct
</p>
<p>on them only stationary sequences with deterministic or almost deterministic de-
</p>
<p>pendence between their elements. If we choose in Example 16.1.1 the variable ξ so
</p>
<p>that all ξ(ωi) are different, then the value ξk(ω)= ξ(T kω) will uniquely determine
T kω and thereby T k+1ω and ξk+1(ω). The same can be said of Example 16.1.2 in
the case when ξ(ω), ω &isin; [0,1), is a monotone function of ω.
</p>
<p>As our argument at the beginning of the section shows, the space Ω = R&infin; is
large enough to construct on it any stationary sequence.
</p>
<p>Thus, we see that the concept of a measure preserving transformation arises in
</p>
<p>a natural way when studying stationary processes. But not only in that case. It also
</p>
<p>arises, for instance, while studying the dynamics of some physical systems. Indeed,
</p>
<p>the whole above argument remains valid if we consider on 〈Ω,F〉 an arbitrary mea-
sure μ instead of the probability P. For example, for Ω = R&infin;, the value μ(A),
A &isin; F, could be the Lebesgue measure (volume) of the set A. The measure preserv-
ing property of the transformation T will mean that any set A, after the transform T
</p>
<p>has acted on it (which, say, corresponds to the change of the physical system&rsquo;s state
</p>
<p>in one unit of time), will retain its volume. This property is rather natural for incom-
</p>
<p>pressible liquids. Many laws to be established below will be equally applicable to
</p>
<p>such physical systems.
</p>
<p>Returning to probabilistic models, i.e. to the case when the measure is a proba-
</p>
<p>bility distribution, it turns out that, in that case, for any set A with P(A) &gt; 0, the
</p>
<p>&ldquo;trajectory&rdquo; T nω will visit A infinitely often for almost all (with respect to the mea-
</p>
<p>sure P) ω &isin;A.
</p>
<p>Theorem 16.1.1 (Poincar&eacute;) Let T be a measure preserving transformation and
A &isin; F. Then, for almost all ω &isin; A, the relation T nω &isin; A holds for infinitely many
n&ge; 1.
</p>
<p>Proof Put N := {ω &isin; A : T nω /&isin; A for all n &ge; 1}. Because {ω : T nω &isin; A} &isin; F, it is
not hard to see that N &isin; F. Clearly, N &cap; T &minus;nN = &empty; for any n &ge; 1, and T &minus;mN &cap;
T &minus;(m+n)N = T &minus;m(N &cap; T &minus;nN)=&empty;. This means that we have infinitely many sets
T &minus;nN , n = 0,1,2, . . . , which are disjoint and have one and the same probability.
This evidently implies that P(N)= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Ergodicity (Metric Transitivity), Mixing and Weak Dependence 497
</p>
<p>Thus, for each ω &isin; A \N , there exists an n1 = n1(ω) such that T n1ω &isin; A. Now
we apply this assertion to the measure preserving mapping Tk = T k , k &ge; 1. Then, for
each ω &isin;A \Nk , P(Nk)= 0, there exists an nk = nk(ω)&ge; 1 such that (T k)nkω &isin;A.
Since knk &ge; k, the theorem is proved. �
</p>
<p>Corollary 16.1.1 Let ξ(ω)&ge; 0 and A= {ω : ξ(ω) &gt; 0}. Then, for almost all ω &isin;A,
</p>
<p>&infin;&sum;
</p>
<p>n=0
ξ
(
T nω
</p>
<p>)
=&infin;.
</p>
<p>Proof Put Ak = {ω : ξ(ω) &ge; 1/k} &sub; A. Then by Theorem 16.1.1 the above series
diverges for almost all ω &isin;Ak . It remains to notice that A=
</p>
<p>⋃
k Ak . �
</p>
<p>Remark 16.1.1 Formally, one does not need condition P(A) &gt; 0 in Theorem 16.1.1
and Corollary 16.1.1. However, in the absence of that condition, the assertions may
</p>
<p>become meaningless, since the set A\N in the proof of Theorem 16.1.1 can turn out
to be empty. Suppose, for example, that in the conditions of Example 16.1.2, A is a
</p>
<p>one-point set: A= {ω}, ω &isin; [0,1). If s is irrational, then T kω will never be in A for
k &ge; 1. Indeed, if we assume the contrary, then we will infer that there exist integers
k and m such that ω + sk &minus; m = ω, s = m/k, which contradicts the irrationality
of s.
</p>
<p>16.2 Ergodicity (Metric Transitivity), Mixing and Weak
</p>
<p>Dependence
</p>
<p>Definition 16.2.1 A set A &isin; F is said to be invariant (with respect to a measure
preserving transformation T ) if T &minus;1A = A. A set A &isin; F is said to be almost in-
variant if the sets T &minus;1A and A differ from each other by a set of probability zero:
P(A&oplus; T &minus;1A)= 0, where A&oplus;B =AB &cup;AB is the symmetric difference.
</p>
<p>It is evident that the class of all invariant (almost invariant) sets forms a σ -algebra
</p>
<p>which will be denoted by I (I&lowast;).
</p>
<p>Lemma 16.2.1 If A is an almost invariant set then there exists an invariant set B
such that P(A&oplus;B)= 0.
</p>
<p>Proof Put B = lim supn&rarr;&infin; T &minus;nA (recall that lim supn&rarr;&infin;An =
⋂&infin;
</p>
<p>n=1
⋃&infin;
</p>
<p>k=nAk is
the set of all points which belong to infinitely many sets Ak). Then
</p>
<p>T &minus;1B = lim sup
n&rarr;&infin;
</p>
<p>T &minus;(n+1)A= B,</p>
<p/>
</div>
<div class="page"><p/>
<p>498 16 Stationary Sequences
</p>
<p>i.e. B &isin; I. It is not hard to see that
</p>
<p>A&oplus;B &sub;
&infin;⋃
</p>
<p>k=0
</p>
<p>(
T &minus;kA&oplus; T &minus;(k+1)A
</p>
<p>)
.
</p>
<p>Since
</p>
<p>P
(
T &minus;kA&oplus; T &minus;(k+1)A
</p>
<p>)
= P
</p>
<p>(
A&oplus; T &minus;1A
</p>
<p>)
= 0,
</p>
<p>we have P(A&oplus;B)= 0. The lemma is proved. �
</p>
<p>Definition 16.2.2 A measure preserving transformation T is said to be ergodic (or
metric transitive) if each invariant set has probability zero or one.
</p>
<p>A stationary sequence {ξk} associated with such T (i.e. the sequence which gen-
erated T or was generated by T ) is also said to be ergodic (metric transitive).
</p>
<p>Lemma 16.2.2 A transformation T is ergodic if and only if each almost invariant
set has probability 0 or 1.
</p>
<p>Proof Let T be ergodic and A &isin; I&lowast;. Then by Lemma 16.2.1 there exists an invariant
set B such that P(A&oplus;B)= 0. Because P(B)= 0 or 1, the probability P(A)= 0 or 1.
The converse assertion is obvious. �
</p>
<p>Definition 16.2.3 A random variable ζ = ζ(ω) is said to be invariant (almost in-
variant) if ζ(ω)= ζ(T ω) for all ω &isin;Ω (for almost all ω &isin;Ω).
</p>
<p>Theorem 16.2.1 Let T be a measure preserving transformation. The following
three conditions are equivalent:
</p>
<p>(1) T is ergodic;
(2) each almost invariant random variable is a.s. constant;
(3) each invariant random variable is a.s. constant.
</p>
<p>Proof (1) &rArr; (2). Assume that T is ergodic and ξ is almost invariant, i.e. ξ(ω) =
ξ(T ω) a.s. Then, for any v &isin; R, we have Av := {ω : ξ(ω) &le; v} &isin; I&lowast; and, by
Lemma 16.2.2, P(Av) equals 0 or 1. Put V := sup{v : P(Av)= 0}. Since Av &uarr;Ω as
v &uarr;&infin; and Av &darr;&empty; as v &darr; &minus;&infin;, one has |V |&lt;&infin; and
</p>
<p>P
(
ξ(ω) &lt; V
</p>
<p>)
= P
</p>
<p>( &infin;⋃
</p>
<p>n=1
</p>
<p>{
ξ(ω) &lt; V &minus; 1
</p>
<p>n
</p>
<p>})
= 0.
</p>
<p>Similarly, P(ξ(ω) &gt; V )= 0. Therefore P(ξ(ω)= V )= 1.
(2) &rArr; (3). Obvious.
(3) &rArr; (1). Let A &isin; I. Then the indicator function IA is an invariant random
</p>
<p>variable, and since it is constant, one has either IA = 0 or IA = 1 a.s. This implies
that P(A) equals 0 or 1. The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Ergodicity (Metric Transitivity), Mixing and Weak Dependence 499
</p>
<p>The assertion of the theorem clearly remains valid if one considers in (3) only
</p>
<p>bounded random variables. Moreover, if ξ is invariant, then the truncated variable
</p>
<p>ξ(N) = min(ξ,N) is also invariant.
Returning to Examples 16.1.1 and 16.1.2, in Example 16.1.1,
</p>
<p>Ω = (ω1, . . . ,ωd), T ωi = ωi+1 (mod d), P(ωi)= 1/d.
</p>
<p>The transformation T is obviously metric transitive.
</p>
<p>In Example 16.1.2, Ω = [0,1), T ω = ω + s (mod 1), and P is the Lebesgue
measure. We will now show that T is ergodic if and only if s is irrational.
</p>
<p>Consider a square integrable random variable ξ = ξ(ω) : Eξ2(ω) &lt;&infin;. Then by
the Parseval equality, the Fourier series
</p>
<p>ξ(ω)=
&infin;&sum;
</p>
<p>n=0
ane
</p>
<p>2πinω
</p>
<p>for this function has the property
&sum;&infin;
</p>
<p>n=0 |c2n|&lt;&infin;. Assume that s is irrational, while
ξ is invariant. Then
</p>
<p>an = Eξ(ω)e&minus;2πinω = Eξ(T ω)e&minus;2πinT ω
</p>
<p>= e&minus;2πinsEξ(T ω)e&minus;2πinω = e&minus;2πinsEξ(ω)e&minus;2πinω = e&minus;2πinsan.
</p>
<p>For irrational s, this equality is only possible when an = 0, n&ge; 1, and ξ(ω)= a0 =
const. By Theorem 16.2.1 this means that T is ergodic.
</p>
<p>Now let s =m/n be rational (m and n are integers). Then the set
</p>
<p>A=
n&minus;1⋃
</p>
<p>k=0
</p>
<p>{
ω : 2k
</p>
<p>2n
&le; ω &lt; 2k+ 1
</p>
<p>2n
</p>
<p>}
</p>
<p>will be invariant and P(A)= 1/2. This means that T is not ergodic. �
</p>
<p>Definition 16.2.4 A measure preserving transformation T is called mixing if, for
any A1,A2 &isin; F, as n&rarr;&infin;,
</p>
<p>P
(
A1 &cap; T &minus;nA2
</p>
<p>)
&rarr; P(A1)P(A2). (16.2.1)
</p>
<p>Now consider the stationary sequence ξ = (ξ0, ξ1, . . .) generated by the transfor-
mation T : ξk(ω)= ξ0(T kω).
</p>
<p>Definition 16.2.5 A stationary sequence ξ is said to be weakly dependent if ξk and
ξk+n are asymptotically independent as n&rarr;&infin;, i.e. for any B1,B2 &isin;B
</p>
<p>P(ξk &isin; B1, ξk+n &isin; B2)&rarr; P(ξ0 &isin; B1)P(ξ0 &isin; B2). (16.2.2)
</p>
<p>Theorem 16.2.2 A measure preserving transformation T is mixing if and only if
any stationary sequence ξ generated by T is weakly dependent.</p>
<p/>
</div>
<div class="page"><p/>
<p>500 16 Stationary Sequences
</p>
<p>Proof Let T be mixing. Put Ai := ξ&minus;10 (Bi), i = 1,2, and set k = 0 in (16.2.2). Then
</p>
<p>P(ξ0 &isin; B1, ξn &isin; B2)= P
(
A1 &cap; T &minus;nA2
</p>
<p>)
&rarr; P(A1)P(A2).
</p>
<p>Now assume any sequence generated by T is weakly dependent. For any given
</p>
<p>A1,A2 &isin; F, define the random variable
</p>
<p>ξ(ω)=
</p>
<p>⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
</p>
<p>0 if ω /&isin;A1 &cup;A2;
1 if ω &isin;A1A2;
2 if ω &isin;A1A2;
3 if ω &isin;A1A2;
</p>
<p>and put ξk(ω) := ξ(T kω). Then, as n&rarr;&infin;,
</p>
<p>P
(
A1 &cap; T &minus;nA2
</p>
<p>)
= P(0 &lt; ξ0 &lt; 3, ξn &gt; 2)&rarr; P(0 &lt; ξ0 &lt; 3)P(ξ0 &gt; 2)
= P(A1)P(A2).
</p>
<p>The theorem is proved. �
</p>
<p>Let {Xn} be a stationary real-valued Markov chain with an invariant distribution
π that satisfies the conditions of the ergodic theorem, i.e. such that, for any B &isin;B
and x &isin;R, as n&rarr;&infin;,
</p>
<p>P(Xn &isin; B |X0 = x)&rarr; π(B).
</p>
<p>Then {Xn} is weakly dependent, and therefore, by Theorem 16.2.2, the respective
transformation T is mixing. Indeed,
</p>
<p>P(X0 &isin; B1,Xn &isin; B2)= EI(X0 &isin; B1)P(Xn &isin; B2 |X0),
</p>
<p>where the last factor converges to π(B2) for each X0. Therefore the above proba-
</p>
<p>bility tends to π(B2)π(B1).
</p>
<p>Further characterisations of the mixing property will be given in Theorems 16.2.4
</p>
<p>and 16.2.5.
</p>
<p>Now we will introduce some notions that are somewhat broader than those from
</p>
<p>Definitions 16.2.4 and 16.2.5.
</p>
<p>Definition 16.2.6 A transformation T is called mixing on the average if, as n&rarr;&infin;,
</p>
<p>1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>k=1
P
(
A1 &cap; T &minus;kA2
</p>
<p>)
&rarr; P(A1)P(A2). (16.2.3)
</p>
<p>A stationary sequence ξ is said to be weakly dependent on the average if
</p>
<p>1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>k=1
P(ξ0 &isin; B1, ξk &isin; B2)&rarr; P(ξ0 &isin; B1)P(ξ0 &isin; B2). (16.2.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Ergodicity (Metric Transitivity), Mixing and Weak Dependence 501
</p>
<p>Theorem 16.2.3 A measure preserving transformation T is mixing on the average
if and only if any stationary sequence ξ generated by T is weakly dependent on the
average.
</p>
<p>The Proof is the same as for Theorem 16.2.2, and is left to the reader. �
</p>
<p>If {Xn} is a periodic real-valued Markov chain with period d such that each
of the embedded sub-chains {Xi+nd}&infin;n=0, i = 0, . . . , d &minus; 1, satisfies the ergodicity
conditions with invariant distributions π (i) on disjoint sets X0, . . . ,Xd&minus;1, then the
&ldquo;common&rdquo; invariant distribution π will be equal to d&minus;1
</p>
<p>&sum;d&minus;1
i=0 π
</p>
<p>(i), and the chain
</p>
<p>{Xn} will be weakly dependent on the average. At the same time, it will clearly not
be weakly dependent for d &gt; 1.
</p>
<p>Theorem 16.2.4 A measure preserving transformation T is ergodic if and only if it
is mixing on the average.
</p>
<p>Proof Let T be mixing on the average, and A1 &isin; F, A2 &isin; I. Then A2 = T &minus;kA2
and hence P(A1 &cap; T &minus;kA2)= P(A1A2) for all k &ge; 1. Therefore, (16.2.3) means that
P(A1A2)= P(A1)P(A2). For A1 = A2 we get P(A2)= P2(A2), and consequently
P(A2) equals 0 or 1.
</p>
<p>We postpone the proof of the converse assertion until the next section. �
</p>
<p>Now we will give one more important property of ergodic transforms.
</p>
<p>Theorem 16.2.5 A measure preserving transformation T is ergodic if and only if,
for any A &isin; F with P(A) &gt; 0, one has
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>n=0
T &minus;nA
</p>
<p>)
= 1. (16.2.5)
</p>
<p>Note that property (16.2.5) means that the sets T &minus;nA, n = 0,1, . . . , &ldquo;exhaust&rdquo;
the whole space Ω , which associates well with the term &ldquo;mixing&rdquo;.
</p>
<p>Proof Let T be ergodic. Put B :=
⋃&infin;
</p>
<p>n=0 T
&minus;nA. Then T &minus;1B &sub; B . Because T is
</p>
<p>measure preserving, one also has that P(T &minus;1B) = P(B). From this it follows that
T &minus;1B = B up to a set of measure 0 and therefore B is almost invariant. Since T is
ergodic, P(B) equals 0 or 1. But P(B)&ge; P(A) &gt; 0, and hence P(B)= 1.
</p>
<p>Conversely, if T is not ergodic, then there exists an invariant set A such that
</p>
<p>0 &lt; P(A) &lt; 1 and, therefore, for this set T &minus;nA=A holds and
</p>
<p>P(B)= P(A) &lt; 1.
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>502 16 Stationary Sequences
</p>
<p>Remark 16.2.1 In Sects. 16.1 and 16.2 we tacitly or explicitly assumed (mainly for
the sake of simplicity of the exposition) that the components ξk of the stationary
</p>
<p>sequence ξ are real. However, we never actually used this, and so we could, as
</p>
<p>we did while studying Markov chains, assume that the state space X , in which
</p>
<p>ξk take their values, is an arbitrary measurable space. In the next section we will
</p>
<p>substantially use the fact that ξk are real- or vector-valued.
</p>
<p>16.3 The Ergodic Theorem
</p>
<p>For a sequence ξ1, ξ2, . . . of independent identically distributed random variables
</p>
<p>we proved in Chap. 11 the strong law of large numbers:
</p>
<p>Sn
</p>
<p>n
</p>
<p>a.s.&minus;&rarr; Eξ1, where Sn =
n&minus;1&sum;
</p>
<p>k=0
ξk.
</p>
<p>Now we will prove the same assertion under much broader assumptions&mdash;for sta-
</p>
<p>tionary ergodic sequences, i.e. for sequences that are weakly dependent on the aver-
</p>
<p>age.
</p>
<p>Let {ξk} be an arbitrary strictly stationary sequence, T be the associated measure
preserving transformation, and I be the σ -algebra of invariant sets.
</p>
<p>Theorem 16.3.1 (Birkhoff&ndash;Khintchin) If E|ξ0|&lt;&infin; then
</p>
<p>1
</p>
<p>n
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
ξk
</p>
<p>a.s.&minus;&rarr; E(ξ0 | I). (16.3.1)
</p>
<p>If the sequence {ξk} (or transformation T ) is ergodic, then
</p>
<p>1
</p>
<p>n
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
ξk
</p>
<p>a.s.&minus;&rarr; Eξ0. (16.3.2)
</p>
<p>Below we will be using the representation ξk = ξ(T kω) for ξ = ξ0. We will need
the following auxiliary result.
</p>
<p>Lemma 16.3.1 Set
</p>
<p>Sn(ω) :=
n&minus;1&sum;
</p>
<p>k=0
ξ
(
T kω
</p>
<p>)
, Mk(ω) := max
</p>
<p>{
0, S1(ω), . . . , Sk(ω)
</p>
<p>}
.
</p>
<p>Then, under the conditions of Theorem 16.3.1,
</p>
<p>E
[
ξ(ω)I{Mn&gt;0}(ω)
</p>
<p>]
&ge; 0
</p>
<p>for any n&ge; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 The Ergodic Theorem 503
</p>
<p>Proof For all k &le; n, one has Sk(T ω)&le;Mn(T ω), and hence
</p>
<p>ξ(ω)+Mn(T ω)&ge; ξ(ω)+ Sk(T ω)= Sk+1(ω).
</p>
<p>Because ξ(ω)&ge; S1(ω)&minus;Mn(T ω), we have
</p>
<p>ξ(ω)&ge; max
{
max(S1(ω), . . . , Sn(ω)
</p>
<p>}
&minus;Mn(T ω).
</p>
<p>Further, since
</p>
<p>{
Mn(ω) &gt; 0
</p>
<p>}
=
{
max
</p>
<p>(
S1(ω), . . . , Sn(ω)
</p>
<p>)
&gt; 0
</p>
<p>}
,
</p>
<p>we obtain that
</p>
<p>E
[
ξ(ω)I{Mn&gt;0}(ω)
</p>
<p>]
&ge; E
</p>
<p>(
max
</p>
<p>(
S1(ω), . . . , Sn(ω)
</p>
<p>)
&minus;Mn(T ω)
</p>
<p>)
I{Mn&gt;0}(ω)
</p>
<p>&ge; E
(
Mn(ω)&minus;Mn(T ω)
</p>
<p>)
I{Mn&gt;0}(ω)
</p>
<p>&ge; E
(
Mn(ω)&minus;Mn(T ω)
</p>
<p>)
= 0.
</p>
<p>The lemma is proved. �
</p>
<p>Proof of Theorem 16.3.1 Assertion (16.3.2) is an evident consequence of (16.3.1),
because, for ergodic T , the σ -algebra I is trivial and E(ξ |I) = Eξ a.s. Hence, it
suffices to prove (16.3.1).
</p>
<p>Without loss of generality, we can assume that E(ξ |I) = 0, for one can always
consider ξ &minus;E(ξ |I) instead of ξ .
</p>
<p>Let S := lim supn&rarr;&infin; n&minus;1Sn and S := lim infn&rarr;&infin; n&minus;1Sn. To prove the theorem,
it suffices to establish that
</p>
<p>0 &le; S &le; S &le; 0 a.s. (16.3.3)
</p>
<p>Since S(ω)= S(T ω), the random variable S is invariant and hence the set A&minus; ε =
{S(ω) &gt; ε} is also invariant for any ε &gt; 0. Introduce the variables
</p>
<p>ξ&lowast;(ω) :=
(
ξ(ω)&minus; ε
</p>
<p>)
IAε (ω),
</p>
<p>S&lowast;k (ω) := ξ&lowast;(ω)+ &middot; &middot; &middot; + ξ&lowast;
(
T k&minus;1ω
</p>
<p>)
,
</p>
<p>M&lowast;k (ω) := max
(
0, S&lowast;1 , . . . , S
</p>
<p>&lowast;
k
</p>
<p>)
.
</p>
<p>Then, by Lemma 16.3.1, for any n&ge; 1, one has
</p>
<p>Eξ&lowast;I{M&lowast;n&gt;0} &ge; 0.
</p>
<p>But, as n&rarr;&infin;,
{
M&lowast;n &gt; 0
</p>
<p>}
=
{
</p>
<p>max
1&le;k&le;n
</p>
<p>S&lowast;k &gt; 0
}
&uarr;
{
</p>
<p>sup
k&ge;1
</p>
<p>S&lowast;k &gt; 0
}</p>
<p/>
</div>
<div class="page"><p/>
<p>504 16 Stationary Sequences
</p>
<p>=
{
</p>
<p>sup
k&ge;1
</p>
<p>S&lowast;k
k
</p>
<p>&gt; 0
</p>
<p>}
=
{
</p>
<p>sup
k&ge;1
</p>
<p>Sk
</p>
<p>k
&gt; ε
</p>
<p>}
&cap;Aε =Aε.
</p>
<p>The last equality follows from the observation that
</p>
<p>Aε = {S &gt; ε} &sub;
{
</p>
<p>sup
k&ge;1
</p>
<p>Sk
</p>
<p>k
&gt; ε
</p>
<p>}
.
</p>
<p>Further, E|ξ&lowast;| &le; E|ξ | + ε. Hence, by the dominated convergence theorem,
</p>
<p>0 &le; Eξ&lowast;I{M&lowast;n&gt;0} &rarr; Eξ
&lowast;IAε .
</p>
<p>Consequently,
</p>
<p>0 &le; Eξ&lowast;IAε = E(ξ &minus; ε)IAε = Eξ IAε &minus; εP(Aε)
= EIAεE(ξ | I)&minus; εP(Aε)=&minus;εP(Aε).
</p>
<p>This implies that P(Aε)= 0 for any ε &gt; 0, and therefore P(S &le; 0)= 1.
In a similar way, considering the variables &minus;ξ instead of ξ , we obtain that
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>(
&minus;Sn
</p>
<p>n
</p>
<p>)
=&minus; lim inf
</p>
<p>n&rarr;&infin;
Sn
</p>
<p>n
=&minus;S,
</p>
<p>and P(&minus;S &le; 0)= 1, P(S &ge; 0)= 1. The required inequalities (16.3.3), and therefore
the theorem itself, are proved. �
</p>
<p>Now we can complete the
</p>
<p>Proof of Theorem 16.2.4 It remains to show that the ergodicity of T implies mixing
on the average. Indeed, let T be ergodic and A1,A2 &isin; F. Then, by Theorem 16.3.1,
we have
</p>
<p>ζn =
1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>k=1
I
(
T &minus;kA2
</p>
<p>) a.s.&minus;&rarr; P(A2), I(A1)ζn
a.s.&minus;&rarr; I(A1)P(A2).
</p>
<p>Since ζnI(A1) are bounded, one also has the convergence
</p>
<p>EζnI(A1)&rarr; P(A2) &middot; P(A1).
</p>
<p>Therefore
</p>
<p>1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>k=1
P
(
A1 &cap; T &minus;kA2
</p>
<p>)
= EI(A1)ζn &rarr; P(A1)P(A2).
</p>
<p>The theorem is proved. �
</p>
<p>Now we will show that convergence in mean also holds in (16.3.1) and (16.3.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 The Ergodic Theorem 505
</p>
<p>Theorem 16.3.2 Under the assumptions of Theorem 16.3.1, one has along with
(16.3.1) and (16.3.2) that, respectively,
</p>
<p>E
</p>
<p>∣∣∣∣
1
</p>
<p>n
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
ξk &minus;E(ξ0|I)
</p>
<p>∣∣∣∣&rarr; 0 (16.3.4)
</p>
<p>and
</p>
<p>E
</p>
<p>∣∣∣∣
1
</p>
<p>n
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
ξk &minus;Eξ0
</p>
<p>∣∣∣∣&rarr; 0 (16.3.5)
</p>
<p>as n&rarr;&infin;.
</p>
<p>Proof The assertion of the theorem follows in an obvious way from Theo-
rems 16.3.1, 6.1.7 and the uniform integrability of the sums
</p>
<p>1
</p>
<p>n
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
ξk,
</p>
<p>which follows from Theorem 6.1.6. �
</p>
<p>Corollary 16.3.1 If {ξk} is a stationary metric transitive sequence and a = Eξk &lt; 0,
then S(ω)= supk&ge;0 Sk(ω) is a proper random variable.
</p>
<p>The proof is obvious since, for 0 &lt; ε &lt; &minus;a, one has Sk &lt; (a + ε)k &lt; 0 for all
k &ge; n(ω) &lt;&infin;. �
</p>
<p>An unusual feature of Theorem 16.3.1 when compared with the strong law of
</p>
<p>large numbers from Chap. 11 is that the limit of
</p>
<p>1
</p>
<p>n
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
ξk
</p>
<p>can be a random variable. For instance, let T ωk := ωk+2 and d = 2l be even in the
situation of Example 16.1.1. Then the transformation T will not be ergodic, since
</p>
<p>the set A= {ω1,ω3, . . . ,ωd&minus;1} will be invariant, while P(A)= 1/2.
On the other hand, it is evident that, for any function ξ(ω), the sum
</p>
<p>1
</p>
<p>n
</p>
<p>n&minus;1&sum;
</p>
<p>k=0
ξ
(
T kω
</p>
<p>)
</p>
<p>will converge with probability 1/2 to
</p>
<p>2
</p>
<p>d
</p>
<p>l&minus;1&sum;
</p>
<p>j=0
ξ(ω2j+1)</p>
<p/>
</div>
<div class="page"><p/>
<p>506 16 Stationary Sequences
</p>
<p>(if ω= ωi and i is odd) and with probability 1/2 to
</p>
<p>2
</p>
<p>d
</p>
<p>l&sum;
</p>
<p>j=1
ξ(ω2j )
</p>
<p>(if ω= ωi and i is even). This limiting distribution is just the distribution of E(ξ |I).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 17
</p>
<p>Stochastic Recursive Sequences
</p>
<p>Abstract The chapter begins with introducing the concept of stochastic random se-
</p>
<p>quences in Sect. 17.1. The idea of renovating events together with the key results
</p>
<p>on ergodicity of stochastic random sequences and the boundedness thereof is pre-
</p>
<p>sented in Sect. 17.2, whereas the Loynes ergodic theorem for the case of monotone
</p>
<p>functions specifying the recursion is proved in Sect. 17.3. Section 17.4 establishes
</p>
<p>ergodicity conditions for contracting in mean Lipschitz transformations.
</p>
<p>17.1 Basic Concepts
</p>
<p>Consider two measurable state spaces 〈X,BX〉 and 〈Y,BY〉, and let {ξn} be a
sequence of random elements taking values in Y. If 〈Ω,F,P〉 is the underlying
probability space, then {ω : ξk &isin; B} &isin; F for any B &isin; BY . Assume, moreover,
that a measurable function f : X &times; Y &rarr; X is given on the measurable space
〈X &times; Y,BX &times; BY 〉, where BX &times; BY denotes the σ -algebra generated by sets
A&times;B with A &isin;BX and B &isin;BY .
</p>
<p>For simplicity&rsquo;s sake, by X and Y we can understand the real line R, and by BX,
</p>
<p>BY the σ -algebras of Borel sets.
</p>
<p>Definition 17.1.1 A sequence {Xn}, n= 0,1, . . . , taking values in 〈X,BX〉 is said
to be a stochastic recursive sequence (s.r.s.) driven by the sequence {ξn} if Xn satis-
fies the relation
</p>
<p>Xn+1 = f (Xn, ξn) (17.1.1)
for all n &ge; 0. For simplicity&rsquo;s sake we will assume that the initial state X0 is inde-
pendent of {ξn}.
</p>
<p>The distribution of the sequence {Xn, ξn} on 〈(X &times; Y)&infin;, (BX &times;BY )&infin;〉 can be
constructed in an obvious way from finite-dimensional distributions similarly to the
</p>
<p>manner in which we constructed on 〈X&infin;,B&infin;
X
〉 the distribution of a Markov chain X
</p>
<p>with values in 〈X,BX〉 from its transition function P(x,B)= P(X1(x) &isin; B). The
finite-dimensional distributions of {(X0, ξ0), . . . , (Xk, ξk)} for the s.r.s. are given by
the relations
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_17, &copy; Springer-Verlag London 2013
</p>
<p>507</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_17">http://dx.doi.org/10.1007/978-1-4471-5201-9_17</a></div>
</div>
<div class="page"><p/>
<p>508 17 Stochastic Recursive Sequences
</p>
<p>P(Xl &isin;Al, ξl &isin; Bl; l = 0, . . . , k)
</p>
<p>=
&int;
</p>
<p>B0
</p>
<p>&middot; &middot; &middot;
&int;
</p>
<p>Bk
</p>
<p>P(ξl &isin; dyl, l = 0, . . . , k)
k&prod;
</p>
<p>l=1
I
(
fl(X0, y0, . . . , yl) &isin;Al
</p>
<p>)
,
</p>
<p>where f1(x, y0) := f (x, y0), fl(x, y0, . . . , yl) := f (fl&minus;1(x, y0, . . . , yl&minus;1), yl).
Without loss of generality, the sequence {ξn} can be assumed to be given for all
</p>
<p>&minus;&infin; &lt; n &lt; &infin; (as we noted in Sect. 16.1, for a stationary sequence, the required
extension to n &lt; 0 can always be achieved with the help of Kolmogorov&rsquo;s theorem).
</p>
<p>A stochastic recursive sequence is a more general object than a Markov chain. It
is evident that if ξk are independent, then the Xn form a Markov chain. A stronger
</p>
<p>assertion is true as well: under broad assumptions about the space 〈X ,BX 〉, for any
Markov chain {Xn} in 〈X,BX〉 one can construct a function f and a sequence of
independent identically distributed random variables {ξn} such that (17.1.1) holds.
We will elucidate this statement in the simplest case when both X and Y coincide
</p>
<p>with the real line R. Let P(x,B), B &isin; B, be the transition function of the chain
{Xn}, and Fx(t)= P(x, (&minus;&infin;, t)) the distribution function of X1(x) (X0 = x). Then
if F&minus;1x (t) is the function inverse (in t) to Fx(t) and α &sub;=U0,1 is a random variable,
then, as we saw before (see e.g. Sect. 6.2), the random variable F&minus;1x (α) will have the
distribution function Fx(t). Therefore, if {αn} is a sequence of independent random
variables uniformly distributed over [0,1], then the sequence Xn+1 = F&minus;1Xn (αn) will
have the same distribution as the original chain {Xn}. Thus the Markov chain is an
s.r.s. with the function f (x, y)= F&minus;1x (y) and driving sequence {αn}, αn &sub;=U0,1.
</p>
<p>For more general state spaces X , a similar construction is possible if the σ -
</p>
<p>algebra BX is countably-generated (i.e. is generated by a countable collection of
</p>
<p>sets from X ). This is always the case for Borel σ -algebras in X = Rd , d &ge; 1 (see
[22]).
</p>
<p>One can always consider f (&middot;, ξn) as a sequence of random mappings of the space
X into itself. The principal problem we will be interested in is again (as in Chap. 13)
</p>
<p>that of the existence of the limiting distribution of Xn as n&rarr;&infin;.
In the following sections we will consider three basic approaches to this problem.
</p>
<p>17.2 Ergodicity and Renovating Events. Boundedness
</p>
<p>Conditions
</p>
<p>17.2.1 Ergodicity of Stochastic Recursive Sequences
</p>
<p>We introduce the σ -algebras
</p>
<p>F
ξ
l,n := σ {ξk; l &le; k &le; n},
</p>
<p>Fξn := σ {ξk; k &le; n} = F
ξ
&minus;&infin;,n,</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Ergodicity and Renovating Events. Boundedness Conditions 509
</p>
<p>Fξ := σ {ξk;&minus;&infin;&lt; k &lt;&infin;}= Fξ&minus;&infin;,&infin;.
</p>
<p>In the sequel, for the sake of definiteness and simplicity, we will assume the initial
</p>
<p>value X0 to be constant unless otherwise stated.
</p>
<p>Definition 17.2.1 An event A &isin; Fξn+m, m&ge; 0, is said to be renovating for the s.r.s.
{Xn} on the segment [n,n+m] if there exists a measurable function g : Ym+1 &rarr;X
such that, on the set A (i.e. for ω &isin;A),
</p>
<p>Xn+m+1 = g(ξn, . . . , ξn+m). (17.2.1)
</p>
<p>It is evident that, for ω &isin;A, relations of the form Xn+m+k+1 = gk(ξn, . . . , ξn+m+k)
will hold for all k &ge; 0, where gk is a function depending on its arguments only and
determined by the event A.
</p>
<p>The sequence of events {An}, An &isin; Fξn+m, where the integer m is fixed, is said
to be renovating for the s.r.s. {Xn} if there exists an integer n0 &ge; 0 such that, for
n&ge; n0, one has relation (17.2.1) for ω &isin;An, the function g being common for all n.
</p>
<p>We will be mainly interested in &ldquo;positive&rdquo; renovating events, i.e. renovating
</p>
<p>events having positive probabilities P(An) &gt; 0.
</p>
<p>The simplest example of a renovating event is the hitting by the sequence Xn of
</p>
<p>a fixed point x0 : An = {Xn = x0} (here m= 0), although such an event could be of
zero probability. Below we will consider a more interesting example.
</p>
<p>The motivation behind the introduction of renovating events is as follows. After
</p>
<p>the trajectory {Xk, ξk}, k &le; n+m, has entered a renovating set A &isin; Fξn+m, the future
evolution of the process will not depend on the values {Xk}, k &le; n+m, but will be
determined by the values of ξk, ξk+1, . . . only. It is not a complete &ldquo;regeneration&rdquo; of
the process which we dealt with in Chap. 13 while studying Markov chains (first of
</p>
<p>all, because the ξk are now, generally speaking, dependent), but it still enables us
</p>
<p>to establish ergodicity of the sequence Xn (in approximately the same sense as in
</p>
<p>Chap. 13).
</p>
<p>Note that, generally speaking, the event A and hence the function g may depend
</p>
<p>on the initial value X0. If X0 is random then a renovating event is to be taken from
</p>
<p>the σ -algebra F
ξ
n+m &times; σ(X0).
</p>
<p>In what follows it will be assumed that the sequence {ξn} is stationary. The sym-
bol U will denote the measure preserving shift transformation of Fξ -measurable ran-
</p>
<p>dom variables generated by {ξn}, so that Uξn = ξn+1, and the symbol T will denote
the shift transformation of sets (events) from the σ -algebra Fξ : ξn+1(ω)= ξn(T ω).
The symbols Un and T n, n&ge; 0, will denote the powers (iterations) of these transfor-
mations respectively (so that U1 =U , T 1 = T ; U0 and T 0 are identity transforma-
tions), while U&minus;n and T &minus;n are transformations inverse to Un and T n, respectively.
</p>
<p>A sequence of events {Ak} is said to be stationary if Ak = T kA0 for all k.
</p>
<p>Example 17.2.1 Consider a real-valued sequence
</p>
<p>Xn+1 = (Xn + ξn)+, X0 = const &ge; 0, n&ge; 0, (17.2.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>510 17 Stochastic Recursive Sequences
</p>
<p>where x+ = max(0, x) and {ξn} is a stationary metric transitive sequence. As we
already know from Sect. 12.4, the sequence {Xn} describes the dynamics of waiting
times for customers in a single-channel service system. The difference is that in
</p>
<p>Sect. 12.4 the initial value has subscript 1 rather than 0, and that now the sequence
</p>
<p>{ξn} has a more general nature. Furthermore, it was established in Sect. 12.4 that
Eq. (17.2.2) has the solution
</p>
<p>Xn+1 = max(Sn,n,X0 + Sn), (17.2.3)
</p>
<p>where
</p>
<p>Sn :=
n&sum;
</p>
<p>k=0
ξk, Sn,k := max&minus;1&le;j&le;k Sn,j , Sn,j :=
</p>
<p>n&sum;
</p>
<p>k=n&minus;j
ξk, Sn,&minus;1 := 0
</p>
<p>(17.2.4)
</p>
<p>(certain changes in the subscripts in comparison to (17.2.4) are caused by different
</p>
<p>indexing of the initial values). From representation (17.2.3) one can see that the
</p>
<p>event
</p>
<p>Bn := {X0 + Sn &le; 0, Sn,n = 0} &isin; Fξn
implies the event {Xn+1 = 0} and so is renovating for m= 0, g(y)&equiv; 0. If Xn+1 = 0
then
</p>
<p>Xn+2 = g1(ξn, ξn+1) := ξ+n+1, Xn+3 = g2(ξn, ξn+1, ξn+2) :=
(
ξ+n+1 + ξn+2
</p>
<p>)+
,
</p>
<p>and so on do not depend on X0.
</p>
<p>Now consider, for some n0 &gt; 1 and any n&ge; n0, the narrower event
</p>
<p>An :=
{
X0 + sup
</p>
<p>j&ge;n0
Sn,j &le; 0, Sn,&infin; := sup
</p>
<p>j&ge;&minus;1
Sn,j = 0
</p>
<p>}
</p>
<p>(we assume that the sequence {ξn} is defined on the whole axis). Clearly, An &sub; Bn &sub;
{Xn+1 = 0}, so An is a renovating event as well. But, unlike Bn, the renovating
event An is stationary: An = T nA0.
</p>
<p>We assume now that Eξ0 &lt; 0 and show that in this case P(A0) &gt; 0 for sufficiently
</p>
<p>large n0. In order to do this, we first establish that P(S0,&infin; = 0) &gt; 0. Since, by the
ergodic theorem, S0,j
</p>
<p>a.s.&minus;&rarr; &minus;&infin; as j &rarr; &infin;, we see that S0,&infin; is a proper random
variable and there exists a v such that P(S0,&infin; &lt; v) &gt; 0. By the total probability
formula,
</p>
<p>0 &lt; P(S0,&infin; &lt; v)=
&infin;&sum;
</p>
<p>j=0
P
(
S0,j&minus;1 &lt; S0,j &lt; v, sup
</p>
<p>k&ge;j
(S0,k &minus; S0,j )= 0
</p>
<p>)
.
</p>
<p>Therefore there exists a j such that
</p>
<p>P
(
</p>
<p>sup
k&ge;j
</p>
<p>(S0,k &minus; S0,j )= 0
)
&gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Ergodicity and Renovating Events. Boundedness Conditions 511
</p>
<p>But the supremum in the last expression has the same distribution as S0,&infin;. This
</p>
<p>proves that p := P(S0,&infin; = 0) &gt; 0. Next, since S0,j
a.s.&minus;&rarr; &minus;&infin;, one also has
</p>
<p>supj&ge;k S0,j
a.s.&minus;&rarr;&minus;&infin; as k&rarr;&infin;. Therefore, P(supj&ge;k S0,j &lt;&minus;X0)&rarr; 1 as k&rarr;&infin;,
</p>
<p>and hence there exists an n0 such that
</p>
<p>P
(
</p>
<p>sup
j&ge;n0
</p>
<p>S0,j &lt;&minus;X0
)
&gt; 1 &minus; p
</p>
<p>2
.
</p>
<p>Since P(AB)&ge; P(A)+P(B)&minus; 1 for any events A and B , the aforesaid implies that
P(A0)&ge; p/2 &gt; 0.
</p>
<p>In the assertions below, we will use the existence of stationary renovating events
</p>
<p>An with P(A0) &gt; 0 as a condition insuring convergence of the s.r.s. Xn to a station-
</p>
<p>ary sequence. However, in the last example such convergence can be established
</p>
<p>directly. Let Eξ0 &lt; 0. Then by (17.2.3), for any fixed v,
</p>
<p>P(Xn+1 &gt; v)= P(Sn,n &gt; v)+ P(Sn,n &le; v,X0 + Sn &gt; v),
</p>
<p>where evidently
</p>
<p>P(X0 + Sn &gt; v)&rarr; 0, P(Sn,n &gt; v) &uarr; P(S0,&infin; &gt; v)
</p>
<p>as n&rarr;&infin;. Hence the following limit exists
</p>
<p>lim
n&rarr;&infin;
</p>
<p>P(Xn &gt; v)= P(S0,&infin; &gt; v). (17.2.5)
</p>
<p>Recall that in the above example the sequence of events An becomes renovating
</p>
<p>for n &ge; n0. But we can define other renovating events Cn along with a number m
and function g : Rm+1 &rarr;R as follows:
</p>
<p>m := n0, Cn := T mAn, g(y0, . . . , ym) := 0.
</p>
<p>The events Cn &isin; Fξn+m are renovating for {Xn} on the segment [n,n + m] for all
n&ge; 0, so in this case the n0 in the definition of a renovating sequence will be equal
to 0.
</p>
<p>A similar argument can also be used in the general case for arbitrary renovat-
</p>
<p>ing events. Therefore we will assume in the sequel that the number n0 from the
</p>
<p>definition of renovating events is equal to zero.
</p>
<p>In the general case, the following assertion is valid.
</p>
<p>Theorem 17.2.1 Let {ξn} be an arbitrary stationary sequence and for the s.r.s. {Xn}
there exists a sequence of renovating events {An} such that
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>j=1
AjT
</p>
<p>&minus;sAj+s
</p>
<p>)
&rarr; 1 as n&rarr;&infin; (17.2.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>512 17 Stochastic Recursive Sequences
</p>
<p>uniformly in s &ge; 1. Then one can define, on a common probability space with {Xn},
a stationary sequence {Xn := UnX0} satisfying the equations Xn+1 = f (Xn, ξn)
and such that
</p>
<p>P
{
Xk =Xk for all k &ge; n
</p>
<p>}
&rarr; 1 as n&rarr;&infin;. (17.2.7)
</p>
<p>If the sequence {ξn} is metric transitive and the events An are stationary, then the
relations P(A0) &gt; 0 and P(
</p>
<p>⋃&infin;
n=0 An) = 1 are equivalent and imply (17.2.6) and
</p>
<p>(17.2.7).
</p>
<p>Note also that if we introduce the measure π(B) = P(X0 &isin; B) (as we did in
Chap. 13), then (17.2.7) will imply convergence in total variation:
</p>
<p>sup
B&isin;BX
</p>
<p>∣∣P(Xn &isin; B)&minus; π(B)
∣∣&rarr; 0 as n&rarr;&infin;.
</p>
<p>Proof of Theorem 17.2.1 First we show that (17.2.6) implies that
</p>
<p>P
</p>
<p>( &infin;⋂
</p>
<p>k=0
</p>
<p>{
Xn+k 
=U&minus;sXn+k+s
</p>
<p>}
)
&rarr; 0 as n&rarr;&infin; (17.2.8)
</p>
<p>uniformly in s &ge; 0. For a fixed s &ge; 1, consider the sequence Xsj = U&minus;sXs+j . It is
defined forj &ge;&minus;s, and
</p>
<p>Xs&minus;s =X0, Xs&minus;s+1 = f
(
Xs&minus;s, ξ&minus;s
</p>
<p>)
= f (X0, ξ&minus;s)
</p>
<p>and so on. It is clear that the event
</p>
<p>{
Xj =Xsj for some j &isin; [0, n]
</p>
<p>}
</p>
<p>implies the event
{
X+ n+ k =Xsn+k for all k &ge; 0
</p>
<p>}
.
</p>
<p>We show that
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>j=1
</p>
<p>{
Xj =Xsj
</p>
<p>}
)
&rarr; 1 as n&rarr;&infin;.
</p>
<p>For simplicity&rsquo;s sake put m= 0. Then, for the event Xj+1 =Xsj+1 to occur, it suf-
fices that the events Aj and T
</p>
<p>&minus;sAj+s occur simultaneously. In other words,
</p>
<p>n&minus;1⋃
</p>
<p>j=0
AjT
</p>
<p>&minus;sAj+s &sub;
n⋃
</p>
<p>j=1
</p>
<p>{
Xj =Xsj
</p>
<p>}
&sub;
</p>
<p>&infin;⋂
</p>
<p>k=0
</p>
<p>{
Xn+k =Xsn+k
</p>
<p>}
.
</p>
<p>Therefore (17.2.6) implies (17.2.8) and convergence
</p>
<p>P
(
Xnk 
=Xn+sk
</p>
<p>)
&rarr; 0 as n&rarr;&infin;</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Ergodicity and Renovating Events. Boundedness Conditions 513
</p>
<p>uniformly in k &ge; 0 and s &ge; 0. If we introduce the metric ρ putting ρ(x, y) := 1 for
x 
= y, ρ(x, x)= 0, then the aforesaid means that, for any δ &gt; 0, there exists an N
such that
</p>
<p>P
(
ρ
(
Xnk ,X
</p>
<p>n+s
k
</p>
<p>)
&gt; δ
</p>
<p>)
= P
</p>
<p>(
ρ
(
Xnk ,X
</p>
<p>n+s
k
</p>
<p>)

= 0
</p>
<p>)
&lt; δ
</p>
<p>for n&ge;N and any k &ge; 0, s &ge; 0, i.e. Xnk is a Cauchy sequence with respect to conver-
gence in probability for each k. Because any space X is complete with such a metric,
</p>
<p>there exists a random variable Xk such that Xnk
p&minus;&rarr;Xk as n&rarr;&infin; (see Lemma 4.2).
</p>
<p>Due to the specific nature of the metric ρ this means that
</p>
<p>P
(
Xnk 
=Xk
</p>
<p>)
&rarr; 0 as n&rarr;&infin;. (17.2.9)
</p>
<p>The sequence Xk is stationary. Indeed, as n&rarr;&infin;,
</p>
<p>P
(
Xk+1 
=UXk
</p>
<p>)
= P
</p>
<p>(
Xnk+1 
=UXnk
</p>
<p>)
+ o(1)= P
</p>
<p>(
Xnk+1 
=Xn&minus;1k+1
</p>
<p>)
+ o(1)= o(1).
</p>
<p>Since the probability P(Xk+1 
=UXk) does not depend on n, Xk+1 =UXk a.s.
Further, Xn+k+1 = f (Xn+k, ξn+k), and therefore
</p>
<p>Xnk+1 =U&minus;nf (Xn+k, ξn+k)= f
(
Xnk , ξk
</p>
<p>)
. (17.2.10)
</p>
<p>The left and right-hand sides here converge in probability to Xk+1 and f (Xk, ξk),
respectively. This means that Xk+1 = f (Xk, ξk).
</p>
<p>To prove convergence (17.2.7) it suffices to note that, by virtue of (17.2.10), the
</p>
<p>values Xnk and X
k , after having become equal for some k, will never be different for
</p>
<p>greater values of k. Therefore, as well as (17.2.9) one has the relation
</p>
<p>P
</p>
<p>(⋃
</p>
<p>k&ge;0
</p>
<p>{
Xnk 
=Xk
</p>
<p>}
)
= P
</p>
<p>(⋃
</p>
<p>k&ge;0
</p>
<p>{
Xk+n 
=Xk+n
</p>
<p>}
)
&rarr; 0 as n&rarr;&infin;,
</p>
<p>which is equivalent to (17.2.7).
</p>
<p>The last assertion of the theorem follows from Theorem 16.2.5. The theorem is
</p>
<p>proved. �
</p>
<p>Remark 17.2.1 It turns out that condition (17.2.6) is also a necessary one for con-
vergence (17.2.7) (see [6]). For more details on convergence of stochastic recursive
</p>
<p>sequences and their generalisations, and also on the relationship between (17.2.6)
</p>
<p>and conditions (I) and (II) from Chap. 13, see [6].
</p>
<p>In Example 17.2.1 the sequence Xk was actually found in an explicit form (see
</p>
<p>(17.2.3) and (17.2.5)):
</p>
<p>Xk = Sk,&minus;&infin; = sup
j&ge;0
</p>
<p>S
j
</p>
<p>k&minus;1. (17.2.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>514 17 Stochastic Recursive Sequences
</p>
<p>These random variables are proper by Corollary 16.3.1. It is not hard to also see
</p>
<p>that, for X0 = 0, one has (see (17.2.3))
</p>
<p>U&minus;1Xn+k &uarr;Xk. (17.2.12)
</p>
<p>17.2.2 Boundedness of Random Sequences
</p>
<p>Consider now conditions of boundedness of an s.r.s. in spaces X = [0,&infin;) and
X = (&minus;&infin;,&infin;). Assertions about boundedness will be stated in terms of existence
of stationary majorants, i.e. stationary sequences Mn such that
</p>
<p>Xn &le;Mn for all n.
</p>
<p>Results of this kind will be useful for constructing stationary renovating sequences.
</p>
<p>Majorants will be constructed for a class of random sequences more general than
</p>
<p>stochastic recursive sequences. Namely, we will consider the class of random se-
</p>
<p>quences satisfying the inequalities
</p>
<p>Xn+1 &le;
(
Xn + h(Xn, ξn)
</p>
<p>)+
, (17.2.13)
</p>
<p>where the measurable function h will in turn be bounded by rather simple functions
</p>
<p>of Xn and ξn. The sequence {ξn} will be assumed given on the whole axis.
</p>
<p>Theorem 17.2.2 Assume that there exist a number N &gt; 0 and a measurable func-
tion g1 with Eg1(ξn) &lt; 0 such that (17.2.13) holds with
</p>
<p>h(x, y)&le;
{
g1(y) for x &gt;N,
</p>
<p>g1(y)+N &minus; x for x &le;N.
(17.2.14)
</p>
<p>If X0 &le;M &lt;&infin;, then the stationary sequence
</p>
<p>Mn = max(M,N)+ sup
j&ge;&minus;1
</p>
<p>Sn&minus;1,j , (17.2.15)
</p>
<p>where Sn,&minus;1 = 0 and Sk,j = g1(ξk)+&middot; &middot; &middot;+g1(ξk&minus;j ) for j &ge; 0, is a majorant forXn.
</p>
<p>Proof For brevity&rsquo;s sake, put ζi := g1(ξi), Z := max(M,N), and Zn := Xn &minus; Z.
Then Zn will satisfy the following inequalities:
</p>
<p>Zn+1 &le;
{
(Zn +Z+ ζn)+ &minus;Z &le; (Zn + ζn)+ for Zn &gt;N &minus;Z,
(N + ζn)+ &minus;Z &le; ζ+n for Zn &le;N &minus;Z.
</p>
<p>Consider now a sequence {Yn} defined by the relations Y0 = 0 and
</p>
<p>Yn+1 = (Yn + ζn)+.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Ergodicity and Renovating Events. Boundedness Conditions 515
</p>
<p>Assume that Zn &le; Yn. If Zn &gt;N &minus;Z then
</p>
<p>Zn+1 &le; (Zn + ζn)+ &le; (Yn + ζn)+ = Yn+1.
</p>
<p>If Zn &le;N &minus;Z then
</p>
<p>Zn+1 &le; ζ+n &le; (Yn + ζn)+ = Yn+1.
</p>
<p>Because Z0 &le; 0 = Y0, it is evident that Zn &le; Yn for all n. But we know the solution
of the equation for Yn and, by virtue of (17.2.11) and (17.2.13),
</p>
<p>Xn &minus;Z &le; sup
j&ge;&minus;1
</p>
<p>Sn&minus;1,j .
</p>
<p>The theorem is proved. �
</p>
<p>Theorem 17.2.2A Assume that there exist a number N &gt; 0 and measurable func-
tions g1 and g2 such that
</p>
<p>Eg1(ξn) &lt; 0, Eg2(ξn) &lt; 0 (17.2.16)
</p>
<p>and
</p>
<p>h(x, y)&le;
{
g1(y) for x &gt;N,
</p>
<p>g1(y)+ g2(y) for x &le;N.
(17.2.17)
</p>
<p>If Z0 &le;M &lt;&infin;, then the conditions of Theorem 17.2.2 are satisfied (possibly for
other N and g1) and for Xn there exists a stationary majorant of the form (17.2.15).
</p>
<p>Proof We set g := &minus;Eg1(ξn) &gt; 0 and find L&gt; 0 such that E(g2(ξn); g2(ξn) &gt; L)&le;
g/2. Introduce the function
</p>
<p>g&lowast;1(y) := g1(y)+ g2(y)I
(
g2(y) &gt; L
</p>
<p>)
.
</p>
<p>Then Eg&lowast;1(ξn)&le;&minus;g/2 &lt; 0 and
</p>
<p>h(x, y)&le; g1(y)+ g2(y)I(x &le;N)
</p>
<p>&le; g&lowast;1(y)+ g2(y)I(x &le;N)&minus; g2(y)I
(
g2(y) &gt; L
</p>
<p>)
</p>
<p>&le; g&lowast;1(y)+LI(x &le;N)&le; g&lowast;1(y)+ (L+N &minus; x)I(x &le;N)
&le; g&lowast;1(y)+ (L+N &minus; x)I(x &le; L+N).
</p>
<p>This means that inequalities (17.2.14) hold with N replaced with N&lowast; = N + L.
The theorem is proved. �
</p>
<p>Note again that in Theorems 17.2.2 and 17.2.2A we did not assume that {Xn} is
an s.r.s.</p>
<p/>
</div>
<div class="page"><p/>
<p>516 17 Stochastic Recursive Sequences
</p>
<p>The reader will notice the similarity of the conditions of Theorems 17.2.2 and
</p>
<p>17.2.2A to the boundedness condition in Sect. 15.5, Theorem 13.7.3 and Corol-
</p>
<p>lary 13.7.1.
</p>
<p>The form of the assertions of Theorems 17.2.2 and 17.2.2A enables one to con-
</p>
<p>struct stationary renovating events for a rather wide class of nonnegative stochastic
</p>
<p>recursive sequences (so that X = [0,&infin;)) having, say, a &ldquo;positive atom&rdquo; at 0. It is
convenient to write such sequences in the form
</p>
<p>Xn+1 =
(
Xn + h(Xn, ξn)
</p>
<p>)+
. (17.2.18)
</p>
<p>Example 17.2.2 Let an s.r.s. (see (17.1.1)) be described by Eq. (17.2.18) and satisfy
conditions (17.2.14) or (17.2.17), where the function h is sufficiently &ldquo;regular&rdquo; to
</p>
<p>ensure that
</p>
<p>Bn,T =
⋂
</p>
<p>t&le;T
</p>
<p>{
h(t, ξn)&le;&minus;t
</p>
<p>}
</p>
<p>is an event for any T . (For instance, it is enough to require h(t, v) to have at most
</p>
<p>a countable set of discontinuity points t . Then the set Bn,T can be expressed as
</p>
<p>the intersection of countably many events
⋂
</p>
<p>k{h(tk, ξn) &le; &minus;tk}, where {tk} form
a countable set dense on [0, T ].) Furthermore, let there exist an L&gt; 0 such that
</p>
<p>P(Mn &lt;L,Bn,L) &gt; 0 (17.2.19)
</p>
<p>(Mn was defined in (17.2.15)). Then the event An = {Mn &lt; L}Bn,L is clearly a
positive stationary renovating event with the function g(y) = (h(0, y))+, m = 0.
(On the set An &isin; Fξn we have Xn+1 = 0, Xn+2 = h(0, ξn+1)+ and so on.) Therefore,
an s.r.s. satisfying (17.2.18) satisfies the conditions of Theorem 17.2.1 and is ergodic
</p>
<p>in the sense of assertion (17.2.7).
</p>
<p>It can happen that, from a point t &le; L, it would be impossible to reach the
point 0 in one step, but it could be done in m&gt; 1 steps. If B is the set of sequences
</p>
<p>(ξn, . . . , ξn+m) that effect such a transition, and P(Mn &lt;L), then An = {Mn &lt;L}B
will also be stationary renovating events.
</p>
<p>17.3 Ergodicity Conditions Related to the Monotonicity of f
</p>
<p>Now we consider ergodicity conditions for stochastic recursive sequences that are
</p>
<p>related to the analytic properties of the function f from (17.1.1). As we already
</p>
<p>noted, the sequence f (x, ξk), k = 1,2, . . . , may be considered as a sequence of
random transformations of the space X . Relation (17.1.1) shows that Xn+1 is the
result of the application of n+ 1 random transformations f (&middot;, ξk), k = 0,1, . . . , n,
to the initial value X0 = x &isin;X . Denoting by ξn+kn the vector ξn+kn = (ξn, . . . , ξn+k)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Ergodicity Conditions Related to the Monotonicity of f 517
</p>
<p>and by fk the k-th iteration of the function f : f1(x, y1)= f (x, y1), f2(x, y1, y2)=
f (f (x, y1), y2) and so on, we can re-write (17.1.1) for X0 = x in the form
</p>
<p>Xn+1 =Xn+1(x)= fn+1
(
x, ξn0
</p>
<p>)
,
</p>
<p>so that the &ldquo;forward&rdquo; and &ldquo;backward&rdquo; equations hold true:
</p>
<p>fn+1
(
x, ξn0
</p>
<p>)
= f
</p>
<p>(
fn
(
x, ξn&minus;10
</p>
<p>)
, ξn
</p>
<p>)
= fn
</p>
<p>(
f (x, ξ0), ξ
</p>
<p>n
1
</p>
<p>)
. (17.3.1)
</p>
<p>In the present section we will be studying stochastic recursive sequences for
</p>
<p>which the function f from representation (17.1.1) is monotone in the first argu-
</p>
<p>ment. To this end, we need to assume that a partial order relation &ldquo;&ge;&rdquo; is defined
in the space X . In the space X = Rd of vectors x = (x(1), . . . , x(d)) (or its sub-
spaces) the order relation can be introduced in a natural way by putting x1 &ge; x2 if
x1(k)&ge; x2(k) for all k.
</p>
<p>Furthermore, we will assume that, for each non-decreasing sequence x1 &le; x2 &le;
&middot; &middot; &middot; &le; xn &le; . . . , there exists a limit x &isin;X , i.e. the smallest element x &isin;X for which
xk &le; x for all k. In that case we will write xk &uarr; x or limk&rarr;&infin; xk = x. In X = Rd
such convergence will mean conventional convergence. To facilitate this, we will
</p>
<p>need to complete the space Rd by adding points with infinite components.
</p>
<p>Theorem 17.3.1 (Loynes) Suppose that the transformation f = f (x, y) and space
X satisfy the following conditions:
</p>
<p>(1) there exists an x0 &isin;X such that f (x0, y)&ge; x0 for all y &isin; Y ;
(2) the function f is monotone in the first argument: f (x1, y)&ge; f (x2, y) if x1 &ge; x2;
(3) the function f is continuous in the first argument with respect to the above
</p>
<p>convergence: f (xn, y) &uarr; f (x, y) if xn &uarr; x.
</p>
<p>Then there exists a stationary random sequence {Xn} satisfying Eq. (17.1.1):
Xn+1 =UXn = f (Xn, ξn), such that
</p>
<p>U&minus;nXn+s(x)) &uarr;Xs as n&rarr;&infin;, (17.3.2)
</p>
<p>where convergence takes place for all elementary outcomes.
</p>
<p>Since the distributions of Xn and U
&minus;nXn coincide, in the case where conver-
</p>
<p>gence of random variables ηn &uarr; η means convergence (in a certain sense) of their
distributions (as is the case when X = Rd ), Theorem 17.2.1 also implies conver-
gence of the distributions of Xn to that of X
</p>
<p>0 as n&rarr;&infin;.
</p>
<p>Remark 17.3.1 A substantial drawback of this theorem is that it holds only for a
single initial value X0 = x0. This drawback disappears if the point x0 is accessible
with probability 1 from any x &isin;X , and ξk are independent. In that case x0 is likely
to be a positive atom, and Theorem 13.6.1 for Markov chains is also applicable.</p>
<p/>
</div>
<div class="page"><p/>
<p>518 17 Stochastic Recursive Sequences
</p>
<p>The limiting sequence Xs in (17.3.2) can be &ldquo;improper&rdquo; (in spaces X = Rd it
may assume infinite values). The sequence Xs will be proper if the s.r.s. Xn satisfies,
</p>
<p>say, the conditions of the theorems of Sect. 15.5 or the conditions of Theorem 17.2.2.
</p>
<p>Proof of Theorem 17.3.1 Put
</p>
<p>v&minus;ks := fk+s
(
x0, ξ
</p>
<p>s&minus;1
&minus;k
</p>
<p>)
=U&minus;kfk+s
</p>
<p>(
x0, ξ
</p>
<p>s+k&minus;1
0
</p>
<p>)
=U&minus;kXk+s(x0).
</p>
<p>Here the superscript &minus;k indicates the number of the element of the driving sequence
{ξn}&infin;n=&minus;&infin; such that the elements of this sequence starting from that number are used
for constructing the s.r.s. The subscript s is the &ldquo;time epoch&rdquo; at which we observe
</p>
<p>the value of the s.r.s. From the &ldquo;backward&rdquo; equation in (17.3.1) we get that
</p>
<p>v&minus;k&minus;1s = fk+s
(
f (x0, ξ&minus;k&minus;1), ξ
</p>
<p>s&minus;1
&minus;k
</p>
<p>)
&ge; fk+s
</p>
<p>(
x0, ξ
</p>
<p>s&minus;1
&minus;k
</p>
<p>)
= v&minus;ks .
</p>
<p>This means that the sequence v&minus;ks increases as k grows, and therefore there exists a
random variable Xs &isin;X such that
</p>
<p>v&minus;ks =U&minus;kXk+s(x0) &uarr;Xs as k&rarr;&infin;.
</p>
<p>Further, v&minus;ks is a function of ξ
s&minus;1
&minus;k . Therefore, X
</p>
<p>s is a function of ξ s&minus;1&minus;&infin; :
</p>
<p>Xs =G
(
ξ s&minus;1&minus;&infin;
</p>
<p>)
.
</p>
<p>Hence
</p>
<p>UXs =UG
(
ξ s&minus;1&minus;&infin;
</p>
<p>)
=G
</p>
<p>(
ξ s&minus;&infin;
</p>
<p>)
=Xs+1,
</p>
<p>which means that {Xs} is stationary. Using the &ldquo;forward&rdquo; equation from (17.3.1),
we obtain that
</p>
<p>v&minus;k&minus;1s = f
(
fk+s
</p>
<p>(
x0, ξ
</p>
<p>s&minus;2
&minus;k&minus;1
</p>
<p>)
, ξs&minus;1
</p>
<p>)
= f
</p>
<p>(
v&minus;k&minus;1s&minus;1 , ξs&minus;1
</p>
<p>)
.
</p>
<p>Passing to the limit as k&rarr;&infin; gives, since f is continuous, that
</p>
<p>Xs = f
(
Xs&minus;1, ξs&minus;1
</p>
<p>)
.
</p>
<p>The theorem is proved. �
</p>
<p>Example 17.2.1 clearly satisfies all the conditions of Theorem 17.3.1 with X =
[0,&infin;), x0 = 0, and f (x, y)= (x + y)+.
</p>
<p>17.4 Ergodicity Conditions for Contracting in Mean Lipschitz
</p>
<p>Transformations
</p>
<p>In this section we will assume that X is a complete separable metric space with
</p>
<p>metric ρ. Consider the following conditions on the iterations Xk(x)= fk(x, ξ k&minus;10 ).</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Ergodicity Conditions 519
</p>
<p>Condition (B) (boundedness). For some x0 &isin; X and any δ &gt; 0, there exists an
N =Nδ such that, for all n&ge; 1,
</p>
<p>P
(
ρ
(
x0,Xn(x0) &gt; N
</p>
<p>))
= P
</p>
<p>(
ρ
(
x0, fn
</p>
<p>(
x0, ξ
</p>
<p>n&minus;1
0
</p>
<p>))
&gt;N
</p>
<p>)
&lt; δ.
</p>
<p>It is not hard to see that condition (B) holds (possibly with a different N ) as soon as
</p>
<p>we can establish that, for some m&ge; 1, the above inequality holds for all n&ge;m.
Condition (B) is clearly met for stochastic random sequences satisfying the con-
</p>
<p>ditions of Theorems 17.2.2 and 17.2.2A or the theorems of Sect. 15.5.
</p>
<p>Condition (C) (contraction in mean). The function f is continuous in the first
argument and there exist m&ge; 1, β &gt; 0 and a measurable function q :Rm &rarr;R such
that, for any x1 and x2,
</p>
<p>ρ
(
fm
</p>
<p>(
x1, ξ
</p>
<p>m&minus;1
0
</p>
<p>)
, fm
</p>
<p>(
x2, ξ
</p>
<p>m&minus;1
0
</p>
<p>))
&le; q
</p>
<p>(
ξm&minus;10
</p>
<p>)
ρ(x1, x2),
</p>
<p>m&minus;1E lnq
(
ξm&minus;10
</p>
<p>)
&le;&minus;β &lt; 0.
</p>
<p>Observe that conditions (B) and (C) are, generally speaking, not related to each
</p>
<p>other. Let, for instance, X = R, X0 &ge; 0, ξn &ge; 0, ρ(x, y) = |x &minus; y|, and f (x, y) =
bx + y, so that
</p>
<p>Xn+1 = bXn + ξn.
</p>
<p>Then condition (C) is clearly satisfied for 0 &lt; b &lt; 1, since
</p>
<p>∣∣f (x1, y)&minus; f (x2, y)
∣∣= b|x1 &minus; x2|.
</p>
<p>At the same time, condition (B) will be satisfied if and only if E ln ξ0 &lt;&infin;. Indeed, if
E ln ξ0 =&infin;, then the event {ln ξk &gt;&minus;2k lnb} occurs infinitely often a.s. But Xn+1
has the same distribution as
</p>
<p>bn+1X0 +
n&sum;
</p>
<p>k=0
bkξk = bn+1X0 +
</p>
<p>n&sum;
</p>
<p>k=0
exp{k lnb+ ln ξk},
</p>
<p>where, in the sum on the right-hand side, the number of terms exceeding exp{&minus;k lnb}
increases unboundedly as n grows. This means that X(n+ 1) p&rarr;&infin; as n&rarr;&infin;. The
case E ln ξ0 &lt;&infin; is treated in a similar way. The fact that (B), generally speaking,
does not imply (C) is obvious.
</p>
<p>As before, we will assume that the &ldquo;driving&rdquo; stationary sequence {ξn}&infin;n=&minus;&infin; is
given on the whole axis. Denote by U the respective distribution preserving shift
</p>
<p>operator.
</p>
<p>Convergence in probability and a.s. of a sequence of X -valued random vari-
</p>
<p>ables ηn &isin; X (ηn
p&minus;&rarr; η, ηn
</p>
<p>a.s.&minus;&rarr; η) is defined in the natural way by the rela-
tions P(ρ(ηn, η) &gt; δ) &rarr; 0 as n &rarr; &infin; and P(ρ(ηk, η) &gt; δ for some k &ge; n) &rarr; 0
as n&rarr;&infin; for any δ &gt; 0, respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>520 17 Stochastic Recursive Sequences
</p>
<p>Theorem 17.4.1 Assume that conditions (B) and (C) are met. Then there exists a
stationary sequence {Xn} satisfying (17.1.1):
</p>
<p>Xn+1 =UXn = f
(
Xn, ξn
</p>
<p>)
</p>
<p>such that, for any fixed x,
</p>
<p>U&minus;nXn+s(x)
a.s.&minus;&rarr;Xs as n&rarr;&infin;. (17.4.1)
</p>
<p>This convergence is uniform in x over any bounded subset of X .
</p>
<p>Theorem 17.2.2 implies the weak convergence, as n&rarr;&infin;, of the distributions
of Xn(x) to that of X
</p>
<p>0. Condition (B) is clearly necessary for ergodicity. As the
</p>
<p>example of a generalised autoregressive process below shows, condition (C) is also
</p>
<p>necessary in some cases.
</p>
<p>Set Yn :=UnXn(x0), where x0 is from condition (B). We will need the following
auxiliary result.
</p>
<p>Lemma 17.4.1 Assume that conditions (B) and (C) are met and the stationary se-
quence {q(ξ km+m&minus;1km )}&infin;k=&minus;&infin; is ergodic. Then, for any δ &gt; 0, there exists an nδ such
that, for all k &ge; 0,
</p>
<p>sup
k&ge;0
</p>
<p>P
(
ρ(Yn+k, Yn) &lt; δ for all n&ge; nδ
</p>
<p>)
&ge; 1 &minus; δ. (17.4.2)
</p>
<p>For ergodicity of {q(ξ km+m&minus;1)}&infin;k=&minus;&infin; it suffices that the transformation T m is met-
ric transitive.
</p>
<p>The lemma means that, with probability 1, the distance ρ(Yn+k, Yn) tends to zero
uniformly in k as n&rarr;&infin;. Relation (17.4.2) can also be written as P(Aδ)&le; δ, where
</p>
<p>Aδ :=
⋃
</p>
<p>n&ge;nδ
</p>
<p>{
ρ(Yn+k, Yn)&ge; δ
</p>
<p>}
.
</p>
<p>Proof of Lemma 17.4.1 By virtue of condition (B), there exists an N = Nδ such
that, for all k &ge; 1,
</p>
<p>P
(
ρ
(
x0,Xk(x0)
</p>
<p>)
&gt;N
</p>
<p>)
&le; δ
</p>
<p>4
.
</p>
<p>Hence
</p>
<p>P(Aδ)&le; δ/3 + P
(
Aδ;ρ(x0, θn,k)&le;N
</p>
<p>)
.
</p>
<p>The random variable θn,k := U&minus;n&minus;kXk(x0) has the same distribution as Xk(x0).
Next, by virtue of (C),
</p>
<p>ρ(Yn+k, Yn)&le; ρ
(
fn+k
</p>
<p>(
x0, ξ
</p>
<p>&minus;1
&minus;n&minus;k
</p>
<p>)
, fn
</p>
<p>(
x0, ξ
</p>
<p>&minus;1
&minus;n
</p>
<p>))</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Ergodicity Conditions 521
</p>
<p>&le; q
(
ξ&minus;1&minus;m
</p>
<p>)
ρ
(
fn+k&minus;m
</p>
<p>(
x0, ξ
</p>
<p>&minus;m&minus;1
&minus;n&minus;k
</p>
<p>)
, fn&minus;m
</p>
<p>(
x0, ξ
</p>
<p>&minus;m&minus;1
&minus;n
</p>
<p>))
</p>
<p>= q
(
ξ&minus;1&minus;m
</p>
<p>)
ρ
(
U&minus;n&minus;kXn+k&minus;m(x0),U
</p>
<p>&minus;nXn&minus;m(x0)
)
. (17.4.3)
</p>
<p>Denote by Bs the set of numbers n of the form n = lm + s, l = 0,1,2, . . . ,
0 &le; s &lt; m, and put
</p>
<p>λj := lnq
(
ξ
&minus;jm+m&minus;1
&minus;jm
</p>
<p>)
, j = 1,2, . . . .
</p>
<p>Then, for n &isin; Bs , we obtain from (17.4.3) and similar relations that
</p>
<p>ρ(Yn+k, Yn)&le; exp
{
</p>
<p>l&sum;
</p>
<p>j=1
λj
</p>
<p>}
ρ
(
U&minus;n&minus;kXk+s(x0),U
</p>
<p>&minus;nXs(x0)
)
, (17.4.4)
</p>
<p>where the last factor (denote it just by ρ) is bounded from above:
</p>
<p>ρ &le; ρ
(
x0,U
</p>
<p>&minus;n&minus;kXk+s(x0)
)
+ ρ
</p>
<p>(
x0,U
</p>
<p>&minus;nXs(x0)
)
.
</p>
<p>The random variables U&minus;nXj (x0) have the same distribution as Xj (x0). By virtue
of (B), there exists an N =Nδ such that, for all j &ge; 1,
</p>
<p>P
(
ρ
(
x0,Xj (x0)
</p>
<p>)
&gt;N
</p>
<p>)
&le; δ
</p>
<p>4m
.
</p>
<p>Hence, for all n, k and s, we have P(ρ &gt; 2N) &lt; δ/(2m), and the right-hand side
</p>
<p>of (17.4.4) does not exceed 2N exp{
&sum;l
</p>
<p>j=1 λj } on the complement set {ρ &le; 2N}.
Because Eλj &le; &minus;mβ &lt; 0 and the sequence {λj } is metric transitive, by the er-
</p>
<p>godic Theorem 16.3.1 we have
</p>
<p>l&sum;
</p>
<p>j=1
λj &lt;&minus;mβl/2
</p>
<p>for all l &ge; l(ω), where l(ω) is a proper random variable. Choose l1 and l2 so that the
inequalities
</p>
<p>&minus;mβl1
2
</p>
<p>&lt; ln δ &minus; ln 2N, P
(
l(ω) &gt; l2
</p>
<p>)
&lt;
</p>
<p>δ
</p>
<p>2
</p>
<p>hold. Then, putting
</p>
<p>lδ := max(l1, l2), nδ :=mlδ, Asδ :=
⋃
</p>
<p>n&ge;nδ, n&isin;Bs
</p>
<p>{
ρ(Yn+k, Yn)&ge; δ
</p>
<p>}
,
</p>
<p>we obtain that
</p>
<p>P
(
Asδ
</p>
<p>)
&le; P(ρ &gt; 2N)+P
</p>
<p>(
Asδ;ρ &le;N
</p>
<p>)
&le; δ
</p>
<p>2m
+P
</p>
<p>(⋃
</p>
<p>l&ge;lδ
</p>
<p>{
2N exp
</p>
<p>{
&minus;
</p>
<p>l&sum;
</p>
<p>j=0
λj
</p>
<p>}
&ge; δ
</p>
<p>})
.</p>
<p/>
</div>
<div class="page"><p/>
<p>522 17 Stochastic Recursive Sequences
</p>
<p>But the intersection of the events from the term with {lδ &ge; l(ω)} is empty. Therefore,
the former event is a subset of the event {l(ω) &gt; lδ}, and
</p>
<p>P
(
Asδ
</p>
<p>)
&le; δ
</p>
<p>m
, P(Aδ)&le;
</p>
<p>m&minus;1&sum;
</p>
<p>s=0
P
(
Asδ
</p>
<p>)
&le; δ.
</p>
<p>The lemma is proved. �
</p>
<p>Lemma 17.4.2 (Completeness of X with respect to convergence in probability) Let
X be a complete metric space. If a sequence of X -valued random elements ηn is
such that, for any δ &gt; 0,
</p>
<p>Pn := sup
k&ge;0
</p>
<p>P
(
ρ(ηn+k, ηn) &gt; δ
</p>
<p>)
&rarr; 0
</p>
<p>as n &rarr; &infin;, then there exists a random element η &isin; X such that η p&rarr; η (that is,
P(ρ(ηn, η) &gt; δ)&rarr; 0 as n&rarr;&infin;).
</p>
<p>Proof For given ε and δ choose nk , k = 0,1, . . . , such that
</p>
<p>sup
s
</p>
<p>P
(
ρ(ηnk+s, ηnk ) &gt; 2
</p>
<p>&minus;kδ
)
&lt; ε2&minus;k,
</p>
<p>and, for the sake of brevity, put ζk := ηnk . Consider the set
</p>
<p>D :=
&infin;⋂
</p>
<p>k=0
Dk, Dk :=
</p>
<p>{
ω ρ(ζk+1, ζk)&le; 2&minus;kδ
</p>
<p>}
.
</p>
<p>Then P(D) &gt; 1 &minus; 2ε and, for any ω &isin; D, one has ρ(ζk+s(ω), ζk(ω)) &lt; δ2k&minus;1 for
all s &ge; 1. Hence ζk(ω) is a Cauchy sequence in X and there exists an η= η(ω) &isin;X
such that ζk(ω)&rarr; η(ω). Since ε is arbitrary, this means that ζk
</p>
<p>a.s.&minus;&rarr; η as k &rarr;&infin;,
and
</p>
<p>P
(
ρ(ζ0, η) &gt; 2δ
</p>
<p>)
&le; P
</p>
<p>( &infin;⋃
</p>
<p>k=0
ρ(ζk+1, ζk) &gt; 2
</p>
<p>&minus;kδ
</p>
<p>)
</p>
<p>&le;
&infin;&sum;
</p>
<p>k=0
P
(
ρ(ζk+1, ζk) &gt; 2
</p>
<p>&minus;kδ
)
&le; 2ε.
</p>
<p>Therefore, for any n&ge; n0,
</p>
<p>P
(
ρ(ηn, η) &gt; 3δ
</p>
<p>)
&le; P
</p>
<p>(
ρ(ηn, ηn0) &gt; δ
</p>
<p>)
+ P
</p>
<p>(
ρ(ζ0, η) &gt; 2δ
</p>
<p>)
&le; 3ε.
</p>
<p>Since ε and δ are arbitrary, the lemma is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Ergodicity Conditions 523
</p>
<p>Proof of Theorem 17.4.1 From Lemma 17.4.1 it follows that
</p>
<p>sup
k
</p>
<p>P
(
ρ(Yn+k, Yn) &gt; δ
</p>
<p>)
&rarr; 0 as n&rarr;&infin;.
</p>
<p>This means that Yn is a Cauchy sequence with respect to convergence in probability,
</p>
<p>and by Lemma 17.4.2 there exists a random variable X0 such that
</p>
<p>Yn
p&minus;&rarr;X0,
</p>
<p>U&minus;nXn+s(x0)=U s
(
U&minus;n&minus;sXn+s(x0)
</p>
<p>)
=U sYn+s &rarr;U sX0 &equiv;Xs .
</p>
<p>(17.4.5)
</p>
<p>By continuity of f ,
</p>
<p>U&minus;nXn+s+1(x0)=U&minus;nf
(
Xn+s(x0), ξn+s
</p>
<p>)
</p>
<p>= f
(
U&minus;nXn+s(x0), ξs
</p>
<p>) p&minus;&rarr; f
(
Xs, ξs
</p>
<p>)
=Xs+1.
</p>
<p>We proved the required convergence for a fixed initial value x0. For an arbitrary
</p>
<p>x &isin; Cn = {z : ρ(x0, z)&le;N}, one has
</p>
<p>ρ
(
U&minus;nXn(x),X
</p>
<p>0
)
&le; ρ
</p>
<p>(
U&minus;nXn(x),U
</p>
<p>&minus;nXn(x0)
)
+ ρ
</p>
<p>(
U&minus;nXn(x0),X
</p>
<p>0
)
, (17.4.6)
</p>
<p>where the first term on the right-hand side converges in probability to 0 uniformly
</p>
<p>in x &isin; CN . For n= lm this follows from the inequality (see condition (C))
</p>
<p>ρ
(
U&minus;nXn(x),U
</p>
<p>&minus;nXn(x0)
)
&le;N exp
</p>
<p>{
l&sum;
</p>
<p>j=1
λj
</p>
<p>}
(17.4.7)
</p>
<p>and the above argument. Similar relations hold for n = lm + s, m &gt; s &gt; 0. This,
together with (17.4.5) and (17.4.6), implies that
</p>
<p>U&minus;nXn+s(x)
p&minus;&rarr;Xs =U sX0
</p>
<p>uniformly in x &isin; CN . This proves the assertion of the theorem in regard to conver-
gence in probability.
</p>
<p>We now prove convergence with probability 1. To this end, one should repeat
</p>
<p>the argument proving Lemma 17.4.1, but bounding ρ(X0,U&minus;nXn(x)) rather than
ρ(Yn+k, Yn). Assuming for simplicity&rsquo;s sake that s = 0 (n is a multiple of m), we
get (similarly to (17.4.4)) that, for any x,
</p>
<p>ρ
(
X0,U&minus;nXn(x)
</p>
<p>)
&le; ρ
</p>
<p>(
x,U&minus;nX0
</p>
<p>)
exp
</p>
<p>{
l&sum;
</p>
<p>j=1
λj
</p>
<p>}
. (17.4.8)
</p>
<p>The rest of the argument of Lemma 17.4.1 remains unchanged. This implies that,
</p>
<p>for any δ &gt; 0 and sufficiently large nδ ,
</p>
<p>P
</p>
<p>( ⋃
</p>
<p>n&ge;nδ
</p>
<p>{
ρ
(
X0,U&minus;nXn(x)
</p>
<p>)
&gt; δ
</p>
<p>})
&lt; δ.</p>
<p/>
</div>
<div class="page"><p/>
<p>524 17 Stochastic Recursive Sequences
</p>
<p>Theorem 17.4.1 is proved. �
</p>
<p>Example 17.4.1 (Generalised autoregression) Let X =R. A generalised autoregres-
sion process is defined by the relations
</p>
<p>Xn+1 =G
(
ζnF(Xn)+ ηn
</p>
<p>)
, (17.4.9)
</p>
<p>where F and G are functions mapping R #&rarr; R and ξn = (ζn, ηn) is a stationary
ergodic driving sequence, so that {Xn} is an s.r.s. with the function
</p>
<p>f (x, y)=G
(
y1,F (x)+ y2
</p>
<p>)
, y = (y1, y2) &isin; Y =R2.
</p>
<p>If the functions F and G are nondecreasing and left continuous, G(x)&ge; 0 for all
x &isin; R, and the elements ζn are nonnegative, then the process (17.4.9) satisfies the
condition of Theorem 17.3.1, and therefore U&minus;n+sXn(0) &uarr;Xs with probability 1 (as
n&rarr;&infin;). To establish convergence to a proper stationary sequence Xs , one has to
prove uniform boundedness in probability (in n) of the sequence Xn(0) (see below).
</p>
<p>Now we will establish under what conditions the sequence (17.4.9) will satisfy
</p>
<p>the conditions of Theorem 17.4.1. Suppose that the functions F and G satisfy the
</p>
<p>Lipschitz condition:
</p>
<p>∣∣G(x1)&minus;G(x2)
∣∣&le; cG|x1 &minus; x2|,
</p>
<p>∣∣F(x1)&minus; F(x2)
∣∣&le; cF |x1 &minus; x2|.
</p>
<p>Then
</p>
<p>∣∣f (x1, ξ0)&minus; f (x2, ξ0)
∣∣&le; cG
</p>
<p>∣∣ζ0
(
F(x1)&minus; F(x2)
</p>
<p>)∣∣&le; cF cG|ζ0||x1 &minus; x2|. (17.4.10)
</p>
<p>Theorem 17.4.2 Under the above assumptions, the sequence (17.4.9) will satisfy
condition (C) if
</p>
<p>ln cGcF +E ln |ζ0|&lt; 0. (17.4.11)
</p>
<p>The sequence (17.4.9) will satisfy condition (B) if (17.4.11) holds and, moreover,
</p>
<p>E
(
ln |η0|
</p>
<p>)+
&lt;&infin;. (17.4.12)
</p>
<p>When (17.4.11) and (17.4.12) hold, the sequence (17.4.9) has a stationary majorant,
i.e. there exists a stationary sequence Mn (depending on X0) such that |Xn| &le;Mn
for all n.
</p>
<p>Proof That condition (C) for ρ(x1, x2)= |x1 &minus; x2| follows from (17.4.10) is obvi-
ous. We prove (B). To do this, we will construct a stationary majorant for |Xn|. One
could do this using Theorems 17.2.2 and 17.2.2A. In our case, it is simpler to prove
</p>
<p>it directly, making use of the inequalities
</p>
<p>∣∣G(x)
∣∣&le;
</p>
<p>∣∣G(0)
∣∣+ cG|x|,
</p>
<p>∣∣F(x)
∣∣&le;
</p>
<p>∣∣F(0)
∣∣+ cF |x|,</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Ergodicity Conditions 525
</p>
<p>where we assume, for simplicity&rsquo;s sake, that G(0) and F(0) are finite. Then
</p>
<p>|Xn+1| &le;
∣∣G(0)
</p>
<p>∣∣+ cG|ζn| &middot;
∣∣F(Xn)
</p>
<p>∣∣+ cG|ηn|
</p>
<p>&le;
∣∣G(0)
</p>
<p>∣∣+ cGcF |ζn| &middot; |Xn| + cG|ζn| &middot;
∣∣F(0)
</p>
<p>∣∣+ cG|ηn| = βn
∣∣X(n)
</p>
<p>∣∣+ γn,
</p>
<p>where
</p>
<p>βn := cGcF |ζn| &ge; 0, γn :=
∣∣G(0)
</p>
<p>∣∣+ cG|ζn| &middot;
∣∣F(0)
</p>
<p>∣∣+ cG|ηn|
</p>
<p>E lnβn &lt; 0, E(lnγn)
+ &lt;&infin;.
</p>
<p>From this we get that, for X0 = x,
</p>
<p>|Xn+1| &le; |x|
n&prod;
</p>
<p>j=0
βj +
</p>
<p>n&minus;1&sum;
</p>
<p>l=0
</p>
<p>(
n&prod;
</p>
<p>j=n&minus;l
βj
</p>
<p>)
γn&minus;l&minus;1 + γn,
</p>
<p>U&minus;n|Xn+1| &le; |x|
0&prod;
</p>
<p>j=&minus;n
βj +
</p>
<p>&infin;&sum;
</p>
<p>l=0
</p>
<p>(
0&prod;
</p>
<p>j=&minus;l
βj
</p>
<p>)
γ&minus;l&minus;1 + γ0.
</p>
<p>(17.4.13)
</p>
<p>Put
</p>
<p>αi := lnβj , Sl :=
0&sum;
</p>
<p>j=&minus;l
αj .
</p>
<p>By the strong law of large numbers, there are only finitely many positive values
</p>
<p>Sl &minus; al, where 2a = Eαj &lt; 0. Therefore, for all l except for those with Sl &minus; al &gt; 0,
</p>
<p>0&prod;
</p>
<p>j=&minus;l
βj &lt; e
</p>
<p>al .
</p>
<p>On the other hand, γ&minus;l&minus;1 exceeds the level l only finitely often. This means that the
series in (17.4.13) (denote it by R) converges with probability 1. Moreover,
</p>
<p>S = sup
k&ge;0
</p>
<p>Sk &ge; Sn
</p>
<p>is a proper random variable. As result, we obtain that, for all n,
</p>
<p>U&minus;n|Xn+1| &le; |x|eS +R + γ0,
</p>
<p>where all the terms on the right-hand side are proper random variables. The required
</p>
<p>majorant
</p>
<p>Mn :=Un&minus;1
(
|x|eS +R + γ0
</p>
<p>)
</p>
<p>is constructed. This implies that (B) is met. The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>526 17 Stochastic Recursive Sequences
</p>
<p>The assertion of Theorem 17.4.2 can be extended to the multivariable case
</p>
<p>X =Rd , d &gt; 1, as well (see [6]).
Note that conditions (17.4.11) and (17.4.12) are, in a certain sense, necessary not
</p>
<p>only for convergence U&minus;n+sXn(x)&rarr; Xs , but also for the boundedness of Xn(x)
(or of X0) only. This fact can be best illustrated in the case when F(t)&equiv;G(t)&equiv; t .
In that case, U&minus;nXn+s+1(x) and Xs+1 admit explicit representations
</p>
<p>U&minus;nXn+s+1(x)= x
s&prod;
</p>
<p>j=&minus;n
ζj +
</p>
<p>n+s&sum;
</p>
<p>l=0
</p>
<p>s&prod;
</p>
<p>j=s&minus;l
ζjηs&minus;l&minus;1 + ηs,
</p>
<p>Xs+1 =
&infin;&sum;
</p>
<p>l=0
</p>
<p>s&prod;
</p>
<p>j=s&minus;l
ζjηs&minus;l&minus;1 + ηs .
</p>
<p>Assume that E ln ζ &ge; 0, η&equiv; 1, and put
</p>
<p>s := 0, zj := ln ζj , Zl :=
0&sum;
</p>
<p>j=&minus;l
zj .
</p>
<p>Then
</p>
<p>X1 = 1 +
&infin;&sum;
</p>
<p>l=0
eZl , where
</p>
<p>&infin;&sum;
</p>
<p>l=0
I(Zl &ge; 0)=&infin;
</p>
<p>with probability 1, and consequently X1 =&infin; and Xn &rarr;&infin; with probability 1.
If E[lnη]+ =&infin; and ζ = b &lt; 1 then
</p>
<p>X1 = η0 + b
&infin;&sum;
</p>
<p>l=0
exp{y&minus;l&minus;1 + l lnb},
</p>
<p>where yj = lnηj ; the event {y&minus;l&minus;1 &gt;&minus;l lnb} occurs infinitely often with probabil-
ity 1. This means that X1 =&infin; and Xn &rarr;&infin; with probability 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 18
</p>
<p>Continuous Time Random Processes
</p>
<p>Abstract This chapter presents elements of the general theory of continuous time
</p>
<p>processes. Section 18.1 introduces the key concepts of random processes, sample
</p>
<p>paths, cylinder sets and finite-dimensional distributions, the spaces of continuous
</p>
<p>functions and functions without discontinuities of the second kind, and equivalence
</p>
<p>of random processes. Section 18.2 presents the fundamental results on regularity
</p>
<p>of processes: Kolmogorov&rsquo;s theorem on existence of a continuous modification and
</p>
<p>Kolmogorov&ndash;Chentsov&rsquo;s theorem on existence of an equivalent process with trajec-
</p>
<p>tories without discontinuities of the second kind. The section also contains discus-
</p>
<p>sions of the notions of separability, stochastic continuity and continuity in mean.
</p>
<p>18.1 General Definitions
</p>
<p>Definition 18.1.1 A random process1 is a family of random variables ξ(t)= ξ(t,ω)
given on a common probability space 〈Ω,F,P〉 and depending on a parameter t
taking values in some set T .
</p>
<p>A random process will be written as {ξ(t), t &isin; T }.
The sequences of random variables ξ1, ξ2, . . . considered in the previous sec-
</p>
<p>tions are random processes for which T = {1,2,3, . . .}. The same is true of the
sums S1, S2, . . . of ξ1, ξ2, . . . Markov chains {Xn, n = 0,1, . . .}, martingales {Xn;
n &isin;N}, stationary and stochastic recursive sequences described in previous chapters
are also random processes. The processes for which the set T can be identified with
</p>
<p>the whole sequence {. . . ,&minus;1,0,1, . . .} or a part thereof are usually called random
processes in discrete time, or random sequences.
</p>
<p>If T coincides with a certain real interval T = [a, b] (this may be the whole real
line &minus;&infin;&lt; t &lt;&infin; or the half-line t &ge; 0), then the collection {ξ(t), t &isin; T } is said to
be a process in continuous time.
</p>
<p>Simple examples of such objects are renewal processes {η(t), t &ge; 0} described
in Chap. 10.
</p>
<p>1As well as the term &ldquo;random process&rdquo; one also often uses the terms &ldquo;stochastic&rdquo; or &ldquo;probabilistic&rdquo;
</p>
<p>processes.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_18, &copy; Springer-Verlag London 2013
</p>
<p>527</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_18">http://dx.doi.org/10.1007/978-1-4471-5201-9_18</a></div>
</div>
<div class="page"><p/>
<p>528 18 Continuous Time Random Processes
</p>
<p>In the present chapter we will be considering continuous time processes only.
Interpretation of the parameter t as time is, of course, not imperative. It appeared
</p>
<p>historically because in most problems from the natural sciences which led to the
</p>
<p>concept of random process the parameter t had the meaning of time, and the value
</p>
<p>ξ(t) was what one would observe at time t .
</p>
<p>The movement of a gas molecule as time passes, the storage level in a water
</p>
<p>reservoir, oscillations of an airplane&rsquo;s wing etc could be viewed as examples of real
</p>
<p>world random processes.
</p>
<p>The random function
</p>
<p>ξ(t)=
&infin;&sum;
</p>
<p>k=1
2&minus;kξk sinkt, t &isin; [0,2π],
</p>
<p>where the ξk are independent and identically distributed, is also an example of a
</p>
<p>random process.
</p>
<p>Consider a random process {ξ(t), t &isin; T }. If ω &isin;Ω is fixed, we obtain a func-
tion ξ(t), t &isin; T , which is often called a sample function, trajectory or path of the
process. Thus, the random values here are functions. As before, we could consider
here a sample probability space, which can be constructed for example as follows.
Consider the space X of functions x(t), t &isin; T , to which the trajectories ξ(t) belong.
Let, further, BT
</p>
<p>X
be the σ -algebra of subsets of X generated by the sets of the form
</p>
<p>C =
{
x &isin;X : x(t1) &isin; B1, . . . , x(tn) &isin; Bn
</p>
<p>}
(18.1.1)
</p>
<p>for any n, any t1, . . . , tn from T , and any Borel sets B1, . . . ,Bn. Sets of this form
</p>
<p>are called cylinders; various finite unions of cylinder sets form an algebra generat-
ing BT
</p>
<p>X
. If a process ξ(t,ω) is given, it defines a measurable mapping of 〈Ω,F〉
</p>
<p>into 〈X,BT
X
〉, since clearly ξ&minus;1(C)= {ω : ξ(&middot;,ω) &isin; C} &isin; F for any cylinder C, and
</p>
<p>therefore ξ&minus;1(B) &isin; F for any B &isin;BT
X
</p>
<p>. This mapping induces a distribution Pξ on
</p>
<p>〈X,BT
X
〉 defined by the equalities Pξ (B)= P(ξ&minus;1(B)). The triplet 〈X,BTX,Pξ 〉 is
</p>
<p>called the sample probability space. In that space, an elementary outcome ω is
identified with the trajectory of the process, and the measure Pξ is said to be the
</p>
<p>distribution of the process ξ .
Now if, considering the process {ξ(t)}, we fix the time epochs t1, t2, . . . , tn, we
</p>
<p>will get a multi-dimensional random variable (ξ(t1,ω), . . . , ξ(tn,ω)). The distri-
</p>
<p>butions of such variables are said to be the finite-dimensional distributions of the
process.
</p>
<p>The following function spaces are most often considered as spaces X in the the-
</p>
<p>ory of random processes with continual sets T .
</p>
<p>1. The set of all functions on T :
</p>
<p>X=RT =
&prod;
</p>
<p>t&isin;T
Rt ,
</p>
<p>where Rt are copies of the real line (&minus;&infin;,&infin;). This space is usually considered with
the σ -algebra BT
</p>
<p>R
of subsets of RT generated by cylinders.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.1 General Definitions 529
</p>
<p>2. The space C(T ) of continuous functions on T (we will write C(a, b) if
</p>
<p>T = [a, b]). In this space, along with the σ -algebra BTC generated by cylinder sub-
sets of C(T ) (this σ -algebra is smaller that the similar σ -algebra in RT ), one also
</p>
<p>often considers the σ -algebra BC(T ) (the Borel σ -algebra) generated by the sets
</p>
<p>open with respect to the uniform distance
</p>
<p>ρ(x, y) := sup
t&isin;T
</p>
<p>∣∣y(t)&minus; x(t)
∣∣, x, y &isin; C(T ).
</p>
<p>It turns out that, in the space C(T ), we always have BC(T ) =BTC (see, e.g., [14]).
3. The space D(T ) of functions having left and right limits x(t &minus; 0) and x(t + 0)
</p>
<p>at each point t , the value x(t) being equal either to x(t &minus; 0) or to x(t + 0). If
T = [a, b], it is also assumed that x(a)= x(a+ 0) and x(b)= x(b&minus; 0). This space
is often called the space of functions without discontinuities of the second kind.2 The
space of functions for which at all other points x(t) = x(t &minus; 0) (x(b) = x(t + 0))
will be denoted by D&minus;(T ) (D+(T )). The space D+(T ) (D&minus;(T )) will be called the
space of right-continuous (left-continuous) functions. For example, the trajectories
</p>
<p>of the renewal processes discussed in Chap. 10 belong to D+(0,&infin;).
In the space D(T ) one can also construct the Borel σ -algebra with respect to
</p>
<p>an appropriate metric, but we will restrict ourselves to using the σ -algebra BTD
of cylindric subsets of D(T ).
</p>
<p>Now we can formulate the following equivalent definition of a random process.
</p>
<p>Let X be a given function space, and G be the σ -algebra of its subsets containing
</p>
<p>the σ -algebra BT
X
</p>
<p>of cylinders.
</p>
<p>Definition 18.1.2 A random process ξ(t)= ξ(t,ω) is a measurable (in ω) mapping
of 〈Ω,F,P〉 into 〈X,G,Pξ 〉 (to each ω one puts into correspondence a function
ξ(t)= ξ(t,ω) so that ξ&minus;1(G)= {ω : ξ(&middot;) &isin;G} &isin; F for G &isin;G). The distribution Pξ
is said to be the distribution of the process.
</p>
<p>The condition BT
X
&sub; G is needed to ensure that the probabilities of cylinder
</p>
<p>sets and, in particular, the probabilities P(ξ(t) &isin; B), B &isin;BT
X
</p>
<p>are correctly defined,
</p>
<p>which means that ξ(t) are random variables.
</p>
<p>So far we have tacitly assumed that the process is given and it is known that
its trajectories lie in X. However, this is rarely the case. More often one tries to
</p>
<p>describe the process ξ(t) in terms of some characteristics of its distribution. One
could, for example, specify the finite-dimensional distributions of the process. From
</p>
<p>Kolmogorov&rsquo;s theorem on consistent distributions3 (see Appendix 2), it follows that
</p>
<p>2A discontinuity of the second kind is associated with either non-fading oscillations of increasing
</p>
<p>frequency or escape to infinity.
</p>
<p>3Recall the definition of consistent distributions. Let Rt , t &isin; T , be real lines and Bt the σ -algebras
of Borel subsets of Rt . Let Tn = {t1, . . . , tn} be a finite subset of T . The finite-dimensional dis-
tribution of (ξ(t1,ω), . . . , ξ(tn,ω)) is the distribution PTn on (R
</p>
<p>Tn ,BTn ), where RTn =
&prod;
</p>
<p>t&isin;Tn Rt
and BTn =
</p>
<p>&prod;
t&isin;Tn Bt . Let two finite subsets T
</p>
<p>&prime; and T &prime;&prime; of T be given, and (R&prime;,B&prime;) and (R&prime;&prime;,B&prime;&prime;)
</p>
<p>be the respective subspaces of (RT ,BT ). The distributions PT &prime; and PT &prime;&prime; on (R
&prime;,B&prime;) and (R&prime;&prime;,B&prime;&prime;)</p>
<p/>
</div>
<div class="page"><p/>
<p>530 18 Continuous Time Random Processes
</p>
<p>finite-dimensional distributions uniquely specify the distribution Pξ of the process
</p>
<p>on the space 〈RT ,BT
R
〉. That theorem can be considered as the existence theorem
</p>
<p>for random processes in 〈RT ,BT
R
〉 with prescribed finite-dimensional distributions.
</p>
<p>The space 〈RT ,BT
R
〉 is, however, not quite convenient for studying random pro-
</p>
<p>cesses. The fact is that by no means all relations frequently used in analysis gener-
</p>
<p>ate events, i.e. the sets which belong to the σ -algebra BT
R
</p>
<p>and whose probabilities
</p>
<p>are defined. Based on the definition, we can be sure that only the elements of the
</p>
<p>σ -algebra generated by {ξ(t) &isin; B}, t &isin; T , B being Borel sets, are events. The set
{supt&isin;T ξ(t) &lt; c}, for instance, does not have to be an event, for we only know its
representation in the form
</p>
<p>⋂
t&isin;T {ξ(t) &lt; c}, which is the intersection of an uncount-
</p>
<p>able collection of measurable sets when T is an interval on the real line.
Another inconvenience occurs as well: the distribution Pξ on 〈RT ,BTR〉 does not
</p>
<p>uniquely specify the properties of the trajectories of ξ(t). The reason is that the
</p>
<p>space RT is very rich, and if we know that x(&middot;) belongs to a set of the form (18.1.1),
this gives us no information about the behaviour of x(t) at points t different from
</p>
<p>t1, . . . , tn. The same is true of arbitrary sets A from B
T
R
</p>
<p>: roughly speaking, the
</p>
<p>relation x(&middot;) &isin; A can determine the values of x(t) at most at a countable set of
points. We will see below that even such a set as {x(t)&equiv; 0} does not belong to BT
</p>
<p>R
.
</p>
<p>To specify the behaviour of the entire trajectory of the process, it is not sufficient to
give a distribution on BT
</p>
<p>R
&mdash;one has to extend this σ -algebra.
</p>
<p>Prior to presenting the respective example, we will give the following definition.
</p>
<p>Definition 18.1.3 Processes ξ(t) and η(t) are said to be equivalent (or stochasti-
cally equivalent) if P(ξ(t) = η(t)) = 1 for all t &isin; T . In this case the process η is
called a modification of ξ .
</p>
<p>Finite-dimensional distributions of equivalent process clearly coincide, and
</p>
<p>therefore the distributions Pξ and Pη on 〈RT ,BTR〉 coincide, too.
</p>
<p>Example 18.1.1 Put
</p>
<p>xa(t) :=
{
</p>
<p>0 if t 
= a,
1 if t = a,
</p>
<p>and complete BT
R
</p>
<p>with the elements xa(t), a &isin; [0,1], and the element x0(t) &equiv; 0.
Let γ &sub;= U0,1. Consider two random processes ξ0(t) and ξ1(t) defined as follows:
ξ0(t)&equiv; x0(t), ξ1(t)= xγ (t). Then clearly
</p>
<p>P
(
ξ0(t)= ξ1(t)
</p>
<p>)
= P(γ 
= t)= 1,
</p>
<p>the processes ξ0 and ξ1 are equivalent, and hence their distributions on 〈RT ,BTR〉
coincide. However, we see that the trajectories of the processes are substantially
</p>
<p>different.
</p>
<p>are said to be consistent if their projections on the common part of subspaces R&prime; and R&prime;&prime; (if it
exists) coincide.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.1 General Definitions 531
</p>
<p>It is easy to see from the above example that the set of all continuous functions
</p>
<p>C(T ), the set {supt&isin;[0,1] x(t) &lt; x}, the one-point set {x(t) &equiv; 0} and many others
do not belong to BT
</p>
<p>R
. Indeed, if we assume the contrary&mdash;say, that C(T ) &isin;BT
</p>
<p>R
&mdash;
</p>
<p>then we would get from the equivalence of ξ0 and ξ1 that P(ξ0 &isin; C(0,1))= P(ξ1 &isin;
C(0,1)), while the former of these probabilities is 1 and the latter is 0.
</p>
<p>The simplest way of overcoming the above difficulties and inconveniences is to
</p>
<p>define the processes in the spaces C(T ) or D(T ) when it is possible. If, for example,
</p>
<p>ξ(t) &isin; C(T ) and η(t) &isin; C(T ), and they are equivalent, then the trajectories of the
processes will completely coincide with probability 1, since in that case
</p>
<p>⋂
</p>
<p>rational t
</p>
<p>{
ξ(t)= η(t)
</p>
<p>}
=
</p>
<p>⋂
</p>
<p>t&isin;T
</p>
<p>{
ξ(t)= η(t)
</p>
<p>}
=
{
ξ(t)= η(t) for all t &isin; T
</p>
<p>}
,
</p>
<p>where the probability of the event on the left-hand side is defined (this is the prob-
</p>
<p>ability of the intersection of a countable collection of sets) and equals 1. Similarly,
</p>
<p>the probabilities, say, of the events
{
</p>
<p>sup
t&isin;T
</p>
<p>ξ(t) &lt; c
}
=
</p>
<p>⋂
</p>
<p>t&isin;T
</p>
<p>{
ξ(t) &lt; c
</p>
<p>}
</p>
<p>are also defined.
</p>
<p>The same argument holds for the spaces D(T ), because each element x(&middot;) of D
is uniquely determined by its values x(t) on a countable everywhere dense set of t
</p>
<p>values (for example, on the set of rationals).
</p>
<p>Now assume that we have somehow established that the original process ξ(t) (let
</p>
<p>it be given on 〈RT ,BT
R
〉) has a continuous modification, i.e. an equivalent process
</p>
<p>η(t) such that its trajectories are continuous with probability 1 (or belong to the
</p>
<p>space D(T )). The above means, first of all, that we have somehow extended the
</p>
<p>σ -algebra BT
R
</p>
<p>&mdash;adding, say, the set C(T )&mdash;and now consider the distribution of ξ
</p>
<p>on the σ -algebra B̃T = σ(BT
R
,C(T )) (otherwise the above would not make sense).
</p>
<p>But the extension of the distribution of ξ from 〈RT ,BT
R
〉 to 〈RT , B̃T 〉 may not be
</p>
<p>unique. (We saw this in Example 18.1.1; the extension can be given by, say, putting
</p>
<p>P(ξ &isin; C(T ))= 0.) What we said above about the process η means that there exists
an extension Pη such that Pη(C(T ))= P(η &isin; C(T ))= 1.
</p>
<p>Further, it is often better not to deal with the inconvenient space 〈RT ,BT
R
〉 at all.
</p>
<p>To avoid it, one can define the distribution of the process η on the restricted space
</p>
<p>〈C(T ),BTC〉. It is clear that
</p>
<p>BTC &sub; B̃T = σ
(
BT
</p>
<p>R
,C(T )
</p>
<p>)
, BTC = B̃T &cap;C(T )
</p>
<p>(the former σ -algebra is generated by sets of the form (18.1.1) intersected with
</p>
<p>C(T )). Therefore, considering the distribution of η concentrated on C(T ), we can
</p>
<p>deal with the restriction of the space 〈RT , B̃T 〉 to 〈C(T ),BTC〉 and define the proba-
bility on the latter as Pη(A)= P(η &isin;A), A &isin;BTC &sub; B̃T . Thus we have constructed
a process η with continuous trajectories which is equivalent to the original process
ξ (if we consider their distributions in 〈RT ,BT
</p>
<p>R
〉).
</p>
<p>To realise this construction, one has now to learn how to find from the distribution
</p>
<p>of a process ξ whether it has a continuous modification η or not.</p>
<p/>
</div>
<div class="page"><p/>
<p>532 18 Continuous Time Random Processes
</p>
<p>Before stating and proving the respective theorems, note once again that the
</p>
<p>above-mentioned difficulties are mainly of a mathematical character, i.e. related
to the mathematical model of the random process. In real life problems, it is usually
</p>
<p>clear in advance whether the process under consideration is continuous or not. If it
</p>
<p>is &ldquo;physically&rdquo; continuous, and we want to construct an adequate model, then, of
</p>
<p>course, of all modifications of the process we have to take the continuous one.
</p>
<p>The same argument remains valid if, instead of continuous trajectories, one con-
</p>
<p>siders trajectories from D(T ). The problem essentially remains the same: the diffi-
</p>
<p>culties are eliminated if one can describe the entire trajectory of the process ξ(&middot;) by
the values ξ(t) on some countable set of t values. Processes possessing this property
</p>
<p>will be called regular.
</p>
<p>18.2 Criteria of Regularity of Processes
</p>
<p>First we will find conditions under which a process has a continuous modification.
</p>
<p>Without loss of generality, we will assume that T is the segment T = [0,1].
A very simple criterion for the existence of a continuous modification is based
</p>
<p>on the knowledge of two-dimensional distributions of ξ(t) only.
</p>
<p>Theorem 18.2.1 (Kolmogorov) Let ξ(t) be a random process given on 〈RT ,BT
R
〉
</p>
<p>with T = [0,1]. If there exist a &gt; 0, b &gt; 0 and c &lt;&infin; such that, for all t and t + h
from the segment [0,1],
</p>
<p>E
∣∣ξ(t + h)&minus; ξ(t)
</p>
<p>∣∣a &le; c|h|1+b, (18.2.1)
then ξ(&middot;) has a continuous modification.
</p>
<p>We will obtain this assertion as a consequence of a more general theorem, of
</p>
<p>which the conditions are somewhat more difficult to comprehend, but have essen-
</p>
<p>tially the same meaning as (18.2.1).
</p>
<p>Theorem 18.2.2 Let for all t , t + h &isin; [0,1],
P
(∣∣ξ(t + h)&minus; ξ(t)
</p>
<p>∣∣&gt; ε(h)
)
&le; q(h),
</p>
<p>where ε(h) and q(h) are decreasing even functions of h such that
&infin;&sum;
</p>
<p>n=1
ε
(
2&minus;n
</p>
<p>)
&lt;&infin;,
</p>
<p>&infin;&sum;
</p>
<p>n=1
2nq
</p>
<p>(
2&minus;n
</p>
<p>)
&lt;&infin;.
</p>
<p>Then ξ(&middot;) has a continuous modification.
</p>
<p>Proof We will make use of approximations of ξ(t) by continuous processes. Put
</p>
<p>tn,r := r2&minus;n, r = 0,1, . . . ,2n,
ξn(t) := ξ(tn,r )+ 2n(t &minus; tn,r)
</p>
<p>[
ξ(tn,r+1)&minus; ξ(tn,r )
</p>
<p>]
for t &isin; [tn,r , tn,r+1].</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Criteria of Regularity of Processes 533
</p>
<p>Fig. 18.1 Illustration to the
</p>
<p>proof of Theorem 18.2.2:
</p>
<p>construction of piece-wise
</p>
<p>linear approximations to the
</p>
<p>process ξ(t)
</p>
<p>From Fig. 18.1 we see that
</p>
<p>∣∣ξn+1(t)&minus; ξn(t)
∣∣&le;
</p>
<p>∣∣∣∣ξ(tn+1,2r+1)&minus;
1
</p>
<p>2
</p>
<p>[
ξ(tn+1,2r )+ ξ(tn+1,2r+2)
</p>
<p>]∣∣∣∣&le;
1
</p>
<p>2
(α + β),
</p>
<p>where α := |ξ(tn+1,2r+1)&minus; ξ(tn+1,2r )|, β := |ξ(tn+1,2r+1)&minus; ξ(tn+1,2r+2)|. This im-
plies that
</p>
<p>Zn := max
t&isin;[tn,r ,tn,r+1]
</p>
<p>∣∣ξn+1(t)&minus; ξn(t)
∣∣&le; 1
</p>
<p>2
(α + β),
</p>
<p>P
(
Zn &gt; ε
</p>
<p>(
2&minus;n
</p>
<p>))
&le; P
</p>
<p>(
α &gt; ε
</p>
<p>(
2&minus;n
</p>
<p>))
+ P
</p>
<p>(
β &gt; ε
</p>
<p>(
2&minus;n
</p>
<p>))
&le; 2q
</p>
<p>(
2&minus;n
</p>
<p>)
</p>
<p>(note that since the trajectories of ξn(t) are continuous, {Zn &gt; ε(2&minus;n)} &isin;BTR , which
is not the case in the general situation). Since here we have altogether 2n segments
</p>
<p>of the form [tn,r , tn,r+1], r = 0,1, . . . ,2n &minus; 1, one has
</p>
<p>P
(
</p>
<p>max
t&isin;[0,1]
</p>
<p>∣∣ξn+1(t)&minus; ξn(t)
∣∣&gt; ε(2&minus;n)
</p>
<p>)
&le; 2n+1q(2&minus;n).
</p>
<p>Because
&sum;&infin;
</p>
<p>n=1 2
nq(2&minus;n) &lt;&infin;, by the Borel&ndash;Cantelli criterion, for almost all ω (i.e.
</p>
<p>for ω &isin;A, P(A)= 1), there exists an n(ω) such that, for all n&ge; n(ω),
max
t&isin;[0,1]
</p>
<p>∣∣ξn+1(t)&minus; ξn(t)
∣∣&equiv; ρ(ξn+1, ξn) &lt; ε
</p>
<p>(
2&minus;n
</p>
<p>)
.
</p>
<p>From this it follows that ξn is a Cauchy sequence a.s., since
</p>
<p>ρ(ξn, ξm)&le; εn :=
&infin;&sum;
</p>
<p>n
</p>
<p>ε(2&minus;k)&rarr; 0
</p>
<p>as n &rarr; &infin; for all m &gt; n, ω &isin; A. Therefore, for ω &isin; A, there exists the limit
η(t)= limn&rarr;&infin; ξn(t), and |ξn(t)&minus; η(t)| &le; εn, so that convergence ξn(t)&rarr; η(t) is
uniform. Together with continuity of ξn(t) this implies that η(t) is also continuous
</p>
<p>(this argument actually shows that the space C(0,1) is complete).
</p>
<p>It remains to verify that ξ and η are equivalent. For t = tn,r one has ξn+k(t) =
ξ(t) for all k &ge; 0, so that η(t)= ξ(t). If t 
= tn,r for all n and r , then there exists a
sequence rn such that tt,rn &rarr; t and 0 &lt; t &minus; tt,rn &lt; 2&minus;n, and hence
</p>
<p>P
(∣∣ξ(tt,rn)&minus; ξ(t)
</p>
<p>∣∣&gt; ε(t &minus; tt,rn)
)
&le; q(t &minus; tt,rn),
</p>
<p>P
(∣∣ξ(tt,rn)&minus; ξ(t)
</p>
<p>∣∣&gt; ε(2&minus;n)
)
&le; q
</p>
<p>(
2&minus;n
</p>
<p>)
.
</p>
<p>By the Borel&ndash;Cantelli criterion this means that ξn,rn &rarr; ξ with probability 1. At
the same time, by virtue of the continuity of η(t) one has η(tt,rn)&rarr; η(t). Because
ξ(tt,rn)= η(tt,rn), we have ξ(t)= η(t) with probability 1.
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>534 18 Continuous Time Random Processes
</p>
<p>Corollary 18.2.1 If
</p>
<p>E
∣∣ξ(t + h)&minus; ξ(t)
</p>
<p>∣∣a &le; c|h|| log |h||1+b (18.2.2)
</p>
<p>for some b &gt; a &gt; 0 and c &lt;&infin;, then the conditions of Theorem 18.2.2 are satisfied
and hence ξ(t) has a continuous modification.
</p>
<p>Condition (18.2.2) will certainly be satisfied if (18.2.1) holds, so that Kol-
</p>
<p>mogorov&rsquo;s theorem is a consequence of Theorem 18.2.2.
</p>
<p>Proof of Corollary 18.2.1 Put ε(h) := | log2 |h||&minus;β , 1 &lt; β &lt; b/a. Then
&infin;&sum;
</p>
<p>n=1
ε
(
2&minus;n
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=1
n&minus;β &lt;&infin;,
</p>
<p>and from Chebyshev&rsquo;s inequality we have
</p>
<p>P
(∣∣ξ(t + a)&minus; ξ(t)
</p>
<p>∣∣&gt; ε(h)
)
&le; c|h|| log2 |h||1+b
</p>
<p>(
ε(h)
</p>
<p>)&minus;a = c|h|| log2 |h||1+δ
=: q(h),
</p>
<p>where δ = b&minus; aβ &gt; 0. It remains to note that
&infin;&sum;
</p>
<p>n=1
2nq
</p>
<p>(
2&minus;n
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>∣∣log2 2&minus;n
∣∣&minus;1&minus;δ &lt;&infin;.
</p>
<p>The corollary is proved. �
</p>
<p>The criterion for ξ(t) to have a modification belonging to the space D(T ) is more
</p>
<p>complicated to formulate and prove, and is related to weaker conditions imposed on
</p>
<p>the process. We confine ourselves here to simply stating the following assertion.
</p>
<p>Theorem 18.2.3 (Kolmogorov&ndash;Chentsov) If, for some α &ge; 0, β &ge; 0, b &gt; 0, and all
t , h1 &le; t &le; 1 &minus; h2, h1 &ge; 0, h2 &ge; 0,
</p>
<p>E
∣∣ξ(t)&minus; ξ(t &minus; h1)
</p>
<p>∣∣α∣∣ξ(t + h2)&minus; ξ(t)β
∣∣&lt; ch1+b, h= h1 + h2, (18.2.3)
</p>
<p>then there exists a modification of ξ(t) in D(0,1).4
</p>
<p>Condition (18.2.3) admits the following extension:
</p>
<p>P
(∣∣ξ(t + h2)&minus; ξ(t)
</p>
<p>∣∣ &middot;
∣∣ξ(t)&minus; ξ(t &minus; h1)
</p>
<p>∣∣&ge; ε(h)
)
&le; q(h), (18.2.4)
</p>
<p>where ε(h) and q(h) have the same meaning as in Theorem 18.2.2. Under condition
</p>
<p>(18.2.4) the assertion of the theorem remains valid.
</p>
<p>The following two examples illustrate, to a certain extent, the character of the
</p>
<p>conditions of Theorems 18.2.1&ndash;18.2.3.
</p>
<p>4For more details, see, e.g., [9].</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Criteria of Regularity of Processes 535
</p>
<p>Example 18.2.1 Assume that a random process ξ(t) has the form
</p>
<p>ξ(t)=
r&sum;
</p>
<p>k=1
ξkϕk(t),
</p>
<p>where ϕk(t) satisfy the H&ouml;lder condition
∣∣ϕk(t + h)&minus; ϕk(t)
</p>
<p>∣∣&le; c |h|α,
</p>
<p>α &gt; 0, and (ξ1, . . . , ξr) is an arbitrary random vector such that all E|ξk|l are finite
for some l &gt; 1/α. Then the process ξ(t) (which is clearly continuous) satisfies con-
</p>
<p>dition (18.2.1). Indeed,
</p>
<p>E
∣∣ξ(t + h)&minus; ξ(t)
</p>
<p>∣∣l &le; c1
r&sum;
</p>
<p>k=1
E|ξk|lcl |h|αl &le; c2|hαl |, al &gt; 1.
</p>
<p>Example 18.2.2 Let γ &sub;=U0,1, ξ(t)= 0 for t &lt; γ , and ξ(t)= 1 for t &ge; γ . Then
</p>
<p>E
∣∣ξ(t + h)&minus; ξ(t)
</p>
<p>∣∣l = P
(
γ &isin; (t, t + h)
</p>
<p>)
= h
</p>
<p>for any l &gt; 0. Here condition (18.2.1) is not satisfied, although |ξ(t+h)&minus;ξ(t)| p&rarr; 0
as h&rarr; 0. Condition (18.2.3) is clearly met, for
</p>
<p>E
∣∣ξ(t)&minus; ξ(t &minus; h1)
</p>
<p>∣∣ &middot;
∣∣ξ(t + h2)&minus; ξ(t)
</p>
<p>∣∣= 0. (18.2.5)
We will get similar results if we take ξ(t) to be the renewal process for a sequence
</p>
<p>γ1, γ2, . . . , where the distribution of γj has a density. In that case, instead of (18.2.5)
</p>
<p>one will obtain the relation
</p>
<p>E
∣∣ξ(t)&minus; ξ(t &minus; h1)
</p>
<p>∣∣ &middot;
∣∣ξ(t + h2)&minus; ξ(t)
</p>
<p>∣∣&le; ch1h2 &le; ch2.
In the general case, when we do not have data for constructing modifications
</p>
<p>of the process ξ in the spaces C(T ) or D(T ), one can overcome the difficulties
</p>
<p>mentioned in Sect. 18.1 with the help of the notion of separability.
</p>
<p>Definition 18.2.1 A process ξ(t) is said to be separable if there exists a countable
set S which is everywhere dense in T and
</p>
<p>P
(
</p>
<p>lim sup
u&rarr;t
u&isin;S
</p>
<p>ξ(u)&ge; ξ(t)&ge; lim inf
u&rarr;t
u&isin;S
</p>
<p>ξ(u) for all t &isin; T
)
= 1. (18.2.6)
</p>
<p>This is equivalent to the property that, for any interval I &sub; T ,
</p>
<p>P
(
</p>
<p>sup
u&isin;l&cap;S
</p>
<p>ξ(u)= sup
u&isin;I
</p>
<p>ξ(u); inf
u&isin;l&cap;S
</p>
<p>ξ(u)= inf
u&isin;I
</p>
<p>ξ(u)
)
= 1.
</p>
<p>It is known (Doob&rsquo;s theorem5) that any random process has a separable modifi-
cation.
</p>
<p>5See [14, 26].</p>
<p/>
</div>
<div class="page"><p/>
<p>536 18 Continuous Time Random Processes
</p>
<p>Constructing a separable modification of a process, as well as constructing mod-
</p>
<p>ifications in spaces C(T ) and D(T ), means extending the σ -algebra BT
R
</p>
<p>, to which
</p>
<p>one adds uncountable intersections of the form
</p>
<p>A=
⋂
</p>
<p>u&isin;I
</p>
<p>{
ξ(u) &isin; [a, b]
</p>
<p>}
=
{
</p>
<p>sup
u&isin;I
</p>
<p>ξ(u)&le; b, inf
u&isin;I
</p>
<p>ξ(u)&ge; a
}
,
</p>
<p>and extending the measure P to the extended σ -algebra using the equalities
</p>
<p>P(A)= P
( ⋂
</p>
<p>u&isin;I&cap;S
{ξ(u) &isin; [a, b]
</p>
<p>)
,
</p>
<p>where in the probability on the right-hand side we already have an element of BT
R
</p>
<p>.
</p>
<p>For separable processes, such sets as the set of all nondecreasing functions, the
</p>
<p>sets C(T ), D(T ) and so on, are events. Processes from C(T ) or D(T ) are automat-
</p>
<p>ically separable. And vice versa, if a process is separable and admits a continuous
</p>
<p>modification (modification from D(T )) then it will be continuous (belong to D(T ))
</p>
<p>itself. Indeed, if η is a continuous modification of ξ then
</p>
<p>P
(
ξ(t)= η(t) for all t &isin; S
</p>
<p>)
= 1.
</p>
<p>From this and (18.2.6) we obtain
</p>
<p>P
(
</p>
<p>lim sup
u&rarr;t
u&isin;S
</p>
<p>η(u)&ge; ξ(t)&ge; lim inf
u&rarr;t
u&isin;S
</p>
<p>η(u) for all t &isin; T
)
= 1.
</p>
<p>Since lim supu&rarr;t η(u)= lim infu&rarr;t η(u)= η(t), one has
P
(
ξ(t)= η(t) for all t &isin; T
</p>
<p>)
= 1.
</p>
<p>In Example 18.1.1, the process ξ1(t) is clearly not separable. The process ξ0(t)
</p>
<p>is a separable modification of ξ1(t).
</p>
<p>As well as pathwise continuity, there is one more way of characterising the con-
</p>
<p>tinuity of a random process.
</p>
<p>Definition 18.2.2 A random process ξ(t) is said to be stochastically continuous if,
for all t &isin; T , as h&rarr; 0,
</p>
<p>ξ(t + h) p&rarr; ξ(t)
(
P
(∣∣ξ(t + h)&minus; ξ(t)
</p>
<p>∣∣&gt; ε
)
&rarr; 0
</p>
<p>)
.
</p>
<p>Here we deal with the two-dimensional distributions of ξ(t) only.
</p>
<p>It is clear that all processes with continuous trajectories are stochastically con-
</p>
<p>tinuous. But not only them. The discontinuous processes from Examples 18.1.1
</p>
<p>and 18.2.2 are also stochastically continuous. A discontinuous process is not
</p>
<p>stochastically continuous if, for a (random) discontinuity point τ (ξ(τ + 0) 
=
ξ(τ &minus; 0)), the probability P(τ = t0) is positive for some fixed point t0.
</p>
<p>Definition 18.2.3 A process ξ(t) is said to be continuous in mean of order r (in
mean when r = 1; in mean quadratic when r = 2) if, for all t &isin; T , as h&rarr; 0,
</p>
<p>ξ(t + h) (r)&minus;&rarr; ξ(t) or, which is the same, E
∣∣ξ(t + h)&minus; ξ(t)
</p>
<p>∣∣r &rarr; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Criteria of Regularity of Processes 537
</p>
<p>The discontinuous process ξ(t) from Example 18.2.2 is continuous in mean of
</p>
<p>any order. Therefore the continuity in mean and stochastic continuity do not say
</p>
<p>much about the pathwise properties (they only say that a jump in a neighbour-
</p>
<p>hood of any fixed point t is unlikely). As Kolmogorov&rsquo;s theorem shows, in or-
</p>
<p>der to characterise the properties of trajectories, one needs quantitative bounds for
E|ξ(t + h)&minus; ξ(t)|r or for P(|ξ(t + h)&minus; ξ(t)|&gt; ε).
</p>
<p>Continuity theorems for moments imply that, for a stochastically continuous pro-
cess ξ(t) and any continuous bounded function g(x), the function Eg(ξ(t)) is con-
tinuous. This assertion remains valid if we replace the boundedness of g(x) with the
condition that
</p>
<p>sup
t
E
∣∣g
(
ξ(t)
</p>
<p>)∣∣α &lt;&infin; for some α &gt; 1.
</p>
<p>The consequent Chaps. 19, 21 and 22 will be devoted to studying random pro-
</p>
<p>cesses which can be given by specifying the explicit form of their finite-dimensional
</p>
<p>distributions. To this class belong:
</p>
<p>1. Processes with independent increments.
</p>
<p>2. Markov processes.
</p>
<p>3. Gaussian processes.
</p>
<p>In Chap. 22 we will also consider some problems of the theory of processes with
</p>
<p>finite second moments. Chapter 20 contains limit theorems for random processes
</p>
<p>generated by partial sums of independent random variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 19
</p>
<p>Processes with Independent Increments
</p>
<p>Abstract Section 19.1 introduces the fundamental concept of infinitely divisible
</p>
<p>distributions and contains the key theorem on relationship of such processes to
</p>
<p>processes with independent homogeneous increments. Section 19.2 begins with a
</p>
<p>definition of the Wiener process based on its finite-dimensional distributions and
</p>
<p>establishes existence of a continuous modification of the process. It also derives the
</p>
<p>distribution of the maximum of the Wiener process on a finite interval. The Laws
</p>
<p>of the Iterated Logarithm for the Wiener process are established in Sect. 19.3. Sec-
</p>
<p>tion 19.4 is devoted to the Poisson processes, while Sect. 19.5 presents a character-
</p>
<p>isation of the class of processes with independent increments (the L&eacute;vy&ndash;Khintchin
</p>
<p>theorem).
</p>
<p>19.1 General Properties
</p>
<p>Definition 19.1.1 A process {ξ(t), t &isin; [a, b]} given on the interval [a, b] is said
to be a process with independent increments if, for any n and t0 &lt; t1 &lt; &middot; &middot; &middot; &lt; tn,
a &le; t0, tn &le; b, the random variables ξ(t0), ξ(t1) &minus; ξ(t0), . . . , ξ(tn) &minus; ξ(tn&minus;1) are
independent.
</p>
<p>A process with independent increments is called homogeneous if the distribu-
tion of ξ(t1)&minus; ξ(t0) is determined by the length of the interval t1 &minus; t0 only and is
independent of t0.
</p>
<p>In what follows, we will everywhere assume for simplicity&rsquo;s sake that a = 0,
ξ(0)= 0 and b= 1 or b=&infin;.
</p>
<p>Definition 19.1.2 The distribution of a random variable ξ is called infinitely di-
visible (cf. Sect. 8.8) if, for any n, the variable ξ can be represented as a sum of
independent identically distributed random variables: ξ = ξ1,n + &middot; &middot; &middot; + ξn,n. If ϕ(λ)
is the ch.f. of ξ , then this is equivalent to the property that ϕ1/n is a ch.f. for any n.
</p>
<p>It is clear from the above definitions that, for a homogeneous process with
</p>
<p>independent increments, the distribution of ξ(t) is infinitely divisible, because
</p>
<p>ξ = ξ1,n + &middot; &middot; &middot; + ξn,n, where ξk,n = ξ(kt/n) &minus; ξ((k &minus; 1)t/n) are independent and
distributed as ξ(t/n).
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_19, &copy; Springer-Verlag London 2013
</p>
<p>539</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_19">http://dx.doi.org/10.1007/978-1-4471-5201-9_19</a></div>
</div>
<div class="page"><p/>
<p>540 19 Processes with Independent Increments
</p>
<p>Theorem 19.1.1
</p>
<p>(1) Let {ξ(t), t &ge; 0} be a stochastically continuous homogeneous process with in-
dependent increments, and let ϕt (λ) = Eeiλξ(t) be the ch.f. of ξ(t), ϕ(λ) :=
ϕ1(λ). Then
</p>
<p>ϕt (λ)= ϕt (λ), (19.1.1)
ϕ(λ) 
= 0 for any λ.
</p>
<p>(2) Let ϕ(λ) be the ch.f. of an infinitely divisible distribution. Then there exists a
random process {ξ(t), t &ge; 0} satisfying the conditions of (1) and such that
</p>
<p>Eeiλξ(1) = ϕ(λ).
</p>
<p>Note that in the theorem the power ϕt (λ) of the complex number ϕ(λ) is under-
</p>
<p>stood as |ϕ(λ)|teiα(λ)t , where α(λ)= argϕ(λ) (ϕ(λ)= |ϕ(λ)|eiα(λ)). But α(λ) is a
multi-valued function, which is defined up to the term 2πk with integer k. There-
</p>
<p>fore, for non-integer t , the function ϕt (λ) will be multi-valued as well. Since any
</p>
<p>ch.f. is continuous, after crossing the level 2πk by α(k) (while changing the value of
</p>
<p>λ from zero, α(0)= 0), we are to take the &ldquo;nearest&rdquo; branch of α(k) so as to ensure
continuity of the function ϕt (λ). For example, for the degenerate distribution I1 we
</p>
<p>have ϕ(λ)= eiλ (α(λ)= λ), so for small t &gt; 0, ε &gt; 0 and for λ= 2π + ε we are to
set ϕt (λ)= ei(2π+ε)t rather than ϕt (λ)= eiεt (although ϕ(λ)= eiε).
</p>
<p>Denote by L the class of ch.f.s of all infinitely divisible distributions and by
</p>
<p>L1 the class of the ch.f.s of the distributions of ξ(t) for stochastically continuous
</p>
<p>homogeneous processes with independent increments. Then it follows from Theo-
</p>
<p>rem 19.1.1 that L=L1. The class L will be characterised in Sect. 19.5.
</p>
<p>Proof (1) Let ξ(t) satisfy the conditions of part (1) of the theorem. Then ξ(t) can
be represented as a sum of independent increments
</p>
<p>ξ(t)=
n&sum;
</p>
<p>j=1
</p>
<p>[
ξ(tj )&minus; ξ(tj&minus;1)
</p>
<p>]
, t0 = 0, tn = t, tj &gt; tj&minus;1.
</p>
<p>From this it follows, in particular, that for tj = j/n, t = 1,
</p>
<p>ϕ(λ)=
[
ϕ1/n(λ)
</p>
<p>]n
, ϕ1/n(λ)= ϕ1/n(λ).
</p>
<p>Raising both sides of the last equality to an integer power k, we obtain that, for any
</p>
<p>rational r = k/n, one has
ϕk/n(λ)= ϕk/n(λ),
</p>
<p>which proves (19.1.1) for t = r . Now let t be irrational and rn := &lfloor;tn&rfloor;/n. Since ξ(t)
is a stochastically continuous process, one has ξ(rn)
</p>
<p>p&rarr; ξ(t) as n&rarr;&infin;, and hence
the corresponding ch.f.s converge: for any λ,
</p>
<p>ϕrn(λ)&rarr; ϕt (λ).
But ϕrn(λ)= ϕrn(λ)&rarr; ϕt (λ). Therefore (19.1.1) necessarily holds true.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.1 General Properties 541
</p>
<p>Further, by stochastic continuity of ξ(&middot;), we have ϕt (λ) = ϕt (λ)&rarr; 1 as t &rarr; 0
for any λ. This implies that ϕ(λ) 
= 0 for any λ. This completes the proof of the first
assertion of the theorem.
</p>
<p>(2) Observe first that if ϕ &isin;L then, for any t &gt; 0, ϕt is again a ch.f. Indeed,
ϕt (λ)= lim
</p>
<p>n&rarr;&infin;
ϕ&lfloor;tn&rfloor;/n(λ),
</p>
<p>so that ϕt (λ) is a limit of ch.f.s which is continuous at the point λ = 0. By the
continuity theorem for ch.f.s, this is again a ch.f.
</p>
<p>Now we will construct a random process ξ(t) with independent increments by
</p>
<p>specifying its finite-dimensional distributions. Put
</p>
<p>0 = t0 &lt; t1 &lt; &middot; &middot; &middot;&lt; tk, ∆j := ξ(tj )&minus; ξ(tj&minus;1), δj := tj &minus; tj&minus;1,
and observe that
</p>
<p>k&sum;
</p>
<p>j=1
λj ξ(tj )=
</p>
<p>k&sum;
</p>
<p>j=1
λj
</p>
<p>j&sum;
</p>
<p>l=1
∆l =
</p>
<p>k&sum;
</p>
<p>j=1
∆j
</p>
<p>k&sum;
</p>
<p>l=j
λl .
</p>
<p>Define the ch.f. of the joint distribution of ξ(t1), . . . , ξ(tk) by the equality (postulat-
</p>
<p>ing independence of ∆j )
</p>
<p>E exp
</p>
<p>{
i
</p>
<p>k&sum;
</p>
<p>1
</p>
<p>λj ξ(tj )
</p>
<p>}
:= E exp
</p>
<p>{
i
</p>
<p>k&sum;
</p>
<p>j=1
∆j
</p>
<p>j&sum;
</p>
<p>l=j
λl
</p>
<p>}
=
</p>
<p>k&prod;
</p>
<p>j=1
ϕ
</p>
<p>(
k&sum;
</p>
<p>l=j
λl
</p>
<p>)δj
.
</p>
<p>Thus, we have used ϕ to define the finite-dimensional distributions of ξ(t) in
</p>
<p>〈RT ,BT
R
〉 with T = [0,&infin;) which, as one can easily see, are consistent. By
</p>
<p>Kolmogorov&rsquo;s theorem, there exists a distribution of a random process ξ(t) in
</p>
<p>〈RT ,BT
R
〉. That process is by definition a homogeneous processes with indepen-
</p>
<p>dent increments.
</p>
<p>To prove stochastic continuity of ξ(t), note that, as h&rarr; 0,
Eeiλ(ξ(t+h)&minus;ξ(t)) = ϕh(λ)&rarr; ϕ0(λ),
</p>
<p>where
</p>
<p>ϕ0(λ)=
{
</p>
<p>1 if ϕ(λ) 
= 0,
0 if ϕ(λ)= 0.
</p>
<p>Thus the limiting function ϕ0(λ) can assume only two values: 0 and 1. But it is
</p>
<p>bound to be a ch.f. since it is continuous at the point λ = 0 (ϕ(λ) 
= 0 in a neigh-
bourhood of the point λ= 0) and is a limit of ch.f.s. Therefore ϕ0(λ) is continuous,
ϕ0(λ)&equiv; 1, ϕh(λ)&rarr; 1, and
</p>
<p>ξ(t + h)&minus; ξ(t) p&rarr; 0 as h&rarr; 0.
The theorem is proved. �
</p>
<p>Corollary 19.1.1 Let the conditions of part (1) of Theorem 19.1.1 be met. If, for
all t , E|ξ(t)|&lt;&infin; then
</p>
<p>Eξ(t)= tEξ(1).</p>
<p/>
</div>
<div class="page"><p/>
<p>542 19 Processes with Independent Increments
</p>
<p>If E(ξ(1))2 &lt;&infin; then
</p>
<p>Var ξ(t)= t Var ξ(1).
</p>
<p>Proof For the sake of brevity, put a := Eξ(1). Then, differentiating (19.1.1) in λ at
the point λ= 0, we obtain
</p>
<p>Eξ(t)=&minus;iϕ&prime;t (0)=&minus;itϕt&minus;1ϕ&prime;(0)= at,
Eξ2(t)=&minus;ϕ&prime;&prime;t (0)=&minus;t (t &minus; 1)ϕt&minus;2(0)
</p>
<p>(
ϕ&prime;(0)
</p>
<p>)2 &minus; tϕt&minus;1(0)ϕ&prime;&prime;(0)
= t (t &minus; 1)a2 + tEξ2(1),
</p>
<p>Var ξ(t)= t
(
Eξ2(1)&minus; a2
</p>
<p>)
= t Var ξ(1).
</p>
<p>The corollary is proved. �
</p>
<p>In the next theorem we put, as before, T = [0,1] or T = [0,&infin;).
</p>
<p>Theorem 19.1.2 Homogeneous stochastically continuous processes with indepen-
dent increments {ξ(t), t &isin; T } have modifications in the space D(T ), i.e. the process
ξ(t) can be given in 〈D(T ),BTD〉 and hence have no discontinuities of the second
type.
</p>
<p>Proof To simplify the argument, assume that Eξ2(1) exists, or, which is the same,
that the second derivative ϕ&prime;&prime;(λ) exists. Then
</p>
<p>E
(
ξ(t)&minus; ξ(t &minus; h)
</p>
<p>)2 = ϕ&prime;&prime;h(0)=&minus;h(h&minus; 1)
[
ϕ&prime;(0)
</p>
<p>]2 &minus; hϕ&prime;&prime;(0)&le; c|h|,
E
(∣∣ξ(t + h2)&minus; ξ(t)
</p>
<p>∣∣2∣∣ξ(t)&minus; ξ(t &minus; h1)
∣∣2)&le; c2h1h2 &le; c2(h1 + h2),
</p>
<p>and the assertion follows from the second criterion of Theorem 18.2.3. The theorem
</p>
<p>is proved. �
</p>
<p>In the general case, the proof is more complicated: one has to make use of crite-
</p>
<p>rion (18.2.4) and bounds for P(ξ(t)&minus; ξ(t &minus; h)|&gt; ε).
Now we will consider the two most important processes with independent incre-
</p>
<p>ments: the so-called Wiener and Poisson processes.
</p>
<p>19.2 Wiener Processes. The Properties of Trajectories
</p>
<p>Definition 19.2.1 The Wiener process is a homogeneous process with independent
increments for which the distribution of ξ(1) is normal.
</p>
<p>In other words, this is a process for which
</p>
<p>ϕ(λ)= eiλα&minus;σ 2λ2/2, ϕt (λ)= ϕt (λ)= eiλtα&minus;σ
2λ2t/2</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Wiener Processes. The Properties of Trajectories 543
</p>
<p>for some α and σ 2 &ge; 0. The second equality means that the increments ξ(t + u)&minus;
ξ(u) are normally distributed with parameters (αt, σ 2t). All joint distributions of
</p>
<p>ξ(t1), . . . , ξ(tn) are clearly also normal.
</p>
<p>The numbers α and σ are called the shift and diffusion coefficients, respectively.
Introducing the process ξ0(t) := (ξ(t)&minus; αt)/σ which is obtained from ξ(t) by an
affine transformation, we obtain that its ch.f. equals
</p>
<p>Eeiλξ0(t) = e&minus;iλαt/σϕt (λ/σ )= e&minus;λ
2t/2.
</p>
<p>Such a process with parameters (0, t) is often called the standard Wiener process.
We consider it in more detail.
</p>
<p>Theorem 19.2.1 The Wiener process has a continuous modification.
</p>
<p>This means, as we know, that the Wiener process {ξ(t), t &isin; [0,1]} can be consid-
ered as given on the measurable space 〈C(0,1),B[0,1]C 〉 of continuous functions.
</p>
<p>Proof We have ξ(t+h)&minus;ξ(t)&sub;=�0,h and h&minus;1/2(ξ(t+h)&minus;ξ(t))&sub;=�0,1. Therefore
</p>
<p>E
(
ξ(t + h)&minus; ξ(t)
</p>
<p>)4 = h2Eξ(1)4 = 3h2.
This means that the conditions of Theorem 18.2.1 are satisfied. �
</p>
<p>Thus we can assume that ξ(&middot;) &isin; C(0,1). The standard Wiener process with con-
tinuous trajectories will be denoted by {w(t), t &isin; T }.
</p>
<p>Now note that the trajectories of the Wiener process w(t), being continuous, are
not differentiable with probability 1 at any given point t .
</p>
<p>By virtue of the homogeneity of the process, it suffices to prove its nondifferen-
</p>
<p>tiability at the point 0. If, with a positive probability, i.e. on an event set A&sub;Ω with
P(A) &gt; 0, there existed the derivative
</p>
<p>w&prime;(0)=w&prime;(0,ω)= lim
t&rarr;0
</p>
<p>w(t)
</p>
<p>t
,
</p>
<p>then, on the same event, there would exist the limit
</p>
<p>lim
k&rarr;&infin;
</p>
<p>w(2&minus;k+1)&minus;w(2&minus;k)
2&minus;k
</p>
<p>= lim
k&rarr;&infin;
</p>
<p>2w(2&minus;k+1)
</p>
<p>2&minus;k+1
&minus; lim
</p>
<p>k&rarr;&infin;
w(2&minus;k)
</p>
<p>2&minus;k
</p>
<p>= 2w&prime;(0)&minus;w&prime;(0)=w&prime;(0).
But this is impossible for the following reason. The independent differences
</p>
<p>w(2&minus;k+1) &minus; w(2&minus;k) have the same distribution as w(2&minus;k), and with the positive
probability p = 1 &minus; Φ(1) they exceed the value
</p>
<p>&radic;
2&minus;k . That is, the independent
</p>
<p>events Bk = {w(2&minus;k+1) &minus; w(2&minus;k) &gt;
&radic;
</p>
<p>2&minus;k} have the property
&sum;&infin;
</p>
<p>k=1 P(Bk) =&infin;.
By the Borel&ndash;Cantelli criterion, this means that with probability 1 there occur in-
</p>
<p>finitely many events Bk , so that
</p>
<p>P
</p>
<p>(
lim sup
k&rarr;&infin;
</p>
<p>w(2&minus;k+1)&minus;w(2&minus;k)&radic;
2&minus;k
</p>
<p>&gt; 1
</p>
<p>)
= 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>544 19 Processes with Independent Increments
</p>
<p>In the same way we find that
</p>
<p>P
</p>
<p>(
lim inf
k&rarr;&infin;
</p>
<p>w(2&minus;k+1)&minus;w(2&minus;k)
2&minus;k
</p>
<p>&lt;&minus;1
)
= 1.
</p>
<p>This implies that, with probability 1,
</p>
<p>lim sup
k&rarr;&infin;
</p>
<p>w(2&minus;k+1)&minus;w(2&minus;k)
2&minus;k
</p>
<p>=&infin;, lim inf
k&rarr;&infin;
</p>
<p>w(2&minus;k+1)&minus;w(2&minus;k)
2&minus;k
</p>
<p>=&minus;&infin;,
</p>
<p>and therefore the process w(t) is nondifferentiable at any given point t with proba-
</p>
<p>bility 1.
</p>
<p>A stronger assertion also takes place: with probability 1 there exists no point t
at which the trajectory of the process w(t) would have a derivative. In other words,
the Wiener process is nowhere differentiable with probability 1. The proof of this
fact is much more complicated and lies beyond the scope of the book.
</p>
<p>The reader can easily verify that w(t) has, in a certain sense, a parabola property.
</p>
<p>Namely, for any c &gt; 0, the process w&lowast;(t)= c&minus;1/2w(ct) is again a Wiener process.
The properties of continuity of trajectories and independence of increments for
</p>
<p>the Wiener process allow us to find, in an explicit form, the distributions of
</p>
<p>w(t)= max
u&isin;[0,t]
</p>
<p>w(u)
</p>
<p>and of the time of the first passage of a given level which is defined, for a given
</p>
<p>x &gt; 0, by
</p>
<p>η(x) := inf
{
t :w(t)&ge; x
</p>
<p>}
= inf
</p>
<p>{
t :w(t)= x
</p>
<p>}
.
</p>
<p>Theorem 19.2.2
</p>
<p>P
(
w(t) &gt; x
</p>
<p>)
= 2P
</p>
<p>(
w(t) &gt; x
</p>
<p>)
= 2
</p>
<p>(
1 &minus;Φ
</p>
<p>(
x&radic;
t
</p>
<p>))
. (19.2.1)
</p>
<p>The distribution of η(1) is stable and has the density
</p>
<p>1&radic;
2π t3/2
</p>
<p>e&minus;
1
2t , t &gt; 0. (19.2.2)
</p>
<p>Distribution (19.2.1) is sometimes called the double normal tail law, while the
distribution with density (19.2.2) is called the L&eacute;vy distribution (see Sect. 8.8).
</p>
<p>Proof Since
</p>
<p>{
η(x)= v
</p>
<p>}
=
</p>
<p>&infin;⋂
</p>
<p>n=1
</p>
<p>{
w(v &minus; 1/n) &lt; x,w(v)= x
</p>
<p>}
&isin; Fv := σ
</p>
<p>{
w(u); u&le; v
</p>
<p>}
</p>
<p>and w(t)&minus;w(v) d=w(t &minus; v) for t &gt; v does not depend on Fv , we have
</p>
<p>P
(
w(t) &gt; x
</p>
<p>)
=
&int; t
</p>
<p>0
</p>
<p>P
(
η(x) &isin; dv
</p>
<p>)
P
(
w(t &minus; v) &gt; 0
</p>
<p>)
</p>
<p>= 1
2
</p>
<p>&int; t
</p>
<p>0
</p>
<p>P
(
η(x) &isin; dv
</p>
<p>)
= 1
</p>
<p>2
P
(
w(t)&ge; x
</p>
<p>)
.
</p>
<p>This implies the first assertion of the theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3 The Laws of the Iterated Logarithm 545
</p>
<p>The same equalities imply that
</p>
<p>P
(
η(x) &lt; v
</p>
<p>)
= P
</p>
<p>(
w(v) &gt; x
</p>
<p>)
= 2
</p>
<p>(
1 &minus;Φ
</p>
<p>(
x&radic;
v
</p>
<p>))
= 2&radic;
</p>
<p>2π
</p>
<p>&int; &infin;
</p>
<p>x/
&radic;
v
</p>
<p>e&minus;s
2/2 ds,
</p>
<p>which yields, for the density fη of the variable η := η(1),
</p>
<p>fη(v)=
e&minus;
</p>
<p>1
2v
</p>
<p>&radic;
2π v3/2
</p>
<p>.
</p>
<p>In order to prove that this distribution is stable, note that
</p>
<p>η(n)= η1 + &middot; &middot; &middot; + ηn,
</p>
<p>where ηi are distributed as η and are independent (since the path of w(t) first attains
</p>
<p>level 1; then level 2, starting at a point with ordinate 1; then level 3, and so on).
</p>
<p>Using the same argument as above, we obtain that
</p>
<p>P
(
η(n) &lt; v
</p>
<p>)
= P
</p>
<p>(
w(v) &gt; n
</p>
<p>)
= P
</p>
<p>(
w
(
vn&minus;2
</p>
<p>)
&gt; 1
</p>
<p>)
= P
</p>
<p>(
η &lt; vn&minus;2
</p>
<p>)
,
</p>
<p>so the distributions of η and η(n) coincide up to a scale transformation. This implies
</p>
<p>the stability of the distribution of η (see Sect. 8.8). Since η &ge; 0 and P(η &gt; x)&sim;
&radic;
</p>
<p>2
πx
</p>
<p>as x &rarr;&infin;, we obtain that it is, up to a scale transformation, the distribution F1/2,1
with parameters β = 1/2, ρ = 1 (cf. Sect. 8.8). The theorem is proved. �
</p>
<p>19.3 The Laws of the Iterated Logarithm
</p>
<p>Using an argument similar to that employed at the end of the previous section, one
</p>
<p>can establish a much stronger assertion: the trajectory of w(t) in the neighbourhood
</p>
<p>of the point t = 0, graphically speaking, &ldquo;completely shades&rdquo; the interior of the
domain bounded by the two curves
</p>
<p>y =&plusmn;
&radic;
</p>
<p>2t ln ln
1
</p>
<p>t
.
</p>
<p>The exterior of this domain remains untouched. This is the so-called law of the
iterated logarithm.
</p>
<p>Theorem 19.3.1
</p>
<p>P
</p>
<p>(
lim sup
t&rarr;0
</p>
<p>w(t)&radic;
2t ln ln 1
</p>
<p>t
</p>
<p>= 1
)
= 1,
</p>
<p>P
</p>
<p>(
lim inf
t&rarr;0
</p>
<p>w(t)&radic;
2t ln ln 1
</p>
<p>t
</p>
<p>=&minus;1
)
= 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>546 19 Processes with Independent Increments
</p>
<p>Thus, if we consider the sequence of random variables w(tn), tn &darr; 0, then, for
any ε &gt; 0,
</p>
<p>(1 &plusmn; ε)
&radic;
</p>
<p>2tn ln ln
1
</p>
<p>tn
</p>
<p>will be upper and lower sequences, respectively, for that sequence.
</p>
<p>For processes, we could introduce in a natural way the notions of upper and
lower functions. If, for example, a process ξ(t) belongs to C(0,&infin;) or D(0,&infin;) (or
is separable on (0,&infin;)), then the respective definition for the case t &rarr;&infin; has the
following form.
</p>
<p>Definition 19.3.1 A function a(t) is said to be upper (lower) for the process ξ(t)
if, for some sequence tn &uarr; &infin;, the events An = {supu&ge;tn(ξ(t) &minus; a(t)) &gt; 0} occur
finitely (infinitely) often with probability 1.
</p>
<p>Along with Theorem 19.3.1, we will obtain here the conventional law of the
</p>
<p>iterated logarithm. The proofs of the both assertions are essentially identical. We
</p>
<p>will prove the latter and derive the former as a consequence.
</p>
<p>Theorem 19.3.2 (The Law of the Iterated Logarithm)
</p>
<p>P
</p>
<p>(
lim sup
t&rarr;&infin;
</p>
<p>w(t)&radic;
2t ln ln t
</p>
<p>= 1
)
= 1,
</p>
<p>P
</p>
<p>(
lim inf
t&rarr;&infin;
</p>
<p>w(t)&radic;
2t ln ln t
</p>
<p>=&minus;1
)
= 1.
</p>
<p>Thus, for any ε &gt; 0, the functions (1 &plusmn; ε)
&radic;
</p>
<p>2t ln ln t are, respectively, upper and
</p>
<p>lower for w(t) as t &rarr;&infin;.
</p>
<p>Proof of Theorem 19.3.2 First observe that, by L&rsquo;Hospital&rsquo;s rule,
</p>
<p>P
(
w(t) &gt; x
</p>
<p>)
= 1&radic;
</p>
<p>2πt
</p>
<p>&int; &infin;
</p>
<p>x
</p>
<p>e&minus;u
2/2t du
</p>
<p>= 1&radic;
2πt
</p>
<p>&int; &infin;
</p>
<p>x/
&radic;
t
</p>
<p>e&minus;u
2/2t du&sim;
</p>
<p>&radic;
t&radic;
</p>
<p>2πx
e&minus;x
</p>
<p>2/2t (19.3.1)
</p>
<p>as x/
&radic;
t &rarr;&infin;.
</p>
<p>Let a &gt; 1 and xk :=
&radic;
</p>
<p>2ak ln lnak . We have to show that, for any ε &gt; 0,
</p>
<p>P
</p>
<p>(
lim sup
t&rarr;&infin;
</p>
<p>w(t)&radic;
2t ln ln t
</p>
<p>&lt; 1 + ε
)
= 1, (19.3.2)
</p>
<p>i.e. that, with probability 1, for all sufficiently large t ,
</p>
<p>w(t) &lt; (1 + ε)
&radic;
</p>
<p>2t ln ln t .</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3 The Laws of the Iterated Logarithm 547
</p>
<p>Fig. 19.1 Illustration to the
</p>
<p>proof of Theorem 19.3.2:
</p>
<p>replacing the curvilinear
</p>
<p>boundary with a step function
</p>
<p>To this end it suffices to establish that, with probability 1, there occur only finitely
</p>
<p>many events
</p>
<p>Bk :=
{
</p>
<p>sup
ak&minus;1&lt;u&le;ak
</p>
<p>w(u) &gt; (1 + ε)xk&minus;1
}
.
</p>
<p>Consider the events
</p>
<p>Ak =
{
</p>
<p>sup
u&le;ak
</p>
<p>w(u) &gt; (1 + ε)xk&minus;1
}
&sup; Bk
</p>
<p>(see Fig. 19.1). Because xk/
&radic;
ak &rarr;&infin; as k&rarr;&infin;, by Theorem 19.2.2 one has
</p>
<p>P(Ak)= 2P
(
w
(
ak
)
&gt; (1 + ε)xk&minus;1
</p>
<p>)
</p>
<p>&sim;
&radic;
</p>
<p>2
</p>
<p>π
</p>
<p>&radic;
ak
</p>
<p>(1 + ε)xk&minus;1
exp
</p>
<p>{
&minus;2(1 + ε)a
</p>
<p>k&minus;1 ln lnak&minus;1
</p>
<p>2ak
</p>
<p>}
</p>
<p>= 1
(1 + ε)
</p>
<p>&radic;
1
</p>
<p>πa ln lnak&minus;1
1
</p>
<p>(lnak&minus;1)(1+ε)2/a
</p>
<p>= c(a, ε) 1&radic;
(ln(k &minus; 1)+ ln lna)(k &minus; 1)(1+ε)2/a
</p>
<p>.
</p>
<p>Put a := 1 + ε &gt; 1. Then clearly
</p>
<p>P(Ak)&sim;
c(ε)
</p>
<p>k1+ε
&radic;
</p>
<p>lnk
</p>
<p>as k&rarr;&infin;.
In the above formulas, c(a, ε) and c(ε) are some constants depending on the in-
</p>
<p>dicated parameters. The obtained relation implies that
&sum;&infin;
</p>
<p>k=1 P(Ak) &lt;&infin; and hence&sum;&infin;
k=1 P(Bk) &lt; &infin; (for Bk &sub; Ak), so that by the Borel&ndash;Cantelli criterion (Theo-
</p>
<p>rem 11.1.1) with probability 1 the events Bk occur only finitely often.
</p>
<p>We now prove that, for an arbitrary ε &gt; 0,
</p>
<p>P
</p>
<p>(
lim sup
t&rarr;&infin;
</p>
<p>w(t)&radic;
2t ln ln t
</p>
<p>&gt; 1 &minus; ε
)
= 1. (19.3.3)
</p>
<p>It is evident that, together with (19.3.2), this will mean that the first assertion of the
</p>
<p>theorem is true.</p>
<p/>
</div>
<div class="page"><p/>
<p>548 19 Processes with Independent Increments
</p>
<p>Consider for a &gt; 1 independent increments w(ak)&minus;w(ak&minus;1) and denote by Bk
the event
</p>
<p>Bk :=
{
w
(
ak
)
&minus;w
</p>
<p>(
ak&minus;1
</p>
<p>)
&gt; (1 &minus; ε/2)rxk
</p>
<p>}
.
</p>
<p>Since w(ak)&minus;w(ak&minus;1) is distributed as w(ak(1 &minus; a&minus;1)), by virtue of (19.3.1) we
find, as before, that
</p>
<p>P(Bk)&sim;
&radic;
ak(1 &minus; a&minus;1)&radic;
</p>
<p>2π(1 &minus; ε/2)xk
exp
</p>
<p>{
&minus; (1 &minus; ε/2)
</p>
<p>22ak ln lna&minus;k
</p>
<p>2ak(1 &minus; a&minus;1)
</p>
<p>}
</p>
<p>&sim; c1(a, ε)&radic;
ln k
</p>
<p>k&minus;(1&minus;ε/2)
2/(1&minus;a&minus;1).
</p>
<p>This implies that, for a &ge; 2/ε, the series
&sum;&infin;
</p>
<p>k=1 P(Bk) diverges, and hence by the
Borel&ndash;Cantelli criterion the events Bk occur infinitely often, with probability 1.
</p>
<p>Further, by the symmetry of the process w(t), it follows from relation (19.3.2)
</p>
<p>that, for all k large enough and any δ &gt; 0,
</p>
<p>w
(
ak
)
&gt;&minus;(1 + δ)xk.
</p>
<p>Together with the preceding argument this shows that the event
</p>
<p>w
(
ak&minus;1
</p>
<p>)
+
[
w
(
ak
)
&minus;w
</p>
<p>(
ak&minus;1
</p>
<p>)]
=w
</p>
<p>(
ak
)
&gt;&minus;(1 + δ)xk&minus;1 + (1 &minus; ε/2)xk
</p>
<p>will occur infinitely often. But the right hand-side of the above inequality can be
</p>
<p>made greater than (1 &minus; ε)xk by choosing an appropriate a. Indeed,
</p>
<p>&minus;(1 + δ)xk&minus;1 +
ε
</p>
<p>2
xk &gt; 0
</p>
<p>once
</p>
<p>(1 + δ)
</p>
<p>&radic;
ln lnak&minus;1
</p>
<p>a ln lnak
&lt;
</p>
<p>ε
</p>
<p>2
,
</p>
<p>which, in turn, can easily be achieved by taking a large enough. Thus relation
</p>
<p>(19.3.3) is proved.
</p>
<p>The second assertion of the theorem clearly follows from the first by virtue of the
</p>
<p>symmetry of the distribution of w(t). �
</p>
<p>Now we can obtain as a consequence the local law of the iterated logarithm for
</p>
<p>the case where t &rarr; 0.
</p>
<p>Proof of Theorem 19.3.1 Consider the process {W(u) := uw(1/u), u &ge; 0}, where
we put W(0) := 0. The remarkable fact is that the process {W(u), u&ge; 0} is also the
standard Wiener process. Indeed, for t &gt; u,
</p>
<p>E exp
{
iλ
(
W(t)&minus;W(u)
</p>
<p>)}
= E exp
</p>
<p>{
iλ
</p>
<p>[
tw
</p>
<p>(
1
</p>
<p>t
</p>
<p>)
&minus; uw
</p>
<p>(
1
</p>
<p>u
</p>
<p>)]}
</p>
<p>= E exp
(
iλ
</p>
<p>[
w
</p>
<p>(
1
</p>
<p>t
</p>
<p>)
(t &minus; u)&minus; u
</p>
<p>(
w
</p>
<p>(
1
</p>
<p>u
</p>
<p>)
&minus;w
</p>
<p>(
1
</p>
<p>t
</p>
<p>))])</p>
<p/>
</div>
<div class="page"><p/>
<p>19.4 The Poisson Process 549
</p>
<p>= exp
{
&minus;λ
</p>
<p>2
</p>
<p>2
(t &minus; u)2 1
</p>
<p>t
&minus; λ
</p>
<p>2u2
</p>
<p>2
</p>
<p>(
1
</p>
<p>u
&minus; 1
</p>
<p>t
</p>
<p>)}
</p>
<p>= exp
{
&minus;λ
</p>
<p>2
</p>
<p>2
(t &minus; u)
</p>
<p>}
.
</p>
<p>The independence of increments is easiest to prove by establishing their noncor-
</p>
<p>relatedness. Indeed,
</p>
<p>E
[
W(u)
</p>
<p>(
W(t)&minus;W(u)
</p>
<p>)]
= E
</p>
<p>[
uw
</p>
<p>(
1
</p>
<p>u
</p>
<p>)(
tw
</p>
<p>(
1
</p>
<p>t
</p>
<p>)
&minus; uw
</p>
<p>(
1
</p>
<p>u
</p>
<p>))]
</p>
<p>= E
[
uw
</p>
<p>(
1
</p>
<p>t
</p>
<p>)
tw
</p>
<p>(
1
</p>
<p>t
</p>
<p>)
&minus; u2w2
</p>
<p>(
1
</p>
<p>u
</p>
<p>)]
= u&minus; u= 0.
</p>
<p>To complete the proof of the theorem, it remains to observe that
</p>
<p>lim sup
t&rarr;&infin;
</p>
<p>w(t)&radic;
2t ln ln t
</p>
<p>= lim sup
u&rarr;0
</p>
<p>uw(1/u)&radic;
2u ln ln 1
</p>
<p>u
</p>
<p>= lim sup
u&rarr;0
</p>
<p>W(u)&radic;
2u ln ln 1
</p>
<p>u
</p>
<p>.
</p>
<p>The theorem is proved. �
</p>
<p>We could also prove the theorem by repeating the argument from the proof of
</p>
<p>Theorem 19.3.2 with a &lt; 1.
</p>
<p>In conclusion we note that Wiener processes play an important role in many
</p>
<p>theoretical probabilistic considerations and serve as models for describing various
</p>
<p>real-life processes. For example, they provide a good model for the movement of
</p>
<p>a diffusing particle. In this connection, the Wiener processes are also often called
</p>
<p>Brownian motion processes.
Wiener processes prove to be, in a certain sense, the limiting processes for ran-
</p>
<p>dom polygons constructed on the vertices (k/n,Sk/
&radic;
n), where Sk are sums of ran-
</p>
<p>dom variables ξj with Eξj = 0 and Var(ξj )= 1. We will discuss this in more detail
in Chap. 20. The concept of the stochastic integral and many other constructions
</p>
<p>and results are also closely related to the Wiener process.
</p>
<p>19.4 The Poisson Process
</p>
<p>Definition 19.4.1 A homogeneous process ξ(t) with independent increments is said
</p>
<p>to be the Poisson process if ξ(t)&minus; ξ(0) has the Poisson distribution.
</p>
<p>For simplicity&rsquo;s sake put ξ(0)= 0. If ξ(1)&sub;=Π&micro;, then
</p>
<p>ϕ(λ) := Eeiλξ(1) = exp
{
&micro;
(
eiλ &minus; 1
</p>
<p>)}
</p>
<p>and, as we know,
</p>
<p>ϕt (λ)= Eeiλξ(t) = ϕt (λ)= exp
{
&micro;t
</p>
<p>(
eiλ &minus; 1
</p>
<p>)}
,</p>
<p/>
</div>
<div class="page"><p/>
<p>550 19 Processes with Independent Increments
</p>
<p>so that ξ(t)&sub;=Π&micro;t . We consider the properties of the Poisson process. First of all,
for each t , ξ(t) takes only integer values 0,1,2, . . . . Divide the interval [0, t) into
segments [0, t1), [t1, t2), . . . , [tn&minus;1, tn) of lengths ∆i = ti &minus; ti&minus;1, i = 1, . . . , n. For
small ∆i the distributions of the increments ξ(ti)&minus; ξ(ti&minus;1) will have the property
that
</p>
<p>P
(
ξ(t)&minus; ξ(ti&minus;1)= 0
</p>
<p>)
= P
</p>
<p>(
ξ(∆i)= 0
</p>
<p>)
= e&minus;mu∆i = 1 &minus;&micro;∆i +O
</p>
<p>(
∆2i
</p>
<p>)
,
</p>
<p>P
(
ξ(t)&minus; ξ(ti&minus;1)= 1
</p>
<p>)
= &micro;∆ie&minus;&micro;∆i = &micro;∆i +O
</p>
<p>(
∆2i
</p>
<p>)
,
</p>
<p>P
(
ξ(t)&minus; ξ(ti&minus;1)&ge; 2
</p>
<p>)
=O
</p>
<p>(
∆2i
</p>
<p>)
.
</p>
<p>(19.4.1)
</p>
<p>Consider &ldquo;embedded&rdquo; rational partitions R(n)= {t1, . . . , tn} of the interval [0, t]
such that R(n)&sub;R(n+ 1) and
</p>
<p>⋃
R(n)=R1 is the set of all rationals in [0, t].
</p>
<p>Note the following three properties.
</p>
<p>(1) Let ν(n) be the number of intervals in the partition R(n) on which the incre-
</p>
<p>ments of the process ξ are non-zero. For each ω, ν(n) is non-decreasing as n&rarr;&infin;.
Furthermore, the number ν(n) can be represented as a sum of independent random
</p>
<p>variables which are equal to 1 if there is an increment on the i-th interval and 0
</p>
<p>otherwise. Therefore, by (19.4.1)
</p>
<p>P
(
ν(n) 
= ξ(t)
</p>
<p>)
= P
</p>
<p>( ⋃
</p>
<p>ti&isin;R(n)
</p>
<p>{
ξ(ti)&minus; ξ(ti&minus;1)&ge; 2
</p>
<p>}
&cup;
{
ξ(t)&minus; ξ(tn)&ge; 1
</p>
<p>})
</p>
<p>=O
(
</p>
<p>n&sum;
</p>
<p>j=1
∆2j + (t &minus; tn)
</p>
<p>)
,
</p>
<p>where
&sum;n
</p>
<p>j=1 ∆
2
j &le; t max∆j &rarr; 0 as n&rarr;&infin;, so that a.s.
</p>
<p>ν(n) &uarr; ξ(t)&sub;=Π&micro;t
as the partitions refine.
</p>
<p>(2) Because the maximum length of the intervals ∆j tends to 0 as n&rarr;&infin;, the
total length of the intervals containing jumps converges to 0.
</p>
<p>Therefore, taking the unions of the remaining adjacent intervals ∆j (i.e. where
</p>
<p>there are no increments of ξ ), for each ω we obtain in the limit, as n&rarr;&infin;, ξ(t)+ 1
intervals (0, T1), (T1, T2), . . . , (Tν, t) on which the increments of ξ are null.
</p>
<p>(3) Finally, by (19.4.1) the probability that at least one of the increments on the
</p>
<p>intervals ∆j exceeds one is
&sum;
</p>
<p>j O(∆
2
j ) = o(1) as n&rarr;&infin;, so that, with probabil-
</p>
<p>ity 1, the jumps at the points Tk are equal to 1.
</p>
<p>Thus we have shown that, on the segment [0, t], for each ω there exists a finite
number ξ(t) of points T1, . . . , Tξ(t) such that ξ(u) takes at the rational points of the
</p>
<p>intervals (Tk, Tk+1) one and the same constant value equal to k. This means that one
can extend the trajectories of the process ξ(u), say, by continuity from the right so
</p>
<p>that ξ(u)= k for all u &isin; [Tk, Tk+1).
Thus, for the original process ξ(t) we have constructed a modification ξ(t) with
</p>
<p>trajectories in D+(T ). The equivalence of ξ and ξ follows from the very construc-
tion since, by virtue of (1),
</p>
<p>P
(
ξ(t)= ξ(t)
</p>
<p>)
= P
</p>
<p>(
lim
n&rarr;&infin;
</p>
<p>ν(n)= ξ(t)
)
= 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.4 The Poisson Process 551
</p>
<p>One usually considers just such right (or left) continuous modifications of the
</p>
<p>Poisson process. We have already dealt with processes of this kind in Chap. 10
</p>
<p>where more general objects&mdash;renewal processes&mdash;were defined from scratch using
</p>
<p>trajectories. That the Poisson process is a renewal process is seen from the following
</p>
<p>considerations. It is easy to establish from relations (19.4.1) that the distributions of
</p>
<p>the random variables T1, T2 &minus; T1, T3 &minus; T2, . . . coincide and that these variables are
independent. Indeed, the difference Tj &minus; Tj&minus;1, j &ge; 1, T0 = 0, can be approximated
by the sum (γj &minus; γj&minus;1)∆ of the lengths of identical intervals of size ∆i =∆, where
γj is the number of the interval in which the j -th non-zero increment of ξ occurred.
</p>
<p>Since the process ξ(t) is homogeneous with independent increments, we have
</p>
<p>P
(
(γj &minus; γj&minus;1)∆ &gt; u
</p>
<p>)
= P
</p>
<p>(
γ1 &gt;
</p>
<p>u
</p>
<p>∆
</p>
<p>)
=
(
e&minus;&micro;∆
</p>
<p>)&lfloor;u/∆&rfloor; &rarr; e&minus;&micro;u,
</p>
<p>P
(
(γj &minus; γj&minus;1)∆ &gt; u
</p>
<p>)
&rarr; P(Tj &minus; Tj&minus;1 &gt; u)
</p>
<p>as ∆&rarr; 0. Hence the variables τj := Tj &minus; Tj&minus;1, j = 1,2,3, . . . , have the exponen-
tial distribution, and the value ξ(t)+ 1 can be considered as the first crossing time
of the level t by the sums Tj :
</p>
<p>ξ(t)= max{k : Tk &le; t}, ξ(t)+ 1 = min{k : Tk &gt; t}.
Thus we obtain that the Poisson process ξ(t) coincides with the renewal process η(t)
</p>
<p>(see Chap. 10) for exponentially distributed variables τ1, τ2, . . . with P(τj&gt;u) =
e&minus;&micro;u.
</p>
<p>The above and the properties of the Poisson process also imply the following
</p>
<p>remarkable property of exponentially distributed random variables. The numbers of
</p>
<p>jump points (i.e. sums Tk) which fall into disjoint time intervals δj are independent,
</p>
<p>these numbers being distributed according to the Poisson laws with parameters &micro;δj .
</p>
<p>Using the last fact, one can construct a more general model of a pure jump ho-
</p>
<p>mogeneous process with independent increments. Consider an arbitrary sequence
</p>
<p>of independent identically distributed random variables ζ1, ζ2, . . . that have a ch.f.
</p>
<p>β(λ) and are independent of the σ -algebra generated by the process ξ(t). Construct
</p>
<p>now a new process ζ(t) as follows. To each ω we put into correspondence a new
</p>
<p>trajectory obtained from the trajectory ξ(t) by replacing the first unit jump with the
</p>
<p>variable ζ1, the second one with the variable ζ2, and so on. It is easy to see that ζ(t)
</p>
<p>will also be a process with independent increments. The value ζ(t) will be equal to
</p>
<p>the sum
</p>
<p>ζ(t)= ζ1 + &middot; &middot; &middot; + ζξ(t) (19.4.2)
of the random number ξ(t) of random variables ζ1, ζ2, . . . , where ξ(t) is indepen-
</p>
<p>dent of {ζk} by construction.
Hence, by the total probability formula,
</p>
<p>Eeiλζ(t) =
&infin;&sum;
</p>
<p>k=0
P
(
ξ(t)= k
</p>
<p>)
Eeiλ(ζ1+&middot;&middot;&middot;+ζk)
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
</p>
<p>(&micro;t)k
</p>
<p>k! e
&minus;&micro;t(β(λ)
</p>
<p>)k = e&minus;&micro;t+&micro;tβ(λ) = e&micro;t(β(λ)&minus;1). (19.4.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>552 19 Processes with Independent Increments
</p>
<p>Definition 19.4.2 The process ζ(t) defined by formula (6) or ch.f. (7) is called a
</p>
<p>compound Poisson process. It is evidently a special case of the generalised renewal
process (see Sect. 10.6).
</p>
<p>As we have already noted, it is again a homogeneous process with independent
</p>
<p>increments. In formula (19.4.3), the parameter &micro; determines the jumps&rsquo; intensity
</p>
<p>in the process ζ(t), while the ch.f. β(λ) specifies their distribution. If we add a
</p>
<p>constant &ldquo;drift&rdquo; qt to the process ζ(t), then ζ̃ (t) = ζ(t) + qt will clearly also be
a homogeneous process with independent increments having the ch.f. Eeiλ̃ζ (t) =
et (iλq+&micro;(β(λ)&minus;1)).
</p>
<p>Finally, if a Wiener process w(t) with zero drift and diffusion coefficient σ is
</p>
<p>given on the same probability space and is independent of ζ̃ (t), and to each ω we
</p>
<p>put into correspondence a trajectory of ζ̃ (t)+w(t), we again obtain a process with
independent increments, with ch.f. exp{t (iλq +&micro;(β(λ)&minus; 1)&minus; λ2σ 2/2)}.
</p>
<p>One should note, however, that these constructions by no means exhaust the
</p>
<p>whole class of processes with independent increments (and therefore the class of
</p>
<p>infinitely divisible distributions).
</p>
<p>A description of the entire class will be given in the next section.
</p>
<p>The Poisson processes, as well as Wiener processes, are often used as mathemat-
</p>
<p>ical models in various applications. For example, the process of counts of cosmic
</p>
<p>particles of certain energy registered by a sensor in a given volume, or of collisions
</p>
<p>of elementary particles in an accelerator are described by the Poisson process. The
</p>
<p>same is true of the process of incoming telephone calls at a switchboard and many
</p>
<p>other processes.
</p>
<p>Due to representation (19.4.2), the study of compound Poisson processes re-
</p>
<p>duces, in many aspects, to the study of the properties of sums of independent random
</p>
<p>variables.
</p>
<p>19.5 Description of the Class of Processes with Independent
</p>
<p>Increments
</p>
<p>We saw in Theorem 19.1.1 that, to describe the class of distributions of stochasti-
</p>
<p>cally continuous processes with independent increments, it suffices to describe the
</p>
<p>class of all infinitely divisible distributions. Let, as before, L be the class of the
</p>
<p>ch.f.s of infinitely divisible distributions.
</p>
<p>Lemma 19.5.1 The classL is closed with respect to the operations of multiplication
and passage to the limit (when the limit is again a ch.f.).
</p>
<p>Proof (1) Let ϕ1 &isin;L and ϕ2 &isin;L. Then ϕ1ϕ2 = (ϕ1/n1 &middot; ϕ
1/n
2 )
</p>
<p>n, where ϕ
1/n
1 &middot; ϕ
</p>
<p>1/n
2 is
</p>
<p>a ch.f.
</p>
<p>(2) Let ϕn &isin; L, ϕn &rarr; ϕ, and ϕ be a ch.f. Then, for any m, ϕ1/mn &rarr; ϕ1/m
as n &rarr; &infin;, where ϕ1/m is continuous at zero and hence is a ch.f. The lemma is
proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Description of the Class of Processes with Independent Increments 553
</p>
<p>Denote by LΠ &sub;L the class of ch.f.s whose logarithms have the form
</p>
<p>lnϕ(λ)= iλq +
&sum;
</p>
<p>k
</p>
<p>ck
(
eiλbk &minus; 1
</p>
<p>)
, ck &ge; 0,
</p>
<p>&sum;
</p>
<p>k
</p>
<p>ck &lt;&infin;.
</p>
<p>We will call this the Poisson class. We already know that it corresponds to com-
pound Poisson processes with drift q and intensities ck of jumps of size bk (note
</p>
<p>that
&sum;
</p>
<p>k ck(e
iλbk &minus; 1) = (
</p>
<p>&sum;
k ck)E(e
</p>
<p>iλξ &minus; 1), where ζ assumes the values bk with
probabilities ck/
</p>
<p>&sum;
j cj ).
</p>
<p>Lemma 19.5.2 A ch.f. ϕ belongs to L if and only if ϕ = limn&rarr;&infin; ϕn, ϕn &isin;LΠ .
</p>
<p>Proof Sufficiency. Let
</p>
<p>lnϕn =
&sum;
</p>
<p>k
</p>
<p>(
iλqk,n + ck,n
</p>
<p>(
eiλbk,n &minus; 1
</p>
<p>))
,
</p>
<p>and ϕ = limϕn be a ch.f. It is evident that ϕ1/mn &isin; LΠ &sub; L and ϕ1/mn &rarr; ϕ1/m.
Therefore ϕ1/m, being a limit of a sequence of ch.f.s which is continuous at zero, is
</p>
<p>a ch.f. itself, so that ϕ &isin;L.
Necessity. Let ϕ &isin; L. Then ϕ(λ) 
= 0 and there exists β := lnϕ with n(ϕ1/n &minus;
</p>
<p>1)&rarr; β , and
</p>
<p>ϕ1/n &minus; 1 =
&int; (
</p>
<p>eiλx &minus; 1
)
dFn(x).
</p>
<p>The integral of the continuous function on the right-hand side can be viewed as a
</p>
<p>Riemann&ndash;Stieltjes integral. This means that for Fn there exists a partition of the real
</p>
<p>axis into intervals ∆nk such that, for xnk &isin;∆nk and rn &lt; cn&minus;2,
&int; (
</p>
<p>eiλx &minus; 1
)
dFn(x)=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>&int; (
eiλx &minus; 1
</p>
<p>)
Pn(∆nk)+ rn
</p>
<p>(Pn(∆) is the probability of hitting the interval ∆ corresponding to Fn). We obtain
</p>
<p>β = limn
(
ϕ1/n &minus; 1
</p>
<p>)
= lim
</p>
<p>n&rarr;&infin;
</p>
<p>[
n
&sum;
</p>
<p>k
</p>
<p>(
eiλxnk &minus; 1
</p>
<p>)
Pn(∆nk)
</p>
<p>]
.
</p>
<p>The lemma is proved. �
</p>
<p>Theorem 19.5.1 (L&eacute;vy&ndash;Khintchin) A ch.f. ϕ belongs to L if and only if the function
β := lnϕ admits a representation of the form
</p>
<p>β = β(λ;a,Ψ )= iλq +
&int; (
</p>
<p>eiλx &minus; 1 &minus; iλx
1 + x2
</p>
<p>)
1 + x2
x2
</p>
<p>dΨ (x), (19.5.1)
</p>
<p>where Ψ is a non-decreasing function of bounded variation (i.e., a distribution func-
tion up to a constant factor), the integrand being assumed equal to &minus;λ2/2 at the
point x = 0 (by continuity).</p>
<p/>
</div>
<div class="page"><p/>
<p>554 19 Processes with Independent Increments
</p>
<p>Proof Assume that β has the form (19.5.1). Then β(λ) is a continuous function,
since it is (up to a continuous additive term iλa) a uniformly convergent integral of
</p>
<p>a continuous bounded function. Further, let xnk 
= 0, k = 1, . . . , n, be points of refin-
ing partitions of intervals [&minus;&radic;n,&radic;n ). Then β0(λ)= β(λ)&minus; iλq can be represented
as β0 = limβn with
</p>
<p>βn(λ) :=
n&sum;
</p>
<p>k=1
</p>
<p>[
iλqkn + ckn
</p>
<p>(
eiλbkn &minus; 1
</p>
<p>)]
&isin;LΠ ,
</p>
<p>where, under a natural notational convention, one should put
</p>
<p>ckn =
1 + x2kn
x2kn
</p>
<p>Ψ
(
[xkn, xk+1,n)
</p>
<p>)
, qkn =
</p>
<p>1
</p>
<p>xkn
Ψ
(
[xkn, xk+1,n)
</p>
<p>)
, bkn = xkn,
</p>
<p>Ψ being used to denote the measure Ψ (A)=
&int;
A
dΨ (x). We obtain that ϕ is a limit
</p>
<p>of the sequence of ch.f.s ϕn &isin;LΠ . It remains to make use of Lemma 19.5.2.
Now let ϕ &isin;L. Then
</p>
<p>β = limn
(
ϕ1/n &minus; 1
</p>
<p>)
= lim
</p>
<p>&int; (
eiλx &minus; 1
</p>
<p>)
ndFn(x)
</p>
<p>= lim
[
iλ
</p>
<p>&int;
nx
</p>
<p>1 + x2 dFn(x)
</p>
<p>+
&int; (
</p>
<p>eiλx &minus; 1 &minus; iλx
1 + x2
</p>
<p>)
1 + x2
x2
</p>
<p>nx2
</p>
<p>1 + x2 dFn(x)
]
. (19.5.2)
</p>
<p>If we put
</p>
<p>qn :=
&int;
</p>
<p>nx
</p>
<p>1 + x2 dFn(x), Ψn(x) :=
nx2
</p>
<p>1 + x2 dFn(x), (19.5.3)
</p>
<p>then on the right-hand side of (19.5.2) we will have limβn, βn = β(λ;qn,Ψn).
Now assume for a moment that the following continuity theorem holds for func-
</p>
<p>tions from L.
</p>
<p>Lemma 19.5.3 If βn = β(λ;qn,Ψn)&rarr; β and β is continuous at the point λ = 0,
then β(λ) has the form β(λ;q,Ψ ), qn &rarr; q and Ψn &rArr; Ψ .
</p>
<p>The symbol &rArr; in the lemma means convergence at the points of continuity of
the limiting function (as in the case of distribution functions) and that Ψn(&plusmn;&infin;)&rarr;
Ψ (&plusmn;&infin;).
</p>
<p>If the lemma is true, the required assertion of the theorem will follow in an obvi-
</p>
<p>ous way from (19.5.2) and (19.5.3). It remains to prove the lemma.
</p>
<p>Proof of Lemma 19.5.3 Observe first that the correspondence β(λ;q,Ψ )&harr; (q,Ψ )
is one-to-one. Since in one direction it is obvious, we only have to verify that β
</p>
<p>uniquely determines q and Ψ . To each β we put into correspondence the function</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Description of the Class of Processes with Independent Increments 555
</p>
<p>γ (λ)=
&int; 1
</p>
<p>0
</p>
<p>[
β(λ)&minus; 1
</p>
<p>2
</p>
<p>(
β(λ+ h)&minus; β(λ&minus; h)
</p>
<p>)]
dh
</p>
<p>=
&int; 1
</p>
<p>0
</p>
<p>&int; (
eiλx &minus; 1
</p>
<p>2
</p>
<p>(
ei(λ+h)x &minus; ei(λ&minus;h)x
</p>
<p>))1 + x2
x2
</p>
<p>dΨ (x)dh,
</p>
<p>where
</p>
<p>1
</p>
<p>2
</p>
<p>(
ei(λ+h)x &minus; ei(λ&minus;h)x
</p>
<p>)
= eiλx coshx,
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>eiλx(1 &minus; coshx)dh= eiλx
(
</p>
<p>1 &minus; sinx
x
</p>
<p>)
,
</p>
<p>0 &lt; c1 &lt;
</p>
<p>(
1 &minus; sinx
</p>
<p>x
</p>
<p>)
1 + x2
x2
</p>
<p>&lt; c2 &lt;&infin;.
</p>
<p>Therefore
</p>
<p>γ (λ)=
&int;
</p>
<p>eiλx dΓ (x),
</p>
<p>where
</p>
<p>Γ (x)=
&int; x
</p>
<p>&minus;&infin;
</p>
<p>(
1 &minus; sinu
</p>
<p>u
</p>
<p>)
1 + u2
u2
</p>
<p>dΨ (u)
</p>
<p>is (up to a constant multiplier) a distribution function, for which γ (λ) plays the role
</p>
<p>of its ch.f. Clearly,
</p>
<p>Ψ (x)=
&int; x
</p>
<p>&minus;&infin;
</p>
<p>1 + u2
u2
</p>
<p>(
1 &minus; sinu
</p>
<p>u
</p>
<p>)&minus;1
dΓ (u),
</p>
<p>so that we obtained a chain of univalent correspondences β &rarr; γ &rarr; Γ &rarr; Ψ which
proves the assertion.
</p>
<p>We return to the proof of Lemma 19.5.3. Because eβn &rarr; eβ , eβn is a ch.f., and
eβ is continuous at the point λ= 0, we see that eβ is a ch.f. and hence a continuous
function. This means that the convergence ϕn &rarr; ϕ is uniform on any interval,
</p>
<p>γn(λ) =
&int; 1
</p>
<p>0
</p>
<p>[
βn(λ)&minus;
</p>
<p>1
</p>
<p>2
</p>
<p>(
βn(λ+ h)+ βn(λ&minus; h)
</p>
<p>)]
dh
</p>
<p>&rarr;
&int; 1
</p>
<p>0
</p>
<p>[
β(λ)&minus; 1
</p>
<p>1
</p>
<p>(
β(λ+ h)+ β(λ&minus; h)
</p>
<p>)]
dh=: γ (λ),
</p>
<p>and the function γ (u) is continuous. By the continuity theorem for ch.f.s, this means
</p>
<p>that γ (u) is a ch.f. (of a finite measure Γ ), Γn &rArr; Γ (where Γn is the preimage of
γn), Ψn &rArr; Ψ , and qn &rarr; q . Thus we establish that
</p>
<p>β = limβn = lim
[
iλqn +
</p>
<p>&int; (
eiλx &minus; 1 &minus; iλx
</p>
<p>1 + x2
)
dΨn(x)
</p>
<p>]
</p>
<p>= iλq +
&int; (
</p>
<p>eiλx &minus; 1 &minus; iλx
1 + x2
</p>
<p>)
dΨ (x)= β(λ;q,Ψ ).
</p>
<p>Lemma 19.5.3 is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>556 19 Processes with Independent Increments
</p>
<p>Theorem 19.5.1 is proved. �
</p>
<p>Now we will make several remarks in regard to the structure of the process ξ(t)
</p>
<p>and its relationship to representation (19.5.1). The function Ψ in (19.5.1) corre-
</p>
<p>sponds to the so-called spectral measure of the process ξ(t) (recall that we agreed
to use the same symbol Ψ for the measure itself: Ψ (A) =
</p>
<p>&int;
A
dΨ (x)). It can be
</p>
<p>represented in the form &micro;Ψ1(x), where &micro;= Ψ (&infin;)&minus; Ψ (&minus;&infin;) and Ψ1(x) is a dis-
tribution function.
</p>
<p>(1) The spectral measure of the Wiener process is concentrated at the point 0. If
</p>
<p>Ψ ({0})= σ 2, then ξ(1)&sub;=�q,σ 2 .
(2) The spectral measure Ψ of a compound Poisson process has the property
</p>
<p>&int;
|x|&minus;2 dΨ (x) &lt;&infin;.
</p>
<p>In that case
</p>
<p>G(x)=
&int; x
</p>
<p>&minus;&infin;
</p>
<p>1 + u2
u2
</p>
<p>dΨ (u)
</p>
<p>possesses the properties of a distribution function, and ψ(λ;q,Ψ ) may be written
in the form
</p>
<p>iλq1 +
&int; (
</p>
<p>eiλx &minus; 1
)
dG(x),
</p>
<p>where
</p>
<p>q1 = q &minus;
&int;
</p>
<p>x&minus;1 dΨ (x).
</p>
<p>(3) Consider now the general case, but under the condition that Ψ ({0})= 0. As
we know, the function ψ can be approximated for small ∆ by expressions of the
</p>
<p>form (we put ∆k = [(k &minus; 1)∆, k∆))
</p>
<p>iλq +
&infin;&sum;
</p>
<p>k=&minus;&infin;
k 
=0
</p>
<p>[
&minus; iλ
k∆
</p>
<p>Ψ (∆k)+
(
eiλk∆ &minus; 1
</p>
<p>)1 + (k∆)2
(k∆)2
</p>
<p>Ψ (∆k)
</p>
<p>]
,
</p>
<p>which corresponds to the sum of Poisson processes with jumps of sizes k∆ of the
</p>
<p>respective intensities
</p>
<p>1 + (k∆)2
(k∆)2
</p>
<p>Ψ (∆k).
</p>
<p>If, say,
&int; &infin;
</p>
<p>+0
</p>
<p>dΨ (x)
</p>
<p>x2
=&infin;,
</p>
<p>then for any ε &gt; 0 the total intensity of these processes with jumps from the interval
</p>
<p>(0, ε) will increase to infinity as ∆&rarr; 0. This means that, with probability 1, on any
time interval δ there will be at least one jump of size smaller than any given ε &gt; 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Description of the Class of Processes with Independent Increments 557
</p>
<p>so that the trajectories of ξ(t) will be everywhere discontinuous. To &ldquo;compensate&rdquo;
</p>
<p>these jumps, a drift of size Ψ (∆k)/k∆ is added, the &ldquo;total value&rdquo; of such drifts being
</p>
<p>possibly unbounded (if
&int;&infin;
+0 x
</p>
<p>&minus;1dΨ (x)=&infin;).
(4) For stable processes (see Sect. 8.8) the functions Ψ (x) have power &ldquo;branches&rdquo;,
</p>
<p>smooth on the half-axes, possessing the property c1Ψ
&prime;(x)= Ψ &prime;(c2x) for appropriate
</p>
<p>c1 and c2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 20
</p>
<p>Functional Limit Theorems
</p>
<p>Abstract The chapter begins with Sect. 20.1 presenting the classical Functional
</p>
<p>Central Limit Theorem in the triangular array scheme. It establishes not only con-
</p>
<p>vergence of the distributions of the scaled trajectories of random walks to that of
</p>
<p>the Wiener process, but also convergence rates for Lipshchitz sets and distribution
</p>
<p>functions of Lipshchitz functionals in the case of finite third moments when the
</p>
<p>Lyapunov condition is met. Section 20.2 uses the Law of the Iterated Logarithm for
</p>
<p>the Wiener process to establish such a low for the trajectory of a random walk with
</p>
<p>independent non-identically distributed jumps. Section 20.3 is devoted to proving
</p>
<p>convergence to the Poisson process of the processes of cumulative sums of indepen-
</p>
<p>dent random indicators with low success probabilities and also that of the so-called
</p>
<p>thinning renewal processes.
</p>
<p>20.1 Convergence to the Wiener Process
</p>
<p>We have already pointed out in Sect. 19.2 that the Wiener processes are, in a certain
</p>
<p>sense, limiting to random polygons with vertices at the points (k/n,Sk/
&radic;
n), where
</p>
<p>Sk = ξ1 + &middot; &middot; &middot; + ξk are partial sums of independent identically distributed random
variables ξ1, ξ2, . . . with zero means and finite variances. Now we will give a more
</p>
<p>precise and general meaning to this statement.
</p>
<p>Let
</p>
<p>ξ1,n, . . . , ξn,n (20.1.1)
</p>
<p>be independent random variables in the triangular array scheme (see Sects. 8.3, 8.4),
</p>
<p>ζk,n :=
k&sum;
</p>
<p>j=1
ξj,n, Eξk,n = 0, Eξ2k,n = σ 2k,n,
</p>
<p>that have finite third moments E|ξk,n|3 = &micro;k,n &lt;&infin;.
We will assume without loss of generality (see Sect. 8.4) that
</p>
<p>Var(ζn,n)=
n&sum;
</p>
<p>j=1
σ 2j,n = 1.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_20, &copy; Springer-Verlag London 2013
</p>
<p>559</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_20">http://dx.doi.org/10.1007/978-1-4471-5201-9_20</a></div>
</div>
<div class="page"><p/>
<p>560 20 Functional Limit Theorems
</p>
<p>Fig. 20.1 The random
</p>
<p>polygon sn(t) constructed
</p>
<p>from the random walk
</p>
<p>ζ0, ζ1, ζ2, . . .
</p>
<p>Put
</p>
<p>tk,n :=
k&sum;
</p>
<p>j=1
σ 2j,n,
</p>
<p>so that t0,n = 0, tn,n = 1, and consider a random polygon with vertices at the points
(tk, ζk), where we suppress the second subscript n for brevity&rsquo;s sake: tk = tk,n,
ζk = ζk,n.
</p>
<p>We obtain a random process on [0,1] with continuous trajectories, which will
be denoted by sn = sn(t) (see Fig. 20.1). The functional limit theorem (or invari-
ance principle; the motivation behind this second name will be commented on be-
</p>
<p>low) states that for any functional f given on the space C(0,1) and continuous in
</p>
<p>the uniform metric, the distribution of f (sn) converges weakly to that of f (w) as
</p>
<p>n&rarr;&infin;:
</p>
<p>f (sn)'&rArr; f (w), (20.1.2)
</p>
<p>where w =w(t) is the standard Wiener process. The conventional central limit the-
orem is a special case of this statement (one should take f (x) to be x(1)).
</p>
<p>The above assertion is equivalent to each of the following two statements:
</p>
<p>1. For any bounded continuous functional f ,
</p>
<p>Ef (sn)&rarr; Ef (w), n&rarr;&infin;. (20.1.3)
</p>
<p>2. For any set G from the σ -algebra BC(0,1) of Borel sets in the space
</p>
<p>C(0,1) (BC(0,1) is generated by open balls in the metric space C(0,1) endowed
</p>
<p>with the uniform distance ρ; as we already noted, BC(0,1) = B[0,1]C ) such that
P(w &isin; &part;G)= 0, where &part;G is the boundary of the set G, one has
</p>
<p>P(sn &isin;G)&rarr; P(w &isin;G), n&rarr;&infin;. (20.1.4)
</p>
<p>Relations (20.1.3) and (20.1.4) are equivalent definitions of weak convergence of
</p>
<p>the distributions Pn of the processes sn to the distribution W of the Wiener process
</p>
<p>w in the space 〈C(0,1),BC(0,1)〉. More details can be found in Appendix 3 and in
[1] and [14].
</p>
<p>The main results of the present section are the following theorems.
</p>
<p>As before, put L3 :=
&sum;n
</p>
<p>k=1 &micro;k,n.
</p>
<p>Theorem 20.1.1 Let L3 &rarr; 0 as n&rarr;&infin; (the Lyapunov condition). Then the con-
vergence relations (20.1.2)&ndash;(20.1.4) hold true.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Convergence to the Wiener Process 561
</p>
<p>Remark 20.1.1 The condition L3 &rarr; 0 can be relaxed here to the Lindeberg condi-
tion. In this version the above convergence theorem is known under the name of the
</p>
<p>Donsker&ndash;Prokhorov invariance principle.
</p>
<p>Along with Theorem 20.1.1 we will obtain a more precise assertion.
</p>
<p>Definition 20.1.1 A set G is said to be Lipschitz if W(G(ε))&minus;W(G)&le; cε for some
c &lt;&infin;, where G(ε) is the ε-neighbourhood of G and W is the measure correspond-
ing to the Wiener process.
</p>
<p>In the sequel we will denote by the letter c (with or without subscripts) absolute
</p>
<p>constants, possibly having different values.
</p>
<p>Theorem 20.1.2 If G is a Lipschitz set, then
∣∣P(sn &isin;G)&minus; P(w &isin;G)
</p>
<p>∣∣&lt; cL1/43 . (20.1.5)
</p>
<p>In the case when ξk,n = ξk/
&radic;
n, where the ξk do not depend on n and are iden-
</p>
<p>tically distributed with Eξk = 0 and Var(ξk) = 1, the right-hand side of (20.1.5)
becomes cn&minus;1/8.
</p>
<p>A similar bound can be obtained for functionals. A functional on C(0,1) is said
</p>
<p>to be Lipschitz if the following two conditions are met:
</p>
<p>(1) |f (x)&minus; f (y)|&lt; cρ(x, y);
(2) the distribution of f (w) has a bounded density.
</p>
<p>Corollary 20.1.1 If f is a Lipschitz functional, then Gv := {f (x) &lt; v} is a Lips-
chitz set (with one and the same constant for all v), so that by Theorem 20.1.2
</p>
<p>sup
v
</p>
<p>∣∣P
(
f (w) &lt; v
</p>
<p>)
&minus; P
</p>
<p>(
f (sn) &lt; v
</p>
<p>)∣∣&le; cL1/43 .
</p>
<p>The above theorems are consequences of Theorem 20.1.3 to be stated below.
</p>
<p>Let
</p>
<p>η1,n, . . . , ηn,n (20.1.6)
</p>
<p>be any other sequence of independent identically distributed random variables in the
</p>
<p>triangular array scheme with the same two first moments Eηk,n = 0, Eη2k,n = σ 4k.n,
and finite third moments. Denote by Fk,n and Φk,n the distribution functions of ξk,n
and ηk,n, respectively, and put
</p>
<p>νk,n := E|ηk,n|3 &lt;&infin;, N3 :=
n&sum;
</p>
<p>k=1
νk,n,
</p>
<p>&micro;0k,n :=
&int;
</p>
<p>|x|3
∣∣d
(
Fk,n(x)&minus;Φk,n(x)
</p>
<p>)∣∣&le; &micro;k,n + νk,n,
</p>
<p>L03 :=
n&sum;
</p>
<p>k=1
&micro;0k,n &le; L3 +N3.</p>
<p/>
</div>
<div class="page"><p/>
<p>562 20 Functional Limit Theorems
</p>
<p>Denote by s&prime;n(t) the random process constructed in the same way as sn(t) but using
the sequence {ηk,n}.
</p>
<p>Theorem 20.1.3 For any A &isin;BC(0,1) and any ε &gt; 0,
</p>
<p>P(sn &isin;A)&le; P
(
s&prime;n &isin;A(2ε)
</p>
<p>)
+
</p>
<p>cL03
</p>
<p>ε3
.
</p>
<p>In order to prove Theorem 20.1.3, we will first obtain its finite-dimensional
</p>
<p>analogue. Denote by ζ and η the vectors ζ = (ζ1, . . . , ζn) and η = (η1, . . . , η2)
respectively, where ζk :=
</p>
<p>&sum;k
j=1 ζj,n and ηk :=
</p>
<p>&sum;k
j=1 ηj,n, and by B
</p>
<p>(ε) the ε-
</p>
<p>neighbourhood of a set B &isin;Rn:
</p>
<p>B(ε) :=
⋃
</p>
<p>x&isin;B
|v|&le;ε
</p>
<p>(x + v),
</p>
<p>where x = (x1, . . . , xn), v = (v1, . . . , vn), and |v| = maxk |vk|.
</p>
<p>Lemma 20.1.1 Let B be an arbitrary Borel subset of Rn. Then, for any ε &gt; 0,
</p>
<p>P(ζ &isin; B)&le; P
(
η &isin; B(2ε)
</p>
<p>)
+
</p>
<p>cL03
</p>
<p>ε3
.
</p>
<p>Proof1 Introduce a collection of nested neighbourhoods
</p>
<p>B(ε)(k) :=
⋃
</p>
<p>x&isin;B
|v|&le;ε
</p>
<p>(x1, . . . , xk, xk+1 + vk+1, . . . , xn + vn), k = 0, . . . , n,
</p>
<p>B := B(ε)(n)&sub; B(ε)(n&minus; 1)&sub; &middot; &middot; &middot; &sub; B(ε)(1)&sub; B(ε)(0)= B(ε)
</p>
<p>and denote by ek the vector (0, . . . ,0,1,0, . . . ,0), where 1 stands in the k-th posi-
</p>
<p>tion. It is obvious that if x &isin; B(ε)(k), then
x + ekvk &isin; B(ε)(k &minus; 1) if |vk| &le; ε. (20.1.7)
</p>
<p>Further, together with arrays (20.1.1) and (20.1.6), consider the collection of
</p>
<p>&ldquo;transitional&rdquo; arrays
</p>
<p>ξ1,n, . . . , ξk,n, ηk+1,n, . . . , ηn,n, k = 0, . . . , n. (20.1.8)
Denote by ζ(k)= (ζ1(k), . . . , ζn(k)) the vectors formed by the cumulative sums of
random variables from the k-th row (20.1.8), so that
</p>
<p>ζj (k)=
{
ζj for j &le; k,
ζk + ηk+1,n + &middot; &middot; &middot; + ηj,n for j &gt; k.
</p>
<p>To continue the proof of Lemma 20.1.1 we need the following.
</p>
<p>1The extension of the approach to proving the central limit theorem used in Sect. 8.5, which is
</p>
<p>used in this demonstration, was suggested by A.V. Sakhanenko.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Convergence to the Wiener Process 563
</p>
<p>Lemma 20.1.2 For any random variable δ such that P(|δ| &le; ε)= 1, one has
</p>
<p>P(ζ &isin; B)&le; P
(
η &isin; B(2ε)
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>k=1
∆k, (20.1.9)
</p>
<p>where
</p>
<p>∆k = P
(
ζ(k)+ δe(k &minus; 1) &isin; B(ε)(k &minus; 1)
</p>
<p>)
&minus; P
</p>
<p>(
ζ(k &minus; 1)+ δe(k &minus; 1) &isin; B(ε)(k &minus; 1)
</p>
<p>)
,
</p>
<p>e(r)=
n&sum;
</p>
<p>j=r+1
ej = (0, . . . ,0,1, . . . ,1).
</p>
<p>Proof Indeed, by virtue of (20.1.7),
</p>
<p>P(ζ &isin; B)= P
(
ζ(n) &isin; B(ε)(n)
</p>
<p>)
&le; P
</p>
<p>(
ζ(n)+ e(n&minus; 1)δ &isin; B(ε)(n&minus; 1)
</p>
<p>)
</p>
<p>&equiv; P
(
ζ(n&minus; 1)+ e(n&minus; 1)δ &isin; B(ε)(n&minus; 1)
</p>
<p>)
+∆n.
</p>
<p>Reapplying the same calculations to the right-hand side, we obtain that
</p>
<p>P
(
ζ(n&minus; 1)+ e(n&minus; 1)δ &isin; B(ε)(n&minus; 1)
</p>
<p>)
</p>
<p>&le; P
(
ζ(n&minus; 1)+ e(n&minus; 1)δ + en&minus;1δ &isin; B(ε)(n&minus; 2)
</p>
<p>)
</p>
<p>= P
(
ζ(n&minus; 1)+ e(n&minus; 2)δ &isin; B(ε)(n&minus; 2)
</p>
<p>)
</p>
<p>&equiv; P
(
ζ(n&minus; 2)+ e(n&minus; 2)δ &isin; B(ε)(n&minus; 2)
</p>
<p>)
+∆n&minus;1
</p>
<p>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
</p>
<p>P
(
ζ(1)+ e(1)δ &isin; B(ε)(1)
</p>
<p>)
&le; P
</p>
<p>(
ζ(1)+ e(1)δ + e1δ &isin; B(ε)(0)
</p>
<p>)
</p>
<p>= P
(
ζ(1)+ e(0)δ &isin; B(ε)(0)
</p>
<p>)
&equiv; P
</p>
<p>(
ζ(0)+ e(0)δ &isin; B(ε)(0)
</p>
<p>)
+∆1.
</p>
<p>Since ζ(0) = η and P(η + e(0)δ &isin; B(ε)) &le; P(η &isin; B(2ε)), inequality (20.1.9) is
proved. Lemma 20.1.2 is proved. �
</p>
<p>To obtain Lemma 20.1.1, we now have to estimate ∆k . It will be convenient to
</p>
<p>consider, along with (20.1.8), the sequences
</p>
<p>ξ1,n, . . . , ξk&minus;1,n, y, ηk+1,n, . . . , ηn,n
</p>
<p>and denote by ζ(k, y)= (ζ1(k, y), . . . , ζn(k, y)) the respective vectors of cumulative
sums, so that
</p>
<p>ζ(k, ξk,n)= ζ(k)= ζ(k,0)+ ξk,ne(k&minus;1),
ζ(k, ηk,n)= ζ(k &minus; 1)= ζ(k,0)+ ηk,ne(k&minus;1).
</p>
<p>Then ∆k can be written in the form
</p>
<p>∆k = P
(
ζ(k,0)+ (δ + ξk,n)e(k &minus; 1) &isin; B(ε)(k &minus; 1)
</p>
<p>)
</p>
<p>&minus; P
(
ζ(k,0)+ (δ + ηk,n)e(k &minus; 1) &isin; B(ε)(k &minus; 1)
</p>
<p>)
. (20.1.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>564 20 Functional Limit Theorems
</p>
<p>Take δ to be a random variable independent of ζ and η. Then it will be convenient
</p>
<p>to use conditional expectation to estimate the probabilities participating in (20.1.10),
</p>
<p>because, for instance, in the equality
</p>
<p>P
(
ζ(k,0)+ (δ + ξk,n)e(k &minus; 1) &isin; B(ε)(k &minus; 1)
</p>
<p>)
</p>
<p>= EP
(
(δ + ξk,n)e(k &minus; 1) &isin; B(ε)(k &minus; 1)&minus; ζ(k,0)
</p>
<p>∣∣ ζ(k,0)
)
</p>
<p>(20.1.11)
</p>
<p>the set C = B(ε)(k &minus; 1)&minus; ζ(k,0) may be assumed fixed (see the properties of con-
ditional expectations; here δ and ξk,n are independent of ζ(k,0)). Denote by D the
</p>
<p>set of all ys for which y e(k &minus; 1) &isin; C. We have to bound the difference
P(δ + ξk,n &isin;D)&minus; P(δ + ηk,n &isin;D). (20.1.12)
</p>
<p>We make use of Lemma 8.5.1. To transform (20.1.12) to a form convenient for
</p>
<p>applying the lemma, take δ to be a random variable having a thrice continuously
</p>
<p>differentiable density g(t) and put for brevity ξk,n = ξ and ηk,n = η. Then δ + ξ
will have a density equal to
</p>
<p>&int;
dFξ (t)g(y &minus; t)= Eg(y &minus; ξ),
</p>
<p>so that
</p>
<p>P(δ + ξ &isin;D)=
&int;
</p>
<p>D
</p>
<p>Eg(y &minus; ξ) dy = E
&int;
</p>
<p>D
</p>
<p>g(y &minus; ξ) dy.
</p>
<p>Now putting
</p>
<p>h(x) :=
&int;
</p>
<p>D
</p>
<p>g(y &minus; x)dy,
</p>
<p>we have
</p>
<p>P(δ + ξ &isin;D)= Eh(ξ),
where h is a thrice continuously differentiable function,
</p>
<p>∣∣h&prime;&prime;&prime;(x)
∣∣&le;
</p>
<p>&int; ∣∣g&prime;&prime;&prime;(y)
∣∣dy =: h3.
</p>
<p>Applying now Lemma 8.5.1 we obtain that
</p>
<p>∣∣P(δ + ξ &isin;D)&minus; P(δ + η &isin;D)
∣∣=
</p>
<p>∣∣E
(
h(ξ)&minus; h(η)
</p>
<p>)∣∣&le; h3
6
&micro;0k,n,
</p>
<p>&micro;0k,n =
&int; ∣∣x3
</p>
<p>∣∣∣∣d
(
Fk,n(x)&minus;Φk,n(x)
</p>
<p>)∣∣.
</p>
<p>Because the right-hand side here does not depend on ξ(k,0) and D in any way, we
</p>
<p>get, returning to (20.1.10) and (20.1.11), the estimate
</p>
<p>|∆k| &le;
h3
</p>
<p>6
&micro;0n,k. (20.1.13)
</p>
<p>Now let g1(x) be a smooth density concentrated on [&minus;1,1]. Then, putting
</p>
<p>g(x) := g1
(
x
</p>
<p>ε
</p>
<p>)
1
</p>
<p>ε
,</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Convergence to the Wiener Process 565
</p>
<p>we obtain that
</p>
<p>h3 =
&int;
</p>
<p>1
</p>
<p>ε4
</p>
<p>∣∣∣∣g
&prime;&prime;&prime;
1
</p>
<p>(
y
</p>
<p>ε
</p>
<p>)∣∣∣∣dy =
1
</p>
<p>ε3
</p>
<p>&int; ∣∣g&prime;&prime;&prime;1 (y)
∣∣dy = c1
</p>
<p>ε3
, c1 = const. (20.1.14)
</p>
<p>The assertion of Lemma 20.1.1 now follows from (20.1.9), (20.1.13) and
</p>
<p>(20.1.14). �
</p>
<p>Proof of Theorem 20.1.3 This theorem is a consequence of Lemma 20.1.1. Indeed,
let B &isin; Rn be such that the events {sn &isin; A} and {ζ &isin; B} are equivalent (sn is com-
pletely determined by ζ ). Then clearly {sn &isin;A(ε)} = {ζ &isin; B(ε)} and the assertion of
Theorem 20.1.3 repeats that of Lemma 20.1.1. Theorem 20.1.3 is proved. �
</p>
<p>Proof of Theorem 20.1.1 Let w(t) be the standard Wiener process. Put ηk,n :=
w(tk,n)&minus;w(tk&minus;1,n). Then the sequence η1,n, . . . , ηn,n satisfies all the required con-
ditions, for
</p>
<p>Eηk,n = 0, Eη2k,n = σ 2k,n, νk,n = E|ξk,n|3 = c3σ 3k,n &lt;&infin;.
Note also that
</p>
<p>σ 3k,n =
(
E|ξk,n|2
</p>
<p>)3/2 &le; E|ξk,n|3 = &micro;k,n,
so that
</p>
<p>N3 =
n&sum;
</p>
<p>k=1
νk,n = c3
</p>
<p>n&sum;
</p>
<p>k=1
σ 3k,n &le; c3L3 &rarr; 0.
</p>
<p>We will need the following
</p>
<p>Lemma 20.1.3 P(ρ(w, s&prime;n) &gt; ε)&le; cN3/ε3.
</p>
<p>Proof The event {ρ(w, s&prime;n) &gt; ε} is equal to
⋃
</p>
<p>k Ak , where
</p>
<p>Ak :=
{
</p>
<p>sup
t&isin;Ik
</p>
<p>∣∣w(t)&minus; s&prime;(t)
∣∣&gt; ε
</p>
<p>}
&sub;
{
</p>
<p>sup
t&isin;Ik
</p>
<p>∣∣w(t)
∣∣&gt; ε
</p>
<p>2
</p>
<p>}
, Ik := [tk&minus;1, tk].
</p>
<p>Therefore, recalling that tk &minus; tk&minus;1 = σ 2k,n and w(t)
d= σw(t/σ 2), we have
</p>
<p>P(Ak)&le; P
(
</p>
<p>sup
t&isin;[0,1)
</p>
<p>∣∣w(t)
∣∣&gt; ε
</p>
<p>2σk,n
</p>
<p>)
&le; 2
</p>
<p>(
1 &minus;Φ
</p>
<p>(
ε
</p>
<p>2σk,n
</p>
<p>))
.
</p>
<p>The function (1 &minus;Φ(t)) vanishes as t &rarr;&infin; much faster than t&minus;3. Hence
</p>
<p>2
</p>
<p>(
1 &minus;Φ
</p>
<p>(
ε
</p>
<p>2σk,n
</p>
<p>))
&le; c
</p>
<p>σ 3k,n
</p>
<p>ε3
, P
</p>
<p>(⋃
</p>
<p>k
</p>
<p>Ak
</p>
<p>)
&le; cN3
</p>
<p>ε3
.
</p>
<p>Lemma 20.1.3 is proved. �
</p>
<p>We see from the proof that the bound stated by Lemma 20.1.3 is rather crude.</p>
<p/>
</div>
<div class="page"><p/>
<p>566 20 Functional Limit Theorems
</p>
<p>We return to the proof of Theorem 20.1.1. Because
</p>
<p>P
(
s&prime;n &isin;G
</p>
<p>)
= P
</p>
<p>(
s&prime;n &isin;G,ρ
</p>
<p>(
w, s&prime;n
</p>
<p>)
&le; ε
</p>
<p>)
+ P
</p>
<p>(
s&prime;n &isin;G,ρ
</p>
<p>(
w, s&prime;n
</p>
<p>)
&gt; ε
</p>
<p>)
,
</p>
<p>we have
</p>
<p>P
(
s&prime;n &isin;G
</p>
<p>)
&le; P
</p>
<p>(
w &isin;G(ε)
</p>
<p>)
+ cN3
</p>
<p>ε3
(20.1.15)
</p>
<p>and, by Theorem 20.1.3,
</p>
<p>P(sn &isin;G)&le; P
(
w &isin;G(3ε)
</p>
<p>)
+
</p>
<p>c(L03 +N3)
ε3
</p>
<p>.
</p>
<p>Now we prove the converse inequality. Introduce the set G(&minus;ε) :=G&minus; (&part;G)(ε).
Then [G(&minus;ε)](ε) =:G0 &sub;G. Swapping sn and s&prime;n in Theorem 20.1.3 and applying
the latter to the set G(2ε), we obtain
</p>
<p>P
(
sn &isin;G0
</p>
<p>)
&ge; P
</p>
<p>(
s&prime;n &isin;G(&minus;2ε)
</p>
<p>)
&minus;
</p>
<p>c(L03 +N3)
ε3
</p>
<p>. (20.1.16)
</p>
<p>Swapping w and s&prime;n in (20.1.15) and applying that relation to G
(&minus;3ε), we find that
</p>
<p>P
(
s&prime;n &isin;G(&minus;2ε)
</p>
<p>)
&ge; P
</p>
<p>(
w &isin;G(&minus;3ε)
</p>
<p>)
&minus; cN3
</p>
<p>ε3
.
</p>
<p>This and (20.1.16) imply that
</p>
<p>P(sn &isin;G)&ge; P
(
sn &isin;G0
</p>
<p>)
&ge; P
</p>
<p>(
w &isin;G(&minus;3ε)
</p>
<p>)
&minus;
</p>
<p>c(L03 +N3)
ε3
</p>
<p>.
</p>
<p>Setting
</p>
<p>P
(
w &isin;G(ε)
</p>
<p>)
&minus; P(w &isin;G)=W
</p>
<p>(
G(ε)
</p>
<p>)
&minus;W(G)=:WG(ε)
</p>
<p>and taking into account that N3 &le; cL3 and L03 &le; L3 +N3, we will obtain that
</p>
<p>&minus;WG(&minus;3ε)+
cL3
</p>
<p>ε3
&le; P(sn &isin;G)&minus;W(G)&le;WG(3ε)+
</p>
<p>cL3
</p>
<p>ε3
. (20.1.17)
</p>
<p>If W(&part;G)= 0 then clearly
W
(
G(3ε)
</p>
<p>)
&minus;W
</p>
<p>(
G(&minus;3ε)
</p>
<p>)
&rarr; 0
</p>
<p>as ε&rarr; 0, and WG(&plusmn;3ε)&rarr; 0. From this and (20.1.17) it is easy to derive that
P(sn &isin;G)&rarr; P(w &isin;G), n&rarr;&infin;.
</p>
<p>Convergence f (sn)'&rArr; f (w) for continuous functionals follows from (20.1.4),
since if v is a point of continuity of the distribution of f (w) then the set Gv = {x &isin;
C(0,1) : f (x) &lt; v} has the property
</p>
<p>W(&part;Gv)= P
(
f (w)= v
</p>
<p>)
= 0
</p>
<p>and therefore
</p>
<p>P
(
f (sn) &lt; v
</p>
<p>)
&rarr; P
</p>
<p>(
f (w) &isin; v
</p>
<p>)
.
</p>
<p>Theorem 20.1.1 is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Convergence to the Wiener Process 567
</p>
<p>Proof of Theorem 20.1.2 If G is a Lipschitz set, then
∣∣∆WG(&plusmn;3ε)
</p>
<p>∣∣&lt; cε,
and by (20.1.17)
</p>
<p>∣∣P(sn &isin;G)&minus;W(G)
∣∣&lt; c
</p>
<p>(
ε+ L3
</p>
<p>ε3
</p>
<p>)
.
</p>
<p>Putting ε := L1/43 we obtain the required assertion. Theorem 20.1.2 is proved. �
</p>
<p>The reason for the name &ldquo;invariance principle&rdquo; used to refer to the main asser-
tions of this section is best illustrated by Theorem 20.1.3. By virtue of the theorem,
</p>
<p>one can approximate the value of P(sn &isin; A) by P(s&prime;n &isin; A) for any other sequence
(20.1.6) having the same first two moments as (20.1.1). In that sense, the asymp-
</p>
<p>totics of P(sn &isin; A) are invariant with respect to particular distributions of the un-
derlying sequences with fixed first two moments. For example, the calculation of
</p>
<p>P(sn &isin; G) or P(w(t) &isin; G) can be replaced with that of P(s&prime;n &isin; G) for a Bernoulli
sequence, which is convenient for various numerical methods. On the other hand, the
</p>
<p>probabilities P(w &isin;G) for a whole class of regions G were found in explicit form
(see e.g. [32]). We know, for example, that P(supt&isin;[0,1]w(t) &gt; y) = 2(1 &minus;Φ(y)).
(This implies, in particular, that G = {x &isin; C(0,1) supt&isin;[0,1] x(t) &gt; y} is a Lips-
chitz set.) Hence for the distribution of the maximum Sn = maxk&le;n Sk of the sums
Sk =
</p>
<p>&sum;k
j=1 ξj , when Eξk = 0 and Var ξk = σ 2, we have
</p>
<p>P(Sn &gt; xσ
&radic;
n )&rarr; 2
</p>
<p>(
1 &minus;Φ(x)
</p>
<p>)
, n&rarr;&infin;,
</p>
<p>and one can use this relation for the approximate calculation of the distribution of
</p>
<p>Sn which is, as we saw in Chap. 12, of substantial interest in applications.
</p>
<p>In the same way we can approximate the joint distribution of Sn, Sn, and Sn :=
mink&le;n Sk (i.e. the probabilities of the form P(Sn &lt; x
</p>
<p>&radic;
n, Sn &gt; y
</p>
<p>&radic;
n, Sn &isin; B))
</p>
<p>using the respective formulas for the Wiener process given in Skorokhod (1991).
</p>
<p>Remark 20.1.2 In conclusion of this section note that all the above assertions will
remain true if, instead of sn(t), we consider in them the step function s
</p>
<p>&lowast;
n(t) = ζk,n
</p>
<p>for t &isin; [tk, tk+1). One can verify this by repeating anew all the arguments for s&lowast;.
Another way to obtain, say, Theorems 20.1.1 and 20.1.2 for s&lowast;n is to make use of the
already obtained results and bound the distance ρ(sn, s
</p>
<p>&lowast;
n). Because
</p>
<p>{
ρ
(
sn, s
</p>
<p>&lowast;
n
</p>
<p>)
&gt; ε
</p>
<p>}
&sub;
</p>
<p>n⋃
</p>
<p>k=1
</p>
<p>{
|ξk,n|&gt; ε
</p>
<p>}
,
</p>
<p>one has
</p>
<p>P
(
ρ
(
sn, s
</p>
<p>&lowast;
n
</p>
<p>)
&gt; ε
</p>
<p>)
&le;
</p>
<p>n&sum;
</p>
<p>k=1
P
(
|ξk,n|&gt; ε
</p>
<p>)
&le;
</p>
<p>n&sum;
</p>
<p>k=1
</p>
<p>&micro;k,n
</p>
<p>ε3
= L3
</p>
<p>ε3
.
</p>
<p>Recall that a similar bound was obtained for ρ(s&prime;n,w), and this allowed us to
replace, where it was needed, the process s&prime;n with w. Therefore, using the same</p>
<p/>
</div>
<div class="page"><p/>
<p>568 20 Functional Limit Theorems
</p>
<p>argument, one can replace sn with s
&lowast;
n . In that case, we can consider convergence
</p>
<p>of the distributions of functionals f (s&lowast;n) defined on D(0,1) (and continuous in the
uniform metric ρ). Sometimes the use of s&lowast;n is more convenient than that of sn. This
is the case, for example, when one has to find the limiting distribution of
</p>
<p>n&sum;
</p>
<p>k=1
g(ζk,n)= n
</p>
<p>&int;
g
(
s&lowast;n(t)
</p>
<p>)
dt
</p>
<p>(ξk,n are identically distributed). It follows from the above representation that
</p>
<p>1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>k=1
g(ζk,n)'&rArr;
</p>
<p>&int;
g
(
w(t)
</p>
<p>)
dt, n&rarr;&infin;.
</p>
<p>20.2 The Law of the Iterated Logarithm
</p>
<p>Let ξ1, ξ2, . . . be a sequence of independent random variables,
</p>
<p>Eξk = 0, Eξ2k = σ 2k , E|ξk|3 = &micro;k,
</p>
<p>Sn =
n&sum;
</p>
<p>k=1
ξk, B
</p>
<p>2
n =
</p>
<p>n&sum;
</p>
<p>k=1
σ 2k , Mn =
</p>
<p>n&sum;
</p>
<p>k=1
&micro;k.
</p>
<p>In this notation, the Lyapunov ratio is equal to
</p>
<p>L3 = L3,n =
Mn
</p>
<p>B3n
.
</p>
<p>In the present section, we will show that the law of the iterated logarithm for the
</p>
<p>Wiener process and Theorem 20.1.2 imply the following.
</p>
<p>Theorem 20.2.1 (The law of the iterated logarithm for sums of random variables)
</p>
<p>If Bn &rarr;&infin; as n&rarr;&infin; and L3,n &lt; c/ lnBn for some c &lt;&infin;, then
</p>
<p>P
</p>
<p>(
lim
n&rarr;&infin;
</p>
<p>Sn
</p>
<p>Bn
&radic;
</p>
<p>2 ln lnBn
= 1
</p>
<p>)
= 1, (20.2.1)
</p>
<p>P
</p>
<p>(
lim
n&rarr;&infin;
</p>
<p>Sn
</p>
<p>Bn
&radic;
</p>
<p>2 ln lnBn
=&minus;1
</p>
<p>)
= 1. (20.2.2)
</p>
<p>Thus all the sequences which lie above
</p>
<p>(1 + ε)Bn
&radic;
</p>
<p>2 ln lnBn
</p>
<p>will be upper for the sequence of sums Sn, while all the sequences below
</p>
<p>(1 &minus; ε)Bn
&radic;
</p>
<p>2 ln lnBn
</p>
<p>will be lower.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 The Law of the Iterated Logarithm 569
</p>
<p>The conditions of the theorem will clearly be satisfied for identically distributed
</p>
<p>ξk , for in that case B
2
n = σ 21 n, L3,n = &micro;1/(σ 31
</p>
<p>&radic;
n ).
</p>
<p>Proof We turn to the proof of the law of the iterated logarithm in Theorem 19.3.2
and apply it to the sequence Sn. We will not need to introduce any essential changes.
</p>
<p>One just has to consider Snk instead of w(a
k), where nk = min{n : B2n &ge; ak}, and
</p>
<p>replace ak with B2nk where it is needed. By the Lyapunov condition, maxk&le;n σ
2
k =
</p>
<p>o(B2n), so that B
2
nk&minus;1 &sim; B
</p>
<p>2
nk
</p>
<p>&sim; ak as k&rarr;&infin;.
The key point in the proof of Theorem 19.3.2 is the proof of convergence (for
</p>
<p>any ε &gt; 0) of the series
</p>
<p>&sum;
</p>
<p>k
</p>
<p>P
(
</p>
<p>sup
u&le;ak
</p>
<p>w(u) &gt; (1 + ε)xk&minus;1
)
</p>
<p>(20.2.3)
</p>
<p>and divergence of the series
</p>
<p>&sum;
</p>
<p>k
</p>
<p>P
</p>
<p>(
w
(
ak
)
&minus;w
</p>
<p>(
ak&minus;1
</p>
<p>)
&gt;
</p>
<p>(
1 &minus; ε
</p>
<p>2
</p>
<p>)
xk
</p>
<p>)
, (20.2.4)
</p>
<p>where
</p>
<p>xk =
&radic;
</p>
<p>2ak ln lnak, w
(
ak
)
&minus;w
</p>
<p>(
ak&minus;1
</p>
<p>) p=w
(
ak
(
1 &minus; a&minus;1
</p>
<p>))
.
</p>
<p>In our case, if one follows the same argument, one has to prove the convergence of
</p>
<p>the series
&sum;
</p>
<p>k
</p>
<p>P
(
Snk &gt; (1 + ε) yk&minus;1
</p>
<p>)
(20.2.5)
</p>
<p>and divergence of the series
</p>
<p>&sum;
</p>
<p>k
</p>
<p>P
</p>
<p>(
Snk &minus; Snk&minus;1 &gt;
</p>
<p>(
1 &minus; ε
</p>
<p>2
</p>
<p>)
yk
</p>
<p>)
, (20.2.6)
</p>
<p>where yk =
&radic;
</p>
<p>2B2nk ln lnB
2
nk
</p>
<p>&sim; xk . But the asymptotic behaviour of the probabilities
of the events in (20.2.3), (20.2.5) and (20.2.4), (20.2.6) under the conditions L3,n &lt;
</p>
<p>c/ lnBn will essentially be the same. To establish this, we will make use of the
</p>
<p>inequality
∣∣∣∣P
(
sn
</p>
<p>Bn
&isin;G
</p>
<p>)
&minus; P
</p>
<p>(
w &isin;G(&plusmn;δ)
</p>
<p>)∣∣∣∣&lt;
cL3,n
</p>
<p>δ3
, (20.2.7)
</p>
<p>which follows from the proof of Theorem 20.1.3. By this inequality,
</p>
<p>P
</p>
<p>(
Sn
</p>
<p>Bn
&gt; (1 + 3ε)x
</p>
<p>)
&le; P
</p>
<p>(
sup
u&le;1
</p>
<p>w(u) &gt; (1 + 2ε) x
)
+ cL3,n
</p>
<p>(εx)3
</p>
<p>= P
(
</p>
<p>sup
u&le;B2n
</p>
<p>w(u) &gt; (1 + 2ε)xBn
)
+ cL3,n
</p>
<p>(εx)3
.</p>
<p/>
</div>
<div class="page"><p/>
<p>570 20 Functional Limit Theorems
</p>
<p>Therefore (see (20.2.5)), putting n := nk and x := yk&minus;1/Bn, we obtain
</p>
<p>P
(
Snk &gt; (1 + 3ε)yk&minus;1
</p>
<p>)
&le; P
</p>
<p>(
sup
</p>
<p>u&le;B2nk
</p>
<p>w(u) &gt; (1 + 2ε) yk&minus;1
)
+ cL3,nk
</p>
<p>ε3(ln lnB2nk )
3/2
</p>
<p>.
</p>
<p>Here
</p>
<p>yk&minus;1 &sim; xk&minus;1, B2nk &ge; a
k, ln lnB2nk &sim; ln lna
</p>
<p>k &sim; lnk,
</p>
<p>L3,nk &le;
c
</p>
<p>lnBnk
&sim; c
</p>
<p>lnak
&sim; c1
</p>
<p>k
.
</p>
<p>Consequently, for all sufficiently large k (recall that the letter c denotes different
</p>
<p>constants),
</p>
<p>P
(
Snk &gt; (1 + 3ε)yk&minus;1
</p>
<p>)
&le; P
</p>
<p>(
sup
u&le;ak
</p>
<p>w(u) &gt; (1 + ε)xk&minus;1
)
+ c
</p>
<p>ε3k(ln k)3/2
.
</p>
<p>Since
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>k(ln k)3/2
&lt;&infin;, (20.2.8)
</p>
<p>the above inequality means that the convergence of series (20.2.3) implies that of se-
</p>
<p>ries (20.2.5). The first part of the theorem is proved.
</p>
<p>The second part is proved in a similar way. Consider series (20.2.6). By (20.2.7),
</p>
<p>P
(
Snk &minus; Snk&minus;1 &gt; (1 &minus; 3ε)yk
</p>
<p>)
</p>
<p>= P
(
snk (1)&minus; snk (rk) &gt; (1 &minus; 3ε)
</p>
<p>yk
</p>
<p>Bnk
</p>
<p>)
</p>
<p>&ge; P
(
w(1)&minus;w(rk) &gt; (1 &minus; 2ε)
</p>
<p>yk
</p>
<p>Bnk
</p>
<p>)
&minus; cL3,nk
</p>
<p>ε3
</p>
<p>(
ln lnB2nk
</p>
<p>)&minus;3/2
, (20.2.9)
</p>
<p>where rk = B2nk&minus;1B
&minus;2
nk
</p>
<p>&rarr; a&minus;1 due to the fact that
</p>
<p>B2nk = a
k + θkσ 2nk , 0 &le; θk &le; 1, σ
</p>
<p>2
nk
</p>
<p>= o
(
B2nk
</p>
<p>)
.
</p>
<p>The first term on the right-hand side of (20.2.9) is equal to
</p>
<p>P
</p>
<p>(
w(1 &minus; rk) &gt; (1 &minus; 2ε)
</p>
<p>yk
</p>
<p>Bnk
</p>
<p>)
&ge; P
</p>
<p>(
w
(
ak(1 &minus; rk)
</p>
<p>)
&gt; (1 &minus; ε)xk
</p>
<p>)
</p>
<p>= P
(
w
(
ak
(
1 &minus; a&minus;1
</p>
<p>))
&gt; (1 &minus; ε)xk
</p>
<p>&radic;
1 &minus; a&minus;1
1 &minus; rk
</p>
<p>)
</p>
<p>&ge; P
(
w
(
ak
)
&minus;w
</p>
<p>(
ak&minus;1
</p>
<p>)
&gt;
</p>
<p>(
1 &minus; ε
</p>
<p>2
</p>
<p>)
xk
</p>
<p>)
.
</p>
<p>As before, the series consisting of the second terms on the right-hand side of (20.2.9)
</p>
<p>converges by virtue of (20.2.8). Therefore the established inequalities mean that
</p>
<p>the divergence of series (20.2.4) implies that of series (20.2.6). The theorem is
</p>
<p>proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 The Law of the Iterated Logarithm 571
</p>
<p>Now we will present an example that we need to complete the proof in Re-
</p>
<p>mark 4.4.1.
</p>
<p>Example 20.2.1 Let ζk be independent and identically distributed, Eζk = 0,
Eζ 2k = 1, E|ζk|3 = &micro; &lt; &infin; and ξk =
</p>
<p>&radic;
2k ζk . Here we have B
</p>
<p>2
n = n2(1 + 1/n).
</p>
<p>In Remark 4.4.1 we used the assertion that (in a somewhat different notation)
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>n=1
{Sn &lt;&minus;n}
</p>
<p>)
= 1
</p>
<p>or, which is the same (as the sign of Sn is inessential),
</p>
<p>P
</p>
<p>( &infin;⋃
</p>
<p>n=1
{Sn &gt; n}
</p>
<p>)
= P
</p>
<p>( &infin;⋃
</p>
<p>n=1
</p>
<p>{
Sn &gt;Bn
</p>
<p>(
1 +O
</p>
<p>(
1
</p>
<p>n
</p>
<p>))})
= 1. (20.2.10)
</p>
<p>To verify it, we will show that any sequence of the form B &prime;n = Bn(1+O(1/n)) is
lower for {Sk}. In our case,
</p>
<p>Mn =
n&sum;
</p>
<p>k=1
(2k)3/2&micro;&sim; cn5/2, L3.n =
</p>
<p>Mn
</p>
<p>B3n
&sim; cn&minus;1/2 ≪ 1
</p>
<p>lnBn
.
</p>
<p>This means that the conditions of Theorem 20.2.1 are met, and hence any sequence
</p>
<p>which lies lower than (1&minus;ε)n
&radic;
</p>
<p>2 ln lnn (in particular, the sequence B &prime;n = n) is lower
for {Sk}. This proves (20.2.10). �
</p>
<p>Let us return to Theorem 20.2.1. As we saw in Sect. 19.3, the proof of the law
</p>
<p>of the iterated logarithm is based on the asymptotics (the rate of decrease) of the
</p>
<p>function 1 &minus;Φ(x) as x &rarr;&infin;. Therefore, the conditions for the law of the iterated
logarithm for the sums Sn are related to the width of the range of x values for which
</p>
<p>the probabilities
</p>
<p>Pn&plusmn;(x) := P
(
&plusmn; Sn
Bn
</p>
<p>&gt; x
</p>
<p>)
</p>
<p>are approximated by the normal law (i.e. by the function 1 &minus; Φ(x)). Here we en-
counter the problem of large deviations (see Chap. 9). If
</p>
<p>Pn&plusmn;(x)
</p>
<p>1 &minus;Φ(x) &rarr; 1 (20.2.11)
</p>
<p>as n&rarr;&infin; for all
x &le;
</p>
<p>&radic;
2 ln lnBn(1 &minus; ε) (20.2.12)
</p>
<p>and some ε &gt; 0 then the proof of the law of the iterated logarithm for the Wiener
</p>
<p>process given in Sect. 19.3 can easily be extended to the sums Sn/Bn (to estimate
</p>
<p>P(Sn/Bn &gt; x) one has to use the Kolmogorov inequality; see Corollary 11.2.1).
</p>
<p>One way to establish (20.2.11) and (20.2.12) is to use estimates for the rate
</p>
<p>of convergence in the central limit theorem. This approach was employed in the
</p>
<p>proof of Theorem 20.2.1, where we used Theorem 20.1.3. However, to ensure that</p>
<p/>
</div>
<div class="page"><p/>
<p>572 20 Functional Limit Theorems
</p>
<p>(20.2.11) and (20.2.12) hold one can use weaker assertions than Theorem 20.1.3. To
</p>
<p>some extent, this fact is illustrated by the following assertion (see [32]):
</p>
<p>Theorem 20.2.2 If Bn &rarr;&infin; and Bn+1/Bn &rarr; 1 as n&rarr;&infin;, and
</p>
<p>sup
x
</p>
<p>∣∣∣∣P
(
Sn
</p>
<p>Bn
&lt; x
</p>
<p>)
&minus;Φ(x)
</p>
<p>∣∣∣∣&le; c(lnBn)
&minus;1&minus;δ
</p>
<p>for some δ &gt; 0 and c &lt;&infin;, then the law of the iterated logarithm holds.
</p>
<p>If ξk
d= ξ are identically distributed then Theorem 20.2.1 implies that the law of
</p>
<p>the iterated logarithm is valid whenever E|ξ |3 exists. In fact, however, for identically
distributed ξk , the law of the iterated logarithm always holds in the case of a finite
</p>
<p>second moment, without any additional conditions.
</p>
<p>Theorem 20.2.3 (Hartman&ndash;Wintner, [32]) If the ξk are identically distributed,
Eξk = 0, and Eξ2k = 1, then (20.2.1) and (20.2.2) hold with B2n replaced with n.
Every point from the segment [&minus;1,1] is a limiting one for the sequence
</p>
<p>Sn&radic;
2n ln lnn
</p>
<p>, n&ge; 1.
</p>
<p>The last assertion of the theorem means that, for each t &isin; [&minus;1,1] and any ε &gt; 0,
the interval (t &minus; ε, t + ε) contains, with probability 1, infinitely many elements of
the sequence
</p>
<p>Sn&radic;
2n ln lnn
</p>
<p>.
</p>
<p>20.3 Convergence to the Poisson Process
</p>
<p>20.3.1 Convergence of the Processes of Cumulative Sums
</p>
<p>The theorems of Sects. 20.1 and 20.2 show that the Wiener process describes rather
</p>
<p>well the evolution of the cumulative sums when summing &ldquo;conventional&rdquo; random
</p>
<p>variables ξk,n satisfying the Lyapunov condition. It turns out that the Poisson process
</p>
<p>describes in a similar way the evolution of the cumulative sums when the random
</p>
<p>variables ξk,n correspond to the occurrence of rare events.
</p>
<p>As in Sect. 5.4, first we will not consider the triangular array scheme, but obtain
</p>
<p>precise inequalities describing the proximity of the processes under study. Consider
</p>
<p>independent random variables ξ1, . . . , ξn with Bernoulli distributions:
</p>
<p>P(ξk = 1)= pk, P(ξk = 0)= 1 &minus; pk,
n&sum;
</p>
<p>k=1
pk = &micro;.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Convergence to the Poisson Process 573
</p>
<p>We will assume that p := maxk&le;n pk is small and the number &micro; is &ldquo;comparable
with 1&rdquo;. Put
</p>
<p>q0 := 0, qk :=
pk
</p>
<p>&micro;
, Qk :=
</p>
<p>k&sum;
</p>
<p>j=0
qj , k &ge; 0,
</p>
<p>and form a random function sn(t) on [0,1] in the following way. Put sn(0) := 0,
</p>
<p>sn(t) := Sk =
k&sum;
</p>
<p>j=1
ξj for t &isin; (Qk&minus;1,Qk], k = 1, . . . , n.
</p>
<p>Here it is more convenient to use a step function rather than a continuous trajectory
</p>
<p>sn(t) (cf. Remark 20.1.2). The assertions to be obtained in this section are similar to
</p>
<p>the invariance principle and state that the process sn(t) converges in a certain sense
</p>
<p>to the Poisson process ξ(t) with intensity &micro; on [0,1]. This convergence could of
course be treated as weak convergence of distributions in the metric space D(0,1).
</p>
<p>But in the framework of the present book, it is apparently inexpedient for at least
</p>
<p>two reasons:
</p>
<p>1. To do that, we would have to introduce a metric in D(0,1) and study its prop-
</p>
<p>erties, which is somewhat complicated by itself.
</p>
<p>2. The trajectories of the processes sn(t) and ξ(t) are of a simple form, and
</p>
<p>characterising their closeness can be done in a simpler and more precise way without
</p>
<p>using more general concepts. Indeed, as we saw, the trajectory of ξ(t) on [0,1] is
completely determined by the collection of random variables (π(1); T1, . . . , Tπ(1)),
where Tk is the epoch of the k-th jump of the process, Tk+1 &minus; Tk &sub;= Ŵ&micro;. A similar
characterisation is valid for the trajectories of sn(t): they are determined by the
</p>
<p>vector (sn(1), θ1, . . . , θsn(1)), where θk =Qγk , γ1, γ2, . . . are the values j for which
ξj = 1. We will say that the distributions of sn(t) and π(t) are close to each other if
the distributions of the above vectors are close. This convention will correspond to
</p>
<p>convergence of the processes in a rather strong and natural sense.
</p>
<p>It is not hard to see from what we said before about the Poisson processes (see
</p>
<p>Sect. 19.4) that the introduced convergence of the distributions of the jump points of
</p>
<p>the process sn(t) is equivalent to convergence of the finite-dimensional distributions
</p>
<p>of sn(t) to those of π(t) (we know that the trajectories of sn(t) are step functions).
</p>
<p>Theorem 20.3.1 The processes sn(t) and π(t) can be constructed on a common
probability space so that
</p>
<p>P
(
sn(1)= π(1); θk &minus; qγk &le; Tk &lt; θk, k = 1, . . . , π(1)
</p>
<p>)
&ge; 1 &minus;
</p>
<p>n&sum;
</p>
<p>j=1
p2j .
</p>
<p>(20.3.1)
</p>
<p>Since
&sum;n
</p>
<p>j=1 p
2
j &le; &micro;p, the smallness of p means that, with probability close to 1,
</p>
<p>the values of sn(1) and π(1) coincide (cf. Theorem 5.4.2) and the positions of the
</p>
<p>respective points of jumps of the processes sn(t) and π(t) do not differ much.</p>
<p/>
</div>
<div class="page"><p/>
<p>574 20 Functional Limit Theorems
</p>
<p>Put q = p/&micro; and, for a fixed k &ge; 1, denote by B(ε) the ε-neighbourhood of the
orthant set B := {(x1, . . . , xk) : xj &lt; vj , j &le; k} for some vj &gt; 0. Theorem 20.3.1
implies the following.
</p>
<p>Corollary 20.3.1 For any k = 1, . . . , n,
</p>
<p>P
(
sn(1)= k, (θ1, . . . , θk) &isin; B
</p>
<p>)
&le; P
</p>
<p>(
π(1)= k, (T1, . . . , Tk) &isin; B
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>j=1
p2j ;
</p>
<p>P
(
π(1)= k, (T1, . . . , Tk) &isin; B
</p>
<p>)
&le; P
</p>
<p>(
sn(1)= k, (θ1, . . . , θk) &isin; B(q)
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>j=1
p2j .
</p>
<p>Proof Let An denote the event appearing on the left-hand side of (20.3.1),
</p>
<p>Dn :=
{
sn(1)= k, (θ1, . . . , θk) &isin; B
</p>
<p>}
,
</p>
<p>Cn :=
{
π(1)= k, (T1, . . . , Tk) &isin; B
</p>
<p>}
.
</p>
<p>Then, by virtue of (20.3.1),
</p>
<p>P(Dn)&le; P(DnAn)+
n&sum;
</p>
<p>j=1
p2j
</p>
<p>&le; P
(
Dn,π(1)= k, (T1, . . . , Tk) &isin; B
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>j=1
p2j
</p>
<p>&le; P
(
π(1)= k, (T1, . . . , Tk) &isin; B
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>j=1
p2j .
</p>
<p>The converse inequality is established similarly. The corollary is proved. �
</p>
<p>Proof of Theorem 20.3.1 Let ηk := π(Qk)&minus; π(Qk&minus;1), k = 1, . . . , n. The theorem
will be proved if we construct {ξk} and {ηk} on a common probability space so that
</p>
<p>P
</p>
<p>(
n⋃
</p>
<p>k=1
{ξk 
= ηk}
</p>
<p>)
&le;
</p>
<p>n&sum;
</p>
<p>j=1
p2j . (20.3.2)
</p>
<p>A construction leading to (20.3.2) has essentially already been used in Theo-
</p>
<p>rem 5.4.2. The required construction will be obtained if we consider independent
</p>
<p>random variables ω1, . . . ,ωn; ωk &sub;=U0,1, and put
</p>
<p>ξk :=
{
</p>
<p>0 if ωk &lt; 1 &minus; pk,
1 if ωk &ge; 1 &minus; pk,
</p>
<p>ηk :=
{
</p>
<p>0 if ωk &lt; e
&minus;pk =: π0,k,
</p>
<p>j &ge; 1 if ωk &isin; [πj&minus;1,k,πj,k),
</p>
<p>where πj,k =�pk ([0, j)), j = 0,1, . . . . Then ηk &sub;=�pk ,
&sum;n
</p>
<p>k=1 ηk &sub;=�&micro;,
</p>
<p>{ξk 
= ηk} =
{
ωk &isin; [1 &minus; pk, e&minus;pk )&cup;
</p>
<p>[
e&minus;pk + pke&minus;pk ,1
</p>
<p>]}
.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Convergence to the Poisson Process 575
</p>
<p>Therefore,
</p>
<p>P(ξk 
= ηk)&le; p2k
and we get (20.3.2). The theorem is proved. �
</p>
<p>If we now consider the triangular array scheme ξ1,n, . . . , ξn,n, for which
</p>
<p>P(ξk,n = 1)= pk,n, P(ξk,n = 0)= 1 &minus; pk,n,
n&sum;
</p>
<p>k=1
pk,n =: &micro;n &rarr; &micro;, pn = max
</p>
<p>k&le;n
pk,n &rarr; 0
</p>
<p>as n&rarr;&infin;, then Theorem 20.3.1 easily implies convergence of the finite-dimensional
distributions of the processes sn(t) to π(t), where sn(t) is constructed as before
and π(t) is the Poisson process with parameter &micro;. Consider, for example, the two-
</p>
<p>dimensional distributions P(sn(t) &ge; j, sn(1) = k) for t &isin; (0,1), j &le; k. In the no-
tation of Theorem 20.3.1 (to be precise, we have to add the subscript n where
</p>
<p>appropriate; e.g., the Poisson processes with parameters &micro;n and &micro; will be denoted
</p>
<p>by πn(t) and π(t), respectively), we obtain
</p>
<p>P
(
sn(t)&ge; j, sn(1)= k
</p>
<p>)
= P
</p>
<p>(
sn(1)= k, θj &lt; t
</p>
<p>)
.
</p>
<p>By Corollary 20.3.1 the right-hand side does not exceed
</p>
<p>P
(
πn(1)= k, Tj &lt; t
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>l=1
p2l,n,
</p>
<p>where, as is easy to see,
</p>
<p>P
(
πn(1)= k, Tj &lt; t
</p>
<p>)
= P
</p>
<p>(
πn(1)= k, πn(t)&ge; j
</p>
<p>)
</p>
<p>=
k&sum;
</p>
<p>l=j
P
(
πn(t)= l
</p>
<p>)
P
(
πn(1 &minus; t)= k &minus; l
</p>
<p>)
</p>
<p>&rarr; P
(
π(1)= k, π(t)&ge; j
</p>
<p>)
</p>
<p>as n&rarr;&infin;, so that
P
(
sn(t)&ge; j, sn(1)= k
</p>
<p>)
&le; P
</p>
<p>(
π(t)&ge; j, π(1)= k
</p>
<p>)
+ o(1).
</p>
<p>The converse inequality is established in a similar way (by using the convergence
</p>
<p>qn &rarr; 0 as n &rarr; &infin;). The required convergence of the finite-dimensional distribu-
tions is proved. �
</p>
<p>20.3.2 Convergence of Sums of Thinning Renewal Processes
</p>
<p>The Poisson process can appear as a limiting one in a somewhat different set-up&mdash;as
</p>
<p>a limit for the sum of a large number of homogeneous &ldquo;slow&rdquo; renewal processes.</p>
<p/>
</div>
<div class="page"><p/>
<p>576 20 Functional Limit Theorems
</p>
<p>We formulate the setting of the problem more precisely. Let ηi(t), i = 1,2, . . . , n,
be mutually independent arbitrary homogeneous renewal processes in the &ldquo;triangu-
lar array scheme&rdquo; (i.e. they depend on n) generated by sequences {τ (i)k }&infin;k=1 for which
(see Chap. 10; τ
</p>
<p>(i)
k
</p>
<p>d= τ (i) for k &ge; 2)
</p>
<p>Eηi(t)=
t
</p>
<p>ai
, ai := ai,n = Eτ (i) &rarr;&infin;,
</p>
<p>n&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>ai
&rarr; &micro;
</p>
<p>for a fixed &micro;, and
</p>
<p>Fi(t) := P
(
τ (i) &lt; t
</p>
<p>)
&le; rt,n &rarr; 0
</p>
<p>and for any fixed t as n&rarr;&infin;, where rt,n does not depend on i.
</p>
<p>Theorem 20.3.2 Under the above conditions, the finite-dimensional distributions
of the process
</p>
<p>ζn(t) :=
n&sum;
</p>
<p>i=1
ηi(t)
</p>
<p>converge as n&rarr;&infin; to those of the Poisson process π(t) with the parameter &micro;: for
any l &ge; 1, 0 &le; k1 &le; k2 &le; &middot; &middot; &middot; &le; kl ,
</p>
<p>P
(
ζn(t1)= k1, . . . , ζn(tl)= kl
</p>
<p>)
&rarr; P
</p>
<p>(
π(t1)= k1, . . . , π(tl)= kl
</p>
<p>)
.
</p>
<p>(On convergence to the Poisson process, see the remark preceding Theo-
</p>
<p>rem 20.3.1.)
</p>
<p>Proof First we will prove convergence of the distributions of the increments
</p>
<p>ζn(t + u)&minus; ζn(u)
to the Poisson distribution with parameter &micro;t . Put ∆i := ηi(t + u) &minus; ηi(u),
pi := t/ai . We have (χi(u) is the excess for the process ηi ; see Sects. 10.2, 10.4)
</p>
<p>E∆i = pi,
P(∆i &ge; l)&le; P
</p>
<p>(
χi(u) &lt; t
</p>
<p>)[
P
(
ξ
(1)
2
</p>
<p>)
&lt; t
</p>
<p>]l&minus;1
</p>
<p>&le; 1
ai
</p>
<p>&int; t
</p>
<p>0
</p>
<p>P
(
ξ
(1)
2 &gt; z
</p>
<p>)
dz &middot; Fi(t)l&minus;1 &le;
</p>
<p>t
</p>
<p>ai
(rt,n)
</p>
<p>l&minus;1 = pir l&minus;1t,n .
</p>
<p>This implies that
</p>
<p>E∆i = pi =
&infin;&sum;
</p>
<p>1
</p>
<p>lP(∆i = l)= P(δi = 1)+ o(pi),
</p>
<p>P(∆i = 1)= pi + o(pi), P(∆i = 0)= 1 &minus; pi + o(pi).
(20.3.3)
</p>
<p>Therefore the conditions of Corollary 5.4.2 are met, which implies that
</p>
<p>ζn(t + u)&minus; ζn(u)=
n&sum;
</p>
<p>i=1
∆i &sub;&rArr;�&micro;t . (20.3.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Convergence to the Poisson Process 577
</p>
<p>It remains to prove the asymptotic independence of the increments. For simplic-
</p>
<p>ity&rsquo;s sake, consider only two increments, on the intervals (u,0) and (u,u+ t), and
assume that ζn(u)= k. Moreover, suppose that the following event A occurred: the
renewals occurred in the processes with numbers i1, . . . , ik . It suffices to verify that,
</p>
<p>given this condition, (20.3.4) will still remain true. Let B be the event that there
</p>
<p>again were renewals on the interval (u,u + t) in the processes with the numbers
i1, . . . , ik . Evidently,
</p>
<p>P(B |A)&le;
k&sum;
</p>
<p>l=1
P
(
τ (il) &lt; t + u
</p>
<p>)
&le; krt+u,n &rarr; 0.
</p>
<p>Thus the contribution of the processes ηil , l = 1, . . . , k, to the sum (20.3.4) given
condition A is negligibly small. Consider the remaining n&minus; k processes. For them,
</p>
<p>P(∆i &ge; 1 |A)=
P(χi(0) &isin; (u,u+ t))
</p>
<p>P(χi(0) &gt; u)
</p>
<p>= 1
ai
</p>
<p>&int; u+t
</p>
<p>u
</p>
<p>(
1 &minus; Fi(z)
</p>
<p>)
da
</p>
<p>[
1 &minus; 1
</p>
<p>ai
</p>
<p>&int; u
</p>
<p>0
</p>
<p>(
1 &minus; Fi(z)
</p>
<p>)
dz
</p>
<p>]&minus;1
</p>
<p>= pi + o(pi). (20.3.5)
Since relation (20.3.3) remains true for conditional distributions of ∆i (given A
</p>
<p>and for i 
= il , l = 1, . . . , k), we obtain, similarly to the above argument (using now
instead of the equality
</p>
<p>&sum;&infin;
i=1 lP(∆i = l) = pi the relation
</p>
<p>&sum;&infin;
i=1 lP(∆i = l |A) =
</p>
<p>pi + o(pi) which follows from (20.3.5)) that
P(∆i = 1 |A)= pi + o(pi), P(∆i = 0 |A)= 1 &minus; pi + o(pi).
</p>
<p>It remains to once again make use of Corollary 5.4.2. �</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 21
</p>
<p>Markov Processes
</p>
<p>Abstract This chapter presents the fundamentals of the theory of general Markov
</p>
<p>processes in continuous time. Section 21.1 contains the definitions and a discus-
</p>
<p>sion of the Markov property and transition functions, and derives the Chapman&ndash;
</p>
<p>Kolmogorov equation. Section 21.2 studies Markov processes in countable state
</p>
<p>spaces, deriving systems of backward and forward differential equations for tran-
</p>
<p>sition probabilities. It also establishes the ergodic theorem and contains examples
</p>
<p>illustrating the presented theory. Section 21.3 deals with continuous time branch-
</p>
<p>ing processes. Then the elements of the general theory of semi-Markov processes
</p>
<p>are presented in Sect. 21.4, including the ergodic theorem and some other related
</p>
<p>results for such processes. Section 21.5 discusses the so-called regenerative pro-
</p>
<p>cesses, establishing their ergodicity and the Laws of Large Numbers and Central
</p>
<p>Limit Theorem for integrals of functions of their trajectories. Section 21.6 is devoted
</p>
<p>to diffusion processes. It begins with the classical definition of diffusion, derives the
</p>
<p>forward and backward Kolmogorov equations for the transition probability function
</p>
<p>of a diffusion process, and gives a couple of examples of using the equations to
</p>
<p>compute important characteristics of the respective processes.
</p>
<p>21.1 Definitions and General Properties
</p>
<p>Markov processes in discrete time (Markov chains) were considered in Chap. 13.
</p>
<p>Recall that their main property was independence of the &ldquo;future&rdquo; of the process of
</p>
<p>its &ldquo;past&rdquo; given its &ldquo;present&rdquo; is fixed. The same principle underlies the definition of
</p>
<p>Markov processes in the general case.
</p>
<p>21.1.1 Definition and Basic Properties
</p>
<p>Let 〈Ω,F,P〉 be a probability space and {ξ(t) = ξ(t,ω), t &ge; 0} a random process
given on it. Set
</p>
<p>F1 := σ
(
ξ(u); u&le; t
</p>
<p>)
, F[t,&infin;) := σ
</p>
<p>(
ξ(u); u&ge; t
</p>
<p>)
,
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_21, &copy; Springer-Verlag London 2013
</p>
<p>579</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_21">http://dx.doi.org/10.1007/978-1-4471-5201-9_21</a></div>
</div>
<div class="page"><p/>
<p>580 21 Markov Processes
</p>
<p>so that the variable ξ(u) is Ft -measurable for u&le; t and F[t,&infin;)-measurable for u&ge; t .
The σ -algebra σ(Ft ,F[t,&infin;)) is generated by the variables ξ(u) for all u and may
coincide with F in the case of the sample probability space.
</p>
<p>Definition 21.1.1 We say that ξ(t) is a Markov process if, for any t , A &isin; Ft , and
B &isin; F[t,&infin;), we have
</p>
<p>P
(
AB
</p>
<p>∣∣ξ(t)
)
= P
</p>
<p>(
A
∣∣ξ(t)
</p>
<p>)
P
(
B
∣∣ξ(t)
</p>
<p>)
. (21.1.1)
</p>
<p>This expresses precisely the fact that the future is independent of the past when the
</p>
<p>present is fixed (conditional independence of Ft and F[t,&infin;) given ξ(t)).
</p>
<p>We will now show that the above definition is equivalent to the following.
</p>
<p>Definition 21.1.2 We say that ξ(t) is a Markov process if, for any bounded F[t,&infin;)-
measurable random variable η,
</p>
<p>E(η|Ft )= E
(
η
∣∣ξ(t)
</p>
<p>)
. (21.1.2)
</p>
<p>It suffices to take η to be functions of the form η= f (ξ(s)) for s &ge; t .
</p>
<p>Proof of the equivalence Let (21.1.1) hold. By the monotone convergence theorem
it suffices to prove (21.1.2) for simple functions η. To this end it suffices, in turn,
</p>
<p>to prove (21.1.2) for η = IB , the indicator of the set B &isin; F[t,&infin;). Let A &isin; Ft . Then,
by (21.1.1),
</p>
<p>P(AB)= EP
(
AB
</p>
<p>∣∣ξ(t)
)
= E
</p>
<p>[
P
(
A
∣∣ξ(t)
</p>
<p>)
P
(
B
∣∣ξ(t)
</p>
<p>)]
</p>
<p>= EE
[
IAP
</p>
<p>(
B
∣∣ξ(t)
</p>
<p>)∣∣ξ(t)
]
= E
</p>
<p>[
IAP
</p>
<p>(
B
∣∣ξ(t)
</p>
<p>)]
. (21.1.3)
</p>
<p>On the other hand,
</p>
<p>P(AB)= E[IAIB ] = E
[
IAP(B|Ft )
</p>
<p>]
. (21.1.4)
</p>
<p>Because (21.1.3) and (21.1.4) hold for any A &isin; Ft , this means that P(B|Ft ) =
P(B|ξ(t)).
</p>
<p>Conversely, let (21.1.2) hold. Then, for A &isin; Ft and B &isin; F[t,&infin;), we have
</p>
<p>P
(
AB
</p>
<p>∣∣ξ(t)
)
= E
</p>
<p>[
E(IAIB |Ft )
</p>
<p>∣∣ξ(t)
]
= E
</p>
<p>[
IAE(IB |Ft )
</p>
<p>∣∣ξ(t)
]
</p>
<p>= E
[
IAE
</p>
<p>(
IB
∣∣ξ(t)
</p>
<p>)∣∣ξ(t)
]
= P
</p>
<p>(
B
∣∣ξ(t)
</p>
<p>)
P
(
A
∣∣ξ(t)
</p>
<p>)
. �
</p>
<p>It remains to verify that it suffices to take η= f (ξ(s)), s &ge; t , in (21.1.2). In order
to do this, we need one more equivalent definition of a Markov process.
</p>
<p>Definition 21.1.3 We say that ξ(t) is a Markov process if, for any bounded function
f and any t1 &lt; t2 &lt; &middot; &middot; &middot;&lt; tn &le; t ,
</p>
<p>E
(
f
(
ξ(t)
</p>
<p>)∣∣ξ(t1), . . . , ξ(tn)
)
= E
</p>
<p>(
f
(
ξ(t)
</p>
<p>∣∣ξ(tn)
))
. (21.1.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.1 Definitions and General Properties 581
</p>
<p>Proof of the equivalence Relation (21.1.5) follows in an obvious way from (21.1.2).
Now assume that (21.1.5) holds. Then, for any A &isin; σ(ξ(t1), . . . , ξ(tn)),
</p>
<p>E
(
f
(
ξ(t)
</p>
<p>)
; A
</p>
<p>)
= E
</p>
<p>[
E
(
f
(
ξ(t)
</p>
<p>)∣∣ξ(tn)
)
; A
</p>
<p>]
. (21.1.6)
</p>
<p>Both parts of (21.1.6) are measures coinciding on the algebra of cylinder sets. There-
</p>
<p>fore, by the theorem on uniqueness of extension of a measure, they coincide on the
</p>
<p>σ -algebra generated by these sets, i.e. on Ftn . In other words, (21.1.6) holds for any
</p>
<p>A &isin; Ftn , which is equivalent to the equality
E
[
f
(
ξ(t)
</p>
<p>)∣∣Ftn
]
= E
</p>
<p>[
f
(
ξ(t)
</p>
<p>)∣∣ξ(tn)
]
</p>
<p>for any tn &le; t . Relation (21.1.2) for η= f (ξ(t)) is proved. �
</p>
<p>We now prove that in (21.1.2) it suffices to take η= f (ξ(s)), s &ge; t . Let t &le; u1 &lt;
&middot; &middot; &middot;&lt; un. We prove that then (21.1.2) is true for
</p>
<p>η=
n&prod;
</p>
<p>i=1
fi
(
ξ(ui)
</p>
<p>)
. (21.1.7)
</p>
<p>We will make use of induction and assume that equality (21.1.2) holds for the
</p>
<p>functions
</p>
<p>γ =
n&minus;1&prod;
</p>
<p>i=1
fi
(
ξ(ui)
</p>
<p>)
</p>
<p>(for n= 1 relation (21.1.2) is true). Then, putting g(un&minus;1) := E[fn(ξ(un))|ξ(un&minus;1)],
we obtain
</p>
<p>E(η|Ft )= E
[
E(η|Fun&minus;1)
</p>
<p>∣∣Ft
]
= E
</p>
<p>[
γE
</p>
<p>(
fn
</p>
<p>(
ξ(un)
</p>
<p>)∣∣Fun&minus;1
)∣∣Ft
</p>
<p>]
</p>
<p>= E
[
γE
</p>
<p>(
fn
(
ξ(un)
</p>
<p>)∣∣ξ(un&minus;1)
)∣∣Ft
</p>
<p>]
= E
</p>
<p>[
γg
</p>
<p>(
ξ(un&minus;1)
</p>
<p>)∣∣Ft
]
.
</p>
<p>By the induction hypothesis this implies that
</p>
<p>E(η|Ft )= E
[
γg
</p>
<p>(
ξ(un&minus;1)
</p>
<p>)∣∣ξ(t)
]
</p>
<p>and, therefore, that E(η|Ft ) is σ(ξ(t))-measurable and
E
(
η
∣∣ξ(t)
</p>
<p>)
= E
</p>
<p>(
E(η|Ft )
</p>
<p>∣∣ξ(t)
)
= E(η|Ft ).
</p>
<p>We proved that (21.1.2) holds for σ(ξ(u1), . . . , ξ(un))-measurable functions of
</p>
<p>the form (21.1.7). By passing to the limit we establish first that (21.1.2) holds for
</p>
<p>simple functions, and then that it holds for any F[t,&infin;)-measurable functions. �
</p>
<p>21.1.2 Transition Probability
</p>
<p>We saw that, for a Markov process ξ(t), the conditional probability
</p>
<p>P
(
ξ(t) &isin; B
</p>
<p>∣∣Fs
)
= P
</p>
<p>(
ξ(t) &isin; B
</p>
<p>∣∣ξ(s)
)
</p>
<p>for t &gt; s</p>
<p/>
</div>
<div class="page"><p/>
<p>582 21 Markov Processes
</p>
<p>is a Borel function of ξ(s) which we will denote by
</p>
<p>P
(
s, ξ(s); t,B
</p>
<p>)
:= P
</p>
<p>(
ξ(t) &isin; B
</p>
<p>∣∣ξ(s)
)
.
</p>
<p>One can say that P(s, x; t,B) as a function of B and x is the conditional distribution
(see Sect. 4.9) of ξ(t) given that ξ(s) = x. By the Markov property, it satisfies the
relation (s &lt; u &lt; t)
</p>
<p>P (s, x; t,B)=
&int;
</p>
<p>P(s, x;u,dy)P (u, y; t,B), (21.1.8)
</p>
<p>which follows from the equality
</p>
<p>P
(
ξ(t) &isin; B
</p>
<p>∣∣ξ(s)= x
)
</p>
<p>= E
[
P
(
ξ(t) &isin; B
</p>
<p>∣∣Fu
)∣∣ξ(s)= x
</p>
<p>]
= E
</p>
<p>[
P
(
u, ξ(u); t,B
</p>
<p>)∣∣ξ(s)= x
]
.
</p>
<p>Equation (21.1.8) is called the Chapman&ndash;Kolmogorov equation.
The function P(s, x; t,B) can be used in an analytic definition of a Markov pro-
</p>
<p>cess. First we need to clarify what properties a function Px,B(s, t) should possess in
</p>
<p>order that there exists a Markov process ξ(t) for which
</p>
<p>Px,B(s, t)= P(s, x; t,B).
</p>
<p>Let 〈X,BX〉 be a measurable space.
</p>
<p>Definition 21.1.4 A function Px,B(s, t) is said to be a transition function on
〈X,BX〉 if it satisfies the following conditions:
</p>
<p>(1) As a function of B , Px,B(s, t) is a probability distribution for each s &le; t , x &isin;X.
(2) Px,B(s, t) is measurable in x for each s &le; t and B &isin;BX.
(3) For 0 &le; s &lt; u &lt; t and all x and B ,
</p>
<p>Px,B(s, t)=
&int;
</p>
<p>Px,dy(s, u)Py,B(u, t)
</p>
<p>(the Chapman&ndash;Kolmogorov equation).
</p>
<p>(4) Px,B(s, t)= IB(x) for s = t .
</p>
<p>Here properties (1) and (2) ensure that Px,B(s, t) can be a conditional distribution
</p>
<p>(cf. Sect. 4.9).
</p>
<p>Now define, with the help of Px,B(s, t), the finite-dimensional distributions of a
</p>
<p>process ξ(t) with the initial condition ξ(0)= a by the formula
</p>
<p>P
(
ξ(t) &isin; dy1, . . . , ξ(tn) &isin; dyn
</p>
<p>)
</p>
<p>= Pa,dy1(0, t1)Py1,dy2(t1, t2) &middot; &middot; &middot;Pyn&minus;1,dyn(tn&minus;1, tn). (21.1.9)
</p>
<p>By virtue of properties (3) and (4), these distributions are consistent and therefore
</p>
<p>by the Kolmogorov theorem define a process ξ(t) in 〈RT ,BTR〉, where T = [0,&infin;).</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Markov Processes with Countable State Spaces. Examples 583
</p>
<p>By formula (21.1.9) and rule (21.1.5),
</p>
<p>P
(
ξ(tn) &isin; Bn
</p>
<p>∣∣(ξ(t1), . . . , ξ(tn&minus;1)
)
= (y1, . . . , yn&minus;1)
</p>
<p>)
</p>
<p>= Pyn&minus;1,Bn(tn&minus;1, tn)= P
(
ξ(tn) &isin; Bn
</p>
<p>∣∣ξ(tn&minus;1)= yn&minus;1
)
</p>
<p>= P(tn&minus;1, yn&minus;1; tn,Bn).
We could also verify this equality in a more formal way using the fact that the
</p>
<p>integrals of both sides over the set {ξ(t1) &isin; B1, . . . , ξ(tn&minus;1) &isin; Bn&minus;1} coincide.
Thus, by virtue of Definition 21.1.3, we have constructed a Markov process ξ(t)
</p>
<p>for which
</p>
<p>P(s, x; t,B)= Px,B(s, t).
This function will also be called the transition function (or transition probability) of
the process ξ(t).
</p>
<p>Definition 21.1.5 A Markov process ξ(t) is said to be homogeneous if P(s, x; t,B),
as a function of s and t , depends on the difference t &minus; s only:
</p>
<p>P(s, x; t,B)= P(t &minus; s;x,B).
This is the probability of transition during a time interval of length t &minus; s from x
to B . If
</p>
<p>P(u; t,B)=
&int;
</p>
<p>B
</p>
<p>p(u; t, y) dy
</p>
<p>then the function p(u;x, y) is said to be a transition density.
</p>
<p>It is not hard to see that the Wiener and Poisson processes are both homogeneous
</p>
<p>Markov processes. For example, for the Wiener process,
</p>
<p>P(u;x, y)= 1&radic;
2πu
</p>
<p>e&minus;(x&minus;y)
2/2u.
</p>
<p>21.2 Markov Processes with Countable State Spaces. Examples
</p>
<p>21.2.1 Basic Properties of the Process
</p>
<p>Assume without loss of generality that the &ldquo;discrete state space&rdquo; X coincides with
</p>
<p>the set of integers {0,1,2, . . .}. For simplicity&rsquo;s sake we will only consider homo-
geneous Markov processes.
</p>
<p>The transition function of such a process is determined by the collection of
</p>
<p>functions P(t; i, j)= pij (t) which form a stochastic matrix P(t)= ‖pij (t)‖ (with
pij (t)&ge; 0,
</p>
<p>&sum;
j pij (t)= 1). Chapman&ndash;Kolmogorov&rsquo;s equation now takes the form
</p>
<p>pij (t + s)=
&sum;
</p>
<p>k
</p>
<p>pik(t)pkj (s),</p>
<p/>
</div>
<div class="page"><p/>
<p>584 21 Markov Processes
</p>
<p>or, which is the same, in the matrix form,
</p>
<p>P(t + s)= P(t)P (s)= P(s)P (t). (21.2.1)
In what follows, we consider only stochastically continuous processes for which
</p>
<p>ξ(t + s) p&rarr; ξ(t) as s &rarr; 0, which is equivalent in the case under consideration to
each of the following three relations:
</p>
<p>P
(
ξ(t + s) 
= ξ(t)
</p>
<p>)
&rarr; 0, P (t + s)&rarr; P(t), P (s)&rarr; P(0)&equiv;E (21.2.2)
</p>
<p>as s &rarr; 0 (component-wise; E is the unit matrix).
We will also assume that convergence in (21.2.2) is uniform (for a finite X this is
</p>
<p>always the case).
</p>
<p>According to the separability requirement, we will assume that ξ(t) cannot
</p>
<p>change its state in &ldquo;zero time&rdquo; more than once (thus excluding the effects illus-
</p>
<p>trated in Example 18.1.1, i.e. assuming that if ξ(t) = j then, with probability 1,
ξ(t + s)= j for s &isin; [0, τ ), τ = τ(ω) &gt; 0). In that case, the trajectories of the pro-
cesses will be piece-wise constant (right-continuous for definiteness), i.e. the time
</p>
<p>axis is divided into half-intervals [0, τ1), [τ1, τ1+τ2), . . . , on which ξ(t) is constant.
Put
</p>
<p>qj (t) := P
(
ξ(u)= j, 0 &le; u &lt; t
</p>
<p>∣∣ξ(0)= j
)
= P(τ1 &ge; t).
</p>
<p>Theorem 21.2.1 Under the above assumptions (stochastic continuity and separa-
bility),
</p>
<p>qi(t)= e&minus;qi t ,
where qi &lt;&infin;; moreover, qi &gt; 0 if pii(t) 
&equiv; 1. There exist the limits
</p>
<p>lim
t&rarr;0
</p>
<p>1 &minus; pii(t)
t
</p>
<p>= qi, lim
t&rarr;0
</p>
<p>pij (t)
</p>
<p>t
= qij , i 
= j, (21.2.3)
</p>
<p>where
&sum;
</p>
<p>j :j 
=i qij = qi .
</p>
<p>Proof By the Markov property,
</p>
<p>qi(t + s)= qi(t)qi(s),
and qi(t) &darr;. Therefore there exists a unique solution qi(t)= e&minus;qi t of this equation,
where qi &lt;&infin;, since P(τ1 &gt; 0)= 1 and qi &gt; 0, because qi(t) &lt; 1 when pii(t) 
&equiv; 1.
</p>
<p>Let further 0 &lt; t0 &lt; t1 &middot; &middot; &middot;&lt; tn &lt; t . Since the events
{
ξ(u)= i for u&le; tr , ξ(tr+1)= j
</p>
<p>}
, r = 0, . . . , n&minus; 1; j 
= i,
</p>
<p>are disjoint,
</p>
<p>pii(t)= qi(t)+
n&minus;1&sum;
</p>
<p>r=0
</p>
<p>&sum;
</p>
<p>j :j 
=i
qi(tr)pij (tr+1 &minus; tr)pji(t &minus; tr+1). (21.2.4)
</p>
<p>Here, by condition (21.2.2), pji(t &minus; tr+1) &lt; εt for all j 
= i, and εt &rarr; 0 as t &rarr; 0,
so that the sum in (21.2.4) does not exceed</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Markov Processes with Countable State Spaces. Examples 585
</p>
<p>εt
</p>
<p>n&minus;1&sum;
</p>
<p>r=0
</p>
<p>&sum;
</p>
<p>j :j 
=i
qi(tr )pij (tr+1 &minus; tr)= εtP
</p>
<p>(
n⋃
</p>
<p>r=1
</p>
<p>{
ξ(tr ) 
= i
</p>
<p>}∣∣ξ(0)= i
)
&lt; εt
</p>
<p>(
1 &minus; qi(t)
</p>
<p>)
,
</p>
<p>pii(t)&le; qi(t)+ εt
(
1 &minus; qi(t)
</p>
<p>)
.
</p>
<p>Together with the obvious inequality pii(t)&ge; qi(t) this gives
1 &minus; qi(t)&ge; 1 &minus; pii(t)&ge;
</p>
<p>(
1 &minus; qi(t)
</p>
<p>)
(1 + εt )
</p>
<p>(i.e. the asymptotic behaviour of 1 &minus; qi(t) and 1 &minus; pii(t) as t &rarr;&infin; is identical).
This implies the second assertion of the theorem (i.e., the first relation in (21.2.3)).
</p>
<p>Now let tr := rt/n. Consider the transition probabilities
</p>
<p>pij (t)&ge;
n&minus;1&sum;
</p>
<p>r=0
qi(tr )pij (t/n)qj (t &minus; tr+1)
</p>
<p>&ge; (1 &minus; εt )pij (t/n)
n&minus;1&sum;
</p>
<p>r=0
e&minus;qirt/n &ge; (1 &minus; εt )pij (t/n)
</p>
<p>(1 &minus; e&minus;qi t )n
qi t
</p>
<p>.
</p>
<p>This implies that
</p>
<p>pij (t)&ge; (1 &minus; εt )
(
</p>
<p>1 &minus; e&minus;qi t
qi
</p>
<p>)
lim sup
δ&rarr;0
</p>
<p>pij (δ)
</p>
<p>δ
,
</p>
<p>and that the upper limit on the right-hand side is bounded. Passing to the limit as
</p>
<p>t &rarr; 0, we obtain
</p>
<p>lim inf
t&rarr;0
</p>
<p>pij (t)
</p>
<p>t
&ge; lim sup
</p>
<p>δ&rarr;0
</p>
<p>pij (δ)
</p>
<p>δ
.
</p>
<p>Since
&sum;
</p>
<p>j :j 
=i pij (t) = 1 &minus; pii(t), we have
&sum;
</p>
<p>j :j 
=i qij = qi . The theorem is
proved. �
</p>
<p>The theorem shows that the quantities
</p>
<p>pij =
qij
</p>
<p>qi
, j 
= i, pii = 0
</p>
<p>form a stochastic matrix and give the probabilities of transition from i to j during
</p>
<p>an infinitesimal time interval ∆ given the process ξ(t) left the state i during that
</p>
<p>time interval:
</p>
<p>P
(
ξ(t +∆)= j
</p>
<p>∣∣ξ(t)= i, ξ(t +∆) 
= i
)
= pij (∆)
</p>
<p>1 &minus; pii(∆)
&rarr; qij
</p>
<p>qi
</p>
<p>as ∆&rarr; 0.
Thus the evolution of ξ(t) can be thought of as follows. If ξ(0)=X0, then ξ(t)
</p>
<p>stays at X0 for a random time τ1 &sub;= ŴqX0 . Then ξ(t) passes to a state X1 with prob-
ability pX0X1 . Further, ξ(t) = X1 over the time interval [τ1, τ1 + τ2), τ2 &sub;= ŴqX1 ,
after which the system changes its state to X2 and so on. It is clear that X0,X1, . . .
</p>
<p>is a homogeneous Markov chain with the transition matrix ‖pij‖. Therefore the</p>
<p/>
</div>
<div class="page"><p/>
<p>586 21 Markov Processes
</p>
<p>further study of ξ(t) can be reduced in many aspects to that of the Markov chain
</p>
<p>{Xn; n&ge; 0}, which was carried out in detail in Chap. 13.
We see that the evolution of ξ(t) is completely specified by the quantities qij and
</p>
<p>qi forming the matrix
</p>
<p>Q= ‖qij‖ = lim
t&rarr;0
</p>
<p>P(t)&minus; P(0)
t
</p>
<p>, (21.2.5)
</p>
<p>where we put qii := &minus;qi , so that
&sum;
</p>
<p>j qij = 0. We can also justify this claim using
an analytical approach. To simplify the technical side of the exposition, we will
</p>
<p>assume, where it is needed, that the entries of the matrix Q are bounded and con-
</p>
<p>vergence in (21.2.3) is uniform in i.
</p>
<p>Denote by eA the matrix-valued function
</p>
<p>eA =E +
&infin;&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>k!A
k.
</p>
<p>Theorem 21.2.2 The transition probabilities pij (t) satisfy the systems of differen-
tial equations
</p>
<p>P &prime;(t)= P(t)Q, (21.2.6)
P &prime;(t)=QP(t). (21.2.7)
</p>
<p>Each of the systems (21.2.6) and (21.2.7) has a unique solution
</p>
<p>P(t)= eQt .
</p>
<p>It is clear that the solution can be obtained immediately by formally integrating
</p>
<p>equation (21.2.6).
</p>
<p>Proof By virtue of (21.2.1), (21.2.2) and (21.2.5),
</p>
<p>P &prime;(t)= lim
s&rarr;0
</p>
<p>P(t + s)&minus; P(t)
s
</p>
<p>= lim
s&rarr;0
</p>
<p>P(t)
P (s)&minus;E
</p>
<p>s
= P(t)Q. (21.2.8)
</p>
<p>In the same way we obtain, from the equality
</p>
<p>P(t + s)&minus; P(t)=
(
P(s)&minus;E
</p>
<p>)
P(t),
</p>
<p>the second equation in (21.2.7). The passage to the limit is justified by the assump-
</p>
<p>tions we made.
</p>
<p>Further, it follows from (21.2.6) that the function P(t) is infinitely differentiable,
</p>
<p>and
</p>
<p>P (k)(t)= P(t)Qk,
</p>
<p>P (t)&minus; P(0)=
&infin;&sum;
</p>
<p>k=1
P (k)(0)
</p>
<p>tk
</p>
<p>k! =
&infin;&sum;
</p>
<p>k=1
</p>
<p>Qktk
</p>
<p>k! ,
</p>
<p>P (t)= P(0)eQt .
The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Markov Processes with Countable State Spaces. Examples 587
</p>
<p>Because of the derivation method, (21.2.6) is called the backward Kolmogorov
equation, and (21.2.7) is known as the forward Kolmogorov equation (the time in-
crement is taken after or before the basic time interval).
</p>
<p>The difference between these equations becomes even more graphical in the case
</p>
<p>of inhomogeneous Markov processes, when the transition probabilities
</p>
<p>P
(
ξ(t)= j
</p>
<p>∣∣ξ(s)= i
)
= pij (s, t), s &le; t,
</p>
<p>depend on two time arguments: s and t . In that case, (21.2.1) becomes the equality
</p>
<p>P(s, t + u)= P(s, t)P (t, t + u), and the backward and forward equations have the
form
</p>
<p>&part;P (s, t)
</p>
<p>&part;s
= P(s, t)Q(s), &part;P (s, t)
</p>
<p>&part;t
=Q(t)P (s, t),
</p>
<p>respectively, where
</p>
<p>Q(t)= lim
u&rarr;0
</p>
<p>P(t, t + u)&minus;E
u
</p>
<p>.
</p>
<p>The reader can derive these relations independently.
</p>
<p>What are the general conditions for existence of a stationary limiting distribu-
</p>
<p>tion? We can use here an approach similar to that employed in Chap. 13.
</p>
<p>Let ξ (i)(t) be a process with the initial value ξ (i)(0) = i and right-continuous
trajectories. For a given i0, put
</p>
<p>ν(i) := min
{
t &ge; 0 : ξ (i)(t)= i0
</p>
<p>}
=: ν0,
</p>
<p>νk := min
{
t &ge; νk&minus;1 + 1 : ξ (i)(t)= i0
</p>
<p>}
, k = 1,2, . . . .
</p>
<p>Here in the second formula we consider the values t &ge; νk&minus;1 + 1, since for t &ge; νk&minus;1
we would have νk &equiv; νk&minus;1. Clearly, P(νk &minus; νk&minus;1 = 1) &gt; 0, and P(νk &minus; νk&minus;1 &isin;
(t, t + h)) &gt; 0 for any t &ge; 1 and h &gt; 0 provided that pi0i0(t) 
&equiv; 1.
</p>
<p>Note also that the variables νk , k = 0,1, . . . , are not defined for all elementary
outcomes. We put ν0 =&infin; if ξ (i)(t) 
= i0 for all t &ge; 0. A similar convention is used
for νk , k &ge; 1. The following ergodic theorem holds.
</p>
<p>Theorem 21.2.3 Let there exist a state i0 such that Eν1 &lt;&infin; and P(ν(i) &lt;&infin;)= 1
for all i &isin;X0 &sub;X. Then there exist the limits
</p>
<p>lim
t&rarr;&infin;
</p>
<p>pij (t)= pj (21.2.9)
</p>
<p>which are independent of i &isin;X0.
</p>
<p>Proof As was the case for Markov chains, the epochs ν1, ν2, . . . divide the time axis
into independent cycles of the same nature, each of them being completed when
</p>
<p>the system returns for the first time (after one time unit) to the state i0. Consider
</p>
<p>the renewal process generated by the sums νk , k = 0,1, . . . , of independent random
variables ν0, νk &minus; νk&minus;1, k = 1,2, . . . . Let
</p>
<p>η(t) := min{k : νk &gt; t}, γ (t) := t &minus; νη(t)&minus;1, H(t) :=
&infin;&sum;
</p>
<p>k=0
P(νk &lt; t).</p>
<p/>
</div>
<div class="page"><p/>
<p>588 21 Markov Processes
</p>
<p>The event Adv := {γ (t) &isin; [v, v + dv)} can be represented as the intersection of the
events
</p>
<p>Bdv :=
⋃
</p>
<p>k&ge;0
</p>
<p>{
νk &isin; (t &minus; v&minus; dv, t &minus; v]
</p>
<p>}
&isin; F1&minus;v
</p>
<p>and Cv := {ξ(u) 
= i0 for u &isin; [t &minus; v + 1, t]} &isin; F[t&minus;v,&infin;). We have
</p>
<p>pij (t)=
&int; t
</p>
<p>0
</p>
<p>P
(
ξ (i)(t)= j, γ (t) &isin; [v, v+ dv)
</p>
<p>)
=
&int; t
</p>
<p>0
</p>
<p>P
(
ξ (i) = j, BdvCv
</p>
<p>)
</p>
<p>=
&int; t
</p>
<p>0
</p>
<p>E
[
IBdvP
</p>
<p>(
ξ (i)(t)= j, Cv
</p>
<p>∣∣Ft&minus;v
)]
</p>
<p>=
&int; t
</p>
<p>0
</p>
<p>E
[
IBdvP
</p>
<p>(
ξ (i)(t)= j,Cv
</p>
<p>∣∣ξ(t &minus; v)
)]
.
</p>
<p>On the set Bdv , one has ξ(t &minus; v) = i0, and hence the probability inside the last
integral is equal to
</p>
<p>P
(
ξ (i0)(v)= j, ξ(u) 
= i0 for u &isin; [1, v]
</p>
<p>)
=: g(v)
</p>
<p>and is independent of t and i. Since P(Bdv)= dH(t &minus; v), one has
</p>
<p>pij (t)=
&int; t
</p>
<p>0
</p>
<p>g(v)P(Bdv)=
&int; t
</p>
<p>0
</p>
<p>g(v) dH(t &minus; v).
</p>
<p>By the key renewal theorem, as t &rarr;&infin;, this integral converges to
1
</p>
<p>Eν1
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>g(v) dv.
</p>
<p>The existence of the last integral follows from the inequality g(v)&le; P(ν1 &gt; v). The
theorem is proved. �
</p>
<p>Theorem 21.2.4 If the stationary distribution
</p>
<p>P = lim
t&rarr;&infin;
</p>
<p>P(t)
</p>
<p>exists with all the rows of the matrix P being identical, then it is a unique solution
of the equation
</p>
<p>PQ= 0. (21.2.10)
</p>
<p>It is evident that Eq. (21.2.10) is obtained by setting P &prime;(t)= 0 in (21.2.6). Equa-
tion (21.2.7) gives the trivial equality QP = 0.
</p>
<p>Proof Equation (21.2.10) is obtained by passing to the limit in (21.2.8) first as
t &rarr; &infin; and then as s &rarr; 0. Now assume that P1 is a solution of (21.2.10), i.e.
P1Q= 0. Then P1P(t)= P1 for t &lt; 1, since
</p>
<p>P1
(
P(t)&minus; P(0)
</p>
<p>)
= P1
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>Qktk
</p>
<p>k! = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Markov Processes with Countable State Spaces. Examples 589
</p>
<p>Further, P1 = P1P k(t) = P1P(kt), P(kt) &rarr; P as k &rarr; &infin;, and hence P1 =
P1P = P . The theorem is proved. �
</p>
<p>Now consider a Markov chain {Xn} in discrete time with transition probabil-
ities pij = qij/qi , i 
= j , pii = 0. Suppose that this chain is ergodic (see Theo-
rem 13.4.1). Then its stationary probabilities {πj } satisfy Eqs. (13.4.2). Now note
that Eq. (21.2.10) can be written in the form
</p>
<p>pj qj =
&sum;
</p>
<p>k
</p>
<p>pkqkpkj
</p>
<p>which has an obvious solution pj = cπj/qj , c= const. Therefore, if
&sum; πj
</p>
<p>qj
&lt;&infin; (21.2.11)
</p>
<p>then there exists a solution to (21.2.10) given by
</p>
<p>pj =
πj
</p>
<p>qj
</p>
<p>(&sum; πj
qj
</p>
<p>)&minus;1
. (21.2.12)
</p>
<p>In Sects. 21.4 and 21.5 we will derive the ergodic theorem for processes of a more
</p>
<p>general form than the one in the present section. That theorem will imply, in partic-
</p>
<p>ular, that ergodicity of {Xn} and convergence (21.2.11) imply (21.2.9). Recall that,
for ergodicity of {Xn}, it suffices, in turn, that Eqs. (13.4.2) have a solution {πj }.
Thus the existence of solution (21.2.12) implies the ergodicity of ξ(t).
</p>
<p>21.2.2 Examples
</p>
<p>Example 21.2.1 The Poisson process ξ(t) with parameter λ is a Markov process for
which qi = λ, qi,i+1 = λ, and pi,i+1 = 1, i = 1,0, . . . . For this process, the station-
ary distribution p = (p0,p1, . . .) does not exist (each trajectory goes to infinity).
</p>
<p>Example 21.2.2 Birth-and-death processes. These are processes for which, for
i &ge; 1,
</p>
<p>pij (∆)=
</p>
<p>⎧
⎨
⎩
</p>
<p>λi∆+ o(∆) for j = i + 1,
&micro;i∆+ o(∆) for j = i &minus; 1,
o(∆) for |j &minus; i| &ge; 2,
</p>
<p>so that
</p>
<p>pij =
{
</p>
<p>λi
λi+&micro;i for j = i + 1,
&micro;i
</p>
<p>λi+&micro;i for j = i &minus; 1
</p>
<p>are probabilities of birth and death, respectively, of a particle in a certain population
</p>
<p>given that the population consisted of i particles and changed its composition. For</p>
<p/>
</div>
<div class="page"><p/>
<p>590 21 Markov Processes
</p>
<p>i = 0 one should put &micro;0 := 0. Establishing conditions for the existence of a station-
ary regime is a rather difficult problem (related mainly to finding conditions under
</p>
<p>which the trajectory escapes to infinity). If the stationary regime exists, then accord-
</p>
<p>ing to Theorem 21.2.4 the stationary probabilities pj can be uniquely determined
</p>
<p>from the recursive relations (see Eq. (21.2.10), in our case qii =&minus;qi =&minus;(λi +&micro;i))
&minus;p0λ0 + p1&micro;1 = 0,
</p>
<p>p0λ0 &minus; p1(λ1 +&micro;1)+ p2&micro;2 = 0,
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
</p>
<p>pk&minus;1λk&minus;1 &minus; pk(λk +&micro;k)+ pk+1&micro;k+1 = 0,
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
</p>
<p>(21.2.13)
</p>
<p>and condition
&sum;
</p>
<p>pj = 1.
</p>
<p>Example 21.2.3 The telephone lines problem from queueing theory. Suppose we
are given a system consisting of infinitely many communication channels which
</p>
<p>are used for telephone conversations. The probability that, for a busy channel, the
</p>
<p>transmitted conversation terminates during a small time interval (t, t +∆) is equal
to λ∆ + o(∆). The probability that a request for a new conversation (a new call)
arrives during the same time interval is &micro;∆+ o(∆). Thus the &ldquo;arrival flow&rdquo; of calls
is nothing else but the Poisson process with parameter λ, and the number ξ(t) of
</p>
<p>busy channels at time t is the value of the birth-and-death process for which λi = λ
and &micro;i = i&micro;.
</p>
<p>In that case, it is not hard to verify with the help of Theorem 21.2.3 that there
</p>
<p>always exists a stationary limiting distribution, for which Eqs. (21.2.13) have the
</p>
<p>form
</p>
<p>λp0 = &micro;p1,
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
</p>
<p>(λ+&micro;k)pk = λpk&minus;1 + (k + 1)&micro;pk+1,
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
</p>
<p>(21.2.14)
</p>
<p>From this we get that
</p>
<p>p1 = p0
λ
</p>
<p>&micro;
, p2 =
</p>
<p>p0
</p>
<p>2
</p>
<p>(
λ
</p>
<p>&micro;
</p>
<p>)2
, . . . , pk =
</p>
<p>(
λ
</p>
<p>&micro;
</p>
<p>)k
p0
</p>
<p>k! , (21.2.15)
</p>
<p>so that p0 = e&minus;λ/&micro;, and the limiting distribution will be the Poisson law with pa-
rameter λ/&micro;.
</p>
<p>If the number of channels n is finite, the calls which find all the lines busy will
</p>
<p>be rejected, and in (21.2.13) one has to put λn = 0, pn+1 = pn+2 = &middot; &middot; &middot; = 0. In
that case, the last equation in (21.2.14) will have the form &micro;npn = λpn&minus;1. Since
the formulas (21.2.15) will remain true for k &le; n, we obtain the so-called Erlang
formulas for the stationary distribution:
</p>
<p>pk =
(
λ
</p>
<p>&micro;
</p>
<p>)k
1
</p>
<p>k!
</p>
<p>[
n&sum;
</p>
<p>j=0
</p>
<p>1
</p>
<p>j !
</p>
<p>(
λ
</p>
<p>&micro;
</p>
<p>)j]&minus;1
</p>
<p>(the truncated Poisson distribution).</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Branching Processes 591
</p>
<p>The next example will be considered in a separate section.
</p>
<p>21.3 Branching Processes
</p>
<p>The essence of the mathematical model describing a branching process remains
</p>
<p>roughly the same as in Sect. 7.7.2. A continuous time branching process can be
</p>
<p>defined as follows. Let ξ (i)(t) denote the number of particles at time t with the
</p>
<p>initial condition ξ (i)(0)= i. Each particle, independently of all others, splits during
the time interval (t, t+∆) with probability &micro;∆+o(∆) into a random number η 
= 1
of particles (if η= 0, we say that the particle dies). Thus,
</p>
<p>ξ (i)(t)= ξ (1)1 (t)+ &middot; &middot; &middot; + ξ
(1)
i (t), (21.3.1)
</p>
<p>where ξ
(1)
k (t) are independent and distributed as ξ
</p>
<p>(1)(t). Moreover,
</p>
<p>pij (∆)= i&micro;∆hj&minus;i+1 + o(∆), j 
= i; hk = P(η= k); h1 = 0;
pii(∆)= 1 &minus; i&micro;∆+ o(∆), (21.3.2)
</p>
<p>so that here qij = i&micro;hj&minus;i+1, qii =&minus;i&micro;.
By formula (21.3.2), i&micro;∆ is the principal part of the probability that at least
</p>
<p>one particle will split. Clearly, the state 0 is absorbing. It will not be absorbing any
</p>
<p>more if one considers processes with immigration when a Poisson process (with
intensity λ) of &ldquo;outside&rdquo; particles is added to the process ξ (i)(t). Then
</p>
<p>pij (∆)= i&micro;∆hj&minus;i+1 + o(∆) for j &minus; i 
= 0,1,
pi,i+1(∆)=∆(i&micro;h2 + λ)+ o(∆).
</p>
<p>We return to the branching process (21.3.1), (21.3.2). By (21.3.1) we have
</p>
<p>r(i)(t, z) := Ezξ (i)(t) =
[
Ezξ
</p>
<p>(1)(t)
]i = r i(t, z)=
</p>
<p>&infin;&sum;
</p>
<p>k=0
zkpik(t),
</p>
<p>where
</p>
<p>r(t, z) := Ezξ (1)(t) =
&infin;&sum;
</p>
<p>k=0
zkp1k(t). (21.3.3)
</p>
<p>Equation (21.2.7) implies
</p>
<p>p&prime;1k(t)=
&infin;&sum;
</p>
<p>l=0
q1lplk(t).
</p>
<p>Therefore, differentiating (21.3.3) with respect to t , we find that
</p>
<p>r &prime;t (t, z)=
&infin;&sum;
</p>
<p>k=0
zkp&prime;1k(t)=
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>&infin;&sum;
</p>
<p>l=0
q1lplk(t)z
</p>
<p>k
</p>
<p>=
&infin;&sum;
</p>
<p>l=0
q1l
</p>
<p>&infin;&sum;
</p>
<p>k=0
zkplk(t)=
</p>
<p>&infin;&sum;
</p>
<p>l=0
q1lr
</p>
<p>l(t, z). (21.3.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>592 21 Markov Processes
</p>
<p>Fig. 21.1 The form of the
</p>
<p>plot of the function f1. The
</p>
<p>smaller root of the equation
</p>
<p>f1(q)= q gives the
probability of the eventual
</p>
<p>extinction of the branching
</p>
<p>process
</p>
<p>But q1l = &micro;pl for l 
= 1, q11 =&minus;&micro;, and putting
</p>
<p>f (s) :=
&infin;&sum;
</p>
<p>l=0
q1ls
</p>
<p>l = &micro;
(
Esη &minus; s
</p>
<p>)
= &micro;
</p>
<p>( &infin;&sum;
</p>
<p>l=0
hls
</p>
<p>l &minus; s
)
,
</p>
<p>we can write (21.3.4) in the form
</p>
<p>r &prime;t (t, z)= f
(
r(t, z)
</p>
<p>)
.
</p>
<p>We have obtained a differential equation for r = r(t, z) (equivalent to (21.3.2))
which is more convenient to write in the form
</p>
<p>dr
</p>
<p>f (r)
= dt, t =
</p>
<p>&int; r(t,z)
</p>
<p>r(0,z)
</p>
<p>dy
</p>
<p>f (y)
=
&int; r(t,z)
</p>
<p>z
</p>
<p>dy
</p>
<p>f (y)
.
</p>
<p>Consider the behaviour of the function f1(y) = Eyη &minus; y on [0,1]. Clearly,
f1(0)= P(η= 0), f1(1)= 0, and
</p>
<p>f &prime;1(1)= Eη&minus; 1, f &prime;&prime;1 (y)= Eη(η&minus; 1)yη&minus;2 &gt; 0.
</p>
<p>Consequently, the function f1(y) is convex and has no zeros in (0,1) if Eη &le; 1.
When Eη &gt; 1, there exists a point q &isin; (0,1) such that f1(q) = 0, f &prime;1(q) &lt; 0 (see
Fig. 21.1), and f1(y)= (y &minus; q)f &prime;1(q)+O((y &minus; q)2) in the vicinity of this point.
</p>
<p>Thus if Eη &gt; 1, z &lt; q and r &uarr; q , then, by virtue of the representation
1
</p>
<p>f1(y)
= 1
</p>
<p>(y &minus; q)f &prime;1(q)
+O(1),
</p>
<p>we obtain
</p>
<p>t =
&int; r
</p>
<p>z
</p>
<p>dy
</p>
<p>f (y)
= 1
</p>
<p>&micro;f &prime;1(q)
ln
</p>
<p>(
r &minus; q
z&minus; q
</p>
<p>)
+O(1).
</p>
<p>This implies that, as t &rarr;&infin;,
</p>
<p>r(t, z)&minus; q = (z&minus; q)e&micro;tf &prime;1(q)+O(1) &sim; (z&minus; q)e&micro;tf &prime;1(q),
r(t, z)= q +O
</p>
<p>(
e&minus;αt
</p>
<p>)
, α =&minus;&micro;f &prime;1(q) &gt; 0.
</p>
<p>(21.3.5)
</p>
<p>In particular, the extinction probability
</p>
<p>p10(t)= r(t,0)= q +O
(
e&minus;αt
</p>
<p>)
</p>
<p>converges exponentially fast to q , p10(&infin;) = q . Comparing our results with those
from Sect. 7.7, the reader can see that the extinction probability for a discrete time</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 Semi-Markov Processes 593
</p>
<p>branching process had the same value (we could also come to this conclusion di-
</p>
<p>rectly). Since pk0(t)= [p10(t)]k , one has pk0(&infin;)= qk .
It follows from (21.3.5) that the remaining &ldquo;probability mass&rdquo; of the distribution
</p>
<p>of ξ(t) quickly moves to infinity as t &rarr;&infin;.
If Eη &lt; 1, the above argument remains valid with q replaced with 1, so that the
</p>
<p>extinction probability is p10(&infin;)= pk0(&infin;)= 1.
If Eη= 1, then
</p>
<p>f1(y)=
(y &minus; 1)2
</p>
<p>2
f &prime;&prime;1 (1)+O
</p>
<p>(
(y &minus; 1)3
</p>
<p>)
,
</p>
<p>t =
&int; r
</p>
<p>z
</p>
<p>dy
</p>
<p>f (y)
&sim;&minus; 2
</p>
<p>&micro;f &prime;&prime;1 (1)
&middot; 1
r &minus; 1 , r(t, z)&minus; 1 &sim;&minus;
</p>
<p>2
</p>
<p>&micro;tf &prime;&prime;1 (1)
.
</p>
<p>Thus the extinction probability r(t,0)= p10(t) also tends to 1 in this case.
</p>
<p>21.4 Semi-Markov Processes
</p>
<p>21.4.1 Semi-Markov Processes on the States of a Chain
</p>
<p>Semi-Markov processes can be described as follows. Let an aperiodic discrete time
irreducible Markov chain {Xn} with the state space X = {0,1,2, . . .} be given. To
each state i we put into correspondence the distribution Fi(t) of a positive random
</p>
<p>variable ζ (i):
</p>
<p>Fi(t)= P
(
ζ (i) &lt; t
</p>
<p>)
.
</p>
<p>Consider independent of the chain {Xn} and of each other the sequences ζ (i)1 ,
ζ
(i)
2 , . . . ; ζ
</p>
<p>(i)
j
</p>
<p>d= ζ (i), of independent random variables with the distribution Fi . Let,
moreover, the distribution of the initial random vector (X0, ζ0), X0 &isin; X, ζ0 &ge; 0, be
given. The evolution of the semi-Markov process ξ(u) is described as follows:
</p>
<p>ξ(u)=X0 for 0 &le; u &lt; ζ0,
ξ(u)=X1 for ζ0 &le; u &lt; ζ0 + ζ (X1)1 ,
ξ(u)=X2 for ζ0 + ζ (X1)1 &le; u &lt; ζ0 + ζ
</p>
<p>(X1)
1 + ζ
</p>
<p>(X2)
2 ,
</p>
<p>&middot; &middot; &middot; ,
ξ(u)=Xn for Zn&minus;1 &le; u &lt; Zn, Zn = ζ0 + ζ (X1)1 + &middot; &middot; &middot; + ζ
</p>
<p>(Xn)
n ,
</p>
<p>(21.4.1)
</p>
<p>and so on. Thus, upon entering state Xn = j , the trajectory of ξ(u) remains in that
state for a random time ζ
</p>
<p>(Xn)
n = ζ (j)n , then switches to state Xn+1 and so on. It
</p>
<p>is evident that such a process is, generally speaking, not Markovian. It will be a
</p>
<p>Markov process only if
</p>
<p>1 &minus; Fi(t)= e&minus;qi t , qi &gt; 0,
and will then coincide with the process described in Sect. 21.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>594 21 Markov Processes
</p>
<p>Fig. 21.2 The trajectories of the semi-Markov process ξ(t) and of the residual sojourn time pro-
</p>
<p>cess χ(t)
</p>
<p>If the distribution Fi is not exponential, then, given the value ξ(t)= i, the time
between t and the next jump epoch will depend on the epoch of the preceding jump
</p>
<p>of ξ(&middot;), because
</p>
<p>P
(
ζ (i) &gt; v+ u
</p>
<p>∣∣ζ (i) &gt; v
)
= 1 &minus; Fi(v+ u)
</p>
<p>1 &minus; Fi(v)
for non-exponential Fi depends on v. It is this property that means that the process
</p>
<p>is non-Markovian, for fixing the &ldquo;present&rdquo; (i.e. the value of ξ(t)) does not make the
</p>
<p>&ldquo;future&rdquo; of the process ξ(u) independent of the &ldquo;past&rdquo; (i.e. of the trajectory of ξ(u)
</p>
<p>for u &lt; t).
</p>
<p>The process ξ(t) can be &ldquo;complemented&rdquo; to a Markov one by adding to it the
</p>
<p>component χ(t) of which the value gives the time u for which the trajectory ξ(t+u),
u &ge; 0, will remain in the current state ξ(t). In other words, χ(t) is the excess of
level t for the random walk Z0,Z1, . . . (see Fig. 21.2):
</p>
<p>χ(t)= Zν(t)+1 &minus; t, ν(t)= max{k : Zk &le; t}.
The process χ(t) is Markovian and has &ldquo;saw-like&rdquo; trajectories deterministic in-
</p>
<p>side the intervals (Zk,Zk+1). The process X(t)= (ξ(t),χ(t)) is obviously Marko-
vian, since the value of X(t) uniquely determines the law of evolution of the process
</p>
<p>X(t + u) for u&ge; 0 whatever the &ldquo;history&rdquo; X(v), v &lt; t , is. Similarly, we could con-
sider the Markov process Y(t)= (ξ(t), γ (t)), where γ (t) is the defect of level t for
the walk Z0,Z1, . . . :
</p>
<p>γ (t)= t &minus;Zν(t).
</p>
<p>21.4.2 The Ergodic Theorem
</p>
<p>In the sequel, we will distinguish between the following two cases.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 Semi-Markov Processes 595
</p>
<p>(A) The arithmetic case when the possible values of ζ (i), i = 0,1, . . ., are mul-
tiples of a certain value h which can be assumed without loss of generality to be
</p>
<p>equal to 1. In that case we will also assume that the g.c.d. of the possible values of
</p>
<p>the sums of the variables ζ (i) is also equal to h = 1. This is clearly equivalent to
assuming that the g.c.d. of the possible values of recurrence times θ (i) of ξ(t) to the
</p>
<p>state i is equal to 1 for any fixed i.
</p>
<p>(NA) The non-arithmetic case, when condition (A) does not hold.
Put ai := Eζ (i).
</p>
<p>Theorem 21.4.1 Let the Markov chain {Xn} be ergodic (satisfy the conditions of
Theorem 13.4.1) and {πj } be the stationary distribution of that chain. Then, in the
non-arithmetic case (NA), for any initial distribution (ζ0,X0) there exists the limit
</p>
<p>lim
t&rarr;&infin;
</p>
<p>P
(
ξ(t)= i, χ(t) &gt; v
</p>
<p>)
= πi&sum;
</p>
<p>πjaj
</p>
<p>&int; &infin;
</p>
<p>v
</p>
<p>P
(
ζ (i) &gt; u
</p>
<p>)
du. (21.4.2)
</p>
<p>In the arithmetic case (A), (21.4.2) holds for integer-valued v (the integral be-
comes a sum in that case). It follows from (21.4.2) that the following limit exists
</p>
<p>lim
t&rarr;&infin;
</p>
<p>P
(
ξ(t)= i
</p>
<p>)
= πiai&sum;
</p>
<p>πjaj
.
</p>
<p>Proof For definiteness we restrict ourselves to the non-arithmetic case (NA). In
Sect. 13.4 we considered the times τ (i) between consecutive visits of {Xn} to state i.
These times could be called &ldquo;embedded&rdquo;, as well as the chain {Xn} itself in regard
to the process ξ(t). Along with the times τ (i), we will need the &ldquo;real&rdquo; times θ (i)
</p>
<p>between the visits of the process ξ(t) to the state i. Let, for instance, X1 = 1. Then
</p>
<p>θ (1)
d= ζ (X1)1 + ζ
</p>
<p>(X2)
2 + &middot; &middot; &middot; + ζ
</p>
<p>(Xτ )
τ ,
</p>
<p>where τ = τ (1). For definiteness and to reduce notation, we fix for the moment the
value i = 1 and put θ (1) =: θ . Let first
</p>
<p>ζ0
d= ζ (1), X0 = 1. (21.4.3)
</p>
<p>Then the whole trajectory of the process X(t) for t &ge; 0 will be divided into iden-
tically distributed independent cycles by the epochs when the process hits the state
</p>
<p>ξ(t)= 1. We denote the lengths of these cycles by θ1, θ2 . . . ; they are independent
and identically distributed. We show that
</p>
<p>Eθ = 1
π1
</p>
<p>&sum;
ajπj . (21.4.4)
</p>
<p>Denote by θ(n) the &ldquo;real&rdquo; time spent on n transitions of the governing
</p>
<p>chain {Xn}. Then
θ1 + &middot; &middot; &middot; + θη(n)&minus;1 &le; θ(n)&le; θ1 + &middot; &middot; &middot; + θη(n), (21.4.5)
</p>
<p>where η(n) := min{k : Tk &gt; n}, Tk =
&sum;k
</p>
<p>j=1 τj , τj are independent and distributed
as τ . We prove that, as n&rarr;&infin;,
</p>
<p>Eθ(n)&sim; nπ1Eθ. (21.4.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>596 21 Markov Processes
</p>
<p>By Wald&rsquo;s identity and (21.4.5),
</p>
<p>Eθ(n)&le; EθEη(n), (21.4.7)
</p>
<p>where Eη(n)&sim; n/Eτ = nπ1.
Now we bound from below the expectation Eθ(n). Put m := &lfloor;nπ1 &minus; εn&rfloor;, Θn :=&sum;n
j=1 θj . Then
</p>
<p>Eθ(n)&ge; E
(
θ(n); η(n) &gt; m
</p>
<p>)
</p>
<p>&ge; E
(
Θm; η(n) &gt; m
</p>
<p>)
=mEθ &minus;E
</p>
<p>(
Θm; η(n)&le;m
</p>
<p>)
. (21.4.8)
</p>
<p>Here the random variable Θm/m&ge; 0 possesses the properties
</p>
<p>Θm/m
p&rarr; Eθ as m&rarr;&infin;, E(Θm/m)= Eθ.
</p>
<p>Therefore it satisfies the conditions of part 4 of Lemma 6.1.1 and is uniformly in-
</p>
<p>tegrable. This, in turn, by Lemma 6.1.2 and convergence P(η(n) &le;m)&rarr; 0 means
that the last term on the right-hand side of (21.4.8) is o(m). By virtue of (21.4.8),
</p>
<p>since ε &gt; 0 is arbitrary, we obtain that
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>n&minus;1Eθ(n)&ge; π1Eθ.
</p>
<p>This together with (21.4.7) proves (21.4.6).
</p>
<p>Now we will calculate the value of Eθ(n) using another approach. The variable
</p>
<p>θ(n) admits the representation
</p>
<p>θ(n)=
&sum;
</p>
<p>j
</p>
<p>(
ζ
(j)
</p>
<p>1 + &middot; &middot; &middot; + ζ
(j)
</p>
<p>N(j,n)
</p>
<p>)
,
</p>
<p>where N(j,n) is the number of visits of the trajectory of {Xk} to the state j during
the first n steps. Since {ζ (j)k }&infin;k=1 and N(j,n) are independent for each j , we have
</p>
<p>Eθ(n)=
&sum;
</p>
<p>j
</p>
<p>ajEN(j,n), EN(j,n)=
n&sum;
</p>
<p>k=1
p1j (k).
</p>
<p>Because p1j (k)&rarr; πj as k&rarr;&infin;, one has
</p>
<p>lim
n&rarr;&infin;
</p>
<p>n&minus;1EN(j,n)= πj .
</p>
<p>Moreover,
</p>
<p>πj =
&sum;
</p>
<p>πlplj (k)&ge; π1p1j (k)
</p>
<p>and, therefore,
</p>
<p>p1j (k)&le; πj/π1.
</p>
<p>Hence
</p>
<p>n&minus;1EN(j,n)&le; πj/π1,</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 Semi-Markov Processes 597
</p>
<p>and in the case when
&sum;
</p>
<p>j ajπj &lt;&infin;, the series
&sum;
</p>
<p>j ajn
&minus;1EN(j,n) converges uni-
</p>
<p>formly in n. Consequently, the following limit exists
</p>
<p>lim
n&rarr;&infin;
</p>
<p>n&minus;1Eθ(n)=
&sum;
</p>
<p>j
</p>
<p>ajπj .
</p>
<p>Comparing this with (21.4.6) we obtain (21.4.4). If Eθ = &infin; then clearly
Eθ(n)=&infin; and
</p>
<p>&sum;
j ajπj =&infin;, and vice versa, if
</p>
<p>&sum;
j ajπj =&infin; then Eθ =&infin;.
</p>
<p>Consider now the random walk {Θk}. To the k-th cycle there correspond Tk tran-
sitions. Therefore, by the total probability formula,
</p>
<p>P
(
ξ(t)= 1, χ(t) &gt; v
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>&int; t
</p>
<p>0
</p>
<p>P
(
Θk &isin; du, ζ (1)Tk+1 &gt; t &minus; u+ v
</p>
<p>)
,
</p>
<p>where ζ
(1)
Tk+1
</p>
<p>is independent of Θk and distributed as ζ
(1) (see Lemma 11.2.1 or
</p>
<p>the strong Markov property). Therefore, denoting by Hθ (u) :=
&sum;&infin;
</p>
<p>k=1 P(Θk &lt; u)
the renewal function for the sequence {Θk}, we obtain for the non-arithmetic case
(NA), by virtue of the renewal theorem (see Theorem 10.4.1 and (10.4.2)), that, as
</p>
<p>t &rarr;&infin;,
</p>
<p>P
(
ξ(t)= 1, χ(t) &gt; v
</p>
<p>)
</p>
<p>=
&int; t
</p>
<p>0
</p>
<p>dHθ (u)P
(
ζ (1) &gt; t &minus; u+ v
</p>
<p>)
</p>
<p>&rarr; 1
Eθ
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P
(
ζ (1) &gt; u+ v
</p>
<p>)
dv = 1
</p>
<p>Eθ
</p>
<p>&int; &infin;
</p>
<p>v
</p>
<p>P
(
ζ (1) &gt; u
</p>
<p>)
du. (21.4.9)
</p>
<p>We have proved assertion (21.4.2) for i = 1 and initial conditions (21.4.3). The
transition to arbitrary initial conditions is quite obvious and is done in exactly the
</p>
<p>same way as in the proof of the ergodic theorems of Chap. 13.
</p>
<p>If
&sum;
</p>
<p>aiπi =&infin; then, as we have already observed, Eθ =&infin; and, by the renewal
theorem and (21.4.9), one has P(ξ(t)= 1, χ(t) &gt; v)&rarr; 0 as t &rarr;&infin;. It remains to
note that instead of i = 1 we can fix any other value of i. The theorem is proved. �
</p>
<p>In the same way we could also prove that
</p>
<p>lim
t&rarr;&infin;
</p>
<p>P
(
ξ(t)= i, γ (t) &gt; v
</p>
<p>)
= πi&sum;
</p>
<p>ajπj
</p>
<p>&int; &infin;
</p>
<p>v
</p>
<p>P
(
ζ (i) &gt; y
</p>
<p>)
dy,
</p>
<p>lim
t&rarr;&infin;
</p>
<p>P
(
ξ(t)= i, χ(t) &gt; u, γ (t) &gt; v
</p>
<p>)
= πi&sum;
</p>
<p>ajπj
</p>
<p>&int; &infin;
</p>
<p>u+v
P
(
ζ (i) &gt; y
</p>
<p>)
dy
</p>
<p>(see Theorem 10.4.3).
</p>
<p>21.4.3 Semi-Markov Processes on Chain Transitions
</p>
<p>Along with the semi-Markov processes ξ(t) described at the beginning of the
</p>
<p>present section, one sometimes considers semi-Markov processes &ldquo;given on the</p>
<p/>
</div>
<div class="page"><p/>
<p>598 21 Markov Processes
</p>
<p>transitions&rdquo; of the chain {Xn}. In that case, the distributions Fij of random variables
ζ (ij) &gt; 0 are given and, similarly to (21.4.1), for the initial condition (X0,X1, ζ0)
</p>
<p>one puts
</p>
<p>ξ(u) := (X0,X1) for 0 &le; u &lt; ζ0
ξ(u) := (X1,X2) for ζ0 &le; u &lt; ζ0 + ζ (X0,X1)1
ξ(u) := (X2,X3) for ζ0 + ζ (X0,X1)1 &le; u &lt; ζ0 + ζ
</p>
<p>(X0,X1)
1 + ζ
</p>
<p>(X1,X2)
2 ,
</p>
<p>(21.4.10)
</p>
<p>and so on. Although at first glance this is a very general model, it can be com-
</p>
<p>pletely reduced to the semi-Markov processes (21.4.1). To that end, one has to notice
</p>
<p>that the &ldquo;two-dimensional&rdquo; sequence Yn = (Xn,Xn+1), n = 0,1, . . . , also forms a
Markov chain. Its transition probabilities have the form
</p>
<p>p(ij)(kl) =
{
pj l for k = j,
0 for k 
= j,
</p>
<p>p(ij)(kl)(n)= pjk(n)pkl for n &gt; 1,
so that if the chain {Xn} is ergodic, then {Yn} is also ergodic and
</p>
<p>p(ij)(kl)(n)&rarr; πkpkl .
This enables one to restate Theorem 21.4.1 easily for the semi-Markov pro-
</p>
<p>cesses (21.4.10) given on the transitions of the Markov chain {Xn}, since the process
(21.4.10) will be an ordinary semi-Markov process given on the chain {Yn}.
</p>
<p>Corollary 21.4.1 If the chain {Xn} is ergodic then, in the non-arithmetic case,
lim
t&rarr;&infin;
</p>
<p>P
(
ξ(t)= (i, j), χ(t) &gt; v
</p>
<p>)
</p>
<p>= πipij&sum;
k,l aklπkpkl
</p>
<p>&int; &infin;
</p>
<p>v
</p>
<p>P
(
ζ (ij) &gt; u
</p>
<p>)
du, akl = Eζ (kl).
</p>
<p>In the arithmetic case v must be a multiple of the lattice span.
</p>
<p>We will make one more remark which could be helpful when studying semi-
</p>
<p>Markov processes and which concerns the so-called semi-Markov renewal functions
</p>
<p>Hij (t). Denote by Tij (n) the epoch (in the &ldquo;real time&rdquo;) of the n-th jump of the
</p>
<p>process ξ(t) from state i to j . Put
</p>
<p>Hij (t) :=
&infin;&sum;
</p>
<p>n=1
P
(
Tij (n) &lt; t
</p>
<p>)
.
</p>
<p>If νij (t) is the number of jumps from state i to j during the time interval [0, t),
then clearly Hij (t)= Eνij (t).
</p>
<p>Set ∆f (t) := f (t +∆)&minus; f (t), ∆&gt; 0.
</p>
<p>Corollary 21.4.2 In the non-arithmetic case,
</p>
<p>lim
t&rarr;&infin;
</p>
<p>∆Hij (t)=
πipij∆&sum;
</p>
<p>l alπl
. (21.4.11)
</p>
<p>In the arithmetic case v must be a multiple of the lattice span.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 Semi-Markov Processes 599
</p>
<p>Proof Denote by ν(k)ij (u) the number of transitions of the process ξ(t) from i to j
during the time interval (0, u) given the initial condition (k,0). Then, by the total
</p>
<p>probability formula,
</p>
<p>E∆νij (t)=
&int; ∆
</p>
<p>0
</p>
<p>&infin;&sum;
</p>
<p>k=0
P
(
ξ(t)= k, χ(t) &isin; du
</p>
<p>)
Eν
</p>
<p>(k)
ij (∆&minus; u).
</p>
<p>Since ν
(k)
ij (u)&le; ν
</p>
<p>(i)
ij (u), by Theorem 21.4.1 one has
</p>
<p>hij (∆) := lim
t&rarr;&infin;
</p>
<p>E∆νij (t)=
1&sum;
l alπl
</p>
<p>&infin;&sum;
</p>
<p>k=0
πk
</p>
<p>&int; ∆
</p>
<p>0
</p>
<p>P
(
ζ (k) &gt; u
</p>
<p>)
Eν
</p>
<p>(k)
ij (∆&minus; u)du.
</p>
<p>(21.4.12)
</p>
<p>Further,
</p>
<p>P
(
ζ (i) &lt;∆&minus; u
</p>
<p>)
&le; Fi(∆)&rarr; 0
</p>
<p>as ∆&rarr; 0, and
</p>
<p>P
(
ν
(k)
ij (∆&minus; u)= s
</p>
<p>)
&le;
(
pijFi(∆)
</p>
<p>)s
, k 
= i,
</p>
<p>P
(
ν
(i)
ij (∆&minus; u)= s + 1
</p>
<p>)
&le;
(
pijFi(∆)
</p>
<p>)s
, s &ge; 1,
</p>
<p>P
(
ν
(i)
ij (∆&minus; u)= 1
</p>
<p>)
= pij + o
</p>
<p>(
Fi(∆)
</p>
<p>)
.
</p>
<p>It follows from the aforesaid that
</p>
<p>Eν
(k)
ij (∆&minus; u)= o
</p>
<p>(
Fi(∆)
</p>
<p>)
, Eν
</p>
<p>(i)
ij (∆&minus; u)= pij + o
</p>
<p>(
Fi(∆)
</p>
<p>)
.
</p>
<p>Therefore,
</p>
<p>hij (∆)=
πipij∆&sum;
</p>
<p>l alπl
+ o(∆). (21.4.13)
</p>
<p>Further, from the equality
</p>
<p>Hij (t + 2∆)&minus;Hij (t)=∆Hij (t)+∆Hij (t +∆)
</p>
<p>we obtain that hij (2∆)= 2hij (∆), which means that hij (∆) is linear. Together with
(21.4.13) this proves (21.4.11). The corollary is proved. �
</p>
<p>The class of processes for which one can prove ergodicity using the same meth-
</p>
<p>ods as the one used for semi-Markov processes and also in Chap. 13, can be some-
</p>
<p>what extended. For this broader class of processes we will prove in the next section
</p>
<p>the ergodic theorem, and also the laws of large numbers and the central limit theo-
</p>
<p>rem for integrals of such processes.</p>
<p/>
</div>
<div class="page"><p/>
<p>600 21 Markov Processes
</p>
<p>21.5 Regenerative Processes
</p>
<p>21.5.1 Regenerative Processes. The Ergodic Theorem
</p>
<p>Let X(t) and X0(t); t &ge; 0, be processes given in the space D(0,&infin;) of functions
without discontinuities of the second type (the state space of these processes could
</p>
<p>be any metric space, not necessarily the real line). The process X(t) is said to be
</p>
<p>regenerative if it possesses the following properties:
(1) There exists a state x0 which is visited by the process X with probability 1.
</p>
<p>After each such visit, the evolution of the process starts anew as if it were the original
</p>
<p>process X(t) starting at the state X(0) = x0. We will denote this new process by
X0(t) where X0(0) = x0. To state this property more precisely, we introduce the
time τ0 of the first visit to x0 by X:
</p>
<p>τ0 := inf
{
t &ge; 0 :X(t)= x0
</p>
<p>}
.
</p>
<p>However, it is not clear from this definition whether τ0 is a random variable. For
</p>
<p>definiteness, assume that the process X is such that for τ0 one has
</p>
<p>{τ0 &gt; t} =
⋃
</p>
<p>n
</p>
<p>⋂
</p>
<p>tk&isin;S
</p>
<p>{∣∣X(tk)&minus; x0
∣∣&gt; 1/n
</p>
<p>}
,
</p>
<p>where S is a countable set everywhere dense in [0, t]. In that case the set {τ0 &gt; t}
is clearly an event and τ0 is a random variable. The above stated property means
</p>
<p>that τ0 is a proper random variable: P(τ0 &lt; &infin;) = 1, and that the distribution of
X(τ0 +u), u&ge; 0, coincides with that of X0(u), u&ge; 0, whatever the &ldquo;history&rdquo; of the
process X(t), t &le; τ0.
</p>
<p>(2) The recurrence time τ of the state x0 has finite expectation Eτ &lt; &infin;,
τ := inf{t :X0(t)= x0}.
</p>
<p>The aforesaid means that the evolution of the process is split into independent
</p>
<p>identically distributed cycles by its visits to the state x0. The visit times to x0 are
</p>
<p>called regeneration times. The behaviour of the process inside the cycles may be
arbitrary, and no further conditions, including Markovity, are imposed.
</p>
<p>We introduce the so-called &ldquo;taboo probability&rdquo;
</p>
<p>P(t,B) := P
(
X0(t) &isin; B, τ &gt; t
</p>
<p>)
.
</p>
<p>We will assume that, as a function of t , P(t,B) is measurable and Riemann inte-
</p>
<p>grable.
</p>
<p>Theorem 21.5.1 Let X(t) be a regenerative process and the random variable τ be
non-lattice. Then, for any Borel set B , as t &rarr;&infin;,
</p>
<p>P
(
X(t) &isin; B
</p>
<p>)
&rarr; π(B)= 1
</p>
<p>Eτ
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P(u,B)du.
</p>
<p>If τ is a lattice variable (which is the case for processes X(t) in discrete time), the
assertion holds true with the following obvious changes: t &rarr;&infin; along the lattice
and the integral is replaced with a sum.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.5 Regenerative Processes 601
</p>
<p>Proof Let T0 := 0, Tk := τ1 + &middot; &middot; &middot; + τk be the epoch of the k-th regeneration of the
process X0(t), and
</p>
<p>H(u) :=
&infin;&sum;
</p>
<p>k=0
P(τk &lt; u)
</p>
<p>(τk
d= τ are independent). Then, using the total probability formula and the key
</p>
<p>renewal theorem, we obtain, as t &rarr;&infin;,
</p>
<p>P
(
X0(t) &isin; B
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>&int; t
</p>
<p>0
</p>
<p>P(Tk &isin; du)P (t &minus; u,B)
</p>
<p>=
&int; t
</p>
<p>0
</p>
<p>dH(u)P (t &minus; u,B)&rarr; 1
Eτ
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>P(u,B)du= π(B).
</p>
<p>For the process X(t) one gets
</p>
<p>P
(
X(t) &isin; B
</p>
<p>)
=
&int; t
</p>
<p>0
</p>
<p>P(t0 &isin; du)P
(
X0(t &minus; u) &isin; B
</p>
<p>)
&rarr; π(B).
</p>
<p>The theorem is proved. �
</p>
<p>21.5.2 The Laws of Large Numbers and Central Limit Theorem
</p>
<p>for Integrals of Regenerative Processes
</p>
<p>Consider a measurable mapping f : X&rarr; R of the state space X of a process X(t)
to the real line R. As in Sect. 21.4.2, for the sake of simplicity, we can assume that
</p>
<p>X = R and the trajectories of X(t) lie in the space D(0,&infin;) of functions without
discontinuities of the second kind. In this case the paths f (X(u)), u &ge; 0, will be
measurable functions, for which the integral
</p>
<p>S(t)=
&int; t
</p>
<p>0
</p>
<p>f
(
X(u)
</p>
<p>)
du
</p>
<p>is well defined. For such integrals we have the following law of large numbers. Set
</p>
<p>ζ :=
&int; τ
</p>
<p>0
</p>
<p>f
(
X0(u)
</p>
<p>)
du, a := Eτ.
</p>
<p>Theorem 21.5.2 Let the conditions of Theorem 21.5.1 be satisfied and there exist
aζ := Eζ . Then, as t &rarr;&infin;,
</p>
<p>S(t)
</p>
<p>t
</p>
<p>p&rarr; aζ
a
.
</p>
<p>For conditions of existence of Eζ , see Theorem 21.5.4 below.</p>
<p/>
</div>
<div class="page"><p/>
<p>602 21 Markov Processes
</p>
<p>Proof The proof of the theorem largely repeats that of the similar assertion (The-
orem 13.8.1) for sums of random variables defined on a Markov chain. Divide the
</p>
<p>domain u&ge; 0 into half-intervals
</p>
<p>(0, T0], (Tk&minus;1, Tk], k &ge; 1, T0 = τ0,
</p>
<p>where Tk are the epochs of hitting the state x0 by the process X(t), τk = Tk &minus; Tk&minus;1
for k &ge; 1 are independent and distributed as τ . Then the random variables
</p>
<p>ζk =
&int; Tk
Tk&minus;1
</p>
<p>f
(
X(u)
</p>
<p>)
du, k &ge; 1
</p>
<p>are independent, distributed as ζ , and have finite expectation aζ . The integral S(t)
</p>
<p>can be represented as
</p>
<p>S(t)= z0 +
ν(t)&sum;
</p>
<p>k=1
ζk + zt ,
</p>
<p>where
</p>
<p>ν(t) := max{k : Tk &le; t}, z0 :=
&int; T0
</p>
<p>0
</p>
<p>f
(
X(u)
</p>
<p>)
du, zt :=
</p>
<p>&int; t
</p>
<p>Tν(t)
</p>
<p>f
(
X(u)
</p>
<p>)
du.
</p>
<p>Since τ0 is a proper random variable, z0 is a proper random variable as well, and
</p>
<p>hence z0/t
a.s.&minus;&rarr; 0 as t &rarr;&infin;. Further,
</p>
<p>zt
d=
&int; γ (t)
</p>
<p>0
</p>
<p>f
(
X0(u)
</p>
<p>)
du,
</p>
<p>where γ (t)= t &minus; Tν(t) has a proper limiting distribution as t &rarr;&infin; (see Chap. 10),
so zt/t
</p>
<p>p&rarr; 0 as t &rarr;&infin;. The sum Sν(t) =
&sum;ν(t)
</p>
<p>k=1 ζk is nothing else but the generalised
renewal process studied in Chaps. 10 and 11. By Theorem 11.5.2, as t &rarr;&infin;,
</p>
<p>Sν(t)
</p>
<p>t
</p>
<p>p&rarr; aζ
a
.
</p>
<p>The theorem is proved. �
</p>
<p>In order to prove the strong law of large numbers we need a somewhat more
</p>
<p>restrictive condition than that in Theorem 21.5.2. Put
</p>
<p>ζ &lowast; :=
&int; τ
</p>
<p>0
</p>
<p>∣∣f
(
X0(u)
</p>
<p>)∣∣du.
</p>
<p>Theorem 21.5.3 Let the conditions of Theorem 21.5.1 be satisfied and Eζ &lowast; &lt;&infin;.
Then
</p>
<p>S(t)
</p>
<p>t
</p>
<p>a.s.&minus;&rarr; aζ
a
.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.6 Diffusion Processes 603
</p>
<p>The proof essentially repeats (as was the case for Theorem 21.5.2) that of the law
of large numbers for sums of random variables defined on a Markov chain (see
</p>
<p>Theorem 13.8.3). One only needs to use, instead of (13.8.18), the relation
</p>
<p>sup
Tk&le;u&le;Tk+1
</p>
<p>∣∣∣∣
&int; u
</p>
<p>Tk
</p>
<p>f
(
X(v)
</p>
<p>)
dv
</p>
<p>∣∣∣∣&le; ζ
&lowast;
k =
</p>
<p>&int; Tk+1
Tk
</p>
<p>∣∣f
(
X(v)
</p>
<p>)∣∣dv
</p>
<p>and the fact that E ζ &lowast;k &lt;&infin;. The theorem is proved. �
</p>
<p>Here an analogue of Theorem 13.8.2, in which the conditions of existence of
</p>
<p>E ζ &lowast; and Eζ are elucidated, is the following.
</p>
<p>Theorem 21.5.4 (Generalisation of Wald&rsquo;s identity) Let the conditions of Theo-
rem 21.5.1 be met and there exist
</p>
<p>E
∣∣f
</p>
<p>(
X(&infin;)
</p>
<p>)∣∣=
&int; ∣∣f (x)
</p>
<p>∣∣π(dx),
</p>
<p>where X(&infin;) is a random variable with the stationary distribution π . Then there
exist
</p>
<p>Eζ &lowast; = EτE
∣∣f
</p>
<p>(
X(&infin;)
</p>
<p>)∣∣, Eζ = EτEf
(
X(&infin;)
</p>
<p>)
.
</p>
<p>The proof of Theorem 21.5.4 repeats, with obvious changes, that of The-
orem 13.8.2. �
</p>
<p>Theorem 21.5.5 (The central limit theorem) Let the conditions of Theorem 21.5.1
be met and Eτ 2 &lt;&infin;, Eζ 2 &lt;&infin;. Then
</p>
<p>S(t)&minus; rt
d
&radic;
t/a
</p>
<p>&sub;&rArr;�0,1, t &rarr;&infin;,
</p>
<p>where r = aζ /a, d2 =D(ζ &minus; rτ ).
</p>
<p>The proof, as in the case of Theorems 21.5.2&ndash;21.5.4, repeats, up to evident
changes, that of Theorem 13.8.4. �
</p>
<p>Here an analogue of Theorem 13.8.5 (on the conditions of existence of variance
</p>
<p>and on an identity for a&minus;1d2) looks more complicated than under the conditions of
Sect. 13.8 and is omitted.
</p>
<p>21.6 Diffusion Processes
</p>
<p>Now we will consider an important class of Markov processes with continuous tra-
</p>
<p>jectories.
</p>
<p>Definition 21.6.1 A homogeneous Markov process ξ(t) with state space 〈R,B〉
and the transition function P(t, x,B) is said to be a diffusion process if, for some
finite functions a(x) and b2(x) &gt; 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>604 21 Markov Processes
</p>
<p>(1) lim∆&rarr;0 1∆
&int;
(y &minus; x)P (∆,x, dy)= a(x),
</p>
<p>(2) lim∆&rarr;0 1∆
&int;
(y &minus; x)2P(∆,x, dy)= b2(x),
</p>
<p>(3) for some δ &gt; 0 and c &lt;&infin;,&int;
|y &minus; x|2+δP(∆,x, dy) &lt; c∆1+δ/2.
</p>
<p>Put ∆ξ(t) := ξ(t +∆)&minus; ξ(t). Then the above conditions can be written in the
form:
</p>
<p>E
[
∆ξ(t)
</p>
<p>∣∣ξ(t)= x
]
&sim; a(x)∆,
</p>
<p>E
[(
∆ξ(t)
</p>
<p>)2∣∣ξ(t)= x
]
&sim; b2(x)∆,
</p>
<p>E
[∣∣∆ξ(t)
</p>
<p>∣∣2+δ∣∣ξ(t)= x
]
&lt; c∆1+δ/2 as ∆&rarr; 0.
</p>
<p>The coefficients a(x) and b(x) are called the shift and diffusion coefficients, re-
spectively. Condition (3) is an analogue of the Lyapunov condition. It could be re-
</p>
<p>placed with a Lindeberg type condition:
</p>
<p>(3a) E[(∆ξ(t))2; |∆ξ(t)|&gt; ε] = o(∆) for any ε &gt; 0 as ∆&rarr; 0.
It follows immediately from condition (3) and the Kolmogorov theorem that a
</p>
<p>diffusion process ξ(t) can be thought of as a process with continuous trajectories.
</p>
<p>The standard Wiener process w(t) is a diffusion process, since in that case
</p>
<p>P(t;x,B)= 1&radic;
2πt
</p>
<p>&int;
</p>
<p>B
</p>
<p>e&minus;(x&minus;y)
2/(2t) dy,
</p>
<p>E∆w(t)= 0, E
[
∆w(t)
</p>
<p>]2 =∆, E
[
∆w(t)
</p>
<p>]4 = 3∆2.
Therefore the Wiener process has zero shift and a constant diffusion coefficient.
</p>
<p>Clearly, the process w(t)+ at will have shift a and the same diffusion coefficient.
We saw in Sect. 21.2 that the &ldquo;local&rdquo; characteristic Q of a Markov process ξ(t)
</p>
<p>with a discrete state space X specifies uniquely the evolution law of the process.
</p>
<p>A similar situation takes place for diffusion processes: the distribution of the process
is determined uniquely by the coefficients a(x) and b(x). The way to establishing
this fact again lies via the Chapman&ndash;Kolmogorov equation.
</p>
<p>Theorem 21.6.1 If the transition probability P(t;x,B) of a diffusion process is
twice continuously differentiable with respect to x, then P(t;x,B) is differentiable
with respect to t and satisfies the equation
</p>
<p>&part;P
</p>
<p>&part;t
= a &part;P
</p>
<p>&part;x
+ b
</p>
<p>2
</p>
<p>2
</p>
<p>&part;2P
</p>
<p>&part;x2
(21.6.1)
</p>
<p>with the initial condition
</p>
<p>P(0;x,B)= IB(x). (21.6.2)
</p>
<p>Remark 21.6.1 The conditions of the theorem on smoothness of the transition func-
tion P can actually be proved under the assumption that a and b are continuous,
</p>
<p>b &ge; b0 &gt; 0, |a| &le; c(|x| + 1) and b2 &le; c(|x| + 1).</p>
<p/>
</div>
<div class="page"><p/>
<p>21.6 Diffusion Processes 605
</p>
<p>Proof of Theorem 21.6.1 For brevity&rsquo;s sake denote by P &prime;t , P
&prime;
x , and P
</p>
<p>&prime;&prime;
x the partial
</p>
<p>derivatives &part;P
&part;t
</p>
<p>, &part;P
&part;x
</p>
<p>and &part;
2P
</p>
<p>&part;x2
, respectively, and make use of the relation
</p>
<p>P(t;y,B)&minus; P(t;x,B)
</p>
<p>= (y &minus; x)P &prime;x +
(y &minus; x)2
</p>
<p>2
P &prime;&prime;x +
</p>
<p>(y &minus; x)2
2
</p>
<p>[
P &prime;&prime;x (t;yx,B)&minus; P &prime;&prime;x (t;x,B)
</p>
<p>]
,
</p>
<p>yx &isin; (x, y). (21.6.3)
</p>
<p>Then by the Chapman&ndash;Kolmogorov equation
</p>
<p>P(t +∆;x,B)&minus; P(t;x,B)=
&int;
</p>
<p>P(∆;x, dy)
[
P(t;y,B)&minus; P(t;x,B)
</p>
<p>]
</p>
<p>= a(x)P &prime;x∆+
b2(x)
</p>
<p>2
P &prime;&prime;x ∆+ o(∆)+R, (21.6.4)
</p>
<p>where
</p>
<p>R =
&int;
</p>
<p>(y &minus; x)2
2
</p>
<p>[
P &prime;&prime;x (t;yx,B)&minus; P &prime;&prime;x (t;x,B)
</p>
<p>]
P(∆;x, dy)=
</p>
<p>&int;
</p>
<p>|y&minus;x|&le;ε
+
&int;
</p>
<p>|y&minus;x|&gt;ε
.
</p>
<p>The first integral, by virtue of the continuity of P &prime;&prime;x , does not exceed
</p>
<p>δ(ε)
</p>
<p>[
b2(x)
</p>
<p>2
∆+ o(∆)
</p>
<p>]
,
</p>
<p>where δ(ε)&rarr; 0 as ε&rarr; 0; the second integral is o(∆) by condition (3a). Since ε is
arbitrary, one has R = o(∆) and it follows from the above that
</p>
<p>P &prime;t = lim
∆&rarr;0
</p>
<p>P(t +∆;x,B)&minus; P(t;x,B)
∆
</p>
<p>= a(x)P &prime;x +
b2(x)
</p>
<p>2
P &prime;&prime;x .
</p>
<p>This proves (21.6.1). The theorem is proved. �
</p>
<p>It is known from the theory of differential equations that, under wide assumptions
</p>
<p>about the coefficients a and b and for B = (&minus;&infin;, z), the Cauchy problem (21.6.1)&ndash;
(21.6.2) has a unique solution P which is infinitely many times differentiable with
</p>
<p>respect to t , x and z. From this it follows that P(t;x,B) has a density p(t;x, z)
which is the fundamental solution of (21.6.1).
</p>
<p>It is also not difficult to derive from Theorem 21.6.1 that, along with P(t;x,B),
the function
</p>
<p>u(t, x)=
&int;
</p>
<p>g(z)P (t;x, dz)= E
[
g
(
ξ (x)(t)
</p>
<p>)]
</p>
<p>will also satisfy Eq. (21.6.1) for any smooth function g with a compact support,
</p>
<p>ξ (x)(t) being the diffusion process with the initial value ξ (x)(0)= x.
In the proof of Theorem 21.6.1 we considered (see (21.6.4)) the time increment
</p>
<p>∆ preceding the main time interval. In this connection Eqs. (21.6.1) are called back-
ward Kolmogorov equations. Forward equations can be derived in a similar way.</p>
<p/>
</div>
<div class="page"><p/>
<p>606 21 Markov Processes
</p>
<p>Theorem 21.6.2 (Forward Kolmogorov equations) Let the transition density
p(t;x, y) be such that the derivatives
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>[
a(y)p(t;x, y)
</p>
<p>]
and
</p>
<p>&part;2
</p>
<p>&part;y2
</p>
<p>[
b2(y)p(t;x, y)
</p>
<p>]
</p>
<p>exist and are continuous. Then p(t, x, y) satisfies the equation
</p>
<p>Dp := &part;p
&part;t
</p>
<p>+ &part;
&part;y
</p>
<p>[
a(y)p(t;x, y)
</p>
<p>]
&minus; 1
</p>
<p>2
</p>
<p>&part;2
</p>
<p>&part;y2
</p>
<p>[
b2(y)p(t;x, y)
</p>
<p>]
= 0. (21.6.5)
</p>
<p>Proof Let g(y) be a smooth function with a bounded support,
</p>
<p>u(t, x) := Eg
(
ξ (x)(t)
</p>
<p>)
=
&int;
</p>
<p>g(y)p(x; t, y) dy.
</p>
<p>Then
</p>
<p>u(t +∆,x)&minus; u(t, x)
</p>
<p>=
&int;
</p>
<p>p(t;x, z)
[
p(∆; z, y)g(y) dy &minus;
</p>
<p>&int;
p(∆,z, y)g(z) dy
</p>
<p>]
dz. (21.6.6)
</p>
<p>Expanding the difference g(y)&minus; g(z) into a series, we obtain in the same way as in
the proof of Theorem 21.4.1 that, by virtue of properties (1)&ndash;(3), the expression in
</p>
<p>the brackets is
[
a(z)g&prime;(z)+ b
</p>
<p>2(z)
</p>
<p>2
g&prime;&prime;(z)
</p>
<p>]
∆+ o(∆).
</p>
<p>This implies that there exists the derivative
</p>
<p>&part;u
</p>
<p>&part;t
=
&int;
</p>
<p>p(t;x, z)
[
a(z)g&prime;(z) dz+ 1
</p>
<p>2
</p>
<p>b2(z)
</p>
<p>2
g&prime;&prime;(z)
</p>
<p>]
dz.
</p>
<p>Integrating by parts we get
</p>
<p>&part;u
</p>
<p>&part;t
=
&int; {
</p>
<p>&minus; &part;
&part;z
</p>
<p>[
a(z)p(t;x, z)
</p>
<p>]
+ 1
</p>
<p>2
</p>
<p>&part;
</p>
<p>&part;z2
</p>
<p>[
b2(z)p(t;x, z)
</p>
<p>]}
g(z) dz= 0
</p>
<p>or, which is the same,
&int;
</p>
<p>Dp(t;x, z)g(z) dz= 0.
</p>
<p>Since g is arbitrary, (21.6.5) follows. The theorem is proved. �
</p>
<p>As in the case of discrete X, the difference between the forward and backward
</p>
<p>Kolmogorov equations becomes more graphical for non-homogeneous diffusion
</p>
<p>processes, when the transition probabilities P(s, x; t,B) depend on two time vari-
ables, while a and b in conditions (1)&ndash;(3) are functions of s and x. Then the back-
</p>
<p>ward Kolmogorov equation (for densities) will relate the derivatives of the transition
</p>
<p>densities p(s, x; t, y) with respect to the first two variables, while the forward equa-
tion will hold for the derivatives with respect to the last two variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.6 Diffusion Processes 607
</p>
<p>We return to homogeneous diffusion processes. One can study conditions ensur-
</p>
<p>ing the existence of the limiting stationary distribution of ξ (x)(t) as t &rarr;&infin; which
is independent of x using the same approach as in Sect. 21.2. Theorem 21.2.3 will
</p>
<p>remain valid (one simply has to replace i0 in it with x0, in agreement with the no-
</p>
<p>tation of the present section). The proof of Theorem 21.2.3 also remains valid, but
</p>
<p>will need a somewhat more precise argument (in the new situation, on the event Bdv
one has ξ(t &minus; v) &isin; dx0 instead of ξ(t &minus; v)= x0).
</p>
<p>If the stationary distribution density
</p>
<p>lim
t&rarr;&infin;
</p>
<p>p(t;x, y)= p(y) (21.6.7)
</p>
<p>exists, how could one find it? Since the dependence of p(t;x, y) of t and x van-
ishes as t &rarr;&infin;, the backward Kolmogorov equations turn into the identity 0 = 0 as
t &rarr;&infin;. Turning to the forward equations and passing in (21.6.6) to the limit first as
t &rarr;&infin; and then as ∆&rarr; 0, we come, using the same argument as in the proof of
Theorem 21.2.3, to the following conclusion.
</p>
<p>Corollary 21.6.1 If (21.6.7) and the conditions of Theorem 21.6.2 hold, then the
stationary density p(y) satisfies the equation
</p>
<p>&minus;
[
a(y)p(y)
</p>
<p>]&prime; + 1
2
</p>
<p>[
b2(y)p(y)
</p>
<p>]&prime;&prime; = 0
</p>
<p>(which is obtained from (21.6.5) if we put &part;p
&part;t
</p>
<p>= 0).
</p>
<p>Example 21.6.1 The Ornstein&ndash;Uhlenbeck process
</p>
<p>ξ (x)(t)= xeat + σeatw
(
</p>
<p>1 &minus; e&minus;2at
2a
</p>
<p>)
,
</p>
<p>where w(u) is the standard Wiener process, is a homogeneous diffusion process
</p>
<p>with the transition density
</p>
<p>p(t;x, y)= 1&radic;
2πσ(t)
</p>
<p>exp
</p>
<p>{
&minus; (y &minus; xe
</p>
<p>at )2
</p>
<p>2σ 2(t)
</p>
<p>}
, σ 2(t)= σ
</p>
<p>2
</p>
<p>2a
</p>
<p>(
e2at &minus; 1
</p>
<p>)
.
</p>
<p>(21.6.8)
</p>
<p>We leave it to the reader to verify that this process has coefficients a(x) = ax,
b(x) = σ = const, and that function (21.6.8) satisfies the forward and backward
equations. For a &lt; 0, there exists a stationary process (the definition is given in the
</p>
<p>next chapter)
</p>
<p>ξ(t)= σeatw
(
e&minus;2at
</p>
<p>2a
</p>
<p>)
,
</p>
<p>of which the density (which does not depend on t) is equal to
</p>
<p>p(y)= lim
t&rarr;&infin;
</p>
<p>p(x; t, y)= 1&radic;
2πσ(&infin;)
</p>
<p>exp
</p>
<p>{
&minus; y
</p>
<p>2
</p>
<p>2σ 2(&infin;)
</p>
<p>}
, σ (&infin;)=&minus;σ
</p>
<p>2
</p>
<p>2a
.</p>
<p/>
</div>
<div class="page"><p/>
<p>608 21 Markov Processes
</p>
<p>In conclusion of this section we will consider the problem, important for various
</p>
<p>applications, of finding the probability that the trajectory of a diffusion process will
</p>
<p>not leave a given strip. For simplicity&rsquo;s sake we confine ourselves to considering
</p>
<p>this problem for the Wiener process. Let c &gt; 0 and d &lt; 0.
</p>
<p>Put
</p>
<p>U(t;x,B) := P
(
w(x)(u) &isin; (d, c) for all u &isin; [0, t]; w(x)(t) &isin; B
</p>
<p>)
</p>
<p>= P
(
</p>
<p>sup
u&le;t
</p>
<p>w(x)(u) &lt; c, inf
u&le;t
</p>
<p>w(x)(u) &gt; d, w(x)(t) &isin; B
)
.
</p>
<p>Leaving out the verification of the fact that the function U is twice continuously
</p>
<p>differentiable, we will only prove the following proposition.
</p>
<p>Theorem 21.6.3 The function U satisfies Eq. (21.6.1) with the initial condition
</p>
<p>U(0;x,B)= IB(x) (21.6.9)
and boundary conditions
</p>
<p>U(t; c,B)=U(t;d,B)= 0. (21.6.10)
</p>
<p>Proof First of all note that the function U(t;x,B) for x &isin; (d, c) satisfies conditions
(1)&ndash;(3) imposed on the transition function P(t;x,B). Indeed, consider, for instance,
property (1).
</p>
<p>We have to verify that
&int; c
</p>
<p>d
</p>
<p>(y &minus; x)U(∆;x, dy)=∆a(x)+ o(∆) (21.6.11)
</p>
<p>(with a(x)= 0 in our case). But U(t, x,B)= P(t;x,B)&minus; V (t;x,B), where
</p>
<p>V (t;x,B)= P
({
</p>
<p>sup
u&le;t
</p>
<p>w(x)(u)&ge; c or inf
u&le;t
</p>
<p>w(x)(u)&le; d
}
&cap;
{
w(x)(t) &isin; B
</p>
<p>})
,
</p>
<p>and
&int; c
</p>
<p>d
</p>
<p>(y &minus; x)V (∆;x, dy)
</p>
<p>&le; max(c,&minus;d)
[
P
(
</p>
<p>sup
u&le;∆
</p>
<p>w(x)(u)&ge; c
)
+ P
</p>
<p>(
inf
u&le;∆
</p>
<p>w(x)(u)&le; d
)]
</p>
<p>.
</p>
<p>The first probability in the brackets is given, as we know (see (20.2.1) and Theo-
</p>
<p>rem 19.2.2), by the value
</p>
<p>2P
(
w(x)(∆) &gt; c
</p>
<p>)
= 2P
</p>
<p>(
w(1) &gt;
</p>
<p>c&minus; x&radic;
∆
</p>
<p>)
&sim; 2&radic;
</p>
<p>2πz
e&minus;z
</p>
<p>2/2, z= c&minus; x&radic;
∆
</p>
<p>.
</p>
<p>For any x &lt; c and k &gt; 0, it is o(∆k). The same holds for the second probability.
</p>
<p>Therefore (21.6.11) is proved. In the same way one can verify properties (2) and (3).
</p>
<p>Further, because by the total probability formula, for x &isin; (d, c),
</p>
<p>U(t +∆;x,B)=
&int; c
</p>
<p>d
</p>
<p>U(∆;x, dy)U(t;y,B),</p>
<p/>
</div>
<div class="page"><p/>
<p>21.6 Diffusion Processes 609
</p>
<p>using an expansion of the form (21.6.3) for the function U , we obtain in the same
</p>
<p>way as in (21.6.4) that
</p>
<p>U(t +∆;x,B)&minus;U(t;x,B)=
&int;
</p>
<p>U(∆;x, dy)
[
U(t;y,B)&minus;U(t;x,B)
</p>
<p>]
</p>
<p>= a(x)&part;U
&part;x
</p>
<p>∆+ b
2(x)
</p>
<p>2
</p>
<p>&part;2U
</p>
<p>&part;x2
∆+ o(∆).
</p>
<p>This implies that &part;U
&part;t
</p>
<p>exists and that Eq. (21.6.1) holds for the function U .
</p>
<p>That the boundary and initial conditions are met is obvious. The theorem is
</p>
<p>proved. �
</p>
<p>The reader can verify that the function
</p>
<p>u(t;x, y) := &part;
&part;y
</p>
<p>U
(
t;x, (&minus;&infin;, y)
</p>
<p>)
, y &isin; (d, c),
</p>
<p>playing the role of the fundamental solution to the boundary problem (21.6.9)&ndash;
</p>
<p>(21.6.10) (the function u satisfies (21.6.1) with the boundary conditions (21.6.10)
</p>
<p>and the initial conditions degenerating into the δ-function), is equal to
</p>
<p>u(t;x, y)= 1&radic;
2πt
</p>
<p>[ &infin;&sum;
</p>
<p>k=&minus;&infin;
exp
</p>
<p>{
&minus;[y + 2k(c&minus; d)]
</p>
<p>2
</p>
<p>2t
</p>
<p>}
</p>
<p>&minus;
&infin;&sum;
</p>
<p>k=0
exp
</p>
<p>{
&minus;[y &minus; 2c&minus; 2k(c&minus; d)]
</p>
<p>2
</p>
<p>2t
</p>
<p>}
</p>
<p>&minus;
&infin;&sum;
</p>
<p>k=0
exp
</p>
<p>{
&minus;[y &minus; 2d &minus; 2k(c&minus; d)]
</p>
<p>2
</p>
<p>2t
</p>
<p>}]
.
</p>
<p>This expression can also be obtained directly from probabilistic considerations (see,
</p>
<p>e.g., [32]).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 22
</p>
<p>Processes with Finite Second Moments.
Gaussian Processes
</p>
<p>Abstract The chapter is devoted to the classical &ldquo;second-order theory&rdquo; of time-
</p>
<p>homogeneous processes with finite second moments. Section 22.1 explores the re-
</p>
<p>lationships between the covariance function properties and those of the process itself
</p>
<p>and proves the ergodic theorem (in quadratic mean) for processes with covariance
</p>
<p>functions vanishing at the infinity. Section 22.2 is devoted to the special case of
</p>
<p>Gaussian processes, while Sect. 22.3 solves the best linear prediction problem.
</p>
<p>22.1 Processes with Finite Second Moments
</p>
<p>Let {ξ(t), &minus;&infin; &lt; t &lt;&infin;} be a random process for which there exist the moments
a(t)= Eξ(t) and R(t, u)= Eξ(t)ξ(u). Since it is always possible to study the pro-
cess ξ(t) &minus; a(t) instead of ξ(t), we can assume without loss of generality that
a(t)&equiv; 0.
</p>
<p>Definition 22.1.1 The function R(t, u) is said to be the covariance function of the
process ξ(t).
</p>
<p>Definition 22.1.2 A function R(t, u) is said to be nonnegative (positive) definite if,
for any k; u1, . . . , uk ; a1, . . . , ak 
= 0,
</p>
<p>&sum;
</p>
<p>i,j
</p>
<p>aiajR(ui, uj )&ge; 0 (&gt; 0).
</p>
<p>It is evident that the covariance function R(t, u) is nonnegative definite, because
</p>
<p>&sum;
</p>
<p>i,j
</p>
<p>aiajR(ui, uj )= E
(&sum;
</p>
<p>i,j
</p>
<p>aj ξ(ui)
</p>
<p>)2
&ge; 0.
</p>
<p>Definition 22.1.3 A process ξ(t) is said to be unpredictable if no linear combination
of the variables ξ(u1), . . . , ξ(uk) is zero with probability 1, i.e. if there exist no
</p>
<p>u1, . . . , uk ; a1, . . . , ak such that
</p>
<p>P
</p>
<p>(&sum;
</p>
<p>i
</p>
<p>aiξ(ui)= 0
)
= 1.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9_22, &copy; Springer-Verlag London 2013
</p>
<p>611</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9_22">http://dx.doi.org/10.1007/978-1-4471-5201-9_22</a></div>
</div>
<div class="page"><p/>
<p>612 22 Processes with Finite Second Moments. Gaussian Processes
</p>
<p>If R(t, u) is the covariance function of an unpredictable process, then R(t, u)
is positive definite. We will see below that the converse assertion is also true in a
certain sense.
</p>
<p>Unpredictability means that we cannot represent ξ(tk) as a linear combination of
</p>
<p>ξ(tj ), j &lt; k.
</p>
<p>Example 22.1.1 The process ξ(t) =
&sum;N
</p>
<p>k=1 ξkgk(t), where gk(t) are linearly inde-
pendent and ξk are independent, is not unpredictable, because from ξ(t1), . . . , ξ(tN )
</p>
<p>we can determine the values ξ(t) for all other t .
</p>
<p>Consider the Hilbert space L2 of all random variables η on 〈Ω,F,P〉 having
finite second moments, Eη = 0, endowed with the inner product (η1, η2) = Eη1η2
corresponding to the distance ‖η1 &minus; η2‖ = [E(η1 &minus; η2)2]1/2. Convergence in L2 is
obviously convergence in mean quadratic.
</p>
<p>A random process ξ(t) may be thought of as a curve in L2.
</p>
<p>Definition 22.1.4 A random process ξ(t) is said to be wide sense stationary if the
function R(t, u) =: R(t &minus; u) depends on the difference t &minus; u only. The function
R(s) is called nonnegative (positive) definite if the function R(t, t + s) is of the re-
spective type. For brevity, we will often call wide sense stationary processes simply
</p>
<p>stationary.
</p>
<p>For the Wiener process, R(t, u) = Ew(t)w(u) = min(t, u), so that w(t) cannot
be stationary. But the process ξ(t)=w(t + 1)&minus;w(t) will already be stationary.
</p>
<p>It is obvious that, for a stationary process, the function R(s) is even and Eξ2(t)=
R(0) = const. For simplicity&rsquo;s sake, put R(0) = 1. Then, by the Cauchy&ndash;Bunja-
kovsky inequality,
</p>
<p>∣∣R(s)
∣∣=
</p>
<p>∣∣Eξ(t)ξ(t + s)
∣∣&le;
</p>
<p>[
Eξ2(t)Eξ2(t + s)
</p>
<p>]1/2 =R(0)= 1.
</p>
<p>Theorem 22.1.1
</p>
<p>(1) A process ξ(t) is continuous in mean quadratic (ξ(t +∆) (2)&minus;&rarr; ξ(t) as ∆&rarr; 0)
if and only if the function R(u) is continuous at zero.
</p>
<p>(2) If the function R(u) is continuous at zero, then it is continuous everywhere.
</p>
<p>Proof
</p>
<p>(1)
∥∥ξ(t +∆)&minus; ξ(t)
</p>
<p>∥∥2 = E
(
ξ(t +∆)&minus; ξ(t)
</p>
<p>)2 = 2R(0)&minus; 2R(∆).
(2) R(t +∆)&minus;R(t)= E
</p>
<p>(
ξ(t +∆)ξ(0)&minus; ξ(t)ξ(0)
</p>
<p>)
</p>
<p>=
(
ξ(0), ξ(t +∆)&minus; ξ(t)
</p>
<p>)
&le;
∥∥ξ(t +∆)&minus; ξ(t)
</p>
<p>∥∥
</p>
<p>=
&radic;
</p>
<p>2
(
R(0)&minus;R(∆)
</p>
<p>)
. (22.1.1)
</p>
<p>The theorem is proved. �
</p>
<p>A process ξ(t) continuous in mean quadratic will be stochastically continuous,
</p>
<p>as we can see from Chaps. 6 and 18. The continuity in mean quadratic does not,</p>
<p/>
</div>
<div class="page"><p/>
<p>22.1 Processes with Finite Second Moments 613
</p>
<p>however, imply path-wise continuity. The reader can verify this by considering the
</p>
<p>example of the process
</p>
<p>ξ(t)= η(t + 1)&minus; η(t)&minus; 1,
where η(t) is the Poisson process with parameter 1. For that process, the covariance
</p>
<p>function
</p>
<p>R(t)=
{
</p>
<p>0 for t &ge; 1,
1 &minus; t for 0 &le; t &le; 1
</p>
<p>is continuous, although the trajectories of ξ(t) are not. If
∣∣R(∆)&minus;R(0)
</p>
<p>∣∣&lt; c∆1+ε (22.1.2)
for some ε &gt; 0 then, by the Kolmogorov theorem (see Theorem 18.2.1), ξ(t) has
</p>
<p>a continuous modification. From this it follows, in particular, that if R(t) is twice
</p>
<p>differentiable at the point t = 0, then the trajectories of ξ(t) may be assumed con-
tinuous. Indeed, in that case, since R(t) is even, one has
</p>
<p>R&prime;(0)= 0 and R(∆)&minus;R(0)&sim; 1
2
R&prime;&prime;(0)∆2.
</p>
<p>As a whole, the smoother the covariance function is at zero, the smoother the
</p>
<p>trajectories of ξ(t) are.
</p>
<p>Assume that the trajectories of ξ(t) are measurable (for example, belong to the
</p>
<p>space D).
</p>
<p>Theorem 22.1.2 (The simplest ergodic theorem) If
</p>
<p>R(s)&rarr; 0 as s &rarr;&infin;, (22.1.3)
then
</p>
<p>ζT :=
1
</p>
<p>T
</p>
<p>&int; T
</p>
<p>0
</p>
<p>ξ(t) dt
(2)&minus;&rarr; 0.
</p>
<p>Proof Clearly,
</p>
<p>‖ζT ‖2 =
1
</p>
<p>T 2
</p>
<p>&int; T
</p>
<p>0
</p>
<p>&int; T
</p>
<p>0
</p>
<p>R(t &minus; u)dt du.
</p>
<p>Since R(s) is even,
</p>
<p>J :=
&int; T
</p>
<p>0
</p>
<p>&int; T
</p>
<p>0
</p>
<p>R(t &minus; u)dt du= 2
&int; T
</p>
<p>0
</p>
<p>&int; T
</p>
<p>u
</p>
<p>R(t &minus; u)dt du.
</p>
<p>Making the orthogonal change of variables v = (t &minus; u)/
&radic;
</p>
<p>2, s = (t + u)/
&radic;
</p>
<p>2, we
</p>
<p>obtain
</p>
<p>J &le; 2
&int; T/&radic;2
</p>
<p>s=0
</p>
<p>&int; T/&radic;2
</p>
<p>v=0
R(v
</p>
<p>&radic;
2) dv ds &le; 2T
</p>
<p>&int; T
</p>
<p>0
</p>
<p>R(v)dv,
</p>
<p>‖ζT ‖2 &le;
2
</p>
<p>T
</p>
<p>&int; T
</p>
<p>0
</p>
<p>R(v)dv&rarr; 0.
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>614 22 Processes with Finite Second Moments. Gaussian Processes
</p>
<p>Example 22.1.2 The stationary white noise process ξ(t) is defined as a process
with independent values, i.e. a process such that, for any t1, . . . , tn, the variables
</p>
<p>ξ(t1), . . . , ξ(tn) are independent. For such a process,
</p>
<p>R(t)=
{
</p>
<p>1 for t = 0,
0 for t 
= 0,
</p>
<p>and thus condition (22.1.3) is met. However, one cannot apply Theorem 22.1.2 here,
</p>
<p>for the trajectories of ξ(t) will be non-measurable with probability 1 (for example,
</p>
<p>the set B = {t : ξ(t) &gt; 0} is non-measurable with probability 1).
</p>
<p>Definition 22.1.5 A process ξ(t) is said to be strict sense stationary if, for any
t1, . . . , tk , the distribution of (ξ(t1 + u), ξ(t2 + u), . . . , ξ(tk + u)) is independent
of u.
</p>
<p>It is obvious that if ξ(t) is a strict sense stationary process then
</p>
<p>Eξ(t)ξ(u)= Eξ(t &minus; u)ξ(0)=R(t &minus; u),
</p>
<p>and ξ(t) will be wide sense stationary. The converse is, of course, not true. However,
</p>
<p>there exists a class of processes for which both concepts of stationarity coincide.
</p>
<p>22.2 Gaussian Processes
</p>
<p>Definition 22.2.1 A process ξ(t) is said to be Gaussian if its finite-dimensional
distributions are normal.
</p>
<p>We again assume that Eξ(t)= 0 and R(t, u)= Eξ(t)ξ(u).
The finite-dimensional distributions are completely determined by the ch.f.s (λ=
</p>
<p>(λ1, . . . , λk), ξ = (ξ(t1), . . . , ξ(tk)))
</p>
<p>Eei(λ,ξ) = Eei
&sum;
</p>
<p>j λj ξ(tj ) = e&minus; 12λRλT ,
</p>
<p>where R = ‖R(ti, tj )‖ and the superscript T stands for transposition, so that
</p>
<p>λRλT =
&sum;
</p>
<p>i,j
</p>
<p>λiλjR(ti, tj ).
</p>
<p>Thus for a Gaussian process the finite-dimensional distributions are completely
</p>
<p>determined by the covariance function R(t, u).
</p>
<p>We saw that for an unpredictable process ξ(t), the function R(t, u) is positive
</p>
<p>definite. A converse assertion may be stated in the following form.
</p>
<p>Theorem 22.2.1 If the function R(t, u) is positive definite, then there exists an un-
predictable Gaussian process with the covariance function R(t, u).</p>
<p/>
</div>
<div class="page"><p/>
<p>22.2 Gaussian Processes 615
</p>
<p>Proof For arbitrary t1, . . . , tk , define the finite-dimensional distribution of the vector
ξ(t1), . . . , ξ(tk) via the density
</p>
<p>pt1,...,tk (x1, . . . , xk)=
&radic;
|A|
</p>
<p>(2π)k/2
exp
</p>
<p>{
&minus;1
</p>
<p>2
xAxT
</p>
<p>}
,
</p>
<p>where A is the matrix inverse to the covariance matrix R = ‖R(ti, tj )‖ (see
Sect. 7.6) and |A| is the determinant of A. These distributions will clearly
be consistent, because the covariance matrices are consistent (the matrix for
</p>
<p>ξ(t1), . . . , ξ(tk&minus;1) is a submatrix of R). It remains to make use of the Kolmogorov
theorem. The theorem is proved. �
</p>
<p>Example 22.2.1 Let w(t) be the standard Wiener process. The process
</p>
<p>w0(t)=w(t)&minus; tw(1), t &isin; [0,1],
</p>
<p>is called the Brownian bridge (its &ldquo;ends are fixed&rdquo;: w0(0) = w0(1) = 0). The co-
variance function of w0(t) is equal to
</p>
<p>R(t, u)= E
(
w(t)&minus; tw(1)
</p>
<p>)(
w(u)&minus; uw(1)
</p>
<p>)
= t (1 &minus; u)
</p>
<p>for u&ge; t .
</p>
<p>A Gaussian wide sense stationary process ξ(t) is strict sense stationary. This
</p>
<p>immediately follows from the fact that for R(t, u)=R(t&minus;u) the finite-dimensional
distributions of ξ(t) become invariant with respect to time shift:
</p>
<p>pt1,...,tk (x1, . . . , xk)= pt1+u,...,tk+u(x1, . . . , xk)
</p>
<p>since ‖R(ti + u, tj + u)‖ = ‖R(ti, tj )‖.
If ξ(t) is a Gaussian process, then conditions ensuring the smoothness of its
</p>
<p>trajectories can be substantially relaxed in comparison with (22.1.2).
</p>
<p>Let for simplicity&rsquo;s sake the Gaussian process ξ(t) be stationary.
</p>
<p>Theorem 22.2.2 If, for h &lt; 1,
</p>
<p>∣∣R(h)&minus;R (0)
∣∣&lt; c
</p>
<p>(
log
</p>
<p>1
</p>
<p>h
</p>
<p>)&minus;α
, α &gt; 3, c &lt;&infin;,
</p>
<p>then the trajectories of ξ(t) can be assumed continuous.
</p>
<p>Proof We make use of Theorem 18.2.2 and put ε(h) = (log 1
h
)&minus;β for 1 &lt; β &lt;
</p>
<p>(α &minus; 1)/2 (we take logarithms to the base 2). Then
&infin;&sum;
</p>
<p>n=1
ε
(
2&minus;n
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=1
n&minus;β &lt;&infin;,
</p>
<p>and, by (22.1.1),</p>
<p/>
</div>
<div class="page"><p/>
<p>616 22 Processes with Finite Second Moments. Gaussian Processes
</p>
<p>P
(∣∣ξ(t + h)&minus; ξ(t)
</p>
<p>∣∣&gt; ε(h)
)
= 2
</p>
<p>[
1 &minus;Φ
</p>
<p>(
ε(h)&radic;
</p>
<p>2(1 &minus;R(h))
</p>
<p>)]
</p>
<p>&le; 2
[
</p>
<p>1 &minus;Φ
(
cε(h)
</p>
<p>(
log
</p>
<p>1
</p>
<p>h
</p>
<p>)α/2)]
= 2
</p>
<p>[
1 &minus;Φ
</p>
<p>(
c
</p>
<p>(
log
</p>
<p>1
</p>
<p>h
</p>
<p>)α/2&minus;β)]
.
</p>
<p>(22.2.1)
</p>
<p>Since the argument of Φ increases unboundedly as h &rarr; 0, γ = α &minus; 2β &gt; 1, and
by (19.3.1)
</p>
<p>1 &minus;Φ(x)&sim; 1&radic;
2πx
</p>
<p>e&minus;x
2/2 as x &rarr;&infin;,
</p>
<p>we see that the right-hand side of (22.2.1) does not exceed
</p>
<p>q(h) := c1
(
</p>
<p>log
1
</p>
<p>h
</p>
<p>)β&minus;α/2
exp
</p>
<p>{
&minus;c2
</p>
<p>(
log
</p>
<p>1
</p>
<p>h
</p>
<p>)α&minus;2β}
,
</p>
<p>so that
</p>
<p>&infin;&sum;
</p>
<p>n=1
2nq
</p>
<p>(
2&minus;n
</p>
<p>)
= c1
</p>
<p>&infin;&sum;
</p>
<p>n=1
n&minus;γ /2 exp
</p>
<p>{
&minus;c2nγ + n ln 2
</p>
<p>}
&lt;&infin;,
</p>
<p>because c2 &gt; 0 and γ &gt; 1. The conditions of Theorem 18.2.2 are met, and so The-
</p>
<p>orem 22.2.2 is proved. �
</p>
<p>22.3 Prediction Problem
</p>
<p>Suppose the distribution of a process ξ(t) is known, and one is given the trajectory of
</p>
<p>ξ(t) on a set B &sub; (&minus;&infin;, t], B being either an interval or a finite collection of points.
What could be said about the value ξ(t + u)? Our aim will be to find a random
variable ζ , which is FB = σ(ξ(v), v &isin; B)-measurable (and called a prediction) and
such that E(ξ(t + u)&minus; ζ )2 assumes the smallest possible value. The answer to that
problem is actually known (see Sect. 4.8):
</p>
<p>ζ = E
(
ξ(t + u)
</p>
<p>∣∣FB
)
.
</p>
<p>Let ξ(t) be a Gaussian process, B = {t1, . . . , tk}, t1 &lt; t2 &lt; &middot; &middot; &middot;&lt; tk &lt; t0 = t + u,
A = (σ 2)&minus;1 = ‖aij‖ and σ 2 = ‖Eξ (ti) ξ (tj )‖i,j=1,...,k,0. Then the distribution of
the vector (ξ(t1), . . . , ξ(t0)) has the density
</p>
<p>f (x1, . . . , xk, x0)=
&radic;
|A|
</p>
<p>(2π)(k+1)/2
exp
</p>
<p>{
&minus;1
</p>
<p>2
</p>
<p>&sum;
</p>
<p>i,j
</p>
<p>xixjaij
</p>
<p>}
,
</p>
<p>and the conditional distribution of ξ(t0) given ξ(t1), . . . , ξ(tk) has density equal to
</p>
<p>the ratio
</p>
<p>f (x1, . . . , xk, x0)&int;&infin;
&minus;&infin; f (x1, . . . , xk, x0) dx0
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>22.3 Prediction Problem 617
</p>
<p>The exponential part of this ratio has the form
</p>
<p>exp
</p>
<p>{
&minus;
a00x
</p>
<p>2
0
</p>
<p>2
&minus;
</p>
<p>k&sum;
</p>
<p>j=1
x0xjaj0
</p>
<p>}
.
</p>
<p>This means that the conditional distribution under consideration is the normal
</p>
<p>law �α,d2 , where
</p>
<p>α =&minus;
&sum;
</p>
<p>j
</p>
<p>xjaj0
</p>
<p>a00
, d2 = 1
</p>
<p>a00
.
</p>
<p>Thus, in our case the best prediction ζ is equal to
</p>
<p>ζ =&minus;
k&sum;
</p>
<p>j=1
</p>
<p>ξ(tj )a0j
</p>
<p>a00
.
</p>
<p>The mean quadratic error of this prediction equals
&radic;
</p>
<p>1/a00.
</p>
<p>We have obtained a linear prediction. In the general case, the linearity property
is usually violated.
</p>
<p>Consider now the problem of the best linear prediction in the case of an arbitrary
</p>
<p>process ξ(t) with finite second moments. For simplicity&rsquo;s sake we assume again that
</p>
<p>B = {t1, . . . , tk}.
Denote by H(ξ) the subspace of L2 generated by the random variables ξ(t),
</p>
<p>&minus;&infin; &lt; t &lt; &infin;, and by HB(ξ) the subspace of H(ξ) generated (or spanned by)
ξ(t1), . . . , ξ(tk). Elements of HB(ξ) have the form
</p>
<p>k&sum;
</p>
<p>j=1
aj ξ(tj ).
</p>
<p>The existence and the form of the best linear prediction in this case are estab-
</p>
<p>lished by the following assertion.
</p>
<p>Theorem 22.3.1 There exists a unique point ζ &isin;HB(ξ) (the projection of ξ(t + u)
onto HB(ξ), see Fig. 22.1) such that
</p>
<p>ξ(t + u)&minus; ζ &perp;HB(ξ). (22.3.1)
</p>
<p>Relation (22.3.1) is equivalent to
∥∥ξ(t + u)&minus; ζ
</p>
<p>∥∥= min
θ&isin;HB (ξ)
</p>
<p>∥∥ξ(t + u)&minus; θ
∥∥. (22.3.2)
</p>
<p>Explicit formulas for the coefficients aj in the representation ζ =
&sum;
</p>
<p>aj ξ(tj ) are
given in the proof.
</p>
<p>Proof Relation (22.3.1) is equivalent to the equations
(
ξ(t + u)&minus; ζ, ξ(tj )
</p>
<p>)
= 0, j = 1, . . . , k.</p>
<p/>
</div>
<div class="page"><p/>
<p>618 22 Processes with Finite Second Moments. Gaussian Processes
</p>
<p>Fig. 22.1 Illustration to
</p>
<p>Theorem 22.3.1: the point ζ
</p>
<p>is the projection of ξ(t + u)
onto HB (ξ)
</p>
<p>Substituting here
</p>
<p>ζ =
k&sum;
</p>
<p>l=1
alξ(tl) &isin;HB(ξ),
</p>
<p>we obtain
</p>
<p>R(t + u, tj )=
k&sum;
</p>
<p>l=1
alR(tj , tl), j = 1, . . . , k, (22.3.3)
</p>
<p>or, in vector form, Rt+u = aR, where
</p>
<p>a = (a1, . . . , ak),
Rt+u =
</p>
<p>(
R(t + u, t1), . . . ,R(t + u, tk)
</p>
<p>)
, R =
</p>
<p>∥∥R(ti, tj )
∥∥.
</p>
<p>If the process ξ(t) is unpredictable, then the matrix R is non-degenerate and
</p>
<p>Eq. (22.3.3) has a unique solution:
</p>
<p>a =Rt+uR&minus;1. (22.3.4)
If ξ(t) is not unpredictable, then either R&minus;1 still exists and then (22.3.4) holds, or
</p>
<p>R is degenerate. In that case, one has to choose from the collection ξ(t1), . . . , ξ(tk)
</p>
<p>only l &lt; k linearly independent elements for which all the above remains true after
</p>
<p>replacing k with l.
</p>
<p>The equivalence of (22.3.1) and (22.3.2) follows from the following considera-
</p>
<p>tions. Let θ be any other element of HB(ξ). Then
</p>
<p>η := θ &minus; ζ &isin;HB(ξ), η&perp; ξ(t + u)&minus; ζ,
so that
</p>
<p>∥∥ξ(t + u)&minus; θ
∥∥=
</p>
<p>∥∥ξ(t + u)&minus; ζ
∥∥+ ‖η‖ &ge;
</p>
<p>∥∥ξ(t + u)&minus; ζ
∥∥.
</p>
<p>The theorem is proved. �
</p>
<p>Remark 22.3.1 It can happen (in the case where the process ξ(t) is not unpre-
dictable) that ξ(t + u) &isin; HB(ξ). Then the error of the prediction ζ will be equal
to zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 1
</p>
<p>Extension of a Probability Measure
</p>
<p>In this appendix we will prove Carath&eacute;odory&rsquo;s theorem, which was used in Sect. 2.1.
</p>
<p>Let A be an algebra of subsets of Ω on which a probability measure P, i.e., a real-
</p>
<p>valued function satisfying conditions P1&ndash;P3 of Chap. 2, is given. Let P denote the
</p>
<p>class of all subsets of Ω . For any A &isin; P, there always exists a sequence {An}&infin;n=1
of disjoint sets from A such that
</p>
<p>⋃&infin;
n=1 An &sup; A (it suffices to take A1 = Ω and
</p>
<p>An =&empty;, n &ge; 2). Denote by γ (A) the class of all such sequences and introduce on
P the real-valued function
</p>
<p>P&lowast;(A) := inf
{ &infin;&sum;
</p>
<p>n=1
P(An); {An} &isin; γ (A)
</p>
<p>}
.
</p>
<p>This function (the outer measure on P induced by the measure P on A) has the
</p>
<p>following properties:
</p>
<p>(1) P&lowast;(A)&le; P&lowast;(B)&le; 1 if A&sub; B .
(2) P&lowast;(
</p>
<p>⋃&infin;
n=1 An)=
</p>
<p>&sum;&infin;
n=1 P(An) if the sets An &isin;A, n= 1,2, . . . , are disjoint.
</p>
<p>(3) P&lowast;(
⋃&infin;
</p>
<p>n=1 An)&le;
&sum;&infin;
</p>
<p>n=1 P
&lowast;(An) for any A1,A2, . . . &isin; P.
</p>
<p>Property (1) is obvious. Property (2) is established by the following argument.
</p>
<p>Let {Bn} be any sequence from γ (A), where A=
⋃&infin;
</p>
<p>n=1 An. Since
⋃&infin;
</p>
<p>m=1 AnBm =
An &isin;A, one has P(An)=
</p>
<p>&sum;&infin;
m=1 P(AnBm). Therefore,
</p>
<p>&infin;&sum;
</p>
<p>n=1
P(An)=
</p>
<p>&sum;
</p>
<p>n
</p>
<p>&sum;
</p>
<p>m
</p>
<p>P(AnBm)=
&infin;&sum;
</p>
<p>m=1
</p>
<p>&infin;&sum;
</p>
<p>n=1
P(AnBm).
</p>
<p>But, for each N &lt;&infin;,
</p>
<p>N&sum;
</p>
<p>n=1
P(AnBm)&le; P(Bm).
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>619</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>620 1 Extension of a Probability Measure
</p>
<p>Hence this equality holds for N =&infin; as well and, for any sequence {Bm}&infin;m=1&isin;γ (A),
</p>
<p>N&sum;
</p>
<p>n=1
P(An)&le; P(Bm).
</p>
<p>This implies that P&lowast;(A)&ge;
&sum;&infin;
</p>
<p>n=1 P(An). Because the converse inequality is obvious,
we have P&lowast;(A)=
</p>
<p>&sum;&infin;
n=1 P(An).
</p>
<p>Proof of property (3) Consider, for some ε &gt; 0, sequences {Ank}&infin;k=1 &isin; γ (An) such
that
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(Ank)&le; P&lowast;(An)+
</p>
<p>ε
</p>
<p>2n
.
</p>
<p>The sequence of sets {Ank}&infin;n,k=1 clearly contains
⋃
</p>
<p>An and therefore
</p>
<p>P&lowast;
(⋃
</p>
<p>An
</p>
<p>)
&le;
&sum;
</p>
<p>n
</p>
<p>&sum;
</p>
<p>k
</p>
<p>P(Ank)&le;
&infin;&sum;
</p>
<p>n=1
P&lowast;(An)+ ε.
</p>
<p>Since ε is arbitrary, property (3) is proved. �
</p>
<p>Introduce now the binary operation of symmetric difference &oplus; on arbitrary sets
A and B from P by means of the equality
</p>
<p>A&oplus;B :=AB &cup;AB.
</p>
<p>It is not hard to see that
</p>
<p>A&oplus;B = B &oplus;A=A&oplus;B &sub;A&cup;B, A&oplus;A=&empty;,
A&oplus;&empty;=A, (A&oplus;B)&oplus;C =A&oplus; (B &oplus;C).
</p>
<p>With the help of this operation and the function P&lowast;, we introduce on P a distance ρ
by putting, for any A,B &isin; P,
</p>
<p>ρ(A,B) := P&lowast;(A&oplus;B).
</p>
<p>This construction is quite similar to the one used in Sect. 3.4 (we considered there
</p>
<p>the distance d(A,B)= P(A&oplus;B) between measurable sets A and B). The properties
of the distance ρ are the same as in (3.4.2). We will need the following properties:
</p>
<p>(1) ρ(A,B)= ρ(B,A)&ge; 0, ρ(A,A)= 0,
(2) ρ(A,B)= ρ(A,B),
(3) ρ(AB,CD)&le; ρ(A,C)+ ρ(B,D),
(4) ρ(
</p>
<p>⋃
Ak,
</p>
<p>⋃
Bk)&le;
</p>
<p>&sum;
k ρ(Ak,Bk).
</p>
<p>We also note that</p>
<p/>
</div>
<div class="page"><p/>
<p>1 Extension of a Probability Measure 621
</p>
<p>(5) |P&lowast;(A) &minus; P&lowast;(B)| &le; ρ(A,B), and therefore P&lowast;(&middot;) is a uniformly continuous
function with respect to ρ.
</p>
<p>Properties (1)&ndash;(3) were listed in (3.4.2); in the present context, they are proved in
</p>
<p>exactly the same way based on the properties of the measure P&lowast;. Property (4) follows
from property (3) of the measure P&lowast; and the relation (we put here A =
</p>
<p>⋃
An and
</p>
<p>B =
⋃
</p>
<p>Bn)
</p>
<p>A&oplus;B &sub;
⋃
</p>
<p>(An &oplus;Bn),
</p>
<p>because
</p>
<p>A&oplus;B =
[(⋃
</p>
<p>An
</p>
<p>)
&cap;
(⋂
</p>
<p>Bn
</p>
<p>)]
&cup;
[(⋂
</p>
<p>An
</p>
<p>)
&cap;
(⋃
</p>
<p>Bn
</p>
<p>)]
</p>
<p>&sub;
[⋃
</p>
<p>AnBn
</p>
<p>]
&cup;
[⋃
</p>
<p>BnAn
</p>
<p>]
=
⋃
</p>
<p>(AnBn &cup;AnBn)=
⋃
</p>
<p>(An &oplus;Bn).
</p>
<p>Property (5) follows from the fact that
</p>
<p>A&sub; B &cup; (A&oplus;B), B &sub;A&cup; (A&oplus;B) (A1.1)
</p>
<p>and therefore
</p>
<p>P&lowast;(A)&minus; P&lowast;(B)&le; P&lowast;(A&oplus;B)= ρ(A,B),
P&lowast;(B)&minus; P&lowast;(A)&le; P&lowast;(A&oplus;B)= ρ(A,B).
</p>
<p>Similarly to the terminology adopted in Sect. 3.4 we call a set A &isin; P approximable
if there exists a sequence An &isin; A for which ρ(A,An)&rarr; 0. The totality of all ap-
proximable sets we denote by A. This is clearly the closure of A with respect to ρ.
</p>
<p>Lemma A1.1 A is a σ -algebra.
</p>
<p>Proof We verify that A satisfies properties A1, A2&prime; and A3 of σ -algebras of Chap. 2.
Property A1: Ω &isin; A is obvious, for A &isin; A. Property A3 (A &isin; A if A &isin; A) follows
from the fact that, for A &isin;A, there exist An &isin;A such that, as n&rarr;&infin;,
</p>
<p>ρ(A,An)&rarr; 0, ρ(A,An)= ρ(A,An)&rarr; 0.
</p>
<p>Finally, consider property A2&prime;. We show first that if An &isin;A, then A=
⋃
</p>
<p>An &isin;A.
Indeed, we can assume without loss of generality that the An are disjoint. Then,
</p>
<p>by virtue of the properties of the measure P&lowast;, for any ε &gt; 0,
&sum;
</p>
<p>P(Ak)&le; P&lowast;(Ω)= 1,
</p>
<p>ρ
</p>
<p>(
A,
</p>
<p>n⋃
</p>
<p>k=1
Ak
</p>
<p>)
= P&lowast;
</p>
<p>( &infin;⋃
</p>
<p>k=n+1
Ak
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>k=n+1
P(Ak) &lt; ε
</p>
<p>for n large enough.</p>
<p/>
</div>
<div class="page"><p/>
<p>622 1 Extension of a Probability Measure
</p>
<p>Now let An &isin;A. We have to show that
</p>
<p>A=
&infin;⋃
</p>
<p>n=1
An &isin;A.
</p>
<p>Let {Bn} be a sequence of sets from A such that ρ(An,Bn) &lt; ε/2n. Then one has
B =
</p>
<p>⋃
Bn &isin;A and, by property (4) of the distance ρ,
</p>
<p>ρ(A,B)&le;
&infin;&sum;
</p>
<p>n=1
ρ(An,Bn) &lt; ε.
</p>
<p>The lemma is proved. �
</p>
<p>Now we can prove the main assertion.1
</p>
<p>Theorem A1.1 The probability P can be extended from the algebra A to some
probability P given on the σ -algebra A.
</p>
<p>Proof For A &isin;A, put
</p>
<p>P(A) := P&lowast;(A).
</p>
<p>It is evident that P(A) = P(A) for A &isin; A, and P(Ω) = 1. To verify that P is a
probability we just have to prove the countable additivity of P. We first prove the
</p>
<p>finite additivity. It suffices to prove it for two sets:
</p>
<p>P&lowast;(A&cup;B)= P&lowast;(A)+ P&lowast;(B), (A1.2)
</p>
<p>where A,B &isin;A and A&cap;B =&empty;. Let An &isin;A and Bn &isin;A be such that ρ(A,An)&rarr; 0
and ρ(B,Bn)&rarr; 0 as n&rarr;&infin;. Then
∣∣P&lowast;(A&cup;B)&minus; P&lowast;(An &cup;Bn)
</p>
<p>∣∣&le; ρ(A&cup;B,An &cup;Bn)&le; ρ(A,An)+ ρ(B,Bn)&rarr; 0,
</p>
<p>P&lowast;(An &cup;Bn)= P(An &cup;Bn)= P(An)+ P(Bn)&minus; P(AnBn). (A1.3)
</p>
<p>Here
</p>
<p>P(An)&rarr; P&lowast;(A), P(Bn)&rarr; P&lowast;(B),
</p>
<p>1The theorem on the extension of a measure to the minimum σ -algebra containing A was obtained
</p>
<p>by C. Carath&eacute;odory. The metrisation of normed Boolean algebras A by the distance ρ(A,B) =
P(A &oplus; B) was used by many authors (see, e.g., the talk by A.N. Kolmogorov at the 6th Polish
Mathematical Congress in 1948 and Halmos [19]).
</p>
<p>It was L.Ya. Savel&rsquo;ev who suggested the use of the continuity properties of the measure with
</p>
<p>respect to the distance ρ(A,B)= P&lowast;(A&oplus;B) in order to extend it.</p>
<p/>
</div>
<div class="page"><p/>
<p>1 Extension of a Probability Measure 623
</p>
<p>P(AnBn) &le; P&lowast;(AnB)+ P&lowast;(BnB)
&le; P&lowast;(AnA)+ P&lowast;(BnB)&le; ρ(A,An)+ ρ(B,Bn)&rarr; 0.
</p>
<p>Hence (A1.3) implies (A1.2).
</p>
<p>We now prove countable additivity. Let An &isin;A be disjoint. Then, putting
</p>
<p>A=
&infin;⋃
</p>
<p>n=1
An,
</p>
<p>we obtain from the finite additivity of P that
</p>
<p>P(A)=
n&sum;
</p>
<p>k=1
P(Ak)+ P
</p>
<p>( &infin;⋃
</p>
<p>k=n+1
Ak
</p>
<p>)
.
</p>
<p>Therefore
</p>
<p>P(A)&ge;
&infin;&sum;
</p>
<p>k=1
P(Ak).
</p>
<p>On the other hand,
</p>
<p>P(A)= P&lowast;(A)&le;
&infin;&sum;
</p>
<p>k=1
P&lowast;(Ak)=
</p>
<p>&infin;&sum;
</p>
<p>k=1
P(Ak).
</p>
<p>The theorem is proved. �
</p>
<p>Theorem A1.2 The extension of the probability P from the algebra A to the σ -
algebra A is unique.
</p>
<p>Proof Assume that there exists another probability P1 on A, which coincides with
P on A and is such that, for some A &isin;A,
</p>
<p>P1(A) 
= P(A).
</p>
<p>Suppose first that ε = P1(A)&minus; P(A) &gt; 0. Consider a sequence {Bn} &isin; γ (A) such
that
</p>
<p>&infin;&sum;
</p>
<p>n=1
P(Bn)&minus; P(A) &lt;
</p>
<p>ε
</p>
<p>2
.
</p>
<p>Then
</p>
<p>P1(A)= P(A)+ ε &ge;
&infin;&sum;
</p>
<p>n=1
P(Bn)+ ε/2</p>
<p/>
</div>
<div class="page"><p/>
<p>624 1 Extension of a Probability Measure
</p>
<p>which contradicts the assumption that A&sub;
⋃&infin;
</p>
<p>n=1 Bn. Therefore
</p>
<p>P1(A)&le; P(A), A &isin;A.
</p>
<p>Since P is ρ-continuous at the point &empty;, it follows that P1 is also ρ-continuous at the
</p>
<p>point &empty;, and hence at any &ldquo;point&rdquo; A &isin;A. Indeed, by virtue of (A1.1),
∣∣P1(A)&minus; P1(B)
</p>
<p>∣∣&le; P1(A&oplus;B)&le; P(A&oplus;B)&rarr; 0
</p>
<p>if only ρ(A,B)= P(A&oplus;B)&rarr; 0. Hence, for A &isin;A,
</p>
<p>P(A)= lim
B&rarr;A
B&isin;A
</p>
<p>P(B)= lim
B&rarr;A
B&isin;A
</p>
<p>P1(B)= P1(A).
</p>
<p>The theorem is proved. �
</p>
<p>Let A&lowast; = σ(A) be the σ -algebra generated by A. Since A&sub;A, we have A&lowast; &isin;A,
and the next statement follows in an obvious way from the above assertions.
</p>
<p>Corollary A1.1 The probability P can be uniquely extended from the algebra A to
the σ -algebra A&lowast; generated by A.
</p>
<p>Remark A1.1 The σ -algebra A defined above as the closure of the algebra A with
respect to the introduced distance ρ is in many cases wider than the σ -algebra A&lowast; =
σ(A) generated by A. This fact is closely related to the concept of the completion of
a measure. To explain the concept, we assume from the very beginning that A= F
is a σ -algebra. Then the measure P can be constructed in a rather simple way. To
</p>
<p>do this we extend the measure P from 〈Ω,F〉 to a σ -algebra which is wider than F
and is constructed as follows. We will say that a subset N of Ω belongs to the class
</p>
<p>N if there exists an A=A(N) &isin; F such that N &sub;A and P(A)= 0. It is not hard to
see that the class of all sets of the form B &cup;N , where B &isin; F and N &isin;N, also forms
a σ -algebra. Denote it by FN. Putting P(B &cup;N) := P(B) we obtain an extension of
P to 〈Ω,FN〉. Such a measure is said to be complete, and the above operation itself
is called the completion of the measure P.
</p>
<p>Now we can say that the measure P constructed in Theorem A1.1 is complete,
</p>
<p>and the σ -algebra A coincides with FN.
</p>
<p>If, for example, Ω = [0,1] and A is the algebra generated by the intervals, then
A&lowast; = σ(A) will, as we already know, be the Borel σ -algebra, and A will be the
Lebesgue extension of A&lowast; consisting of all &ldquo;Lebesgue measurable&rdquo; sets.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 2
</p>
<p>Kolmogorov&rsquo;s Theorem on Consistent
Distributions
</p>
<p>In this appendix we will prove the Kolmogorov theorem asserting that consistent
</p>
<p>distributions define a unique probability measure such that the consistent distribu-
</p>
<p>tions are its projections. We used this theorem in Sect. 5.5 and in some other places,
</p>
<p>where distributions on infinite-dimensional spaces were considered.
</p>
<p>Let T be an index set and, for each t &isin; T , Rt be the real line (&minus;&infin;,&infin;). Let
N &isin; T be a finite subset of T . Then the product space
</p>
<p>&prod;
</p>
<p>t&isin;T
Rt =RN
</p>
<p>is a Euclidean space of dimension equal to the number n of elements in N , spanned
</p>
<p>on n axes of the space
</p>
<p>R
T =
</p>
<p>&prod;
</p>
<p>t&isin;T
Rt .
</p>
<p>Assume that, for any finite subset N &sub; T , a probability measure PN is given on
〈RN ,BN 〉, where BN is the σ -algebra of Borel subsets of RN . Thereby a family of
measures is given on RT . The family is said to be consistent if, for any L&sub;N and
any Borel set B from RL,
</p>
<p>PL(B)= PN
(
B &times;RN&minus;L
</p>
<p>)
.
</p>
<p>The measure PL is said to be the projection of PN onto RL. A set from RT that
can be represented in the form B &times;RT&minus;N , where B &isin;BN and N is a finite set, is
called a cylinder set in RT . The set B is said to be the base of the cylinder.
</p>
<p>Denote by BT the σ -algebra of sets from RT generated by all cylinder sets.
</p>
<p>TheoremA2.1 (Kolmogorov) If a consistent family of probability measures is given
on RT , then there exists a unique probability measure P on 〈RT ,BT 〉 such that, for
any N , the measure PN coincides with the projection of P onto RN .
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>625</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>626 2 Kolmogorov&rsquo;s Theorem on Consistent Distributions
</p>
<p>Proof The cylinder subsets of RT form an algebra. We show that, for B &isin;BN , the
relations
</p>
<p>P
(
B &times;RT&minus;N
</p>
<p>)
= PN (B) (A2.1)
</p>
<p>define a measure on this algebra. First of all, by consistency of the measures PN ,
</p>
<p>this definition of probability on cylinder sets is consistent (we mean the cases when
</p>
<p>B = B1 &times;RN&minus;L for B1 &isin;BL; then the left-hand side of (A2.1) will also be equal
to P(B1 &times;RT&minus;L)). Further, the thus defined probability is additive. Indeed, let B1 &times;
R
T&minus;N1 and B2 &times;RT&minus;N2 be two disjoint cylinder sets. Then, putting N =N1 &cup;N2,
</p>
<p>we will have
</p>
<p>P
((
B1 &times;RT&minus;N1
</p>
<p>)
&cup;
(
B2 &times;RT&minus;N2
</p>
<p>))
</p>
<p>= P
({(
</p>
<p>B1 &times;RN&minus;N1
)
&cup;
(
B2 &times;RN&minus;N2
</p>
<p>)}
&times;RT&minus;N
</p>
<p>)
</p>
<p>= PN
({(
</p>
<p>B1 &times;RN&minus;N1
)
&cup;
(
B2 &times;RN&minus;N2
</p>
<p>)})
</p>
<p>= PN
(
B1 &times;RN&minus;N1
</p>
<p>)
+ PN
</p>
<p>(
B2 &times;RN&minus;N2
</p>
<p>)
</p>
<p>= P
(
B1 &times;RT&minus;N1
</p>
<p>)
+ P
</p>
<p>(
B2 &times;RT&minus;N2
</p>
<p>)
.
</p>
<p>To verify that P is countably additive, we make use of the equivalence of prop-
</p>
<p>erties P3 and P3&prime; (see Chap. 2). By this equivalence, it suffices to show that if Bn,
n = 1,2, . . . , is a decreasing sequence of cylinder sets and, for some ε &gt; 0, we
have P(B) &gt; ε, n = 1,2, . . . , then B =
</p>
<p>⋂&infin;
n=1 Bn is not empty. Since the Bn are
</p>
<p>enclosed in all the preceding sets, in the representation Bn = Bn &times;RT&minus;Nn one has
Nn &sub;Nn+1 and Bn+1 &cap;RNn &sub; Bn. Without loss of generality, we will assume that
the number of elements in the set Nn = {t1, . . . , tn} is equal to n, and denote by xi
(with various superscripts) the coordinates in the space Rti .
</p>
<p>Thus, let
</p>
<p>P(Bn)= PNn(Bn)&ge; ε &gt; 0.
</p>
<p>We prove that the intersection
</p>
<p>B=
&infin;⋂
</p>
<p>n=1
Bn
</p>
<p>is non-empty. For any Borel set Bn &sub;RNn , there exists a compactum Kn such that
</p>
<p>Kn &sub; Bn, PNn(Bn &minus;Kn) &lt;
ε
</p>
<p>2n+1
.
</p>
<p>Setting Kn :=Kn &times;RT&minus;Nn , we obtain
</p>
<p>P(Bn &minus;Kn)= PNn(Bn &minus;Kn) &lt;
ε
</p>
<p>2n+1
.</p>
<p/>
</div>
<div class="page"><p/>
<p>2 Kolmogorov&rsquo;s Theorem on Consistent Distributions 627
</p>
<p>Introduce the sets Dn :=
⋂n
</p>
<p>k=1 Kk . It is easy to see that Dn &sub;Bn are also cylinders.
Because
</p>
<p>Bn &minus;
n⋂
</p>
<p>k=1
Kk &sub;
</p>
<p>n⋂
</p>
<p>k=1
(Bk &minus;Kk),
</p>
<p>we have
</p>
<p>P(Bn &minus;Dn)&le; P
(
</p>
<p>n⋂
</p>
<p>k=1
(Bk &minus;Kk)
</p>
<p>)
&le;
</p>
<p>n&sum;
</p>
<p>k=1
P(Bk &minus;Kk)&le;
</p>
<p>ε
</p>
<p>2
;
</p>
<p>P(Dn)&ge; P(Bn)&minus;
ε
</p>
<p>2
&ge; ε
</p>
<p>2
.
</p>
<p>It follows that Dn is a decreasing sequence of non-empty cylinder sets. Denote by
</p>
<p>Xn = (xn1 , xn2 , . . . , xnn) an arbitrary point of the base
</p>
<p>Dn =
n⋂
</p>
<p>k=1
Kk &times;RNn&minus;Nk
</p>
<p>of the cylinder Dn. The point specifies a cylinder subset X of R
T . Since the sets Dn
</p>
<p>decrease, we have (xn+r1 , x
n+r
2 , . . . , x
</p>
<p>n+r
n ) &isin; Kn for any r &ge; 0. By compactness of
</p>
<p>Kn, we can choose a subsequence n1k such that x
n1k
1 &rarr; x1 as k &rarr;&infin;. From this
</p>
<p>subsequence, one can choose a subsequence n2k such that x
n2k
2 &rarr; x2, and so on.
</p>
<p>Now consider the diagonal sequence of the points (or, more precisely, cylinder
</p>
<p>sets) Xnkk = (xnkk1 , x
nkk
2 , . . . , x
</p>
<p>nkk
nkk ). It is clear that
</p>
<p>Xnkk &rarr;X = (x1, x2, . . .)
</p>
<p>(component-wise) as k&rarr;&infin;, and that
(
x
nkk
1 , x
</p>
<p>nkk
2 , . . . , x
</p>
<p>nkk
m
</p>
<p>)
&rarr; (x1, . . . , xm) &isin;Km
</p>
<p>for any m. This means that, for the set X corresponding to the point X, one has
</p>
<p>X :=
{
y(t) &isin;RT : y(t1)= x1, y(t2)= x2, . . .
</p>
<p>}
&sub;Km &sub;Bm
</p>
<p>for any m, and therefore
</p>
<p>X&sub;
&infin;⋂
</p>
<p>m=1
Bm.
</p>
<p>Thus B is non-empty, and the countable additivity of P on the algebra of cylinder
</p>
<p>sets is proved. Hence P is a measure, and it remains to make use of the theorem
</p>
<p>on the extension of a measure from an algebra to the σ -algebra generated by that
</p>
<p>algebra.
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 3
</p>
<p>Elements of Measure Theory and Integration
</p>
<p>In this appendix, the properties of integrals with respect to a measure are presented
</p>
<p>in more detail than in Chaps. 4 and 6. We also prove the basic theorems on decom-
</p>
<p>position of measure and on convergence of sequences of measures.
</p>
<p>3.1 Measure Spaces
</p>
<p>Let 〈Ω,F〉 be a measurable space. We will say that a measure space 〈Ω,F,μ〉 is
given if μ is a nonnegative countably additive set function on F, i.e. a function
</p>
<p>having the following properties:
</p>
<p>(1) μ(
⋃
</p>
<p>j Aj ) =
&sum;
</p>
<p>j μ(Aj ) for any countable collection of disjoint sets Aj &isin; F
(σ -additivity);
</p>
<p>(2) μ(A)&ge; 0 for any A &isin; F;
(3) μ(&empty;)= 0, where &empty; is the empty set.
</p>
<p>The value μ(A) is called the measure of the set A. We will only consider finite
and σ -finite measures. In the former case one assumes that μ(Ω) &lt;&infin;. In the latter
case there exists a partition of Ω into countably many sets Aj such that μ(Aj ) &lt;&infin;.
</p>
<p>A probability space is an example of a space with a finite (unit) measure. The
</p>
<p>space 〈R,B,μ〉, where R is the real line, B is the σ -algebra of Borel sets, and μ is
the Lebesgue measure, is an example of a space with a σ -finite measure.
</p>
<p>We can also consider such set functions μ(A) that satisfy conditions (1) and (3)
</p>
<p>only, but are not necessarily nonnegative. Such functions are called signed measures.
Any finite signed measure (i.e., such that supAμ(A) &lt; &infin; and infAμ(A) &gt; &minus;&infin;)
can be represented as a difference of two nonnegative measures (the Hahn decompo-
</p>
<p>sition theorem, see Sect. 3.5 of the present appendix). We will need signed measures
</p>
<p>in Sect. 3.5 only. Everywhere else, unless otherwise specified, by measures we will
</p>
<p>understand set functions possessing properties (1)&ndash;(3).
</p>
<p>In the same manner as when establishing the simplest properties of probability,
</p>
<p>one easily establishes the following properties of measures:
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>629</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>630 3 Elements of Measure Theory and Integration
</p>
<p>(1) μ(A)&le; μ(B) if A&sub; B ,
(2) μ(
</p>
<p>⋃
j Aj )&le;
</p>
<p>&sum;
j μ(Aj ) for any Aj ,
</p>
<p>(3) if An &sub;An+1 and
⋃
</p>
<p>nAn =A then μ(An)&rarr; μ(A), or, which is the same,
(3&prime;) if An &sup;An+1,
</p>
<p>⋂
nAn =A, and μ(A1) &lt;&infin; then μ(An)&rarr; μ(A).
</p>
<p>Consider further measurable functions on 〈Ω,F〉, i.e., functions ξ(ω) having the
property {ω : ξ(ω) &isin; B} &isin; F for any Borel subset B of the real line.
</p>
<p>The notions of convergence in measure and convergence almost everywhere are
introduced similarly to the case of probability measure.
</p>
<p>We will say that a sequence of measurable functions ξn converges to ξ almost
</p>
<p>everywhere (a.e.): ξn
a.e.&minus;&rarr; ξ as n&rarr;&infin; if ξn(ω)&rarr; ξ(ω) for all ω except from a set
</p>
<p>of measure 0.
</p>
<p>We will say that the ξn converge to ξ in measure: ξn
&micro;&minus;&rarr; ξ if, for any ε &gt; 0, as
</p>
<p>n&rarr;&infin;,
</p>
<p>μ
({
|ξn &minus; ξ |&gt; ε
</p>
<p>})
&rarr; 0.
</p>
<p>Now we turn to the construction of integrals and the study of their properties.
</p>
<p>First we consider finite measures assuming them without loss of generality to be
probability measures. In that case we will write P(A) instead of μ(A). We will turn
to integrals with respect to arbitrary measures in Sect. 3.4.
</p>
<p>3.2 The Integral with Respect to a Probability Measure
</p>
<p>3.2.1 The Integrals of a Simple Function
</p>
<p>A measurable function ξ(ω) is said to be simple if its range is finite. The indicator
of a set F &isin; F is the simple function
</p>
<p>IF (ω)=
{
</p>
<p>1, if ω &isin; F,
0, if ω /&isin; F.
</p>
<p>Clearly, any simple function ξ(ω) can be written in the form
</p>
<p>ξ(ω)=
n&sum;
</p>
<p>k=1
xkIFk (ω),
</p>
<p>where xk , k = 1,2, . . . , n, are values assumed by ξ , and Fk = {ω : ξ(ω)= xk}. The
sets Fk &isin; F are disjoint, and
</p>
<p>⋃n
k=1 Fk =Ω . The integral of the simple function ξ(ω)
</p>
<p>with respect to a measure P is defined as the quantity
</p>
<p>&int;
ξ dP=
</p>
<p>&int;
ξ(ω)dP(ω)=
</p>
<p>n&sum;
</p>
<p>k=1
xkP(Fk)= Eξ.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 The Integral with Respect to a Probability Measure 631
</p>
<p>The integral of the simple function ξ(ω) over a set A &isin; F is defined as
&int;
</p>
<p>A
</p>
<p>ξ dP=
&int;
</p>
<p>ξ(ω)IA(ω)dP(ω).
</p>
<p>That these definitions are consistent (the partitions into sets Fk may be different)
</p>
<p>can be verified in an obvious way.
</p>
<p>3.2.2 The Integrals of an Arbitrary Function
</p>
<p>Lemma A3.2.1 Let ξ(ω) &gt; 0. There exists a sequence ξn(ω) of simple functions
such that ξn(ω) &uarr; ξ(ω) as n&rarr;&infin; for all ω &isin;Ω .
</p>
<p>Proof Partition the segment [0, n] into n2n equal intervals. Let
</p>
<p>x0 = 0, x1 = 2&minus;n, . . . , xn2n = n,
</p>
<p>denote the partition points, so that xi+1 &minus; xi = 2&minus;n. Put
</p>
<p>Fi :=
{
ω : xi &le; ξ(ω) &lt; xi+1
</p>
<p>}
, i = 1,2, . . . , n2n &minus; 1;
</p>
<p>F0 :=
{
0 &le; ξ(ω) &lt; x1
</p>
<p>}
&cup;
{
ξ(ω)&ge; n
</p>
<p>}
, ξn(ω) :=
</p>
<p>n2n&minus;1&sum;
</p>
<p>i=0
xiIFi (ω)&le; ξ(ω).
</p>
<p>The function ξn(ω) is clearly simple, ξn(ω)&le; ξn+1(ω)&le; ξ(ω) for all ω, and has the
property that if n &gt; ξ(ω) at a point ω &isin;Ω then
</p>
<p>0 &le; ξ(ω)&minus; ξn(ω)&le;
1
</p>
<p>2n
.
</p>
<p>The lemma is proved. �
</p>
<p>Lemma A3.2.2 Let ξn &uarr; ξ &ge; 0 and ηn &uarr; ξ &ge; 0 be sequences of simple functions.
Then
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&int;
ξn dP= lim
</p>
<p>n&rarr;&infin;
</p>
<p>&int;
ηn dP.
</p>
<p>Proof We verify that, for any m,
&int;
</p>
<p>ξm dP&le; lim
n&rarr;&infin;
</p>
<p>&int;
ηn dP.
</p>
<p>The function ξn is simple. Therefore it is bounded by some constant: ξm &le; cm.
Hence, for any integer n and ε &gt; 0,
</p>
<p>ξm &minus; ηn &le; cm &middot; I{ξm&ge;ηn+ε} + ε.</p>
<p/>
</div>
<div class="page"><p/>
<p>632 3 Elements of Measure Theory and Integration
</p>
<p>This implies that
</p>
<p>Eξm &le; cmP{ξm &ge; ηn + ε} + ε+Eηn.
The probability on the right-hand side vanishes as n&rarr;&infin;:
</p>
<p>P{ξm &ge; ηn + ε} &le; P{ξ &ge; ηn + ε}&rarr; 0,
</p>
<p>because ηn converges almost surely (and hence in probability) to ξ . Therefore
</p>
<p>Eξm &le; ε+ limn&rarr;&infin;Eηn. Since ε is arbitrary,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>Eξn &le; lim
n&rarr;&infin;
</p>
<p>Eηn.
</p>
<p>Swapping {ξn} and {ηn}, we obtain the converse inequality.
The lemma is proved. �
</p>
<p>The assertions of Lemmas A3.2.1 and A3.2.2 make the following definitions
</p>
<p>consistent.
</p>
<p>The integral of a nonnegative measurable function ξ(ω) (with respect to measure
P) is the quantity
</p>
<p>&int;
ξ dP= lim
</p>
<p>n&rarr;&infin;
</p>
<p>&int;
ξn dP, (A3.2.1)
</p>
<p>where ξn is a sequence of simple functions such that ξn &uarr; ξ as n&rarr;&infin;.
The integral
</p>
<p>&int;
ξ dP will also be denoted by Eξ . We will say that the integral&int;
</p>
<p>ξ dP exists and ξ is integrable if Eξ &lt;&infin;.
The integral of an arbitrary function (assuming values of both signs) ξ(ω) (with
</p>
<p>respect to measure P) is the quantity
</p>
<p>Eξ = Eξ+ &minus;Eξ&minus;, ξ&plusmn; := max(0,&plusmn;ξ),
</p>
<p>which is defined when at least one of the values Eξ&plusmn; is finite. Otherwise Eξ is
undefined. The integral Eξ exists if and only if E|ξ |&lt;&infin; exists (for |ξ | = ξ++ ξ&minus;).
If Eξ exists then
</p>
<p>E(ξ ;A) :=
&int;
</p>
<p>A
</p>
<p>ξ dP= Eξ IA
</p>
<p>exists for any A &isin; F as well.
</p>
<p>Lemma A3.2.3 If Eξ exists and Bn &isin; F is a sequence of sets such that P(Bn)&rarr; 0
as n&rarr;&infin;, then
</p>
<p>E(ξ ;Bn)&rarr; 0.
</p>
<p>Proof For any sequence |ξm| &uarr; |ξ | of simple functions and Am := {|ξ | &le;m} one has
</p>
<p>E|ξ | &ge; lim
m&rarr;&infin;
</p>
<p>E|ξ |IAm &ge; limm&rarr;&infin;E|ξm|IAm = E|ξ |,</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 The Integral with Respect to a Probability Measure 633
</p>
<p>since |ξm|IAm &uarr; |ξ |. This implies that
</p>
<p>E|ξ | = lim
m&rarr;&infin;
</p>
<p>E|ξ |IAm = limm&rarr;&infin;E
(
|ξ |; |ξ | &le;m
</p>
<p>)
,
</p>
<p>and hence, for any ε &gt; 0, there exists an m(ε) such that
</p>
<p>E|ξ | &minus;E
(
|ξ |; |ξ | &le;m
</p>
<p>)
&lt; ε
</p>
<p>for m&gt;m(ε). Consequently, for such m, one has
</p>
<p>E
(
|ξ |;Bn
</p>
<p>)
= E
</p>
<p>(
|ξ |;
</p>
<p>{
|ξ | &le;m
</p>
<p>}
Bn
</p>
<p>)
+E
</p>
<p>(
|ξ |;
</p>
<p>{
|ξ |&gt;m
</p>
<p>}
Bn
</p>
<p>)
&le;mP(Bn)+ ε,
</p>
<p>and hence
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>E
(
|ξ |;Bn
</p>
<p>)
&le; ε.
</p>
<p>The lemma is proved. �
</p>
<p>Note that Lemma 6.1.2 somewhat extends Lemma A3.2.3.
</p>
<p>Corollary A3.2.1 If Eξ is well-defined (the values &plusmn;&infin; not being excluded) and
Bn &isin; F is a sequence of sets such that P(Bn)&rarr; 1 as n&rarr;&infin;, then
</p>
<p>E(ξ ;Bn)&rarr; Eξ.
</p>
<p>Proof If Eξ exists then the required assertion follows from Lemma A3.2.3.
Now let Eξ =&infin;. Then Eξ&minus; &lt;&infin; and Eξ+ =&infin;, where ξ&plusmn; = max(0,&plusmn;ξ). It
</p>
<p>follows that E(ξ&minus;;Bn)&rarr; Eξ&minus; as n&rarr;&infin;. We show that
</p>
<p>E
(
ξ+;Bn
</p>
<p>)
&rarr;&infin;. (A3.2.2)
</p>
<p>Let Ak := {ξ &isin; [2k&minus;1,2k)}, k = 1,2, . . . ; pk := P(Ak). We can assume with-
out loss of generality that all pk &gt; 0 (if this is not the case we can consider a
</p>
<p>subsequence kj such that all pkj &gt; 0). Since Eξ
+ &le; 1 +
</p>
<p>&sum;&infin;
k=1 2
</p>
<p>kpk , we have&sum;&infin;
k=1 2
</p>
<p>kpk =&infin;. For a given N &gt; 1, choose n large enough such that P(BnAk) &gt;
pk/2 for all k &le;N . Then
</p>
<p>E
(
ξ+;Bn
</p>
<p>)
&ge;
</p>
<p>N&sum;
</p>
<p>k=1
2k&minus;2pk,
</p>
<p>where the right-hand side can be made arbitrarily large by an appropriate choice
</p>
<p>of N . This proves (A3.2.2). Since ξ = ξ+&minus; ξ&minus;, the required convergence is proved.
The case Eξ =&minus;&infin; can be dealt with in the same way. The corollary is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>634 3 Elements of Measure Theory and Integration
</p>
<p>3.2.3 Properties of Integrals
</p>
<p>I1. If sets Aj &isin; F are disjoint and
⋃
</p>
<p>j Aj =Ω then
&int;
</p>
<p>ξ dP=
&sum;
</p>
<p>j
</p>
<p>&int;
</p>
<p>Aj
</p>
<p>ξ dP. (A3.2.3)
</p>
<p>Proof It suffices to prove this relation for ξ(ω) &ge; 0. For simple functions equal-
ity (A3.2.3) is obvious, because
</p>
<p>&int;
ξ dP=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>xkP(ξ = xk)=
&sum;
</p>
<p>j
</p>
<p>&sum;
</p>
<p>k
</p>
<p>xkP(ξ = xk;Aj ).
</p>
<p>In the general case, using definition (A3.2.1) one gets
</p>
<p>&int;
ξ dP= lim
</p>
<p>n&rarr;&infin;
</p>
<p>&int;
ξn dP = lim
</p>
<p>n&rarr;&infin;
</p>
<p>&sum;
</p>
<p>j
</p>
<p>&int;
</p>
<p>Aj
</p>
<p>ξn dP
</p>
<p>=
&sum;
</p>
<p>j
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&int;
</p>
<p>Aj
</p>
<p>ξn dP=
&sum;
</p>
<p>j
</p>
<p>&int;
</p>
<p>Aj
</p>
<p>ξ dP. (A3.2.4)
</p>
<p>Swapping summation and passage to the limit is justified here, for by Lemma A3.2.3
</p>
<p>&infin;&sum;
</p>
<p>j=N
</p>
<p>&int;
</p>
<p>Aj
</p>
<p>ξn dP= E
(
ξn;
</p>
<p>&infin;⋃
</p>
<p>j=N
Aj
</p>
<p>)
&le; E
</p>
<p>(
ξ ;
</p>
<p>&infin;⋃
</p>
<p>j=N
Aj
</p>
<p>)
&rarr; 0
</p>
<p>as N &rarr;&infin; uniformly in n. �
</p>
<p>I2. &int;
(ξ + η)dP=
</p>
<p>&int;
ξ dP+
</p>
<p>&int;
η dP.
</p>
<p>Proof For simple functions this property is obvious. Hence, for ξ &ge; 0 and η &ge; 0,
this property follows from the additivity of the limit.
</p>
<p>In the general case we have (ξ&plusmn; and η&plusmn; are defined here as before)
&int;
(ξ + η)dP=
</p>
<p>&int; (
ξ+ + η+
</p>
<p>)
dP&minus;
</p>
<p>&int; (
ξ&minus; + η&minus;
</p>
<p>)
dP
</p>
<p>=
&int;
</p>
<p>ξ+ dP&minus;
&int;
</p>
<p>ξ&minus; dP+
&int;
</p>
<p>η+ dP&minus;
&int;
</p>
<p>η&minus; dP=
&int;
</p>
<p>ξ dP+
&int;
</p>
<p>η dP. �
</p>
<p>I3. If c is an arbitrary constant, then
&int;
</p>
<p>cξ dP= c
&int;
</p>
<p>ξ dP.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Further Properties of Integrals 635
</p>
<p>I4. If ξ &le; η, then
&int;
ξ dP&le;
</p>
<p>&int;
η dP.
</p>
<p>The proof of properties I3 and I4 is obvious. Since
</p>
<p>&int;
ξ dP= Eξ,
</p>
<p>we can write down properties I1&ndash;I4 in terms of expectations as follows:
</p>
<p>I1. Eξ =
&sum;
</p>
<p>j E(ξ ;Aj ) if Aj are disjoint and
⋃
</p>
<p>j Aj =Ω .
I2. E(ξ + η)= Eξ +Eη.
I3. Eaξ = aEξ .
I4. Eξ &le; Eη, if ξ &le; η.
</p>
<p>Note also the following properties of integrals which easily follow from I1&ndash;I4.
</p>
<p>I5. |Eξ | &le; E|ξ |.
I6. If c1 &le; ξ &le; c2, then c1 &le; Eξ &le; c2.
I7. If ξ &ge; 0 and Eξ = 0, then P(ξ = 0)= 1.
</p>
<p>This property follows from the Chebyshev inequality: P(ξ &ge; ε)&le; Eξ/ε = 0 for
any ε &gt; 0.
</p>
<p>I8. If P(ξ = η)= 1 and Eξ exists then Eξ = Eη.
Indeed,
</p>
<p>Eη= lim
n&rarr;&infin;
</p>
<p>E
(
η; |η| &le; n
</p>
<p>)
= lim
</p>
<p>n&rarr;&infin;
E
(
ξ ; |ξ | &le; n
</p>
<p>)
= Eξ.
</p>
<p>3.3 Further Properties of Integrals
</p>
<p>3.3.1 Convergence Theorems
</p>
<p>A number of convergence theorems were proved in Sect. 6.1. One of them was the
</p>
<p>dominated convergence theorem (Corollary 6.1.3):
</p>
<p>If ξn
p&minus;&rarr; ξ as n&rarr;&infin; and |ξn| &le; η, Eη &lt;&infin;, then the expectation Eξ exists and
</p>
<p>Eξn &rarr; Eξ .
Now we will present some further useful assertions concerning convergence of
</p>
<p>integrals.
</p>
<p>Theorem A3.3.1 (Monotone convergence) If 0 &le; ξn &uarr; ξ , then Eξ = limn&rarr;&infin;Eξn.
</p>
<p>Proof In addition to Corollary 6.1.3, here we only need to prove that Eξn &rarr; &infin;
if Eξ =&infin;. Put ξNn := min(ξn,N) and ξN := min(ξ,N). Then clearly ξNn &uarr; ξN as
n&rarr;&infin;, and EξNn &uarr; EξN . Therefore the value EξNn &le; Eξn can be made arbitrarily
large by choosing appropriate n and N . The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>636 3 Elements of Measure Theory and Integration
</p>
<p>These theorems can be generalised in the following way. To make the extension
</p>
<p>of the convergence theorems to the case of integrals with respect to signed measures
</p>
<p>in Sect. 3.4 more convenient, we will now write Eξ in the form of the integral&int;
ξ dP.
</p>
<p>Theorem A3.3.2 (Fatou&ndash;Lebesgue) Let η and ζ be integrable. If ξn &le; η then
</p>
<p>lim sup
n&rarr;&infin;
</p>
<p>&int;
ξn dP&le;
</p>
<p>&int;
lim sup
n&rarr;&infin;
</p>
<p>ξn dP. (A3.3.1)
</p>
<p>If ξn &ge; ζ then
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>&int;
ξn dP&ge;
</p>
<p>&int;
lim inf
n&rarr;&infin;
</p>
<p>ξn dP. (A3.3.2)
</p>
<p>If ξn &uarr; ξ and ξn &ge; ζ , or ξn
a.e.&minus;&rarr; ξ and ζ &le; ξn &le; η, then
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&int;
ξn dP=
</p>
<p>&int;
ξ dP. (A3.3.3)
</p>
<p>Proof We prove for instance (A3.3.2). Assume without loss of generality that ζ &equiv; 0.
In this case, as n&rarr;&infin;,
</p>
<p>ξ &ge; ηn := inf
k&ge;n
</p>
<p>ξk &uarr; lim inf
k&rarr;&infin;
</p>
<p>ξk, ηn &ge; 0,
</p>
<p>and by the monotone convergence theorem
</p>
<p>lim inf
n&rarr;&infin;
</p>
<p>&int;
ξn dP&ge; lim
</p>
<p>n&rarr;&infin;
</p>
<p>&int;
ηn dP=
</p>
<p>&int;
lim inf
n&rarr;&infin;
</p>
<p>ξn dP.
</p>
<p>Applying (A3.3.2) to the sequence η&minus;ξn we obtain (A3.3.1); (A3.3.3) follows from
the previous theorems. The theorem is proved. �
</p>
<p>3.3.2 Connection to Integration with Respect to a Measure on the
</p>
<p>Real Line
</p>
<p>Let g(x) be a Borel function given on the real line R (if B is the σ -algebra of Borel
</p>
<p>sets on the line and B &isin; B, then {x : g(x) &isin; B} &isin; B). If ξ is a random variable
then η := g(ξ(ω)) will clearly also be a random variable. As we saw in Sect. 3.2,
a random variable ξ induces the probability space 〈R,B,Fξ 〉 with measure Fξ on
the line such that Fξ (B)= P(ξ &isin; B). Therefore one can speak about integrals with
respect to that measure.
</p>
<p>Theorem A3.3.3 If η= g(ξ(ω)) and Eη exists, then
</p>
<p>Eη=
&int;
</p>
<p>Ω
</p>
<p>η dP=
&int;
</p>
<p>R
</p>
<p>g(x)Fξ (dx)
</p>
<p>(on the right-hand side we used a somewhat different notation for
&int;
g dFξ ).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Further Properties of Integrals 637
</p>
<p>Proof Let first g(x)= IB(x) be the indicator of a set B &isin;B. Then η = g(ξ(ω))=
I{ξ&isin;B}(ω) and Eη= P(ξ &isin; B). Therefore
</p>
<p>&int;
g(x)Fξ (dx)=
</p>
<p>&int;
IB(x)Fξ (dx)= Fξ (B)= P(ξ &isin; B)= Eη.
</p>
<p>Using the properties of the integral it is easy to establish that the assertion of the
</p>
<p>theorem holds for simple functions g. Passing to the limit extends that assertion to
</p>
<p>bounded functions. Now let g &ge; 0. If the function g(ξ)IB(ξ) = η(ω)I{ξ&isin;B}(ω) is
bounded, then
</p>
<p>&int;
</p>
<p>B
</p>
<p>g(x)Fξ (dx)= E(η; ξ &isin; B).
</p>
<p>Therefore
&int;
</p>
<p>{g&le;n}
g dFξ = E(η;η &le; n).
</p>
<p>Passing to the limit as n&rarr;&infin; we get the assertion of the theorem. Considering the
case when g takes values of both signs does not create any difficulties. The theorem
</p>
<p>is proved. �
</p>
<p>Introducing the notation
</p>
<p>Fξ (x)= P(ξ &lt; x),
</p>
<p>we can also consider, along with the integral just discussed,
</p>
<p>&int;
</p>
<p>R
</p>
<p>g(x)Fξ (dx), (A3.3.4)
</p>
<p>the Riemann&ndash;Stieltjes integral
</p>
<p>&int;
g(x)dFξ (x), (A3.3.5)
</p>
<p>the definition of which was given in Sect. 3.6. It was also shown there that, for con-
tinuous functions g(x), these integrals coincide. Moreover, we discussed in Sect. 3.6
some other conditions for these integrals to coincide.
</p>
<p>Also recall that if
</p>
<p>Fξ (x)=
&int; x
</p>
<p>&minus;&infin;
fξ (t) dt
</p>
<p>and the functions g(x) and fξ (x) are Riemann integrable, then integrals (A3.3.4)
</p>
<p>and (A3.3.5) coincide with the Riemann integral
</p>
<p>&int;
g(x)fξ (x) dx.</p>
<p/>
</div>
<div class="page"><p/>
<p>638 3 Elements of Measure Theory and Integration
</p>
<p>3.3.3 Product Measures and Iterated Integrals
</p>
<p>Consider a two-dimensional random variable ζ = (ξ, η) given on 〈Ω,F,P〉. The
random variables ξ and η induce a sample probability space 〈R2,B2,Fξ,η〉 with the
measure Fξ,η given on elements of the σ -algebra B
</p>
<p>2 of Borel sets on the plane (the
</p>
<p>σ -algebra generated by rectangles) and such that
</p>
<p>Fξ,η(A&times;B)= P(ξ &isin;A,η &isin; B).
</p>
<p>Here A&times; B is the set of points (x, y) for which x &isin; A and y &isin; B . If g(x, y) is a
Borel function ({(x, y) : g(x, y) &isin; B} &isin;B2 for each B &isin;B), then it easily follows
from the above that
</p>
<p>Eg(ξ, η)=
&int;
</p>
<p>R2
g(x, y)Fξ,η(dx dy), (A3.3.6)
</p>
<p>since both integrals are equal to
&int;
R
xFθ (dx) for θ = g(ξ, η).
</p>
<p>Now let ξ and η be independent random variables, i.e.
</p>
<p>P(ξ &isin;A, η &isin; B)= P(ξ &isin;A)P(η &isin; B)
</p>
<p>for any A,B &isin;B.
</p>
<p>Theorem A3.3.4 (Fubini&rsquo;s theorem on iterated integrals) If g(x, y) &ge; 0 is a Borel
function and ξ and η are independent, then
</p>
<p>Eg(ξ, η)= E
[
Eg(x, η)|x=ξ
</p>
<p>]
.
</p>
<p>For arbitrary Borel functions g(x, y) the above equality holds if Eg(ξ, η) exists.
</p>
<p>This very assertion we stated in Chap. 3 in the form
</p>
<p>&int;
g(x, y)Fξ,η(dx dy)=
</p>
<p>&int; [&int;
g(x, y)Fη(dy)
</p>
<p>]
Fξ (dx). (A3.3.7)
</p>
<p>We will need the following.
</p>
<p>Lemma A3.3.1 1. The section
</p>
<p>Bx :=
{
y : (x, y) &isin; B
</p>
<p>}
</p>
<p>of any set B &isin;B2 is measurable: Bx &isin;B.
2. The section gx(y) = g(x, y) of any Borel function g (B2-measurable) is a
</p>
<p>Borel function.
3. The integral
</p>
<p>&int;
g(x, y)Fη(dy) (A3.3.8)
</p>
<p>of a Borel function g is a Borel function of x.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Further Properties of Integrals 639
</p>
<p>Proof 1. Let K1 be the class of all sets from B2 of which all x-sections are measur-
able. It is evident that K1 contains all rectangles B = B(1) &times; B(2), where B(1) &isin;B
and B(2) &isin; B. Moreover, K1 is a σ -algebra. Indeed, consider for example the set
B =
</p>
<p>⋃
k B
</p>
<p>(k) where B(k) &isin;K1. The operation
⋃
</p>
<p>on the sets B(k) leads to the same
</p>
<p>operation on their sections, so that Bx =
⋃
</p>
<p>k B
(k)
x &isin; B. For the other operations
</p>
<p>(&cap; and taking complements) the situation is similar. Thus, K1 is a σ -algebra con-
taining all rectangles. This means that B2 &sub;K1.
</p>
<p>2. For B &isin;B, one has
</p>
<p>g&minus;1x (B) =
{
y : gx(y) &isin; B
</p>
<p>}
=
{
y : g(x, y) &isin; B
</p>
<p>}
</p>
<p>=
{
y : (x, y) &isin; g&minus;1(B)
</p>
<p>}
=
[
g&minus;1(B)
</p>
<p>]
x
&isin;B.
</p>
<p>3. Integral (A3.3.8) is, as a function of x, the result of passing to the limit in
</p>
<p>a sequence of measurable functions, and hence is measurable itself. The lemma is
</p>
<p>proved. �
</p>
<p>Proof of Theorem A3.3.4 First we prove (A3.3.7) in the case where g(x, y) =
IB(x, y), so that the theorem turns into the formula for consecutive computation
</p>
<p>of the measure of the set B &isin;B2:
</p>
<p>P
(
(ξ, η) &isin; B
</p>
<p>)
=
&int;
</p>
<p>Fη
(
(x, y) &isin; B
</p>
<p>)
Fξ (dx)=
</p>
<p>&int;
Fη(Bx)Fξ (dx). (A3.3.9)
</p>
<p>We introduce the set function
</p>
<p>Q(B) :=
&int;
</p>
<p>Fη(Bx)Fξ (dx).
</p>
<p>Clearly, Q(B) &ge; 0 and Q(&empty;) = 0. Further, if B =
⋃
</p>
<p>k B
(k) and B(k) are disjoint,
</p>
<p>then Bx =
⋃
</p>
<p>k B
(k)
x and B
</p>
<p>(k)
x are also disjoint, and
</p>
<p>Q(B)=
&int;
</p>
<p>Fη
</p>
<p>(⋃
</p>
<p>k
</p>
<p>B(k)x
</p>
<p>)
Fξ (dx)=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>&int;
Fη
</p>
<p>(
B(k)x
</p>
<p>)
Fξ (dx)=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>Q
(
B(k)
</p>
<p>)
.
</p>
<p>This means that Q(B) is a measure.
</p>
<p>The measure Q(B) coincides with Fξ,η(B) = P((ξ, η) &isin; B) on rectangles B =
B(1) &times;B(2). Indeed, for rectangles,
</p>
<p>Bx =
{
B(2) for x &isin; B(1),
&empty; for x /&isin; B(1),
</p>
<p>and
</p>
<p>P
(
(ξ, η) &isin; B
</p>
<p>)
= Fξ (B(1))Fη(B(2))
</p>
<p>=
&int;
</p>
<p>B(1)
</p>
<p>Fη(B(2))Fξ (dx)=
&int;
</p>
<p>Fη(Bx)Fξ (dx)=Q(B).</p>
<p/>
</div>
<div class="page"><p/>
<p>640 3 Elements of Measure Theory and Integration
</p>
<p>This means that the measures Q and Fξ,η coincide on the algebra generated by
</p>
<p>rectangles. By the measure extension theorem we obtain that Q= Fξ,η .
We have proved (A3.3.9). This implies that Fubini&rsquo;s theorem holds for simple
</p>
<p>functions gN =
&sum;N
</p>
<p>j=1 cj IAj , because
</p>
<p>EgN (ξ, η) =
N&sum;
</p>
<p>j=1
cjEIAj (ξ, η)
</p>
<p>=
N&sum;
</p>
<p>j=1
cj
</p>
<p>&int;
EIAj (x, η)Fξ (dx)=
</p>
<p>&int;
EgN (x, η)Fξ (dx).(A3.3.10)
</p>
<p>Now if g &ge; 0 is an arbitrary Borel function then there exists a sequence of simple
functions gN &uarr; g and, as in (A3.2.1), it remains to pass to the limit:
</p>
<p>Eg(ξ, η) = lim
N&rarr;&infin;
</p>
<p>EgN (ξ, η)
</p>
<p>= lim
N&rarr;&infin;
</p>
<p>&int;
EgN (ξ, η)Fξ (dx)=
</p>
<p>&int;
EgN (ξ, η)Fξ (dx).
</p>
<p>For an arbitrary function g one has to use the representation g = g+ &minus; g&minus;, g+ &ge; 0,
g&minus; &ge; 0. The theorem is proved. �
</p>
<p>Remark A3.1 We see from the proof of the theorem that the random variables ξ and
η do not need to be scalar. The assertion remains true in a more general form (see
</p>
<p>property 5A in Sect. 4.8) and, in particular, for vector-valued ξ and η.
</p>
<p>3.4 The Integral with Respect to an Arbitrary Measure
</p>
<p>If &micro; is a finite measure on 〈Ω,F〉, μ(Ω) &lt; &infin;, then the definition of the integral&int;
ξ dμ with respect to the measure μ does not differ from that of the integral with
</p>
<p>respect to a probability measure (one could just put
&int;
A
ξ dμ= μ(Ω)
</p>
<p>&int;
A
ξ dP, where
</p>
<p>P(B)= μ(B)/μ(Ω) is a probability distribution). If μ is σ -finite and μ(Ω)=&infin;,
then the situation is somewhat more complicated, although it can still be reduced to
</p>
<p>the already used constructions. First we will make several preliminary remarks.
</p>
<p>Let 〈Ω,F,P〉 be a probability space and f = f (ω)&ge; 0 an a.e. finite nonnegative
measurable function (i.e., a random variable). Consider the set function
</p>
<p>μ(A) :=
&int;
</p>
<p>A
</p>
<p>f dP. (A3.4.1)
</p>
<p>If f is integrable (μ(Ω) &lt;&infin;) then μ(A) is a finite σ -additive measure (see prop-
erty I1) satisfying conditions (1)&ndash;(3) of Sect. 3.1 of the present appendix. In other</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 The Integral with Respect to an Arbitrary Measure 641
</p>
<p>words, μ is a finite measure on 〈Ω,F〉. But if f is not integrable, then μ is a σ -finite
measure, which immediately follows from the representation
</p>
<p>μ(A)=
&infin;&sum;
</p>
<p>k=1
</p>
<p>&int;
</p>
<p>A&cap;{k&minus;1&le;f&lt;k}
f dP
</p>
<p>(the integrals in the sum that are equal to
&int;
A
f I(k&minus;1&le;f&lt;k) dP are clearly finite mea-
</p>
<p>sures).
</p>
<p>Thus, the integral of the form (A3.4.1) is a measure for any distribution P and
</p>
<p>function f &ge; 0. It turns out that the following assertion, converse in a certain sense
to the above, also holds.
</p>
<p>Lemma A3.4.1 For any measure μ on 〈Ω,F〉, there exists a distribution P on that
space and a measurable function f &gt; 0 such that representation (A3.4.1) holds.
</p>
<p>Thus, any measure can be represented as an integral with respect to a probability
</p>
<p>measure (i.e., in the form E(f ;A) for the respective function f and distribution P).
</p>
<p>Proof Let μ be a σ -finite measure on 〈Ω,F〉, and sets Bj &isin; F, j = 1,2, . . . , possess
the properties
</p>
<p>⋃&infin;
j=1 Bj =Ω , BiBj =&empty; for i 
= j , and μ(Bj ) &lt;&infin;. Put
</p>
<p>P(A) :=
&infin;&sum;
</p>
<p>k=1
</p>
<p>μ(ABk)
</p>
<p>2kμ(Bk)
. (A3.4.2)
</p>
<p>Obviously, P(Ω)= 1 and P is a measure. Further, if A&sub; Bk then
</p>
<p>μ(A)= 2kμ(Bk)P(A).
</p>
<p>This means that we should put f (ω) := 2kμ(Bk) for ω &isin; Bk . Then the set function
</p>
<p>λ(A) :=
&int;
</p>
<p>A
</p>
<p>f dP=
&int;
</p>
<p>Ω
</p>
<p>f IA dP
</p>
<p>will coincide with μ(A):
</p>
<p>λ(A)=
&infin;&sum;
</p>
<p>k=1
2kμ(Bk)P(ABk)
</p>
<p>=
&infin;&sum;
</p>
<p>k=1
2kμ(Bk)
</p>
<p>&infin;&sum;
</p>
<p>j=1
</p>
<p>μ(ABkBj )
</p>
<p>2jμ(Bj )
=
</p>
<p>&infin;&sum;
</p>
<p>k=1
μ(ABk)= μ(A).
</p>
<p>The lemma is proved. �
</p>
<p>Besides the required assertion, we also obtain that in representation (A3.4.1) the
</p>
<p>range of values of the function f can be assumed without loss of generality to be
</p>
<p>countable.</p>
<p/>
</div>
<div class="page"><p/>
<p>642 3 Elements of Measure Theory and Integration
</p>
<p>The function f for which equality (A3.4.1) holds is called the density of the
measure μ with respect to P (or Radon&ndash;Nikodym derivative of the measure μ with
respect to P) and is denoted by dμ/dP. It is evident that alteration of the function
</p>
<p>f = dμ/dP on a set of zero P-measure leaves the equality (A3.4.1) unchanged.
Now let μ and P be two given arbitrary measures. The question of under what
</p>
<p>conditions these two measures μ and P could be related by (A3.4.1) and whether
</p>
<p>the function f is determined uniquely thereby (up to values on a set of zero P-
</p>
<p>measure) is rather important for probability theory. (We stress that, in the preceding
</p>
<p>considerations, the measure P was constructed in a special way from the measure
</p>
<p>μ, or vice versa.) Answers to these questions are given by the Radon&ndash;Nikodym
</p>
<p>theorem to be discussed in the next section.
</p>
<p>Now, using the simple assertion of Lemma A3.4.1 we have just proved, we will
</p>
<p>give the definition of the integral with respect to an arbitrary measure μ.
</p>
<p>Let μ be an arbitrary σ -finite measure on 〈Ω,F〉 and ξ &ge; 0 a F-measurable
function.
</p>
<p>The integral
&int;
A
ξ dμ over a set A &isin; F of the function ξ &ge; 0 with respect to the
</p>
<p>measure μ is the integral
</p>
<p>&int;
</p>
<p>A
</p>
<p>ξ dμ=
&int;
</p>
<p>A
</p>
<p>(
ξ
dμ
</p>
<p>dP
</p>
<p>)
dP (A3.4.3)
</p>
<p>with respect to any distribution P satisfying equality (A3.4.1) (for example, with
</p>
<p>respect to measure (A3.4.2)).
</p>
<p>This definition is consistent because it does not depend on the choice of P. In-
deed, for simple functions ξ (ξ(ω)= xk for ω &isin; Fk),
</p>
<p>&int;
</p>
<p>A
</p>
<p>ξ dμ=
&sum;
</p>
<p>k
</p>
<p>xk
</p>
<p>&int;
</p>
<p>A
</p>
<p>dμ
</p>
<p>d P
IBk dP=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>xk
</p>
<p>&int;
</p>
<p>ABk
</p>
<p>dμ
</p>
<p>d P
d P=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>xkμ(ABk).
</p>
<p>If now ξ &ge; 0 is an arbitrary function, then by the monotone convergence theorem
the integral
</p>
<p>&int;
A
ξ dμ is equal to
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&int;
</p>
<p>A
</p>
<p>ξ (n)
dμ
</p>
<p>dP
dP= lim
</p>
<p>n&rarr;&infin;
</p>
<p>&int;
</p>
<p>A
</p>
<p>ξ (n)dμ,
</p>
<p>where ξ (n) &uarr; ξ is a sequence of simple functions which converge monotonically to
ξ (see Lemma A3.2.1). In both cases, the result does not depend on the choice of P.
</p>
<p>The integral
&int;
</p>
<p>A
</p>
<p>ξ dμ
</p>
<p>of an arbitrary measurable function ξ is defined by
</p>
<p>&int;
</p>
<p>A
</p>
<p>ξ dμ=
&int;
</p>
<p>A
</p>
<p>ξ+dμ&minus;
&int;
</p>
<p>A
</p>
<p>ξ&minus;dμ,</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 The Lebesgue Decomposition Theorem 643
</p>
<p>when both expressions on the right-hand side are finite. (In that case one says
</p>
<p>that the integral
&int;
A
ξ dμ exists.) Here, as before, ξ+ = max(0, ξ) &ge; 0 and ξ&minus; =
</p>
<p>max(0,&minus;ξ)&ge; 0, so that ξ = ξ+ &minus; ξ&minus;.
Thus we see that the above definition of the integral with respect to an arbitrary
</p>
<p>measure is essentially equivalent to the construction used in Sect. 3.2 of the present
</p>
<p>appendix. However, the definition in the form (A3.4.3) saves us from the necessity
</p>
<p>of repeating what we have already done (and now in a more complex setting) and
</p>
<p>enables one to transfer all the properties of the integrals
&int;
ξ dP to the general case.
</p>
<p>We will list the basic properties preserving the existing numeration.
</p>
<p>I1.
&int;
ξ dμ=
</p>
<p>&sum;
j
</p>
<p>&int;
Aj
</p>
<p>ξ dμ if Aj are disjoint and
⋃
</p>
<p>j Aj =Ω .
I2.
</p>
<p>&int;
(ξ + η)dμ=
</p>
<p>&int;
ξ dμ+
</p>
<p>&int;
η dμ.
</p>
<p>I3.
&int;
aξ dμ= a
</p>
<p>&int;
ξ dμ.
</p>
<p>I4. ξ dμ&le;
&int;
η dμ if ξ &le; η.
</p>
<p>I5. |
&int;
ξ dμ| &le;
</p>
<p>&int;
|ξ |dμ.
</p>
<p>I6. If c1 &le; ξ(ω)&le; c2 for ω &isin;A, then c1μ(A)&le;
&int;
A
ξ dμ&le; c2μ(A).
</p>
<p>I7. If ξ &ge; 0 and
&int;
ξ dμ= 0, then μ(ξ &gt; 0)= 0.
</p>
<p>I8. If μ(ξ 
= η)= 0, then
&int;
ξ dμ=
</p>
<p>&int;
η dμ.
</p>
<p>It is clear that all the convergence theorems remain valid as well.
</p>
<p>Theorem A3.4.1 (The dominated convergence theorem) Let |ξn| &le; η and
&int;
η dμ
</p>
<p>exist. If ξn
μ&minus;&rarr; or ξn &rarr; ξ a.e. as n&rarr;&infin; then
</p>
<p>&int;
ξn dμ&rarr;
</p>
<p>&int;
ξ dμ.
</p>
<p>Theorem A3.4.2 (The monotone convergence theorem) If 0 &le; ξn &uarr; ξ as n &rarr; &infin;
then &int;
</p>
<p>ξn dμ&rarr;
&int;
</p>
<p>ξ dμ.
</p>
<p>Theorem A3.4.3 (Fatou&ndash;Lebesgue) The statement and proof of this theorem is ob-
tained from those of Theorem A3.3.2 by replacing P with μ.
</p>
<p>In conclusion we note that if Ω = R = (&minus;&infin;,&infin;), F = B is the σ -algebra of
Borel sets, μ is the Lebesgue measure, and the function g(x) is continuous, then
</p>
<p>the integral
&int;
[a,b] g(x)dμ(x) coincides with the Riemann integral
</p>
<p>&int; b
a
g(x)dx. This
</p>
<p>follows from the preceding remarks in part 2 of Sect. 3.3 of this appendix.
</p>
<p>3.5 The Lebesgue Decomposition Theorem and the
</p>
<p>Radon&ndash;Nikodym Theorem
</p>
<p>We return to a question that has already been asked in the previous section. Un-
</p>
<p>der what conditions on measures μ and λ given on 〈Ω,F〉 can the measure μ be</p>
<p/>
</div>
<div class="page"><p/>
<p>644 3 Elements of Measure Theory and Integration
</p>
<p>represented as
</p>
<p>μ(A)=
&int;
</p>
<p>A
</p>
<p>f dλ?
</p>
<p>We do not assume here that λ is a probability measure.
</p>
<p>Definition A3.5.1 A measure μ is said to be absolutely continuous with respect to
a measure λ (we write μ≺ λ) if, for any A such that λ(A)= 0, one has μ(A)= 0.
</p>
<p>Definition A3.5.2 A set Nμ is said to be a support1 of measure μ if
μ(Ω &minus;Nμ)= 0.
</p>
<p>Definition A3.5.2 specifies a rather wide class of sets which can be called the
</p>
<p>support of the measure μ when μ is concentrated on a part of the space Ω . If Ω =R
is the real line (and in some other cases as well), one can use another definition
</p>
<p>which specifies a unique set for each measure. Consider the collection of all intervals
</p>
<p>(a, b) &sub; R with rational endpoints a and b. This collection is countable. Remove
from Ω =R all such intervals for which μ((a, b))= 0. The remaining set (which is
measurable) is called the support of the measure μ.
</p>
<p>Definition A3.5.3 One says that a measure μ is singularwith respect to λ if there
exists a support Nλ of the measure λ such that μ(Nλ)= 0. Or, which is the same, if
there exists a support Nμ of the measure μ such that λ(Nμ)= 0.
</p>
<p>The last definition, in contrast to Definition A3.5.1, is symmetric, so one can
</p>
<p>speak about mutually singular measures μ and λ (this relation is often written as
μ&perp; λ).
</p>
<p>Theorem A3.5.1 (Radon&ndash;Nikodym) A necessary and sufficient condition for
the absolute continuity μ ≺ λ is that there exists a function f unique up to λ-
equivalence (i.e., up to values on a set of zero λ-measure) such that2
</p>
<p>μ(A)=
&int;
</p>
<p>A
</p>
<p>f dλ.
</p>
<p>As we have already noted, the function f is called the Radon&ndash;Nikodym derivative
dμ/dλ of the measure μ with respect to λ (or density of μ with respect to λ).
</p>
<p>Since sufficiency in the assertion of the theorem is obvious, we will obtain the
</p>
<p>Radon&ndash;Nikodym theorem as a consequence of the following Lebesgue decomposi-
</p>
<p>tion theorem.
</p>
<p>1The conventional definition of support refers to the case when Ω is a topological space. Then the
</p>
<p>support of μ is the smallest closed set such that its complement is of μ-measure zero.
</p>
<p>2This equality is sometimes adopted as a definition of absolute continuity.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 The Lebesgue Decomposition Theorem 645
</p>
<p>TheoremA3.5.2 (Lebesgue) Let μ and λ be two σ -finite measures given on 〈Ω,F〉.
There exists a unique decomposition of the measure μ into two components μa and
μs such that
</p>
<p>μa ≺ λ, μs &perp; λ.
Moreover, there exists a function f , unique up to λ-equivalence, such that
</p>
<p>μa(A)=
&int;
</p>
<p>A
</p>
<p>f dλ.
</p>
<p>It is obvious that if μ ≺ λ then μs = 0, and the Lebesgue theorem then implies
the Radon&ndash;Nikodym theorem.
</p>
<p>Proof Since μ and λ are σ -finite, there exist increasing sequences of sets Ωμn
and Ωλn such that
</p>
<p>μ
(
Ωμn
</p>
<p>)
&lt;&infin;, λ
</p>
<p>(
Ωλn
</p>
<p>)
&lt;&infin;,
</p>
<p>⋃
</p>
<p>n
</p>
<p>Ωμn =Ω,
⋃
</p>
<p>n
</p>
<p>Ωλn =Ω.
</p>
<p>Putting Ωn :=Ωμn &cap;Ωλn we obtain a sequence of sets increasing to Ω for which
</p>
<p>μ(Ωn) &lt;&infin;, λ(Ωn) &lt;&infin;.
</p>
<p>If we prove the decomposition theorem for restrictions of the measures μ and λ
</p>
<p>to 〈Bn,Fn〉, where Bn =Ωn+1 &minus;Ωn and Fn is formed by sets BnA, A &isin; F, we will
thereby prove it for the whole Ω . It will suffice to take μa and μs to be the sums of
</p>
<p>the respective components for each of the restrictions. This remark means that we
</p>
<p>can consider the case of finite measures only.
</p>
<p>Thus let μ and λ be finite measures.
</p>
<p>(a) Let F be the class of functions f &ge; 0 such that
&int;
</p>
<p>A
</p>
<p>f dλ&le; μ(A) for all A &isin; F (A3.5.1)
</p>
<p>(the class F is non-empty, for the function f = 0 belongs to F). Set
</p>
<p>α := sup
f&isin;F
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>f dλ&le; μ(Ω) &lt;&infin;
</p>
<p>and choose a sequence fn such that, as n&rarr;&infin;,
&int;
</p>
<p>fn dλ&rarr; α.
</p>
<p>Put f̂n := max(f1, . . . , fn). Then clearly f̂n &uarr; f̂ := supfn and by the monotone
convergence theorem
</p>
<p>&int;
</p>
<p>A
</p>
<p>f̂n dλ&rarr;
&int;
</p>
<p>A
</p>
<p>f̂ dλ. (A3.5.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>646 3 Elements of Measure Theory and Integration
</p>
<p>We now show that f̂ &isin; F, i.e., that (A3.5.1) holds for f̂ . To this end, it suffices
by virtue of (A3.5.2) to notice that f̂n &isin; F. Let Ak , k = 1, . . . , n be disjoint sets on
which f̂n = fk . Then
</p>
<p>⋃n
k=1 Ak =Ω and
</p>
<p>&int;
</p>
<p>A
</p>
<p>f̂n dλ=
n&sum;
</p>
<p>k=1
</p>
<p>&int;
</p>
<p>AAk
</p>
<p>fk dλ&le;
n&sum;
</p>
<p>k=1
μ(AAk)= μ(A).
</p>
<p>Thus, for the &ldquo;maximum&rdquo; element f &prime; of F, (A3.5.1) also holds.
(b) Putting
</p>
<p>μa(A) :=
&int;
</p>
<p>A
</p>
<p>f̂ dλ, μs = μ&minus;μa (A3.5.3)
</p>
<p>we prove that μs is singular with respect to λ. We will need the following asser-
</p>
<p>tion about the decomposition of an arbitrary signed measure (for the definition, see
</p>
<p>Sect. 3.3.1 of this appendix).
</p>
<p>Theorem A3.5.3 (The Hahn theorem on decomposition of a measure) For any
signed finite measure γ , there exist disjoint sets D+ &isin; F and D&minus; &isin; F such that, for
any A &isin; F,
</p>
<p>γ
(
AD+
</p>
<p>)
&ge; 0, γ
</p>
<p>(
AD&minus;
</p>
<p>)
&le; 0.
</p>
<p>Proof We first prove that there exists a set D &isin; F on which γ (A) attains its upper
bound.
</p>
<p>Let Bn &isin; F be a sequence such that γ (Bn) &rarr; Γ = supA γ (A) as n &rarr; &infin;.
Put B :=
</p>
<p>⋃
k Bk and consider, for a fixed n, the decomposition of T into 2
</p>
<p>n sets
</p>
<p>Bn,m, m= 1, . . . ,2n, of the form
⋂n
</p>
<p>k=1 B
&prime;
k , where B
</p>
<p>&prime;
k = Bk or B &minus; Bk , k &le; n. For
</p>
<p>n &lt; N , each Bn,m is a finite union of sets BN,M , 1 &le;M &le; 2N . Denote by Dn the
sum of all Bn,m for which γ (Bn,m)&ge; 0. Then γ (Bn)&le; γ (Dn).
</p>
<p>On the other hand, for N &gt; n, each BN,M either belongs to Dn or is disjoint with
</p>
<p>it. Therefore
</p>
<p>γ (Dn)&le; γ (Dn +Dn+1 + &middot; &middot; &middot; +DN ).
This implies that, for the set D = lim
</p>
<p>⋃&infin;
k=nDk , one has γ (Bn) &le; γ (D), Γ &le;
</p>
<p>γ (D). Recalling the definition of Γ , we obtain that γ (D)= Γ .
Thus we have proved the existence of a set D on which γ (D) attains its max-
</p>
<p>imum. We now show that, for any A &isin; F, one has γ (AD) &ge; 0 and γ (AD) &le; 0,
where D =Ω &minus;D. Indeed, assuming, for instance, that γ (AD) &lt; 0, we come to a
contradiction, for in that case
</p>
<p>γ (D &minus;AD)= γ (D)&minus; γ (AD) &gt; γ (D).
</p>
<p>Similarly, assuming that γ (AD) &gt; 0, we would get
</p>
<p>γ (D +AD)= γ (D)+ γ (AD) &gt; γ (D).
</p>
<p>It remains to put D+ :=D, D&minus; :=D. The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 The Lebesgue Decomposition Theorem 647
</p>
<p>Corollary A3.5.1 Any finite signed measure γ can be represented as γ = γ+&minus;γ&minus;,
where γ&plusmn; are finite nonnegative measures.
</p>
<p>To prove the corollary, it suffices to put
</p>
<p>γ&plusmn;(A) := &plusmn;γ
(
A&cap;D&plusmn;
</p>
<p>)
,
</p>
<p>where D&plusmn; are the sets from the Hahn decomposition theorem. �
</p>
<p>We return to the proof of the fact that the measure μs in equality (A3.5.3) is
</p>
<p>singular. Let D+n be the set in the Hahn decomposition of the signed measure
</p>
<p>νn = μs &minus;
1
</p>
<p>n
λ.
</p>
<p>Put N =
⋂
</p>
<p>nD
&minus;
n . Then N =
</p>
<p>⋃
nD
</p>
<p>+
n and, for all n and A &isin; F,
</p>
<p>0 &le; μs(AN)&le;
1
</p>
<p>n
λ(AN).
</p>
<p>From here, letting n &rarr; &infin;, we obtain μs(AN) = 0 and hence μs(A) = μs(AN).
That is, the set N is a support of the measure μs .
</p>
<p>Further, because
</p>
<p>μa(A)= μ(A)&minus;μs(AN)&le; μ(A)&minus;μs
(
AD+n
</p>
<p>)
,
</p>
<p>we have
</p>
<p>&int;
</p>
<p>A
</p>
<p>(
f̂ + 1
</p>
<p>n
ID+n
</p>
<p>)
dλ= μa(A)+
</p>
<p>1
</p>
<p>n
λ
(
AD+n
</p>
<p>)
&le; μ(A)&minus; νn
</p>
<p>(
AD+n
</p>
<p>)
&le; μ(A).
</p>
<p>This means that f̂ + 1
n
</p>
<p>ID+n &isin; F and hence
</p>
<p>α &ge;
&int; (
</p>
<p>f̂ + 1
n
</p>
<p>ID+n
</p>
<p>)
dλ= α + 1
</p>
<p>n
λ
(
D+n
</p>
<p>)
.
</p>
<p>This implies λ(D+n )= 0 and λ(N)= 0, so that μs is singular with respect to λ since
N is a support of μs . �
</p>
<p>Uniqueness of the decomposition μ = μa + μs can be established as follows.
Assume that μ= μ&prime;a+μ&prime;s is another decomposition. Then γ := μ&prime;a&minus;μa = μs&minus;μ&prime;s .
By singularity, there exist sets N and N &prime; such that μs(N)= 0, λ(N)= 0, μ&prime;s(N
</p>
<p>&prime;
)=
</p>
<p>0, and λ(N &prime;)= 0. Clearly, λ(D)= 0, where D = N &cup;N &prime;. If we assumed that γ =
μ&prime;a &minus; μa = μs &minus; μ&prime;s 
= 0, then there would exist an A &isin; F such that γ (A) 
= 0.
Therefore, either γ (AD) 
= 0 or γ (AD) 
= 0. However, the former is impossible,
for λ(D)= 0 implies μ&prime;a(D)= μa(D)= 0. The latter is also impossible, since D =
N N
</p>
<p>&prime;
and hence μs(D)= μ&prime;s(D)= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>648 3 Elements of Measure Theory and Integration
</p>
<p>Uniqueness of the function f (up to λ-equivalence) follows from the observation
that the equalities
</p>
<p>μa(A)=
&int;
</p>
<p>A
</p>
<p>f dλ=
&int;
</p>
<p>A
</p>
<p>f &prime;dλ,
&int;
</p>
<p>A
</p>
<p>(
f &minus; f &prime;
</p>
<p>)
dλ= 0
</p>
<p>for all A imply the equality f &minus; f &prime; = 0 a.e. Assuming, say, that λ(A) &gt; 0 for
A= {ω : f &minus; f &prime; &gt; ε} would yield for such A the relation
</p>
<p>&int;
A
(f &minus; f &prime;) dλ&gt; 0. The
</p>
<p>theorem is proved. �
</p>
<p>One of the most important applications of the Radon&ndash;Nikodym theorem is the
</p>
<p>proof of existence and uniqueness of conditional expectations.
</p>
<p>Proof Let F0 be a σ -subalgebra of F and ξ a random variable on 〈Ω,F,P〉 such
that Eξ exists. In Sect. 4.8 we defined the conditional expectation E(ξ |F0) of the
variable ξ given F0 as an F0-measurable random variable η for which
</p>
<p>E(η;B)= E(ξ ;B) (A3.5.4)
</p>
<p>for any B &isin; F. We can assume without loss of generality that ξ &ge; 0 (an arbitrary
function ξ can be represented as a difference of two positive functions). Then
</p>
<p>the right-hand side of (A3.5.4) will be a measure on 〈Ω,F0〉. Since E(ξ ;B) = 0
if P(B) = 0, this measure will be absolutely continuous with respect to P. This
implies, by the Radon&ndash;Nikodym theorem, the existence of a unique (up to P-
</p>
<p>equivalence) measurable function η on 〈Ω,F0〉 such that, for any B &isin; F0,
</p>
<p>E(ξ ;B)=
&int;
</p>
<p>B
</p>
<p>η dP.
</p>
<p>This relation is clearly equivalent to (A3.5.4). It establishes the required existence
</p>
<p>and uniqueness of the conditional expectation. �
</p>
<p>Another consequence of the assertions proved in the present section was men-
</p>
<p>tioned in Sect. 3.6 and is related to the Lebesgue theorem stating that any distribu-
</p>
<p>tion P on the real line R= (&minus;&infin;,&infin;) (or the respective distribution function) has a
unique representation as a sum of the three components P= Pa + Ps + P&part; , where
the component Pa is absolutely continuous with respect to Lebesgue measure:
</p>
<p>Pa(A)=
&int;
</p>
<p>A
</p>
<p>f (x)dx;
</p>
<p>P&part; is the discrete component concentrated on an at most countable set of points
</p>
<p>x1, x2, . . . such that P({xk}) &gt; 0, and the component Ps has a support of Lebesgue
measure zero and a continuous distribution function. This is an immediate conse-
</p>
<p>quence of the Lebesgue decomposition theorem. One just has to extract the dis-
</p>
<p>crete part from the singular (with respect to Lebesgue measure λ) component of P,
</p>
<p>first removing all the points x for which P({x}) &ge; 1/2, then all points x for which</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Convergence in Arbitrary Spaces 649
</p>
<p>P({x})&ge; 1/3, and so on. It is clear that in this way we will get at most a countable
set of xs, and that this process determines uniquely the discrete component P&part; .
</p>
<p>All the aforesaid clearly also applies to distributions in n-dimensional Euclidean
</p>
<p>spaces Rn.
</p>
<p>3.6 Weak Convergence and Convergence in Total Variation of
</p>
<p>Distributions in Arbitrary Spaces
</p>
<p>3.6.1 Weak Convergence
</p>
<p>In Sects. 6.2 and 7.6 we studied weak convergence of distributions of random vari-
</p>
<p>ables and vectors, i.e., weak convergence of distributions in Rk , k &ge; 1. Now we
want to introduce the notion of weak convergence in more general spaces X. As the
</p>
<p>definitions given in Sect. 6.2 show, we will need continuous functions f (x) on X.
</p>
<p>This is possible only if the space X is endowed with a topology. For simplicity&rsquo;s
</p>
<p>sake, we restrict ourselves to the case where the space X is endowed with a met-
</p>
<p>ric ρ(x, y). Thus, assume we are given a measurable space 〈X,B〉 with a metric ρ
which is &ldquo;consistent&rdquo; with the σ -algebra B, i.e., all open (with respect to the met-
</p>
<p>ric ρ) sets from X belong to B (cf. Sect. 16.1), so that any continuous (with respect
</p>
<p>to ρ) functional will be B-measurable. This means that if a distribution Q is given
</p>
<p>on 〈X,B〉 (i.e., a probability space 〈X,B,Q〉 is given), then {x : f (x) &lt; t} &isin;B for
any t , and the probabilities of these sets are defined.
</p>
<p>Now let 〈Ω,F,P〉 be the basic probability space. A measurable mapping
ξ = ξ(ω) of the space 〈Ω,F〉 to 〈X,B〉 is called an X-valued random element.
If 〈Ω,F〉 = 〈X,B〉, the mapping ξ may be the identity mapping. The space 〈X,B〉
is said to be the sample or state space of the random element ξ . When a functional
f is continuous, f (ξ) is a random variable in 〈R,B〉.
</p>
<p>Definition A3.6.1 Let a distribution P and a sequence of distributions Pn be given
</p>
<p>on the space 〈X,B〉. The sequence Pn is said to converge weakly to P: Pn &rArr; P as
n&rarr;&infin; if, for any bounded continuous functional f (f &isin; Cb(X)),
</p>
<p>&int;
f (x)dPn(x)&rarr;
</p>
<p>&int;
f (x)dP(x). (A3.6.1)
</p>
<p>If ξn and ξ are random elements having the distributions Pn and P, respectively,
</p>
<p>then (A3.6.1) is equivalent to
</p>
<p>Ef (ξn)&rarr; Ef (ξ). (A3.6.2)
</p>
<p>This, in turn, for any continuous functional f (f &isin; C(X)), is equivalent to
</p>
<p>f (ξn)&rArr; f (ξ). (A3.6.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>650 3 Elements of Measure Theory and Integration
</p>
<p>Indeed, (A3.6.3) means that, for any bounded continuous function g (g &isin; Cb(R)),
</p>
<p>Eg
(
f (ξn)
</p>
<p>)
&rarr; Eg
</p>
<p>(
f (ξ)
</p>
<p>)
, (A3.6.4)
</p>
<p>which is equivalent to (A3.6.2).
</p>
<p>If X=X(T ) is the space of real-valued functions x(t), t &isin; T , given on a paramet-
ric set T , and a measurable mapping ξ(ω) of the basic probability space 〈Ω,F,P〉
into 〈X,B〉 is given, then the random element ξ(ω)= ξ(ω, t) will be a random pro-
cess (see Sect. 18.1) if {x : x(t) &lt; u} &isin;B for all t, u. In that case (A3.6.1)&ndash;(A3.6.4)
will refer to the weak convergence of the distributions of random processes which
</p>
<p>has already been studied in Chap. 20.
</p>
<p>In the metric space X, for any A &isin;X, one can define its boundary
</p>
<p>&part;A= [A] &minus; (A),
</p>
<p>where [A] is the closure of A, (A) being its interior ((A)=X&minus; [A], where A is the
complement of A).
</p>
<p>Definition A3.6.2 A set A is said to be a continuity set of the distribution P (or
P-continuous set) if P(&part;A) = 0. We will denote the class of all P-continuous sets
by DP .
</p>
<p>The following criterion of weak convergence of distributions holds true.
</p>
<p>Theorem A3.6.1 The following four conditions are equivalent:
</p>
<p>(i) Pn &rArr; P,
(ii) limn&rarr;&infin; Pn(A)= P(A) for all A &isin;DP ,
</p>
<p>(iii) lim supn&rarr;&infin; Pn(F )&le; P(F ) for all closed F &sub;X,
(iv) lim infn&rarr;&infin; Pn(G)&ge; P(G) for all open G&sub;X.
</p>
<p>Observe that if Pn &rArr; P, then convergence (A3.6.1)&ndash;(A3.6.3) takes place for
a wider class of functionals than Cb(X) (C(X)), namely, for the so-called P-
</p>
<p>continuous functionals (or functionals continuous with P-probability 1). We will
</p>
<p>call so the functionals f for which f (xn)&rarr; f (x) as ρ(xn, x)&rarr; 0 not for all x &isin;X,
but only for x &isin;A, P(A)= 1. The class of P-continuous functionals will be denoted
by CP (X).
</p>
<p>The classes DP and CP (X), and also the classes of all closed and open sets par-
</p>
<p>ticipating in Theorem A3.6.1, are very wide which makes verifying the conditions
</p>
<p>of Theorem A3.6.1 rather difficult and cumbersome. These classes can be substan-
</p>
<p>tially restricted if we consider not arbitrary but only relatively compact sequences
Pn (from any subsequence P
</p>
<p>&prime;
n one can choose a convergent subsequence; this ap-
</p>
<p>proach was already used in Sect. 6.3).
</p>
<p>Definition A3.6.3 A class D of sets from B is said to determine the measure P if,
for a measure Q, the equalities P(A)=Q(A) for all A &isin;DDP imply Q= P.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Convergence in Arbitrary Spaces 651
</p>
<p>A class D determines the measure P if D is an algebra and σ(DDP ) = BX
(condition σ(D)=BX is insufficient (see e.g. [4])).
</p>
<p>In a similar way we introduce the class F of functionals f determining the distri-
</p>
<p>bution P of a random element ξ = ξP : for any Q, the coincidence of the distributions
of f (ξP ) and f (ξQ) for all f &isin; FCP (X) implies P=Q.
</p>
<p>Theorem A3.6.2 A necessary and sufficient condition for convergence Pn &rArr; P is
that:
</p>
<p>(1) the sequence Pn is relatively compact; and
(2) there exists a class of sets D &sub;BX determining the measure P and such that
</p>
<p>Pn(A)&rarr; P(A) for any A &isin;DDP .
</p>
<p>An alternative to condition (2) is the existence of a class of functionals F which
</p>
<p>determines the measure P and is such that P(f (ξn) &lt; t) &rArr; P(f (ξ) &lt; t) for all
f &isin; FCP (X).
</p>
<p>The following notion of tightness plays an important role in establishing the com-
</p>
<p>pactness of {Pn}.
</p>
<p>Definition A3.6.4 A family of distributions {Pn} on 〈X,B〉 is said to be tight if,
for any ε &gt; 0, there exists a compact set K =Kε &sub; X such that Pn(K) &gt; 1 &minus; ε for
all n.
</p>
<p>Theorem A3.6.3 (Prokhorov) If {Pn} is a tight family of distributions then it is
relatively compact. If X is a complete separable space, the converse assertion is
also true.
</p>
<p>Since, for many functional spaces (in particular, for spaces C(0, T ) and D(0, T )),
</p>
<p>there exist simple explicit criteria for compactness of sets, one can now establish
</p>
<p>conditions ensuring convergence Pn &rArr; P in these spaces. It is well known, for ex-
ample, that in the above-mentioned spaces compacta are, roughly speaking, of the
</p>
<p>form {x : ω△(x) &le; ε(△)}, where ω△(x) is the so-called &ldquo;modulus of continuity&rdquo;
(in the space C or D, respectively) of the element x, and ε(△) &ge; 0 is an arbitrary
function vanishing as △&darr; 0.
</p>
<p>The proofs of Theorems A3.6.1&ndash;A3.6.3 can be found, for example, in [1]. We do
not present them here as they are somewhat beyond the scope of this book and, on
</p>
<p>the other hand, the theorems themselves are not used in the body of the text. We use
</p>
<p>only the special cases of these theorems given in Sects. 6.2 and 6.3.
</p>
<p>The invariance principle of Sect. 20.1 is a theorem about weak convergence of
</p>
<p>distributions in the space C(0,1). In order to use Theorems A3.6.2 and A3.6.3 to
</p>
<p>prove this result, one has to choose the class D to be the class of cylinder sets.
</p>
<p>Convergence of Pn to P on sets from this class D is the convergence of finite-
</p>
<p>dimensional distributions of processes sn(t) generated by sums of random vari-
</p>
<p>ables (see Sect. 20.1). Since the increments of sn(t) are essentially independent,
</p>
<p>the demonstration of that part of the theorem reduces to proving asymptotic normal-
</p>
<p>ity of these increments, which follows immediately from the central limit theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>652 3 Elements of Measure Theory and Integration
</p>
<p>The condition of compactness of the family of distributions in C(0,1) requires, ac-
</p>
<p>cording to Theorem A3.6.3, a proof that the modulus of continuity of the trajectory
</p>
<p>sn(t) converges to zero in probability (for more details, see e.g. [1]). This could be
</p>
<p>proved using the Kolmogorov inequality from Corollary 11.2.1.
</p>
<p>3.6.2 Convergence in Total Variation
</p>
<p>So, to consider weak convergence of distributions in spaces 〈X,B〉 of a general
nature, one has to introduce a topology in the space, which is not always convenient
</p>
<p>and feasible. There exists another type of convergence of distributions on 〈X,B〉
which does not require the introduction of topologies. This is convergence in total
</p>
<p>variation.
</p>
<p>Definition A3.6.5 Let γ be a finite signed measure on 〈X,B〉. The total variation
of γ (or the total variation norm ‖γ ‖) is the quantity
</p>
<p>‖γ ‖ = sup
f :|f |&le;1
</p>
<p>∣∣∣∣
&int;
</p>
<p>f (x)dγ (x)
</p>
<p>∣∣∣∣, (A3.6.5)
</p>
<p>where the supremum is taken over the class of all B-measurable functions f (x)
</p>
<p>such that |f (x)| &le; 1 for all x &isin;X.
</p>
<p>The supremum in (A3.6.5) is clearly attained on such functions f for which,
</p>
<p>roughly speaking, f (x) = 1 at points x such that dγ (x) &gt; 0, and f (x) = &minus;1 at
points x for which dγ (x) &lt; 0. Therefore (A3.6.5) can be written in the form
</p>
<p>‖γ ‖ =
&int; ∣∣dγ (x)
</p>
<p>∣∣. (A3.6.6)
</p>
<p>An exact meaning to this expression can be given using the Hahn decomposition
</p>
<p>theorem (see Corollary A3.5.1), which implies
</p>
<p>‖γ ‖ = γ+(X)+ γ&minus;(X). (A3.6.7)
</p>
<p>The right-hand side of this equality may be taken as a definition of
&int;
|dγ (x)|.
</p>
<p>Lemma A3.6.2 If γ (X)= 0, then ‖γ ‖ = 2 supB&isin;B γ (B).
</p>
<p>Proof From (A3.6.5) it follows that, for any B (B is the complement of B , γ (B)&cup;
γ (B)=0),
</p>
<p>‖γ ‖ &ge;
∣∣γ (B)
</p>
<p>∣∣+
∣∣γ (B)
</p>
<p>∣∣= 2
∣∣γ (B)
</p>
<p>∣∣.
Therefore ‖γ ‖ &ge; 2 supB&isin;B |γ (B)|.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Convergence in Arbitrary Spaces 653
</p>
<p>To obtain the converse inequality, we will make use of Corollary A3.5.1 of the
</p>
<p>Hahn decomposition theorem. As we have already noted, according to that theorem
</p>
<p>(for the definition of the set D&plusmn; see the Hahn theorem),
</p>
<p>‖γ ‖ = γ+(X)+ γ&minus;(X)= γ+
(
D+
</p>
<p>)
+ γ&minus;
</p>
<p>(
D+
</p>
<p>)
</p>
<p>= γ
(
D+
</p>
<p>)
&minus; γ
</p>
<p>(
D+
</p>
<p>)
= 2γ
</p>
<p>(
D+
</p>
<p>)
&le; 2 sup
</p>
<p>B&isin;B
γ (B).
</p>
<p>The lemma is proved. �
</p>
<p>Definition A3.6.6 Let P be a distribution and Pn, n= 1,2, . . . , a sequence of dis-
tributions given on 〈X,B〉. We will say that Pn converges to P in total variation:
Pn
</p>
<p>T V&minus;&rarr; P, if ‖Pn &minus; P‖&rarr; 0 as n&rarr;&infin;.
</p>
<p>Convergence in total variation is a very strong form of convergence. If 〈X,B〉 is
a metric space and Pn
</p>
<p>T V&minus;&rarr; P, then Pn &rArr; P. Indeed, since any functional f &isin; Cb(X)
is bounded: |f (x)|&lt; b, we have
</p>
<p>∣∣∣∣
&int;
</p>
<p>f (dPn &minus; dP)
∣∣∣∣&le; b
</p>
<p>&int; ∣∣d(Pn &minus; P)
∣∣= b‖Pn &minus; P‖&rarr; 0.
</p>
<p>Thus in that case &int;
f dPn &rarr;
</p>
<p>&int;
f dP
</p>
<p>even without assuming the continuity of f .
</p>
<p>The converse assertion about convergence Pn
T V&minus;&rarr; P if Pn &rArr; P is not true.
</p>
<p>Let, for example, X = [0,1], Pn be the uniform distribution on the set of n + 1
points {0,1/n, . . . , n/n}, and P = U0,1. It is clear that all Pn are concentrated on
the countable set N of all rational numbers. Therefore Pn(N) = 1, P(N) = 0, and
‖Pn &minus; P‖ = Pn(N)+ P(X \N)= 2. At the same time, clearly Pn &rArr; P.
</p>
<p>Now let the distribution P have a density p with respect to a measure μ (one
</p>
<p>could take, in particular, μ= P, in which case p(x)&equiv; 1). Denote by pn the density
(with respect to μ) of the absolutely continuous (with respect to μ) component Pan
of the distribution Pn.
</p>
<p>Theorem A3.6.4 A necessary and sufficient condition for convergence Pn
T V&minus;&rarr; P is
</p>
<p>that pn converges to p in measure μ, i.e., for any ε &gt; 0,
</p>
<p>μ
{
x :
</p>
<p>∣∣pn(x)&minus; p(x)
∣∣&gt; ε
</p>
<p>}
&rarr; 0 as n&rarr;&infin;.
</p>
<p>Proof We have
&int; ∣∣d(Pn &minus; P)
</p>
<p>∣∣=
&int; ∣∣pn(x)&minus; p(x)
</p>
<p>∣∣μ(dx)+
∥∥Psn
</p>
<p>∥∥,
</p>
<p>where Psn is the singular component of Pn with respect to the measure μ.</p>
<p/>
</div>
<div class="page"><p/>
<p>654 3 Elements of Measure Theory and Integration
</p>
<p>Let ‖Pn &minus; P‖&rarr; 0. Then
&int;
</p>
<p>|pn &minus; p|dμ&rarr; 0, (A3.6.8)
</p>
<p>and hence
</p>
<p>μ
{
x :
</p>
<p>∣∣pn(x)&minus; p(x)
∣∣&gt; ε
</p>
<p>}
&le; ε&minus;1
</p>
<p>&int;
|pn &minus; p|dμ&rarr; 0.
</p>
<p>Now let pn
μ&minus;&rarr; p. Put
</p>
<p>Bε =
{
x : p(x)&ge; ε
</p>
<p>}
, An,ε =
</p>
<p>{
x :
</p>
<p>∣∣pn(x)&minus; p(x)
∣∣&le; ε2
</p>
<p>}
.
</p>
<p>Then
</p>
<p>1 &ge;
&int;
</p>
<p>Bε
</p>
<p>p dμ&ge; εμ(Bε), μ(Bε)&le;
1
</p>
<p>ε
.
</p>
<p>Consider &int;
|pn &minus; p|dμ=
</p>
<p>&int;
</p>
<p>BεAn,ε
</p>
<p>+
&int;
</p>
<p>BεAn,ε
</p>
<p>. (A3.6.9)
</p>
<p>Here the first integral on the right-hand side does not exceed ε. Since
</p>
<p>lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>Bε
</p>
<p>p dμ&rarr; 1,
</p>
<p>we will have, for a given δ &gt; 0 and sufficiently small ε, the inequality
</p>
<p>&int;
</p>
<p>Bε
</p>
<p>p dμ&gt; 1 &minus; δ
</p>
<p>and, for n large enough,
</p>
<p>&int;
</p>
<p>BεAn,ε
</p>
<p>p dμ&gt; 1 &minus; 2δ,
&int;
</p>
<p>BnAn,ε
</p>
<p>pn dμ&gt; 1 &minus; 3δ. (A3.6.10)
</p>
<p>It follows from these two inequalities that the second integral in (A3.6.9) does not
</p>
<p>exceed 5δ, which proves (A3.6.8). Furthermore, (A3.6.10) implies that ‖Pan‖&gt; 1 &minus;
3δ and ‖Psn‖&lt; 3δ. The theorem is proved. �
</p>
<p>The theorem implies that if Pn
T V&minus;&rarr; P then the absolutely continuous with respect
</p>
<p>to μ= P component Pan of the distribution Pn has a density pn(x)
p&rarr; 1, Pan(X)&rarr; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 4
</p>
<p>The Helly and Arzel&agrave;&ndash;Ascoli Theorems
</p>
<p>In this appendix we will prove Helly&rsquo;s theorem and the Arzel&agrave;&ndash;Ascoli theorem. The
</p>
<p>former theorem was used in Sect. 6.3, and both theorems will be used in the proof
</p>
<p>of the main theorem of Appendix 9.
</p>
<p>Let F be the class of all distribution functions, and G the class of functions G
</p>
<p>possessing properties F1 and F2 from Sect. 3.2 (monotonicity and left continuity),
</p>
<p>and the properties G(&minus;&infin;)&ge; 0 and G(&infin;)&le; 1. We will write Gn &rArr;G as n&rarr;&infin;,
G &isin; G, if Gn(x)&rarr;G(x) at all points of continuity of the function G.
</p>
<p>Theorem A4.1 (Helly) Any sequence Fn &isin; F contains a convergent subsequence
Fnn &rArr; F &isin; G.
</p>
<p>We will need the following.
</p>
<p>Lemma A4.1 A sufficient condition for convergence Fn &rArr; F &isin; G is that
</p>
<p>Fn(x)&rarr; F(x), x &isin;D,
</p>
<p>as n&rarr;&infin; on some everywhere dense set D of the reals.
</p>
<p>Proof Let x be an arbitrary point of continuity of F(x). For arbitrary x&prime;, x&prime;&prime; &isin; D
such that x&prime; &le; x &le; x&prime;&prime;, one has
</p>
<p>Fn
(
x&prime;
)
&le; Fn(x)&le; Fn
</p>
<p>(
x&prime;&prime;
</p>
<p>)
.
</p>
<p>Consequently,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>Fn
(
x&prime;
)
&le; lim inf
</p>
<p>n&rarr;&infin;
Fn(x)&le; lim sup
</p>
<p>n&rarr;&infin;
Fn(x)&le; lim
</p>
<p>n&rarr;&infin;
Fn
</p>
<p>(
x&prime;&prime;
</p>
<p>)
.
</p>
<p>From here and the conditions of the lemma we obtain
</p>
<p>F
(
x&prime;
)
&le; lim inf
</p>
<p>n&rarr;&infin;
Fn(x)&le; lim sup
</p>
<p>n&rarr;&infin;
Fn(x)&le; F
</p>
<p>(
x&prime;&prime;
</p>
<p>)
.
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>655</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>656 4 The Helly and Arzel&agrave;&ndash;Ascoli Theorems
</p>
<p>Letting x&prime; &uarr; x and x&prime;&prime; &darr; x along the set D and taking into account that x is a point
of continuity of F , we get
</p>
<p>lim
n&rarr;&infin;
</p>
<p>Fn(x)= F(x).
</p>
<p>The lemma is proved. �
</p>
<p>Proof of Theorem A4.1 Let D = {xn} be an arbitrary countable everywhere dense
set of real numbers. The numerical sequence {Fn(x1)} is bounded and hence con-
tains a convergent sequence {F1n(x1)}. Denote the limit of this sequence by F(x1).
Consider now the numerical sequence {F1n(x2)}. It also contains a convergent sub-
sequence {F2n(x2)} with a limit F(x2). Moreover,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>F2n(x1)= F(x1).
</p>
<p>Continuing this process, we will obtain, for any number k, k sequences
</p>
<p>{
Fkn(xi)
</p>
<p>}
, i = 1, . . . , k,
</p>
<p>such that limn&rarr;&infin; Fkn(xi)= F(xi).
Consider the diagonal sequence of the distribution functions {Fnn(x)}. For any
</p>
<p>xk &isin; D, only k &minus; 1 first elements of the numerical sequence {Fnn(xk)} may not
belong to the sequence Fkn(xk). Therefore
</p>
<p>lim
n&rarr;&infin;
</p>
<p>Fnn(xk)= F(xk).
</p>
<p>It is clear that F(x) is a non-decreasing bounded function given on D. It can
</p>
<p>easily be extended by continuity from the left to a non-decreasing function on the
</p>
<p>whole real line. Now we see that the sequence {Fnn} and the function F satisfy the
conditions of Lemma A4.1. The theorem is proved. �
</p>
<p>The conditions of Helly&rsquo;s theorem can be weakened. Namely, instead of F we
</p>
<p>could consider a wider class H of non-decreasing left continuous (i.e., satisfy-
</p>
<p>ing properties F1 and F3) functions H majorised by a fixed function: for any x,
</p>
<p>|H(x)| &le; N(x) &lt;&infin;, where N is a given function characterising the class H. We
do not exclude the case when |H(x)| (or N(x)) grow unboundedly as |x| &rarr;&infin;. The
following generalised version of Helly&rsquo;s theorem is true.
</p>
<p>Theorem A4.2 (Generalised Helly theorem) Any sequence Hn &isin; F contains a sub-
sequence Hnn which converges to a function H &isin; H at each point of continuity
of H .
</p>
<p>The Proof repeats the above proof of Helly&rsquo;s theorem. �
</p>
<p>To each function Hn &isin;H we can associate a measure μn by putting
</p>
<p>μn
(
[a, b)
</p>
<p>)
=Hn(b)&minus;Hn(a).</p>
<p/>
</div>
<div class="page"><p/>
<p>4 The Helly and Arzel&agrave;&ndash;Ascoli Theorems 657
</p>
<p>The generalised Helly theorem will then mean that, for any sequence of measures μn
generated by functions from H, there exists a subsequence μnn converging weakly
</p>
<p>on each finite interval of which the endpoints are not atoms of the limiting mea-
</p>
<p>sure μn.
</p>
<p>We give one more analogue of Helly&rsquo;s theorem which refers to a collection of
</p>
<p>equicontinuous functions gn. Recall that a sequence of functions {gn} is said to be
equicontinuous if, for any ε &gt; 0, there exists a δ &gt; 0 such that |x1 &minus; x2|&lt; δ implies
|gn(x1)&minus; gn(x2)|&lt; ε for all n.
</p>
<p>Theorem A4.3 (Arzel&agrave;&ndash;Ascoli) Let {gn} be a sequence of uniformly bounded and
equicontinuous functions of a real variable. Then there exists a subsequence gnk
converging to a continuous limit g uniformly on each finite interval.
</p>
<p>Proof Choose again a countable everywhere dense subset {xn} of the real line, and
a subsequence {gnk } converging at the points x1, x2, . . . Denote its limit at the point
xj by g(xj ). We have
</p>
<p>∣∣gnk (x)&minus; gnr (x)
∣∣ &le;
</p>
<p>∣∣gnk (x)&minus; gnk (xj )
∣∣+
</p>
<p>∣∣gnr (x)&minus; gnr (xj )
∣∣
</p>
<p>+
∣∣gnk (xj )&minus; gnr (xj )
</p>
<p>∣∣. (A4.1)
</p>
<p>By assumption, the last term on the right-hand side tends to 0 as nk &rarr;&infin;, nr &rarr;&infin;.
By virtue of equicontinuity, for any point x there exists a point xj such that, for
</p>
<p>all n,
∣∣gn(x)&minus; gn(xj )
</p>
<p>∣∣&lt; ε. (A4.2)
In any given finite interval I there exists a finite collection of points xj such that
</p>
<p>(A4.2) will hold for all points xj &isin; I . This implies that the right-hand side of (A4.1)
will be less than 3ε for all sufficiently large nk , nr uniformly over xj &isin; I . Thus there
exists the limit g(x)= limgnk (x), for which by (A4.2) we have |g(x)&minus; g(xj )| &le; ε,
which implies that g is continuous. The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 5
</p>
<p>The Proof of the Berry&ndash;Esseen Theorem
</p>
<p>In this appendix we prove the following assertion stated in Sect. 8.5.
</p>
<p>Theorem A5.1 (Berry&ndash;Esseen) Let ξk be independent identically distributed ran-
dom variables,
</p>
<p>Eξk = 0, Var(ξk)= 1, &micro;= E|ξk|3 &lt;&infin;, Sn =
n&sum;
</p>
<p>k=1
ξk, ζn =
</p>
<p>Sn&radic;
n
.
</p>
<p>Then, for all n,
</p>
<p>∆n := sup
x
</p>
<p>∣∣P(ζn &lt; x)&minus;Φ(x)
∣∣&lt; c&micro;&radic;
</p>
<p>n
,
</p>
<p>where Φ is the standard normal distribution function and c is an absolute constant.
</p>
<p>Proof We will make use of the composition method. As in Sect. 8.5, we will bound
∆n based on estimates of smallness of Eg(ζn) &minus; Eg(η), η &sub;=�0,1, for smooth g.
To get a bound for ∆n in Sect. 8.5, we chose g to be a function constant outside a
</p>
<p>small interval. The next lemma shows that such a choice is not obligatory. Let G be
</p>
<p>a distribution function and γ &sub;=G be independent of ζn and η. Put
</p>
<p>g(z) :=G
(
x &minus; z
ε
</p>
<p>)
,
</p>
<p>so that
</p>
<p>Eg(ζn)= EG
(
x &minus; ζn
</p>
<p>ε
</p>
<p>)
= P
</p>
<p>(
γ &lt;
</p>
<p>x &minus; ζn
ε
</p>
<p>)
= P(ζn + εγ &lt; x),
</p>
<p>Eg(η)= P(η+ εγ &lt; x).
</p>
<p>Set
</p>
<p>∆n,ε := sup
x
</p>
<p>∣∣∣∣EG
(
x &minus; ζn
</p>
<p>ε
</p>
<p>)
&minus;EG
</p>
<p>(
x &minus; η
ε
</p>
<p>)∣∣∣∣
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>659</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>660 5 The Proof of the Berry&ndash;Esseen Theorem
</p>
<p>= sup
x
</p>
<p>∣∣P(ζn + εγ &lt; x)&minus; P(η+ εγ &lt; x)
∣∣
</p>
<p>= sup
x
</p>
<p>∣∣∣∣
&int;
</p>
<p>dG(y)
[
P(ζn &lt; x &minus; εy)&minus; P(η &lt; x &minus; εy)
</p>
<p>]∣∣∣∣.
</p>
<p>Clearly, ∆n,ε &le;∆n. Our aim will be to obtain a converse inequality for ∆n.
</p>
<p>Lemma A5.1 Let v &gt; 0 be such that G(v)&minus;G(&minus;v)&ge; 3/4. Then, for any ε &gt; 0,
</p>
<p>∆n &le; 2∆n,ε +
3vε&radic;
</p>
<p>2π
.
</p>
<p>Proof Assume that the supx in the definition of ∆n is attained on a positive value
∆n(x) := Fn(x)&minus;Φ(x) (the case of a negative value ∆n(x) is similar) and that, for
a given δ &gt; 0, the value xδ is such that
</p>
<p>∆n(xδ)= Fn(xδ)&minus;Φ(xδ)&ge;∆n &minus; δ,
</p>
<p>where Fn is the distribution function of ζn. When the argument increases, the value
</p>
<p>of ∆n(xδ) varies little in the following sense. Let |y|&lt; v. Then v&minus; y &gt; 0 and
</p>
<p>∆n
(
xδ + ε(v &minus; y)
</p>
<p>)
= Fn
</p>
<p>(
xδ + ε(v&minus; y)
</p>
<p>)
&minus;Φ
</p>
<p>(
xδ + ε(v&minus; y)
</p>
<p>)
</p>
<p>&ge; Fn(xδ)&minus;Φ(xδ)&minus;
[
Φ
(
xδ + ε(v&minus; y)
</p>
<p>)
&minus;Φ(xδ)
</p>
<p>]
.
</p>
<p>Here the difference in the brackets does not exceed ε(v&minus; y)Φ &prime;(0)&le; 2vε/
&radic;
</p>
<p>2π , and
</p>
<p>hence
</p>
<p>∆n
(
xδ + ε(v &minus; y)
</p>
<p>)
&ge;∆n &minus; δ&minus;
</p>
<p>2vε&radic;
2π
</p>
<p>.
</p>
<p>Therefore
</p>
<p>∆n,ε &ge;
&int;
</p>
<p>dG(y)∆n(xδ + εv&minus; εy)=
&int;
</p>
<p>|y|&lt;v
+
&int;
</p>
<p>|y|&ge;v
</p>
<p>&ge; 3
4
</p>
<p>(
∆n &minus; δ &minus;
</p>
<p>2vε&radic;
2π
</p>
<p>)
&minus; 1
</p>
<p>4
∆n =
</p>
<p>∆n
</p>
<p>2
&minus; 3
</p>
<p>4
</p>
<p>(
δ + 2vε&radic;
</p>
<p>2π
</p>
<p>)
.
</p>
<p>Since δ is arbitrary, the assertion of the lemma follows. �
</p>
<p>Corollary A5.1 For G=Φ(γ &sub;=�0,1) the value v = 6/5 satisfies the condition of
Lemma A5.1, and
</p>
<p>∆n &le; 2(∆n,ε + ε). (A5.1)
</p>
<p>At the next stage of the proof we bound ∆n,ε , and it is at that stage where the
</p>
<p>composition method will be used. Put
</p>
<p>u(n) := max
k&le;n
</p>
<p>∆k
</p>
<p>&radic;
k
</p>
<p>&micro;
, α2 := ε2n.</p>
<p/>
</div>
<div class="page"><p/>
<p>5 The Proof of the Berry&ndash;Esseen Theorem 661
</p>
<p>By letters c (with or without indices) we will denote absolute constants, not neces-
</p>
<p>sarily the same ones.
</p>
<p>Lemma A5.2 For α &ge; 1,
</p>
<p>∆n,ε &le; c&micro;
(
</p>
<p>1&radic;
n
+ &micro;u(n&minus; 1)
</p>
<p>α
&radic;
n
</p>
<p>)
. (A5.2)
</p>
<p>Proof Set Hn :=
&sum;n
</p>
<p>k=1 ηk , where ηk &sub;=�0,1 are independent of each other and of
Hn and γ . The composition method is based on the following identity (cf. Theo-
</p>
<p>rem 8.5.1 and identity (8.5.3), η &isin;�0,1):
</p>
<p>P(ζn + εγ &lt; x)&minus; P(η+ εγ &lt; x)= P(Sn + αγ &lt; x
&radic;
n )&minus; P(Hn + αγ &lt; x
</p>
<p>&radic;
n )
</p>
<p>=
n&sum;
</p>
<p>m=1
</p>
<p>[
P
(
Sm&minus;1 + (Hn &minus;Hm)+ ξm + αγ &lt; x
</p>
<p>&radic;
n
)
</p>
<p>&minus; P
(
Sm&minus;1 + (Hn &minus;Hm)+ ηm + αγ &lt; x
</p>
<p>&radic;
n
)]
.
</p>
<p>Since for γ &sub;=�0,1 one has Hn &minus;Hm + αγ &sub;=�0,n&minus;m+α2 , the last sum is equal to&sum;n
m=1 Dm, where
</p>
<p>Dm := E
[
Φ
</p>
<p>(
x
&radic;
n&minus; Sm&minus;1 &minus; ξm
</p>
<p>dm
</p>
<p>)
&minus;Φ
</p>
<p>(
x
&radic;
n&minus; Sm&minus;1 &minus; ηm
</p>
<p>dm
</p>
<p>)]
</p>
<p>= E
[
Φ
</p>
<p>(
Tm &minus;
</p>
<p>ξm
</p>
<p>dm
</p>
<p>)
&minus;Φ
</p>
<p>(
Tm &minus;
</p>
<p>ηm
</p>
<p>dm
</p>
<p>)]
,
</p>
<p>d2m := n&minus;m+ α2, Tm :=
x
&radic;
n&minus; Sm&minus;1
dm
</p>
<p>.
</p>
<p>To bound Dm we will adopt the same approach as in Lemma 8.5.1. Because the
</p>
<p>first two moments of ξm and ηm coincide, expanding Φ into a series yields
</p>
<p>|Dm| &le;
2&micro;
</p>
<p>d3m
sup
t
Eφ&prime;&prime;(Tm + t),
</p>
<p>where φ(x)=Φ &prime;(x) and φ&prime;&prime; =Φ &prime;&prime;&prime;. Since the function φ&prime;&prime; is bounded,
</p>
<p>|Dm| &le;
c&micro;
</p>
<p>d3m
. (A5.3)
</p>
<p>We will also need another bound for Dm. To obtain it, consider the quantity
</p>
<p>Rm := sup
t
</p>
<p>∣∣Eφ&prime;&prime;(Tm + t)
∣∣
</p>
<p>&le; sup
t
</p>
<p>∣∣E
[
φ&prime;&prime;(Tm + t)&minus; φ&prime;&prime;(Vm + t)
</p>
<p>]∣∣+ sup
t
</p>
<p>∣∣Eφ&prime;&prime;(Vm + t)
∣∣, (A5.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>662 5 The Proof of the Berry&ndash;Esseen Theorem
</p>
<p>where Vm is defined in the same way as Tm but with Sm&minus;1 replaced by Hm&minus;1.
Integrating by parts yields
</p>
<p>∣∣E
[
φ&prime;&prime;(Tm + t)&minus; φ&prime;&prime;(Vm + t)
</p>
<p>]∣∣ =
∣∣∣∣
&int;
</p>
<p>φ&prime;&prime;(u+ t) d
[
P(Tm &lt; u)&minus; P(Vm &lt; u)
</p>
<p>]∣∣∣∣
</p>
<p>=
∣∣∣∣
&int;
</p>
<p>φ&prime;&prime;&prime;(u+ t)
[
P(Tm &lt; u)&minus; P(Vm &lt; u)
</p>
<p>]
du
</p>
<p>∣∣∣∣
</p>
<p>&le; ∆m&minus;1
&int; ∣∣φ&prime;&prime;&prime;(u)
</p>
<p>∣∣du= c∆m&minus;1,
</p>
<p>since |P(Tm &lt; u) &minus; P(Vm &lt; u)| &le; ∆m&minus;1 (the variables Tm and Vm are obtained
from Sm&minus;1/
</p>
<p>&radic;
m&minus; 1 and Hm&minus;1/
</p>
<p>&radic;
m&minus; 1, respectively, by one and the same linear
</p>
<p>transformation).
</p>
<p>To bound the second summand on the right-hand side of (A5.4), note that
</p>
<p>Eφ&prime;&prime;(Vm + t)=
&int;
</p>
<p>φ&prime;&prime;(u+ t) 1
rm
</p>
<p>φ
</p>
<p>(
u&minus; am
rm
</p>
<p>)
du, (A5.5)
</p>
<p>where
</p>
<p>am = x
&radic;
n
</p>
<p>dm
, rm =
</p>
<p>&radic;
m&minus; 1
</p>
<p>n&minus;m+ α2 ,
</p>
<p>so that 1
rm
φ(
</p>
<p>u&minus; am
rm
</p>
<p>) is the density of Vm = (x
&radic;
n&minus;Hm&minus;1)
dm
</p>
<p>. Integrating the right-hand
</p>
<p>side of (A5.5) twice by parts, we obtain
</p>
<p>∣∣Eφ&prime;&prime;(Vm + t)
∣∣= 1
</p>
<p>r3m
</p>
<p>∣∣∣∣
&int;
</p>
<p>φ(u+ t)φ&prime;&prime;
(
u&minus; am
rm
</p>
<p>)
du
</p>
<p>∣∣∣∣&le;
c
</p>
<p>r3m
.
</p>
<p>Thus,
</p>
<p>Rm &le; c
(
∆m&minus;1 +
</p>
<p>1
</p>
<p>r3m
</p>
<p>)
, Dm &le; c&micro;
</p>
<p>(
∆m&minus;1
d3m
</p>
<p>+ 1
(m&minus; 1)3/2
</p>
<p>)
.
</p>
<p>The bounds derived for Dm do not depend on x. Therefore, using the bound just
</p>
<p>obtained for m&gt; n/2, and bound (A5.3) for m&le; n/2 (the latter bound implies then
that |Dm| &le; c&micro;/n3/2), we get
</p>
<p>∆n,ε &le; c&micro;
[ &sum;
</p>
<p>m&le;n/2
n&minus;3/2 +
</p>
<p>n&sum;
</p>
<p>m&gt;n/2
</p>
<p>∆m&minus;1
d3m
</p>
<p>+
n&sum;
</p>
<p>m&gt;n/2
</p>
<p>1
</p>
<p>(m&minus; 1)3/2
</p>
<p>]
. (A5.6)
</p>
<p>Here the first sum does not exceed (n/2)n&minus;3/2 = 1/(2&radic;n ) and the last sum does
not exceed
</p>
<p>&int; n
</p>
<p>n/2&minus;1
</p>
<p>ds
</p>
<p>s3/2
&le; c&radic;
</p>
<p>n
.</p>
<p/>
</div>
<div class="page"><p/>
<p>5 The Proof of the Berry&ndash;Esseen Theorem 663
</p>
<p>It remains to bound the middle sum. Setting (u(n) := maxk&le;n (∆k
&radic;
k )/&micro;), we have
</p>
<p>n&sum;
</p>
<p>m&gt;n/2
</p>
<p>∆m&minus;1
d3m
</p>
<p>&le; &micro;u(n&minus; 1)
&radic;
</p>
<p>2
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>m&gt;n/2
</p>
<p>1
</p>
<p>(n&minus;m+ α2)3/2 .
</p>
<p>The last sum does not exceed
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>(k + α2)3/2 &le;
1
</p>
<p>α3
+
&int; &infin;
</p>
<p>0
</p>
<p>dt
</p>
<p>(t + α2)3/2 =
1
</p>
<p>α3
+ 1
</p>
<p>2α
&le; 3
</p>
<p>2α
,
</p>
<p>provided that α &ge; 1. Collecting (A5.6) and the above estimates together, we obtain
the assertion of the lemma. �
</p>
<p>We now turn directly to the proof of the theorem. By virtue of (A5.1) and (A5.2),
</p>
<p>v(n) := ∆n
&radic;
n
</p>
<p>&micro;
&le; 2
</p>
<p>&micro;
</p>
<p>&radic;
n∆n,ε +
</p>
<p>2α
</p>
<p>&micro;
&le; 2c+ 2c&micro;u(n&minus; 1)
</p>
<p>α
+ 2α
</p>
<p>&micro;
.
</p>
<p>Put here α := max(4 c&micro;,1). Then (&micro;&ge; 1)
</p>
<p>v(n)&le; c1 +
u(n+ 1)
</p>
<p>2
.
</p>
<p>This implies that u(n) &le; 2c1 for all n. To verify this, we make use of induction.
Clearly, u(1) = v(1) &le; 1 &le; 2c1. Let u(n&minus; 1) &le; 2c1. Then v(n) &le; 2c1 and u(n) =
max(v(n),u(n&minus; 1))&le; 2c1. The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 6
</p>
<p>The Basic Properties of Regularly Varying
Functions and Subexponential Distributions
</p>
<p>The properties of regularly varying functions and subexponential distributions were
</p>
<p>used in Sects. 8.8, 9.4&ndash;9.6 and 12.7 and will be used in Appendices 7 and 8.
</p>
<p>6.1 General Properties of Regularly Varying Functions
</p>
<p>Definition A6.1.1 A positive measurable function L(t) is called a slowly varying
function (s.v.f.) as t &rarr;&infin; if, for any fixed v &gt; 0,
</p>
<p>L(vt)
</p>
<p>L(t)
&rarr; 1 as t &rarr;&infin;. (A6.1.1)
</p>
<p>A function V (t) is called a regularly varying function (r.v.f.) (with exponent
&minus;β &isin;R) as t &rarr;&infin; if it can be represented as
</p>
<p>V (t)= t&minus;βL(t), (A6.1.2)
</p>
<p>where L(t) is an s.v.f. as t &rarr;&infin;. We will denote the class of all r.v.f.s by R.
</p>
<p>The definitions of an s.v.f. and r.v.f. as t &darr; 0 are quite similar. In what follows,
the term s.v.f. (r.v.f.) will (unless specified otherwise) always refer to a slowly (reg-
</p>
<p>ularly) varying function at infinity.
</p>
<p>It is easy to see that, similarly to (A6.1.1), a characteristic property of regularly
</p>
<p>varying functions is the convergence, for any fixed v &gt; 0,
</p>
<p>V (vt)
</p>
<p>V (t)
&rarr; v&minus;β as t &rarr;&infin;. (A6.1.3)
</p>
<p>Thus, an s.v.f. is an r.v.f. with exponent zero.
</p>
<p>Typical representatives of the class of s.v.f.s are the logarithmic function and its
</p>
<p>powers lnγ t , γ &isin; R, their linear combinations, multiple logarithms, functions with
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>665</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>666 6 Regularly Varying Functions
</p>
<p>the property L(t) &rarr; L = const 
= 0 as t &rarr; &infin;, etc. As an example of a bounded
oscillating s.v.f. one can give
</p>
<p>L0(t)= 2 + sin(ln ln t), t &gt; 1.
</p>
<p>We will need the following two basic properties of s.v.f.s.
</p>
<p>Theorem A6.1.1 (Uniform convergence theorem) If L(t) is an s.v.f. as t &rarr; &infin;
then convergence (A6.1.1) holds uniformly in v on any segment [v1, v2], 0 &lt; v1 &lt;
v2 &lt;&infin;.
</p>
<p>The theorem implies that the uniform convergence (A6.1.1) on the segment
</p>
<p>[1/M,M] also takes place in the case when, as t &rarr; &infin;, the quantity M = M(t)
grows unboundedly slowly enough.
</p>
<p>Theorem A6.1.2 (Integral representation) A function L(t) is an s.v.f. as t &rarr;&infin; if
and only if, for some t0 &gt; 0, one has
</p>
<p>L(t)= c(t) exp
{&int; t
</p>
<p>t0
</p>
<p>ε(u)
</p>
<p>u
du
</p>
<p>}
, t &ge; t0, (A6.1.4)
</p>
<p>where the functions c(t) and ε(t) are measurable and such that c(t)&rarr; c &isin; (0,&infin;)
and ε(t)&rarr; 0 as t &rarr;&infin;.
</p>
<p>For instance, for L(t)= ln t representation (A6.1.4) is valid with c(t)= 1, t0 = e
and ε(t)= (ln t)&minus;1.
</p>
<p>Proof of Theorem A6.1.1 Put
</p>
<p>h(x) := lnL
(
ex
)
. (A6.1.5)
</p>
<p>Then property (A6.1.1) of s.v.f.s is equivalent, for each u &isin;R, to the condition that
the convergence
</p>
<p>h(x + u)&minus; h(x)&rarr; 0 (A6.1.6)
</p>
<p>takes place as x &rarr;&infin;. To prove the theorem, we need to show that this convergence
is uniform in u &isin; [u1, u2] for any fixed ui &isin; R. In order to do that, it suffices to
verify that convergence (A6.1.6) is uniform on the segment [0,1]. Indeed, from the
obvious inequality
∣∣h(x + u1 + u2)&minus; h(x)
</p>
<p>∣∣&le;
∣∣h(x + u1 + u2)&minus; h(x + u1)
</p>
<p>∣∣+
∣∣h(x + u1)&minus; h(x)
</p>
<p>∣∣
</p>
<p>(A6.1.7)
</p>
<p>we have
∣∣h(x + u)&minus; h(x)
</p>
<p>∣∣&le; (u2 &minus; u1 + 1) sup
y&isin;[0,1]
</p>
<p>∣∣h(x + y)&minus; h(x)
∣∣, u &isin; [u1, u2].</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 General Properties of Regularly Varying Functions 667
</p>
<p>For a given ε &isin; (0,1) and an x &gt; 0, set Ix := [x, x + 2],
</p>
<p>I &lowast;x :=
{
u &isin; Ix :
</p>
<p>∣∣h(u)&minus;h(x)
∣∣&ge; ε/2
</p>
<p>}
, I &lowast;0,x :=
</p>
<p>{
u &isin; I0 :
</p>
<p>∣∣h(x+ u)&minus;h(x)
∣∣&ge; ε/2
</p>
<p>}
.
</p>
<p>Clearly, the sets I &lowast;x and I
&lowast;
0,x are measurable and differ from each other by a transla-
</p>
<p>tion by x, so that μ(I &lowast;x )= μ(I &lowast;0,x), where μ is the Lebesgue measure. By (A6.1.6)
the indicator function of the set I &lowast;0,x converges, at each point u &isin; I0, to 0 as x &rarr;&infin;.
Therefore, by the dominated convergence theorem, the integral of this function, be-
</p>
<p>ing equal to μ(I &lowast;0,x), converges to 0, so that μ(I
&lowast;
x ) &lt; ε/2 for x &ge; x0, where x0 is
</p>
<p>large enough.
</p>
<p>Further, for s &isin; [0,1], the segment Ix &cap;Ix+s = [x+s, x+2] has length 2&minus;s &ge; 1,
so that, for x &ge; x0, the set
</p>
<p>(Ix &cap; Ix+s) \
(
I &lowast;x &cup; I &lowast;x+s
</p>
<p>)
</p>
<p>has measure &ge; 1 &minus; ε &gt; 0 and hence is non-empty. Let y be a point from this set.
Then
</p>
<p>∣∣h(x + s)&minus; h(x)
∣∣&le;
</p>
<p>∣∣h(x + s)&minus; h(y)
∣∣+
</p>
<p>∣∣h(y)&minus; h(x)
∣∣&lt; ε/2 + ε/2 = ε
</p>
<p>for x &ge; x0, which proves the required uniformity on [0,1] and hence on any fixed
segment. The theorem is proved. �
</p>
<p>Proof of Theorem A6.1.2 The fact that the right-hand side of (A6.1.4) is an s.v.f. is
almost obvious: for any fixed v 
= 1,
</p>
<p>L(vt)
</p>
<p>L(t)
= c(vt)
</p>
<p>c(t)
exp
</p>
<p>{&int; vt
</p>
<p>t
</p>
<p>ε(u)
</p>
<p>u
du
</p>
<p>}
, (A6.1.8)
</p>
<p>where c(vt)/c(t)&rarr; c/c= 1 and, as t &rarr;&infin;,
&int; vt
</p>
<p>t
</p>
<p>ε(u)
</p>
<p>u
du= o
</p>
<p>(&int; vt
</p>
<p>t
</p>
<p>du
</p>
<p>u
</p>
<p>)
= o(lnv)= o(1). (A6.1.9)
</p>
<p>We now prove that any s.v.f. admits the representation (A6.1.4). The required rep-
</p>
<p>resentation in terms of the function (A6.1.5) is equivalent (after substituting t = ex )
to the relation
</p>
<p>h(x)= d(x)+
&int; x
</p>
<p>x0
</p>
<p>δ(y) dy, (A6.1.10)
</p>
<p>where d(x) = ln c(ex) &rarr; d &isin; R and δ(x) = ε(ex) &rarr; 0 as x &rarr; &infin;, x0 = ln t0.
Therefore it suffices to establish representation (A6.1.10) for the function h(x).
</p>
<p>First of all note that h(x) (as well as L(t)) is a &ldquo;locally bounded&rdquo; function. In-
</p>
<p>deed, Theorem A6.1.1 implies that, for x0 large enough and all x &ge; x0,
</p>
<p>sup
0&le;y&le;1
</p>
<p>∣∣h(x + y)&minus; h(x)
∣∣&lt; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>668 6 Regularly Varying Functions
</p>
<p>Hence, for any x &gt; x0, we have by virtue of (A6.1.7) the bound
</p>
<p>∣∣h(x)&minus; h(x0)
∣∣&le; x &minus; x0 + 1.
</p>
<p>Further, the local boundedness and measurability of the function h mean that it is
</p>
<p>locally integrable on [x0,&infin;) and hence can be represented for x &ge; x0 as
</p>
<p>h(x)=
&int; x0+1
</p>
<p>x0
</p>
<p>h(y)dy +
&int; 1
</p>
<p>0
</p>
<p>(
h(x)&minus; h(x + y)
</p>
<p>)
dy +
</p>
<p>&int; x
</p>
<p>x0
</p>
<p>(
h(y + 1)&minus; h(y)
</p>
<p>)
dy.
</p>
<p>(A6.1.11)
</p>
<p>The first integral in (A6.1.11) is a constant, which will be denoted by d . The second
</p>
<p>integral, by virtue of Theorem A6.1.1, converges to zero as x &rarr;&infin;, so that
</p>
<p>d(x) := d +
&int; 1
</p>
<p>0
</p>
<p>(
h(x)&minus; h(x + y)
</p>
<p>)
dy &rarr; d, x &rarr;&infin;.
</p>
<p>As for the third integral in (A6.1.11), by the definition of an s.v.f., the integrand
</p>
<p>satisfies
</p>
<p>δ(y) := h(y + 1)&minus; h(y)&rarr; 0
</p>
<p>as y &rarr;&infin;, which completes the proof of representation (A6.1.10). �
</p>
<p>6.2 The Basic Asymptotic Properties
</p>
<p>In this section we will obtain a number of consequences of Theorems A6.1.1 and
</p>
<p>A6.1.2 that are related to the asymptotic behaviour of s.v.f.s and r.v.f.s.
</p>
<p>Theorem A6.2.1 (i) If L1 and L2 are s.v.f.s then L1 + L2, L1L2, Lb1 and L(t) :=
L1(at + b), where a &ge; 0 and b &isin;R, are also s.v.f.s
</p>
<p>(ii) If L is an s.v.f. then, for any δ &gt; 0, there exists a tδ &gt; 0 such that
</p>
<p>t&minus;δ &le; L(t)&le; tδ for all t &ge; tδ. (A6.2.1)
</p>
<p>In other words, L(t)= to(1) as t &rarr;&infin;.
(iii) If L is an s.v.f. then, for any δ &gt; 0 and v0 &gt; 1, there exists a tδ &gt; 0 such that,
</p>
<p>for all v &ge; v0 and t &ge; tδ ,
</p>
<p>v&minus;δ &le; L(vt)
L(t)
</p>
<p>&le; vδ. (A6.2.2)
</p>
<p>(iv) (Karamata&rsquo;s theorem) If an r.v.f. V in (A6.1.2) has exponent&minus;β , β &gt; 1, then
</p>
<p>V I (t) :=
&int; &infin;
</p>
<p>t
</p>
<p>V (u)du&sim; tV (t)
β &minus; 1 as t &rarr;&infin;. (A6.2.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 The Basic Asymptotic Properties 669
</p>
<p>If β &lt; 1 then
</p>
<p>VI (t) :=
&int; t
</p>
<p>0
</p>
<p>V (u)du&sim; tV (t)
1 &minus; β as t &rarr;&infin;. (A6.2.4)
</p>
<p>If β = 1 then
VI (t)= tV (t)L1(t) (A6.2.5)
</p>
<p>and
</p>
<p>V I (t)= tV (t)L2(t) if
&int; &infin;
</p>
<p>0
</p>
<p>V (u)du &lt;&infin;, (A6.2.6)
</p>
<p>where Li(t)&rarr;&infin; as t &rarr;&infin;, i = 1,2, are s.v.f.s.
(v) For an r.v.f. V with exponent &minus;β &lt; 0, put
</p>
<p>b(t) := V (&minus;1)(1/t)= inf
{
u : V (u) &lt; 1/t
</p>
<p>}
.
</p>
<p>Then b(t) is an r.v.f. with exponent 1/β:
</p>
<p>b(t)= t1/βLb(t), (A6.2.7)
</p>
<p>where Lb is an s.v.f. If the function L possesses the property
</p>
<p>L
(
tL1/β(t)
</p>
<p>)
&sim; L(t) (A6.2.8)
</p>
<p>as t &rarr;&infin; then
Lb(t)&sim; L1/β
</p>
<p>(
t1/β
</p>
<p>)
. (A6.2.9)
</p>
<p>Similar assertions hold for functions slowly/regularly varying as t &darr; 0.
Note that Theorem A6.1.1 and inequality (A6.2.2) imply the following property
</p>
<p>of s.v.f.s: for any δ &gt; 0 there exists a tδ &gt; 0 such that, for all t and v satisfying the
inequalities t &ge; tδ and vt &ge; tδ , we have
</p>
<p>(1 &minus; δ)min
{
vδ, v&minus;δ
</p>
<p>}
&le; L(vt)
</p>
<p>L(t)
&le; (1 + δ)max
</p>
<p>{
vδ, v&minus;δ
</p>
<p>}
. (A6.2.10)
</p>
<p>Proof of Theorem A6.2.1 Assertion (i) is evident (just note that, in order to prove
the last part of (i), one needs Theorem A6.1.1).
</p>
<p>(ii) This property follows immediately from representation (A6.1.4) and the
</p>
<p>bound
∣∣∣∣
&int; t
</p>
<p>t0
</p>
<p>ε(u)
</p>
<p>u
du
</p>
<p>∣∣∣∣=
∣∣∣∣
&int; ln t
</p>
<p>t0
</p>
<p>+
&int; t
</p>
<p>ln t
</p>
<p>∣∣∣∣=O
(&int; ln t
</p>
<p>t0
</p>
<p>du
</p>
<p>u
</p>
<p>)
+ o
</p>
<p>(&int; t
</p>
<p>ln t
</p>
<p>du
</p>
<p>u
</p>
<p>)
= o(ln t)
</p>
<p>as t &rarr;&infin;.
(iii) In order to prove this property, notice that on the right-hand side of (A6.1.8),
</p>
<p>for any fixed δ &gt; 0 and v0 &gt; 1 and all t large enough, we have
</p>
<p>v&minus;δ/2 &le; v&minus;δ/20 &le;
c(vt)
</p>
<p>c(t)
&le; vδ/20 &le; v
</p>
<p>δ/2, v &ge; v0,</p>
<p/>
</div>
<div class="page"><p/>
<p>670 6 Regularly Varying Functions
</p>
<p>and ∣∣∣∣
&int; vt
</p>
<p>t
</p>
<p>ε(u)
</p>
<p>u
du
</p>
<p>∣∣∣∣&le;
δ
</p>
<p>2
lnv
</p>
<p>(by virtue of (A6.1.9)). This implies (A6.2.2).
</p>
<p>(iv) By the dominated convergence theorem, we can choose an M =M(t)&rarr;&infin;
as t &rarr; &infin; such that the convergence in (A6.1.1) will be uniform in v &isin; [1,M].
Changing the variable u= vt , we obtain
</p>
<p>V I (t)= t&minus;β+1L(t)
&int; &infin;
</p>
<p>1
</p>
<p>v&minus;β
L(vt)
</p>
<p>L(t)
dv = t&minus;β+1L(t)
</p>
<p>[&int; M
</p>
<p>1
</p>
<p>+
&int; &infin;
</p>
<p>M
</p>
<p>]
. (A6.2.11)
</p>
<p>If β &gt; 1 then, as t &rarr;&infin;,
&int; M
</p>
<p>1
</p>
<p>&sim;
&int; M
</p>
<p>1
</p>
<p>v&minus;β dv&rarr; 1
β &minus; 1 ,
</p>
<p>whereas by property (iii), for δ = (β &minus; 1)/2, we have
&int; &infin;
</p>
<p>M
</p>
<p>&lt;
</p>
<p>&int; &infin;
</p>
<p>M
</p>
<p>v&minus;β+δ dv =
&int; &infin;
</p>
<p>M
</p>
<p>v&minus;(β+1)/2 dv&rarr; 0.
</p>
<p>These relations together imply
</p>
<p>V I (t)&sim; t
&minus;β+1
</p>
<p>β &minus; 1 L(t)=
tV (t)
</p>
<p>β &minus; 1 .
</p>
<p>The case β &lt; 1 can be treated quite similarly, but taking into account the uniform
</p>
<p>in v &isin; [1/M,1] convergence in (A6.1.1) and the equality
&int; 1
</p>
<p>0
</p>
<p>v&minus;βdv = 1
1 &minus; β .
</p>
<p>If β = 1 then the first integral on the right-hand side of (A6.2.11) is
&int; M
</p>
<p>1
</p>
<p>&sim;
&int; M
</p>
<p>1
</p>
<p>v&minus;1 dv = lnM,
</p>
<p>so that if
&int; &infin;
</p>
<p>0
</p>
<p>V (u)du &lt;&infin; (A6.2.12)
</p>
<p>then
</p>
<p>V I (t)&ge;
(
1 + o(1)
</p>
<p>)
L(t) lnM ≫ L(t) (A6.2.13)
</p>
<p>and hence
</p>
<p>L2(t) :=
V I (t)
</p>
<p>tV (t)
= V
</p>
<p>I (t)
</p>
<p>L(t)
&rarr;&infin; as t &rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 The Basic Asymptotic Properties 671
</p>
<p>Note now that, by property (i), the function L2 will be an s.v.f. whenever V
I (t) is
</p>
<p>an s.v.f. But, for v &gt; 1,
</p>
<p>V I (t)= V I (vt)+
&int; vt
</p>
<p>t
</p>
<p>V (u)du,
</p>
<p>where the last integral clearly does not exceed (v&minus; 1)L(t)(1+ o(1)). By (A6.2.13)
this implies that V I (vt)/V I (t) &rarr; 1 as t &rarr; &infin;, which completes the proof
of (A6.2.6).
</p>
<p>That relation (A6.2.5) is true in the subcase when (A6.2.12) holds is almost ob-
</p>
<p>vious, since
</p>
<p>VI (t)= tV (t)L1(t)= L(t)L1(t)=
&int; t
</p>
<p>0
</p>
<p>V (u)du&rarr;
&int; &infin;
</p>
<p>0
</p>
<p>V (u)du,
</p>
<p>so that, firstly, L1 is an s.v.f. by property (i) and, secondly, L1(t) &rarr; &infin; because
L(t)&rarr; 0 by (A6.2.13).
</p>
<p>Now let β = 1 and
&int;&infin;
</p>
<p>0 V (u)du=&infin;. Then, as M =M(t)&rarr;&infin; slowly enough,
similarly to (A6.2.11) and (A6.2.13), by the uniform convergence theorem we have
</p>
<p>VI (t)=
&int; 1
</p>
<p>0
</p>
<p>v&minus;1L(vt) dv &ge;
&int; 1
</p>
<p>1/M
</p>
<p>v&minus;1L(vt) dv &sim; L(t) lnM ≫ L(t).
</p>
<p>Therefore L1(t) := VI (t)/L(t) &rarr; &infin; as t &rarr; &infin;. Further, also similarly to the
above, we have, as v &isin; (0,1),
</p>
<p>VI (t)= VI (vt)+
&int; t
</p>
<p>vt
</p>
<p>V (u)du,
</p>
<p>where the last integral does not exceed (1&minus;v)L(t)(1+o(1))≪ VI (t), so that VI (t)
(as well as L1(t) by virtue of property (i)) is an s.v.f. This completes the proof of
</p>
<p>property (iv).
</p>
<p>(v) Clearly, by the uniform convergence theorem the quantity b= b(t) is a solu-
tion to the &ldquo;asymptotic equation&rdquo;
</p>
<p>V (b)&sim; 1
t
</p>
<p>as t &rarr;&infin; (A6.2.14)
</p>
<p>(where the symbol &sim; can be replaced by the equality sign if the function V is
continuous and monotonically decreasing). Substituting t1/βLb(t) for b, we obtain
</p>
<p>an equivalent relation
</p>
<p>L
&minus;β
b L
</p>
<p>(
t1/βLb
</p>
<p>)
&sim; 1, (A6.2.15)
</p>
<p>where clearly
</p>
<p>t1/βLb &rarr;&infin; as t &rarr;&infin;. (A6.2.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>672 6 Regularly Varying Functions
</p>
<p>Fix an arbitrary v &gt; 0. Substituting vt for t in (A6.2.15) and setting, for brevity&rsquo;s
</p>
<p>sake, L2 = L2(t) := Lb(vt), we get the relation
</p>
<p>L
&minus;β
2 L
</p>
<p>(
t1/βL2
</p>
<p>)
&sim; 1, (A6.2.17)
</p>
<p>since L(v1/β t1/βL2) &sim; L(t1/βL2) by virtue of (A6.2.16) (with Lb replaced
with L2). Now we will show by contradiction that (A6.2.15)&ndash;(A6.2.17) imply that
</p>
<p>Lb &sim; L2 as t &rarr;&infin;, which obviously means that Lb is an s.v.f.
Indeed, the contrary assumption means that there exist v0 &gt; 1 and a sequence
</p>
<p>tn &rarr;&infin; such that
</p>
<p>un := L2(tn)/Lb(tn) &gt; v0, n= 1,2, . . . (A6.2.18)
</p>
<p>(the possible alternative case can be dealt with in the same way). Clearly, t&lowast;n :=
t
1/β
n Lb(tn)&rarr;&infin; by (A6.2.16), so we obtain from (A6.2.15)&ndash;(A6.2.16) and property
</p>
<p>(iii) with δ = β/2 that
</p>
<p>1 &sim;
L
&minus;β
2 (tn)L(t
</p>
<p>1/β
n L2(tn))
</p>
<p>L
&minus;β
b (tn)L(t
</p>
<p>1/β
n Lb(tn))
</p>
<p>= u&minus;βn
L(unt
</p>
<p>&lowast;
n )
</p>
<p>L(t&lowast;n )
&le; u&minus;β/2n &lt; v&minus;β/20 &lt; 1.
</p>
<p>We get a contradiction.
</p>
<p>Note that the above argument proves the uniqueness (up to asymptotic equiva-
</p>
<p>lence) of the solution to Eq. (A6.2.14).
</p>
<p>Finally, relation (A6.2.9) can be proved by a direct verification of (A6.2.14) for
</p>
<p>b := t1/βL1/β(t1/β): using (A6.2.8), we have
</p>
<p>V (b)= b&minus;βL(b)= L(t
1/βL1/β(t1/β))
</p>
<p>tL(t1/β)
&sim; L(t
</p>
<p>1/β)
</p>
<p>tL(t1/β)
= 1
</p>
<p>t
.
</p>
<p>The required assertion follows now by the aforementioned uniqueness of the solu-
</p>
<p>tion to the asymptotic equation (A6.2.14). Theorem A6.2.1 is proved. �
</p>
<p>6.3 The Asymptotic Properties of the Transforms of R.V.F.s
</p>
<p>(Abel-Type Theorems)
</p>
<p>For an r.v.f. V (t), its Laplace transform
</p>
<p>ψ(λ) :=
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;λtV (t) dt &lt;&infin;
</p>
<p>is defined for all λ &gt; 0. The following asymptotic relations hold true for the trans-
</p>
<p>form.
</p>
<p>Theorem A6.3.1 Assume that V (t) &isin;R (i.e. V (t) has the form (A6.1.2)).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Asymptotic Properties of the Transforms of R.V.F.s 673
</p>
<p>(i) If β &isin; [0,1) then
</p>
<p>ψ(λ)&sim; Γ (1 &minus; β)
λ
</p>
<p>V (1/λ) as λ &darr; 0. (A6.3.1)
</p>
<p>(ii) If β = 1 and
&int;&infin;
</p>
<p>0 V (t) dt =&infin; then
</p>
<p>ψ(λ)&sim; VI (1/λ) as λ &darr; 0, (A6.3.2)
</p>
<p>where VI (t)=
&int; t
</p>
<p>0 V (u)du&rarr;&infin; is an s.v.f. such that VI (t)≫ L(t) as t &rarr;&infin;.
(iii) In any case, ψ(λ) &uarr; VI (&infin;)=
</p>
<p>&int;&infin;
0 V (t) dt &le;&infin; as λ &darr; 0.
</p>
<p>Assertions (i) and (ii) are called Abelian theorems.
If we resolve relation (A6.3.1) for V then we obtain
</p>
<p>V (t)&sim; ψ(1/t)
tΓ (1 &minus; β) as t &rarr;&infin;.
</p>
<p>Relations of this type will also be valid in the case when, instead of the regularity
</p>
<p>of the function V , one requires the monotonicity of V and assumes that ψ(λ) is an
</p>
<p>r.v.f. as λ &darr; 0. Statements of such type are called Tauberian theorems. We will not
need these theorems and so will not dwell on them.
</p>
<p>Proof of Theorem A6.3.1 (i) For any fixed ε &gt; 0 we have
</p>
<p>ψ(λ)=
&int; ε/λ
</p>
<p>0
</p>
<p>+
&int; &infin;
</p>
<p>ε/λ
</p>
<p>,
</p>
<p>where, for the first integral on the right-hand side, for β &lt; 1, by virtue of (A6.2.4)
</p>
<p>we have the following relation
</p>
<p>&int; ε/λ
</p>
<p>0
</p>
<p>e&minus;λtV (t) dt &le;
&int; ε/λ
</p>
<p>0
</p>
<p>V (t) dt &sim; εV (ε/λ)
λ(1 &minus; β) as λ &darr; 0. (A6.3.3)
</p>
<p>Changing the variable λt = u, we can rewrite the second integral in the above rep-
resentation for ψ(λ) as
</p>
<p>&int; &infin;
</p>
<p>ε/λ
</p>
<p>= V (1/λ)
λ
</p>
<p>&int; &infin;
</p>
<p>ε
</p>
<p>e&minus;uu&minus;β
L(u/λ)
</p>
<p>L(1/λ)
du= V (1/λ)
</p>
<p>λ
</p>
<p>[&int; 2
</p>
<p>ε
</p>
<p>+
&int; &infin;
</p>
<p>2
</p>
<p>]
. (A6.3.4)
</p>
<p>Each of the integrals on the right-hand side converges, as λ &darr; 0, to the corresponding
integral of e&minus;uu&minus;β : the former integral converges by the uniform convergence the-
orem (the convergence L(u/λ)/L(1/λ)&rarr; 1 is uniform in u &isin; [ε,2]), and the latter
converges by virtue of (A6.1.1) and the dominated convergence theorem, since by
</p>
<p>Theorem A6.2.1(iii), for all λ small enough, we have L(u/λ)/L(1/λ) &lt; u for u&ge; 2.
Therefore,
</p>
<p>&int; &infin;
</p>
<p>ε/λ
</p>
<p>&sim; V (1/λ)
λ
</p>
<p>&int; &infin;
</p>
<p>ε
</p>
<p>u&minus;βe&minus;u du. (A6.3.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>674 6 Regularly Varying Functions
</p>
<p>Now note that, as λ &darr; 0,
</p>
<p>εV (ε/λ)
</p>
<p>λ
</p>
<p>/
V (1/λ)
</p>
<p>λ
= ε1&minus;β L(ε/λ)
</p>
<p>L(1/λ)
&rarr; ε1&minus;β .
</p>
<p>Since ε &gt; 0 can be chosen arbitrarily small, this relation together with (A6.3.3) and
</p>
<p>(A6.3.5) completes the proof of (A6.3.1).
</p>
<p>(ii) Integrating by parts and changing the variable λt = u, we obtain, for β = 1
and M &gt; 0, that
</p>
<p>ψ(λ)=
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;λtdVI (t)=&minus;
&int; &infin;
</p>
<p>0
</p>
<p>VI (t) de
&minus;λt
</p>
<p>=
&int; &infin;
</p>
<p>0
</p>
<p>VI (u/λ)e
&minus;udu=
</p>
<p>&int; 1/M
</p>
<p>0
</p>
<p>+
&int; M
</p>
<p>1/M
</p>
<p>+
&int; &infin;
</p>
<p>M
</p>
<p>. (A6.3.6)
</p>
<p>By Theorem A6.2.1(iv), VI (t) ≫ L(t) is an s.v.f. as t &rarr; &infin;. Therefore, by the
uniform convergence theorem, for M =M(λ)&rarr;&infin; slowly enough as λ&rarr; 0, the
middle integral on the right-hand side of (A6.3.6) is
</p>
<p>VI (1/λ)
</p>
<p>&int; M
</p>
<p>1/M
</p>
<p>VI (u/λ)
</p>
<p>VI (1/λ)
e&minus;udu&sim; VI (1/λ)
</p>
<p>&int; M
</p>
<p>1/M
</p>
<p>e&minus;udu&sim; VI (1/λ).
</p>
<p>The remaining two integrals are negligibly small: since VI (t) is an increasing func-
</p>
<p>tion, the first integral does not exceed VI (1/λM)/M = o(VI (1/λ)), while for the
last integral we have by Theorem A6.2.1(iii) that
</p>
<p>VI (1/λ)
</p>
<p>&int; &infin;
</p>
<p>M
</p>
<p>VI (u/λ)
</p>
<p>VI (1/λ)
e&minus;udu&le; VI (1/λ)
</p>
<p>&int; &infin;
</p>
<p>M
</p>
<p>ue&minus;udu= o
(
VI (1/λ)
</p>
<p>)
.
</p>
<p>Hence (ii) is proved. Assertion (iii) is evident. �
</p>
<p>6.4 Subexponential Distributions and Their Properties
</p>
<p>Let ξ, ξ1, ξ2, . . . be independent identically distributed random variables with distri-
</p>
<p>bution F, and let the right tail of this distribution
</p>
<p>F+(t) := F
(
[t,&infin;)
</p>
<p>)
= P(ξ &ge; t), t &isin;R,
</p>
<p>be an r.v.f. as t &rarr;&infin; of the form (A6.1.2), which we will denote by V (t). Recall
that we denoted the class of all such distributions by R.
</p>
<p>In this section we will introduce one more class of distributions, which is sub-
</p>
<p>stantially wider than R.
</p>
<p>Let ζ &isin; R be a random variable with distribution G: G(B) = P(ζ &isin; B) for any
Borel set B (recall that in this case we write ζ &sub;=G). Denote by G(t) the right tail
of the distribution of the random variable ζ :
</p>
<p>G(t) := P(ζ &ge; t), t &isin;R.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Subexponential Distributions and Their Properties 675
</p>
<p>The convolution of tails G1(t) and G2(t) is the function
</p>
<p>G1 &lowast;G2(t) := &minus;
&int;
</p>
<p>G(t &minus; y)dG2(y)=
&int;
</p>
<p>G1(t &minus; y)G2(dy)= P(Z2 &ge; t),
</p>
<p>where Z2 = ζ1 + ζ2 is the sum of independent random variables ζi &sub;=Gi , i = 1,2.
Clearly, G1 &lowast;G2(t)=G2 &lowast;G1(t). Denote by G2&lowast;(t) :=G &lowast;G(t) the convolution
of the tail G(t) with itself and put G(n+1)&lowast;(t) :=G &lowast;Gn&lowast;(t), n&ge; 2.
</p>
<p>Definition A6.4.1 A distribution G on [0,&infin;) belongs to the class S+ of subexpo-
nential distributions on the positive half-line if
</p>
<p>G2&lowast;(t)&sim; 2G(t) as t &rarr;&infin;. (A6.4.1)
</p>
<p>A distribution G on the whole line (&minus;&infin;,&infin;) belongs to the class S of subexponen-
tial distributions if the distribution G+ of the positive part ζ+ = max{0, ζ } of the
random variable ζ &sub;=G belongs to S+. A random variable is called subexponential
if its distribution is subexponential.
</p>
<p>As we will see below (Theorem A6.4.3), the subexponentiality property of a dis-
</p>
<p>tribution G is essentially the property of the asymptotics of the tail G(t) as t &rarr;&infin;.
Therefore we can also speak about subexponential functions.
</p>
<p>A nondecreasing function G1(t) on (0,&infin;) is called subexponential if a distri-
bution G with the tail G(t) &sim; cG1(t) as t &rarr; &infin; with some c &gt; 0 is subexpo-
nential. (For example, distributions with the tails G(t) = G1(t)/G1(0) or G(t) =
min(1,G1(t))).
</p>
<p>Remark A6.4.1 Since we obviously always have
</p>
<p>(
G+
</p>
<p>)2&lowast;
(t)= P
</p>
<p>(
ζ+1 + ζ
</p>
<p>+
2 &ge; t
</p>
<p>)
&ge; P
</p>
<p>({
ζ+1 &ge; t
</p>
<p>}
&cup;
{
ζ+2 &ge; t
</p>
<p>})
</p>
<p>= P(ζ1 &ge; t)+ P(ζ2 &ge; t)&minus; P(ζ1 &ge; t, ζ2 &ge; t)
</p>
<p>= 2G(t)&minus;G2(t)= 2G+(t)
(
1 + o(1)
</p>
<p>)
</p>
<p>as t &rarr;&infin;, subexponentiality is equivalent to the following property:
</p>
<p>lim sup
t&rarr;&infin;
</p>
<p>(G+)2&lowast;(t)
</p>
<p>G+(t)
&le; 2. (A6.4.2)
</p>
<p>Note also that, since relation (A6.4.1) makes sense only when G(t) &gt; 0 for all t &isin;R,
the support of any subexponential distribution is unbounded from the right.
</p>
<p>We show that regularly varying distributions are subexponential, i.e., that R&sub; S.
Let F &isin;R and P(ξ &ge; t)= V (t) be r.v.f.s. We need to show that
</p>
<p>P(ξ1 + ξ2 &ge; x) = V 2&lowast;(x) := V &lowast; V (x)
</p>
<p>= &minus;
&int; &infin;
</p>
<p>&minus;&infin;
V (x &minus; t) dV (t)&sim; 2V (x). (A6.4.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>676 6 Regularly Varying Functions
</p>
<p>In order to do that, we introduce events A := {ξ1 + ξ2 &ge; x} and Bi := {ξi &lt; x/2},
i = 1,2. Clearly,
</p>
<p>P(A)= P(AB1)+ P(AB2)&minus; P(AB1B2)+ P(AB1B2),
</p>
<p>where P(AB1B2) = 0, P(AB1B2) = P(B1B2) = V 2(x/2) (here and in what fol-
lows, B denotes the event complementary to B) and
</p>
<p>P(AB1)= P(AB2)=
&int; x/2
</p>
<p>&minus;&infin;
V (x &minus; t)F(dt).
</p>
<p>Therefore
</p>
<p>V 2&lowast;(x)= 2
&int; x/2
</p>
<p>&minus;&infin;
V (x &minus; t)F(dt)+ V 2(x/2). (A6.4.4)
</p>
<p>(The same result can be obtained by integrating the convolution in (A6.4.3) by
</p>
<p>parts.) It remains to note that V 2(x/2)= o(V (x)) and
&int; x/2
</p>
<p>&minus;&infin;
V (x &minus; t)F(dt)=
</p>
<p>&int; &minus;M
</p>
<p>&minus;&infin;
+
&int; M
</p>
<p>&minus;M
+
&int; x/2
</p>
<p>M
</p>
<p>, (A6.4.5)
</p>
<p>where, as one can easily see, for any M =M(x)&rarr;&infin; as x &rarr;&infin; such that M =
o(x), we have
</p>
<p>&int; M
</p>
<p>&minus;M
&sim; V (x) and
</p>
<p>&int; &minus;M
</p>
<p>&minus;&infin;
+
&int; x/2
</p>
<p>M
</p>
<p>= o
(
V (x)
</p>
<p>)
,
</p>
<p>which proves (A6.4.3).
</p>
<p>The same argument is valid for distributions with a right tail of the form
</p>
<p>F+(t)= e&minus;t
βL(t), β &isin; (0,1), (A6.4.6)
</p>
<p>where L(t) is an s.v.f. as t &rarr; &infin; satisfying a certain smoothness condition (for
instance, that L is differentiable with L&prime;(t)= o(L(t)/t) as t &rarr;&infin;).
</p>
<p>One of the basic properties of subexponential distributions G is that their tails
</p>
<p>G(t) are asymptotically locally constant in the following sense.
</p>
<p>Definition A6.4.2 We will call a function G(t) &gt; 0 (asymptotically) locally con-
stant (l.c.) if, for any fixed v,
</p>
<p>G(t + v)
G(t)
</p>
<p>&rarr; 1 as t &rarr;&infin;. (A6.4.7)
</p>
<p>In the literature, distributions with l.c. tails are often referred to as long-tailed
</p>
<p>distributions; however, we feel that the term &ldquo;locally constant function&rdquo; better re-
</p>
<p>flects the meaning of the concept. Denote the class of all distributions G with l.c.
</p>
<p>tails G(t) by L.
</p>
<p>For future reference, we will state the basic properties of l.c. functions as a sepa-
</p>
<p>rate theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Subexponential Distributions and Their Properties 677
</p>
<p>Theorem A6.4.1 (i) For an l.c. function G(t) the convergence in (A6.4.7) is uni-
form in v on any fixed finite interval.
</p>
<p>(ii) A functionG(t) is l.c. if and only if, for some t0 &gt; 0, it admits a representation
of the form
</p>
<p>G(t)= c(t) exp
{&int; t
</p>
<p>t0
</p>
<p>ε(u)du
</p>
<p>}
, t &ge; t0, (A6.4.8)
</p>
<p>where the functions c(t) and ε(t) are measurable and such that c(t)&rarr; c &isin; (0,&infin;)
and ε(t)&rarr; 0 as t &rarr;&infin;.
</p>
<p>(iii) IfG1(t) andG2(t) are l.c. functions thenG1(t)+G2(t), G1(t)G2(t), Gb1(t),
and G(t) :=G1(at + b), where a &ge; 0 and b &isin;R, are also l.c.
</p>
<p>(iv) If G(t) is an l.c. function then, for any ε &gt; 0,
</p>
<p>eεtG(t)&rarr;&infin; as t &rarr;&infin;.
</p>
<p>In other words, any l.c. function G(t) can be represented as
</p>
<p>G(t)= e&minus;l(t), l(t)= o(t) as t &rarr;&infin;. (A6.4.9)
</p>
<p>(v) Let
</p>
<p>GI (t) :=
&int; &infin;
</p>
<p>t
</p>
<p>G(u)du &lt;&infin;
</p>
<p>and at least one of the following conditions be satisfied:
</p>
<p>(a) G(t) is an l.c. function; or
(b) GI (t) is an l.c. function and G(t) is monotone.
</p>
<p>Then
</p>
<p>G(t)= o
(
GI (t)
</p>
<p>)
as t &rarr;&infin;. (A6.4.10)
</p>
<p>(vi) If G &isin;L then G2&lowast;(t)&sim; (G+)2&lowast;(t) as t &rarr;&infin;.
</p>
<p>Remark A6.4.2 Assertion (i) of the theorem implies that the uniform convergence
in (A6.4.7) on the interval [&minus;M,M] persists in the case when, as t &rarr;&infin;, M =M(t)
grows unboundedly slowly enough.
</p>
<p>Proof of Theorem A6.4.1, (i)&ndash;(iii) It is clear from Definitions A6.4.1 and A6.4.2
that G(t) is l.c. if and only if L(t) :=G(ln t) is an s.v.f. Having made this observa-
tion, assertion (i) follows directly from Theorem A6.1.1 (on uniform convergence
</p>
<p>of s.v.f.s), while assertions (ii) and (iii) follow from Theorems A6.1.2 and A6.2.1(i),
</p>
<p>respectively.
</p>
<p>Assertion (iv) follows from the integral representation (A6.4.8).
</p>
<p>(v) If (a) holds then, for any M &gt; 0 and all t large enough,
</p>
<p>GI (t) &gt;
</p>
<p>&int; t+M
</p>
<p>t
</p>
<p>G(u)du &gt;
1
</p>
<p>2
MG(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>678 6 Regularly Varying Functions
</p>
<p>Since M is arbitrary, GI (t)≫G(t). Further, if (b) holds then
</p>
<p>G(t)
</p>
<p>GI (t)
&le; 1
</p>
<p>GI (t)
</p>
<p>&int; t
</p>
<p>t&minus;1
G(u)du= G
</p>
<p>I (t &minus; 1)
GI (t)
</p>
<p>&minus; 1 &rarr; 0
</p>
<p>as t &rarr;&infin;.
(vi) Let ζ1 and ζ2 be independent copies of a random variable ζ , Z2 := ζ1 + ζ2,
</p>
<p>Z
(+)
2 := ζ
</p>
<p>+
1 + ζ
</p>
<p>+
2 . Clearly, ζi &le; ζ
</p>
<p>+
i , so that
</p>
<p>G2&lowast;(t)= P(Z2 &ge; t)&le; P
(
Z
(+)
2 &ge; t
</p>
<p>)
=
(
G+
</p>
<p>)2&lowast;
(t). (A6.4.11)
</p>
<p>On the other hand, for any M &gt; 0,
</p>
<p>G2&lowast;(t)&ge; P(Z2 &ge; t, ζ1 &gt; 0, ζ2 &gt; 0)+
2&sum;
</p>
<p>i=1
P
(
Z2 &ge; t, ζi &isin; [&minus;M,0]
</p>
<p>)
,
</p>
<p>where the first term on the right-hand side is equal to P(Z
(+)
2 &ge; t, ζ
</p>
<p>+
1 &gt; 0, ζ
</p>
<p>+
2 &gt; 0),
</p>
<p>and the last two terms can be bounded as follows: since G &isin; L, then, for any ε &gt; 0
and M and t large enough,
</p>
<p>P
(
Z2 &ge; t, ζ1 &isin; [&minus;M,0]
</p>
<p>)
&ge; P
</p>
<p>(
ζ2 &ge; t +M, ζ1 &isin; [&minus;M,0]
</p>
<p>)
</p>
<p>=G(t)G(t +M)
G(t)
</p>
<p>[
P(ζ1 &le; 0)&minus; P(ζ1 &lt;&minus;M)
</p>
<p>]
</p>
<p>&ge; (1 &minus; ε)G(t)P
(
ζ+1 = 0
</p>
<p>)
= (1 &minus; ε)P
</p>
<p>(
Z
(+)
2 &ge; t, ζ
</p>
<p>+
1 = 0
</p>
<p>)
.
</p>
<p>Thus we obtain for G2&lowast;(t) the lower bound
</p>
<p>G2&lowast;(t)&ge; P
(
Z
(+)
2 &ge; t, ζ
</p>
<p>+
1 &gt; 0, ζ
</p>
<p>+
2 &gt; 0
</p>
<p>)
+ (1 &minus; ε)
</p>
<p>2&sum;
</p>
<p>i=1
P
(
Z
(+)
2 &ge; t, ζ
</p>
<p>+
i = 0
</p>
<p>)
</p>
<p>&ge; (1 &minus; ε)P
(
Z
(+)
2 &ge; t
</p>
<p>)
= (1 &minus; ε)
</p>
<p>(
G&lowast;
</p>
<p>)2&lowast;
(t).
</p>
<p>Therefore (vi) is proved, as ε can be arbitrarily small. The theorem is proved. �
</p>
<p>We return now to our discussion of subexponential distributions. First of all, we
</p>
<p>turn to the relationship between the classes S and L.
</p>
<p>Theorem A6.4.2 We have S&sub; L, and hence all the assertions of Theorem A6.4.1
are valid for subexponential distributions as well.
</p>
<p>Remark A6.4.3 The coinage of the term &ldquo;subexponential distribution&rdquo; was appar-
ently due mostly to the fact that the tail of such a distribution decreases as t &rarr;&infin;
slower than any exponential function e&minus;εt , as shown in Theorems A6.4.1(iv) and
A6.4.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Subexponential Distributions and Their Properties 679
</p>
<p>Remark A6.4.4 In the case when the distribution G is not concentrated on [0,&infin;),
the tails&rsquo; additivity condition (A6.4.1) alone is not sufficient for the function G(t)
</p>
<p>to be l.c. (and hence for ensuring the &ldquo;subexponential decay&rdquo; of the distribution tail,
</p>
<p>cf. Remark A6.4.3). This explains the necessity of defining subexponentiality in the
</p>
<p>general case in terms of condition (A6.4.1) on the distribution G+ of the random
variable ζ+. Actually, as we will see below (Corollary A6.4.1), the subexponential-
ity of a distribution G on R is equivalent to the combination of conditions (A6.4.1)
(on G itself) and G &isin;L.
</p>
<p>The next example shows that, for random variables assuming values of both
signs, condition (A6.4.1), generally speaking, does not imply the subexponential
behaviour of G(t).
</p>
<p>Example A6.4.1 Let &micro;&gt; 0 be fixed and the right tail of the distribution G have the
form
</p>
<p>G(t)= e&minus;&micro;tV (t), (A6.4.12)
where V (t) is an r.v.f. vanishing as t &rarr;&infin; and such that
</p>
<p>g(&micro;) :=
&int; &infin;
</p>
<p>&minus;&infin;
e&micro;yG(dy) &lt;&infin;.
</p>
<p>Similarly to (A6.4.4) and (A6.4.5), we have
</p>
<p>G2&lowast;(t)= 2
&int; t/2
</p>
<p>&minus;&infin;
G(t &minus; y)G(dy)+G2(t/2),
</p>
<p>where
</p>
<p>&int; t/2
</p>
<p>&minus;&infin;
G(t &minus; y)G(dy) = e&minus;&micro;t
</p>
<p>&int; t/2
</p>
<p>&minus;&infin;
e&micro;yV (t &minus; y)G(dy)
</p>
<p>= e&minus;&micro;t
[&int; &minus;M
</p>
<p>&minus;&infin;
+
&int; M
</p>
<p>&minus;M
+
&int; t/2
</p>
<p>M
</p>
<p>]
.
</p>
<p>One can easily see that, for M =M(t)&rarr;&infin; slowly enough as t &rarr;&infin;, we have
&int; M
</p>
<p>&minus;M
e&micro;yV (t &minus; y)G(dy)&sim; g(&micro;)V (t),
</p>
<p>&int; &minus;M
</p>
<p>&minus;&infin;
+
&int; t/2
</p>
<p>M
</p>
<p>= o
(
G(t)
</p>
<p>)
,
</p>
<p>while
</p>
<p>G2(t/2)= e&minus;&micro;tV 2(t/2)&le; ce&minus;&micro;tV 2(t)= o
(
G(t)
</p>
<p>)
.
</p>
<p>Thus, we obtain
</p>
<p>G2&lowast;(t)&sim; 2g(&micro;)e&minus;&micro;tV (t)= 2g(&micro;)G(t), (A6.4.13)
and it is clear that we can always find a distribution G (with a negative mean) such
</p>
<p>that g(&micro;)= 1. In that case relation (A6.4.1) from the definition of subexponentiality</p>
<p/>
</div>
<div class="page"><p/>
<p>680 6 Regularly Varying Functions
</p>
<p>will be satisfied, although G(t) decreases exponentially fast and hence is not an l.c.
</p>
<p>function.
</p>
<p>On the other hand, note that the class of distributions satisfying relation (A6.4.1)
</p>
<p>only is an extension of the class S. Distributions in the former class possess many
</p>
<p>of the properties of distributions from S.
</p>
<p>Proof of Theorem A6.4.2 We have to prove that S&sub;L. Since the definitions of both
classes are given in terms of the right distribution tails, we can assume without loss
</p>
<p>of generality, that G &isin; S+ (or just consider the distribution G+). For independent
(nonnegative) ζi &sub;=G we have, for t &gt; 0,
</p>
<p>G2&lowast;(t)= P(ζ1 + ζ2 &ge; t)= P(ζ1 &ge; t)+ P(ζ1 + ζ2 &ge; t, ζ1 &lt; t)
</p>
<p>=G(t)+
&int; t
</p>
<p>0
</p>
<p>G(t &minus; y)G(dy). (A6.4.14)
</p>
<p>Since G(t) is non-increasing and G(0)= 1, it follows that, for t &gt; v &gt; 0,
</p>
<p>G2&lowast;(t)
</p>
<p>G(t)
= 1 +
</p>
<p>&int; v
</p>
<p>0
</p>
<p>G(t &minus; y)
G(t)
</p>
<p>G(dy)+
&int; t
</p>
<p>v
</p>
<p>G(t &minus; y)
G(t)
</p>
<p>G(dy)
</p>
<p>&ge; 1 +
[
1 &minus;G(v)
</p>
<p>]
+ G(t &minus; v)
</p>
<p>G(t)
</p>
<p>[
G(v)&minus;G(t)
</p>
<p>]
.
</p>
<p>Therefore, for t large enough (such that G(v)&minus;G(t) &gt; 0),
</p>
<p>1 &le; G(t &minus; v)
G(t)
</p>
<p>&le; 1
G(v)&minus;G(t)
</p>
<p>[
G2&lowast;(t)
</p>
<p>G(t)
&minus; 2 +G(v)
</p>
<p>]
.
</p>
<p>Since G &isin; S+, the right-hand side of the last formula converges as t &rarr; &infin; to the
quantity G(v)/G(v)= 1 and hence G &isin;L. The theorem is proved. �
</p>
<p>The next theorem contains several important properties of subexponential distri-
</p>
<p>butions.
</p>
<p>Theorem A6.4.3 Let G &isin; S.
(i) If Gi(t)/G(t)&rarr; ci as t &rarr;&infin;, ci &ge; 0, i = 1,2, c1 + c2 &gt; 0, then
</p>
<p>G1 &lowast;G2(t)&sim;G1(t)+G2(t)&sim; (c1 + c2)G(t).
</p>
<p>(ii) If G0(t)&sim; cG(t) as t &rarr;&infin;, c &gt; 0, then G0 &isin; S.
(iii) For any fixed n&ge; 2,
</p>
<p>Gn&lowast;(t)&sim; nG(t) as t &rarr;&infin;. (A6.4.15)
</p>
<p>(iv) For any ε &gt; 0 there exists a b= b(ε) &lt;&infin; such that
Gn&lowast;(t)
</p>
<p>G(t)
&le; b(1 + ε)n
</p>
<p>for all n&ge; 2 and t .</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Subexponential Distributions and Their Properties 681
</p>
<p>In addition to assertions (i) and (ii) of the theorem, we can also show that if G &isin; S
and the function m(t) &isin;L possesses the property
</p>
<p>0 &lt;m1 &le;m(t)&le;m2 &lt;&infin;
</p>
<p>then G1(t)=m(t)G(t) &isin; S.
Theorems A6.4.1(vi), A6.4.2 and A6.4.3(iii) imply the following simple state-
</p>
<p>ment elucidating the subexponentiality condition for random variables taking values
</p>
<p>of both signs.
</p>
<p>Corollary A6.4.1 A distribution G belongs to S if and only if G &isin;L and G2&lowast;(t)&sim;
2G(t) as t &rarr;&infin;.
</p>
<p>Remark A6.4.5 Evidently the asymptotic relation G1(t) &sim; G2(t) as t &rarr; &infin; is an
equivalence relation on the set of distributions on R. Theorem A6.4.3(ii) means
that the class S is closed with respect to that equivalence. One can easily see that in
each of the equivalence subclasses of the class S with respect to this relation there
is always a distribution with an arbitrarily smooth tail G(t).
</p>
<p>Indeed, let p(t) be an infinitely differentiable probability density on R vanishing
</p>
<p>outside [0,1] (we can take, e.g., p(x)= c &middot; e&minus;1/(x(1&minus;x)) if x &isin; (0,1) and p(x)= 0
if x /&isin; (0,1)). Now we &ldquo;smooth&rdquo; the function l(t) := &minus; lnG(t), G &isin; S, putting
</p>
<p>l0(t) :=
&int;
</p>
<p>p(t &minus; u)l(u) du, and let G0(t) := e&minus;l0(t). (A6.4.16)
</p>
<p>Clearly, G0(t) is an infinitely differentiable function and, since l(t) is nondecreasing
</p>
<p>and we actually integrate over [t &minus; 1, t] only, one has l(t &minus; 1) &le; l0(t) &le; l(t) and
hence by Theorem A6.4.2
</p>
<p>1 &le; G0(t)
G(t)
</p>
<p>&le; G(t &minus; 1)
G(t)
</p>
<p>&rarr; 1 as t &rarr;&infin;.
</p>
<p>Thus, the distribution G0 is equivalent to the original G. A simpler smoothing pro-
</p>
<p>cedure leading to a less smooth asymptotically equivalent tail consists of replacing
</p>
<p>the function l(t) with its linear interpolation with nodes at points (k, l(k)), k being
</p>
<p>an integer.
</p>
<p>Therefore, up to a summand o(1), we can always assume the function l(t) =
&minus; lnG(t), G &isin; S, to be arbitrarily smooth.
</p>
<p>The aforesaid is clearly applicable to the class L as well: it is also closed with
</p>
<p>respect to the introduced equivalence, and each of its equivalence subclass contains
</p>
<p>arbitrarily smooth representatives.
</p>
<p>Remark A6.4.6 Theorem A6.4.3(ii) and (iii) immediately implies that if G &isin; S then
also Gn&lowast; &isin; S, n = 2,3, . . . . Moreover, if we denote by Gn&or; the distribution of the</p>
<p/>
</div>
<div class="page"><p/>
<p>682 6 Regularly Varying Functions
</p>
<p>maximum of independent identically distributed random variables ζ1, . . . , ζn &sub;=G,
then the evident relation
</p>
<p>Gn&or;(t)= 1 &minus;
(
1 &minus;G(t)
</p>
<p>)n &sim; nG(t) as t &rarr;&infin; (A6.4.17)
</p>
<p>and Theorem A6.4.3(ii) imply that Gn&or; also belongs to S.
Relations (A6.4.17) and (A6.4.15) show that, in the case of a subexponential
</p>
<p>G, the tail Gn&lowast;(t) of the distribution of the sum of a fixed number n of indepen-
dent identically distributed random variables ζi &sub;= G is asymptotically equivalent
(as t &rarr;&infin;) to the tail Gn&or;(t) of the maximum of these random variables, i.e., the
&ldquo;large&rdquo; values of this sum are mainly due to by the presence of one &ldquo;large&rdquo; term ζi
in the sum. It is easy to see that this property is characteristic of subexponentiality.
</p>
<p>Remark A6.4.7 Note also that an assertion converse to what was stated at the be-
ginning of Remark A6.4.6 is also valid: if Gn&lowast; &isin; S for some n &ge; 2 then G &isin; S
as well. That Gn&or; &isin; S implies G &isin; S evidently follows from (A6.4.17) and Theo-
rem A6.4.3(ii).
</p>
<p>Proof of Theorem A6.4.3 (i) First assume that c1c2 &gt; 0 and that both distributions
Gi are concentrated on [0,&infin;). Fix an arbitrary ε &gt; 0 and choose M large enough
to have Gi(M) &lt; ε, i = 1,2, and G(M) &lt; ε, and such that, for t &gt;M ,
</p>
<p>(1 &minus; ε)ci &lt;
Gi(t)
</p>
<p>G(t)
&lt; (1 + ε)ci, i = 1,2, 1 &minus; ε &lt;
</p>
<p>G(t &minus;M)
G(t)
</p>
<p>&lt; 1 + ε
</p>
<p>(A6.4.18)
</p>
<p>(the last inequality holds by virtue of Theorem A6.4.2).
</p>
<p>Let ζ &sub;= G and ζi &sub;= Gi , i = 1,2, be independent random variables. Then, for
t &gt; 2M , we have the representation
</p>
<p>G1 &lowast;G2(t)= P1 + P2 + P3 + P4, (A6.4.19)
</p>
<p>where
</p>
<p>P1 := P
(
ζ1 &ge; t &minus; ζ2, ζ2 &isin; [0,M)
</p>
<p>)
,
</p>
<p>P2 := P
(
ζ2 &ge; t &minus; ζ1, ζ1 &isin; [0,M)
</p>
<p>)
,
</p>
<p>P3 := P
(
ζ2 &ge; t &minus; ζ1, ζ1 &isin; [M, t &minus;M)
</p>
<p>)
,
</p>
<p>P4 := P(ζ2 &ge;M,ζ1 &ge; t &minus;M)
</p>
<p>(see Fig. A.1).
</p>
<p>We show that the first two terms on the right-hand side of (A6.4.19) are asymp-
</p>
<p>totically equivalent to c1G(t) and c2G(t), respectively, while the last two terms are
</p>
<p>negligibly small compared with G(t). Indeed, for P1 we have the obvious two-sided
</p>
<p>bounds</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Subexponential Distributions and Their Properties 683
</p>
<p>Fig. A.1 Illustration to the
</p>
<p>proof of Theorem A6.4.3,
</p>
<p>showing the regions Pi ,
</p>
<p>i = 1,2,3,4
</p>
<p>(1 &minus; ε)2c1G(t) &lt;G1(t)
(
1 &minus;G2(M)
</p>
<p>)
= P
</p>
<p>(
ζ1 &ge; t, ζ2 &isin; [0,M)
</p>
<p>)
</p>
<p>&le; P1 &le; P(ζ1 &ge; t &minus;M)=G1(t &minus;M)&le; (1 + ε)2c1G(t)
</p>
<p>by (A6.4.18); the term P2 can be bounded in a similar way. Further,
</p>
<p>P4 = P(ζ2 &ge;M, ζ1 &ge; t &minus;M)=G2(M)G1(t &minus;M)&lt; ε(1 + ε)2c2G(t).
</p>
<p>It remains to estimate P3 (note that it is here that we will need the condition G &isin; S;
so far we have only used the fact that G &isin;L). We have
</p>
<p>P3 =
&int;
</p>
<p>[M,t&minus;M)
G2(t &minus; y)G1(dy)&le; (1 + ε)c2
</p>
<p>&int;
</p>
<p>[M,t&minus;M)
G(t &minus; y)G1(dy),
</p>
<p>(A6.4.20)
</p>
<p>where it is clear that, by (A6.4.18), the last integral is equal to
</p>
<p>P
(
ζ + ζ1 &ge; t, ζ1 &isin; [M, t &minus;M)
</p>
<p>)
</p>
<p>= P
(
ζ &ge; t &minus;M, ζ1 &isin; [M, t &minus;M)
</p>
<p>)
+ P
</p>
<p>(
ζ + ζ1 &ge; t, ζ &isin; [M, t &minus;M)
</p>
<p>)
</p>
<p>=G(t &minus;M)G1
(
[M, t &minus;M)
</p>
<p>)
+
&int;
</p>
<p>[M,t&minus;M)
G1(t &minus; y)G(dy)
</p>
<p>&le; ε(1 + ε)G(t)+ (1 + ε)c1
&int;
</p>
<p>[M,t&minus;M)
G(t &minus; y)G(dy). (A6.4.21)
</p>
<p>Now note that similarly to the above argument we can easily obtain (setting
</p>
<p>G1 =G2 =G) that
</p>
<p>G2&lowast;(t)= (1 + θ1ε)2G(t)+
&int;
</p>
<p>[M,t&minus;M)
G(t &minus; y)G(dy)+ ε(1 + θ2ε)G(t),
</p>
<p>where |θi | &le; 1, i = 1,2. Since G2&lowast;(t) &sim; 2G(t) by virtue of G &isin; S+, this equality
means that the integral on the right-hand side is o(G(t)). Now (A6.4.21) immedi-
</p>
<p>ately implies that also P3 = o(G(t)), and hence the required assertion is established
for the case G &isin; S+.</p>
<p/>
</div>
<div class="page"><p/>
<p>684 6 Regularly Varying Functions
</p>
<p>To extend the desired result to the case of distributions Gi on R, it suffices to
</p>
<p>repeat the argument from the proof of Theorem A6.4.1(vi).
</p>
<p>The case when one of the ci can be zero can be reduced to the case c1c2 &gt; 0,
</p>
<p>which has already been considered. If, say, c1 = 0 and c2 &gt; 0, then we can introduce
the distribution G̃1 := (G1 +G)/2, for which clearly G̃1(t)/G(t)&rarr; c̃1 = 1/2, and
hence by the already proved assertion, as t &rarr;&infin;,
</p>
<p>1
</p>
<p>2
+ c2 &sim;
</p>
<p>G̃1 &lowast;G2(t)
G(t)
</p>
<p>= G1 &lowast;G2(t)+G &lowast;G2(t)
2G(t)
</p>
<p>= G1 &lowast;G2(t)
2G(t)
</p>
<p>+
(
1 + o(1)
</p>
<p>)1 + c2
2
</p>
<p>,
</p>
<p>so that G1 &lowast;G2(t)/G(t)&rarr; c2 = c1 + c2.
(ii) Denote by G+0 the distribution of the random variable ζ
</p>
<p>+
0 , where ζ0 &sub;=G0.
</p>
<p>Since G+0 (t)=G0(t) for t &gt; 0, it follows immediately from (i) with G1 =G2 =G
+
0
</p>
<p>that (G+0 )
2&lowast;(t)&sim; 2G+0 (t), i.e. G0 &isin; S.
</p>
<p>(iii) If G &isin; S then by Theorems A6.4.1(vi) and A6.4.2 we have, as t &rarr;&infin;,
</p>
<p>G2&lowast;(t)&sim;
(
G+
</p>
<p>)2&lowast;
(t)&sim; 2G(t).
</p>
<p>Now relation (A6.4.15) follows immediately from (i) by induction.
</p>
<p>(iv) Similarly to (A6.4.11), we have Gn&lowast;(t)&le;Gn&lowast;+ (t), n&ge; 1. Therefore it is clear
that it suffices to consider the case G &isin; S+. Put
</p>
<p>αn := sup
t&ge;0
</p>
<p>Gn&lowast;(t)
</p>
<p>G(t)
.
</p>
<p>Similarly to (A6.4.14), for n&ge; 2, we have
</p>
<p>Gn&lowast;(t)=G(t)+
&int; t
</p>
<p>0
</p>
<p>G(n&minus;1)&lowast;(t &minus; y)G(dy),
</p>
<p>and hence, for each M &gt; 0,
</p>
<p>αn &le; 1 + sup
0&le;t&le;M
</p>
<p>&int; t
</p>
<p>0
</p>
<p>G(n&minus;1)&lowast;(t &minus; y)
G(t)
</p>
<p>G(dy)
</p>
<p>+ sup
t&gt;M
</p>
<p>&int; t
</p>
<p>0
</p>
<p>G(n&minus;1)&lowast;(t &minus; y)
G(t &minus; y)
</p>
<p>G(t &minus; y)
G(t)
</p>
<p>G(dy)
</p>
<p>&le; 1 + 1
G(M)
</p>
<p>+ αn&minus;1 sup
t&gt;M
</p>
<p>G2&lowast;(t)&minus;G(t)
G(t)
</p>
<p>.
</p>
<p>Since G &isin; S, for any ε &gt; 0 there exists an M =M(ε) such that
</p>
<p>sup
t&gt;M
</p>
<p>G2&lowast;(t)&minus;G(t)
G(t)
</p>
<p>&lt; 1 + ε</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Subexponential Distributions and Their Properties 685
</p>
<p>and hence
</p>
<p>αn &le; b0 + αn&minus;1(1 + ε), b0 := 1 + 1/G(M), α1 = 1.
</p>
<p>This recurrently implies
</p>
<p>αn &le; b0 + b0(1 + ε)+ αn&minus;2(1 + ε)2 &le; &middot; &middot; &middot; &le; b0
n&minus;1&sum;
</p>
<p>j=0
(1 + ε)j &le; b0
</p>
<p>ε
(1 + ε)n.
</p>
<p>The theorem is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 7
</p>
<p>The Proofs of Theorems on Convergence
to Stable Laws
</p>
<p>In this appendix we will prove Theorems 8.8.1&ndash;8.8.4.
</p>
<p>7.1 The Integral Limit Theorem
</p>
<p>In this section we will prove Theorem 8.8.1 on convergence of the distributions of
</p>
<p>normalised sums Sn =
&sum;n
</p>
<p>k=1 ξk to stable laws. Recall the basic notation:
</p>
<p>F+(t) := P(ξ &ge; t), F&minus;(t) := P(ξ &lt;&minus;t),
F0(t) := F+(t)+ F&minus;(t)= P
</p>
<p>(
ξ /&isin; [&minus;t, t)
</p>
<p>)
.
</p>
<p>The main condition used in the theorem has this form:
</p>
<p>[Rβ,ρ] The total tail F0(x) = F&minus;(x)+ F+(x) is a r.v.f. as x &rarr;&infin;, i.e., can be
represented as
</p>
<p>F0(x)= t&minus;βLF0(x), β &isin; (0,2], (A7.1.1)
where LF0(x) is an s.v.f., and there exists the limit
</p>
<p>ρ+ := lim
x&rarr;&infin;
</p>
<p>F+(x)
</p>
<p>F0(x)
&isin; [0,1], ρ := 2ρ+ &minus; 1. (A7.1.2)
</p>
<p>In the case β &lt; 2 we put
</p>
<p>b(n) := F (&minus;1)0 (1/n), (A7.1.3)
</p>
<p>while for β = 2 we set
b(n) := Y (&minus;1)(1/n), (A7.1.4)
</p>
<p>where
</p>
<p>Y(t) := 2t&minus;2
&int; t
</p>
<p>0
</p>
<p>yF0(y) dy = t&minus;2E
(
ξ2;&minus;t &le; ξ &lt; t
</p>
<p>)
= t&minus;2LY (t), (A7.1.5)
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>687</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>688 7 The Proofs of Theorems on Convergence to Stable Laws
</p>
<p>LY (t) is an s.v.f., so that (see Theorem A6.2.1(v) of Appendix 6)
</p>
<p>b(n)= n1/αLb(n), Lb is an s.v.f.
</p>
<p>In the case when F+(t) and F&minus;(t) are regularly varying functions (for instance,
when condition [Rβ,ρ] is satisfied and ρ = 0), we will denote these functions
by V (t) and W(t), respectively, and put
</p>
<p>VI (t) :=
&int; t
</p>
<p>0
</p>
<p>V (y)dy, V I (t) :=
&int; &infin;
</p>
<p>t
</p>
<p>V (y)dy;
</p>
<p>the same notational convention will be used for W .
</p>
<p>If F+(t) = o(F0(t)) as t &rarr;&infin; (ρ = &minus;1), then F+(t) is not necessarily a regu-
larly varying function, but everything we say below regarding the sums V (t)+W(t)
and V I (t)+W I (t) remains valid if we understand by their first summands quantities
negligibly small compared to the second summands (the first summands can also be
</p>
<p>replaced by zeros). This is also true for the sums VI (t)+WI (t), except for the case
when Emax(0, ξ) exists and VI (t) has to be replaced by E(ξ ; ξ &ge; 0)+ o(1).
</p>
<p>Theorem A7.1.1 Let condition [Rβ,ρ] be satisfied and ζn := Snb(n) .
(i) For β &isin; (0,2), β 
= 1, and scaling factor (A7.1.3), as n&rarr;&infin;,
</p>
<p>ζn &rArr; ζ (β,ρ), (A7.1.6)
</p>
<p>where the distribution Fβ,ρ of the random variable ζ (β,ρ) depends on the parameters
β and ρ only and has ch.f.
</p>
<p>ϕ(β,ρ)(t) := Eeitζ (β,ρ) = exp
{
|t |βB(β,ρ,ϑ)
</p>
<p>}
, (A7.1.7)
</p>
<p>where ϑ := sign t ,
</p>
<p>B(β,ρ,ϑ) := Γ (1 &minus; β)
[
iρϑ sin
</p>
<p>βπ
</p>
<p>2
&minus; cos βπ
</p>
<p>2
</p>
<p>]
(A7.1.8)
</p>
<p>and, for β &isin; (1,2), we assume that Γ (1 &minus; β)= Γ (2 &minus; β)/(1 &minus; β).
(ii) When β = 1, for the sequence ζn with scaling factor (A7.1.3) to converge to
</p>
<p>a limiting law the former, generally speaking, needs to be centred. More precisely,
we have, as n&rarr;&infin;,
</p>
<p>ζn &minus;An &rArr; ζ (1,ρ), (A7.1.9)
where
</p>
<p>An :=
n
</p>
<p>b(n)
</p>
<p>[
VI
</p>
<p>(
b(n)
</p>
<p>)
&minus;WI
</p>
<p>(
b(n)
</p>
<p>)]
&minus; ρC, (A7.1.10)
</p>
<p>C &asymp; 0.5772 is the Euler constant, and
</p>
<p>ϕ(1,ρ)(t) := Eeitζ (1,ρ) = exp
{
&minus;π |t |
</p>
<p>2
&minus; iρt ln |t |
</p>
<p>}
. (A7.1.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 The Integral Limit Theorem 689
</p>
<p>If n[VI (b(n))&minus;WI (b(n))] = o(b(n)), then ρ = 0 and we can put An = 0.
If there exists Eξ = 0 then
</p>
<p>An =
n
</p>
<p>b(n)
</p>
<p>[
W I
</p>
<p>(
b(n)
</p>
<p>)
&minus; V I
</p>
<p>(
b(n)
</p>
<p>)]
&minus; ρC.
</p>
<p>If Eξ = 0 and ρ 
= 0 then ρAn &rarr;&minus;&infin; as n&rarr;&infin;.
(iii) For β = 2 and scaling factor (A7.1.4), as n&rarr;&infin;,
</p>
<p>ζn &rArr; ζ (2,ρ), ϕ(2,ρ)(t) := Eeitζ = e&minus;t
2/2,
</p>
<p>so that ζ (2,ρ) has the standard normal distribution which does not depend on ρ.
</p>
<p>Proof We will use the same approach as in the proof of the central limit theorem
using relation (8.8.1). We will study the asymptotic properties of the ch.f. ϕ(t) =
Eeitξ in the vicinity of zero (more precisely, the asymptotics of
</p>
<p>ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1 &rarr; 0
</p>
<p>as b(n)&rarr;&infin;) and show that, under condition [Rβ,ρ], for each &micro; &isin;R, we have
</p>
<p>n
</p>
<p>(
ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>)
&rarr; lnϕ(β,ρ)(&micro;) as n&rarr;&infin; (A7.1.12)
</p>
<p>(or some modification of this relation, see (A7.1.48)). This will imply that, for ζn =
S(n)/b(n), as n&rarr;&infin;, there holds the relation (cf. Lemma 8.3.2)
</p>
<p>ϕζn(&micro;)&rarr; ϕ(β,ρ)(&micro;). (A7.1.13)
</p>
<p>Indeed,
</p>
<p>ϕζn(&micro;)= ϕn
(
</p>
<p>&micro;
</p>
<p>b(n)
</p>
<p>)
.
</p>
<p>Since ϕ(t)&rarr; 1 as t &rarr; 0, one has
</p>
<p>lnϕζn(&micro;)= n lnϕ
(
</p>
<p>&micro;
</p>
<p>b(n)
</p>
<p>)
</p>
<p>= n ln
[
</p>
<p>1 +
(
ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>)]
= n
</p>
<p>[
ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>]
+Rn,
</p>
<p>where |Rn| &le; n|ϕ(&micro;/b(n))&minus;1|2 for all n large enough, and hence Rn &rarr; 0 by virtue
of (A7.1.12). It follows that (A7.1.12) implies (A7.1.13).
</p>
<p>So first we will study the asymptotics of ϕ(t) as t &rarr; 0 and then estab-
lish (A7.1.12).</p>
<p/>
</div>
<div class="page"><p/>
<p>690 7 The Proofs of Theorems on Convergence to Stable Laws
</p>
<p>(i) First let β &isin; (0,1). We have
</p>
<p>ϕ(t)=&minus;
&int; &infin;
</p>
<p>0
</p>
<p>eitx dV (x)&minus;
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;itx dW(x). (A7.1.14)
</p>
<p>Consider the former integral
</p>
<p>&minus;
&int; &infin;
</p>
<p>0
</p>
<p>eitx dV (x)= V (0)+ it
&int; &infin;
</p>
<p>0
</p>
<p>eitxV (x)dx, (A7.1.15)
</p>
<p>where the substitution |t |x = y, |t | = 1/m yields
</p>
<p>I+(t) := it
&int; &infin;
</p>
<p>0
</p>
<p>eitxV (x)dx = iϑ
&int; &infin;
</p>
<p>0
</p>
<p>eiϑyV (my)dy, (A7.1.16)
</p>
<p>ϑ = sign t (we will henceforth exclude the trivial case t = 0).
Assume for the present that ρ+ &gt; 0. Then V (x) is an r.v.f. as x &rarr;&infin; and, for
</p>
<p>each y, by virtue of the properties of s.v.f.s we have, as |m| &rarr; 0,
</p>
<p>V (my)&sim; y&minus;βV (m).
</p>
<p>Therefore it is natural to expect that, as |t | &rarr; 0,
</p>
<p>I+(t)&sim; iϑV (m)
&int; &infin;
</p>
<p>0
</p>
<p>eiϑyy&minus;β dy = iϑV (m)A(β,ϑ), (A7.1.17)
</p>
<p>where
</p>
<p>A(β,ϑ) :=
&int; &infin;
</p>
<p>0
</p>
<p>eiϑyy&minus;β dy. (A7.1.18)
</p>
<p>Assume that relation (A7.1.17) holds and similarly (in the case when ρ&minus; &gt; 0)
</p>
<p>&minus;
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;itx dW(x)=W(0)+ I&minus;(t), (A7.1.19)
</p>
<p>where
</p>
<p>I&minus;(t) := &minus;it
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;itxW(x)dx &sim;&minus;iϑW(m)
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;iϑyy&minus;β dy
</p>
<p>=&minus;iϑW(m)A(β,&minus;ϑ). (A7.1.20)
</p>
<p>Since V (0)+W(0)= 1, relations (A7.1.14)&ndash;(A7.1.20) mean that, as t &rarr; 0,
</p>
<p>ϕ(t)= 1 + F0(m)iϑ
[
ρ+A(β,ϑ)&minus; ρ&minus;A(β,&minus;ϑ)
</p>
<p>](
1 + o(1)
</p>
<p>)
. (A7.1.21)
</p>
<p>We can find an explicit form of the integral A(β,ϑ). Observe that the integral
</p>
<p>along the boundary of the positive quadrant (closed as a contour) in the complex</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 The Integral Limit Theorem 691
</p>
<p>plane of the function eizz&minus;β , which, as |t | &rarr; 0, is equal to zero. From this it is not
hard to obtain that
</p>
<p>A(β,ϑ)= Γ (1 &minus; β)eiϑ(1&minus;β)π/2, β &gt; 0. (A7.1.22)
</p>
<p>(Note also that (A7.1.18) is a table integral and its value can be found in handbooks,
</p>
<p>see, e.g., integrals 3.761.4 and 3.761.9 in [18].)
</p>
<p>Thus, in (A7.1.21) one has
</p>
<p>iϑ
[
ρ+A(β,ϑ)&minus; ρ&minus;A(β,&minus;ϑ)
</p>
<p>]
= iϑ Γ (1 &minus; β)
</p>
<p>[
ρ+ cos
</p>
<p>(1 &minus; β)π
2
</p>
<p>+ iϑρ+ sin
(1 &minus; β)π
</p>
<p>2
&minus; ρ&minus; cos
</p>
<p>(1 &minus; β)π
2
</p>
<p>+ iϑρ&minus; sin
(1 &minus; β)π
</p>
<p>2
</p>
<p>]
</p>
<p>= Γ (1 &minus; β)
[
iϑ(ρ+ &minus; ρ&minus;) cos
</p>
<p>(1 &minus; β)π
2
</p>
<p>&minus; sin (1 &minus; β)π
2
</p>
<p>]
</p>
<p>= Γ (1 &minus; β)
[
iϑρ sin
</p>
<p>βπ
</p>
<p>2
&minus; cos βπ
</p>
<p>2
</p>
<p>]
= B(β,ρ,ϑ),
</p>
<p>where B(β,ρ,ϑ) is defined in (A7.1.8). Hence, as t &rarr; 0,
</p>
<p>ϕ(t)&minus; 1 = F0(m)B(β,ρ,ϑ)
(
1 + o(1)
</p>
<p>)
. (A7.1.23)
</p>
<p>Putting t = &micro;/b(n) (so that m= b(n)/|&micro;|), where b(n) is defined in (A7.1.3), and
taking into account that F0(b(n))&sim; 1/n, we obtain
</p>
<p>n
</p>
<p>[
ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>]
= nF0
</p>
<p>(
b(n)
</p>
<p>|&micro;|
</p>
<p>)
B(β,ρ,ϑ)
</p>
<p>(
1 + o(1)
</p>
<p>)
&sim; |&micro;|βB(β,ρ,ϑ).
</p>
<p>(A7.1.24)
</p>
<p>We have established the validity of (A7.1.12) and therefore that of assertion (i) of
</p>
<p>the theorem in the case β &lt; 1, ρ+ &gt; 0.
If ρ+ = 0 (ρ&minus; = 0) then, as was already mentioned, the above argument remains
</p>
<p>valid if we replace V (m) (W(m)) by zero. This follows from the fact that in this
</p>
<p>case F+(t) (F&minus;(t)) admits a regularly varying majorant V &lowast;(t)= o(W(t)) (W &lowast;(t)=
o(V (t))).
</p>
<p>It remains only to justify the asymptotic equivalence in (A7.1.17). To do that, it
</p>
<p>is sufficient to verify that the integrals
</p>
<p>&int; ε
</p>
<p>0
</p>
<p>eiϑyV (my)dy,
</p>
<p>&int; &infin;
</p>
<p>M
</p>
<p>eiϑyV (my)dy (A7.1.25)
</p>
<p>can be made arbitrarily small compared to V (m) by choosing appropriate ε and M .
</p>
<p>Note first that by Theorem A6.2.1(iii) of Appendix 6 (see (A6.1.2) in Appendix 6),
</p>
<p>for any δ &gt; 0, there exists an xδ &gt; 0 such that, for all v &le; 1 and vx &ge; xδ , we have
</p>
<p>V (vx)
</p>
<p>V (x)
&le; (1 + δ)v&minus;β&minus;δ.</p>
<p/>
</div>
<div class="page"><p/>
<p>692 7 The Proofs of Theorems on Convergence to Stable Laws
</p>
<p>Therefore, for δ &lt; 1 &minus; β and x &gt; xδ ,
&int; x
</p>
<p>0
</p>
<p>V (u)du &le; xδ +
&int; x
</p>
<p>xδ
</p>
<p>V (u)du= xδ + xV (x)
&int; 1
</p>
<p>xδ/x
</p>
<p>V (vx)
</p>
<p>V (x)
dv
</p>
<p>&le; xδ + xV (x)(1 + δ)
&int; 1
</p>
<p>0
</p>
<p>v&minus;β&minus;δdv
</p>
<p>= xδ +
xV (x)(1 + δ)
</p>
<p>1 &minus; β &minus; δ &le; cxV (x) (A7.1.26)
</p>
<p>since xV (x)&rarr;&infin; as x &rarr;&infin;. It follows that
∣∣∣∣
&int; ε
</p>
<p>0
</p>
<p>eiϑyV (my)dy
</p>
<p>∣∣∣∣&le;
1
</p>
<p>m
</p>
<p>&int; εm
</p>
<p>0
</p>
<p>V (u)du&le; cεV (εm)&sim; cε1&minus;βV (m).
</p>
<p>Since ε1&minus;β &rarr; 0 as ε &rarr; 0, the first assertion in (A7.1.25) is proved. The second
integral in (A7.1.25) is equal to
</p>
<p>&int; &infin;
</p>
<p>M
</p>
<p>eiϑyV (my)dy = 1
iϑ
</p>
<p>eiϑyV (my)
</p>
<p>∣∣∣∣
&infin;
</p>
<p>M
</p>
<p>&minus; 1
iϑ
</p>
<p>&int; &infin;
</p>
<p>M
</p>
<p>eiϑydV (my)
</p>
<p>= &minus; 1
iϑ
</p>
<p>eiϑMV (mM)&minus; 1
iϑ
</p>
<p>&int; &infin;
</p>
<p>mM
</p>
<p>eiϑu/mdV (u),
</p>
<p>so its absolute value does not exceed
</p>
<p>2V (mM)&sim; 2M&minus;βV (m) (A7.1.27)
</p>
<p>as m&rarr;&infin;. Hence the value of the second integral in (A7.1.25) can also be made ar-
bitrarily small compared to V (m) by choosing an appropriate M . Relation (A7.1.17)
</p>
<p>together with the assertion of the theorem in the case β &lt; 1 are proved.
</p>
<p>Let now β &isin; (1,2) and hence there exist a finite expectation Eξ which, according
to our condition, will be assumed to be equal to zero. In this case,
</p>
<p>ϕ(t)&minus; 1 = ϑ
&int; |t |
</p>
<p>0
</p>
<p>ϕ&prime;(ϑu)du, ϑ = sign t, (A7.1.28)
</p>
<p>and we have to find the asymptotic behaviour of
</p>
<p>ϕ&prime;(t)=&minus;i
&int; &infin;
</p>
<p>0
</p>
<p>xeitxdV (x)+ i
&int; &infin;
</p>
<p>0
</p>
<p>xe&minus;itxdW(x)=: I (1)+ (t)+ I
(1)
&minus; (t) (A7.1.29)
</p>
<p>as t &rarr; 0. Since x dV (x)= d(xV (x))&minus; V (x)dx, integration by parts yields
</p>
<p>I
(1)
+ (t) := &minus;i
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>xeitxdV (x)=&minus;i
&int; &infin;
</p>
<p>0
</p>
<p>eitxd
(
xV (x)
</p>
<p>)
+ i
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>eitxV (x)dx
</p>
<p>= &minus;t
&int; &infin;
</p>
<p>0
</p>
<p>xV (x)eitx dx + iV I (0)&minus; t
&int; &infin;
</p>
<p>0
</p>
<p>V I (x)eitx dx</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 The Integral Limit Theorem 693
</p>
<p>= iV I (0)&minus; t
&int; &infin;
</p>
<p>0
</p>
<p>Ṽ (x)eitx dx, (A7.1.30)
</p>
<p>where, by Theorem A6.2.1(iv) of Appendix 6, both functions
</p>
<p>V I (x) :=
&int; &infin;
</p>
<p>x
</p>
<p>V (u)du&sim; xV (x)
β &minus; 1 as x &rarr;&infin;, V
</p>
<p>I (0) &lt;&infin;,
</p>
<p>and
</p>
<p>Ṽ (x) := xV (x)+ V I (x)&sim; βxV (x)
β &minus; 1
</p>
<p>are regularly varying.
</p>
<p>Letting, as before, m= 1/|t |, m&rarr;&infin; (cf. (A7.1.16), (A7.1.17)), we get
</p>
<p>&minus;t
&int; &infin;
</p>
<p>0
</p>
<p>Ṽ (x)eitx dx = &minus;ϑṼ (m)
&int; &infin;
</p>
<p>0
</p>
<p>Ṽ (my)eiϑy dy
</p>
<p>&sim; &minus;ϑ
&int; &infin;
</p>
<p>0
</p>
<p>y&minus;β+1eiϑy dy =&minus; βV (m)
t (β &minus; 1)A(β &minus; 1, ϑ),
</p>
<p>I
(1)
+ (t)= iV I (0)&minus;
</p>
<p>βρ+F0(m)
</p>
<p>t (β &minus; 1) A(β &minus; 1, ϑ)
(
1 + o(1)
</p>
<p>)
, (A7.1.31)
</p>
<p>where the function A(β,ϑ) defined in (A7.1.18) is equal to (A7.1.22).
</p>
<p>Similarly,
</p>
<p>I
(1)
&minus; (t) := i
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>te&minus;itx dW(x)
</p>
<p>= &minus;t
&int; &infin;
</p>
<p>0
</p>
<p>xW(x)e&minus;itx dx &minus; iW I (0)&minus; t
&int; &infin;
</p>
<p>0
</p>
<p>W I (x)e&minus;itx dx
</p>
<p>= &minus;iW I (0)&minus; t
&int; &infin;
</p>
<p>0
</p>
<p>W̃ (x)e&minus;itx dx,
</p>
<p>where
</p>
<p>W I (x) :=
&int; &infin;
</p>
<p>x
</p>
<p>W(u)du, W̃ (x) := xW(x)+W I (x)&sim; βxW(x)
β &minus; 1 ,
</p>
<p>and
</p>
<p>&minus;t
&int; &infin;
</p>
<p>0
</p>
<p>W̃ (x) e&minus;itx dx &sim;&minus; βW(m)
t (β &minus; 1)A(β &minus; 1,&minus;ϑ).
</p>
<p>Therefore
</p>
<p>I
(1)
&minus; (t)= iW I (0)&minus;
</p>
<p>βρ&minus;F0(m)
</p>
<p>t (β &minus; 1) A(β &minus; 1,&minus;ϑ)
(
1 + o(1)
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>694 7 The Proofs of Theorems on Convergence to Stable Laws
</p>
<p>and hence, by virtue of (A7.1.29), (A7.1.31), and the equality V I (0) &minus;W I (0) =
Eξ = 0, we have
</p>
<p>ϕ&prime;(t)=&minus; βF0(m)
t (β &minus; 1)
</p>
<p>[
ρ+A(β &minus; 1, ϑ)+ ρ&minus;A(β &minus; 1,&minus;ϑ)
</p>
<p>](
1 + o(1)
</p>
<p>)
.
</p>
<p>We return now to relation (A7.1.28). Since
</p>
<p>&int; |t |
</p>
<p>0
</p>
<p>u&minus;1 F0
(
u&minus;1
</p>
<p>)
du&sim; β&minus;1F0
</p>
<p>(
|t |&minus;1
</p>
<p>)
= β&minus;1F0(m)
</p>
<p>(see Theorem A6.2.1(iii) of Appendix 6), we obtain, again using (A7.1.22) and an
</p>
<p>argument similar to the one in the proof for the case β &lt; 1, that
</p>
<p>ϕ(t)&minus; 1 = &minus; 1
β &minus; 1 F0(m)
</p>
<p>[
ρ+A(β &minus; 1, ϑ)+ ρ&minus;A(β &minus; 1,&minus;ϑ)
</p>
<p>](
1 + o(1)
</p>
<p>)
</p>
<p>= &minus;Γ (2 &minus; β)
β &minus; 1 F0(m)
</p>
<p>[
ρ+
</p>
<p>(
cos
</p>
<p>(2 &minus; β)π
2
</p>
<p>+ iϑ sin (2 &minus; β)π
2
</p>
<p>)
</p>
<p>+ ρ&minus;
(
</p>
<p>cos
(2 &minus; β)π
</p>
<p>2
&minus; iϑ sin (2 &minus; β)π
</p>
<p>2
</p>
<p>)](
1 + o(1)
</p>
<p>)
</p>
<p>= Γ (2 &minus; β)
β &minus; 1 F0(m)
</p>
<p>[
cos
</p>
<p>βπ
</p>
<p>2
&minus; iϑρ sin βπ
</p>
<p>2
</p>
<p>](
1 + o(1)
</p>
<p>)
</p>
<p>= F0(m)B(β,ρ,ϑ)
(
1 + o(1)
</p>
<p>)
. (A7.1.32)
</p>
<p>We arrive once again at relation (A7.1.23) which, by virtue of (A7.1.24), implies the
</p>
<p>assertion of the theorem for β &isin; (1,2).
(ii) Case β = 1. In this case, the computation is somewhat more complicated. We
</p>
<p>again follow relations (A7.1.14)&ndash;(A7.1.16), according to which
</p>
<p>ϕ(t)= 1 + I+(t)+ I&minus;(t). (A7.1.33)
</p>
<p>Rewrite expression (A7.1.16) for I+(t) as
</p>
<p>I+(x)= iϑ
&int; &infin;
</p>
<p>0
</p>
<p>eiϑyV (my)dy = iϑ
&int; &infin;
</p>
<p>0
</p>
<p>V (my) cosy dy &minus;
&int; &infin;
</p>
<p>0
</p>
<p>V (my) siny dy,
</p>
<p>(A7.1.34)
</p>
<p>where the first integral on the right-hand side can be represented as the sum of two
</p>
<p>integrals:
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>V (my)dy +
&int; &infin;
</p>
<p>0
</p>
<p>g(y)V (my)dy, (A7.1.35)
</p>
<p>g(y)=
{
</p>
<p>cosy &minus; 1 if y &le; 1,
cosy if y &gt; 1.
</p>
<p>(A7.1.36)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 The Integral Limit Theorem 695
</p>
<p>Note that (see, e.g., integral 3.782 in [18]) the value of the integral
</p>
<p>&minus;
&int; &infin;
</p>
<p>0
</p>
<p>g(y)y&minus;1 dy = C &asymp; 0.5772 (A7.1.37)
</p>
<p>is the Euler constant. Since V (ym)/V (m)&rarr; y&minus;1 as m&rarr;&infin;, similarly to the above
argument we obtain for the second integral in (A7.1.35) the relation
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>g(y)V (my)dy &sim;&minus;CV (m). (A7.1.38)
</p>
<p>Consider now the first integral in (A7.1.35):
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>V (my)dy =m&minus;1
&int; m
</p>
<p>0
</p>
<p>V (u)du=m&minus;1VI (m), (A7.1.39)
</p>
<p>where
</p>
<p>VI (x) :=
&int; t
</p>
<p>0
</p>
<p>V (u)du (A7.1.40)
</p>
<p>can easily be seen to be an s.v.f. in the case β = 1 (see Theorem A6.2.1(iv) of
Appendix 6). Here if E|ξ | =&infin; then VI (x)&rarr;&infin; as x &rarr;&infin;, and if E|ξ |&lt;&infin; then
VI (x)&rarr; VI (&infin;) &lt;&infin;.
</p>
<p>Thus, for the first term on the right-hand side of (A7.1.34) we have
</p>
<p>Im I+(t)= ϑ
(
&minus;CV (m)+m&minus;1VI (m)
</p>
<p>)
+ o
</p>
<p>(
V (m)
</p>
<p>)
. (A7.1.41)
</p>
<p>Now we will determine how VI (vx) depends on v as x &rarr;&infin;. For any fixed v &gt; 0,
</p>
<p>VI (vx)= VI (x)+
&int; vx
</p>
<p>x
</p>
<p>V (u)du= VI (x)+ xV (x)
&int; v
</p>
<p>1
</p>
<p>V (yx)
</p>
<p>V (x)
dy.
</p>
<p>By Theorem A6.2.1 of Appendix 6,
</p>
<p>&int; v
</p>
<p>1
</p>
<p>V (yx)
</p>
<p>V (x)
dy &sim;
</p>
<p>&int; v
</p>
<p>1
</p>
<p>dy
</p>
<p>y
= lnv,
</p>
<p>so that
</p>
<p>VI (vx)= VI (x)+
(
1 + o(1)
</p>
<p>)
xV (x) lnv =:AV (v, x)+ xV (x) lnv, (A7.1.42)
</p>
<p>where evidently
</p>
<p>AV (v, x)= VI (x)+ o
(
xV (x)
</p>
<p>)
as x &rarr;&infin; (A7.1.43)
</p>
<p>and VI (x)≫ xV (x) by Theorem A6.2.1(iv) of Appendix 6.</p>
<p/>
</div>
<div class="page"><p/>
<p>696 7 The Proofs of Theorems on Convergence to Stable Laws
</p>
<p>Therefore, for t = &micro;/b(n) (so that m= b(n)/|&micro;| and hence V (m) &sim; ρ+|&micro;|/n),
we obtain from (A7.1.41) and (A7.1.42) (where one has to put x = b(n), v = 1/|&micro;|)
that the following representation is valid as n&rarr;&infin;:
</p>
<p>Im I+(t) = &minus;C
ρ+&micro;
</p>
<p>n
+ &micro;
</p>
<p>b(n)
</p>
<p>[
AV
</p>
<p>(
|&micro;|&minus;1, b(n)
</p>
<p>)
&minus; ρ+&micro;
</p>
<p>n
ln |&micro;|
</p>
<p>]
+ o
</p>
<p>(
n&minus;1
</p>
<p>)
</p>
<p>= &micro;
b(n)
</p>
<p>AV
(
|&micro;|&minus;1, b(n)
</p>
<p>)
&minus; ρ+&micro;
</p>
<p>n
</p>
<p>(
C + ln |&micro;|
</p>
<p>)
+ o
</p>
<p>(
n&minus;1
</p>
<p>)
. (A7.1.44)
</p>
<p>For the second term on the right-hand side of (A7.1.34) we have
</p>
<p>Re I+(t)=&minus;
&int; &infin;
</p>
<p>0
</p>
<p>V (my) siny dy &sim;&minus;V (m)
&int; &infin;
</p>
<p>0
</p>
<p>y&minus;1 siny dy.
</p>
<p>Because siny &sim; y as y &rarr; 0, the last integral converges. Since Γ (γ )&sim; 1/γ as γ &rarr;
0, the value of this integral can be found to be (see (A7.1.22) and (A7.1.22))
</p>
<p>lim
γ&rarr;0
</p>
<p>Γ (γ ) sin
γπ
</p>
<p>2
= π
</p>
<p>2
. (A7.1.45)
</p>
<p>Thus, for t = &micro;/b(n),
</p>
<p>Re I+(t)=&minus;
π |&micro;|
2n
</p>
<p>+ o
(
n&minus;1
</p>
<p>)
. (A7.1.46)
</p>
<p>In a similar way we can find an asymptotic representation for the integral I&minus;(t)
(see (A7.1.14)&ndash;(A7.1.20)):
</p>
<p>I&minus;(t) := &minus;iϑ
&int; &infin;
</p>
<p>0
</p>
<p>W(my)e&minus;iϑy dy
</p>
<p>= &minus;iϑ
&int; &infin;
</p>
<p>0
</p>
<p>W(my) cosy dy &minus;
&int; &infin;
</p>
<p>0
</p>
<p>W(my) siny dy.
</p>
<p>Comparing this with (A7.1.34) and the subsequent computation of I+(t), we can
immediately conclude that, for t = &micro;/b(n) (cf. (A7.1.44), (A7.1.46)),
</p>
<p>Im I&minus;(t) = &minus;
&minus;&micro;AW (|&micro;|&minus;1, b(n))
</p>
<p>b(n)
+ ρ&minus;&micro;
</p>
<p>n
</p>
<p>(
C + ln |&micro;|
</p>
<p>)
+ o
</p>
<p>(
n&minus;1
</p>
<p>)
,
</p>
<p>Re I&minus;(t) = &minus;
π |&micro;|ρ&minus;
</p>
<p>2n
+ o
</p>
<p>(
n&minus;1
</p>
<p>)
.
</p>
<p>(A7.1.47)
</p>
<p>Thus we obtain from (A7.1.33), (A7.1.44) and (A7.1.46) that (A7.1.47) imply
</p>
<p>ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1 =&minus;π |&micro;|
</p>
<p>n
&minus; iρ&micro;
</p>
<p>n
</p>
<p>(
C + ln |&micro;|
</p>
<p>)
</p>
<p>+ i&micro;
b(n)
</p>
<p>[
AV
</p>
<p>(
|&micro;|&minus;1, b(n)
</p>
<p>)
&minus;AW
</p>
<p>(
|&micro;|&minus;1, b(n)
</p>
<p>)]
+ o
</p>
<p>(
n&minus;1
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 The Integral Limit Theorem 697
</p>
<p>It follows from (A7.1.43) that the penultimate term here is equal to
</p>
<p>i&micro;
</p>
<p>b(n)
</p>
<p>[
VI
</p>
<p>(
b(n)
</p>
<p>)
&minus;WI
</p>
<p>(
b(n)
</p>
<p>)]
+ o
</p>
<p>(
n&minus;1
</p>
<p>)
,
</p>
<p>so that finally,
</p>
<p>ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1 =&minus;π |&micro;|
</p>
<p>2n
&minus; iρ&micro;
</p>
<p>n
ln |&micro;| + i&micro;An
</p>
<p>n
+ o
</p>
<p>(
n&minus;1
</p>
<p>)
, (A7.1.48)
</p>
<p>where
</p>
<p>An =
n
</p>
<p>b(n)
</p>
<p>[
VI
</p>
<p>(
b(n)
</p>
<p>)
&minus;WI
</p>
<p>(
b(n)
</p>
<p>)]
&minus; ρC.
</p>
<p>Therefore, similarly to (A7.1.12) and (A7.1.13), we obtain
</p>
<p>ϕζn&minus;An(&micro;) = e&minus;i&micro;Anϕ n
(
</p>
<p>&micro;
</p>
<p>b(n)
</p>
<p>)
= exp
</p>
<p>{
&minus;i&micro;An + n ln
</p>
<p>[
1 +
</p>
<p>(
ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>)]}
</p>
<p>= exp
{
&minus;i&micro;An + n
</p>
<p>(
ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>)
+ nO
</p>
<p>(∣∣∣∣ϕ
(
</p>
<p>&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>∣∣∣∣
2)}
</p>
<p>.
</p>
<p>As, for β = 1, by Theorem A6.2.1(iv) of Appendix 6, the functions VI and WI are
slowly varying, by (A7.1.48) one has
</p>
<p>n
</p>
<p>∣∣∣∣ϕ
(
</p>
<p>&micro;
</p>
<p>b(n)
</p>
<p>)
&minus;1
</p>
<p>∣∣∣∣
2
</p>
<p>&le; c
(
</p>
<p>1
</p>
<p>n
+ A
</p>
<p>2
n
</p>
<p>n
</p>
<p>)
&le; c1
</p>
<p>(
1
</p>
<p>n
+ 1
</p>
<p>b(n)
</p>
<p>[
VI
</p>
<p>(
b(n)
</p>
<p>)2 +WI
(
b(n)
</p>
<p>)2]
)
&rarr; 0.
</p>
<p>Since clearly
</p>
<p>&minus;i&micro;An + n
(
ϕ
</p>
<p>(
&micro;
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>)
&rarr;&minus;π |&micro;|
</p>
<p>2
&minus; iρ&micro; ln |&micro;|,
</p>
<p>we have
</p>
<p>ϕζn&minus;An(&micro;)&rarr; exp
{
&minus;π |&micro;|
</p>
<p>2
&minus; iρ&micro; ln |&micro;|
</p>
<p>}
,
</p>
<p>so relation (A7.1.9) is proved. The subsequent assertions regarding the centring se-
</p>
<p>quence {An} are evident. �
</p>
<p>(iii) It remains to consider the case β = 2. We will follow representations
(A7.1.28)&ndash;(A7.1.30), according to which we have to find, as m = 1/|t | &rarr;&infin;, the
asymptotics of
</p>
<p>ϕ&prime;(t)= I (1)+ (t)+ I
(1)
&minus; (t), (A7.1.49)
</p>
<p>where
</p>
<p>I
(1)
+ (t) := iV I (0)&minus; t
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>Ṽ (x)eitx dx = iV I (0)&minus; ϑ
&int; &infin;
</p>
<p>0
</p>
<p>Ṽ (my)eiϑy dy
</p>
<p>(A7.1.50)</p>
<p/>
</div>
<div class="page"><p/>
<p>698 7 The Proofs of Theorems on Convergence to Stable Laws
</p>
<p>and, by Theorem A6.2.1(iv) of Appendix 6,
</p>
<p>V I (x)=
&int; &infin;
</p>
<p>x
</p>
<p>V (y)dy &sim; xV (x), Ṽ (x)= xV (x)+ V I (x)&sim; 2xV (x) (A7.1.51)
</p>
<p>as x &rarr;&infin;. Further,
&int; &infin;
</p>
<p>0
</p>
<p>Ṽ (my) eiϑydy =
&int; &infin;
</p>
<p>0
</p>
<p>Ṽ (my) cosy dy + ϑ
&int; &infin;
</p>
<p>0
</p>
<p>Ṽ (my) siny dy. (A7.1.52)
</p>
<p>Here the second integral on the right-hand side is asymptotically equivalent, as
</p>
<p>m&rarr;&infin;, to (see (A7.1.45))
</p>
<p>Ṽ (m)
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>y&minus;1 siny dy = π
2
Ṽ (m).
</p>
<p>The first integral on the right-hand side of (A7.1.52) is equal to
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>Ṽ (my)dy +
&int; &infin;
</p>
<p>0
</p>
<p>g(y)Ṽ (my)dy,
</p>
<p>where the function g(y) was defined in (A7.1.35), and
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>Ṽ (my)dy = 1
m
</p>
<p>&int; m
</p>
<p>0
</p>
<p>Ṽ (u) du= 1
m
</p>
<p>ṼI (m),
</p>
<p>ṼI (x) :=
&int; x
</p>
<p>0 Ṽ (u) du being an s.v.f. by (A7.1.51). Since
</p>
<p>&int; x
</p>
<p>0
</p>
<p>uV (u)du = x
2V (x)
</p>
<p>2
&minus; 1
</p>
<p>2
</p>
<p>&int; x
</p>
<p>0
</p>
<p>u2dV (u),
</p>
<p>&int; x
</p>
<p>0
</p>
<p>V I (u) du = xV I (x)+
&int; x
</p>
<p>0
</p>
<p>uV (u)du
</p>
<p>and V I (x)&sim; xV (x), we have
</p>
<p>ṼI (x) =
&int; x
</p>
<p>0
</p>
<p>(
uV (u)+ V I (u)
</p>
<p>)
du
</p>
<p>= xV I (x)+ x2V (x)&minus;
&int; x
</p>
<p>0
</p>
<p>u2 dV (u)
</p>
<p>= &minus;
&int; x
</p>
<p>0
</p>
<p>u2 dV (y)+O
(
x2V (x)
</p>
<p>)
, (A7.1.53)
</p>
<p>where the last term is negligibly small, because
</p>
<p>&int; x
</p>
<p>0
</p>
<p>uV (u)du≫ x2V (x)
</p>
<p>(see Theorem A6.2.1(iv) of Appendix 6).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 The Integro-Local and Local Limit Theorems 699
</p>
<p>It is also clear that, as x &rarr;&infin;,
</p>
<p>ṼI (x)&rarr; ṼI (&infin;)= E
(
ξ2; ξ &gt; 0
</p>
<p>)
&isin; (0,&infin;].
</p>
<p>As a result, we obtain (see also (A7.1.38))
</p>
<p>I
(1)
+ (t) = iV I (0)&minus;
</p>
<p>iπ
</p>
<p>2
Ṽ (m)&minus; t ṼI (m)+ ϑCṼ (m)+ o
</p>
<p>(
Ṽ (m)
</p>
<p>)
</p>
<p>= iV I (0)&minus; t ṼI (m)
(
1 + o(1)
</p>
<p>)
</p>
<p>since ṼI (x)≫ t Ṽ (x).
Quite similarly we get
</p>
<p>I
(1)
&minus; (t)=&minus;iW I (0)&minus; tW̃I (m)
</p>
<p>(
1 + o(1)
</p>
<p>)
,
</p>
<p>where W̃I is an s.v.f. which is obtained from the function W in the same way as ṼI
from V . Since V I (0)=W I (0), relation (A7.1.49) now yields that
</p>
<p>ϕ&prime;(t)=&minus;t
[
ṼI (m)+ W̃I (m)
</p>
<p>](
1 + o(1)
</p>
<p>)
.
</p>
<p>Hence from (A7.1.28) we obtain the representation
</p>
<p>ϕ(t)&minus; 1 = ϑ
&int; 1/m
</p>
<p>0
</p>
<p>ϕ&prime;(ϑu)du=&minus;
&int; 1/m
</p>
<p>0
</p>
<p>u
[
ṼI (1/u)+ W̃I (1/u)
</p>
<p>]
du
</p>
<p>&sim; &minus; 1
2m2
</p>
<p>[
ṼI (m)+ W̃I (m)
</p>
<p>]
&sim;&minus; 1
</p>
<p>2m2
E
(
ξ2; &minus;m&le; ξ &lt; m
</p>
<p>)
</p>
<p>by virtue of (A7.1.53) and a similar relation for W̃I . Turning now to the definition
</p>
<p>of the function Y(x)= x&minus;2LY (x) in (A7.1.5) and putting
</p>
<p>b(n) := Y (&minus;1)(1/n), t = &micro;/b(n),
</p>
<p>we get
</p>
<p>n
(
ϕ(t)&minus; 1
</p>
<p>)
&sim;&minus;n
</p>
<p>2
Y
(
b(n)/|&micro;|
</p>
<p>)
&sim;&minus;n&micro;
</p>
<p>2
</p>
<p>2
Y
(
b(n)
</p>
<p>)
&rarr;&minus;&micro;
</p>
<p>2
</p>
<p>2
.
</p>
<p>The theorem is proved. �
</p>
<p>7.2 The Integro-Local and Local Limit Theorems
</p>
<p>In this section we will prove Theorems 8.8.2&ndash;8.8.4. We will begin with the integro-
</p>
<p>local theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>700 7 The Proofs of Theorems on Convergence to Stable Laws
</p>
<p>Theorem A7.2.1 (Integro-local Stone&rsquo;s theorem) Let ξ be a non-lattice random
variable and the conditions of Theorem A7.1.1 be satisfied. Then, for each fixed
∆&gt; 0,
</p>
<p>P
(
Sn &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>b(n)
f (β,ρ)
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
+ o
</p>
<p>(
1
</p>
<p>b(n)
</p>
<p>)
as n&rarr;&infin;,
</p>
<p>where the remainder term o( 1
b(n)
</p>
<p>) is uniform in x.
</p>
<p>Proof of Theorem A7.2.1 The Proof is analogous to the proof of Theorem 8.7.1. We
will again use the smoothing approach and consider, along with the sums Sn, the
</p>
<p>sums
</p>
<p>Zn = Sn + θη,
where θ = const and η is chosen so that its ch.f. is equal to 0 outside a fi-
nite interval. For instance, we can choose η as in Sect. 8.7.3, i.e., with the ch.f.
</p>
<p>ϕη(t)= max(0, 1 &minus; |t |). Then equality (8.7.19) will still be valid with the same de-
composition of the integral on its right-hand side into the subintegral I1 over the
</p>
<p>domain |t | &lt; γ and I2 over the domain γ &le; |t | &le; 1/θ . Here estimating I2 can be
done in the same way as in Theorem 8.7.1.
</p>
<p>For the sake of brevity, put ϕ̂(t) := ϕη∆(t)ϕθη(t). Then, for the integral I1 with
x = vb(n), we have
</p>
<p>I1 =
&int;
</p>
<p>|t |&lt;γ
e&minus;itxϕn(t)ϕ̂(t) dt = 1
</p>
<p>b(n)
</p>
<p>&int;
</p>
<p>|u|&lt;γb(n)
e&minus;iuvϕn
</p>
<p>(
u
</p>
<p>b(n)
</p>
<p>)
ϕ̂
</p>
<p>(
u
</p>
<p>b(n)
</p>
<p>)
du.
</p>
<p>(A7.2.1)
</p>
<p>As was shown in the proof of Theorem 8.1.1, for each u we have
</p>
<p>ϕn
(
</p>
<p>u
</p>
<p>b(n)
</p>
<p>)
&rarr; ϕ(β,ρ)(u) as n&rarr;&infin;,
</p>
<p>and, moreover, for some c &gt; 0 and γ &gt; 0 small enough, by, virtue of, say, (A7.1.23)
</p>
<p>and (A7.1.32), we have
</p>
<p>Re
(
ϕ(t)&minus; 1
</p>
<p>)
&le;&minus;cF0
</p>
<p>(
1
</p>
<p>|t |
</p>
<p>)
,
</p>
<p>and, for any ε &gt; 0 and all n large enough,
</p>
<p>nRe
</p>
<p>(
ϕ
</p>
<p>(
u
</p>
<p>b(n)
</p>
<p>)
&minus; 1
</p>
<p>)
&le;&minus;cnF0
</p>
<p>(
b(n)
</p>
<p>|u|
</p>
<p>)
&le;&minus;c |u|β&minus;ε.
</p>
<p>Here we used the properties of the r.v.f. F0. Moreover,
</p>
<p>ϕ̂
</p>
<p>(
u
</p>
<p>b(n)
</p>
<p>)
&rarr; 1 as n&rarr;&infin;,
</p>
<p>∣∣ϕ̂(u)/b(n)
∣∣&le; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 The Integro-Local and Local Limit Theorems 701
</p>
<p>The above also implies that, for all u such that |u|&lt; γb(n),
∣∣∣∣ϕ
</p>
<p>n
</p>
<p>(
u
</p>
<p>b(n)
</p>
<p>)∣∣∣∣&le; e
&minus;c |u|β&minus;ε . (A7.2.2)
</p>
<p>The obtained relations mean that we can use the dominated convergence theorem
</p>
<p>in (A7.2.1) which implies
</p>
<p>lim
n&rarr;&infin;
</p>
<p>b(n)I1 =
&int;
</p>
<p>e&minus;iuvϕ(β,ρ)(u) du (A7.2.3)
</p>
<p>uniformly in v, since the right-hand side of (A7.2.1) is uniformly continuous in v.
</p>
<p>On the right-hand side of (A7.2.3) is the result of the application of the inversion
</p>
<p>formula (up to the factor 1/2π ) to the ch.f. ϕ(α,ρ). This means that
</p>
<p>lim
n&rarr;&infin;
</p>
<p>b(n)I1 = 2πf (β,ρ)(v).
</p>
<p>We have established that, for x = vb(n), as n&rarr;&infin;,
</p>
<p>P
(
Zn &isin;∆[x)
</p>
<p>)
= ∆
</p>
<p>b(n)
f (β,ρ)
</p>
<p>(
x
</p>
<p>b(n)
</p>
<p>)
+ o
</p>
<p>(
1
</p>
<p>b(n)
</p>
<p>)
</p>
<p>uniformly in v (and hence in x).
</p>
<p>To prove the theorem it remains to use Lemma 8.7.1.
</p>
<p>The theorem is proved. �
</p>
<p>The proofs of the local Theorems 8.8.3 and 8.8.4 can be obtained by an obvious
</p>
<p>similar modification of the proofs of Theorems 8.7.2 and 8.7.3 under the conditions
</p>
<p>of Theorem 8.8.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 8
</p>
<p>Upper and Lower Bounds for the Distributions
of the Sums and the Maxima of the Sums
of Independent Random Variables
</p>
<p>Let ξ, ξ1, ξ2, . . . be independent identically distributed random variables,
</p>
<p>Sn =
n&sum;
</p>
<p>i=1
ξi, Sn = max
</p>
<p>1&le;k&le;n
Sk.
</p>
<p>The main goal of this appendix is to obtain upper and lower bounds for the proba-
</p>
<p>bilities P(Sn &ge; x) and P(Sn &ge; x). These bounds were used in Sect. 9.5 to find the
asymptotics of the probabilities of large deviations for Sn and Sn.
</p>
<p>8.1 Upper Bounds Under the Cram&eacute;r Condition
</p>
<p>In this section we will assume that the following one-sided Cram&eacute;r condition is met:
</p>
<p>[C] There exists a λ &gt; 0 such that
</p>
<p>ψ(λ)= Eeλξ &lt;&infin;. (A8.1.1)
</p>
<p>The following analogue of the exponential Chebyshev inequality holds true for
</p>
<p>P(Sn &ge; x).
</p>
<p>Theorem A8.1.1 For all n&ge; 1, x &ge; 0 and λ&ge; 0, we have
</p>
<p>P(Sn &ge; x)&le; e&minus;λx max
(
1,ψn(λ)
</p>
<p>)
. (A8.1.2)
</p>
<p>Proof As η(x) := inf{k &ge; 1 : Sk &ge; x} &le;&infin; is a Markov time, the event {η(x)= k}
is independent of the random variables Sn &minus; Sk . Therefore
</p>
<p>ψn(λ) = E eλSn &ge;
n&sum;
</p>
<p>k=1
E
(
eλSn;η(x)= k
</p>
<p>)
&ge;
</p>
<p>n&sum;
</p>
<p>k=1
E
(
eλ(x+Sn&minus;Sk);η(x)= k
</p>
<p>)
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>703</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>704 8 Bounds for Sums and Maximum of Sums
</p>
<p>= eλx
n&sum;
</p>
<p>k=1
ψn&minus;k(λ)P
</p>
<p>(
η(x)= k
</p>
<p>)
&ge; eλx min
</p>
<p>(
1,ψn(λ)
</p>
<p>)
P(Sn &ge; x).
</p>
<p>This immediately implies (A8.1.2). The theorem is proved. �
</p>
<p>If ψ(λ) &ge; 1 for λ &ge; 0 (this is always the case if there exists E ξ &ge; 0) then the
right-hand side of (A8.1.2) is equal to e&minus;λxψn(λ), and the equality (A8.1.2) itself
can also be obtained as a consequence of the well-known Kolmogorov&ndash;Doob in-
</p>
<p>equality for submartingales (see Theorem 15.3.4, where one has to put Xn := Sn).
Thus, if Eξ &ge; 0 then
</p>
<p>P(Sn &ge; x)&le; e&minus;λx+n lnψ(λ).
</p>
<p>Choosing the best possible value of λ we obtain the following inequality.
</p>
<p>Corollary A8.1.1 If Eξ &ge; 0 then, for all n&ge; 1 and x &ge; 0, we have
</p>
<p>P(Sn &ge; x)&le; e&minus;nΛ(α),
</p>
<p>where
</p>
<p>α := x
n
, Λ(α) := sup
</p>
<p>λ
</p>
<p>(
λα&minus; lnψ(λ)
</p>
<p>)
.
</p>
<p>The function Λ(α) is the rate function introduced in Sect. 9.1. Its basic proper-
</p>
<p>ties were stated in that section. In particular, for E ξ = 0 and E ξ2 = σ 2 &lt;&infin;, the
asymptotic equivalence Λ(α) &sim; α2
</p>
<p>2σ 2
as α &rarr; 0 takes place, which yields that, for
</p>
<p>x = o(n),
</p>
<p>P(Sn &ge; x)&le; exp
{
&minus; x
</p>
<p>2
</p>
<p>2nσ 2
</p>
<p>(
1 + o(1)
</p>
<p>)}
. (A8.1.3)
</p>
<p>8.2 Upper Bounds when the Cram&eacute;r Condition Is Not Met
</p>
<p>In this section we will assume that
</p>
<p>Eξ = 0, Eξ2 = σ 2 &lt;&infin;. (A8.2.1)
</p>
<p>For simplicity&rsquo;s sake, without losing generality, in what follows we will put σ = 1.
The bounds will be obtained for the deviation zone x &gt;
</p>
<p>&radic;
n which is adjacent to the
</p>
<p>zone of &ldquo;normal deviations&rdquo; where
</p>
<p>P(Sn &ge; x)&sim; 1 &minus;Φ
(
</p>
<p>x&radic;
n
</p>
<p>)
(A8.2.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Upper Bounds when the Cram&eacute;r Condition Is Not Met 705
</p>
<p>(uniformly in x &isin; (0,Nn
&radic;
n), where Nn &rarr; &infin; slowly enough as n &rarr; &infin;; see
</p>
<p>Sect. 8.2). Moreover, it was established in Sect. 19.1 that, in the normal deviations
</p>
<p>zone,
</p>
<p>P(Sn &ge; x)&sim; 2
(
</p>
<p>1 &minus;Φ
(
</p>
<p>x&radic;
n
</p>
<p>))
. (A8.2.3)
</p>
<p>To derive upper bounds in the zone x &gt;
&radic;
n when the Cram&eacute;r condition [C]
</p>
<p>is not met, we will need additional conditions on the behaviour of the right tail
</p>
<p>F+(t)= P(ξ &ge; t) of the distribution F.
Namely, we will assume that the following condition is satisfied.
</p>
<p>[&lt;] For the right tail F+(t) = P(ξ &ge; t) there exists a regularly varying (see
Appendix 6) majorant V (t):
</p>
<p>F+(t)&le; V (t) := t&minus;βL(t) for all t &gt; 0,
</p>
<p>where β &gt; 2 and L is a slowly varying function (s.v.f., see Appendix 6).
</p>
<p>By virtue of (A8.2.2) and (A8.2.3), for deviations x &lt; Nn
&radic;
n, n&rarr;&infin;, it would
</p>
<p>be natural to expect upper bounds with an exponential right-hand side e&minus;x
2/(2n)
</p>
<p>(cf. (A8.1.3)). On the other hand, Theorem A6.4.3(iii) of Appendix 6 implies that,
</p>
<p>for F+(t)= V (t) &isin;R and any fixed n we have, as x &rarr;&infin;,
</p>
<p>P(Sn &ge; x)&sim; nV (x). (A8.2.4)
</p>
<p>This relation clearly holds true if n&rarr;&infin; slowly enough (as x &rarr;&infin;).
The asymptotics (A8.2.2) and (A8.2.4) merge with each other remarkably as
</p>
<p>follows:
</p>
<p>P(Sn &ge; x)&sim;
(
</p>
<p>1 &minus;Φ
(
</p>
<p>x&radic;
n
</p>
<p>))
+ nV (x) (A8.2.5)
</p>
<p>as n &rarr; &infin; for all x &gt; &radic;n (for more details see, e.g., [8] and the bibliography
therein). Relation (A8.2.5) allows us to &ldquo;guess&rdquo; the threshold values of x = b(n)
for which asymptotics (A8.2.2) changes to asymptotics (A8.2.4). To find such x it
</p>
<p>suffices to equate the logarithms of the right-hand sides of (A8.2.2) and (A8.2.4):
</p>
<p>&minus;x
2
</p>
<p>2n
= lnnV (x)= lnn&minus; β lnx + o(lnx).
</p>
<p>The main part b(n) of the solution to this equation, as it is not hard to see, has the
</p>
<p>form
</p>
<p>b(n)=
&radic;
(β &minus; 2)n lnn
</p>
<p>(we exclude the trivial case n= 1).
In what follows, we will represent deviations x as x = sb(n). Based on the above,
</p>
<p>it is natural to expect (and it can be easily verified) that the first term will dominate</p>
<p/>
</div>
<div class="page"><p/>
<p>706 8 Bounds for Sums and Maximum of Sums
</p>
<p>on the right-hand side of (A8.2.5) if s &lt; 1, while the second will dominate if s &gt; 1.
</p>
<p>Accordingly, for small s (but such that x &gt;
&radic;
n), we will have the above-mentioned
</p>
<p>exponential bounds for P(Sn &ge; x), while for large s there will hold bounds of the
form nV (x) (note that nV (x)&rarr; 0 for x &gt; b(n) and β &gt; 2).
</p>
<p>The above claim is confirmed by the assertions below. Along with x introduce
</p>
<p>deviations
</p>
<p>y = x
r
,
</p>
<p>where r &ge; 1 is fixed, and put
</p>
<p>Bj := {ξj &lt; y}, B :=
n⋂
</p>
<p>j=1
Bj .
</p>
<p>Theorem A8.2.1 Let conditions (A8.2.1) and [&lt;] be satisfied.
(1) For any fixed h &gt; 1, s0 &gt; 0, x = sb(n), s &ge; s0 and all Π := nV (x) small
</p>
<p>enough, we have
</p>
<p>P := P(Sn &ge; x;B)&le; er
(
Π(y)
</p>
<p>r
</p>
<p>)r&minus;θ
, (A8.2.6)
</p>
<p>where
</p>
<p>Π(y) := nV (y), θ := hr
2
</p>
<p>4s2
</p>
<p>(
1 + b ln s
</p>
<p>lnn
</p>
<p>)
, b := 2β
</p>
<p>β &minus; 2 .
</p>
<p>(2) For any fixed h &gt; 1, τ &gt; 0, for x = sb(n) &gt;&radic;n, s2 &lt; (h&minus; τ)/2, and all n
large enough, we have
</p>
<p>P &le; e&minus;x2/(2nh). (A8.2.7)
</p>
<p>Corollary A8.2.1 (a) If s &rarr;&infin; then
</p>
<p>P(Sn &ge; x)&le; nV (x)
(
1 + o(1)
</p>
<p>)
. (A8.2.8)
</p>
<p>(b) If s2 &ge; s20 for some fixed s0 &gt; 1 then, for all nV (x) small enough,
</p>
<p>P(Sn &ge; x)&le; cnV (x), c= const. (A8.2.9)
</p>
<p>(c) For any fixed h &gt; 1, τ &gt; 0, for s2 &lt; (h&minus; τ )/2, x &gt; &radic;n, and all n large
enough,
</p>
<p>P(Sn &ge; x)&le; e&minus;x
2/(2nh). (A8.2.10)
</p>
<p>Remark A8.2.1 It is not hard to verify (see the proofs of Theorem A8.2.1 and Corol-
lary A8.2.1) that there exists a function ε(t) &darr; 0 as t &uarr;&infin; such that one has, along</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Upper Bounds when the Cram&eacute;r Condition Is Not Met 707
</p>
<p>with (A8.2.8), the relation
</p>
<p>sup
x:s&ge;t
</p>
<p>P(Sn &gt; x)
</p>
<p>nV (x)
&le; 1 + ε(t).
</p>
<p>Proof of Corollary A8.2.1 The proof is based on the inequality
</p>
<p>P(Sn &gt; x)&le; P(B)+ P(Sn &ge; x; B)&le; nV (y)+ P. (A8.2.11)
</p>
<p>Since θ &rarr; 0 as s &rarr;&infin;, we see that, for any fixed ε &gt; 0 and all Π = nV (x) small
enough, we have P &le; c(nV (y))r&minus;ε . Putting r := 1 + 2ε, we obtain from (A8.2.11)
and (A8.2.6) that
</p>
<p>P(Sn &ge; x)&le; nV (y)+ c
(
nV (y)
</p>
<p>)1+ε &sim; n(1 + 2ε)&minus;βV (x).
</p>
<p>Since the left-hand side of this inequality does not depend on ε, relation (A8.2.8)
</p>
<p>follows.
</p>
<p>We now prove (b). If s &rarr; &infin; then (b) follows from (a). If s is bounded then
necessarily n&rarr;&infin; (since nV (x)&rarr; 0) and hence
</p>
<p>r &minus; θ = r &minus; hr
2
</p>
<p>4s2
</p>
<p>(
1 + b ln s
</p>
<p>lnn
</p>
<p>)
=ψ(r, s)+ o(1),
</p>
<p>where the function
</p>
<p>ψ(r, s) := r &minus; hr
2
</p>
<p>4s2
</p>
<p>attains its maximum ψ(r0, s)= s2/h in r at the point r0 = 2s2/h. Moreover, ψ(r, s)
strictly decreases in s. Therefore, for r0 = 2s2/h, we obtain
</p>
<p>ψ(r0, s) =
s2
</p>
<p>h
, (A8.2.12)
</p>
<p>r0 &minus; θ &ge;
s2
</p>
<p>h
+ o(1) as n&rarr;&infin;. (A8.2.13)
</p>
<p>Choose h so close to 1 and τ &gt; 0 so small that h + τ &le; s20 . Putting r := r0, for
s2 &ge; s20 &ge; h+ τ and as n&rarr;&infin;, we get from (A8.2.6), (A8.2.12) and (A8.2.13) that
</p>
<p>P(Sn &ge; x)&le; nV (y)+ c
(
nV (y)
</p>
<p>)1+τ/2 &sim; nV
(
x
</p>
<p>r0
</p>
<p>)
&sim; rβ0 nV (x).
</p>
<p>This proves (b).
</p>
<p>Relation (c) for y = x follows from the inequality (see (A8.2.7) and (A8.2.11))
</p>
<p>P(Sn &ge; x)&le; nV (x)+ e&minus;x
2/(2nh), (A8.2.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>708 8 Bounds for Sums and Maximum of Sums
</p>
<p>where, for s2 &lt; (h&minus; τ )/2,
</p>
<p>e&minus;x
2(2nh) &gt; exp
</p>
<p>{
&minus; (h&minus; τ)
</p>
<p>2
</p>
<p>(β &minus; 2)n lnn
2nh
</p>
<p>}
&gt; n&minus;(β&minus;2)/4.
</p>
<p>On the other hand, we have x &gt;
&radic;
n,
</p>
<p>nV (x)&le; nV (
&radic;
n)= n&minus;(β&minus;2)/2L&lowast;(n),
</p>
<p>where L&lowast; is a s.v.f. Therefore the second term dominates on the right-hand side
of (A8.2.14). Slightly changing h if necessary, we obtain (c). Corollary A8.2.1 is
</p>
<p>proved. �
</p>
<p>Remark A8.2.2 One can see from the proof of the corollary that the main contribu-
tion to the bound for the probability P(Sn &ge; x) under the conditions of assertions
(a) and (b) comes from the event B = {maxj&le;n ξj &ge; y} with y close to x, so that
the most probable trajectory of {Sk}nk=1 that reaches the level x contains at least one
jump ξj of size comparable to x.
</p>
<p>Proof of Theorem A8.2.1 In our case, the Cram&eacute;r condition [C] is not met. In order
to use Theorem A8.1.1 in such a situation, we introduce &ldquo;truncated&rdquo; random vari-
</p>
<p>ables with distributions that coincide with the conditional distribution of ξ given
</p>
<p>{ξ &lt; y} for some level y the choice of which will be at our disposal. Namely, we
introduce independent identically distributed random variables ξ
</p>
<p>(y)
j , j = 1,2, . . . ,
</p>
<p>with the distribution function
</p>
<p>P
(
ξ
(y)
</p>
<p>j &lt; t
)
= P(ξ &lt; t | ξ &lt; y)= P(ξ &lt; t)
</p>
<p>P(ξ &lt; y)
, t &le; y,
</p>
<p>and put
</p>
<p>S
(y)
n :=
</p>
<p>n&sum;
</p>
<p>j=1
ξ
(y)
</p>
<p>j , S
(y)
</p>
<p>n := max
k&le;n
</p>
<p>S
(y)
</p>
<p>k .
</p>
<p>Then
</p>
<p>P = P(Sn &ge; x,B)=
(
P(ξ &lt; y)
</p>
<p>)n
P
(
S
(y)
</p>
<p>n &ge; x
)
. (A8.2.15)
</p>
<p>Applying Theorem A8.1.1 to the variables ξ
(y)
j , we obtain that, for any λ&ge; 0,
</p>
<p>P
(
S
(y)
</p>
<p>n &ge; x
)
&le; e&minus;λx
</p>
<p>[
max
</p>
<p>{
1,E eλξ
</p>
<p>(y)}]n
.
</p>
<p>Since
</p>
<p>Eeλξ
(y) = R(λ,y)
</p>
<p>F (y)
, where R(λ,y) :=
</p>
<p>&int; y
</p>
<p>&infin;
eλtF(dt),
</p>
<p>we arrive at the following basic inequality. For x, y, λ&ge; 0,
</p>
<p>P = P(Sn &ge; x,B) &le; e&minus;λx
[
max
</p>
<p>{
P(ξ &lt; y),R(λ, y)
</p>
<p>}]n</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Upper Bounds when the Cram&eacute;r Condition Is Not Met 709
</p>
<p>&le; e&minus;λx max
{
1,Rn(λ, y)
</p>
<p>}
. (A8.2.16)
</p>
<p>Thus, the main problem is to bound the integral R(λ,y). Put
</p>
<p>M(v) := v
λ
</p>
<p>and represent R(λ,y) as
</p>
<p>R(λ,y)= I1 + I2,
where, for a fixed ε &gt; 0,
</p>
<p>I1 :=
&int; M(ε)
</p>
<p>&minus;&infin;
eλtF(dt)=
</p>
<p>&int; M(ε)
</p>
<p>&minus;&infin;
</p>
<p>(
1 + λ t + λ
</p>
<p>2t2
</p>
<p>2
eλθ(t)
</p>
<p>)
F(dt), 0 &le; θ(t)
</p>
<p>t
&le; 1.
</p>
<p>(A8.2.17)
</p>
<p>Here
</p>
<p>&int; M(ε)
</p>
<p>&minus;&infin;
F(dt) = 1 &minus; V
</p>
<p>(
M(ε)
</p>
<p>)
&le; 1,
</p>
<p>&int; M(ε)
</p>
<p>&minus;&infin;
tF(dt) = &minus;
</p>
<p>&int; &infin;
</p>
<p>M(ε)
</p>
<p>tF(dt)&le; 0, (A8.2.18)
</p>
<p>&int; M(ε)
</p>
<p>&minus;&infin;
t2eλθ(t)F(dt) &le; eε
</p>
<p>&int; M(ε)
</p>
<p>&minus;&infin;
t2F(dt)&le; eε =: h. (A8.2.19)
</p>
<p>Therefore,
</p>
<p>I1 &le; 1 +
λ2h
</p>
<p>2
. (A8.2.20)
</p>
<p>Estimate now
</p>
<p>I2 := &minus;
&int; y
</p>
<p>M(ε)
</p>
<p>eλt dF+(t)&le; V
(
M(ε)
</p>
<p>)
eε + λ
</p>
<p>&int; y
</p>
<p>M(ε)
</p>
<p>V (t)eλt dt. (A8.2.21)
</p>
<p>First consider, for M(ε) &lt;M(2β) &lt; y, the subintegral
</p>
<p>I2,1 := λ
&int; M(2β)
</p>
<p>M(ε)
</p>
<p>V (t)eλt dt.
</p>
<p>For t = v/λ, as λ&rarr; 0, we have
</p>
<p>V (t)eλt = V
(
v
</p>
<p>λ
</p>
<p>)
ev &sim; V
</p>
<p>(
1
</p>
<p>λ
</p>
<p>)
f (v), (A8.2.22)
</p>
<p>where the function
</p>
<p>f (v) := v&minus;βev</p>
<p/>
</div>
<div class="page"><p/>
<p>710 8 Bounds for Sums and Maximum of Sums
</p>
<p>is convex on (0,&infin;). Therefore
</p>
<p>I2,1 &le;
λ
</p>
<p>2
</p>
<p>(
M(2β)&minus;M(ε)
</p>
<p>)
V
</p>
<p>(
1
</p>
<p>λ
</p>
<p>)(
f (ε)+f (2β)
</p>
<p>)(
1+o(1)
</p>
<p>)
&le; cV
</p>
<p>(
1
</p>
<p>λ
</p>
<p>)
. (A8.2.23)
</p>
<p>We now proceed to estimating the remaining subintegral
</p>
<p>I2,2 := λ
&int; y
</p>
<p>M(2β)
</p>
<p>V (t)eλt dt.
</p>
<p>For brevity&rsquo;s sake, put M(2β)=:M . We will choose λ so that
</p>
<p>&micro;= λy &rarr;&infin; (y ≫ 1/λ) (A8.2.24)
</p>
<p>as x &rarr;&infin;. Substituting the variable (y &minus; t)λ=: u we obtain
</p>
<p>λI2,2 = eλyV (y)
&int; (y&minus;M)λ
</p>
<p>0
</p>
<p>V
</p>
<p>(
y &minus; u
</p>
<p>λ
</p>
<p>)
V &minus;1(y)e&minus;u du. (A8.2.25)
</p>
<p>Consider the integral on the right-hand side of (A8.2.25). Since 1/λ≪ y, the inte-
grand
</p>
<p>ry,λ(u) :=
V (y &minus; u/λ)
</p>
<p>V (y)
</p>
<p>converges to 1 for each fixed u. In order to use the dominated convergence theorem
</p>
<p>which implies that the integral on the right-hand side of (A8.2.25) converges, as
</p>
<p>y &rarr;&infin;, to
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;u du= 1, (A8.2.26)
</p>
<p>it remains to estimate the growth rate of the function ry,λ(u) as u increases. By the
</p>
<p>properties of r.v.f.s (see Theorem A6.2.1(iii) in Appendix 6), for all λ small enough
</p>
<p>(or M large enough; recall that y&minus;u/λ&ge;M in the integrand in (A8.2.25)), we have
</p>
<p>ry,λ(u)&le;
(
</p>
<p>1 &minus; u
λy
</p>
<p>)&minus;2β/2
=: g(u).
</p>
<p>Since g(0)= 1 and λy &minus; u&ge;Mλ= 2β , in this domain
</p>
<p>(
lng(u)
</p>
<p>)&prime; = 3β
2(λy &minus; u) &le;
</p>
<p>3β
</p>
<p>4β
= 3
</p>
<p>4
,
</p>
<p>lng(u) &le; 3u
4
, ry,λ(u)&le; e3u/4.
</p>
<p>This means that the integrand in (A8.2.25) is dominated by the exponential e&minus;u/4,
and the use of the dominated convergence theorem is justified. Therefore, due to the</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Upper Bounds when the Cram&eacute;r Condition Is Not Met 711
</p>
<p>convergence of the integral in (A8.2.25) to the limit (A8.2.26), we obtain
</p>
<p>λI2,2 &sim; eλyV (y)
&int; &infin;
</p>
<p>0
</p>
<p>eu du= e&micro;V (y),
</p>
<p>and it is not hard to find a function ε(&micro;) &darr; 0 as &micro; &uarr;&infin; such that
</p>
<p>λI2,2 &le; eλyV (y)
(
1 + ε(&micro;)
</p>
<p>)
. (A8.2.27)
</p>
<p>Summarising (A8.2.20)&ndash;(A8.2.23) and (A8.2.27), we obtain
</p>
<p>R(λ,y) &le; 1 + λ
2h
</p>
<p>2
+ cV
</p>
<p>(
1
</p>
<p>λ
</p>
<p>)
+ V (y)eλy
</p>
<p>(
1 + ε(&micro;)
</p>
<p>)
, (A8.2.28)
</p>
<p>Rn(λ, y) &le; exp
{
nλ2h
</p>
<p>2
+ cnV
</p>
<p>(
1
</p>
<p>λ
</p>
<p>)
+ nV (y)eλy
</p>
<p>(
1 + ε(&micro;)
</p>
<p>)}
. (A8.2.29)
</p>
<p>First take λ to be the value
</p>
<p>λ= 1
y
</p>
<p>lnT
</p>
<p>that &ldquo;almost minimises&rdquo; the function &minus;λx + nV (y)eλy , where T := r
nV (y)
</p>
<p>, so
</p>
<p>that &micro; = lnT . Note that, for such a choice of &micro; (or of λ = y&minus;1 ln(r/Π(y))), for
Π(y)&rarr; 0 we have that &micro;= λy &sim;&minus; lnΠ(y)&rarr;&infin; and hence that the assumption
y ≫ 1/λ we made in (A8.2.24) holds true. For such λ,
</p>
<p>Rn(λ, y)&le; exp
{
nλ2h
</p>
<p>2
+ cnV
</p>
<p>(
1
</p>
<p>λ
</p>
<p>)
+ r
</p>
<p>(
1 + ε(&micro;)
</p>
<p>)}
, (A8.2.30)
</p>
<p>where, by the properties of r.v.f.s,
</p>
<p>nV
</p>
<p>(
1
</p>
<p>&micro;
</p>
<p>)
&sim; nV
</p>
<p>(
y
</p>
<p>lnT
</p>
<p>)
&sim; cnV
</p>
<p>(
y
</p>
<p>| lnnV (y)|
</p>
<p>)
&le; cnV (y)
</p>
<p>∣∣lnnV (y)
∣∣β+δ &rarr; 0,
</p>
<p>δ &gt; 0, (A8.2.31)
</p>
<p>as nV (y)&rarr; 0. Therefore
</p>
<p>lnP &le; &minus;r lnT + r + nh
2y2
</p>
<p>ln2 T + ε1(T )
</p>
<p>=
[
&minus;r + nh
</p>
<p>2y2
lnT
</p>
<p>]
lnT + r + ε1(T ), (A8.2.32)
</p>
<p>where ε1(T ) &darr; 0 as T &uarr;&infin;. If x = sb(n), b(n) =
&radic;
(β &minus; 2)n lnn, and nV (x)&rarr; 0
</p>
<p>then
</p>
<p>lnT = &minus; lnnV (x)+O(1)=&minus; lnn+ β ln s + β
2
</p>
<p>lnn+O
(
lnL
</p>
<p>(
sσ (n)
</p>
<p>))
+O(1)</p>
<p/>
</div>
<div class="page"><p/>
<p>712 8 Bounds for Sums and Maximum of Sums
</p>
<p>= β &minus; 2
2
</p>
<p>lnn
</p>
<p>[
1 + b ln s
</p>
<p>lnn
</p>
<p>](
1 + o(1)
</p>
<p>)
, (A8.2.33)
</p>
<p>where b= 2β
β&minus;2 (the term o(1) in the last equality appears because in our case either
</p>
<p>n&rarr;&infin; or s &rarr;&infin;.) Hence, by (A8.2.32),
</p>
<p>nh
</p>
<p>2y2
lnT = hr
</p>
<p>2
</p>
<p>4s2
</p>
<p>[
1 + b ln s
</p>
<p>lnn
</p>
<p>](
1 + o(1)
</p>
<p>)
,
</p>
<p>lnP &le; r &minus;
[
r &minus; h
</p>
<p>&prime;r2
</p>
<p>4s2
</p>
<p>(
1 + b ln s
</p>
<p>lnn
</p>
<p>)]
lnT
</p>
<p>for any h&prime; &gt; h &gt; 1 and nV (x) small enough. This proves the first assertion of the
theorem.
</p>
<p>We now prove the second assertion of the theorem for &ldquo;small&rdquo; values of s such
</p>
<p>that, for some τ &gt; 0,
</p>
<p>s2 &lt;
h&minus; τ
</p>
<p>2
.
</p>
<p>Since we always assume that x &gt;
&radic;
n, we also have
</p>
<p>s = x
b(n)
</p>
<p>&gt;
1&radic;
</p>
<p>(β &minus; 2) lnn
</p>
<p>and we can assume that s2 &ge; n&minus;γ for some γ &gt; 0 to be chosen below. This corre-
sponds to the following domain of the values of x2:
</p>
<p>cn1&minus;γ lnn &lt; x2 &lt;
(h&minus; τ)(β &minus; 2)
</p>
<p>2
n lnn. (A8.2.34)
</p>
<p>For such x, as will be shown below, the main contribution to the exponent on the
</p>
<p>right-hand side of (A8.2.29) comes from the quadratic term nλ2h/2, and we will set
</p>
<p>λ := x
nh
</p>
<p>.
</p>
<p>Then, for y = x (r = 1, &micro;= x2/(nh)),
</p>
<p>lnP &le; &minus;λx + nλ
2h
</p>
<p>2
+ cnV
</p>
<p>(
1
</p>
<p>λ
</p>
<p>)
+ nV (y)eλy
</p>
<p>(
1 + ε(&micro;)
</p>
<p>)
</p>
<p>= &minus; x
2
</p>
<p>2nh
+ cnV
</p>
<p>(
nh
</p>
<p>x
</p>
<p>)
+ nV (x)e x
</p>
<p>2
</p>
<p>nh
(
1 + ε(&micro;)
</p>
<p>)
. (A8.2.35)
</p>
<p>We show that the last two terms on the right-hand side are negligibly small as
</p>
<p>n&rarr;&infin;. Indeed, by the second inequality in (A8.2.34),
</p>
<p>nV
</p>
<p>(
nh
</p>
<p>x
</p>
<p>)
&le; cnV
</p>
<p>(&radic;
n
</p>
<p>lnn
</p>
<p>)
&rarr; 0 as n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Lower Bounds 713
</p>
<p>Further, by the first inequality in (A8.2.34),
</p>
<p>nV (x)&le; n(2&minus;β)/2+γ &prime; ,
</p>
<p>where we can choose γ &prime;. Moreover, by (A8.2.34),
</p>
<p>x2
</p>
<p>nh
&le; (h&minus; τ)(β &minus; 2) lnn
</p>
<p>2h
=
[
β &minus; 2
</p>
<p>2
&minus; τ(β &minus; 2)
</p>
<p>2h
</p>
<p>]
lnn.
</p>
<p>Therefore
</p>
<p>nV (x)ex
2/(nh) &le; n&minus;τ(β&minus;2)/(2h)+γ &prime; &rarr; 0
</p>
<p>for γ &prime; &lt; τ(β&minus;2)
2h
</p>
<p>as n&rarr;&infin;.
Thus,
</p>
<p>lnP &le;&minus; x
2
</p>
<p>2nh
+ o(1).
</p>
<p>Since x2/n &gt; 1, the term o(1) in the last relation can be omitted by slightly chang-
</p>
<p>ing h &gt; 1. (Formally, we proved that, for any h &gt; 1 and all n large enough, inequal-
</p>
<p>ity (A8.2.7) is valid with the h on its right-hand side replaced with h&prime; &gt; h, where we
can take, for instance, h&prime; = h+ (h&minus; 1)/2. Since h&prime; &gt; 1 can also be made arbitrarily
close to 1 by the choice of h, the obtained relation is equivalent to the one from
</p>
<p>Theorem A8.2.1.) This proves (A8.2.7).
</p>
<p>The theorem is proved. �
</p>
<p>Comparing the assertions of Theorem A8.2.1 and Corollary A8.1.1, we see that,
</p>
<p>roughly speaking, for s &lt; 1/2 and for s &gt; 1 one can obtain quite satisfactory and, in
</p>
<p>a certain sense, unimprovable upper bounds for the probabilities P and P(Sn &gt; x).
</p>
<p>8.3 Lower Bounds
</p>
<p>In this section we will again assume that conditions (A8.2.1) are satisfied. The lower
</p>
<p>bounds for P(Sn &ge; x) (they will clearly hold for P(Sn&ge;x) as well) can be obtained
in a much simpler way than the upper bounds and need essentially no assumptions.
</p>
<p>Theorem A8.3.1 Let E ξj = 0 and Eξ2j = 1. Then, for y = x + t
&radic;
n&minus; 1,
</p>
<p>P(Sn &ge; x)&ge; nF+(y)
[
</p>
<p>1 &minus; t&minus;2 &minus; n&minus; 1
2
</p>
<p>F+(y)
</p>
<p>]
. (A8.3.1)
</p>
<p>Proof Put Gn := {Sn &ge; x} and Bj := {ξj &lt; y}. Then
</p>
<p>P(Sn &ge; x) &ge; P
(
Gn;
</p>
<p>n⋃
</p>
<p>j=1
Bj
</p>
<p>)
&ge;
</p>
<p>n&sum;
</p>
<p>j=1
P(GnBj )&minus;
</p>
<p>&sum;
</p>
<p>i&lt;j&le;n
P(GnBiBj )</p>
<p/>
</div>
<div class="page"><p/>
<p>714 8 Bounds for Sums and Maximum of Sums
</p>
<p>&ge;
n&sum;
</p>
<p>j=1
P(GnBj )&minus;
</p>
<p>n(n&minus; 1)
2
</p>
<p>F 2+(y).
</p>
<p>Here, for y = x + t
&radic;
n&minus; 1,
</p>
<p>P(GnBj ) =
&int; &infin;
</p>
<p>y
</p>
<p>P(Sn&minus;1 &ge; x &minus; u)F(du)&ge; P(Sn&minus;1 &ge; x &minus; y)F+(y)
</p>
<p>= P(Sn&minus;1 &ge;&minus;t
&radic;
n&minus; 1 )F+(y)=
</p>
<p>(
1 &minus; P(Sn&minus;1 &lt;&minus;t
</p>
<p>&radic;
n&minus; 1 )
</p>
<p>)
F+(t),
</p>
<p>where, by the Chebyshev inequality,
</p>
<p>P(Sn&minus;1 &lt;&minus;t
&radic;
n&minus; 1 )&le; t&minus;2.
</p>
<p>As a result we get
</p>
<p>P(Sn &ge; x)&ge; nF+(t)
(
1 &minus; t&minus;2
</p>
<p>)
&minus; n(n&minus; 1)
</p>
<p>2
F 2+(t),
</p>
<p>which is equivalent to (A8.3.1).
</p>
<p>The theorem is proved. �
</p>
<p>Corollary A8.3.1 If x &rarr;&infin; and x ≫&radic;n then, as t &rarr;&infin;,
</p>
<p>P(Sn &ge; x)&ge; nF+(y)
(
1 + o(1)
</p>
<p>)
. (A8.3.2)
</p>
<p>If, moreover, F+(u)&ge; V (u) &isin;R then
</p>
<p>P(Sn &ge; x)&ge; nV (x)
(
1 + o(1)
</p>
<p>)
.
</p>
<p>Proof Since y &ge; x, we have
</p>
<p>nF+(y)≪ ny&minus;2 &lt; nx&minus;2 = o(1).
</p>
<p>This together with (A8.3.1) implies the first assertion of the corollary as t &rarr;&infin;. To
obtain the second one, in (A8.3.2) one should take t &rarr;&infin; such that t = o(x/&radic;n).
Then y &sim; x and V (y)&sim; V (x).
</p>
<p>The corollary is proved. �</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 9
</p>
<p>Renewal Theorems
</p>
<p>The main goal of the present section is to prove Theorem 10.4.1, the key renewal
</p>
<p>theorem in the non-arithmetic case (in the terminology of Chap. 10). We will also
</p>
<p>consider some refinements and extensions of the theorem.
</p>
<p>First consider positive independent identically distributed random variables
</p>
<p>τj
d= τ with distribution function F and finite mean a := Eτ &lt;&infin;. Here it will be
</p>
<p>more convenient to understand by the renewal function its left-continuous version
</p>
<p>H(t) :=
&infin;&sum;
</p>
<p>k=0
F &lowast;k(t), t &ge; 0,
</p>
<p>where F &lowast;k is the k-fold convolution of the distribution F with itself, which is the
distribution function of the sum Tk = τ1 + &middot; &middot; &middot;+ τk . We first prove the following key
assertion.
</p>
<p>Theorem A9.1 If g is a directly integrable function and τj are non-arithmetic (see
Chap. 10) then, as t &rarr;&infin;,
</p>
<p>&int; t
</p>
<p>0
</p>
<p>g(t &minus; u)dH(u)&rarr; 1
a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>g(u)du.
</p>
<p>The proof of the theorem mostly follows the argument suggested in [13] and will
</p>
<p>need several auxiliary assertions.
</p>
<p>Lemma A9.1 Let g be a bounded measurable function. The integral
</p>
<p>G(t)=
&int; t
</p>
<p>0
</p>
<p>g(t &minus; u)dH(u)=: g &lowast;H(t) (A9.1)
</p>
<p>is the unique solution of the equation
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>715</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>716 9 Renewal Theorems
</p>
<p>G(t)= g(t)+
&int; t
</p>
<p>0
</p>
<p>G(t &minus; u)dF (u)= g(t)+G &lowast; F(t) (A9.2)
</p>
<p>in the class of functions bounded on finite intervals.
The function G=H is the solution of (A9.2) when g &equiv; 1. The function G&equiv; 1 is
</p>
<p>the solution of (A9.2) when g = 1 &minus; F .
</p>
<p>Equation (A9.2) is called the renewal equation.
As we already noted in Theorem 10.4.1, one can associate, in an obvious way,
</p>
<p>measures H and F with the functions H and F , and write the integrals in (A9.1) and
</p>
<p>(A9.2) as integrals with respect to the measures:
</p>
<p>&int; t
</p>
<p>0
</p>
<p>g(t &minus; u)H(du) and
&int; t
</p>
<p>0
</p>
<p>G(t &minus; u)F(du),
</p>
<p>respectively.
</p>
<p>Proof of Lemma A9.1 Put
</p>
<p>Hn(t) :=
n&sum;
</p>
<p>k=0
F &lowast;k(t).
</p>
<p>The functions Gn = g &lowast;Hn satisfy the equation Gn+1 = g +Gn &lowast; F and form an
increasing sequence Gn &uarr; which is bounded by Lemma 10.2.3. Therefore Gn &uarr;G,
and passing to the limit in the equation for Gn we obtain that G satisfies (A9.1).
</p>
<p>To prove uniqueness note that the difference V =G(1) &minus;G(2) of two solutions G(1)
and G(2) must satisfy the homogeneous equation V = V &lowast; F and therefore also the
relations V = V &lowast; (F k&lowast;) or, which is the same,
</p>
<p>V (t)=
&int; t
</p>
<p>0
</p>
<p>V (t &minus; u)dF &lowast;k(u).
</p>
<p>But F &lowast;k(u)&rarr; 0 as k &rarr;&infin; for u &isin; [0, t]. Since by the assumption |V (u)| &lt; c on
[0, t], we have V (t)&rarr; 0 as k&rarr;&infin;. But V does not depend on k, so that V (t)&equiv; 0.
The last assertion of the lemma can be verified directly. The lemma is proved. �
</p>
<p>Note that if we considered functions g of bounded variation, the assertion of
</p>
<p>Lemma A9.1 would immediately follow from the equation for the Laplace&ndash;Stieltjes
</p>
<p>transform G̃(λ)=
&int;&infin;
</p>
<p>0 e
&minus;λt dG(t) of G which follows from (A9.2):
</p>
<p>G̃(λ)= g̃(λ)+ G̃(λ)ψ(λ), (A9.3)
</p>
<p>where
</p>
<p>g̃(λ) :=
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;λt dg(t), ψ(λ) :=
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;λt dF(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>9 Renewal Theorems 717
</p>
<p>Indeed, it follows from (A9.3) that
</p>
<p>G̃(λ)= g̃(λ)
1 &minus;ψ(λ) ,
</p>
<p>which is equivalent to (A9.1).
</p>
<p>A point t is said to be a point of growth of the distribution function F provided
that F(t + ε)&minus; F(t) &gt; 0 for any ε &gt; 0.
</p>
<p>Lemma A9.2 Let the distribution F be non-arithmetic and Z be the set of all points
of growth of H , i.e. points of growth of the functions F,F &lowast;2,F &lowast;3, . . . . Then Z is
&ldquo;asymptotically dense at infinity&rdquo;, i.e., for any given ε &gt; 0 and all x large enough,
the intersection (x, x + ε)&cap;Z is non-empty.
</p>
<p>Proof Observe first that if t1 is a point of growth of the distribution F1 of a random
variable τ , and t2 is a point of growth of the distribution F2 of a random variable ζ
</p>
<p>which is independent of τ , then t = t1 + t2 will be a point of growth of the distribu-
tion F1 &lowast; F2 of the variable τ + ζ . Indeed,
</p>
<p>P(t &le; τ + ζ &lt; t + ε)&ge; P
(
t1 &le; τ &lt; t1 +
</p>
<p>ε
</p>
<p>2
</p>
<p>)
P
</p>
<p>(
t2 &le; ζ &lt; t2 +
</p>
<p>ε
</p>
<p>2
</p>
<p>)
.
</p>
<p>Let, further, x &lt; y be two points of the set Z, and ∆ := y &minus; x. The following
alternative takes place: either
</p>
<p>(1) for any ε &gt; 0 there exist x and y such that ∆&lt; ε, or
</p>
<p>(2) there exists a δ &gt; 0 such that ∆&ge; δ for all x and y from Z.
Put In := [xn,yn]. If n∆ &gt; x then that interval contains [nx, (n + 1)x] as a
</p>
<p>subset, and therefore any point v &gt; v0 = x2/∆ belongs to at least one of the intervals
I1, I2, . . . .
</p>
<p>By virtue of the above observation, the n+1 points nx+k∆= (n&minus;k)x+ky, k =
0, . . . , n, belong to Z and divide In into n subintervals of length ∆. This means that,
</p>
<p>for any point v &gt; v0, the distance between v and the points from Z is at most ∆/2.
</p>
<p>This implies the assertion of the lemma when (1) holds.
</p>
<p>If (2) is true, we can assume that x and y are chosen so that ∆ &lt; 2δ. Then the
</p>
<p>points of the form nx + k∆ exhaust all the points from Z lying inside In. Since the
point (n + 1)x is among these points, the value x is a multiple of ∆, and all the
points of Z lying inside In are multiples of ∆. Now let z be an arbitrary point of
</p>
<p>growth of F . For sufficiently large n, the interval In contains a point of the form
</p>
<p>z+ k∆, and since the latter belongs to Z, the value z is also a multiple of ∆. Thus
F is an arithmetic distribution, so that case (2) cannot take place. The lemma is
</p>
<p>proved. �
</p>
<p>Lemma A9.3 Let q(x) be a bounded uniformly continuous function given on
(&minus;&infin;,&infin;) such that, for all x, q(x)&le; q(0) for all x, and
</p>
<p>q(x)=
&int; &infin;
</p>
<p>0
</p>
<p>q(x &minus; y)dF (y). (A9.4)
</p>
<p>Then q(x)&equiv; q(0).</p>
<p/>
</div>
<div class="page"><p/>
<p>718 9 Renewal Theorems
</p>
<p>Proof Equation (A9.4) means that q = q &lowast; F = &middot; &middot; &middot; = q &lowast; F &lowast;k for all k &ge; 1. The
right-hand side of (A9.4) does not exceed q(0), and hence, for x = 0, the equality
(A9.4) is only possible if q(&minus;y)= q(0) for all y &isin; Zk , where Zk is the set of points
of growth of F &lowast;k , and therefore q(&minus;y)= q(0) for all y &isin; Z. By Lemma A9.2 and
the uniform continuity of q this means that q(&minus;y) &rarr; q(0) as y &rarr; &infin;. Further,
for an arbitrarily large N we can choose k such that q(x) will be arbitrarily close
</p>
<p>to
&int;&infin;
N
</p>
<p>q(x &minus; y)dF &lowast;k(y), since F &lowast;k(N)&rarr; 0 as k &rarr;&infin;. This means, in turn, that
q(x) will be close to q(0). Since q(x) depends neither on N nor on k, we have
</p>
<p>q(x)= q(0). The lemma is proved. �
</p>
<p>LemmaA9.4 Let g be a continuous function vanishing outside segment [0, b]. Then
the solution G of the renewal equation (A9.2) is uniformly continuous and, for
any u,
</p>
<p>G(x + u)&minus;G(x)&rarr; 0 (A9.5)
</p>
<p>as x &rarr;&infin;.
</p>
<p>Proof By virtue of Lemma 10.2.3,
</p>
<p>∣∣G(x + δ)&minus;G(x)
∣∣ =
</p>
<p>∣∣∣∣
&int; x+δ
</p>
<p>x&minus;b
</p>
<p>(
g(x + δ&minus; y)&minus; g(x &minus; y)
</p>
<p>)
dH(y)
</p>
<p>∣∣∣∣
</p>
<p>&le; max
0&le;x&le;b+δ
</p>
<p>∣∣g(x + δ)&minus; g(x)
∣∣(c1 + c2(b+ δ)
</p>
<p>)
. (A9.6)
</p>
<p>This means that the uniform continuity of g implies that of G.
</p>
<p>Now assume that g has a continuous derivative g&prime;. Then G&prime; exists and satisfies
the renewal equation
</p>
<p>G&prime;(x)= g&prime;(x)+
&int; x
</p>
<p>0
</p>
<p>G&prime;(x &minus; y)dF (y).
</p>
<p>Therefore the derivative G&prime; is bounded and uniformly continuous. Let
</p>
<p>lim sup
x&rarr;&infin;
</p>
<p>G&prime;(x)= s.
</p>
<p>Choose a sequence tn &rarr;&infin; such that G&prime;(tn)&rarr; s. The family of functions qn de-
fined by the equalities
</p>
<p>qn(x)=G&prime;(tn + x)
</p>
<p>is equicontinuous, and
</p>
<p>qn(x)= g&prime;(tn+ x)+
&int; x+tn
</p>
<p>0
</p>
<p>qn(x&minus; y)dF (y)= g&prime;(tn+ x)+
&int; &infin;
</p>
<p>0
</p>
<p>qn(x&minus; y)dF (y).
(A9.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>9 Renewal Theorems 719
</p>
<p>By the Arzel&agrave;&ndash;Ascoli theorem (see Appendix 4) there exists a subsequence tnr
such that qnr converges to a limit q . From (A9.7) it follows that this limit satis-
</p>
<p>fies the conditions of Lemma A9.3, and therefore q(x) = q(0) = s for all x. Thus
G&prime;(tnr+x)&rarr; s for all x, and hence
</p>
<p>G(tnr + x)&minus;G(tnr )&rarr; sx.
</p>
<p>Since the last relation holds for any x and the function g is bounded, we get s = 0.
We have proved the lemma for continuously differentiable g. But an arbitrary
</p>
<p>continuous function g vanishing outside [0, b] can be approximated by a continu-
ously differentiable function g1 which also vanishes outside that interval. Let G1
be the solution of the renewal equation corresponding to the function g1. Then
</p>
<p>|g&minus;g1|&lt; ε implies |G&minus;G1|&lt; cε, c= c1 +c2b (see Lemma 10.2.3), and therefore
∣∣G(x + u)&minus;G(x)
</p>
<p>∣∣&lt; (2c+ 1)ε
</p>
<p>for all sufficiently large x. This proves (A9.5) for arbitrary continuous functions g.
</p>
<p>The lemma is proved. �
</p>
<p>Proof of Theorem A9.1 Consider an arbitrary sequence tn &rarr;&infin; and the measures
μn generated by the functions
</p>
<p>H(n)(u)=H(tn + u)&minus;H(tn)
(
μn
</p>
<p>(
[u,v)
</p>
<p>)
=H(n)(v)&minus;H(n)(u)).
</p>
<p>These functions satisfy the conditions of the generalised Helly theorem (see Ap-
</p>
<p>pendix 4). Therefore there exists a subsequence tnn, the respective subsequence of
</p>
<p>measures μnn, and the limiting measure μ such that μnn converges weakly to μ on
</p>
<p>any finite interval as n&rarr;&infin;.
Now let g be a continuous function vanishing outside [0, b]. Then
</p>
<p>G(tnn + x)=
&int; 0
</p>
<p>&minus;b
g(&minus;u)dH(tnn + x + u)
</p>
<p>=
&int; 0
</p>
<p>&minus;b
g(&minus;u)d
</p>
<p>(
H(tnn + x + u)&minus;H(tnn)
</p>
<p>)
&rarr;
</p>
<p>&int; b
</p>
<p>0
</p>
<p>g(u)μ(x + du).
</p>
<p>By Lemma A9.4, the sequence G(tnn+ y) will have the same limit. This means that
the measure μ(x+du) does not depend on x, and therefore μ([u,v)) is proportional
to the length of the interval (u, v):
</p>
<p>μ
(
(u, v)
</p>
<p>)
= c(v &minus; u), μ(du)= c du.
</p>
<p>Thus, we have proved that
</p>
<p>G(tnn + x)&rarr; c
&int; &infin;
</p>
<p>0
</p>
<p>g(u)du (A9.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>720 9 Renewal Theorems
</p>
<p>for any continuous function g vanishing outside [0, b]. But for any Riemann inte-
grable function g on [0, b] and given ε &gt; 0 there exist continuous functions g1 and
g2, g1 &lt; g &lt; g2, which are equal to 0 outside [0, b+ 1] and such that
</p>
<p>&int; b
</p>
<p>0
</p>
<p>(g2 &minus; g1) du &lt; ε.
</p>
<p>This means that convergence (A9.8) also holds for any Riemann integrable function
</p>
<p>vanishing outside [0, b].
Now consider an arbitrary directly integrable function g. By property (2) of such
</p>
<p>functions (see Definition 10.4.1) one can choose a b &gt; 0 such that for the function
</p>
<p>g(b)(u)=
{
g(u) if u&le; b,
0 if u &gt; b,
</p>
<p>the left- and right-hand sides of (A9.8) will be arbitrarily close to the respective
</p>
<p>expressions corresponding to the original function g (for the right-hand side it is
</p>
<p>obvious, while for the left-hand side it follows from the convergence
</p>
<p>∣∣∣∣
&int; t
</p>
<p>0
</p>
<p>g(t &minus; s) dH(s)&minus;
&int;
</p>
<p>g(b)(t &minus; s) dH(s)
∣∣∣∣
</p>
<p>=
∣∣∣∣
&int; t&minus;b
</p>
<p>0
</p>
<p>g(t &minus; s) dH(s)
∣∣∣∣&le;
</p>
<p>&sum;
</p>
<p>k&gt;b&minus;1
(c1 + c2)gk &rarr; 0
</p>
<p>as b &rarr;&infin; (see Lemma 10.2.3)). Therefore (A9.8) is proved for any directly inte-
grable function g. Putting g := 1 &minus; F we obtain from Lemma A9.1
</p>
<p>1 = c
&int; &infin;
</p>
<p>0
</p>
<p>(
1 &minus; F(u)
</p>
<p>)
du= ac, c= 1
</p>
<p>a
.
</p>
<p>Thus the limit in (A9.8) is one and the same for any initial sequence tn. From this it
</p>
<p>follows that, as t &rarr;&infin;,
</p>
<p>G(t)&rarr; 1
a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>g(u)du.
</p>
<p>The theorem is proved. �
</p>
<p>Theorem 10.4.1 is a simple consequence of Theorem A9.1 and the argument
</p>
<p>used in the proof of Theorem 10.2.3 that extends the key renewal theorem in the
</p>
<p>arithmetic case was extended to the setting where τj , j &ge; 2, can assume values
of different signs, while τ1 is arbitrary. We will leave it to the reader to apply the
</p>
<p>argument in the non-arithmetic case.
</p>
<p>Now we will give several further consequences of Theorem A9.1. In Sect. 10.4
</p>
<p>we obtained a refinement of the renewal theorem in the case when m2 :=
Eτ 2j &lt;&infin;. Approaches developed while proving Theorem A9.1 enable one to obtain
an alternative proof of the following assertion coinciding with Theorem 10.4.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>9 Renewal Theorems 721
</p>
<p>Theorem A9.2 Let the conditions of Theorem A9.1 be met and m2 &lt;&infin;. Then
</p>
<p>0 &le;H(t)&minus; t
a
&rarr; m2
</p>
<p>2a2
as t &rarr;&infin;.
</p>
<p>Proof The function G(t) := H(t) &minus; t/a is the solution of the renewal equation
(A9.2) corresponding to the function
</p>
<p>g(t) := 1
a
</p>
<p>&int; &infin;
</p>
<p>t
</p>
<p>(
1 &minus; F(u)
</p>
<p>)
du.
</p>
<p>Since g is directly integrable, we have
</p>
<p>G(t)&rarr; 1
a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>v
</p>
<p>(
1 &minus; F(u)
</p>
<p>)
dudv = m2
</p>
<p>2a2
.
</p>
<p>The theorem is proved. �
</p>
<p>Theorem A9.3 (The local renewal theorem for densities) Assume that F has a den-
sity f = F &prime; and this density is directly integrable. Then H has a density h = H &prime;,
and
</p>
<p>h(t)&rarr; 1
a
</p>
<p>as t &rarr;&infin;.
</p>
<p>Proof Denote by fn(x) the density of the sum Tn = τ1 + &middot; &middot; &middot; + τn. We have
</p>
<p>h(t)=H &prime;(t)=
&infin;&sum;
</p>
<p>n=1
fn(t)= f (t)+
</p>
<p>&int;
h(t &minus; u)f (u)du= f (t)+ h &lowast; F(t).
</p>
<p>This means that h(t) satisfies the renewal equation with the function g = f . There-
fore by Theorem A9.1,
</p>
<p>h(t)&rarr; 1
a
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>f (u)du= 1
a
.
</p>
<p>The theorem is proved. �
</p>
<p>Consider now some extensions of Theorem A9.1. A function g given on the
</p>
<p>whole line (&minus;&infin;,&infin;) is said to be directly integrable if both functions g(t) and
g(&minus;t), t &ge; 0, are directly integrable.
</p>
<p>Theorem A9.4 If the conditions of Theorem A9.1 are met and g is directly inte-
grable, then
</p>
<p>G(t)=
&int; &infin;
</p>
<p>0
</p>
<p>g(t &minus; u)H(du)&rarr; 1
a
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
g(u)du as t &rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>722 9 Renewal Theorems
</p>
<p>The Proof can be obtained by making several small and quite obvious modifica-
tions to the argument in the demonstration of Theorem A9.1. The main change is
</p>
<p>that instead of functions g vanishing outside [0, b] one should now consider func-
tions vanishing outside [&minus;b, b].
</p>
<p>Another extension refers to the second version of the renewal function
</p>
<p>U(t) :=
&infin;&sum;
</p>
<p>k=0
F &lowast;k(t), &minus;&infin;&lt; t &lt;&infin;,
</p>
<p>in the case when τj can assume values of different signs.
</p>
<p>Theorem A9.5 If g is directly integrable and Eτj = a &gt; 0, then
</p>
<p>G(t)=
&int; &infin;
</p>
<p>&minus;&infin;
g(t &minus; u)U(du)&rarr; 1
</p>
<p>a
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
g(u)du as t &rarr;&infin;,
</p>
<p>and, for any fixed u, U(t + u)&minus;U(t)&rarr; 0 as t &rarr;&infin;.
</p>
<p>The proof is also obtained by modifying the argument proving Theorem A9.1
</p>
<p>(see [13]).</p>
<p/>
</div>
<div class="page"><p/>
<p>References
</p>
<p>1. Billingsley, P.: Convergence of Probability Measures. Wiley, New York (1968)
</p>
<p>2. Billingsley, P.: Probability and Measure. Anniversary edn. Wiley, Hoboken (2012)
</p>
<p>3. Borovkov, A.A.: Stochastic Processes in Queueing Theory. Springer, New York (1976)
</p>
<p>4. Borovkov, A.A.: Convergence of measures and random processes. Russ. Math. Surv. 31, 1&ndash;69
</p>
<p>(1976)
</p>
<p>5. Borovkov, A.A.: Probability Theory. Gordon &amp; Breach, Amsterdam (1998)
</p>
<p>6. Borovkov, A.A.: Ergodicity and Stability of Stochastic Processes. Wiley, Chichester (1998)
</p>
<p>7. Borovkov, A.A.: Mathematical Statistics. Gordon &amp; Breach, Amsterdam (1998)
</p>
<p>8. Borovkov, A.A., Borovkov, K.A.: Asymptotic Analysis of Random Walks. Heavy-Tailed Dis-
</p>
<p>tributions. Cambridge University Press, Cambridge (2008)
</p>
<p>9. Cram&eacute;r, H., Leadbetter, M.R.: Stationary and Related Stochastic Processes. Willey, New York
</p>
<p>(1967)
</p>
<p>10. Dudley, R.M.: Real Analysis and Probability. Cambridge University Press, Cambridge (2002)
</p>
<p>11. Feinstein, A.: Foundations of Information Theory. McGraw-Hill, New York (1995)
</p>
<p>12. Feller, W.: An Introduction to Probability Theory and Its Applications, vol. 1. Wiley, New
</p>
<p>York (1968)
</p>
<p>13. Feller, W.: An Introduction to Probability Theory and Its Applications, vol. 2. Wiley, New
</p>
<p>York (1971)
</p>
<p>14. Gikhman, I.I., Skorokhod, A.V.: Introduction to the Theory of Random Processes. Saunders,
</p>
<p>Philadelphia (1969)
</p>
<p>15. Gnedenko, B.V.: The Theory of Probability. Chelsea, New York (1962)
</p>
<p>16. Gnedenko, B.V., Kolmogorov, A.N.: Limit Distributions for Sums of Independent Random
</p>
<p>Variables. Addison-Wesley, Reading (1968)
</p>
<p>17. Gradsteyn, I.S., Ryzhik, I.M.: Table of Integrals, Series, and Products. Academic Press, New
</p>
<p>York (1965). Fourth edition prepared by Ju.V. Geronimus and M.Ju. Ceitlin. Translated from
</p>
<p>the Russian by Scripta Technica, Inc. Translation edited by Alan Jeffrey
</p>
<p>18. Grenander, U.: Probabilities on Algebraic Structures. Almqvist &amp; Wiskel, Stockholm (1963)
</p>
<p>19. Halmos, P.R.: Measure Theory. Van Nostrand, New York (1950)
</p>
<p>20. Ibragimov, I.A., Linnik, Yu.V.: Independent and Stationary Sequences of Random Variables.
</p>
<p>Wolters-Noordhoff, Croningen (1971)
</p>
<p>21. Khinchin, A.Ya.: Ponyatie entropii v teorii veroyatnostei (The concept of entropy in the theory
</p>
<p>probability). Usp. Mat. Nauk 8, 3&ndash;20 (1953) (in Russian)
</p>
<p>22. Kifer, Yu.: Ergodic Theory of Random Transformations. Birkh&auml;user, Boston (1986)
</p>
<p>23. Kolmogorov, A.N.: Markov chains with a countable number of possible states. In: Shiryaev,
</p>
<p>A.N. (ed.) Selected Works of A.N. Kolmogorov, vol. 2, pp. 193&ndash;208. Kluwer Academic, Dor-
</p>
<p>drecht (1986)
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>723</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>724 References
</p>
<p>24. Kolmogorov, A.N.: The theory of probability. In: Aleksandrov, A.D., et al. (eds.) Mathematics,
</p>
<p>Its Content, Methods, and Meaning, vol. 2, pp. 229&ndash;264. MIT Press, Cambridge (1963)
</p>
<p>25. Lamperti, J.: Probability: A Survey of the Mathematical Theory. Wiley, New York (1996)
</p>
<p>26. Loeve, M.: Probability Theory. Springer, New York (1977)
</p>
<p>27. Meyn, S.P., Tweedie, R.L.: Markov Chains and Stochastic Stability. Springer, New York
</p>
<p>(1993)
</p>
<p>28. Natanson, I.P.: Theory of Functions of a Real Variable. Ungar, New York (1961)
</p>
<p>29. Nummelin, E.: General Irreducible Markov Chains and Nonnegative Operators. Cambridge
</p>
<p>University Press, New York (1984)
</p>
<p>30. Petrov, V.V.: Sums of Independent Random Variables. Springer, New York (1975)
</p>
<p>31. Shiryaev, A.N.: Probability. Springer, New York (1984)
</p>
<p>32. Skorokhod, A.V.: Random Processes with Independent Increments. Kluwer Academic, Dor-
</p>
<p>drecht (1991)
</p>
<p>33. Tyurin, I.S.: An improvement of the residual in the Lyapunov theorem. Theory Probab. Appl.
</p>
<p>56(4) (2011)</p>
<p/>
</div>
<div class="page"><p/>
<p>Index of Basic Notation
</p>
<p>Spaces and σ -algebras
F&mdash;a σ -algebra, 14
</p>
<p>〈Ω,F〉&mdash;a measurable space, 14
R&mdash;the real line, 17
</p>
<p>R
n&mdash;n-dimensional Euclidean space, 18
</p>
<p>B&mdash;the σ -algebra of Borel-measurable subsets of R, 17
</p>
<p>Bn&mdash;the σ -algebra of Borel-measurable subsets of Rn, 18
</p>
<p>〈Ω,F,P〉&mdash;the probability space, 17
(Note that Ω and F can take specific values, i.e. R and B, respectively.)
</p>
<p>Distributions1
</p>
<p>Fξ , F&mdash;the distribution of the random variable ξ , 32, 32
</p>
<p>Ia&mdash;the degenerate distribution (concentrated at the point a), 37
</p>
<p>Ua,b&mdash;the uniform distribution on [a, b], 37
Bp , B
</p>
<p>n
p&mdash;the binomial distributions, 37
</p>
<p>multinomial distributions, 47
</p>
<p>�α,σ 2&mdash;the normal (Gaussian) distribution with parameters (α,σ
2), 37, 48
</p>
<p>φα,σ 2(x)&mdash;the density of the normal law with parameters (α,σ
2), 41
</p>
<p>Fβ,ρ&mdash;the stable distribution with parameters β , ρ, 231, 233
</p>
<p>f (β,ρ)(x)&mdash;the density of the stable distribution with parameters Fβ,ρ , 235
</p>
<p>ϕ(β,ρ)(t)&mdash;the characteristic function of distribution Fβ,ρ , 231
</p>
<p>Kα,σ&mdash;the Cauchy distribution with parameters (α,σ ), 38
</p>
<p>Ŵα&mdash;the exponential distribution with parameter α, 38, 177
</p>
<p>Ŵα,λ&mdash;the gamma-distribution with parameters (α,λ), 176
</p>
<p>�λ&mdash;the Poisson distribution with parameter λ, 39
</p>
<p>χ2&mdash;the χ2-distribution, 177
</p>
<p>Λ(α)&mdash;the large deviation rate function, 244
</p>
<p>1(All distributions and measures are denoted by bold letters).
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>725</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>726 Index of Basic Notation
</p>
<p>Relations
:= means that the left-hand side is defined by the right-hand side, xi
=: means that the right-hand side is defined by the left-hand side, xi
&sim; notation an &sim; bn (a(x)&sim; b(x)) means that limn&rarr;&infin; anbn = 1 (limx&rarr;&infin;
</p>
<p>a(x)
b(x)
</p>
<p>= 1),
109
p&rarr;&mdash;convergence of random variables in probability, 129
a.s.&minus;&rarr;&mdash;almost sure convergence of random variables, 130
(r)&minus;&rarr;&mdash;convergence of random variables in the mean, 132
d=&mdash;notation ξ d= η means that the distributions of ξ and η coincide, 144
d
&le;&mdash;relation ξ
</p>
<p>d
&le; γ means that P(ξ &ge; t)&le; P(γ &ge; t) for all t , 302
</p>
<p>d
&gt;&mdash;relation ξ &gt;
</p>
<p>d
η means that P(ξ &gt; t)&ge; P(η &gt; t) for all t , 302
</p>
<p>&sub;=&mdash;notation ξn &sub;= F means that ξ has the distribution F, 36
ξn &sub;=&rArr; F means that the distribution of ξn converges weakly to F, 144
&rArr;&mdash;relation Fn &rArr; F means weak convergence of the distributions Fn to F, 141,
</p>
<p>for random variables ξn &rArr; ξ means that Fn &rArr; F, where ξn&sub;=Fn, ξ &sub;=F, 143
Conditions
[C]&mdash;the Cram&eacute;r condition, 240
[Rβ,ρ]&mdash;conditions of convergence to the stable law Fβ,ρ , 229</p>
<p/>
</div>
<div class="page"><p/>
<p>Subject Index
</p>
<p>A
</p>
<p>Abelian theorem, 673
</p>
<p>Absolutely continuous distribution, 40
</p>
<p>Absorbing state, 393
</p>
<p>Absorption, 391
</p>
<p>Algebra, 14
</p>
<p>Almost invariant
</p>
<p>random variable, 498
</p>
<p>set, 497
</p>
<p>Amount of information, 448
</p>
<p>Aperiodic Markov chain, 419
</p>
<p>Arithmetic distribution, 40
</p>
<p>Arzel&agrave;&ndash;Ascoli theorem, 657
</p>
<p>Asymptotically normal sequence, 187
</p>
<p>Atom, 419
</p>
<p>positive, 420
</p>
<p>B
</p>
<p>Basic coding theorem, 455
</p>
<p>Bayes formula, 27
</p>
<p>Bernoulli scheme, local limit theorems for, 113
</p>
<p>Bernstein polynomial, 109
</p>
<p>Berry&ndash;Esseen theorem, 659
</p>
<p>Beta distribution, 179
</p>
<p>Binomial distribution, 37
</p>
<p>Bochner&ndash;Khinchin theorem, 158
</p>
<p>Borel
</p>
<p>σ -algebra, 15
</p>
<p>set, 15
</p>
<p>Branching process, 180, 591
</p>
<p>extinction of, 182
</p>
<p>Brownian motion process, 549
</p>
<p>C
</p>
<p>Carath&eacute;odory theorem, 19, 622
</p>
<p>Cauchy sequence, 132
</p>
<p>Cauchy&ndash;Bunjakovsky inequality, 87, 97
</p>
<p>Central limit theorem, 187
</p>
<p>for renewal processes, 299
</p>
<p>Central moment, 87
</p>
<p>Chain, Markov, 390, 414
</p>
<p>Chapman&ndash;Kolmogorov equation, 582
</p>
<p>Characteristic function, 153
</p>
<p>for multivariate distribution, 171
</p>
<p>Chebyshev inequality, 89, 96
</p>
<p>exponential, 248
</p>
<p>Chi-squared distribution, 177
</p>
<p>Class
</p>
<p>of distributions
</p>
<p>exponential, 373
</p>
<p>superexponential, 373
</p>
<p>of functions, distribution determining, 148
</p>
<p>Coefficient
</p>
<p>diffusion, 604
</p>
<p>shift, 604
</p>
<p>Common probability space method, 118
</p>
<p>Communicating states, 392
</p>
<p>Complement, 16
</p>
<p>Completion of measure, 624
</p>
<p>Component, factorisation, 334
</p>
<p>Compound Poisson process, 552
</p>
<p>Condition
</p>
<p>Cram&eacute;r, 240, 703
</p>
<p>Cram&eacute;r on ch.f., 217
</p>
<p>[D1], 188
(D2), 199
</p>
<p>Lyapunov, 202, 560
</p>
<p>[Rβ,ρ ], 229, 687
Conditional
</p>
<p>density, 100
</p>
<p>distribution, 99
</p>
<p>distribution function, 70
</p>
<p>entropy, 451
</p>
<p>expectation, 70, 92, 94, 95
</p>
<p>A.A. Borovkov, Probability Theory, Universitext,
DOI 10.1007/978-1-4471-5201-9, &copy; Springer-Verlag London 2013
</p>
<p>727</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5201-9">http://dx.doi.org/10.1007/978-1-4471-5201-9</a></div>
</div>
<div class="page"><p/>
<p>728 Subject Index
</p>
<p>Conditional (cont.)
probability, 22, 95
</p>
<p>Consistent distributions, 530
</p>
<p>Continuity axiom, 16
</p>
<p>Continuity theorem, 134, 167, 173
</p>
<p>Converge
</p>
<p>in measure, 630
</p>
<p>Convergence
</p>
<p>almost everywhere, 630
</p>
<p>almost surely, with probability 1, 130
</p>
<p>in distribution, 143
</p>
<p>in measure, 630
</p>
<p>in probability, 129
</p>
<p>in the mean, 132
</p>
<p>in total variation, 653
</p>
<p>weak, 141, 173, 649
</p>
<p>Correlation coefficient, 86
</p>
<p>Coupling method, 118
</p>
<p>Covariance function, 611
</p>
<p>Cram&eacute;r
</p>
<p>condition, 240, 703
</p>
<p>on ch.f., 217
</p>
<p>range, 256
</p>
<p>series, 248
</p>
<p>transform, 473
</p>
<p>Crossing times, 237
</p>
<p>Cumulant, 242
</p>
<p>Cylinder, 528
</p>
<p>D
</p>
<p>Defect, 290
</p>
<p>Degenerate distribution, 37
</p>
<p>De Moivre&ndash;Laplace theorem, 115, 124
</p>
<p>Density
</p>
<p>conditional, 100
</p>
<p>of distribution, 40
</p>
<p>of measure, 642
</p>
<p>transition, 583
</p>
<p>Derivative, Radon&ndash;Nikodym, 644
</p>
<p>Deviation, standard, 83
</p>
<p>Diffusion
</p>
<p>coefficient, 604
</p>
<p>process, 603
</p>
<p>Directly integrable function, 293
</p>
<p>Distance, total variation, 420
</p>
<p>Distribution, 17
</p>
<p>absolutely continuous, 40
</p>
<p>arithmetic, 40
</p>
<p>beta, 179
</p>
<p>binomial, 37
</p>
<p>chi-squared, 177
</p>
<p>conditional, 99
</p>
<p>consistent, 530
</p>
<p>degenerate, 37
</p>
<p>Erlang, 177
</p>
<p>exponential, 38, 71, 177
</p>
<p>finite-dimensional, 528
</p>
<p>function, 32
</p>
<p>conditional, 70
</p>
<p>properties, 33
</p>
<p>gamma, 176
</p>
<p>Gaussian, 37
</p>
<p>geometric, 38
</p>
<p>infinitely divisible, 539
</p>
<p>invariant, 404, 419
</p>
<p>lattice, 40
</p>
<p>Levy, 235
</p>
<p>multinomial, 47
</p>
<p>multivariate normal (Gaussian), 48, 173
</p>
<p>non-lattice, 160
</p>
<p>normal, 37
</p>
<p>of process, 528
</p>
<p>of random process, 529
</p>
<p>of random variable, 32
</p>
<p>Poisson, 26, 39
</p>
<p>singular, 41, 325
</p>
<p>stable, 233
</p>
<p>stationary, 404, 419
</p>
<p>of waiting time, 350
</p>
<p>subexponential, 376, 675
</p>
<p>tail of, 228
</p>
<p>uniform, 18, 37, 325
</p>
<p>uniform on a cube, 18
</p>
<p>Dominated convergence theorem, 139
</p>
<p>Donsker&ndash;Prokhorov invariance principle, 561
</p>
<p>Doubly stochastic matrix, 410
</p>
<p>E
</p>
<p>Element
</p>
<p>random, 649
</p>
<p>Entropy, 448
</p>
<p>conditional, 451
</p>
<p>Equality
</p>
<p>Parseval, 161
</p>
<p>Equation
</p>
<p>backward (forward) Kolmogorov, 587, 605
</p>
<p>Chapman&ndash;Kolmogorov, 582
</p>
<p>renewal, 716
</p>
<p>Equivalent
</p>
<p>processes, 530
</p>
<p>sequences, 109
</p>
<p>Ergodic
</p>
<p>Markov chain, 404
</p>
<p>sequence, 498
</p>
<p>state, 411
</p>
<p>transformation, 498
</p>
<p>Erlang distribution, 177
</p>
<p>Essential state, 392</p>
<p/>
</div>
<div class="page"><p/>
<p>Subject Index 729
</p>
<p>Event, 2
</p>
<p>certain, 16
</p>
<p>impossible, 16
</p>
<p>random, xiv
</p>
<p>renovating, 509
</p>
<p>tail, 316
</p>
<p>Events
</p>
<p>disjoint (mutually exclusive), 16
</p>
<p>independent, 22
</p>
<p>Excess, 280
</p>
<p>Existence
</p>
<p>of expectation, 65
</p>
<p>of integral, 643
</p>
<p>Expectation, 65
</p>
<p>conditional, 70, 92, 94, 95
</p>
<p>existence of, 65
</p>
<p>Exponential
</p>
<p>Chebyshev inequality, 248
</p>
<p>class of distributions, 373
</p>
<p>distribution, 38, 177
</p>
<p>polynomial, 355, 366
</p>
<p>Extinction of branching process, 182
</p>
<p>F
</p>
<p>Factorisation, 334
</p>
<p>component, 334
</p>
<p>Fair game, 72
</p>
<p>Finite-dimensional distribution, 528
</p>
<p>First nonnegative sum, 336
</p>
<p>First passage time, 278
</p>
<p>Flow of σ -algebras, 457
</p>
<p>Formula
</p>
<p>Bayes, 27
</p>
<p>total probability, 25
</p>
<p>Function
</p>
<p>covariance, 611
</p>
<p>directly integrable, 293
</p>
<p>distribution, 32
</p>
<p>properties, 33
</p>
<p>large deviation rate, 244
</p>
<p>locally constant, 373
</p>
<p>lower, 546
</p>
<p>rate, 244
</p>
<p>regularly varying, 266, 665
</p>
<p>renewal, 279
</p>
<p>sample, 528
</p>
<p>slowly varying, 228, 665
</p>
<p>subexponential, 376
</p>
<p>test (Lyapunov), 430
</p>
<p>transition, 582, 583
</p>
<p>upper, 546
</p>
<p>G
</p>
<p>Gamma distribution, 176
</p>
<p>Gaussian
</p>
<p>distribution, 37
</p>
<p>process, 614
</p>
<p>Generating function, 161
</p>
<p>Geometric distribution, 38
</p>
<p>Gnedenko local limit theorem, 221
</p>
<p>H
</p>
<p>Hahn&rsquo;s theorem on decomposition of measure,
</p>
<p>646
</p>
<p>Harris (irreducible) Markov chain, 424
</p>
<p>Helly theorem, 655
</p>
<p>H&ouml;lder inequality, 88
</p>
<p>Homogeneous
</p>
<p>Markov chain, 391, 416
</p>
<p>Markov process, 583
</p>
<p>process, 539
</p>
<p>renewal process, 285
</p>
<p>I
</p>
<p>Identity
</p>
<p>Pollaczek&ndash;Spitzer, 345
</p>
<p>Wald, 469
</p>
<p>Immigration, 591
</p>
<p>Improper random variable, 32
</p>
<p>Independent
</p>
<p>classes of events, 51
</p>
<p>events, 22
</p>
<p>random variables, 153
</p>
<p>trials, 24
</p>
<p>Indicator of event, 66
</p>
<p>Inequality
</p>
<p>Cauchy&ndash;Bunjakovsky, 87, 97
</p>
<p>Chebyshev, 89, 96
</p>
<p>Chebyshev exponential, 248
</p>
<p>H&ouml;lder, 88
</p>
<p>Jensen, 88, 97
</p>
<p>Kolmogorov, 478
</p>
<p>Minkowski, 88, 133
</p>
<p>Schwarz, 88
</p>
<p>Inessential state, 392
</p>
<p>Infinitely divisible distribution, 539
</p>
<p>Information, 448
</p>
<p>amount of, 448
</p>
<p>Integrability, uniform, 135
</p>
<p>Integral, 630, 632, 642
</p>
<p>of a nonnegative measurable function, 632
</p>
<p>over a set, 631
</p>
<p>Integro-local theorems, 216
</p>
<p>Invariance principle, 567
</p>
<p>Invariant
</p>
<p>distribution, 419
</p>
<p>random variable, 498
</p>
<p>set, 497</p>
<p/>
</div>
<div class="page"><p/>
<p>730 Subject Index
</p>
<p>Irreducible Markov chain, 393
</p>
<p>Iterated logarithm, law of, 545, 546, 568
</p>
<p>J
</p>
<p>Jensen inequality, 88, 97
</p>
<p>K
</p>
<p>Karamata theorem, 668
</p>
<p>Kolmogorov
</p>
<p>equation, backward (forward), 587, 605
</p>
<p>inequality, 478
</p>
<p>theorem on consistent distributions, 56, 625
</p>
<p>L
</p>
<p>Laplace transform, 156, 241
</p>
<p>Large deviation
</p>
<p>probabilities, 126
</p>
<p>rate function, 244
</p>
<p>Large numbers, law of, 107, 188
</p>
<p>for renewal processes, 298
</p>
<p>strong, 108
</p>
<p>Lattice distribution, 40
</p>
<p>Law
</p>
<p>of iterated logarithm, 545, 546, 568
</p>
<p>of large numbers, 90, 107, 188
</p>
<p>for renewal processes, 298
</p>
<p>strong, 108
</p>
<p>Lebesgue theorem, 644
</p>
<p>Legendre transform, 244
</p>
<p>Levy distribution, 235
</p>
<p>Limit theorems, local for Bernoulli scheme,
</p>
<p>113
</p>
<p>Linear prediction, 617
</p>
<p>Local limit theorem, 219
</p>
<p>Locally constant function, 373
</p>
<p>Lower
</p>
<p>function, 546
</p>
<p>sequence, 318
</p>
<p>Lyapunov condition, 202, 560
</p>
<p>M
</p>
<p>Markov
</p>
<p>chain, 390, 414, 585
</p>
<p>aperiodic, 419
</p>
<p>ergodic, 404
</p>
<p>Harris (irreducible), 424
</p>
<p>homogeneous, 391, 416
</p>
<p>periodic, 397, 419
</p>
<p>reducible (irreducible), 393
</p>
<p>process, 580
</p>
<p>homogeneous, 583
</p>
<p>property, 390
</p>
<p>strong, 418
</p>
<p>time, 75
</p>
<p>Martingale, 457, 459
</p>
<p>Matrix
</p>
<p>doubly stochastic, 410
</p>
<p>stochastic, 391
</p>
<p>transition, 391
</p>
<p>Mean value, 65
</p>
<p>Measurable space, 14
</p>
<p>Measure, 629
</p>
<p>density of, 642
</p>
<p>extension, 19, 622
</p>
<p>theorem, 19, 622
</p>
<p>outer, 619
</p>
<p>signed, 629
</p>
<p>singular, 644
</p>
<p>space, 629
</p>
<p>Measure preserving transformation, 494
</p>
<p>Measure Space, 629
</p>
<p>Metric transitive
</p>
<p>sequence, 498
</p>
<p>transformation, 498
</p>
<p>Minkowski inequality, 88
</p>
<p>Mixed moment, 87
</p>
<p>Mixing transformation, 499
</p>
<p>Modification of process, 530
</p>
<p>Moment
</p>
<p>central, 87
</p>
<p>k-th order, 87
</p>
<p>mixed, 87
</p>
<p>Multinomial distribution, 47
</p>
<p>Multivariate normal (Gaussian) distribution,
</p>
<p>48, 173
</p>
<p>N
</p>
<p>Negatively correlated random variables, 87
</p>
<p>Non-lattice distribution, 160
</p>
<p>Normal distribution, 37
</p>
<p>Null state, 394
</p>
<p>O
</p>
<p>Oscillating random walk, 435
</p>
<p>Outer measure, 619
</p>
<p>Overshoot, 280
</p>
<p>P
</p>
<p>Parseval equality, 161
</p>
<p>Passage time, 336
</p>
<p>Path, 528
</p>
<p>Pathwise shift transformation, 496
</p>
<p>Periodic
</p>
<p>Markov chain, 397, 419
</p>
<p>state of Markov chain, 394
</p>
<p>Persistent state, 394
</p>
<p>Poisson
</p>
<p>distribution, 26, 39</p>
<p/>
</div>
<div class="page"><p/>
<p>Subject Index 731
</p>
<p>Poisson (cont.)
process, 297, 549
</p>
<p>theorem, 121
</p>
<p>Pollaczek&ndash;Spitzer identity, 345
</p>
<p>Polynomial
</p>
<p>Bernstein, 109
</p>
<p>exponential, 355, 366
</p>
<p>Positive atom, 420
</p>
<p>Positive state, 394, 411
</p>
<p>Positively correlated random variables, 87
</p>
<p>Posterior probability, 28
</p>
<p>Prediction, 616
</p>
<p>linear, 617
</p>
<p>Prior probability, 28
</p>
<p>Probability, 16
</p>
<p>conditional, 22, 95
</p>
<p>distribution, 17
</p>
<p>posterior, 28
</p>
<p>prior, 28
</p>
<p>properties of, 20
</p>
<p>space, 17
</p>
<p>sample, 528
</p>
<p>wide-sense, 17
</p>
<p>transition, 583
</p>
<p>Process
</p>
<p>branching, 180, 591
</p>
<p>Brownian motion, 549
</p>
<p>compound Poisson, 552
</p>
<p>continuous in mean, 536
</p>
<p>diffusion, 603
</p>
<p>distribution of, 528, 529
</p>
<p>Gaussian, 614
</p>
<p>homogeneous, 539
</p>
<p>Markov, 580
</p>
<p>modification of, 530
</p>
<p>Poisson, 297, 549
</p>
<p>random (stochastic), 527, 529
</p>
<p>regenerative, 600
</p>
<p>regular, 532
</p>
<p>renewal, 278
</p>
<p>homogeneous (stationary), 285
</p>
<p>semi-Markov, 593
</p>
<p>separable, 535
</p>
<p>stochastically continuous, 536, 584
</p>
<p>strict sense stationary, 614
</p>
<p>unpredictable, 611
</p>
<p>Wiener, 542
</p>
<p>with immigration, 591
</p>
<p>with independent increments, 539
</p>
<p>Prokhorov theorem, 651
</p>
<p>Proper random variable, 73
</p>
<p>Property, strong Markov, 418
</p>
<p>Pseudomoment, 210
</p>
<p>Q
</p>
<p>Quantile, 43
</p>
<p>transform, 43
</p>
<p>R
</p>
<p>Radon&ndash;Nikodym derivative, 642, 644
</p>
<p>Radon&ndash;Nikodym theorem, 644
</p>
<p>Random
</p>
<p>element, 414, 649
</p>
<p>event, xiv
</p>
<p>process, 527, 529
</p>
<p>sequence, 527
</p>
<p>variable, 31
</p>
<p>almost invariant, 498
</p>
<p>complex-valued, 153
</p>
<p>defined on Markov chain, 437
</p>
<p>distribution of, 32
</p>
<p>improper, 32
</p>
<p>independent of the future, 75
</p>
<p>invariant, 498
</p>
<p>proper, 73
</p>
<p>standardised, 85
</p>
<p>subexponential, 376, 675
</p>
<p>symmetric, 157
</p>
<p>tail, 317
</p>
<p>variables
</p>
<p>independent, 153
</p>
<p>positively (negatively) correlated, 87
</p>
<p>vector, 44
</p>
<p>walk, 277, 278, 335
</p>
<p>oscillating, 435
</p>
<p>skip-free, 384
</p>
<p>symmetric, 400, 401
</p>
<p>with reflection, 434
</p>
<p>Range, Cram&eacute;r, 256
</p>
<p>Rate function, 244
</p>
<p>Recurrent state, 394
</p>
<p>Reflection, 391, 434
</p>
<p>Regeneration time, 600
</p>
<p>Regenerative process, 600
</p>
<p>Regression line, 103
</p>
<p>Regular process, 532
</p>
<p>Regularly varying function, 266, 665
</p>
<p>Renewal
</p>
<p>equation, 716
</p>
<p>function, 279
</p>
<p>integral theorem, 280
</p>
<p>local theorem, 294
</p>
<p>process, 278
</p>
<p>Renovating
</p>
<p>event, 509
</p>
<p>sequence of events, 509
</p>
<p>Right closed martingale (semimartingale), 459
</p>
<p>Ring, 14</p>
<p/>
</div>
<div class="page"><p/>
<p>732 Subject Index
</p>
<p>S
</p>
<p>Sample
</p>
<p>function, 528
</p>
<p>probability space, 528
</p>
<p>space, 414, 649
</p>
<p>Schwarz inequality, 88
</p>
<p>Semi-invariant, 242
</p>
<p>Semi-Markov process, 593
</p>
<p>Semimartingale, 458
</p>
<p>Separable process, 535
</p>
<p>Sequence
</p>
<p>asymptotically normal, 187
</p>
<p>Cauchy (in probability, a.s., in the mean),
</p>
<p>132
</p>
<p>ergodic, 498
</p>
<p>generated by transformation, 495
</p>
<p>lower, 318
</p>
<p>metric transitive, 498
</p>
<p>renovating, 509
</p>
<p>stationary, 493
</p>
<p>stochastic, 457
</p>
<p>stochastic recursive, 507
</p>
<p>tight, 148
</p>
<p>uniformly integrable, 135
</p>
<p>upper, 318
</p>
<p>weakly dependent, 499
</p>
<p>Series, Cram&eacute;r, 248
</p>
<p>Set
</p>
<p>almost invariant, 497
</p>
<p>invariant, 497
</p>
<p>Shift coefficient, 604
</p>
<p>σ -algebra, 14
</p>
<p>Signed measure, 629
</p>
<p>Singular
</p>
<p>distribution, 41, 325
</p>
<p>measure, 644
</p>
<p>Skip-free walk, 384
</p>
<p>Slowly varying function, 228, 665
</p>
<p>Space
</p>
<p>measurable, 14
</p>
<p>measure, 629
</p>
<p>of functions without discontinuities of the
</p>
<p>second kind, 529
</p>
<p>probability, 17
</p>
<p>sample, 414, 649
</p>
<p>sample probability, 528
</p>
<p>Spectral measure, 556
</p>
<p>Stable distribution, 233
</p>
<p>Standard deviation, 83
</p>
<p>Standardised random variable, 85
</p>
<p>State
</p>
<p>absorbing, 393
</p>
<p>ergodic, 411
</p>
<p>essential, 392
</p>
<p>inessential, 392
</p>
<p>periodic, 394
</p>
<p>persistent, 394
</p>
<p>positive, 411
</p>
<p>recurrent, 394
</p>
<p>transient, 394
</p>
<p>State, null, 394
</p>
<p>State, positive, 394
</p>
<p>Stationary
</p>
<p>distribution, 404, 419
</p>
<p>of waiting time, 350
</p>
<p>process, 614
</p>
<p>sequence, 493
</p>
<p>of events, 509
</p>
<p>Stochastic
</p>
<p>matrix, 391
</p>
<p>process, 527, 529
</p>
<p>recursive sequence, 507
</p>
<p>sequence, 457
</p>
<p>Stochastically continuous process, 536, 584
</p>
<p>Stone&ndash;Shepp integro-local theorem, 216
</p>
<p>Stopping time, 75, 462
</p>
<p>improper, 466
</p>
<p>Strong law of large numbers, 108
</p>
<p>Strong Markov property, 418
</p>
<p>Subexponential
</p>
<p>distribution, 376, 675
</p>
<p>function, 376
</p>
<p>random variable, 376, 675
</p>
<p>Submartingale, 458, 459
</p>
<p>Sum, first nonnegative, 336
</p>
<p>Superexponential class of distributions, 373
</p>
<p>Supermartingale, 458, 459
</p>
<p>Symmetric
</p>
<p>random variable, 157
</p>
<p>random walk, 401
</p>
<p>T
</p>
<p>Tail
</p>
<p>event, 316
</p>
<p>of distribution, 228
</p>
<p>random variable, 317
</p>
<p>Tauberian theorem, 673
</p>
<p>Test function, 430
</p>
<p>Theorem
</p>
<p>Abelian, 673
</p>
<p>Arzel&agrave;&ndash;Ascoli, 657
</p>
<p>basic coding, 455
</p>
<p>Berry&ndash;Esseen, 659
</p>
<p>Bochner&ndash;Khinchin, 158
</p>
<p>Carath&eacute;odory (measure extension), 19, 622
</p>
<p>central limit, 187
</p>
<p>central limit for renewal processes, 299
</p>
<p>continuity, 134, 167, 173</p>
<p/>
</div>
<div class="page"><p/>
<p>Subject Index 733
</p>
<p>Theorem (cont.)
de Moivre&ndash;Laplace, 115, 124
</p>
<p>dominated convergence, 139
</p>
<p>Gnedenko local limit, 221
</p>
<p>Hahn&rsquo;s on decomposition of a measure, 646
</p>
<p>Helly, 655
</p>
<p>integral renewal, 280
</p>
<p>integro-local, 216
</p>
<p>Karamata, 668
</p>
<p>Kolmogorov, on consistent distributions,
</p>
<p>56, 625
</p>
<p>Lebesgue, 644
</p>
<p>local limit, 219
</p>
<p>local renewal, 294
</p>
<p>measure extension, 19, 622
</p>
<p>Poisson, 121
</p>
<p>Prokhorov, 651
</p>
<p>Radon&ndash;Nikodym, 644
</p>
<p>Stone&ndash;Shepp integro-local, 216
</p>
<p>Tauberian, 673
</p>
<p>two series, 322
</p>
<p>Weierstrass, 109
</p>
<p>Tight family of distributions, 651
</p>
<p>Tight sequence, 148
</p>
<p>Time
</p>
<p>first passage, 278
</p>
<p>Markov, 75
</p>
<p>passage, 336
</p>
<p>regeneration, 600
</p>
<p>stopping, 75
</p>
<p>waiting, 349
</p>
<p>Total probability formula, 25, 71, 98
</p>
<p>Total variation, 652
</p>
<p>convergence in, 653
</p>
<p>distance, 420
</p>
<p>Trajectory, 528
</p>
<p>Transform
</p>
<p>Cram&eacute;r, 473
</p>
<p>Laplace, 156, 241
</p>
<p>Legendre, 244
</p>
<p>quantile, 43
</p>
<p>Transformation
</p>
<p>bidirectional preserving measure, 495
</p>
<p>ergodic, 498
</p>
<p>metric transitive, 498
</p>
<p>mixing, 499
</p>
<p>pathwise shift, 496
</p>
<p>preserving measure, 494
</p>
<p>Transient state, 394
</p>
<p>Transition
</p>
<p>density, 583
</p>
<p>function, 582, 583
</p>
<p>matrix, 391
</p>
<p>probability, 583
</p>
<p>Triangular array scheme, 121, 188
</p>
<p>Two series theorem, 322
</p>
<p>U
</p>
<p>Undershoot, 290
</p>
<p>Uniform distribution, 18, 37, 325
</p>
<p>Uniform integrability, 135
</p>
<p>right (left), 139
</p>
<p>Unpredictable process, 611
</p>
<p>Upper
</p>
<p>function, 546
</p>
<p>sequence, 318
</p>
<p>V
</p>
<p>Variable, random, 31
</p>
<p>Variance, 83
</p>
<p>Vector, random, 44
</p>
<p>W
</p>
<p>Waiting time, 349
</p>
<p>stationary distribution of, 350
</p>
<p>Wald identity, 469
</p>
<p>fundamental, 471
</p>
<p>Walk, random, 277, 278, 335
</p>
<p>Weak convergence, 141, 173, 649
</p>
<p>Weakly dependent sequence, 499
</p>
<p>Weierstrass theorem, 109
</p>
<p>Wiener process, 542</p>
<p/>
</div>
<ul>	<li>Probability Theory</li>
<ul>	<li>Foreword</li>
	<li>Foreword to the Third and Fourth Editions</li>
	<li>For the Reader's Attention</li>
	<li>Introduction</li>
	<li>Contents</li>
</ul>
	<li>Chapter 1: Discrete Spaces of Elementary Events</li>
<ul>	<li>1.1 Probability Space</li>
	<li>1.2 The Classical Scheme</li>
	<li>1.3 The Bernoulli Scheme</li>
	<li>1.4 The Probability of the Union of Events. Examples</li>
</ul>
	<li>Chapter 2: An Arbitrary Space of Elementary Events</li>
<ul>	<li>2.1 The Axioms of Probability Theory. A Probability Space</li>
	<li>2.2 Properties of Probability</li>
	<li>2.3 Conditional Probability. Independence of Events and Trials</li>
	<li>2.4 The Total Probability Formula. The Bayes Formula</li>
</ul>
	<li>Chapter 3: Random Variables and Distribution Functions</li>
<ul>	<li>3.1 Deﬁnitions and Examples</li>
	<li>3.2 Properties of Distribution Functions. Examples</li>
<ul>	<li>3.2.1 The Basic Properties of Distribution Functions</li>
	<li>3.2.2 The Most Common Distributions</li>
	<li>3.2.3 The Three Distribution Types</li>
	<li>3.2.4 Distributions of Functions of Random Variables</li>
</ul>
	<li>3.3 Multivariate Random Variables</li>
	<li>3.4 Independence of Random Variables and Classes of Events</li>
<ul>	<li>3.4.1 Independence of Random Vectors</li>
	<li>3.4.2 Independence of Classes of Events</li>
	<li>3.4.3 Relations Between the Introduced Notions</li>
</ul>
	<li>3.5 -* On Inﬁnite Sequences of Random Variables</li>
	<li>3.6 Integrals</li>
<ul>	<li>3.6.1 Integral with Respect to Measure</li>
	<li>3.6.2 The Stieltjes Integral</li>
	<li>3.6.3 Integrals of Multivariate Random Variables. The Distribution of the Sum of Independent Random Variables</li>
</ul>
</ul>
	<li>Chapter 4: Numerical Characteristics of Random Variables</li>
<ul>	<li>4.1 Expectation</li>
	<li>4.2 Conditional Distribution Functions and Conditional Expectations</li>
	<li>4.3 Expectations of Functions of Independent Random Variables</li>
	<li>4.4 Expectations of Sums of a Random Number of Random Variables</li>
	<li>4.5 Variance</li>
	<li>4.6 The Correlation Coefﬁcient and Other Numerical Characteristics</li>
	<li>4.7 Inequalities</li>
<ul>	<li>4.7.1 Moment Inequalities</li>
	<li>4.7.2 Inequalities for Probabilities</li>
</ul>
	<li>4.8 Extension of the Notion of Conditional Expectation</li>
<ul>	<li>4.8.1 Deﬁnition of Conditional Expectation</li>
	<li>4.8.2 Properties of Conditional Expectations</li>
</ul>
	<li>4.9 Conditional Distributions</li>
</ul>
	<li>Chapter 5: Sequences of Independent Trials with Two Outcomes</li>
<ul>	<li>5.1 Laws of Large Numbers</li>
	<li>5.2 The Local Limit Theorem and Its Reﬁnements</li>
<ul>	<li>5.2.1 The Local Limit Theorem</li>
	<li>5.2.2 Reﬁnements of the Local Theorem</li>
	<li>5.2.3 The Local Limit Theorem for the Polynomial Distributions</li>
</ul>
	<li>5.3 The de Moivre-Laplace Theorem and Its Reﬁnements</li>
	<li>5.4 The Poisson Theorem and Its Reﬁnements</li>
<ul>	<li>5.4.1 Quantifying the Closeness of Poisson Distributions to Those of the Sums Sn</li>
	<li>5.4.2 The Triangular Array Scheme. The Poisson Theorem</li>
</ul>
	<li>5.5 Inequalities for Large Deviation Probabilities in the Bernoulli Scheme</li>
</ul>
	<li>Chapter 6: On Convergence of Random Variables and Distributions</li>
<ul>	<li>6.1 Convergence of Random Variables</li>
<ul>	<li>6.1.1 Types of Convergence</li>
	<li>6.1.2 The Continuity Theorem</li>
	<li>6.1.3 Uniform Integrability and Its Consequences</li>
</ul>
	<li>6.2 Convergence of Distributions</li>
	<li>6.3 Conditions for Weak Convergence</li>
</ul>
	<li>Chapter 7: Characteristic Functions</li>
<ul>	<li>7.1 Deﬁnition and Properties of Characteristic Functions</li>
<ul>	<li>7.1.1 Properties of Characteristic Functions</li>
	<li>7.1.2 The Properties of Ch.F.s Related to the Structure of the Distribution of xi</li>
</ul>
	<li>7.2 Inversion Formulas</li>
<ul>	<li>7.2.1 The Inversion Formula for Densities</li>
	<li>7.2.2 The Inversion Formula for Distributions</li>
	<li>7.2.3 The Inversion Formula in L2. The Class of Functions that Are Both Densities and Ch.F.s</li>
</ul>
	<li>7.3 The Continuity (Convergence) Theorem</li>
	<li>7.4 The Application of Characteristic Functions in the Proof of the Poisson Theorem</li>
	<li>7.5 Characteristic Functions of Multivariate Distributions. The Multivariate Normal Distribution</li>
	<li>7.6 Other Applications of Characteristic Functions. The Properties of the Gamma Distribution</li>
<ul>	<li>7.6.1 Stability of the Distributions Phialpha,sigma2   and Kalpha,sigma</li>
	<li>7.6.2 The Gamma-distribution and its properties</li>
</ul>
	<li>7.7 Generating Functions. Application to Branching Processes. A Problem on Extinction</li>
<ul>	<li>7.7.1 Generating Functions</li>
	<li>7.7.2 The Simplest Branching Processes</li>
</ul>
</ul>
	<li>Chapter 8: Sequences of Independent Random Variables. Limit Theorems</li>
<ul>	<li>8.1 The Law of Large Numbers</li>
	<li>8.2 The Central Limit Theorem for Identically Distributed Random Variables</li>
	<li>8.3 The Law of Large Numbers for Arbitrary Independent Random Variables</li>
	<li>8.4 The Central Limit Theorem for Sums of Arbitrary Independent Random Variables</li>
	<li>8.5 *Another Approach to Proving Limit Theorems. Estimating Approximation Rates</li>
	<li>8.6 The Law of Large Numbers and the Central Limit Theorem in the Multivariate Case</li>
	<li>8.7 Integro-Local and Local Limit Theorems for Sums of Identically Distributed Random Variables with Finite Variance</li>
<ul>	<li>8.7.1 Integro-Local Theorems</li>
	<li>8.7.2 Local Theorems</li>
	<li>8.7.3 The Proof of Theorem 7.1 in the General Case</li>
	<li>8.7.4 Uniform Versions of Theorems 7.1-7.3 for Random Variables Depending on a Parameter</li>
</ul>
	<li>8.8 Convergence to Other Limiting Laws</li>
<ul>	<li>8.8.1 The Integral Theorem</li>
	<li>8.8.2 The Integro-Local and Local Theorems</li>
	<li>8.8.3 An Example</li>
</ul>
</ul>
	<li>Chapter 9: Large Deviation Probabilities for Sums of Independent Random Variables</li>
<ul>	<li>9.1 Laplace's and Cram&eacute;r's Transforms. The Rate Function</li>
<ul>	<li>9.1.1 The Cram&eacute;r Condition. Laplace's and Cram&eacute;r's Transforms</li>
	<li>9.1.2 The Large Deviation Rate Function</li>
</ul>
	<li>9.2 A Relationship Between Large Deviation Probabilities for Sums of Random Variables and Those for Sums of Their Cram&eacute;r Transforms. The Probabilistic Meaning of the Rate Function</li>
<ul>	<li>9.2.1 A Relationship Between Large Deviation Probabilities for Sums of Random Variables and Those for Sums of Their Cram&eacute;r Transforms</li>
	<li>9.2.2 The Probabilistic Meaning of the Rate Function</li>
	<li>9.2.3 The Large Deviations Principle</li>
</ul>
	<li>9.3 Integro-Local, Integral and Local Theorems on Large Deviation Probabilities in the Cram&eacute;r Range</li>
<ul>	<li>9.3.1 Integro-Local and Integral Theorems</li>
	<li>9.3.2 Local Theorems</li>
</ul>
	<li>9.4 Integro-Local Theorems at the Boundary of the Cram&eacute;r Range</li>
<ul>	<li>9.4.1 Introduction</li>
	<li>9.4.2 The Probabilities of Large Deviations of Sn in an o(n)-Vicinity of the Point alpha+ n; the Case psi"(lambda+)&lt;infty</li>
	<li>9.4.3 The Class of Distributions ER. The Probability of Large Deviations of Sn in an o(n)-Vicinity of the Point alpha+ n for Distributions F from the Class ER in Case psi"(lambda+)=infty</li>
	<li>9.4.4 On the Large Deviation Probabilities in the Range alpha&gt;alpha+ for Distributions from the Class ER</li>
</ul>
	<li>9.5 Integral and Integro-Local Theorems on Large Deviation Probabilities for Sums Sn when the Cram&eacute;r Condition Is not Met</li>
<ul>	<li>9.5.1 Integral Theorems</li>
	<li>9.5.2 Integro-Local Theorems</li>
</ul>
	<li>9.6 Integro-Local Theorems on the Probabilities of Large Deviations of Sn Outside the Cram&eacute;r Range (Under the Cram&eacute;r Condition)</li>
</ul>
	<li>Chapter 10: Renewal Processes</li>
<ul>	<li>10.1 Renewal Processes. Renewal Functions</li>
<ul>	<li>10.1.1 Introduction</li>
	<li>10.1.2 The Integral Renewal Theorem for Non-identically Distributed Summands</li>
</ul>
	<li>10.2 The Key Renewal Theorem in the Arithmetic Case</li>
	<li>10.3 The Excess and Defect of a Random Walk. Their Limiting Distribution in the Arithmetic Case</li>
	<li>10.4 The Renewal Theorem and the Limiting Behaviour of the Excess and Defect in the Non-arithmetic Case</li>
	<li>10.5 The Law of Large Numbers and the Central Limit Theorem for Renewal Processes</li>
<ul>	<li>10.5.1 The Law of Large Numbers</li>
	<li>10.5.2 The Central Limit Theorem</li>
	<li>10.5.3 A Theorem on the Finiteness of the Inﬁmum of the Cumulative Sums</li>
	<li>10.5.4 Stochastic Inequalities. The Law of Large Numbers and the Central Limit Theorem for the Maximum of Sums of Non-identically Distributed Random Variables Taking Values of Both Signs</li>
	<li>10.5.5 Extension of Theorems 10.5.1 and 10.5.2 to Random Variables Assuming Values of Both Signs</li>
	<li>10.5.6 The Local Limit Theorem</li>
</ul>
	<li>10.6 Generalised Renewal Processes</li>
<ul>	<li>10.6.1 Deﬁnition and Some Properties</li>
	<li>10.6.2 The Central Limit Theorem</li>
	<li>10.6.3 The Integro-Local Theorem</li>
</ul>
</ul>
	<li>Chapter 11: Properties of the Trajectories of Random Walks. Zero-One Laws</li>
<ul>	<li>11.1 Zero-One Laws. Upper and Lower Functions</li>
<ul>	<li>11.1.1 Zero-One Laws</li>
	<li>11.1.2 Lower and Upper Functions</li>
</ul>
	<li>11.2 Convergence of Series of Independent Random Variables</li>
	<li>11.3 The Strong Law of Large Numbers</li>
	<li>11.4 The Strong Law of Large Numbers for Arbitrary Independent Variables</li>
	<li>11.5 The Strong Law of Large Numbers for Generalised Renewal Processes</li>
<ul>	<li>11.5.1 The Strong Law of Large Numbers for Renewal Processes</li>
	<li>11.5.2 The Strong Law of Large Numbers for Generalised Renewal Processes</li>
</ul>
</ul>
	<li>Chapter 12: Random Walks and Factorisation Identities</li>
<ul>	<li>12.1 Factorisation Identities</li>
<ul>	<li>12.1.1 Factorisation</li>
	<li>12.1.2 The Canonical Factorisation of the Function fz(lambda)=1-zphi(lambda)</li>
	<li>12.1.3 The Second Factorisation Identity</li>
</ul>
	<li>12.2 Some Consequences of Theorems 12.1.1-12.1.3</li>
<ul>	<li>12.2.1 Direct Consequences</li>
	<li>12.2.2 A Generalisation of the Strong Law of Large Numbers</li>
</ul>
	<li>12.3 Pollaczek-Spitzer's Identity. An Identity for S=supk&gt;=0Sk</li>
<ul>	<li>12.3.1 Pollaczek-Spitzer's Identity</li>
	<li>12.3.2 An Identity for S=supk&gt;=0Sk</li>
</ul>
	<li>12.4 The Distribution of S in Insurance Problems and Queueing Theory</li>
<ul>	<li>12.4.1 Random Walks in Risk Theory</li>
	<li>12.4.2 Queueing Systems</li>
	<li>12.4.3 Stochastic Models in Continuous Time</li>
</ul>
	<li>12.5 Cases Where Factorisation Components Can Be Found in an Explicit Form. The Non-lattice Case</li>
<ul>	<li>12.5.1 Preliminary Notes on the Uniqueness of Factorisation</li>
	<li>12.5.2 Classes of Distributions on the Positive Half-Line with Rational Ch.F.s</li>
	<li>12.5.3 Explicit Canonical Factorisation of the Function v(lambda) in the Case when the Right Tail of the Distribution F Is an Exponential Polynomial</li>
	<li>12.5.4 Explicit Factorisation of the Function v(lambda) when the Left Tail of the Distribution F Is an Exponential Polynomial</li>
	<li>12.5.5 Explicit Canonical Factorisation for the Function v0(lambda)</li>
</ul>
	<li>12.6 Explicit Form of Factorisation in the Arithmetic Case</li>
<ul>	<li>12.6.1 Preliminary Remarks on the Uniqueness of Factorisation</li>
	<li>12.6.2 The Classes of Distributions on the Positive Half-Line with Rational Generating Functions</li>
	<li>12.6.3 Explicit Canonical Factorisation of the Function v(z) in the Case when the Right Tail of the Distribution F Is an Exponential Polynomial</li>
	<li>12.6.4 Explicit Canonical Factorisation of the Function v(z) when the Left Tail of the Distribution F Is an Exponential Polynomial</li>
	<li>12.6.5 Explicit Factorisation of the Function v 0(z)</li>
</ul>
	<li>12.7 Asymptotic Properties of the Distributions of chi&plusmn; and S</li>
<ul>	<li>12.7.1 The Asymptotics of P (chi+&gt;x|eta+&lt;infty) and P(chi-0&lt;-x) in the Case Exi&lt;=0</li>
	<li>12.7.2 The Asymptotics of P (S&gt;x)</li>
	<li>12.7.3 The Distribution of the Maximal Values of Generalised Renewal Processes</li>
</ul>
	<li>12.8 On the Distribution of the First Passage Time</li>
<ul>	<li>12.8.1 The Properties of the Distributions of the Times eta&plusmn;</li>
	<li>12.8.2 The Distribution of the First Passage Time of an Arbitrary Level x by Arithmetic Skip-Free Walks</li>
</ul>
</ul>
	<li>Chapter 13: Sequences of Dependent Trials. Markov Chains</li>
<ul>	<li>13.1 Countable Markov Chains. Deﬁnitions and Examples. Classiﬁcation of States</li>
<ul>	<li>13.1.1 Deﬁnition and Examples</li>
	<li>13.1.2 Classiﬁcation of States</li>
</ul>
	<li>13.2 Necessary and Sufﬁcient Conditions for Recurrence of States. Types of States in an Irreducible Chain. The Structure of a Periodic Chain</li>
	<li>13.3 Theorems on Random Walks on a Lattice</li>
<ul>	<li>13.3.1 Symmetric Random Walks in Rk, k &gt;=2</li>
	<li>13.3.2 Arbitrary Symmetric Random Walks on the Line</li>
</ul>
	<li>13.4 Limit Theorems for Countable Homogeneous Chains</li>
<ul>	<li>13.4.1 Ergodic Theorems</li>
	<li>13.4.2 The Law of Large Numbers and the Central Limit Theorem for the Number of Visits to a Given State</li>
</ul>
	<li>13.5 * The Behaviour of Transition Probabilities for Reducible Chains</li>
	<li>13.6 Markov Chains with Arbitrary State Spaces. Ergodicity of Chains with Positive Atoms</li>
<ul>	<li>13.6.1 Markov Chains with Arbitrary State Spaces</li>
	<li>13.6.2 Markov Chains Having a Positive Atom</li>
</ul>
	<li>13.7 * Ergodicity of Harris Markov Chains</li>
<ul>	<li>13.7.1 The Ergodic Theorem</li>
	<li>13.7.2 On Conditions (I) and (II)</li>
</ul>
	<li>13.8 Laws of Large Numbers and the Central Limit Theorem for Sums of Random Variables Deﬁned on a Markov Chain</li>
<ul>	<li>13.8.1 Random Variables Deﬁned on a Markov Chain</li>
	<li>13.8.2 Laws of Large Numbers</li>
	<li>13.8.3 The Central Limit Theorem</li>
</ul>
</ul>
	<li>Chapter 14: Information and Entropy</li>
<ul>	<li>14.1 The Deﬁnitions and Properties of Information and Entropy</li>
	<li>14.2 The Entropy of a Finite Markov Chain. A Theorem on the Asymptotic Behaviour of the Information Contained in a Long Message; Its Applications</li>
<ul>	<li>14.2.1 The Entropy of a Sequence of Trials Forming a Stationary Markov Chain</li>
	<li>14.2.2 The Law of Large Numbers for the Amount of Information Contained in a Message</li>
	<li>14.2.3 The Asymptotic Behaviour of the Number of the Most Common Outcomes in a Sequence of Trials</li>
</ul>
</ul>
	<li>Chapter 15: Martingales</li>
<ul>	<li>15.1 Deﬁnitions, Simplest Properties, and Examples</li>
	<li>15.2 The Martingale Property and Random Change of Time. Wald's Identity</li>
	<li>15.3 Inequalities</li>
<ul>	<li>15.3.1 Inequalities for Martingales</li>
	<li>15.3.2 Inequalities for the Number of Crossings of a Strip</li>
</ul>
	<li>15.4 Convergence Theorems</li>
	<li>15.5 Boundedness of the Moments of Stochastic Sequences</li>
</ul>
	<li>Chapter 16: Stationary Sequences</li>
<ul>	<li>16.1 Basic Notions</li>
	<li>16.2 Ergodicity (Metric Transitivity), Mixing and Weak Dependence</li>
	<li>16.3 The Ergodic Theorem</li>
</ul>
	<li>Chapter 17: Stochastic Recursive Sequences</li>
<ul>	<li>17.1 Basic Concepts</li>
	<li>17.2 Ergodicity and Renovating Events. Boundedness Conditions</li>
<ul>	<li>17.2.1 Ergodicity of Stochastic Recursive Sequences</li>
	<li>17.2.2 Boundedness of Random Sequences</li>
</ul>
	<li>17.3 Ergodicity Conditions Related to the Monotonicity of f</li>
	<li>17.4 Ergodicity Conditions for Contracting in Mean Lipschitz Transformations</li>
</ul>
	<li>Chapter 18: Continuous Time Random Processes</li>
<ul>	<li>18.1 General Deﬁnitions</li>
	<li>18.2 Criteria of Regularity of Processes</li>
</ul>
	<li>Chapter 19: Processes with Independent Increments</li>
<ul>	<li>19.1 General Properties</li>
	<li>19.2 Wiener Processes. The Properties of Trajectories</li>
	<li>19.3 The Laws of the Iterated Logarithm</li>
	<li>19.4 The Poisson Process</li>
	<li>19.5 Description of the Class of Processes with Independent Increments</li>
</ul>
	<li>Chapter 20: Functional Limit Theorems</li>
<ul>	<li>20.1 Convergence to the Wiener Process</li>
	<li>20.2 The Law of the Iterated Logarithm</li>
	<li>20.3 Convergence to the Poisson Process</li>
<ul>	<li>20.3.1 Convergence of the Processes of Cumulative Sums</li>
	<li>20.3.2 Convergence of Sums of Thinning Renewal Processes</li>
</ul>
</ul>
	<li>Chapter 21: Markov Processes</li>
<ul>	<li>21.1 Deﬁnitions and General Properties</li>
<ul>	<li>21.1.1 Deﬁnition and Basic Properties</li>
	<li>21.1.2 Transition Probability</li>
</ul>
	<li>21.2 Markov Processes with Countable State Spaces. Examples</li>
<ul>	<li>21.2.1 Basic Properties of the Process</li>
	<li>21.2.2 Examples</li>
</ul>
	<li>21.3 Branching Processes</li>
	<li>21.4 Semi-Markov Processes</li>
<ul>	<li>21.4.1 Semi-Markov Processes on the States of a Chain</li>
	<li>21.4.2 The Ergodic Theorem</li>
	<li>21.4.3 Semi-Markov Processes on Chain Transitions</li>
</ul>
	<li>21.5 Regenerative Processes</li>
<ul>	<li>21.5.1 Regenerative Processes. The Ergodic Theorem</li>
	<li>21.5.2 The Laws of Large Numbers and Central Limit Theorem for Integrals of Regenerative Processes</li>
</ul>
	<li>21.6 Diffusion Processes</li>
</ul>
	<li>Chapter 22: Processes with Finite Second Moments. Gaussian Processes</li>
<ul>	<li>22.1 Processes with Finite Second Moments</li>
	<li>22.2 Gaussian Processes</li>
	<li>22.3 Prediction Problem</li>
</ul>
	<li>Appendix 1: Extension of a Probability Measure</li>
	<li>Appendix 2: Kolmogorov's Theorem on Consistent Distributions</li>
	<li>Appendix 3: Elements of Measure Theory and Integration</li>
<ul>	<li>3.1 Measure Spaces</li>
	<li>3.2 The Integral with Respect to a Probability Measure</li>
<ul>	<li>3.2.1 The Integrals of a Simple Function</li>
	<li>3.2.2 The Integrals of an Arbitrary Function</li>
	<li>3.2.3 Properties of Integrals</li>
</ul>
	<li>3.3 Further Properties of Integrals</li>
<ul>	<li>3.3.1 Convergence Theorems</li>
	<li>3.3.2 Connection to Integration with Respect to a Measure on the Real Line</li>
	<li>3.3.3 Product Measures and Iterated Integrals</li>
</ul>
	<li>3.4 The Integral with Respect to an Arbitrary Measure</li>
	<li>3.5 The Lebesgue Decomposition Theorem and the Radon-Nikodym Theorem</li>
	<li>3.6 Weak Convergence and Convergence in Total Variation of Distributions in Arbitrary Spaces</li>
<ul>	<li>3.6.1 Weak Convergence</li>
	<li>3.6.2 Convergence in Total Variation</li>
</ul>
</ul>
	<li>Appendix 4: The Helly and Arzel&agrave;-Ascoli Theorems</li>
	<li>Appendix 5: The Proof of the Berry-Esseen Theorem</li>
	<li>Appendix 6: The Basic Properties of Regularly Varying Functions and Subexponential Distributions</li>
<ul>	<li>6.1 General Properties of Regularly Varying Functions</li>
	<li>6.2 The Basic Asymptotic Properties</li>
	<li>6.3 The Asymptotic Properties of the Transforms of R.V.F.s (Abel-Type Theorems)</li>
	<li>6.4 Subexponential Distributions and Their Properties</li>
</ul>
	<li>Appendix 7: The Proofs of Theorems on Convergence to Stable Laws</li>
<ul>	<li>7.1 The Integral Limit Theorem</li>
	<li>7.2 The Integro-Local and Local Limit Theorems</li>
</ul>
	<li>Appendix 8: Upper and Lower Bounds for the Distributions of the Sums and the Maxima of the Sums of Independent Random Variables</li>
<ul>	<li>8.1 Upper Bounds Under the Cram&eacute;r Condition</li>
	<li>8.2 Upper Bounds when the Cram&eacute;r Condition Is Not Met</li>
	<li>8.3 Lower Bounds</li>
</ul>
	<li>Appendix 9: Renewal Theorems</li>
	<li>References</li>
	<li>Index of Basic Notation</li>
	<li>Subject Index</li>
</ul>
</body></html>