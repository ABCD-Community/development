<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Statistics in
</p>
<p>Criminal Justice
</p>
<p>Fourth Edition
</p>
<p>David Weisburd
</p>
<p>Chester Britt</p>
<p/>
</div>
<div class="page"><p/>
<p>Statistics in Criminal Justice</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Statistics in 
</p>
<p>Criminal Justice 
 
</p>
<p> Edition 
</p>
<p>David Weisburd 
</p>
<p>Hebrew University of Jerusalem, Jerusalem, Israel
</p>
<p>Fourth
</p>
<p>  
</p>
<p>Chester Britt 
</p>
<p>Northeastern University, Boston, MA, USA
</p>
<p>and 
</p>
<p>and George Mason University, Fairfax, VA, USA</p>
<p/>
</div>
<div class="page"><p/>
<p>David Weisburd Chester Britt 
</p>
<p>Institute of Criminology 
School of Criminology and Criminal Justice Faculty of Law 
Northeastern University 
</p>
<p>Hebrew University of Jerusalem    
Jerusalem, Israel 
</p>
<p> 
ISBN 978-1-4614-9169-9 ISBN 978-1-4614-9170-5 (eBook) 
DOI 10.1007/978-1-4614-9170-5 
Springer New York Heidelberg Dordrecht London 
 
Library of Congress Control Number: 
 
&copy; Springer Science+Business Media New York 2014 
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is 
concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on 
microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, 
computer software, or by similar or dissimilar methodology now known or hereafter developed. Exempted from this legal 
reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied specifically for the purpose 
of being entered and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of this 
publication or parts thereof is permitted only under the provisions of the Copyright Law of the Publisher&rsquo;s location, in its 
current version, and permission for use must always be obtained from Springer. Permissions for use may be obtained through 
RightsLink at the Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, 
even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and 
therefore free for general use. 
While the advice and information in this book are believed to be true and accurate at the date of publication, neither  
the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. 
The publisher makes no warranty, express or implied, with respect to the material contained herein. 
 
Printed on acid-free paper 
 
Springer is part of Springer Science+Business Media (www.springer.com) 
 
 
</p>
<p>Boston, MA, USA 
</p>
<p>2013952914
</p>
<p>and 
 
</p>
<p>George Mason University 
Fairfax, VA, USA 
</p>
<p>Department of Criminology, Law and Society
</p>
<p>Additional material to this book can be downloaded from http://extras.springer.com</p>
<p/>
<div class="annotation"><a href="http://www.springer.com">http://www.springer.com</a></div>
</div>
<div class="page"><p/>
<p>For Bryan, who made the desert bloom, used
</p>
<p>sun to brighten the night, and brought such joy
</p>
<p>to family and friends
</p>
<p>D. W.
</p>
<p>v
</p>
<p>For my parents, Chester and Lila, who have been
</p>
<p>a constant source of support
</p>
<p>C. B.</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>iv C H A P T E R N U M B E R :  C H A P T E R T I T L E
</p>
<p>Contents
</p>
<p>1
</p>
<p>Statistics Are Used to Solve Problems 4
</p>
<p>The Uses of Statistics 7
</p>
<p>Measurement: The Basic Building Block of Research 13
Science and Measurement: Classification as a First Step in Research 14
</p>
<p>Levels of Measurement 15
</p>
<p>Relating Interval, Ordinal, and Nominal Scales: The Importance of Collecting Data 
</p>
<p>at the Highest Level Possible 22
</p>
<p>What Is a Good Measure? 23
</p>
<p>Representing and Displaying Data 36
What Are Frequency Distributions and Histograms? 37
</p>
<p>Extending Histograms to Multiple Groups: Using Bar Charts 43
</p>
<p>Using Bar Charts with Nominal or Ordinal Data 50
</p>
<p>Pie Charts 51
</p>
<p>Time Series Data 52
</p>
<p>Describing the Typical Case: Measures of Central Tendency 65
The Mode: Central Tendency in Nominal Scales 66
</p>
<p>The Median: Taking into Account Position 68
</p>
<p>The Mean: Adding Value to Position 74
</p>
<p>Statistics in Practice: Comparing the Median and the Mean 82
</p>
<p>How Typical Is the Typical Case?: Measuring Dispersion 94
</p>
<p>The Purpose of Statistics Is to Clarify 3
</p>
<p>Measuring Dispersion in Interval Scales: The Range, Variance, and Standard Deviation 102
</p>
<p>Introduction: Statistics as a Research Tool
</p>
<p>P r e f a c e
</p>
<p>C h a p t e r  o n e
</p>
<p>C h a p t e r  t w o
</p>
<p>C h a p t e r  t h r e e
</p>
<p>C h a p t e r  f o u r
</p>
<p>C h a p t e r  f i v e
</p>
<p>x i i i
</p>
<p>vii
</p>
<p>Basic Principles Apply Across Statistical Techniques  5
</p>
<p>Measures of Dispersion for Nominal- and Ordinal-Level Data 95</p>
<p/>
</div>
<div class="page"><p/>
<p>The Logic of Statistical Inference: Making Statements 
About Populations from Sample Statistics 125
</p>
<p>The Dilemma: Making Statements About Populations from Sample Statistics 126
</p>
<p>The Research Hypothesis 129
</p>
<p>The Null Hypothesis 131
</p>
<p>Risks of Error in Hypothesis Testing 133
</p>
<p>Risks of Error and Statistical Levels of Significance 135
</p>
<p>Departing from Conventional Significance Criteria 1 7
</p>
<p>Defining the Observed Significance Level of a Test: 
A Simple Example Using the Binomial Distribution 145
</p>
<p>The Fair Coin Toss 147
</p>
<p>Different Ways of Getting Similar Results 151
</p>
<p>Solving More Complex Problems 154
</p>
<p>The Binomial Distribution 155
</p>
<p>Using the Binomial Distribution to Estimate the Observed Significance Level of a Test 159
</p>
<p>Steps in a Statistical Test: Using the Binomial Distribution 
to Make Decisions About Hypotheses
</p>
<p>Chi-Square: A Test Commonly Used for Nominal-Level Measures
</p>
<p>Extending the Chi-Square Test to a Relationship Between Two Ordinal Variables: Identification with Fathers
</p>
<p>The Normal Distribution and Its Application to Tests 
</p>
<p>171
The Problem: The Impact of Problem-Oriented Policing on Disorderly Activity at Violent-Crime Hot Spots 172
</p>
<p>Assumptions: Laying the Foundations for Statistical Inference 174
</p>
<p>Selecting a Sampling Distribution 180
</p>
<p>Significance Level and Rejection Region 182
</p>
<p>The Test Statistic 187
</p>
<p>Making a Decision 187
</p>
<p>197
Testing Hypotheses Concerning the Roll of a Die 198
</p>
<p>Relating Two Nominal-Scale Measures in a Chi-Square Test 206
</p>
<p>Extending the Chi-Square Test to Multicategory Variables: The Example of Cell Allocations in Prison 212
</p>
<p>and Delinquent Acts 217
</p>
<p>The Use of Chi-Square When Samples Are Small: A Final Note 222
</p>
<p>of Statistical Significance 234
The Normal Frequency Distribution, or Normal Curve 235
</p>
<p>Applying Normal Sampling Distributions to Nonnormal Populations 247
</p>
<p>Comparing a Sample to an Unknown Population: The Single-Sample z-Test for Proportions 252
</p>
<p>Comparing a Sample to an Unknown Population: The Single-Sample t-Test for Means 257
</p>
<p>C h a p t e r  s i x
</p>
<p>C h a p t e r  s e v e n
</p>
<p>C h a p t e r  e i g h t
</p>
<p>C h a p t e r  n i n e
</p>
<p>C h a p t e r  t e n
</p>
<p>C O N T E N T Sviii
</p>
<p>3</p>
<p/>
</div>
<div class="page"><p/>
<p>Distinguishing Statistical Significance and Strength of Relationship: 
</p>
<p>Measuring Association for Interval-Level Data: 
Pearson&rsquo;s Correlation Coefficient
</p>
<p>Testing the Statistical Significance of Pearson&rsquo;s r
</p>
<p>Testing the Statistical Significance of Spearman&rsquo;s r
</p>
<p>An Introduction to Bivariate Regression
</p>
<p>Multivariate Regression
</p>
<p>Comparing Means and Proportions in Two Samples 269
Comparing Sample Means 270
</p>
<p>Comparing Sample Proportions: The Two-Sample t-Test for Differences of Proportions 282
</p>
<p>The t-Test for Dependent Samples 288
</p>
<p>A Note on Using the t-Test for Ordinal Scales 293
</p>
<p>Comparing Means Among More Than Two Samples: Analysis of Variance
Analysis of Variance 307
</p>
<p>Defining the Strength of the Relationship Observed 328
</p>
<p>Making Pairwise Comparisons Between the Groups Studied 331
</p>
<p>A Nonparametric Alternative: The Kruskal-Wallis Test 334
</p>
<p>Measures of Association for Nominal and Ordinal Variables 351
</p>
<p>The Example of the Chi-Square Statistic 352
</p>
<p>Measures of Association for Nominal Variables 355
</p>
<p>Measures of Association for Ordinal-Level Variables 367
</p>
<p>Choosing the Best Measure of Association for Nominal- and Ordinal-Level Variables 385
</p>
<p>398
Measuring Association Between Two Interval-Level Variables 399
</p>
<p>Pearson&rsquo;s Correlation Coefficient 401
</p>
<p>Spearman&rsquo;s Correlation Coefficient 419
</p>
<p>421
</p>
<p>428
</p>
<p>439
Estimating the Influence of One Variable on Another: The Regression Coefficient 440
</p>
<p>Prediction in Regression: Building the Regression Line 445
</p>
<p>Evaluating the Regression Model 453
</p>
<p>The F-Test for the Overall Regression 467
</p>
<p>481
The Importance of Correct Model Specifications 482
</p>
<p>Correctly Specifying the Regression Model 494
</p>
<p>C h a p t e r  e l e v e n
</p>
<p>C h a p t e r  t w e l v e
</p>
<p>C h a p t e r  t h i r t e e n
</p>
<p>C h a p t e r  f o u r t e e n
</p>
<p>C h a p t e r  f i f t e e n
</p>
<p>C h a p t e r  s i x t e e n
</p>
<p>C O N T E N T S ix
</p>
<p>306</p>
<p/>
</div>
<div class="page"><p/>
<p>Logistic Regression
</p>
<p>Multivariate Regression: Additional Topics 
Non-linear Relationships 516
</p>
<p>Interaction Effects 522 
</p>
<p>An Example: Punishment Severity 533
</p>
<p>An Example: Race and Punishment Severity 525
</p>
<p>The Problem of Multicollinearity 534
</p>
<p>548
Why Is It Inappropriate to Use OLS Regression for a Dichotomous Dependent Variable? 550
</p>
<p>Logistic Regression 555
</p>
<p>Interpreting Logistic Regression Coefficients 567
</p>
<p>Comparing Logistic Regression Coefficients 577
</p>
<p>Evaluating the Logistic Regression Model 583
</p>
<p>Statistical Significance in Logistic Regression 587
</p>
<p>C h a p t e r  s e v e n t e e n
</p>
<p>C h a p t e r  e i g h t e e n
</p>
<p>C O N T E N T Sx
</p>
<p>514
</p>
<p>C h a p t e r  n i n e t e e n
</p>
<p>Special Topics: Randomized Experiments    674 
</p>
<p>Sample Size, Equivalence, and Statistical Power 683 
</p>
<p>C h a p t e r  t w e n t y  o n e
</p>
<p>Statistical Power
</p>
<p>Examining Interaction Terms in Experimental Research 695
</p>
<p>The Structure of a Randomized Experiment
</p>
<p>The Main Advantage of Experiments: Isolating Causal Effects 677 
</p>
<p>Internal Validity 682
</p>
<p>and Block Randomization 691
</p>
<p>Using Covariates to Increase Statistical Power in Experimental Studies 693
</p>
<p>C h a p t e r  t w e n t y  
</p>
<p>Multilevel Regression Models    637
Variance Components Model 640
</p>
<p>Random Intercept Model 646
</p>
<p>Random Coefficient Model 655
</p>
<p>Adding Cluster (Level 2) Characteristics 660
</p>
<p>Special Topics: Confidence Intervals
</p>
<p>Constructing Confidence Intervals
</p>
<p>Confidence Intervals 704
</p>
<p>C h a p t e r  t w e n t y  t w o
</p>
<p>676
</p>
<p>Multivariate Regression with Multiple Category Nominal or Ordinal Measures: 
Extending the Basic Logistic Regression Model    601
</p>
<p>Multinomial Logistic Regression 603
</p>
<p>Ordinal Logistic Regression 615
</p>
<p>Substantive Example: Severity of Punishment Decisions 619
</p>
<p>702
</p>
<p>708</p>
<p/>
</div>
<div class="page"><p/>
<p>C O N T E N T S xi
</p>
<p>C h a p t e r  t w e n t y  t h r e e
</p>
<p>Critical Values of �2
</p>
<p>Critical Value for P (Pcrit
</p>
<p>Factorials 759
</p>
<p>Distribution 760
</p>
<p>Areas of the Standard Normal Distribution 761
</p>
<p>Critical Values of Student&rsquo;s t Distribution 762
</p>
<p>Critical Values of the F-Statistic 763
</p>
<p>), Tukey&rsquo;s HSD Test 766
</p>
<p>Critical Values for Spearman&rsquo;s Rank-Order Correlation Coefficient 767
</p>
<p>Fisher r-to-Z* Transformation 768
</p>
<p>Glossary 770
</p>
<p>Index 778
</p>
<p>Appendix 1
</p>
<p>Appendix 2
</p>
<p>Appendix 3
</p>
<p>Appendix 4
</p>
<p>Appendix 5
</p>
<p>Appendix 6
</p>
<p>Appendix 7
</p>
<p>Appendix 8
</p>
<p>Special Topics: Statistical Power
Statistical Power
</p>
<p>Estimating Statistical Power and Sample Size for a Statistically Powerful Study 738
</p>
<p>Summing Up: Avoiding Studies Designed for Failure 747
</p>
<p>726
728
</p>
<p>Components of Statistical Power 731</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>Oliver Wendell Holmes, the distinguished associate justice of the
</p>
<p>Supreme Court, was noted for his forgetfulness. On a train leaving
</p>
<p>Washington, D.C., he is said to have been approached by a
</p>
<p>awkward moments, the conductor recognized the distinctive-
</p>
<p>however, is said to have looked sternly at the conductor and
</p>
<p>responded, &ldquo;Young man, the problem is not where is my ticket;
</p>
<p>the problem is where am I going.&rdquo;
</p>
<p>basic understanding of statistics in this field. In the first chapter, the main
</p>
<p>themes of the text are outlined and discussed. This preface describes
</p>
<p>how the text is organized.
</p>
<p>The text takes a building-block approach. This means that each chap-
</p>
<p>ter helps prepare you for the chapters that follow. It also means that the
</p>
<p>level of sophistication of the text increases as the text progresses. Basic
</p>
<p>concepts discussed in early chapters provide a foundation for the intro-
</p>
<p>duction of more complex statistical issues later. One advantage to this
</p>
<p>approach is that it is easy to see, as time goes on, how much you have
</p>
<p>learned about statistics. Concepts that would have seemed impossible to
</p>
<p>of the book now, you will see equations that are quite forbidding. How-
</p>
<p>ever, when you come to these equations after covering the material in
</p>
<p>earlier chapters, you will be surprised at how easy they are to under-
</p>
<p>stand.
</p>
<p>Throughout the text, there is an emphasis on comprehension and not
</p>
<p>his case and his pockets, could not locate his pass. After a few
</p>
<p>conductor who requested his ticket. Holmes, searching through 
</p>
<p>the rail company the ticket when he found it. Justice Holmes,
</p>
<p>looking and well-known jurist and suggested that he just send 
</p>
<p>For the student of statistics, a textbook is like a train ticket. Not only does
</p>
<p>it provide a pass the student can use for entering a new and useful area
</p>
<p>of study; it also defines the route that will be taken and the goals that
</p>
<p>are important to achieve. Different textbooks take different approaches
</p>
<p>and emphasize different types of material. Statistics in Criminal Jus-
</p>
<p>tice emphasizes the uses of statistics in research in crime and justice.
</p>
<p>This text is meant for students and professionals who want to gain a
</p>
<p>examine real-life criminal justice problems. In the opening chapters of the
</p>
<p>sible but sophisticated understanding of statistics that can be used to 
</p>
<p>simple when you encounter them later on. If you turn to the final chapters
</p>
<p>understand, had they been introduced at the outset, are surprisingly
</p>
<p>xiii
</p>
<p>computation. This approach is meant to provide readers with an acces-</p>
<p/>
</div>
<div class="page"><p/>
<p>book, basic themes and materials are presented. Chapter 1 provides an
</p>
<p>introduction to how we use statistics in criminal justice and the problems
</p>
<p>we face in applying statistics to real-life research problems. Chapters 2
</p>
<p>through 5 introduce basic concepts of measurement and basic methods
</p>
<p>build on the themes covered in these early chapters.
</p>
<p>One of the fundamental problems researchers face is that they seek
</p>
<p>to make statements about large populations (such as all U.S. citizens)
</p>
<p>but are generally able to collect information or data on only a sample,
</p>
<p>or smaller group, drawn from such populations. In Chapters 6 through
</p>
<p>12, the focus is on how researchers use statistics to overcome this
</p>
<p>special problems are encountered in criminal justice research, and how
</p>
<p>should the researcher approach them? Some texts skip over the basics,
</p>
<p>Having examined how we can make statements about populations
</p>
<p>from information gained from samples, we turn to how we describe the
</p>
<p>strength of association between variables. In the social sciences, it is
</p>
<p>often essential not only to determine whether factors are related but also
</p>
<p>to define the strength and character of those relationships. Accordingly,
</p>
<p>in Chapters 13 and 14, we look at measures of association, and in Chap-
</p>
<p>remember that the more advanced statistics presented in later chapters
</p>
<p>Many of the statistics provided here will be familiar to you; however, 
</p>
<p>ing statements about populations based on samples? What are the  
</p>
<p>problem. What is the logic that underlies the statistics we use for mak-
</p>
<p>for graphically representing data and using statistics to describe data.
</p>
<p>different types of statistical procedures or tests that can be used? What
</p>
<p>moving students from test to test before they understand the logic 
</p>
<p>behind the tests. The approach here is to focus in greater detail on 
</p>
<p>relatively simple statistical decisions before moving on to more com-
</p>
<p>plex ones.
</p>
<p>P R E F A C Exiv
</p>
<p>In the concluding chapters, we look at three special topics. Chapter 
</p>
<p>estimates that you obtain from a sample. Because our emphasis is on
</p>
<p>research in criminal justice, we conclude the text with a chapter that
</p>
<p>examines methods for evaluating and improving the design of a research
</p>
<p>project. The statistical concept that is central to Chapter 23&mdash;statistical
</p>
<p>ters 15 through 20, we examine bivariate and different types of multi-
</p>
<p>a method for assessing how much trust you can place in the specific
</p>
<p>power&mdash;follows directly from the concepts developed in prior chapters.
</p>
<p>it has become a central concern in criminal justice research and accord-
</p>
<p>Statistical power is often ignored in introductory statistics texts. However,
</p>
<p>While it is always difficult in statistics to decide where an introduc-
</p>
<p>tory text should stop, with an understanding of these techniques you 
</p>
<p>variate regression. These are likely to be new topics for you, though they 
</p>
<p>are statistics commonly used in criminal justice.
</p>
<p>criminal justice interventions. Chapter 22 describes confidence intervals,
</p>
<p>21 focuses on the design of randomized experiments. Randomized experi-
</p>
<p>ments allow criminal justice researchers to be confident about the causal
</p>
<p>relationships between variables, and are often used in the evaluation of
</p>
<p> in this text.ingly is given emphasis</p>
<p/>
</div>
<div class="page"><p/>
<p>point in learning statistics, but also to leave you with the confidence and
</p>
<p>A working knowledge of computers is not required to understand
</p>
<p>the statistical concepts or procedures presented in the text. However,
</p>
<p>computers have become a very important part of research in statistics,
</p>
<p>and thus we provide computer exercises for relevant chapters and a
</p>
<p>tools to tackle more complex problems on your own. Each chapter starts
</p>
<p>with a statement of the basic concepts and problems addressed and ends
</p>
<p>with a full chapter summary. There is also a list of equations, when
</p>
<p>relevant, at the end of the chapter. These materials should help you to
</p>
<p>review what you have learned and to identify the basic knowledge you
</p>
<p>need to move on to subsequent chapters. 
</p>
<p>All of the chapters contain a list of key terms with short definitions. 
</p>
<p>The key terms appear in boldface the first time they are mentioned in 
</p>
<p>the chapter. Sometimes a term may have been briefly explained in an 
</p>
<p>earlier chapter, but is designated as a key term in the chapter where 
</p>
<p>the concept is more central. A general glossary of key terms appears at 
</p>
<p>at the end. The questions are designed to make you think about the
</p>
<p>subjects covered in the chapter. Sometimes they are straightforward,
</p>
<p>following directly from the text. Sometimes they ask you to develop 
</p>
<p>ideas in slightly different ways than in the text. In constructing the ques-
</p>
<p>tions, we sought to make working on statistical issues as much fun as
</p>
<p>possible. In statistics, it is crucial to go over material more than once.
</p>
<p>The questions are meant to reinforce the knowledge you have gained.
</p>
<p>will have the basic tools to comprehend and conduct criminal justice
</p>
<p>research. Of course, these tools constitute a building block for more
</p>
<p>advanced methods. The goal of the text is not only to bring you to this
</p>
<p>web site where you can access the data needed for those exercises
</p>
<p>P R E F A C E xv
</p>
<p>the end of the book. Chapters 2 through 23 each have a set of questions
</p>
<p>Statistics in Criminal Justice will allow you to approach statistics in a
</p>
<p>familiar context. It emphasizes the statistics and the problems that are
</p>
<p>commonly encountered in criminal justice research. It focuses on under-
</p>
<p>standing rather than computation. However, it takes a serious approach
</p>
<p>to statistics, which is relevant to the real world of research in crime and
</p>
<p>justice. The text is meant not only as an introduction for students but
</p>
<p>also as a reference for researchers. The approach taken will help both
</p>
<p>students who want an introduction to statistics and professionals who
</p>
<p>seek a straightforward explanation for statistics that have become a 
</p>
<p>routine tool in contemporary criminal justice systems.
</p>
<p>You are encouraged to use the web site. It will help you to see the
</p>
<p>connection between the topics discussed in the chapters and statistical
</p>
<p>computing.
</p>
<p>(see the Computer Exercises at the end of Chapter 2 for details).</p>
<p/>
</div>
<div class="page"><p/>
<p>A c k n o w l e d g m e n t s
</p>
<p>xvi
</p>
<p>Professor Joseph Naus of the Department of Statistics of Rutgers Univer-
</p>
<p>In the development of any academic enterprise, many students and 
</p>
<p>colleagues provide support and advice. We are particularly indebted to 
</p>
<p>of the new material for the third edition. A number of current and former 
</p>
<p>original edition of this book, and to Daniel Salem, a graduate of the Insti-
</p>
<p>sity, who played a crucial advisory role in the preparation of the
</p>
<p>Stephen Schnebly of the University of Illinios at Springfield, and Marc
</p>
<p>work and provided comments over the previous editions, particularly the 
</p>
<p>tute of Criminology of the Hebrew University, who played a major role
</p>
<p>of South Carolina was particularly helpful in helping us to refine some
</p>
<p>in the production of the first edition. Robert Brame of the University  
</p>
<p>graduate students helped us in revising and editing the manuscript 
</p>
<p>across the four editions, including SueMing Yang and Kristen Miggans at 
</p>
<p>the University of Maryland, and Shomron Moyal, Rochelle Schnurr, Tal 
</p>
<p>Yonaton and Gali Weissman of the Hebrew University, and Arian Ferrer 
</p>
<p>We also want to thank the many scholars who have read and used our
Emmanuelle Klossou, and Michael Rocque of Northeastern University.
</p>
<p>comments and suggestions made by Todd Armstrong of Sam Houston 
</p>
<p>Swatt of Justice &amp; Security Strategies, Inc. The final product reflects their 
</p>
<p>keen insights and thoughtful suggestions.
</p>
<p>State University, Pamela Lattimore of the Research Triangle Institute,</p>
<p/>
</div>
<div class="page"><p/>
<p>xvii
</p>
<p>A b o u t  t h e  A u t h o r s
</p>
<p>David Weisburd is a Distinguished Professor of Criminology, Law and 
</p>
<p>Society at George Mason University (and Director of the Center for 
</p>
<p>Evidence Based Crime Policy) and Walter E. Meyer Professor of Law and 
</p>
<p>Criminal Justice at the Hebrew University. He also serves as a Senior 
</p>
<p>Fellow at the Police Foundation in Washington DC and is Chair of its 
</p>
<p>Research Advisory Committee. Professor Weisburd is an elected Fellow of 
</p>
<p>the American Society of Criminology and of the Academy of Experimental 
</p>
<p>Criminology. He is a member of the Science Advisory Board of the Office 
</p>
<p>of Justice Programs, the Steering Committee of the Campbell Crime and 
</p>
<p>Justice Group, the National Institute of Justice/Harvard Executive Session 
</p>
<p>in Policing, the Scientific Commission of the International Society of 
</p>
<p>Criminology, and the Committee on Law and Justice of the National 
</p>
<p>Research Council of the National Academy of Sciences (USA). Professor 
</p>
<p>Weisburd is one of the leading international researchers in crime and 
</p>
<p>justice. He is author or editor of more than twenty books and more than 
</p>
<p>100 scientific articles that cover a wide range of criminal justice research 
</p>
<p>topics, including crime at place, white collar crime, policing, illicit markets, 
</p>
<p>terrorism, criminal justice statistics and social deviance. Professor Weisburd 
</p>
<p>is the recipient of the 2010 Stockholm Prize in Criminology and the 2011 
</p>
<p>Klachky Family Prize for the Advancement of the Frontiers of Science. 
</p>
<p>Professor Weisburd is also the founding editor of the Journal of Experi-
</p>
<p>mental Criminology. 
</p>
<p>Chester L. Britt is a researcher and scholar in criminology and criminal
</p>
<p>from the University of Arizona, Professor Britt taught at the University
</p>
<p>journals as Law and Society Review, Justice Quarterly, journal of Quanti-
</p>
<p>and Journal of Research in Crime and Deliquency.
</p>
<p>justice. He is Dean of the School of Criminology and Criminal Justice at
</p>
<p>of Illinois, Penn State University, and Arizona State University. He served 
</p>
<p>Justice Quarterly from 2005 through 2007. He has also 
</p>
<p>co-edited a volume on criminological theory entitled Control Theories
</p>
<p>of Crime and Delinquency. His research articles have appeared in such
</p>
<p>Northeastern University in Boston. After receiving his Ph.D. in sociology
</p>
<p>as the Editor of 
</p>
<p>tative Criminonology, </p>
<p/>
</div>
<div class="page"><p/>
<p>Introduction: Statistics as a Research Tool
</p>
<p>Do Statisticians Have to Be Experts in Mathematics?
</p>
<p>Are Computers Making Statisticians Redundant?
</p>
<p>What Basic Principles Apply to Different Types of Statistics?
</p>
<p>What Are the Different Uses of Statistics in Research?
</p>
<p>C h a p t e r  o n e
</p>
<p>I n i t i a l  h u r d l e s
</p>
<p>K e y  p r i n c i p l e s
</p>
<p>What is Our Aim in Choosing a Statistic?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_1,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>tool for answering questions. It allows us to take large bodies of infor-
</p>
<p>mation and summarize them with a few simple statements. It lets us
</p>
<p>come to solid conclusions even when the realities of the research world
</p>
<p>make it difficult to isolate the problems we seek to study. Without statis-
</p>
<p>tics, conducting research about crime and justice would be virtually im-
</p>
<p>possible. Yet, there is perhaps no other subject in their university studies
</p>
<p>that criminal justice students find so difficult to approach.
</p>
<p>A good part of the difficulty lies in the links students make between
</p>
<p>statistics and math. A course in statistics is often thought to mean long
</p>
<p>hours spent solving equations. In developing your understanding of sta-
</p>
<p>tistics in criminal justice research, you will come to better understand the
</p>
<p>formulas that underlie statistical methods, but the focus will be on con-
</p>
<p>cepts and not on computations. There is just no way to develop a good
</p>
<p>understanding of statistics without doing some work by hand. But in the
</p>
<p>age of computers, the main purpose of doing computations is to gain a
</p>
<p>deeper understanding of how statistics work.
</p>
<p>Researchers no longer spend long hours calculating statistics. In the
</p>
<p>1950s, social scientists would work for months developing results that
</p>
<p>can now be generated on a computer in a few minutes. Today, you do
</p>
<p>not need to be a whiz kid in math to carry out a complex statistical
</p>
<p>analysis. Such analyses can be done with user-friendly computer pro-
</p>
<p>grams. Why then do you need a course in statistics? Why not just leave it
</p>
<p>to the computer to provide answers? Why do you still need to learn the
</p>
<p>basics?
</p>
<p>The computer is a powerful tool and has made statistics accessible to
</p>
<p>a much larger group of criminal justice students and researchers. How-
</p>
<p>ever, the best researchers still spend many hours on statistical analysis.
</p>
<p>Now that the computer has freed us from long and tedious calculations,
</p>
<p>what is left is the most challenging and important part of statistical analy-
</p>
<p>sis: identifying the statistical tools that will best serve researchers in inter-
</p>
<p>preting their research for others.
</p>
<p>2
</p>
<p>THE PURPOSE OF STATISTICAL ANALYSIS is to clarify and not confuse. It is a</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E P U R P O S E O F S T A T I S T I C S I S T O 3
</p>
<p>need to choose statistics for research and interpret them. It is meant for
</p>
<p>students of criminology and criminal justice. As in other fields, there are
</p>
<p>specific techniques that are commonly used and specific approaches that
</p>
<p>have been developed over time by researchers who specialize in this
</p>
<p>area of study. These are the focus of this text. Not only do we draw our
</p>
<p>examples from crime and justice issues; we also pay particular attention
</p>
<p>to the choices that criminal justice researchers make when approaching
</p>
<p>statistical problems.
</p>
<p>Before we begin our study of statistics in criminal justice, it is useful
</p>
<p>to state some basic principles that underlie the approach taken in this
</p>
<p>text. They revolve around four basic questions. First, what should our
</p>
<p>purpose be in choosing a statistic? Second, why do we use statistics to
</p>
<p>answer research questions? Third, what basic principles apply across
</p>
<p>very different types of statistics? And finally, what are the different uses
</p>
<p>for statistics in research?
</p>
<p>guage. In this sense, statistics provide a way for the initiated to share
</p>
<p>ideas and concepts without including the rest of us. Of course, it is nec-
essary to use a common language to report research results. This is one
</p>
<p>reason why it is important for you to take a course in statistics. But the
</p>
<p>reason we use statistics is to make research results easier&mdash;not more dif-
</p>
<p>would be just to tell us about your subjects. You could describe each
</p>
<p>offender and his or her criminal history without creating any real
</p>
<p>confusion. But what if you wanted to report on 20 offenders? It would
</p>
<p>take quite a long time to tell us about each one in some detail, and it 
</p>
<p>is likely that we would have difficulty remembering who was who. 
</p>
<p>It would be even more difficult to describe 100 offenders. With
</p>
<p>thousands of offenders, it would be just about impossible to take this
</p>
<p>approach.
</p>
<p>This is one example of how statistics can help to simplify and clarify
</p>
<p>the research process. Statistics allow you to use a few summary state-
ments to provide a comprehensive portrait of a large group of offenders.
</p>
<p>For example, instead of providing the name of each offender and telling
</p>
<p>us how many crimes he or she committed, you could present a single
</p>
<p>ficult&mdash;to understand. For example, if you wanted to provide a description 
</p>
<p>of three offenders you had studied, you would not need to search for statis-
</p>
<p>tics to summarize your results. The simplest way to describe your sample
</p>
<p>The goal of this text is to provide you with the basic skills you will
</p>
<p>It sometimes seems as if researchers use statistics as a kind of secret lan-
</p>
<p>C L A R I F Y
</p>
<p>T h e  P u r p o s e  o f  S t a t i s t i c s  I s  t o  C l a r i f y  </p>
<p/>
</div>
<div class="page"><p/>
<p>4 C H A P T E R O N E :  I N T R O D U C T I O N
</p>
<p>statistic that described the average number of crimes committed by the
</p>
<p>people you studied. You might say that, on average, the people you
</p>
<p>studied committed two to three crimes in the last year. Thus, although it
</p>
<p>might be impossible to describe each person you studied, you could, by
</p>
<p>using a statistic, give your audience an overall picture. Statistics make it
</p>
<p>possible to summarize information about a large number of subjects with
</p>
<p>a few simple statements.
</p>
<p>Given that statistics should simplify the description of research results,
</p>
<p>it follows that the researcher should utilize the simplest statistics appro-
</p>
<p>priate for answering the research questions that he or she raises.
</p>
<p>Nonetheless, it sometimes seems as if researchers go out of their way to
</p>
<p>identify statistics that few people recognize and even fewer understand.
</p>
<p>This approach does not help the researcher or his or her audience.
</p>
<p>There is no benefit in using statistics that are not understood by those in-
</p>
<p>terested in your research findings. Using a more complex statistic when a
</p>
<p>simpler one is appropriate serves no purpose beyond reducing the num-
</p>
<p>ber of people who will be influenced by your work.
</p>
<p>The best presentation of research findings will communicate results in
</p>
<p>a clear and understandable way. When using complex statistics, the re-
</p>
<p>searcher should present them in as straightforward a manner as possible.
</p>
<p>The mark of good statisticians is not that they can mystify their audi-
</p>
<p>ences, but rather that they can communicate even complex results in a
</p>
<p>way that most people can understand.
</p>
<p>S t a t i s t i c s  A r e  U s e d  t o  S o l v e  P r o b l e m s
</p>
<p>Statistics develop because of a need to deal with a specific type of ques-
</p>
<p>tion or problem. In the example above, you were faced with the
</p>
<p>dilemma that you could not describe each person in a very large study
</p>
<p>without creating a good deal of confusion. We suggested that an average
</p>
<p>might provide a way of using one simple statistic to summarize a charac-
</p>
<p>teristic of all the people studied. The average is a statistical solution. It is
</p>
<p>a tool for solving the problem of how to describe many subjects with a
</p>
<p>short and simple statement.
</p>
<p>As you will see in later chapters, statistics have been developed to
</p>
<p>deal with many different types of problems that researchers face. Some
</p>
<p>of these may seem difficult to understand at the outset, and indeed it is
</p>
<p>natural to be put off by the complexities of some statistics. However, the
</p>
<p>solutions that statisticians develop are usually based on simple common
</p>
<p>sense. Contrary to what many people believe, statistics follow a logic
</p>
<p>that you will find quite easy to follow. Once you learn to trust your com-
</p>
<p>mon sense, learning statistics will turn out to be surprisingly simple. Indeed,</p>
<p/>
</div>
<div class="page"><p/>
<p>5
</p>
<p>encounter in this text. Stating them at the outset will help you to see
</p>
<p>how statistical procedures in later chapters are linked one to another. To
</p>
<p>understand these principles, you do not need to develop any computa-
</p>
<p>tions or formulas; rather, you need to think generally about what we are
</p>
<p>trying to achieve when we develop statistics.
</p>
<p>The first is simply that in developing statistics we seek to reduce the
</p>
<p>level of error as much as possible. The purpose of research is to provide
</p>
<p>answers to research questions. In developing these answers, we want to
</p>
<p>be as accurate as we can. Clearly, we want to make as few mistakes as
</p>
<p>possible. The best statistic is one that provides the most accurate state-
</p>
<p>ment about your study. Accordingly, a major criterion in choosing which
</p>
<p>statistic to use&mdash;or indeed in defining how a statistic is developed&mdash;is the
</p>
<p>amount of error that a statistic incorporates. In statistics, we try to mini-
</p>
<p>mize error whenever possible.
</p>
<p>Unfortunately, it is virtually impossible to develop any description
</p>
<p>without some degree of error. This fact is part of everyday reality. For
</p>
<p>example, we do not expect that our watches will tell perfect time or that
</p>
<p>our thermostats will be exactly correct. At the same time, we all know
</p>
<p>that there are better watches and thermostats and that one of the factors
</p>
<p>that leads us to define them as &ldquo;better&rdquo; is that they provide information
</p>
<p>with less error. Similarly, although we do not expect our stockbroker to
</p>
<p>possible. Fear of statistics is a greater barrier to learning than any of the com-
</p>
<p>putations or formulas that we will use. It is difficult to learn anything when 
</p>
<p>you approach it with great foreboding. Statistics is a lot easier than you think.
</p>
<p>The job of this text is to take you step by step through the principles and 
</p>
<p>ideas that underlie basic statistics for criminal justice researchers. At the 
</p>
<p>beginning, we will spend a good deal of time examining the logic behind 
</p>
<p>statistics and illustrating how and why statisticians choose a particular 
</p>
<p>that the solutions statisticians use make very good sense.
</p>
<p>solution to a particular statistical problem. What you must do at the outset 
</p>
<p>is take a deep breath and give statistics a chance. Once you do, you will find 
</p>
<p>our experience is that students who have good common sense, even
</p>
<p>if they have very little formal background in this area, tend to become
</p>
<p>A few basic principles underlie much of the statistical reasoning you will
</p>
<p>be correct all of the time, we are likely to choose the broker who we
</p>
<p>believe will make the fewest mistakes.
</p>
<p>the best criminal justice statisticians. But in order to be able to use com- 
</p>
<p>B A S I C  P R I N C I P L E S  A P P L Y  A C R O S S  S T A T I S T I C A L  T E C H N I Q U E S
</p>
<p>mon sense, it is important to approach statistics with as little fear as
</p>
<p>B a s i c  P r i n c i p l e s  A p p l y  A c r o s s  S t a t i s t i c a l  T e c h n i q u e s</p>
<p/>
</div>
<div class="page"><p/>
<p>6 C H A P T E R O N E :  I N T R O D U C T I O N
</p>
<p>In choosing a statistic, we also use a second principle to which we
</p>
<p>will return again and again in this text: Statistics based on more informa-
</p>
<p>tion are generally preferred over those based on less information. This
</p>
<p>principle is common to all forms of intelligence gathering and not just
</p>
<p>those that we use in research. Good decision making is based on infor-
</p>
<p>mation. The more information available to the decision maker, the better
</p>
<p>he or she can weigh the different options that are presented. The same
</p>
<p>goes for statistics. A statistic that is based on more information, all else
</p>
<p>being equal, will be preferred over one that utilizes less information.
</p>
<p>There are exceptions to this rule, often resulting from the quality or form
</p>
<p>of the information or data collected. We will discuss these in detail in the
</p>
<p>text. But as a rule, the best statistic utilizes the maximum amount of
</p>
<p>information.
</p>
<p>Our third principle relates to a danger that confronts us in using statis-
</p>
<p>tics as a tool for describing information. In many studies, there are cases
</p>
<p>that are very different from all of the others. Indeed, they are so different
</p>
<p>that they might be termed deviant cases or, as statisticians sometimes call
</p>
<p>them, &ldquo;outliers.&rdquo; For example, in a study of criminal careers, there may
</p>
<p>be one or two offenders who have committed thousands of crimes,
</p>
<p>whereas the next most active criminal in the sample has committed only
</p>
<p>choice of statistics and your presentation of results.
</p>
<p>In almost every statistic we will study, outliers present a distinct and
</p>
<p>troublesome problem. A deviant case can make it look as if your offend-
</p>
<p>ers are younger or older than they really are&mdash;or less or more criminally
</p>
<p>active than they really are. Importantly, deviant cases often have the
</p>
<p>most dramatic effects on more complex statistical procedures. And it is
</p>
<p>precisely here, where the researcher is often preoccupied with other sta-
</p>
<p>tistical issues, that deviant cases go unnoticed. But whatever statistic is
</p>
<p>used, the principle remains the same: Outliers present a significant prob-
</p>
<p>lem in choosing and interpreting statistics.
</p>
<p>The final principle is one that is often unstated in statistics, because it
</p>
<p>is assumed at the outset: Whatever the method of research, the researcher
</p>
<p>must strive to systematize the procedures used in data collection and
</p>
<p>analysis. As Albert J. Reiss, Jr., a pioneer in criminal justice methodolo-
</p>
<p>gies, has noted, &ldquo;systematic&rdquo; means in part &ldquo;that observation and record-
</p>
<p>ing are done according to explicit procedures which permit replication
</p>
<p>and that rules are followed which permit the use of scientific inference.&rdquo;1
</p>
<p>1A. J. Reiss, Jr., &ldquo;Systematic Social Observation of Social Phenomenon,&rdquo; in Herbert
</p>
<p>Costner (ed.), Sociological Methodology (San Francisco: Jossey Bass, 1971), pp. 3&ndash;33.
</p>
<p>a few hundred crimes. Although such cases form a natural part of the
</p>
<p>research process, they often have very significant implications for your</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E U S E S O F S T A T I S T I C S 7
</p>
<p>While Reiss&rsquo;s comment will become clearer as statistical concepts are
</p>
<p>defined in coming chapters, his point is simply that you must follow
</p>
<p>clearly stated procedures and rules in developing and presenting statisti-
</p>
<p>follow a consistent logic from start to finish. You should not jump from
</p>
<p>statistic to statistic merely because the outcomes are favorable to the the-
</p>
<p>sis you raise. In learning about statistics, it is also important to go step by
</p>
<p>step&mdash;and to be well organized and prepared. You cannot learn statistics
</p>
<p>by cramming in the last week of classes. The key to learning statistics is
</p>
<p>to adopt a systematic process and follow it each week.
</p>
<p>Statistical procedures are built on all of the research steps that pre-
</p>
<p>cede them. If these steps are faulty, then the statistics themselves will be
</p>
<p>faulty. In later chapters, we often talk about this process in terms of the
</p>
<p>assumptions of the statistics that we use. We assume that all of the rules
</p>
<p>of good research have been followed up to the point where we decide
</p>
<p>on a statistic and calculate it. Statistics cannot be disentangled from the
</p>
<p>larger research process that comes before them. The numbers that we
</p>
<p>systematic approach is crucial not only to the statistical procedures that
</p>
<p>you will learn about in this text but to the whole research process.
</p>
<p>three ways in which statistics are used in criminal justice. The first is
</p>
<p>called descriptive statistics, because it helps in the summary and de-
</p>
<p>scription of research findings. The second, inferential or inductive sta-
</p>
<p>tistics, allows us to make inferences or statements about large groups
</p>
<p>from studies of smaller groups, or samples, drawn from them. Finally,
</p>
<p>we introduce multivariate statistics toward the end of the text. Multi-
</p>
<p>variate statistics, as the name implies, allow us to examine a series of
</p>
<p>variables at one time.
</p>
<p>Descriptive Statistics
</p>
<p>We are all familiar in some way with descriptive statistics. We use them
</p>
<p>often in our daily lives, and they appear routinely in newspapers and on
</p>
<p>television. Indeed, we use them so often that we sometimes don&rsquo;t think
</p>
<p>cal findings. It is important to approach statistics in a systematic way. You 
</p>
<p>cannot be sloppy or haphazard, at least if the statistic is to provide a good
</p>
<p>answer to the research question you raise. The choice of a statistic should
</p>
<p>In the chapters that follow, we will examine three types of statistics or
</p>
<p>employed. Very complex statistics cannot hide bad research methods. A
</p>
<p>use are only as good as the data collection techniques that we have 
</p>
<p>T h e  U s e s  o f  S t a t i s t i c s</p>
<p/>
</div>
<div class="page"><p/>
<p>8 C H A P T E R O N E :  I N T R O D U C T I O N
</p>
<p>of them as statistics at all. During an election year, everyone is con-
</p>
<p>cerned about the percentage support that each candidate gains in the
</p>
<p>primaries. Students at the beginning of the semester want to know what
</p>
<p>proportion of their grades will be based on weekly exercises. In deciding
</p>
<p>whether our salaries are fair, we want to know what the average salary is
</p>
<p>for other people in similar positions. These are all descriptive statistics.
</p>
<p>They summarize in one simple statement the characteristics of many
</p>
<p>people. As discussed above in the example concerning criminal histories,
</p>
<p>descriptive statistics make it possible for us to summarize or describe
</p>
<p>large amounts of information.
</p>
<p>In the chapters that follow, we will be concerned with two types of
</p>
<p>descriptive statistics: measures of central tendency and measures of
</p>
<p>dispersion. Measures of central tendency are measures of typicality.
</p>
<p>They tell us in one statement what the average case is like. If we could
</p>
<p>take only one person as the best example for all of the subjects we stud-
</p>
<p>ied, who would it be? If we could choose only one level of criminal ac-
</p>
<p>tivity to typify the frequency of offending of all subjects, what level
</p>
<p>would provide the best snapshot? If we wanted to give our audience a
</p>
<p>general sense of how much, on average, a group of offenders stole in a
</p>
<p>year, what amount would provide the best portrait? Percentages, propor-
</p>
<p>tions, and means are all examples of measures of central tendency that
</p>
<p>we commonly use. In the coming chapters, you will learn more about
</p>
<p>these statistics, as well as more complex measures with which you may
</p>
<p>not be familiar, such as correlation and regression coefficients.
</p>
<p>Having a statistic that describes the average case is very helpful in de-
</p>
<p>scribing research results. However, we might also want to know how
</p>
<p>typical this average case is of the subjects in our study. The answer to
</p>
<p>this question is provided by measures of dispersion. They tell us to what
</p>
<p>extent the other subjects we studied are similar to the case or statistic we
</p>
<p>have chosen to represent them. Although we don&rsquo;t commonly use mea-
</p>
<p>sures of dispersion in our daily lives, we do often ask similar questions
</p>
<p>without the use of such statistics.
</p>
<p>For example, in deciding whether our income is fair, we might want
</p>
<p>to know not only the average income of others in similar positions, but
</p>
<p>also the range of incomes that such people have. If the range was very
</p>
<p>small, we would probably decide that the average provides a fairly good
</p>
<p>portrait of what we should be making. If the range was very large, we
</p>
<p>might want to investigate more carefully why some people make so
</p>
<p>much more or less than the average. The range is a measure of disper-
</p>
<p>sion. It tells us about the spread of scores around our statistic. In the
</p>
<p>chapters that follow, we will look at other measures of dispersion&mdash;for
</p>
<p>example, the standard deviation and variance, which may be less famil-
</p>
<p>iar to you. Without these measures, our presentation of research findings</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E U S E S O F S T A T I S T I C S 9
</p>
<p>would be incomplete. It is not enough simply to describe the typical
</p>
<p>case; we must also describe to what degree other cases in our study are
</p>
<p>different from or similar to it.
</p>
<p>Inferential Statistics
</p>
<p>Inferential statistics allow us to make statements about a population, or
</p>
<p>the larger group of people we seek to study, on the basis of a sample
</p>
<p>drawn from that population. Without this very important and powerful
</p>
<p>tool, it would be very difficult to conduct research in criminal justice.
</p>
<p>The reason is simple. When we conduct research, we do so to answer
</p>
<p>questions about populations. But in reality we seldom are able to collect
</p>
<p>information on the whole population, so we draw a sample from it. Sta-
</p>
<p>tistical inference makes it possible for us to infer characteristics from that
</p>
<p>sample to the population.
</p>
<p>Why is it that we draw samples if we are really interested in making
</p>
<p>statements about populations? In good part it is because gaining informa-
</p>
<p>tion on most populations is impractical and/or too expensive. For exam-
</p>
<p>ple, if we seek to examine the attitudes of U.S. citizens toward criminal
</p>
<p>justice processing, we are interested in how all citizens feel. However,
</p>
<p>studying all citizens would be a task of gigantic proportion and would
</p>
<p>cost billions of dollars. Such surveys are done every few years and are
</p>
<p>called censuses. The last census in the United States took many years to
</p>
<p>prepare and implement and cost over $5 billion to complete. If every re-
</p>
<p>search study of the American population demanded a census, then we
</p>
<p>would likely cost millions of dollars to complete a simple study of their
</p>
<p>attitudes. This is because the most inexpensive data collection can still
</p>
<p>cost tens of dollars for each subject studied. When you consider that the
</p>
<p>National Institute of Justice, the primary funder of criminal justice re-
</p>
<p>search in the United States, provides a total of about $100 million a year
</p>
<p>for all research, it is clear that criminal justice research cannot rely on
</p>
<p>studies of whole populations.
</p>
<p>It is easy to understand, then, why we want to draw a sample or sub-
</p>
<p>set of the larger population to study, but it is not obvious why we should
</p>
<p>believe that what we learn from that sample applies to the population
</p>
<p>from which it is drawn. How do we know, for example, that the atti-
</p>
<p>tudes toward criminal justice expressed by a sample of U.S. citizens are
</p>
<p>similar to the attitudes of all citizens? The sample is a group of people
</p>
<p>would have very few research projects indeed. Even when we are interested
</p>
<p>in much smaller populations in the criminal justice system, examination of
</p>
<p>the entire population is often beyond the resources of the criminal justice
</p>
<p>researcher. For example, to study all U.S. prisoners, we would have to
</p>
<p>study over 1 million people.
</p>
<p>Even if we wanted to look at the 100,000 or so women prisoners, it</p>
<p/>
</div>
<div class="page"><p/>
<p>10 C H A P T E R O N E :  I N T R O D U C T I O N
</p>
<p>drawn from the population; it is not the population itself. How much
</p>
<p>surveys that now form so much a part of public life or the studies that
</p>
<p>you read about in your other college classes. When a news organization
</p>
<p>sions about populations&mdash;whether of offenders, criminal justice agents,
</p>
<p>crime-prone places, or criminal justice events&mdash;on samples. Statistical in-
</p>
<p>ference provides a method for deciding to what extent you can have
</p>
<p>faith in such results. It allows you to decide when the outcome observed
</p>
<p>in a sample can be generalized to the population from which it was
</p>
<p>drawn. Statistical inference is a very important part of statistics and one
</p>
<p>we will spend a good deal of time discussing in this text.
</p>
<p>Taking into Account Competing Explanations: Multivariate Statistics
</p>
<p>Multivariate statistics allow us to solve a different type of problem in re-
</p>
<p>search. It is often the case that the issue on which we want to focus is
</p>
<p>confounded by other factors in our study. Multivariate statistics allow us
</p>
<p>to isolate one factor while taking into account a host of others. For ex-
</p>
<p>ample, a number of criminal justice studies examine the impact of im-
</p>
<p>prisonment on the future criminal behavior of offenders. In general, they
</p>
<p>compare offenders who are found guilty in court and sentenced to
</p>
<p>once they are released into the community, is different from that of non-
</p>
<p>prisoners. Researchers conducting these studies face a very difficult re-
</p>
<p>search problem. Prisoners and nonprisoners are often very different
</p>
<p>types of people, and some of these differences are likely to affect their
</p>
<p>impact of imprisonment on future offending. If we discover that prison-
</p>
<p>ers, once released into the community, are more likely than nonprison-
</p>
<p>ers to commit a crime, how can we tell whether this was a result of the
</p>
<p>can we rely on such estimates? And to what extent can we trust such statis-
</p>
<p>tics?  You have probably raised such issues already, in regard to either the
</p>
<p>conducts a survey of 1,000 people to tell us how all voters will vote in the
</p>
<p>next election, it is using a sample to make statements about a population.
</p>
<p>The criminal justice studies you read about also base their conclu-
</p>
<p>prison with those who are found guilty but do not receive a prison sanction.
</p>
<p> Such studies focus on whether the criminal behavior of prisoners,
</p>
<p>criminal behavior in the community. For example, prisoners are more likely
</p>
<p>than nonprisoners to have been arrested before, since a prior arrest is often
an important factor in the judge&rsquo;s decision to incarcerate a convicted off-
</p>
<p>ender in the first place. And we know from research about criminal careers 
</p>
<p>that people with a prior history of arrest are much more likely than people 
</p>
<p>without such a history to commit a crime in the future. Accordingly, prisoners 
</p>
<p>are more likely to commit a crime in the future, irrespective of the fact that 
</p>
<p>they have served a prison sentence. This makes it very difficult to assess the</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S U M M A R Y 11
</p>
<p>experience of imprisonment? It might be due to the simple fact that pris-
</p>
<p>oners are more likely than nonprisoners to commit crimes in the first
</p>
<p>place. Their more serious arrest histories would predict this result.
</p>
<p>The complex task facing the criminal justice researcher is to isolate
</p>
<p>the specific impact of imprisonment itself from all of the other possible
</p>
<p>explanations for differences in reoffending between prisoners and non-
</p>
<p>prisoners. Multivariate analysis provides a statistical solution to this prob-
</p>
<p>lem. It allows the criminal justice researcher to isolate the impact of one
</p>
<p>factor&mdash;in this case, imprisonment&mdash;from those of other factors, such as
</p>
<p>prior criminal history, that might confound the researcher&rsquo;s conclusions.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>Statistics seem intimidating because they are associated with complex
</p>
<p>mathematical formulas and computations. Although some knowledge of
</p>
<p>math is required, an understanding of the concepts is much more im-
</p>
<p>portant than an in-depth understanding of the computations. Today&rsquo;s
</p>
<p>computers, which can perform complex calculations in a matter of sec-
</p>
<p>onds or fractions of seconds, have drastically cut the workload of the
</p>
<p>researcher. They cannot, however, replace the key role a researcher
</p>
<p>plays in choosing the most appropriate statistical tool for each research
</p>
<p>problem.
</p>
<p>The researcher&rsquo;s aim in using statistics is to communicate findings in a
</p>
<p>clear and simple form. As a result, the researcher should always choose
</p>
<p>the simplest statistic appropriate for answering the research question.
</p>
<p>ing principles apply to all types of statistics: (1) In developing statistics,
</p>
<p>we seek to reduce the level of error as much as possible. (2) Statistics
</p>
<p>based on more information are generally preferred over those based on
</p>
<p>less information. (3) Outliers present a significant problem in choosing
</p>
<p>and interpreting statistics. (4) The researcher must strive to systematize
</p>
<p>the procedures used in data collection and analysis.
</p>
<p>There are three principal uses of statistics discussed in this book. In
</p>
<p>descriptive statistics, the researcher summarizes large amounts of in-
</p>
<p>formation in an efficient manner. Two types of descriptive statistics that
</p>
<p>go hand in hand are measures of central tendency, which describe
</p>
<p>the characteristics of the average case, and measures of dispersion,
</p>
<p>which tell us just how typical this average case is. We use inferential
</p>
<p>statistics to make statements about a population on the basis of a sam-
</p>
<p>ple drawn from that population. Finally, in multivariate statistics, we
</p>
<p>isolate the impact of one factor from others that may distort our results.
</p>
<p>Statistics offer commonsense solutions to research problems. The follow-</p>
<p/>
</div>
<div class="page"><p/>
<p>12 C H A P T E R O N E :  I N T R O D U C T I O N
</p>
<p>K e y  T e r m s
</p>
<p>descriptive statistics A broad area of sta-
</p>
<p>tistics that is concerned with summarizing
</p>
<p>large amounts of information in an efficient
</p>
<p>manner. Descriptive statistics are used to
</p>
<p>describe or represent in summary form the
</p>
<p>characteristics of a sample or population.
</p>
<p>inferential, or inductive, statistics A
</p>
<p>broad area of statistics that provides the re-
</p>
<p>searcher with tools for making statements
</p>
<p>about populations on the basis of knowl-
</p>
<p>edge about samples. Inferential statistics
</p>
<p>allow the researcher to make inferences re-
</p>
<p>garding populations from information
</p>
<p>gained in samples.
</p>
<p>measures of central tendency Descrip-
</p>
<p>tive statistics that allow us to identify the
</p>
<p>typical case in a sample or population.
</p>
<p>Measures of central tendency are measures
</p>
<p>of typicality.
</p>
<p>measures of dispersion Descriptive sta-
</p>
<p>tistics that tell us how tightly clustered or
</p>
<p>dispersed the cases in a sample or popula-
</p>
<p>tion are. They answer the question &ldquo;How
</p>
<p>typical is the typical case?&rdquo;
</p>
<p>multivariate statistics Statistics that 
</p>
<p>examine the relationships among vari-
</p>
<p>ables while taking into account the possi-
</p>
<p>variable from others that may distort his
</p>
<p>or her results.
</p>
<p>ble influences of other confounding factors.
</p>
<p>Multivariate statistics allow the resear-
</p>
<p>cher to isolate the impact of one</p>
<p/>
</div>
<div class="page"><p/>
<p>Measurement: The Basic Building Block 
</p>
<p>of Research
</p>
<p>How Do Criminal Justice Researchers Develop Knowledge?
</p>
<p>How Do the Different Scales Interconnect?
</p>
<p>C h a p t e r  t w o
</p>
<p>C r i m i n a l  j u s t i c e  r e s e a r c h  a s  a  s c i e n c e
</p>
<p>T h e  f o u r  s c a l e s  o f  m e a s u r e m e n t
</p>
<p>D e f i n i n g  a  g o o d  m e a s u r e
</p>
<p>What are They?
</p>
<p>What are Their Characteristics?
</p>
<p>Which is the Most Appropriate Scale of Measurement?
</p>
<p>What is Meant by the &ldquo;Validity&rdquo; of a Measure?
</p>
<p>What is Meant by the &ldquo;Reliability&rdquo; of a Measure?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_2,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>MEASUREMENT LIES AT THE HEART of statistics. Indeed, no statistic would
be possible without the concept of measurement. Measurement is also an
</p>
<p>integral part of our everyday lives. We routinely classify and assign values
</p>
<p>to people and objects without giving much thought to the processes that
</p>
<p>underlie our decisions and evaluations. In statistics, such classification and
</p>
<p>ordering of values must be done in a systematic way. There are clear rules
</p>
<p>for developing different types of measures and defined criteria for decid-
</p>
<p>ing which are most appropriate for answering a specific research question.
</p>
<p>Although it is natural to focus on the end products of research, it is im-
</p>
<p>portant for the researcher to remember that measurement forms the first
</p>
<p>building block of every statistic. Even the most complex statistics, with
</p>
<p>numbers that are defined to many decimal places, are only as accurate as
</p>
<p>the measures upon which they are built. Accordingly, the relatively simple
</p>
<p>rules we discuss in this chapter are crucial for developing solid research
</p>
<p>findings. A researcher can build a very complex structure of analysis. But if
</p>
<p>the measures that form the foundation of the research are not appropriate
</p>
<p>for the analyses that are conducted, the findings cannot be relied upon.
</p>
<p>We begin Chapter 2 by examining the basic idea of measurement in sci-
</p>
<p>ence. We then turn to a description of the main types of measures in statis-
</p>
<p>tics and the criteria used to distinguish among them. We are particularly
</p>
<p>concerned with how statisticians rank measurement based on the amount
</p>
<p>of information that a measure includes. This concept, known as levels of
</p>
<p>measurement, is very important in choosing which statistical procedures are
</p>
<p>appropriate in research. Finally, we discuss some basic criteria for defining
</p>
<p>a good measure.
</p>
<p>S c i e n c e  a n d  M e a s u r e m e n t :  
C l a s s i f i c a t i o n  a s  a  F i r s t  S t e p  i n  R e s e a r c h
</p>
<p>Criminal justice research is a scientific enterprise that seeks to develop
</p>
<p>knowledge about the nature of crimes, criminals, and the criminal justice
</p>
<p>14</p>
<p/>
</div>
<div class="page"><p/>
<p>L E V E L S O F M E A S U R E M E N T 15
</p>
<p>system. The development of knowledge can, of course, be carried out in
</p>
<p>a number of different ways. Criminal justice researchers may, for exam-
</p>
<p>ple, observe the actions of criminal justice agents or speak to offenders.
</p>
<p>They may examine routine information collected by government or crim-
</p>
<p>inal justice agencies or develop new information through analyses of the
</p>
<p>content of records in the criminal justice system. Knowledge may be de-
</p>
<p>veloped through historical review or even through examination of ar-
</p>
<p>chaeological records of legal systems or sanctions of ancient civilizations.
</p>
<p>The methods that criminal justice researchers use vary. What they
</p>
<p>have in common is an underlying philosophy about how knowledge
</p>
<p>may be gained and what scientific research can tell us. This philosophy,
</p>
<p>which is predominant in scientific study in the modern world, is usually
</p>
<p>called positivism.1 At its core is the idea that science is based on facts
</p>
<p>and not values. Science cannot make decisions about the way the world
</p>
<p>should be (although scientific observation may inform such decisions).
</p>
<p>Rather, it allows us to examine and investigate the realities of the world
</p>
<p>as we know it. The major tool for defining this reality in science is mea-
</p>
<p>surement.
</p>
<p>Measurement in science begins with the activity of distinguishing
</p>
<p>groups or phenomena from one another. This process, which is gener-
</p>
<p>ally termed classification, implies that we can place units of scientific
</p>
<p>study&mdash;such as victims, offenders, crimes, or crime places&mdash;in clearly de-
</p>
<p>fined categories. The classification process leads to the creation of vari-
</p>
<p>ables. A variable is a trait, characteristic, or attribute that can be mea-
</p>
<p>sured. What differentiates measurement in science from measurement in
</p>
<p>our everyday lives is that there must be systematic criteria for determin-
</p>
<p>ing both what each category of a variable represents and the boundaries
</p>
<p>between categories. We now turn to a discussion of these criteria as they
</p>
<p>relate to different levels of measurement.
</p>
<p>L e v e l s  o f  M e a s u r e m e n t
</p>
<p>Classification forms the first step in measurement. There are a number of
</p>
<p>different ways we can classify the people, places, or phenomena we
</p>
<p>wish to study. We may be content to simply distinguish one category
</p>
<p>from another. But we may also be interested in how those categories re-
</p>
<p>late to one another. Do some represent more serious crime or less seri-
</p>
<p>ous crime? Can we rank how serious various crimes are in a clear and
</p>
<p>1See D. Black, &ldquo;The Boundaries of Legal Sociology,&rdquo; in D. Black and M. Mileski (eds.),
</p>
<p>The Social Organization of Law (New York: Seminar Press, 1973), pp. 41&ndash;47.</p>
<p/>
</div>
<div class="page"><p/>
<p>16 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>defined order? Is it possible to define exactly how serious one crime is
</p>
<p>relative to another?
</p>
<p>As these types of questions suggest, measurement can be a lot more
</p>
<p>complex than simply distinguishing one group from another. Recogniz-
</p>
<p>ing this complexity, statisticians have defined four basic groups of mea-
</p>
<p>sures, or scales of measurement, based on the amount of information
</p>
<p>that each takes advantage of. The four are generally seen as occupying
</p>
<p>different positions, or levels, on a ladder of measurement (see Figure
</p>
<p>2.1). Following a principle stated in Chapter 1&mdash;that statistics based on
</p>
<p>more information are generally preferred&mdash;measures that include more
</p>
<p>information rank higher on the ladder of measurement.
</p>
<p>Nominal Scales
</p>
<p>At the bottom of the ladder of measurement are nominal scales. Nominal-
</p>
<p>scale variables simply distinguish one phenomenon from another. Sup-
</p>
<p>pose, for example, that you want to measure crime types. In your study,
</p>
<p>you are most interested in distinguishing between violent crime and
</p>
<p>other types of crime. To fulfill the requirements of a nominal scale, and
</p>
<p>thus the minimum requirements of measurement, you need to be able to
</p>
<p>take all of the crime events in your study and place them in one of two
</p>
<p>categories: either violent crime or other crime. There can be no overlap.
</p>
<p>In practice, you might come across many individual events that seem dif-
</p>
<p>ficult to classify. For example, what would you do with a crime event in
</p>
<p>which the offender first stole from his victim and then assaulted him?
</p>
<p>This event includes elements of both violent and property crime. What
</p>
<p>about the case where the offender did not assault the victim, but merely
</p>
<p>Ratio
</p>
<p>Interval
</p>
<p>Ordinal
</p>
<p>Nominal
Categorization
</p>
<p>Order + Categorization
</p>
<p>True Zero + Set Intervals
+ Order + Categorization
</p>
<p>Set Intervals
+ Order + Categorization
</p>
<p>Ladder of MeasurementFigure 2.1
</p>
<p>threatened her? Would you decide to include this in the category of 
</p>
<p>violent crime or other crime?</p>
<p/>
</div>
<div class="page"><p/>
<p>L E V E L S O F M E A S U R E M E N T 17
</p>
<p>In criminology and criminal justice, we often make use of nominal-
</p>
<p>scale variables. Many of these reflect simple dichotomies, like the distinc-
</p>
<p>tion between violent and other crime. For example, criminologists often
</p>
<p>seek to examine differences between men and women in their involve-
</p>
<p>ment in criminality or treatment in the criminal justice system. It is com-
</p>
<p>mon as well to distinguish between those who are sentenced to prison
</p>
<p>and those who are not or those who commit more than one crime (&ldquo;re-
</p>
<p>cidivists&rdquo;) and those who are only one-shot offenders.
</p>
<p>It is often necessary to distinguish among multiple categories of a
</p>
<p>nominal-level variable. For example, if you wanted to describe legal rep-
</p>
<p>resentation in court cases, you would provide a very simplistic picture if
</p>
<p>you simply distinguished between those who had some type of legal
</p>
<p>representation and those who did not. Some of the offenders would be
</p>
<p>likely to have private attorneys and others court-appointed legal repre-
</p>
<p>sentation. Still others might gain help from a legal aid organization or a
</p>
<p>public defender. In order to provide a full portrait of legal representa-
</p>
<p>tion, you would likely want to create a nominal-scale variable with five
</p>
<p>distinct categories: No attorney, Legal aid, Court appointed, Public de-
</p>
<p>fender, and Private attorney. Table 2.1 presents a number of examples of
</p>
<p>nominal-level scales commonly used in criminal justice.
</p>
<p>Nominal-scale measures can include any number of different cate-
</p>
<p>gories. The Uniform Crime Reporting system, which keeps track of ar-
</p>
<p>rests in the United States, includes some 29 categories of crime. These
</p>
<p>Nominal-Scale Variables Commonly Found in Criminal Justice Research
</p>
<p>VARIABLE COMMON CATEGORIES
</p>
<p>Gender Male, Female
Race-Ethnicity Non-Hispanic Black, Non-Hispanic White, Hispanic (any race)
Marital Status Single, Married, Separated, Divorced, Widowed
Pretrial Release Status Detained, Released
Type of Case Disposition Dismissed, Acquitted, Diverted, Convicted
Method of Conviction Negotiated guilty plea, Nonnegotiated guilty plea, Bench trial, Jury trial
Type of Punishment Incarceration, Nonincarceration
</p>
<p>Table 2.1
</p>
<p>In measurement, you must make systematic choices that can be applied
</p>
<p>across events. You cannot decide one way for one event and another 
</p>
<p>way for another. In the situation described above, you might conclude
</p>
<p>that the major issue in your study was the presence of violence. Thus,
</p>
<p>all cases with any violent events would be placed in the violent cate-
</p>
<p>gory. Similarly, you might conclude that violence had to include physical
</p>
<p>victimization. Whatever your choice, to meet the requirements of mea-
</p>
<p>surement you must define clearly where all events in your study are to
</p>
<p>be placed.</p>
<p/>
</div>
<div class="page"><p/>
<p>18 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>range from violent crimes, such as murder or robbery, to vagrancy and
</p>
<p>vandalism. Although there is no statistical difficulty with defining many
</p>
<p>categories, the more categories you include, the more confusing the de-
</p>
<p>scription of the results is likely to be. If you are trying to provide a sense
</p>
<p>of the distribution of crime in your study, it is very difficult to practically
</p>
<p>describe 20 or 30 different crime categories. Keeping in mind that the
</p>
<p>purpose of statistics is to clarify and simplify, you should try to use the
</p>
<p>smallest number of categories that will accurately describe the research
</p>
<p>problem you are examining.
</p>
<p>At the same time, do not confuse collection of data with presentation
</p>
<p>of your findings. You do not lose anything by collecting information in
</p>
<p>the most detailed way that you can. If you collect information with a
</p>
<p>large number of categories, you can always collapse a group of cate-
</p>
<p>gories into one. For example, if you collect information on arrest events
</p>
<p>utilizing the very detailed categories of the criminal law, you can always
</p>
<p>combine them later into more general categories. But if you collect infor-
</p>
<p>mation in more general categories (for example, just violent crime and
</p>
<p>property crime), you cannot identify specific crimes such as robbery or
</p>
<p>car theft without returning to the original source of your information.
</p>
<p>Though nominal-scale variables are commonly used in criminology
</p>
<p>and criminal justice, they provide us with very limited knowledge about
</p>
<p>the phenomenon we are studying. As you will see in later chapters, they
</p>
<p>also limit the types of statistical analyses that the researcher can employ.
</p>
<p>In the hierarchy of measurement, nominal-scale variables form the low-
</p>
<p>est step in the ladder. One step above are ordinal scales.
</p>
<p>Ordinal Scales
</p>
<p>What distinguishes an ordinal from a nominal scale is the fact that we as-
</p>
<p>sign a clear order to the categories included. Now not only can we dis-
</p>
<p>tinguish between one category and another; we also can place these
</p>
<p>categories on a continuum. This is a very important new piece of infor-
</p>
<p>mation; it allows us to rank events and not just categorize them. In the
</p>
<p>case of crime, we might decide to rank in order of seriousness. In mea-
</p>
<p>suring crime in this way, we would not only distinguish among cate-
</p>
<p>gories, such as violent, property, and victimless crimes; we might also
</p>
<p>argue that violent crimes are more serious than property crimes and that
</p>
<p>victimless crimes are less serious than both violent and property crimes.
</p>
<p>We need not make such decisions arbitrarily. We could rank crimes by
</p>
<p>the amount of damage done or the ways in which the general popula-
</p>
<p>tion rates or evaluates different types of crime.
</p>
<p>Ordinal-scale variables are also commonly used in criminal justice
</p>
<p>and criminology. Indeed, many important criminal justice concepts are
</p>
<p>measured in this way. For example, in a well-known London survey of</p>
<p/>
</div>
<div class="page"><p/>
<p>L E V E L S O F M E A S U R E M E N T 19
</p>
<p>2
</p>
<p>Ranking crime by seriousness and measuring people&rsquo;s fear of crime
</p>
<p>tences, damage to victims, complexity of crime, or seriousness of prior
</p>
<p>records of offenders, as illustrated in Table 2.2. What all of these vari-
</p>
<p>ables have in common is that they classify events and order them along
</p>
<p>a continuum. What is missing is a precise statement about how various
</p>
<p>categories differ one from another.
</p>
<p>Interval and Ratio Scales
</p>
<p>Interval scales not only classify and order people or events; they also
</p>
<p>define the exact differences between them. An interval scale requires
</p>
<p>that the intervals measured be equal for all of the categories of the scale
</p>
<p>examined. Thus, an interval-scale measure of prior record would not
</p>
<p>simply rank prior record by seriousness; it would allow us to say how
</p>
<p>much more serious one offender&rsquo;s record was than another&rsquo;s in a stan-
</p>
<p>dard unit of measurement&mdash;for example, number of arrests, convictions,
</p>
<p>or prison stays.
</p>
<p>2See R. Sparks, H. Genn, and D. Dodd, Surveying Victims: A Study of the Measurement
</p>
<p>of Criminal Victimization (New York: Wiley, 1977).
</p>
<p>Ordinal Scale Variables Commonly Found in Criminal Justice Research
</p>
<p>VARIABLE COMMON CATEGORIES
</p>
<p>Level of Education Less than high school, Some high school, High school graduation,
Some college or trade school, College graduate, Graduate/
professional school
</p>
<p>Severity of Injury in an Assault None, Minor&mdash;no medical attention, Minor&mdash;medical attention
required, Major&mdash;medical attention required with no
hospitalization, Major&mdash;medical attention required with
hospitalization
</p>
<p>Attitude and Opinion Survey Strongly disagree, Disagree, No opinion, Agree, Strongly agree; 
Questions Very high, High, Moderate, Low, Very low
</p>
<p>Bail-Release Decision Released on own recognizance, Released on bail, Detained&mdash;
unable to post bail, Denied release
</p>
<p>Type of Punishment Probation/community service, Jail incarceration, Prison
incarceration, Death sentence
</p>
<p>Table 2.2
</p>
<p>victimization, fear of crime was measured using a simple four-level
</p>
<p>ordinal scale. Researchers asked respondents: &ldquo;Are you personally con-
</p>
<p>cerned about crime in London as a whole? Would you say you are (1)
</p>
<p>very concerned, (2) quite concerned, (3) a little concerned, or (4) not
</p>
<p>concerned at all?&rdquo;
</p>
<p>are only two examples of the use of ordinal scales in criminal justice
</p>
<p>research. We could also draw examples regarding severity of court sen-</p>
<p/>
</div>
<div class="page"><p/>
<p>20 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>Most criminal justice variables that meet the criteria of an interval scale
</p>
<p>also meet the criteria of a ratio scale. A ratio scale has all of the charac-
</p>
<p>teristics of an interval scale but also requires that there be a non-arbitrary,
</p>
<p>or true, zero value. This means simply that zero represents the absence
</p>
<p>that the former has 15 more arrests than the latter. We have an important
</p>
<p>piece of information that we would not have gained with an ordinal
</p>
<p>scale. Now, not only can we say that the prior record of one offender is
</p>
<p>more serious than that of another, but we can specify exactly how many
</p>
<p>more arrests the offender has. This variable thus meets the requirements
</p>
<p>of an interval scale. But it also meets the additional requirement of a
</p>
<p>ratio scale that there be a true zero value, since we can state that some-
</p>
<p>one with 20 arrests has 4 times as many arrests as someone with 5 ar-
</p>
<p>rests. If the zero value were arbitrary, we could not make this statement.
</p>
<p>This fact is best illustrated with an example. Suppose we alter our
</p>
<p>measure of prior record to focus on the degree to which offenders ex-
</p>
<p>ceed a specific threshold of prior offending. Let&rsquo;s say that our threshold
</p>
<p>is 4 prior arrests and we are interested only in offenders who have 4 or
</p>
<p>more prior arrests. An offender with 5 arrests would gain a score of 1 on
</p>
<p>this new measure, and an offender with 20 arrests would have a score of
</p>
<p>16. An offender with 4 arrests would have a score of 0. This variable
</p>
<p>meets the criteria of an interval scale because we can distinguish scores,
</p>
<p>rank them, and define the exact difference between them. A score of 16
</p>
<p>represents a more serious prior criminal record than a score of 1. In turn,
</p>
<p>an offender with a score of 16 has 15 more arrests than an offender with
</p>
<p>a score of 1. However, we cannot say that the offender with a score of
</p>
<p>16 on this measure had 16 times as many prior arrests as the offender
</p>
<p>with a score of 1. This is because the scale has an arbitrary zero point.
</p>
<p>Zero represents not the absence of a prior record, but the fact that the
</p>
<p>offender has 4 prior arrests. Thus, the scale is an interval scale but not a
</p>
<p>ratio scale.
</p>
<p>Nearly all the statistics that we use in criminal justice (and all those
</p>
<p>that we describe in this text) are also appropriate for interval scales if
</p>
<p>they are appropriate for ratio scales. For this reason, most statistics texts
</p>
<p>do not differentiate between the scales in practice, even if they identify
</p>
<p>how they differ in theory. We follow the same approach. For the rest of
</p>
<p>the chapter and indeed the rest of this text, we will concentrate on the
</p>
<p>differences among nominal, ordinal, and at least interval scales.
</p>
<p>Criminal justice researchers use interval scales to present findings
</p>
<p>about criminal justice agency resources, criminal sentences, and a whole
</p>
<p>of the trait under study. To understand how interval scales differ from 
</p>
<p>ordinal scales and from ratio scales, it is useful to examine a concrete
</p>
<p>example. We commonly measure prior offending in terms of the number 
</p>
<p>of arrests on an offender&rsquo;s criminal history record. If we compare an
</p>
<p>offender who has 20 arrests with one who has only 5 arrests, we know</p>
<p/>
</div>
<div class="page"><p/>
<p>L E V E L S O F M E A S U R E M E N T 21
</p>
<p>host of other issues related to crimes and criminals. For example, we can
</p>
<p>measure the amount spent by criminal justice agencies to pay the salaries
</p>
<p>of police officers or to pay for the health care costs of prison inmates.
</p>
<p>We can measure the financial costs of different types of crime by mea-
</p>
<p>suring the amount stolen by offenders or the amount of time lost from
</p>
<p>meet the requirements of at least an interval level of measurement.
</p>
<p>Now that we have defined each step in the ladder of measurement,
</p>
<p>we can summarize. As is illustrated in Table 2.4, as you move up the
</p>
<p>ladder of measurement, the amount of information that is gained in-
</p>
<p>creases. At the lowest level, you have only categorization. At the next
</p>
<p>level, you add knowledge about the order of the categories included.
</p>
<p>With interval scales, you not only classify and order your measure but
</p>
<p>also define how much categories differ one from another. A ratio scale
</p>
<p>requires all of these characteristics as well as a non-arbitrary, or true,
</p>
<p>zero value.
</p>
<p>Variables Commonly Found in Criminal Justice Research 
</p>
<p>That Are Measured on at Least Interval Scales
</p>
<p>VARIABLE COMMON CATEGORIES
</p>
<p>Age Years
Education Years
Income or Salary Dollars, etc.
Number of Crimes in a
</p>
<p>City/County State Nation Count
Crime Rates for a
</p>
<p>City/County/State/Nation Count of crimes, adjusted for the size of the population
Self-Reported Delinquent Acts Count
</p>
<p>Table 2.3
</p>
<p>Summary of the Information Required for Each Level of Measurement
</p>
<p>TRUE ZERO �
</p>
<p>ORDER � SET INTERVALS � SET INTERVALS �
</p>
<p>LEVEL OF CATEGOR- CATEGOR- ORDER � ORDER �
</p>
<p>MEASUREMENT IZATION IZATION CATEGORIZATION CATEGORIZATION
</p>
<p>Ratio X X X X
</p>
<p>Interval X X X
</p>
<p>Ordinal X X
</p>
<p>Nominal X
</p>
<p>Table 2.4
</p>
<p>arrested. Table 2.3 provides examples of criminal justice variables that
</p>
<p>of prison served or sentenced or the age at which offenders were first 
</p>
<p>work by violent crime victims. We can measure the number of years </p>
<p/>
</div>
<div class="page"><p/>
<p>22 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>R e l a t i n g  I n t e r v a l ,  O r d i n a l ,  
a n d  N o m i n a l  S c a l e s :  T h e  I m p o r t a n c e  
o f  C o l l e c t i n g  D a t a  a t  t h e  H i g h e s t  L e v e l  P o s s i b l e
</p>
<p>Take, for example, the measurement of victimization. If you decided
</p>
<p>to simply compare the types of victimization involved in a crime event,
</p>
<p>you would measure victimization using a nominal scale. You might
</p>
<p>choose the following categories: events involving loss of money or prop-
</p>
<p>erty, events including physical harm, a combination of such events, and
</p>
<p>all other events. But let us assume, for a moment, that at some time after
</p>
<p>you collected your data, a colleague suggests that it is important to dis-
</p>
<p>tinguish not only the type of event but also the seriousness of crimes
</p>
<p>within each type. In this case, you would want to distinguish not only
</p>
<p>whether a crime included monetary loss or violence but also the serious-
</p>
<p>ness of each loss. However, because your variable is measured on a
</p>
<p>nominal scale, it does not include information on the seriousness of loss.
</p>
<p>Accordingly, from the information available to you, you cannot create an
</p>
<p>ordinal-level measure of how much money was stolen or how serious
</p>
<p>the physical harm was.
</p>
<p>Similarly, if you had begun with information only on the order of
</p>
<p>crime seriousness, you could not transform that variable into one that
</p>
<p>defined the exact differences between categories you examined. Let&rsquo;s
</p>
<p>say, for example, that you received data from the police that ranked
</p>
<p>($10,001 and above). If you decide that it is important to know not just
</p>
<p>the general order of monetary harm but also the exact differences in
</p>
<p>harm between crimes, these data are insufficient. Such information
</p>
<p>would be available only if you had received data about harm at an in-
</p>
<p>terval level of measurement. In this case, the police would provide in-
</p>
<p>formation not on which of the four categories of harm a crime belonged
</p>
<p>to, but rather on the exact amount of harm in dollars caused by each
</p>
<p>crime.
</p>
<p>is that you should measure variables in a study at the highest level of
</p>
<p>One important lesson we can draw from the ladder of measurement 
</p>
<p>measurement your data allow. This is because each higher level of mea-
</p>
<p>information at the outset, you may not be able to add it at the end of
</p>
<p>surement cannot be transformed easily into measures higher on the
</p>
<p>surement requires additional information. And if you fail to collect that
</p>
<p>your study. In general, variables measured lower on the ladder of mea-
</p>
<p>ladder. Conversely, variables measured higher on the ladder of mea-
</p>
<p>surement can be transformed easily into measures lower on the ladder.
</p>
<p>moderate monetary harm ($501&ndash;10,000), and serious monetary harm
</p>
<p>monetary victimization for each crime into four ordinally scaled
</p>
<p>categories: no monetary harm, minor monetary harm (up to $500), </p>
<p/>
</div>
<div class="page"><p/>
<p>W H A T I S A G O O D M E A S U R E ? 23
</p>
<p>While you cannot move up the ladder of measurement, you can move
</p>
<p>down it. Thus, for example, if you have information collected at an inter-
</p>
<p>val level, you can easily transform that information into an ordinal-scale
</p>
<p>measure. In the case of victimization, if you have information on the
</p>
<p>exact amount of harm caused by a crime in dollars, you could at any
</p>
<p>point decide to group crimes into levels of seriousness. You would sim-
</p>
<p>ply define the levels and then place each crime in the appropriate level.
</p>
<p>For example, if you defined crimes involving harm between $501 and
</p>
<p>$10,000 as being of moderate victimization, you would take all of the
</p>
<p>crimes that included this degree of victimization and redefine them as
</p>
<p>falling in this moderate category. Similarly, you could transform this mea-
</p>
<p>sure into a nominal scale just by distinguishing between those crimes
</p>
<p>that included monetary harm and those that did not.
</p>
<p>Beyond illustrating the connections among different levels of mea-
</p>
<p>surement, our discussion here emphasizes a very important rule of
</p>
<p>thumb for research. You should always collect information at the highest
</p>
<p>level of measurement possible. You can always decide later to collapse
</p>
<p>such measures into lower-level scales. However, if you begin by collect-
</p>
<p>ing information lower on the ladder of measurement, you will not be
</p>
<p>able to decide later to use scales at a higher level.
</p>
<p>W h a t  I s  a  G o o d  M e a s u r e ?
</p>
<p>In analysis and reporting of research results, measures that are of a
</p>
<p>higher scale are usually preferred over measures that are of a lower
</p>
<p>scale. Higher-level measures are considered better measures, based on
</p>
<p>the principle that they take into account more information. Nonetheless,
</p>
<p>this is not the only criterion we use in deciding what is a good variable
</p>
<p>in research. The researcher must raise two additional concerns. First,
</p>
<p>does the variable reflect the phenomenon to be described? Second, will
</p>
<p>the variable yield results that can be trusted?
</p>
<p>The first question involves what those who study research methods
</p>
<p>call validity. Validity addresses the question of whether the variable
</p>
<p>used actually reflects the concept or theory you seek to examine. Thus,
</p>
<p>for example, collecting information on age in a sample is not a valid way
</p>
<p>of measuring criminal history. Age, although related to criminal history,
</p>
<p>is not a measure of criminal history. Similarly, work history may be re-
</p>
<p>lated to criminality, but it does not make a valid measure of criminality.
</p>
<p>But even if we restrict ourselves to variables that directly reflect criminal
</p>
<p>history, there are often problems of validity to address.
</p>
<p>Let&rsquo;s say that you wanted to describe the number of crimes that of-
</p>
<p>fenders committed over a one-year period. One option you might have</p>
<p/>
</div>
<div class="page"><p/>
<p>24 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>The most valid measure of frequency of offending is the one that
</p>
<p>most directly assesses how many crimes an individual has committed.
</p>
<p>Associated with each of the three variables included on the rap sheet is
</p>
<p>some degree of threat to validity. This means that each can be criticized
</p>
<p>because it does not quite reflect the concept we wish to study. Incarcera-
</p>
<p>tion, for example, is more a measure of seriousness of crime than fre-
</p>
<p>quency of offending. This is because judges may impose a number of
</p>
<p>different types of sanctions, and they are more likely to impose a prison
</p>
<p>sentence for more serious crimes. Many crimes that result in a conviction
</p>
<p>lead not to incarceration but rather to probation, fines, or community
</p>
<p>service. Thus, if we use incarceration to measure frequency of offending,
</p>
<p>we are likely to miss many crime events in an offender&rsquo;s criminal record.
</p>
<p>Accordingly, incarceration provides a biased picture of the number of of-
</p>
<p>fenses committed by an offender. It is not a highly valid measure of this
</p>
<p>concept.
</p>
<p>Using this logic, criminologists have generally assumed that arrest is
</p>
<p>the most valid measure of frequency of offending that can be gained
</p>
<p>from official data sources, such as the FBI rap sheet. Arrests are much
</p>
<p>closer in occurrence to the actual behavior we seek to study and are not
</p>
<p>filtered by the negotiations found at later stages of the legal process.
</p>
<p>While criminologists have assumed that arrests reflect criminal behavior
</p>
<p>more accurately than convictions or incarceration, some legal scholars
</p>
<p>contend that arrests are a less valid measure of criminality precisely be-
</p>
<p>cause they come before the court reaches a conclusion regarding the in-
</p>
<p>nocence or guilt of a defendant. They contend that someone has not
</p>
<p>committed a crime until the legal system defines an act as such.
</p>
<p>Self-report surveys are generally considered to provide the most valid
</p>
<p>measure of frequency of offending. This is because an individual can be
</p>
<p>asked directly how many crimes he or she has committed. But self-report
</p>
<p>studies are often criticized in terms of another concern in measurement,
</p>
<p>which is termed reliability.
</p>
<p>Reliability addresses the question of whether a measure gains infor-
</p>
<p>mation in a consistent manner. Will you get the same result if you repeat
</p>
<p>measurement of the same case or person? If different people have similar
</p>
<p>characteristics, will your measure reflect that similarity? Returning to the
</p>
<p>above example of criminal history, we would ask not whether the mea-
</p>
<p>sure reflects the concept of frequency of offending, but whether mea-
</p>
<p>surement of the concept is reliable across different subjects.
</p>
<p>is to examine their criminal history as it is recorded on the Federal
</p>
<p>Bureau of Investigation&rsquo;s (FBI) criminal history record, or rap sheet.
</p>
<p>The rap sheet includes information on arrests, convictions, and incar-
</p>
<p>cerations. Although each of these variables tells us something about a
</p>
<p>person s criminal history, they are not all equally valid in terms of ans-
</p>
<p>wering the research question we have proposed.
</p>
<p>&rsquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S U M M A R Y 25
</p>
<p>Self-reports, which allow us to ask valid questions about the number
</p>
<p>of crimes that a person has committed, have been challenged on the basis
</p>
<p>of their reliability. One problem is that people may lie about their crimi-
</p>
<p>nal histories. Crime is a sensitive issue, and no matter what efforts the re-
</p>
<p>searcher makes to assure subjects of confidentiality, people may be hesi-
</p>
<p>tant to talk about crimes in their past. Accordingly, depending on the
</p>
<p>degree of hesitancy of subjects, a researcher might gain different answers,
</p>
<p>irrespective of a person&rsquo;s actual criminal history. But even if a person is
</p>
<p>willing to provide accurate responses to such questions, he or she may
</p>
<p>not be able to. Some people have better memories than others, and the
</p>
<p>reliability of this measure depends in part on a person&rsquo;s ability to recall
</p>
<p>events generally. Such issues of reliability have begun to be addressed di-
</p>
<p>rectly by criminologists, who are trying to increase the reliability of self-
</p>
<p>report methods by improving interview techniques and protocols.
</p>
<p>Returning to the FBI rap sheets, we can also assess their reliability. In
</p>
<p>general, not only is arrest assumed to be the most valid of official mea-
</p>
<p>sures; it is also the measure most reliably recorded on the FBI rap sheets.
</p>
<p>This is the case in good part because the rap sheets are built around fin-
</p>
<p>gerprint records, which police agencies have come to routinely send to
</p>
<p>the FBI. This helps the police agencies as well, because they often use
</p>
<p>this information to check the identities of arrestees and to assess their
</p>
<p>criminal histories. Other types of agencies are less consistent in their
</p>
<p>transfer of information to the FBI, and as a result convictions and incar-
</p>
<p>cerations are less reliably recorded.
</p>
<p>The issues raised in connection with the validity and reliability of
</p>
<p>criminal history information are good examples of the kinds of problems
</p>
<p>you will encounter in assessing measures in criminal justice. You should
</p>
<p>keep in mind that no variable is perfect. Some threat to validity is likely
</p>
<p>velop or choose the best measure you can. The best measure is the one
</p>
<p>that most closely reflects the concept you wish to study and assesses it in
</p>
<p>a consistent and reliable way across subjects or events.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>In science, we use measurement to make accurate observations. All
</p>
<p>measurement must begin with a classification process&mdash;a process
</p>
<p>that in science is carried out according to systematic criteria. This
</p>
<p>process implies that we can place units of scientific study in clearly
</p>
<p>defined categories. The end result of classification is the development
</p>
<p>of variables.
</p>
<p>reliability is almost always present in measurement. Your task is to de-
</p>
<p>to be encountered, no matter how careful you are. Some degree of un-</p>
<p/>
</div>
<div class="page"><p/>
<p>26 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>There are four scales of measurement: nominal, ordinal, interval,
</p>
<p>and ratio. With a nominal scale, information is organized by simple
</p>
<p>classification. The aim is merely to distinguish between different phe-
</p>
<p>nomena. There can be no overlap between categories nor can there be
</p>
<p>cases that do not fit any one category. There is no theoretical limit to the
</p>
<p>number of nominal categories possible. With an ordinal scale, not only
</p>
<p>is information categorized, but these categories are then placed in order
</p>
<p>of magnitude. An interval scale is one that, in addition to permitting the
</p>
<p>processes of categorization and ordering, also defines the exact differ-
</p>
<p>ence between objects, characteristics, or events. A ratio scale is an inter-
</p>
<p>val scale for which a non-arbitrary, or true, zero value can be identified.
</p>
<p>Data collected at a higher level of measurement may subsequently be
</p>
<p>reduced to a lower level, but data collected at a lower level may not be
</p>
<p>transformed to a higher one. For this reason, it is always advisable to
</p>
<p>collect data at the highest level of measurement possible.
</p>
<p>There are three separate factors that affect the quality of a measure.
</p>
<p>The researcher should strive for a measure that has (1) a high scale of
</p>
<p>measurement (one that uses the most information); (2) a high level of
</p>
<p>validity (one that provides an accurate reflection of the concept being
</p>
<p>studied); and (3) a high level of reliability (one that provides consistent
</p>
<p>results across subjects or units of study).
</p>
<p>K e y  T e r m s
</p>
<p>classification The process whereby data
</p>
<p>are organized into categories or groups.
</p>
<p>data Information used to answer a re-
</p>
<p>search question.
</p>
<p>interval scale A scale of measurement
</p>
<p>that uses a common and standard unit and
</p>
<p>enables the researcher to calculate exact
</p>
<p>differences between scores, in addition to
</p>
<p>categorizing and ordering data.
</p>
<p>levels of measurement Types of mea-
</p>
<p>surement that make use of progressively
</p>
<p>larger amounts of information.
</p>
<p>measurement The assignment of numeri-
</p>
<p>cal values to objects, characteristics, or
</p>
<p>events in a systematic manner.
</p>
<p>nominal scale A scale of measurement
</p>
<p>that assigns each piece of information to an
</p>
<p>appropriate category without suggesting
</p>
<p>any order for the categories created.
</p>
<p>ordinal scale A scale of measurement that
</p>
<p>categorizes information and assigns it an
</p>
<p>order of magnitude without using a stan-
</p>
<p>dard scale of equal intervals.
</p>
<p>ratio scale A scale of measurement identi-
</p>
<p>cal to an interval scale in every respect ex-
</p>
<p>cept that, in addition, a value of zero on
</p>
<p>the scale represents the absence of the
</p>
<p>phenomenon.
</p>
<p>reliability The extent to which a measure
</p>
<p>provides consistent results across subjects
</p>
<p>or units of study.
</p>
<p>scale of measurement Type of catego-
</p>
<p>rization used to arrange or assign values to
</p>
<p>data.</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 27
</p>
<p>validity The extent to which a variable
</p>
<p>accurately reflects the concept being
</p>
<p>measured.
</p>
<p>variable A trait, characteristic, or attribute
</p>
<p>of a person/object/event that can be mea-
</p>
<p>sured at least at the nominal-scale level.
</p>
<p>E x e r c i s e s
</p>
<p>2.1 For each of the following examples of criminal justice studies, state
whether the scale of measurement used is nominal, ordinal, or at least
interval (i.e., interval or ratio). Explain your choice.
</p>
<p>a. In a door-to-door survey, residents of a neighborhood are asked
how many times over the past year they (or anyone in their house-
hold) have been the victims of any type of crime.
</p>
<p>b. Parole-board members rate inmate behavior on a scale with
values ranging from 1 to 10; a score of 1 represents exemplary
behavior.
</p>
<p>c. One hundred college students are asked whether they have ever
been arrested.
</p>
<p>d. A researcher checks prison records to determine the racial back-
ground of prisoners assigned to a particular cell block.
</p>
<p>e. In a telephone survey, members of the public are asked which of
the following phrases best matches how they feel about the perfor-
</p>
<p>f. A criminologist measures the diameters (in centimeters) of the
skulls of inmates who have died in prison, in an attempt to develop
a biological theory of the causes of criminality.
</p>
<p>g. Secretaries at a top legal firm are asked the following question:
&ldquo;Over the past year, have you been the victim of sexual harass-
</p>
<p>2.2 You have been given access to a group of 12 jurors, with a mandate
from your senior researcher to &ldquo;go and find out about their prior jury
experience.&rdquo; Under each of the following three sets of restrictions, de-
vise a question to ask the jurors about the number of experiences they
have had with previous juries.
</p>
<p>a. The information may be recorded only on a nominal scale of
measurement.
</p>
<p>indifferent, satisfied, or very satisfied.
mance of their local police force: totally dissatisfied, dissatisfied, 
</p>
<p>follows: never, once, two or three times, more than three times, or 
ment&mdash;and if so, how many times?&rdquo; Answers are categorized as 
</p>
<p>refused to answer.</p>
<p/>
</div>
<div class="page"><p/>
<p>28 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>b. The information may be recorded on an ordinal scale but not on
any higher scale of measurement.
</p>
<p>c. The information may be recorded on an interval scale.
</p>
<p>Your senior researcher subsequently informs you that she wishes to
know the answers to the following five questions:
</p>
<p>&mdash;How many of the jurors have served on a jury before?
</p>
<p>&mdash;Who is the juror with the most prior experience?
</p>
<p>&mdash;What is the sum total of previous jury experience?
</p>
<p>&mdash;Is there anyone on the jury who has served more than three times?
</p>
<p>&mdash;What is the average amount of prior jury experience for this group?
</p>
<p>d. If you had collected data at the nominal level, which (if any) of the
above questions would you be in a position to answer?
</p>
<p>e. If you had collected data at the ordinal level, which (if any) of the
above questions would you be in a position to answer?
</p>
<p>f. If you had collected data at the interval level, which (if any) of the
above questions would you be in a position to answer?
</p>
<p>2.3 You have been asked to measure the public&rsquo;s level of support for using
the death penalty. Devise questions to gauge each of the following:
</p>
<p>a. Overall support for using the death penalty.
</p>
<p>b. Support for using the death penalty if there are other punishment
options.
</p>
<p>c. Support for using the death penalty if the chances of an innocent
person being executed are
</p>
<p>i. 1 in 1,000.
</p>
<p>ii. 1 in 100.
</p>
<p>iii. 1 in 10.
</p>
<p>2.4 You are investigating the effects of a defendant&rsquo;s prior record on vari-
ous punishment decisions made by the court. One variable that you
have access to in local court records is the total number of prior
felony arrests for each defendant.
</p>
<p>a. What kinds of questions would you be able to answer with prior
record measured in this way?
</p>
<p>b. Explain how you would recode this information on a nominal scale
</p>
<p>c. Explain how you would recode this information on an ordinal scale
</p>
<p>answer with prior record measured in this way?
</p>
<p>answer with prior record measured in this way?
</p>
<p>of measurement. What kinds of questions would you be able to 
</p>
<p>of measurement. What kinds of questions would you be able to </p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 29
</p>
<p>2.5 Because the Ministry of Transport (MOT) is concerned about the num-
ber of road accidents caused by motorists driving too close together, it
has, on an experimental 2-km stretch of road, painted &ldquo;chevrons&rdquo;
(lane markings) every few meters in each lane. By the roadside it has
erected a sign that reads: &ldquo;KEEP YOUR DISTANCE: STAY AT LEAST 3
CHEVRONS FROM THE CAR IN FRONT!&rdquo; The MOT has asked you to
measure the extent to which this instruction is being followed. There
are a number of possible measures at your disposal. Assess the relia-
bility and validity of each approach suggested below. Which is the
best measure?
</p>
<p>a. Stand on a bridge over the experimental stretch of road and count
how many of the cars passing below do not keep the required
distance.
</p>
<p>b. Compare police figures on how many accidents were recorded on
that stretch of road over the periods before and after it was painted.
</p>
<p>c. Study the film from a police camera situated 5 km farther down the
same stretch of road (after the end of the experimental stretch) and
count how many cars do not keep a safe distance.
</p>
<p>2.6 The police are planning to introduce a pilot &ldquo;community relations
strategy&rdquo; in a particular neighborhood and want you to evaluate
whether it has an effect on the willingness of citizens to report crimes
to the police. There are a number of possible measures at your dis-
posal. Assess the reliability and validity of each approach suggested
below. Which is the best measure?
</p>
<p>a. Telephone every household and ask respondents to measure, on a
scale of 1 to 10, how willing they are to report particular types of
crime to the police. Repeat the experiment after the scheme has
been in operation six months.
</p>
<p>b. Compare a list of offenses reported by members of the neighbor-
hood in the six months before introduction of the scheme with a
similar list for the six months after introduction of the scheme. (It is
standard procedure for the police to record the details of the com-
plainant every time a crime is reported to them.)
</p>
<p>2.7 You are comparing the psychological condition of three inmates serv-
ing out long terms in different high-security prisons, and you are in-
terested in the amount of contact each one has with the outside
world. You wish to determine how many letters each one has sent
over the past 12 months. No official records of this exist. There are a
number of possible measures at your disposal. Assess the reliability
and validity of each approach suggested below. Which is the best
measure?
</p>
<p>a. Ask each prisoner how many letters he or she sent over the past
year.</p>
<p/>
</div>
<div class="page"><p/>
<p>30 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>b. Check the rules in each of the prisons to see how many letters high
security prisoners are allowed to send each year.
</p>
<p>c. Check the records of the prison postal offices to see how many
times each prisoner bought a stamp over the past year.
</p>
<p>2.8 The government is interested in the link between employment and
criminal behavior for persons released from prison. In a study de-
signed to test for an effect of employment, a group of people released
from prison are randomly assigned to a job training program, where
they will receive counseling, training, and assistance with job place-
ment. The other offenders released from prison will not receive any
special assistance. There are a number of possible measures at your
disposal. Assess the reliability and validity of each approach suggested
below. Which is the best measure?
</p>
<p>a. Eighteen months after their release from prison, interview all the of-
fenders participating in the study and ask about their criminal activ-
ity to determine how many have committed criminal acts.
</p>
<p>2.9 In a recent issue of a criminology or criminal justice journal, locate a
research article on a topic of interest to you. In this article, there
should be a section that describes the data. A well-written article will
describe how the variables were measured.
</p>
<p>a. Make a list of the variables included in the article and how each
was measured.
</p>
<p>b. What is the level of measurement for each variable&mdash;nominal, ordi-
nal, or at least interval? Explain why.
</p>
<p>c. Consider the main variable of interest in the article. Assess its relia-
bility and validity.
</p>
<p>returned to prison within 18 months of release.
</p>
<p>arrested for a new crime within 18 months of release.
</p>
<p>b. Look at prison records to determine how many offenders were 
</p>
<p>c. Look at arrest records to determine how many offenders were 
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>There are a number of statistical software packages available for data analysis. 
</p>
<p>Most spreadsheet programs will also perform the basic statistical analyses of the 
</p>
<p>kind described in this text through Chapter 17. The computer exercises included 
</p>
<p>in this text focus on the use of two different software programs: SPSS and Stata. 
</p>
<p>Most universities that we are aware of make at least one of these two statistical 
</p>
<p>programs available in student computer labs. There are also student versions of 
</p>
<p>each program that can be purchased separately, sometimes through a university 
</p>
<p>bookstore or other offices on campus&mdash;see each company&rsquo;s Web site for details 
</p>
<p>(www.spss.com or www.stata.com). There are many excellent reference books </p>
<p/>
<div class="annotation"><a href="http://www.spss.comorwww.stata.com">http://www.spss.comorwww.stata.com</a></div>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 31
</p>
<p>is not to repeat what is said in those books. Rather, our goal with the computer 
</p>
<p>exercises is to illustrate some of the power available to you in two widely used 
</p>
<p>packages. In real-world situations where you are perform some type of statistical 
</p>
<p>analysis, you rarely work through a problem by hand, especially if the number of 
</p>
<p>observations is large.
</p>
<p>SPSS
</p>
<p>To begin our exploration of SPSS, we will focus here on some of the data 
</p>
<p> management features available to users. After starting the SPSS program on 
</p>
<p>your computer, you will need to open the National Youth Survey data file from 
</p>
<p>the Web site (nys_1.sav). For those readers working with the Student Version 
</p>
<p>of SPSS, you are limited to data files with no more than 50 variables and 1,500 
</p>
<p>cases. We have also included a smaller version of the NYS data (nys_1_student.
</p>
<p>sav) that contains a random sample of 1,000 cases (of the original 1,725).
</p>
<p>After you start SPSS and open the data file, the raw data should appear in a 
</p>
<p>window that looks much like a spreadsheet. Each column represents a different 
</p>
<p>variable, while each row represents a different observation (individual, here). If 
</p>
<p>you scroll down to the end of the data file, you should see that there are 1,725 
</p>
<p>lines of data (or 1,000 if using the student version of the data file).
</p>
<p>There are three direct ways to learn about the variables included in this data 
</p>
<p>file. First, notice the buttons in the lower center of the spreadsheet. One button 
</p>
<p>(which should appear as the darker shade of the two buttons) is labeled &ldquo;Data 
</p>
<p>View,&rdquo; and the other is labeled &ldquo;Variable View.&rdquo; The data view button presents 
</p>
<p>us with the spreadsheet of values for each observation and variable. If you click 
</p>
<p>on the button labeled &ldquo;Variable View,&rdquo; you should now see another spreadsheet, 
</p>
<p>in which variable names are listed in the first column and the other columns 
</p>
<p>contain additional information about each variable. For example, the first col-
</p>
<p>umn provides the name of the variable, another column provides a label for the 
</p>
<p>Several SPSS and Stata files are available at the following Web address: 
</p>
<p>http://extras.springer.com. The data file we will use first represents a subset of 
</p>
<p>the data from the National Youth Survey, Wave 1. The sample of 1,725 youth 
</p>
<p>is representative of persons aged 11&ndash;17 years in the USA in 1976, when the first 
</p>
<p>wave of data was collected. While these data may seem old, researchers continue 
</p>
<p>to publish reports based on new findings and interpretations of these data. One 
</p>
<p>of the apparent strengths of this study was its design; the youth were inter-
</p>
<p>viewed annually for 5 years from 1976 to 1980 and then were interviewed again 
</p>
<p>in 1983 and 1987. The data file on our Web site was constructed from the full 
</p>
<p>data source available at the Inter-University Consortium of Political and Social 
</p>
<p>Research, which is a national data archive. Data from studies funded by the 
</p>
<p>National Institute of Justice (NIJ) are freely available to anyone with an Internet 
</p>
<p>connection; go to http://www.icpsr.umich.edu/NACJD. All seven waves of data 
</p>
<p>from the National Youth Survey are available, for example.
</p>
<p>on the use of either SPSS or Stata for statistical data analysis&mdash;our intent here </p>
<p/>
<div class="annotation"><a href="http://extras.springer.com">http://extras.springer.com</a></div>
<div class="annotation"><a href="http://www.icpsr.umich.edu/NACJD">http://www.icpsr.umich.edu/NACJD</a></div>
</div>
<div class="page"><p/>
<p>32 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>variable (allowing us to add a more informative description of our variable), and 
</p>
<p>an additional column provides value labels. It is from this column that we will 
</p>
<p>be able to learn more about each variable. For example, click on the cell in this 
</p>
<p>column for the sex variable, and you should see a small gray box appear in the 
</p>
<p>cell. Now click on this small gray box and you will be presented with a new win-
</p>
<p>dow that lists possible values for sex and the corresponding labels. Here, we see 
</p>
<p>that males have been coded as &ldquo;1&rdquo; and females as &ldquo;2.&rdquo; If you click on &ldquo;OK&rdquo; or 
</p>
<p>&ldquo;Cancel,&rdquo; the window disappears. You can then perform this same operation for 
</p>
<p>every other variable.
</p>
<p>A second way of obtaining information about the variables in an SPSS data 
</p>
<p>file involves using the &ldquo;Variables&rdquo; command. To execute this command, click 
</p>
<p>on &ldquo;Utilities&rdquo; on the menu bar; then click on &ldquo;Variables.&rdquo; What you should see 
</p>
<p>is a list of variables on the left and another window on the right that presents 
</p>
<p>information about the highlighted variable. If you click on the sex variable, you 
</p>
<p>should see information on its coding and values in the window on the right. This 
</p>
<p>command is particularly useful if you are working with an SPSS data file and 
</p>
<p>simply need a reminder of how the variables are coded and what categories or 
</p>
<p>values are included. This feature is useful if you are working with a data set and 
</p>
<p>need to know what a particular variable refers to or how it is measured in order 
</p>
<p>to continue working.
</p>
<p>A third way of obtaining information about the variables in an SPSS data file 
</p>
<p>involves the &ldquo;Display Data File Information&rdquo; command. To run this command, 
</p>
<p>click on &ldquo;File&rdquo; on the menu bar; then click on &ldquo;Display Data File Information,&rdquo; 
</p>
<p>and then select the option for &ldquo;Working File&rdquo; (the data you have already opened 
</p>
<p>up in SPSS). This command generates text for the output window in SPSS. This 
</p>
<p>output contains all the information SPSS has on every variable in a data file. 
</p>
<p>Executing this command is equivalent to executing the &ldquo;Variables&rdquo; command 
</p>
<p>for every variable in the data set and saving that information in another file. Be 
</p>
<p>aware that using this command on a data file with many variables will produce a 
</p>
<p>very large output file. This command is most useful when you are first working 
</p>
<p>with an SPSS data set that someone else has conveniently set up for you and you 
</p>
<p>need to verify the contents of the data set and the nature of the variables includ-
</p>
<p>ed in the data set. Similar to what you are now doing with the NYS data file.
</p>
<p>In subsequent chapters, the computer exercises will make use of syntax com-
</p>
<p>mands and files. These are the command lines that each program uses to run a 
</p>
<p>particular procedure. To begin, open a syntax window in SPSS by clicking on 
</p>
<p>&ldquo;File,&rdquo; then &ldquo;New&rdquo; and &ldquo;Syntax.&rdquo; A blank window opens in which you may 
</p>
<p>enter command lines and save for future use. Alternatively, instead of open-
</p>
<p>ing a blank syntax window, you could open one that had been saved, such as 
</p>
<p>Chapter_2.sps included on the text Web site.
</p>
<p>Open the Chapter_2.sps syntax file. As a convention throughout the text, we 
</p>
<p>will use CAPS to denote required or really useful components to a command, 
</p>
<p>although you are free to use lowercase letters in any command that you run in 
</p>
<p>SPSS. The first line of this file begins with /*&mdash;this is a comment line. Any of </p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 33
</p>
<p>the SPSS syntax files that you open and begin with this line do not run com-
</p>
<p>mands, but are included to help explain what it is you are reading in the file.
</p>
<p>The first two command lines of Chapter_2.sps are:
</p>
<p>The GET FILE command reads the NYS data file into SPSS. Note that you 
</p>
<p>will need to provide a directory name that contains your copy of the nys_1.sav 
</p>
<p>data file. The DATASET command gives you the option of naming the data 
</p>
<p>file something else internally, while you are working in SPSS. Here, the name 
</p>
<p>given is simply &ldquo;nys.&rdquo; The WINDOW = FRONT option is useful for those times 
</p>
<p>when you may have more than one data file opened in SPSS&mdash;this forces it to 
</p>
<p>keep this spreadsheet in the position where it is the first one you will see on your 
</p>
<p>computer screen.
</p>
<p>It is important to note that each command line ends with a period&mdash;this 
</p>
<p>ensures that SPSS will know where the command ends and when to start execut-
</p>
<p>ing the instruction.
</p>
<p>To run these command lines&mdash;after you have edited the directory 
</p>
<p> information&mdash;highlight the two lines and do one of the following:
</p>
<p>Click on the right-pointing triangle toward the top right of  the syntax window.
</p>
<p>Click on &ldquo;Run&rdquo; on the menu bar and then select the option for &ldquo;Selection&rdquo;&mdash;
</p>
<p>The keystrokes for this same procedure will appear when you click on &ldquo;Run.&rdquo; 
</p>
<p>They are slightly different for Mac and Windows users, but may simplify your 
</p>
<p>To obtain the file information&mdash;the complete listing of variables and their char-
</p>
<p>acteristics, run the third line of Chapter_2.sps:
</p>
<p>DISPLAY DICTIONARY.
</p>
<p>This will display all of the information we highlighted above in using the 
</p>
<p> point-and-click interactive method to display the file information.
</p>
<p>Stata
</p>
<p>Much like our discussion of SPSS, we begin our exploration of Stata by focusing 
</p>
<p>on some of the basic data management features available to users. After starting 
</p>
<p>the Stata program on your computer, you will see a window that has several dif-
</p>
<p>ferent boxes. On the far left, the box labeled &ldquo;Review&rdquo; will contain a growing 
</p>
<p>list of commands that you run during a Stata session. The largest box (top and 
</p>
<p>center) is labeled &ldquo;Results&rdquo; and will present all of the output from various com-
</p>
<p>mand that you have run as well as any error messages. Just below this box is the 
</p>
<p>&ldquo;Command&rdquo; box, where you will enter commands, if going one command at a 
</p>
<p>time. On the right side of the Stata window is a box labeled &ldquo;Variables&rdquo;&mdash;the 
</p>
<p>upper half will contain a complete list of all the variables in the data file, while 
</p>
<p>the lower half provides various details about the variable that is highlighted in the 
</p>
<p>upper half.
</p>
<p>GET FILE = '[directory_name]nys_1.sav'.
</p>
<p>DATASET NAME nys WINDOW = FRONT.</p>
<p/>
</div>
<div class="page"><p/>
<p>34 C H A P T E R T W O :  M E A S U R E M E N T
</p>
<p>Begin by opening the National Youth Survey data file obtained from the Web 
</p>
<p>site (nys_1.dta) using &ldquo;File&rdquo; from the menu bar and the &ldquo;Open&hellip;&rdquo; option listed. 
</p>
<p>Note that the &ldquo;Review&rdquo; box on the left now contains the Stata command that 
</p>
<p>was run to open the data file, the &ldquo;Variables&rdquo; box on the right contains the list-
</p>
<p>ing of variables, and the &ldquo;Results&rdquo; window simply lists the Stata command that 
</p>
<p>was run.
</p>
<p>If you would like to view the data in a spreadsheet format, click on the box at 
</p>
<p>the top center of the Stata window for &ldquo;Data Browser&rdquo; or &ldquo;Data Editor&rdquo;&mdash;the 
</p>
<p>&ldquo;Browser&rdquo; view will allow you to look at the data, while the &ldquo;Editor&rdquo; view will 
</p>
<p>allow you to make changes to the data. The window that opens will present each 
</p>
<p>variable as a separate column, each observation as a separate row, and the con-
</p>
<p>tents of each cell will either be a numeric value or a value label that is associated 
</p>
<p>with a numeric value. If you click on a cell with a value label, the corresponding 
</p>
<p>numeric value appears in the white horizontal bar just above the spreadsheet.
</p>
<p>There are several different ways of learning about the variables included in 
</p>
<p>this data file. As noted already, the &ldquo;Variables&rdquo; box on the right contains infor-
</p>
<p>mation about each variable and is displayed as soon as the variable name is high-
</p>
<p>lighted (i.e., clicked on) in the upper box.
</p>
<p>A much more comprehensive method for obtaining information about the 
</p>
<p> variables in the data file is to use the &ldquo;Codebook&rdquo; command, which will list all 
</p>
<p>of  the variables in the data file, their values, value labels, as well as other infor-
</p>
<p>mation that will become useful in subsequent chapters of  this text. To run the 
</p>
<p>&ldquo;Codebook&rdquo; command, click on &ldquo;Data&rdquo; on the menu bar, then the options for 
</p>
<p>&ldquo;Describe data&rdquo; and selecting the option for &ldquo;Describe data contents (code-
</p>
<p>book).&rdquo; The &ldquo;Results&rdquo; window will generate a great deal of  output that will list 
</p>
<p>each variable, the variable label, the possible (numeric) values when there are 
</p>
<p>fewer than 10 unique values, and any value labels. The other information pre-
</p>
<p>sented will be discussed later.
</p>
<p>Similar to the observation made in regard to the use of SPSS, in subsequent 
</p>
<p>chapters, the computer exercises will make use of syntax commands and files, 
</p>
<p>rather than the point-and-click options. Although Stata syntax is different from 
</p>
<p>SPSS syntax, how it works is very much the same: the command line(s) inform 
</p>
<p>Stata what procedure to run, whether it is opening a date file for use, or some 
</p>
<p>other statistical procedure. To open a new syntax window in Stata, where they 
</p>
<p>are called &ldquo;do files,&rdquo; start by clicking on &ldquo;File,&rdquo; then &ldquo;New Do-file. A blank 
</p>
<p>window opens in which you may enter command lines and save for future use. 
</p>
<p>Alternatively, instead of opening a blank do file window, you could open one 
</p>
<p>that had been saved, such as Chapter_2.do included on the text Web site.
</p>
<p>Stata commands must be in lowercase letters, unless otherwise noted in the 
</p>
<p>documentation for the command. As a convention throughout our text, we will 
</p>
<p>place Stata commands and useful or required options in boldface font.
</p>
<p>Open the Chapter_2.do file. The first line of this file begins with /* and ends 
</p>
<p>with */&mdash;this is a comment line. In Stata, comments are bounded by these two 
</p>
<p>mirror sets of characters, so without the */, Stata would interpret everything </p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 35
</p>
<p> following the initial /* as a comment&mdash;so it is important to pay attention to your 
</p>
<p>use of these in any Stata do file.
</p>
<p>The first command line of Chapter_2.do is:
</p>
<p>The use command reads the NYS data file into Stata. Note that you will 
</p>
<p>need to provide a directory name that contains your copy of the nys_1.dta data 
</p>
<p>file. Also note that after the file name and location, there is a comma followed by 
</p>
<p>the clear option. Using this option tells Stata to clear out any existing data file 
</p>
<p>currently being used without saving any changes made to that data file.
</p>
<p>To run this command line&mdash;after you have edited the directory information&mdash;
</p>
<p>highlight the command line and do one of the following:
</p>
<p>will run the command quietly, meaning there is no output in the &ldquo;Results&rdquo; 
</p>
<p>window, other than a line indicating a command was run.
</p>
<p>will run the command and generate output in the &ldquo;Results&rdquo; window.
</p>
<p>Keystroke options are available for both of  these commands, to determine 
</p>
<p>what they are for your operating system (Mac or Windows), click on &ldquo;View&rdquo; 
</p>
<p>the keystroke shortcuts that can be used to run the command quietly or not.
</p>
<p>To obtain the file information&mdash;the complete listing of variables and their char-
</p>
<p>acteristics, run the second command line of Chapter_2.do:
</p>
<p>codebook
</p>
<p>This will display all of the information we highlighted above in using the point-
</p>
<p>and-click interactive method to display the file information.
</p>
<p>Problems
</p>
<p>Using one of the ways described above, work through all the variables included 
</p>
<p>in the NYS data file:
</p>
<p> 1. Note the level of  measurement for each variable and then briefly explain 
</p>
<p>why it is what it is. (You should not rely on the level of  measurement 
</p>
<p>information given in the SPSS data file, especially if  someone else has 
</p>
<p>constructed the SPSS data file for you.)
</p>
<p> 2. Describe the likely levels of  reliability and validity for each variable and 
</p>
<p>explain why they are what they are.
</p>
<p>use &ldquo;[directory_name]nys_1.dta&rdquo;, clear</p>
<p/>
</div>
<div class="page"><p/>
<p>Representing and Displaying Data
</p>
<p>When Should a Pie Chart Be Used?
</p>
<p>t i m e  s e r i e s  d a t a  v i s u a l l y
</p>
<p>How Can the Researcher Present Time Series Data Graphically?
</p>
<p>C h a p t e r  t h r e e
</p>
<p>T h e  f r e q u e n c y  d i s t r i b u t i o n
</p>
<p>U s i n g  b a r  c h a r t s  t o  r e p r e s e n t  d a t a
</p>
<p>U s i n g  p i e  c h a r t s  t o  r e p r e s e n t  d a t a
</p>
<p>R e p r e s e n t i n g  h i s t o r i c a l  o r  
</p>
<p>What is It?
</p>
<p>When are Bar Charts Useful?
</p>
<p>How are Bar Charts Created?
</p>
<p>How are Pie Charts Created?
</p>
<p>How are Data Represented Visually in a Histogram?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_3,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>THE GRAPHICAL DISPLAY OF DATA is an important tool for presenting sta-
tistical results in such a way that the key features or characteristics of an
</p>
<p>analysis are highlighted. There are many different ways the same data
</p>
<p>might be displayed. Indeed, many books have been written that focus
</p>
<p>entirely on graphical presentation of data. In this chapter, we introduce
</p>
<p>some common ways of representing data in graphical form, along with
</p>
<p>suggestions for effectively presenting information in an accurate way.
</p>
<p>We begin by discussing the most basic way of summarizing&mdash;and then
</p>
<p>graphing&mdash;data: frequency distributions and histograms. Building on the
</p>
<p>discussion of histograms, we move on to more general bar charts, noting
</p>
<p>W h a t  A r e  F r e q u e n c y  D i s t r i b u t i o n s  a n d  H i s t o g r a m s ?
</p>
<p>When we array scores according to their value and frequency, we con-
</p>
<p>struct what is called a frequency distribution. Let&rsquo;s take the following
</p>
<p>data on previous arrests of 100 known offenders as an example:
</p>
<p>14 0 34 8 7 22 12 12 2 8
6 1 8 1 18 8 1 10 10 2
</p>
<p>12 26 8 7 9 9 3 2 7 16
8 65 8 2 4 2 4 0 7 2
1 2 11 2 1 1 5 7 4 10
</p>
<p>11 3 41 15 1 23 10 5 2 10
20 0 7 6 9 0 3 1 15 5
27 8 26 8 1 1 11 2 4 4
8 41 29 18 8 5 2 10 1 0
5 36 3 4 9 5 10 8 0 7
</p>
<p>We first group all of the cases with the same value together. Accordingly,
</p>
<p>we group together the cases with no prior arrests, one prior arrest, two
</p>
<p>Finally, we examine how graphs can be used to represent a series of 
</p>
<p>the variety of information that can be presented in bar and pie charts. 
</p>
<p>observations over time.
</p>
<p>37</p>
<p/>
</div>
<div class="page"><p/>
<p>prior arrests, and so forth, until we have covered all of the potential
</p>
<p>scores in the distribution. Then we arrange these scores in order of mag-
</p>
<p>nitude, as is done in Table 3.1. Looking at the data in this way allows us
</p>
<p>to get a sense of the nature of the distribution of scores.
</p>
<p>In practice, creating a frequency distribution is usually the first step a
</p>
<p>researcher takes in analyzing the results of a study. Looking at the distri-
</p>
<p>bution of scores not only provides a first glance at the results of a study;
</p>
<p>it also allows the researcher to see whether there are scores that do not
</p>
<p>make sense. For example, coding errors in the data set may have given
</p>
<p>rise to impossible scores. In our example, a result of thousands of arrests
</p>
<p>would be very unlikely and would thus lead the researcher to take an-
</p>
<p>other look at that particular case.
</p>
<p>Constructing frequency distributions by hand can be very time and
</p>
<p>labor intensive. Researchers today seldom construct frequency distribu-
</p>
<p>tions by hand. This task can be done simply and easily with packaged
</p>
<p>statistical software, such as SPSS.
</p>
<p>Frequency Distribution of Prior Arrests for 100 Known Offenders
</p>
<p>VALUE FREQUENCY
</p>
<p>0 6
1 11
2 11
3 4
4 6
5 6
6 2
7 7
8 12
9 4
</p>
<p>10 7
11 3
12 3
14 1
15 2
16 1
18 2
20 1
22 1
23 1
26 2
27 1
29 1
34 1
36 1
41 2
65 1
Total 100
</p>
<p>Table 3.1
</p>
<p>38 C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A</p>
<p/>
</div>
<div class="page"><p/>
<p>If you decide to present a frequency distribution of the results of a
</p>
<p>study, you must choose what particular format to use. For example, the
</p>
<p>distribution of prior arrests could simply be presented as in Table 3.1. Or
</p>
<p>the same information could be presented graphically in what is called a
</p>
<p>histogram. To make a histogram, we take the scores and values from a
</p>
<p>frequency distribution and represent them in pictorial form. In this case,
</p>
<p>we use a bar to represent each value in the frequency distribution. The
</p>
<p>x-axis (the horizontal axis) of the histogram represents the values of the
</p>
<p>variable we are analyzing&mdash;here, the number of arrests. The y-axis (the
</p>
<p>vertical axis) captures the height of the bars and indicates the number of
</p>
<p>scores&mdash;the frequency&mdash;found in each category. A histogram of the data
</p>
<p>on prior arrests is provided in Figure 3.1. The information presented in
</p>
<p>the histogram is identical to the information presented in the frequency
</p>
<p>distribution in Table 3.1, but the histogram conveys to the reader an im-
</p>
<p>mediate sense of the range of values, the location of clusters of cases,
</p>
<p>and the overall shape of the distribution&mdash;information that is not as eas-
</p>
<p>ily obtainable from a frequency distribution.
</p>
<p>It is important to note that the x-axis in Figure 3.1 correctly represents
</p>
<p>the full range of values for the variable. In particular, note that there is a
</p>
<p>large gap in the distribution from 41 arrests to 65 arrests. Depending on
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>14
</p>
<p>12
</p>
<p>10
</p>
<p>8
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>0
</p>
<p>Number of Arrests
</p>
<p>0 5 10 15 20 25 30 35 40 45 50 55 60 65
</p>
<p>Histogram of Frequency DistributionFigure 3.1
</p>
<p>W H A T A R E F R E Q U E N C Y D I S T R I B U T I O N S A N D H I S T O G R A M S ? 39</p>
<p/>
</div>
<div class="page"><p/>
<p>the software used to generate a histogram, this information may or may
</p>
<p>not be represented. Excluding this information and placing all the bars
</p>
<p>adjacent to each other essentially means ignoring the level of measure-
</p>
<p>ment of the variable and treating the variable as if it were nominal or or-
</p>
<p>dinal. Why would you want to include information about those values
</p>
<p>that have an observed frequency of zero? If you look at Figure 3.1, you
</p>
<p>can see that there are relatively few observations for increasingly greater
</p>
<p>numbers of arrests and that the distance between categories starts to in-
</p>
<p>crease. If the bars were adjacent to each other and correctly labeled, it
</p>
<p>would still be possible for the reader to discern the spread of observa-
</p>
<p>tions. But when the values with an observed frequency of zero are por-
</p>
<p>trayed as well, it is often easier to interpret the histogram.
</p>
<p>Before we get into working with more complex graphs, it is worth
</p>
<p>noting that frequency distributions of interval-level variables are often
</p>
<p>dispersed across a large number of values. Thus, in presenting a fre-
</p>
<p>people, we would likely not want to present the simple distribution of
</p>
<p>income scores. If we did, we might end up with thousands of scores,
</p>
<p>most of which would include only one or two cases. It would take pages
</p>
<p>and pages to illustrate these data in the form of either a frequency distri-
</p>
<p>bution or a histogram. The solution to this problem is to &ldquo;group&rdquo; data
</p>
<p>together in larger categories&mdash;for example, by thousands or tens of thou-
</p>
<p>sands of dollars in the case of incomes. Although there is no hard-and-
</p>
<p>fast rule about how to create such larger groupings, it should be done in
</p>
<p>a way that fairly represents the raw distribution of scores. Do not create
</p>
<p>such a small group of categories that important variation in your data is
</p>
<p>hidden.
</p>
<p>A common source of confusion for students of statistics is the fact that
</p>
<p>statisticians often represent distributions as &ldquo;curves&rdquo; rather than his-
</p>
<p>tograms or frequency distributions. For example, Figure 3.2 uses a curve
</p>
<p>to represent the 2001 SAT math scores of over 1.2 million college-bound
</p>
<p>test takers.1 What is the relationship between a frequency distribution or
</p>
<p>histogram and a distribution represented by a curve, such as the one in
</p>
<p>Figure 3.2?
</p>
<p>When we represent a distribution as a curve, it is usually a distribu-
</p>
<p>tion with a very large number of cases, such as that of SAT math scores
</p>
<p>of 2001 college-bound seniors. We can represent these distributions as
</p>
<p>curves because, with a true interval-scale measure, as the number of
</p>
<p>cases becomes very large, we can construct a histogram in such a way
</p>
<p>1The College Board, 2001 College-Bound Seniors: A Profile of SAT Program Test Tak-
</p>
<p>ers, accessed at http://www.collegeboard.com/sat/cbsenior/yr2001/pdf/NATL.pdf.
</p>
<p>if we were looking at the incomes of a random sample of thousands of
</p>
<p>together into categories that represent a range of values. For example, 
</p>
<p>quency distribution or histogram, it is often necessary to group scores
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A40</p>
<p/>
<div class="annotation"><a href="http://www.collegeboard.com/sat/cbsenior/yr2001/pdf/NATL.pdf">http://www.collegeboard.com/sat/cbsenior/yr2001/pdf/NATL.pdf</a></div>
</div>
<div class="page"><p/>
<p>that it begins to approximate a curve. We do this by making the intervals
</p>
<p>of scores smaller and smaller.
</p>
<p>This process is illustrated in Figure 3.3. We begin with a histogram of
</p>
<p>almost 10,000 cases, in which all of the scores are placed within 10
</p>
<p>200          300          400          500          600          700          800
</p>
<p>Smooth Curve Representation of a Histogram: 
</p>
<p>SAT Math Scores of 2001 College-Bound Seniors
Figure 3.2
</p>
<p>4,000
</p>
<p>3,000
</p>
<p>2,000
</p>
<p>1,000
</p>
<p>0
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Std. dev. = 1.00
Mean = 0.00
N = 9,994.00
</p>
<p>Values
</p>
<p>Constructing a Distribution That Approximates a CurveFigure 3.3
</p>
<p>(a) Distribution with 10 Intervals
</p>
<p>W H A T A R E F R E Q U E N C Y D I S T R I B U T I O N S A N D H I S T O G R A M S ? 41</p>
<p/>
</div>
<div class="page"><p/>
<p>broad categories (see part a). Here each category is represented by one
</p>
<p>large bar. When we increase the number of intervals, or categories, in
</p>
<p>the histogram to 30 (part b), we can still see the individual bars but the
</p>
<p>shape of the distribution is not as jagged. When the number of cate-
</p>
<p>gories is increased to 650 (part c), the histogram looks more like a
</p>
<p>smooth curve than a histogram, although if you look closely you will be
</p>
<p>1,200
</p>
<p>1,000
</p>
<p>800
</p>
<p>600
</p>
<p>400
</p>
<p>200
</p>
<p>0
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Std. dev. = 1.00
Mean = 0.00
N = 9,994.00
</p>
<p>Values
</p>
<p>(b) Distribution with 30 Intervals
</p>
<p>50
</p>
<p>40
</p>
<p>30
</p>
<p>20
</p>
<p>10
</p>
<p>0
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Std. dev. = 1.00
Mean = 0.00
N = 9,994.00
</p>
<p>Values
</p>
<p>(c) Distribution with 650 Intervals
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A42</p>
<p/>
</div>
<div class="page"><p/>
<p>able to identify the bars that make up the curve. If our distribution had
</p>
<p>included an even larger number of scores and categories, the curve
</p>
<p>would have become even smoother.
</p>
<p>E x t e n d i n g  H i s t o g r a m s  
t o  M u l t i p l e  G r o u p s :  U s i n g  B a r  C h a r t s
</p>
<p>Although histograms provide a quick and easy way to present data from
</p>
<p>a frequency distribution, they can be used to present the frequency dis-
</p>
<p>tribution for only a single group. What happens when we have fre-
</p>
<p>quency distributions for the same variable for more than one group?
</p>
<p>There are many instances where an investigator will want to present re-
</p>
<p>sults graphically for two or more groups, such as treatment and control
</p>
<p>groups, males and females, or states. We could simply construct a his-
</p>
<p>togram for each group we were interested in, but we would not be able
</p>
<p>to make direct comparisons across the groups in the form and shape of
</p>
<p>the distributions. A simple extension of the histogram to multiple groups
</p>
<p>makes use of the bar chart. Bar charts allow us to present information
</p>
<p>for multiple groups simultaneously. Bar charts are constructed in much
</p>
<p>the same way as histograms. The x-axis generally represents the values
</p>
<p>of the variable, and the y-axis the size of the bar.2 Most statistical soft-
</p>
<p>ware and spreadsheet packages allow the user to construct bar charts to
</p>
<p>depict patterns in the various groups. Two of the more common ap-
</p>
<p>proaches involve placing the bars side by side and on top of each other.
</p>
<p>Presenting data for each group in adjacent bars gives the reader a sense
</p>
<p>of the distribution for each group and allows for immediate comparison
</p>
<p>of distributions across groups.
</p>
<p>Table 3.2 presents simulated frequency distributions for numbers of
</p>
<p>prior convictions among 100 men and 100 women arrested for drug of-
</p>
<p>fenses. The frequencies for male and female arrestees suggest that males
</p>
<p>In addition to being able to incorporate data from more than one
</p>
<p>group, a bar chart has other benefits. For example, bars may be pre-
</p>
<p>2Most statistical software and spreadsheet packages allow for manipulation of many
</p>
<p>characteristics of a bar chart, including color, shading, patterning, and dimensions
</p>
<p>(two vs. three). While this allows for the construction of unique charts, the investiga-
</p>
<p>tor should be wary of adding so much detail to a chart that the reader loses the point
</p>
<p>the investigator is trying to make.
</p>
<p>females. Figure 3.4 portrays the male and female frequency distributions 
</p>
<p>have had more prior contact with the criminal justice system than the 
</p>
<p>in a bar chart.
</p>
<p>43E X T E N D I N G H I S T O G R A M S T O M U L T I P L E G R O U P S</p>
<p/>
</div>
<div class="page"><p/>
<p>sented either vertically, as in Figure 3.4, or horizontally, as in Figure 3.5.
</p>
<p>The only difference between the bar charts in Figures 3.4 and 3.5 is that
</p>
<p>in the latter the axes have been flipped: The y-axis now represents the
</p>
<p>rules about which form of bar chart is better. Some people like vertical
</p>
<p>bar charts because they can draw an imaginary horizontal line across the
</p>
<p>graph to get a sense of which bars are larger and smaller. Alternatively,
</p>
<p>other people prefer horizontal bars because looking at them mimics the
</p>
<p>Frequency Distributions of Number of Prior Convictions 
</p>
<p>for Male and Female Drug Arrestees
</p>
<p>FREQUENCY
</p>
<p>Number of Prior Convictions Male Female
</p>
<p>0 25 40
1 20 25
2 15 15
3 10 7
4 8 4
5 6 0
6 2 3
7 6 3
8 5 1
9 0 2
</p>
<p>10 3 0
Total 100 100
</p>
<p>Table 3.2
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>40
</p>
<p>30
</p>
<p>20
</p>
<p>10
</p>
<p>0
</p>
<p>Number of Prior Convictions 
</p>
<p>0     1      2      3      4      5      6     7      8      9     10
</p>
<p>Male
</p>
<p>Female
</p>
<p>Bar Chart for Male and Female Frequency DistributionsFigure 3.4
</p>
<p>values of the variable (here, the number of prior convictions), and the 
</p>
<p>x-axis represents the size of the bar (the frequency). There are no specific
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A44</p>
<p/>
</div>
<div class="page"><p/>
<p>process of reading text from left to right. Since the research on visual
</p>
<p>perception is mixed about whether vertical or horizontal bars are more
</p>
<p>effective&mdash;there are benefits and drawbacks to both approaches&mdash;the
</p>
<p>preferences of the investigator typically determine which approach is
</p>
<p>used.3
</p>
<p>A cautionary note is needed about the use of bar charts for comparing
</p>
<p>multiple groups. In Figure 3.4, the number of individuals in each group
</p>
<p>was equal, allowing us to make a direct comparison of each group&rsquo;s fre-
</p>
<p>quency distribution. An investigator would run into trouble, however, if
</p>
<p>the two groups did not have the same number of cases. Say one group
</p>
<p>had two or three times as many cases as a second group. If we simply
</p>
<p>presented a bar chart of the observed frequencies, then we would be
</p>
<p>limited to discussing the shape of each group&rsquo;s distribution; we would
</p>
<p>be unable to make valid comparisons across the two groups. For exam-
</p>
<p>ple, in the frequency distributions of prior convictions in Table 3.2, sup-
</p>
<p>Male
</p>
<p>Female
</p>
<p>10
</p>
<p>9
</p>
<p>8
</p>
<p>7
</p>
<p>6
</p>
<p>5
</p>
<p>4
</p>
<p>3
</p>
<p>2
</p>
<p>1
</p>
<p>0
</p>
<p>0           10          20         30         40  
</p>
<p>Frequency
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>P
ri
</p>
<p>or
 C
</p>
<p>on
vi
</p>
<p>ct
io
</p>
<p>n
s
</p>
<p>Horizontal Bar Chart for Male and Female Frequency DistributionsFigure 3.5
</p>
<p>3For a good discussion of the benefits and drawbacks of various approaches, see Gary
</p>
<p>T. Henry, Graphing Data: Techniques for Display and Analysis (Thousand Oaks, CA:
</p>
<p>Sage, 1995).
</p>
<p>4E X T E N D I N G H I S T O G R A M S T O M U L T I P L E G R O U P S 5</p>
<p/>
</div>
<div class="page"><p/>
<p>pose that we doubled the frequencies for the male arrestees, giving us a
</p>
<p>total of 200 males in the sample. In Figure 3.6, the bars representing
</p>
<p>male arrests are twice as large as they are in Figure 3.4, and one might
</p>
<p>be tempted to say that more males have zero, one, or two arrests. Strictly
</p>
<p>speaking, such an observation is correct&mdash;the frequency is larger for one
</p>
<p>group than for the other group&mdash;but it misrepresents the relative sizes of
</p>
<p>the two groups and the relative distribution of cases within each group,
</p>
<p>which are unchanged by doubling the number of male arrests.
</p>
<p>In research on crime and criminal justice, the groups we are inter-
</p>
<p>ested in comparing rarely have an equal number of cases. For example,
</p>
<p>Table 3.3 presents the frequency distributions of years of education for
</p>
<p>capital offenders executed in the United States between 1977 and 1995,
</p>
<p>distinguished by the recorded race of the offender. These data come
</p>
<p>from a public data file archived by the Bureau of Justice Statistics.4 The
</p>
<p>bar chart for these data is presented in Figure 3.7. It is important to note
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>50
</p>
<p>40
</p>
<p>30
</p>
<p>20
</p>
<p>10
</p>
<p>0
</p>
<p>Number of Prior Convictions 
</p>
<p>0      1      2      3     4      5      6      7     8      9     10
</p>
<p>Male
</p>
<p>Female
</p>
<p>Bar Chart for 200 Male and 100 Female ArresteesFigure 3.6
</p>
<p>4Capital Punishment in the United States, ICPSR Study #6956, available through the
</p>
<p>National Archive of Criminal Justice Data (NACJD) at http://www.icpsr.umich.edu/
</p>
<p>NACJD.
</p>
<p>that whites outnumber African Americans at a rate of about 3 to 2. This tells 
</p>
<p>us that although we can evaluate the frequency distributions for whites and
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A46</p>
<p/>
<div class="annotation"><a href="http://www.icpsr.umich.edu/NACJD">http://www.icpsr.umich.edu/NACJD</a></div>
<div class="annotation"><a href="http://www.icpsr.umich.edu/NACJD">http://www.icpsr.umich.edu/NACJD</a></div>
</div>
<div class="page"><p/>
<p>Number of Years of Education Among Offenders Executed 
</p>
<p>in the United States, 1977 to 1995, by Race of Offender
</p>
<p>RACE OF OFFENDER
</p>
<p>Years of Education White African American
</p>
<p>7 24 7
8 14 17
9 19 16
</p>
<p>10 24 20
11 14 18
12 60 31
13 8 1
14 12 2
15 2 0
16 3 0
17 1 0
Total 181 112
</p>
<p>Table 3.3
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>70
</p>
<p>60
</p>
<p>50
</p>
<p>40
</p>
<p>30
</p>
<p>20
</p>
<p>10
</p>
<p>0
</p>
<p>Years of Education 
</p>
<p>7       8     9     10     11    12   13    14    15    16    17   
</p>
<p>Whites
</p>
<p>African Americans
</p>
<p>Bar Chart for Number of Years of Education Among Offenders 
</p>
<p>Executed in the United States, 1977 to 1995, by Race of Offender
Figure 3.7
</p>
<p>4E X T E N D I N G H I S T O G R A M S T O M U L T I P L E G R O U P S 7</p>
<p/>
</div>
<div class="page"><p/>
<p>African Americans separately, we should avoid any direct comparisons
</p>
<p>across the two groups.
</p>
<p>How do we address the problem of unequal group sizes? The most
</p>
<p>direct way is to convert the observed frequencies into proportions or
</p>
<p>percentages. A proportion has a value between 0 and 1 and represents
</p>
<p>the fraction of all observations that fall into a category. We calculate a
</p>
<p>proportion as:
</p>
<p>Equation 3.1
</p>
<p>where Ncat refers to the number of observations in a given category and
</p>
<p>Ntotal refers to the total number of observations. For example, to find the
</p>
<p>proportion of whites who had 8 years of education, we would take the
</p>
<p>number of white offenders who had 8 years of education (14) and divide
</p>
<p>it by the total number of whites executed (181):
</p>
<p>Proportion � 
Ncat
Ntotal
</p>
<p>By convention, we generally round a proportion to the second decimal
</p>
<p>place. (However, as we will discuss in more detail in the next chapter,
</p>
<p>your decision as to how precisely to present a statistic should be based
</p>
<p>on the specific research problem you are examining.) The proportion of
</p>
<p>executed white offenders who had 8 years of education is thus about
</p>
<p>0.08.
</p>
<p>Sometimes researchers like to transform a proportion into a percent-
</p>
<p>age, because percentages are more commonly used in our everyday
</p>
<p>lives. We obtain the percentage simply by multiplying the proportion
</p>
<p>by 100:
</p>
<p>Equation 3.2Percentage � � NcatNtotal� � 100
</p>
<p>W orking It Out
</p>
<p> � 0.077
</p>
<p> � 
14
181
</p>
<p> Proportion � 
Ncat
Ntotal
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A48</p>
<p/>
</div>
<div class="page"><p/>
<p>For our example of executed white offenders with 8 years of education,
</p>
<p>we multiply 14/181 by 100:
</p>
<p>W orking It Out
</p>
<p> � 7.7%
</p>
<p> � � 14181� � 100
 Percentage � � NcatNtotal� � 100
</p>
<p>This tells us that about 8% of executed white offenders had 8 years of
</p>
<p>education. Table 3.4 presents the observed frequencies and correspond-
</p>
<p>ing percentages (calculated to the third decimal place) for every cell in
</p>
<p>Table 3.3.
</p>
<p>At this point, we can graph either the proportions or the percent-
</p>
<p>ages in a bar chart. The selection of proportions or percentages does
</p>
<p>not matter, as it will not have any bearing on the shape of the distribu-
</p>
<p>tions. The bar chart in Figure 3.8 uses percentages to display years of
</p>
<p>education among executed offenders in the United States, distinguished
</p>
<p>by race.
</p>
<p>Frequency Distributions and Percentages for Number of Years 
</p>
<p>of Education Among Executed Offenders, by Race
</p>
<p>RACE OF OFFENDER
</p>
<p>WHITE AFRICAN AMERICAN
</p>
<p>Years of Education Freq. % Freq. %
</p>
<p>7 24 13.260 7 6.250
8 14 7.735 17 15.179
9 19 10.497 16 14.286
</p>
<p>10 24 13.260 20 17.857
11 14 7.735 18 16.071
12 60 33.149 31 27.679
13 8 4.420 1 0.893
14 12 6.630 2 1.786
15 2 1.105 0 0.000
16 3 1.657 0 0.000
17 1 0.552 0 0.000
Total 181 112
</p>
<p>Table 3.4
</p>
<p>4E X T E N D I N G H I S T O G R A M S T O M U L T I P L E G R O U P S 9</p>
<p/>
</div>
<div class="page"><p/>
<p>U s i n g  B a r  C h a r t s  w i t h  N o m i n a l  o r  O r d i n a l  D a t a
</p>
<p>The previous examples of bar charts and histograms have focused on
</p>
<p>variables measured at the interval level. Bar charts are also quite useful
</p>
<p>for visually representing nominally or ordinally measured variables. In
</p>
<p>much of the research on public opinion about use of the death penalty in
</p>
<p>the United States, there are sharp differences between the views of whites
</p>
<p>and those of African Americans. Table 3.5 presents the level of support
</p>
<p>for using the death penalty for first-degree murderers, distinguished by
</p>
<p>P
er
</p>
<p>ce
n
</p>
<p>ta
ge
</p>
<p>35
</p>
<p>30
</p>
<p>25
</p>
<p>20
</p>
<p>15
</p>
<p>10
</p>
<p>5
</p>
<p>0
</p>
<p>Years of Education 
</p>
<p>7       8     9      10     11    12   13    14    15    16    17   
</p>
<p>Whites
</p>
<p>African Americans
</p>
<p>Percentages of Executed Offenders with Various Numbers of Years of Education, by RaceFigure 3.8
</p>
<p>Level of Agreement with Use of the Death Penalty 
</p>
<p>for Convicted First-Degree Murderers
</p>
<p>RACE OF RESPONDENT
</p>
<p>WHITE AFRICAN AMERICAN
</p>
<p>Level of Agreement Freq. % Freq. %
</p>
<p>Strongly agree 567 51.266 57 37.748
Agree 290 26.221 32 21.192
Neutral/no opinion 76 6.872 24 15.894
Disagree 106 9.584 25 16.556
Strongly disagree 67 6.058 13 8.609
Total 1,106 151
</p>
<p>Table 3.5
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A50</p>
<p/>
</div>
<div class="page"><p/>
<p>race of the respondent. The data come from the General Social Survey
</p>
<p>(GSS), administered by the National Opinion Research Center at the Uni-
</p>
<p>versity of Chicago. Respondents were asked: &ldquo;How strongly do you agree
</p>
<p>with the following statement: The death penalty should be used for per-
</p>
<p>sons convicted of first-degree murder.&rdquo; The responses were strongly
</p>
<p>agree, agree, neutral/no opinion, disagree, and strongly disagree.5
</p>
<p>To chart these data, we cannot graph the numbers of white and
</p>
<p>African American respondents in each category and hope to make sense
</p>
<p>of the patterns, since there are about seven times more white respon-
</p>
<p>dents than African American respondents. Again, we construct a bar
</p>
<p>chart using the proportion or percentage of respondents who fall into
</p>
<p>each category. A bar chart presenting percentages of white and African
</p>
<p>American respondents in each category appears in Figure 3.9.
</p>
<p>P i e  C h a r t s
</p>
<p>Pie charts offer another way of displaying data graphically if the num-
</p>
<p>ber of categories of a variable is relatively small. Each wedge in a pie
</p>
<p>chart is a proportional representation of the number of cases in that cate-
</p>
<p>gory. When we present data on the percentage or proportion of cases in
</p>
<p>5The entire GSS database is publicly available and can be accessed at http://www.
</p>
<p>icpsr.umich.edu/GSS. Data presented here are drawn from a 1991 study.
</p>
<p>P
er
</p>
<p>ce
n
</p>
<p>ta
ge
</p>
<p>60
</p>
<p>50
</p>
<p>40
</p>
<p>30
</p>
<p>20
</p>
<p>10
</p>
<p>0
</p>
<p>Use of Death Penalty 
</p>
<p>Whites
</p>
<p>African Americans
</p>
<p>Strongly Agree            Agree            Neutral         Disagree       Strongly Disagree
</p>
<p>Bar Chart for Level of Agreement with Use of the Death Penalty 
</p>
<p>for Convicted First-Degree Murderers
Figure 3.9
</p>
<p>51P I E  C H A R T S</p>
<p/>
<div class="annotation"><a href="http://www.fbi.gov/ucr/ucr.htm">http://www.fbi.gov/ucr/ucr.htm</a></div>
<div class="annotation"><a href="http://www.icpsr.umich.edu/GSS">http://www.icpsr.umich.edu/GSS</a></div>
</div>
<div class="page"><p/>
<p>a pie chart, the information contained in a pie chart is identical to that
</p>
<p>We will use the death penalty opinion data in Table 3.5 to illustrate
</p>
<p>The information presented in Figure 3.10 is identical to that in Figure
</p>
<p>3.9. But the need for two separate pie charts to represent the data for the
</p>
<p>two groups of respondents points out one of the limitations of using pie
</p>
<p>charts for comparing multiple groups. In Figure 3.9, the bars for the two
</p>
<p>groups of respondents are presented side by side. The reader is able to
</p>
<p>quickly assess which group is larger, which group is smaller, and the mag-
</p>
<p>nitude of the difference. With the two pie charts represented in parts a and
</p>
<p>b of Figure 3.10, the reader has to go back and forth between the pies,
</p>
<p>match up the category of interest, and then try to infer the magnitude of the
</p>
<p>difference in size between the two wedges. Although pie charts are an easy
</p>
<p>and effective way of representing the relative sizes of different categories of
</p>
<p>a variable, we discourage the use of pie charts for any type of cross-group
</p>
<p>comparison, to avoid the potential for confusing or misleading the reader.
</p>
<p>T i m e  S e r i e s  D a t a
</p>
<p>Study of many important issues in crime and criminal justice requires the
</p>
<p>use of time series data. Time series data include measures on the same
</p>
<p>variable for the same unit of analysis at more than one point in time. For
</p>
<p>Strongly Disagree (6.1%)
</p>
<p>Agree (26.2%)
</p>
<p>Neutral (6.9%)
</p>
<p>Disagree (9.6%) Strongly
Agree (51.3%)
</p>
<p>Strongly
Disagree (8.6%)
</p>
<p>Agree (21.2%)
</p>
<p>Neutral
(15.9%)
</p>
<p>Disagree
(16.6%)
</p>
<p>Strongly
Agree (37.7%)
</p>
<p>Percentage of Agreement with Use of the Death Penalty for Convicted First-Degree MurderersFigure 3.10
</p>
<p>(a) White Respondents (b) African American Respondents
</p>
<p>aspects&mdash;size, shape, orientation, dimensions (two or three), position 
</p>
<p>of wedges, colors, shades, and so on.
</p>
<p>and spreadsheet packages allow the researcher to manipulate various 
</p>
<p>presented in a bar chart. As with bar charts, most statistical software 
</p>
<p>responses for white and African American respondents, respectively.
</p>
<p>the construction of pie charts. Parts a and b of Figure 3.10 present the
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A52</p>
<p/>
</div>
<div class="page"><p/>
<p>example, in a study of adolescents, the same individuals may be inter-
</p>
<p>viewed two or more times. Or, in an examination of the effectiveness of
</p>
<p>a treatment program, information on key characteristics of participants
</p>
<p>may be collected before and after the treatment, to test for change.
</p>
<p>An example of time series data more familiar to many students comes
</p>
<p>from the Federal Bureau of Investigation&rsquo;s annual Uniform Crime Re-
</p>
<p>ports.6 Included in the reports are the &ldquo;official&rdquo; crime rates, which tell us
</p>
<p>the total crime rate, the murder rate, the burglary rate, and so on. We
</p>
<p>can easily locate a crime rate for the United States&mdash;as well as smaller
</p>
<p>geopolitical units, such as states or counties&mdash;for some period of time.
</p>
<p>Table 3.6 presents the total crime rate and murder rate for the United
</p>
<p>States per 100,000 people for the years 1990 to 2000.
</p>
<p>In a time series plot, data are graphed over time, with the measure
</p>
<p>of time along the x-axis. For the data in Table 3.6, the measure of time is
</p>
<p>years, but in other cases, we may have daily, weekly, monthly, or quar-
</p>
<p>terly data. The y-axis then represents the value&mdash;here, the crime rate.
</p>
<p>Figure 3.11 presents a plot of the total crime rate per 100,000 people in
</p>
<p>the United States for the period 1990 to 2000. As you can see, the time
</p>
<p>series plot provides a very powerful way to present data over time. The
</p>
<p>&ldquo;crime drop&rdquo; in the United States during this period is clearly illustrated
</p>
<p>in Figure 3.11.
</p>
<p>It is also possible to plot more than one time series on the same
</p>
<p>graph, but be careful about the values of the different series. If one se-
</p>
<p>ries has values that are many times larger than those in the other series,
</p>
<p>you run the risk of generating a plot in which the line of one group
</p>
<p>looks misleadingly like a straight line. Figure 3.12, which shows total
</p>
<p>Total Crime Rate and Murder Rate in the United States, 1990 to 2000
</p>
<p>TOTAL CRIME RATE MURDER RATE 
</p>
<p>YEAR (PER 100,000) (PER 100,000)
</p>
<p>1990 5,820.300 9.4000
1991 5,897.800 9.8000
1992 5,660.200 9.3000
1993 5,484.400 9.5000
1994 5,373.500 9.0000
1995 5,275.900 8.2000
1996 5,086.600 7.4000
1997 4,930.000 6.8000
1998 4,619.300 6.3000
1999 4,266.800 5.7000
2000 4,124.800 5.5000
</p>
<p>Table 3.6
</p>
<p>6Federal Bureau of Investigation, Crime in the United States, available at http://www.
</p>
<p>fbi.gov/ucr/ucr.htm.
</p>
<p>T I M E S E R I E S D A T A 53</p>
<p/>
<div class="annotation"><a href="http://www.fbi.gov/ucr/ucr.htm">http://www.fbi.gov/ucr/ucr.htm</a></div>
<div class="annotation"><a href="http://www.fbi.gov/ucr/ucr.htm">http://www.fbi.gov/ucr/ucr.htm</a></div>
</div>
<div class="page"><p/>
<p>crime rates and murder rates for the 1990 to 2000 period, illustrates this
</p>
<p>problem. Since the total crime rate ranges between about 4,000 and
</p>
<p>6,000 per 100,000 while the murder rate varies between about 5 and 10
</p>
<p>per 100,000, the line representing the murder rate appears straight and
</p>
<p>indeed can hardly be seen. In such a case, one solution is to construct a
</p>
<p>R
at
</p>
<p>e
</p>
<p>7,000
</p>
<p>6,500
</p>
<p>6,000
</p>
<p>5,500
</p>
<p>5,000
</p>
<p>4,500
</p>
<p>4,000
</p>
<p>3,500
</p>
<p>3,000
</p>
<p>Year
19
</p>
<p>90
19
</p>
<p>91
19
</p>
<p>92
19
</p>
<p>93
19
</p>
<p>94
19
</p>
<p>95
19
</p>
<p>96
19
</p>
<p>97
19
</p>
<p>98
19
</p>
<p>99
20
</p>
<p>00
</p>
<p>Total Crime Rate
</p>
<p>Total Crime Rate (per 100,000 People) in the United States, 1990 to 2000Figure 3.11
</p>
<p>R
at
</p>
<p>e
</p>
<p>7,000
</p>
<p>6,000
</p>
<p>5,000
</p>
<p>4,000
</p>
<p>3,000
</p>
<p>2,000
</p>
<p>1,000
</p>
<p>0
</p>
<p>Year
</p>
<p>19
90
</p>
<p>19
91
</p>
<p>19
92
</p>
<p>19
93
</p>
<p>19
94
</p>
<p>19
95
</p>
<p>19
96
</p>
<p>19
97
</p>
<p>19
98
</p>
<p>19
99
</p>
<p>20
00
</p>
<p>Total Crime Rate
</p>
<p>Murder Rate
</p>
<p>Total Crime Rate and Murder Rate (per 100,000 People) 
</p>
<p>in the United States, 1990 to 2000: Single Vertical Axis
Figure 3.12
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A54</p>
<p/>
</div>
<div class="page"><p/>
<p>time series plot with a second y-axis on the right side of the chart. Figure
</p>
<p>3.13 adds a second y-axis to account for the smaller values associated
</p>
<p>with the murder rate, providing a better depiction of the annual variation
</p>
<p>in both rates.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>A frequency distribution is an arrangement of data according to the
</p>
<p>frequency with which each value occurs. The data may be represented
</p>
<p>in a table or in graphic form in a histogram, which uses a bar to repre-
</p>
<p>sent the frequency for each value.
</p>
<p>Bar charts can be used to represent the frequencies, percentages,
</p>
<p>or proportions of variables, regardless of whether they have been mea-
</p>
<p>sured at the nominal, ordinal, or interval level. In a vertical bar chart, the
</p>
<p>sizes of the bars are indicated along the y-axis and correspond to the fre-
</p>
<p>quency, the percentage, or the proportion. The values, or categories, of
</p>
<p>the variable are represented along the x-axis. When comparing two or
</p>
<p>more groups in a bar chart, it is important to have the values represent
</p>
<p>the percentage or proportion of cases, since the presentation of frequen-
</p>
<p>cies could be misleading if the groups are of very different sizes.
</p>
<p>Pie charts are another common way to represent variables with a
</p>
<p>small number of categories or values. The size of each wedge in a pie
</p>
<p>chart corresponds to the relative size of that category&rsquo;s frequency count.
</p>
<p>T
ot
</p>
<p>al
 C
</p>
<p>ri
m
</p>
<p>e 
R
</p>
<p>at
e
</p>
<p>M
u
</p>
<p>rd
er
</p>
<p> R
at
</p>
<p>e
</p>
<p>7,000
</p>
<p>6,000
</p>
<p>5,000
</p>
<p>4,000
</p>
<p>3,000
</p>
<p>2,000
</p>
<p>1,000
</p>
<p>0
</p>
<p>Year
</p>
<p>19
90
</p>
<p>19
91
</p>
<p>19
92
</p>
<p>19
93
</p>
<p>19
94
</p>
<p>19
95
</p>
<p>19
96
</p>
<p>19
97
</p>
<p>19
98
</p>
<p>19
99
</p>
<p>20
00
</p>
<p>Total Crime Rate
</p>
<p>Murder Rate
</p>
<p>12
</p>
<p>10
</p>
<p>8
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>0
</p>
<p>Total Crime Rate and Murder Rate (per 100,000 People) in the United States, 
</p>
<p>1990 to 2000: Two Vertical Axes
Figure 3.13
</p>
<p>55C H A P T E R S U M M A R Y</p>
<p/>
</div>
<div class="page"><p/>
<p>Pie charts are better suited to describing data from a single sample or
</p>
<p>group than data from multiple groups.
</p>
<p>Time series data may be graphically displayed in a time series plot,
</p>
<p>a line graph that portrays the values of a variable over some period of
</p>
<p>time, such as days, weeks, months, or years. To allow for the compari-
</p>
<p>son of multiple variables, additional lines can be easily incorporated into
</p>
<p>the line graph. If the magnitudes of the variables differ by a large degree,
</p>
<p>it is possible to add an additional y-axis so that the lines can be overlaid
</p>
<p>and common or unique trends in the plotted variables discerned.
</p>
<p>K e y  T e r m s
</p>
<p>bar chart A graph in which bars represent
</p>
<p>frequencies, percentages, or proportions
</p>
<p>for the categories or values of a variable.
</p>
<p>frequency The number of times that a
</p>
<p>score or value occurs.
</p>
<p>frequency distribution An arrangement
</p>
<p>of scores in order from the lowest to the
</p>
<p>highest, accompanied by the number of
</p>
<p>times each score occurs.
</p>
<p>histogram A bar graph used to represent
</p>
<p>a frequency distribution.
</p>
<p>percentage A relation between two num-
</p>
<p>bers in which the whole is accorded a
</p>
<p>value of 100 and the other number is given
</p>
<p>a numerical value corresponding to its
</p>
<p>share of the whole.
</p>
<p>pie chart A graph in which a circle (called
</p>
<p>a pie) is cut into wedges to represent the
</p>
<p>relative size of each category&rsquo;s frequency
</p>
<p>count.
</p>
<p>proportion A relation between two num-
</p>
<p>bers in which the whole is accorded a
</p>
<p>value of 1 and the other number is given a
</p>
<p>numerical value corresponding to its share
</p>
<p>of the whole.
</p>
<p>time series data Repeated measures of
</p>
<p>the same variable over some regularly oc-
</p>
<p>curring time period, such as days, months,
</p>
<p>or years.
</p>
<p>time series plot A line graph that con-
</p>
<p>nects repeated measures of the same vari-
</p>
<p>able over some regularly occurring time
</p>
<p>period, such as days, months, or years.
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>Ncat Number of cases in a category of a variable
</p>
<p>Ntotal Total number of cases
</p>
<p>To calculate the proportion of cases falling into a category:
</p>
<p>Proportion � 
Ncat
Ntotal
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A56</p>
<p/>
</div>
<div class="page"><p/>
<p>To calculate the percentage of cases falling into a category:
</p>
<p>E x e r c i s e s
</p>
<p>3.1 A team of researchers observed the behavior of 20 children during
breaktime on a school playground. The observers recorded how many
times each child performed a &ldquo;violent act&rdquo;&mdash;be it a push, a kick, or a
punch&mdash;against another child. The scores of the 20 children were as
follows:
</p>
<p>2 0 1 0 0 4 0 10 2 1 3 3 0 1 4 11 0 0 2 0
</p>
<p>a. Construct a frequency distribution table for the above results.
</p>
<p>b. Construct a histogram of the frequency distribution.
</p>
<p>c. How might you interpret the results?
</p>
<p>3.2 Workers at an inner-city rape crisis center asked all the callers on a
given night for their ages. For a total of 50 callers, the results were as
follows:
</p>
<p>28 39 17 18 22 31 26 27 16 20
</p>
<p>34 35 29 26 17 23 22 23 37 28
</p>
<p>24 19 14 25 27 19 24 26 41 27
</p>
<p>21 24 17 16 35 25 19 23 29 18
</p>
<p>23 26 24 43 28 21 21 36 26 27
</p>
<p>a. Construct a frequency distribution table for the above results.
</p>
<p>b. Based on the frequency distribution, construct three different his-
tograms, with the data grouped in 1-year intervals, 3-year intervals,
and 10-year intervals.
</p>
<p>3.3 A review of the records of 20 male and 20 female adjudicated delin-
quents revealed the following numbers of prior arrests for violent
crimes:
</p>
<p>Male: 0 2 1 7 4 2 2 1 6 5 0 0 1 2 4 1 0 0 2 9
</p>
<p>Female: 0 0 1 4 2 3 1 1 5 1 0 0 0 1 0 3 2 1 0 1
</p>
<p>a. Construct three frequency distribution tables: one for males, one for
females, and one for the male and female data combined.
</p>
<p>b. Construct a histogram for each of the three frequency distributions.
</p>
<p>c. How might you interpret the results?
</p>
<p>Percentage � � NcatNtotal� � 100
</p>
<p>E X E R C I S E S 57</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 In a survey of 75 offenders convicted through guilty pleas, respon-
dents were asked who had the greatest influence on their decision to
plead guilty. Here are their responses: defense attorney (n � 35),
spouse/partner (n � 25), prosecutor (n � 10), and judge (n � 5).
</p>
<p>a. Graph these data on three bar charts using frequencies, propor-
tions, and percentages.
</p>
<p>b. Graph these data on a pie chart.
</p>
<p>3.5 Suppose a researcher conducted a survey to assess the link between
</p>
<p>researcher found that 150 thought the punishments were &ldquo;too harsh,&rdquo;
100 thought the punishments were &ldquo;about right,&rdquo; and 50 thought the
punishments were &ldquo;too lenient.&rdquo;
</p>
<p>b. Explain your choice of chart.
</p>
<p>c. Describe the patterns that you find in this type of chart.
</p>
<p>3.6 The burglary rates for two cities over a 10-year period are reported in
the following table:
</p>
<p>Year City A City B
</p>
<p>1991 2,576 875
</p>
<p>1992 2,675 966
</p>
<p>1993 2,892 1,015
</p>
<p>1994 3,014 1,325
</p>
<p>1995 2,852 1,779
</p>
<p>1996 2,651 1,954
</p>
<p>1997 2,443 2,333
</p>
<p>1998 2,519 2,121
</p>
<p>1999 2,999 2,657
</p>
<p>2000 2,840 2,005
</p>
<p>a. Present this information in the way that you think is most
informative.
</p>
<p>b. Explain why you selected this type of chart.
</p>
<p>c. Describe the patterns that you observe in this chart.
</p>
<p>separately.
</p>
<p>a. Present this information graphically in the way that you think is
most informative&mdash;either for the full sample or for each gender
</p>
<p>ments were &ldquo;too harsh.&rdquo; Among 300 female respondents, the
</p>
<p>gender and attitudes about the criminal justice system. Among 850 male
respondents, the researcher found that 500 thought the punishments
given by the courts were &ldquo;too lenient,&rdquo; 200 thought the punish-
ments were &ldquo;about right,&rdquo; and the remaining 150 thought the punish-
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A58</p>
<p/>
</div>
<div class="page"><p/>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>SPSS and Stata both contain a wide range of graphics tools that are comparable 
</p>
<p>to those found in many of the better spreadsheet packages. The following com-
</p>
<p>puter exercises are intended to guide you through the basic steps in producing 
</p>
<p>graphs in the two different programs. In fact, the graphics commands we illus-
</p>
<p>trate from SPSS are what are referred to as &ldquo;legacy&rdquo; commands in SPSS&rsquo;s win-
</p>
<p>dowed environment. The change in SPSS nomenclature is a consequence of the 
</p>
<p>graphics capabilities of the program having expanded greatly in recent years. The 
</p>
<p>graphics options in SPSS are now so complex that the best way to learn about 
</p>
<p>and work with the graphics commands is to do it interactively. The description 
</p>
<p>and discussion of each command is meant only as a brief overview of some of 
</p>
<p>the capabilities of either SPSS or Stata that will produce figures comparable to 
</p>
<p>those discussed in this chapter. We encourage you to experiment and to explore 
</p>
<p>the range of features available in the graphics commands. On the Web site for 
</p>
<p>the text, we have included syntax files that provide examples of each of the 
</p>
<p>graphics commands in either SPSS (Chapter_3.sps) or Stata (Chapter_3.do).
</p>
<p>SPSS
</p>
<p>Frequency Distributions and Histograms
</p>
<p>Frequency distributions and histograms are produced quite easily in SPSS through 
</p>
<p>the use of  the FREQUENCIES command (which can also be shortened as 
</p>
<p>FREQ):
</p>
<p>Following VARIABLES = would be a list of the variables for which you wanted 
</p>
<p>SPSS to compute frequency distributions. The /HISTOGRAM option will gen-
</p>
<p>erate a histogram for the corresponding variables included in the list. You should 
</p>
<p>note that the default in SPSS is to construct intervals for the values of the varia-
</p>
<p>ble to be graphed. The size and the number of intervals can be modified by edit-
</p>
<p>ing the chart in SPSS, which is accomplished by double-clicking on the chart in 
</p>
<p>the SPSS output window. This will open an additional window that allows you to 
</p>
<p>make numerous changes to your chart and customize its appearance. We encour-
</p>
<p>age you to experiment with many of the options available to you.
</p>
<p>The GRAPH command with the /HISTOGRAM option provides an 
</p>
<p> alternative way of obtaining just a histogram, without the corresponding frequen-
</p>
<p>cy distribution. The simplest way of generating a histogram using SPSS syntax is 
</p>
<p>the following:
</p>
<p>The histogram generated by this command will be identical to that produced by 
</p>
<p>the FREQUENCIES command.
</p>
<p>C O M P U T E R E X E R C I S E S 59
</p>
<p>FREQUENCIES VARIABLES = variable_names /HISTOGRAM.
</p>
<p>GRAPH /HISTOGRAM = variable_name.</p>
<p/>
</div>
<div class="page"><p/>
<p>This will generate a bar chart where each bar reflects the number of cases with 
</p>
<p>that value, much like the histogram, but with one important difference. Bar 
</p>
<p>charts are not sensitive to the values of the variable being graphed&mdash;the values 
</p>
<p>are treated as ordinal and are all positioned along the x-axis at constant intervals. 
</p>
<p>Histograms make use of the level of measurement, so that a variable measured at 
</p>
<p>the interval level of measurement will be appropriately scaled on the x-axis.
</p>
<p>To produce a bar chart that includes bars for multiple groups, there are sev-
</p>
<p>eral different ways of doing this in SPSS. A simple bar chart for multiple groups 
</p>
<p>can be obtained by again executing the GRAPH command. To generate a mul-
</p>
<p>tiple group bar chart that uses percentages as the point of comparison across 
</p>
<p>groups, use the following command:
</p>
<p>To generate a multiple group bar chart that uses the number of cases in each 
</p>
<p>group, use the following command:
</p>
<p>Note that the type of graph is now /BAR(GROUPED), the percentage of cases 
</p>
<p>by the PCT option, the number of cases by the COUNT option, the variable 
</p>
<p>we are interested in plotting is listed after the first BY and the grouping variable 
</p>
<p>after the second BY option.
</p>
<p>For example, if we were interested in plotting a measure of delinquency by 
</p>
<p>sex of respondent in a self-report survey using percentages as the comparison, 
</p>
<p>we would use the following command:
</p>
<p>The resulting graph would have two bars&mdash;one for each sex&mdash;at each level of the 
</p>
<p>delinquency measure.
</p>
<p>Pie Charts
</p>
<p>Pie charts are also very easy to produce in SPSS with the GRAPH command, but 
</p>
<p>changing the type of chart to /PIE=
</p>
<p>Similar to the bar chart options, rather than the number of cases (here the 
</p>
<p>COUNT option), you can also ask for SPSS to use percentages (PCT option in 
</p>
<p>Bar Charts
</p>
<p>To have SPSS produce bar charts, you will need to use the GRAPH command 
</p>
<p>again, but simply change the option to /BAR=
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A60
</p>
<p>GRAPH /BAR = variable_name.
</p>
<p>GRAPH /BAR(GROUPED) = PCT BY variable_name BY group_var_
</p>
<p>name.
</p>
<p>GRAPH /BAR(GROUPED) = COUNT BY variable_name BY  
</p>
<p>group_var_name.
</p>
<p>GRAPH /BAR(GROUPED) = PCT BY delinquency BY sex.
</p>
<p>GRAPH /PIE = COUNT BY variable_name.</p>
<p/>
</div>
<div class="page"><p/>
<p>the command line, replacing COUNT). As with every other chart in SPSS, you 
</p>
<p>can edit various dimensions of the chart after it has been created and presented 
</p>
<p>in the output window.
</p>
<p>Line Graphs
</p>
<p>The fourth type of chart we highlight here is the line graph. Again, the GRAPH 
</p>
<p>command is used and the type of chart is changed to /LINE
</p>
<p>This command will generate a simple line graph that defaults to plotting the 
</p>
<p>number of cases at each value of the variable being plotted&mdash;equivalent to the 
</p>
<p>simple bar chart, but with a line connecting what would have been the top of 
</p>
<p>each bar.
</p>
<p>To produce a time plot, such as that seen in Figure 3.11, we begin with the 
</p>
<p>same GRAPH command:
</p>
<p>The VALUE option tells SPSS to plot the value of the variable included in the 
</p>
<p>parentheses. So, in Figure 3.11, we plotted a crime rate measure&mdash;we would 
</p>
<p>simply include the name of that variable here. After the BY option, we would 
</p>
<p>include the name of the time variable, such as month, quarter, year, etc.
</p>
<p>Figure 3.12 includes two lines that represent two different crime rates. To 
</p>
<p>include additional lines in a time plot in SPSS, we would simply add the addi-
</p>
<p>tional variable name to a list included in the parentheses following the VALUE 
</p>
<p>option:
</p>
<p>NOTE: SPSS will not produce a line graph with two y-axes, such as that found 
</p>
<p>in Figure 3.13.
</p>
<p>Stata
</p>
<p>Frequency Distributions and Histograms
</p>
<p>Frequency distributions are computed in Stata with the tab1 command and have 
</p>
<p>the following format:
</p>
<p>If more than one variable name is listed, multiple frequency distributions will be 
</p>
<p>computed. Note that some of the other options for computing frequency dis-
</p>
<p>tributions in Stata will produce more complicated tables should more than one 
</p>
<p>variable be listed (e.g., the tab command).
</p>
<p>C O M P U T E R E X E R C I S E S 61
</p>
<p>GRAPH /LINE(SIMPLE) = variable_name.
</p>
<p>GRAPH /LINE(SIMPLE) = VALUE(variable_name)  
</p>
<p>BY time_variable.
</p>
<p>GRAPH /LINE(SIMPLE) = VALUE(variable_name1 
</p>
<p>tab1 variable_name(s)
</p>
<p>variable_name2) BY time_variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>Histograms are also easy to produce in Stata with the histogram command:
</p>
<p>Note that you are limited to one variable at a time in running the histogram 
</p>
<p>command.
</p>
<p>Bar Charts
</p>
<p>Although Stata has an impressive set of graphics commands, the ability to cre-
</p>
<p>ate simple bar charts like those discussed in this chapter is limited. Rather than 
</p>
<p>rely on one of the built-in Stata commands, we will use a user-written command 
</p>
<p>called fbar that is available for installation on your local computer running Stata. 
</p>
<p>The command to type and run (one time only) is:
</p>
<p>This command will install fbar from an archive maintained by Stata and will make it 
</p>
<p>available to you while you&rsquo;re running Stata.1 Once the command has been installed, 
</p>
<p>the structure to the fbar command is:
</p>
<p>Where everything appearing after the comma is an option. The default for fbar 
</p>
<p>is to produce a bar chart using frequencies. To have it compute percentages, use 
</p>
<p>the percent option (which can be abbreviated perc). If  there is an interest in 
</p>
<p>comparing bar charts for different groups, then use the by option and include the 
</p>
<p>grouping variable&rsquo;s name in the parentheses and use the option totalpc, which 
</p>
<p>will calculate percentages in the multiple group bar chart using all of  the catego-
</p>
<p>ries within each group as the reference, as we have in this chapter (see Figure 3.8).
</p>
<p>Similar to our discussion of multiple bar charts in SPSS, if we were to 
</p>
<p> compare delinquency counts by sex, we would run the following command:
</p>
<p>Pie Charts
</p>
<p>Pie charts are very easy to produce in Stata using the graph command:
</p>
<p>Where the over(&hellip;) option tells Stata how to classify the information into 
</p>
<p> categories.
</p>
<p>1 Throughout the text, we will make use of  user-written procedures for Stata. The process will 
</p>
<p>be very much the same every time we execute one of  these commands to make a procedure 
</p>
<p>available within Stata. If  you are working on a computer in a student computer lab setting, you 
</p>
<p>will need to install these kinds of  procedures every time you move to a different machine and 
</p>
<p>run Stata.
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A62
</p>
<p>histogram variable_name
</p>
<p>ssc install fbar
</p>
<p>fbar variable_name, by(group_variable_name)  
</p>
<p>percent totalpc
</p>
<p>fbar delinquency, by(sex) totalpc
</p>
<p>graph pie, over(variable_name)</p>
<p/>
</div>
<div class="page"><p/>
<p>Where the y_variable_name and x_variable_name refer to the placement of the 
</p>
<p>variable on the two axes of the plot.
</p>
<p>To produce a time plot, such as that seen in Figure 3.11, we begin with the 
</p>
<p>same twoway command, and just make explicit reference to the x-axis being 
</p>
<p>the indicator of time:
</p>
<p>When we want to add an additional line to the time series plot, we just include 
</p>
<p>another line graph and pair of variable names in parentheses:
</p>
<p>Note that the time variable should be the same, if the comparison is between two 
</p>
<p>(or more) measures taken at the same points in time.
</p>
<p>One of the options available in Stata, but not SPSS, is the ability to add a 
</p>
<p>second y-axis, which as we saw in Figure 3.13 can be useful for comparing trends 
</p>
<p>measured on different scales. The key is to include a reference to the axis within 
</p>
<p>each set of parentheses in the twoway command:
</p>
<p>Where the axis(#) is inserted following a comma that follows the name of the 
</p>
<p>time variable. The Stata do file for Chapter 3 (Chapter_3.do) illustrates how to 
</p>
<p>use this command to reproduce Figures 3.11 through 3.13.
</p>
<p>Problems
</p>
<p> 1. Enter the data from Exercise 3.1.
</p>
<p>a. Produce a frequency distribution table and a histogram.
</p>
<p>b. How does the histogram produced by the statistical software differ 
</p>
<p>from the one you constructed?
</p>
<p>c. Experiment with the interval length to see if  you can reproduce in the 
</p>
<p>statistical software the chart you constructed by hand.
</p>
<p> 2. Enter the data from Exercise 3.3.
</p>
<p>a. Produce two frequency distribution tables and two histograms, one 
</p>
<p>each for males and for females.
</p>
<p>Line Graphs
</p>
<p>The final chart we highlight here is the line graph, many of  which can be 
</p>
<p> produced with the twoway command in Stata, which for simple examples takes 
</p>
<p>on the  following form:
</p>
<p>C O M P U T E R E X E R C I S E S 63
</p>
<p>twoway (line y_variable_name x_variable_name)
</p>
<p>twoway (line y_variable_name time_variable_name)
</p>
<p>twoway (line y_var_name1 time_variable_name)  
</p>
<p>(line y_var_name2 time_variable_name)
</p>
<p>twoway (line y_var_name1 time_variable_name, axis(1)) (line 
</p>
<p>y_var_name2 time_variable_name, axis(2))</p>
<p/>
</div>
<div class="page"><p/>
<p>b. Produce a bar chart that presents the information for males and 
</p>
<p>females simultaneously, in terms of
</p>
<p>i. Number of  cases
</p>
<p>ii. Percentage of  cases.  
</p>
<p> 3. Enter the data from Exercise 3.6. Produce three time plots:
</p>
<p>a. One for City A  
</p>
<p>b. One for City B  
</p>
<p>c. One that contains lines representing both City A and City B.
</p>
<p> 4. Open the NYS data file (nys_1.sav, nys_1_student.sav, or nys_1.dta).
</p>
<p>a. Produce a frequency distribution table and a histogram for each of  the 
</p>
<p>delinquency variables.
</p>
<p>b. Describe the shapes of  the distributions produced in part a.
</p>
<p>c. Choose one of  the delinquency variables:
</p>
<p>i. Produce a bar chart using percentages for each gender. Describe how 
</p>
<p>the patterns are similar and different across gender.
</p>
<p>ii. Produce a bar chart using percentages for each race. Describe how 
</p>
<p>the patterns are similar and different across race.
</p>
<p>C H A P T E R T H R E E :  R E P R E S E N T I N G A N D D I S P L A Y I N G D A T A64
</p>
<p>d. Select two of  the nominal or ordinal variables (excluding gender and 
</p>
<p>race).
</p>
<p>i. Produce pie charts for both variables.
</p>
<p>ii. Produce pie charts for both variables, distinguished by gender.
</p>
<p>iii. Produce pie charts for both variables, distinguished by race.
</p>
<p>iv. Describe how the patterns are similar and different across gender and 
</p>
<p>race.</p>
<p/>
</div>
<div class="page"><p/>
<p>Describing the Typical Case: 
</p>
<p>Measures of Central Tendency
</p>
<p>What Information Does It Use?
</p>
<p>What Information Does It Use?
</p>
<p>C h a p t e r  f o u r
</p>
<p>T h e  m o d e
</p>
<p>T h e  m e d i a n
</p>
<p>T h e  m e a n
</p>
<p>How is It Calculated?
</p>
<p>What are Its Advantages and Disadvantages?
</p>
<p>How is It Calculated?
</p>
<p>What are Its Advantages and Disadvantages?
</p>
<p>How is It Calculated?
</p>
<p>What are Its Advantages and Disadvantages?
</p>
<p>What are Its Other Unique Properties?
</p>
<p>What Information Does It Use?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_4,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>THE NATURAL FIRST STEP in summarizing research is to provide a basic
portrait of the characteristics of a sample or population. What is the typ-
</p>
<p>ical case? If the researcher could choose one case to represent all oth-
</p>
<p>ers, which would it be? When a sample is very small, it is possible
</p>
<p>merely to show the array of cases and let the reader decide. However,
</p>
<p>as the number of cases grows, it becomes difficult to make a decision
</p>
<p>about typicality from the distribution as a whole. This is the function of
</p>
<p>measures of central tendency in statistics. They provide us with a simple
</p>
<p>snapshot of our data that can be used to gain a picture of the average
</p>
<p>case.
</p>
<p>In this chapter, three commonly used measures of central tendency
</p>
<p>are discussed and compared. The first, the mode, is used primarily with
</p>
<p>nominal-level data. It is the simplest measure of central tendency, draw-
</p>
<p>ing information only about the frequency of events in each category. The
</p>
<p>second measure, the median, takes into account not only frequency but
</p>
<p>also the order or ranking of study subjects. Finally, the mean adds the
</p>
<p>additional factor of the exact scores associated with each subject studied.
</p>
<p>As in the discussion of levels of measurement, we emphasize in this
</p>
<p>chapter the benefits gained from statistics that use more information. But
</p>
<p>we also illustrate the importance of looking carefully at the distribution
</p>
<p>of cases in your study before deciding which measure of central ten-
</p>
<p>dency is most appropriate.
</p>
<p>T h e  M o d e :  C e n t r a l  T e n d e n c y  i n  N o m i n a l  S c a l e s
</p>
<p>Faced with a nominal-scale measure, how would you define a typical
</p>
<p>case? Take as an example Table 4.1. Here you have a nominal scale of
</p>
<p>legal representation for a sample of offenders convicted of white-collar
</p>
<p>crimes in U.S. federal courts. Offenders were placed into one of five cat-
</p>
<p>egories, indicating the type of legal representation they had: no attorney
</p>
<p>66</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E M O D E :  C E N T R A L T E N D E N C Y I N N O M I N A L S C A L E S 67
</p>
<p>present, legal-aid attorney, court-appointed attorney, public defender,
</p>
<p>and privately retained attorney. The number of individuals who fall in
</p>
<p>each category&mdash;or, in statistical language, the N of cases&mdash;is reported.
</p>
<p>Clearly, you have very limited information in this example on which
</p>
<p>to base a choice about typicality. Here, as with other nominal-scale mea-
</p>
<p>sures, you simply know how many cases fall into one category or an-
</p>
<p>other. You would probably choose the category &ldquo;private attorney&rdquo; as
</p>
<p>most representative of this sample, because it contains by far the most
</p>
<p>cases (380). And indeed, this is precisely how statisticians define typical-
</p>
<p>ity for nominal-level variables. We call the category with the largest N, or
</p>
<p>number of cases, the mode. In this sample of white-collar offenders, the
</p>
<p>modal category for type of representation is &ldquo;private attorney.&rdquo;
</p>
<p>By defining one category as the modal category, we are able to pro-
</p>
<p>vide a summary of the type of case that is typical of our sample or popu-
</p>
<p>lation. Such statements are common in criminal justice research. We
</p>
<p>often are interested in the racial category that appears most often in our
</p>
<p>data or the type of offense that is most common. The modal category
</p>
<p>can also provide a basis for making comparisons among samples. For
</p>
<p>example, let&rsquo;s say that a sample of offenders convicted of nonviolent
</p>
<p>property crimes that would not ordinarily be defined as white collar was
</p>
<p>compared to this larger sample of offenders convicted of white-collar
</p>
<p>crimes. For the former group, as is apparent from Table 4.2, the modal
</p>
<p>category is not &ldquo;private attorney&rdquo; but rather &ldquo;court-appointed attorney.&rdquo;
</p>
<p>Although this comparison of the two samples is not a complex one, it
</p>
<p>Legal Representation for White-Collar Crime
</p>
<p>CATEGORY FREQUENCY (N)
</p>
<p>No Attorney 20
Legal Aid 26
Court Appointed 92
Public Defender 153
Private Attorney 380
Total (�) 671
</p>
<p>Table 4.1
</p>
<p>Legal Representation for Common Crime
</p>
<p>CATEGORY FREQUENCY (N)
</p>
<p>No Attorney 40
Legal Aid 7
Court Appointed 91
Public Defender 22
Private Attorney 70
Total (�) 230
</p>
<p>Table 4.2</p>
<p/>
</div>
<div class="page"><p/>
<p>68 C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>In general, we do not use the mode to describe central tendency with
</p>
<p>ordinal or interval scales. The reason, in good part, is that the mode does
</p>
<p>not take advantage of the additional information that such scales pro-
</p>
<p>vide. The average case should not be chosen simply on the basis of the
</p>
<p>frequency of events in a particular category, because higher-level scales
</p>
<p>also provide information on the order or nature of the differences be-
</p>
<p>tween categories.
</p>
<p>Nonetheless, there are cases where researchers choose to use the
</p>
<p>mode to describe ordinal- or interval-level measures. Generally this oc-
</p>
<p>curs when there is a very large group of cases in one particular category.
</p>
<p>Table 4.3, for example, provides an ordinal-level measure of the financial
</p>
<p>harm caused by a sample of convicted offenders. Because almost two-
</p>
<p>thirds of the individuals studied fall in the category &ldquo;$101&ndash;$2,500,&rdquo; you
</p>
<p>might want to describe typicality in this case by saying that this category
</p>
<p>is the modal category. Similarly, if you were examining prior arrests and
</p>
<p>two-thirds of the offenders in your sample had no prior arrests, you
</p>
<p>might want to report no arrests as the modal category. Even though this
</p>
<p>measure is an interval measure, the mode provides a fairly good sum-
</p>
<p>mary of the typical case in your sample.
</p>
<p>T h e  M e d i a n :  T a k i n g  i n t o  A c c o u n t  P o s i t i o n
</p>
<p>In constructing the median, we utilize information not only on the num-
</p>
<p>ber of cases found in a particular category, but also on the positions of
</p>
<p>the categories. The median may be defined simply as the middle score in
</p>
<p>a distribution. For ordinal scales, it is the category in which the middle
</p>
<p>score lies. For interval scales, the median is the value that splits the dis-
</p>
<p>tribution of scores in half.
</p>
<p>There are two general steps in determining the median for a distribu-
</p>
<p>tion of scores. First, the values need to be arranged from low to high
</p>
<p>Financial Harm for a Sample of Convicted Offenders
</p>
<p>CATEGORY FREQUENCY (N)
</p>
<p>Less than $100 15
$101&ndash;$2,500 92
$2,501&ndash;$10,000 20
More than $10,000 19
Total (�) 146
</p>
<p>Table 4.3
</p>
<p>illustrates the different backgrounds of the two groups. White-collar
</p>
<p>offenders are much more likely than common criminals to have the
</p>
<p>resources to pay for private legal representation.</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E M E D I A N :  T A K I N G I N T O A C C O U N T P O S I T I O N 69
</p>
<p>scores. As we saw in Chapter 3, a frequency distribution allows us to
</p>
<p>represent our data in this way. Table 4.4 presents a frequency distribu-
</p>
<p>tion of views of public drunkenness, drawn from a survey of students.
</p>
<p>The students were presented with an ordinal-scale measure that allowed
</p>
<p>them to rate the seriousness of a series of crimes. The ratings ranged
</p>
<p>from &ldquo;not serious at all&rdquo; to &ldquo;most serious.&rdquo;
</p>
<p>Second, we need to determine which observation splits the distribu-
</p>
<p>tion. A simple formula, Equation 4.1, allows us to define which observa-
</p>
<p>tion is the median when the number of observations in the distribution is
</p>
<p>odd, as is the case with our example of views of public drunkenness.
</p>
<p>Equation 4.1
</p>
<p>In this case, we add 1 to the total number of observations in the sample
</p>
<p>or population we are studying and then divide by 2. For the frequency
</p>
<p>distribution in Table 4.4, the median observation is the 141st score:
</p>
<p>Median observation � 
N � 1
</p>
<p>2
</p>
<p>Student Views on Public Drunkenness
</p>
<p>CATEGORY FREQUENCY (N) CUMULATIVE N
</p>
<p>Not serious at all 73 73
A bit serious 47 120
Somewhat serious 47 167
Serious 27 194
Very serious 26 220
Extremely serious 39 259
Most serious 22 281
Total (�) 281
</p>
<p>Table 4.4
</p>
<p>W orking It Out
</p>
<p> � 141
</p>
<p> � 
281 � 1
</p>
<p>2
</p>
<p> Median observation � 
N � 1
</p>
<p>2
</p>
<p>However, because our variable, student views on drunkenness, is mea-
</p>
<p>sured on an ordinal scale, it does not make sense to simply state that the
</p>
<p>141st observation is the median score. To give a substantive meaning to
</p>
<p>the median, it is important to define which category the median score</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>falls in. The 141st observation in our distribution of ordered scores falls
</p>
<p>in the category labeled &ldquo;somewhat serious.&rdquo;
</p>
<p>The advantage of the median over the mode for describing ordinal
</p>
<p>scales is well illustrated by our example of views of public drunkenness.
</p>
<p>If we used the mode to describe typicality in student assessments of the
</p>
<p>seriousness of public drunkenness, we would conclude that the typical
</p>
<p>student did not see drunkenness as at all serious. But even though the
</p>
<p>&ldquo;not serious at all&rdquo; category includes the largest number of cases, almost
</p>
<p>three-quarters of the students rate this behavior more seriously. The me-
</p>
<p>dian takes this fact into consideration by placing the typical case in the
</p>
<p>middle of a distribution. It is concerned with not only the number of
</p>
<p>cases in the categories, but also their position.
</p>
<p>If the number of observations or cases in your distribution is even, then
</p>
<p>you cannot identify a single observation as the median. While statisticians
</p>
<p>recognize that the median is ambiguously defined in this case, by conven-
</p>
<p>tion they continue to use Equation 4.1 to identify the median for an ordi-
</p>
<p>nal-level distribution. In practice, this places the median score between two
</p>
<p>observations. For example, consider the distribution of 146 scores in Table
</p>
<p>4.3, representing financial harm in a sample of offenders. Here the number
</p>
<p>1
</p>
<p>W orking It Out
</p>
<p>73rd observation: $101&ndash;$2,500
</p>
<p>74th observation: $101&ndash;$2,500
</p>
<p> � 73.5
</p>
<p> � 
146 � 1
</p>
<p>2
</p>
<p> Median observation � 
N � 1
</p>
<p>2
</p>
<p>1With this method, it is possible that the defined median value will fall between two
</p>
<p>categories of an ordinally measured variable. In that case, you simply note that the
</p>
<p>median falls between these two categories.
</p>
<p>The median is sometimes used for defining typicality with interval
</p>
<p>scales. For example, Table 4.5 presents the average number of minutes
</p>
<p>of public disorder (per 70-minute period) observed in a sample of 31
</p>
<p>of scores is even, and thus there is not a single observation that can be
</p>
<p>defined as the median. Using Equation 4.1, we can see that the median is
</p>
<p>defined as the halfway point between the 73rd and the 74th observation.
</p>
<p>This means that the median falls in the category defined as $101 to $2,500.
</p>
<p>70</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E M E D I A N :  T A K I N G I N T O A C C O U N T P O S I T I O N
</p>
<p>&ldquo;hot spots of crime,&rdquo; or city blocks with high levels of crime. The hot
</p>
<p>spots are arranged in ascending order on the basis of the number of
</p>
<p>minutes of disorder observed. In this case, the distribution has an odd
</p>
<p>number of observations, and thus the median is the score in the middle
</p>
<p>of the distribution, or the 16th observation, which has a value of 2.12.
</p>
<p>Hot Spots: Minutes of Public Disorder (A)
</p>
<p>HOT SPOT 
</p>
<p>SCORE FREQUENCY (N) CUMULATIVE (N)
</p>
<p>0.35 1 1
0.42 1 2
0.46 1 3
0.47 1 4
0.52 1 5
0.67 1 6
1.00 1 7
1.06 1 8
1.15 1 9
1.19 2 11
1.48 1 12
1.60 1 13
1.63 1 14
2.02 1 15
2.12 1 16
2.21 1 17
2.34 1 18
2.45 1 19
2.66 1 20
3.04 1 21
3.19 1 22
3.23 1 23
3.46 1 24
3.51 1 25
3.72 1 26
4.09 1 27
4.47 1 28
4.64 1 29
4.65 1 30
6.57 1 31
Total (�) 31 31
</p>
<p>Table 4.5
</p>
<p>W orking It Out
</p>
<p> � 16
</p>
<p> � 
31 � 1
</p>
<p>2
</p>
<p> Median observation � 
N � 1
</p>
<p>2
</p>
<p>71</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>Accordingly, using the median, we would describe the average hot spot as
</p>
<p>having a little more than two minutes of disorder in each 70-minute period.
</p>
<p>As noted above, when the number of observations in a distribution is
</p>
<p>even, the median is ambiguously defined. Let&rsquo;s, for example, delete the hot
</p>
<p>spot with a score of 6.57 from Table 4.5. In this case, there is no single mid-
</p>
<p>dle value for the array of cases in the table. If we use Equation 4.1 to define
</p>
<p>the median observation, we get a value of 15.5.
</p>
<p>2Sometimes the median for ordinal-level variables is also calculated using this method.
</p>
<p>In such cases, the researcher should realize that he or she is treating the variable
</p>
<p>under consideration as an interval-level measure. Only for an interval-level measure
</p>
<p>can we assume that the units of measurement are constant across observations.
</p>
<p>W orking It Out
</p>
<p> � 15.5
</p>
<p> � 
30 � 1
</p>
<p>2
</p>
<p> Median observation � 
N � 1
</p>
<p>2
</p>
<p>But what is the value or score associated with an observation that lies
</p>
<p>between two scores in an interval-level scale? If both the 15th and the
</p>
<p>16th observation are in the same category, then the solution is easy. You
</p>
<p>simply define the median as the score associated with both the 15th and
</p>
<p>the 16th observation. However, it will sometimes be the case with an
</p>
<p>interval-level variable that each of these observations will have a differ-
</p>
<p>ent value on the scale, as we find here. There is no true median value
</p>
<p>for this example. By convention, however, we define the median with
</p>
<p>interval-level measures as the midpoint between the observation directly
</p>
<p>below and the observation directly above the median observation. In our
</p>
<p>example, this is the midpoint on our scale between the scores 2.02 and
</p>
<p>2.12. The median in this case is defined as 2.07.2
</p>
<p>W orking It Out
</p>
<p> � 2.07
</p>
<p> Median � 
2.02 � 2.12
</p>
<p>2
</p>
<p> 16th case � 2.12
</p>
<p> 15th case � 2.02
</p>
<p>72</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E M E D I A N :  T A K I N G I N T O A C C O U N T P O S I T I O N
</p>
<p>The median is generally more appropriate than the mode for assess-
</p>
<p>ing central tendency for both ordinal- and interval-level measures. How-
</p>
<p>ever, the median does not take advantage of all the information included
</p>
<p>in interval-level scales. Although it recognizes the positions of the values
</p>
<p>of a measure, it does not take into account the exact differences among
</p>
<p>these values. In many cases, this can provide for a misleading estimate of
</p>
<p>typicality for interval-level measures.
</p>
<p>For example, let&rsquo;s say that the distribution of disorder in hot spots is
</p>
<p>that represented in Table 4.6. In this case, the median is 1.83. But is 1.83
</p>
<p>a good estimate of central tendency for this distribution? The 17th score
</p>
<p>is 3.34, which is not very close to 1.83 at all. The score of 1.83 is not an
</p>
<p>the values of the 16th and 17th cases. This is because it looks only at
</p>
<p>Hot Spots: Minutes of Public Disorder (B)
</p>
<p>HOT SPOT 
</p>
<p>SCORE FREQUENCY (N) CUMULATIVE (N) CUMULATIVE %
</p>
<p>0.35 1 1 3.2
0.42 1 2 6.5
0.46 1 3 9.7
0.47 1 4 12.9
0.52 1 5 16.1
0.67 1 6 19.4
1.00 1 7 22.6
1.06 1 8 25.8
1.15 1 9 29.0
1.19 2 11 35.5
1.48 1 12 38.7
1.60 1 13 41.9
1.63 1 14 45.2
1.73 1 15 48.4
1.83 1 16 51.6
3.34 1 17 54.9
3.44 1 18 58.1
3.45 1 19 61.3
3.66 1 20 64.5
4.04 1 21 67.7
4.19 1 22 71.0
4.23 1 23 74.2
4.46 1 24 77.4
4.51 1 25 80.6
4.72 1 26 83.9
5.09 1 27 87.1
5.47 1 28 90.3
5.64 1 29 93.5
5.65 1 30 96.8
5.57 1 31 100.0
Total (�) 31 31 100.0
</p>
<p>Table 4.6
</p>
<p>bution. The median is not sensitive to the gap in our measure between
</p>
<p>ideal estimate of typicality, as it is far below half the scores in the distri-
</p>
<p>73</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>position and not at the size of the differences between cases. The me-
</p>
<p>dian does not take advantage of all the information provided by interval-
</p>
<p>level measures.
</p>
<p>Another way to describe the median in interval-level measures is to
</p>
<p>say that it is the 50th percentile score. A percentile score is the point or
</p>
<p>score below which a specific proportion of the cases is found. The 50th
</p>
<p>percentile score is the score below which 50% of the cases in a study lie.
</p>
<p>For the data in Table 4.6, if we defined the median in this way, we again
</p>
<p>choose 1.83 as the median minutes of disorder observed in the hot spots.
</p>
<p>In this case, if we add the percentage of cases for all of the scores up
</p>
<p>until the middle, or 16th, score, we come to a total (or cumulative per-
</p>
<p>centage) of 51.6. At the 15th score, or 1.73, the cumulative percentage is
</p>
<p>only 48.4, less than 50%.
</p>
<p>T h e  M e a n :  A d d i n g  V a l u e  t o  P o s i t i o n
</p>
<p>The mean takes into account not only the frequency of cases in a cate-
</p>
<p>gory and the positions of scores on a measure, but also the values of
</p>
<p>these scores. To calculate the mean, we add up the scores for all of the
</p>
<p>subjects in our study and then divide the total by the total number of
</p>
<p>subjects. In mathematical language, the mean can be written as a short
</p>
<p>equation:
</p>
<p>Equation 4.2
</p>
<p>Even though equations sometimes put students off, they are an im-
</p>
<p>portant part of statistics. Indeed, equations are the language of statistics.
</p>
<p>They show how a statistic is constructed and the method we use to cal-
</p>
<p>culate it. Equations provide a short way of writing out what would often
</p>
<p>take a number of sentences to describe in English. One of our tasks in
</p>
<p>this text is to help you to translate such equations and to become more
</p>
<p>comfortable with them.
</p>
<p>In the case of the mean, we introduce what are for most students of
</p>
<p>criminal justice some new symbols and concepts. First, to express the
</p>
<p>mean, statisticians provide us with a shorthand symbol, &mdash;in English,
</p>
<p>&ldquo;X bar.&rdquo; The equation also includes the summation symbol, �. Under the
</p>
<p>symbol is i � 1, and above it is N. What this means is that you should
</p>
<p>start summing your cases with the first subject in the sample and end
</p>
<p>X
</p>
<p>X � 
�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N
</p>
<p>74</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E M E A N :  A D D I N G V A L U E T O P O S I T I O N
</p>
<p>with the last one (represented by N because, as we have already dis-
</p>
<p>cussed, N is the number of cases in your sample). But what should you
</p>
<p>sum? X represents the measure of interest&mdash;in the case of our example,
</p>
<p>minutes of disorder. We use the subscript i to denote each of the obser-
</p>
<p>vations of the variable X. If, for example, we wrote X3, we would be re-
</p>
<p>ferring only to the 3rd observation of the variable. So Equation 4.2 says
</p>
<p>that you should sum the scores for minutes of disorder from the first to
</p>
<p>the last case in your study. Then you should divide this number by the
</p>
<p>total number of cases.
</p>
<p>Table 4.7 presents information about the total number of prior arrests
</p>
<p>for a sample of 20 individuals arrested for felony offenses. To calculate
</p>
<p>the mean, we first sum all of the scores, as shown in the numerator of
</p>
<p>Equation 4.2:
</p>
<p>Total Number of Prior Arrests
</p>
<p>TOTAL NUMBER 
</p>
<p>OF ARRESTS FREQUENCY (N) CUMULATIVE (N)
</p>
<p>0 4 4
1 1 5
2 2 7
4 3 10
5 3 13
7 4 17
8 2 19
</p>
<p>10 1 20
</p>
<p>Table 4.7
</p>
<p>W orking It Out
</p>
<p> � 86
</p>
<p> � 5 � 5 � 5 � 7 � 7 � 7 � 7 � 8 � 8 � 10
</p>
<p> � 0 � 0 � 0 � 0 � 1 � 2 � 2 � 4 � 4 � 4 
</p>
<p> �
N
</p>
<p>i�1
</p>
<p> Xi � �
20
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>We then take the sum of the values, 86, and divide by the number of ob-
</p>
<p>servations in the sample.
</p>
<p>75</p>
<p/>
</div>
<div class="page"><p/>
<p>76 C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>The result, 4.3, tells us that in this sample the typical person arrested for
</p>
<p>a felony has, on average, 4.3 prior arrests.
</p>
<p>As another example, let&rsquo;s take the data from Table 4.5 on minutes of
</p>
<p>disorder in crime hot spots. According to Equation 4.2, the first step is to
</p>
<p>sum all of the scores:
</p>
<p>W orking It Out
</p>
<p> � 4.3
</p>
<p> � 
86
20
</p>
<p> X � 
�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N
</p>
<p>W orking It Out
</p>
<p> � 71.56
</p>
<p> � 3.51 � 3.72 � 4.09 � 4.47 � 4.64 � 4.65 � 6.57
</p>
<p> � 2.21 � 2.34 � 2.45 � 2.66 � 3.04 � 3.19 � 3.23 � 3.46
</p>
<p> � 1.15 � 1.19 � 1.19 � 1.48 � 1.60 � 1.63 � 2.02 � 2.12
</p>
<p> � 0.35 � 0.42 � 0.46 � 0.47 � 0.52 � 0.67 � 1.00 � 1.06 
</p>
<p> �
N
</p>
<p>i�1
</p>
<p> Xi � �
31
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>We then take this number, 71.56, and divide it by N, or 31, the number
</p>
<p>of cases in our sample.
</p>
<p>W orking It Out
</p>
<p> � 2.308387097
</p>
<p> � 
71.56
</p>
<p>31
</p>
<p> X � 
�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E M E A N :  A D D I N G V A L U E T O P O S I T I O N 77
</p>
<p>The result, 2.308387097 (rounded to the ninth decimal place), brings up
</p>
<p>an issue that often arises in reporting statistics. Do you really need to
</p>
<p>provide your audience with the level of precision that is given by your
</p>
<p>statistic? In this case, for example, at what level of precision should min-
</p>
<p>utes of disorder be presented?
</p>
<p>A basic rule of thumb is to use your common sense in answering such
</p>
<p>questions. Don&rsquo;t provide statistics developed out to a large number of
</p>
<p>decimal places just to impress others. In making this decision, you
</p>
<p>should ask: What is the simplest presentation of my results that will pro-
</p>
<p>vide the reader or listener with enough information to understand and
</p>
<p>evaluate my work? Overall, criminal justice researchers seldom report the
</p>
<p>mean to more than two decimal places. This is a good choice in our ex-
</p>
<p>ample. Rounding to the second decimal place gives a mean of 2.31. Pro-
</p>
<p>viding a more precise representation of the mean here would not add
</p>
<p>important information for the reader.
</p>
<p>In some cases, it is useful to develop estimates with much greater pre-
</p>
<p>cision. In particular, if the values for the cases you are examining are
</p>
<p>very small in the first place, you will want to present a more precise
</p>
<p>mean. For example, Lawrence Sherman and his colleagues looked at the
</p>
<p>mean daily rate of reported domestic violence in a study that compared
</p>
<p>the impact of arrests versus warnings as a strategy for controlling spouse
</p>
<p>abusers.3 Had they reported their findings only to the second decimal
</p>
<p>place, as recommended above, they would have ended up with a mean
</p>
<p>daily rate over the longest follow-up period (361&ndash;540 days) of 0.00 for
</p>
<p>short arrest and 0.00 for warning. The difficulty here is that individuals
</p>
<p>are unlikely to report cases of domestic violence on a very frequent
</p>
<p>basis. Sherman et al. needed a much higher degree of precision to exam-
</p>
<p>ine the differences between the two groups they studied. Accordingly,
</p>
<p>they reported their results to the fourth decimal place. For arrests, the
</p>
<p>rate was 0.0019, and for warnings it was 0.0009. These differences,
</p>
<p>though small, were found to be meaningful in their research.
</p>
<p>Comparing Results Gained Using the Mean and Median
</p>
<p>Returning to the example from Table 4.5, we see that the mean for min-
</p>
<p>utes of disorder, 2.31, is very similar to the median of 2.12 calculated
</p>
<p>earlier. In this case, adding knowledge about value does not change our
</p>
<p>portrait of the typical hot spot very much. However, we get a very differ-
</p>
<p>ent sense of the average case if we use the data from Table 4.6. Here,
</p>
<p>the median provided a less than satisfying representation of the average
</p>
<p>3L. Sherman, J. D. Schmidt, D. Rogan, P. Gartin, E. G. Cohn, D. J. Collins, and A. R.
</p>
<p>Bacich, &ldquo;From Initial Deterrence to Long-Term Escalation: Short-Custody Arrest for
</p>
<p>Poverty Ghetto Domestic Violence,&rdquo; Criminology 29 (1991): 821&ndash;850.</p>
<p/>
</div>
<div class="page"><p/>
<p>78 C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>case. It was not sensitive to the fact that there was a large gap in the
</p>
<p>scores between the 16th and 17th cases. Accordingly, the median, 1.83,
</p>
<p>was very close in value to the first half of the cases in the sample, but
</p>
<p>very far from those hot spots with higher values. The mean should pro-
</p>
<p>vide a better estimate of typicality here, because it recognizes the actual
</p>
<p>values of the categories and not just their positions. Let&rsquo;s see what hap-
</p>
<p>pens when we calculate the mean for Table 4.6.
</p>
<p>Following our equation, we first sum the individual cases:
</p>
<p>W orking It Out
</p>
<p> � 84.41
</p>
<p> � 4.51 � 4.72 � 5.09 � 5.47 � 5.64 � 5.65 � 5.77
</p>
<p> � 3.34 � 3.44 � 3.45 � 3.66 � 4.04 � 4.19 � 4.23 � 4.46
</p>
<p> � 1.15 � 1.19 � 1.19 � 1.48 � 1.60 � 1.63 � 1.73 � 1.83
</p>
<p> � 0.35 � 0.42 � 0.46 � 0.47 � 0.52 � 0.67 � 1.00 � 1.06
</p>
<p> �
N
</p>
<p>i�1
</p>
<p> Xi � �
31
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>We then divide this number by the total number of cases:
</p>
<p>W orking It Out
</p>
<p> � 2.7229
</p>
<p> � 
84.41
</p>
<p>31
</p>
<p> X � 
�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N
</p>
<p>Here, we gain an estimate of typicality of 2.72 (rounding to the second
</p>
<p>decimal place). As you can see, this score is much better centered in
</p>
<p>our distribution than is the median. The reason is simple. The median
</p>
<p>does not take into account the values of the categories. The mean does
</p>
<p>take value into account and thus is able to adjust for the gap in the
</p>
<p>distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E M E A N :  A D D I N G V A L U E T O P O S I T I O N 79
</p>
<p>There are cases in which the sensitivity of the mean to the values of
</p>
<p>the categories in a measure can give misleading results. For example,
</p>
<p>let&rsquo;s say that one case in your study is very different from the others. As
</p>
<p>noted in Chapter 1, researchers call such a case an outlier, because it is
</p>
<p>very much outside the range of the other cases you studied. Taking the
</p>
<p>example of minutes of disorder from Table 4.5, let&rsquo;s say that the last case
</p>
<p>had 70 minutes of disorder (the maximum amount possible) rather than
</p>
<p>6.57 minutes. When we calculate the mean now, the sum of the cases is
</p>
<p>much larger than before:
</p>
<p>W orking It Out
</p>
<p> � 134.99
</p>
<p> � 3.51 � 3.72 � 4.09 � 4.47 � 4.64 � 4.65 � 70.0
</p>
<p> � 2.21 � 2.34 � 2.45 � 2.66 � 3.04 � 3.19 � 3.23 � 3.46
</p>
<p> � 1.15 � 1.19 � 1.19 � 1.48 � 1.60 � 1.63 � 2.02 � 2.12
</p>
<p> � 0.35 � 0.42 � 0.46 � 0.47 � 0.52 � 0.67 � 1.00 � 1.06
</p>
<p> �
N
</p>
<p>i�1
</p>
<p> Xi � �
31
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>Dividing this sum by the total number of cases provides us with a mean
</p>
<p>of 4.35 (rounded to the second decimal place):
</p>
<p>W orking It Out
</p>
<p> � 4.3545
</p>
<p> � 
134.99
</p>
<p>31
</p>
<p> X � 
�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N
</p>
<p>The mean we calculated with the original score was 2.31 (see page 76).
</p>
<p>Accordingly, merely by changing one score to an outlier, we have almost
</p>
<p>doubled our estimate of typicality. In this case, the sensitivity of the
</p>
<p>mean to an extreme value in the distribution led it to overestimate the</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>average case. This illustrates the general principle that the mean is sensi-
</p>
<p>tive to outliers. Because the mean is used to develop many other more
</p>
<p>complex statistics, this principle is relevant not only to the mean itself
</p>
<p>but also to a number of other important statistical techniques used by re-
</p>
<p>searchers.
</p>
<p>So what should you do if outliers lead to a misleading conclusion re-
</p>
<p>garding typicality in your study? One solution is simply to exclude the
</p>
<p>outliers from specific analyses and let your readers or audience know
</p>
<p>that some cases have been excluded and why. If the number of extreme
</p>
<p>cases is large enough, you may want to analyze these cases separately.
</p>
<p>Other Characteristics of the Mean
</p>
<p>Two other traits of the mean are important because they play a role in
</p>
<p>how we develop other statistics. The first concerns what happens when
</p>
<p>we look at deviations (or differences) from the mean. This will be-
</p>
<p>come an issue in the next chapter, when we discuss measures of disper-
</p>
<p>sion. The second, often termed the least squares property of the
</p>
<p>mean, will become important to us in Chapter 15, when we discuss re-
</p>
<p>gression.
</p>
<p>If we take each score in a distribution, subtract the mean from it, and
</p>
<p>sum these differences, we will always get a result of 0. In equation form,
</p>
<p>this principle is represented as follows:
</p>
<p>Equation 4.3
</p>
<p>In English, this equation says that if we sum the deviations from the
</p>
<p>mean, from the first to the last case, we will always get a result of 0. This
</p>
<p>principle is illustrated in Table 4.8, using the data on minutes of public
</p>
<p>disorder from Table 4.5. Here we have taken the 31 scores and sub-
</p>
<p>tracted the mean from each one. We then added these differences. Be-
</p>
<p>cause the positive scores balance out the negative ones, the result is 0.
</p>
<p>This will always happen when we use the mean.
</p>
<p>The second trait, the least squares property, is very important for un-
</p>
<p>derstanding regression analysis (introduced in Chapter 15), a technique
</p>
<p>commonly used for describing relationships among variables in criminal
</p>
<p>justice. For the moment, it is enough to note this fact and that the issues
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X ) � 0
</p>
<p>Another solution is to transform the outliers. That is, you may want to
</p>
<p>highest value that is not an outlier). In this way, you can include the
</p>
<p>replace them with values closer to the rest of the distribution (e.g., the
</p>
<p>cases, but minimize the extent to which they affect your estimate of
</p>
<p>typicality. However, you should be cautious in developing such trans-
</p>
<p>formations of your scores, keeping in mind that you are changing the
</p>
<p>character of the distribution examined in your study.
</p>
<p>80</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E M E A N :  A D D I N G V A L U E T O P O S I T I O N
</p>
<p>we address early on in statistics are often the bases for much more com-
</p>
<p>plex types of analysis. &ldquo;Don&rsquo;t forget the basics&rdquo; is a good rule. Many mis-
</p>
<p>takes that researchers make in developing more complex statistics come
</p>
<p>from a failure to think about the basic issues raised in the first few chap-
</p>
<p>ters of this text.
</p>
<p>The least squares property is written in equation form as follows:
</p>
<p>Equation 4.4
</p>
<p>What this says in English is that if we sum the squared deviations from
</p>
<p>the mean for all of our cases, we will get the minimum possible result.
</p>
<p>That is, suppose we take each individual&rsquo;s score on a measure, subtract
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2 � minimum
</p>
<p>Deviations from the Mean for Minutes of Public Disorder (A)
</p>
<p>SCORE (X) DEVIATION FROM THE MEAN (Xi � )
</p>
<p>0.35 0.35 � 2.31 � �1.96
0.42 0.42 � 2.31 � �1.89
0.46 0.46 � 2.31 � �1.85
0.47 0.47 � 2.31 � �1.84
0.52 0.52 � 2.31 � �1.79
0.67 0.67 � 2.31 � �1.64
1.00 1.00 � 2.31 � �1.31
1.06 1.06 � 2.31 � �1.25
1.15 1.15 � 2.31 � �1.16
1.19 1.19 � 2.31 � �1.12
1.19 1.19 � 2.31 � �1.12
1.48 1.48 � 2.31 � �0.83
1.60 1.60 � 2.31 � �0.71
1.63 1.63 � 2.31 � �0.68
2.02 2.02 � 2.31 � �0.29
2.12 2.12 � 2.31 � �0.19
2.21 2.21 � 2.31 � �0.10
2.34 2.34 � 2.31 � �0.03
2.45 2.45 � 2.31 � �0.14
2.66 2.66 � 2.31 � �0.35
3.04 3.04 � 2.31 � �0.73
3.19 3.19 � 2.31 � �0.88
3.23 3.23 � 2.31 � �0.92
3.46 3.46 � 2.31 � �1.15
3.51 3.51 � 2.31 � �1.20
3.72 3.72 � 2.31 � �1.41
4.09 4.09 � 2.31 � �1.78
4.47 4.47 � 2.31 � �2.16
4.64 4.64 � 2.31 � �2.33
4.65 4.65 � 2.31 � �2.34
6.57 6.57 � 2.31 � �4.26
Total (�) 0*
</p>
<p>*Because of rounding error, the actual column total is slightly less than zero.
</p>
<p>X
</p>
<p>Table 4.8
</p>
<p>81</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>the mean from that score, and then square the difference. If we then
</p>
<p>sum all of these values, the result we get will be smaller than the result
</p>
<p>we would have gotten if we had subtracted any other number besides
</p>
<p>the mean. You might try this by calculating the result for minutes of dis-
</p>
<p>order using the mean. Then try other values and see if you can find
</p>
<p>some other number of minutes that will give you a smaller result. The
</p>
<p>least squares property says you won&rsquo;t.
</p>
<p>Using the Mean for Noninterval Scales
</p>
<p>The mean is ordinarily used for measuring central tendency only with in-
</p>
<p>terval scales. However, in practice, researchers sometimes use the mean
</p>
<p>with ordinal scales as well. Is this wrong? In a pure statistical sense, it is.
</p>
<p>However, some ordinal scales have a large number of categories and
</p>
<p>thus begin to mimic some of the characteristics of interval-level mea-
</p>
<p>sures.
</p>
<p>This is particularly true in cases where the movements from one cate-
</p>
<p>gory to another in an ordinal scale can be looked at as equivalent, no
</p>
<p>matter which category you move from. Taking our example of student
</p>
<p>attitudes toward public drunkenness in Table 4.4, a researcher might
</p>
<p>argue that the difference between &ldquo;somewhat serious&rdquo; and &ldquo;a bit serious&rdquo;
</p>
<p>is about equivalent to that between &ldquo;very serious&rdquo; and &ldquo;extremely seri-
</p>
<p>ous,&rdquo; and so forth. Thus, the difference between these categories is not
</p>
<p>just a difference of position; it is also a movement of equal units up the
</p>
<p>scale. Taking this approach, we can say that this measure takes into ac-
</p>
<p>count both position and value, although the values here are not as
</p>
<p>straightforward as those gained from true interval scales such as number
</p>
<p>of crimes or dollar amount stolen.
</p>
<p>A researcher might argue that the mean is appropriate for presenting
</p>
<p>findings on views of public drunkenness because this ordinal-scale mea-
</p>
<p>sure of attitudes is like an interval-scale measure. Although it is easy to
</p>
<p>see the logic behind this decision, it is important to note that such a de-
</p>
<p>cision takes a good deal of justification. In general, you should be very
</p>
<p>cautious about using the mean for ordinal-level scales, even when the
</p>
<p>above criteria are met.
</p>
<p>S t a t i s t i c s  i n  P r a c t i c e :  
C o m p a r i n g  t h e  M e d i a n  a n d  t h e  M e a n
</p>
<p>The general rule is that the mean provides the best measure of central
</p>
<p>tendency for an interval scale. This follows a principle stated in Chap-
</p>
<p>ter 1: In statistics, as in other decision-making areas, more information 
</p>
<p>is better than less information. When we use the mean, we take into 
</p>
<p>82</p>
<p/>
</div>
<div class="page"><p/>
<p>S T A T I S T I C S I N P R A C T I C E
</p>
<p>tion, but also the values or scores of those categories. Because more in-
</p>
<p>formation is used, the mean is less likely than other measures of central
</p>
<p>tendency to be affected by changes in the nature of the sample that a re-
</p>
<p>searcher examines. It is useful to note as well that the mean has some al-
</p>
<p>gebraic characteristics that make it more easily used in developing other
</p>
<p>types of statistics.
</p>
<p>The mean is generally to be preferred, but when the distribution of a
</p>
<p>variable is strongly skewed, the median provides a better estimate of
</p>
<p>central tendency than the mean. &ldquo;Skewed&rdquo; means that the scores on the
</p>
<p>cases. A distribution that has extreme values lower than the main cluster
</p>
<p>of observations (i.e., there is a &ldquo;tail&rdquo; to the left in the distribution) is said
</p>
<p>to be negatively skewed, while a distribution that has extreme values
</p>
<p>greater than the main cluster of observations (i.e., there is a &ldquo;tail&rdquo; to the
</p>
<p>right in the distribution) is said to be positively skewed.4
</p>
<p>A good example of a skewed distribution in criminal justice is crimi-
</p>
<p>nal history as measured by self-reports of prisoners. Horney and Mar-
</p>
<p>shall, for example, reported results on the frequency of offending for a
</p>
<p>sample of prisoners.5 As is apparent from Figure 4.1, most of the offend-
</p>
<p>ers in their sample had a relatively low offending rate&mdash;between 1 and
</p>
<p>20 offenses in the previous year. But a number of offenders had rates of
</p>
<p>more than 100, and a fairly large group had more than 200. The mean
</p>
<p>for this distribution is 175.
</p>
<p>Clearly, 175 offenses provides a misleading view of typical rates of of-
</p>
<p>fending for their sample. Because the mean is sensitive to value, it is in-
</p>
<p>flated by the very high frequency scores of a relatively small proportion
</p>
<p>of the sample. One solution suggested earlier to the problem of outliers
</p>
<p>4A formal statistic for measuring the degree of skewness of a distribution is given by
</p>
<p>the following equation:
</p>
<p>In words, this equation tells us to take the deviation between a value and the mean
</p>
<p>and cube it, then sum these values over all observations; the sum of the cubed devia-
</p>
<p>tions is then divided by the sample size (N) multiplied by the standard deviation
</p>
<p>cubed. The measure of skewness will have a value of 0 if the distribution is symmetri-
</p>
<p>cal, a negative value if the distribution is negatively skewed, and a positive value if
</p>
<p>the distribution is positively skewed. The greater the value of the measure, the greater
</p>
<p>the degree of positive or negative skewness.
5J. Horney and I. H. Marshall, &ldquo;An Experimental Comparison of Two Self-Report Meth-
</p>
<p>ods for Measuring Lambda,&rdquo; Journal of Research in Crime and Delinquency 29 (1992):
</p>
<p>102&ndash;121.
</p>
<p>skewness �
(Xi � X )
</p>
<p>3
</p>
<p>Ns3
</p>
<p>account not only the frequency of events in each category and their posi-
</p>
<p>variable are very much weighted to one side and that frequencies of 
</p>
<p>extreme values trail off in one  direction away from the main cluster of
</p>
<p>83
</p>
<p>�
N
</p>
<p>i�1</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>was to exclude such cases. But here, this would mean excluding almost
</p>
<p>30% of the sample. Thus, these are not outliers in the traditional sense.
</p>
<p>Another option mentioned earlier is to analyze the &ldquo;outliers&rdquo; separately.
</p>
<p>But again, there is quite a spread of scores even if we look at those
</p>
<p>above 50 or 100 separately, and the analysis of the outliers might in itself
</p>
<p>provide a misleading view of central tendency. A common solution used
</p>
<p>for describing this type of skewed interval-level distribution is to use the
</p>
<p>median rather than the mean to describe central tendency. The median
</p>
<p>for this distribution is 4, which is certainly more representative of the av-
</p>
<p>erage case than is the mean. But even if you choose this solution, it is
</p>
<p>very important to note to your audience that the distribution is skewed
</p>
<p>and to tell them a bit about the nature of the distribution.
</p>
<p>How should you decide when a distribution is so skewed that it is
</p>
<p>preferable to use the median as opposed to the mean? You should begin
</p>
<p>by comparing the mean and the median. When there is a very large dif-
</p>
<p>ference between them, it may be the result of skewness. In such cases,
</p>
<p>you should look at the distribution of the scores to see what is causing
</p>
<p>the mean and median to differ widely. But there is no solid boundary
</p>
<p>line to guide your choice.
</p>
<p>cases where the mean and median provide relatively close estimates,
</p>
<p>P
er
</p>
<p>ce
n
</p>
<p>t 
of
</p>
<p> R
es
</p>
<p>p
on
</p>
<p>d
en
</p>
<p>ts
</p>
<p>Frequency of Offending
</p>
<p>1&ndash;20
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>21&ndash;40 41&ndash;60 61&ndash;80 81&ndash;100 101&ndash;120 121&ndash;140 141&ndash;160 161&ndash;180181&ndash;200 200+
</p>
<p>Individual Frequency of Offending for a Sample of Offenders: 
</p>
<p>A Case Where the Mean Is a Misleading Measure of Central Tendency
Figure 4.1
</p>
<p>your choice will be clear. In the former case you would choose the 
</p>
<p>84
</p>
<p>In extreme cases (such as that of criminal history in our example) or</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S U M M A R Y
</p>
<p>where in between, you will have to use common sense and the experi-
</p>
<p>ences of other researchers working with similar data as guidelines. What
</p>
<p>seems to make sense? What have other researchers chosen to do? One
</p>
<p>way of being fair to your audience is to provide results for both the
</p>
<p>mean and the median, irrespective of which you choose as the best mea-
</p>
<p>sure of typicality.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>The mode is calculated by identifying the category that contains the
</p>
<p>greatest number of cases. It may be applied to any scale of measure-
</p>
<p>ment. Because the mode uses very little information, it is rarely used
</p>
<p>with scales of measurement higher than the nominal scale. It can occa-
</p>
<p>sionally serve as a useful summary tool for higher-level scales, however,
</p>
<p>when a large number of cases are concentrated in one particular cate-
</p>
<p>gory.
</p>
<p>The median is calculated by locating the middle score in a distribu-
</p>
<p>tion and identifying in which category it falls. It is also known as the
</p>
<p>50th percentile score, or the score below which 50% of the cases lie. The
</p>
<p>information used includes both the number of cases in a particular cate-
</p>
<p>gory and the positions of the categories. The median uses more informa-
</p>
<p>tion than does the mode and requires a scale of measurement that is at
</p>
<p>least ordinal in magnitude.
</p>
<p>The mean is calculated by dividing the sum of the scores by the
</p>
<p>number of cases. The information used includes not only the number of
</p>
<p>cases in a category and the relative positions of the categories, but also
</p>
<p>the actual value of each category. Such information normally requires at
</p>
<p>least an interval scale of measurement. For this reason, the researcher
</p>
<p>should be cautious about using the mean to describe an ordinal scale.
</p>
<p>The mean uses more information than the mode and the median. It is,
</p>
<p>however, sensitive to extreme cases&mdash;outliers. Faced with the distorting
</p>
<p>effect of outliers, the researcher may choose to keep them, to transform
</p>
<p>them to other values, or to delete them altogether. If a distribution of
</p>
<p>scores is substantially skewed, then it may be more appropriate to use
</p>
<p>the median than to use the mean. 
</p>
<p>The sum derived by adding each score&rsquo;s deviation from the mean
</p>
<p>will always be 0. If the deviation of each score from the mean is
</p>
<p>squared, then the sum of these squares will be less than it would be if
</p>
<p>any number other than the mean were used. This is called the least
</p>
<p>squares property.
</p>
<p>median, and in the latter the mean. However, when your results fall some-
</p>
<p>85</p>
<p/>
</div>
<div class="page"><p/>
<p>86 C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>K e y  T e r m s
</p>
<p>deviation from the mean The extent to
</p>
<p>which each individual score differs from
</p>
<p>the mean of all the scores.
</p>
<p>least squares property A characteristic 
</p>
<p>of the mean whereby the sum of all the
</p>
<p>squared deviations from the mean is a
</p>
<p>minimum&mdash;it is lower than the sum of the
</p>
<p>squared deviations from any other fixed
</p>
<p>point.
</p>
<p>mean A measure of central tendency cal-
</p>
<p>culated by dividing the sum of the scores
</p>
<p>by the number of cases.
</p>
<p>median A measure of central tendency cal-
</p>
<p>culated by identifying the value or category
</p>
<p>of the score that occupies the middle posi-
</p>
<p>tion in the distribution of scores.
</p>
<p>mode A measure of central tendency cal-
</p>
<p>culated by identifying the score or category
</p>
<p>that occurs most frequently.
</p>
<p>outlier(s) A single or small number of ex-
</p>
<p>ceptional cases that substantially deviate
</p>
<p>from the general pattern of scores.
</p>
<p>skewed Describing a spread of scores that
</p>
<p>is clearly weighted to one side.
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>X Individual score
</p>
<p>Mean
</p>
<p>N Number of cases
</p>
<p>� Sum
</p>
<p>To calculate the median observation:
</p>
<p>To calculate the mean:
</p>
<p>To show how the sum of the deviations from the mean equals 0:
</p>
<p>To express the least squares property:
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2 � minimum
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X) � 0
</p>
<p>X � 
�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N
</p>
<p>Median observation � 
N � 1
</p>
<p>2
</p>
<p>X</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 87
</p>
<p>E x e r c i s e s
</p>
<p>4.1 Drivers cited for moving violations are required by a state&rsquo;s laws to
take a driving safety course taught by the local police department. The
sign-in sheet asks individuals to note why they received a ticket. The
14 participants at a recent class noted the following:
</p>
<p>Speeding, Running a red light, Running a stop sign, Speeding, Speeding,
Running a red light, Tailgating, Speeding, Running a red light,
Recklessness, Speeding, Running a red light, Speeding, Running 
a stop sign
</p>
<p>a. Categorize these data and calculate the mode.
</p>
<p>b. Explain why the median would not be an appropriate measure of
central tendency for these data.
</p>
<p>4.2 Calculate the mode, median, and mean for the following data:
</p>
<p>a. Number of previous employments held by 25 convicts:
</p>
<p>3 3 1 1 0 1 0 2 1 0 8 4 3
</p>
<p>1 2 1 9 0 1 7 0 7 2 0 1
</p>
<p>b. Weeks of training undergone by 20 prison guards:
</p>
<p>10 16 12 16 16 16 10 8 10 12
</p>
<p>16 18 12 16 16 8 0 12 10 16
</p>
<p>c. Height (in meters) of 30 convicts:
</p>
<p>1.72 1.78 1.73 1.70 1.81 1.64 1.76 1.72 1.75 1.74
</p>
<p>1.88 1.79 2.01 1.80 1.77 1.79 1.69 1.74 1.75 1.66
</p>
<p>1.77 1.73 1.72 1.91 1.80 1.74 1.72 1.82 1.86 1.79
</p>
<p>4.3 A researcher checked the response times of police to ten emergency
telephone calls. The data below record the number of minutes that
elapsed from when the telephone call ended to when the police
arrived:
</p>
<p>24 26 14 27 198 22 27 17 19 29
</p>
<p>a. Calculate the mode, the median, and the mean.
</p>
<p>b. Which of these measures is the most suitable for this particular
case? Explain your choice.
</p>
<p>4.4 Airport officials wished to check the alertness of their security officers
over the two busiest weeks of the summer. During this period, they
sent out 50 undercover staff carrying suspicious items of hand lug-
gage. Five of them were stopped at the entrance to the airport. Six
made it into the airport, but were stopped at check-in. Thirteen more
got into the airport and through check-in, only to be stopped at the</p>
<p/>
</div>
<div class="page"><p/>
<p>88 C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>hand-luggage inspection point. Two passed the airport entrance,
check-in, and hand-luggage inspection, but were stopped when
presenting their boarding cards at the gate. Four people made it 
past every one of these stages, only to be stopped when boarding 
the plane. Twenty of the undercover staff were not detected 
at all.
</p>
<p>a. Categorize the data and calculate the median category.
</p>
<p>b. Is the median a good measure of central tendency in this case? Ex-
plain your answer. If you think it is not, suggest an alternative and
explain why.
</p>
<p>4.5 On the first day of the term in a statistics course, the professor admin-
istered a brief questionnaire to the students, asking how many statis-
tics courses they had ever taken before the current term. Of the 33
students who answered the question, 17 said none, 9 said one, 3 said
two, 2 said three, 1 said four, and 1 said five.
</p>
<p>a. Calculate the mode, median, and mean for number of prior statis-
tics classes.
</p>
<p>b. Which one of these measures of central tendency best measures the
typicality of these data?
</p>
<p>4.6 As part of her undergraduate thesis, a criminal justice student asked
ten other criminal justice majors to rate the fairness of the criminal jus-
tice system. The students were asked to say whether they strongly
agreed, agreed, were uncertain, disagreed, or strongly disagreed with
the following statement: &ldquo;The criminal justice system in our country
treats all defendants fairly.&rdquo; The ten responses were
</p>
<p>Strongly agree, Strongly agree, Strongly disagree, Strongly disagree,
Uncertain, Disagree, Disagree, Agree, Strongly disagree, Uncertain
</p>
<p>a. Categorize these data and calculate an appropriate measure of cen-
tral tendency.
</p>
<p>b. Explain why this measure of central tendency best represents the
typicality of these data.
</p>
<p>4.7 There are five prisoners in the high-security wing in a prison&mdash;Albert,
Harry, Charlie, Dave, and Eddie. Only Eddie&rsquo;s biographical details
have been lost. The information available is as follows:
</p>
<p>Age Previous Convictions
</p>
<p>Albert 23 1
</p>
<p>Harry 28 4
</p>
<p>Charlie 18 1
</p>
<p>Dave 41 1
</p>
<p>Eddie ? ?</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 89
</p>
<p>a. Can we compute any of the following for previous convictions of
the five prisoners: the mode, the median, or the mean? If any (or
all) of these three measures may be calculated, what are their
values?
</p>
<p>b. If the mean number of previous convictions is 2.0, how many con-
victions does Eddie have?
</p>
<p>c. If we know that the median age for the five prisoners is 28, what
does this tell us about Eddie&rsquo;s age? Explain why.
</p>
<p>d. If the mean age for the five prisoners is 28.2, how old is Eddie?
</p>
<p>4.8 A researcher sat on a bench along the main shopping street of a city
center on ten successive Saturdays from 11:00 A.M. to 2:00 P.M.&mdash;the
three busiest shopping hours of the day&mdash;and recorded the number of
times a police officer passed by. The results for the ten weeks are as
follows:
</p>
<p>Week No.: 1 2 3 4 5 6 7 8 9 10
</p>
<p>No. of Police Officers Observed: 4 3 6 4 4 5 4 35 3 5
</p>
<p>On week 8, local unionists held a demonstration in the city center,
and the high number of observations for that week can be explained
by the extra officers called in to police the rally.
</p>
<p>b. Which measure of central tendency best represents typicality for
these data? Discuss the issues involved in choosing the most appro-
priate means of describing the data.
</p>
<p>c. Imagine that the unionists had decided to hold a regular demon-
stration in the city center on alternating weeks. The results
recorded for the same study would be as follows:
</p>
<p>Week No.: 1 2 3 4 5 6 7 8 9 10
</p>
<p>No. of Police Officers Observed: 4 30 6 31 6 52 4 35 4 34
</p>
<p>4.9 On a recent evening, a police crackdown on prostitution solicitation
resulted in 19 arrests. The ages of the persons arrested were
</p>
<p>17 18 24 37 32 49 61 20 21 21
</p>
<p>25 24 24 26 30 33 35 22 19
</p>
<p>a. Calculate an appropriate measure of central tendency.
</p>
<p>b. Explain why this measure of central tendency best represents typi-
cality for these data.
</p>
<p>officers observed.
a. Calculate the mode, median, and mean for the number of police 
</p>
<p>Would the measure of central tendency you recommended in part b
still be the best measure of typicality? Explain why.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E
</p>
<p>4.10 Using your answers from part a of Exercise 4.5, calculate
</p>
<p>a. The sum of the deviations from the mean.
</p>
<p>b. The sum of the squared deviations from the mean.
</p>
<p>c. The sum of the squared deviations from the median.
</p>
<p>d. The sum of the squared deviations from the mode.
</p>
<p>e. Which of these sums of squared deviations has the smallest value?
</p>
<p>90
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>Measures of central tendency are reported along with many other statistics in 
</p>
<p>many of the software programs you might encounter. The commands that we 
</p>
<p>describe below will be the same as those that we highlight at the end of  
</p>
<p>Chapter 5.
</p>
<p>SPSS
</p>
<p>There are two primary ways of obtaining measures of central tendency in SPSS. 
</p>
<p>The quickest way to obtain information on the mean for one or more variables 
</p>
<p>measured at the interval level of measurement is to use the DESCRIPTIVES 
</p>
<p>command
</p>
<p>The output window will contain quite a bit of information for each variable 
</p>
<p>that you have named&mdash;much of it will make more sense after reading Chapter 
</p>
<p>5. It is important to note that in regard to measures of central tendency, the 
</p>
<p>DESCRIPTIVES command will report only the mean, not the mode or the 
</p>
<p>median. Since the only measure of central tendency this command will calculate 
</p>
<p>is the mean, this command is generally useful only for interval-level data. This 
</p>
<p>command is most useful when you are working with a data set that contains 
</p>
<p>almost exclusively interval-level variables.
</p>
<p>An alternative to obtaining measures of central tendency is to again use the 
</p>
<p>FREQUENCIES command discussed at the end of Chapter 3. As you may recall 
</p>
<p>from the previous chapter&rsquo;s computer exercises, this command will produce 
</p>
<p>frequency distributions for the variables whose names are included in the list of 
</p>
<p>variables. To obtain measures of central tendency for the variables of interest, we 
</p>
<p>simply need to add an option requesting these values:
</p>
<p>DESCRIPTIVES VARIABLES = variable_names.
</p>
<p>FREQUENCIES VARIABLES = variable_names
</p>
<p>/FORMAT = NOTABLE
</p>
<p>/STATISTICS = MEAN MEDIAN MODE.</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 91
</p>
<p>Where the /STATISTICS = option lists the three measures of central tendency 
</p>
<p>we are interested in. The /FORMAT = NOTABLE option suppresses the print-
</p>
<p>ing of the frequency distributions for all the variables included in the list. Should 
</p>
<p>you want the frequency distributions, just omit this line. Also, the separation of 
</p>
<p>the command across three lines is simply for ease of reading&mdash;all of this material 
</p>
<p>can be included on a single line in a syntax file. The formatting on this page of 
</p>
<p>the text would have made it difficult to read what was being done within SPSS.
</p>
<p>After running this command, the output window will contain a box labeled 
</p>
<p>&ldquo;Statistics.&rdquo; Each column of this table refers to a separate variable. As you move 
</p>
<p>down the rows, you should see the reported values for the mode, the median, 
</p>
<p>and the mean for each variable.
</p>
<p>Caution: The mode and the median are listed as numbers, even though the data 
</p>
<p>may be nominal or ordinal and you have entered value labels. To report correctly 
</p>
<p>the value of the mode or median, you need to report the category represented 
</p>
<p>by that number. For example, suppose you had analyzed the variable labeled 
</p>
<p>&ldquo;gender,&rdquo; where males were coded as 1 and females as 2, the mode would be 
</p>
<p>reported by SPSS as either 1 or 2, but it would be up to you to report correctly 
</p>
<p>whether the modal category was male or female&ndash;not a 1 or a 2.
</p>
<p>Specific examples of the use of each of these commands are provided in the 
</p>
<p>accompanying SPSS syntax file for Chapter 4 (Chapter_4.sps).
</p>
<p>Stata
</p>
<p>In Stata, there are also two primary ways of obtaining median and mean&mdash;both 
</p>
<p>methods are fairly straightforward. The summarize command is
</p>
<p>The basic output will include the mean and other measures on the variables 
</p>
<p>included in the list. To obtain the median, you will need to add the detail option:
</p>
<p>You should note that the output will label the median as the 50th percentile.
</p>
<p>Alternatively, if we want to avoid looking at a variety of other statistics that 
</p>
<p>may not be of interest, we can use the tabstat command and explicitly ask for 
</p>
<p>only the median and the mean:
</p>
<p>The output will list the variables named across column and the median and the 
</p>
<p>mean will appear in separate rows of the table.
</p>
<p>The mode is not reported in any of the standard Stata output, but is easily 
</p>
<p>determined by running the tab1 command to obtain a frequency distribution 
</p>
<p>that was described in the Computer Exercises at the end of Chapter 3.
</p>
<p>Specific examples of the use of each of these commands are provided in the 
</p>
<p>accompanying Stata do file for Chapter 4 (Chapter_4.do).
</p>
<p>summarize variable_names
</p>
<p>summarize variable_names, detail
</p>
<p>tabstat variable_names, statistics ( median mean)</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F O U R :  D E S C R I B I N G T H E T Y P I C A L C A S E92
</p>
<p>Recoding Variables
</p>
<p>A common situation in statistical analysis is the need to recode the values for 
</p>
<p>some variable included in our data file. There are many reasons for why we may 
</p>
<p>need to recode, such as collapsing categories to simplify the categories of a vari-
</p>
<p>able or defining some values as &ldquo;missing&rdquo; or inappropriate for our analysis.
</p>
<p>The recode commands in SPSS and Stata are quite similar. In SPSS, 
</p>
<p>RECODE takes the following form:
</p>
<p>The structure of RECODE will have a series of old and new values listed in 
</p>
<p>parentheses and can refer to specific values, ranges of values as well as missing 
</p>
<p>values. For the values that are not going to be changed, the (ELSE = COPY) 
</p>
<p>ensures that they are copied to the new variable. We would also like to empha-
</p>
<p>size the importance of using new variable names to contain the recoded  
</p>
<p>values&mdash;it is good practice and helps to protect the data file that you are  working 
</p>
<p>with. More than one researcher has made the mistake of sending the recodes 
</p>
<p>back into the original variable, realizing a mistake was made, and just damaged 
</p>
<p>the data file that was being used and the need to start over.
</p>
<p>The EXECUTE command following the RECODE command is necessary to 
</p>
<p>force SPSS to perform the recodes now, rather than waiting for a call to another 
</p>
<p>procedure and performing the recodes at that time.
</p>
<p>In Stata, the recode command takes the form
</p>
<p>The recoding of old to new values occurs in however many parentheses are 
</p>
<p>required, just as in SPSS. There is no need to refer to the other values in the  
</p>
<p>original variable&mdash;they are automatically carried over to the new variable. 
</p>
<p>Of special use is the option of coding one or more values as missing. For 
</p>
<p>example, a person responding to a survey writes down an incorrect number that 
</p>
<p>is beyond the range of acceptable responses. In SPSS, system missing values are 
</p>
<p>noted by SYSMIS in the RECODE command. In Stata, a period (.) denotes a 
</p>
<p>missing value.
</p>
<p>Problems
</p>
<p> 1. Open the NYS data file (nys_1.sav, nys_1_student.sav, or nys_1.dta). 
</p>
<p>Consider the level of  measurement for each variable, and then compute 
</p>
<p>and report appropriate measures of  central tendency for each variable.
</p>
<p>RECODE variable_name (old = new)(old = new) (ELSE = Copy) INTO 
</p>
<p>new_variable_name.
</p>
<p>EXECUTE.
</p>
<p>recode variable_name (old = new), gen(new_variable_name)
</p>
<p>Following all of  the recodes in parentheses, the creation of  a new variable is noted by 
</p>
<p>adding a comma to the command and then the gen(new_variable_name)  
</p>
<p>command to generate a new variable that contains the recoded values.</p>
<p/>
</div>
<div class="page"><p/>
<p>93
</p>
<p> 2. Do any of  the variables included in the data file appear to have potential 
</p>
<p>outliers? (You may want to consult your histograms from the Chapter 3 
</p>
<p>computer exercises or create histograms now for the interval-level vari-
</p>
<p>ables included in the data file.)
</p>
<p> 3. If  you find one or more potential outliers, take the following steps to 
</p>
<p> investigate their effect on the measures of  central tendency.
</p>
<p>a. Use the recode command to create two new variables: one that recodes 
</p>
<p>the outliers as &ldquo;System Missing&rdquo; values and one that recodes the 
</p>
<p> outliers as the next smaller value.
</p>
<p>Hints:
</p>
<p>b. Using your variable that recodes the outliers as missing, report how 
</p>
<p>the values of  the mean and the median change when potential outliers 
</p>
<p>are removed from the analysis.
</p>
<p>c. Using your variable that recodes the outliers as the next smaller value, 
</p>
<p> report how the values of  the mean and the median change when 
</p>
<p> potential outliers are made less extreme.
</p>
<p>d. Which approach for handling potential outliers do you think is more 
</p>
<p>appropriate for analyzing these data? Explain why. Faced with potential 
</p>
<p>outliers, which measure of  central tendency would you report?
</p>
<p>C O M P U T E R E X E R C I S E S
</p>
<p>&#149; You will need to look at frequency distributions to determine the 
</p>
<p>maximum and next to maximum values.
</p>
<p>&#149; Keep in mind that SPSS will look for SYSMIS in the RECODE  
</p>
<p>command, while Stata will look for a period (.) in the recode command.</p>
<p/>
</div>
<div class="page"><p/>
<p>How Typical Is the Typical Case?:
</p>
<p>Measuring Dispersion
</p>
<p>What Do They Tell Us About Our Data?
</p>
<p>a n d  o r d i n a l  s c a l e s :  p r o p o r t i o n s ,  
</p>
<p>p e r c e n t a g e s ,  a n d  t h e  v a r i a t i o n  r a t i o
</p>
<p>r a n g e ,  v a r i a n c e ,  a n d  s t a n d a r d  d e v i a t i o n
</p>
<p>C h a p t e r  f i v e
</p>
<p>M e a s u r e s  o f  d i s p e r s i o n
</p>
<p>M e a s u r i n g  d i s p e r s i o n  i n  n o m i n a l  
</p>
<p>M e a s u r i n g  d i s p e r s i o n  i n  i n t e r v a l  s c a l e s :  
</p>
<p>How are They Calculated?
</p>
<p>What are Their Characteristics?
</p>
<p>How are They Calculated?
</p>
<p>What are Their Characteristics?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>MEASURES OF CENTRAL TENDENCY provide a snapshot of the typical
case; however, the same statistic may be obtained from samples or pop-
</p>
<p>ulations that are in fact quite dissimilar. For example, a sample of police
</p>
<p>recruits with a mean or median age of 23 is not likely to include people
</p>
<p>younger than 18 or older than 30, because most police departments have
</p>
<p>age requirements for incoming officers. A sample of offenders with a
</p>
<p>mean or median age of 23, however, will include offenders younger than
</p>
<p>18 and much older than 30. In both these samples, the average person
</p>
<p>studied is 23 years old. But the sample of offenders will include more
</p>
<p>younger and older people than the sample of police recruits. The ages of
</p>
<p>the offenders are dispersed more widely around the average age.
</p>
<p>Measures of dispersion allow us to fill a gap in our description of the
</p>
<p>samples or populations we study. They ask the question: How typical is
</p>
<p>the typical case? They tell us to what extent the subjects we studied are
</p>
<p>similar to the case we have chosen to represent them. Are most cases
</p>
<p>clustered closely around the average case? Or, as with the sample of of-
</p>
<p>fenders above, is there a good deal of dispersion of cases both above
</p>
<p>and below the average?
</p>
<p>M e a s u r e s  o f  D i s p e r s i o n  
f o r  N o m i n a l -  a n d  O r d i n a l - L e v e l  D a t a
</p>
<p>With nominal scales, we define the typical case as the category with
</p>
<p>the largest number of subjects. Accordingly, in Chapter 4 we chose
</p>
<p>&ldquo;private attorney&rdquo; as the modal category for legal representation for a
</p>
<p>sample of white-collar offenders. But how would we describe to what
</p>
<p>extent the use of a private attorney is typical of the sample as a whole?
</p>
<p>Put another way, to what degree are the cases concentrated in the
</p>
<p>modal category?
</p>
<p>95</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>The Proportion in the Modal Category
</p>
<p>The most straightforward way to answer this question is to describe the
</p>
<p>proportion of cases that fall in the modal category. Recall from Chapter 3
</p>
<p>that a proportion is represented by the following equation:
</p>
<p>Accordingly, we can represent the proportion of cases in the modal cate-
</p>
<p>gory using Equation 5.1:
</p>
<p>Equation 5.1
</p>
<p>That is, we take the number of cases in the modal category and divide it
</p>
<p>by the total number of cases in the sample.
</p>
<p>Taking the example of legal representation, we divide the N of cases
</p>
<p>in the modal category (private attorney) by the total N of cases in the
</p>
<p>sample (see Table 5.1):
</p>
<p>Proportion � 
Nmodal cat.
</p>
<p>Ntotal
</p>
<p>Proportion � 
Ncat
Ntotal
</p>
<p>Legal Representation for White-Collar Crime
</p>
<p>CATEGORY FREQUENCY (N )
</p>
<p>No Attorney 20
Legal Aid 26
Court Appointed 92
Public Defender 153
Private Attorney 380
Total (�) 671
</p>
<p>Table 5.1
</p>
<p>W orking It Out
</p>
<p> � 0.5663
</p>
<p> � 
380
</p>
<p>671
</p>
<p> Proportion � 
Nmodal cat.
</p>
<p>Ntotal
</p>
<p>Following our earlier suggestions regarding rounding to the second deci-
</p>
<p>mal place, we say that the proportion of white-collar offenders in the
</p>
<p>modal category was about 0.57.
</p>
<p>96</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F D I S P E R S I O N F O R N O M I N A L A N D O R D I N A L D A T A
</p>
<p>Table 5.2 presents information about the method of execution used
</p>
<p>on the 683 persons executed in the United States from 1977 to 2000. The
</p>
<p>modal category is lethal injection, so the proportion in the modal cate-
</p>
<p>gory is found by dividing the N of cases in that category by the total N of
</p>
<p>cases:
</p>
<p>Method of Execution in the United States, 1977&ndash;2000
</p>
<p>CATEGORY FREQUENCY (N )
</p>
<p>Lethal Injection 518
Electrocution 149
Lethal Gas 11
Hanging 3
Firing Squad 2
Total (�) 683
</p>
<p>Source: Tracy L. Snell, &ldquo;Capital Punishment 2000,&rdquo;
Bureau of Justice Statistics Bulletin, 2001, p. 12.
</p>
<p>Table 5.2
</p>
<p>W orking It Out
</p>
<p> � 0.7584
</p>
<p> � 
518
</p>
<p>683
</p>
<p> Proportion � 
Nmodal cat.
</p>
<p>Ntotal
</p>
<p>Of persons executed in the United States from 1977 to 2000, the propor-
</p>
<p>tion killed through lethal injection was about 0.76.
</p>
<p>The Percentage in the Modal Category
</p>
<p>Alternatively, we may refer to the percentage in the modal category.
</p>
<p>Most people find percentages easier to understand than proportions. Re-
</p>
<p>call that a percentage is obtained by taking a proportion and multiplying
</p>
<p>it by 100. Accordingly, we can take Equation 5.1 and multiply the result
</p>
<p>by 100 to get the percentage of cases in the modal category.
</p>
<p>Percentage � 
Nmodal cat.
</p>
<p>Ntotal
 � 100
</p>
<p>97</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>For our legal representation example,
</p>
<p>W orking It Out
</p>
<p> � 56.6319
</p>
<p> � 
380
</p>
<p>671
 � 100
</p>
<p> Percentage � 
Nmodal cat.
</p>
<p>Ntotal
 � 100
</p>
<p>That is, about 57% of the cases in the sample fall in the modal category.
</p>
<p>Similarly, for the method of execution example, the percentage in the
</p>
<p>modal category is
</p>
<p>The Variation Ratio
</p>
<p>Another way to describe the degree to which the modal category repre-
</p>
<p>sents the cases in a sample is to use a statistic called the variation ratio
</p>
<p>(VR). The variation ratio is based on the same logic as a proportion, but
</p>
<p>it examines the extent to which the cases are spread outside the modal
</p>
<p>category, rather than concentrated within it. The proportion of cases in
</p>
<p>the modal category is subtracted from 1:
</p>
<p>Equation 5.2
</p>
<p>For the legal representation example, the variation ratio is
</p>
<p>VR � 1 � �Nmodal cat.Ntotal �
</p>
<p> � 75.8419
</p>
<p> Percentage � 
518
</p>
<p>683
 � 100
</p>
<p>W orking It Out
</p>
<p> � 0.4337
</p>
<p> � 1 � �380671�
 VR � 1 � �Nmodal cat.Ntotal �
</p>
<p>involved the use of lethal injection.
</p>
<p>About 76% of all executions in the United States from 1977 to 2000 
</p>
<p>98</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F D I S P E R S I O N F O R N O M I N A L A N D O R D I N A L D A T A 99
</p>
<p>The variation ratio for legal representation in this sample of white-collar
</p>
<p>offenders is about 0.43. But what does this say about the extent to
</p>
<p>which cases in the sample are clustered around the typical case? Is a
</p>
<p>variation ratio of 0.43 large or small? What rule can we use for deciding
</p>
<p>more generally whether the distribution we are examining is strongly
</p>
<p>clustered?
</p>
<p>One approach is to define at the outset the upper and lower limits for
</p>
<p>the variation ratio or proportion for a particular measure. Obviously, the
</p>
<p>largest proportion, regardless of the study, is 1.0, which would mean that
</p>
<p>all of the cases were in the modal category. Having all of the cases in
</p>
<p>the modal category would lead to a variation ratio of 0, indicating no
</p>
<p>dispersion.
</p>
<p>The smallest proportion (or largest VR) depends, however, on the
</p>
<p>number of categories in your measure. The mode is defined as the cate-
</p>
<p>gory in your measure with the most cases, so it must have at least one
</p>
<p>more case than any other category. If you have only two categories, then
</p>
<p>the modal category must include one more than half of the cases in your
</p>
<p>study. So, in the instance of two categories, the least possible concentra-
</p>
<p>tion is just over 0.50 of the cases. The least possible dispersion, as mea-
</p>
<p>sured by the variation ratio, would be 1 minus this proportion, or just
</p>
<p>under 0.50. If you have four categories, the modal category must have
</p>
<p>more than one-quarter of the cases. Accordingly, the smallest variation
</p>
<p>ratio would be a bit smaller than 0.75.
</p>
<p>What about our example of legal representation? We have five cate-
</p>
<p>gories and 671 cases. The smallest number of cases the modal cate-
</p>
<p>gory could have with these numbers is 135. In this instance, each of
</p>
<p>the other four categories would have 134 cases. This is the maximum
</p>
<p>amount of dispersion that could exist in this sample, and it amounts to
</p>
<p>about 20.12% of the total number of cases in the sample, or a variation
</p>
<p>ratio of 0.7988. As noted earlier, the greatest degree of concentration
</p>
<p>in the modal category would yield a proportion of 1 and a variation
</p>
<p>ratio of 0. The estimates we calculated for legal representation
</p>
<p>(proportion � 0.57; VR � 0.43) lie somewhere between these two
</p>
<p>extremes.
</p>
<p>Is this dispersion large or small? As with many of the statistics we will
</p>
<p>examine, the answer depends on the context in which you are working.
</p>
<p>&ldquo;Large&rdquo; or &ldquo;small&rdquo; describes a value, not a statistical concept. Statistically,
</p>
<p>you know that your estimate falls somewhere between the largest possi-
</p>
<p>ble degree of concentration and the largest possible degree of disper-
</p>
<p>sion. But whether this is important or meaningful depends on the prob-
</p>
<p>lem you are examining and the results that others have obtained in prior
</p>
<p>research.
</p>
<p>For example, if, in a study of legal representation for white-collar
</p>
<p>crime in England, it had been found that 90% of the cases were concen-
</p>
<p>trated in the private attorney category, then we might conclude that our</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>results reflected a relatively high degree of dispersion of legal represen-
</p>
<p>tation in the United States. If, in England, only 25% of the cases had
</p>
<p>been in the modal category, we might conclude that there was a rela-
</p>
<p>tively low degree of dispersion of legal representation in the United
</p>
<p>States.
</p>
<p>The proportion and the variation ratio are useful primarily for describ-
</p>
<p>ing dispersion with nominal-level measures. In some circumstances,
</p>
<p>however, they can be useful for describing ordinal-level variables as
</p>
<p>well. This is true primarily when there are just a few categories in a mea-
</p>
<p>sure or when there is a very high degree of concentration of cases in
</p>
<p>one category. The problem in using a simple proportion or variation
</p>
<p>ratio for ordinal-level measures is that the mode, upon which these sta-
</p>
<p>tistics are based, is often a misleading measure for ordinal scales. As dis-
</p>
<p>cussed in Chapter 4, the mode does not take into account the positions
</p>
<p>of scores in a measure, and thus it may provide a misleading view of the
</p>
<p>average case.
</p>
<p>Index of Qualitative Variation
</p>
<p>One measure of dispersion that is not based on the mode&mdash;and that
</p>
<p>can be used for both nominal and ordinal scales&mdash;is the index of
</p>
<p>qualitative variation (IQV). The IQV compares the amount of varia-
</p>
<p>tion observed in a sample to the total amount of variation possible,
</p>
<p>given the number of cases and categories in a study. It is a standard-
</p>
<p>ized measure. This means that whatever the number of cases or cate-
</p>
<p>gories, the IQV can vary only between 0 and 100. An IQV of 0 means
</p>
<p>that there is no variation in the measure, or all of the cases lie in one
</p>
<p>category. An IQV of 100 means that the cases are evenly dispersed
</p>
<p>across the categories.
</p>
<p>Equation 5.3
</p>
<p>Equation 5.3 provides a guide for how to compute the IQV. You are
</p>
<p>already familiar with the summation symbols within the parentheses.
</p>
<p>Here we are summing not across cases, but across products of distinct
</p>
<p>categories. Nobs represents the number of cases we observe within a cate-
</p>
<p>gory in our study. Nexp represents the number of cases we would expect
</p>
<p>in a category if the measure were distributed equally across the cate-
</p>
<p>gories. That is, it is the N we would expect if there were the maximum
</p>
<p>amount of dispersion of our cases. We use the subscripts i, j, and k as a
</p>
<p>IQV � ��
k�1
</p>
<p>i�1
</p>
<p> �
k
</p>
<p>j�i�1
</p>
<p> Nobsi Nobsj
</p>
<p>�
k�1
</p>
<p>i�1
</p>
<p> �
k
</p>
<p>j�i�1
 Nexpi Nexpj � � 100
</p>
<p>100</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F D I S P E R S I O N F O R N O M I N A L A N D O R D I N A L D A T A
</p>
<p>shorthand way to say that we should multiply all of the potential pairs of
</p>
<p>categories. Here&rsquo;s how this works: k represents the total number of cate-
</p>
<p>gories of a variable. In the legal representation example, k � 5. Sub-
</p>
<p>scripts i and j index the categories of the variable. Use of the subscripts i
</p>
<p>and j provides us with a way of keeping track and making sure that we
</p>
<p>have multiplied all possible pairs of observed frequencies from each of
</p>
<p>the categories.
</p>
<p>For example, if a variable had three categories, then the numerator
</p>
<p>(the measure of observed variation) would be equal to
</p>
<p>If a variable had four categories, then the numerator would be equal to
</p>
<p>A concrete example will make it much easier to develop this statistic
</p>
<p>in practice. Let&rsquo;s say that we wanted to describe dispersion of an ordinal-
</p>
<p>scale measure of fear of crime in a college class of 20 students. The stu-
</p>
<p>dents were asked whether they were personally concerned about crime
</p>
<p>on campus. The potential responses were &ldquo;very concerned,&rdquo; &ldquo;quite con-
</p>
<p>cerned,&rdquo; &ldquo;a little concerned,&rdquo; and &ldquo;not concerned at all.&rdquo; The responses
</p>
<p>of the students are reported under the &ldquo;N observed&rdquo; column in Table 5.3.
</p>
<p>As you can see, the cases are fairly spread out, although there are more
</p>
<p>students in the &ldquo;very concerned&rdquo; and &ldquo;quite concerned&rdquo; categories than
</p>
<p>in the &ldquo;a little concerned&rdquo; and &ldquo;not concerned at all&rdquo; categories. The ex-
</p>
<p>pected number of cases in each category under the assumption of maxi-
</p>
<p>mum dispersion is 5. That is, if the cases were equally spread across the
</p>
<p>categories, we would expect the same number in each. Following Equa-
</p>
<p>tion 5.3, we first multiply the number of cases observed in each category
</p>
<p>by the number observed in every other category and then sum. We then
</p>
<p>divide this total by the sum of the number expected in each category
</p>
<p>Nobs1Nobs2 � Nobs1Nobs3 � Nobs1Nobs4 � Nobs2Nobs3 � Nobs2Nobs4 � Nobs3Nobs4
</p>
<p>Nobs1Nobs2 � Nobs1Nobs3 � Nobs2Nobs3
</p>
<p>Fear of Crime Among Students
</p>
<p>CATEGORY N OBSERVED N EXPECTED
</p>
<p>Not Concerned at All 3 20/4 � 5
A Little Concerned 4 20/4 � 5
Quite Concerned 6 20/4 � 5
Very Concerned 7 20/4 � 5
Total (�) 20 20
</p>
<p>Table 5.3
</p>
<p>101</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>multiplied by the number expected in every other category. This amount
</p>
<p>is then multiplied by 100:
</p>
<p>W orking It Out
</p>
<p> � 96.6667
</p>
<p> � �145150� � 100
 � �(3 � 4) � (3 � 6) � (3 � 7) � (4 � 6) � (4 � 7) � (6 � 7)(5 � 5) � (5 � 5) � (5 � 5) � (5 � 5) � (5 � 5) � (5 � 5)� � 100
</p>
<p> IQV � 
�
k�1
</p>
<p>i�1
</p>
<p> �
k
</p>
<p>j�i�1
</p>
<p> Nobsi Nobsj
</p>
<p>�
k�1
</p>
<p>i�1
</p>
<p> �
k
</p>
<p>j�i�1
</p>
<p> Nexpi Nexpj 
</p>
<p> � 100
</p>
<p>The observed variation is 145. The expected variation is 150, represent-
</p>
<p>ing the maximum amount of dispersion possible for the measure. The
</p>
<p>IQV for this measure is 96.67, meaning that the cases studied are very
</p>
<p>dispersed among the categories of the measure.
</p>
<p>M e a s u r i n g  D i s p e r s i o n  i n  I n t e r v a l  S c a l e s :  
T h e  R a n g e ,  V a r i a n c e ,  a n d  S t a n d a r d  D e v i a t i o n
</p>
<p>A common method of describing the spread of scores on interval or
</p>
<p>higher scales is to examine the range between the highest and lowest
</p>
<p>scores. Take, for example, the distribution of cases in Table 5.4. Let&rsquo;s say
</p>
<p>that this was a distribution of crime calls at hot spots over a one-year pe-
</p>
<p>riod. In describing typicality in this distribution, we would report the
</p>
<p>mean number of calls for the 12 places, which is 21.50. In describing
</p>
<p>how dispersed the scores are, we would report that the scores range be-
</p>
<p>tween 2 and 52, or that the range of scores is 50.
</p>
<p>The range is very simple and easy to present. Its attraction lies pre-
</p>
<p>cisely in the fact that everyone understands what a range represents.
</p>
<p>However, the range is an unstable statistic because it uses very little of
</p>
<p>the information available in interval-level scales. It bases its estimate of
</p>
<p>dispersion on just two observations, the highest and lowest scores. This
</p>
<p>means that a change in just one case in a distribution can completely
</p>
<p>102</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R I N G D I S P E R S I O N I N I N T E R V A L S C A L E S
</p>
<p>alter your description of dispersion. For example, if we changed the case
</p>
<p>with the most calls in Table 5.4 from 52 to 502, the range would change
</p>
<p>from 50 to 500.
</p>
<p>One method for reducing the instability of the range is to examine
</p>
<p>cases that are not at the extremes of your distribution. In this way, you
</p>
<p>are likely to avoid the problem of having the range magnified by a few
</p>
<p>very large or small numbers. For example, you might choose to look at
</p>
<p>the range between the 5th and 95th percentile scores, rather than that
</p>
<p>between the lowest and highest scores. It is also common to look at the
</p>
<p>range between the 25th and 75th percentile scores or between the 20th
</p>
<p>and 80th percentile scores. But however you change the points at which
</p>
<p>the range is calculated, you still rely on just two scores in determining
</p>
<p>the spread of cases in your distribution. The range provides no insight
</p>
<p>into whether the scores below or above these cases are clustered to-
</p>
<p>gether tightly or dispersed widely. Its portrait of dispersion for interval
</p>
<p>scales is thus very limited.
</p>
<p>How can we gain a fuller view of dispersion for interval scales? Re-
</p>
<p>member that we became interested in the problem of dispersion because
</p>
<p>we wanted to provide an estimate of how well the average case repre-
</p>
<p>sented the distribution of cases as a whole. Are scores clustered tightly
</p>
<p>around the average case or dispersed widely from it? Given that we have
</p>
<p>already described the mean as the most appropriate measure of central
</p>
<p>tendency for such scales, this is the natural place to begin our assess-
</p>
<p>ment. Why not simply examine how much the average scores differ from
</p>
<p>the mean?
</p>
<p>In fact, this is the logic that statisticians have used to develop the
</p>
<p>main measures of dispersion for interval scales. However, they are faced
</p>
<p>with a basic problem in taking this approach. As we discussed in Chap-
</p>
<p>ter 4, if we add up all of the deviations from the mean, we will always
</p>
<p>Crime Calls at Hot Spots in a Year
</p>
<p>HOT SPOT NUMBER NUMBER OF CALLS
</p>
<p>1 2
2 9
3 11
4 13
5 20
6 20
7 20
8 24
9 27
</p>
<p>10 29
11 31
12 52
</p>
<p>Table 5.4
</p>
<p>103</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>come up with a value of 0. You can see this again by looking at the data
</p>
<p>on crime calls at hot spots in Table 5.5. If we take the sum of the differ-
</p>
<p>ences between each score and the mean, written in equation form as
</p>
<p>the total, as expected, is 0.
</p>
<p>As discussed in Chapter 4, when we add up the deviations above and
</p>
<p>below the mean, the positive and negative scores cancel each other out.
</p>
<p>In order to use deviations from the mean as a basis for a measure of dis-
</p>
<p>persion, we must develop a method for taking the sign, or direction, out
</p>
<p>of our statistic. One solution is to square each deviation from the mean.
</p>
<p>Squaring will always yield a positive result because multiplying a posi-
</p>
<p>tive number or a negative number by itself will result in a positive out-
</p>
<p>come. This is the method that statisticians have used in developing the
</p>
<p>measures of dispersion most commonly used for interval scales.
</p>
<p>The Variance
</p>
<p>When we take this approach, the variance (s 2) provides an estimate of
</p>
<p>the dispersion around the mean. It is the sum of the squared deviations
</p>
<p>from the mean divided by the number of cases. Written in equation
</p>
<p>form, it is
</p>
<p>Equation 5.4s 2 � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � X )
2
</p>
<p>N
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(Xi � X )
</p>
<p>Deviations from the Mean for Crime Calls at Hot Spots in a Year
</p>
<p>HOT SPOT NUMBER DEVIATIONS FROM
</p>
<p>NUMBER OF CALLS THE MEAN (Xi � )
</p>
<p>1 2 2 � 21.5 � �19.5
2 9 9 � 21.5 � �12.5
3 11 11 � 21.5 � �10.5
4 13 13 � 21.5 � �8.5
5 20 20 � 21.5 � �1.5
6 20 20 � 21.5 � �1.5
7 20 20 � 21.5 � �1.5
8 24 24 � 21.5 � 2.5
9 27 27 � 21.5 � 5.5
</p>
<p>10 29 29 � 21.5 � 7.5
11 31 31 � 21.5 � 9.5
12 52 52 � 21.5 � 30.5
</p>
<p>Total (�) � 0.0
</p>
<p>X
</p>
<p>Table 5.5
</p>
<p>104</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R I N G D I S P E R S I O N I N I N T E R V A L S C A L E S
</p>
<p>In practice, you must take the following steps to compute the variance
</p>
<p>(as we do for our example in Table 5.6):
</p>
<p>1. Take each case and subtract the mean from it, to get the deviation
</p>
<p>from the mean. For our example of crime calls at hot spots, we first
</p>
<p>take the case with 2 calls and subtract the mean of 21.5 from it, to
</p>
<p>get a score of �19.5.
</p>
<p>2. Square each of these scores. For the first case, our result is 380.25.
</p>
<p>3. Sum the results obtained in step 2. For our example of hot spots of
</p>
<p>crime, this yields a total of 1,839.
</p>
<p>4. Finally, divide this result by the number of cases in the study. For our
</p>
<p>12 cases, this leads to a variance of 153.25.1
</p>
<p>Variance for Crime Calls at Hot Spots in a Year
</p>
<p>HOT SPOT NUMBER
</p>
<p>NUMBER OF CALLS (Xi � ) (Xi � )
2
</p>
<p>1 2 2 � 21.5 � �19.5 380.25
2 9 9 � 21.5 � �12.5 156.25
3 11 11 � 21.5 � �10.5 110.25
4 13 13 � 21.5 � �8.5 72.25
5 20 20 � 21.5 � �1.5 2.25
6 20 20 � 21.5 � �1.5 2.25
7 20 20 � 21.5 � �1.5 2.25
8 24 24 � 21.5 � 2.5 6.25
9 27 27 � 21.5 � 5.5 30.25
</p>
<p>10 29 29 � 21.5 � 7.5 56.25
11 31 31 � 21.5 � 9.5 90.25
12 52 52 � 21.5 � 30.5 930.25
</p>
<p>Total (�) � 0.0 Total (�) � 1,839.00
</p>
<p>XX
</p>
<p>Table 5.6
</p>
<p>1If you are working with SPSS or another computer package, you will notice that the
</p>
<p>result you get computing the variance by hand using this formula and the result pro-
</p>
<p>vided by the computer package are slightly different. For example, SPSS computes a
</p>
<p>variance of 167.18 for the distribution provided in Table 5.6. The difference develops
</p>
<p>from the computer&rsquo;s use of a correction for the bias of sample variances: 1 is sub-
</p>
<p>tracted from the N in the denominator of Equation 5.4. The correction is used primar-
</p>
<p>ily as a tool in inferential statistics and is discussed in Chapter 10. Though it is our
</p>
<p>view that the uncorrected variance should be used in describing sample statistics,
</p>
<p>many researchers report variances with the correction factor for sample estimates.
</p>
<p>When samples are larger, the estimates obtained with and without the correction are
</p>
<p>very similar, and thus it generally makes very little substantive difference which ap-
</p>
<p>proach is used.
</p>
<p>105</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>amounts required for a group of 15 defendants. The mean bail amount is
</p>
<p>$3,263.33. Following the same procedure as before, we subtract the
</p>
<p>mean from each of the individual observations. These values are pre-
</p>
<p>cases, we gain a variance of $6,984,155.56 for the dollar amount of bail.
</p>
<p>W orking It Out
</p>
<p> � 153.25
</p>
<p> � 
1,839
</p>
<p>12
</p>
<p> � 
�
12
</p>
<p>i�1
</p>
<p>(Xi � 21.5)
2
</p>
<p>12
</p>
<p> s 2 � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � X )
2
</p>
<p>N
</p>
<p>Variance for Bail Amounts for a Sample of Persons Arrested for Felonies
</p>
<p>BAIL
</p>
<p>DEFENDANT AMOUNT (Xi � ) (Xi � )
2
</p>
<p>1 500 �2,763.33 7,635,992.69
2 1,000 �2,263.33 5,122,662.69
3 1,000 �2,263.33 5,122,662.69
4 1,000 �2,263.33 5,122,662.69
5 1,200 �2,063.33 4,257,330.69
6 1,500 �1,763.33 3,109,332.69
7 2,500 �763.33 582,672.69
8 2,500 �763.33 582,672.69
9 2,500 �763.33 582,672.69
</p>
<p>10 2,750 �513.33 263,507.69
11 5,000 1,736.67 3,016,022.69
12 5,000 1,736.67 3,016,022.69
13 5,000 1,736.67 3,016,022.69
14 7,500 4,236.67 17,949,372.69
15 10,000 6,736.67 45,382,722.69
</p>
<p>Total (�) � 0.05 Total (�) � 104,762,333.33
</p>
<p>XX
</p>
<p>Table 5.7
</p>
<p>sented in the third column. The squared deviations from the mean 
</p>
<p>appears at the bottom of the column. When we divide the total by the N of
</p>
<p>appear in the fourth column, and the sum of the squared deviations 
</p>
<p>As another example, consider the data presented in Table 5.7 on bail
</p>
<p>106</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R I N G D I S P E R S I O N I N I N T E R V A L S C A L E S
</p>
<p>With the variance, we now have a statistic for computing dispersion
</p>
<p>based on deviations from the mean. However, how can we interpret
</p>
<p>whether the variance for a distribution is large or small? If you are having
</p>
<p>trouble making sense of this from our two examples, you are not alone.
</p>
<p>While squaring solves one problem (the fact that the raw deviations from
</p>
<p>the mean sum to 0), it creates another. By squaring, we generally obtain
</p>
<p>numbers that are much larger than the actual units in the distributions
</p>
<p>we are examining.2
</p>
<p>The Standard Deviation
</p>
<p>Another measure of dispersion based on the variance provides a solution
</p>
<p>to the problem of interpretation. This measure, the standard deviation,
</p>
<p>is calculated by taking the square root of the variance. Accordingly, it re-
</p>
<p>duces our estimate of dispersion, using a method similar to the one we
</p>
<p>employed to solve the problem of positive and negative differences from
</p>
<p>the mean adding to 0. The standard deviation (s) provides an estimate of
</p>
<p>dispersion in units similar to those of our original scores. It is described
</p>
<p>in equation form as
</p>
<p>Equation 5.5s � ��
N
</p>
<p>i�1
</p>
<p>(Xi � X )
2
</p>
<p>N
</p>
<p>W orking It Out
</p>
<p> � 6,984,155.56
</p>
<p> � 
104,762,333.33
</p>
<p>15
</p>
<p> � 
�
15
</p>
<p>i�1
</p>
<p>(Xi � 3,263.33)
2
</p>
<p>15
</p>
<p> s 2 � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � X )
2
</p>
<p>N
</p>
<p>2In the special case of a fraction, the result will be smaller numbers.
</p>
<p>107</p>
<p/>
</div>
<div class="page"><p/>
<p>Although Equations 5.4 and 5.5 provide a useful way of conceptualizing and
</p>
<p>measuring the variance and standard deviation, you can also use a com-
</p>
<p>puting formula that has fewer steps and is less likely to result in computa-
</p>
<p>tional error. In Table 5.6, we rounded the mean and then calculated squared
</p>
<p>deviations based on values that were rounded at each step. In an attempt
</p>
<p>to limit the amount of rounding, and consequently decrease the chances of
</p>
<p>a mistake, an alternative equation that can be used for the variance is
</p>
<p>And an alternative for the standard deviation is
</p>
<p>Let&rsquo;s reconsider the data in Table 5.6 on hot spots. The following table
</p>
<p>illustrates the key calculations:
</p>
<p>HOT SPOT NUMBER NUMBER OF CALLS (Xi)
</p>
<p>1 2 4
</p>
<p>2 9 81
</p>
<p>3 11 121
</p>
<p>4 13 169
</p>
<p>5 20 400
</p>
<p>6 20 400
</p>
<p>7 20 400
</p>
<p>8 24 576
</p>
<p>9 27 729
</p>
<p>10 29 841
</p>
<p>11 31 961
</p>
<p>12 52 2,704
</p>
<p>Total (�) 258 7,386
</p>
<p>X 2i
</p>
<p>s � ��Ni�1X 2i  � ��
N
</p>
<p>i�1
</p>
<p>Xi�2
N
</p>
<p>N
</p>
<p>s 2 � 
�
N
</p>
<p>i�1
</p>
<p>X 2i  � 
��N
</p>
<p>i�1
</p>
<p>Xi�2
N
</p>
<p>N
</p>
<p>Computational Equations for the Variance 
and Standard Deviation</p>
<p/>
</div>
<div class="page"><p/>
<p>The variance (s 2) is then calculated with the computational equation as
</p>
<p>And the standard deviation is simply the square root of the variance:
</p>
<p>Similarly, let&rsquo;s revisit the bail data in Table 5.7 and compute the vari-
</p>
<p>ance with the computational formula. The following table illustrates the
</p>
<p>key calculations.
</p>
<p>DEFENDANT BAIL AMOUNT (Xi )
</p>
<p>1 500 250,000
</p>
<p>2 1,000 1,000,000
</p>
<p>3 1,000 1,000,000
</p>
<p>4 1,000 1,000,000
</p>
<p>5 1,200 1,440,000
</p>
<p>6 1,500 2,250,000
</p>
<p>7 2,500 6,250,000
</p>
<p>8 2,500 6,250,000
</p>
<p>9 2,500 6,250,000
</p>
<p>10 2,750 7,562,500
</p>
<p>11 5,000 25,000,000
</p>
<p>12 5,000 25,000,000
</p>
<p>13 5,000 25,000,000
</p>
<p>14 7,500 56,250,000
</p>
<p>15 10,000 100,000,000
</p>
<p>Total (�) 48,950 264,502,500
</p>
<p>The variance is
</p>
<p>And the standard deviation is
</p>
<p>s � �6,984,155.56 � 2,642.76
</p>
<p>s 2 � 
</p>
<p>264,502,500 � 
(48,950)2
</p>
<p>15
15
</p>
<p> � 6,984,155.56
</p>
<p>X 2i
</p>
<p>s � �153.25 � 12.38
</p>
<p>s 2 � 
</p>
<p>7,386 � 
(258)2
</p>
<p>12
12
</p>
<p> � 153.25</p>
<p/>
</div>
<div class="page"><p/>
<p>110 C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>In calculating the standard deviation, we add one step to our calcula-
</p>
<p>tion of variance: We take the square root of our result. For the example
</p>
<p>of crime calls at 12 hot spots (where the variance equaled 153.25), we
</p>
<p>obtain a standard deviation of .3 If you were to define,
</p>
<p>on average, how much the scores differed from the mean just by looking
</p>
<p>at these 12 cases, you would probably come to a conclusion close to that
</p>
<p>provided by the standard deviation. Similarly, if we take the square root
</p>
<p>of the variance for the bail example above, we come up with a figure
</p>
<p>that makes much more intuitive sense than the variance. In this case, the
</p>
<p>standard deviation is , or $2,642.76.
</p>
<p>The standard deviation has some basic characteristics, which relate
</p>
<p>generally to its use:
</p>
<p>1. A standard deviation of 0 means that a measure has no variability.
</p>
<p>For this to happen, all of the scores on a measure have to be the
</p>
<p>same. For example, if you examine a group of first-time offenders,
</p>
<p>there will be no variation in the number of offenses in their criminal
</p>
<p>records. By definition, because they are all first-time offenders, the
</p>
<p>standard deviation (and the variance) will be 0.
</p>
<p>2. The size of the standard deviation (and the variance) is dependent
</p>
<p>on both the amount of dispersion in the measure and the units of
</p>
<p>analysis that are used. When cases are spread widely from the mean,
</p>
<p>there is more dispersion and the standard deviation will be larger.
</p>
<p>When cases are tightly clustered around the mean, the standard
</p>
<p>deviation will be smaller.
</p>
<p>Similarly, when the units of analysis in the measure are large, the
</p>
<p>standard deviation will reflect the large units. For example, if you
</p>
<p>report the standard deviation of police salaries in a particular city in
</p>
<p>dollars, your standard deviation will be larger than if you reported
</p>
<p>those salaries in units of thousands of dollars. If the standard
</p>
<p>deviation is 3,350 in dollars, the standard deviation would be 3.35
</p>
<p>using the unit of thousands of dollars.
</p>
<p>3. Extreme deviations from the mean have the greatest weight in
</p>
<p>constructing the standard deviation. What this means is that here, as
</p>
<p>with the mean, you should be concerned with the problem of
</p>
<p>outliers. In this case, the effect of outliers is compounded because
</p>
<p>they affect not only the mean itself, which is used in computing the
</p>
<p>standard deviation, but also the individual deviations that are
</p>
<p>obtained by subtracting the mean from individual cases.
</p>
<p>�6,984,155.56
</p>
<p>�153.25 � 12.38
</p>
<p>3As discussed in footnote 1, SPSS and many other computer packages would provide
</p>
<p>a slightly different result, based on the use of a correction of �1 in the denominator.</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R I N G D I S P E R S I O N I N I N T E R V A L S C A L E S 111
</p>
<p>The standard deviation is a useful statistic for comparing the extent to
</p>
<p>which characteristics are clustered or dispersed around the mean in dif-
</p>
<p>4
</p>
<p>samples are very similar (61.05 for antitrust violators; 59.27 for bribery
</p>
<p>offenders), but the standard deviation for those convicted of bribery is
</p>
<p>about twice that of those convicted of antitrust violations.
</p>
<p>Figure 5.1 illustrates why these two samples yield similar means but
</p>
<p>very different standard deviations. The scores for most antitrust offend-
</p>
<p>the distribution, including many more cases between 75 and 90 and
</p>
<p>below 50. What this tells us is that the antitrust sample includes a fairly
</p>
<p>homogeneous group of offenders, ranking on average relatively high
</p>
<p>on the Duncan socioeconomic index. Bribery is a much more diverse
</p>
<p>category. Although the means are similar, the bribery category includes
</p>
<p>many more lower- and higher-status individuals than does the antitrust
</p>
<p>category.
</p>
<p>The Coefficient of Relative Variation
</p>
<p>For the data on bribery and antitrust offenders in Table 5.8, in which the
</p>
<p>means of the two groups are fairly similar, a direct comparison of stan-
</p>
<p>dard deviations provides a good view of the differences in dispersion.
</p>
<p>When the means of two groups are very different, however, this compar-
</p>
<p>ison may not be a fair one. If the mean Duncan score for one group was
</p>
<p>10 and for the other was 50, we might expect a larger standard deviation
</p>
<p>in the latter group simply because the mean was larger and there was
</p>
<p>Duncan SEI for Bribery and Antitrust Offenders
</p>
<p>CATEGORY N s
</p>
<p>Bribery 83 59.27 19.45
Antitrust 112 61.05 11.13
Total (�) 195
</p>
<p>X
</p>
<p>Table 5.8
</p>
<p>4See Albert J. Reiss, Occupations and Social Status (New York: Free Press, 1961).
</p>
<p>victed of antitrust violations is compared to a sample of offenders convicted
</p>
<p>ferent samples. For example, in Table 5.8, a sample of offenders con-
</p>
<p>of bribery. The characteristic examined is social status, as measured
</p>
<p>by the interval-scale Duncan socioeconomic index  (SEI). The  index
</p>
<p>is based on the average income, education, and prestige associated
</p>
<p>with different occupations. The mean Duncan scores for these two
</p>
<p>offenders, in contrast, the scores are much more widely spread across
</p>
<p>ers are clustered closely within the range of 55 to 75. For bribery </p>
<p/>
</div>
<div class="page"><p/>
<p>112 C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>O
ff
</p>
<p>en
d
</p>
<p>er
s
</p>
<p>Std. dev. = 11.13
Mean = 61.05
N = 112.00
</p>
<p>0                      20  40      60           80 
</p>
<p>100
</p>
<p>80
</p>
<p>60
</p>
<p>40
</p>
<p>20
</p>
<p>0
</p>
<p>SEI
</p>
<p>Std. dev. = 19.45
Mean = 59.27
N = 83.00
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>O
ff
</p>
<p>en
d
</p>
<p>er
s
</p>
<p>SEI
</p>
<p>30
</p>
<p>20
</p>
<p>10
</p>
<p>0
0     20               40               60           80               
</p>
<p>Socioeconomic IndicesFigure 5.1
</p>
<p>(a) Antitrust Offenders
</p>
<p>(b) Bribery Offenders</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R I N G D I S P E R S I O N I N I N T E R V A L S C A L E S 113
</p>
<p>greater potential for dispersion. Similarly, if two measures use different
</p>
<p>units of analysis&mdash;for example, dollars and number of offenses&mdash;a direct
</p>
<p>comparison of standard deviations does not make sense.
</p>
<p>One solution to this problem is to use the coefficient of relative
</p>
<p>variation (CRV). The coefficient of relative variation looks at the size of
</p>
<p>the standard deviation of a measure relative to the size of its mean:
</p>
<p>Equation 5.6
</p>
<p>In the example of the SEI for antitrust offenders, we divide the standard
</p>
<p>deviation (11.13) by the mean (61.05) to obtain a CRV of 0.18, meaning
</p>
<p>that the standard deviation is about one-fifth the size of the mean. Be-
</p>
<p>cause the CRV expresses dispersion in a measure in a standardized form
</p>
<p>relative to the mean, we can compare the CRV across measures that have
</p>
<p>widely different means and standard deviations.
</p>
<p>CRV � 
s
</p>
<p>X
</p>
<p>W orking It Out
</p>
<p> � 0.1823
</p>
<p> � 
11.13
</p>
<p>61.05
</p>
<p> CRV � 
s
</p>
<p>X
</p>
<p>A Note on the Mean Deviation
</p>
<p>The standard deviation allows us to measure dispersion in interval
</p>
<p>scales, taking into account the deviation from the mean of each case in
</p>
<p>our sample or population. But it is not the only measure that allows us
</p>
<p>to do this. The mean deviation takes a similar approach, but relies on
</p>
<p>absolute values, rather than squaring, to overcome the fact that the sum
</p>
<p>of the deviations from the mean equals 0. When you take the absolute
</p>
<p>value of a number, you ignore its sign. Accordingly, �8 and 8 both have
</p>
<p>an absolute value of 8; in mathematical notation, � �8 � � � 8 � � 8.
The equation for the mean deviation is similar to that for the vari-
</p>
<p>ance. The only difference is that we take the absolute value of the
</p>
<p>antitrust offenders.
</p>
<p>A measure that has a CRV of 1, for example, may be considered to 
</p>
<p>include much greater relative variation than is found in our sample of </p>
<p/>
</div>
<div class="page"><p/>
<p>114 C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>Mean Deviation for Crime Calls at Hot Spots in a Year
</p>
<p>HOT SPOT NUMBER DEVIATIONS FROM
</p>
<p>NUMBER OF CALLS THE MEAN �Xi � �
</p>
<p>1 2 � 2 � 21.5 � � 19.5
2 9 � 9 � 21.5 � � 12.5
3 11 � 11 � 21.5 � � 10.5
4 13 � 13 � 21.5 � � 8.5
5 20 � 20 � 21.5 � � 1.5
6 20 � 20 � 21.5 � � 1.5
7 20 � 20 � 21.5 � � 1.5
8 24 � 24 � 21.5 � � 2.5
9 27 � 27 � 21.5 � � 5.5
</p>
<p>10 29 � 29 � 21.5 � � 7.5
11 31 � 31 � 21.5 � � 9.5
12 52 � 52 � 21.5 � � 30.5
</p>
<p>Total (�) � 111.0
</p>
<p>X
</p>
<p>Table 5.9
</p>
<p>difference between each score and the mean, rather than the square of
</p>
<p>the difference:
</p>
<p>Using the data on crime calls in hot spots from Table 5.4, we take the
</p>
<p>following steps to obtain the mean deviation. We first take the absolute
</p>
<p>value of the difference between each score and the mean (see Table
</p>
<p>5.9). We then sum up the 12 scores. Notice that we obtain a positive
</p>
<p>number now (111), and not 0, because we are taking the absolute values
</p>
<p>of the differences. Dividing this sum by the number of cases, N, we get a
</p>
<p>mean deviation of 9.25.
</p>
<p>Mean deviation � 
�
N
</p>
<p>i�1
�Xi � X �
</p>
<p>N
</p>
<p>W orking It Out
</p>
<p> � 9.25
</p>
<p> � 
111
12
</p>
<p> � 
�
12
</p>
<p>i�1
�Xi � 21.5 �
</p>
<p>N
</p>
<p> Mean deviation � 
�
N
</p>
<p>i�1
�Xi � X �
</p>
<p>N
</p>
<p>Equation 5.7</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S U M M A R Y 115
</p>
<p>The mean deviation and the standard deviation provide similar esti-
</p>
<p>mates of dispersion, but the mean deviation here is a bit smaller than the
</p>
<p>standard deviation of 12.38 that we calculated earlier. Which is the better
</p>
<p>estimate of dispersion? In some sense, the mean deviation is more
</p>
<p>straightforward. It simply looks at the average deviation from the mean.
</p>
<p>In obtaining the standard deviation, we first must square the deviations;
</p>
<p>then later, to return our result to units similar to those of the original dis-
</p>
<p>tribution, we must take the square root of the variance.
</p>
<p>Given our rule that we should use the least complex presentation
</p>
<p>that is appropriate to answering our research question, you may won-
</p>
<p>der why the standard deviation is almost always preferred over the
</p>
<p>mean deviation in criminal justice research. As you will see in the next
</p>
<p>few chapters, the answer is that the standard deviation is relevant to a
</p>
<p>number of other statistics that we use in analyzing and describing
</p>
<p>data.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>Measures of dispersion describe to what extent cases are distributed
</p>
<p>around the measure of central tendency. They tell us just how typical the
</p>
<p>typical case is.
</p>
<p>There are several measures of dispersion for nominal and ordinal
</p>
<p>scales. Proportions and percentages describe the extent to which cases
</p>
<p>are concentrated in the modal category. The variation ratio (VR) de-
</p>
<p>scribes the extent to which cases are spread outside the modal category.
</p>
<p>A proportion of 1 (VR of 0) means that all the cases are in the modal cat-
</p>
<p>egory. This represents the least possible amount of dispersion. The value
</p>
<p>for the greatest possible dispersion can be determined by calculating the
</p>
<p>minimum possible value of the modal category and then translating that
</p>
<p>into a proportion or VR value. These measures can, in principle, be used
</p>
<p>with ordinal-level data, but the results may be misleading, as they take
</p>
<p>into account only the value of the mode. As an alternative, the index of
</p>
<p>qualitative variation (IQV) is a standardized measure that takes into ac-
</p>
<p>count variability across all the categories of a nominal- or ordinal-level
</p>
<p>variable. An IQV of 0 means that there is no variation; an IQV of 100
</p>
<p>means that there is maximum variation across the categories.
</p>
<p>A different set of measures is used to measure dispersion for inter-
</p>
<p>val and ratio scales. The range measures the difference between the
</p>
<p>highest and lowest scores. It has the advantage of simplicity, but it
</p>
<p>uses very little information (only two scores) and the scores used are
</p>
<p>taken from the two extremes. It is also very sensitive to outliers. A </p>
<p/>
</div>
<div class="page"><p/>
<p>116 C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>95th and the 5th percentile. Such measures, however, are still based
</p>
<p>on minimal information and thus are generally considered unstable
</p>
<p>statistics. A more stable statistic for measuring dispersion in interval-
</p>
<p>level scales is the variance. The variance is the sum of the squared
</p>
<p>deviations of each score from the mean divided by the number of
</p>
<p>cases. The standard deviation (s) is the square root of the variance.
</p>
<p>The advantage of the standard deviation over the variance is that the
</p>
<p>results are more easily interpreted. If all the scores in a sample are the
</p>
<p>same, s will be 0. The more widely the scores are spread around the
</p>
<p>mean, the greater will be the value of s. Outliers have a considerable
</p>
<p>impact on the standard deviation.
</p>
<p>Comparing the standard deviations of means is problematic when
</p>
<p>the means are very different or when their units of measurement are
</p>
<p>different. An alternative measure, the coefficient of relative varia-
</p>
<p>tion (CRV), enables comparisons among samples with different
</p>
<p>means. A less often used measure of dispersion for interval scales is
</p>
<p>the mean deviation. The mean deviation is computed by taking the
</p>
<p>sum of the absolute values of the deviations from the mean divided by
</p>
<p>the number of cases.
</p>
<p>K e y  T e r m s
</p>
<p>coefficient of relative variation A mea-
</p>
<p>sure of dispersion calculated by dividing
</p>
<p>the standard deviation by the mean.
</p>
<p>index of qualitative variation A measure
</p>
<p>of dispersion calculated by dividing the
</p>
<p>sum of the possible pairs of observed
</p>
<p>scores by the sum of the possible pairs of
</p>
<p>expected scores (when cases are equally
</p>
<p>distributed across categories).
</p>
<p>mean deviation A measure of dispersion
</p>
<p>calculated by adding the absolute deviation
</p>
<p>of each score from the mean and then di-
</p>
<p>viding the sum by the number of cases.
</p>
<p>range A measure of dispersion calculated
</p>
<p>by subtracting the smallest score from the
</p>
<p>largest score. The range may also be calcu-
</p>
<p>lated from specific points in a distribution,
</p>
<p>such as the 5th and 95th percentile scores.
</p>
<p>standard deviation A measure of disper-
</p>
<p>sion calculated by taking the square root of
</p>
<p>the variance.
</p>
<p>variance (s2) A measure of dispersion cal-
</p>
<p>culated by adding together the squared de-
</p>
<p>viation of each score from the mean and
</p>
<p>then dividing the sum by the number of
</p>
<p>cases.
</p>
<p>variation ratio A measure of dispersion
</p>
<p>calculated by subtracting the proportion of
</p>
<p>cases in the modal category from 1.
</p>
<p>researcher may instead choose to measure the range between, say, the</p>
<p/>
</div>
<div class="page"><p/>
<p>S Y M B O L S A N D F O R M U L A S 117
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>Nmodal cat. Number of cases in the modal category
</p>
<p>Ntotal Total number of cases
</p>
<p>Nobs Number of cases observed in each category
</p>
<p>Nexp Number of cases expected in each category
</p>
<p>s Standard deviation
</p>
<p>s 2 Variance
</p>
<p>To calculate the proportion of cases falling in the modal category:
</p>
<p>To calculate the percentage of cases falling in the modal category:
</p>
<p>To calculate the variation ratio:
</p>
<p>To calculate the index of qualitative variation:
</p>
<p>To calculate the variance:
</p>
<p>To calculate the standard deviation:
</p>
<p>s � ��
N
</p>
<p>i�1
</p>
<p>(Xi � X )
2
</p>
<p>N
</p>
<p>s 2 � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � X )
2
</p>
<p>N
</p>
<p>IQV � ��
k�1
</p>
<p>i�1
</p>
<p> �
k
</p>
<p>j�i�1
</p>
<p> Nobsi Nobsj
</p>
<p>�
k�1
</p>
<p>i�1
</p>
<p> �
k
</p>
<p>j�i�1
</p>
<p> Nexpi Nexpj� � 100
VR � 1 � �Nmodal cat.Ntotal �
</p>
<p>Percentage � 
Nmodal cat.
</p>
<p>Ntotal
 � 100
</p>
<p>Proportion � 
Nmodal cat.
</p>
<p>Ntotal</p>
<p/>
</div>
<div class="page"><p/>
<p>118 C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>To calculate the coefficient of relative variation:
</p>
<p>To calculate the mean deviation:
</p>
<p>E x e r c i s e s
</p>
<p>5.1 Police records for 105 rape victims were analyzed to determine
whether any prior relationship existed between the victim and the
offender. The results were as follows:
</p>
<p>Spouse 41
</p>
<p>Family member other than spouse 14
</p>
<p>Acquaintance 22
</p>
<p>No prior relationship 28
</p>
<p>a. Calculate the modal proportion and the variation ratio.
</p>
<p>b. What are the minimum and maximum possible values for the varia-
tion ratio?
</p>
<p>c. Calculate the index of qualitative variation.
</p>
<p>5.2 As part of a larger study on the influence of delinquent peers, a sam-
ple of high school youth were asked how much they wanted to be
like their best friend. The responses were coded as follows: in every
way, 26; in most ways, 36; in some ways, 41; and not at all, 8.
</p>
<p>a. Calculate the variation ratio for these data.
</p>
<p>b. Calculate the index of qualitative variation for these data.
</p>
<p>5.3 People convicted of minor traffic offenses who appeared in the
magistrate&rsquo;s court of a given locality on a given day were sentenced
as follows: conditional discharge, 14; fine, 35; and license dis-
qualification, 11.
</p>
<p>a. Calculate the variation ratio.
</p>
<p>b. Calculate the index of qualitative variation.
</p>
<p>c. Why do these two results differ?
</p>
<p>5.4 A sample of women was drawn from town A, and another sample was
drawn from town B. All the women were asked how safe or unsafe
</p>
<p>Mean deviation � 
�
N
</p>
<p>i�1
�Xi � X �
</p>
<p>N
</p>
<p>CRV � 
s
</p>
<p>X</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 119
</p>
<p>they felt walking alone at night in their neighborhoods. The results
were recorded on a scale as follows: totally unsafe (town A: 40; town
B: 25), quite unsafe (town A: 29; town B: 23), quite safe (town A: 10;
town B: 15), and totally safe (town A: 21; town B: 17).
</p>
<p>a. For each town, describe the typical case, using an appropriate mea-
sure of central tendency. Explain why this is the best measure of
central tendency for these data.
</p>
<p>b. For each town, describe how typical the typical case is, using an
appropriate measure of dispersion. Explain why this is the best
measure of dispersion for these data.
</p>
<p>c. In comparing the measures of central tendency and dispersion for
the two towns, what conclusions may be drawn about the attitudes
of the women?
</p>
<p>5.5 For a sample of 12 offenders convicted of weapons violations, the
length of prison sentence in months was recorded as:
</p>
<p>6 6 2 12 36 48 60 24 24 20 18 15
</p>
<p>a. Calculate the range for these data.
</p>
<p>b. Calculate the mean and the variance for these data.
</p>
<p>5.6 A group of 20 prisoners in a particular cell block were tested on their
knowledge of the rules of the institution. The marks (out of a possible
70) were as follows:
</p>
<p>31 28 27 19 18 18 41 0 30 27
</p>
<p>27 36 41 64 27 39 20 28 35 30
</p>
<p>a. Calculate the range.
</p>
<p>b. Remove the largest and smallest scores. Calculate the range for the
remaining cases.
</p>
<p>c. How do you account for the difference between the values of the
above two measures of dispersion?
</p>
<p>5.7 Police crack a drug ring of 18 suppliers and discover that of the 18,
only 4 have no previous convictions for drug- or theft-related offenses.
Eight of those arrested have 1 previous conviction, and the others
have 2, 3, 4, 5, 6, and 8, respectively.
</p>
<p>a. Calculate the mean and the standard deviation of the 18 cases.
</p>
<p>b. If each of the drug suppliers is convicted this time around, does the
extra conviction on each of their criminal records affect the mean
or the standard deviation in any way? Explain your answer.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>5.8 Use the data collected from tests of prisoners&rsquo; knowledge of institution
rules in Exercise 5.6.
</p>
<p>a. Calculate the mean and the standard deviation for these data.
</p>
<p>b. If you remove the two most extreme scores, 0 and 64, what are the
new mean and standard deviation?
</p>
<p>c. How do you account for this effect?
</p>
<p>5.9 When asked about how often in the last year they drank more than
four beers in one evening, a sample of college students reported the
following:
</p>
<p>Number 
</p>
<p>of Times Frequency
</p>
<p>0 187
</p>
<p>1 213
</p>
<p>2 162
</p>
<p>3 94
</p>
<p>4 71
</p>
<p>5 55
</p>
<p>6 39
</p>
<p>7 12
</p>
<p>8 9
</p>
<p>9 5
</p>
<p>10 13
</p>
<p>a. Calculate an appropriate measure of dispersion for these data.
Explain why this measure is most appropriate for these data.
</p>
<p>b. Describe one way these data could be recoded to reduce the number
of categories. Calculate an appropriate measure of dispersion for the
recoded data and explain why this measure is most appropriate.
</p>
<p>5.10 A researcher takes a sample of shop owners in Tranquiltown and a
sample of shop owners in Violenceville and asks them to estimate the
value of goods stolen from their shops in the past 12 months. The
mean figure is $11.50 (s � $2.50) for Tranquiltown and $4,754.50 
(s � $1,026.00) for Violenceville. When the study is published, the
mayor of Violenceville protests, claiming that the mean sum for his
town is a misleading figure. Because the standard deviation for Vio-
lenceville is much bigger than that for Tranquiltown, he argues, it is
clear that the mean from Violenceville is a much less typical descrip-
tion of the sample than the mean from Tranquiltown.
</p>
<p>a. What statistic might help the researcher to refute this criticism?
Why?</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 121
</p>
<p>b. Calculate this statistic for each town. What should the researcher
conclude?
</p>
<p>5.11 A researcher investigating differences in violence among preschool-
age boys and girls found that the average number of violent acts per
week was 7.6 (s � 4.8) for boys and 3.1 (s � 1.9) for girls.
</p>
<p>a. Calculate the coefficient of relative variation for boys and for
girls.
</p>
<p>b. How can the coefficient of relative variation be used to compare
these two groups? What does it tell you?
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>Similar to the measures of central tendency discussed in Chapter 4, there are 
</p>
<p> several ways to obtain measures of dispersion for interval-level variables in SPSS 
</p>
<p>and Stata (Neither program computes measures of dispersion, such as the index 
</p>
<p>of qualitative variation, for nominal and ordinal variables.).
</p>
<p>SPSS
</p>
<p>The same two commands that we used to obtain measures of central tendency&mdash;
</p>
<p>DESCRIPTIVES and FREQUENCIES&mdash;will be used to obtain measures of 
</p>
<p>dispersion. Since you have already used these commands, our discussion here is 
</p>
<p>focused on the default measures of dispersion and additional options available 
</p>
<p>for measuring dispersion.
</p>
<p>The DESCRIPTIVES command allows you to compute the standard devia-
</p>
<p>tion and variance. The default is to compute the standard deviation, as well as 
</p>
<p>minimum and maximum values. To obtain the variance and/or range, add the 
</p>
<p>option /STATISTICS = to the command line:
</p>
<p>The abbreviations should be clear, but STDDEV is the standard deviation, MIN 
</p>
<p>is the minimum value, and MAX is the maximum value.
</p>
<p>In computing the variance, SPSS uses a correction for the bias of sample 
</p>
<p>measures of variance and dispersion: 1 is subtracted from the N in the denomi-
</p>
<p>nator of Equations 5.4 and 5.5. The correction is used primarily as a tool in 
</p>
<p>inferential statistics and is discussed in greater detail in Chapter 10. Though it is 
</p>
<p>our view that the uncorrected variance and standard deviation should be used 
</p>
<p>in describing sample statistics, many researchers report these statistics with the 
</p>
<p>correction factor for sample estimates. When samples are larger, the estimates 
</p>
<p>obtained with and without the correction are very similar, and thus it generally 
</p>
<p>makes very little substantive difference which approach is used.
</p>
<p>DESCRIPTIVES VARIABLES = variable_names
</p>
<p>/STATISTICS = MEAN STDDEV VARIANCE RANGE MIN MAX.</p>
<p/>
</div>
<div class="page"><p/>
<p>122 C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>The FREQUENCIES command provides similar measures of disper-
</p>
<p>sion through use of the /STATISTICS = option. You may request the stand-
</p>
<p>ard  deviation, variance, range, minimum, and maximum. In addition to these 
</p>
<p> measures of dispersion, the FREQUENCIES command has the option of com-
</p>
<p>puting  percentiles. Since calculation of percentiles by hand can be very difficult 
</p>
<p>(and error prone), this is a nice feature. There are three options for calculating 
</p>
<p>percentiles in SPSS: quartiles (25th, 50th, and 75th percentiles) or cut points for 
</p>
<p>some number of equally spaced groups (e.g., 10th, 20th, &hellip;, 90th percentiles) 
</p>
<p>by using the NTILES = option, as well as specific percentiles that you may be 
</p>
<p>interested in (e.g., 5th, 95th, 99th) by using the /PERCENTILES = option. The 
</p>
<p>general form, including all the various options, is:
</p>
<p>If you are not interested in percentiles or equally spaced groups (e.g., quartiles, 
</p>
<p>deciles, etc.), then omit those lines from the command.
</p>
<p>Specific examples of the use of each of these commands are provided in the 
</p>
<p>accompanying SPSS syntax file for Chapter 5 (Chapter_5.sps).
</p>
<p>Stata
</p>
<p>To obtain measures of dispersion in Stata, the same two commands used in 
</p>
<p>obtaining measures of central tendency will again be used. To obtain detailed 
</p>
<p>measures of dispersion, it is best to simply add the detail option to the  
</p>
<p>command line:
</p>
<p>This will produce the variance, standard deviation, and numerous percentiles. 
</p>
<p>Stata makes the correction to the computation of the variance in the same way 
</p>
<p>that SPSS does by subtracting 1 from the total sample size.
</p>
<p>The tabstat command is also useful in trimming out the unnecessary pieces 
</p>
<p>of information and printing only those items we want:
</p>
<p>Where min is the minimum, max is the maximum, sd is the standard deviation, 
</p>
<p>var is the variance, cv is the coefficient of variation (not available in SPSS), and 
</p>
<p>FREQUENCIES VARIABLES = variable_names
</p>
<p>/PERCENTILES = list_of_percentiles
</p>
<p>/NTILES = number_of_equally_spaced_groups
</p>
<p>/STATISTICS = STDDEV VARIANCE RANGE MINIMUM MAXIMUM
</p>
<p>MEAN MEDIAN MODE.
</p>
<p>summarize variable_names, detail
</p>
<p>tabstat variable_names, statistics (median mean min max 
</p>
<p>range sd var cv p#)</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 123
</p>
<p>in the 5th, 10th, 90th, and 95th percentiles, we would include p5 p10 p90 p95 
</p>
<p>on the command line (and within the parentheses).
</p>
<p>Specific examples of the use of each of these commands are provided in the 
</p>
<p>accompanying Stata do file for Chapter 5 (Chapter_5.do).
</p>
<p>Problems
</p>
<p> 1. Enter the data from Exercise 5.6 on the 20 prisoners&rsquo; test scores.
</p>
<p>a. What is the range?
</p>
<p>b. What are the 5th and 95th percentiles? What is the range between the 
</p>
<p>5th and 95th percentiles?
</p>
<p>c. How does your answer to part b compare to your answer to part b in 
</p>
<p>Exercise 5.6?
</p>
<p> 2. Enter the data from Exercise 5.7. (Be sure that you have 18 lines of  data, 
</p>
<p>since there are 18 observations listed in the question.)
</p>
<p>a. What are the mean and the standard deviation? How does the standard 
</p>
<p>deviation differ from the value you calculated in Exercise 5.7?
</p>
<p>b. To add 1 to each person&rsquo;s number of  prior convictions, you will need to 
</p>
<p>create a new variable.
</p>
<p>In SPSS:
</p>
<p>In Stata:
</p>
<p> 3. Open the NYS data file (nys_1.sav, nys_1_ student.sav, or nys_1.dta).
</p>
<p>a. Choose five of  the delinquency measures. What are the quartiles for 
</p>
<p>b. What is the range between the 25th and 75th percentiles for each 
</p>
<p>delinquency measure? (This difference is known as the &ldquo;inter- quartile 
</p>
<p>range.&rdquo;) What do the differences in inter-quartile ranges appear to 
</p>
<p>indicate about the dispersion of  these different measures self-reported 
</p>
<p>delinquency?
</p>
<p>p# refers to a specific percentile we may be interested in and included for as 
</p>
<p>many percentiles as we would like reported. For example, if we were interested 
</p>
<p>COMPUTE new_var_name = old_var_name + 1.
</p>
<p>EXECUTE.
</p>
<p>gen new_var_name = old_var_name +1
</p>
<p>What are the mean and standard deviation for this new  
</p>
<p>variable? What has changed? What has remained the same?</p>
<p/>
</div>
<div class="page"><p/>
<p>124 C H A P T E R F I V E :  H O W T Y P I C A L I S T H E T Y P I C A L C A S E ?
</p>
<p>-
</p>
<p>able (Remember, this measure is available in the tabstat command in 
Stata, but will need to be calculated by hand if  using SPSS.). What do 
</p>
<p>variable indicate about the relative dispersion of  these variables?
</p>
<p>c. What number of  delinquent acts would mark the 15 % least delinquent 
</p>
<p>youth? The 20 % most delinquent youth?</p>
<p/>
</div>
<div class="page"><p/>
<p>The Logic of Statistical Inference: 
</p>
<p>Making Statements About Populations
</p>
<p>from Sample Statistics
</p>
<p>How Can a Sample Teach Us About a Population?
</p>
<p>C h a p t e r  s i x
</p>
<p>S a m p l e  d i s t r i b u t i o n s  a n d  p o p u l a t i o n  d i s t r i b u t i o n s
</p>
<p>A s k i n g  t h e  r e s e a r c h  q u e s t i o n
</p>
<p>A n s w e r i n g  t h e  r e s e a r c h  q u e s t i o n
</p>
<p>How are They Defined?
</p>
<p>What Symbols are Used?
</p>
<p>How are the Two Interrelated?
</p>
<p>What Types of Error are Possible?
</p>
<p>What is an &ldquo;Acceptable&rdquo; Risk of Error?
</p>
<p>When Might It be Necessary to Accept a Different Level of Risk?
</p>
<p>What are the Research and Null Hypotheses?
</p>
<p>How are They Set Up?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_6,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>IN THIS CHAPTER, we look at an important dilemma that researchers face
in conducting criminal justice research. Although they seek to make
</p>
<p>statements about populations, generally they collect data on samples
</p>
<p>drawn from such populations. Statistical inference provides a solution to
</p>
<p>about the characteristics of a population from data collected from a sam-
</p>
<p>ple drawn from the population. We begin our discussion of statistical in-
</p>
<p>ference by explaining the dilemma researchers face in making statements
</p>
<p>about populations from samples. We then examine the logic of statistical
</p>
<p>inference and the statistical risks associated with using this logic. You
</p>
<p>will be introduced to how null and research hypotheses are set up, how
</p>
<p>risks of error are assessed, and how levels of statistical significance are
</p>
<p>used to limit this error.
</p>
<p>T h e  D i l e m m a :  M a k i n g  S t a t e m e n t s  
A b o u t  P o p u l a t i o n s  f r o m  S a m p l e  S t a t i s t i c s
</p>
<p>In descriptive statistics, we are concerned with two basic types of distrib-
</p>
<p>utions. One is the distribution of scores in the sample, or the sample
</p>
<p>distribution. The second is the distribution of scores in the population
</p>
<p>from which the sample is drawn. This is referred to as the population
</p>
<p>distribution. One of the fundamental problems in research in criminal
</p>
<p>justice, as in other fields, is that we want to make statements about the
</p>
<p>characteristics of the population distribution, but we generally have in-
</p>
<p>formation only about the distribution of sample scores. For example,
</p>
<p>when we draw a sample of 2,000 voters in an election survey, we are
</p>
<p>not interested per se in how those people will vote. Rather, we examine
</p>
<p>their voting preference to learn something about how all people will
</p>
<p>vote in the election. In statistical terms, we want to use information on
</p>
<p>characteristics of the distribution of sample scores to make statements
</p>
<p>about characteristics of the distribution of population scores.
</p>
<p>126
</p>
<p>this dilemma: it allows the researcher to make statements, or inferences,</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E D I L E M M A 127
</p>
<p>It is important to note at the outset that populations can be defined in
</p>
<p>a number of different ways. There is, for example, the population of the
</p>
<p>entire United States or the population of a particular state. There is the
</p>
<p>population of all prisoners in the United States or the population of pris-
</p>
<p>oners only in a specific state. Although the population of cases is fixed at
</p>
<p>any particular time, actual populations are constantly changing across
</p>
<p>time. For example, we can speak of the population of prisoners on any
</p>
<p>particular day. But every day new people enter prisons and some prison-
</p>
<p>ers are freed. Since the population of prisoners changes every day, the
</p>
<p>population of prisoners at any one time is only a sample of the popula-
</p>
<p>tion of prisoners across a longer period of time&mdash;for example, a year or
</p>
<p>two.
</p>
<p>Statisticians use different symbols to distinguish statistics on a popula-
</p>
<p>tion from statistics on a sample (see Table 6.1). Population statistics, or
</p>
<p>parameters, are defined using Greek letters. For example, the parame-
</p>
<p>ter for the mean in a distribution of population scores is represented by
</p>
<p>� and the standard deviation by �. Sample statistics are represented by
</p>
<p>roman letters. We denote the mean in a distribution of sample scores as
</p>
<p>and the standard deviation as s.
</p>
<p>Why do we study sample statistics if we really want to say something
</p>
<p>about population parameters? It certainly makes more sense to collect in-
</p>
<p>formation on the population if that is what is of interest in the long run.
</p>
<p>In practice, however, it is usually very difficult to gain information on the
</p>
<p>universe, or total group of cases in the population. One reason is sim-
</p>
<p>ply financial. As was pointed out in Chapter 1, to carry out just one sur-
</p>
<p>vey of the U.S. population regarding their attitudes toward crime would
</p>
<p>exhaust the budget of the National Institute of Justice (the major funder
</p>
<p>of research in criminal justice in the United States) for many years. But
</p>
<p>beyond the costs of such studies, there is the problem of their manage-
</p>
<p>ment. A study of an entire population will often demand contact with
</p>
<p>hundreds of thousands or even millions of people. Such an effort is
</p>
<p>likely to be not just expensive, but difficult to manage and time consum-
</p>
<p>ing to complete.
</p>
<p>Because of the difficulty of gaining information on the characteristics
</p>
<p>of an entire population, such parameters are generally unknown. How-
</p>
<p>ever, when a parameter is available, there is no point in drawing statis-
</p>
<p>tics from a sample. In recent years, advances in computer technology
</p>
<p>X
</p>
<p>Representing Population Parameters and Sample Statistics
</p>
<p>MEAN VARIANCE STANDARD DEVIATION
</p>
<p>Sample distribution s 2 s
Population distribution � �2 �
</p>
<p>X
</p>
<p>Table 6.1</p>
<p/>
</div>
<div class="page"><p/>
<p>128 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>and recognition by public officials of the importance of data in making
</p>
<p>policy decisions about the criminal justice system have led to the devel-
</p>
<p>opment of a number of databases that include information on the popu-
</p>
<p>lation of cases. For example, we now have population parameters on
</p>
<p>characteristics of sentencing in the federal courts and basic demographic
</p>
<p>characteristics of offenders held in jails and prisons. Information on the
</p>
<p>population of arrests and emergency calls to the police is routinely col-
</p>
<p>lected and computerized in most cities. One scholar suggests that this
</p>
<p>trend means that criminologists in the future will have to pay less and
</p>
<p>less attention to samples and the problems they create for researchers.1
</p>
<p>However, whatever the future will bring, at present criminal justice re-
</p>
<p>searchers must rely primarily on sample statistics in trying to say some-
</p>
<p>thing about the characteristics of a population.
</p>
<p>Given our reliance on sample statistics, it is important at the outset to
</p>
<p>define how they might differ from population parameters. One obvious
</p>
<p>difference is that sample statistics are generally known&mdash;put differently,
</p>
<p>they can be defined by the researcher in the context of a research study.
</p>
<p>In contrast, parameters are generally unknown, although, as we noted
</p>
<p>above, there is a trend toward development of parameters about major
</p>
<p>issues in criminal justice.
</p>
<p>Even though parameters are often unknown, they are assumed to be
</p>
<p>fixed. By that we mean that there is one true parameter for any measure.
</p>
<p>For example, there is a true mean age at first arrest for the population of
</p>
<p>all criminals in the United States at a specific time. In contrast, sample
</p>
<p>statistics vary from sample to sample. For example, if you were to draw
</p>
<p>10 samples from a population, using exactly the same method each time,
</p>
<p>each sample would likely provide different sample statistics.
</p>
<p>This is illustrated in Table 6.2. Ten random samples of 100 offenders
</p>
<p>were drawn from a population of 1,940 offenders. Sample statistics are
</p>
<p>presented for mean age and number of prior arrests. Although the sample
</p>
<p>statistics obtained are generally similar to the population parameters, each
</p>
<p>sample provides a somewhat different group of estimates, and in some
</p>
<p>cases the differences are relatively large. In the case of sample 10, for ex-
</p>
<p>ample, the average number of arrests for the sample is more than a third
</p>
<p>lower than the population score. In sample 4, the average age is more
</p>
<p>than two years older than the population parameter. This occurs despite
</p>
<p>the fact that we drew each of the samples using the same technique and
</p>
<p>from the same population of scores. You might want to try this yourself
</p>
<p>by drawing a series of samples from your class or dormitory. Using the
</p>
<p>same method, you will almost always obtain different sample statistics.
</p>
<p>1M. Maltz, &ldquo;Deviating from the Mean: The Declining Significance of Significance,&rdquo;
</p>
<p>Journal of Research in Crime and Delinquency 31 (1994): 434&ndash;463.</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E R E S E A R C H H Y P O T H E S I S 129
</p>
<p>This fact is one of the fundamental problems we face in statistics. We
</p>
<p>want to make statements about populations, but we generally must rely
</p>
<p>on sample statistics to do so. If sample statistics vary from sample to
</p>
<p>sample, how can we use them to make reliable statements about the pa-
</p>
<p>rameters associated with a population? Put differently, what is the use of
</p>
<p>most studies in criminal justice, if they are based on samples rather than
</p>
<p>populations? Fortunately, there is an area of statistics that provides us
</p>
<p>with a systematic way to make decisions about population parameters
</p>
<p>based on sample statistics. This area is called statistical inference, and
</p>
<p>in the remaining sections of this chapter we focus on the logic that un-
</p>
<p>derlies statistical inference.
</p>
<p>T h e  R e s e a r c h  H y p o t h e s i s
</p>
<p>Statistical inference begins with the definition of the questions that the
</p>
<p>researcher seeks to answer in a research project. Sometimes research
</p>
<p>questions in criminal justice are focused on specific agencies in the
</p>
<p>criminal justice system. For example, we may want to learn more about
</p>
<p>the police, the courts, or probation services. Other times research ques-
</p>
<p>tions revolve around broad theoretical concerns that can be applied
</p>
<p>across criminal justice agencies. We may, for example, seek to define
</p>
<p>common features of criminal justice programs that lead to a reduction in
</p>
<p>recidivism (reoffending). Sometimes our questions relate to offenders,
</p>
<p>other times to victims of crime or criminal justice agents.
</p>
<p>To answer a research question, we have to set up at least one and
</p>
<p>sometimes several research hypotheses related to it. A hypothesis is a
</p>
<p>proposed answer to our research question that we can then test in the
</p>
<p>Ten Random Samples of 100 Offenders Drawn 
</p>
<p>from a Population of 1,940 Offenders
</p>
<p>MEAN AGE MEAN ARRESTS
</p>
<p>Population 39.7 2.72
Sample 1 41.4 2.55
Sample 2 41.2 2.19
Sample 3 38.8 2.09
Sample 4 42.1 3.45
Sample 5 37.9 2.58
Sample 6 41.1 2.62
Sample 7 39.2 2.79
Sample 8 39.2 2.48
Sample 9 37.8 2.55
Sample 10 37.7 1.72
</p>
<p>Table 6.2</p>
<p/>
</div>
<div class="page"><p/>
<p>130 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>context of a study. Stating a research hypothesis does not mean that we
</p>
<p>assume that the hypothesis is true. Rather, it focuses our research ques-
</p>
<p>tion in such a way that it can be directly examined in the context of a
</p>
<p>study. When the research hypothesis does not indicate a specific type of
</p>
<p>outcome, stating only that there is a relationship or a difference, we say
</p>
<p>that it is a nondirectional hypothesis. However, in those cases where
</p>
<p>a researcher has a very clear idea of what to expect&mdash;based on prior re-
</p>
<p>search evidence and/or theory&mdash;the research hypothesis may be more
</p>
<p>Suppose we are interested in comparing the arrest records of drug-
</p>
<p>involved offenders with those of offenders who do not use drugs. Our
</p>
<p>research hypothesis might be simply that the arrest records of drug-
</p>
<p>behavior among drug-involved offenders, we might want to state a direc-
</p>
<p>tional hypothesis&mdash;that drug-involved offenders have more serious arrest
</p>
<p>records than do non&ndash;drug-involved offenders. One problem with choos-
</p>
<p>ing the latter option is that if we state our research hypothesis as a direc-
</p>
<p>tional hypothesis, we are stating that we are not interested in outcomes
</p>
<p>that fall in the opposite direction. In criminal justice research, we can
</p>
<p>often be surprised by what we learn in a study. Accordingly, researchers
</p>
<p>generally are cautious in defining a directional research hypothesis.
</p>
<p>Having defined a research hypothesis, we want to examine whether it
</p>
<p>is true for the population in which we are interested. For our example of
</p>
<p>drug-involved offenders, if we could collect information about all offend-
</p>
<p>ers, we could simply look at the parameters drawn from our study to see
</p>
<p>whether they support the research hypothesis and, if so, to what degree.
</p>
<p>In this case, we would not need to use the logic of statistical inference.
</p>
<p>We would collect data directly on the population parameters. But ordi-
</p>
<p>narily we cannot collect information on the population parameters and
</p>
<p>must rely on the statistics drawn from a sample in making our decision.
</p>
<p>Our problem is that we cannot come to an absolute conclusion regard-
</p>
<p>ing the research hypothesis because we know that statistics vary from
</p>
<p>sample to sample.
</p>
<p>On the basis of a sample, we can never be sure of the true value of a
</p>
<p>population parameter. Accordingly, we can never be absolutely certain
</p>
<p>as to whether the research hypothesis is true. But does the fact that we
</p>
<p>cannot be sure mean that we cannot come to a reasonable conclusion
</p>
<p>regarding our hypotheses?
</p>
<p>In fact, we often make decisions about hypotheses on the basis of
</p>
<p>samples in our daily lives. For example, let&rsquo;s say that you are deciding
</p>
<p>involved offenders and offenders who do not use drugs are different 
</p>
<p>(a nondirectional hypothesis). But based on prior knowledge of criminal
</p>
<p>precise. In this case, the researcher may specify the nature of the relation-
</p>
<p>ship that is expected. Such a research hypothesis is called a directional
</p>
<p>hypothesis. When a directional hypothesis is used, the researcher
</p>
<p>states at the outset that he or she is interested in a specific type of
</p>
<p>outcome&mdash;for example, that one group has more arrests than another.</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E N U L L H Y P O T H E S I S 131
</p>
<p>whether to sign up for a course taught by an instructor named Professor
</p>
<p>Justice. One issue that you are particularly concerned about is the impact
</p>
<p>the course will have on your grade point average. To make an informed
</p>
<p>decision about the course, you might decide to ask friends of yours who
</p>
<p>took the course last year how Professor Justice grades in comparison to
</p>
<p>others at your college. Although you might not think of them quite in
</p>
<p>this way, your friends represent your sample. In turn, the hypothesis that
</p>
<p>the professor grades differently from other faculty members in your col-
</p>
<p>lege is similar to a research hypothesis.
</p>
<p>If your friends gave a mixed view or generally were unable to say
</p>
<p>whether Professor Justice grades more harshly or more easily than other
</p>
<p>professors, you would likely conclude that the course would not have
</p>
<p>much impact on your grade point average. Put differently, you would
</p>
<p>decide that the research hypothesis is probably false. If most of your
</p>
<p>friends said that the professor is an easy grader or, conversely, that she
</p>
<p>was a hard grader, you would take this as evidence that the research hy-
</p>
<p>pothesis is most likely correct&mdash;that the professor grades differently and
</p>
<p>that the course is likely to have an impact on your grade point average.
</p>
<p>Once you have made the decision that Professor Justice is different
</p>
<p>from others, you are likely to assess how she is different. If your friends
</p>
<p>define the professor as a hard grader, you might decide to avoid the
</p>
<p>course because you fear you would get a lower grade than is usual with
</p>
<p>other professors. If they define the professor as an easy grader, you
</p>
<p>might be encouraged to take the course, with the expectation that your
</p>
<p>grade will be higher than usual.
</p>
<p>In effect, you make a decision about the research hypothesis based
</p>
<p>on information that you draw from your &ldquo;sample&rdquo; of friends. Your confi-
</p>
<p>dence in making a decision will depend greatly on how reliable you be-
</p>
<p>lieve your friends&rsquo; observations to be and to what degree they represent
</p>
<p>other students in the class. This is very similar to the logic we use in
</p>
<p>making statistical inferences from samples to populations. However, in
</p>
<p>statistical inference, we test hypotheses not in reference to the research
</p>
<p>hypothesis but in reference to a type of hypothesis that statisticians call
</p>
<p>the null hypothesis.
</p>
<p>T h e  N u l l  H y p o t h e s i s
</p>
<p>The null hypothesis&mdash;or H0&mdash;gains its name from the fact that it usually
</p>
<p>states that there is no relationship, or no difference. It is the flip side of
</p>
<p>the research hypothesis (H1), which usually posits that there is a relation-
</p>
<p>ship. In the example of the professor&rsquo;s grading, the null hypothesis
</p>
<p>would simply be that &ldquo;there is no difference between the grading of Pro-
</p>
<p>fessor Justice and that of others in the university.&rdquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>132 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>In practice, in statistics, we make decisions about hypotheses in rela-
</p>
<p>tion to the null hypothesis rather than the research hypothesis. This is
</p>
<p>because the null hypothesis states that the parameter in which we are in-
</p>
<p>terested is a particular value. For example, returning to the comparison
</p>
<p>of drug-involved and other offenders, your null hypothesis (H0) might be
</p>
<p>that there is no difference between the two groups in the average num-
</p>
<p>ber of crimes committed, or, put differently, that the difference is equal
</p>
<p>to zero. In the case of Professor Justice&rsquo;s grading, the null hypothesis
</p>
<p>also states that there is no difference, or again that the difference be-
</p>
<p>tween the average grade given by Professor Justice and the average
</p>
<p>grade given by her colleagues is equal to zero.
</p>
<p>In contrast, the research hypothesis is ordinarily not stated in exact
</p>
<p>terms. A number of potential outcomes can satisfy the research hypothe-
</p>
<p>sis. In the example of the professor&rsquo;s grading, any average grade that is
</p>
<p>different from that of other professors in the college is consistent with
</p>
<p>the research hypothesis. But only one result, that the professor&rsquo;s grading
</p>
<p>is the same as that of others, is consistent with the null hypothesis. The
</p>
<p>null hypothesis, accordingly, has the advantage of defining a specific
</p>
<p>value for the population parameter.2
</p>
<p>By stating the null and research hypotheses, we have taken a first
</p>
<p>very important step in making statistical inferences. However, we still
</p>
<p>have the problem of how to make decisions about these hypotheses on
</p>
<p>the basis of sample statistics.
</p>
<p>Whenever we rely on sample statistics to make statements about popu-
</p>
<p>This means that when we test hypotheses in research, we generally
</p>
<p>do not ask whether a hypothesis is true or false. To make such a state-
</p>
<p>ment would require knowledge about the population parameters.
</p>
<p>Rather, we ask whether we can make an inference, or draw a conclu-
</p>
<p>sion, about our hypotheses based on what we know from a sample. In
</p>
<p>statistical inference, we use sample statistics to infer to, or draw conclu-
</p>
<p>sions about, population parameters.
</p>
<p>2Some statisticians prefer to call the research hypothesis the &ldquo;alternative&rdquo; hypothesis,
</p>
<p>because we can, in theory, choose any value as the null hypothesis, and not just the
</p>
<p>value of zero or no difference. The alternative hypothesis, in this case, can be defined
</p>
<p>as all other possible outcomes or values. For example, you could state in your null
</p>
<p>hypothesis that the professor&rsquo;s grades are, on average, five points higher than those of
</p>
<p>other professors in the college. The alternative hypothesis would be that the profes-
</p>
<p>sor&rsquo;s grades are not, on average, five points higher than those of other professors.
</p>
<p>lation parameters, we must always accept that our conclusions are 
</p>
<p>tentative. The only way to come to a definitive conclusion regarding the 
</p>
<p>population parameter is to actually examine the entire population. This 
</p>
<p>happens more and more with criminal justice data today. However, in most 
</p>
<p>research, we are still able to collect information only about sample statistics.</p>
<p/>
</div>
<div class="page"><p/>
<p>R I S K S O F E R R O R I N H Y P O T H E S I S T E S T I N G 133
</p>
<p>In order to understand the logic of making inferences, it will help to
</p>
<p>return to our example of drug-involved offenders. Let&rsquo;s say we are inter-
</p>
<p>ested in the number of crimes that offenders commit in a given year. We
</p>
<p>decide to use arrests as our measure of criminal behavior. We might state
</p>
<p>our null hypothesis as follows: &ldquo;Drug-involved offenders and offenders
</p>
<p>who do not use drugs have, on average, the same number of arrests in a
</p>
<p>given year.&rdquo; To test our hypothesis, we take a sample of drug-involved
</p>
<p>As illustrated earlier in regard to Professor Justice&rsquo;s grading, in every-
</p>
<p>day life we make such decisions through a combination of intuition,
</p>
<p>prior experience, and guesswork. In statistical inference, we take a sys-
</p>
<p>tematic approach to this decision making, which begins with the recog-
</p>
<p>nition that whatever decision we make has a risk of error.
</p>
<p>R i s k s  o f  E r r o r  i n  H y p o t h e s i s  T e s t i n g
</p>
<p>What types of error do we risk when making a decision about a popula-
</p>
<p>pothesis concerning arrests among drug-involved and non&ndash;drug-involved
</p>
<p>offenders, there are only two possible scenarios for the population. In
</p>
<p>the first case, the null hypothesis is true, meaning that there is no differ-
</p>
<p>ence in average number of arrests between offenders in the population
</p>
<p>who use drugs and those who do not. Alternatively, the null hypothesis
</p>
<p>may be false, meaning that there is a difference in the population be-
</p>
<p>Based on our sample statistic, we can, as well, come to only two pos-
</p>
<p>sible conclusions regarding the null hypothesis. We can reject the null
</p>
<p>hypothesis and infer that there is a difference in the average numbers of
</p>
<p>arrests of drug-involved and other offenders in the population. Alterna-
</p>
<p>tively, we can fail to reject the null hypothesis and state that our sample
</p>
<p>does not provide sufficient evidence to conclude that there is a differ-
</p>
<p>ence in the average numbers of arrests of drug-involved offenders and
</p>
<p>offenders in the population who do not use drugs.
</p>
<p>tion parameter from a sample? A simple way to examine this question 
</p>
<p>is to compare the potential decisions that can be made about a null 
</p>
<p>hypothesis with the value of the population parameter. For our null hy-
</p>
<p>do offenders who do not use drugs.
</p>
<p>offenders have, on average, either fewer or more arrests in a year than 
</p>
<p>tween drug-involved and other offenders. In this case, drug-involved 
</p>
<p>offenders and another sample of offenders who do not use drugs. We find
</p>
<p>that drug-involved offenders in our sample have a mean of five arrests
</p>
<p>per year, whereas offenders who do not use drugs have a mean of three
</p>
<p>arrests per year. Should we reject the null hypothesis? Should we conclude
</p>
<p>that there is a difference in the numbers of arrests in the population
</p>
<p>based on results from our sample?</p>
<p/>
</div>
<div class="page"><p/>
<p>134 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>If we cross these two sets of possibilities, we define four possible situ-
</p>
<p>ations, as represented in Figure 6.1. Two of these are desirable, because
</p>
<p>they suggest that our decision about the null hypothesis is consistent
</p>
<p>with the population parameter. In one case (box 1), we fail to reject the
</p>
<p>null hypothesis, and it is in fact true. In the second (box 4), we reject the
</p>
<p>null hypothesis on the basis of our sample results, and the null hypothe-
</p>
<p>sis is false.
</p>
<p>In the remaining situations, however, our decisions are not consistent
</p>
<p>with the population parameter. In one (box 2), we fail to reject the null
</p>
<p>hypothesis on the basis of our sample statistic, but it is in fact false. In
</p>
<p>this case, we have made what statisticians call a Type II (or beta)
</p>
<p>error. A Type II error would occur in our example of arrests among
</p>
<p>offenders if we did not reject the null hypothesis on the basis of our
</p>
<p>sample results when in fact the average numbers of arrests for drug-
</p>
<p>involved and other offenders were different in the population to which
</p>
<p>we want to infer. We make a Type I (or alpha) error (see box 3)
</p>
<p>when we reject the null hypothesis on the basis of sample statistics but
</p>
<p>H0 is true. In this case, we infer from our sample that drug offenders are
</p>
<p>different from offenders who do not use drugs, when in fact they are
</p>
<p>similar in the population.
</p>
<p>Whenever we make a decision about a population parameter from a
</p>
<p>sample statistic, we risk one of these two types of statistical error. If we
</p>
<p>Box 1
✔ 
</p>
<p>Box 3
�
</p>
<p>Type I
error
</p>
<p>Box 2
�
</p>
<p>Type II
error
</p>
<p>Box 4
✔
</p>
<p>H0 = True H0 = False
</p>
<p>Population
</p>
<p>Reject H0
</p>
<p>Decision
</p>
<p>Fail to Reject H0
</p>
<p>Types of Error in a Statistical TestFigure 6.1</p>
<p/>
</div>
<div class="page"><p/>
<p>S T A T I S T I C A L L E V E L S O F S I G N I F I C A N C E 135
</p>
<p>fail to reject the null hypothesis, there is always the possibility that we
</p>
<p>have failed to reject it when it was false (Type II error). If we reject the
</p>
<p>null hypothesis, there is always the possibility that we have rejected it
</p>
<p>when it was true (Type I error). Although we cannot avoid the possibil-
</p>
<p>ity of error when we study samples, we can decide at the outset how
</p>
<p>R i s k s  o f  E r r o r  a n d  S t a t i s t i c a l  L e v e l s  o f  S i g n i f i c a n c e
</p>
<p>In statistical inference, we assess the risk of making a wrong decision
</p>
<p>about the population parameter in reference to Type I error. This is why
</p>
<p>it is very important in statistical inference that we use the phrase &ldquo;reject&rdquo;
</p>
<p>or &ldquo;fail to reject&rdquo; the null hypothesis, rather than the simpler statement
</p>
<p>that we &ldquo;reject&rdquo; or &ldquo;accept&rdquo; the null hypothesis. Type I error is concerned
</p>
<p>with rejection of the null hypothesis when it is true. It does not refer di-
</p>
<p>rectly to the risk of accepting the null hypothesis when it is false (Type II
</p>
<p>error). This latter problem will be the focus of a discussion in Chapter
</p>
<p>make a decision either to &ldquo;reject&rdquo; or to &ldquo;fail to reject&rdquo; the null hypothesis
</p>
<p>on the basis of the amount of Type I error we are willing to risk.
</p>
<p>We define the amount of Type I error we are willing to risk as the
</p>
<p>significance level of a test of statistical significance. In a test of statis-
</p>
<p>tical significance, we make a decision to reject or to fail to reject the null
</p>
<p>hypothesis on the basis of a sample statistic. The significance criterion,
</p>
<p>or level, of a test of statistical significance is ordinarily represented by the
</p>
<p>symbol �. The estimate of the risk of Type I error that is associated with
</p>
<p>rejecting the null hypothesis in a test of statistical significance (based on
</p>
<p>a sample statistic) is called the observed significance level and is ordi-
</p>
<p>narily represented by the symbol p. In statistical inference, we first iden-
</p>
<p>tify the amount of Type I error we are willing to risk, or the significance
</p>
<p>level of a test. We then estimate the observed significance level from our
</p>
<p>sample statistics. Finally, we compare the observed significance level
</p>
<p>gained from our study with the criterion significance level we set at the
</p>
<p>outset of our test of statistical significance. If the observed significance
</p>
<p>level is less than the significance criterion, or level, that we set at the
</p>
<p>outset of the test, we reject the null hypothesis. In the next chapter, we
</p>
<p>examine how statisticians estimate the observed significance level of a
</p>
<p>test. At this juncture, it is important to consider how we decide on the
</p>
<p>amount of Type I error we are willing to risk. How do we choose the
</p>
<p>significance level in a test of statistical significance?
</p>
<p>hypotheses.
</p>
<p>much risk or error we are willing to take in making decisions about 
</p>
<p>21. For now, it is important to remember that in statistical inference we</p>
<p/>
</div>
<div class="page"><p/>
<p>136 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>If we are willing to take a good deal of risk of Type I error, we set a
</p>
<p>very lenient significance level. This means that we are willing to reject
</p>
<p>the null hypothesis on the basis of our sample statistic, even if the risk of
</p>
<p>a Type I error is fairly large. If we set a very strict significance level, this
</p>
<p>means that we are unwilling to reject the null hypothesis unless we are
</p>
<p>fairly certain that our decision is correct.
</p>
<p>If we return to the example of Professor Justice&rsquo;s grading, the impor-
</p>
<p>tance of Type I error in making statistical inferences will become clearer.
</p>
<p>Let&rsquo;s say that every one of the friends you ask reports that the professor
</p>
<p>is a much easier grader than other professors at your college. In this
</p>
<p>case, you would probably conclude that the observed significance level,
</p>
<p>or risk of a Type I error, in your study was very small. It is unlikely that
</p>
<p>all of your friends would say that the professor was an easy grader if she
</p>
<p>was in reality very similar to others in the college. Of course, your
</p>
<p>friends could provide a mistaken view of the professor&rsquo;s grading habits.
</p>
<p>But you would probably assume that this is not very likely.
</p>
<p>But what if your friends provided you with a mixed view of Professor
</p>
<p>Justice&rsquo;s grading? What if 40% of your friends say that Professor Justice
</p>
<p>grades similarly to other professors and 60% say that she is easier? Would
</p>
<p>you be so confident in rejecting the null hypothesis with these results?
</p>
<p>Overall, the majority of your friends still say the professor is a relatively
</p>
<p>easy grader. But in this case there is a substantial group reporting that
</p>
<p>she grades much as other professors do. If you set a strict significance
</p>
<p>level, you might not be willing to reject the null hypothesis based on
</p>
<p>your observations (your observed significance level), and you might sim-
</p>
<p>ply conclude that there is not enough evidence, on the basis of your
</p>
<p>sample of friends, to say that Professor Justice grades differently.
</p>
<p>It might seem at first that we would want to be fairly lenient in setting
</p>
<p>our significance level. Often, in defining the research hypothesis, we are
</p>
<p>expressing what we believe to be true. Why, then, would we want to
</p>
<p>make it difficult to reject the null hypothesis? By rejecting the null hy-
</p>
<p>pothesis that there is no difference, we are led to infer that the research
</p>
<p>hypothesis is correct. This would seem in our best interest. In fact, by
</p>
<p>convention we set a fairly strict significance level. In order to reject the
</p>
<p>null hypothesis, we are expected to provide convincing evidence that
</p>
<p>our conclusions reflect the true population parameters.
</p>
<p>What is &ldquo;convincing evidence&rdquo;? How much risk of a Type I error
</p>
<p>should we be willing to take in a test of statistical significance? In
</p>
<p>criminal justice, and in most of the social sciences, a 5% level of statis-
</p>
<p>tical significance is generally considered rigorous enough for tests of
</p>
<p>hypotheses. This means that if the observed significance level of our
</p>
<p>test is greater than 0.05, we will fail to reject the null hypothesis. If the
</p>
<p>observed significance level is less than 0.05, we will reject the null
</p>
<p>hypothesis.</p>
<p/>
</div>
<div class="page"><p/>
<p>D E P A R T I N G F R O M C O N V E N T I O N A L C R I T E R I A 137
</p>
<p>D e p a r t i n g  f r o m  C o n v e n t i o n a l  S i g n i f i c a n c e  C r i t e r i a
</p>
<p>In statistics, as in everyday life, it is simplest to follow the accepted con-
</p>
<p>research may be better served by departing from established norms.
</p>
<p>Sometimes this is the case because our criterion is not cautious enough
</p>
<p>for the issue we are examining.
</p>
<p>For example, a criminal justice agency making a decision about its
</p>
<p>policies may want you to use a stricter standard than you would ordinar-
</p>
<p>ily apply to your research. This may be the case if your conclusions
</p>
<p>could lead to expensive or time-consuming changes in the structure or
</p>
<p>activities of the agency. The agency will likely not want to make such
</p>
<p>changes unless you are very confident that your conclusions are correct.
</p>
<p>In this situation, you might decide to set your significance level at 0.01 or
</p>
<p>0.001, meaning that you are willing to reject the null hypothesis only if
</p>
<p>the observed significance level of a test is less than 1% or less than 0.1%.
</p>
<p>Sometimes a researcher may decide to use a more lenient significance
</p>
<p>level than 5%. This ordinarily occurs when the researcher is particularly
</p>
<p>concerned with a Type II error rather than a Type I error. In the Min-
</p>
<p>neapolis hot spots experiment, for example, which evaluated the impact
</p>
<p>of police patrol on street blocks with high crime activity, the principal in-
</p>
<p>vestigators discussed the dangers of a Type II error at the outset of their
</p>
<p>study.3 They argued that conventional significance criteria might be too
</p>
<p>strict for assessing the effectiveness of new police initiatives. Failure to
</p>
<p>reject the null hypothesis of no program impact in this case, if it were
</p>
<p>false, would lead the police not to pursue a potentially effective new
</p>
<p>method of police patrol. The principal investigators, accordingly, de-
</p>
<p>cided to use a 0.10, rather than a 0.05, significance level.
</p>
<p>Why would a concern with a Type II error lead us to change the sig-
</p>
<p>nificance level of our test of hypotheses? As we noted earlier, the signifi-
</p>
<p>cance level is based on a Type I error, not a Type II error. However, the
</p>
<p>two types of statistical error are related. When we increase the risk of a
</p>
<p>Type I error, we reduce the risk of a Type II error. When we decrease
</p>
<p>the risk of a Type I error, we increase the risk of a Type II error. This
</p>
<p>3See Lawrence Sherman and David Weisburd, &ldquo;General Deterrent Effects of Police
</p>
<p>Patrol in Crime &lsquo;Hot Spots&rsquo;: A Randomized Study,&rdquo; Justice Quarterly 12:4 (1995):
</p>
<p>625&ndash;648.
</p>
<p>ventions. Accordingly, most criminal justice researchers apply the 5% 
</p>
<p>significance level fairly automatically to the research questions they 
</p>
<p>consider. The problem with this approach is that the purposes of the
</p>
<p>reject the null hypothesis in your study.
</p>
<p>Whenever you use a significance level more stringent than 5%, it is 
</p>
<p>important to explain clearly why you have chosen to make it harder to </p>
<p/>
</div>
<div class="page"><p/>
<p>138 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>relationship is an obvious one, because setting a stricter significance
</p>
<p>level (by decreasing the risk of a Type I error) naturally makes it less
</p>
<p>likely that you will reject the null hypothesis. Similarly, when we make it
</p>
<p>easier to reject the null hypothesis by increasing the risk of a Type I
</p>
<p>error, it is that much harder in practice to fail to reject the null hypothe-
</p>
<p>sis (and therefore to make a Type II error). The relationship between
</p>
<p>Type I and Type II error is easy to understand, but it is not directly pro-
</p>
<p>cept of &ldquo;statistical power,&rdquo; other factors in addition to the significance
</p>
<p>level of a study affect the risk of a Type II error.
</p>
<p>You should always consider carefully the implications of risks of error in
</p>
<p>your research before setting the significance level of a test of statistical sig-
</p>
<p>nificance. Even though you are likely in the end to rely on the conventional
</p>
<p>norm of 5%, there are, as discussed above, some cases for which you might
</p>
<p>want to consider stricter or more lenient levels of significance. If you do
</p>
<p>choose a level other than 0.05, you must explain to your audience the fac-
</p>
<p>tors that led you to depart from common practice in criminal justice.
</p>
<p>The level of significance in a study should be defined at the outset,
</p>
<p>and not after your results are in. If you wait until you have your sample
</p>
<p>data, there will always be the temptation to adjust your significance level
</p>
<p>to fit your sample statistics. This is particularly important if you decide to
</p>
<p>use more stringent or more lenient criteria. If you use a more lenient cri-
</p>
<p>terion, others might argue that you have made that decision in order to
</p>
<p>allow rejection of the null hypothesis (and thus support for the research
</p>
<p>hypothesis). If you use a stricter criterion, others might argue that you
</p>
<p>are trying to avoid rejecting the null hypothesis. In many funded re-
</p>
<p>search studies, researchers specify in their original proposals the signifi-
</p>
<p>cance levels that they intend to apply to tests of hypotheses in order to
</p>
<p>prevent such criticism later on.
</p>
<p>In this chapter, we discussed the logic underlying statistical inference.
</p>
<p>In the coming chapters, we will examine how statisticians define the risk
</p>
<p>of Type I error associated with a specific outcome in a study, or the ob-
</p>
<p>served significance level of a test. We also detail how such tests are ap-
</p>
<p>plied to different types of statistics. You should not expect to have a full
</p>
<p>understanding of statistical inference at this point. We have only begun
</p>
<p>to develop these ideas and will return to them again and again.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>In descriptive statistics we are concerned with two types of distribu-
</p>
<p>tions. The sample distribution is the distribution of scores in the sam-
</p>
<p>ple. The population distribution is the distribution of scores in the
</p>
<p>portional. As we will discuss in Chapter 21 when we examine the con-</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S U M M A R Y 139
</p>
<p>population from which the sample is drawn. Population statistics are
</p>
<p>known as parameters, and they have symbols different from those
</p>
<p>which you have encountered so far. The mean in a population distribu-
</p>
<p>tion is represented by � and the standard deviation by �. Parameters
</p>
<p>are assumed to be fixed and are generally unknown. Sample statistics
</p>
<p>are by definition known, but they vary from sample to sample. Statisti-
</p>
<p>cians are faced with a fundamental dilemma in that they are usually in-
</p>
<p>terested in making statements about population parameters, but they
</p>
<p>generally study sample statistics. Statistical inference provides a solu-
</p>
<p>tion to this dilemma.
</p>
<p>Statistical inference begins with the definition of the research hy-
</p>
<p>pothesis and the null hypothesis. These hypotheses are set up by the
</p>
<p>researcher to answer the broader research question. The research hy-
</p>
<p>pothesis is the proposed answer to a specific research question. When
</p>
<p>the research hypothesis does not indicate a specific type of outcome,
</p>
<p>stating only that there is a relationship or a difference, we say that it is a
</p>
<p>nondirectional hypothesis. When the research hypothesis identifies
</p>
<p>the nature of the relationship that is expected, it is called a directional
</p>
<p>hypothesis. The null hypothesis generally posits that there is no such
</p>
<p>relationship or no difference. The null hypothesis is stated in exact
</p>
<p>terms. The research hypothesis encompasses a range of possible alterna-
</p>
<p>Given the difficulties involved in collecting information on an entire
</p>
<p>population, we are forced to work with samples. The tools of statistical
</p>
<p>inference enable us to infer from a sample to a population by identifying
</p>
<p>the risk of making a mistaken decision and determining the amount of
</p>
<p>risk we are prepared to take. Two possible errors can be made when
</p>
<p>making decisions about a population from a sample. A researcher who
</p>
<p>rejects the null hypothesis when it is in fact true has made a Type I
</p>
<p>error. A researcher who fails to reject the null hypothesis when it is in
</p>
<p>fact false has made a Type II error.
</p>
<p>In a test of statistical significance, we make a decision to reject or
</p>
<p>to fail to reject the null hypothesis on the basis of a sample statistic. The
</p>
<p>significance level defines the risk of Type I error that a researcher is
</p>
<p>willing to take in a test of statistical significance. The risk of Type I error
</p>
<p>associated with a specific sample statistic is the observed significance
</p>
<p>level of a test. A commonly accepted standard significance level is 5%,
</p>
<p>but researchers may choose to set a lower level if they want it to be
</p>
<p>more difficult to reject the null hypothesis&mdash;or a higher level if they want
</p>
<p>to make it easier. A researcher who wishes to depart from the accepted
</p>
<p>standard should explain the reasons for such a decision at the outset.
</p>
<p>Decreasing the risk of a Type I error increases the risk of a Type II error,
</p>
<p>and vice versa.
</p>
<p>hypothesis.
</p>
<p>tives. It is thus easier to focus decisions on whether to reject the null </p>
<p/>
</div>
<div class="page"><p/>
<p>140 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>K e y  T e r m s
</p>
<p>directional hypothesis A research hy-
</p>
<p>pothesis that indicates a specific type of
</p>
<p>outcome by specifying the nature of the re-
</p>
<p>lationship that is expected.
</p>
<p>nondirectional hypothesis A research
</p>
<p>hypothesis that does not indicate a specific
</p>
<p>type of outcome, stating only that there is a
</p>
<p>relationship or a difference.
</p>
<p>null hypothesis A statement that reduces
</p>
<p>the research question to a simple assertion
</p>
<p>to be tested by the researcher. The null hy-
</p>
<p>pothesis normally suggests that there is no
</p>
<p>relationship or no difference.
</p>
<p>observed significance level The risk of
</p>
<p>Type I error associated with a specific sam-
</p>
<p>ple statistic in a test. When the observed
</p>
<p>significance level is less than the criterion
</p>
<p>significance level in a test of statistical sig-
</p>
<p>nificance, the researcher will reject the null
</p>
<p>hypothesis.
</p>
<p>parameter A characteristic of the pop-
</p>
<p>ulation&mdash;for example, the mean number 
</p>
<p>of previous convictions for all U.S.
</p>
<p>prisoners.
</p>
<p>population The universe of cases that the
</p>
<p>researcher seeks to study. The population
</p>
<p>of cases is fixed at a particular time (e.g.,
</p>
<p>the population of the United States). How-
</p>
<p>ever, populations usually change across
</p>
<p>time.
</p>
<p>population distribution The frequency
</p>
<p>distribution of a particular variable within a
</p>
<p>population.
</p>
<p>research hypothesis The antithesis of the
</p>
<p>null hypothesis. The statement normally
</p>
<p>answers the initial research question by
</p>
<p>suggesting that there is a relationship or a
</p>
<p>difference.
</p>
<p>sample A set of actual observations or
</p>
<p>cases drawn from a population.
</p>
<p>sample distribution The frequency distri-
</p>
<p>bution of a particular variable within a
</p>
<p>sample drawn from a population.
</p>
<p>sample statistic A characteristic of a sam-
</p>
<p>ple&mdash;for example, the mean number of
</p>
<p>previous convictions in a random sample
</p>
<p>of 1,000 prisoners.
</p>
<p>significance level The level of Type I
</p>
<p>error a researcher is willing to risk in a sta-
</p>
<p>tistical test of significance.
</p>
<p>statistical inference The process of mak-
</p>
<p>ing generalizations from sample statistics to
</p>
<p>population parameters.
</p>
<p>test of statistical significance A test in
</p>
<p>which a researcher makes a decision to re-
</p>
<p>ject or to fail to reject the null hypothesis
</p>
<p>on the basis of a sample statistic.
</p>
<p>Type I error Also known as alpha error.
</p>
<p>The mistake made when a researcher re-
</p>
<p>jects the null hypothesis on the basis of a
</p>
<p>sample statistic (i.e., claiming that there is a
</p>
<p>relationship) when in fact the null hypothe-
</p>
<p>Type II error Also known as beta error.
</p>
<p>The mistake made when a researcher fails
</p>
<p>to reject the null hypothesis on the basis of
</p>
<p>a sample statistic (i.e., failing to claim that
</p>
<p>there is a relationship) when in fact the
</p>
<p>null hypothesis is false (i.e., there actually
</p>
<p>is a relationship).
</p>
<p>universe The total population of cases.
</p>
<p>by means of a study.
</p>
<p>researcher hopes to be able to answer 
</p>
<p>research question The question the 
</p>
<p>relationship in the population).
</p>
<p>sis is true (i.e., there is actually no such </p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 141
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>H0 Null hypothesis
</p>
<p>H1 Research hypothesis
</p>
<p>� Significance level of a test
</p>
<p>� Population mean
</p>
<p>p Observed significance level of a test
</p>
<p>�2 Population variance
</p>
<p>E x e r c i s e s
</p>
<p>6.1 For each of the following random samples, describe the population to
which the results could be generalized:
</p>
<p>a. Subscribers to a magazine on hunting are asked about gun legislation.
</p>
<p>b. Youth aged 15 to 19 years in the United States are asked about per-
sonal drug use.
</p>
<p>c. Registered voters in Los Angeles are asked whom they will vote for
in the next election for mayor.
</p>
<p>d. Visitors to a domestic violence shelter are assessed for psychologi-
cal distress.
</p>
<p>e. Members of the National Organization for Women (NOW) are
asked about sexual harassment in the workplace.
</p>
<p>f. Judges in New Jersey are asked about their sentencing philosophy.
</p>
<p>6.2 A foundation sponsored a review of all studies carried out over the
past 15 years into the link between smoking and juvenile delinquency.
Eric, a criminologist commissioned by the foundation, unearthed five
studies that sought to determine at what age delinquents who smoke
began their habit. The five studies, conducted at the same time, use
identical sampling techniques and sample sizes and draw their sam-
ples from a fixed database of delinquents. The mean age, however, is
different for each of the samples:
</p>
<p>Study sample no. 1: mean age � 12.2
</p>
<p>Study sample no. 2: mean age � 11.6
</p>
<p>Study sample no. 3: mean age � 14.0
</p>
<p>Study sample no. 4: mean age � 11.3
</p>
<p>Study sample no. 5: mean age � 12.8
</p>
<p>Overall computed mean � 12.38</p>
<p/>
</div>
<div class="page"><p/>
<p>142 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>a. During Eric&rsquo;s presentation, one member of the foundation asks him
to explain how it can be that each of the study samples produced a
different result: Does this mean that there was something wrong
with the sampling techniques used? How should Eric respond?
</p>
<p>b. Another foundation member also seems confused. She asks if the
overall computed mean is the mean age of the population. How
should Eric respond?
</p>
<p>6.3 A researcher collects data on the families of 20 delinquent and 20
nondelinquent children from the records of a school and checks how
many children from each group come from broken homes.
</p>
<p>a. State the null hypothesis.
</p>
<p>b. State a nondirectional research hypothesis.
</p>
<p>c. State a directional research hypothesis.
</p>
<p>6.4 A researcher is interested in whether electronic monitoring of offend-
ers on probation is an effective means of reducing crime. To test for
an effect of electronic monitoring, probationers are randomly assigned
either to a monitored group or to a control group that has no elec-
tronic monitoring.
</p>
<p>a. State the null hypothesis.
</p>
<p>b. State a nondirectional research hypothesis.
</p>
<p>c. State a directional research hypothesis.
</p>
<p>6.5 A study published in a distinguished journal reported the results of a
series of tests carried out on 50 convicted burglars. One of the claims
of the investigators is that the average IQ of convicted burglars is 120.
</p>
<p>a. From the following list of options, choose an appropriate null hy-
pothesis and research hypothesis for testing this claim.
</p>
<p>IQ � 120 IQ � 120 IQ � 120 IQ � 120
</p>
<p>b. Explain your choice of null and research hypotheses.
</p>
<p>6.6 A gang of criminals is planning to rob a supermarket. Eddy, the gang
leader, reports that he &ldquo;staked the store out&rdquo; the day before and saw the
store detective going for a 15-minute coffee break at 9.15 A.M. He sug-
gests that this would be the best time to strike. Clive, a gang member,
thinks that this plan is too risky&mdash;how do they know that the detective
takes his break at the same time each day? Eddy, who is desperate for
the money, thinks that the plan is safe enough and wants to carry out
the robbery the next day. After an argument, they agree to compromise
and watch the supermarket for three more days. On each of the three
days, the store detective indeed takes his 15-minute break at 9:15 A.M.
The gang decides to go ahead with the robbery on the fourth day.
</p>
<p>The robbers can be seen as having set themselves a research ques-
tion and having made a statistical decision based on a simple study.</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 143
</p>
<p>a. How would you frame the robbers&rsquo; null hypothesis and research
hypothesis?
</p>
<p>b. Based on these hypotheses, what is their decision?
</p>
<p>c. How would the robbers make a Type I error? How would the rob-
bers make a Type II error? What type of statistical error ought the
robbers to be most concerned with making? Explain why.
</p>
<p>d. How does the argument between Eddy and Clive relate to the con-
cept of statistical significance?
</p>
<p>6.7 The government wishes to launch a pre-Christmas advertising cam-
paign warning about the dangers of drunk driving. It suspects that dri-
vers aged 18 to 21 are most likely to drive while under the influence
of alcohol and is considering targeting the campaign specifically at this
age group. A preliminary study gathers data on the ages of drunk dri-
vers apprehended in a particular district over a six-month period.
</p>
<p>b. How might the government make a Type I error?
</p>
<p>c. How might the government make a Type II error?
</p>
<p>d. The government accepts that targeting this specific age group in the
advertising campaign will not cost any extra money. There is a feel-
ing that the new campaign will be &ldquo;worth a try,&rdquo; even if the study
doesn&rsquo;t find enormous differences between the offending rate of
18- to 21-year-olds and that of other ages. How should these con-
siderations affect the researchers&rsquo; decision on what level of signifi-
cance to set?
</p>
<p>6.8 The head of the police force in the city of Cheadle suspects that in-
creasing the pay of his officers might increase their efficiency. A police
researcher is assigned to test whether there is a difference between
the crime-solving rates of a group of detectives who have been ran-
domly awarded pay raises and a control group of detectives who have
not been awarded pay raises. In writing up his report, the researcher
concludes as follows:
</p>
<p>The results show that the observed significance level is 0.14, mean-
ing that rejecting the null hypothesis would run a 14% risk of a
Type I error. Although a 5% significance level is considered stan-
dard, in light of the potential benefits of salary increases for crime
control rates, a higher 15% threshold is justified here, and the H0
may therefore be rejected.
</p>
<p>a. What is the null hypothesis to which the researcher refers?
</p>
<p>b. Explain why the researcher&rsquo;s statistical reasoning is problematic.
</p>
<p>research hypothesis is directional or nondirectional.
a. What are the null and research hypotheses? Explain why the </p>
<p/>
</div>
<div class="page"><p/>
<p>144 C H A P T E R S I X :  T H E L O G I C O F S T A T I S T I C A L I N F E R E N C E
</p>
<p>6.9 A study explored whether there is a link between male aggression and
climate. The researcher recorded her results as follows: 5% signifi-
cance level set, H0 could not be rejected.
</p>
<p>a. Explain these results in plain English.
</p>
<p>b. Why is it important for the researcher to set the significance level at
the beginning of the research and not at the end?
</p>
<p>6.10 A private research foundation claims that increased regulation of
handguns would reduce homicides in the United States. To examine
this relationship, the foundation funds a study to assess the impact of
handgun legislation on homicides in four states that recently passed
laws restricting handgun ownership. The foundation expects that the
reduced availability of handguns following the change in law will re-
duce the number of opportunities for lethal violence. The researcher
collects data on homicides in the four states for one year before and
one year after the change in handgun laws.
</p>
<p>b. Explain how the researcher could make a Type I error.
</p>
<p>c. Explain how the researcher could make a Type II error.
</p>
<p>dation wants the researcher to increase the significance level of the
study to 10% and reject the null hypothesis. Should the researcher
increase the significance level of the study? Why?
</p>
<p>6.11 A group of researchers at a private think tank claims that the increased
use of incarceration in the United States has not been harmful to the
social fabric of the country. To support this claim, the researchers con-
duct a study looking at rates of incarceration and rates of divorce in
all fifty states for a 20-year period. The group of researchers tests for a
relationship between rates of incarceration and rates of divorce, but
does not expect to find a relationship between these two variables.
</p>
<p>b. Explain how the research group could make a Type I error.
</p>
<p>c. Explain how the research group could make a Type II error.
</p>
<p>d. The observed significance level developed in their study is 0.03.
The research group initially set a 5% risk of Type I error. One
member of the research group suggests that they simply decrease
the significance level of the study to 1% and fail to reject the null
hypothesis. Should the research group decrease the significance
level of the study? Why?
</p>
<p>a. What are the null and research hypotheses? Explain why the 
research hypothesis is directional or nondirectional.
</p>
<p>d. The results show an observed significance level of 0.06. The 
researcher wants to conclude that the null hypothesis cannot be 
rejected based on a 5% risk of Type I error. An official from the foun-
</p>
<p>research hypothesis is directional or nondirectional.
a. What are the null and research hypotheses? Explain why the </p>
<p/>
</div>
<div class="page"><p/>
<p>Defining the Observed Significance Level
</p>
<p>of a Test: A Simple Example Using 
</p>
<p>the Binomial Distribution
</p>
<p>What Role Do They Play in Inferential Statistics?
</p>
<p>How Does One Calculate the Probability of a Given Outcome?
</p>
<p>C h a p t e r  s e v e n
</p>
<p>S a m p l i n g  d i s t r i b u t i o n s
</p>
<p>P r o b a b i l i t i e s  a n d  p r o b a b i l i t y  d i s t r i b u t i o n s
</p>
<p>T h e  b i n o m i a l  d i s t r i b u t i o n
</p>
<p>What are They?
</p>
<p>What are Probability Distributions?
</p>
<p>How are They Used?
</p>
<p>What is It?
</p>
<p>How is It Calculated?
</p>
<p>What are Its Characteristics?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_7,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>WHEN WE MAKE INFERENCES to a population, we rely on a statistic in
our sample to make a decision about a population parameter. At the
</p>
<p>heart of our decision is a concern with Type I error. Before we reject
</p>
<p>our null hypothesis, we want to be fairly confident that it is in fact false
</p>
<p>for the population we are studying. For this reason, we want the ob-
</p>
<p>served risk of a Type I error in a test of statistical significance to be as
</p>
<p>small as possible. But how do statisticians calculate that risk? How do
</p>
<p>they define the observed significance level associated with the outcome
</p>
<p>of a test?
</p>
<p>The methods that statisticians use for calculating the observed sig-
</p>
<p>nificance level of a test of statistical significance vary depending on
</p>
<p>the statistics examined. Sometimes these methods are very complex.
</p>
<p>But the overall logic that underlies these calculations is similar, irre-
</p>
<p>spective of the statistic used. Thus, we can take a relatively simple
</p>
<p>example and use it as a model for understanding how the observed
</p>
<p>significance level of a test is defined more generally in statistics. This
</p>
<p>is fortunate for us as researchers, because it means that we do not
</p>
<p>have to spend all of our time developing complex calculations to de-
</p>
<p>fine risks of error. Once we understand how risks of error are defined
</p>
<p>for one problem, we can let statisticians calculate the risks for other
</p>
<p>more complex problems. Our concern is not with the calculations
</p>
<p>themselves, but with understanding the general logic that underlies
</p>
<p>them.
</p>
<p>We begin this chapter by discussing a very simple decision. When
</p>
<p>should we begin to suspect that a coin used in a coin toss is unfair or bi-
</p>
<p>ased? Ordinarily, we might come to a conclusion based on common
</p>
<p>sense or intuition. In statistics, we take a more systematic approach, rely-
</p>
<p>ing on the logic of hypothesis testing and a type of distribution called a
</p>
<p>sampling distribution. Using this example of the coin toss and a sam-
</p>
<p>pling distribution called the binomial distribution, we illustrate how sta-
</p>
<p>tisticians use probability theory to define the observed significance level,
</p>
<p>or risk of Type I error, for a test of statistical significance.
</p>
<p>146</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E F A I R C O I N T O S S 147
</p>
<p>T h e  F a i r  C o i n  T o s s
</p>
<p>Imagine that you and your friends play a volleyball game each week
</p>
<p>against a group of criminal justice students from another school. You al-
</p>
<p>ways begin the game with a coin toss to decide who will serve the ball
</p>
<p>first. Your opponents bring an old silver dollar, which you have agreed
</p>
<p>to use for the toss. They choose heads and continue to choose heads
</p>
<p>each time you play. At first, this does not seem like a problem. However,
</p>
<p>each week you play, the coin comes up heads and they serve the ball.
</p>
<p>Suppose that this happened for four straight weeks. Would you begin
</p>
<p>to become suspicious? What if it went on for six weeks? How many
</p>
<p>times in a row would they have to win the coin toss before you and
</p>
<p>your team accused them of cheating? Would they have to win for ten or
</p>
<p>twenty weeks? You might worry about accusing them too quickly, be-
</p>
<p>cause you know that even if the coin is fair it sometimes happens that
</p>
<p>someone is lucky and just keeps on winning. You would want to be
</p>
<p>fairly certain that the coin was biased before concluding that something
</p>
<p>was wrong and taking some action.
</p>
<p>In everyday life, you are likely to make this decision based on intu-
</p>
<p>ition or prior experience. If you ask your classmates, each one is likely
</p>
<p>to come up with a slightly different number of coin tosses before he or
</p>
<p>she would become suspicious. Some students may be willing to tolerate
</p>
<p>only four or five heads in a row before concluding that they have
</p>
<p>enough evidence to accuse their opponents of cheating. Others may be
</p>
<p>unwilling to reach this conclusion even after ten or fifteen tosses that
</p>
<p>come up heads. In part, the disagreement comes from personality differ-
</p>
<p>ences. But more important, guesswork or common sense does not give
</p>
<p>you a common yardstick for deciding how much risk you take in coming
</p>
<p>to one conclusion or another.
</p>
<p>Sampling Distributions and Probability Distributions
</p>
<p>Statistical inference provides a more systematic method for making deci-
</p>
<p>sions about risk. The coin toss can be thought of as a simple test of statisti-
</p>
<p>cal significance. The research hypothesis is that the coin is biased in favor
</p>
<p>of your opponents. The null hypothesis is that the coin is fair. Each toss of
</p>
<p>the coin is an event that is part of a sample. If you toss the coin ten times,
</p>
<p>you have a sample of ten tosses. Recall from Chapter 6 that Type I error
</p>
<p>is the error of falsely rejecting the null hypothesis that the coin is fair. If
</p>
<p>you follow the common norm in criminal justice, then you are willing to
</p>
<p>reject the null hypothesis if the risk of a Type I error is less than 5%.
</p>
<p>But how can we calculate the risk of a Type I error associated with a
</p>
<p>specific outcome in a test of statistical significance, or what we generally
</p>
<p>term the observed significance level of a test? One simple way to gain an</p>
<p/>
</div>
<div class="page"><p/>
<p>148 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>estimate of the risk of unfairly accusing your friends is to check how
</p>
<p>often a fair coin would give the same result as is observed in your series
</p>
<p>of volleyball games. For example, let&rsquo;s say that you played ten games in a
</p>
<p>season and in all ten games the old silver dollar came up heads (meaning
</p>
<p>that the opposing team won the toss). To check out how often this might
</p>
<p>happen just by chance when the coin is in fact a fair one, you might go
</p>
<p>to a laboratory with a fair coin and test this out in practice. One problem
</p>
<p>you face is deciding how many samples or trials you should conduct. For
</p>
<p>example, should you conduct one trial or sample by flipping your fair
</p>
<p>coin just ten times and stopping? Or should you conduct multiple trials or
</p>
<p>samples, each with ten tosses of the fair coin? Clearly, one trial or sample
</p>
<p>of ten tosses will not tell you very much. Indeed, one of the reasons you
</p>
<p>have gone to the laboratory is that you know it sometimes happens that a
</p>
<p>fair coin will come out heads ten times in a row. What you want to know
</p>
<p>is how rare an event this is. How often would you gain ten heads in a
</p>
<p>row in a very large number of samples or trials of a fair coin?
</p>
<p>The distribution that is gained from taking a very large number of
</p>
<p>samples or trials is called a sampling distribution. In principle, one
</p>
<p>could create a sampling distribution by drawing thousands and thou-
</p>
<p>sands of samples from a population. For example, in the case of our
</p>
<p>coin toss, we might conduct thousands of trials of ten flips of a fair coin.
</p>
<p>If we recorded the outcome for each trial and placed our results in a fre-
</p>
<p>quency distribution, we would have a sampling distribution for a sample
</p>
<p>of ten tosses of a fair coin.
</p>
<p>This sampling distribution would allow us to define the risk of a Type
</p>
<p>I error we would face in rejecting the null hypothesis that the old silver
</p>
<p>dollar is fair. For example, suppose that in the sampling distribution we
</p>
<p>gained a result of ten heads in only 1 in 1,000 samples. If we reject the
</p>
<p>null hypothesis in this case, our risk of making a Type I error, according
</p>
<p>to the sampling distribution, is only 0.001. This is the observed signifi-
</p>
<p>cance level of our test of statistical significance. In only 1 in 1,000 sam-
</p>
<p>ples of ten tosses of a fair coin would we expect to gain a result of ten
</p>
<p>heads. If the old silver dollar was indeed a fair coin, it would seem very
</p>
<p>unlikely that on our one trial of ten tosses of the coin each toss would
</p>
<p>come out heads. Of course, in making our decision we cannot be certain
</p>
<p>that the silver dollar used in the volleyball toss is not a fair coin. While
</p>
<p>the occurrence of ten heads in ten tosses of a fair coin is rare, it can hap-
</p>
<p>pen about once in every 1,000 samples.
</p>
<p>Building a sampling distribution provides a method for defining our
</p>
<p>risk of a Type I error. However, it is very burdensome to create a sam-
</p>
<p>pling distribution by hand or even in the laboratory. If you try out our
</p>
<p>example of ten tosses of a fair coin, you will see that developing even
</p>
<p>100 samples is not easy. If we had to actually construct a sampling distri-
</p>
<p>bution every time we wanted to make a decision about a hypothesis, it
</p>
<p>would be virtually impossible to make statistical inferences in practice.</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E F A I R C O I N T O S S 149
</p>
<p>Fortunately, there is another method we can use for creating sampling
</p>
<p>distributions. This method relies on probability theory, rather than a bur-
</p>
<p>densome effort to collect samples in the real world. Because we use
</p>
<p>probabilities, the distributions that are created using this method are
</p>
<p>called probability distributions. Importantly, though we rely on prob-
</p>
<p>ability theory because it is very difficult to develop sampling distributions
</p>
<p>in practice, we do not suffer for our approach. This is because probabil-
</p>
<p>ity theory allows us to calculate the outcomes one would expect in a
</p>
<p>perfect world. In the real world, we might flip the coin slightly differ-
</p>
<p>The Multiplication Rule
</p>
<p>In order to estimate the risk of a Type I error in the case of a series of
</p>
<p>tosses of a fair coin, we can use the multiplication rule, a simple rule
</p>
<p>about probabilities drawn from probability theory. The multiplication
</p>
<p>rule tells you how likely we are to gain a series of events one after
</p>
<p>another&mdash;in this case, a series of outcomes in a toss of a coin. It allows
</p>
<p>us to estimate theoretically how often we would gain a specific series of
</p>
<p>events if we drew an infinite number of samples. The multiplication rule
</p>
<p>generally used to establish probabilities in statistics is based on the as-
</p>
<p>sumption that each event in a sample is independent of every other
</p>
<p>event. In the case of the coin toss, this means that the outcome of one
</p>
<p>toss of a coin is unaffected by what happened on the prior tosses. Each
</p>
<p>time you toss the coin, it is as if you started with a clean slate. That
</p>
<p>would seem a fairly reasonable assumption for our problem. What wor-
</p>
<p>ries us is that the coin is unfair overall, not that it is becoming less or
</p>
<p>more unfair as time goes on.
</p>
<p>An example of a series of events that are not independent is draws
</p>
<p>from a deck of cards. Each time you draw a card, you reduce the num-
</p>
<p>ber of cards left in the deck, thus changing the likelihood of drawing any
</p>
<p>card in the future. For example, let&rsquo;s say that on your first draw from a
</p>
<p>deck of 52 cards you drew an ace of spades. On your second draw, you
</p>
<p>cannot draw an ace of spades because you have already removed it from
</p>
<p>the deck. The likelihood of drawing an ace of spades on the second
</p>
<p>draw has thus gone from 1 in 52 to 0 in 51. You have also influenced the
</p>
<p>likelihood of drawing any other card because there are now 51, not 52,
</p>
<p>cards left in the deck. If you want a series of draws from a deck to be
</p>
<p>independent of one another, you have to return each card to the deck
</p>
<p>after you draw it. For example, if you returned the ace of spades to the
</p>
<p>deck, the chance of choosing it (assuming the deck was mixed again)
</p>
<p>would be the same as it was on the first draw. The chances of choosing
</p>
<p>any other card would also be the same because you once again have all
</p>
<p>52 cards from which to draw.
</p>
<p>another, thus affecting the outcomes we gain. In probability theory, we 
</p>
<p>remove the imperfections of the real world from our estimates.
</p>
<p>ently as we got tired or the coin might become worn on one side or </p>
<p/>
</div>
<div class="page"><p/>
<p>150 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>The multiplication rule for four independent events is stated in Equa-
</p>
<p>tion 7.1. It says that the likelihood of any series of events, represented as
</p>
<p>A, B, C, and D, happening one after another is equal to the probability
</p>
<p>of event A times the probability of event B times the probability of event
</p>
<p>C times the probability of event D. The rule can be extended to as many
</p>
<p>events as you like. We have chosen four here, because this was the
</p>
<p>number of volleyball games we began with. If you wanted to extend the
</p>
<p>rule, you would simply include the additional events on the left side of
</p>
<p>the equation (for example, E and F ) and include the probability of each
</p>
<p>on the right side of the equation [e.g., P (E ) &bull; P (F )].
</p>
<p>P (A &amp; B &amp; C &amp; D) � P (A) &bull; P (B) &bull; P (C ) &bull; P (D) Equation 7.1
</p>
<p>Extending this logic to our example of the coin toss is straightforward.
</p>
<p>The probability of A and B and C and D can represent the probability
</p>
<p>that four tosses in a row come up heads. Our main problem is to estab-
</p>
<p>lish what the probability is of a head coming up on any particular toss of
</p>
<p>the coin. In this, we are helped by our null hypothesis, which states that
</p>
<p>the coin is fair. If the coin is fair, then there should be an even chance of
</p>
<p>a head or a tail coming up on any particular toss of the coin. Put differ-
</p>
<p>ently, under the assumption of the null hypothesis that the coin is fair,
</p>
<p>the likelihood of a head coming up is 0.50.
</p>
<p>What, then, does the multiplication rule tell us about the chances of
</p>
<p>getting four heads in a row on four tosses of a fair coin? In part a of
</p>
<p>Table 7.1, we calculate that probability by multiplying 0.50 (the likeli-
</p>
<p>hood of gaining a head on any toss of a fair coin) by itself four times, to
</p>
<p>represent four tosses of an unbiased coin. The result is 0.0625. If you
</p>
<p>had decided at the outset to make a decision about the null hypothesis&mdash;
</p>
<p>that the coin is fair&mdash;after four tosses of the coin, then you have con-
</p>
<p>ducted a type of test of statistical significance. For this test, the observed
</p>
<p>significance level (or risk of a Type I error) of rejecting the null hypothe-
</p>
<p>sis on the basis of four heads is 0.0625.
</p>
<p>If you use the norms of criminal justice research, this is not enough,
</p>
<p>however, for you to reject the null hypothesis that the coin is fair. Crimi-
</p>
<p>nal justice researchers generally want the risk of falsely rejecting the null
</p>
<p>hypothesis to be less than 5%. A bit over 6% is still more than the 5% sig-
</p>
<p>nificance criterion that is used by convention. So if you had decided to
</p>
<p>Probabilities Associated with Tosses of a Fair Coin
</p>
<p>a. P (A &amp; B &amp; C &amp; D) � P (A) &bull; P (B) &bull; P (C) &bull; P (D) � (0.50)(0.50)(0.50)(0.50) � 0.0625
</p>
<p>b. P (A &amp; B &amp; C &amp; D &amp; E) � P (A) &bull; P (B) &bull; P (C) &bull; P (D) &bull; P (E) � (0.50)(0.50)(0.50)(0.50)(0.50) � 0.0313
</p>
<p>c. P (A &amp; B &amp; C &amp; D &amp; E &amp; F &amp; G &amp; H &amp; I &amp; J) � P (A) &bull; P (B) &bull; P (C) &bull; P (D) &bull; P (E) &bull; P (F ) &bull; P (G) &bull; P (H) &bull; P (I ) &bull; P (J)
� (0.50)(0.50)(0.50)(0.50)(0.50)(0.50)(0.50)(0.50)(0.50)(0.50) � 0.0010
</p>
<p>Table 7.1</p>
<p/>
</div>
<div class="page"><p/>
<p>D I F F E R E N T W A Y S O F G E T T I N G S I M I L A R R E S U L T S 151
</p>
<p>make a decision about the fairness of the coin after four coin tosses, you
</p>
<p>would probably not want to reject the null hypothesis that the coin is fair
</p>
<p>and confront your opponents. Under this criterion, the likelihood of
</p>
<p>falsely rejecting the null hypothesis, or the observed significance level of
</p>
<p>your test, would have to be below 0.05.
</p>
<p>What if you had decided at the outset to make a decision about the
</p>
<p>null hypothesis after five tosses of a coin? Would a result of five heads in
</p>
<p>a row lead you to reject the null hypothesis? As illustrated in part b of
</p>
<p>Table 7.1, the multiplication rule tells you that the likelihood of getting
</p>
<p>five heads in a row if the coin is fair is 0.0313. This is less than our
</p>
<p>threshold of 0.05, and thus would lead you to reject the null hypothesis.
</p>
<p>Is this consistent with your earlier commonsense conclusions? Students
</p>
<p>are usually surprised at how quickly they reach the 0.05 significance
</p>
<p>threshold in this example.
</p>
<p>If you had decided at the outset that you would need ten or fifteen
</p>
<p>heads in a row, you may want to reconsider, given what we have
</p>
<p>learned from the multiplication rule. The likelihood of getting ten heads
</p>
<p>in a row in ten tosses of a fair coin is only 1 in 1,000 (see part c of Table
</p>
<p>7.1). The likelihood of getting fifteen heads in a row in fifteen tosses of a
</p>
<p>fair coin is even lower: about 3 in 100,000. In both of these cases, you
</p>
<p>would take a very small risk of a Type I error if you rejected the null hy-
</p>
<p>pothesis. Nonetheless, the multiplication rule tells us that, even if the
</p>
<p>coin is fair, it is possible to get ten or even fifteen heads in a row. It just
</p>
<p>does not happen very often.
</p>
<p>The multiplication rule allows us to estimate how often we would ex-
</p>
<p>pect to get a series of specific outcomes in a very large number of trials
</p>
<p>or samples, without actually going out and doing the hard work of con-
</p>
<p>structing a sampling distribution in the real world. However, the problem
</p>
<p>as examined so far assumes that the coin will come up heads every time.
</p>
<p>What if the coin comes up heads generally, but not all the time? For ex-
</p>
<p>ample, what if you play ten games and the coin comes up heads nine
</p>
<p>times? The situation here is not as one-sided. Nonetheless, it still seems
</p>
<p>unlikely that your opponents would win most of the time if the coin
</p>
<p>were fair. The multiplication rule alone, however, does not allow us to
</p>
<p>define how likely we are to get such a result.
</p>
<p>D i f f e r e n t  W a y s  o f  G e t t i n g  S i m i l a r  R e s u l t s
</p>
<p>The multiplication rule allows us to calculate the probability of getting a
</p>
<p>specific ordering of events. This is fine so far in our coin toss because in
</p>
<p>each example we have chosen there is only one way to get our out-
</p>
<p>come. For example, there is only one way to get five heads in five coin</p>
<p/>
</div>
<div class="page"><p/>
<p>152 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>tosses or ten heads in ten coin tosses. In each case, your opponents
</p>
<p>must toss a head before each game. This would be the situation as well
</p>
<p>if your opponents tossed tails ten times in ten coin tosses. However, for
</p>
<p>any outcome in between, there is going to be more than one potential
</p>
<p>way to achieve the same result.
</p>
<p>For example, if your opponents tossed nine heads in ten coin tosses,
</p>
<p>they could win the coin toss nine times (with a head) and then lose the
</p>
<p>toss in the tenth game (with a tail). Or they could lose the first toss (with
</p>
<p>a tail) and then win the remaining nine. Similarly, they could lose the
</p>
<p>second, third, fourth, fifth, sixth, seventh, eighth, or ninth coin toss and
</p>
<p>win all the others. Each of these possible ordering of events is called an
</p>
<p>arrangement. As is illustrated in Table 7.2, there are ten possible
</p>
<p>arrangements, or different ways that you could get nine heads in ten
</p>
<p>coin tosses. In the case of ten heads in ten coin tosses, there is only one
</p>
<p>possible arrangement.
</p>
<p>It is relatively simple to list all of the arrangements for our example of
</p>
<p>nine heads in ten coin tosses, but listing becomes very cumbersome in
</p>
<p>practice as the split of events becomes more even. For example, if we
</p>
<p>were interested in how many ways there are of getting eight heads in ten
</p>
<p>coin tosses, we would have to take into account a much larger number
</p>
<p>of arrangements. As Table 7.3 illustrates, it takes a good deal of effort to
</p>
<p>list every possible arrangement even for eight heads. In the case of a
</p>
<p>more even split of events&mdash;for example, five heads in ten tosses&mdash;it be-
</p>
<p>comes extremely cumbersome to list each arrangement one by one. Be-
</p>
<p>cause of this, we generally use the formula in Equation 7.2 to define the
</p>
<p>number of arrangements in any series of events.
</p>
<p>Equation 7.2
</p>
<p>On the left side of this equation we have N &ldquo;choose&rdquo; r, where N is the
</p>
<p>number of events in the sample and r is the number of successes in the
</p>
<p>�Nr� � N !r !(N � r)!
</p>
<p>Arrangements for Nine Successes in Ten Tosses of a Coin
</p>
<p>Arrangement 1 � � � � � � � � � �
Arrangement 2 � � � � � � � � � �
Arrangement 3 � � � � � � � � � �
Arrangement 4 � � � � � � � � � �
Arrangement 5 � � � � � � � � � �
Arrangement 6 � � � � � � � � � �
Arrangement 7 � � � � � � � � � �
Arrangement 8 � � � � � � � � � �
Arrangement 9 � � � � � � � � � �
Arrangement 10 � � � � � � � � � �
</p>
<p>� � Head; � � Tail
</p>
<p>Table 7.2</p>
<p/>
</div>
<div class="page"><p/>
<p>D I F F E R E N T W A Y S O F G E T T I N G S I M I L A R R E S U L T S 153
</p>
<p>total number of events. In our case, N is the number of coin tosses and r
</p>
<p>is the number of times that the coin comes up heads. Put together, this
</p>
<p>statement establishes our question: How many ways are there of gaining
</p>
<p>r heads in N tosses of a coin? To answer our question, we need to solve
</p>
<p>the right side of the equation. Each of the terms in the equation is de-
</p>
<p>fined as a factorial, indicated by the symbol !. When we take a factorial
</p>
<p>of a number, we merely multiply it by all of the whole numbers smaller
</p>
<p>than it. For example, 3! is equal to (3)(2)(1), or 6. Because factorials get
</p>
<p>very large very quickly, a table of factorials is presented in Appendix 1.
</p>
<p>Note that 0! � 1. Applied to our problem of nine heads in ten coin
</p>
<p>tosses, Equation 7.2 is worked out below:
</p>
<p>Arrangements for Eight Successes in Ten Tosses of a Coin
</p>
<p>1: ���������� 16: ���������� 31: ����������
2: ���������� 17: ���������� 32: ����������
3: ���������� 18: ���������� 33: ����������
4: ���������� 19: ���������� 34: ����������
5: ���������� 20: ���������� 35: ����������
6: ���������� 21: ���������� 36: ����������
7: ���������� 22: ���������� 37: ����������
8: ���������� 23: ���������� 38: ����������
9: ���������� 24: ���������� 39: ����������
</p>
<p>10: ���������� 25: ���������� 40: ����������
11: ���������� 26: ���������� 41: ����������
12: ���������� 27: ���������� 42: ����������
13: ���������� 28: ���������� 43: ����������
14: ���������� 29: ���������� 44: ����������
15: ���������� 30: ���������� 45: ����������
</p>
<p>� � Head; � � Tail
</p>
<p>Table 7.3
</p>
<p>W orking It Out
</p>
<p> � 10
</p>
<p> � 
3,628,800
</p>
<p>362,880(1)
</p>
<p> � 
10!
</p>
<p>9! 1!
</p>
<p> �109 � � 10!9!(10 � 9)!
</p>
<p> �Nr� � N !r !(N � r)!</p>
<p/>
</div>
<div class="page"><p/>
<p>154 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>Using this method, we get the same result as before. There are ten possi-
</p>
<p>ble arrangements to get nine heads in ten tosses of a coin. When we
</p>
<p>apply Equation 7.2 to the problem of five heads in ten coin tosses, its
</p>
<p>usefulness becomes even more apparent. There are 252 different ways of
</p>
<p>getting five heads in ten tosses. Listing each would have taken us consid-
</p>
<p>erably longer than the calculation below.
</p>
<p>W orking It Out
</p>
<p> � 252
</p>
<p> � 
3,628,800
</p>
<p>120(120)
</p>
<p> � 
10!
</p>
<p>5! 5!
</p>
<p> �105 � � 10!5!(10 � 5)!
</p>
<p> �Nr� � N !r !(N � r)!
</p>
<p>S o l v i n g  M o r e  C o m p l e x  P r o b l e m s
</p>
<p>Now that we have a method for calculating arrangements, we can return
</p>
<p>to our original problem, which was to define the probability of your op-
</p>
<p>ponents tossing the coin in ten games and getting heads nine times. Be-
</p>
<p>cause there are ten different ways of getting nine heads in ten coin
</p>
<p>tosses, you need to add up the probabilities associated with these ten se-
</p>
<p>quences. This is what is done in Table 7.4. The multiplication rule is
</p>
<p>used to calculate the probability for each sequence, or arrangement,
</p>
<p>under the assumption of the null hypothesis that the coin is fair. Because
</p>
<p>our null hypothesis states that the coin is fair, we can assume that the
</p>
<p>chances of gaining a head and a tail are even. The probability of any
</p>
<p>event, whether a head or a tail, is 0.50, and the probability of a sequence
</p>
<p>of ten events is always the same. This makes our task easier. But it is im-
</p>
<p>portant to note that if the null hypothesis specified an uneven split (for
</p>
<p>example, 0.75 for a head and 0.25 for a tail), then each of the sequences
</p>
<p>would have a different probability associated with it. In any case, the
</p>
<p>likelihood of getting any one of these sequences is about 0.001, rounded
</p>
<p>to the nearest thousandth. When we add together the ten sequences, we
</p>
<p>get a probability of 0.010.</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E B I N O M I A L D I S T R I B U T I O N 155
</p>
<p>This means that we would expect to get nine heads in ten coin tosses
</p>
<p>of a fair coin in only about 1 in 100 samples in a very large number of
</p>
<p>trials of a fair coin. But is this the observed significance level of a test of
</p>
<p>statistical significance in which we gain nine heads in ten tosses of a
</p>
<p>coin? Or put in terms of Type I error, is this the total amount of risk we
</p>
<p>face of falsely rejecting the null hypothesis when we gain nine heads?
</p>
<p>The answer to this question is no, although it may be difficult at first to
</p>
<p>understand why. If we are willing to reject the null hypothesis based on
</p>
<p>an outcome of nine heads in ten trials, then we are, by implication, also
</p>
<p>willing to reject the null hypothesis if our outcome is ten heads in ten tri-
</p>
<p>als. In calculating our total risk of a Type I error, we must add together
</p>
<p>the risk of all potential outcomes that would lead us to reject the null hy-
</p>
<p>pothesis. This is why, when testing hypotheses, we generally do not
</p>
<p>begin with an estimate of the specific probability associated with a single
</p>
<p>outcome, but rather with the sampling distribution of probabilities of all
</p>
<p>possible outcomes.
</p>
<p>T h e  B i n o m i a l  D i s t r i b u t i o n
</p>
<p>To construct a probability or sampling distribution for all of the possible
</p>
<p>outcomes of ten coin tosses, we could continue to compute the number
</p>
<p>of permutations and the likelihood of any particular arrangement. How-
</p>
<p>ever, Equation 7.3 provides us with a more direct method for calculating
</p>
<p>the probability associated with each of the potential outcomes in our
</p>
<p>The Sum of Probabilities for All Arrangements 
of Nine Heads in Ten Tosses of a Fair Coin
</p>
<p>PROBABILITY
</p>
<p>Arrangement 1 � � � � � � � � � � 0.001
Arrangement 2 � � � � � � � � � � 0.001
Arrangement 3 � � � � � � � � � � 0.001
Arrangement 4 � � � � � � � � � � 0.001
Arrangement 5 � � � � � � � � � � 0.001
Arrangement 6 � � � � � � � � � � 0.001
Arrangement 7 � � � � � � � � � � 0.001
Arrangement 8 � � � � � � � � � � 0.001
Arrangement 9 � � � � � � � � � � 0.001
Arrangement 10 � � � � � � � � � � 0.001
</p>
<p>Total Probability: 0.01
</p>
<p>Probability of throwing each arrangement of 10 throws
� P (A) &bull; P (B) &bull; P (C) &bull; P (D) &bull; P (E) &bull; P (F ) &bull; P (G) &bull; P (H) &bull; P (I ) &bull; P (J ) 
� (0.50)(0.50)(0.50)(0.50)(0.50)(0.50)(0.50)(0.50)(0.50)(0.50)
� 0.001
</p>
<p>Table 7.4</p>
<p/>
</div>
<div class="page"><p/>
<p>156 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>sample. Equation 7.3 is generally defined as the binomial formula, and
</p>
<p>the distribution created from it is called the binomial distribution. As
</p>
<p>the name suggests, the binomial distribution is concerned with events in
</p>
<p>which there are only two possible outcomes&mdash;in our example, heads
</p>
<p>and tails.
</p>
<p>Equation 7.3
</p>
<p>The binomial formula may look confusing, but most of it is familiar from
</p>
<p>material already covered in this chapter. The left-hand side of the equa-
</p>
<p>tion represents the quantity in which we are interested&mdash;the probability
</p>
<p>of getting r successes (in our case, r heads) in a sample of N events (for
</p>
<p>us, ten tosses of a coin). The first part of the equation provides us with
</p>
<p>the number of arrangements for that number of heads. This quantity is
</p>
<p>then multiplied by pr(1 � p)N�r, where p is the probability of a successful
</p>
<p>outcome (a head) under the null hypothesis and r is the number of suc-
</p>
<p>cesses. This formula gives us the probability associated with each
</p>
<p>arrangement. Although this part of the equation looks somewhat differ-
</p>
<p>ent from the multiplication rule we used earlier, it provides a shortcut for
</p>
<p>getting the same result, as the example below illustrates.
</p>
<p>We have already calculated the likelihood of getting nine or ten heads
</p>
<p>in ten coin tosses if the coin is fair. To complete our sampling distribu-
</p>
<p>tion, we need to compute probabilities associated with zero through
</p>
<p>eight heads as well. Let&rsquo;s begin with eight heads in ten coin tosses of an
</p>
<p>unbiased coin:
</p>
<p>Step 1: Calculating the number of arrangements
</p>
<p>P �108 � � 10!8!(10 � 8)! (0.50)8(1 � 0.50)10�8
</p>
<p>P �Nr� � N !r !(N � r)! pr(1 � p)N�r
</p>
<p>W orking It Out
</p>
<p> � 45
</p>
<p> � 
3,628,800
</p>
<p>40,320(2)
</p>
<p> � 
10!
</p>
<p>8! 2!
</p>
<p> �108 � � 10!8!(10 � 8)!</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E B I N O M I A L D I S T R I B U T I O N 157
</p>
<p>In step 1 we simply follow the same method as we did earlier in estab-
</p>
<p>lishing the number of ways of getting eight heads in ten tosses of a coin.
</p>
<p>Our conclusion is that there are 45 different arrangements.
</p>
<p>Step 2: Calculating the probability of any specific arrangement
</p>
<p>W orking It Out
</p>
<p> � 0.00098
</p>
<p> � (0.50)10
</p>
<p> � (0.50)8(0.50)2
</p>
<p> pr(1 � p)N�r � (0.50)8(1 � 0.50)10�8
</p>
<p>Step 2 provides us with the likelihood of getting any particular arrange-
</p>
<p>ment under the assumption of the null hypothesis that the coin is fair. By
</p>
<p>the null hypothesis, p is defined as 0.50, and r is the number of suc-
</p>
<p>cesses (heads) in our example, or 8. So pr is (0.50)8, and (1 � p)N�r is 
</p>
<p>(1 � 0.50)10�8, or (0.50)2. The outcome of this part of the equation can
</p>
<p>be simplified to (0.50)10. This in turn is the same outcome that we would
</p>
<p>obtain using the multiplication rule, because the expression (0.50)10
</p>
<p>means that we multiply the quantity 0.50 by itself 10 times. Using the
</p>
<p>multiplication rule, we would have done just that.
</p>
<p>Step 3: Combining the two outcomes
</p>
<p>W orking It Out
</p>
<p> � 0.0441
</p>
<p> P �108 � � 45(0.00098)
 P �Nr� � N !r !(N � r)! pr(1 � p)N�r
</p>
<p>of tossing eight heads in ten tosses of a fair coin is about 0.044. In 
</p>
<p>Combining the two parts of the equation, we find that the likelihood 
</p>
<p>Table 7.5, we calculate the probabilities associated with all the potential </p>
<p/>
</div>
<div class="page"><p/>
<p>158 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>The probability or sampling distribution for ten tosses of a fair coin il-
</p>
<p>lustrates how likely you are to get any particular outcome. All of the out-
</p>
<p>comes together add up to a probability of 1.1 Put differently, there is a
</p>
<p>100% chance that in ten tosses of a coin you will get one of these 11 po-
</p>
<p>tential outcomes. This is obvious, but the sampling distribution allows
</p>
<p>you to illustrate this fact. Following what our common sense tells us, it
</p>
<p>also shows that an outcome somewhere in the middle of the distribution
</p>
<p>Computation of Probability Distribution for Ten Tosses of a Fair Coin
</p>
<p>0 heads
</p>
<p>1 head
</p>
<p>2 heads
</p>
<p>3 heads
</p>
<p>4 heads
</p>
<p>5 heads
</p>
<p>6 heads
</p>
<p>7 heads
</p>
<p>8 heads
</p>
<p>9 heads
</p>
<p>10 heads
</p>
<p>� � 1.0*
</p>
<p>*The total in the last column is in fact slightly greater than 100%. This is due to rounding the numbers to the nearest
decimal place in order to make the calculation more manageable.
</p>
<p> 
3,628,800
</p>
<p>3,628,800(10 � 10)!
 � 
</p>
<p>3,628,800
</p>
<p>3,628,800
 � 1           1(0.00098) � 0.0010
</p>
<p> 
3,628,800
</p>
<p>362,880(10 � 9)!
 � 
</p>
<p>3,628,800
</p>
<p>362,880
 � 10           10(0.00098) � 0.0098
</p>
<p> 
3,628,800
</p>
<p>40,320(10 � 8)!
 � 
</p>
<p>3,628,800
</p>
<p>80,640
 � 45           45(0.00098) � 0.0441
</p>
<p> 
3,628,800
</p>
<p>5,040(10 � 7)!
 � 
</p>
<p>3,628,800
</p>
<p>30,240
 � 120           120(0.00098) � 0.1176
</p>
<p> 
3,628,800
</p>
<p>720(10 � 6)!
 � 
</p>
<p>3,628,800
</p>
<p>17,280
 � 210           210(0.00098) � 0.2058
</p>
<p> 
3,628,800
</p>
<p>120(10 � 5)!
 � 
</p>
<p>3,628,800
</p>
<p>14,400
 � 252           252(0.00098) � 0.2470
</p>
<p> 
3,628,800
</p>
<p>24(10 � 4)!
 � 
</p>
<p>3,628,800
</p>
<p>17,280
 � 210           210(0.00098) � 0.2058
</p>
<p> 
3,628,800
</p>
<p>6(10 � 3)!
 � 
</p>
<p>3,628,800
</p>
<p>30,240
 � 120           120(0.00098) � 0.1176
</p>
<p> 
3,628,800
</p>
<p>2(10 � 2)!
 � 
</p>
<p>3,628,800
</p>
<p>80,640
 � 45            45(0.00098) � 0.0441
</p>
<p> 
3,628,800
</p>
<p>1(10 � 1)!
 � 
</p>
<p>3,628,800
</p>
<p>362,880
 � 10           10(0.00098) � 0.0098
</p>
<p> 
3,628,800
</p>
<p>1(10 � 0)!
 � 
</p>
<p>3,628,800
</p>
<p>3,628,800
 � 1           1(0.00098) � 0.0010
</p>
<p>�Nr �pr (1 � p)N�r�Nr � � N !r ! (N � r )!
Table 7.5
</p>
<p>1Because of rounding error, the total for our example is actually slightly larger than 1
</p>
<p>(see Table 7.5).
</p>
<p>outcomes in this binomial distribution. The resulting sampling distribution
</p>
<p>is displayed in Table 7.6.</p>
<p/>
</div>
<div class="page"><p/>
<p>U S I N G T H E B I N O M I A L D I S T R I B U T I O N 159
</p>
<p>is most likely. If the coin is fair, then we should more often than not get
</p>
<p>about an even split of heads and tails.
</p>
<p>The largest proportion (0.247) in the sampling distribution is found at
</p>
<p>five heads in ten tosses of a coin. As you move farther away from the
</p>
<p>center of the distribution, the likelihood of any particular result declines.
</p>
<p>The smallest probabilities are associated with gaining either all heads or
</p>
<p>no heads. Like many of the distributions that we use in statistics, this dis-
</p>
<p>tribution is symmetrical. This means that the same probabilities are asso-
</p>
<p>ciated with outcomes on both sides.
</p>
<p>In Chapter 6, we talked about the fact that samples vary from one an-
</p>
<p>other. This is what makes it so difficult to make inferences from a sam-
</p>
<p>ple to a population. Based on a sample statistic, we can never be sure
</p>
<p>about the actual value of the population parameter. However, as illus-
</p>
<p>trated in this sampling distribution, samples drawn from the same popu-
</p>
<p>lation vary in a systematic way in the long run. It is very unlikely to draw
</p>
<p>a sample with ten heads in ten tosses of a fair coin. On the other hand, it
</p>
<p>is very likely to draw a sample with four, five, or six heads in ten tosses.
</p>
<p>U s i n g  t h e  B i n o m i a l  D i s t r i b u t i o n  
t o  E s t i m a t e  t h e  O b s e r v e d  S i g n i f i c a n c e  L e v e l  o f  a  T e s t
</p>
<p>Using the sampling distribution, we can now return to the problem of
</p>
<p>identifying the risks of error associated with rejecting the null hypothesis
</p>
<p>that the coin brought by the other volleyball team is fair. Earlier we sug-
</p>
<p>gested that you might want to use a 5% significance level for this test, in
</p>
<p>part because it is the standard or conventional significance level used by
</p>
<p>Probability or Sampling Distribution for Ten Tosses of a Fair Coin
</p>
<p>0 heads 0.001
1 head 0.010
2 heads 0.044
3 heads 0.118
4 heads 0.206
5 heads 0.247
6 heads 0.206
7 heads 0.118
8 heads 0.044
9 heads 0.010
10 heads 0.001
</p>
<p>Table 7.6
</p>
<p>most criminal justice researchers. This means that in order to reject the null
</p>
<p>hypothesis you would require that the observed significance level (p) of your  
</p>
<p>test (or the risk of making a Type1 error by incorrectly rejecting the null hypo- 
</p>
<p>to reject the null hypothesis that the coin is fair and confront your opponents?
</p>
<p>thesis) be less than 5% (or p &lt; .05). Using this level, when would you be willing  </p>
<p/>
</div>
<div class="page"><p/>
<p>Applying the Binomial Distribution 
to Situations Where p � 0.5
</p>
<p>The examples in the text focus on applying the binomial distribution to sit-
</p>
<p>uations where the probability of a success is equal to 0.5. There are other
</p>
<p>situations where we are interested in the probability of multiple suc-
</p>
<p>cesses (or failures), but success and failure are not equally likely. For ex-
</p>
<p>ample, many of the games of chance that a person might play at a casino
</p>
<p>are constructed in such a way that winning and losing are not equally
</p>
<p>likely&mdash;the chances of losing are greater than the chances of winning&mdash;
</p>
<p>but use of the binomial distribution would allow for calculation of the
</p>
<p>chances of winning over several plays of the game.
</p>
<p>Consider the following more detailed example. Suppose that we have a
</p>
<p>quiz with five questions and we are interested in the probability of a student
</p>
<p>sponse on any single question is p � 1/2 � 0.5. We can then apply the bino-
</p>
<p>mial in the same way as we have in the previous examples to determine the
</p>
<p>probability of some number of correct answers. The following table pre-
</p>
<p>sents the numbers of correct answers and the corresponding probabilities.
</p>
<p>NUMBER OF CORRECT ANSWERS
</p>
<p>0 correct
</p>
<p>1 correct
</p>
<p>2 correct
</p>
<p>3 correct
</p>
<p>4 correct
</p>
<p>5 correct
</p>
<p>Now suppose that the questions are worded as multiple-choice items
</p>
<p>and the student has to choose one answer from four possibilities. For any
</p>
<p>single question, the probability of guessing the correct answer is p � 1/4
</p>
<p>� 0.25. Given that we have multiple questions, we can again calculate the
</p>
<p>5!
</p>
<p>5!(5 � 5)!
 0.55 (1 � 0.5)5�5 � 0.03125
</p>
<p>5!
</p>
<p>4!(5 � 4)!
 0.54 (1 � 0.5)5�4 � 0.15625
</p>
<p>5!
</p>
<p>3!(5 � 3)!
 0.53 (1 � 0.5)5�3 � 0.3125
</p>
<p>5!
</p>
<p>2!(5 � 2)!
 0.52 (1 � 0.5)5�2 � 0.3125
</p>
<p>5!
</p>
<p>1!(5 � 1)!
 0.51 (1 � 0.5)5�1 � 0.15625
</p>
<p>5!
</p>
<p>0!(5 � 0)!
 0.50 (1 � 0.5)5�0 � 0.03125
</p>
<p>�Nr �pr (1 � p)N�rComputation 
of Binomial
</p>
<p>Probabilities for
</p>
<p>Five True-False
</p>
<p>Questions
</p>
<p>answers are true or false, then the probability of guessing the correct re-
</p>
<p>correctly guessing all of the answers on the quiz. If the only possible</p>
<p/>
</div>
<div class="page"><p/>
<p>probability for the number of correct responses using the binomial distri-
</p>
<p>bution, but we need to replace p � 0.5 with p � 0.25 in the equations to
</p>
<p>reflect the different probability of a correct answer. The following table
</p>
<p>presents the numbers of correct responses and the corresponding prob-
</p>
<p>abilities for the multiple-choice response set.
</p>
<p>NUMBER OF CORRECT ANSWERS
</p>
<p>0 correct
</p>
<p>1 correct
</p>
<p>2 correct
</p>
<p>3 correct
</p>
<p>4 correct
</p>
<p>5 correct
</p>
<p>It is important to note that the distribution presented in the second table
</p>
<p>is no longer symmetrical, reflecting the fact that the probability of a cor-
</p>
<p>rect response is no longer equal to the probability of an incorrect re-
</p>
<p>sponse. For the true-false questions, where the probabilities of correct
</p>
<p>and incorrect answers are the same, we see that the probabilities of zero
</p>
<p>and five correct responses are equal, the probabilities of one and four cor-
</p>
<p>rect responses are equal, and the probabilities of two and three correct
</p>
<p>responses are equal. In contrast, when we look at the probabilities for
</p>
<p>multiple-choice questions with four possible answers, there is no such
</p>
<p>symmetry. The most likely outcome is one correct response, with a proba-
</p>
<p>bility of 0.3955. The probability of guessing four or five correct multiple-
</p>
<p>choice answers is much lower than the probability of guessing four or five
</p>
<p>correct true-false answers. In general, the probabilities in the table show
</p>
<p>that increasing the number of possible answers makes it much more diffi-
</p>
<p>cult for the student to correctly guess all the answers and increases the
</p>
<p>chances of getting no correct responses or only one correct response.
</p>
<p>5!
</p>
<p>5!(5 � 5)!
 0.255 (1 � 0.25)5�5 � 0.0010
</p>
<p>5!
</p>
<p>4!(5 � 4)!
 0.254 (1 � 0.25)5�4 � 0.0146
</p>
<p>5!
</p>
<p>3!(5 � 3)!
 0.253 (1 � 0.25)5�3 � 0.0879
</p>
<p>5!
</p>
<p>2!(5 � 2)!
 0.252 (1 � 0.25)5�2 � 0.2637
</p>
<p>5!
</p>
<p>1!(5 � 1)!
 0.251 (1 � 0.25)5�1 � 0.3955
</p>
<p>5!
</p>
<p>0!(5 � 0)!
 0.250 (1 � 0.25)5�0 � 0.2373
</p>
<p>�Nr �pr (1 � p)N�rComputation
of Binomial
</p>
<p>Probabilities
</p>
<p>for Five
</p>
<p>Multiple-
</p>
<p>Choice
</p>
<p>Questions</p>
<p/>
</div>
<div class="page"><p/>
<p>162 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>At first glance, you might decide to reject the null hypothesis for out-
</p>
<p>comes of zero, one, two, eight, nine, and ten heads. Each of these is
</p>
<p>below the threshold of 0.05 that we have suggested. However, at the
</p>
<p>outset we stated in our research hypothesis that we were concerned not
</p>
<p>that the coin was biased per se, but that it was biased against your team.
</p>
<p>This means that we set up our research hypotheses in such a way that
</p>
<p>we would reject the null hypothesis only if the outcomes were mostly
</p>
<p>heads. Although tossing zero, one, or two heads is just as unlikely as
</p>
<p>tossing eight, nine, or ten heads, our research hypothesis states our in-
</p>
<p>tention not to consider the former outcomes.
</p>
<p>What about the risk of falsely rejecting the null hypothesis in the case
</p>
<p>of eight, nine, or ten heads? As we noted earlier, in calculating the risk of
</p>
<p>a Type I error, we must add up the probabilities associated with all the
</p>
<p>outcomes for which we would reject the null hypothesis. So, for exam-
</p>
<p>ple, if we want to know the risk of falsely rejecting the null hypothesis
</p>
<p>on the basis of eight heads in ten coin tosses, we have to add together
</p>
<p>the risks associated with eight, nine, and ten heads in ten tosses. The
</p>
<p>question we ask is, What is the risk of falsely rejecting the null hypothe-
</p>
<p>sis if we gain eight or more heads in ten coin tosses? The total risk, or
</p>
<p>observed significance level, would be about 0.055 (that is, 0.044 � 0.010
</p>
<p>� 0.001), which is greater than our threshold of 0.05 for rejecting the
</p>
<p>null hypothesis. It is too large an outcome for you to confront your op-
</p>
<p>ponents and accuse them of cheating.
</p>
<p>In the case of nine heads, the outcome is well below the threshold of
</p>
<p>a Type I error we have chosen. By adding together the probabilities as-
</p>
<p>sociated with gaining nine or ten heads in ten coin tosses, we arrive at a
</p>
<p>risk of 0.011 of falsely rejecting the null hypothesis. If we decided to re-
</p>
<p>ject the null hypothesis that the coin is fair on the basis of an outcome of
</p>
<p>nine heads, then the observed significance value for our test would be
</p>
<p>0.011. For ten heads, as we noted earlier, the risk of a Type I error is
</p>
<p>even lower (p � 0.001). Because there are no outcomes more extreme
</p>
<p>than ten heads in our distribution, we do not have to add any probabili-
</p>
<p>ties to it to arrive at an estimate of the risk of a Type I error.
</p>
<p>You would take a very large risk of a Type I error if you decided in
</p>
<p>advance to reject the null hypothesis that the coin is fair based on six
</p>
<p>heads in ten tosses of a coin. Here, you would have to add the probabil-
</p>
<p>ities associated with six (0.206), seven (0.118), eight (0.044), nine
</p>
<p>(0.010), and ten heads (0.001).
</p>
<p>As the coin toss example illustrates, sampling distributions play a very
</p>
<p>important role in inferential statistics. They allow us to define the ob-
</p>
<p>served significance level, or risk of a Type I error, we take in rejecting
</p>
<p>the null hypothesis based on a specific outcome of a test of statistical
</p>
<p>significance. Although most sampling distributions we use in statistics are
</p>
<p>considerably more difficult to develop and involve much more complex</p>
<p/>
</div>
<div class="page"><p/>
<p>K E Y T E R M S 163
</p>
<p>mathematical reasoning than the binomial distribution, they follow a
</p>
<p>logic similar to what we have used here. For each distribution, statisti-
</p>
<p>cians use probabilities to define the likelihood of gaining particular out-
</p>
<p>comes. These sampling distributions provide us with a precise method
</p>
<p>for defining risks of error in tests of statistical significance.
</p>
<p>What you have learned here provides a basic understanding of how
</p>
<p>sampling distributions are developed from probability theory. In later
</p>
<p>chapters, we will rely on already calculated distributions. However, you
</p>
<p>should keep in mind that steps similar to those we have taken here have
</p>
<p>been used to construct these distributions.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>Whereas a sample distribution is the distribution of the results of one
</p>
<p>sample, a sampling distribution is the distribution of outcomes of a
</p>
<p>very large number of samples, each of the same size. A sampling distrib-
</p>
<p>ution that is derived from the laws of probability (without the need to
</p>
<p>take countless samples) may also be called a probability distribution.
</p>
<p>A sampling distribution allows us to define the observed significance
</p>
<p>level of a test of statistical significance, or the estimated risk of a Type I
</p>
<p>error we take in rejecting the null hypothesis based on sample statistics.
</p>
<p>To guide our decision as to whether to reject or fail to reject the null hy-
</p>
<p>pothesis, we compare the observed significance level with the criterion
</p>
<p>significance level set at the outset of the test of statistical significance.
</p>
<p>By using the multiplication rule, we can calculate the probability of
</p>
<p>obtaining a series of results in a specific order. The number of arrange-
</p>
<p>ments is the number of different ways of obtaining the same result. The
</p>
<p>total probability of obtaining any result is the individual probability mul-
</p>
<p>tiplied by the number of different possible arrangements.
</p>
<p>The binomial distribution is the sampling distribution for events
</p>
<p>with only two possible outcomes&mdash;success or failure, heads or tails, etc.
</p>
<p>It is calculated using the binomial formula. When deciding whether
</p>
<p>the result achieved, or observed significance level, passes the desired
</p>
<p>threshold for rejecting the null hypothesis, it is important to remember to
</p>
<p>take a cumulative total of risk.
</p>
<p>K e y  T e r m s
</p>
<p>arrangements The different ways events
</p>
<p>can be ordered and yet result in a single
</p>
<p>outcome. For example, there is only one
</p>
<p>arrangement for gaining the outcome of ten
</p>
<p>heads in ten tosses of a coin. There are,
</p>
<p>however, ten different arrangements for
</p>
<p>gaining the outcome of nine heads in ten
</p>
<p>tosses of a coin.</p>
<p/>
</div>
<div class="page"><p/>
<p>164 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>! Factorial
</p>
<p>r Number of successes
</p>
<p>N Number of trials
</p>
<p>p
</p>
<p>To determine the probability of events A, B, C, and D occurring jointly
</p>
<p>under the assumption of independence (the multiplication rule):
</p>
<p>P (A &amp; B &amp; C &amp; D) � P (A) &bull; P (B) &bull; P (C ) &bull; P (D)
</p>
<p>To determine the number of arrangements of any combination of
</p>
<p>events:
</p>
<p>To determine the probability of any binomial outcome occurring in all
</p>
<p>its possible arrangements (the binomial formula):
</p>
<p>P�Nr� � N !r !(N � r)! pr(1 � p)N�r
</p>
<p>�Nr� � N !r !(N � r)!
</p>
<p>binomial distribution The probability or
</p>
<p>sampling distribution for an event that has
</p>
<p>only two possible outcomes.
</p>
<p>binomial formula The means of deter-
</p>
<p>mining the probability that a given set of
</p>
<p>binomial events will occur in all its possi-
</p>
<p>ble arrangements.
</p>
<p>factorial The product of a number and all
</p>
<p>the positive whole numbers lower than it.
</p>
<p>independent Describing two events when
</p>
<p>the occurrence of one does not affect the
</p>
<p>occurrence of the other.
</p>
<p>multiplication rule The means for deter-
</p>
<p>mining the probability that a series of
</p>
<p>events will jointly occur.
</p>
<p>probability distribution A theoretical
</p>
<p>distribution consisting of the probabilities
</p>
<p>expected in the long run for all possible
</p>
<p>outcomes of an event.
</p>
<p>sampling distribution A distribution 
</p>
<p>of all the results of a very large number 
</p>
<p>of samples, each one of the same size
</p>
<p>and drawn from the same population
</p>
<p>under the same conditions. Ordinarily,
</p>
<p>sampling distributions are derived using
</p>
<p>probability theory and are based on prob-
</p>
<p>ability distributions.
</p>
<p>The probability of a success in the binomial formula. It is also used as a 
</p>
<p>significance.
symbol of the observed significance level of a test of statistical </p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 165
</p>
<p>E x e r c i s e s
</p>
<p>7.1 Calculate the probability for each of the following:
</p>
<p>a. Two tails in two tosses of a fair coin.
</p>
<p>b. Three heads in three tosses of a fair coin.
</p>
<p>c. Four heads in four tosses of an unfair coin where the probability of
a head is 0.75.
</p>
<p>d. Three sixes in three rolls of a fair die.
</p>
<p>e. Five fours in five rolls of an unfair die where the probability of a
four is 0.25.
</p>
<p>7.2 All of Kate&rsquo;s children are boys.
</p>
<p>a. Intuitively, how many boys do you think Kate would have to have
in succession before you would be willing to say with some cer-
tainty that, for some biological reason, she is more likely to give
birth to boys than girls?
</p>
<p>b. Now calculate the number of successive births required before you
could make such a decision statistically with a 5% risk of error.
</p>
<p>c. How many successive boys would have to be born before you
would be prepared to come to this conclusion with only a 1% risk
of error?
</p>
<p>7.3 The Federal Bureau of Investigation trains sniffer dogs to find explo-
sive material. At the end of the training, Lucy, the FBI&rsquo;s prize dog, is
let loose in a field with four unmarked parcels, one of which contains
Semtex explosives. The exercise is repeated three times, and on each
occasion, Lucy successfully identifies the suspicious parcel.
</p>
<p>a. What is the chance of an untrained dog performing such a feat?
(Assume that the untrained dog would always approach one of the
parcels at random.)
</p>
<p>b. If there had been five parcels instead of four and the exercise had
been carried out only twice instead of three times, would the
chances of the untrained dog finding the single suspicious parcel
have been greater or less?
</p>
<p>7.4 Alex, an attorney, wishes to call eight witnesses to court for an impor-
tant case. In his mind, he has categorized them into three &ldquo;strong&rdquo; wit-
nesses and five &ldquo;weaker&rdquo; witnesses. He now wishes to make a tactical
decision on the order in which to call the strong and the weaker
witnesses.</p>
<p/>
</div>
<div class="page"><p/>
<p>166 C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K
</p>
<p>For example, one of his options is
</p>
<p>Strong Weak Weak Strong Weak Weak Weak Strong
</p>
<p>a. In how many different sequences can he call his strong and weaker
witnesses?
</p>
<p>b. If Alex decides that one of his three strong witnesses is in fact more
suited to the weaker category, how many options does he now have?
</p>
<p>7.5 In a soccer match held at a low-security prison, the inmates beat the
guards 4 to 2.
</p>
<p>a. How many different arrangements are there for the order in which
the goals were scored?
</p>
<p>b. What would your answer be if the final score were 5 to 1?
</p>
<p>7.6 At the end of each year, the police force chooses its &ldquo;Police Officer of
the Year.&rdquo; In spite of the fact that there are equal numbers of men and
women on the force, in the last 15 years, 11 of the winners have been
men and 4 have been women. Paul has been investigating whether
women and men are treated differently in the police force.
</p>
<p>a. Do these figures provide Paul with a reasonable basis to suspect
that the sex of the officer is an active factor? Explain your answer.
</p>
<p>b. Looking back further into the records, Paul discovers that for the
three years before the 15-year span initially examined, a woman
was chosen each time. Does this affect his conclusion? Explain your
answer.
</p>
<p>7.7 Use the binomial distribution to calculate each of the following
probabilities:
</p>
<p>a. Three heads in eight tosses of a fair coin.
</p>
<p>b. Six tails in thirteen tosses of a fair coin.
</p>
<p>c. Four fives in five rolls of a fair die.
</p>
<p>d. Two ones in nine rolls of a fair die.
</p>
<p>e. Five sixes in seven rolls of a fair die.
</p>
<p>7.8 Tracy, a teacher, gives her class a ten-question test based on the
homework she assigned the night before. She strongly suspects that
Mandy, a lazy student, did not do the homework. Tracy is surprised to
see that of the ten questions, Mandy answers seven correctly. What is
the probability that Mandy successfully guessed seven of the ten an-
swers to the questions if
</p>
<p>a. The questions all required an answer of true or false?
</p>
<p>b. The questions were all in the multiple-choice format, with students
having to circle one correct answer from a list of five choices?</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S 167
</p>
<p>7.9 After a supermarket robbery, four eyewitnesses each report seeing a
man with glasses fleeing from the scene. The police suspect Eddy and
make up an identity parade of five men with glasses. Eddy takes his
place in the parade alongside four randomly chosen stooges. Of the
four eyewitnesses who are brought in, three identify Eddy and the
fourth points to one of the stooges. The detective in charge decides
that there is enough evidence to bring Eddy to trial.
</p>
<p>a. The detective&rsquo;s superior wishes to know the probability that Eddy
would have been chosen by three out of the four eyewitnesses if
each witness had chosen a member of the identity parade entirely
at random. What is the probability?
</p>
<p>b. What is the probability of Eddy being chosen at random by only
two of the four witnesses?
</p>
<p>7.10 A gang of five child thieves draws straws each time before they go
shoplifting. Whoever draws the short straw is the one who does the
stealing. By tradition, Anton, the leader, always draws first. On the
four occasions that the gang has performed this ritual, Anton has
drawn the short straw three times.
</p>
<p>a. Construct a table to illustrate the binomial distribution of Anton&rsquo;s
possible successes and failures for each of the four draws.
</p>
<p>b. Should he accuse his fellow gang members of rigging the draw if
</p>
<p>i. He is willing to take a 5% risk of falsely accusing his friends?
</p>
<p>ii. He is willing to take only a 1% risk of falsely accusing his friends?
</p>
<p>7.11 Baron, a gambler, plays 11 rounds at a casino roulette wheel, each
time placing a $100 note on either black or red.
</p>
<p>a. Construct a table to illustrate the binomial distribution of Baron&rsquo;s
possible successes and failures for each of the 11 rounds.
</p>
<p>b. The casino croupiers have been told to inform the management if a
client&rsquo;s winning streak arouses suspicion that he might be cheating.
The threshold of suspicion is set at 0.005. How many successes does
Baron need on 11 trials to arouse the management&rsquo;s suspicion?
</p>
<p>7.12 Nicola is playing roulette on an adjacent table. On 12 successive spins
of the wheel, she places a $100 note on either the first third (numbers
1&ndash;12), the second third (numbers 13&ndash;24), or the final third (numbers
25&ndash;36).
</p>
<p>a. Construct a table to illustrate the binomial distribution of Nicola&rsquo;s
possible successes and failures for each of the 12 spins.
</p>
<p>b. How many times out of the 12 would Nicola need to win in order
to arouse the suspicion of the casino manager that she was cheat-
ing, if the management policy is to limit the risk of falsely accusing
a customer to 0.001?</p>
<p/>
</div>
<div class="page"><p/>
<p>7.13 A security consultant hired by store management thinks that the prob-
ability of store security detecting an incident of shoplifting is 0.1. Sup -
pose the consultant decides to test the effectiveness of security by try -
ing to steal an item ten different times.
</p>
<p>a. Construct a table to illustrate the binomial distribution of possible
detections for each of the ten attempted thefts.
</p>
<p>b. Store management claims that the chances of detection are greater
than 0.1. If the consultant set the threshold for detection at 0.05,
how many times would she have to be detected to increase the
probability of detection?
</p>
<p>7.14 In a crime spree, Joe commits six robberies.
</p>
<p>a. If the probability of arrest for a single robbery is 0.7, what is the
probability that Joe will be arrested for three of the robberies?
</p>
<p>b. If the probability of detection for a single robbery is 0.4, what is the
probability that Joe will not be arrested for any of his crimes?
</p>
<p>7.15 The arrest histories for a sample of convicted felons revealed that,
with ten previous arrests, the probability of a drug arrest was 0.25. If
an offender has been arrested ten times, what is the probability of
</p>
<p>a. two drug arrests?
</p>
<p>b. five drug arrests?
</p>
<p>c. seven drug arrests?
</p>
<p>C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K168
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>The computation of binomial probabilities by hand can be quite tedious and time 
</p>
<p>consuming. Spreadsheet packages typically include a function that can be inserted 
</p>
<p>into a cell that will allow for the computation of a binomial probability. To com-
</p>
<p>pute the probability correctly, you will need to enter three pieces of information: 
</p>
<p>the number of &ldquo;successes&rdquo; (i.e., events of interest), the total number of trials, 
</p>
<p>and the probability of success on a single trial. Another item that you will need 
</p>
<p>to pay attention to is whether the binomial function you are using computes a 
</p>
<p>cumulative probability&mdash;the default in many spreadsheet packages. Throughout 
</p>
<p>this chapter, we have not computed cumulative probabilities, but rather, what are 
</p>
<p>labeled, &ldquo;probability densities&rdquo; in many spreadsheet packages.
</p>
<p>Although not quite as flexible as a spreadsheet package, the computation of 
</p>
<p>binomial probabilities in both SPSS and Stata is not complicated. We illustrate 
</p>
<p>how the various commands work in each program.
</p>
<p>The computation of binomial probabilities in SPSS requires the use of the 
</p>
<p>COMPUTE command that was noted in the Chapter 5 Computer Exercises. As 
</p>
<p>a means of illustrating the computation of binomial probabilities in SPSS, we will 
</p>
<p>SPSS</p>
<p/>
</div>
<div class="page"><p/>
<p>169C O M P U T E R E X E R C I S E S
</p>
<p>a new data set in SPSS that contains one variable: the number of success. This 
</p>
<p>new variable will have values ranging from 0 to 10&mdash;enter them in order for ease 
</p>
<p>of interpretation later. (After you enter these data, you should have 11 values for 
</p>
<p>your new variable. For ease of illustration below, rename this variable to &ldquo;suc-
</p>
<p>cesses&rdquo;&mdash;without the quotation marks.)
</p>
<p>To compute the binomial probabilities, the general form of the COMPUTE 
</p>
<p>command will be:
</p>
<p>The PDF.BINOM function will compute binomial probabilities for any given 
</p>
<p>combination of successes (q), number of trials (n), and probability of success on 
</p>
<p>a single trial (p). Prior to executing this command, we need to insert values for 
</p>
<p>each item in the PDF.BINOM function.
</p>
<p>of  successes&mdash;this is the variable that you created with values ranging from 0 
</p>
<p>to 10. Enter the variable name here.
</p>
<p>The value for &ldquo;n&rdquo; (the second value referenced in the parentheses) is the total 
</p>
<p>number of  trials. For our example, enter the number 10.
</p>
<p>The value for &ldquo;p&rdquo; (the third value referenced in the parentheses) is the 
</p>
<p> probability of  success for a single trial. For our example, enter the value 0.5.
</p>
<p>Assuming that you named the new variable successes, the COMPUTE command 
</p>
<p>would look like:
</p>
<p>We have named the new variable &ldquo;binom_prob&rdquo; and once this command has 
</p>
<p>been run, you should see a new variable in the second column of the SPSS data 
</p>
<p>window that contains binomial probabilities. With the exception of rounding 
</p>
<p> differences, these values are identical to those presented in Table 7.5.
</p>
<p>Stata
</p>
<p>The computation of binomial probabilities in Stata is nearly identical to that in 
</p>
<p>SPSS and requires the use of the gen command that was noted in the Chapter 
</p>
<p>5 Computer Exercises. We walk through the same illustration of the computa-
</p>
<p>tion of binomial probabilities as we did in SPSS, and will reproduce the binomial 
</p>
<p>probabilities listed in Table 7.5 with the commands in Stata.
</p>
<p>To create a new data set in Stata, click on the &ldquo;Data Editor&rdquo; button at the 
</p>
<p>top center of the Stata window. You should see a spreadsheet layout, much like 
</p>
<p>in SPSS. Again, we will begin by creating a new variable representing the number 
</p>
<p>of success. Enter values for this new variable that range from 0 to 10. (After you 
</p>
<p>reproduce the binomial probabilities listed in Table 7.5. To begin, we will create 
</p>
<p>COMPUTE new_var_name = PDF.BINOM(q,n,p).
</p>
<p>EXECUTE.
</p>
<p>COMPUTE binom_prob = PDF.BINOM(successes,10,0.5).
</p>
<p>EXECUTE.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S E V E N :  D E F I N I N G T H E O B S E R V E D R I S K170
</p>
<p>enter these data, you should have 11 values for your new variable. For ease of 
</p>
<p>illustration below, rename this variable to &ldquo;successes&rdquo;&mdash;without the quotation 
</p>
<p>marks.)
</p>
<p>To compute the binomial probabilities, the general form of the gen com-
</p>
<p>mand will be:
</p>
<p>Where the binomialp function will compute binomial probabilities for any 
</p>
<p>given combination of number of trials (n), of number of successes (k), and 
</p>
<p>probability of success on a single trial (p). Prior to executing this command, we 
</p>
<p>need to insert values for each item in the binomialp function.
</p>
<p>The value for n -
</p>
<p>ber of  trials. For our example, enter the number 10.
</p>
<p>The value for k (the second value referenced in the parentheses) is the number 
</p>
<p>of  successes&mdash;this is the variable that you created with values ranging from 0 
</p>
<p>to 10. Enter the variable name here.
</p>
<p>The value for p (the third value referenced in the parentheses) is the 
</p>
<p> probability of  success for a single trial. For our example, enter the value 0.5.
</p>
<p>Assuming that you named the new variable successes, the gen command would 
</p>
<p>look like:
</p>
<p>The new variable is named &ldquo;binom_prob&rdquo; and once this command has been run, 
</p>
<p>you should see a new variable in the second column of the data window that 
</p>
<p>contains binomial probabilities. Similar to the analysis with SPSS, these values  
</p>
<p>are identical to those presented in Table 7.5, with the exception of rounding  
</p>
<p>differences.
</p>
<p>Problems
</p>
<p> 2. Verify the probabilities you calculated for any of  the Exercises you worked 
</p>
<p>through at the end of  Chapter 7.
</p>
<p> 3. Construct a table of  binomial probabilities for each of  the following  
</p>
<p>combinations:
</p>
<p>a. Number of  trials = 10, probability of  success = 0.2.
</p>
<p>b. Number of  trials = 10, probability of  success = 0.7.
</p>
<p>c. Number of  trials = 15, probability of  success = 0.3.
</p>
<p>d. Number of  trials = 15, probability of  success = 0.5.
</p>
<p>e. Number of  trials = 15, probability of  success = 0.8.
</p>
<p>gen new_var_name = binomialp(n,k,p)
</p>
<p>gen binom_prob = binomialp(10,successes,0.5)
</p>
<p> 1. Reproduce the tables of  binomial probabilities on pages 160 and 161 in 
</p>
<p>the box applying binomial probabilities when p &ne; 0.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>Steps in a Statistical Test: 
</p>
<p>Using the Binomial Distribution 
</p>
<p>to Make Decisions About Hypotheses
</p>
<p>Are Assumptions Made About the Population Distribution?
</p>
<p>C h a p t e r  e i g h t
</p>
<p>S t a t i s t i c a l  a s s u m p t i o n s
</p>
<p>S a m p l i n g  d i s t r i b u t i o n
</p>
<p>S i g n i f i c a n c e  l e v e l
</p>
<p>T e s t  s t a t i s t i c  a n d  d e c i s i o n
</p>
<p>What Type of Measurement is Being Used?
</p>
<p>What Sampling Method is Being Used?
</p>
<p>What are the Hypotheses?
</p>
<p>Which Sampling Distribution is Appropriate?
</p>
<p>What is the Rejection Region?
</p>
<p>Should a One-Tailed or a Two-Tailed Test be Used?
</p>
<p>What is the Test Statistic?
</p>
<p>How is a Final Decision Made?
</p>
<p>Where is It Placed?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_8,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>IN THE PREVIOUS CHAPTER, you saw how probability theory is used to
identify the observed significance level in a test of statistical significance.
</p>
<p>But you cannot simply rely on mathematical calculations to determine
</p>
<p>whether to reject the null hypothesis. You must make sure at the outset
</p>
<p>that the methods used are appropriate to the problem examined. You
</p>
<p>must clearly state the assumptions made. You must define the specific
</p>
<p>hypotheses to be tested and the specific significance criteria to be used.
</p>
<p>It is best to take a careful step-by-step approach to tests of statistical sig-
</p>
<p>nificance. Using this approach, you will be much less likely to make seri-
</p>
<p>ous mistakes in developing such tests.
</p>
<p>In this chapter, we introduce the basic elements of this step-by-step
</p>
<p>approach. To place this approach in context, we illustrate each step with
</p>
<p>a specific research problem that can be addressed using the binomial
</p>
<p>distribution. Although we use the binomial distribution as an example,
</p>
<p>you should not lose sight of the fact that our purpose here is to establish
</p>
<p>a general model for presenting tests of statistical significance, which can
</p>
<p>be used whichever sampling distribution is chosen.
</p>
<p>T h e  P r o b l e m :  T h e  I m p a c t  o f  P r o b l e m - O r i e n t e d  
P o l i c i n g  o n  D i s o r d e r l y  A c t i v i t y  a t  V i o l e n t - C r i m e  H o t  S p o t s
</p>
<p>In Jersey City, New Jersey, researchers developed a problem-oriented
</p>
<p>policing program directed at violent-crime hot spots.1 Computer map-
</p>
<p>ping techniques were used to identify places in the city with a very high
</p>
<p>level of violent-crime arrests or emergency calls to the police. Jersey City
</p>
<p>police officers, in cooperation with staff of the Rutgers University Center
</p>
<p>for Crime Prevention Studies, developed strategies to solve violent-crime
</p>
<p>1See Anthony Braga, &ldquo;Solving Violent Crime Problems: An Evaluation of the Jersey
</p>
<p>City Police Department&rsquo;s Pilot Program to Control Violent Crime Places,&rdquo; unpublished
</p>
<p>dissertation, Rutgers University, Newark, NJ, 1996.
</p>
<p>172</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E P R O B L E M
</p>
<p>problems at a sample of 11 places. The strategies followed a problem-
</p>
<p>oriented policing (POP) approach, in which police collect a wide variety
</p>
<p>of information about each hot spot, analyze that information to identify
</p>
<p>the source of the problem, develop tailor-made responses to do some-
</p>
<p>thing about the problem, and finally assess whether their approach actu-
</p>
<p>ally had an impact.2
</p>
<p>The evaluation involved a number of different components. One part
</p>
<p>of the research sought to identify whether &ldquo;disorderly&rdquo; activity at the hot
</p>
<p>spots had declined during the period of the study. For example, the re-
</p>
<p>searchers wanted to see whether the number of loiterers or homeless
</p>
<p>people had been reduced as a result of the efforts of the police. The
</p>
<p>treatment areas were compared to a matched group, or control group, of
</p>
<p>similar but untreated violent-crime places. Table 8.1 presents the overall
</p>
<p>2Problem-oriented policing is an important new approach to police work formulated
</p>
<p>by Herman Goldstein of the University of Wisconsin Law School. See H. Goldstein,
</p>
<p>Problem-Oriented Policing (New York: McGraw-Hill, 1990).
</p>
<p>Results at Treatment and Control Locations Derived from Observations 
</p>
<p>of Disorderly Behavior Before and After Intervention
</p>
<p>TRIAL PLACE OUTCOME
</p>
<p>1 Journal Square East �
Newport Mall
</p>
<p>2 Stegman &amp; Ocean �
Clerk &amp; Carteret
</p>
<p>3 Glenwood &amp; JFK �
Journal Square West
</p>
<p>4 Bergen &amp; Academy �
Westside &amp; Duncan
</p>
<p>5 Westside &amp; Clendenny �
Franklin &amp; Palisade
</p>
<p>6 Belmont &amp; Monticello �
MLK &amp; Wade
</p>
<p>7 MLK &amp; Atlantic �
Neptune &amp; Ocean
</p>
<p>8 MLK &amp; Armstrong �
Ocean &amp; Eastern
</p>
<p>9 Westside &amp; Virginia �
JFK &amp; Communipaw
</p>
<p>10 Park &amp; Prescott �
Dwight &amp; Bergen
</p>
<p>11 Old Bergen &amp; Danforth �
Bramhall &amp; Arlington
</p>
<p>Note: Experimental or treatment hot spots are listed in boldface type.
� � Relative improvement in experimental locations
� � Relative improvement in control locations
</p>
<p>Table 8 .1
</p>
<p>173</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>results of pre- and posttest comparisons of outcomes for the 11 matched
</p>
<p>pairs of locations. In 10 of the 11 pairs, the experimental hot spots (those
</p>
<p>receiving POP intervention) improved relative to the control locations.
</p>
<p>The research question asked by the evaluator was whether the POP
</p>
<p>approach has an impact on disorderly activity at violent-crime hot spots.
</p>
<p>The statistical problem faced is that the 11 comparisons are only a sample
</p>
<p>of such comparisons. What conclusions can the researcher make regard-
</p>
<p>A s s u m p t i o n s :  L a y i n g  t h e  F o u n d a t i o n s  
f o r  S t a t i s t i c a l  I n f e r e n c e
</p>
<p>The first step in a test of statistical significance is to establish the as-
</p>
<p>sumptions on which the test is based. These assumptions form the
</p>
<p>foundation of a test. No matter how elegant the statistics used and the
</p>
<p>approach taken, if the assumptions on which they are built are not solid,
</p>
<p>then the whole structure of the test is brought into question.
</p>
<p>Level of Measurement
</p>
<p>Our first assumption is related to the type of measurement used. Differ-
</p>
<p>ent types of tests of statistical significance demand different levels of
</p>
<p>measurement.
</p>
<p>Accordingly, it is important to state at the outset the type of measure-
</p>
<p>ment required by a test. For the binomial test, which is based on the bi-
</p>
<p>nomial distribution, a nominal-level binary measure is required. A binary
</p>
<p>measure has only two possible outcomes, as was the case with the coin
</p>
<p>toss example in Chapter 7. The type of outcome measure used to evalu-
</p>
<p>ate the impact of problem-oriented policing on disorderly activity&mdash;
</p>
<p>whether the treatment hot spot improved (or got worse) relative to the
</p>
<p>control location&mdash;fits this assumption. In stating our assumptions (as is
</p>
<p>done at the end of this section), we include a specific definition of the
</p>
<p>level of measurement required:
</p>
<p>Level of Measurement: Nominal binary scale.
</p>
<p>Shape of the Population Distribution
</p>
<p>The second assumption refers to the shape of the population distribu-
</p>
<p>tion. In statistical inference, we are generally concerned with two types
</p>
<p>of tests. In the first type&mdash;termed parametric tests&mdash;we make an as-
</p>
<p>sumption about the shape of the population distribution. For example, in
</p>
<p>a number of tests we will examine in later chapters, there is a require-
</p>
<p>appropriate for our problem is based on the binomial sampling distribution.
</p>
<p>question, we use a test of statistical significance. The specific test that is 
</p>
<p>ing the larger population of violent-crime hot spots? To answer this 
</p>
<p>174</p>
<p/>
</div>
<div class="page"><p/>
<p>A S S U M P T I O N S :  L A Y I N G T H E F O U N D A T I O N S
</p>
<p>ment that for the population to which you infer, the scores on the vari-
</p>
<p>able be normally distributed.
</p>
<p>The second type of test of statistical significance does not make a spe-
</p>
<p>cific assumption regarding the population distribution. These tests are
</p>
<p>called nonparametric tests or distribution-free tests. The advantage
</p>
<p>of nonparametric tests is that we make fewer assumptions. The disad-
</p>
<p>vantage is that nonparametric tests do not allow us to analyze data at
</p>
<p>higher levels of measurement. They are generally appropriate only for
</p>
<p>nominal and ordinal scales. The binomial test is a nonparametric test. Ac-
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method
</p>
<p>The third assumption concerns the sampling method used. When we
</p>
<p>conduct a test of statistical significance, we want our sample to be a
</p>
<p>good representation of the population from which it is drawn. Put in sta-
</p>
<p>tistical terms, we want our study to have high external validity.
</p>
<p>Let&rsquo;s suppose you are interested in attitudes toward the death penalty.
</p>
<p>Would a sample of your friends provide an externally valid sample of all
</p>
<p>Americans? Clearly not, because a sample of only your friends is not
</p>
<p>likely to include age or ethnic or class differences that typify the U.S.
</p>
<p>population. Even if we used your friends as a sample of U.S. college stu-
</p>
<p>dents, we could still identify threats to the external validity of the study.
</p>
<p>Colleges have differing criteria for admission, so it is not likely that one
</p>
<p>college will be representative of all colleges. Even as a sample of stu-
</p>
<p>dents at your college, your friends may not provide a valid sample. They
</p>
<p>may be drawn primarily from a specific year of college or have other
</p>
<p>characteristics that make them attractive as friends but also mean that
</p>
<p>they are a poor representation of others in the college.
</p>
<p>How can we draw a representative sample? The most straightfor-
</p>
<p>ward approach is to choose cases at random from the population. This
</p>
<p>type of sampling is called random sampling. Random samples are as-
</p>
<p>sumed to have high external validity compared with what may be termed
</p>
<p>convenience samples. A convenience sample consists of whatever sub-
</p>
<p>jects are readily available to the researcher. Your friends form a conve-
</p>
<p>nience sample of students at your college or of all college students.
</p>
<p>It is important to note that convenience samples are not always bad
</p>
<p>samples. For example, if you choose to examine prisoners in one prison on
</p>
<p>the assumption that prisoners there provide a cross section of the different
</p>
<p>types of prisoners in the United States, you might argue that it is a repre-
</p>
<p>sentative sample. However, if you use a convenience sample, such as pris-
</p>
<p>oners drawn from a single prison, you must always be wary of potential
</p>
<p>threats to external validity. Convenience samples are prone to systematic
</p>
<p>biases precisely because they are convenient. The characteristics that make
</p>
<p>175
</p>
<p>cordingly, in stating our assumptions we write:</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>them easy for the researcher to define are likely as well to differentiate
</p>
<p>them in one way or another from the population the researcher seeks to
</p>
<p>study.
</p>
<p>Statistical tests of significance generally assume that the researcher has
</p>
<p>used a type of random sampling called independent random sampling.
</p>
<p>Independent random sampling requires not only that cases be identified at
</p>
<p>random, but also that the selection of cases be independent. As discussed
</p>
<p>in the previous chapter, two events are statistically independent when the
</p>
<p>occurrence of one does not affect the occurrence of the other. In sam-
</p>
<p>pling, this means that the choice of one case or group of cases will not
</p>
<p>have any impact on the choice of another case or group of cases. This is a
</p>
<p>useful assumption in assuring the external validity of a study because it
</p>
<p>prevents biases that might be brought into the process of sampling.
</p>
<p>For example, suppose you want to select 1,000 prisoners from the
</p>
<p>population of all prisoners in the United States. Each time you select a
</p>
<p>prisoner for your sample, you use a random method of selection. How-
</p>
<p>ever, prison officials have told you that if you select one prisoner from a
</p>
<p>cell then you cannot select any other prisoner from that cell. Accord-
</p>
<p>In order to ensure independent random sampling, the same popula-
</p>
<p>the deck. If we didn&rsquo;t replace the card, we would influence the likeli-
</p>
<p>hood of a specific card being chosen on the next draw from the deck.
</p>
<p>For example, if we started with a full deck of 52 cards, the likelihood of
</p>
<p>getting the queen of spades would be 1 in 52. However, if we drew, say,
</p>
<p>a jack of hearts and didn&rsquo;t return it to the deck, what would be the likeli-
</p>
<p>hood of getting a queen of spades on our next draw? This time we
</p>
<p>would have only 51 cards to draw from, so our likelihood would change
</p>
<p>to 1 in 51. In order to gain a fully independent random sample, we must
</p>
<p>use a method of sampling called sampling with replacement. This
</p>
<p>means that we must use the same population each time we select a case.
</p>
<p>For every selection, the sampling frame must remain exactly the same. In
</p>
<p>this way, we can ensure that the choice of one case cannot have any im-
</p>
<p>pact on the choice of another.
</p>
<p>Though this method ensures independence, it also means that a par-
</p>
<p>ticular case may be selected more than once. For example, suppose you
</p>
<p>choose a particular prisoner as case number five in your sample. Be-
</p>
<p>cause you must use the same sampling frame each time you select a
</p>
<p>case, that prisoner is returned to the sampling frame after selection. Later
</p>
<p>ingly, after each selection of a prisoner, you must remove all of his 
</p>
<p>cellmates from your sampling frame, or universe of eligible cases. The 
</p>
<p>result is that there are now systematic reasons why you might suspect 
</p>
<p>that your sample is not representative of the population.
</p>
<p>tion of cases must be used in drawing each case for a sample. As we 
</p>
<p>discussed in Chapter 7, if we want each draw from a deck of cards to be 
</p>
<p>independent, we have to return the card chosen on any specific draw to
</p>
<p>176</p>
<p/>
</div>
<div class="page"><p/>
<p>A S S U M P T I O N S :  L A Y I N G T H E F O U N D A T I O N S
</p>
<p>in your study, you might choose that prisoner again. Accordingly, while
</p>
<p>sampling with replacement, or returning sampled cases to the sampling
</p>
<p>frame after each selection, makes statistical sense, it often does not make
</p>
<p>practical sense when you are carrying out research in the real world. If
</p>
<p>you are conducting an interview study, for example, independent ran-
</p>
<p>dom sampling would allow individuals to be interviewed more than
</p>
<p>once. It is likely that subjects would find it strange to be reinterviewed
</p>
<p>using the same interview schedule. Moreover, their responses would
</p>
<p>likely be influenced by their knowledge of the survey. Similarly, if a sub-
</p>
<p>ject or place is chosen twice in a study that involves a specific treatment
</p>
<p>or intervention, then that subject or place should be given the treatment
</p>
<p>after each selection. Here there is the difficulty that it may be harmful to
</p>
<p>provide the treatment more than once.
</p>
<p>Even when there are no specific practical barriers to sampling with re-
</p>
<p>placement, it is difficult to explain to practitioners or even many re-
</p>
<p>searchers why an individual may appear twice in the same sample. As a
</p>
<p>result, many, if not most, criminal justice studies do not replace individu-
</p>
<p>als in the sampling frame once they have been selected. Although this
</p>
<p>represents a formal violation of the assumptions of your test, in most
</p>
<p>cases its impact on your test result is negligible. This is because samples
</p>
<p>are generally very small relative to populations, and thus in practice
</p>
<p>there is little chance of selecting a case more than once even when sam-
</p>
<p>pling with replacement. If, however, your sample reaches one-fifth or
</p>
<p>more of the size of your population, you may want to include a correc-
</p>
<p>tion factor in your test.3
</p>
<p>For this test of statistical significance, we assume that researchers in
</p>
<p>the Jersey City POP study sampled cases randomly from a large popula-
</p>
<p>tion of hot spots during the sample selection month. Because it would
</p>
<p>not have been practical to implement treatments more than once at any
</p>
<p>site, the researchers did not sample with replacement.
</p>
<p>3The correction factor adjusts your test to account for the fact that you have not al-
</p>
<p>lowed individuals to be selected from the population more than once. Not including a
</p>
<p>correction factor makes it more difficult to reject the null hypothesis. That is, the in-
</p>
<p>clusion of a correction factor will make it easier for you to reject the null hypothesis.
</p>
<p>One problem criminal justice scholars face in using a correction factor is that they
</p>
<p>often want to infer to populations that are beyond their sampling frame. For example,
</p>
<p>a study of police patrol at hot spots in a particular city may sample 50 of 200 hot
</p>
<p>spots in the city during a certain month. However, researchers may be interested in
</p>
<p>making inferences to hot spots generally in the city (not just those that exist in a par-
</p>
<p>ticular month) or even to hot spots in other places. For those inferences, it would be
</p>
<p>misleading to adjust the test statistic based on the small size of the sampling frame.
</p>
<p>For a discussion of how to correct for sampling without replacement, see Paul S. Levy
</p>
<p>and Stanley Lemeshow, Sampling of Populations: Methods and Applications (New
</p>
<p>York: Wiley, 1991).
</p>
<p>177</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>The binomial test, however, like most tests of statistical significance
</p>
<p>examined in this book, assumes independent random sampling. Accord-
</p>
<p>ingly, in stating our assumptions, it is important to note both the require-
</p>
<p>ment for this test and our failure to meet that requirement. Therefore we
</p>
<p>state our assumption:
</p>
<p>Sampling Method: Independent random sampling (no replacement; sam-
</p>
<p>ple is small relative to population).
</p>
<p>Throughout this text, we state the assumptions of a test and then place
</p>
<p>any violations of assumptions in parentheses. This is good practice, as it
</p>
<p>will alert you to the fact that in many studies there are violations of one
</p>
<p>type or another of assumptions. Some of these violations are not impor-
</p>
<p>tant. For example, not sampling with replacement in this study does not
</p>
<p>affect the test outcome because the population of hot spots is assumed
</p>
<p>to be very large relative to the sample. However, you will sometimes
</p>
<p>find more serious violations of assumptions. In those cases, you will
</p>
<p>have to take a more critical view of the results of the test.
It is good practice to define not only the sampling method used but
</p>
<p>also the sampling frame of your study. In our example, we can make in-
</p>
<p>ferences based on our random sample to the population of hot spots in
</p>
<p>Jersey City during the month of sample selection. Accordingly, we state
</p>
<p>in our assumptions:
</p>
<p>Sampling Frame: Hot spots of violent crime in one month in Jersey
</p>
<p>City.
</p>
<p>Our sampling frame reminds us of the specific population to which our
</p>
<p>sample infers. However, researchers usually want to infer beyond the
</p>
<p>specific population identified by their sampling frame. For example, the
</p>
<p>population of interest for the POP study is likely to be hot spots through-
</p>
<p>out the year, not just those in a specific month. Researchers may even
</p>
<p>want to infer to violent-crime hot spots generally, not just those in Jersey
</p>
<p>City.
</p>
<p>We cannot assume that our sample is a representative sample for
</p>
<p>these inferences based on our sampling method, since these populations
</p>
<p>did not constitute our sampling frame. However, we can ask whether
</p>
<p>our sample is likely to provide valid inferences to those populations. In
</p>
<p>the case of hot spots in Jersey City, we would need to question whether
</p>
<p>there is any reason to suspect that hot spots chosen in the month of
</p>
<p>study were different from those that would be found in other months of
</p>
<p>the year. For inferences to the population of hot spots in other locations,
</p>
<p>we would have to assume that Jersey City hot spots are similar to those
</p>
<p>in other places and would respond similarly to POP interventions. In
</p>
<p>making any inference beyond your sampling frame, you must try to
</p>
<p>identify all possible threats to external validity.
</p>
<p>178</p>
<p/>
</div>
<div class="page"><p/>
<p>A S S U M P T I O N S :  L A Y I N G T H E F O U N D A T I O N S
</p>
<p>The Hypotheses
</p>
<p>The final assumptions we make in a test of statistical inference refer to
</p>
<p>the hypotheses of our study. As discussed in Chapter 6, hypotheses are
</p>
<p>developed from the research questions raised in a project. Hypotheses
</p>
<p>must be stated before the researcher collects outcome data for a study. If
</p>
<p>hypotheses are stated only after data have been collected and analyzed,
</p>
<p>the researcher might be tempted to make changes in the hypotheses that
</p>
<p>unfairly affect the tests of statistical significance that are conducted.
</p>
<p>As discussed in Chapter 6, the researcher ordinarily begins by defin-
</p>
<p>ing the research hypothesis. In the problem-oriented policing study, we
</p>
<p>might state our research hypothesis in three different ways:
</p>
<p>Hypothesis 1. Incivilities in treatment hot spots decline relative to incivil-
</p>
<p>ities in control hot spots after POP intervention.
</p>
<p>Hypothesis 2. Incivilities in treatment hot spots increase relative to inci-
</p>
<p>vilities in control hot spots after POP intervention.
</p>
<p>Hypothesis 3. The level of incivilities in treatment hot spots relative to
</p>
<p>incivilities in control hot spots changes after POP intervention.
</p>
<p>hypotheses. The first two research hypotheses are directional hypotheses
</p>
<p>because they specify the direction, or type of relationship, that is ex-
</p>
<p>pected. For example, hypothesis 1 is concerned only with whether the
</p>
<p>POP program is successful in reducing incivilities. If the researcher
</p>
<p>adopts this hypothesis, then he or she is stating that the statistical test
</p>
<p>employed will not be concerned with the second hypothesis&mdash;that the
</p>
<p>intervention makes matters worse and increases incivilities. The third hy-
</p>
<p>pothesis is a nondirectional hypothesis. In this case, the researcher is in-
</p>
<p>terested in testing the possibility that the intervention improves hot spots
</p>
<p>or makes them worse.
</p>
<p>In the POP study, researchers wanted to assess both positive and neg-
</p>
<p>ative outcomes. Although they believed that problem-oriented policing
</p>
<p>should reduce incivilities at violent-crime hot spots, they did not want to
</p>
<p>preclude at the outset a finding that the program actually made matters
</p>
<p>worse. Accordingly, they used a nondirectional research hypothesis:
</p>
<p>&ldquo;The level of incivilities in treatment hot spots relative to incivilities in
</p>
<p>control hot spots changes after POP intervention.&rdquo; The null hypothesis is
</p>
<p>&ldquo;The level of incivilities in treatment hot spots does not change relative
</p>
<p>to incivilities in control hot spots after POP intervention.&rdquo;
</p>
<p>In practice, the null hypothesis may be stated in terms of probabili-
</p>
<p>ties, just as we could state the coin toss hypothesis in the last chapter
</p>
<p>in terms of probabilities. In this study, the researchers examined (for
</p>
<p>each matched pair of hot spots) whether the hot spot that received the
</p>
<p>problem-oriented policing intervention improved or worsened relative to
</p>
<p>Recall from Chapter 6 that we distinguish directional from nondirectional
</p>
<p>179</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>the control location. The null hypothesis suggests that the treatment and
</p>
<p>control hot spots are equally likely to improve. Put in terms of probabili-
</p>
<p>ties, there is a 0.50 chance of success (P � 0.50) for the intervention
</p>
<p>under the null hypothesis. The research hypothesis represents all other
</p>
<p>possible outcomes (P � 0.50). Remember that our hypotheses are state-
</p>
<p>ments about the populations examined. Accordingly, in stating the hy-
</p>
<p>potheses, we use symbols appropriate for population parameters&mdash;in
</p>
<p>this case P rather than p. Stating our assumptions, we write
</p>
<p>Hypotheses:
</p>
<p>H0: The level of incivilities in treatment hot spots does not change rela-
</p>
<p>tive to incivilities in control hot spots after POP intervention, P � 0.50.
</p>
<p>H1: The level of incivilities in treatment hot spots relative to incivilities in
</p>
<p>control hot spots changes after POP intervention, P � 0.50.
</p>
<p>Stating All of the Assumptions
</p>
<p>Our assumptions may be stated as follows:
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Nominal binary scale.
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method: Independent random sampling (no replacement; sam-
</p>
<p>ple is small relative to population).
</p>
<p>Sampling Frame: Hot spots of violent crime in one month in Jersey City.
</p>
<p>Hypotheses:
</p>
<p>H0: The level of incivilities in treatment hot spots does not change rela-
</p>
<p>tive to incivilities in control hot spots after POP intervention, P � 0.50.
</p>
<p>H1: The level of incivilities in treatment hot spots relative to incivilities in
</p>
<p>control hot spots changes after POP intervention, P � 0.50.
</p>
<p>S e l e c t i n g  a  S a m p l i n g  D i s t r i b u t i o n
</p>
<p>In stating our hypotheses, we already noted the specific requirements of
</p>
<p>the binomial sampling distribution. Now we must state why we have
</p>
<p>chosen the binomial distribution and identify the specific characteristics
</p>
<p>of the sampling distribution that will be used to assess the risk of falsely
</p>
<p>rejecting the null hypothesis in our problem-oriented policing example.
</p>
<p>Choosing a sampling distribution is one of the most important decisions
</p>
<p>that researchers make in statistical inference. As we will show in later
</p>
<p>chapters, there are a number of different types of sampling distributions.
</p>
<p>Moreover, as with the binomial distribution, a single type of sampling
</p>
<p>distribution may have different forms depending on the problem 
</p>
<p>180</p>
<p/>
</div>
<div class="page"><p/>
<p>S E L E C T I N G A S A M P L I N G D I S T R I B U T I O N
</p>
<p>Because our measure is nominal and binary (see assumptions), we se-
</p>
<p>lected the binomial distribution for our test. The specific distribution that we
</p>
<p>use is based on our null hypothesis and the size of our sample. As illus-
</p>
<p>trated in Chapter 7, the binomial distribution provides the likelihood of gain-
</p>
<p>ing a particular number of successes (heads in the example of the coin toss)
</p>
<p>in a fixed number of trials. In order to assess that likelihood, we also need
</p>
<p>to know what the probability of a success or failure is on any particular trial.
</p>
<p>In our example, there are 11 trials, or 11 matched comparisons. Our
</p>
<p>null hypothesis states that the likelihood of a success for any comparison
</p>
<p>is 0.50. To build our sampling distribution, we apply the binomial for-
</p>
<p>mula to each of the 12 possible outcomes that could be gained in our
</p>
<p>study, under the assumption that P � 0.50. This is done in Table 8.2.
</p>
<p>The resulting distribution is presented in Table 8.3.
</p>
<p>Computation of Sampling Distribution of Success or Failure in 11 Trials
</p>
<p>0 successes 1(0.00049) � 0.00049
</p>
<p>1 success 11(0.00049) � 0.00537
</p>
<p>2 successes 55(0.00049) � 0.02686
</p>
<p>3 successes 165(0.00049) � 0.08057
</p>
<p>4 successes 330(0.00049) � 0.16113
</p>
<p>5 successes 432(0.00049) � 0.22638*
</p>
<p>6 successes 432(0.00049) � 0.22638*
</p>
<p>7 successes 330(0.00049) � 0.16113
</p>
<p>8 successes 165(0.00049) � 0.08057
</p>
<p>9 successes 55(0.00049) � 0.02686
</p>
<p>10 successes 11(0.00049) � 0.00537
</p>
<p>11 successes 1(0.00049) � 0.00049
</p>
<p>*Probabilities contain rounding error.
</p>
<p> 
39,916,800
</p>
<p>39,916,800(11 � 11)!
 � 
</p>
<p>39,916,800
</p>
<p>39,916,800
 � 1
</p>
<p> 
39,916,800
</p>
<p>3,628,800(11 � 10)!
 � 
</p>
<p>39,916,800
</p>
<p>3,628,800
 � 11
</p>
<p> 
39,916,800
</p>
<p>362,880(11 � 9)!
 � 
</p>
<p>39,916,800
</p>
<p>725,760
 � 55
</p>
<p> 
39,916,800
</p>
<p>40,320(11 � 8)!
 � 
</p>
<p>39,916,800
</p>
<p>241,920
 � 165
</p>
<p> 
39,916,800
</p>
<p>5,040(11 � 7)!
 � 
</p>
<p>39,916,800
</p>
<p>120,960
 � 330
</p>
<p> 
39,916,800
</p>
<p>720(11 � 6)!
 � 
</p>
<p>39,916,800
</p>
<p>86,400
 � 462
</p>
<p> 
39,916,800
</p>
<p>120(11 � 5)!
 � 
</p>
<p>39,916,800
</p>
<p>86,400
 � 462
</p>
<p> 
39,916,800
</p>
<p>24(11 � 4)!
 � 
</p>
<p>39,916,800
</p>
<p>120,960
 � 330
</p>
<p> 
39,916,800
</p>
<p>6(11 � 3)!
 � 
</p>
<p>39,916,800
</p>
<p>241,920
 � 165
</p>
<p> 
39,916,800
</p>
<p>2(11 � 2)!
 � 
</p>
<p>39,916,800
</p>
<p>725,760
 � 55
</p>
<p> 
39,916,800
</p>
<p>1(11 � 1)!
 � 
</p>
<p>39,916,800
</p>
<p>3,628,800
 � 11
</p>
<p> 
39,916,800
</p>
<p>1(11 � 0)!
 � 
</p>
<p>39,916,800
</p>
<p>39,916,800
 � 1
</p>
<p>�Nr �pr (1 � p)N�r�Nr � � N !r !(N � r )!
</p>
<p>Table 8.2
</p>
<p>research problem examined, then the conclusion reached will be suspect.
</p>
<p>examined. If the sampling distribution used is inappropriate for the 
</p>
<p>181</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>S i g n i f i c a n c e  L e v e l  a n d  R e j e c t i o n  R e g i o n
</p>
<p>Having selected the distribution that will be used to assess Type I error,
</p>
<p>we are ready to define the outcomes that will lead us to reject the null
</p>
<p>hypothesis. Our first step is to choose the significance level of our test.
</p>
<p>As described in Chapter 6, the significance level of a test is the amount
</p>
<p>of Type I error we are willing to risk in rejecting the null hypothesis. By
</p>
<p>convention, criminal justice researchers use a 5% significance threshold.
</p>
<p>But, as discussed in Chapter 6, we should consider at the outset whether
</p>
<p>a more lenient or more stringent significance level is appropriate for our
</p>
<p>study.
</p>
<p>As researchers in the problem-oriented policing study do not present
</p>
<p>any special reason for altering conventionally accepted levels of signifi-
</p>
<p>cance, we will set a 5% significance threshold for our test of statistical
</p>
<p>significance. As noted in Chapter 6, in articles and books the significance
</p>
<p>level is often expressed by the Greek letter �. For our test, � � 0.05.
</p>
<p>The significance level defines the Type I error we are willing to risk in
</p>
<p>our test. But it does not tell us directly what outcomes in our sample
</p>
<p>would lead us to reject the null hypothesis. For this, we need to turn to
</p>
<p>our sampling distribution and define an area within it called a rejection
</p>
<p>region. The rejection region of a test is the area in the sampling distribu-
</p>
<p>tion that includes those outcomes that would lead to rejection of the null
</p>
<p>hypothesis. If the observed significance level of a test, or the p value of
</p>
<p>the test, falls within the rejection region, then the researcher rejects the
</p>
<p>null hypothesis and concludes that the outcome is statistically significant.
</p>
<p>The area covered by the rejection region is equivalent to the significance
</p>
<p>level of a test. The point at which the rejection region begins is called
</p>
<p>the critical value because it is the point at which the test becomes criti-
</p>
<p>cal and leads the researcher to reject the null hypothesis.
</p>
<p>Sampling Distribution of Success or Failure in 11 Trials
</p>
<p>OUTCOME OF TRIALS OVERALL PROBABILITY
</p>
<p>0 successes 0.00049
1 success 0.00537
2 successes 0.02686
3 successes 0.08057
4 successes 0.16113
5 successes 0.22559
6 successes 0.22559
7 successes 0.16113
8 successes 0.08057
9 successes 0.02686
</p>
<p>10 successes 0.00537
11 successes 0.00049
</p>
<p>Table 8.3
</p>
<p>182</p>
<p/>
</div>
<div class="page"><p/>
<p>S I G N I F I C A N C E L E V E L A N D R E J E C T I O N R E G I O N
</p>
<p>In the problem-oriented policing example, the rejection region in-
</p>
<p>cludes 5% of the sampling distribution. Our initial problem is to define
</p>
<p>which 5%. Should we define the rejection region to be in the middle of
</p>
<p>the distribution represented in Table 8.3&mdash;for example, at 5 or 6 suc-
</p>
<p>cesses in 11 comparisons? Or should we look only at the extreme values
</p>
<p>on the positive side of the distribution, where there are mostly successes?
</p>
<p>Or should we include the area on the negative side of the distribution,
</p>
<p>where there are no successes?
</p>
<p>Choosing a One-Tailed or a Two-Tailed Rejection Region
</p>
<p>The answer to our questions comes in part from common sense and in
</p>
<p>part from our assumptions. It just would not make sense to place the re-
</p>
<p>jection region in the middle of the sampling distribution. We are trying
</p>
<p>to decide whether the outcomes observed in our sample are very differ-
</p>
<p>ent from the outcomes that would be expected if problem-oriented
</p>
<p>policing had no impact. Putting the rejection region in the middle of the
</p>
<p>distribution would place it among those outcomes that are most likely
</p>
<p>under the null hypothesis. Clearly, we want the rejection region to be on
</p>
<p>the edges of the distribution, or in what statisticians call the tails of the
</p>
<p>distribution. These are the unlikely events&mdash;those that we would not
</p>
<p>expect if the null hypothesis were true. As indicated in our sampling dis-
</p>
<p>tribution in Table 8.3, we would expect to get 11 successes in a row in
</p>
<p>about 5 of 10,000 samples if the program had no impact on the popula-
</p>
<p>tion. This is a very unlikely event and one that would lead us to reject
</p>
<p>the null hypothesis.
</p>
<p>But zero successes is also an unlikely event, with the same probability
</p>
<p>of occurrence as 11 successes. Should we include only one tail of the dis-
</p>
<p>tribution in our rejection region&mdash;the tail that assesses whether the pro-
</p>
<p>gram was a success? Or should we also include the opposite side of the
</p>
<p>distribution, which suggests that the program led to more disorder? Our
</p>
<p>answer is drawn from the research hypothesis that we stated in our as-
</p>
<p>sumptions. We chose a nondirectional research hypothesis, meaning that
</p>
<p>we are interested in evaluating both the possibility that the experimental
</p>
<p>sites improved relative to the control hot spots and the potential outcome
</p>
<p>that they got worse relative to the control hot spots. In terms of the sam-
</p>
<p>pling distribution, our research hypothesis suggests that the rejection re-
</p>
<p>gion for our test should be split between both tails of the distribution.
</p>
<p>This type of test is called a two-tailed test of significance. If we had
</p>
<p>stated a directional research hypothesis, we would be concerned with out-
</p>
<p>comes on only one side of the sampling distribution. Such a test is called a
</p>
<p>one-tailed test of significance. For example, if our research hypothesis
</p>
<p>were that incivilities in treatment hot spots decrease relative to incivilities
</p>
<p>in control hot spots after POP intervention, we would be concerned only
</p>
<p>with outcomes on the side of the distribution that shows program success.
</p>
<p>183</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>The choice of a one-tailed or two-tailed test of statistical significance
</p>
<p>has important implications for the types of study outcomes that will lead
</p>
<p>to rejection of the null hypothesis. Because our test is a two-tailed test,
</p>
<p>the rejection region must be divided between both sides of the sampling
</p>
<p>distribution. This means in practice that the total significance level of
</p>
<p>0.05 must be divided in half. Half of the rejection region, or 0.025, is
</p>
<p>found in the tail associated with success of the program, and half, or
</p>
<p>0.025, in the tail associated with failure.
</p>
<p>What outcomes would lead to rejection of the null hypothesis in our
</p>
<p>example? When we add 0 and 1 successes or 10 and 11 successes, we
</p>
<p>gain a probability value of 0.00586 (in each tail of the distribution,
</p>
<p>0.00049 � 0.00537). This is less than the 0.025 value that we have de-
</p>
<p>fined as the rejection region for each tail of our test. Accordingly, an out-
</p>
<p>come of 0, 1, 10 or 11 would lead to an observed significance level less
</p>
<p>than the significance level of 0.05 that we have set, and thus we would
</p>
<p>reject the null hypothesis (p � 0.05). However, including 2 or 9 suc-
</p>
<p>cesses, each of which has a probability value of 0.027, increases the area
</p>
<p>of the distribution to 0.066. This area is larger than our rejection region.
</p>
<p>An outcome of 9 or 2 would result in an observed significance level
</p>
<p>greater than 0.05, and thus we would fail to reject the null hypothesis.
</p>
<p>Figure 8.1 presents the binomial probabilities for our example and high-
</p>
<p>lights the two tails of the distribution that are used to test our nondirec-
</p>
<p>tional hypothesis.
</p>
<p>P
ro
</p>
<p>b
ab
</p>
<p>il
it
</p>
<p>y
</p>
<p>0.25
</p>
<p>0.2
</p>
<p>0.15
</p>
<p>0.1
</p>
<p>0.05
</p>
<p>0
</p>
<p>Number of Successes
</p>
<p>0      1     2      3     4      5     6       7     8      9     10     11
</p>
<p>0.0005 0.005
</p>
<p>0.03 0.03
</p>
<p>0.08 0.08
</p>
<p>0.16 0.16
</p>
<p>0.23 0.23
</p>
<p>0.005 0.0005
</p>
<p>Outcomes in the 
rejection region
</p>
<p>Outcomes
in the 
rejection
region
</p>
<p>Outcomes That Would Lead to Rejecting the Null Hypothesis 
</p>
<p>for a Two-Tailed Test of Significance (� � 0.05)
Figure 8.1
</p>
<p>184</p>
<p/>
</div>
<div class="page"><p/>
<p>S I G N I F I C A N C E L E V E L A N D R E J E C T I O N R E G I O N
</p>
<p>But what if we state a directional research hypothesis? How does
</p>
<p>this affect our rejection region? In this case, we calculate the area of the
</p>
<p>rejection region on only one side of the sampling distribution. Parts a
</p>
<p>and b of Figure 8.2 present the binomial probabilities for our two dis-
</p>
<p>tinct directional hypotheses and highlight the tail of the distribution
</p>
<p>that is potentially of interest. For example, if our research hypothesis is
</p>
<p>that incivilities in treatment hot spots decline relative to incivilities in
</p>
<p>control hot spots after POP intervention, we look at outcomes only on
</p>
<p>the tail of the distribution that shows program success (Figure 8.2b).
</p>
<p>Because we are concerned only about these outcomes, all 5% of the re-
</p>
<p>jection region is placed in this one tail of the distribution. We do not
</p>
<p>have to split the area of the rejection region. In this example, outcomes
</p>
<p>of 9, 10, and 11 successes are all within the rejection region, because
</p>
<p>adding their probabilities results in a value of 0.033 (0.00049 � 0.00537
</p>
<p>� 0.02686). An outcome of 9, 10, or 11 results in an observed signifi-
</p>
<p>cance level that is less than the 5% significance threshold of our test
</p>
<p>(see Figure 8.2b). Adding the probability of 8 successes (or 0.08057)
</p>
<p>puts us above that threshold. If our research hypothesis is that incivili-
</p>
<p>ties increase in treatment hot spots relative to control hot spots, then
</p>
<p>we look at outcomes only on the opposite tail of the distribution (Fig-
</p>
<p>ure 8.2a). In this case, outcomes of 0, 1, and 2 successes lead us to re-
</p>
<p>ject the null hypothesis.
</p>
<p>This example reinforces a rule described earlier: It is important to
</p>
<p>state the research hypothesis before you gain study outcomes. What if
</p>
<p>the problem-oriented policing hot spots improved relative to control
</p>
<p>locations in nine comparisons? With a one-tailed test, the result would
</p>
<p>fall within our rejection region and lead to rejection of the null hypoth-
</p>
<p>esis. With a two-tailed test, the result would be outside our rejection re-
</p>
<p>gion. The choice of a directional or nondirectional research hypothesis
</p>
<p>can have an important impact on our conclusions. Merely by stating
</p>
<p>the research hypothesis a bit differently, we can change the outcome of
</p>
<p>the test.
</p>
<p>A one-tailed test makes it easier to reject the null hypothesis based on
</p>
<p>outcomes on one side of a sampling distribution because it precludes re-
</p>
<p>jection of the null hypothesis based on outcomes on the opposite side.
</p>
<p>The price of a larger rejection region in one-tail of the sampling distribu-
</p>
<p>tion is no rejection region in the other tail. Similarly, the price of being
</p>
<p>able to examine outcomes on both sides of the distribution, as is the
</p>
<p>case with a two-tailed test, is that the rejection region will be smaller on
</p>
<p>each side. The benefit is that you can assess results in both directions. If
</p>
<p>you already know the outcomes of a test, you might be tempted to ad-
</p>
<p>just the direction of the test according to the observed outcomes of a
</p>
<p>study. Taking such an approach unfairly adjusts the rejection region to
</p>
<p>your advantage.
</p>
<p>185</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>P
ro
</p>
<p>b
ab
</p>
<p>il
it
</p>
<p>y
</p>
<p>0.25
</p>
<p>0.2
</p>
<p>0.15
</p>
<p>0.1
</p>
<p>0.05
</p>
<p>0
</p>
<p>Number of Successes
</p>
<p>0      1     2      3     4      5     6      7     8      9     10     11
</p>
<p>0.0005 0.005
</p>
<p>0.03 0.03
</p>
<p>0.08 0.08
</p>
<p>0.16 0.16
</p>
<p>0.23 0.23
</p>
<p>0.005 0.0005
</p>
<p>Outcomes
in the 
rejection
region
</p>
<p>P
ro
</p>
<p>b
ab
</p>
<p>il
it
</p>
<p>y
</p>
<p>0.25
</p>
<p>0.2
</p>
<p>0.15
</p>
<p>0.1
</p>
<p>0.05
</p>
<p>0
</p>
<p>Number of Successes
</p>
<p>0      1     2      3     4      5     6       7     8      9     10     11
</p>
<p>0.0005 0.005
</p>
<p>0.03 0.03
</p>
<p>0.08 0.08
</p>
<p>0.16 0.16
</p>
<p>0.23 0.23
</p>
<p>0.005 0.0005
</p>
<p>Outcomes
in the 
rejection
region
</p>
<p>Outcomes That Would Lead to Rejecting the Null Hypothesis 
</p>
<p>for a One-Tailed Test of Significance (� � 0.05)
Figure 8.2
</p>
<p>(b) Focus on Program Successes
</p>
<p>(a) Focus on Program Failures
</p>
<p>186</p>
<p/>
</div>
<div class="page"><p/>
<p>M A K I N G A D E C I S I O N
</p>
<p>T h e  T e s t  S t a t i s t i c
</p>
<p>In most tests of statistical significance, it is necessary to convert the spe-
</p>
<p>cific outcome of a study to a test statistic. A test statistic expresses the
</p>
<p>value of your outcome in units of the sampling distribution employed in
</p>
<p>your test. For the binomial distribution, the units are simply the number
</p>
<p>of successes in the total number of trials. The test statistic for our POP
</p>
<p>intervention example is 10.
</p>
<p>M a k i n g  a  D e c i s i o n
</p>
<p>The final step in a test of statistical significance is making a decision. If
</p>
<p>you have laid out all of the steps discussed above, then your choice
</p>
<p>should be easy. If your test statistic falls within the rejection region, then
</p>
<p>you reject the null hypothesis. This means in practice that the observed
</p>
<p>significance level of your test is less than the criterion significance level
</p>
<p>that you set when you defined the significance level and rejection region
</p>
<p>for your test. If the test statistic does not fall in the rejection region, you
</p>
<p>cannot reject the null hypothesis. In our example, the test statistic (10)
</p>
<p>does fall in the rejection region, which includes 0, 1, 10, and 11 suc-
</p>
<p>cesses. In this case, our observed significance level is less than the 0.05
</p>
<p>threshold we set earlier. Our decision, then, is to reject the null hypothe-
</p>
<p>sis that incivilities in treatment hot spots do not change relative to incivil-
</p>
<p>ities in control hot spots after POP intervention. We conclude that the
</p>
<p>differences observed are statistically significant.
</p>
<p>But what does this mean? When we say that a result is statistically
</p>
<p>significant, we are not claiming that it is substantively important. The
</p>
<p>importance of a result depends on such issues as whether the research
</p>
<p>affects real-life criminal justice decision making or whether it con-
</p>
<p>tributes new knowledge to a specific area of criminology or criminal
</p>
<p>justice. We also are not stating that we are certain that the null hypothe-
</p>
<p>sis is untrue for the population. Without knowledge of the population
</p>
<p>parameter, we cannot answer this question with certainty. Statistical sig-
</p>
<p>nificance has a very specific interpretation. The fact that an outcome is
</p>
<p>statistically significant means that it falls within the rejection region of
</p>
<p>your test. This happens when the observed significance level for a test
</p>
<p>is smaller than the significance criterion, or significance level, set at the
</p>
<p>outset of the test. A statistically significant result is one that is unlikely if
</p>
<p>the null hypothesis is true for the population. Whenever we make a
</p>
<p>statement that a result is statistically significant, we do it with the recog-
</p>
<p>nition that we are risking a certain level of Type I error. In this test, as
</p>
<p>187</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>in most tests of statistical significance in criminal justice, we were will-
</p>
<p>ing to take a 5% risk of falsely rejecting the null hypothesis.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>The first stage in a test of statistical significance is to state one&rsquo;s as-
</p>
<p>sumptions. The first assumption is about the type of measurement
</p>
<p>used. The second assumption concerns the shape of the population
</p>
<p>distribution. A parametric test is one that makes assumptions about
</p>
<p>the shape of the population distribution. A nonparametric test
</p>
<p>makes no such assumptions. Although nonparametric tests have the
</p>
<p>advantage of making fewer assumptions, they are generally used only
</p>
<p>for nominal and ordinal scales. The third assumption relates to the
</p>
<p>sampling method. A random sample is generally considered to be
</p>
<p>pling, it is in theory necessary to return the subject to the sampling
</p>
<p>frame after selection. Sampling with replacement creates practical
</p>
<p>problems, however, and is generally not required if the sample is
</p>
<p>small relative to the population. The fourth assumption states the null
</p>
<p>and research hypotheses. Care should be taken in framing them and in
</p>
<p>deciding whether the research hypothesis should be directional.
</p>
<p>The second stage is to select an appropriate sampling distribution.
</p>
<p>The third stage is to select a significance level. The significance level
</p>
<p>determines the size of the rejection region and the location of the
</p>
<p>critical values of the test. If a test result falls within the rejection re-
</p>
<p>gion, the researcher is prepared to reject the null hypothesis. This
</p>
<p>means that the observed significance level of the test is less than the
</p>
<p>significance level the researcher set at the outset of the test. If the hy-
</p>
<p>potheses are directional, then the researcher will be concerned only
</p>
<p>with one tail of the distribution, and the entire rejection region will
</p>
<p>be placed on one side of the distribution (a one-tailed test of signifi-
</p>
<p>cance). If the hypotheses are nondirectional, then the researcher is
</p>
<p>concerned with results in both tails, and the rejection region will be
</p>
<p>divided equally between both sides of the distribution (a two-tailed
</p>
<p>test of significance).
</p>
<p>The fourth stage involves calculating a test statistic. The study re-
</p>
<p>sult is now converted into the units of the sampling distribution. Fi-
</p>
<p>nally, a decision is made: The null hypothesis will be rejected if the
</p>
<p>test statistic falls within the rejection region. When such a decision can
</p>
<p>be made, the results are said to be statistically significant.
</p>
<p>accepted form of sampling. To ensure the independence of the sam-
</p>
<p>more representative, or to have greater external validity, than a 
</p>
<p>convenience sample. Independent random sampling is the most 
</p>
<p>188</p>
<p/>
</div>
<div class="page"><p/>
<p>K E Y T E R M S
</p>
<p>assumptions Statements that identify the
</p>
<p>requirements and characteristics of a test of
</p>
<p>statistical significance. These are the foun-
</p>
<p>dations on which the rest of the test is built.
</p>
<p>convenience sample A sample chosen not
</p>
<p>at random, but according to criteria of expe-
</p>
<p>dience or accessibility to the researcher.
</p>
<p>critical value The point at which the re-
</p>
<p>jection region begins.
</p>
<p>distribution-free tests Another name for
</p>
<p>nonparametric tests.
</p>
<p>external validity The extent to which a
</p>
<p>study sample is reflective of the population
</p>
<p>from which it is drawn. A study is said to
</p>
<p>have high external validity when the sam-
</p>
<p>ple used is representative of the population
</p>
<p>to which inferences are made.
</p>
<p>independent random sampling A form
</p>
<p>of random sampling in which the fact that
</p>
<p>one subject is drawn from a population in
</p>
<p>no way affects the probability of drawing
</p>
<p>any other subject from that population.
</p>
<p>nonparametric tests Tests of statistical
</p>
<p>significance that make no assumptions as
</p>
<p>to the shape of the population distribution.
</p>
<p>one-tailed test of significance A test of
</p>
<p>statistical significance in which the region
</p>
<p>for rejecting the null hypothesis falls on
</p>
<p>only one side of the sampling distribution.
</p>
<p>One-tailed tests are based on directional re-
</p>
<p>search hypotheses.
</p>
<p>parametric tests Tests of statistical signifi-
</p>
<p>cance that make assumptions as to the
</p>
<p>shape of the population distribution.
</p>
<p>random sampling Drawing samples from
</p>
<p>the population in a manner that ensures
</p>
<p>every individual in that population an
</p>
<p>equal chance of being selected.
</p>
<p>rejection region The area of a sampling
</p>
<p>distribution containing the test statistic val-
</p>
<p>ues that will cause the researcher to reject
</p>
<p>the null hypothesis.
</p>
<p>representative sample A sample that
</p>
<p>reflects the population from which it is
</p>
<p>drawn.
</p>
<p>sampling frame The universe of eligible
</p>
<p>cases from which a sample is drawn.
</p>
<p>sampling with replacement A sampling
</p>
<p>method in which individuals in a sample
</p>
<p>are returned to the sampling frame after
</p>
<p>they have been selected. This raises the
</p>
<p>possibility that certain individuals in a pop-
</p>
<p>ulation may appear in a sample more than
</p>
<p>once.
</p>
<p>statistically significant Describing a test
</p>
<p>statistic that falls within the rejection region
</p>
<p>defined by the researcher. When this oc-
</p>
<p>curs, the researcher is prepared to reject the
</p>
<p>null hypothesis and state that the outcome
</p>
<p>or relationship is statistically significant.
</p>
<p>tails of the distribution The extremes on
</p>
<p>the sides of a sampling distribution. The
</p>
<p>events represented by the tails of a sam-
</p>
<p>pling distribution are those deemed least
</p>
<p>likely to occur if the null hypothesis is true
</p>
<p>for the population.
</p>
<p>test statistic The outcome of the study,
</p>
<p>expressed in units of the sampling distribu-
</p>
<p>tion. A test statistic that falls within the re-
</p>
<p>jection region will lead the researcher to re-
</p>
<p>ject the null hypothesis.
</p>
<p>two-tailed test of significance A test of
</p>
<p>statistical significance in which the region
</p>
<p>for rejecting the null hypothesis falls on
</p>
<p>both sides of the sampling distribution.
</p>
<p>Two-tailed tests are based on nondirec-
</p>
<p>tional research hypotheses.
</p>
<p>K e y  T e r m s
</p>
<p>189</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>E x e r c i s e s
</p>
<p>8.1 Answer the following conceptual questions:
</p>
<p>a. Is it better to have more or fewer assumptions at the beginning of a
test of statistical significance? Explain your answer.
</p>
<p>b. Why is it important to state all of the assumptions at the outset of
the test?
</p>
<p>c. In what sense can stating the null and research hypotheses be seen
as making assumptions?
</p>
<p>8.2 Gatley University is an elite university of 1,000 students. Nadia, a stu-
dent studying Chinese at the university, wishes to determine the aver-
age IQ of students at Gatley. She has decided that her sample size will
be 50, and she is considering several different sampling methods. For
each method, state the sampling frame and discuss whether the sam-
pling method is random and whether it is independent.
</p>
<p>a. Nadia chooses 50 names at random from the list of language stu-
dents at the university.
</p>
<p>b. Nadia asks 50 of her acquaintances at the university if they would
mind taking an IQ test.
</p>
<p>c. Nadia chooses the first two students from the alphabetical list of
each of the 25 university departments.
</p>
<p>d. Nadia takes all 1,000 names and puts them into a hat. She draws
out a name, writes it down, and then puts it back in the hat and
draws again. This procedure is repeated 50 times.
</p>
<p>8.3 Hale Prison is renowned for its poor internal discipline. The new
prison governor wants to tackle this problem and decides to investi-
gate whether removing prisoners&rsquo; visiting privileges will act as a deter-
rent against future misbehaving. From 100 prisoners who recently
took part in a violent prison riot, he selects the 25 inmates with the
worst disciplinary records, removes their visiting privileges, and begins
to monitor their progress relative to the others.
</p>
<p>a. Does this method meet the criteria of independent random
sampling?
</p>
<p>b. Is independent sampling possible in this case?
</p>
<p>c. Describe a more appropriate sampling method.
</p>
<p>8.4 For each of the following hypotheses, state whether a one-tailed or a
two-tailed test of statistical significance would be appropriate. In each
case, explain your choice.
</p>
<p>a. H1: Citizens over the age of 50 are more likely to be the victims of
assault than citizens under the age of 50.
</p>
<p>190</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S
</p>
<p>b. H1: Children raised by adopted parents have rates of delinquency
different from those of children raised by their biological parents.
</p>
<p>c. H1: The experience of imprisonment has an impact on the chances
of an ex-convict reoffending.
</p>
<p>d. H1: Women are more likely than men to support increased sentences
for rapists.
</p>
<p>e. H1: Persons who are not victims of assault have lower levels of
anger than persons who have been victims of assault.
</p>
<p>f. H1: White offenders are less likely to be sentenced to prison than
Hispanic offenders.
</p>
<p>g. H1: Teenagers have rates of crime that are different from adult rates
of crime.
</p>
<p>h. H1: Defendants charged with property crimes have different rates of
pretrial misconduct than defendants charged with violent crimes.
</p>
<p>i. H1: Male defendants are more likely to be held on bail than female
defendants.
</p>
<p>j. H1: Women are more supportive of capital punishment than men.
</p>
<p>k. H1: States with higher unemployment rates have higher rates of
property crime.
</p>
<p>l. H1: The level of poverty in a neighborhood affects the neighbor-
hood&rsquo;s crime rate.
</p>
<p>m. H1: Democrats are less supportive of cutting taxes than Republicans.
</p>
<p>n. H1: Graduates from private law schools are more likely to become
federal judges than graduates from state law schools.
</p>
<p>8.5 In Chapter 7, we constructed a binomial distribution showing the
chances of success and failure for ten tosses of a fair coin. The distri-
bution was as follows:
</p>
<p>0 heads  0.001
</p>
<p>1 head 0.010
</p>
<p>2 heads  0.044
</p>
<p>3 heads  0.118
</p>
<p>4 heads  0.206
</p>
<p>5 heads  0.247
</p>
<p>6 heads  0.206
</p>
<p>7 heads  0.118
</p>
<p>8 heads  0.044
</p>
<p>9 heads  0.010
</p>
<p>10 heads  0.001
</p>
<p>191</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T
</p>
<p>Consider the following alternative hypotheses:
</p>
<p>Alternative 1: H0: The coin is fair.
H1: The coin is biased.
</p>
<p>Alternative 2: H0: The coin is fair.
H1: The coin is biased in favor of heads.
</p>
<p>a. Would a one-tailed or a two-tailed test be more appropriate for a
researcher who chose alternative 1? Explain why.
</p>
<p>b. For a sequence of ten throws, what results would cause a re-
searcher operating under the hypotheses listed under alternative 1
to reject the null hypothesis at a significance level of 5%?
</p>
<p>c. Would a one-tailed or a two-tailed test be more appropriate for a
researcher who chose alternative 2? Explain why.
</p>
<p>d. For a sequence of ten throws, what results would cause a re-
searcher operating under the hypotheses listed under alternative 2
to reject the null hypothesis at a significance level of 5%?
</p>
<p>8.6 Use the following binomial distribution showing the chances of suc-
cess and failure for 12 trials.
</p>
<p>Number of Successes Probability
</p>
<p>0 successes 0.00118
</p>
<p>1 success 0.01065
</p>
<p>2 successes 0.04418
</p>
<p>3 successes 0.11110
</p>
<p>4 successes 0.18857
</p>
<p>5 successes 0.22761
</p>
<p>6 successes 0.20032
</p>
<p>7 successes 0.12953
</p>
<p>8 successes 0.06107
</p>
<p>9 successes 0.02048
</p>
<p>10 successes 0.00463
</p>
<p>11 successes 0.00064
</p>
<p>12 successes 0.00004
</p>
<p>Using a significance level of 0.05, what outcomes would lead you to
reject the null hypothesis for each of the following pairs of hypotheses?
</p>
<p>a. H0: P � 0.50
H1: P � 0.50
</p>
<p>b. H0: P � 0.50
H1: P � 0.50
</p>
<p>192</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S
</p>
<p>c. H0: P � 0.50
H1: P � 0.50
</p>
<p>d. If you changed the significance level to 0.01, how would your an-
swers to parts a, b, and c change?
</p>
<p>8.7 Use the following binomial distribution showing the chances of suc-
cess and failure for 15 trials.
</p>
<p>Number of Successes Probability
</p>
<p>0 successes 0.00000
</p>
<p>1 success 0.00000
</p>
<p>2 successes 0.00001
</p>
<p>3 successes 0.00006
</p>
<p>4 successes 0.00042
</p>
<p>5 successes 0.00228
</p>
<p>6 successes 0.00930
</p>
<p>7 successes 0.02928
</p>
<p>8 successes 0.07168
</p>
<p>9 successes 0.13650
</p>
<p>10 successes 0.20051
</p>
<p>11 successes 0.22313
</p>
<p>12 successes 0.18210
</p>
<p>13 successes 0.10288
</p>
<p>14 successes 0.03598
</p>
<p>15 successes 0.00587
</p>
<p>Using a significance level of 0.05, what outcomes would lead you to
reject the null hypothesis for each of the following pairs of hypotheses?
</p>
<p>a. H0: P � 0.50
H1: P � 0.50
</p>
<p>b. H0: P � 0.50
H1: P � 0.50
</p>
<p>c. H0: P � 0.50
H1: P � 0.50
</p>
<p>d. If you changed the significance level to 0.01, how would your an-
swers to parts a, b, and c change?
</p>
<p>8.8 Locate a research article in a recent issue of a criminology or criminal
justice journal.
</p>
<p>a. State the research hypotheses tested by the researcher(s).
</p>
<p>b. Describe the sampling method, the sample, and the sampling frame
used by the researcher(s).
</p>
<p>193</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T194
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>SPSS and Stata both have the capability to test hypotheses using the binomial 
</p>
<p> distribution. As discussed in each subsection below, there are sample syntax files 
</p>
<p>in both SPSS (Chapter_8.sps) and Stata (Chapter_8.do) that illustrate the com-
</p>
<p>mands for testing against the binomial distribution.
</p>
<p>SPSS
</p>
<p>The NPTESTS command will use a two-tailed test to calculate an observed 
</p>
<p>significance level for a binary variable (e.g., success vs. failure) and compare the 
</p>
<p>probabilities observed to a binomial distribution (among many other options 
</p>
<p>in the NPTESTS command). The default probability of a single success in the 
</p>
<p>BINOMIAL command is p = 0.50, meaning that this command tests the follow-
</p>
<p>ing hypotheses:
</p>
<p>After executing this command, the output window presents a table of results 
</p>
<p>that indicates the null hypothesis being tested and the observed significance level 
</p>
<p>(labeled &ldquo;Exact Significance&rdquo; in the table). You will see from this output window 
</p>
<p>that the observed significance level is 0.012, which is identical to the value calcu-
</p>
<p>lated on p. 184 in the text.
</p>
<p>As we illustrate below, the probability can be changed easily in the NPTESTS 
</p>
<p>command line.
</p>
<p>To try out the NPTESTS command and apply it to a binomial distribution, 
</p>
<p>open the SPSS syntax file Chapter_8.sps. A small data file (ex_8_1.sav) will be 
</p>
<p>read into SPSS when you execute the first two lines of command syntax. This 
</p>
<p>small data file contains the data from Table 8.1 in the text. Relative decreases in 
</p>
<p>post-intervention crime are indicated by a value of 1, and relative increases in 
</p>
<p>post-intervention crime are indicated by a value of 0.
</p>
<p>The structure to the NPTESTS command for a comparison to a binomial 
</p>
<p>distribution is:
</p>
<p>Where /ONESAMPLE indicates that we have data from only one sample that 
</p>
<p>we are going to compare with a hypothesized population. The TEST(crime) 
</p>
<p>statement indicates that the variable &ldquo;crime&rdquo; (the only one in the data file) is 
</p>
<p>to be the focus of the test. Within the BINOMIAL option&mdash;which tells SPSS 
</p>
<p>to compare &ldquo;crime&rdquo; against a binomial distribution&mdash;the TESTVALUE is the 
</p>
<p>hypothesized value of the probability of a success (i.e., P in the hypotheses 
</p>
<p>above) and SUCCESSCATEGORICAL = LIST(1) tells SPSS that we have coded 
</p>
<p>our measure so that a value of 1 is a success. All other values for a variable 
</p>
<p>would be interpreted as failures.
</p>
<p>H
0
:P=0.50
</p>
<p>H
1
:P&ne;0.50
</p>
<p>NPTESTS /ONESAMPLE TEST (crime) BINOMIAL(TESTVALUE = 0.5 
</p>
<p>SUCCESSCATEGORICAL = LIST(1)).</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 195
</p>
<p>Stata
</p>
<p>The binomial test in Stata is remarkably simple in form:
</p>
<p>The output from running the bitest command will be a table indicating the 
</p>
<p>observed number of success and then both one-tail and the two-tail tests of the 
</p>
<p>hypothesized probability. Should you be interested in testing a different  
</p>
<p>hypothesized probability, just alter the value from a presumed 0.5 to the value 
</p>
<p>you want to test.
</p>
<p>Open the file with the Stata do file Chapter_8.do to reproduce the results 
</p>
<p>from Table 8.1 in the text. The first command line opens the data file, which is 
</p>
<p>identical to that referred to in the discussion of SPSS above.
</p>
<p>The binomial test of the crime variable is then simply:
</p>
<p>The results for the two-tail test show a significance level of 0.012 (rounded), 
</p>
<p>exactly the same as reported above.
</p>
<p>Problems
</p>
<p> 1. The director of  a special drug treatment program claims to have found 
</p>
<p>a cure to drug addiction. As supporting evidence, the director produces 
</p>
<p>information on a random sample of  13 former clients who were followed 
</p>
<p>for 12 months after completing the program. Here is how the director 
</p>
<p>classified each former client:
</p>
<p>Success, Failure, Success, Success, Success, Success, Success, Failure, 
</p>
<p>Success, Success, Failure, Success, Success
</p>
<p>Enter these data into SPSS.
</p>
<p>a. State all the assumptions of  the hypothesis test.
</p>
<p>b. What is the test statistic?
</p>
<p>c. What decision can be made about the null hypothesis? (Assume that the 
</p>
<p>d. Can the director conclude that the program is effective? Explain why.
</p>
<p> 2. A group of  researchers wanted to replicate previous research on hot  
</p>
<p>spot interventions in another city, using a sample of  25 hot spots. When 
</p>
<p>comparing post-intervention crime levels, they classified the 25 locations 
</p>
<p>as follows:
</p>
<p>Decrease, Decrease, Increase, Decrease, Decrease, Increase, Increase, 
</p>
<p>Decrease, Decrease, Decrease, Decrease, Decrease, Decrease,
Decrease, Decrease, Increase, Increase, Decrease, Decrease, Decrease, 
</p>
<p>Decrease, Decrease, Increase, Decrease, Decrease
</p>
<p>bitest variable_name == hypothesized_probability
</p>
<p>bitest crime == 0.5</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R E I G H T :  S T E P S I N A S T A T I S T I C A L T E S T196
</p>
<p>Enter these data into SPSS.
</p>
<p>a. State all the assumptions of  the hypothesis test.
</p>
<p>b. What is the test statistic?
</p>
<p>c. What decision can be made about the null hypothesis? (Assume that  
</p>
<p>d. Did this study show a post-intervention change in crime?
</p>
<p>come to the same conclusion? Explain why.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chi-Square: A Test Commonly Used 
</p>
<p>for Nominal-Level Measures
</p>
<p>How Do Degrees of Freedom Affect the Distribution?
</p>
<p>How Does One Interpret a Chi-Square Statistic?
</p>
<p>Two Nominal-Scale Variables?
</p>
<p>Can the Chi-Square Test Be Used to Examine the Relationship Between
</p>
<p>Ordinal-Level Variables?
</p>
<p>Nominal-Scale Measure?
</p>
<p>C h o o s i n g  t h e  c h i - s q u a r e  d i s t r i b u t i o n
</p>
<p>C a l c u l a t i n g  t h e  c h i - s q u a r e  s t a t i s t i c
</p>
<p>S u b s t a n t i v e  e x a m p l e s  u s i n g  t h e  c h i - s q u a r e  t e s t
</p>
<p>C h a p t e r  n i n e
</p>
<p>When is the Chi-Square Distribution Appropriate?
</p>
<p>What are Degrees of Freedom?
</p>
<p>How is the Chi-Square Statistic Calculated?
</p>
<p>How is the Chi-Square Test Carried Out When There Is Only One 
</p>
<p>How is the Chi-Square Test Applied in the Case of the Relationship Between
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_9,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>THE BINOMIAL TEST provides a good introduction to the problem of sta-
tistical inference because it examines relatively simple statistical deci-
</p>
<p>sions. Using the binomial test, we illustrated how statisticians build a
</p>
<p>sampling distribution from probabilities. But the binomial distribution
</p>
<p>can be applied only to a single binary variable. In this chapter, we look
</p>
<p>at a more commonly used nonparametric test of statistical significance
</p>
<p>for nominal-level measures: chi-square. The chi-square test allows the re-
</p>
<p>searcher to examine multicategory nominal-level variables as well as the
</p>
<p>relationship between nominal-level measures.
</p>
<p>We begin our discussion of chi-square with an example similar to the
</p>
<p>one used to introduce the binomial distribution in Chapter 7. In this
</p>
<p>case, we examine the problem of a fair roll of a die. We then turn to ap-
</p>
<p>plications of the chi-square test in criminal justice.
</p>
<p>T e s t i n g  H y p o t h e s e s  C o n c e r n i n g  t h e  R o l l  o f  a  D i e
</p>
<p>In Chapter 7, we examined how you might make a decision about
</p>
<p>whether to challenge the fairness of a coin used to decide who would
</p>
<p>serve first in a weekly volleyball match. But what if you had the same
</p>
<p>question regarding a die used in a friendly game of chance at a local
</p>
<p>club? Each week, you and a few friends go down to the club and play a
</p>
<p>game of chance that involves the toss of a die. Let&rsquo;s say that the house
</p>
<p>(the club) wins whenever you roll a two or a six. You win whenever
</p>
<p>you roll a three or a four, and no one wins when you roll a one or a
</p>
<p>five. Over the month, you have played the game 60 times. Of the 60 rolls
</p>
<p>of the die, you have lost 24, rolling six 20 times and rolling two 4 times
</p>
<p>(see Table 9.1). You have won 10 times in total, rolling three 6 times and
</p>
<p>rolling four 4 times. The remaining 26 rolls of the die were split, with 16
</p>
<p>ones and 10 fives.
</p>
<p>198</p>
<p/>
</div>
<div class="page"><p/>
<p>As in the case of the coin toss, you and your friends have begun to be
</p>
<p>suspicious. Does it make sense that there should be such an uneven split
</p>
<p>in the outcomes of the game if the die is fair? Should you raise this issue
</p>
<p>with the club and suggest that they change their die? You don&rsquo;t want to
</p>
<p>appear to be a sore sport. Nonetheless, if the distribution of rolls of the
</p>
<p>die that you observed is very unlikely given a fair die, you would be
</p>
<p>willing to make a protest.
</p>
<p>The Chi-Square Distribution
</p>
<p>You cannot use the binomial distribution to assess the fairness of the die
</p>
<p>because the binomial distribution assumes that there are only two poten-
</p>
<p>tial outcomes for each event&mdash;for example, a head or tail on each toss of
</p>
<p>a coin. For the die, there are six potential outcomes: a roll of one, two,
</p>
<p>three, four, five, or six. In such cases, you can make use of another sam-
2
</p>
<p>ducted, the chi-square distribution varies from problem to problem.
</p>
<p>However, the chi-square distribution varies not according to the number
</p>
<p>of trials that are conducted but according to the number of degrees of
</p>
<p>freedom (df) associated with a test. The number of degrees of freedom
</p>
<p>refers to how much a mathematical operation is free to vary, or take on
</p>
<p>any value, after an agreed-upon set of limitations has been imposed.
</p>
<p>In the chi-square distribution, these limitations are associated with
</p>
<p>the number of categories, or potential outcomes, examined. To define
</p>
<p>the degrees of freedom of a chi-square test, we ask how many cate-
</p>
<p>gories would have to be known for us to predict the remaining cate-
</p>
<p>gories with certainty. For example, if we know that there are 60 rolls of
</p>
<p>the die and we also know the precise number of events that fall in five
</p>
<p>of the six categories, we will be able to predict the sixth category sim-
</p>
<p>ply by subtracting from the total number of events (60) the number in
</p>
<p>the five known categories (see Table 9.2). If two categories are blank,
</p>
<p>we can predict the total of both, but not the exact split between them.
</p>
<p>Accordingly, the number of degrees of freedom for this example is 5.
</p>
<p>Once we know the number of events or observations in five categories,
</p>
<p>Frequency Distribution for 60 Rolls of a Die
</p>
<p>1 No winner 16
2 You lose 4
3 You win 6
4 You win 4
5 No winner 10
6 You lose 20
</p>
<p>60
</p>
<p>Table 9.1
</p>
<p>pling  distribution, called  the chi-square (� ) distribution. Like the 
</p>
<p>binomial distribution, which varies depending on the number of trials con-
</p>
<p>�Total ( ) 
</p>
<p>T E S T I N G H Y P O T H E S E S C O N C E R N I N G T H E R O L L O F A D I E 199</p>
<p/>
</div>
<div class="page"><p/>
<p>we can predict the number in the sixth with certainty. More generally,
</p>
<p>you can identify the degrees of freedom for a one-variable chi-square
</p>
<p>distribution using the equation df � k � 1, where k equals the number
</p>
<p>of categories in your measure (for our example, 6 � 1 � 5).
</p>
<p>Figure 9.1 shows how chi-square distributions vary according to the
</p>
<p>number of degrees of freedom. The height of the distribution represents
</p>
<p>the proportion of cases found at any specific value of the chi-square
</p>
<p>statistic. As the number of degrees of freedom grows, the height of the
</p>
<p>chi-square distribution decreases, with a longer and longer tail to the
</p>
<p>right. This means that the proportion of cases found above higher values
</p>
<p>of the chi-square statistic grows as the number of degrees of freedom in-
</p>
<p>creases. To understand what this means substantively, as well as how the
</p>
<p>chi-square distribution is used in making decisions about hypotheses, it
</p>
<p>is important to see how the chi-square statistic is calculated.
</p>
<p>Frequency Distribution for 60 Rolls of a Die 
with Information Missing
</p>
<p>1 No winner 16
2 You lose 4
3 You win 6
4 You win 4
5 No winner 10
6 You lose ?
</p>
<p>60
</p>
<p>Frequency of category 6 � (total frequency) � (sum of categories 1 to 5)
20 � 60 � 40
</p>
<p>Table 9.2
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Values of χ2
</p>
<p>df = 1
</p>
<p>df = 2
df = 6
</p>
<p>df = 15
</p>
<p>Chi-Square Distributions for Various Degrees of FreedomFigure 9.1
</p>
<p>�Total ( ) 
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E200</p>
<p/>
</div>
<div class="page"><p/>
<p>Calculating the Chi-Square Statistic
</p>
<p>The formula for the chi-square statistic is presented in Equation 9.1.
</p>
<p>Equation 9.1
</p>
<p>The summation symbol in the body of the equation has i � 1 below it
</p>
<p>and k above it. This means that we sum the quantity that follows the
</p>
<p>summation symbol for each category from the first to the k th, or last,
</p>
<p>category. Since there are six categories in our example, we will have to
</p>
<p>carry out the same calculation six times, once for each of the six poten-
</p>
<p>tial outcomes of the roll of a die.
</p>
<p>The quantity that follows the summation symbol includes two sym-
</p>
<p>bols, fo and fe. The symbol fo represents the frequency of the events ob-
</p>
<p>served in a category, or the observed frequencies. For example, in 20
</p>
<p>of the 60 trials, a six was rolled (see Table 9.1). The observed frequency
</p>
<p>for a roll of six is 20. The symbol fe represents the expected frequency
</p>
<p>of a category. The expected frequencies are ordinarily defined by the
</p>
<p>null hypothesis. In our example, they represent the number of events
</p>
<p>that would be expected in each category in the long run if the die were
</p>
<p>fair. Because a fair die is one for which there is an equal chance of ob-
</p>
<p>taining any of the six potential outcomes, we divide the 60 observations
</p>
<p>evenly across the six categories. This leads to an expected frequency of
</p>
<p>10 for each potential outcome. Table 9.3 shows the expected and ob-
</p>
<p>served frequencies for our example.
</p>
<p>To calculate the chi-square statistic, Equation 9.1 tells us first to sub-
</p>
<p>tract the expected frequency from the observed frequency in each cate-
</p>
<p>gory. We then square the result and divide that quantity by the expected
</p>
<p>frequency of the category. For example, for a roll of six, we subtract 10
</p>
<p>(the expected frequency) from 20 (the observed frequency). We then
</p>
<p>square that quantity (to get 100) and divide the result by 10. This gives
</p>
<p>us 10 for a roll of six. After carrying out this computation for each cate-
</p>
<p>gory, as is done in Table 9.4, we add up the results for all six categories
</p>
<p>�2 � �
k
</p>
<p>i�1
</p>
<p> 
( fo � fe)
</p>
<p>2
</p>
<p>fe
</p>
<p>Expected and Observed Frequencies for 60 Rolls of a Fair Die
</p>
<p>fe fo
</p>
<p>1 10 16
2 10 4
3 10 6
4 10 4
5 10 10
6 10 20
</p>
<p>60 60
</p>
<p>Table 9.3
</p>
<p>�Total ( ) 
</p>
<p>T E S T I N G H Y P O T H E S E S C O N C E R N I N G T H E R O L L O F A D I E 201</p>
<p/>
</div>
<div class="page"><p/>
<p>to obtain the total chi-square statistic. The chi-square statistic for this ex-
</p>
<p>ample is 22.4.
</p>
<p>The chi-square statistic measures how much the observed distribution
</p>
<p>differs from that expected under the null hypothesis. If the observed fre-
</p>
<p>quencies are similar to the expected frequencies, the chi-square statistic
</p>
<p>is small. If the observed frequencies are the same as the expected fre-
</p>
<p>quencies, the chi-square statistic equals 0. The more the observed fre-
</p>
<p>quencies differ from the expected frequencies, the larger the chi-square
</p>
<p>statistic will be. What does this mean in terms of making a decision
</p>
<p>about the fairness of the die? To find out, we have to turn to a table of
</p>
<p>probabilities associated with the chi-square distribution.
</p>
<p>Linking the Chi-Square Statistic to Probabilities: The Chi-Square Table
</p>
<p>In Chapters 7 and 8, we used the binomial formula to calculate the prob-
</p>
<p>ability associated with each of the possible outcomes in our sample. For
</p>
<p>other tests of statistical significance, including chi-square, we can take
</p>
<p>advantage of already calculated probability distributions. Appendix 2
</p>
<p>presents a table of probabilities associated with chi-square distributions
</p>
<p>with degrees of freedom from 1 to 30. The chi-square table does not give
</p>
<p>us the probability associated with every possible outcome, but rather
</p>
<p>provides probabilities and then lists the chi-square statistics associated
</p>
<p>with them.
</p>
<p>As illustrated in the chi-square table in Appendix 2, a larger chi-
</p>
<p>square statistic is associated with a smaller significance level, or � value.
</p>
<p>For example, under one degree of freedom, a statistic of 2.706 is associ-
</p>
<p>ated with a significance level of 0.10, a statistic of 3.841 with an � value
</p>
<p>of 0.05, and a statistic of 10.827 with an � value of 0.001. This also
</p>
<p>means that the larger the chi-square statistic obtained in a test, the less
</p>
<p>likely it is that the observed distribution is drawn from the expected dis-
</p>
<p>tribution. This logic makes good common sense. For our example of the
</p>
<p>roll of a die, it is reasonable to become more suspicious about the fair-
</p>
<p>ness of the die as the number of events in the different categories
</p>
<p>Computation of Chi-Square for 60 Rolls of a Die
</p>
<p>OUTCOME A fo fe (fo � fe) (fo � fe)
2
</p>
<p>1 16 10 6 36 3.6
2 4 10 �6 36 3.6
3 6 10 �4 16 1.6
4 4 10 �6 36 3.6
5 10 10 0 0 0
6 20 10 10 100 10.0
</p>
<p>� � 22.4
</p>
<p>(fo � fe)
2
</p>
<p>fe
</p>
<p>Table 9.4
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E202</p>
<p/>
</div>
<div class="page"><p/>
<p>becomes more uneven. If we expect 10 events in each category and ac-
</p>
<p>tually get one with 20, one with 16, and two others with only 4, this
</p>
<p>should begin to make us suspicious. If one or two categories have 25
</p>
<p>cases and two or three have none, it seems even more likely that the die
</p>
<p>is not a fair one. But if each category has about 10 cases, which is to be
</p>
<p>expected in the long run with 60 rolls of a fair die, both common sense
</p>
<p>and chi-square give us little reason to suspect a biased die.
</p>
<p>Notice as well in Appendix 2 that as the number of degrees of free-
</p>
<p>dom gets larger, a larger chi-square statistic is needed to arrive at the
</p>
<p>same probability value. For example, with one degree of freedom, a chi-
</p>
<p>square statistic of 3.841 is associated with an � value of 0.05. With 30 de-
</p>
<p>grees of freedom, a statistic of 43.773 is needed to achieve the same
</p>
<p>threshold. This reflects the difference in the shape of chi-square distribu-
</p>
<p>tions with different degrees of freedom and makes good sense if you
</p>
<p>consider how the chi-square statistic is calculated. A separate addition is
</p>
<p>made to the chi-square statistic for each possible category. Accordingly,
</p>
<p>it makes sense to demand a larger statistic as the number of categories in
</p>
<p>the test increases.
</p>
<p>What about our decision regarding the roll of the die? Looking at Ap-
</p>
<p>pendix 2, we can see that with five degrees of freedom a chi-square sta-
</p>
<p>tistic of 11.070 is associated with a significance level of 0.05. This means
</p>
<p>that in the long run we would expect to obtain a chi-square statistic of
</p>
<p>11.070 in only 5 in 100 samples if the die is fair. In fact, we obtained a
</p>
<p>chi-square statistic of 22.4. This number is even larger than that needed
</p>
<p>for a significance level of 0.001. Accordingly, the observed significance
</p>
<p>level for this test is less than 0.001 ( p � 0.001). If the die were fair, the
</p>
<p>probability of getting a distribution like the one observed in our 60 rolls
</p>
<p>of a die would be less than 1 in 1,000. Given this result, we would likely
</p>
<p>come to the conclusion that the die was not a fair one and call for the
</p>
<p>club to use a new one.
</p>
<p>A Substantive Example: The Relationship Between Assault 
</p>
<p>Victims and Offenders
</p>
<p>We can illustrate the chi-square test for a single variable by considering
</p>
<p>the responses from a random sample survey of Illinois residents.1 One of
</p>
<p>the primary purposes of the survey was to examine the effect of victim-
</p>
<p>ization on the physical and mental health of adults. Each respondent was
</p>
<p>asked about a variety of possible victimization experiences. When the
</p>
<p>person claimed to have experienced a crime, a series of follow-up ques-
</p>
<p>tions were asked about the circumstances of the event. Table 9.5 pre-
</p>
<p>sents the frequency distribution of the responses to a question about the
</p>
<p>1See Chester L. Britt, &ldquo;Health Consequences of Criminal Victimization,&rdquo; International
</p>
<p>Review of Victimology 8 (2001): 63&ndash;73 for a description of the study.
</p>
<p>T E S T I N G H Y P O T H E S E S C O N C E R N I N G T H E R O L L O F A D I E 203</p>
<p/>
</div>
<div class="page"><p/>
<p>relationship between the victim and the offender for those persons who
</p>
<p>claimed to have been assaulted.
</p>
<p>A simple research question using these data might focus on whether
</p>
<p>the victim-offender relationship was unevenly distributed among the
</p>
<p>population of assault victims. To answer our research question, we fol-
</p>
<p>low the form of a statistical test introduced in Chapter 8.
</p>
<p>We begin by stating the assumptions of our chi-square test. The level of
</p>
<p>measurement required for chi-square is nominal. We make no specific as-
</p>
<p>sumptions regarding the shape of the population distribution, as the chi-
</p>
<p>square test is a nonparametric test of statistical significance. Although the
</p>
<p>chi-square test ordinarily requires a fully independent random sample, this
</p>
<p>sample was selected without replacement.2 This is not a serious violation
</p>
<p>of our assumptions because the sample is very small relative to the popu-
</p>
<p>lation of interest. Note that our null hypothesis is that the victim-offender
</p>
<p>relationship in the population is evenly or randomly distributed across the
</p>
<p>categories examined. The research hypothesis is that the victim-offender
</p>
<p>relationship is not randomly or evenly distributed in the population.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Nominal scale.
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method: Independent random sampling (no replacement; sam-
</p>
<p>ple is small relative to population).
</p>
<p>Sampling Frame: Persons aged 18 and over in the state of Illinois.
</p>
<p>Hypotheses:
</p>
<p>H0: The type of victim-offender relationship for assault victims is ran-
</p>
<p>domly distributed.
</p>
<p>H1: The type of victim-offender relationship for assault victims is not ran-
</p>
<p>domly distributed.
</p>
<p>Relationship Between Assault Victim and Offender
</p>
<p>CATEGORY FREQUENCY (N )
</p>
<p>Stranger 166
Acquaintance 61
Friend 35
Boyfriend/girlfriend 38
Spouse 66
Other relative 44
Total (�) 410
</p>
<p>Table 9.5
</p>
<p>2There are certain specific situations in which the chi-square test does not require
</p>
<p>sampling with replacement; see B. S. Everitt, The Analysis of Contingency Tables (Lon-
</p>
<p>don: Chapman and Hall, 1997).
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E204</p>
<p/>
</div>
<div class="page"><p/>
<p>The Sampling Distribution Since we are analyzing the distribution of
</p>
<p>cases for a nominal variable, the chi-square distribution provides an ap-
</p>
<p>propriate means of assessing whether the observations are randomly dis-
</p>
<p>tributed across the six categories of victim-offender relationships. For a
</p>
<p>single nominal variable, the number of degrees of freedom for the chi-
</p>
<p>square test is df � k � 1 � 6 � 1 � 5.
</p>
<p>Significance Level and Rejection Region Since we have no reason to im-
</p>
<p>pose a stricter or more lenient level of statistical significance on our analy-
</p>
<p>sis, we will use a significance level (�) of 0.05. Given that the number of
</p>
<p>degrees of freedom associated with this chi-square test is 5 and the signifi-
</p>
<p>cance level is 0.05, we see from Appendix 2 that the corresponding critical
</p>
<p>value of the chi-square distribution is 11.070. Accordingly, if the calculated
</p>
<p>value of the chi-square statistic in our example is greater than 11.070, we
</p>
<p>will reject the null hypothesis and conclude that type of victim-offender re-
</p>
<p>lationship among assault victims is not randomly distributed.
</p>
<p>The Test Statistic Equation 9.1 provides the formula for calculating the chi-
</p>
<p>square statistic to test for random assignment of cases to each category or
</p>
<p>value. We begin by calculating the expected frequency (fe) for each cell in
</p>
<p>the table. Again, as in the example of the die, we would expect under the
</p>
<p>null hypothesis that there would be an equal number of cases in each of
</p>
<p>the categories examined. To calculate the expected frequency mathemati-
</p>
<p>cally, we divide the total N of cases by the number of categories. This is
</p>
<p>done below, where we get an expected value for each category of 68.333:
</p>
<p>After calculating the expected frequency, we can proceed to calculate the
</p>
<p>chi-square statistic. Table 9.6 presents the observed and expected frequen-
</p>
<p>cies for each cell and the appropriate calculations for determining the value
</p>
<p>of the chi-square statistic. We find the value of the test statistic to be 178.85.
</p>
<p>fe � 
N
k
</p>
<p> � 
410
6
</p>
<p> � 68.333
</p>
<p>Computation of Chi-Square for Type of Victim-Offender Relationship
</p>
<p>CATEGORY fo fe (fo � fe) (fo � fe)
2
</p>
<p>Stranger 166 68.333 97.667 9,538.843 139.593
Acquaintance 61 68.333 �7.333 53.773 0.787
Friend 35 68.333 �33.333 1,111.089 16.260
Boyfriend/girlfriend 38 68.333 �30.333 920.091 13.465
Spouse 66 68.333 �2.333 5.443 0.080
Other relative 44 68.333 �24.333 592.095 8.665
</p>
<p>� � 178.849
</p>
<p>(fo � fe)
2
</p>
<p>fe
</p>
<p>Table 9.6
</p>
<p>T E S T I N G H Y P O T H E S E S C O N C E R N I N G T H E R O L L O F A D I E 205</p>
<p/>
</div>
<div class="page"><p/>
<p>The Decision The critical value for our test of statistical significance was
</p>
<p>11.070, meaning that a calculated chi-square statistic greater than this
</p>
<p>critical value would lead to rejection of the null hypothesis. The value of
</p>
<p>our test statistic is 178.85, which is much larger than our critical chi-
</p>
<p>square value. Accordingly, the observed significance level of our test is
</p>
<p>less than the significance criterion we set at the outset (p � 0.05). On
</p>
<p>the basis of this outcome, we reject the null hypothesis and conclude
</p>
<p>that type of victim-offender relationship among assault victims is not ran-
</p>
<p>domly distributed. Of course, we cannot be certain that the null hypothe-
</p>
<p>sis is false for the population we are examining. We make our decision
</p>
<p>with a set risk of a Type I error defined at the outset of our test.
</p>
<p>R e l a t i n g  T w o  N o m i n a l - S c a l e  
M e a s u r e s  i n  a  C h i - S q u a r e  T e s t
</p>
<p>In criminal justice and criminology, we seldom examine research issues like
</p>
<p>the fairness of a die or the randomness of type of victim-offender relation-
</p>
<p>ship, which are concerned with outcomes on only one measure. More often,
</p>
<p>we are interested in describing the relationships among two or more vari-
</p>
<p>ables. For example, we may want to assess whether men and women are
</p>
<p>likely to be placed in different types of treatment facilities or whether differ-
</p>
<p>ent ethnic groups receive different types of sanctions. For each of these ex-
</p>
<p>amples, two measures must be assessed at the same time. In the former, we
</p>
<p>examine both gender and type of treatment facility. In the latter, we exam-
</p>
<p>ine type of sentence and ethnicity. Below, we use the example of a study of
</p>
<p>white-collar criminals to illustrate the use of chi-square in making inferences
</p>
<p>about the relationship between two variables: recidivism and sanction type.
</p>
<p>A Substantive Example: Type of Sanction and Recidivism 
</p>
<p>Among Convicted White-Collar Criminals
</p>
<p>In a study of white-collar offenders, data on reoffending from FBI records
</p>
<p>over a ten-year period were examined. The sample included offenders
</p>
<p>from seven U.S. district courts, convicted of eight different white-collar
</p>
<p>crimes (antitrust violations, securities fraud, mail and wire fraud, false
</p>
<p>claims and statements, credit and lending institution fraud, bank embez-
</p>
<p>zlement, income tax fraud, and bribery). The sample was chosen ran-
</p>
<p>domly without replacement.3 The research question concerned whether
</p>
<p>imprisonment of white-collar offenders impacted upon reoffending.
</p>
<p>3In this case, a stratified random sample was selected in order to ensure a broad sampling
</p>
<p>of white-collar offenders. For our example here, we treat the sample as a simple random
</p>
<p>sample. See David Weisburd, Elin Waring, and Ellen Chayet, &ldquo;Specific Deterrence in a
</p>
<p>Sample of Offenders Convicted of White Collar Crimes,&rdquo; Criminology 33 (1995): 587&ndash;607.
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E206</p>
<p/>
</div>
<div class="page"><p/>
<p>The likelihood of rearrest for a group of offenders who received a
</p>
<p>prison sanction was compared with that of a matched group who did not
</p>
<p>receive a prison sanction. The researchers found that 33.0% of the prison
</p>
<p>group (N � 100) was rearrested during the follow-up period, in contrast
</p>
<p>to 28.4% of the no-prison group (N � 67). What conclusions can we
</p>
<p>come to concerning white-collar criminals generally?
</p>
<p>To answer our research question, we follow the standard format of a
</p>
<p>test of statistical significance. We begin by stating our assumptions. Re-
</p>
<p>member that to state the assumptions you must choose the type of test
</p>
<p>you will use. In this case, we have chosen a chi-square test for relating
</p>
<p>two nominal-level measures.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Nominal scales.
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method: Independent random sampling (no replacement; sam-
</p>
<p>ple is small relative to population).
</p>
<p>Sampling Frame: Offenders from seven federal judicial districts convicted
</p>
<p>of eight different white-collar crimes.
</p>
<p>Hypotheses:
</p>
<p>H0: There is no difference in the likelihood of rearrest among similar
</p>
<p>white-collar offenders sentenced or not sentenced to prison. (Or, likeli-
</p>
<p>hood of rearrest and imprisonment are independent.)
</p>
<p>H1: There is a difference in the likelihood of rearrest among similar
</p>
<p>white-collar offenders sentenced or not sentenced to prison. (Or, likeli-
</p>
<p>hood of rearrest and imprisonment are not independent.)
</p>
<p>The level of measurement required for a chi-square test is nominal.
</p>
<p>Our example includes two variables: rearrest and type of sanction. Each
</p>
<p>is measured as a binary nominal variable. For rearrest, we examine those
</p>
<p>rearrested versus those not rearrested in the follow-up period. For type
</p>
<p>of sanction, we differentiate between those who were sentenced to
</p>
<p>prison and those who did not receive a prison sanction. In regard to the
</p>
<p>population distribution, chi-square is a nonparametric test and therefore
</p>
<p>requires no specific assumption.
</p>
<p>The sample was selected randomly, but as is the case with most crim-
</p>
<p>inal justice studies, the investigators did not sample with replacement. At
</p>
<p>the same time, the population from which the sample was drawn is very
</p>
<p>large relative to the sample examined, and thus we have no reason to
</p>
<p>suspect that this violation of the assumptions will affect our test result.
</p>
<p>The sampling frame includes offenders from seven federal judicial dis-
</p>
<p>tricts. As discussed in Chapter 8, it is necessary to explain why your
</p>
<p>tricts convicted of eight different white-collar crimes. Accordingly, our 
</p>
<p>inferences relate directly to the population of those offenses and those 
</p>
<p>R E L A T I N G T W O N O M I N A L - S C A L E M E A S U R E S 207</p>
<p/>
</div>
<div class="page"><p/>
<p>sample is representative of a broader population of cases if you want to
</p>
<p>make inferences beyond your sampling frame. In this study, the seven dis-
</p>
<p>tricts examined were seen as providing a sample with geographic spread
</p>
<p>throughout the United States, and the selected white-collar offenses were
</p>
<p>defined as offering a &ldquo;broad sampling of white-collar offenders.&rdquo;
</p>
<p>In most applications of the chi-square test, one cannot assign a direc-
</p>
<p>tional research hypothesis. This is because chi-square requires a nominal
</p>
<p>ables, as examined here, the researcher can choose between a direc-
</p>
<p>tional and a nondirectional research hypothesis. In our example, a
</p>
<p>directional hypothesis would be that the prison group is more likely than
</p>
<p>the no-prison group to be rearrested or that the no-prison group is more
</p>
<p>likely than the prison group to be rearrested. The research hypothesis
</p>
<p>stated by the investigators in this study was nondirectional. It stated sim-
</p>
<p>ply that the two groups (prison and no-prison) differ in terms of likeli-
</p>
<p>hood of rearrest during the follow-up period. The null hypothesis was
</p>
<p>that there is no difference between the prison and no-prison groups.
</p>
<p>Researchers often state the hypotheses of a chi-square test in terms of
</p>
<p>the independence of the variables that are examined. Stated this way, the
</p>
<p>null hypothesis would be that prison group (prison or no-prison) is inde-
</p>
<p>pendent, or unrelated to rearrest, in the follow-up period. The research
</p>
<p>hypothesis is that prison group is not independent. While this method of
</p>
<p>stating the hypotheses for your test sounds different, it leads to the same
</p>
<p>conclusions. If the two groups differ in terms of likelihood of rearrest in
</p>
<p>the follow-up period, then prison group and recidivism are related and
</p>
<p>thus not independent. If there is no difference, then prison group is un-
</p>
<p>related to, or independent of, recidivism in the follow-up period.
</p>
<p>The Sampling Distribution Because we are examining the relationship
</p>
<p>between two nominal-scale variables, the chi-square distribution pro-
</p>
<p>vides an appropriate sampling distribution for our test. However, our de-
</p>
<p>cision about degrees of freedom is not as straightforward as that in the
</p>
<p>example of a roll of a die. In this case, we must take into account the
</p>
<p>Recidivism Among 167 White-Collar Criminals According to Whether
They Did or Did Not Receive Prison Sentences
</p>
<p>Imprisoned Not imprisoned
</p>
<p>Subsequently arrested Cell A Cell B 52
33 19
</p>
<p>Not subsequently arrested Cell C Cell D 115
67 48
</p>
<p>100 67 167
</p>
<p>Table 9.7
</p>
<p>catagories examined. Nonetheless, in the special case of two binary vari-
</p>
<p>level of measurement, which does not assign order or value to the 
</p>
<p>Row total
</p>
<p>Column total
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E208</p>
<p/>
</div>
<div class="page"><p/>
<p>joint distribution of our measures. This is illustrated in Table 9.7, which
</p>
<p>shows two potential outcomes for the prison variable and two potential
</p>
<p>outcomes for the arrest variable. We have four cells, or four possible
</p>
<p>combined outcomes. Cell A is for offenders who received a prison sanc-
</p>
<p>tion and were arrested in the follow-up period. Cell B is for offenders
</p>
<p>who did not receive a prison sanction and were arrested in the follow-up
</p>
<p>period. Cell C is for offenders who received a prison sanction and were
</p>
<p>not arrested in the follow-up period. Cell D is for offenders who did not
</p>
<p>receive a prison sanction and were not arrested in the follow-up period.
</p>
<p>If we sum across and down the cells, we gain two row marginals and
</p>
<p>two column marginals. The row marginals represent the totals for the rows:
</p>
<p>52 for those arrested and 115 for those not arrested. The column marginals
</p>
<p>represent the totals for the columns: N � 100 for the prison group, and N �
</p>
<p>67 for the no-prison group. If you know the row and column marginals, as
</p>
<p>is assumed in computing the degrees of freedom for chi-square, you can
</p>
<p>predict with certainty the remaining cells, once the value of any one cell is
</p>
<p>known (see Table 9.8). Degrees of freedom for a two-variable chi-square
</p>
<p>can be gained more simply through the formula df � (r � 1)(c � 1), where
</p>
<p>r represents the number of rows and c the number of columns. For our ex-
</p>
<p>ample, there are two rows and two columns, so df � (2 � 1)(2 � 1) � 1.
</p>
<p>Significance Level and Rejection Region We stated no reason at the outset
</p>
<p>for choosing for our example a stricter or more lenient significance thresh-
</p>
<p>old than is used by convention. Accordingly, we use a significance level of
</p>
<p>0.05 for our test. Our rejection region is defined by the chi-square table
</p>
<p>(see Appendix 2). Importantly, the chi-square distribution is not concerned
</p>
<p>with the direction of outcomes in a test. It tells us to what extent the ob-
</p>
<p>served frequencies in our example differ from those that would be ex-
</p>
<p>pected under the null hypothesis of no difference. Whether they differ in
</p>
<p>one direction or another, the chi-square statistic will always be positive.
</p>
<p>Predicting the Missing Cells in a Two-Variable Chi-Square Test
</p>
<p>Row 
</p>
<p>Cell B 52
33 ?
</p>
<p>Cell C Cell D 115
? ?
</p>
<p>67 167
</p>
<p>Given that cell A � 33:
Cell B � (52 � 33) � 19
Cell C � (100 � 33) � 67
Cell D � (115 � 67) � 48
</p>
<p>Table 9.8
</p>
<p>Imprisoned Not imprisoned total
</p>
<p>Column total 100
</p>
<p>Subsequently arrested
</p>
<p>Not subsequently arrested
</p>
<p>Cell A
</p>
<p>R E L A T I N G T W O N O M I N A L - S C A L E M E A S U R E S 209</p>
<p/>
</div>
<div class="page"><p/>
<p>The terms &ldquo;directional&rdquo; and &ldquo;nondirectional&rdquo; are very tenuous ones in
</p>
<p>a chi-square test. Chi-square assumes nominal-scale variables, which by
</p>
<p>definition do not provide information about the order of values in a
</p>
<p>measure. If we cannot specify the order of two measures, we cannot
</p>
<p>speak of the direction of their relationship. As noted earlier, in most situ-
</p>
<p>ations a directional hypothesis is not appropriate for a chi-square test. In
</p>
<p>the special case of two binary variables, however, researchers do some-
</p>
<p>times use chi-square to examine directional research hypotheses. For ex-
</p>
<p>ample, we might have stated our research hypothesis as &ldquo;The likelihood
</p>
<p>of arrest in the follow-up period for white-collar offenders sentenced to
</p>
<p>prison is lower than that of similar white-collar offenders not sentenced
</p>
<p>to prison.&rdquo;
</p>
<p>However, our research hypothesis is nondirectional, as is the table of
</p>
<p>chi-square values. To define our rejection region, we turn to the row of
</p>
<p>the table associated with one degree of freedom. Under a significance
</p>
<p>level (�) of 0.05, we see a score of 3.841. For us to reject the null hy-
</p>
<p>pothesis, our test statistic will have to be greater than this value.4
</p>
<p>The Test Statistic To apply chi-square to the two-variable case, we need
</p>
<p>to adapt our original equation. The formula for the chi-square statistic for
</p>
<p>relating two measures is presented in Equation 9.2.5
</p>
<p>Equation 9.2
</p>
<p>The only difference between Equation 9.2 and Equation 9.1 is that we
</p>
<p>have an additional summation symbol. In this case, we do not sum
</p>
<p>simply across the categories of one measure; rather, we sum across
</p>
<p>each row (r) and column (c) of the joint distribution of two measures.
</p>
<p>Accordingly, Equation 9.2 reminds us that we must examine the ex-
</p>
<p>pected and observed frequencies for every potential outcome we can
</p>
<p>observe&mdash;or, in terms of the chi-square equation, for every cell in our
</p>
<p>table.
</p>
<p>�2 � �
r
</p>
<p>i�1
</p>
<p> �
c
</p>
<p>j�1
</p>
<p> 
( fo � fe)
</p>
<p>2
</p>
<p>fe
</p>
<p>4What if we had defined a directional research hypothesis? In this case, we look to the
</p>
<p>column of the table for twice the value of the desired significance level, since we now
</p>
<p>have placed all risk of falsely rejecting the null hypothesis in only one direction. For
</p>
<p>example, for a 0.05 significance level, we turn to the test statistic for a 0.10 level.
5When a chi-square test has only one degree of freedom, it is recommended that a
</p>
<p>correction factor be added if the expected frequencies of any cell fall below 20. The
</p>
<p>correction provides a more conservative, or smaller, chi-square statistic:
</p>
<p>�2 � �
r
</p>
<p>i�1
</p>
<p> �
c
</p>
<p>j�1
</p>
<p> 
(� fo � fe � � 0.5)2
</p>
<p>fe
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E210</p>
<p/>
</div>
<div class="page"><p/>
<p>For our example, this means we must sum across cells A, B, C, and D.
</p>
<p>As before, we want to compare the observed frequency to the expected
</p>
<p>frequency in each cell. The observed frequencies are those gained in our
</p>
<p>research. The expected frequencies are defined through the null hypoth-
</p>
<p>esis. The null hypothesis states that there is no difference in arrest rates
</p>
<p>between the prison and no-prison groups. If this is true, then we should
</p>
<p>expect the same proportion of arrests in both groups. To calculate the
</p>
<p>expected frequencies, accordingly, we first need to define the overall
</p>
<p>proportion of offenders arrested in the follow-up period.
</p>
<p>The proportion of offenders arrested overall in the sample in the follow-
</p>
<p>up period is obtained by dividing the total number of offenders arrested
</p>
<p>(Ncat � 52) by the total number of offenders in the sample (Ntotal � 167):
</p>
<p>To get the expected frequency for cell A, we multiply this proportion by
</p>
<p>the marginal total of 100 ( fe � 31.14). For the no-prison group, we have
</p>
<p>a total of 67 offenders. Applying the proportion of the total sample to
</p>
<p>this group, we multiply 0.3114 by 67 and get an expected frequency of
</p>
<p>20.86 for cell B. In practice, we do not need to compute the expected
</p>
<p>frequencies for the remaining two cells, C and D. Indeed, we could have
</p>
<p>assigned all of the cells expected frequencies based on knowledge of
</p>
<p>only one cell. This is what the number of degrees of freedom for this ex-
</p>
<p>ample tells us. If you know the number of cases in one cell, you can
</p>
<p>predict with certainty the rest. The expected and observed frequencies
</p>
<p>for our example are shown in Table 9.9.
</p>
<p>Now that we have calculated the observed and expected frequencies
</p>
<p>for each potential outcome, or cell, we can calculate the chi-square sta-
</p>
<p>Proportion � 
Ncat
</p>
<p>N  total 
 � 
</p>
<p>52
</p>
<p>167
 � 0.3114
</p>
<p>Expected and Observed Frequencies of Recidivism and Nonrecidivism
for White-Collar Offenders According to Whether They Received
Prison Sentences
</p>
<p>Imprisoned Not imprisoned
</p>
<p>Subsequently arrested Cell A Cell B 52
fo � 33 fo � 19
</p>
<p>fe � 31.14 fe � 20.86
</p>
<p>Not subsequently arrested Cell C Cell D 115
fo � 67 fo � 48
</p>
<p>fe � 68.86 fe � 46.14
</p>
<p>100 67 167
</p>
<p>Table 9.9
</p>
<p>tistic. To do this, we first square the difference of the observed and 
</p>
<p>Row total
</p>
<p>Column total
</p>
<p>R E L A T I N G T W O N O M I N A L - S C A L E M E A S U R E S 211</p>
<p/>
</div>
<div class="page"><p/>
<p>expected frequency of the cell:
</p>
<p>This is done in Table 9.10 for each of the four cells in our problem.
</p>
<p>Using cell A as an example, we first subtract the expected frequency of
</p>
<p>31.14 from the observed frequency of 33. We then square this quantity
</p>
<p>(1.86), obtaining a result of 3.4596. Dividing this result by the expected
</p>
<p>frequency in the cell (31.14) gives us 0.1111. The sum of all four cells,
</p>
<p>0.4021, is our test statistic.
</p>
<p>The Decision Our rejection region was defined as including any chi-
</p>
<p>square statistic greater than 3.841. The test statistic for our example is
</p>
<p>only 0.402. Accordingly, we choose not to reject the null hypothesis. The
</p>
<p>observed significance level for our test is greater than the significance
</p>
<p>level, or threshold, we set at the outset (p � 0.05). We conclude that
</p>
<p>there is no significant difference in the likelihood of recidivism between
</p>
<p>white-collar offenders who have and have not been sentenced to prison.
</p>
<p>Our inferences are made directly to the specific offenses and judicial dis-
</p>
<p>tricts defined in the sampling frame.
</p>
<p>E x t e n d i n g  t h e  C h i - S q u a r e  T e s t  t o  M u l t i c a t e g o r y  
V a r i a b l e s :  T h e  E x a m p l e  o f  C e l l  A l l o c a t i o n s  i n  P r i s o n
</p>
<p>The previous example illustrates the use of chi-square in the case of two
</p>
<p>binary variables. We now turn to an extension of the chi-square test to
</p>
<p>an example including a multicategory nominal-level variable. Our exam-
</p>
<p>ple is drawn from a study of the relationship between prisoners&rsquo; race
</p>
<p>and their cell assignments in a large state prison in the northeastern
</p>
<p>( fo � fe)
2
</p>
<p>fe
</p>
<p>Computation of Chi-Square for 167 White-Collar Criminals
</p>
<p>CELL fo fe (fo � fe) (fo � fe)
2
</p>
<p>A 33 31.14 1.86 3.4596 0.1111
B 19 20.86 �1.86 3.4596 0.1658
C 67 68.86 �1.86 3.4596 0.0502
D 48 46.14 1.86 3.4596 0.0750
</p>
<p>� � 0.4021
</p>
<p>(fo � fe)
2
</p>
<p>fe
</p>
<p>Table 9.10
</p>
<p>expected frequencies for each cell, and then we divide this quantity by the
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E212</p>
<p/>
</div>
<div class="page"><p/>
<p>United States.6 We examine the placement of non-Hispanic white and
</p>
<p>&ldquo;nonwhite&rdquo; inmates (including Hispanics) into seven cell blocks. The
</p>
<p>sample includes all prisoners in the general prison population for a sin-
</p>
<p>gle day. The distribution of cases is presented in Table 9.11.
</p>
<p>If cell assignments were made on considerations unrelated to race, we
</p>
<p>would expect to find the proportion of non-Hispanic whites in each cell
</p>
<p>block roughly equivalent to the proportion of non-Hispanic whites in the
</p>
<p>general prison population (19.9%; see the marginal for non-Hispanic
</p>
<p>whites in Table 9.11). Such equivalence is not evident. In block G, for ex-
</p>
<p>ample, non-Hispanic whites constituted 12.2% of the inmates. In block H,
</p>
<p>they comprised 60.7%. Do results for this sample allow us to conclude that
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Nominal scales.
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method: Independent random sampling (the entire sampling
</p>
<p>frame is examined).
</p>
<p>Sampling Frame: All prisoners in the general prison population on a
</p>
<p>specific day.
</p>
<p>6See Douglas McDonald and David Weisburd, &ldquo;Segregation and Hidden Discrimina-
</p>
<p>tion in Prisons: Reflections on a Small Study of Cell Assignments,&rdquo; in C. Hartchen
</p>
<p>(ed.), Correctional Theory and Practice (Chicago: Nelson Hall, 1991).
</p>
<p>Proportions of Non-Hispanic White Prisoners in Seven Cell Blocks
</p>
<p>Non-Hispanic
</p>
<p>whites Nonwhites
</p>
<p>Cell block C 48 208 256
18.7% 81.3% 100%
</p>
<p>Cell block D 17 37 54
31.5% 68.5% 100%
</p>
<p>Cell block E 28 84 112
25.0% 75.0% 100%
</p>
<p>Cell block F 32 79 111
28.8% 71.2% 100%
</p>
<p>Cell block G 37 266 303
12.2% 87.8% 100%
</p>
<p>Cell block H 34 22 56
60.7% 39.3% 100%
</p>
<p>Cell block I 44 268 312
14.1% 85.9% 100%
</p>
<p>240 964 1,204
19.9% 80.1% 100%
</p>
<p>Table 9.11
</p>
<p>Row
</p>
<p>total
</p>
<p>total
</p>
<p>Column
</p>
<p>there is disparity in cell-block assignments throughout the year in the prison.
</p>
<p>E X T E N D I N G T H E T E S T T O M U L T I C A T E G O R Y V A R I A B L E S 213</p>
<p/>
</div>
<div class="page"><p/>
<p>Hypotheses:
</p>
<p>H0: Cell-block assignment and race are independent.
</p>
<p>H1: Cell-block assignment and race are not independent.
</p>
<p>As in our previous example, we assume a nominal level of measure-
</p>
<p>ment for our test and do not make assumptions regarding the form of
</p>
<p>the population distribution. Prisoner race is measured at the binary nom-
</p>
<p>inal level, and cell block is a multicategory nominal scale.
</p>
<p>The sample includes all cases in the sampling frame. Accordingly, we
</p>
<p>do not need to use statistical inference to make statements about that
</p>
<p>population. However, the study was designed not only to describe
</p>
<p>prison-cell allocations on that day, but also to make more general state-
</p>
<p>ments about cell allocations in the prison studied throughout the year.
</p>
<p>This is not an uncommon scenario in criminal justice research, in good
</p>
<p>part because the realities of the criminal justice system often preclude
</p>
<p>sampling beyond specific institutions or outside of specific time frames.
</p>
<p>This means, however, that the researchers seek to make inferences be-
</p>
<p>yond their sampling frame.
</p>
<p>If cell allocations on the day examined in this study are representative
</p>
<p>of cell allocations more generally throughout the year, then the infer-
</p>
<p>ences made on the basis of the test will be reliable. If not, then the test
</p>
<p>will not provide for valid inferences. In our example, the investigators
</p>
<p>argue:
</p>
<p>There was no reason to suspect that the cell assignments of prisoners
</p>
<p>on that day differed substantially from assignments on other days.
</p>
<p>Moreover, these cell assignments represented the results of decisions
</p>
<p>made over the course of months and perhaps years prior to the date 
</p>
<p>of drawing the sample. There was every reason to believe, conse-
</p>
<p>quently, that cell assignments on that date constituted a valid repre-
</p>
<p>sentation of cell assignment decisions made during the several months
</p>
<p>prior to that day.
</p>
<p>Our research question asks whether we would be likely to obtain the
</p>
<p>distribution we observe in our sample if assignment to cell blocks were
</p>
<p>colorblind in the population. Stated in the form of hypotheses, we ask
</p>
<p>whether race and cell-block assignment are independent. If they are in-
</p>
<p>dependent, as proposed in our null hypothesis, then we would expect
</p>
<p>about the same proportion of nonwhite and non-Hispanic white prison-
</p>
<p>ers in each cell block. Our research hypothesis is nondirectional. It states
</p>
<p>that race and cell-block assignment are not independent. In this exam-
</p>
<p>ple, as in most chi-square tests, use of nominal-scale measures, which do
</p>
<p>not assign order or value to categories, means that one cannot define a
</p>
<p>directional research hypothesis.
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E214</p>
<p/>
</div>
<div class="page"><p/>
<p>The Sampling Distribution Because we are examining the relationship
</p>
<p>between two nominal variables, one binary and one multicategory, we
</p>
<p>use the chi-square sampling distribution. The number of degrees of free-
</p>
<p>dom for our problem is defined as in the previous example:
</p>
<p>W orking It Out
</p>
<p> � 6
</p>
<p> � (7 � 1)(2 � 1)
</p>
<p> df � (r � 1)(c � 1)
</p>
<p>In this case, we have seven categories for our row variable (cell block)
</p>
<p>and two categories for our column variable (ethnicity). The number of
</p>
<p>degrees of freedom for our sampling distribution is six.
</p>
<p>Significance Level and Rejection Region As we have no reason to pro-
</p>
<p>pose more lenient or stricter significance criteria than are used by con-
</p>
<p>vention, we will set a 0.05 significance level. To define our rejection re-
</p>
<p>gion, we turn to the row of the chi-square table associated with six
</p>
<p>degrees of freedom. Under the 0.05 column, a chi-square statistic of
</p>
<p>12.592 is listed. If the test statistic is greater than this critical value, then it
</p>
<p>falls within the rejection region of the test.
</p>
<p>The Test Statistic To calculate the test statistic in this multicategory ex-
</p>
<p>ample, we follow the same procedure used for the two-by-two table in
</p>
<p>the previous section. Our first task is to define the expected frequency
</p>
<p>for each cell of the table. We do this, as before, by dividing a marginal of
</p>
<p>the table by the total proportion of cases. Taking the overall number of
</p>
<p>non-Hispanic whites in the sample, we obtain a proportion of 0.1993:
</p>
<p>W orking It Out
</p>
<p> � 0.199335
</p>
<p> � 
240
</p>
<p>1,204
</p>
<p> Proportion � 
Ncat
N total
</p>
<p>E X T E N D I N G T H E T E S T T O M U L T I C A T E G O R Y V A R I A B L E S 215</p>
<p/>
</div>
<div class="page"><p/>
<p>To calculate the expected frequency in each cell in the non-Hispanic
</p>
<p>whites column, we multiply this proportion by the marginal total for
</p>
<p>each row. So, for example, for cell block C, we multiply 256 by
</p>
<p>0.199335, leading to an expected frequency for non-Hispanic whites of
</p>
<p>51.030. We then replicate this procedure for each of the six other cells in
</p>
<p>the non-Hispanic whites column. To calculate the expected frequencies
</p>
<p>for the nonwhites column, we simply subtract the expected frequency
</p>
<p>for the non-Hispanic whites column from the row marginal. So, for ex-
</p>
<p>ample, for nonwhites in cell block C, the expected frequency is 256 (the
</p>
<p>marginal total) minus 51.030 (the expected frequency for non-Hispanic
</p>
<p>whites for that cell block), or 204.970. Table 9.12 includes the expected
</p>
<p>and observed frequencies for the 14 cells in our example.
</p>
<p>To obtain the test statistic, we use Equation 9.2, which may be ap-
</p>
<p>plied to any two-variable chi-square problem:
</p>
<p>Again we begin by subtracting the expected frequency from the observed
</p>
<p>frequency in each cell and squaring the result. This quantity is then di-
</p>
<p>vided by the expected frequency of the cell. The chi-square statistic is
</p>
<p>found by summing the result across all 14 cells. The full set of calculations
</p>
<p>for the test statistic is presented in Table 9.13. The chi-square score for our
</p>
<p>example is 88.3610.
</p>
<p>�2 � �
r
</p>
<p>i�1
</p>
<p> �
c
</p>
<p>j�1
</p>
<p> 
( fo � f e)
</p>
<p>2
</p>
<p>fe
</p>
<p>Observed Frequencies and Expected Frequencies for Non-Hispanic
White and Nonwhite Prisoners in Seven Cell Blocks
</p>
<p>Non-Hispanic
</p>
<p>whites Nonwhites
</p>
<p>Cell block C f � 48 fo � 208 256
fe � 51.030 fe � 204.970
</p>
<p>Cell block D fo � 17 fo � 37 54
fe � 10.764 fe � 43.236
</p>
<p>Cell block E fo � 28 fo � 84 112
fe � 22.326 fe � 89.674
</p>
<p>Cell block F fo � 32 fo � 79 111
fe � 22.126 fe � 88.874
</p>
<p>Cell block G fo � 37 fo � 266 303
fe � 60.399 fe � 242.601
</p>
<p>Cell block H fo � 34 fo � 22 56
fe � 11.163 fe � 44.837
</p>
<p>Cell block I fo � 44 fo � 268 312
fe � 62.193 fe � 249.807
</p>
<p>240 964 1,204
</p>
<p>Table 9.12
</p>
<p>Row
</p>
<p>total
</p>
<p>Colum total
</p>
<p>o
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E216</p>
<p/>
</div>
<div class="page"><p/>
<p>The Decision The outcome of 88.3610 is much greater than the critical
</p>
<p>value for our test of 12.592. Accordingly, we reject the null hypothesis
</p>
<p>that race and cell-block allocation are independent (using a 5% signifi-
</p>
<p>cance level). We conclude that there is a statistically significant relation-
</p>
<p>ship between the distribution of prisoners across cell blocks and their
</p>
<p>race.
</p>
<p>E x t e n d i n g  t h e  C h i - S q u a r e  T e s t  
t o  a  R e l a t i o n s h i p  B e t w e e n  T w o  O r d i n a l  V a r i a b l e s :
I d e n t i f i c a t i o n  w i t h  F a t h e r s  a n d  D e l i n q u e n t  A c t s
</p>
<p>The examples of the application of the chi-square test presented so far
</p>
<p>have used only nominal-scale variables. This is consistent with the as-
</p>
<p>sumptions of the chi-square test. But in practice researchers sometimes
</p>
<p>use chi-square to test for independence when one or both of the vari-
</p>
<p>ables have been measured at the ordinal level of measurement. This test
</p>
<p>for independence can provide important information to the researcher.
</p>
<p>However, because the chi-square test assumes a nominal scale of mea-
</p>
<p>surement, it does not pay attention to the order of the categories in an
</p>
<p>ordinal scale. This means that a statistically significant finding can tell the
</p>
<p>researcher only that the distribution of scores observed is different from
</p>
<p>that expected had there been no relationship. It cannot test for whether
</p>
<p>the values of one variable increase as the values of the other increase or,
</p>
<p>conversely, whether the scores on one measure increase as those on the
</p>
<p>Computation of Chi-Square for Non-Hispanic White (W) 
and Nonwhite (NW) Prisoners in Seven Cell Blocks
</p>
<p>CELL BLOCK RACE fo fe (fo � fe) (fo � fe)
2
</p>
<p>C W 48 51.030 �3.030 9.1809 0.1799
C NW 208 204.970 3.030 9.1809 0.0448
D W 17 10.764 6.236 38.8877 3.6128
D NW 37 43.236 �6.236 38.8877 0.8994
E W 28 22.326 5.674 32.1943 1.4420
E NW 84 89.674 �5.674 32.1943 0.3590
F W 32 22.126 9.874 97.4959 4.4064
F NW 79 88.874 �9.874 97.4959 1.0970
G W 37 60.399 �23.399 547.5132 9.0649
G NW 266 242.601 23.399 547.5132 2.2568
H W 34 11.163 22.837 521.5286 46.7194
H NW 22 44.837 �22.837 521.5286 11.6317
I W 44 62.193 �18.193 330.9852 5.3219
I NW 268 249.807 18.193 330.9852 1.3250
</p>
<p>� � 88.3610
</p>
<p>(fo � fe)
2
</p>
<p>fe
</p>
<p>Table 9.13
</p>
<p>E X T E N D I N G T H E T E S T T O O R D I N A L V A R I A B L E S 217</p>
<p/>
</div>
<div class="page"><p/>
<p>other decrease. When you use the chi-square test for ordinal-scale vari-
</p>
<p>ables, the test itself treats the variables as if they were simply composed
</p>
<p>of a group of nominal categories.
</p>
<p>Table 9.14 presents data from the Richmond Youth Survey report on
</p>
<p>the relationship between number of delinquent acts and affectional iden-
</p>
<p>tification with one&rsquo;s father. The distribution of cases presented refers
</p>
<p>only to the white males who responded to the survey.7 The sample was
</p>
<p>a random sample (drawn without replacement) for all high school&ndash;age
</p>
<p>white males in Richmond, California, in 1965. The size of the sample is
</p>
<p>small relative to the sampling frame.
</p>
<p>If delinquency were unrelated to attachment to one&rsquo;s family&mdash;here in-
</p>
<p>dicated by the level of affectional identification with one&rsquo;s father&mdash;we
</p>
<p>would expect to find the distribution of cases for each level of delin-
</p>
<p>quency to be roughly equal across levels of identification. The distribu-
</p>
<p>tion of cases provides some indication that these variables are not, in
</p>
<p>fact, independent. For example, among the youths who wanted to be
</p>
<p>like their father in every way, 63% reported that they had not committed
</p>
<p>a delinquent act. This was true for only 41% of those who did not want
</p>
<p>to be at all like their fathers. Our question is whether the differences we
</p>
<p>observe in our sample are large enough for us to conclude that identifi-
</p>
<p>cation with one&rsquo;s father and delinquency are related in the population
</p>
<p>from which our sample has been drawn.
</p>
<p>7David F. Greenberg, &ldquo;The Weak Strength of Social Control Theory,&rdquo; Crime and Delin-
</p>
<p>quency 45:1 (1999): 66&ndash;81.
</p>
<p>Affectional Identification with Father by Number of Delinquent Acts
</p>
<p>AFFECTIONAL D E L I N Q U E N T  A C T S
IDENTIFICATION 
WITH FATHER None One Two or More
</p>
<p>In every way 77 25 19 121
63.636% 20.661% 15.702% 100%
</p>
<p>In most ways 263 97 44 404
65.099% 24.010% 10.891% 100%
</p>
<p>In some ways 224 97 66 387
57.881% 25.065% 17.054% 100%
</p>
<p>In just a few ways 82 52 38 172
47.674% 30.233% 22.093% 100%
</p>
<p>Not at all 56 30 52 138
40.580% 21.739% 37.681% 100%
</p>
<p>702 301 219 1,222
57.447% 24.632% 17.921% 100%
</p>
<p>Table 9.14
</p>
<p> Row total
</p>
<p>Column total
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E218</p>
<p/>
</div>
<div class="page"><p/>
<p>Assumptions:
</p>
<p>Level of Measurement: Nominal scales (our study examines two ordinal-
</p>
<p>scale measures).
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method: Independent random sampling (no replacement;
</p>
<p>sample is small relative to population).
</p>
<p>Sampling Frame: High school&ndash;age white males in Richmond, California,
</p>
<p>in 1965.
</p>
<p>Hypotheses:
</p>
<p>H0: Affectional identification with father and delinquency are independent.
</p>
<p>H1: Affectional identification with father and delinquency are not
</p>
<p>independent.
</p>
<p>The Sampling Distribution Although we are using two ordinal-scale mea-
</p>
<p>sures rather than two nominal-scale measures, we have chosen to use
</p>
<p>the chi-square sampling distribution to test for a relationship. This viola-
</p>
<p>tion of the nominal-scale assumption for the chi-square test is accept-
</p>
<p>able. However, by placing the violation of the assumption in parentheses
</p>
<p>next to the test requirement of a nominal level of measurement, we re-
</p>
<p>mind ourselves that chi-square is not concerned with the order of the
</p>
<p>categories in the measures examined. It treats the two ordinal-scale mea-
</p>
<p>sures as if they were nominal-scale measures and simply tests for
</p>
<p>whether the distributions among the categories depart from what we
</p>
<p>would expect under an assumption of independence. As we did in the
</p>
<p>two previous examples, we calculate degrees of freedom as follows:
</p>
<p>W orking It Out
</p>
<p> � 8
</p>
<p> � (5 � 1)(3 � 1)
</p>
<p> df � (r � 1)(c � 1)
</p>
<p>Significance Level and Rejection Region There is no reason to propose a
</p>
<p>more lenient or stricter significance level for this analysis, so we will
</p>
<p>stick with a 0.05 significance level. Given that we have eight degrees of
</p>
<p>freedom and a significance level of 0.05, we can consult the chi-square
</p>
<p>table and determine that our critical value of the chi-square statistic is
</p>
<p>15.507. If the test statistic is greater than this value, it falls in the rejection
</p>
<p>E X T E N D I N G T H E T E S T T O O R D I N A L V A R I A B L E S 219</p>
<p/>
</div>
<div class="page"><p/>
<p>region of the test, and we can conclude that delinquency is significantly
</p>
<p>related to affectional identification.
</p>
<p>The Test Statistic To determine the expected frequency for each cell in
</p>
<p>the table, we follow the same format we have used in the previous two
</p>
<p>examples. As before, we start with the calculation of the marginal for no
</p>
<p>delinquent acts and divide by the total number of cases, which gives us
</p>
<p>a value of 0.574468:
</p>
<p>W orking It Out
</p>
<p> � 0.574468
</p>
<p> � 
702
</p>
<p>1,222
</p>
<p> Proportion � 
Ncat
N total
</p>
<p>To calculate the expected frequency for each cell in the no delinquent
</p>
<p>acts column, we take this proportion and multiply it by the marginal total
</p>
<p>for the row. For the first row, we multiply 0.574468 by 121, which gives
</p>
<p>us an expected frequency of 69.511. Similarly, for the second row, we
</p>
<p>multiply 0.574468 by 404, giving us an expected frequency of 232.085.
</p>
<p>We continue this procedure for the remaining three rows in the no delin-
</p>
<p>quent acts column.
</p>
<p>For the second column, we need to determine the marginal proportion
</p>
<p>for those cases with one delinquent act. Since there are 301 cases in the
</p>
<p>marginal for one delinquent act, the corresponding proportion is
</p>
<p>(301/1,222) � 0.246318. To obtain the expected frequencies for this sec-
</p>
<p>ond column, we multiply 0.246318 by the corresponding row marginal.
</p>
<p>So, for the first row of the second column, the expected frequency is ob-
</p>
<p>tained by multiplying 0.246318 by 121, which gives us 29.804. This proce-
</p>
<p>dure is repeated to complete the remaining cells in the second column.
</p>
<p>Finally, to determine the expected frequencies for the cells in the
</p>
<p>third column, we simply add the expected frequencies for the first two
</p>
<p>columns and subtract that sum from the row marginal. For example, in
</p>
<p>the first row, the two expected frequencies obtained thus far are 69.511
</p>
<p>and 29.804. If we add these two values (69.511 � 29.804 � 99.315) and
</p>
<p>subtract this sum from the row marginal (121), we find that the ex-
</p>
<p>pected frequency for the cell in the third column of the first row is equal to
</p>
<p>(121 � 99.315) � 21.685. To complete the table of expected frequencies,
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E220</p>
<p/>
</div>
<div class="page"><p/>
<p>we repeat this operation for the remaining cells in the third column.
</p>
<p>Table 9.15 contains all the observed and expected frequencies.
</p>
<p>To obtain the test statistic, we again use Equation 9.2, which may be
</p>
<p>applied to any two-variable chi-square problem:
</p>
<p>Again we begin by subtracting the expected from the observed fre-
</p>
<p>quency in each cell and squaring the result. This quantity is then divided
</p>
<p>by the expected frequency of the cell. The chi-square statistic is found
</p>
<p>by summing the result across all cells. The full set of calculations neces-
</p>
<p>sary for obtaining the value of the chi-square test statistic appears in
</p>
<p>Table 9.16. The chi-square statistic for our test has a value of 61.532.
</p>
<p>The Decision
</p>
<p>The calculated chi-square statistic of 61.532 is much larger than the criti-
</p>
<p>cal value of 15.507 for the chi-square distribution with eight degrees of
</p>
<p>freedom. This means that the observed significance level for our test is
</p>
<p>less than the criterion significance level we set at the outset (p � 0.05).
</p>
<p>Thus, we reject the null hypothesis that affectional identification with fa-
</p>
<p>ther is not related to number of delinquent acts (at a 5% significance
</p>
<p>level). In turn, we conclude that for adolescent males there is a statisti-
</p>
<p>cally significant relationship between delinquency and affectional identi-
</p>
<p>fication with father. Importantly, this statistical inference refers directly to
</p>
<p>our sampling frame: high school&ndash;age white males in Richmond, Califor-
</p>
<p>nia, in 1965.
</p>
<p>�2 � �
r
</p>
<p>i�1
</p>
<p> �
c
</p>
<p>j�1
</p>
<p> 
( fo � fe)
</p>
<p>2
</p>
<p>fe
</p>
<p>Observed and Expected Frequencies for Affectional Identification
with Father and Number of Delinquent Acts
</p>
<p>AFFECTIONAL D E L I N Q U E N T  A C T S
IDENTIFICATION 
WITH FATHER None One Two or More
</p>
<p>In every way fo � 77 fo � 25 fo � 19 121
fe � 69.511 fe � 29.804 fe � 21.685
</p>
<p>In most ways fo � 263 fo � 97 fo � 44 404
fe � 232.085 fe � 99.512 fe � 72.403
</p>
<p>In some ways fo � 224 fo � 97 fo � 66 387
fe � 222.319 fe � 95.325 fe � 69.355
</p>
<p>In just a few ways fo � 82 fo � 52 fo � 38 172
fe � 98.809 fe � 42.367 fe � 30.824
</p>
<p>Not at all fo � 56 fo � 30 fo � 52 138
fe � 79.277 fe � 33.992 fe � 24.731
</p>
<p>702 301 219 1,222
</p>
<p>Table 9.15
</p>
<p>Row total
</p>
<p>Column total
</p>
<p>E X T E N D I N G T H E T E S T T O O R D I N A L V A R I A B L E S 221</p>
<p/>
</div>
<div class="page"><p/>
<p>T h e  U s e  o f  C h i - S q u a r e  W h e n  
S a m p l e s  A r e  S m a l l :  A  F i n a l  N o t e
</p>
<p>The chi-square test is often used by criminal justice researchers. How-
</p>
<p>ever, it has a very important limitation in its application to studies with
</p>
<p>small or highly skewed samples. When more than one in five (20%) of
</p>
<p>the cells in your table has an expected frequency of five or less, it is gen-
</p>
<p>erally considered inappropriate to use a chi-square test. In such situa-
</p>
<p>tions, it is recommended that you combine categories of your variables
</p>
<p>until you meet the minimum expected-frequencies requirement.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>Whereas the binomial distribution is relevant only for binary variables,
</p>
<p>the chi-square distribution can be used to examine a variable with
</p>
<p>more than two categories.
</p>
<p>The shape of the chi-square distribution chosen depends on the
</p>
<p>degrees of freedom associated with the test. The formula for degrees of
</p>
<p>freedom defines how many categories would have to be known for us to
</p>
<p>the number of degrees of freedom, the flatter the distribution. In practi-
</p>
<p>cal terms, as the number of degrees of freedom increases, a larger chi-
</p>
<p>square statistic is required to reject the null hypothesis.
</p>
<p>Computation of Chi-Square for Affectional Identification 
with Father and Delinquency
</p>
<p>IDENTIFICATION DELINQUENCY fo fe (fo � fe) (fo � fe)
2
</p>
<p>Every way None 77 69.511 7.489 56.085 0.807
Every way One 25 29.804 �4.804 23.078 0.774
Every way Two or more 19 21.685 �2.685 7.209 0.332
Most ways None 263 232.085 30.915 955.737 4.118
Most ways One 97 99.512 �2.512 6.310 0.063
Most ways Two or more 44 72.403 �28.403 806.730 11.142
Some ways None 224 222.319 1.681 2.826 0.013
Some ways One 97 95.325 1.675 2.806 0.029
Some ways Two or more 66 69.355 �3.355 11.256 0.162
Few ways None 82 98.809 �16.809 282.543 2.859
Few ways One 52 42.367 9.633 92.795 2.190
Few ways Two or more 38 30.824 7.176 51.495 1.671
Not at all None 56 79.277 �23.277 541.819 6.835
Not at all One 30 33.992 �3.992 15.936 0.469
Not at all Two or more 52 24.731 27.269 743.598 30.067
</p>
<p>� � 61.532
</p>
<p>(fo � fe )
2
</p>
<p>fe
</p>
<p>Table 9.16
</p>
<p>8
</p>
<p>8Another alternative solution is to use another group of non-parametric tests, defined as 
</p>
<p>   
</p>
<p>be able to predict the remaining categories with certainty. The greater
</p>
<p>&lsquo;e xact &rsquo;  tests, to estimate the observed significance level (see Alan Agresti, Categorical Data
Analysis, New York, John Wiley, 1990). Such tests (e.g., Fisher &rsquo;s Exact Test) which develop
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E222</p>
<p/>
</div>
<div class="page"><p/>
<p>The chi-square test of statistical significance is a nonparametric test. To
</p>
<p>calculate the test statistic, the researcher must first identify the observed
</p>
<p>frequency and the expected frequency of each category. The expected
</p>
<p>frequencies are those one would expect under the assumption of the null
</p>
<p>hypothesis. They are distributed in the same proportions as the marginal
</p>
<p>frequencies. The chi-square formula is then applied to each category, or
</p>
<p>cell, in the table. If the observed frequencies differ substantially from the
</p>
<p>square statistic will be small. If the two frequencies are the same, the statis-
</p>
<p>tic will be 0. The larger the statistic (and the smaller the number of degrees
</p>
<p>of freedom), the easier it will be to reject the null hypothesis. The chi-
</p>
<p>square statistic is always positive. Because the chi-square test relies on nom-
</p>
<p>inal nonordered data, it is not concerned with the direction of outcomes.
</p>
<p>K e y  T e r m s
</p>
<p>cells The various entries in a table, each of
</p>
<p>which is identified by a particular row and
</p>
<p>column. When we use a table to compare
</p>
<p>two variables, it is convenient to refer to
</p>
<p>each combination of categories as a cell.
</p>
<p>chi-square distribution A sampling distri-
</p>
<p>bution that is used to conduct tests of sta-
</p>
<p>tistical significance with binary or multicat-
</p>
<p>egory nominal variables. The distribution is
</p>
<p>nonsymmetrical and varies according to de-
</p>
<p>grees of freedom. All the values in the dis-
</p>
<p>tribution are positive.
</p>
<p>chi-square statistic The test statistic re-
</p>
<p>sulting from applying the chi-square for-
</p>
<p>mula to the observed and expected fre-
</p>
<p>quencies for each cell. This statistic tells
</p>
<p>us how much the observed distribution
</p>
<p>differs from that expected under the null
</p>
<p>hypothesis.
</p>
<p>degrees of freedom A mathematical
</p>
<p>index that places a value on the extent to
</p>
<p>which a particular operation is free to vary
</p>
<p>after certain limitations have been imposed.
</p>
<p>Calculating the degrees of freedom for a
</p>
<p>chi-square test determines which chi-
</p>
<p>square probability distribution we use.
</p>
<p>expected frequency The number of ob-
</p>
<p>servations one would predict for a cell if
</p>
<p>the null hypothesis were true.
</p>
<p>marginal The value in the margin of a
</p>
<p>table that totals the scores in the appropri-
</p>
<p>ate column or row.
</p>
<p>observed frequency The observed result
</p>
<p>of the study, recorded in a cell.
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>�2 Chi-square
</p>
<p>df Degrees of freedom
</p>
<p>observed frequencies are similar to the expected frequencies, then the chi-
</p>
<p>expected frequencies, then the chi-square statistic will be large. If the 
</p>
<p> begun to provide exact test options for larger tables.
</p>
<p>a sampling distribution for each problem examined, have been made more practical with
</p>
<p>Computational options for cross tabulations. A few statistical programs (e.g., SAS) have 
the advent of powerful Computers. SPSS provides exact tests for two by two tables as
</p>
<p>S Y M B O L S A N D F O R M U L A S 223</p>
<p/>
</div>
<div class="page"><p/>
<p>fo Observed frequency
</p>
<p>fe Expected frequency
</p>
<p>c Number of columns
</p>
<p>r Number of rows
</p>
<p>k Number of categories
</p>
<p>To determine the degrees of freedom for a chi-square test including only
</p>
<p>one variable:
</p>
<p>df � k � 1
</p>
<p>To determine the degrees of freedom for a chi-square test including two
</p>
<p>variables:
</p>
<p>df � (r � 1)(c � 1)
</p>
<p>To determine the chi-square statistic for one variable:
</p>
<p>To determine the chi-square statistic for two variables:
</p>
<p>E x e r c i s e s
</p>
<p>9.1 Local community leaders are concerned about the distribution of
homicides in their small town. The local police department broke the
city into six recognizable neighborhoods of the same size and discov-
ered the following distribution of homicides:
</p>
<p>Neighborhood Number of Homicides
</p>
<p>A 14
</p>
<p>B 9
</p>
<p>C 17
</p>
<p>D 3
</p>
<p>E 7
</p>
<p>F 10
</p>
<p>�2 � �
r
</p>
<p>i�1
</p>
<p> �
c
</p>
<p>j�1
</p>
<p> 
( fo � fe)
</p>
<p>2
</p>
<p>fe
</p>
<p>�2 � �
k
</p>
<p>i�1
</p>
<p> 
( fo � fe)
</p>
<p>2
</p>
<p>fe
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E224</p>
<p/>
</div>
<div class="page"><p/>
<p>Community leaders would like to know whether the homicides are
randomly distributed across these six neighborhoods.
</p>
<p>a. Use a 5% level of significance and outline each of the steps re-
quired in a test of statistical significance.
</p>
<p>b. What can you conclude about the distribution of homicides across
these six neighborhoods?
</p>
<p>9.2 Sergeant Bob is in charge of the duty roster at Gatley police station.
Every week, it is his responsibility to randomly assign the five beat of-
ficers, including his son Bob Jr., to patrol in each of the five zones
that make up the city of Gatley. Zones A and D are favored by all the
officers because they are usually quiet. Of the others, Zone C is noto-
riously dangerous. The officers have recently begun to suspect
Sergeant Bob of favoritism toward his son. In the last 30 weeks, Bob
Jr. has been assigned to Zone A 12 times, Zone B and Zone C 2 times
each, Zone D 9 times, and Zone E 5 times.
</p>
<p>a. Do the other officers have reason to believe that Sergeant Bob is
not assigning zones in a random manner? Use a 5% level of signifi-
cance and outline each of the steps required in a test of statistical
significance.
</p>
<p>b. Would your answer be any different if a 1% level of significance
were used?
</p>
<p>9.3 In the past 100 years, there have been more than 250 successful
breakouts from Didsbury Prison. Mike is a researcher who has been
hired by the prison governor to investigate the phenomenon. Details
are available only for those breakouts that took place in the past ten
years&mdash;a total of 30. Using the records of these 30 breakouts as a sam-
ple, Mike decides to break the figures down to see whether breakouts
were more common in certain wings of the prison than in others. It
transpires that of the 30 breakouts, 4 have been from A-Wing, 8 from
B-Wing, 15 from C-Wing, and 3 from D-Wing.
</p>
<p>a. Does Mike have enough evidence to conclude that, over the 100-
year period, breakouts were more (or less) likely to occur from cer-
tain wings than from others? Use a 5% level of significance and out-
line each of the steps required in a test of statistical significance.
</p>
<p>b. Would your answer be any different if a 1% level of significance
were used?
</p>
<p>c. Are there any problems with Mike&rsquo;s choice of a sample? Explain
your answer.
</p>
<p>9.4 A study of death penalty cases (all first-degree murder charges with
aggravating circumstances) revealed the following relationship be-
tween the victim&rsquo;s race and the chances the offender was sentenced to
death: In 100 cases involving white victims, 20 offenders were sen-
</p>
<p>E X E R C I S E S 225</p>
<p/>
</div>
<div class="page"><p/>
<p>tenced to death. In 100 cases involving black victims, 10 offenders
were sentenced to death.
</p>
<p>a. Is there a relationship between the race of the victim and the likeli-
hood an offender was sentenced to death? Use a 5% level of signifi-
cance and outline each of the steps required in a test of statistical
significance.
</p>
<p>b. Would your answer be different if a 1% level of significance were
used?
</p>
<p>9.5 At a local school, 46 children were accused of cheating on exams over
the course of a semester. In an innovation, the principal decided that
every second child accused of cheating would be brought before a
&ldquo;peer jury&rdquo; to decide guilt or innocence. In all other cases, the deci-
sion would be made by the examiners as usual. Of the 30 children
who were adjudged guilty over the course of the semester, 18 were
convicted by the peer jury, and the rest were convicted by the exam-
iners. Of the children who were adjudged not guilty, 5 were acquitted
by their peers.
</p>
<p>a. The principal is mainly interested in the educational value of the
experiment, but he will discontinue it if it becomes clear that the
peer jury and the examiners make different decisions to a statisti-
cally significant degree. He is willing to take a 5% risk of error.
Should the scheme be continued? Outline each of the steps of a test
of statistical significance.
</p>
<p>b. Could the principal base the test on a directional hypothesis? If so,
what would that hypothesis be, and would it make a difference in
his final decision?
</p>
<p>9.6 In the course of a year, Jeremy, a law student, observed a total of 55
cases in which an accused male pleaded guilty to a serious traffic of-
fense. He observed that of the 15 who were sentenced to prison, 6
wore a shirt and tie in court. Of the 40 who were not sentenced to
prison, 8 wore a shirt and tie in court.
</p>
<p>a. Can Jeremy conclude that there is a link between the physical ap-
pearance of the accused and whether he is imprisoned? Use a 5%
level of significance and outline each of the steps required in a test
of statistical significance.
</p>
<p>b. What level of significance would be required for his decision to be
reversed?
</p>
<p>9.7 Sasha was interested in the extent to which people are prepared to in-
tervene to help a stranger and whether the race of the stranger is rele-
vant to the likelihood of intervention. She hired four male actors: one
of African ancestry, one of Asian ancestry, one of European ancestry,
and one of Indian ancestry. The actors were each told to fake a fall 
in a busy shopping street and pretend to be in some pain. Sasha
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E226</p>
<p/>
</div>
<div class="page"><p/>
<p>observed from nearby and recorded whether, within five minutes of
the actor&rsquo;s fall, anyone had stopped to see if he was okay. Each actor
repeated the experiment 40 times.
</p>
<p>The results were as follows:
</p>
<p>Ancestry � �
</p>
<p>African 4 36
</p>
<p>Asian 0 40
</p>
<p>European 20 20
</p>
<p>Indian 8 32
</p>
<p>(� � Intervention within 5 mins; � � no intervention)
</p>
<p>a. Can Sasha conclude that there is a link between race of victim and
readiness to intervene? Use a 5% level of significance and outline
each of the steps required in a test of statistical significance.
</p>
<p>b. Would your answer be any different if a 1% level of significance
were used?
</p>
<p>9.8 Dave takes a random sample of the speeches, interviews, and official
statements given by the prime minister and the interior minister of a
particular country over the course of a year in which reference is
made to &ldquo;prison policy.&rdquo; He analyzes the content of the statements in
his sample and discovers five different types of justification for the
government&rsquo;s prison policy. Dave then records each time the prime
minister or interior minister refers to any of the five justification types.
The results are as follows:
</p>
<p>Justification Type Prime Minister Interior Minister
</p>
<p>Incapacitation or protecting society 6 16
</p>
<p>Specific deterrence 2 14
</p>
<p>General deterrence 4 20
</p>
<p>Rehabilitation 0 15
</p>
<p>Retribution 13 10
</p>
<p>a. Is there a statistically significant difference between the policy state-
ments of the prime minister and those of the interior minister? Use
a 5% level of significance and outline each of the steps required in
a test of statistical significance.
</p>
<p>b. Would your answer be any different if a 1% level of significance
were used?
</p>
<p>9.9 The Television Complaints Board monitors the standards of morality
for a nation&rsquo;s TV channels. It has recently set up a telephone hotline
for viewers who wish to complain about sex, violence, or foul lan-
guage on any of the nation&rsquo;s three TV channels. In its first month of
operation, the board received the following complaints:
</p>
<p>E X E R C I S E S 227</p>
<p/>
</div>
<div class="page"><p/>
<p>Channel 1 Channel 2 Channel 3
</p>
<p>Sex 2 8 10
</p>
<p>Violence 10 12 10
</p>
<p>Foul language 3 10 15
</p>
<p>a. Which of the following questions would a chi-square test of these
results seek to answer?
</p>
<p>i. Is there a statistically significant difference between the number
of complaints made against each channel?
</p>
<p>ii. Is there a statistically significant difference between the number
of each type of complaint made?
</p>
<p>iii. Is there a statistically significant difference between the types of
different complaints received about the three different stations?
</p>
<p>b. Answer the question you chose in part a by running a chi-square test
at a 5% level of significance. Should the null hypothesis be overturned?
</p>
<p>9.10 A survey of public opinion about the criminal justice system asked re-
spondents to complete the following statement: &ldquo;The criminal justice
system treats offenders. . . .&rdquo; The researchers found the following dis-
tribution of responses by gender of the respondent:
</p>
<p>Gender of Respondent Too Lenient About Right Too Harsh
</p>
<p>Female 15 50 35
</p>
<p>Male 40 35 25
</p>
<p>a. Is there a relationship between the gender of the respondent and per-
ceptions of punishment severity? Use a 5% level of significance and
outline each of the steps required in a test of statistical significance.
</p>
<p>b. Would your answer be different if the significance level were 0.01?
</p>
<p>c. What can you conclude about the relationship between gender and
perceptions of punishment severity?
</p>
<p>9.11 A researcher is interested in the link between the type of offense a de-
fendant is charged with and the manner in which a conviction is ob-
tained. An examination of court records of a random sample of con-
victed offenders reveals the following distribution of cases:
</p>
<p>How Conviction Was Obtained
</p>
<p>Type of Charge Offense Jury Trial Bench Trial Guilty Plea
</p>
<p>Violent 19 13 67
</p>
<p>Property 5 8 92
</p>
<p>Drug 8 11 83
</p>
<p>Other 10 6 74
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E228</p>
<p/>
</div>
<div class="page"><p/>
<p>a. Is there a relationship between type of charge offense and method
of conviction? Use a 5% level of significance and outline each of
the steps required in a test of statistical significance.
</p>
<p>b. Would your answer be any different if a 1% level of significance
were used?
</p>
<p>c. What can you conclude about the relationship between type of
charge offense and method of conviction?
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>Up to this point, all of our computer examples have relied on individual-level 
</p>
<p>data, where each line of data in the spreadsheet view, whether SPSS or Stata, 
</p>
<p>referred to one case. However, there are many other times where we have been 
</p>
<p>given data in tabular form, say from a government publication, and we wish to 
</p>
<p>perform some analysis on that data. In these instances, we may have data on 
</p>
<p>several hundred, or even thousand, cases, but the data appear in aggregated form. 
</p>
<p>All of the examples in this chapter take that form, too.
</p>
<p>Prior to discussing how to obtain chi-square values to test for a relationship 
</p>
<p>between two categorical variables, we illustrate how to enter tabular data into the 
</p>
<p>software program.
</p>
<p>SPSS
</p>
<p>Entering Tabular Data
</p>
<p>The most direct way of entering tabular data is to think of the rows and columns 
</p>
<p>in a table as two separate variables and the number of cases in each cell of the 
</p>
<p>table as a third variable (representing a count of the number of cases with the 
</p>
<p>combination of the values for the two variables). For example, consider the data 
</p>
<p>presented in Table 9.7:
</p>
<p>Imprisoned Not Imprisoned Row Margin
</p>
<p>Subsequently Arrested  33 19  52
</p>
<p>Not Subsequently Arrested  67 48 115
</p>
<p>Column Margin 100 67 167
</p>
<p>We can enter the information from this table as three variables: the row, 
</p>
<p>the column, and the count. Note that as you enter data into SPSS, the default 
</p>
<p>variable names will be VAR00001, VAR00002, and VAR00003. There are two 
</p>
<p>equivalent ways of renaming these variables in SPSS. We will rename the row 
</p>
<p>variable from VAR00001 to arrested (to represent whether or not the person was 
</p>
<p>subsequently arrested), the column variable from VAR00002 to prison (to repre-
</p>
<p>sent whether or not the person had been imprisoned), and the number of cases 
</p>
<p>in each cell from VAR00003 to count. The equivalent syntax for renaming these 
</p>
<p>variables is:
</p>
<p>C O M P U T E R E X E R C I S E S 229</p>
<p/>
</div>
<div class="page"><p/>
<p>After entering these data into SPSS and changing the variable names, you would 
</p>
<p>have three variables and four lines of data like the following (with the variable 
</p>
<p>names appearing at the top of each column of data):
</p>
<p>arrest prison count
</p>
<p>1.0000 1.0000 33.0000
</p>
<p>1.0000 2.0000 19.0000
</p>
<p>2.0000 1.0000 67.0000
</p>
<p>2.0000 2.0000 48.0000
</p>
<p>If you were to begin working with SPSS at this point, you would not 
</p>
<p>obtain the correct results, since SPSS will treat these data as representing 
</p>
<p>only four observations (You may want to confirm this for yourself&mdash;run the 
</p>
<p>FREQUENCIES command on either arrest or prison and see how many obser-
</p>
<p>vations SPSS thinks it is working with.).
</p>
<p>To have SPSS recognize the 167 observations represented by these four lines, 
</p>
<p>you need to tell SPSS to weight the cases by the &ldquo;count&rdquo; variable. This is done 
</p>
<p>with the WEIGHT command:
</p>
<p>For our example, this would be:
</p>
<p>The WEIGHT command tells SPSS that you have entered data in tabular form.
</p>
<p>Obtain the Chi-Square Statistic
</p>
<p>To continue our example using the data from Table 9.7, we would run the fol-
</p>
<p>lowing command:
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E230
</p>
<p>The chi-square statistic is obtained by using the CROSSTABS command. The 
</p>
<p>basic format of the CROSSTABS command is
</p>
<p>RENAME VARIABLES VAR00001 = arrest VAR00002 = prison 
</p>
<p>VAR00003 = count.
</p>
<p>Or,
</p>
<p>RENAME VARIABLES (VAR00001 VAR00002 VAR00003 = arrest  
</p>
<p>prison count).
</p>
<p>WEIGHT BY variable_name.
</p>
<p>WEIGHT BY count.
</p>
<p>CROSSTABS
</p>
<p>/TABLES = row_variable BY column_variable
</p>
<p>/STATISTICS = CHISQ.
</p>
<p>CROSSTABS
</p>
<p>/TABLES = arrest BY prison
</p>
<p>/STATISTICS = CHISQ.</p>
<p/>
</div>
<div class="page"><p/>
<p>The output produced by executing this command will contain a cross-tabulation 
</p>
<p>of the data that should be identical to Table 9.7. Immediately below this table 
</p>
<p>will be another table labeled &ldquo;Chi-square tests.&rdquo; The &ldquo;Pearson Chi-Square&rdquo; is the 
</p>
<p>name of the chi-square statistic that you have learned to calculate in this chap-
</p>
<p>ter. The value reported by SPSS is 0.403, which differs from the value reported 
</p>
<p>above (0.402) by 0.001, which can be attributed to rounding error above.
</p>
<p>All of the commands discussed above are illustrated in the SPSS syntax file 
</p>
<p>Chapter_9.sps.
</p>
<p>Stata
</p>
<p>A similar process is used in Stata to enter the data from Table 9.7. Recall that to 
</p>
<p>enter data, you will need to click on the &ldquo;Data Editor&rdquo; button at the top center 
</p>
<p>of the Stata window. Note that as you enter the data, the default variable  
</p>
<p>names are var1, var2, and var3. To change these to arrest, prison, and count,  
</p>
<p>respectively, enter the following commands:
</p>
<p>Alternatively, you could edit the variable name in the far right box in the Stata 
</p>
<p>window (be sure to click on the lock symbol to &ldquo;unlock&rdquo; the variable properties 
</p>
<p>and allow you to make changes to the variable name).
</p>
<p>In contrast to SPSS, there is no direct weighting of the data directly. Rather, 
</p>
<p>Stata includes an option in nearly all of its statistical procedures that will allow 
</p>
<p>the user to include a variable that represents frequency counts, as we have in our 
</p>
<p>example.
</p>
<p>To obtain the chi-square statistic and a two-way cross-tabulation, we enter the 
</p>
<p>following command:
</p>
<p>Note that we have assumed we are not working with tabular data in this example, 
</p>
<p>but individual-level data as we have in all previous chapters. The addition of the 
</p>
<p>chi2 option after the comma requests the chi-square statistic.
</p>
<p>For our example, we would enter the following command:
</p>
<p>We have added the fweight = option [in required brackets] to indicate that we 
</p>
<p>have a frequency weight variable named count that indicates the number of cases 
</p>
<p>with each characteristic of the two variables.
</p>
<p>The output from running this command includes a cross-tabulation that 
</p>
<p>should be identical to Table 9.7. Just below the table is the chi-square statistic, 
</p>
<p>C O M P U T E R E X E R C I S E S 231
</p>
<p>rename var1 arrest
</p>
<p>rename var2 prison
</p>
<p>rename var3 count
</p>
<p>tabulate row_variable column_variable, chi2
</p>
<p>tabulate arrest prison [fweight = count], chi2</p>
<p/>
</div>
<div class="page"><p/>
<p>reported as &ldquo;Pearson chi2(1) = 0.4031.&rdquo; The value &ldquo;1&rdquo; in the parentheses refers 
</p>
<p>to the number of degrees of freedom for the chi-square statistic. This is a com-
</p>
<p>mon reporting format in Stata output for a chi-square statistics&mdash;the degrees of 
</p>
<p>freedom associated with the test will be listed in the adjacent parentheses.
</p>
<p>All of these commands are illustrated in the Stata do file Chapter_9.do.
</p>
<p>Problems
</p>
<p> 1. Input the data on race and cell-block assignment from Table 9.11. 
</p>
<p>Compute the value of  the chi-square statistic for these data. How does it 
</p>
<p>compare to the value reported in the text?
</p>
<p> 2. Input the data on affectional identification with father and delinquency 
</p>
<p>from Table 9.14. Compute the value of  the chi-square statistic for these 
</p>
<p>data. How does it compare to the value reported in the text?
</p>
<p> 3. Enter the data from Exercise 9.7. Compute the value of  the chi-square 
</p>
<p>statistic for these data. How does it compare with the value that you have 
</p>
<p>calculated for this exercise?
</p>
<p> 4. Enter the data from Exercise 9.11. Compute the value of  the chi-square 
</p>
<p>statistic for these data. How does it compare with the value that you have 
</p>
<p>calculated for this exercise?
</p>
<p> 5. Open the NYS data file (nys_1.sav, nys_1_ student.sav, or nys_1.dta). Use 
</p>
<p>a 5% level of  significance and outline each of  the steps required in a test 
</p>
<p>of  statistical significance for each of  the following possible relationships:
</p>
<p>a. Is ethnicity related to grade point average?
</p>
<p>b. Is marijuana use among friends related to the youth&rsquo;s attitudes about 
</p>
<p>marijuana use?
</p>
<p>c. Is the importance of  going to college related to the importance of  
</p>
<p> having a job?
</p>
<p>d. Is grade point average related to the importance of  having a job?
</p>
<p>e. Is the sex of  the youth related to the importance of  having friends?
</p>
<p>f. Is the importance of  having a job related to the youth&rsquo;s attitudes about 
</p>
<p>marijuana use?
</p>
<p>g. SPSS notes at the bottom of  each cross-tabulation the number and 
</p>
<p>percentage of  all cells that had expected frequencies less than 5. For 
</p>
<p>parts a through f, are there any cross-tabulations that produce expected 
</p>
<p>frequencies of  less than 5 for 20 % or more of  all cells in the table? If  
</p>
<p>so, what are the consequences for interpreting the chi-square statistic? 
</p>
<p>Explain how the categories of  one or more variables could be com-
</p>
<p>bined to produce a table that has fewer cells with expected frequencies 
</p>
<p>of  less than 5.
</p>
<p>C H A P T E R N I N E :  C H I - S Q U A R E232</p>
<p/>
</div>
<div class="page"><p/>
<p> 6. Open the data file pcs_98.sav. These data represent a random sample 
</p>
<p>of  1,300 offenders convicted of  drug, property, or violent offenses and 
</p>
<p>sentenced in Pennsylvania in 1998. The full data file contains information 
</p>
<p>on all offenders (more than 40,000) who were convicted of  misdemea-
</p>
<p>nor and felony offenses and sentenced to some form of  punishment in 
</p>
<p>Pennsylvania in 1998. The full data file is available through the National 
</p>
<p>Archive of  Criminal Justice Data at http://www.icpsr.umich.edu/NACJD 
</p>
<p>(Prior to answering the following questions, you may find it helpful to 
</p>
<p>review the list of  variables and how each is coded.). Use a 5% level of  
</p>
<p>significance and outline each of  the steps required in a test of  statistical 
</p>
<p>significance for each of  the following relationships:
</p>
<p>a. Is the sex of  the offender related to the method of  conviction (i.e., 
</p>
<p>plea, bench trial, or jury trial)?
</p>
<p>b. Is the race-ethnicity of  the offender related to whether the offender was 
</p>
<p>incarcerated or not?
</p>
<p>c. Is the method of  conviction related to the type of  punishment received 
</p>
<p>(i.e., probation, jail, or prison)?
</p>
<p>d. Is the type of  conviction offense (i.e., drug, property, or violent) related 
</p>
<p>to the method of  conviction?
</p>
<p>C O M P U T E R E X E R C I S E S 233</p>
<p/>
<div class="annotation"><a href="http://www.icpsr.umich.edu/NACJD">http://www.icpsr.umich.edu/NACJD</a></div>
</div>
<div class="page"><p/>
<p>The Normal Distribution 
</p>
<p>and Its Application 
</p>
<p>to Tests of Statistical Significance
</p>
<p>w h e n  p o p u l a t i o n  p a r a m e t e r s  a r e  u n k n o w n
</p>
<p>How Can We Make Assumptions About an Unknown Population?
</p>
<p>When Can It Be Used?
</p>
<p>How Can We Define a Sampling Distribution 
</p>
<p>When the Parameters Are Unknown?
</p>
<p>p o p u l a t i o n  d i s t r i b u t i o n
</p>
<p>P a r a m e t r i c  t e s t s  f o r  a  n o r m a l  
</p>
<p>U s i n g  t h e  n o r m a l  s a m p l i n g  d i s t r i b u t i o n  
</p>
<p>T w o  e x a m p l e s
</p>
<p>C h a p t e r  t e n
</p>
<p>What are the Characteristics of the Normal Frequency Distribution?
</p>
<p>What is the z-Score?
</p>
<p>When can We Use the Normal Sampling Distribution?
</p>
<p>What are the Assumptions of the One-Sample z-Test for Means?
</p>
<p>What is the Central Limit Theorem?
</p>
<p>What is the z-Test for Proportions?
</p>
<p>What is the t-Test for Means?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_10,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>IN CHAPTERS 8 AND 9, tests of statistical significance were presented that
did not make assumptions about the population distribution of the char-
</p>
<p>acteristics studied. We now turn to a different type of test of statistical
</p>
<p>significance in which the researcher must make certain assumptions
</p>
<p>about the population distribution. These tests, called parametric tests, are
</p>
<p>widely used in criminal justice and criminology because they allow the
</p>
<p>researcher to test hypotheses in reference to interval-level scales.
</p>
<p>inference is to make statements about populations from what is known
</p>
<p>about samples. However, parametric tests require that we make assump-
</p>
<p>tions about the population at the outset. If population parameters are
</p>
<p>generally unknown, how can we make assumptions about them? In this
</p>
<p>chapter, we examine this dilemma in the context of two types of para-
</p>
<p>metric tests that are based on the normal distribution.
</p>
<p>T h e  N o r m a l  F r e q u e n c y  D i s t r i b u t i o n ,  o r  N o r m a l  C u r v e
</p>
<p>In Chapter 3, we noted that frequency distributions may take many dif-
</p>
<p>ferent forms. Sometimes there is no pattern to a distribution of scores.
</p>
<p>This is the case for the example in Figure 10.1, in which the frequency of
</p>
<p>scores goes up and down without consistency. But often a distribution
</p>
<p>begins to take a specific shape. For example, Floyd Allport suggested
</p>
<p>more than half a century ago that the distribution of deviant behavior is
</p>
<p>shaped like a J.1 His J curve, represented in Figure 10.2, fits many types
</p>
<p>1F. H. Allport, &ldquo;The J-Curve Hypothesis of Conforming Behavior,&rdquo; Journal of Social
</p>
<p>Psychology 5 (1934): 141&ndash;183.
</p>
<p>We begin by introducing the normal sampling distribution and its
</p>
<p>application to tests of significance for measures that are normally dis-
</p>
<p>researchers in the use of parametric tests. The purpose of statistical
</p>
<p>tributed in the population. We then turn to a basic dilemma faced by
</p>
<p>235</p>
<p/>
</div>
<div class="page"><p/>
<p>of rule-breaking behavior and suggests a theory of deviance in which so-
</p>
<p>cial control leads most people to conform more or less to societal rules.
</p>
<p>Allport fit a J curve to behaviors as diverse as parking violations, confor-
</p>
<p>mity to religious rituals in church, and stopping at a stop sign.
</p>
<p>The most widely utilized distributional form in statistics is what is de-
</p>
<p>fined as the normal frequency distribution or normal curve. The
</p>
<p>normal distribution is the basis for a number of parametric statistical
</p>
<p>Random Frequency DistributionFigure 10.1
</p>
<p>The J CurveFigure 10.2
</p>
<p>Conformity Deviance
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N236</p>
<p/>
</div>
<div class="page"><p/>
<p>tests. This is the case in good part because of a set of special characteris-
</p>
<p>tics associated with the normal curve.
</p>
<p>Characteristics of the Normal Frequency Distribution
</p>
<p>A normal distribution is always symmetrical and bell shaped. By that we
</p>
<p>mean that it is shaped exactly the same on both sides of its mean. If you
</p>
<p>represent a normal distribution as a curve, you can fold it over at its
</p>
<p>mean and gain two half-curves that are exactly alike. Of course, there
</p>
<p>are many different potential bell-shaped curves that are symmetrical, as
</p>
<p>illustrated in Figure 10.3. The curve in part a of Figure 10.3, for example,
</p>
<p>is fairly flat. What this means is that the scores are fairly widely spread
</p>
<p>around the mean. The curve in part b, in contrast, is very peaked. Here,
</p>
<p>the scores are tightly clustered around the mean. In the statistical lan-
</p>
<p>guage developed in earlier chapters, we can say that the standard devia-
</p>
<p>tion of the first distribution is much larger than that of the second.
</p>
<p>In a true normal distribution, the mean, mode, and median are always
</p>
<p>the same. This can be seen in the normal curves in Figure 10.3. If the
</p>
<p>distribution is completely symmetrical, then the 50th percentile score, or
</p>
<p>the median, must be right in the middle of the distribution. In turn, since
</p>
<p>Two Examples of Normal CurvesFigure 10.3
</p>
<p>(a) Normal Curve with a Large Standard 
Deviation
</p>
<p>(b) Normal Curve with a Small Standard 
Deviation
</p>
<p>T H E N O R M A L F R E Q U E N C Y D I S T R I B U T I O N ,  O R N O R M A L C U R V E 237</p>
<p/>
</div>
<div class="page"><p/>
<p>the middle of the distribution represents its highest peak, and thus the
</p>
<p>largest frequency of scores, it is also the location of the mode for the
</p>
<p>normal distribution. Finally, given that there is an exactly equal distribu-
</p>
<p>tion of scores below and above that peak, the same value must also be
</p>
<p>the mean for a normal distribution.
</p>
<p>All of these traits help to define a normal distribution. However, the
</p>
<p>most useful characteristic of a normal distribution develops from the fact
</p>
<p>that the percentage of cases between its mean and points at a measured
</p>
<p>distance from the mean is always fixed. The measure in this case is the
</p>
<p>standard deviation unit. A standard deviation unit is simply the stan-
</p>
<p>dard deviation for the particular distribution being examined. For exam-
</p>
<p>ple, let&rsquo;s say that you were examining the results of a standardized test
</p>
<p>for assessing adjustment of prisoners and that the distribution obtained
</p>
<p>was a normal distribution. You obtained a mean score of 90 and a stan-
</p>
<p>dard deviation of 10 for your sample. The standard deviation unit of this
</p>
<p>distribution would be 10. That is, if you measured one standard devia-
</p>
<p>tion unit from the mean in either direction, you would move 10 points
</p>
<p>from the mean, to 100 and 80. If you measured two standard deviation
</p>
<p>units from the mean, you would move 20 points, to 110 and 70.
</p>
<p>In a normal distribution, 68.26% of the cases in the distribution are
</p>
<p>found within one standard deviation unit above and below the mean
</p>
<p>(see Figure 10.4). Because the normal distribution is symmetrical, this
</p>
<p>means that 34.13% of the cases lie within one standard deviation unit to
</p>
<p>either the right (positive side) or the left (negative side) of the mean.
</p>
<p>Percentage of Cases Under Portions of the Normal CurveFigure 10.4
</p>
<p>Standard Deviations
</p>
<p>&ndash;4 &ndash;3 &ndash;2 &ndash;1 0 +1 +2 +3 +4
</p>
<p>0.13% 2.14% 13.59% 34.13% 34.13% 13.59% 2.14% 0.13%
</p>
<p>68.26% 
</p>
<p>95.46% 
</p>
<p>σσσσ σ σ σ σ
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N238</p>
<p/>
</div>
<div class="page"><p/>
<p>Fully 95.46% of the cases are found within two standard deviation units
</p>
<p>above and below the mean. Virtually all of the cases in a distribution
</p>
<p>with a normal form are within three standard deviation units of the
</p>
<p>mean, although in theory the tails (or extremes) of this distribution go on
</p>
<p>forever. For the sample of inmates discussed above, we thus know that
</p>
<p>slightly more than two-thirds have adjustment scores of between 80 and
</p>
<p>100 (one standard deviation unit above and below the mean). Very few
</p>
<p>members of the sample have adjustment scores above 120, which repre-
</p>
<p>sents a score that is three standard deviation units from the mean.
</p>
<p>z-Scores
</p>
<p>scores in our sample or population to z-scores, which represent stan-
</p>
<p>dard deviation units for the standard normal distribution. This distribu-
</p>
<p>tion has a mean of 0 and a standard deviation unit of 1. The formula for
</p>
<p>converting a specific score to a z-score is represented by Equation 10.1.
</p>
<p>Equation 10.1
</p>
<p>For this equation, we take the score of interest and subtract from it the
</p>
<p>mean score for the population distribution (represented by �). We then
</p>
<p>divide that result by the standard deviation of the population distribution
</p>
<p>we are examining (represented by �). In practice, what this formula does
</p>
<p>is allow us to convert any specific score in any normal distribution to a
</p>
<p>z-score in a standard normal distribution. We can then use a standard-
</p>
<p>ized table to identify the location of that score. A concrete example will
</p>
<p>make this conversion easier to understand.
</p>
<p>Intelligence quotient (IQ) scores are normally distributed in the U.S.
</p>
<p>population, with a mean of 100 and a standard deviation of about 15.
</p>
<p>Suppose a probation officer is writing a report on a young offender. She
</p>
<p>finds that the young man has an IQ of 124. She wants to give the sen-
</p>
<p>tencing judge a good sense of what this means in terms of how this
</p>
<p>young man compares to others. She can do this by transforming the
</p>
<p>mean IQ of the offender (124) to a z-score and then identifying where
</p>
<p>this z-score fits in the standard normal distribution. We use Equation
</p>
<p>10.1 for this purpose.
</p>
<p>As shown in the numerator of the equation, we subtract the popula-
</p>
<p>tion mean (�) of IQ scores, which we already noted was 100, from the
</p>
<p>score of 124. By doing this we shift the position of our score. We now
</p>
<p>have its location if the mean of our distribution were 0&mdash;the mean of a
</p>
<p>standard normal distribution.
</p>
<p>z � 
Xi � �
</p>
<p>�
</p>
<p>Using a simple equation, we can convert all normal distributions, irrespec-
</p>
<p>normal distribution. This distribution can then be used to identify
</p>
<p>the exact location of any score. We do this by converting the actual
</p>
<p>tive of their particular mean or standard deviation, to a single standard
</p>
<p>T H E N O R M A L F R E Q U E N C Y D I S T R I B U T I O N ,  O R N O R M A L C U R V E 239</p>
<p/>
</div>
<div class="page"><p/>
<p>If the mean were 0, then the score for this offender would be 24 (and
</p>
<p>not 124). As a second step, we divide this result by 15, the standard devi-
</p>
<p>ation (�) of IQ scores in the U.S. population. This is equivalent to con-
</p>
<p>verting our sample standard deviation unit to 1, the standard deviation of
</p>
<p>the standard normal distribution, since each score of 15 is equivalent to
</p>
<p>one z standard deviation unit. The result is 1.60.
</p>
<p>Our final step is to compare this z-score to an already prepared table
</p>
<p>of the standard normal distribution, provided in Appendix 3. You will
</p>
<p>notice that the z table goes up to only 0.50. This is because it provides
</p>
<p>us with only half of the normal curve. On this half of the normal curve,
</p>
<p>W orking It Out
</p>
<p> � 1.60
</p>
<p> � 
124 � 100
</p>
<p>15
</p>
<p> z � 
Xi � �
</p>
<p>�
</p>
<p>IQ Score of Young Prisoner Compared to Average IQ Score of the General PopulationFigure 10.5
</p>
<p>z = 1.60
score for prisoners
</p>
<p>94.52% 5.48% 
44.52% 
</p>
<p>&micro;
</p>
<p>our z-score of 1.60 is equivalent to 0.4452, meaning that 44.52% of
</p>
<p>the scores lie between 0 and �1.60 standard deviations from 0. In
</p>
<p>Figure 10.5, our result is illustrated in the context of the normal curve.
</p>
<p>Because our result is a positive score, we place the value on the right-
</p>
<p>hand side of the normal distribution.
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N240</p>
<p/>
</div>
<div class="page"><p/>
<p>To identify the percentage of people in the general population with
</p>
<p>IQ scores higher than that of the young offender, we subtract our result
</p>
<p>of 0.4452 from 0.50 (the proportion of cases in this half of the curve).
</p>
<p>Our result of 0.0548 means that only a bit more than 5% of the general
</p>
<p>population has higher IQ scores than this offender. Conversely, almost
</p>
<p>95% of the population has lower IQ scores than this offender. By con-
</p>
<p>verting the offender&rsquo;s score to a score on the standard normal distribu-
</p>
<p>tion, we are able to place his intelligence in context. From our finding,
</p>
<p>we can see that he is indeed a highly intelligent young man, based on
</p>
<p>his IQ score.
</p>
<p>Developing Tests of Statistical Significance Based on the Standard Normal
</p>
<p>Distribution: The Single-Sample z-Test for Known Populations
</p>
<p>The normal distribution can also be used as a sampling distribution.
</p>
<p>population characteristics for all Americans are, as discussed above,
</p>
<p>known. The mean score for the population is 100, and the standard devi-
</p>
<p>ation of the population mean is 15. You conduct a study of 125 prisoners
</p>
<p>selected through an independent random sampling procedure from the
</p>
<p>population of American prisoners. You find that the mean IQ in your
</p>
<p>sample is 90.2 This mean is different from the mean of the American
</p>
<p>population. But we know that samples vary, and thus you might get a
</p>
<p>mean of 90 even if the mean for American prisoners were the same as
</p>
<p>that for the general population. What we want to know is how likely we
</p>
<p>are to get such an outcome in our sample if the distribution of American
</p>
<p>prisoners is the same as that of the general American population.3 Be-
</p>
<p>2Our hypothesized results mirror those found in prior studies; see R. J. Hernstein,
</p>
<p>&ldquo;Some Criminogenic Traits of Offenders,&rdquo; in J. Q. Wilson (ed.), Crime and Public Pol-
</p>
<p>icy (San Francisco: Institute for Contemporary Studies, 1983). Whether these differ-
</p>
<p>ences mean that offenders are, on average, less intelligent than nonoffenders is an
</p>
<p>issue of some controversy in criminology, in part because of the relationship of IQ to
</p>
<p>other factors, such as education and social status.
3By implication, we are asking whether it is reasonable to believe that our sample of
</p>
<p>prisoners was drawn from the general population. For this reason, the z-test can also
</p>
<p>be used to test for random sampling. If you have reason to doubt the sampling meth-
</p>
<p>ods of a study, you can conduct this test, comparing the observed characteristics of
</p>
<p>your sample with the known parameters of the population from which your sample
</p>
<p>was drawn.
</p>
<p>distribution of sample means will also be distributed normally. This means 
</p>
<p>When a population of scores is distributed normally, the sampling
</p>
<p>in practice that we can use the normal distribution as our sampling
</p>
<p>This is the case for the IQ test, so we will continue to use it as an example.
</p>
<p>Let&rsquo;s say that you were interested in whether American prisoners
</p>
<p>differ from Americans generally in terms of average IQ scores. The
</p>
<p>that the variable of interest is normally distributed in the population.
</p>
<p>distribution for a test of statistical significance if we know at the outset 
</p>
<p>T H E N O R M A L F R E Q U E N C Y D I S T R I B U T I O N ,  O R N O R M A L C U R V E 241</p>
<p/>
</div>
<div class="page"><p/>
<p>cause the population parameters of the American population are known,
</p>
<p>a single-sample z-test for known populations is appropriate.
</p>
<p>We set up our test of statistical significance the same way we did
</p>
<p>other tests in previous chapters.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution.
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: The American prison population.
</p>
<p>Hypotheses:
</p>
<p>H0: The mean IQ of the population from which our sample of prisoners
</p>
<p>was drawn is the same as the mean IQ of the general population of
</p>
<p>Americans (� � 100).
</p>
<p>H1: The mean IQ of the population from which our sample of prisoners
</p>
<p>was drawn is not the same as the mean IQ of the general population of
</p>
<p>Americans (� � 100).
</p>
<p>As required by the single-sample z-test for known populations, IQ
</p>
<p>scores are measured at an interval level. As already noted, IQ is also nor-
</p>
<p>mally distributed in the general population, meaning that it is appropri-
</p>
<p>ate to use a normal sampling distribution to conduct our test of statistical
</p>
<p>significance. Our sample, as required by our test, is drawn randomly
</p>
<p>pothesis is that the mean IQ of American prisoners is the same as the
</p>
<p>mean IQ of the general American population (� � 100). Our research
</p>
<p>hypothesis is that the mean IQ of prisoners is different from that of the
</p>
<p>The Sampling Distribution
</p>
<p>population distribution and apply it to the sampling distribution for our
</p>
<p>test, because the standard deviation of the sampling distribution is influ-
</p>
<p>enced by the number of observations in a sample. This is illustrated in
</p>
<p>Figure 10.6, which presents three different sampling distributions for the
</p>
<p>same population distribution of scores. In the first, there are only 10
</p>
<p>cases in the samples from which the sampling distribution is developed.
</p>
<p>In the second, there are 25 cases in each sample. Finally, in the third
</p>
<p>that the mean of the sampling distribution is the same as the mean of the
</p>
<p>However, we cannot simply take the standard deviation of scores for the
</p>
<p>ledge of the standard deviation of the population distribution of scores. 
</p>
<p>The mean of the sampling distribution we use
</p>
<p>average American (� � 100).
</p>
<p>with replacement from the American prison population. Our null hy-
</p>
<p>for our test of statistical inference is defined, as in other tests, by our null
</p>
<p>hypothesis. In statistical tests using the normal distribution we can assume
</p>
<p>population distribution. In this case, it is 100, or the mean IQ for the
American population. The standard deviation is drawn from our know-
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N242</p>
<p/>
</div>
<div class="page"><p/>
<p>that the spread of scores is reduced as the size of samples in the distrib-
</p>
<p>ution increases. This fact is a very important one in statistics and follows
</p>
<p>samples increases. Put in lay terms, larger samples are more trustworthy
</p>
<p>or more likely to reflect the true population score, all else being equal,
</p>
<p>than are smaller samples.
</p>
<p>In order to differentiate between the standard deviation of a popula-
</p>
<p>tion distribution of scores and that of a sampling distribution, statisticians
</p>
<p>call the standard deviation of a sampling distribution the standard
</p>
<p>error. Using Equation 10.2, we adjust our standard error for the fact that
</p>
<p>the dispersion of sample means decreases as sample size increases. In
</p>
<p>order to distinguish the standard deviation (�) from the standard error in
</p>
<p>this text, we will use the subscripts sd (for sampling distribution) when-
</p>
<p>ever we refer to the standard error of a sampling distribution. Accord-
</p>
<p>ingly, the standard error of a sampling distribution is represented as �sd
in Equation 10.2.
</p>
<p>Equation 10.2
</p>
<p>For our example, we find the standard error of the sampling distribu-
</p>
<p>tion by dividing the population standard deviation of IQ, 15, by the
</p>
<p>square root of our sample N. The result is 1.342.
</p>
<p>Standard error � �sd � 
�
</p>
<p>�N
</p>
<p>Normal Distribution of Scores from Samples of Varying Sizes: N � 10, N � 25, and N � 100Figure 10.6
</p>
<p>distribution, there are 100 cases in each sample. What is clear here is
</p>
<p>what our common sense tells us: Our sampling distribution becomes
</p>
<p>more tightly clustered around the mean as N increases. This implies,
</p>
<p>in practice, that we are less likely to draw deviant samples (those far
</p>
<p>from the mean of the sampling distribution) as the N of cases in our
</p>
<p>T H E N O R M A L F R E Q U E N C Y D I S T R I B U T I O N ,  O R N O R M A L C U R V E 243</p>
<p/>
</div>
<div class="page"><p/>
<p>Significance Level and Rejection Region Given that no special concerns
</p>
<p>have been stated in regard to the risk of either a Type I or a Type II
</p>
<p>error, we use a conventional 0.05 significance threshold. As our research
</p>
<p>hypothesis is nondirectional, we use a two-tailed test. What this means
</p>
<p>for our rejection region is illustrated in Figure 10.7. On the right-hand
</p>
<p>side of the distribution are outcomes greater than the average American
</p>
<p>IQ of 100. On the left-hand side of the distribution are outcomes less
</p>
<p>than the average. Because our research hypothesis is not directional, we
</p>
<p>split our total rejection region of 5% between both tails of the distribu-
</p>
<p>tion. This is represented by the shaded area. Each shaded area repre-
</p>
<p>sents half the total rejection region, or 0.025.
</p>
<p>W orking It Out
</p>
<p> � 1.342
</p>
<p> � 
15
</p>
<p>�125
</p>
<p> Standard error � 
�
</p>
<p>�N
</p>
<p>Rejection Region on a Normal Frequency Distribution for a 0.05 Two-Tailed 
</p>
<p>Significance Test
Figure 10.7
</p>
<p>z = &ndash;1.96
</p>
<p>Total Rejection Region,     = 0.025 + 0.025 = 0.05
</p>
<p>Rejection
Region
    = 0.025
</p>
<p>Rejection
Region
    = 0.025
</p>
<p>z = +1.96
</p>
<p>Outcomes Supporting
Lower IQs for Prisoners
</p>
<p>Outcomes Supporting
Higher IQs for Prisoners
</p>
<p>&micro;
</p>
<p>α
</p>
<p>α 
</p>
<p>α 
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N244</p>
<p/>
</div>
<div class="page"><p/>
<p>To define the z-score that corresponds with our rejection region,
</p>
<p>we must turn to the table of probability values associated with the z
</p>
<p>distribution in Appendix 3. As discussed earlier in the chapter, the z
</p>
<p>table represents only half of the normal curve. We look at the value
</p>
<p>associated with 0.4750 (0.5000 � 0.0250) in the table, which is 1.96. If
</p>
<p>we observe a test statistic either greater than 1.96 or less than �1.96,
</p>
<p>we will reject the null hypothesis of our test (see Figure 10.7). In this
</p>
<p>case, our observed significance level would be less than the 0.05 crite-
</p>
<p>rion for our test.
</p>
<p>If we had stated a directional research hypothesis, we would place
</p>
<p>the entire rejection region (� � 0.05) in one of the two tails of the nor-
</p>
<p>mal distribution. In this case, we would conduct a one-tailed statistical
</p>
<p>test. Parts a and b of Figure 10.8 represent the rejection regions for two
</p>
<p>different one-tailed tests of statistical significance. If our research hypoth-
</p>
<p>esis stated that average IQs for prisoners were less than those for the
</p>
<p>U.S. population, we would place the entire rejection region of 0.0500 in
</p>
<p>the left tail of the distribution (see Figure 10.8a). We again consult the z
</p>
<p>table in Appendix 3 to identify the z-score associated with a value of
</p>
<p>0.4500 (0.5000 � 0.0500). We observe that 0.4500 falls exactly halfway
</p>
<p>between two values in the table &mdash;0.4495 and 0.4505&mdash;corresponding to
</p>
<p>z-scores of �1.64 and �1.65, respectively. How do we determine the
</p>
<p>value of z in such a case? The most accurate value for z would be found
</p>
<p>by interpolating between �1.64 and �1.65, which would give �1.645,
</p>
<p>The Test Statistic To calculate our test statistic, we can use the same for-
</p>
<p>mula we did in examining the relative position of a score in the standard
</p>
<p>normal distribution, with two important differences. In this case, we
</p>
<p>have to take into account the fact that sampling distributions become
</p>
<p>more tightly spread around their mean as the N of sample cases becomes
</p>
<p>larger. As discussed in defining the sampling distribution above, we need
</p>
<p>to adjust the standard deviation of the population distribution by divid-
</p>
<p>ing it by the square root of the N of our sample. This provides us with
</p>
<p>the standard error (�sd) for our distribution. We also need to subtract the
</p>
<p>average IQ for prisoners is less than the U.S. average. If our research
</p>
<p>than �1.645,  then we reject our null hypothesis and conclude that the
</p>
<p>portions reported in the z table. In this case, if our test statistic is less 
</p>
<p>since the value we are looking for is halfway between the two pro-
</p>
<p>hypothesis stated that the average IQ of prisoners was greater than the
</p>
<p>U.S. average, we would place the rejection region on the right side
</p>
<p>of the distribution (see Figure 10.8b). In such a case, our critical value
</p>
<p>would be �1.645, meaning that if our test statistic was greater than
</p>
<p>1.645, we would reject the null hypothesis and conclude that the ave-
</p>
<p>rage IQ for prisoners was greater than the U.S. average.
</p>
<p>T H E N O R M A L F R E Q U E N C Y D I S T R I B U T I O N ,  O R N O R M A L C U R V E 245</p>
<p/>
</div>
<div class="page"><p/>
<p>Rejection Region on a Normal Frequency Distribution for a 0.05 One-Tailed Significance TestFigure 10.8
</p>
<p>z = &ndash;1.645 &micro;
</p>
<p>Rejection
Region
   = 0.05
</p>
<p>Outcomes Supporting
Lower IQs for Prisoners
</p>
<p>Outcomes Supporting
Higher IQs for Prisoners
</p>
<p>α 
</p>
<p>Rejection
Region
    = 0.05
</p>
<p>z = +1.645
</p>
<p>Outcomes Supporting
Lower IQs for Prisoners
</p>
<p>Outcomes Supporting
Higher IQs for Prisoners
</p>
<p>&micro;
</p>
<p>α 
</p>
<p>(a) Lower IQs for Prisoners
</p>
<p>(b) Higher IQs for Prisoners
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N2 64</p>
<p/>
</div>
<div class="page"><p/>
<p>mean (�) of the population score from , rather than Xi. These adjust-
</p>
<p>ments are made in Equation 10.3.
</p>
<p>Equation 10.3
</p>
<p>Inserting into our equation the mean value of our sample and its N of
</p>
<p>cases and the mean and standard deviation for the population of scores,
</p>
<p>we obtain a z-test statistic of �7.453.
</p>
<p>z � 
X � �
</p>
<p>�sd
 � 
</p>
<p>X � �
</p>
<p>�/�N
</p>
<p>X
</p>
<p>W orking It Out
</p>
<p> � �7.453
</p>
<p> � 
90 � 100
</p>
<p>15/�125
</p>
<p> z � 
X � �
</p>
<p>�/�N
</p>
<p>The Decision Because our test statistic is less than our negative critical
</p>
<p>value (�7.453 � �1.96) and falls in the rejection region, we reject the
</p>
<p>null hypothesis. We conclude on the basis of our study that the mean IQ
</p>
<p>of the population from which our sample was drawn is different from
</p>
<p>that of the general American population.
</p>
<p>A p p l y i n g  N o r m a l  S a m p l i n g  
D i s t r i b u t i o n s  t o  N o n n o r m a l  P o p u l a t i o n s
</p>
<p>The example of IQ presents a case where the single-sample z-test can be
</p>
<p>used to test hypotheses involving interval-scale measures. However, it re-
</p>
<p>quires that the population distribution for the measure be normal. In
</p>
<p>some fields in the social sciences, measures are constructed in such a way
</p>
<p>that they are normally distributed in practice.4 But in criminology, there
</p>
<p>4In principle, any distribution may be arranged in such a way that it conforms to a
</p>
<p>normal shape. This can be done simply by ranking scores and then placing the appro-
</p>
<p>priate number within standard deviation units appropriate for constructing a standard
</p>
<p>normal distribution.
</p>
<p>A P P L Y I N G N O R M A L S A M P L I N G D I S T R I B U T I O N S 247</p>
<p/>
</div>
<div class="page"><p/>
<p>has been much less use of distributions that are standardized in normal
</p>
<p>form, in part because the distributions of the behaviors and populations
</p>
<p>that we confront do not often conform to the shape of a normal curve.
</p>
<p>Even measures that do begin to approximate the shape of the normal dis-
</p>
<p>tribution seldom meet all the requirements of a true normal distribution.
</p>
<p>How, then, can parametric tests based on the normal distribution be
</p>
<p>widely used to make statistical inferences? Not only do they demand that
</p>
<p>we make an assumption about a population we usually know little
</p>
<p>about, but the assumption we are being asked to make does not make
</p>
<p>very much sense for criminal justice measures. The answer may be
</p>
<p>found in an important distinction between population distributions on
</p>
<p>the one hand and sampling distributions on the other. While we have
</p>
<p>every reason to be hesitant in assuming that the population distribution
</p>
<p>of scores is normal for criminal justice measures, we can assume with a
</p>
<p>good deal of confidence that the sampling distributions for such mea-
</p>
<p>sures are approximately normal. Using the toss of a fair coin as an exam-
</p>
<p>ple, we can provide a simple illustration of this fact.
</p>
<p>In Figure 10.9, we overlay the distribution of scores for a population
</p>
<p>of 1,000 tosses of a fair coin over the normal distribution. As is apparent,
</p>
<p>outcomes in a coin toss are not distributed normally. This makes good
</p>
<p>sense, since there are only two possible scores for the coin toss: heads
</p>
<p>and tails. No matter what the outcome, it is impossible for a coin toss to
</p>
<p>approximate the form of the normal distribution.
</p>
<p>But let&rsquo;s now turn to a sampling distribution for the coin toss. In
</p>
<p>this case, we want to know the likelihood of gaining a specific num-
</p>
<p>ber of heads in a set number of coin tosses. This is the logic we used
</p>
<p>Distribution of 1,000 Tosses of a Fair Coin Contrasted to the Normal DistributionFigure 10.9
</p>
<p>Heads Tails
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Population Distribution
of 1,000 Tosses of a
Fair Coin
</p>
<p>Normal Distribution
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N248</p>
<p/>
</div>
<div class="page"><p/>
<p>in developing the binomial probability distribution in Chapter 7. Fig-
</p>
<p>ure 10.10 presents the binomial distribution for different-size samples
</p>
<p>of the coin toss under the null hypothesis that the coin is fair.
</p>
<p>For a sample size of 1 (Figure 10.10a), the shape of the sampling dis-
</p>
<p>tribution is the same as the shape of the population distribution of
</p>
<p>scores. However, notice what happens as the size of the samples used to
</p>
<p>construct the sampling distributions grows. For a sample of 10 (Figure
</p>
<p>10.10b), the histogram for the distribution of scores is still jagged, but it
</p>
<p>has begun to take a shape similar to the normal distribution. Importantly,
</p>
<p>for a sample of 10, we do not have two potential outcomes, which
</p>
<p>Sampling Distribution of Coin TossesFigure 10.10
</p>
<p>Number of Heads
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Normal Curve
</p>
<p>Number of Heads
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Normal Curve
</p>
<p>(a) 1 Toss of a Fair Coin
</p>
<p>(b) 10 Tosses of a Fair Coin
</p>
<p>A P P L Y I N G N O R M A L S A M P L I N G D I S T R I B U T I O N S 249</p>
<p/>
</div>
<div class="page"><p/>
<p>would make a normal shape impossible, but 11 potential outcomes (no
</p>
<p>heads, one head, two heads, three heads, four heads, . . . to ten heads).
</p>
<p>This is the case because we are flipping the coin ten times for each sam-
</p>
<p>ple. The sampling distribution is telling us the number of times we
</p>
<p>would expect to gain a specific number of heads in ten tosses of a fair
</p>
<p>coin in a very large number of trials. For a sample of 100 flips of a fair
</p>
<p>coin (Figure 10.10c), the sampling distribution even more closely ap-
</p>
<p>proximates the normal curve. By the time we get to a sample of 400 flips
</p>
<p>of the coin (Figure 10.10d), the sampling distribution of a fair coin is al-
</p>
<p>most indistinguishable from the normal curve.
</p>
<p>Number of Heads
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Normal Curve
</p>
<p>Number of Heads
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Normal Curve
</p>
<p>(c) 100 Tosses of a Fair Coin
</p>
<p>(d) 400 Tosses of a Fair Coin
</p>
<p>Sampling Distribution of Coin Tosses (cont.)Figure 10.10
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N250</p>
<p/>
</div>
<div class="page"><p/>
<p>The population distribution of scores for a fair coin is very far from a
</p>
<p>normal form. Yet sampling distributions for the same population begin
</p>
<p>to approximate the normal distribution as the size of the sample of coin
</p>
<p>tosses grows. This remarkable fact is summarized in a very important
</p>
<p>theorem, or statement, about sampling distributions called the central
</p>
<p>limit theorem. The central limit theorem allows us to overcome our ini-
</p>
<p>tial dilemma because it says that under many circumstances we can use
</p>
<p>a normal sampling distribution for making inferences about a population
</p>
<p>that is not normal in shape.
</p>
<p>Central Limit Theorem
</p>
<p>If repeated independent random samples of size N are drawn from a
</p>
<p>population, then as N grows large, the sampling distribution of sample
</p>
<p>means will be approximately normal.
</p>
<p>The central limit theorem tells us that when the number of cases in a
</p>
<p>sample is large, we can assume that the sampling distribution of sample
</p>
<p>means is approximately normal even if the population distribution itself
</p>
<p>is not normal. This is what is meant by the statement &ldquo;then as N grows
</p>
<p>large, the sampling distribution of sample means will be approximately
</p>
<p>normal.&rdquo; However, the theorem does not provide us with a clear state-
</p>
<p>ment about how large the number of cases in a sample must be before
</p>
<p>we can make this assumption.
</p>
<p>One reason for this ambiguity is that the number of cases needed be-
</p>
<p>fore the sampling distribution begins to approximate normality depends
</p>
<p>in part on the actual distribution of the measure examined in the popula-
</p>
<p>tion. As can be seen from the example of the coin toss, even when the
</p>
<p>population distribution departs markedly from the normal distribution,
</p>
<p>the sampling distribution fits fairly closely to the normal curve with a
</p>
<p>sample size of 100. For this reason, you will find wide agreement that a
</p>
<p>normal sampling distribution can be assumed for samples of 100 or
</p>
<p>more, irrespective of the distribution of scores in a population.
</p>
<p>There is much less agreement about what to do when a sample is
</p>
<p>smaller than 100 cases. Some statisticians argue that with 50 cases you
</p>
<p>can be fairly confident that the central limit theorem applies in most cir-
</p>
<p>cumstances. Others apply this yardstick to 25 or 30 cases, and still others
</p>
<p>argue that under certain circumstances&mdash;for example, when prior studies
</p>
<p>suggest a population distribution fairly close to normality&mdash;only 15 cases
</p>
<p>is enough. In conducting research in criminal justice, you should recog-
</p>
<p>nize that there is no hard and fast rule regarding sample size and the
</p>
<p>central limit theorem. In practice, in criminal justice, researchers gener-
</p>
<p>ally assume that 30 cases is enough for applying the central limit theo-
</p>
<p>rem. However, when a distribution strongly departs from normality, as is
</p>
<p>the case with a proportion, it is safer to require more than 100 cases.
</p>
<p>A P P L Y I N G N O R M A L S A M P L I N G D I S T R I B U T I O N S 251</p>
<p/>
</div>
<div class="page"><p/>
<p>While the central limit theorem solves a major problem in applying
</p>
<p>normal distribution tests to criminological questions, we are still faced
</p>
<p>with a barrier in actually carrying out such tests. As we saw earlier (see
</p>
<p>Equation 10.3), the standard error of the z sampling distribution is gained
</p>
<p>from knowledge about the standard deviation of the population distribu-
</p>
<p>tion. How can we identify the standard error of a sampling distribution if
</p>
<p>we do not know the standard deviation of the population distribution? In
</p>
<p>the following sections, we illustrate two methods for defining � for an
</p>
<p>unknown population. In the first, we take advantage of a special rela-
</p>
<p>tionship between the mean and the standard deviation of a proportion.
</p>
<p>In the second, we estimate the unknown parameter based on informa-
</p>
<p>tion gained in our sample.
</p>
<p>C o m p a r i n g  a  S a m p l e  t o  a n  U n k n o w n  P o p u l a t i o n :  
T h e  S i n g l e - S a m p l e  z - T e s t  f o r  P r o p o r t i o n s
</p>
<p>One implication of the central limit theorem is that we can use a normal
</p>
<p>begins to approximate a normal distribution when the number of cases
</p>
<p>for the sample becomes large. The central tendency of this distribution
</p>
<p>and its dispersion are measured by the mean and standard error, just as in
</p>
<p>distributions that develop from interval-level data. Accordingly, although
</p>
<p>it would be inappropriate to use the mean and standard deviation to de-
</p>
<p>scribe a sample or population distribution of a proportion, the mean and
</p>
<p>standard error are appropriate statistics for describing the normal sam-
</p>
<p>pling distribution that is associated with the same proportion.
</p>
<p>Computing the Mean and Standard Deviation 
</p>
<p>for the Sampling Distribution of a Proportion
</p>
<p>How do we compute the mean and standard deviation of a proportion?
</p>
<p>One way to do this would be to simply apply the formula for the mean
</p>
<p>and the standard deviation to the scores associated with a proportion.
</p>
<p>However, there is a simpler way to arrive at the same result. It turns out
</p>
<p>that the mean of a proportion is equal to the proportion itself. This is il-
</p>
<p>lustrated in Table 10.1, which shows an example in which the mean and
</p>
<p>proportion are calculated for five heads in ten tosses of a coin.
</p>
<p>sampling distribution to test hypotheses involving proportions. This might 
</p>
<p>seem strange at first, since we estimate the shape of a normal distribu-
</p>
<p>tion through knowledge of its mean and standard deviation. As discus-
</p>
<p>sed in Chapter 4, the mean and standard deviation are not appropriate
</p>
<p>statistics to use with a nominal-level measure such as a proportion.
</p>
<p>bution of a proportion&mdash;in our example, the coin toss (see Figure 10.10)&mdash;
</p>
<p>Nonetheless, as illustrated in the previous section, the sampling distri-
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N252</p>
<p/>
</div>
<div class="page"><p/>
<p>For the numerator of the mean, we sum the scores on the ten trials
</p>
<p>(five ones and five zeros) and get 5. The numerator of a proportion is
</p>
<p>the N of cases in the category of interest. If the category is heads, then
</p>
<p>we also get a result of 5. The denominators for both equations are the
</p>
<p>same (10), and thus the outcomes are also the same. As a general rule,
</p>
<p>we state that for a proportion � � P.
</p>
<p>What about the standard deviation of a proportion? It turns out that
</p>
<p>we can calculate the standard deviation with knowledge of only the pro-
</p>
<p>portion itself. This is illustrated in Table 10.2.
</p>
<p>Taking the sum of the squared deviations from the mean and divid-
</p>
<p>ing it by N, we get a result of 0.25. But we can get this same result by
</p>
<p>multiplying the proportion of heads (P) by the proportion of tails 
</p>
<p>(Q)&mdash;in our case, multiplying 0.5 by 0.5. Accordingly, we can substitute
</p>
<p>P &bull; Q for
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2
</p>
<p>N
</p>
<p>Calculating the Mean and Proportion 
of 5 Heads in 10 Tosses of a Coin
</p>
<p>CALCULATING THE PROPORTION 
</p>
<p>CALCULATING THE MEAN FOR FIVE HEADS FOR FIVE HEADS
</p>
<p>Proportion � 
Nsuccesses
</p>
<p>Ntotal
 � 
</p>
<p>5
</p>
<p>10
 � 0.5X � 
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N
 � 
</p>
<p>1 � 1 � 1 � 1 � 1 � 0 � 0 � 0 � 0 � 0
</p>
<p>10
 � 0.5
</p>
<p>Table 10.1
</p>
<p>Calculating the Standard Deviation of 5 Heads in 10 Tosses of a Coin
</p>
<p>CALCULATING THE STANDARD DEVIATION CALCULATING THE STANDARD 
</p>
<p>FROM THE RAW SCORES DEVIATION FROM P AND Q
</p>
<p> � �0.25  � 0.5
</p>
<p> � �0.25 � 0.25 � 0.25 � 0.25 �0.25 � 0.25 �0.25 � 0.25 �0.25 � 0.2510
</p>
<p>� � �PQ � �(0.5)(0.5) � �0.25 � 0.5 � � ��
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2
</p>
<p>N
</p>
<p>Table 10.2
</p>
<p>Note: head � 1� tail � 0
</p>
<p>T H E S I N G L E - S A M P L E z - T E S T F O R P R O P O R T I O N S 253</p>
<p/>
</div>
<div class="page"><p/>
<p>in the equation for the standard deviation for the mean:
</p>
<p>Equation 10.4
</p>
<p>Because of this relationship between the mean and the standard devi-
</p>
<p>ation of a proportion, when we state the proportion of successes ex-
</p>
<p>pected under the null hypothesis, we also state by implication the mean
</p>
<p>and the standard deviation for the population distribution of scores. So if
</p>
<p>we state in the null hypothesis that the proportion of successes in the
</p>
<p>population is 0.50, we know that the mean of the population distribution
</p>
<p>of scores for our test of the null hypothesis is 0.50 and its standard devia-
</p>
<p>tion is .
</p>
<p>What this means in practice is that we need not have any a priori
</p>
<p>knowledge of the shape of the population distribution to construct a
</p>
<p>sampling distribution for our test of proportions. With a large N, we can
</p>
<p>assume a normal sampling distribution, irrespective of the actual form of
</p>
<p>the population distribution. Through our null hypothesis, we can define
</p>
<p>both the mean and the standard deviation of the population distribution
</p>
<p>for our test. We are now ready to use the normal distribution to test hy-
</p>
<p>potheses about unknown population parameters.
</p>
<p>Testing Hypotheses with the Normal Distribution: 
</p>
<p>The Case of a New Prison Program
</p>
<p>Suppose that you were asked to evaluate a new prison education pro-
</p>
<p>ported by the foundation. Managers of the program claim that the suc-
</p>
<p>cess rate is actually much greater than the criteria set by the foundation.
</p>
<p>However, a recent newspaper expos&eacute; claims that the success rate of the
</p>
<p>program is actually much below 75%. You are able to collect information
</p>
<p>on 150 prisoners, selected using independent random sampling. You
</p>
<p>find that 85% of your sample successfully completed the course. What
</p>
<p>conclusions can you make, based on your sample results, about the
</p>
<p>claims of managers and the newspaper expos&eacute;?
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale (program success is measured as a
</p>
<p>proportion).
</p>
<p>Population Distribution: Normal distribution (relaxed because N is
</p>
<p>large).
</p>
<p>0.25 (� � �PQ � �(0.50)(0.50) � �0.25 � 0.50)
</p>
<p>� � ��
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2
</p>
<p>N
 � �PQ
</p>
<p>gram. The foundation sponsoring the effort sought to achieve a pro-
</p>
<p>gram success rate of 75% among the 100,000 prisoners enrolled in the
</p>
<p>program. Success was defined as completion of a six-month course sup-
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N254</p>
<p/>
</div>
<div class="page"><p/>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: 100,000 prisoners in the program.
</p>
<p>Hypotheses:
</p>
<p>H0: The success rate of the program is 0.75 (P � 0.75).
</p>
<p>H1: The success rate of the program is not 0.75 (P � 0.75).
</p>
<p>Because the number of cases in our sample is greater than the thresh-
</p>
<p>old of 100 suggested for invoking the central limit theorem in the case of
</p>
<p>a proportion, we can ignore&mdash;or, in statistical terms, relax&mdash;assump-
</p>
<p>tions regarding the shape of the population distribution. In the special
</p>
<p>case of a proportion, we can also relax the assumption of an interval
</p>
<p>scale of measurement.5 Our sample, as assumed by our test, is drawn
</p>
<p>randomly with replacement from the sampling frame of 100,000 prison-
</p>
<p>ers in the program.
</p>
<p>Our research hypothesis is nondirectional. Managers of the program
</p>
<p>claim that the program has a success rate of greater than 0.75 (P � 0.75).
</p>
<p>The newspaper expos&eacute; claims that the success rate is much below 75%
</p>
<p>(P � 0.75). Accordingly, we want to be able to examine both of these
</p>
<p>potential outcomes in our test. The null hypothesis is that the rate of suc-
</p>
<p>cess for the program is 0.75 (P � 0.75).
</p>
<p>The Sampling Distribution In calculating the mean and standard devia-
</p>
<p>tion or standard error for our sampling distribution, we rely on our null
</p>
<p>hypothesis. Our null hypothesis states that the proportion of successes in
</p>
<p>the population is 75%. This means that the mean of the sampling distrib-
</p>
<p>ution is also 0.75. We can calculate the standard error of the sampling
</p>
<p>5It would not make sense, however, to use a normal distribution test for nominal-scale
</p>
<p>measures with more than two categories. The normal distribution assumes scores
</p>
<p>above and below a mean. The sampling distribution of a proportion follows this
</p>
<p>pattern because it includes only two potential outcomes, which then are associated
</p>
<p>with each tail of the distribution. In a multicategory nominal-scale measure, we have
</p>
<p>more than two outcomes and thus cannot fit each outcome to a tail of the normal
</p>
<p>curve. Because the order of these outcomes is not defined, we also cannot place them
</p>
<p>on a continuum within the normal distribution. This latter possibility would suggest
</p>
<p>that the normal distribution could be applied to ordinal-level measures. However, be-
</p>
<p>cause we do not assume a constant unit of measurement between ordinal categories,
</p>
<p>the normal distribution is often considered inappropriate for hypothesis testing with
</p>
<p>ordinal scales. In the case of a proportion, there is a constant unit of measurement
</p>
<p>between scores simply because there are only two possible outcomes (e.g., success
</p>
<p>and failure).
</p>
<p>T H E S I N G L E - S A M P L E z - T E S T F O R P R O P O R T I O N S 255</p>
<p/>
</div>
<div class="page"><p/>
<p>distribution by adjusting Equation 10.2 to the case of a proportion, as il-
</p>
<p>lustrated in Equation 10.5:
</p>
<p>Equation 10.5
</p>
<p>Applying this equation to our problem, we obtain a standard error of
</p>
<p>0.035 for the normal sampling distribution associated with our null
</p>
<p>hypothesis:
</p>
<p>�sd � 
�
</p>
<p>�N
 � 
</p>
<p>�PQ
</p>
<p>�N
 � �PQN
</p>
<p>W orking It Out
</p>
<p> � 0.0353
</p>
<p> � 
0.433
12.25
</p>
<p> � 
�0.1875
</p>
<p>�150
</p>
<p> � �(0.75)(0.25)150
</p>
<p> �sd � �PQN
</p>
<p>In order to test our hypothesis, we will convert this sampling distribu-
</p>
<p>tion, with mean 0.75 and standard error 0.035, to the standard normal
</p>
<p>distribution (or z), which has a mean of 0 and a standard deviation or
</p>
<p>standard error of 1. This calculation is done when we calculate the test
</p>
<p>statistic below.
</p>
<p>Significance Level and Rejection Region Given that no special concerns
</p>
<p>have been stated in regard to the risk of either a Type I or a Type II
</p>
<p>error, we use a conventional 0.05 significance threshold. As our research
</p>
<p>hypothesis is nondirectional, we use a two-tailed test. As our level of sig-
</p>
<p>nificance is the same as in our previous problem, we follow the same
</p>
<p>procedure and arrive at a critical value of 1.96. If we observe a test statis-
</p>
<p>tic either greater than 1.96 or less than �1.96, we will reject the null hy-
</p>
<p>pothesis of our test.
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N256</p>
<p/>
</div>
<div class="page"><p/>
<p>The Test Statistic We can rely on the same formula used in the single-
</p>
<p>sample z-test for known populations, presented earlier in Equation 10.3.
</p>
<p>However, in Equation 10.6, we express the formula with proportions
</p>
<p>rather than means:
</p>
<p>Equation 10.6
</p>
<p>The mean of the sample (p) is 0.85, since this is the outcome of the
</p>
<p>study. The mean of the sampling distribution P (0.75) is taken from our
</p>
<p>null hypothesis. The standard error of the sampling distribution (�sd) was
</p>
<p>calculated earlier based on our null hypothesis that the proportion of
</p>
<p>successes was 0.75. Our result is a z-score of 2.833.
</p>
<p>z � 
X � �
</p>
<p>�sd
 � 
</p>
<p>X � �
</p>
<p>�/�N
 � 
</p>
<p>p � P
</p>
<p>�PQ/N
</p>
<p>W orking It Out
</p>
<p> � 2.8329
</p>
<p> � 
0.10
</p>
<p>0.0353
</p>
<p> � 
0.85 � 0.75
</p>
<p>�(0.75)(0.25)/150
</p>
<p> z �
p � P
</p>
<p>The Decision Our test statistic is well within the rejection region of our
</p>
<p>test (which includes scores greater than 1.96 or less than �1.96), meaning
</p>
<p>that our observed significance level is less than the significance level we
</p>
<p>set for our test at the outset (p � 0.05). We therefore reject the null hy-
</p>
<p>pothesis at a 0.05 significance level. We come out on the side of the man-
</p>
<p>agers of the program. Our sample results support their position that the
</p>
<p>overall program has exceeded the criterion for success of the foundation.
</p>
<p>C o m p a r i n g  a  S a m p l e  t o  a n  U n k n o w n  
P o p u l a t i o n :  T h e  S i n g l e - S a m p l e  t - T e s t  f o r  M e a n s
</p>
<p>The proportion provides us with a special case in which we can calcu-
</p>
<p>late the standard error of our sampling distribution based on our null hy-
</p>
<p>pothesis. But this is not possible when our null hypothesis relates to a
</p>
<p>mean of an interval-level measure. In this case, there is not one specific
</p>
<p>variance or standard deviation associated with a mean but an infinite
</p>
<p>�PQ/N
</p>
<p>T H E S I N G L E - S A M P L E t - T E S T F O R M E A N S 257</p>
<p/>
</div>
<div class="page"><p/>
<p>number of potential variances or standard deviations. How, then, can we
</p>
<p>test hypotheses about unknown parameters in the case of the mean?
</p>
<p>One obvious method is to simply use the variance of our sample as a
</p>
<p>&ldquo;guesstimate&rdquo; of the variance of the population distribution. The problem
</p>
<p>with this solution is that the variance of a sample is a somewhat biased
</p>
<p>estimate of the variance of the population. By this we mean that the av-
</p>
<p>erage of repeated observations of the variance (s2) tends in the long run
</p>
<p>not to be equivalent to the value of �2. We can transform s2 to a better
</p>
<p>estimate of �2 through a very small correction to the equation for the
</p>
<p>variance. This new statistic (expressed as since it is an estimate of �2)
</p>
<p>is represented in Equation 10.7.6 An estimate of the standard deviation
</p>
<p>can be gained by taking the square root of this value.
</p>
<p>Equation 10.7
</p>
<p>In order to use this new statistic to test hypotheses, we must also use
</p>
<p>a slightly different sampling distribution, called the t distribution. It is
</p>
<p>sometimes called Student&rsquo;s t because its inventor, W. S. Gossett
</p>
<p>(1876&ndash;1936), first described the distribution under the pen name Student
</p>
<p>in 1908.
</p>
<p>The t distribution (Appendix 4) is very similar to the z distribution
</p>
<p>(Appendix 3). However, as with the chi-square test, the shape of the t
</p>
<p>distribution is dependent on the number of degrees of freedom. The
</p>
<p>number of degrees of freedom for a single-sample t-test is defined as 
</p>
<p>N � 1. When the number of cases in a sample is greater than 500, then
</p>
<p>the t and z distributions are virtually identical. However, as the number
</p>
<p>of cases in a sample gets smaller and smaller, and so accordingly does
</p>
<p>the number of degrees of freedom, the t distribution becomes flatter and
</p>
<p>a larger and larger test statistic is needed to reject the null hypothesis.
</p>
<p>This fact can be illustrated by looking at the t table in Appendix 4. As
</p>
<p>you can see, the t table lists the critical values associated with six signifi-
</p>
<p>cance thresholds for both one- and two-tailed tests. Let&rsquo;s focus on the
</p>
<p>fourth column, which is the critical value associated with a two-tailed,
</p>
<p>5% significance level. When the number of degrees of freedom is 500,
</p>
<p>the critical value for the t-statistic is the same as for the z distribution:
</p>
<p>1.960. At 120, the t value needed to reject the null hypothesis is still al-
</p>
<p>most the same: 1.980. At 100, the value is 1.982: at 50, it is 2.008; and at
</p>
<p>25, it is 2.060. The largest differences come for even smaller degrees of
</p>
<p>freedom.
</p>
<p> �̂2 � 
�
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2
</p>
<p>N � 1
</p>
<p>(�̂)
</p>
<p>�̂2
</p>
<p>6As noted on page 105 (footnote 1), computerized statistical analysis packages, such as
</p>
<p>SPSS, use this corrected estimate in calculating the variance and standard deviation for
</p>
<p>sample estimates.
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N258</p>
<p/>
</div>
<div class="page"><p/>
<p>The t distribution presents a new problem as well in making infer-
</p>
<p>ences to unknown populations. Relaxing the assumption of normality is
</p>
<p>generally considered more risky in a t-test than in a z-test. This makes
</p>
<p>good sense because we are now using an estimate of � rather than the
</p>
<p>actual population parameter. As the number of cases increases, our con-
</p>
<p>fidence in this estimate grows.7 How large should N be before you are
</p>
<p>willing to use a t-test? With samples of more than 30 cases, your statisti-
</p>
<p>cal conclusions are not likely to be challenged. However, the t distribu-
</p>
<p>tion is particularly sensitive to outliers. Conclusions based on smaller
</p>
<p>samples should be checked carefully to make sure that one or two ob-
</p>
<p>servations are not the cause of a very large statistical outcome.
</p>
<p>Testing Hypotheses with the t Distribution
</p>
<p>We are now ready to turn to a practical example. Suppose that the study
</p>
<p>described earlier also examined the average test scores for those prison-
</p>
<p>ers who had completed the program. The foundation set a standard of
</p>
<p>success of 65 on the test. Program managers say that prisoners who have
</p>
<p>completed the program achieve average scores much higher than this.
</p>
<p>The newspaper expos&eacute; again claims that the average scores are consid-
</p>
<p>erably lower than those expected by the foundation. In this case, you are
</p>
<p>able to take an independent random sample of 51 prisoners who have
</p>
<p>completed the test. You find that the test mean for the sample is 60, and
</p>
<p>the standard deviation is 15. What conclusions about the larger popula-
</p>
<p>tion of prisoners can you come to based on your sample results?
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution (relaxed because 
</p>
<p>N is large).
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: Prisoners who have completed the program.
</p>
<p>Hypotheses:
</p>
<p>H0: The mean test score for prisoners who have completed the program
</p>
<p>is 65 (� � 65).
</p>
<p>H1: The mean test score for prisoners who have completed the program
</p>
<p>is not 65 (� � 65).
</p>
<p>7Our statistical problem is that we assume that � and � are independent in developing
</p>
<p>the t distribution. When a distribution is normal, this is indeed the case. However, for
</p>
<p>other types of distributions, we cannot make this assumption, and when N is small, a
</p>
<p>violation of this assumption is likely to lead to misleading approximations of the ob-
</p>
<p>served significance level of a test.
</p>
<p>T H E S I N G L E - S A M P L E t - T E S T F O R M E A N S 259</p>
<p/>
</div>
<div class="page"><p/>
<p>Following the assumptions of our test, we use an interval scale (the
</p>
<p>mean of test scores) and an independent random sampling method. We
</p>
<p>relax the assumption of normality because N is larger than the minimum
</p>
<p>threshold of 30 recommended for interval-level measures. Our research hy-
</p>
<p>pothesis is once again nondirectional so that we can examine the positions
</p>
<p>of both the managers of the program and the newspaper expos&eacute;. The null
</p>
<p>hypothesis is that the mean test score for the population of prisoners com-
</p>
<p>pleting the program is 65 (the foundation standard), or that � � 65.
</p>
<p>The Sampling Distribution Because � is unknown and cannot be deduced
</p>
<p>from our null hypothesis, we will use the t distribution. The number of
</p>
<p>degrees of freedom for our example is defined as N � 1, or 51 � 1 � 50.
</p>
<p>Significance Level and Rejection Region Again, we have no reason in this
</p>
<p>example to depart from the 0.05 significance threshold. Because our re-
</p>
<p>search hypothesis is not directional, we use a two-tailed test. Turning to
</p>
<p>the t table, we find that a t-score of 2.008 is associated with a two-tailed,
</p>
<p>5% significance threshold (at 50 degrees of freedom). This means that we
</p>
<p>will reject our null hypothesis if we obtain a test statistic greater than
</p>
<p>2.008 or less than �2.008. For these observed values of our test statistic,
</p>
<p>the observed significance level of our test is less than the criterion of
</p>
<p>0.05 that we have selected.
</p>
<p>The Test Statistic The test statistic for the t distribution is similar to that
</p>
<p>for the z distribution. The only difference is that we now use an estimate
</p>
<p>of the standard deviation rather than � itself.
</p>
<p>Equation 10.8
</p>
<p>Although we can get an estimate of � by adjusting the calculation for
</p>
<p>s, the formula for t may also be written in a way that allows us to calcu-
</p>
<p>late t from the unadjusted sample standard deviation.
</p>
<p> � 
X � �
</p>
<p>��
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2
</p>
<p>N
    �N � 1
</p>
<p> t � 
X � �
</p>
<p>�̂/�N
 � 
</p>
<p>X � �
</p>
<p>��
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2
</p>
<p>N � 1
   �N
</p>
<p>t � 
X � �
</p>
<p>�sd
 � 
</p>
<p>X � �
</p>
<p>�̂/�N
</p>
<p>(�̂)
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N260</p>
<p/>
</div>
<div class="page"><p/>
<p>This means that we can simplify the equation for the t-test as follows:
</p>
<p>Equation 10.9
</p>
<p>Applying the t formula to our example, we use the mean of the sample,
</p>
<p>60, as ; � is defined by the null hypothesis as 65; s is our sample stan-
</p>
<p>dard deviation of 15; and N is the number of cases for our sample (51).
</p>
<p>X
</p>
<p>t � 
X � �
</p>
<p>s/�N � 1
</p>
<p>W orking It Out
</p>
<p> � �2.3570
</p>
<p> � 
�5
</p>
<p>2.1213
</p>
<p> � 
�5
</p>
<p>15/�50
</p>
<p> � 
60 � 65
</p>
<p>15/�51 � 1
</p>
<p> t � 
X � �
</p>
<p>s/�N � 1
</p>
<p>The Decision Because the test statistic of �2.3570 is less than �2.008,
</p>
<p>we reject the null hypothesis and conclude that the result is significantly
</p>
<p>different from the goal set by the foundation. In this case, our decision is
</p>
<p>on the side of the newspaper expos&eacute;. We can conclude from our sample
</p>
<p>(with a 5% level of risk of falsely rejecting the null hypothesis) that the
</p>
<p>test scores in the population of prisoners who have completed the pro-
</p>
<p>gram are below the foundation goal of 65.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>Parametric tests of statistical significance allow us to make inferences
</p>
<p>about a population from samples using interval-level data. In a paramet-
</p>
<p>ric test, we make certain assumptions about the shape of the population
</p>
<p>distribution at the outset.
</p>
<p>The normal distribution, or normal curve, is widely used in statis-
</p>
<p>tics. It is symmetrical and bell shaped. Its mean, mode, and median are
</p>
<p>always the same. There will always be a set number of cases between
</p>
<p>C H A P T E R S U M M A R Y 261</p>
<p/>
</div>
<div class="page"><p/>
<p>the mean and points a measured distance from the mean. The measure
</p>
<p>of this distance is the standard deviation unit. All normal distributions,
</p>
<p>irrespective of their mean or standard deviations, can be converted to a
</p>
<p>single standard normal distribution by converting the actual scores in the
</p>
<p>sample or population to z-scores. To use a normal sampling distribution
</p>
<p>for a test of statistical significance, we must assume that the characteristic
</p>
<p>studied is normally distributed in the population.
</p>
<p>An important dilemma in statistical inference is created by this as-
</p>
<p>sumption. How can we make assumptions about the population distribu-
</p>
<p>tion when its characteristics are generally unknown? The central limit
</p>
<p>theorem describes an important fact that allows us to solve this prob-
</p>
<p>lem. As stated in the theorem, when the number of cases in a sample is
</p>
<p>large, the sampling distribution will be approximately normal in shape,
</p>
<p>even if the population distribution itself is not. In the field of criminal
</p>
<p>justice, it is generally assumed that the central limit theorem can be ap-
</p>
<p>plied where the sample size is 30 or greater. When dealing with propor-
</p>
<p>tions, though, it is safer to require a sample size of at least 100. In such
</p>
<p>circumstances, we may relax the assumption of normality. We can
</p>
<p>now make inferences using a normal sampling distribution, even though
</p>
<p>the shape of the population distribution is unknown.
</p>
<p>In order to define the sampling distribution, we need information
</p>
<p>about the population parameters&mdash;information that is not usually avail-
</p>
<p>able. In the case of a test involving proportions, the null hypothesis can
</p>
<p>be used to define both the mean and the standard error of the popula-
</p>
<p>tion distribution. Once the population parameters have been defined by
</p>
<p>the null hypothesis, we can apply the formula for the z-test of statistical
</p>
<p>significance. In the case of a test of means, the null hypothesis cannot be
</p>
<p>used directly to define the standard error. We may, however, use the t
</p>
<p>K e y  T e r m s
</p>
<p>central limit theorem A theorem that
</p>
<p>states: &ldquo;If repeated independent random
</p>
<p>samples of size N are drawn from a
</p>
<p>population, as N grows large, the 
</p>
<p>sampling distribution of sample means 
</p>
<p>will be approximately normal.&rdquo; The 
</p>
<p>central limit theorem enables the researcher
</p>
<p>to make inferences about an unknown
</p>
<p>population using a normal sampling
</p>
<p>distribution.
</p>
<p>normal curve A normal frequency distrib-
</p>
<p>ution represented on a graph by a continu-
</p>
<p>ous line.
</p>
<p>normal frequency distribution A bell-
</p>
<p>shaped frequency distribution, symmetrical
</p>
<p>in form. Its mean, mode, and median are
</p>
<p>always the same. The percentage of cases
</p>
<p>between the mean and points at a mea-
</p>
<p>sured distance from the mean is fixed.
</p>
<p>sampling distribution, which relies on an estimate of the standard error.
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N262</p>
<p/>
</div>
<div class="page"><p/>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>p Proportion of successes (sample)
</p>
<p>P Proportion of successes (population)
</p>
<p>Q Proportion of failures (population)
</p>
<p>�sd
</p>
<p>t t-score
</p>
<p>Estimate of �
</p>
<p>To determine the z-score for a single observation:
</p>
<p>To determine the standard error of a sampling distribution:
</p>
<p>�sd � 
�
</p>
<p>�N
</p>
<p>z � 
Xi � �
</p>
<p>�
</p>
<p>�̂
</p>
<p>relaxing an assumption Deciding that
</p>
<p>we need not be concerned with that as-
</p>
<p>sumption. For example, the assumption
</p>
<p>that a population is normal may be relaxed
</p>
<p>if the sample size is sufficiently large to in-
</p>
<p>voke the central limit theorem.
</p>
<p>single-sample t-test A test of statistical
</p>
<p>significance that is used to examine
</p>
<p>whether a sample is drawn from a specific
</p>
<p>population with a known or hypothesized
</p>
<p>mean. In a t-test, the standard deviation of
</p>
<p>the population to which the sample is
</p>
<p>being compared is unknown.
</p>
<p>single-sample z-test A test of statistical sig-
</p>
<p>nificance that is used to examine whether a
</p>
<p>sample is drawn from a specific population
</p>
<p>with a known or hypothesized mean. In a 
</p>
<p>z-test, the standard deviation of the popula-
</p>
<p>tion to which the sample is being compared
</p>
<p>either is known or&mdash;as in the case of a pro-
</p>
<p>portion&mdash; is defined by the null hypothesis.
</p>
<p>standard deviation unit A unit of mea-
</p>
<p>surement used to describe the deviation of
</p>
<p>a specific score or value from the mean in
</p>
<p>a z distribution.
</p>
<p>standard error The standard deviation of
</p>
<p>a sampling distribution.
</p>
<p>standard normal distribution A normal
</p>
<p>frequency distribution with a mean of 0
</p>
<p>and a standard deviation of 1. Any normal
</p>
<p>frequency distribution can be transformed
</p>
<p>into the standard normal distribution by
</p>
<p>using the z formula.
</p>
<p>z-score Score that represents standard
</p>
<p>deviation units for a standard normal
</p>
<p>distribution.
</p>
<p>Standard error of a normal distribution
</p>
<p>S Y M B O L S A N D F O R M U L A S 263</p>
<p/>
</div>
<div class="page"><p/>
<p>To determine the z-score for a sample mean:
</p>
<p>To determine the standard deviation of a proportion:
</p>
<p>To determine the z-score for a sample proportion:
</p>
<p>To estimate the value of � from data in a sample:
</p>
<p>To determine the value of t:
</p>
<p>E x e r c i s e s
</p>
<p>10.1 In which of the following circumstances would a researcher be justi-
fied in using a normal sampling distribution? Explain how or why for
each case.
</p>
<p>a. A sample of 10 subjects is drawn to study a variable known to be
normally distributed in the population.
</p>
<p>b. A sample of 50 subjects is drawn to study a variable known to be
normally distributed in the population.
</p>
<p>c. A sample of 10 subjects is drawn to study a variable. The shape of
the distribution of this variable in the population is unknown.
</p>
<p>d. A sample of 50 subjects is drawn to study a variable. The 
shape of the distribution of this variable in the population is
unknown.
</p>
<p>e. A sample of 50 subjects is drawn to study a proportion. The shape
of the distribution of this proportion in the population is un-
known.
</p>
<p>t � 
X � �
</p>
<p>s/�N � 1
</p>
<p>�̂ � ��
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2
</p>
<p> N � 1
</p>
<p>z � 
X � �
</p>
<p>�sd
 � 
</p>
<p>X � �
</p>
<p>�/�N
 � 
</p>
<p>p � P
</p>
<p>�PQ /N
</p>
<p>�  � �PQ
</p>
<p>z � 
X � �
</p>
<p>�sd
 � 
</p>
<p>X � �
</p>
<p>�/�N
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N264</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 A team of psychologists has created an index they claim measures an
individual&rsquo;s &ldquo;ability to control anger.&rdquo; The index is calculated from the
answers to a detailed questionnaire and is normally distributed among
U.S. adult males, with a mean of 100 and a standard deviation of 30.
Researchers assess a group of ten prisoners, all of whom have been
convicted for violent rapes. They discover that the mean score for the
group is 50.8.
</p>
<p>a. What percentage of U.S. adult males would be expected to obtain a
score equal to or less than that of the rapists?
</p>
<p>b. The psychologists who constructed the index consider the bottom
10% of U.S. adult males on their distribution to be &ldquo;strongly in-
clined to use violence to solve social problems.&rdquo; Albert is a re-
spectable businessman who scores 60.6 on the scale. Is Albert in-
cluded in this category? Explain why.
</p>
<p>c. What percentage of U.S. adult males would be expected to score
between 110 and 120 on the &ldquo;anger index&rdquo;?
</p>
<p>10.3 A teacher gives the following assignment to 200 students: Check the
local newspaper every morning for a week and count how many times
the word &ldquo;gun&rdquo; is mentioned on the &ldquo;local news&rdquo; pages. At the end of
the week, the students report their totals. The mean result is 85, with a
standard deviation of 8. The distribution of scores is normal.
</p>
<p>a. How many students would be expected to count fewer than 70
cases?
</p>
<p>b. How many students would be expected to count between 80 and
90 cases?
</p>
<p>c. Karen is a notoriously lazy student. She reports a total of 110 cases
at the end of the week. The professor tells her that he is convinced
she has not done the assignment, but has simply made up the
number. Are his suspicions justified?
</p>
<p>10.4 The professors who teach the Introduction to Psychology course at
State University pride themselves on the normal distributions of exam
scores. After the first exam, the current professor reports to the class
that the mean for the exam was 73, with a standard deviation of 7.
</p>
<p>a. What proportion of student would be expected to score above 80?
</p>
<p>b What proportion of students would be expected to score between
55 and 75?
</p>
<p>c. What proportion of students would be expected to score less than 65?
</p>
<p>d. If the top 10% of the class receive an A for the exam, what score
would be required for a student to receive an A?
</p>
<p>e. If the bottom 10% of the class fail the exam, what score would earn
a student a failing grade?
</p>
<p>E X E R C I S E S 265</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 A noted criminologist, Leslie Wilkins, has suggested that the distribu-
tion of deviance in the population follows a normal bell-shaped curve,
with &ldquo;sinners&rdquo; at one extreme, &ldquo;saints&rdquo; at the other, and most of us
falling somewhere in between the two. Working on the basis of this
theory, a researcher constructs a detailed self-report survey whereby
individuals are given a score based on the offenses they have commit-
ted in the past year, with the score weighted according to the relative
triviality or seriousness of each offense. The lower the score, the
nearer the individual approximates &ldquo;sinner&rdquo; status, and the higher the
score, the closer he or she is to being a &ldquo;saint.&rdquo; From his initial sample
of 100 adults in a specific state, the researcher computes a mean score
of 30, with a standard deviation of 5.
</p>
<p>a. If the researcher&rsquo;s model is correct, below which score should he
expect to find the 5% of U.S. society with the greatest propensity to
deviance?
</p>
<p>b. In his sample of 100, the researcher is surprised to discover that 50
subjects score greater than 35 on the deviance test. How many
cases would be expected under the assumption of a normal distrib-
ution of saints and sinners? What does this suggest about the origi-
nal theory?
</p>
<p>10.6 An established test measuring &ldquo;respect for authority&rdquo; has a mean
among U.S. adults of 73 and a standard error of 13.8. Brenda gives the
test to 36 prison inmates and finds the mean score to be 69.
</p>
<p>a. Is this enough evidence to suggest that the prisoners belong to a
population that has significantly less respect for authority than the
general U.S. adult population?
</p>
<p>b. Assuming there is enough information, test whether this sample dif-
fers significantly from the population. Use a significance level of 5%
and outline each of the stages of a test of statistical significance.
</p>
<p>10.7 The governor of Stretford Prison has a biographical record of all the
inmates. The mean age of all the inmates is 22, with a standard devia-
tion of 7.5. A recent survey by a hostile researcher makes damaging
criticisms of the educational standards in the prison. The prison gover-
nor suspects that the 50 prisoners interviewed for the study were not
chosen at random. The mean age of the prisoners chosen is 20. Show
how a test for statistical significance can be used by the governor to
cast doubt on the sampling method of the survey. Use a significance
level of 5% and outline each of the stages of a test of statistical
significance.
</p>
<p>10.8 A hundred years ago, an anonymous scientist wrote a famous indict-
ment of a notoriously cruel prison somewhere in the United States.
Without ever referring to the prison by name, the scientist checked the
records of all those who were imprisoned over its 50-year history and
found that 15% of those who entered died within. Henry, a historian,
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N266</p>
<p/>
</div>
<div class="page"><p/>
<p>is intrigued by the old report and publishes an article in a historical
journal in which he states his conviction that the report was referring
to Grimsville Prison, which existed about that time. In a subsequent
issue of the journal, a rival historian claims that Henry has shown no
evidence to support his theory.
</p>
<p>Henry finds the records from Grimsville, and from a sample of 80
prisoner records he discovers that 11% of the prisoners died inside.
Can he use this information to substantiate his claim that the object of
the report is indeed Grimsville? Use a significance level of 5% and out-
line each of the stages of a test of statistical significance.
</p>
<p>10.9 Every pupil at Foggy Lane College was asked a series of questions,
which led to an overall score grading &ldquo;satisfaction&rdquo; with the college&rsquo;s
discipline procedures. The overall mean score was 65. Roger suspects
that the black students at the college feel differently. He takes a ran-
dom sample of 25 black students from the college and finds that their
mean satisfaction score is 61, with a standard deviation of 8.
</p>
<p>Are the black students&rsquo; views on discipline significantly different
from those of the general student population? Use a significance level
of 1% and outline each of the stages of a test of statistical significance.
</p>
<p>10.10 A special police unit has spent several years tracking all the members
of a large child-abuse ring. In an interview with a daily newspaper, a
junior detective on the unit claims that the ringleaders have been
tracked down and will shortly be arrested. In response to questions
from the interviewer about the makeup of the child-abuse ring, the
detective replies, &ldquo;We have gathered details on every last member of
this criminal group&mdash;they come from very varied backgrounds and
their average age is 36.&rdquo;
</p>
<p>X is the chairperson of a charitable club, which is in fact a front for
a substantial child-abuse circle. He reads the newspaper article and
fears that it might refer to him and his group. He looks through the
club&rsquo;s membership files and draws a sample of 50 members, finding
an average age of 40 with a standard deviation of 9.
</p>
<p>Can X be confident that the detective interviewed in the newspaper
was not referring to his criminal group?
</p>
<p>10.11 A civil rights group is concerned that Hispanic drug offenders are
being treated more severely than all drug offenders in Border State. A
state government web site reports that all drug offenders were sen-
tenced to an average of 67 months in prison. The group conducts a
small study by taking a random sample of public court records. For
the 13 Hispanic drug offenders in the sample, the average sentence
was 72 months (s = 8.4). Use a 5% significance level and test whether
Hispanic drug offenders in Border State are sentenced more severely.
Be sure to outline the steps in a test of statistical significance.
</p>
<p>10.12 A researcher believes that offenders who are arrested for committing
homicides in her city are younger than the national average. A review
</p>
<p>E X E R C I S E S 267</p>
<p/>
</div>
<div class="page"><p/>
<p>of FBI arrest statistics for recent years indicates that the mean age of
homicide offenders is 18.7. The researcher collects information on a
random sample of 25 persons arrested for homicide in her city and
finds the mean age to be 16.8, with a standard deviation of 4.1. Can
the researcher conclude that homicide offenders in her city are
younger than the national average? Use a significance level of 0.05. Be
sure to outline the steps in a test of statistical significance.
</p>
<p>10.13 Following a revolution, the new leadership of the nation of Kippax
decides to hold a national referendum on whether the practice of cap-
ital punishment should be introduced. In the buildup to the referen-
dum, a leading army general wishes to gauge how the people are
likely to vote so that he can make a public statement in line with pop-
ular feeling on the issue. He commissions Greg, a statistician, to carry
out a secret poll of how people expect to vote. The results of Greg&rsquo;s
poll are as follows: The sample proportion in favor of introducing
capital punishment is 52%.
</p>
<p>Do the results indicate that the majority of the population favors in-
troducing capital punishment? Use a significance level of 5% and out-
line each of the stages of a test of statistical significance.
</p>
<p>10.14 The Silver Star Treatment Center claims to be effective at reducing
drug addiction among the persons who go through its treatment regi-
men. As evidence of the effectiveness of the Silver Star treatment, the
director claims that 63% of all drug users nationally have a relapse
within 12 months of treatment, but in a random sample of 91 cases
treated by Silver Star, only 52% had a relapse within 12 months of
completing the treatment. Use a 1% level of significance to test
whether Silver Star&rsquo;s treatment is effective at reducing drug use. Be
sure to outline the steps in a test of statistical significance.
</p>
<p>10.15 A federal judge issues an opinion claiming that nonviolent drug of-
fenders should make up no more than 20% of the local jail popula-
tion. If a jail is found to have more than 20% nonviolent drug offend-
ers, the jail will fall under court order and be required to release
inmates until the composition of the jail population conforms to the
judge&rsquo;s standard. The local sheriff draws a random sample of 33 in-
mates and finds that 23% have been convicted of nonviolent drug of-
fenses. Should the sheriff be concerned about the jail coming under
court supervision? Use a significance level of 0.05. Be sure to outline
the steps in a test of statistical significance.
</p>
<p>C H A P T E R T E N :  T H E N O R M A L D I S T R I B U T I O N268</p>
<p/>
</div>
<div class="page"><p/>
<p>Comparing Means and Proportions 
</p>
<p>in Two Samples
</p>
<p>t h a t  a r e  n o t  i n d e p e n d e n t
</p>
<p>C h a p t e r  e l e v e n
</p>
<p>C o m p a r i n g  s a m p l e  m e a n s
</p>
<p>C o m p a r i n g  s a m p l e  p r o p o r t i o n s
</p>
<p>C o m p a r i n g  m e a n s  i n  s a m p l e s  
</p>
<p>What is the Two-Sample t-Test for Means?
</p>
<p>What are the Assumptions of the Test?
</p>
<p>How is the Test Carried Out?
</p>
<p>What is the Two-Sample t-Test for Proportions?
</p>
<p>What are the Assumptions of the Test?
</p>
<p>How is the Test Carried Out?
</p>
<p>What is the t-Test for Dependent Samples?
</p>
<p>What are the Assumptions of the Test?
</p>
<p>How is the Test Carried Out?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_11,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>IN CHAPTER 10, we used parametric significance tests to compare the
mean or proportion of a single sample with a population goal or para-
</p>
<p>meter. In this chapter, we turn to a more commonly used application of
</p>
<p>parametric tests of statistical significance: comparisons between samples.
</p>
<p>Let&rsquo;s say, for example, that you are interested in whether there is a differ-
</p>
<p>ence in the mean salaries of male and female police officers or in the
</p>
<p>proportions of African Americans and others arrested last year. Your
</p>
<p>question in either of these cases is not whether the population parame-
</p>
<p>ters have particular values, but whether the parameters for the groups
</p>
<p>examined in each case are different. This involves comparing means and
</p>
<p>proportions for two populations. If you take samples from these popula-
</p>
<p>tions, you can make inferences regarding the differences between them
</p>
<p>by building on the normal distribution tests covered in Chapter 10.
</p>
<p>C o m p a r i n g  S a m p l e  M e a n s
</p>
<p>The Case of Anxiety Among Police Officers and Firefighters
</p>
<p>In a study conducted by University of Washington researchers, police of-
</p>
<p>ficers were compared to firefighters in terms of the amount of stress and
</p>
<p>anxiety they experienced on the job.1 One measure the researchers used
</p>
<p>was derived by creating an interval-scale index from questions about the
</p>
<p>occurrence on the job of symptoms of anxiety, such as sweating and &ldquo;the
</p>
<p>jitters.&rdquo; The researchers drew a sample of police officers by going to po-
</p>
<p>lice stations and asking officers to be paid participants in their study. For
</p>
<p>firefighters, the researchers randomly selected subjects. The final sample,
</p>
<p>1Michael Pendleton, Ezra Stotland, Philip Spiers, and Edward Kirsch, &ldquo;Stress and Strain
</p>
<p>among Police, Firefighters, and Government Workers: A Comparative Analysis,&rdquo; Crim-
</p>
<p>inal Justice and Behavior 16 (1989): 196&ndash;210.
</p>
<p>270</p>
<p/>
</div>
<div class="page"><p/>
<p>all drawn from one city, included 127 firefighters and 197 police officers.
</p>
<p>For this sample, the researchers found that the mean anxiety-on-the-job
</p>
<p>score for police officers was 12.8 (s1 � 2.76), whereas that for firefighters
</p>
<p>was 8.8 (s2 � 2.85). What conclusions regarding the larger populations
</p>
<p>of firefighters and police officers can the researchers draw from these
</p>
<p>sample statistics?
</p>
<p>As in other problems involving comparisons between the means of
</p>
<p>two groups, we are not able to define the standard deviations of the
</p>
<p>population distributions for our test. Indeed, we conduct a test of statisti-
</p>
<p>cal significance for the differences between the two samples precisely
</p>
<p>because we do not have information on the population parameters. Ac-
</p>
<p>cordingly, we turn again to the t-test introduced in Chapter 10. In this
</p>
<p>case, we use a two-sample t-test for means.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution in both populations (re-
</p>
<p>laxed because N is large).
</p>
<p>Sampling Method: Independent random sampling (a nonrandom sam-
</p>
<p>pling technique was used for police officers; random sampling without
</p>
<p>replacement was used for firefighters).
</p>
<p>Sampling Frame: All police officers and firefighters in one city.
</p>
<p>Hypotheses:
</p>
<p>H0: The mean anxiety-on-the-job score for the population of police offi-
</p>
<p>cers is the same as that for the population of firefighters (�1 � �2).
</p>
<p>H1: The mean anxiety-on-the-job score for the population of police offi-
</p>
<p>cers is different from that for the population of firefighters (�1 � �2).
</p>
<p>The assumptions for the two-sample t-test are similar to those for the
</p>
<p>one-sample t-test. An interval level of measurement is assumed, and in-
</p>
<p>deed the characteristic being examined, anxiety on the job, is measured
</p>
<p>at the interval-scale level. The two-sample t-test also requires that both
</p>
<p>population distributions be normal in form. When this is the case, the
</p>
<p>sampling distribution of the difference between means&mdash;the focus of our
</p>
<p>test&mdash;is also normally distributed. Even when the populations examined
</p>
<p>are not normally distributed, the sampling distribution of the difference
</p>
<p>between the sample means will be normally distributed if the N of cases
</p>
<p>for both samples is large.
</p>
<p>The definition of how large samples must be to invoke the central
</p>
<p>limit theorem is again a matter of debate. In Chapter 10, we noted that a
</p>
<p>sample size of 30 or more was generally large enough to apply the cen-
</p>
<p>tral limit theorem in a single-sample test for means. For a two-sample
</p>
<p>test, we need a minimum of 30 cases in each sample. In our example,
</p>
<p>C O M P A R I N G S A M P L E M E A N S 271</p>
<p/>
</div>
<div class="page"><p/>
<p>both samples include a much larger number of subjects, and thus we can
</p>
<p>relax the assumption of normality.
</p>
<p>As with other tests we have examined, here we are required to use an
</p>
<p>independent random sampling method. For a two-sample t-test, we must
</p>
<p>assume that both samples are independent random samples. In practice,
</p>
<p>researchers do not ordinarily use separate sampling procedures to iden-
</p>
<p>tify the samples representing each population of interest. Rather, they
</p>
<p>draw a random sample from all members of a population and then as-
</p>
<p>sume that specific samples within the larger sample are also independent
</p>
<p>and random. For example, researchers interested in attitudes toward
</p>
<p>crime in the United States generally draw an independent random sam-
</p>
<p>ple of all U.S. residents. They may, however, also have an interest in
</p>
<p>comparing attitudes of men and women or of college graduates and
</p>
<p>non&ndash;college graduates. If the larger sample has been drawn as an inde-
</p>
<p>pendent random sample, the subsamples are also independent random
</p>
<p>samples.2
</p>
<p>The one practical difficulty with this assumption arises when the num-
</p>
<p>ber of subjects in a particular subpopulation is small. For example, in a
</p>
<p>survey of U.S. residents, a very small group of Jews or Muslims is likely
</p>
<p>to be sampled when researchers draw a simple independent random
</p>
<p>sample. Thus, even though such a subsample will still be independent
</p>
<p>and random (if the larger sample is independent and random), re-
</p>
<p>searchers may not end up with many cases because such a group repre-
</p>
<p>sents a small proportion of the U.S. population. When there is interest in
</p>
<p>a subpopulation that is small, researchers often identify such groups for
</p>
<p>special attention and attempt to draw larger samples from them.
</p>
<p>For the firefighters in our example, the researchers used a random
</p>
<p>sampling method, but they did not sample with replacement. This viola-
</p>
<p>tion of assumptions is not serious because the sample of firefighters
</p>
<p>drawn was small relative to the number of subjects in the sampling
</p>
<p>frame. The method of sampling for police officers represents a more seri-
</p>
<p>ous violation of the assumptions of the two-sample t-test. The re-
</p>
<p>searchers did not draw a random sample. Nonetheless, they argued that
</p>
<p>2The logic here follows simple common sense. If you select each case independently
</p>
<p>and randomly from a population, on each selection you have an equal probability of
</p>
<p>choosing any individual, whether male or female, college-educated or not, and so on.
</p>
<p>From the perspective of a particular group&mdash;for example, males&mdash;each time you
</p>
<p>choose a man, the method can be seen as independent and random. That is, the like-
</p>
<p>lihood of choosing any male from the sample is the same each time you draw a case.
</p>
<p>Of course, sometimes you will draw a female. However, within the population of
</p>
<p>males, each male has an equal chance of selection on each draw. And if the sampling
</p>
<p>method is independent, then each male has an equal chance of being selected every
</p>
<p>time a case is selected.
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S272</p>
<p/>
</div>
<div class="page"><p/>
<p>their sample was still representative of the population of police officers
</p>
<p>in the city:
</p>
<p>Participant officers were compared with nonparticipant officers on avail-
</p>
<p>able data (which were acquired by the police department independently
</p>
<p>of the study). These data included entrance psychological tests, current
</p>
<p>departmental physical fitness tests, age, sex, and so on. . . . The partici-
</p>
<p>pant and nonparticipant groups did not differ significantly on 25 com-
</p>
<p>parison variables.
</p>
<p>generally. For this inference to be justified, the researchers would have
</p>
<p>to explain why firefighters and police officers in this city are representa-
</p>
<p>tive of firefighters and police officers in other cities.
</p>
<p>1
</p>
<p>(�2) is the same (�1 � �2). The research hypothesis was that there is a
</p>
<p>difference (�1 � �2). The researchers did not define the direction of this
</p>
<p>difference. Their research hypothesis allows the possibility that police of-
</p>
<p>ficers experience more anxiety at work than firefighters as well as the op-
</p>
<p>tion that firefighters experience more anxiety at work than police officers.
</p>
<p>The Sampling Distribution For a difference of means test, we use the t
</p>
<p>sampling distribution. The number of degrees of freedom for the distri-
</p>
<p>bution is obtained by adding the numbers of cases in the two samples
</p>
<p>and subtracting 2: df � N1 � N2 � 2. For our example, the number of
</p>
<p>degrees of freedom is 322.
</p>
<p>The mean of the sampling distribution is defined, as in the case of a
</p>
<p>difference of proportions test, by the null hypothesis. It is represented by
</p>
<p>�1 � �2, or the hypothesized difference between the means of the two
</p>
<p>populations studied. Since the null hypothesis states that �1 � �2, the
</p>
<p>mean of the sampling distribution is 0.
</p>
<p> � 322
</p>
<p> � 197 � 127 � 2
</p>
<p> df � N1 � N2 � 2
</p>
<p>The validity of our inferences to the larger population of police officers
</p>
<p>in the city depends on how persuasive we find the researchers&rsquo; claims
</p>
<p>that their sample was representative. But irrespective of the generali-
</p>
<p>zability of these samples to the population of police officers and fire-
</p>
<p>fighters in the city, the researchers also want to infer their findings 
</p>
<p>beyond their sampling frame to police officers and firefighters more
</p>
<p>The null hypothesis for a difference of means test is generally that there
</p>
<p>is no difference, and this was the case in the University of Washington
</p>
<p>research. The null hypothesis stated simply that the mean anxiety-on-the-
</p>
<p>) and firefightersjob score for the populations of police officers (�
</p>
<p>C O M P A R I N G S A M P L E M E A N S 273</p>
<p/>
</div>
<div class="page"><p/>
<p>In defining the standard error of the sampling distribution for compar-
</p>
<p>ing two samples, we take into account the variances of the two popula-
</p>
<p>tions. Accordingly, the standard error of a sampling distribution of the
</p>
<p>difference of sample means is the square root of the sum of
</p>
<p>the two sample variances, each divided by its sample N:
</p>
<p>Equation 11.1
</p>
<p>In calculating this standard error, we can use either of two ap-
</p>
<p>proaches. The first assumes that the two population distributions not
</p>
<p>only have equal means but also have equal variances. In this case, we
</p>
<p>are assuming that the two population distributions are the same. This is
</p>
<p>often called the pooled variance method. The assumption we make in
</p>
<p>this approach, a common one in statistical tests, is often referred to as
</p>
<p>homoscedasticity (from the Greek for &ldquo;same scatter [or spread]&rdquo;). It can
</p>
<p>be written in mathematical form as follows:
</p>
<p>A second approach, called the separate variance method, does not
</p>
<p>make a specific assumption that the variances of the two populations are
</p>
<p>equal. You should note, however, that when samples are very small or
</p>
<p>one sample is much larger than the other, the simple estimate of degrees
</p>
<p>of freedom noted above must be corrected if the separate variance
</p>
<p>method is used. The correction commonly employed involves a good
</p>
<p>deal of computation.3 For our problem, which involves large samples of
</p>
<p>relatively similar size, it is unnecessary to take this approach.
</p>
<p>Given that the pooled variance method requires an additional assump-
</p>
<p>tion, that of homoscedasticity, you might question why researchers would
</p>
<p>choose this approach to analyze the statistical significance of their study
</p>
<p>results. One advantage of the pooled variance method is that you will
</p>
<p>generally get a more efficient estimate of the standard error of your sam-
</p>
<p>pling distribution. This means that the pooled variance method often
</p>
<p>leads to a larger t-statistic (though this is not always the case, as illustrated
</p>
<p>later in the chapter). But should you take advantage of this method if it
</p>
<p>means that you add the risk of violating an additional assumption?
</p>
<p>The separate variance method should be used in most circumstances.
</p>
<p>As a general rule, it is better to make fewer assumptions, because this cre-
</p>
<p>ates less potential for violating them and coming to a mistaken conclusion.
</p>
<p>Nonetheless, sometimes your sample results or prior research suggests
</p>
<p>�21 � �
2
2 � �
</p>
<p>2   or   �1 � �2 � �
</p>
<p>�sd(X1 � X2) � ��
2
1
</p>
<p>N1
 � 
</p>
<p>�22
N2
</p>
<p>(�sd(X1 �  X2))
</p>
<p>3See H. M. Blalock, Social Statistics (New York: McGraw-Hill, 1979), p. 231.
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S274</p>
<p/>
</div>
<div class="page"><p/>
<p>strongly that an assumption of equal variances can be made. For example,
</p>
<p>if there is little difference in the standard deviations you find in your sam-
</p>
<p>ples, you may be able to conclude with confidence that the population
</p>
<p>standard deviations do not differ.4 If, in turn, prior studies show that the
</p>
<p>standard deviations between the groups studied are very similar, this
</p>
<p>might also lead you to apply this assumption in your test. Most statistical
</p>
<p>How are the two methods different in practice? Let&rsquo;s start with the
</p>
<p>pooled variance approach. 
</p>
<p>If we assume that the two populations of interest have equal vari-
</p>
<p>ances, we can simplify Equation 11.1, which defines the standard error
</p>
<p>of the sampling distribution for a difference of means test.
</p>
<p>plified formula is given in Equation 11.2.
</p>
<p>Equation 11.2
</p>
<p>Because we do not know the actual value of �, we rewrite the equation,
</p>
<p>substituting an estimate of �, or , as shown in Equation 11.3.
</p>
<p>Equation 11.3
</p>
<p>This, of course, creates another problem for us. How do we calculate ?
</p>
<p>We now have two estimates of the sample variance, one from each sample.
</p>
<p>And we also need to take into account the bias associated with using s 2 to
</p>
<p>estimate , as discussed in Chapter 10. Our solution to the former problem
</p>
<p>is to weight the two sample variances by the N of cases in each sample.
</p>
<p>This is only fair, because the larger sample is likely to provide a better
</p>
<p>estimate of the joint standard deviation than the smaller sample. We in-
</p>
<p>clude a correction for bias of the sample variances directly in our esti-
</p>
<p>mate of � by subtracting 2 (1 for each sample) in the denominator of the
</p>
<p>equation, as shown in Equation 11.4.
</p>
<p>Equation 11.4�̂ � � N1s
2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
</p>
<p>�̂
</p>
<p>�̂
</p>
<p>�̂sd(X1 � X2) � �̂�N1 � N2N1N2
</p>
<p>�̂
</p>
<p>�sd(X1 � X2) � ��N1 � N2N1N2
</p>
<p>(�sd(X1 � X2))
</p>
<p>4A test of statistical significance may be performed to assess differences in variances. It
</p>
<p>is based on the F distribution, which is discussed in detail in Chapter 12. The test
</p>
<p>takes a ratio of the two variances being examined:
</p>
<p>F � 
�̂2larger variance
</p>
<p>�̂2smaller variance
</p>
<p>analysis computer programs provide test outcomes for both of these
</p>
<p>options with the correct degrees of freedom applied.
</p>
<p>This simplification process is outlined in the box on page 264. The sim-
</p>
<p>C O M P A R I N G S A M P L E M E A N S 275</p>
<p/>
</div>
<div class="page"><p/>
<p>To work out the pooled variance method for our example, we first es-
</p>
<p>timate the pooled standard deviation for the two populations, which pro-
</p>
<p>vides a result of 2.8043. We then calculate the standard error (
</p>
<p>for our sampling distribution. Our result is 0.3191.
</p>
<p>�̂sd(X1 � X2))
</p>
<p>�sd(X1 � X2) � ��
2
1
</p>
<p>N1
 � 
</p>
<p>�22
</p>
<p>N2
 � �� 1N1 � 1N2 � ��
</p>
<p>N1 � N2
N1N2
</p>
<p>Simplification of the Equation for the Standard Error of the
Sampling Distribution for Two Samples (�1 � �2)
</p>
<p>W orking It Out
</p>
<p> � 2.8043
</p>
<p> � �7.86405
</p>
<p> � �2,532.22322
</p>
<p> � �(197)(2.76)
2 � (127)(2.85)2
</p>
<p>197 � 127 � 2
</p>
<p> �̂ � � N1s
2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
</p>
<p>W orking It Out
</p>
<p> � 0.3191
</p>
<p> � 2.804�0.01295
</p>
<p> � 2.804�197 � 127(197)(127)
</p>
<p> �̂sd(X1 � X2) � �̂�N1 � N2N1N2</p>
<p/>
</div>
<div class="page"><p/>
<p>How does the pooled variance method differ from the separate vari-
</p>
<p>ance method? We once again begin with Equation 11.1. Because and
</p>
<p>are unknown, we use and to gain an estimate of . In
</p>
<p>turn, as before, the variances of our samples are not considered unbi-
</p>
<p>ased estimates of the variances of the population distributions. Accord-
</p>
<p>ingly, in order to obtain an unbiased estimate of the standard error using
</p>
<p>this method, we need once more to adjust the equations&mdash;in this case,
</p>
<p>by subtracting 1 from the denominator of each variance estimate, as
</p>
<p>shown in Equation 11.5.
</p>
<p>Equation 11.5
</p>
<p>Based on the sample variances of police officers and firefighters in our
</p>
<p>example, we get 0.321 as an estimate of the standard error of the two-
</p>
<p>sample t-text using the separate variance method.
</p>
<p>�̂sd(X1 � X2) � � s 
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s 22
N2 � 1
</p>
<p>sd(X1 � X2)
s22s
</p>
<p>2
1�
</p>
<p>2
2
</p>
<p>�21
</p>
<p>As you can see, the result found using the pooled variance method
</p>
<p>(0.319) is very similar to that found using the separate variance method
</p>
<p>(0.321). This will often be the case, especially when samples are rela-
</p>
<p>tively large or evenly divided between the two groups. Nonetheless,
</p>
<p>even small differences can sometimes affect the conclusions you reach in
</p>
<p>a two-sample t-test.
</p>
<p>Significance Level and Rejection Region The University of Washington
</p>
<p>researchers used a 0.05 significance level and a two-tailed significance
</p>
<p>test. The two-tailed test was based on their nondirectional research hy-
</p>
<p>job scores between firefighters and police officers.
</p>
<p>W orking It Out
</p>
<p> � 0.3214
</p>
<p> � � (2.76)
2
</p>
<p>197 � 1
 � 
</p>
<p>(2.85)2
</p>
<p>127 � 1
</p>
<p> �̂sd(X1 � X2) � � s 
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s 22
N2 � 1
</p>
<p>pothesis, which stated simply that there is a difference in anxiety-on-the-
</p>
<p>�
</p>
<p>C O M P A R I N G S A M P L E M E A N S 277</p>
<p/>
</div>
<div class="page"><p/>
<p>Interpolating from the t table (see Appendix 4), we find that a 
</p>
<p>t-value of about 1.97 is associated with a two-tailed 5% significance
</p>
<p>threshold (with 322 degrees of freedom). This means that a test
</p>
<p>statistic greater than 1.97 or less than �1.97 is needed to reject the
</p>
<p>null hypothesis.
</p>
<p>The Test Statistic To define the t-score appropriate for our test, we must
</p>
<p>alter the single-sample t-test equation to account for the fact that we are
</p>
<p>comparing two samples. First, we must adjust the numerator to reflect
</p>
<p>our comparisons of the differences in the means observed in our study
</p>
<p>with those defined in the null hypothesis. Second, we must adjust the
</p>
<p>denominator to reflect the standard error of the difference between
</p>
<p>means. Because we now have two methods for defining the standard
</p>
<p>error of the sampling distribution, we have two separate equations. The
</p>
<p>first reflects the difference of means test using a separate variance esti-
</p>
<p>mate (11.6a), and the second the difference of means test using a pooled
</p>
<p>variance estimate (11.6b).
</p>
<p>Both Equation 11.6a and Equation 11.6b have two quantities in the
</p>
<p>numerator. The first is the difference between the two sample means
</p>
<p>(represented by ). The second is the difference between the two
</p>
<p>population means (�1 � �2) as defined by the null hypothesis. Because
</p>
<p>the null hypothesis is that the two populations are equal, this quantity is
</p>
<p>equal to 0.
</p>
<p>In the denominator in each equation, the standard error used for a
</p>
<p>sampling distribution when comparing a sample mean to a population
</p>
<p>mean has been replaced with the standard error used when comparing
</p>
<p>sample means drawn from two populations. This quantity was defined in
</p>
<p>our discussion of the sampling distribution.
</p>
<p>The t-score for this problem is 12.461 using the separate variance esti-
</p>
<p>mate and 12.539 using the pooled variance estimate. As recommended
</p>
<p>(although the differences are small in this case), we use the separate
</p>
<p>variance method in making our decision.
</p>
<p>X1 � X2
</p>
<p>Equation 11.6b
</p>
<p>Pooled Variance Methodt � 
(X1 � X2) � (�1 � �2)
</p>
<p>�̂�N1 � N2N1N2
</p>
<p>Equation 11.6a
</p>
<p>Separate Variance Methodt � 
(X1 � X2) � (�1 � �2)
</p>
<p>� s 
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s 22
N2 � 1
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S278</p>
<p/>
</div>
<div class="page"><p/>
<p>The Decision Because our test statistic of 12.4456 is larger than the criti-
</p>
<p>cal value of our rejection region of 1.97, we reject the null hypothesis
</p>
<p>that there is no difference in anxiety-on-the-job scores between the pop-
</p>
<p>ulations of police officers and firefighters to which our test infers. For
</p>
<p>our test, the observed significance level is less than the significance
</p>
<p>threshold we set at the outset (p � 0.05).
</p>
<p>W orking It Out Separate Variance
</p>
<p> � 12.4456
</p>
<p> � 
4
</p>
<p>0.3214
</p>
<p> � 
(12.8 � 8.8) � 0
</p>
<p>� (2.76)
2
</p>
<p>197 � 1
 � 
</p>
<p>(2.85)2
</p>
<p>127 � 1
</p>
<p> t � 
(X1 � X2) � (�1 � �2)
</p>
<p>� s
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s 22
N2 � 1
</p>
<p>W orking It Out Pooled Variance
</p>
<p> � 12.5353
</p>
<p> � 
4
</p>
<p>0.3191
</p>
<p> � 
(12.8 � 8.8) � 0
</p>
<p>�(197)(2.76)
2 � (127)(2.85)2
</p>
<p>197 � 127 � 2
 �197 � 127(197)(127)
</p>
<p> t � 
(X1 � X2) � (�1 � �2)
</p>
<p>�N1s
2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
 �N1 � N2N1N2
</p>
<p> t � 
(X1 � X2) � (�1 � �2)
</p>
<p>�̂�N1 � N2N1N2
,  where �̂ � �N1s
</p>
<p>2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
</p>
<p>C O M P A R I N G S A M P L E M E A N S 279</p>
<p/>
</div>
<div class="page"><p/>
<p>Bail in Los Angeles County: Another Example 
</p>
<p>of the Two-Sample t-Test for Means
</p>
<p>As a second example of the two-sample t-test for means, we will
</p>
<p>examine a study of bail setting in Los Angeles County in the 1990s.
</p>
<p>The State Court Processing Statistics database represents a random
</p>
<p>sample of felony defendants from more than 50 urban court districts 
</p>
<p>in the United States. Since Los Angeles County participated in the
</p>
<p>study throughout the 1990s, data are available for 1990, 1992, 1994,
</p>
<p>and 1996.5
</p>
<p>An important issue in criminal justice decision making has been the
</p>
<p>impact of the defendant&rsquo;s race or ethnicity on the type of decision
</p>
<p>made. We can focus on the amount of bail set as one way to begin to
</p>
<p>test for racial or ethnic differences in criminal case processing. In Los
</p>
<p>Angeles County in the 1990s, a sample of 1,121 African Americans were
</p>
<p>required to post a mean bail amount of $50,841 (s � 115,565). A sample
</p>
<p>of 1,798 Hispanics of any race were required to post a mean bail
</p>
<p>amount of $66,552 (s � 190,801). Although the difference in mean bail
</p>
<p>amounts for these two samples of defendants appears to be large (ap-
</p>
<p>proximately $16,000), can we conclude that this difference is statistically
</p>
<p>significant?
</p>
<p>Assumptions
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution in both populations (re-
</p>
<p>laxed because N is large).
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: All felony arrestees in Los Angeles County in 
</p>
<p>the 1990s.
</p>
<p>Hypotheses:
</p>
<p>H0: The mean bail amount set for the population of African American
</p>
<p>felony defendants is the same as the mean bail amount set for the popu-
</p>
<p>lation of Hispanic felony defendants of any race (�1 � �2).
</p>
<p>H1: The mean bail amount set for the population of African American
</p>
<p>felony defendants is different from the mean bail amount set for the
</p>
<p>population of Hispanic felony defendants of any race (�1 � �2).
</p>
<p>5These data are available through the National Archive of Criminal Justice Data and
</p>
<p>can be accessed at http://www.icpsr.umich.edu/NACJD.
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S280</p>
<p/>
<div class="annotation"><a href="http://www.icpsr.umich.edu/NACJD">http://www.icpsr.umich.edu/NACJD</a></div>
</div>
<div class="page"><p/>
<p>The Sampling Distribution Because we are interested in comparing
</p>
<p>means and � for the population distributions is unknown, we use a t-test
</p>
<p>for means. Since the number of cases in each sample is large, we can
</p>
<p>relax the normality assumption for this test. The number of degrees of
</p>
<p>freedom for the test is df � N1 � N2 � 2 � 1,121 � 1,798 � 2 � 2,917.
</p>
<p>Significance Level and Rejection Region Let&rsquo;s assume that we want to set
</p>
<p>a fairly strict level of statistical significance for this test&mdash;say, 0.01. We
</p>
<p>might argue that we are particularly concerned with a Type I error in this
</p>
<p>example, since concluding that there are racial differences may have
</p>
<p>very important implications for the criminal justice system. At the same
</p>
<p>time, there is no stated reason for expecting one group to have higher
</p>
<p>bail amounts than the other group, so we use a two-tailed test. Given
</p>
<p>that we have 2,917 degrees of freedom, a significance level of 0.01, and
</p>
<p>a two-tailed test, we can consult the t distribution table and determine
</p>
<p>that our critical values for this analysis are �2.576. If the test statistic is
</p>
<p>greater than �2.576 or less than �2.576, then it falls into the rejection re-
</p>
<p>gion for the test, and we will conclude that bail amounts set are not
</p>
<p>equal across the two felony groups of defendants.
</p>
<p>The Test Statistic We calculate the test statistic using both the separate
</p>
<p>variance and the pooled variance methods. As we demonstrate below,
</p>
<p>the t-score is �2.7694 using the separate variance method and �2.4863
</p>
<p>using the pooled variance method. Following the earlier recommenda-
</p>
<p>tion, we will use the separate variance method in making our decision
</p>
<p>about the null hypothesis.
</p>
<p>W orking It Out Separate Variance Method
</p>
<p> � �2.7694
</p>
<p> � 
�15,711
</p>
<p>5,673.02
</p>
<p> � 
(50,841 � 66,552) � 0
</p>
<p>� 115,565
2
</p>
<p>1,121 � 1
 � 
</p>
<p>190,8012
</p>
<p>1,798 � 1
</p>
<p> t � 
(X1 � X2) � (�1 � �2)
</p>
<p>� s
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s22
N2 � 1
</p>
<p>C O M P A R I N G S A M P L E M E A N S 281</p>
<p/>
</div>
<div class="page"><p/>
<p>The Decision Because our test statistic of �2.7694 is less than the criti-
</p>
<p>cal value of �2.576, we reject the null hypothesis that there is no differ-
</p>
<p>ence in bail amounts set for African Americans and Hispanics of any race
</p>
<p>in Los Angeles County. In this case, it is interesting to note that if we had
</p>
<p>used the pooled variance method, we would have failed to reject the
</p>
<p>null hypothesis. This points to the importance of making your assump-
</p>
<p>tions before you see the study results. 
</p>
<p>C o m p a r i n g  S a m p l e  P r o p o r t i o n s :  
T h e  T w o - S a m p l e  t - T e s t  f o r  D i f f e r e n c e s  o f  P r o p o r t i o n s
</p>
<p>As we noted in Chapter 10, one implication of the central limit theorem
</p>
<p>portion, the sampling distribution of a proportion begins to approximate
</p>
<p>comes large. The central tendency of this distribution and its dispersion
</p>
<p>are measured by the mean and standard error, just as for distributions
</p>
<p>that develop from interval-level data. In a difference of proportions test,
</p>
<p>our interest is in the difference between the populations studied. This
</p>
<p>W orking It Out Pooled Variance Method
</p>
<p> � �2.4863
</p>
<p> � 
�15,711
</p>
<p>6,319.07
</p>
<p> � 
(50,841 � 66,552) � 0
</p>
<p>�(1,121)(115,565)
2 � (1,798)(190,801)2
</p>
<p>1,121 � 1,798 � 2
 �1,121 � 1,798(1,121)(1,798)
</p>
<p> � 
(X1 � X2) � (�1 � �2)
</p>
<p>� N1s
2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
 �N1 � N2N1N2
</p>
<p> t � 
(X1 � X2) � (�1 � �2)
</p>
<p>�̂�N1 � N2N1N2
, � where �̂ � � N1s
</p>
<p>2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
</p>
<p>a normal distribution when the number of cases for the sample be-
</p>
<p>is that we can use a normal sampling distribution to test hypotheses
</p>
<p>involving proportions. While the mean and standard deviation are not 
</p>
<p>appropriate statistics to use with a nominal-level measure such as a pro-
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S2 28</p>
<p/>
</div>
<div class="page"><p/>
<p>difference is also a proportion. Though it would be inappropriate to use
</p>
<p>the mean and standard deviation to describe the sample or population
</p>
<p>distribution of this proportion, the mean and standard error are appro-
</p>
<p>At the same time, we generally cannot use the z-test for conducting a
</p>
<p>difference of proportions test. Rather, as in the previous examples, we
</p>
<p>rely on the t distribution to test our null hypothesis. You may wonder
</p>
<p>why we use the t-test rather than the z-test for making statistical infer-
</p>
<p>ences in the case of a difference of proportions. A t-test is used when the
</p>
<p>standard deviation of the population distribution is unknown and must
</p>
<p>be estimated. In Chapter 10, we noted that when we stated the propor-
</p>
<p>tion of successes expected under the null hypothesis, we also stated by
</p>
<p>implication the mean and the standard deviation of the population distri-
</p>
<p>bution of scores and thus the mean and the standard error of the sam-
</p>
<p>In fact, we again define the mean of the sampling distribution for
</p>
<p>such a test through the null hypothesis. For a difference of proportions
</p>
<p>test, the null hypothesis is ordinarily that there is no difference between
</p>
<p>the proportions of the two populations to which we seek to infer. This
</p>
<p>null hypothesis defines the mean of our sampling distribution: no differ-
</p>
<p>ence, or zero.
</p>
<p>As noted earlier in the chapter, in defining the standard error of our
</p>
<p>sampling distribution, we take into account the variances of the two
</p>
<p>populations. Our problem is that defining the standard error requires
</p>
<p>knowing the values of P and Q for each of the two populations we are
</p>
<p>interested in. (This is the case because we obtain the variance of a pro-
</p>
<p>the null hypothesis states only that the proportions of the two popula-
</p>
<p>tions are equal; it does not tell us the value of those proportions. Be-
</p>
<p>cause of this, when testing for differences of proportions, the researcher
6
</p>
<p>6In practice, many statistics texts use the z-test for examples involving proportions.
</p>
<p>Generally this is done because a difference of proportions test is appropriate only for
</p>
<p>larger samples, and with larger samples, there is substantively little difference between
</p>
<p>the outcomes of these two normal distribution tests. We illustrate a difference of pro-
</p>
<p>portions problem using a t-test because it follows the logic outlined in Chapter 10.
</p>
<p>That is, in the case where � is unknown, a t-test should be used. Moreover, most
</p>
<p>packaged statistical programs provide outcomes only in terms of t-tests.
</p>
<p>associated with this proportion. 
</p>
<p>priate statistics for describing the normal sampling distribution that is 
</p>
<p>pling distribution for our test. Why can&rsquo;t we just rely on the same logic
</p>
<p>to produce the mean and the standard error for a test comparing two
</p>
<p>proportions?
</p>
<p>must apply a t rather than a z distribution to his or her test. If the
</p>
<p>standard deviations for each distribution were known, it would not be
</p>
<p>portion by taking the product of P and Q; see Chapter 10, page 253.) But
</p>
<p>C O M P A R I N G S A M P L E P R O P O R T I O N S 283</p>
<p/>
</div>
<div class="page"><p/>
<p>The Case of Drug Testing and Pretrial Misconduct
</p>
<p>In a study conducted in Maricopa County, Arizona, criminal justice
</p>
<p>researchers examined whether drug testing of defendants released
</p>
<p>before trial had an impact on pretrial misconduct.7 They compared
</p>
<p>two groups of defendants who were released before trial. The first
</p>
<p>group was monitored through drug testing twice a week. The second
</p>
<p>group was released without subsequent drug testing. The sample 
</p>
<p>was chosen over a six-month period. The researchers identified sub-
</p>
<p>jects for the study through identification numbers kept in a computer-
</p>
<p>ized case-management system. Defendants with odd identification
</p>
<p>numbers were placed in the drug-testing group. Defendants with even
</p>
<p>identification numbers were placed in the control, or no-drug-testing,
</p>
<p>group. The drug-testing group had 118 subjects. The control group
</p>
<p>had 116 subjects.
</p>
<p>The researchers followed up on both of these groups of defendants
</p>
<p>for 90 days. One measure of pretrial misconduct was failure to appear at
</p>
<p>a hearing during the follow-up period. A total of 38% of the control
</p>
<p>group and 30% of the drug-testing group failed to appear at a hearing
</p>
<p>during this period. The question the researchers wanted to answer was
</p>
<p>whether they could infer from the difference between these two samples
</p>
<p>that there was in fact a difference in pretrial misconduct between the
</p>
<p>populations these samples represent. A two-sample t-test is an appropri-
</p>
<p>ate statistical test with which to answer this question.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale (failure to appear is measured as a
</p>
<p>proportion).
</p>
<p>Population Distribution: Normal distribution in both populations (re-
</p>
<p>laxed because N is large).
</p>
<p>Sampling Method: Independent random sampling (all cases for six
</p>
<p>months are selected).
</p>
<p>Sampling Frame: Defendants released before trial for a six-month period
</p>
<p>in Maricopa County, Arizona.
</p>
<p>7See Chester Britt III, Michael Gottfredson, and John S. Goldkamp, &ldquo;Drug Testing and
</p>
<p>Pretrial Misconduct: An Experiment on the Specific Deterrent Effects of Drug Monitor-
</p>
<p>ing Defendants on Pretrial Release,&rdquo; Journal of Research in Crime and Delinquency 29
</p>
<p>(1992): 62&ndash;78.
</p>
<p>necessary for the researcher to conduct a statistical test of significance
</p>
<p>at all. In this case, the value of the proportion for each of the two popu-
</p>
<p>lations would be known by implication.
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S284</p>
<p/>
</div>
<div class="page"><p/>
<p>Hypotheses:
</p>
<p>H0: The two populations do not differ in terms of the proportion who
</p>
<p>fail to appear for a pretrial hearing (P1 � P2).
</p>
<p>H1: Defendants subject to drug testing will be more likely to appear for
</p>
<p>a pretrial hearing (P1 � P2).
</p>
<p>The two-sample t-test requires an interval level of measurement, as
</p>
<p>well as a normal population distribution for each of the two samples ex-
</p>
<p>amined. The actual level of measurement for our example (as stated in
</p>
<p>parentheses in our assumptions) is nominal&mdash;we compare two propor-
</p>
<p>tions. As with a single-sample test of proportions, when the sample sizes
</p>
<p>are large, we can relax assumptions regarding the level of measurement
</p>
<p>used and the shape of the populations examined. Because we have two
</p>
<p>samples and not just one, the central limit theorem applies only if both
</p>
<p>samples are large. The definition of how large samples must be to in-
</p>
<p>voke the central limit theorem in the case of a difference of proportions
</p>
<p>test is a matter of debate. However, when each sample includes more
</p>
<p>than 100 cases, as is true for the Arizona study, there will be little argu-
</p>
<p>ment regarding the use of this parametric test for proportions.
</p>
<p>We must again assume that both samples are independent random
</p>
<p>samples. In our example, the researchers did not draw an independent
</p>
<p>random sample for either the drug-testing group or the control group.
</p>
<p>Rather, as was the case with the cell-allocation study examined in Chap-
</p>
<p>ter 9, they sampled all defendants released before trial in Maricopa
</p>
<p>County for a specific period of time&mdash;in this case, six months. In order to
</p>
<p>create the two samples, the researchers assigned the defendants to the
</p>
<p>groups according to their identification numbers: Even-numbered sub-
</p>
<p>jects were assigned to the control group, and odd-numbered subjects to
</p>
<p>the drug-testing group.
</p>
<p>In making statistical inferences, the researchers were clearly interested
</p>
<p>in inferring beyond their sampling frame (defendants released before
</p>
<p>8
</p>
<p>8In fact, although we do not examine their findings here, Britt and colleagues con-
</p>
<p>ducted their study in two Arizona counties.
</p>
<p>Maricopa County but also to other jurisdictions and other programs
</p>
<p>trial during the six-month period), not only to other time periods in
</p>
<p>likely to apply to other &ldquo;sophisticated and experienced pretrial services
</p>
<p>agencies.  They also noted that it &ldquo;is reasonable to assume that the
</p>
<p>programs that were implemented are comparable to the programs that
</p>
<p>are likely to be implemented in similar agencies.&rdquo; When drawing con-
</p>
<p>clusions from this research, we would have to consider whether the sample
</p>
<p>used can in fact be seen as representative of these larger populations.
</p>
<p>similar to the one they studied. They argued that their findings were
</p>
<p>&rdquo;
</p>
<p>C O M P A R I N G S A M P L E P R O P O R T I O N S 285</p>
<p/>
</div>
<div class="page"><p/>
<p>Our final assumptions relate to the hypotheses. The null hypothesis,
</p>
<p>as for earlier tests, is that there is no difference. It assumes that those
</p>
<p>monitored through drug testing and those not so monitored (the control
</p>
<p>group) will have the same proportion of defendants who fail to appear.
</p>
<p>Another way of expressing this is to say that the proportion of failures to
</p>
<p>appear in the drug-tested population (P1) of released defendants is the
</p>
<p>same as that in the population that is not drug-tested (P2), or that P1 �
</p>
<p>P2. The researchers chose a directional research hypothesis because they
</p>
<p>were concerned only with the possibility that the program decreased the
</p>
<p>likelihood that offenders would fail to appear for a pretrial hearing.
</p>
<p>Accordingly, the research hypothesis was stated as P1 � P2. The re-
</p>
<p>searchers were interested in testing whether drug testing would increase
</p>
<p>compliance.
</p>
<p>The Sampling Distribution Because N is large for both samples, we can
</p>
<p>use a t distribution as the sampling distribution for testing the difference
</p>
<p>between proportions. The number of degrees of freedom for the distri-
</p>
<p>bution is obtained by adding the numbers of cases in the two samples
</p>
<p>N1 � N2
</p>
<p>As we noted earlier, the null hypothesis defines the mean of the sam-
</p>
<p>pling distribution we will use for our test. The mean of the sampling dis-
</p>
<p>tribution is P1 � P2, or simply 0, because the null hypothesis states that
</p>
<p>the two population proportions are the same.
</p>
<p>In defining the standard error of the sampling distribution, we can
</p>
<p>rely on the pooled variance approach. This is always the case when we
</p>
<p>are examining differences between proportions. When we assume in the
</p>
<p>null hypothesis that the two population proportions are the same, then
</p>
<p>by implication we also assume that the two standard deviations for these
</p>
<p>population distributions are also equal. This fact derives from the
</p>
<p>method by which the population variances are calculated. As noted in
</p>
<p>Chapter 10, the variance of a proportion is
</p>
<p>�2 � PQ
</p>
<p>and the standard deviation of a proportion is
</p>
<p>Accordingly, if P is the same for two populations, then we can also as-
</p>
<p>sume that the variances of those populations are equal. In statistical
</p>
<p>� � �PQ
</p>
<p>� 2. In our example, the number of and subtracting 2: df �
</p>
<p>degrees of freedom equals 118 � 116 � 2, or 232.
</p>
<p>terms, as we noted earlier, this is defined as the assumption of homos-
</p>
<p>cedasticity.
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S286</p>
<p/>
</div>
<div class="page"><p/>
<p>Significance Level and Rejection Region The researchers in the Mari-
</p>
<p>copa County study decided to use &ldquo;conventional levels&rdquo; of statistical
</p>
<p>significance&mdash;that is, a rejection region of � � 0.05. Following their re-
</p>
<p>search hypothesis, they also used a one-tailed test of statistical signifi-
</p>
<p>cance. Given that we have 232 degrees of freedom, a significance level
</p>
<p>of 0.05, and a one-tailed test, we can consult the t distribution table
</p>
<p>and determine that the critical value for this analysis is �1.654. If the
</p>
<p>this test.
</p>
<p>The Test Statistic To define the t-score appropriate for our test, we
</p>
<p>must alter the formula for a t-test of means to take into account the
</p>
<p>fact that we are examining sample proportions. Accordingly, we re-
</p>
<p>place the difference between the sample means with the dif-
</p>
<p>ference between the sample proportions (p1 � p2). We also replace
</p>
<p>the assumed differences between the population means with (P1 �
</p>
<p>P2), as stated by the null hypothesis. Because the null hypothesis
</p>
<p>states that the two populations are equal, this quantity is equal to 0.
</p>
<p>Equation 11.7 presents a modified formula for calculating the t-statistic
</p>
<p>for proportions.
</p>
<p>Equation 11.7
</p>
<p>Below we calculate the t-statistic for our test. Note that we must 
</p>
<p>first calculate the variance for each of the sample proportions. To cal-
</p>
<p>culate the variance of a sample proportion, we use the formula 
</p>
<p>s2 � pq. This formula is identical to that presented above for the
</p>
<p>variance of a population proportion. The only difference here is the
</p>
<p>use of the symbols p, q, and s to represent sample rather than popula-
</p>
<p>tion estimates. For our example, � (0.3)(0.7) � 0.21 and �
</p>
<p>(0.38)(0.62) � 0.2356.
</p>
<p>After inserting the values for the two sample variances and into
</p>
<p>Equation 11.7, we obtain a t-statistic equal to �1.29.
</p>
<p>s 22)(s 
2
1
</p>
<p>s 22s 
2
1
</p>
<p> � 
(p1 � p2) � (P1 � P2)
</p>
<p>� N1s 
2
1 � N2s 
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
 �N1 � N2N1N2
</p>
<p> t � 
(p1 � p2) � (P1 � P2)
</p>
<p>�̂�N1 � N2N1N2
,  where �̂ � � N1s 
</p>
<p>2
1 � N2s 
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
</p>
<p>(X1 � X2)
</p>
<p>test statistic is less than �1.654, then it falls in the rejection region for
</p>
<p>C O M P A R I N G S A M P L E P R O P O R T I O N S 287</p>
<p/>
</div>
<div class="page"><p/>
<p>The Decision Because our test statistic of �1.29 is greater than the criti-
</p>
<p>cal value of �1.654, we fail to reject the null hypothesis that there is no
</p>
<p>difference in failure to appear at hearings for the drug testing and control
</p>
<p>populations. Based on these and other similar results, the researchers in
</p>
<p>this study concluded that &ldquo;systematic drug testing and monitoring in the
</p>
<p>pretrial setting, in programs such as those described above [i.e., exam-
</p>
<p>ined in this research], is not likely to achieve significant&rdquo; change in pre-
</p>
<p>trial misconduct.
</p>
<p>T h e  t - T e s t  f o r  D e p e n d e n t  S a m p l e s
</p>
<p>One of the requirements of the two-sample t-test is that the samples ex-
</p>
<p>W orking It Out Pooled Variance Method
</p>
<p> � �1.29
</p>
<p> � 
�0.08
0.062
</p>
<p> � 
(0.30 � 0.38) � 0
</p>
<p>�(118)(0.21) � (116)(0.2356)118 � 116 � 2  �118 � 116(118)(116)
</p>
<p> � 
(p1 � p2) � (P1 � P2)
</p>
<p>� N1s
2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
 �N1 � N2N1N2
</p>
<p> t � 
(p1 � p2) � (P1 � P2)
</p>
<p>�̂�N1 � N2N1N2
, � where �̂ � � N1s
</p>
<p>2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
</p>
<p>amined be independent. However, sometimes criminal justice resear-
</p>
<p>chers examine samples that are not independent. For example, subjects
</p>
<p>may be matched and placed in like pairs based on such characteristics
</p>
<p>as social status, education, gender, age, and IQ. Dependent samples
</p>
<p>will also result when a researcher takes measurements on the same
</p>
<p>subject or unit of analysis over time. For example, a researcher may
</p>
<p>examine the attitudes of juvenile delinquents before and after partici-
</p>
<p>pation in a specific program or may study changes at specific crime
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S288</p>
<p/>
</div>
<div class="page"><p/>
<p>hot spots before and after some type of police intervention. Sometimes
</p>
<p>the same individuals are compared at different ages or stages in their de-
</p>
<p>velopment. Even though in such instances the researcher has two sam-
</p>
<p>ples of observations&mdash;for example, before and after the program&mdash;the
</p>
<p>samples are not independent.
</p>
<p>The t-test for dependent samples is commonly used in such situa-
</p>
<p>tions.9 It focuses on the differences between the pairs in developing the
</p>
<p>sampling distribution of the test statistic. Each pair in a t-test for depen-
</p>
<p>dent samples is considered a single observation.
</p>
<p>The Effect of Police Presence Near High-Crime Addresses
</p>
<p>Let&rsquo;s suppose that a police department took an independent random
</p>
<p>sample of 35 high-crime addresses from all high-crime addresses in a
</p>
<p>city. The department then assigned a police officer to walk the beat on
</p>
<p>each block where one of the addresses was located for a full month. As-
</p>
<p>sume we are asked to assess whether the strategy was effective in reduc-
</p>
<p>ing calls for police service. We have data on the number of emergency
</p>
<p>calls for police service for the month before the officer walked the beat
</p>
<p>and for the month during which the officer walked the beat. These data
</p>
<p>are given in Table 11.1. The mean number of calls for service the month
</p>
<p>before was 30, and the mean for the month when the officer was present
</p>
<p>was 20. Can we conclude from this that the program would be effective
</p>
<p>in reducing calls for service if applied generally to high-crime addresses?
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution (relaxed because N is large).
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: All high-crime addresses in the city.
</p>
<p>Hypotheses:
</p>
<p>H0: There is no difference in the number of calls for service at high-
</p>
<p>crime addresses whether an officer walks the beat or not (�1 � �2).
</p>
<p>1
</p>
<p>1 � �2).
</p>
<p>9Here we examine the t-test for dependent samples only in reference to mean differ-
</p>
<p>ences for interval-level data. However, this test may also be used for dichotomous
</p>
<p>nominal-level data. Suppose you were assessing the absence or presence of some char-
</p>
<p>acteristic or behavior at two points in time. If each observation were coded as 0 or 1,
</p>
<p>then you would calculate the mean difference and the standard deviation of the
</p>
<p>difference (sd) using the same equations as in this section. The only difference from the
</p>
<p>example discussed in the text is that you would work only with zeros and ones.
</p>
<p>(Xd)
</p>
<p>H : There are fewer calls for service at high-crime addresses when an 
</p>
<p>officer walks the beat (�
</p>
<p>T H E t - T E S T F O R D E P E N D E N T S A M P L E S 289</p>
<p/>
</div>
<div class="page"><p/>
<p>Number of calls for service is an interval-scale measure, as required
</p>
<p>by the t-test. The test also requires that the population of differences be-
</p>
<p>tween pairs be normally distributed. Because our sample is large (greater
</p>
<p>than 30), we are able to relax this assumption for our test.
</p>
<p>Our null hypothesis states that there is no difference in the number of
</p>
<p>calls for police service at high-crime addresses whether a police officer
</p>
<p>walks the beat or not. Because the police department is concerned only
</p>
<p>with whether the presence of a police officer walking the beat is effec-
</p>
<p>tive in reducing emergency calls for service, we use a directional re-
</p>
<p>search hypothesis. It states that the number of calls for police service will
</p>
<p>be lower when a police officer walks the beat.
</p>
<p>Emergency Calls to Police for the Month Before 
and the Month During Which an Officer Walked the Beat
</p>
<p>CALLS CALLS CALLS CALLS
</p>
<p>LOCATION BEFORE DURING LOCATION BEFORE DURING
</p>
<p>1 29 14 19 18 22
2 50 28 20 27 24
3 14 8 21 42 16
4 16 6 22 31 14
5 11 20 23 51 30
6 31 17 24 28 8
7 33 4 25 26 11
8 37 22 26 14 19
9 21 20 27 29 21
</p>
<p>10 40 27 28 39 26
11 30 29 29 40 20
12 22 30 30 30 20
13 30 18 31 26 11
14 36 20 32 30 28
15 30 22 33 27 13
16 29 26 34 33 20
17 24 19 35 35 34
18 41 33 � 1,050 700
</p>
<p>30 20
s 9.21 7.52
X
</p>
<p>Table 11.1
</p>
<p>A t-test for dependent samples requires that the pairs examined be 
</p>
<p>selected randomly and independently from the target population of 
</p>
<p>dependent (i.e., they are related to one another), the pairs themselves 
</p>
<p>are independent from one another. Because we began with an inde-
</p>
<p>pendent random sample of high-crime addresses, we can assume that 
</p>
<p>the paired observations taken before and during the police interven-
</p>
<p>tion are random and independent.
</p>
<p>pairs. Accordingly, although the scores for the subjects in the pairs are 
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S290</p>
<p/>
</div>
<div class="page"><p/>
<p>The Sampling Distribution The number of degrees of freedom for a t-test
</p>
<p>for dependent samples is obtained by taking the number of pairs studied
</p>
<p>and subtracting 1: df � N � 1. In our example, which involves paired
</p>
<p>observations for the same subjects over two time periods, the number of
</p>
<p>degrees of freedom is 35 � 1, or 34. If we had examined subjects
</p>
<p>The mean of the sampling distribution is defined by the null hypothe-
</p>
<p>sis. It is represented by �d, or the mean of the population of differences
</p>
<p>between crime calls when a police officer is and is not walking the beat.
</p>
<p>Because the null hypothesis states that there is no difference in crime
</p>
<p>calls during these time periods, �d for the sampling distribution is 0. The
</p>
<p>estimated standard error of the sampling distribution is found using
</p>
<p>Equation 11.8.
</p>
<p>Equation 11.8
</p>
<p>Using the estimates of variance calculated in Table 11.2, we find that the
</p>
<p>estimated standard error for the sampling distribution is 1.559.
</p>
<p>�̂sd � � s
2
d
</p>
<p>N � 1
</p>
<p>(�̂sd)
</p>
<p>Significance Level and Rejection Region Following conventional norms,
</p>
<p>we use a 0.05 level of statistical significance. However, our research hy-
</p>
<p>pothesis suggests a directional test, which means that we place the entire
</p>
<p>rejection region on one side of the t distribution. Because the research
</p>
<p>hypothesis states that the period during which a police officer is walking
</p>
<p>the beat will have a lower number of crime calls, we are interested in the
</p>
<p>negative area of the t distribution below the mean. Looking at the t table
</p>
<p>(Appendix 4) under a one-tailed significance test with 34 degrees of free-
</p>
<p>dom, we see that a critical value of t less than �1.691 is needed to place
</p>
<p>the score in the rejection region.
</p>
<p>W orking It Out
</p>
<p> � 1.5595
</p>
<p> � �82.685735 � 1
</p>
<p> �̂sd �� s
2
d
</p>
<p>N � 1
</p>
<p>matched by common traits, the degrees of freedom would also be
</p>
<p>obtained by taking the number of pairs of subjects and subtracting 1.
</p>
<p>T H E t - T E S T F O R D E P E N D E N T S A M P L E S 291</p>
<p/>
</div>
<div class="page"><p/>
<p>The Test Statistic To define the t-score appropriate for our test, we alter
</p>
<p>Equation 10.9 (used for the single-sample test of means in Chapter 10) to
</p>
<p>take into account the fact that we are now concerned with the mean differ-
</p>
<p>ence between pairs of observations. Accordingly, instead of comparing the
</p>
<p>mean of one sample to the hypothesized population parameter ,
</p>
<p>we now compare the mean of the observed differences between the pairs
</p>
<p>with the hypothesized difference between the pairs based on the null hy-
</p>
<p>pothesis . As discussed in the section on the sampling distribu-
</p>
<p>tion, the hypothesized difference is 0. We also adjust the denominator of
</p>
<p>the equation to reflect the standard error of the differences between the
</p>
<p>pairs of observations. The revised formula is presented in Equation 11.9.
</p>
<p>Equation 11.9t � 
Xd � �d
</p>
<p>� s
2
d
</p>
<p>N � 1
</p>
<p>(Xd � �d)
</p>
<p>(X � �)
</p>
<p>Calculation of the Standard Deviation for the Differences Between 
the Two Sets of Dependent Observations
</p>
<p>DIFFER- DIFFER-
</p>
<p>LOCATION DURING BEFORE ENCE Xi LOCATION DURING BEFORE ENCE Xi
</p>
<p>1 14 29 �15 25 19 22 18 �4 196
2 28 50 �22 144 20 24 27 �3 49
3 8 14 �6 16 21 16 42 �26 256
4 6 16 �10 0 22 14 31 �17 49
5 20 11 �9 361 23 30 51 �21 121
6 17 31 �14 16 24 8 28 �20 100
7 4 33 �29 361 25 11 26 �15 25
8 22 37 �15 25 26 19 14 �5 225
9 20 21 �1 81 27 21 29 �8 4
</p>
<p>10 27 40 �13 9 28 26 39 �13 9
11 29 30 �1 81 29 20 40 �20 100
12 30 22 �8 324 30 20 30 �10 0
13 18 30 �12 4 31 11 26 �15 25
14 20 36 �16 36 32 28 30 �2 64
15 22 30 �8 4 33 13 27 �14 16
16 26 29 �3 49 34 20 33 �13 9
17 19 24 �5 25 35 34 35 �1 81
18 33 41 �8 4 Totals (�) �350 2,894
</p>
<p>� 82.6857� �10
</p>
<p>� 
2,894
</p>
<p>35
� 
</p>
<p>�350
</p>
<p>35
</p>
<p>s2d � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � Xd)
2
</p>
<p>N
Xd � 
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N
</p>
<p>(Xi � Xd)
2(Xi � Xd)
</p>
<p>2
</p>
<p>Table 11.2
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S2 29</p>
<p/>
</div>
<div class="page"><p/>
<p>Substituting the values from our example, we obtain a t-score of �6.412.
</p>
<p>10See Chapter 12 for an example of a rank-order test (the Kruskal-Wallis one-way
</p>
<p>analysis of variance).
</p>
<p>W orking It Out
</p>
<p> � �6.4123
</p>
<p> � 
�10 � 0
</p>
<p>�82.685735 � 1
</p>
<p> t � 
Xd � �d
</p>
<p>� s
2
d
</p>
<p>N � 1
</p>
<p>The Decision Because our test statistic of �6.4123 is less than the crit-
</p>
<p>ical value of our rejection region, �1.691, we reject the null hypothe-
</p>
<p>sis. The observed significance level of our test is less than the criterion
</p>
<p>significance level we set when defining the significance level and
</p>
<p>rejection region for our test (p � 0.05). We can conclude that there 
</p>
<p>is a statistically significant decrease in the number of calls for police
</p>
<p>services at high-crime addresses when a police officer is walking 
</p>
<p>the beat.
</p>
<p>A  N o t e  o n  U s i n g  t h e  t - T e s t  f o r  O r d i n a l  S c a l e s
</p>
<p>Ordinal scales create a special problem in conducting tests of statistical
</p>
<p>significance. Most tests we have examined so far assume either a nomi-
</p>
<p>nal or an interval level of measurement. There are nonparametric tests
</p>
<p>for ordinal-level measures; however, these generally assume that the re-
</p>
<p>searcher can rank order all scores in a distribution.10 Typically, with ordi-
</p>
<p>nal measures, there are a limited number of ordinal categories and many
</p>
<p>observations, so such tests are not appropriate.
</p>
<p>There is no simple guideline for deciding which test to use for ordinal-
</p>
<p>scale variables. In practice, when there are a number of categories in an
</p>
<p>T H E t - T E S T E 293 O N U S I N G F O R O R D I N A L  S C A L E S A  N O T</p>
<p/>
</div>
<div class="page"><p/>
<p>ordinal scale, researchers use the t-test for means to calculate statistical
</p>
<p>significance. When N is large and the number of categories is more than
</p>
<p>five, this approach is generally accepted. However, you should keep 
</p>
<p>in mind when you use this approach that the t-test assumes not only 
</p>
<p>that the categories in the scale are ordered but also that the intervals
</p>
<p>represented by the categories are equal for all of the categories. To 
</p>
<p>the extent that this can be assumed, you will be on more solid ground
</p>
<p>using the t-test for ordinal scales. When the number of categories is less
</p>
<p>than five, it may be better to use the chi-square statistic, discussed in
</p>
<p>Chapter 9. In Chapter 13, we will discuss other descriptive statistics and
</p>
<p>associated significance tests that are often appropriate for ordinal-level
</p>
<p>scales.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>The two-sample t-test is a parametric test of statistical significance that
</p>
<p>may be used to test for equality of two population means or propor-
</p>
<p>tions. Although the test requires an interval level of measurement and
</p>
<p>normal population distributions, it is nonetheless appropriate for un-
</p>
<p>Like other tests of statistical significance examined in previous chap-
</p>
<p>ters, the two-sample t-test requires independent random sampling. The
</p>
<p>null hypothesis states that the population means or proportions for the
</p>
<p>two samples studied are the same. A critical value for the test is identi-
</p>
<p>fied on the t sampling distribution, after first determining the degrees
</p>
<p>of freedom. The mean of the sampling distribution is again defined
</p>
<p>with reference to the null hypothesis. There are two options for calcu-
</p>
<p>lating the standard error of the sampling distribution for a difference of
</p>
<p>means test. The first is termed the pooled variance method; it gener-
</p>
<p>ally provides a more efficient statistical estimate but requires the addi-
</p>
<p>tional assumption of homoscedasticity&mdash;that the standard deviations
</p>
<p>variance method because it does not make an assumption about the
</p>
<p>equality of variances between the two population distributions. For a
</p>
<p>difference of proportions test, the pooled variance method is always
</p>
<p>used.
</p>
<p>When the two samples examined are not independent, the t-test for
</p>
<p>dependent samples should be used. The calculation of this statistic is
</p>
<p>based on the mean difference between pairs of samples and the standard
</p>
<p>deviation of the differences between the pairs.
</p>
<p>of the two populations are the same. The second is called the separate
</p>
<p>both samples.
</p>
<p>known populations and binary variables when N is sufficiently large for 
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S294</p>
<p/>
</div>
<div class="page"><p/>
<p>K e y  T e r m s
</p>
<p>homoscedasticity A statement that the
</p>
<p>variances and standard deviations of two or
</p>
<p>more populations are the same.
</p>
<p>pooled variance A method of obtaining
</p>
<p>the standard error of the sampling distribu-
</p>
<p>tion for a difference of means test. The
</p>
<p>pooled variance method requires an as-
</p>
<p>sumption of homoscedasticity.
</p>
<p>separate variance A method of obtaining
</p>
<p>the standard error of the sampling distribu-
</p>
<p>tion for a difference of means test. The
</p>
<p>separate variance method does not require
</p>
<p>an assumption of homoscedasticity.
</p>
<p>t-test for dependent samples A test of
</p>
<p>statistical significance that is used when
</p>
<p>two samples are not independent.
</p>
<p>two-sample t-test A test of statistical sig-
</p>
<p>nificance that examines the difference ob-
</p>
<p>served between the means or proportions
</p>
<p>of two samples.
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>Estimate of the standard deviation of a population
</p>
<p>Estimate of the standard error of a sampling distribution
</p>
<p>To calculate degrees of freedom for the two-sample t-test:
</p>
<p>df � N1 � N2 � 2
</p>
<p>To calculate an unbiased estimate of the standard error for the sampling
</p>
<p>distribution in a two-sample t-test (separate variance method):
</p>
<p>To estimate a pooled joint standard deviation of two populations for the
</p>
<p>pooled variance method:
</p>
<p>To estimate the standard error for the sampling distribution in a two-
</p>
<p>sample t-test (pooled variance method):
</p>
<p>�̂sd(X1 � X2) � �̂�N1 � N2N1N2
</p>
<p>�̂ � � N1s
2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
</p>
<p>�̂sd(X1 � X2) � � s
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s22
N2 � 1
</p>
<p>�̂sd
</p>
<p>�̂
</p>
<p>S Y M B O L S A N D F O R M U L A S 295</p>
<p/>
</div>
<div class="page"><p/>
<p>To calculate the two-sample t-test statistic for means (separate variance
</p>
<p>method):
</p>
<p>To calculate the two-sample t-test statistic for means (pooled variance
</p>
<p>method):
</p>
<p>To calculate the variance of a sample proportion:
</p>
<p>s2 � pq
</p>
<p>To calculate the two-sample t-test statistic for proportions:
</p>
<p>To calculate the two-sample t-test statistic for means of dependent
</p>
<p>samples:
</p>
<p>E x e r c i s e s
</p>
<p>11.1 Test the following pairs of hypotheses using the information given.
Assume that the variable has been measured at the interval level and
the cases have been selected at random. For each test, answer the fol-
lowing questions:
</p>
<p>&mdash; Does the test require a one-tailed or a two-tailed test of statistical
significance?
</p>
<p>&mdash; What is (are) the critical value(s) for the stated level of significance?
</p>
<p>&mdash; What is the value of the test statistic?
</p>
<p>&mdash; What is the decision regarding the null hypothesis?
</p>
<p>t � 
Xd � �d
</p>
<p>� s
2
d
</p>
<p>N � 1
</p>
<p>t � 
(p1 � p2) � (P1 � P2)
</p>
<p>� N1s
2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2
 �N1 � N2N1N2
</p>
<p>t � 
(X1 � X2) � (�1 � �2)
</p>
<p>�̂�N1 � N2N1N2
</p>
<p>t � 
(X1 � X2) � (�1 � �2)
</p>
<p>� s
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s22
N2 � 1
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S296</p>
<p/>
</div>
<div class="page"><p/>
<p>&mdash; Does the sample size suggest the need for caution in drawing
conclusions?
</p>
<p>a. H0: �1 � �2
H1: �1 � �2
� � 0.05
</p>
<p>s1 � 4 s2 � 6
N1 � 14 N2 � 18
</p>
<p>b. H0: �1 � �2
H1: �1 � �2
� � 0.01
</p>
<p>s1 � 8 s2 � 10
N1 � 41 N2 � 41
</p>
<p>c. H0: �1 � �2
H1: �1 � �2
� � 0.05
</p>
<p>s1 � 6 s2 � 6
N1 � 122 N2 � 215
</p>
<p>d. H0: �1 � �2
H1: �1 � �2
� � 0.05
</p>
<p>s1 � 2 s2 � 3
N1 � 29 N2 � 33
</p>
<p>e. H0: �1 � �2
H1: �1 � �2
� � 0.01
</p>
<p>s1 � 35 s2 � 25
N1 � 513 N2 � 476
</p>
<p>f. H0: �1 � �2
H1: �1 � �2
� � 0.05
</p>
<p>s1 � 1 s2 � 2
N1 � 85 N2 � 93
</p>
<p>11.2 Greg wishes to investigate whether there is any difference in the
amount of violent crowd behavior that supporters of two soccer teams
report having seen in one season. He distributes questionnaires at ran-
dom to season-ticket holders at United and at City. The mean number
</p>
<p>X2 � 4X1 � 2
</p>
<p>X2 � 32X1 � 45
</p>
<p>X2 � 6X1 � 15
</p>
<p>X2 � 28X1 � 33
</p>
<p>X2 � 20X1 � 10
</p>
<p>X2 � 30X1 � 24
</p>
<p>E X E R C I S E S 297</p>
<p/>
</div>
<div class="page"><p/>
<p>of matches at which the sample of 110 United fans remember seeing
violent incidents is 15 (s � 4.7). For the sample of 130 City fans, the
mean number of such matches is 8 (s � 4.2).
</p>
<p>a. Can Greg conclude that there are differences in the amount of
violent crowd behavior observed between the two populations of
season-ticket holders? Outline all the steps required in the test of sta-
tistical significance. Choose an appropriate level of significance and
calculate the t-test statistic according to the separate variance
method.
</p>
<p>b. Would Greg&rsquo;s conclusion be any different if he were to use the
pooled variance method?
</p>
<p>c. Which of the two methods is preferred in this case?
</p>
<p>11.3 To see if there is truth in the claim by a prominent graduate of the
police academy that white officers are awarded more promotions
than African American officers, an independent random sample is
drawn from the 1,000 police officers in Bluesville who graduated
from the academy ten years earlier. For the 42 white officers sam-
pled, the mean number of promotions received in the ten years
since graduation was 3.2 (s � 0.8). For the 20 African American
officers sampled, the mean number of promotions received was 2.8
(s � 0.65).
</p>
<p>a. From these data, can you conclude that white officers who gradu-
ated ten years ago have been awarded more promotions than their
African American counterparts? Use the separate variance method
and set a 5% significance level.
</p>
<p>b. Would your answer be any different if you used the pooled vari-
ance method?
</p>
<p>c. If the level of significance had been set at 1%, would there be any
difference in the decisions you would make based on the separate
variance and pooled variance methods?
</p>
<p>d. Does the sample size have any relevance to the extent to which
you can rely on the results?
</p>
<p>11.4 By surveying a random sample of 100 students from Partytime High
School and 100 students from Funtime High School, a researcher
learns that those from Partytime High School have smoked marijuana
an average of 9.8 times (s � 4.2) in the last six months, while those
from Funtime High School have smoked marijuana an average of 4.6
times (s � 3.6) in the last six months. Can the researcher conclude
that use of marijuana differs between Partytime and Funtime high
schools? Use the separate variance method and set a significance level
of 0.01. Be sure to state the assumptions of the statistical test.
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S298</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Test the following pairs of hypotheses using the information 
given. Assume that the variable has been measured at the nominal
level, the value reported is the proportion, and the cases have 
been selected at random. For each test, answer the following
questions:
</p>
<p>&mdash; Does the test require a one-tailed or a two-tailed test of statistical
significance?
</p>
<p>&mdash; What is (are) the critical value(s) for the stated level of significance?
</p>
<p>&mdash; What is the value of the test statistic?
</p>
<p>&mdash; What is the decision regarding the null hypothesis?
</p>
<p>&mdash; Does the sample size suggest the need for caution in drawing
conclusions?
</p>
<p>a. H0: P1 � P2
H1: P1 � P2
� � 0.05
p1 � 0.80 p2 � 0.60
N1 � 6 N2 � 8
</p>
<p>b. H0: P1 � P2
H1: P1 � P2
� � 0.01
p1 � 0.73 p2 � 0.75
N1 � 211 N2 � 376
</p>
<p>c. H0: P1 � P2
H1: P1 � P2
� � 0.05
p1 � 0.46 p2 � 0.54
N1 � 86 N2 � 76
</p>
<p>d. H0: P1 � P2
H1: P1 � P2
� � 0.01
p1 � 0.28 p2 � 0.23
N1 � 192 N2 � 161
</p>
<p>e. H0: P1 � P2
H1: P1 � P2
� � 0.01
p1 � 0.12 p2 � 0.10
N1 � 57 N2 � 45
</p>
<p>f. H0: P1 � P2
H1: P1 � P2
� � 0.05
p1 � 0.88 p2 � 0.94
N1 � 689 N2 � 943
</p>
<p>E X E R C I S E S 299</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 After a long political battle, certain categories of prisoners in Rainy
State have been given the right to vote in upcoming local elections.
Carolyn wishes to know whether there is any difference between the
proportion of eligible prisoners and the proportion of eligible nonpris-
oners in Rainy State who will take advantage of their right to vote. She
draws two random independent samples&mdash;one of 125 prisoners, and
the other of 130 nonprisoners. The samples are drawn from the entire
eligible prisoner and nonprisoner populations of Rainy State. She finds
that 60% of her prisoner sample and 44% of her nonprisoner sample
intend to vote.
</p>
<p>a. Why is a statistical test of significance necessary here?
</p>
<p>b. Carry out a test of statistical significance, remembering to outline
each step carefully. Can Carolyn conclude that the two populations
are different in terms of their respective members&rsquo; intentions to
vote?
</p>
<p>11.7 Eric takes a random sample of 200 offenders convicted of bribery and
a random sample of 200 offenders convicted of robbery over the past
five years in Sunny State. By checking court records, he finds that 9%
of the bribery offenders and 1% of the robbery offenders in his sam-
ples have university educations.
</p>
<p>a. By using a two-tailed test with a significance level of 0.01, can 
Eric conclude that the differences he observes are statistically
significant?
</p>
<p>b. What steps would you recommend that Eric take if he wishes to ex-
tend his conclusions to the prisoner population of neighboring
Rainy State?
</p>
<p>11.8 Three hundred prisoners, all convicted of violent crimes against
persons, have enrolled in a six-month course in anger control. A
random sample of 41 of the prisoners are chosen to complete the
same questionnaire on two separate occasions&mdash;once during the
first lesson and once during the last lesson. The questionnaire
measures how likely respondents are to resort to violence to solve
problems. The results are translated into an index from 0 to 10, with
higher scores indicating a higher tendency to seek nonviolent
solutions to problems. The 41 prisoners&rsquo; scores are shown in the
</p>
<p>a. What is the mean change in scores?
</p>
<p>b. What is the standard deviation for the differences between 
scores?
</p>
<p>c. Carry out a test of statistical significance, remembering to outline all
of the steps required by the test. Can you reject the null hypothesis
for the test on the basis of the differences observed?
</p>
<p>table on page 301.
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S300</p>
<p/>
</div>
<div class="page"><p/>
<p>First Last First Last
</p>
<p>Subject Lesson Lesson Subject Lesson Lesson
</p>
<p>1 1 2 22 6 6
</p>
<p>2 2 4 23 4 4
</p>
<p>3 1 6 24 9 6
</p>
<p>4 3 2 25 9 7
</p>
<p>5 4 5 26 0 1
</p>
<p>6 7 9 27 1 4
</p>
<p>7 6 6 28 1 4
</p>
<p>8 4 3 29 3 3
</p>
<p>9 4 7 30 2 2
</p>
<p>10 1 1 31 2 4
</p>
<p>11 2 1 32 1 1
</p>
<p>12 3 4 33 0 3
</p>
<p>13 4 9 34 4 5
</p>
<p>14 6 7 35 4 5
</p>
<p>15 7 8 36 6 7
</p>
<p>16 2 2 37 6 6
</p>
<p>17 2 7 38 7 7
</p>
<p>18 3 3 39 3 6
</p>
<p>19 1 4 40 1 1
</p>
<p>20 6 4 41 1 2
</p>
<p>21 2 4
</p>
<p>11.9 A random sample of adults in a midwestern state were interviewed
twice over a period of two years. Each time, as part of the survey,
they were asked how many times their home had been burglarized
in the previous 12 months. The numbers of burglaries reported by
27 of the respondents in each interview are shown in the table on
</p>
<p>a. What is the mean change in scores?
</p>
<p>b. What is the standard deviation for the differences between 
scores?
</p>
<p>c. Carry out a test of statistical significance. Use a significance level of
5% and outline all of the steps required by the test. Can you reject
the null hypothesis for the test on the basis of the differences
observed?
</p>
<p>d. Would your answer have been any different if you had used a sig-
nificance level of 1%? Explain why.
</p>
<p>page 302.
</p>
<p>E X E R C I S E S 301</p>
<p/>
</div>
<div class="page"><p/>
<p>Respondent First Interview Second Interview
</p>
<p>1 0 2
</p>
<p>2 1 1
</p>
<p>3 2 0
</p>
<p>4 1 0
</p>
<p>5 0 1
</p>
<p>6 0 0
</p>
<p>7 3 0
</p>
<p>8 0 0
</p>
<p>9 0 1
</p>
<p>10 5 1
</p>
<p>11 2 2
</p>
<p>12 2 2
</p>
<p>13 1 0
</p>
<p>14 1 0
</p>
<p>15 0 0
</p>
<p>16 0 2
</p>
<p>17 0 1
</p>
<p>18 0 1
</p>
<p>19 1 3
</p>
<p>20 0 1
</p>
<p>21 1 0
</p>
<p>22 2 0
</p>
<p>23 0 0
</p>
<p>24 0 0
</p>
<p>25 2 1
</p>
<p>26 0 2
</p>
<p>27 1 0
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>The commands for computing independent (two-sample) and dependent  samples 
</p>
<p>t-tests are generally straightforward in both SPSS and Stata. As in previous 
</p>
<p> chapters, we have included syntax files for both SPSS (Chapter_11.sps) and Stata 
</p>
<p>(Chapter_11.do) that contain examples of the commands we illustrate below.
</p>
<p>SPSS
</p>
<p>Independent Samples t-Test
</p>
<p>The two-sample t-test for differences in group means is performed by using the 
</p>
<p>T-TEST command:
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S302
</p>
<p>T-TEST GROUPS = grouping_variable(#1 #2)
</p>
<p>/VARIABLES = variable_name(s).</p>
<p/>
</div>
<div class="page"><p/>
<p>where the GROUPS= indicates the variable containing information on the 
</p>
<p>groups being tested, which are then listed within the parentheses. A nice fea-
</p>
<p>ture of this option is presented when a categorical variable has three or more 
</p>
<p> categories, say a measure of race&ndash;ethnicity, and the interest is in comparing any 
</p>
<p>two of the groups, such as the 3rd and the 5th groups. We would then insert 
</p>
<p>the values &ldquo;3&rdquo; and &ldquo;5&rdquo; into the parentheses to designate these as the groups. All 
</p>
<p>other cases would be dropped from that particular run of the T-TEST command. 
</p>
<p>The /VARIABLES= option is where you would list all of the interval-level 
</p>
<p> variables that you are interested in testing for differences across the two groups.
</p>
<p>The output generated by the T-TEST command will contain two lines in the 
</p>
<p>main results table. The first line will present the t-test results using the pooled 
</p>
<p>variance method, while the second line will present the t-test results using the 
</p>
<p>separate variance method.
</p>
<p>Dependent Samples t-Test
</p>
<p>A dependent samples t-test is obtained using the T-TEST command but using 
</p>
<p>the PAIRS option to specify the type of t-test to be performed:
</p>
<p>The output generated by executing a dependent samples t-test will show the 
</p>
<p>the standard deviation of the difference, and the corresponding t-statistic.
</p>
<p>Stata
</p>
<p>Independent Samples t-Test
</p>
<p>This command assumes that the variances are equal. To run the ttest command 
</p>
<p>assuming unequal variance, add the option unequal to the command line:
</p>
<p>In the output generated by this command, Stata will show the results for both 
</p>
<p>one-tail as well as two-tail t-test for differences in the group means.
</p>
<p>Dependent Samples t-Test
</p>
<p>The syntax for a dependent samples t-test in Stata is remarkably simple:
</p>
<p>C O M P U T E R E X E R C I S E S 303
</p>
<p>The order the variables are listed in the PAIRS= option is important and should 
</p>
<p>match exactly the order of the variables as they appear in your null and research 
</p>
<p>hypotheses.
</p>
<p>The two-sample t-test for differences in group means is performed by using the 
</p>
<p>ttest command:
</p>
<p>T-TEST PAIRS = var_1_name WITH var_2_name (PAIRED).
</p>
<p>ttest variable_name, by(grouping_variable)
</p>
<p>ttest variable_name, by(grouping_variable) unequal
</p>
<p>ttest var_1_name == var_2_name</p>
<p/>
</div>
<div class="page"><p/>
<p>The key to the command syntax is the double equal sign (==) between the 
</p>
<p>two variable names. Similar to the SPSS command, the order of the variables 
</p>
<p>is important, and they should be listed here in exactly the same order as they 
</p>
<p>appear in the null and research hypotheses.
</p>
<p>Problems
</p>
<p> 1. Open the LA bail data (labail.sav or labail.dta). This data file  
</p>
<p>contains the values used for the bail example in the text. There are only 
</p>
<p>two variables included in this data set: bail amount and race (coded as 
</p>
<p>t-test command to  
</p>
<p>t-test command to test the hypotheses listed below. For each hypothesis test, 
</p>
<p>use a significance level of  5 %, state each of  the assumptions, and explain 
</p>
<p>whether there is a statistically significant difference between the two groups.
</p>
<p>a. The number of  times a youth has stolen something valued at $5 to $50 
</p>
<p>is different for whites and African Americans.
</p>
<p>b. The number of  times a youth has smoked marijuana is greater for males 
</p>
<p>than for females.
</p>
<p>c. The number of  times a youth has physically attacked another student is 
</p>
<p>different for males and females.
</p>
<p>d. The number of  times a youth has hit his or her parents is greater for 
</p>
<p>e. The number of  times a youth has cheated on schoolwork is greater for 
</p>
<p>students earning mostly C&rsquo;s than for students earning mostly A&rsquo;s.
</p>
<p>dta). Use a t-test command to test the hypotheses listed below. For each 
</p>
<p>hypothesis test, use a significance level of  5%, state each of  the  
</p>
<p>assumptions, and explain whether there is a statistically significant  
</p>
<p>difference between the two groups.
</p>
<p>a. The length of  incarceration sentence is shorter for female than for male 
</p>
<p>offenders.
</p>
<p>offenders.
</p>
<p>c. The length of  incarceration sentence is longer for offenders convicted 
</p>
<p>in a jury trial than for offenders who plead guilty.
</p>
<p>d. The length of  incarceration sentence is different for offenders  
</p>
<p>convicted of  drug and property offenses.
</p>
<p>C H A P T E R E L E V E N :  C O M P A R I N G M E A N S A N D P R O P O R T I O N S304
</p>
<p>reproduce the results on pages 281 and 282 in the text.</p>
<p/>
</div>
<div class="page"><p/>
<p>paired samples t
</p>
<p>paired samples t
</p>
<p>C O M P U T E R E X E R C I S E S 305</p>
<p/>
</div>
<div class="page"><p/>
<p>Comparing Means Among More 
</p>
<p>Than Two Samples: Analysis of Variance
</p>
<p>How Can the Strength of the Relationship Be Defined?
</p>
<p>How Does One Make Comparisons Between the Groups?
</p>
<p>C h a p t e r  t w e l v e
</p>
<p>A n a l y s i s  o f  v a r i a n c e  ( A N O V A )
</p>
<p>T h e  K r u s k a l - W a l l i s  t e s t
</p>
<p>What is the Logic Underlying ANOVA?
</p>
<p>What are the Assumptions of the Test?
</p>
<p>How is the Test Carried Out?
</p>
<p>When is the Test Used?
</p>
<p>How is the Test Carried Out?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_12,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>IN CHAPTER 11, we used the t distribution to test hypotheses about
means from two independent samples. But what if we are interested in
</p>
<p>looking at more than two samples at a time? This is a common problem
</p>
<p>in criminology and criminal justice, where many important questions
</p>
<p>can be raised across a number of different samples. For example, race is
</p>
<p>a central concern in criminal justice and criminology, and often it does
</p>
<p>not make sense to restrict comparisons involving race to just two
</p>
<p>groups. Similarly, in many criminal justice studies, a number of interven-
</p>
<p>tions are compared simultaneously. In such studies, researchers want to
</p>
<p>compare not just two but three or more means in the context of one
</p>
<p>statistical test.
</p>
<p>Analysis of variance (ANOVA) is a commonly used parametric test of
</p>
<p>statistical significance that allows the researcher to compare multiple
</p>
<p>groups on specific characteristics. ANOVA also provides an opportunity
</p>
<p>to introduce several important statistical concepts used in more complex
</p>
<p>types of analysis. In this chapter, we examine in this context the con-
</p>
<p>cepts of explained and unexplained variation and consider how they re-
</p>
<p>late to the total variation found in a specific measure. This chapter also
</p>
<p>introduces a nonparametric test, the Kruskal-Wallis test, which can be
</p>
<p>used for comparisons across multiple groups when the assumptions un-
</p>
<p>derlying ANOVA are difficult to meet.
</p>
<p>A n a l y s i s  o f  V a r i a n c e
</p>
<p>Analysis of variance is based on a simple premise. As the differences
</p>
<p>between the means of samples become larger relative to the variability
</p>
<p>of scores within each sample, our confidence in making inferences
</p>
<p>grows. Why is this the case? Certainly it makes sense that the more the
</p>
<p>mean differs from one sample to another, the stronger the evidence
</p>
<p>307</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>supporting the position that there are differences between the popula-
</p>
<p>tion means. All else being equal, the larger the differences between the
</p>
<p>samples, the more confident we are likely to be in rejecting the position
</p>
<p>that the population means are equal.
</p>
<p>But we are faced with a problem in making such an inference. How
</p>
<p>much confidence can we place in the observed means of our samples?
</p>
<p>As we have stated many times in this book, sample estimates vary from
</p>
<p>sample to sample. If the sample means are likely to vary considerably,
</p>
<p>then we want to be cautious in drawing strong conclusions from our
</p>
<p>study. If the sample means are not likely to vary greatly, we can have
</p>
<p>more confidence in conclusions drawn from them. Analysis of variance
</p>
<p>uses the variability within the observed samples to come to conclusions
</p>
<p>about this variability.
</p>
<p>Suppose, for example, that you are examining two separate studies,
</p>
<p>each including three samples. In the first study, the scores are widely
</p>
<p>dispersed around the mean for each group. In contrast, in the second
</p>
<p>study, the scores are tightly clustered around the group means. If you
</p>
<p>take the variability you observe in these samples as an indication of the
</p>
<p>variability in the populations from which they were drawn, you are
</p>
<p>likely to have more confidence in estimates gained from the second
</p>
<p>study than from the first. Those estimates appear to be more stable, evi-
</p>
<p>dencing much less variability.
</p>
<p>This is precisely the approach taken in analysis of variance. The vari-
</p>
<p>ability between the groups studied is contrasted with the variability
</p>
<p>within these groups to produce a ratio:
</p>
<p>The larger this ratio&mdash;the larger the differences between the groups
</p>
<p>relative to the variability within them&mdash;the more confidence we can
</p>
<p>have in a conclusion that the population means are not equal. When
</p>
<p>the ratio is smaller, meaning that the differences between the groups
</p>
<p>are small relative to the variability within them, we have less reason to
</p>
<p>conclude that differences exist in the populations to which we want to
</p>
<p>infer.
</p>
<p>Developing Estimates of Variance Between and Within Groups
</p>
<p>The first step in analysis of variance is to define the variability be-
</p>
<p>tween and within the groups studied. To make this process more con-
</p>
<p>Table 12.1).
</p>
<p>Variability between groups
</p>
<p>Variability within groups
</p>
<p>crete, let&rsquo;s use a hypothetical study of depression among 12 prison 
</p>
<p>inmates drawn from high-, moderate-, and low-security prisons (see
</p>
<p>308</p>
<p/>
</div>
<div class="page"><p/>
<p>A N A L Y S I S O F V A R I A N C E
</p>
<p>Between-group variability is measured by first subtracting the grand
</p>
<p>mean, or overall mean, of the three samples from the mean of each
</p>
<p>sample. This difference must then be adjusted to take into account the
</p>
<p>number of scores or observations in each sample. Equation 12.1 repre-
</p>
<p>sents this process in mathematical language. The sample (or category)
</p>
<p>means are represented by , the overall mean (or grand mean) is repre-
</p>
<p>sented by , Nc represents the number of scores or observations in the 
</p>
<p>sample (or category), and tells us to sum the results from the first 
</p>
<p>sample (or category) mean (c � 1) to the last sample (or category) mean
</p>
<p>(c � k).
</p>
<p>Equation 12.1
</p>
<p>As illustrated in Table 12.1, the overall, or grand, mean is found by
</p>
<p>adding up all the scores in the three samples and dividing by the total
</p>
<p>number of scores (N � 12). To calculate the amount of between-group
</p>
<p>variability for our example, we take just three quantities&mdash;the mean de-
</p>
<p>pression score of high-security inmates minus the overall mean,
</p>
<p>the mean depression score of moderate-security inmates minus
</p>
<p>the overall mean, and the mean depression score of low-security inmates
</p>
<p>minus the overall mean&mdash;and multiply each by its sample size.
</p>
<p>Within-group variability is identified by summing the difference be-
</p>
<p>tween each subject&rsquo;s score and the mean for the sample in which the
</p>
<p>subject is found. In Equation 12.2, Xi represents a score from one of the
</p>
<p>(X � 4)
</p>
<p>(X � 8)
</p>
<p>(X � 9)
</p>
<p>�
k
</p>
<p>c�1
</p>
<p> [Nc(Xc � Xg)]
</p>
<p>�
k
</p>
<p>c�1
</p>
<p>Xg
</p>
<p>Xc
</p>
<p>Depression Scores for 12 Prison Inmates Drawn 
from High-, Moderate-, and Low-Security Prisons
</p>
<p>LOW SECURITY MODERATE SECURITY HIGH SECURITY
</p>
<p>(GROUP 1) (GROUP 2) (GROUP 3)
</p>
<p>3 9 9
5 9 10
4 8 7
4 6 10
</p>
<p>To calculate the grand mean: Xg � 
�
N
</p>
<p>i�1
</p>
<p> Xi
</p>
<p>N
 � 
</p>
<p>84
</p>
<p>12
 � 7
</p>
<p>X � 9X � 8X � 4
</p>
<p>� � 36� � 32� � 16
</p>
<p>Table 12.1
</p>
<p>309</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>three samples and, as before, represents the mean for that sample.
</p>
<p>Here we sum from i � 1 to N, or from the first to the last score in the
</p>
<p>overall study.
</p>
<p>Equation 12.2
</p>
<p>For within-group variability, we have four calculations to carry out for
</p>
<p>each sample (group). For the first subject in the low-security prison sam-
</p>
<p>ple, for example, we subtract from the subject&rsquo;s score of 3 the mean
</p>
<p>score of low-security inmates in the study (4). The same is done for each
</p>
<p>of the other three members of this sample. For the moderate-security
</p>
<p>sample, we repeat the process, starting with the first subject, with a score
</p>
<p>of 9, and using the mean of 8 for the group as a whole. The same is
</p>
<p>done for the high-security sample.
</p>
<p>When we add up the deviations between the groups and those within
</p>
<p>them, as is done in Tables 12.2 and 12.3, we find that both are 0. This
</p>
<p>does not mean that there is an absence of variability either within or be-
</p>
<p>tween the samples we are examining. Rather, this outcome reflects a rule
</p>
<p>stated in Chapter 4: The sum of the deviations from a mean equals 0.
</p>
<p>Clearly, we cannot use the sum of the deviations from the mean as an in-
</p>
<p>dicator of variation. As in other similar problems, it makes sense to
</p>
<p>square the deviations from the mean. The squares of all the deviations
</p>
<p>from the mean will be positive or 0.
</p>
<p>The result when we square these quantities and then add them is
</p>
<p>commonly referred to as a sum of squares. The variability between
</p>
<p>groups measured in this way is called the between sum of squares
</p>
<p>(BSS). It is calculated by taking the sum of the squared deviation of each
</p>
<p>sample mean from the overall mean multiplied by the number
</p>
<p>of cases (Nc) in that sample.
</p>
<p>Equation 12.3BSS � �
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>(Xg)(Xc )
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � Xc )
</p>
<p>Xc
</p>
<p>Summing the Deviations of the Group Means from the Grand Mean
for the Three Groups in the Inmate Depression Study
</p>
<p>N c
</p>
<p>4 7 �3 �12
8 7 1 4
9 7 2 8
</p>
<p>� � 0� � 0
</p>
<p>(Xc � Xg)(Xc � Xg)XgXc
</p>
<p>Table 12.2
</p>
<p>310</p>
<p/>
</div>
<div class="page"><p/>
<p>A N A L Y S I S O F V A R I A N C E
</p>
<p>To calculate the between sum of squares for our hypothetical study,
</p>
<p>we take the same approach as is shown in Table 12.2. The one differ-
</p>
<p>ence is that after we subtract the overall mean from a category mean, we
</p>
<p>square the result. Our final result is 56.
</p>
<p>Summing the Deviations of the Scores from the Group Means 
Within the Three Groups in the Inmate Depression Study
</p>
<p>X i
</p>
<p>3 4 �1
5 4 1
4 4 0
4 4 0
9 8 1
9 8 1
8 8 0
6 8 �2
9 9 0
</p>
<p>10 9 1
7 9 �2
</p>
<p>10 9 1
</p>
<p>� � 0
</p>
<p>(Xi � Xc)Xc
</p>
<p>Table 12.3
</p>
<p>W orking It Out
</p>
<p> � 56
</p>
<p> � 4(4 � 7)2  � 4(8 � 7)2  � 4(9 � 7)2
</p>
<p> BSS � �
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>is defined as the within sum of squares (WSS). The within sum of
</p>
<p>squares is obtained by taking the sum of the squared deviation of each
</p>
<p>score from its category mean, as represented in Equation 12.4. As before,
</p>
<p>we first take the score for each subject and subtract from it the sample
</p>
<p>mean. However, before adding these deviations together, we square
</p>
<p>each one. The within sum of squares for our hypothetical example is
</p>
<p>equal to 14.
</p>
<p>Equation 12.4WSS � �
N
</p>
<p>i�1
</p>
<p> (Xi � Xc )
2
</p>
<p>When we measure variability within groups using this method, the result
</p>
<p>311</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>Partitioning Sums of Squares
</p>
<p>We can also calculate a third type of variability for our inmate depres-
</p>
<p>sion study: total sum of squares (TSS). The total sum of squares
</p>
<p>takes into account all of the variability in our three samples. It is
</p>
<p>calculated by taking the sum of the squared deviation of each score
</p>
<p>from the overall mean of scores for the three groups, as shown in
</p>
<p>Equation 12.5.
</p>
<p>Equation 12.5
</p>
<p>In practice, we first take the deviation of a score from the overall
</p>
<p>mean and then square it. For example, the first subject in the low-secu-
</p>
<p>rity prison sample has a score of 3. We subtract from this score the over-
</p>
<p>all mean of 7 and then square the result (�4), to obtain a value of 16. To
</p>
<p>arrive at the total sum of squares, we do this for each of the 12 scores in
</p>
<p>the study and then sum the results.
</p>
<p>TSS � �
N
</p>
<p>i�1
</p>
<p> (Xi � Xg)
2
</p>
<p>W orking It Out
</p>
<p> � 14
</p>
<p>  �� (7 � 9)2 � (10 � 9)2
  �� (9 � 8)2 � (8 � 8)2 � (6 � 8)2 � (9 � 9)2 � (10 � 9)2 
</p>
<p> � (3 � 4)2 � (5 � 4)2 � (4 � 4)2 � (4 � 4)2 � (9 � 8)2 
</p>
<p> WSS � �
N
</p>
<p>i�1
</p>
<p> (Xi � Xc )
2
</p>
<p>W orking It Out
</p>
<p> � 70
</p>
<p>   � (10 � 7)2 � (7 � 7)2 � (10 � 7)2
   � (9 � 7)2 � (9 � 7)2 � (8 � 7)2 � (6 � 7)2 � (9 � 7)2
 � (3 � 7)2 � (5 � 7)2 � (4 � 7)2 � (4 � 7)2
</p>
<p> TSS � �
N
</p>
<p>i�1
</p>
<p> (Xi � Xg)
2
</p>
<p>312</p>
<p/>
</div>
<div class="page"><p/>
<p>A N A L Y S I S O F V A R I A N C E
</p>
<p>squares and the within sum of squares. That is, the total variability across
</p>
<p>all of the scores is made up of the variability between the samples and
</p>
<p>the variability within the samples. More generally, the three types of vari-
</p>
<p>ability discussed so far may be expressed in terms of a simple formula
</p>
<p>that partitions the total sum of squares into its two component parts: the
</p>
<p>between sum of squares and the within sum of squares.
</p>
<p>Total sum of squares � between sum of squares 
</p>
<p>� within sum of squares Equation 12.6
</p>
<p>(For our example, 70 � 56 � 14.)
</p>
<p>Another way to express the relationship among the three types of
</p>
<p>sums of squares is to partition the total sum of squares into explained
</p>
<p>and unexplained components:
</p>
<p>Total sum of squares � explained sum of squares 
</p>
<p>� unexplained sum of squares Equation 12.7
</p>
<p>In this equation, the between sum of squares is represented by the ex-
</p>
<p>plained sum of squares (ESS) because the between sum of squares
</p>
<p>represents the part of the total variability that is accounted for by the dif-
</p>
<p>ferences between the groups. For our hypothetical example, this is the
</p>
<p>proportion of the total variability in depression that is &ldquo;explained&rdquo; by the
</p>
<p>type of prison in which the subject is incarcerated.
</p>
<p>The within sum of squares is represented in Equation 12.7 by the un-
</p>
<p>explained variability, or the unexplained sum of squares (USS). This
</p>
<p>is the part of the total variability that differences between the groups do
</p>
<p>not explain. We usually do not know the cause of this variability.
</p>
<p>Developing Estimates of Population Variances
</p>
<p>So far we have defined the sums of squares associated with between-
</p>
<p>group and within-group variability. But analysis of variance, as its name
</p>
<p>implies, is concerned with variance, not just variability. Accordingly, we
</p>
<p>have to adjust our sums by taking into account the appropriate number
</p>
<p>of degrees of freedom. In Chapter 5, when we developed estimates of
</p>
<p>variance, we divided the squared deviations from the mean by the num-
</p>
<p>ber of cases in the sample or population. For analysis of variance, we di-
</p>
<p>vide the between and within sums of squares by the appropriate degrees
</p>
<p>of freedom.
</p>
<p>For the between-group estimate of variance, we define the number of
</p>
<p>degrees of freedom as k � 1, where k is the number of samples or cate-
</p>
<p>gories examined. If we are comparing three sample means, the number
</p>
<p>of degrees of freedom associated with the between-group estimate 
</p>
<p>of variance is thus 2. As illustrated by Equation 12.8, an estimate of the
</p>
<p>The quantity obtained is equivalent to the sum of the between sum of
</p>
<p>313</p>
<p/>
</div>
<div class="page"><p/>
<p>The ANOVA model is often presented using a different set of statistical
</p>
<p>notation than that used in this text. In this book, we define the total sum of
</p>
<p>squares as equal to the sum of the within sum of squares and the be-
</p>
<p>tween sum of squares:
</p>
<p>In many other statistics texts, the following equation is used for the de-
</p>
<p>composition of the total sum of squares:
</p>
<p>The double summation symbols indicate that we need to first sum over
</p>
<p>each group&mdash;denoted by the subscript j&mdash;and then sum over each indi-
</p>
<p>vidual observation&mdash;denoted by the subscript i. In terms of this book&rsquo;s no-
</p>
<p>tation, .
</p>
<p>Although we have reduced the number of summation signs to one for
</p>
<p>each sum of squares, the calculations with the two equations are identi-
</p>
<p>cal for the total sum of squares and the within sum of squares. The one
</p>
<p>difference between the two equations lies in the calculation of the be-
</p>
<p>tween sum of squares. The notation we offer simplifies this calculation
</p>
<p>by taking into account the fact that all of the individual scores in a single
</p>
<p>group (Nc) will have the same value. Rather than repeat the same calcu-
</p>
<p>lation for all the individuals in the same group, we produce the identical
</p>
<p>answer by multiplying the squared difference of the sample mean and
</p>
<p>the overall mean by the number of observations in the corresponding
</p>
<p>sample.
</p>
<p>X &bull; &bull; � Xg and X &bull;j � Xc
</p>
<p>�
N
</p>
<p>i�1
�
</p>
<p>k
</p>
<p>j�1
</p>
<p>(Xij � X&bull;&bull;)
2  � �
</p>
<p>N
</p>
<p>i�1
�
</p>
<p>k
</p>
<p>j�1
</p>
<p>(Xij � X&bull;j)
2 � �
</p>
<p>N
</p>
<p>i�1
�
</p>
<p>k
</p>
<p>j�1
</p>
<p>(X&bull;j � X&bull;&bull;)
2
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(Xi � Xg)
2 � �
</p>
<p>N
</p>
<p>i�1
</p>
<p>(Xi � Xc)
2 � �
</p>
<p>k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>in Different Formats
Representing Sums of Squares: ANOVA Notation </p>
<p/>
</div>
<div class="page"><p/>
<p>A N A L Y S I S O F V A R I A N C E
</p>
<p>between-group variance is obtained by dividing the between sum
</p>
<p>of squares by k � 1.
</p>
<p>Equation 12.8
</p>
<p>The number of degrees of freedom for the within-group estimate of
</p>
<p>variance is N � k, or the number of cases in the sample minus the num-
</p>
<p>ber of samples or categories examined. The within-group variance esti-
</p>
<p>mate is calculated by dividing the within sum of squares by N � k
</p>
<p>(see Equation 12.9).
</p>
<p>Equation 12.9
</p>
<p>A Substantive Example: Age and White-Collar Crimes
</p>
<p>Now that we have defined the two types of variance that are compared
</p>
<p>in analysis of variance, let&rsquo;s look at a substantive problem. Table 12.4
</p>
<p>presents data on age for three samples of offenders convicted of white-
</p>
<p>collar crimes in seven federal district courts over a three-year period.1
</p>
<p>�̂2wg � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � Xc)
2
</p>
<p>N � k
</p>
<p>(�̂2wg)
</p>
<p>�̂2bg � 
�
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>k � 1
</p>
<p>(�̂2bg)
</p>
<p>Ages of 30 White-Collar Criminals Convicted 
of Three Different Offenses
</p>
<p>OFFENSE 1 OFFENSE 2 OFFENSE 3
</p>
<p>BANK EMBEZZLEMENT BRIBERY ANTITRUST VIOLATION
</p>
<p>19 28 35
21 29 46
23 32 48
25 40 53
29 42 58
30 48 61
31 58 62
35 58 62
42 64 62
49 68 75
</p>
<p>s � 8.98 s � 13.99 s � 10.54
</p>
<p>Grand mean � 1333/30 � 44.43(Xg)
</p>
<p>X � 56.2X � 46.7X � 30.4
</p>
<p>Table 12.4
</p>
<p>1The data are drawn from S. Wheeler, D. Weisburd, and N. Bode, Sanctioning of
</p>
<p>White Collar Crime, 1976&ndash;1978: Federal District Courts (Ann Arbor, MI: Inter-
</p>
<p>University Consortium for Political and Social Research, 1988).
</p>
<p>315</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>The first sample was drawn from offenders convicted of bank embezzle-
</p>
<p>ment, the second from offenders convicted of bribery, and the third from
</p>
<p>offenders convicted under criminal antitrust statutes. Ten subjects were
</p>
<p>drawn randomly from each of these populations.
</p>
<p>The values listed in Table 12.4 represent the ages of the sampled of-
</p>
<p>fenders. The mean age of the bank embezzlers is 30.4 years; of the
</p>
<p>bribery offenders, 46.7 years; and of the antitrust offenders, 56.2 years.
</p>
<p>Can we conclude from the differences found among these samples that
</p>
<p>there are differences among the means of the populations from which
</p>
<p>these samples were drawn?
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution for each population (the
</p>
<p>shape of the population distributions is unknown, and the sizes of the
</p>
<p>samples examined are small).
</p>
<p>Sampling Method: Independent random sampling (no replacement;
</p>
<p>sample is small relative to population).
</p>
<p>Sampling Frame: All white-collar offenders convicted of the 
</p>
<p>crimes examined in seven federal judicial districts over a three-year
</p>
<p>period.
</p>
<p>Population variances are equal .
</p>
<p>Hypotheses:
</p>
<p>H0: Population means of age for bank embezzlers, bribery offenders,
</p>
<p>and antitrust offenders are equal (�1 � �2 � �3).
</p>
<p>H1: Population means of age for bank embezzlers, bribery offenders,
</p>
<p>and antitrust offenders are not equal (�1 � �2 � �3).
</p>
<p>ance requires that an interval level of measurement be used for the
</p>
<p>scores to be examined. Our example meets this assumption because
</p>
<p>age is an interval measure. Some statisticians add an assumption of
</p>
<p>nominal measurement because a comparison of means across samples
</p>
<p>requires that we define categories (or samples) for comparison. For
</p>
<p>example, in the case of age and white-collar crime, the three samples
</p>
<p>represent three categories of offenses. Our interest in this case is in
</p>
<p>the relationship between age (an interval-scale variable) and category
</p>
<p>of crime (a nominal-scale variable). In the hypothetical study dis-
</p>
<p>cussed earlier in this chapter, we were interested in the relationship
</p>
<p>between depression (measured at an interval level) and type of prison
</p>
<p>(a nominal-scale variable).
</p>
<p>(�21 � �
2
2 � �
</p>
<p>2
3)
</p>
<p>Like other parametric tests of statistical significance, analysis of vari-
</p>
<p>316</p>
<p/>
</div>
<div class="page"><p/>
<p>A N A L Y S I S O F V A R I A N C E
</p>
<p>Analysis of variance also requires that the populations underlying the
</p>
<p>samples examined be normally distributed. For our example, this is the
</p>
<p>most troubling assumption. We do not have evidence from prior studies
</p>
<p>that age is distributed normally within categories of crime. Nor are our
</p>
<p>samples large enough to allow us to invoke the central limit theorem.
</p>
<p>For ANOVA, as for the two-sample t-test, we want to have at least 30
</p>
<p>cases per sample in order to relax the normality assumption. Because the
</p>
<p>computations for analysis of variance are complex, having only ten cases
</p>
<p>in each sample makes it easier to learn about ANOVA. However, our test
</p>
<p>will provide valid results only if the population distributions we infer to
</p>
<p>are in fact normally distributed.
</p>
<p>We must also assume that the samples being compared were drawn
</p>
<p>randomly and independently. In practice, as we discussed in Chapter
</p>
<p>11, researchers often make comparisons between groups within a sin-
</p>
<p>gle larger sample. For example, we might draw an independent ran-
</p>
<p>dom sample of white-collar offenders and then compare the means
</p>
<p>found in this larger sample for bank embezzlement, bribery, and an-
</p>
<p>titrust offenders. As explained in Chapter 11, if the larger sample was
</p>
<p>drawn as an independent random sample, then we may assume that
</p>
<p>subsamples consisting of offenders convicted of different types of
</p>
<p>crimes are also independent random samples. In this study, random
</p>
<p>samples were drawn independently from each category of crime. Al-
</p>
<p>districts studied.
</p>
<p>For analysis of variance, we must also assume that the population
</p>
<p>variances of the three groups are equal. This assumption of homoscedas-
</p>
<p>ticity is similar to that introduced for the two-sample t-test (using the
</p>
<p>pooled-variance method) in Chapter 11. However, in contrast to the 
</p>
<p>2
</p>
<p>One reason researchers, as opposed to statisticians, are not very
</p>
<p>concerned about the assumption of homoscedasticity is that even large
</p>
<p>2For example, see G. Hornsnell, &ldquo;The Effect of Unequal Group Variances on the 
</p>
<p>F Test for Homogeneity of Group Means,&rdquo; Biometrika 40 (1954): 128&ndash;136; G. E. P.
</p>
<p>Box, &ldquo;Some Theorems on Quadratic Forms Applied in the Study of Analysis of
</p>
<p>Variance Problems. I. Effect of Inequality of Variance in the One Way Classification,&rdquo;
</p>
<p>Annals of Mathematical Statistics 25 (1954): 290&ndash;302.
</p>
<p>though the researchers did not sample with replacement, we can assume
</p>
<p>that this violation of assumptions is not serious because the sample
</p>
<p>drawn is very small relative to the population of offenders in the
</p>
<p>t-test, ANOVA has no alternative test if we cannot assume equal variances
</p>
<p>between the groups. Although this seems at first to be an important
</p>
<p>barrier to using analysis of variance, in practice it is generally accepted
</p>
<p>that violations of this assumption must be very large before the results
</p>
<p>of a test come into question.
</p>
<p>317</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>violations of this assumption affect the estimates of statistical signifi-
</p>
<p>cance to only a small degree. Sometimes it is suggested that you simply
</p>
<p>define a more conservative level of statistical significance when you 
</p>
<p>are concerned with a serious violation of the homoscedasticity assump-
</p>
<p>tion.3 Accordingly, you might select a 1% significance threshold as
</p>
<p>opposed to the more conventional 5% threshold. In general, large devi-
</p>
<p>ations in variance are not likely to occur across all of the groups stud-
</p>
<p>ied. If one group is very different from the others, you might choose to
</p>
<p>conduct your test two ways&mdash;both including the group that is very dif-
</p>
<p>ferent from the others and excluding it&mdash;and compare your results. In
</p>
<p>our example, the variances do not differ widely one from another (see
</p>
<p>Table 12.4).
</p>
<p>Our final assumptions for this analysis relate to the null and research
</p>
<p>hypotheses. For analysis of variance, the null hypothesis is that the
</p>
<p>means of the groups are the same. In our example, the null hypothesis
</p>
<p>is that the mean ages of offenders in the populations of the three crime
</p>
<p>categories are equal. Our research hypothesis is that the means are not
</p>
<p>equal. As is true for ANOVA more generally, our research hypothesis is
</p>
<p>nondirectional. If we are making inferences to three or more popula-
</p>
<p>tions, it is not possible to define the direction of the differences among
</p>
<p>them.4
</p>
<p>The Sampling Distribution The sampling distribution used for making de-
</p>
<p>cisions about hypotheses in analysis of variance is called the F distribu-
</p>
<p>tion, after R. A. Fisher, the statistician who first described it. The shape of
</p>
<p>the F distribution varies, depending on the number of degrees of free-
</p>
<p>dom of the variance estimates being compared. The number of degrees
</p>
<p>3Most packaged statistical programs provide a test for equivalence of variances as an
</p>
<p>option with their ANOVA program. However, be careful not to automatically reject
</p>
<p>use of analysis of variance on the basis of a statistically significant result. In smaller
</p>
<p>studies, with samples of less than 50 per group, a finding of a statistically significant
</p>
<p>difference should make you cautious about using analysis of variance. In such a
</p>
<p>study, you may want to adjust the significance level, as suggested here, or consider al-
</p>
<p>ternative nonparametric tests (discussed later in the chapter). With larger samples, 
</p>
<p>a statistically significant result at conventional significance levels should not necessar-
</p>
<p>ily lead to any adjustments in your test. For such adjustments to be made, the differ-
</p>
<p>ence should be highly significant and reflect large actual differences among variance
</p>
<p>estimates.
4However, in the special case of analysis of variance with only two samples, the re-
</p>
<p>searcher can use a directional research hypothesis. This will sometimes be done in ex-
</p>
<p>perimental studies when the researcher seeks to examine differences across experi-
</p>
<p>mental and control groups, taking into account additional factors [e.g., see L. W.
</p>
<p>Sherman and D. Weisburd, &ldquo;General Deterrent Effects of Police Patrol in Crime &lsquo;Hot
</p>
<p>Spots.&rsquo; A Randomized Study,&rdquo; Justice Quarterly 12 (1995): 625&ndash;648.]
</p>
<p>318</p>
<p/>
</div>
<div class="page"><p/>
<p>A N A L Y S I S O F V A R I A N C E
</p>
<p>of freedom is represented by k � 1 for the between-group variance and
</p>
<p>N � k for the within-group variance:
</p>
<p>W orking It Out
</p>
<p>df for between-group variance � k � 1 � 3 � 1 � 2
</p>
<p>df for within-group variance � N � k � 30 � 3 � 27
</p>
<p>Because we need to take into account two separate degrees of freedom
</p>
<p>at the same time, a different table of probability estimates is given for
</p>
<p>each significance threshold. Accordingly, Appendix 5 provides F tables
</p>
<p>for 0.05, 0.01, and 0.001 significance levels.
</p>
<p>Each table provides the F-scores, adjusted for degrees of freedom,
</p>
<p>that correspond to the particular significance threshold. Thus, for
</p>
<p>example, in the table for � � 0.05, the values given are the critical
</p>
<p>values for the test. For all tests using the F distribution, we need to ob-
</p>
<p>tain an F-score greater than this critical value to reject the null hypothesis
</p>
<p>of equal means. Looking at the table for � � 0.05, we can identify two
</p>
<p>interesting characteristics of the F distribution.
</p>
<p>First, the F distribution is unidirectional, consisting only of positive
</p>
<p>values. Consistent with the fact that the research hypothesis in analysis
</p>
<p>of variance with three or more population means states simply that the
</p>
<p>means are not equal, the F distribution is concerned only with the ab-
</p>
<p>solute size of the statistic obtained.
</p>
<p>Second, as the number of degrees of freedom associated with the
</p>
<p>within-group variance grows, the F-value needed to reject the null hy-
</p>
<p>pothesis gets smaller. Remember that the number of degrees of freedom
</p>
<p>for the within-group variance is equal to N � k. Accordingly, as the
</p>
<p>number of cases in the sample gets larger, the number of degrees of
</p>
<p>freedom also gets larger. Why should the F-value needed to reject the
</p>
<p>null hypothesis be related to the size of the sample? As with a t-test, as
</p>
<p>the number of cases increases, so too does our confidence in the esti-
</p>
<p>mate we obtain from an F-test.5
</p>
<p>Significance Level and Rejection Region Given that no special concerns
</p>
<p>have been stated in regard to the risk of either a Type I or a Type II
</p>
<p>error, we use a conventional 0.05 significance level. Looking at the F
</p>
<p>table for � � 0.05 (Appendix 5), with 2 and 27 degrees of freedom,
</p>
<p>5Indeed, note that the values of F with 1 degree of freedom for the between sum of
</p>
<p>squares are simply the values of t squared.
</p>
<p>319</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>respectively, we find a critical value of 3.35. This tells us that we need an
</p>
<p>F-score greater than 3.35 to reject our null hypothesis of no difference
</p>
<p>between the population means. An observed F-score greater than 3.35
</p>
<p>means that the observed significance level for our test is less than the
</p>
<p>0.05 criterion level we have set.
</p>
<p>The Test Statistic To calculate the F-ratio, we must compute estimates of
</p>
<p>the between-group and within-group population variances based on our
</p>
<p>three samples. Computing the between-group variance is relatively easy.
</p>
<p>As noted earlier, the formula for between-group variance is
</p>
<p>Applying this formula to our example, we first take the mean for each
</p>
<p>group, subtract from it the overall mean of the three groups (44.43),
</p>
<p>square the result, and multiply by 10&mdash;the number of observations in
</p>
<p>each group. After this process has been carried out for each of the three
</p>
<p>groups, the totals are then added together and divided by the degrees of
</p>
<p>freedom for the between-group variance (2). These calculations are illus-
</p>
<p>trated below. The between sum of squares for our example is 3,405.267.
</p>
<p>Dividing it by the number of degrees of freedom (2) results in a
</p>
<p>between-group variance estimate of 1,702.634.
</p>
<p>�̂2bg � 
�
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>k � 1
</p>
<p>(�̂2bg)
</p>
<p>W orking It Out
</p>
<p> � 1,702.6335
</p>
<p> � 
3,405.267
</p>
<p>2
</p>
<p> � 
10(30.4 � 44.43)2 � 10(46.7 � 44.43)2 �10(56.2 � 44.43)2
</p>
<p>3 � 1
</p>
<p> �̂2bg � 
�
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>k � 1
</p>
<p>Applying the formula for within-group variance is more difficult in large
</p>
<p>part because the calculation of a within-group sum of squares demands a
</p>
<p>good deal of computation even for small samples. For that reason, some
</p>
<p>texts provide an alternative estimating technique for the within-group sum
</p>
<p>assume that you will turn to statistical computing packages when conduct-
</p>
<p>ing research in the future and the purpose here is to gain a better under-
</p>
<p>320
</p>
<p>of squares (see the box on p. 322). However, because it is probably safe to</p>
<p/>
</div>
<div class="page"><p/>
<p>A N A L Y S I S O F V A R I A N C E
</p>
<p>standing of analysis of variance, we will focus on the raw computation. Al-
</p>
<p>though cumbersome, it illustrates more directly the logic behind ANOVA.
</p>
<p>As discussed earlier, the formula for within-group variance is
</p>
<p>For our example, we first take each individual score as illustrated in Table
</p>
<p>12.5, and subtract from it the mean for its group: . We then
</p>
<p>square this quantity: . This is done for all 30 individual scores,
</p>
<p>and the results are then summed. The within-group sum of squares is
</p>
<p>3,874.1. When we divide this quantity by the correct degrees of freedom
</p>
<p>(N � k, or 27), we obtain a within-group variance estimate of 143.485.
</p>
<p>To obtain the F-statistic for our example, we simply calculate the ratio
</p>
<p>of the between- and within-group variances (see Equation 12.10), obtain-
</p>
<p>ing 11.866.
</p>
<p>Equation 12.10F � 
between-group variance
</p>
<p>within-group variance
</p>
<p>(X � Xc )
2
</p>
<p>(X � Xc )
</p>
<p>�̂2wg � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � Xc )
2
</p>
<p>N � k
</p>
<p>(�̂2wg)
</p>
<p>Calculating the Within-Group Sum of Squares
</p>
<p>OFFENSE 1 OFFENSE 2 OFFENSE 3
</p>
<p>BANK EMBEZZLEMENT BRIBERY ANTITRUST VIOLATION 
</p>
<p>� 30.4 � 46.7 � 56.2
</p>
<p>X X X
</p>
<p>19 �11.4 129.96 28 �18.7 349.69 35 �21.2 449.44
21 �9.4 88.36 29 �17.7 313.29 46 �10.2 104.04
23 �7.4 54.76 32 �14.7 216.09 48 �8.2 67.24
25 �5.4 29.16 40 �6.7 44.89 53 �3.2 10.24
29 �1.4 1.96 42 �4.7 22.09 58 1.8 3.24
30 �0.4 0.16 48 1.3 1.69 61 4.8 23.04
31 0.6 0.36 58 11.3 127.69 62 5.8 33.64
35 4.6 21.16 58 11.3 127.69 62 5.8 33.64
42 11.6 134.56 64 17.3 299.29 62 5.8 33.64
49 18.6 345.96 68 21.3 453.69 75 18.8 353.44
</p>
<p>� � 3,874.1
</p>
<p>(Xi � Xc )
2(Xi � Xc )(Xi � Xc )
</p>
<p>2(Xi � Xc )(Xi � Xc )
2(Xi � Xc )
</p>
<p>XXX
</p>
<p>Table 12.5
</p>
<p>W orking It Out
</p>
<p> � 11.8663
</p>
<p> F � 
1,702.6335
</p>
<p>143.4852
</p>
<p> F � 
between-group variance
</p>
<p>within-group variance
</p>
<p>321
</p>
<p>i
</p>
<p>i</p>
<p/>
</div>
<div class="page"><p/>
<p>While it is important for you to understand the concepts underlying the
</p>
<p>equations used in the computations for ANOVA, the actual calculation of
</p>
<p>the within-group sum of squares can be quite tedious. Since it is often
</p>
<p>easier to calculate the total sum of squares and the between-group sum
</p>
<p>of squares, the simplest way of obtaining the within-group sum of squares
</p>
<p>is to rely on the relationship stated in Equation 12.6:
</p>
<p>Total sum of squares � between-group sum of squares 
</p>
<p>� within-group sum of squares
</p>
<p>This equation can be rearranged and solved for the within-group sum of
</p>
<p>squares:
</p>
<p>Within-group sum of squares � total sum of squares 
</p>
<p>� between-group sum of squares
</p>
<p>A formula for computing the total sum of squares is
</p>
<p>This equation tells us to square the value of each observation and add the
</p>
<p>resulting squared values together . From this quantity, we then
</p>
<p>subtract the square of the sum of all the values divided by the total num-
</p>
<p>ber of observations .
</p>
<p>For an illustration of the use of this formula, we can turn to the data on
</p>
<p>the ages of white-collar criminals in Table 12.4. In the following table, we
</p>
<p>take each offender&rsquo;s age (X ) and square it (X 2). We then sum each column.
</p>
<p>To calculate the total sum of squares, we just enter the two sums into
</p>
<p>the computational formula:
</p>
<p> � 7,279.367
</p>
<p> � 66,509 � 
(1,333)2
</p>
<p>30
</p>
<p>TSS � �
N
</p>
<p>i �1
</p>
<p>X 2i  � 
</p>
<p>(�
N
</p>
<p>i�1
</p>
<p> X i)
2
</p>
<p>N
</p>
<p>�(� N
i�1
</p>
<p>X i)
2/N�
</p>
<p>��N
i�1
</p>
<p> X 2i�
</p>
<p>TSS � �
N
</p>
<p>i �1
</p>
<p>X 2i  � 
</p>
<p>(�
N
</p>
<p>i�1
</p>
<p> X i)
2
</p>
<p>N
</p>
<p>Computational Formulas for the Within-Group 
Sum of Squares</p>
<p/>
</div>
<div class="page"><p/>
<p>Age (X ) Age Squared (X 2)
</p>
<p>19 361
</p>
<p>21 441
</p>
<p>23 529
</p>
<p>25 625
</p>
<p>29 841
</p>
<p>30 900
</p>
<p>31 961
</p>
<p>35 1,225
</p>
<p>42 1,764
</p>
<p>49 2,401
</p>
<p>28 784
</p>
<p>29 841
</p>
<p>32 1,024
</p>
<p>40 1,600
</p>
<p>42 1,764
</p>
<p>48 2,304
</p>
<p>58 3,364
</p>
<p>58 3,364
</p>
<p>64 4,096
</p>
<p>68 4,624
</p>
<p>35 1,225
</p>
<p>46 2,116
</p>
<p>48 2,304
</p>
<p>53 2,809
</p>
<p>58 3,364
</p>
<p>61 3,721
</p>
<p>62 3,844
</p>
<p>62 3,844
</p>
<p>62 3,844
</p>
<p>75 5,625
</p>
<p>At this point, since we know the total sum of squares (7,279.367) and have
</p>
<p>already calculated the between-group sum of squares (3,405.267), we can
</p>
<p>easily see that the within-group sum of squares is 3,874.1:
</p>
<p>Within-group sum of squares � total sum of squares 
</p>
<p>� between-group sum of squares
</p>
<p>� 7,279.367 � 3,405.267
</p>
<p>� 3,874.1
</p>
<p>� � 66,509� � 1,333</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>The Decision Because the test statistic for our example (11.866) is larger
</p>
<p>than 3.35, the critical value of the rejection region, our result is statisti-
</p>
<p>cally significant at the 0.05 level. Accordingly, we reject the null hypothe-
</p>
<p>sis of no difference between the population means and conclude (with a
</p>
<p>conventional level of risk of falsely rejecting the null hypothesis) that the
</p>
<p>average age of offenders differs across the three types of crime exam-
</p>
<p>ined. However, given our concern about violating the assumption of
</p>
<p>normality, our conclusion will be valid only if age is indeed normally
</p>
<p>distributed in the three populations studied.
</p>
<p>Another ANOVA Example: Race and Bail Amounts 
</p>
<p>Among Felony Drug Defendants
</p>
<p>Table 12.6 presents data on bail amounts set for three samples of
</p>
<p>felony drug defendants in large urban court districts in the United
</p>
<p>States in the 1990s.6 The first sample is taken from non-Hispanic
</p>
<p>whites, the second sample from non-Hispanic African Americans, and
</p>
<p>the third sample from Hispanics of any race. Twelve defendants were
</p>
<p>drawn at random from the population of each group. The mean bail
</p>
<p>amount is $4,833.33 for non-Hispanic whites, $8,833.33 for non-
</p>
<p>Hispanic African Americans, and $30,375.00 for Hispanics of any race. Do
</p>
<p>Bail Amounts (in Dollars) for 36 Felony Drug Defendants
</p>
<p>NON-HISPANIC NON-HISPANIC HISPANICS 
</p>
<p>WHITES BLACKS OF ANY RACE
</p>
<p>1,000 1,000 1,000
1,000 1,000 2,000
1,500 2,000 4,000
2,000 2,500 5,000
2,500 3,000 10,000
3,000 4,000 12,500
5,000 5,000 25,000
7,000 10,000 25,000
7,500 12,500 25,000
7,500 20,000 40,000
</p>
<p>10,000 20,000 65,000
10,000 25,000 150,000
</p>
<p>s � 3,287.18 s � 8,201.46 s � 42,028.74
</p>
<p>g
</p>
<p>X � 30,375.00X � 8,833.33X � 4,833.33
</p>
<p>Table 12.6
</p>
<p>6The data are taken from State Court Processing Statistics: 1990, 1992, 1994, 1996 and
</p>
<p>can be accessed through the National Archive of Criminal Justice Data web site at
</p>
<p>http://www.icpsr.umich.edu/NACJD.
</p>
<p>324
</p>
<p>Grand mean (X ) � 528500/36 = 14,680.56</p>
<p/>
<div class="annotation"><a href="http://www.icpsr.umich.edu/NACJD">http://www.icpsr.umich.edu/NACJD</a></div>
</div>
<div class="page"><p/>
<p>Most statistical analysis software presents the results of an analysis of
</p>
<p>variance in the form of an ANOVA table. An ANOVA table provides a com-
</p>
<p>pact and convenient way to present the key elements in an analysis of
</p>
<p>variance. In addition to ensuring that the researcher has all the neces-
</p>
<p>sary information, it also allows the researcher to reproduce the estimates
</p>
<p>of the variance and the F-statistic. The general form of an ANOVA table is
</p>
<p>as follows:
</p>
<p>Source df Sum of Squares Mean Square F
</p>
<p>Between k � 1
</p>
<p>Within N � k
</p>
<p>Total N � 1
</p>
<p>Each row gives the pieces of information needed and the formulas for the
</p>
<p>calculations. For example, the &ldquo;Between&rdquo; row gives the corresponding
</p>
<p>degrees of freedom and the formulas for calculating between-group vari-
</p>
<p>ability, between-group variance (in the &ldquo;Mean Square&rdquo; column), and the
</p>
<p>F-statistic. The &ldquo;Within&rdquo; row gives the corresponding degrees of freedom
</p>
<p>and the formulas for calculating within-group variability and within-group
</p>
<p>variance.
</p>
<p>Following is the ANOVA table for the results of our calculations using
</p>
<p>the data on the ages of white-collar criminals:
</p>
<p>Source df Sum of Squares Mean Square F
</p>
<p>Between 2 3,405.267 1,702.6335 11.8663
</p>
<p>Within 27 3,874.100 143.4852
</p>
<p>Total 29 7,279.367
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(Xi � Xg)
2
</p>
<p>WSS
</p>
<p>N � k�
N
</p>
<p>i�1
</p>
<p>(Xi � Xc)
2
</p>
<p>�̂2bg
</p>
<p>�̂2wg
</p>
<p>BSS
</p>
<p>k � 1�
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>The ANOVA Table</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>the differences in sample means indicate that there are differences in
</p>
<p>the population means?
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution for each population (the
</p>
<p>shape of the population distributions is unknown, and the sizes of the
</p>
<p>samples examined are small).
</p>
<p>Sampling Method: Independent random sampling (no replacement; sam-
</p>
<p>ples are small relative to populations).
</p>
<p>Sampling Frame: Felony drug defendants in large urban court districts in
</p>
<p>the United States in the 1990s.
</p>
<p>Population variances are equal .
</p>
<p>Hypotheses:
</p>
<p>H0: Population means of bail amounts set for felony drug defendants
</p>
<p>who are non-Hispanic whites, non-Hispanic African Americans, and His-
</p>
<p>panics of any race are equal (�1 � �2 � �3).
</p>
<p>H1: Population means of bail amounts set for felony drug defendants
</p>
<p>who are non-Hispanic whites, non-Hispanic African Americans, and His-
</p>
<p>panics of any race are not equal (�1 � �2 � �3).
</p>
<p>The Sampling Distribution We again use the F distribution to test for dif-
</p>
<p>ferences among our three sample means. Recall that we need two indi-
</p>
<p>cators of degrees of freedom: one for the between-group variance and
</p>
<p>one for the within-group variance.
</p>
<p>df for between-group variance � k � 1 � 3 � 1 � 2
</p>
<p>df for within-group variance � N � k � 36 � 3 � 33
</p>
<p>Significance Level and Rejection Region As in the preceding example, we
</p>
<p>do not have any particular concerns about the risk of a Type I or Type II
</p>
<p>error, so we can use the conventional 0.05 significance level. Given that
</p>
<p>we have degrees of freedom equal to 2 and 33 with a 0.05 significance
</p>
<p>level, the critical value of the F-statistic is about 3.29. If our calculated 
</p>
<p>F-statistic is greater than 3.29, then we will reject our null hypothesis of
</p>
<p>equal means.
</p>
<p>The Test Statistic We begin our calculation of the F-statistic by comput-
</p>
<p>ing estimates of the between-group variance and the within-group vari-
</p>
<p>to be 2,264,840,905.56.�̂2bg
</p>
<p>(�21 � �
2
2 � �
</p>
<p>2
3)
</p>
<p>ance. Applying the formula for between-group variance, we find the 
</p>
<p>estimate of 
</p>
<p>326</p>
<p/>
</div>
<div class="page"><p/>
<p>A N A L Y S I S O F V A R I A N C E
</p>
<p>W orking It Out
</p>
<p> � 2,264,840,905.56
</p>
<p> � 
4,529,681,811.11
</p>
<p>2
</p>
<p> � 
� 12(4,833.33 � 14,680.56)
</p>
<p>2
</p>
<p>� 12(8,833.33 � 14,680.56)2 � 12(30,375.00 � 14,680.56)2�
3 � 1
</p>
<p> �̂2bg � 
�
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>k � 1
</p>
<p>The value of the within-group variance is 617,193,813.13. Table
</p>
<p>12.7 presents the calculation of the within-group sum of squares, which
</p>
<p>turns out to be equal to 20,367,395,833.33. We then divide the value of
</p>
<p>the within-group sum of squares by the corresponding degrees of free-
</p>
<p>dom (N � k � 36 � 3 � 33), which gives us an estimate for the within-
</p>
<p>group variance of 617,193,813.13.
</p>
<p>(�̂2wg)
</p>
<p>W orking It Out
</p>
<p> � 617,193,813.13
</p>
<p> � 
20,367,395,833.33
</p>
<p>36 � 3
</p>
<p> �̂2wg � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � Xc)
2
</p>
<p>N � k
</p>
<p>The value of the F-statistic is obtained by dividing the estimate of
</p>
<p>between-group variance by the estimate of within-group variance. For
</p>
<p>our example, F is found to be 3.67.
</p>
<p>W orking It Out
</p>
<p> � 3.67
</p>
<p> � 
2,264,840,905.56
</p>
<p>617,193,813.13
</p>
<p> F � 
between-group variance
</p>
<p>within-group variance
</p>
<p>327</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>The Decision Since our test statistic of 3.67 is greater than the critical
</p>
<p>value of 3.29 for the F distribution, we reject the null hypothesis of equal
</p>
<p>population means; we conclude that there is a statistically significant re-
</p>
<p>lationship between bail amount and race of defendant. However, as in
</p>
<p>our example concerning age and white-collar crime, we began our test
</p>
<p>with strong doubts about whether we could meet a core assumption of
</p>
<p>analysis of variance. The samples examined are not large, and thus we
</p>
<p>cannot relax the normality assumption for our test. In turn, we do not
</p>
<p>have strong reason to believe that the populations to which we want to
</p>
<p>infer actually meet the criteria of this assumption. As has been noted
</p>
<p>throughout the text, statistical conclusions are only as solid as the as-
</p>
<p>sumptions the researchers make. In this case, our statistical conclusions
</p>
<p>clearly do not stand on solid ground.
</p>
<p>D e f i n i n g  t h e  S t r e n g t h  o f  t h e  R e l a t i o n s h i p  O b s e r v e d
</p>
<p>Even though analysis of variance is concerned with comparing means
</p>
<p>from independent samples, in practice the samples are usually defined
</p>
<p>as representing a multicategory nominal-level variable. For example, as
</p>
<p>noted earlier, in comparing three samples of white-collar criminals, we
</p>
<p>could define each as one category in a nominal-scale measure of type of
</p>
<p>white-collar crime. Similarly, in our example concerning the relationship
</p>
<p>between bail amount and race, we spoke about differences among three
</p>
<p>samples of offenders: non-Hispanic whites, non-Hispanic blacks, and
</p>
<p>Calculating the Within-Group Sum of Squares
</p>
<p>NON-HISPANIC NON-HISPANIC HISPANICS 
</p>
<p>WHITES BLACKS OF ANY RACE
</p>
<p>X X X
</p>
<p>1,000 14,694,418.89 1,000 61,361,058.89 1,000 862,890,625.00
1,000 14,694,418.89 1,000 61,361,058.89 2,000 805,140,625.00
1,500 11,111,088.89 2,000 46,694,398.89 4,000 695,640,625.00
2,000 8,027,758.89 2,500 40,111,068.89 5,000 643,890,625.00
2,500 5,444,428.89 3,000 34,027,738.89 10,000 415,140,625.00
3,000 3,361,098.89 4,000 23,361,078.89 12,500 319,515,625.00
5,000 27,778.89 5,000 14,694,418.89 25,000 28,890,625.00
7,000 4,694,458.89 10,000 1,361,118.89 25,000 28,890,625.00
7,500 7,111,128.89 12,500 13,444,468.89 25,000 28,890,625.00
7,500 7,111,128.89 20,000 124,694,518.89 40,000 92,640,625.00
</p>
<p>10,000 26,694,478.89 20,000 124,694,518.89 65,000 1,198,890,625.00
10,000 26,694,478.89 25,000 261,361,218.89 150,000 14,310,140,625.00
</p>
<p>� � 20,367,395,833.33
</p>
<p>(Xi � Xc )
2(Xi � Xc )
</p>
<p>2(Xi � Xc )
2
</p>
<p>Table 12.7
</p>
<p>328</p>
<p/>
</div>
<div class="page"><p/>
<p>D E F I N I N G T H E S T R E N G T H O F T H E R E L A T I O N S H I P O B S E R V E D
</p>
<p>Hispanics of any race. Nonetheless, these three samples can be seen as
</p>
<p>three groups in a nominal-level measure of race of defendant.
</p>
<p>ences between the means of the samples. With just three samples, we can
</p>
<p>get a pretty good sense of the strength of a relationship using this method.
</p>
<p>But even with three samples, it is difficult to summarize the extent of the
</p>
<p>relationship observed because we must look at three separate compar-
</p>
<p>isons (that between group 1 and group 2, that between group 2 and group
</p>
<p>3, and that between group 3 and group 1). With four samples, the number
</p>
<p>of comparisons is six; for seven samples, there are 21 comparisons.
</p>
<p>Clearly, it is useful, especially as the number of samples grows, to have a
</p>
<p>single statistic for establishing the strength of the observed relationship.
</p>
<p>A commonly used measure of association for ANOVA is a statistic
</p>
<p>called eta (�). Eta relies on the partialing of sums of squares to establish
</p>
<p>the relationship, or correlation, between the interval-level variable in
</p>
<p>ANOVA and the nominal-level variable. To calculate eta, we simply take
</p>
<p>the square root of the ratio of the between sum of squares to the total
</p>
<p>sum of squares (see Equation 12.11).
</p>
<p>Equation 12.11
</p>
<p>Although it might not seem so at first glance, this measure makes good
</p>
<p>sense. Understanding eta, however, will be easier if we start with an-
</p>
<p>other statistic, eta squared (�2), which is sometimes referred to as the
</p>
<p>percent of variance explained (see Equation 12.12).
</p>
<p>Equation 12.12
</p>
<p>Eta squared is the proportion of the total sum of squares that is accounted
</p>
<p>for by the between sum of squares. As previously noted, the between
</p>
<p>sum of squares is also defined as the explained sum of squares because it
</p>
<p>represents the part of the total variation that is accounted for by the dif-
</p>
<p>ferences between the samples. Eta squared thus identifies the proportion
</p>
<p>of the total sum of squares that is accounted for by the explained sum of
</p>
<p>squares&mdash;hence its identification as the percent of variance explained.
</p>
<p>The larger the proportion of total variance that is accounted for by the
</p>
<p>between sum of squares, the stronger the relationship between the nom-
</p>
<p>inal- and interval-level variables being examined. When the means of the
</p>
<p>samples are the same, eta squared will be 0. This means that there is no
</p>
<p>relationship between the nominal- and interval-level measures the study
</p>
<p>is examining. The largest value of eta squared is 1, meaning that all of
</p>
<p>�2 � 
BSS
TSS
</p>
<p>� � �BSSTSS
</p>
<p>identified?&rdquo; The simplest way to answer this question is to look at the differ-
</p>
<p>significant result is &ldquo;How strong is the overall relationship we have 
</p>
<p>Accordingly, one question we might ask after finding a statistically 
</p>
<p>329</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>the variability in the samples is accounted for by the between sum of
</p>
<p>squares. In practice, as eta squared increases in value between 0 and 1,
</p>
<p>the relationship being examined gets stronger.
</p>
<p>The square root of eta squared is a measure more sensitive to small
</p>
<p>relationships. For example, a value for eta squared of 0.04 is equal to a
</p>
<p>value for eta of 0.20, and a value for eta squared of 0.1 is equivalent to a
</p>
<p>value for eta of 0.32, as shown in Table 12.8. In criminal justice, where
</p>
<p>the relationships examined are often not very large, measures such as
</p>
<p>this one, which allow us to distinguish relatively small values more
</p>
<p>clearly, are particularly useful.
</p>
<p>Turning to our example concerning age and white-collar crime, we
</p>
<p>can see that the differences between the groups account for a good deal
</p>
<p>of the variability in the total sum of squares. Taking the between sum of
</p>
<p>squares for that example and dividing it by the total sum of squares
</p>
<p>gives a value for eta squared of 0.468.
</p>
<p>Comparing Eta Squared with Eta
</p>
<p>� 2 �
</p>
<p>0.00 0.00
0.01 0.10
0.02 0.14
0.03 0.17
0.04 0.20
0.05 0.22
0.10 0.32
0.25 0.50
0.50 0.71
0.75 0.87
1.00 1.00
</p>
<p>Table 12.8
</p>
<p>W orking It Out
</p>
<p> � 0.4678
</p>
<p> � 
3,405.267
</p>
<p>7,279.367
</p>
<p> �2 � 
BSS
TSS
</p>
<p> � 7,279.367
</p>
<p> � 3,405.267 � 3,874.100
</p>
<p> TSS � BSS � WSS
</p>
<p> BSS � 3,405.267
</p>
<p>330</p>
<p/>
</div>
<div class="page"><p/>
<p>M A K I N G P A I R W I S E C O M P A R I S O N S B E T W E E N T H E G R O U P S S T U D I E D
</p>
<p>By taking the square root of this value, we obtain a correlation coeffi-
</p>
<p>cient, or eta, of 0.684.
</p>
<p>W orking It Out
</p>
<p> � 0.6840
</p>
<p> � �0.4678
</p>
<p> � � �BSSTSS � �3,405.2677,279.367
</p>
<p>Does an eta of 0.684 signify a large or small relationship? To some ex-
</p>
<p>tent, differentiating between &ldquo;large&rdquo; and &ldquo;small&rdquo; in this context is a value
</p>
<p>judgment rather than a statistical decision. We might decide whether a
</p>
<p>particular value of eta is large or small based on results from other stud-
</p>
<p>ies in other areas of criminal justice or perhaps similar studies that drew
</p>
<p>different samples. There is no clear yardstick for making this decision.
</p>
<p>One psychologist suggests that any value for eta greater than 0.371 rep-
</p>
<p>resents a large effect.7 A moderate-size effect is indicated by a value of
</p>
<p>0.243. Using this criterion, we would define the relationship between age
</p>
<p>and type of white-collar crime as very strong. However, in this example,
</p>
<p>we should be cautious about relying on the results obtained. With small
</p>
<p>samples, the values of eta are not considered very reliable.8
</p>
<p>M a k i n g  P a i r w i s e  C o m p a r i s o n s  B e t w e e n  t h e  G r o u p s  S t u d i e d
</p>
<p>Once you have established through an analysis of variance that there is a
</p>
<p>statistically significant difference across the samples studied, you may
</p>
<p>want to look at differences between specific pairs of the samples. To do
</p>
<p>this, you make comparisons between two sample means at a time. Such
</p>
<p>comparisons within an analysis of variance are often called pairwise
</p>
<p>comparisons.
</p>
<p>7See Jacob Cohen, Statistical Power Analysis for the Behavorial Sciences (Hillsdale, NJ:
</p>
<p>Lawrence Erlbaum, 1988), pp. 285&ndash;287.
8Once again, there is no universally accepted definition of what is &ldquo;small.&rdquo; There will
</p>
<p>be little question regarding the validity of your estimate of eta if your samples meet
</p>
<p>the 30 cases minimum defined for invoking the central limit theorem. Some statisti-
</p>
<p>cians suggest that you will gain relatively reliable estimates even for samples as small
</p>
<p>as 10 cases.
</p>
<p>331</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>It would seem, at first glance, that you could simply apply the two-
</p>
<p>sample t-test discussed in Chapter 11 to test hypotheses related to such
</p>
<p>comparisons. However, you are faced with a very important statistical
</p>
<p>problem. If you run a number of t-tests at the same time, you are un-
</p>
<p>fairly increasing your odds of obtaining a statistically significant finding
</p>
<p>along the way. For example, let&rsquo;s say that you have conducted an analy-
</p>
<p>sis of variance comparing seven samples and obtained a statistically sig-
</p>
<p>nificant result. You now want to look at the pairwise comparisons to see
</p>
<p>which of the specific comparisons are significantly different from one an-
</p>
<p>other. There are a total of 21 separate comparisons to make (see Table
</p>
<p>12.9). For each test, you set a significance level of 0.05, which means
</p>
<p>that you are willing to take a 1 in 20 chance of falsely rejecting the null
</p>
<p>hypothesis. Thus, if you run 20 tests, you might expect to get at least one
</p>
<p>statistically significant result just by chance.
</p>
<p>Here a finding of a significant result could simply be attributed to the
</p>
<p>fact that you have run a large number of tests. Accordingly, to be fair, you
</p>
<p>should adjust your tests to take into account the change in the probabili-
</p>
<p>ties that results from looking at a series of pairwise comparisons. A num-
</p>
<p>ber of different tests allow you to do this; many are provided in standard
</p>
<p>statistical packages.9 One commonly used test is the honestly significant
</p>
<p>difference (HSD) test developed by John Tukey (see Equation 12.13).
</p>
<p>Equation 12.13
</p>
<p>HSD defines the value of the difference between the pairwise compar-
</p>
<p>isons that is required to reject the null hypothesis at a given level of sta-
</p>
<p>tistical significance.
</p>
<p>HSD � Pcrit � �̂
2
wg
</p>
<p>Nc
</p>
<p>The 21 Separate Pairwise Comparisons To Be Made 
for an Analysis of Variance with Seven Samples (Categories)
</p>
<p>SAMPLE 1 2 3 4 5 6 7
</p>
<p>1
2 �
3 � �
4 � � �
5 � � � �
6 � � � � �
7 � � � � � �
</p>
<p>Table 12.9
</p>
<p>9For a discussion of pairwise comparison tests, see A. J. Klockars and G. Sax, Multiple
</p>
<p>Comparisons (Quantitative Applications in the Social Science, Vol. 61) (London: Sage,
</p>
<p>1986).
</p>
<p>332</p>
<p/>
</div>
<div class="page"><p/>
<p>M A K I N G P A I R W I S E C O M P A R I S O N S B E T W E E N T H E G R O U P S S T U D I E D
</p>
<p>For our white-collar crime example, with a conventional 5% signifi-
</p>
<p>cance threshold, we first identify the critical value (Pcrit) associated with
</p>
<p>that significance threshold by looking at Appendix 6. With three samples
</p>
<p>and 27 degrees of freedom in the within sum of squares estimate, the
</p>
<p>critical value is about 3.51. We then multiply this value by the square
</p>
<p>root of the within-group variance estimate ( ) divided by the number
</p>
<p>of cases in each sample (Nc)&mdash;in our example, 10.
10 Our result is 13.296,
</p>
<p>meaning that the absolute value of the difference in mean age between
</p>
<p>the pairwise comparisons must be greater than 13.296 to reject the null
</p>
<p>hypothesis of no difference (using a 5% significance threshold).
</p>
<p>�̂ 2wg
</p>
<p>10Most pairwise comparison tests, including Tukey&rsquo;s HSD test, require that the sample
</p>
<p>sizes of the groups examined be equal. While most statistical software packages pro-
</p>
<p>vide adjustments of these tests to account for unequal sample sizes, there is still de-
</p>
<p>bate over whether the estimates gained can be relied upon [e.g., see Robert R. J. Sokal
</p>
<p>and F. J. Rohlf, Biometry: The Principles and Practice of Statistics in Biological Re-
</p>
<p>search, 3rd ed. (New York: W. H. Freeman, 1995), Chap. 9]. Irrespective of this de-
</p>
<p>bate, when unequal sample sizes are examined, the adjusted estimates are to be pre-
</p>
<p>ferred over the unadjusted estimates.
</p>
<p>W orking It Out
</p>
<p> � 13.296
</p>
<p> � 3.51 �143.4910
</p>
<p>HSD � Pcrit � �̂
2
wg
</p>
<p>Nc
</p>
<p>Table 12.10 shows the absolute differences found for the three com-
</p>
<p>parisons between means. Two of the three comparisons are statistically
</p>
<p>significant at the 5% level&mdash;the absolute differences are greater than
</p>
<p>13.296 for the difference between bank embezzlers and bribery offend-
</p>
<p>ers and for that between bank embezzlers and antitrust offenders. How-
</p>
<p>have to conclude that our sample results do not provide persuasive evi-
</p>
<p>dence for stating that the mean ages of bribery and antitrust offenders
</p>
<p>are different in the larger populations from which these two samples
</p>
<p>were drawn.
</p>
<p>result just misses the value needed to reject the null hypothesis. We would
</p>
<p>ever, for the difference between bribery and antitrust offenders, our 
</p>
<p>333</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>The comparisons we have made so far have been based on a statis-
</p>
<p>tically significant overall result for ANOVA. Should such comparisons
</p>
<p>be made if the overall differences across the means are not statistically
</p>
<p>significant? In general, it is not a good idea to look for pairwise com-
</p>
<p>parisons if the overall analysis of variance is not statistically signifi-
</p>
<p>cant. This is a bit like going fishing for a statistically significant result.
</p>
<p>However, sometimes one or another of the pairwise comparisons is of
</p>
<p>pothesis for a pairwise comparison, it is acceptable to examine it, irre-
</p>
<p>spective of the outcomes of the larger test. In such circumstances, it is
</p>
<p>also acceptable to use a simple two-sample t-test to examine group
</p>
<p>differences.
</p>
<p>A  N o n p a r a m e t r i c  A l t e r n a t i v e :  T h e  K r u s k a l - W a l l i s  T e s t
</p>
<p>For studies where you cannot meet the parametric assumptions of the
</p>
<p>analysis of variance test, you may want to consider a nonparametric
</p>
<p>rank-order test. In performing a rank-order test, you lose some
</p>
<p>crucial information because you focus only on the order of scores and
</p>
<p>not on the differences in values between them. However, such tests
</p>
<p>have the advantage of not requiring assumptions about the population
</p>
<p>distribution.
</p>
<p>One rank-order test is the Kruskal-Wallis test. As a nonparametric
</p>
<p>test of statistical significance, it requires neither a normal distribution
</p>
<p>nor equal variances between the groups studied. The test asks simply
</p>
<p>whether the distribution of ranked scores in the three groups is what
</p>
<p>would be expected under a null hypothesis of no difference. When the
</p>
<p>Results of the Pairwise Comparison Tests
</p>
<p>OFFENSE 1 OFFENSE 3
</p>
<p>BANK OFFENSE 2 ANTITRUST 
</p>
<p>EMBEZZLEMENT BRIBERY VIOLATION
</p>
<p>Offense 1
Bank Embezzlement
</p>
<p>Offense 2 16.300*
Bribery
</p>
<p>Offense 3 25.800* 9.500
Antitrust Violation
</p>
<p>*p � 0.5
</p>
<p>Table 12.10
</p>
<p>develop your analyses. However, if you do start off with a strong hy-
</p>
<p>particular interest. Such interest should be determined before you 
</p>
<p>334</p>
<p/>
</div>
<div class="page"><p/>
<p>A N O N P A R A M E T R I C A L T E R N A T I V E
</p>
<p>number of cases in each group is greater than 5, the sampling distribu-
</p>
<p>tion of the Kruskal-Wallis test score, denoted H, is approximately chi-
</p>
<p>square.
</p>
<p>As an illustration, let&rsquo;s examine whether this nonparametric test sug-
</p>
<p>gests significant differences in terms of age across the white-collar crime
</p>
<p>categories of our earlier example.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Ordinal scale.
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method: Independent random sampling (no replacement; sam-
</p>
<p>ple is small relative to population).
</p>
<p>Sampling Frame: All white-collar offenders convicted of the crimes ex-
</p>
<p>amined in seven federal judicial districts over a three-year period.
</p>
<p>Hypotheses:
</p>
<p>H0: The distribution of ranked scores is identical in the three populations.
</p>
<p>H1: The distribution of ranked scores differs across the three populations.
</p>
<p>the sample. To obtain this measure, we simply rank the 30 subjects stud-
</p>
<p>ied according to age, with the youngest offender given a rank of 1 and
</p>
<p>the oldest a rank of 30 (see Table 12.11). In the case of ties, subjects
</p>
<p>share a rank. For example, the two subjects aged 29 share the rank of
</p>
<p>6.5 (the average of ranks 6 and 7), and the three subjects aged 62 share
</p>
<p>the rank of 26 (the average of ranks 25, 26, and 27).
</p>
<p>White-Collar Offenders Ranked According to Age
</p>
<p>OFFENSE 1 OFFENSE 3
</p>
<p>BANK OFFENSE 2 ANTITRUST 
</p>
<p>EMBEZZLEMENT BRIBERY VIOLATION
</p>
<p>Age Rank Age Rank Age Rank
</p>
<p>19 1 28 5 35 11.5
21 2 29 6.5 46 16
23 3 32 10 48 17.5
25 4 40 13 53 20
29 6.5 42 14.5 58 22
30 8 48 17.5 61 24
31 9 58 22 62 26
35 11.5 58 22 62 26
42 14.5 64 28 62 26
49 19 68 29 75 30
</p>
<p>� � 219� � 167.5� � 78.5
</p>
<p>Table 12.11
</p>
<p>In this test, we use an ordinal-level measure: the rank order of ages in
</p>
<p>335</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>Our null hypothesis is that the distribution of ranked scores in the
</p>
<p>three populations is identical. Our research hypothesis is that it is not
</p>
<p>identical across the three populations.
</p>
<p>Sampling Distribution The sampling distribution H is distributed approx-
</p>
<p>imately according to chi-square because the number of cases in each
</p>
<p>group is greater than 5. The number of degrees of freedom for the distri-
</p>
<p>bution is defined as k � 1, where k refers to the number of samples (or
</p>
<p>categories). Because our example involves three samples, the number of
</p>
<p>degrees of freedom for the chi-square distribution is 3 � 1, or 2.
</p>
<p>Significance Level and Rejection Region Consistent with our earlier
</p>
<p>choice of a 0.05 significance threshold, we turn to the 0.05 value with 2
</p>
<p>degrees of freedom in the chi-square table (see Appendix 2). The critical
</p>
<p>value identified is 5.991.
</p>
<p>The Test Statistic The formula for H given in Equation 12.14 looks
</p>
<p>complex. However, it is relatively simple to compute if broken into
</p>
<p>pieces.
</p>
<p>Equation 12.14
</p>
<p>There is only one complex term in the equation. It is
</p>
<p>This term tells us to take the sum of the ranks in each sample, square it,
</p>
<p>and divide it by the number of cases in the sample; then we sum these
</p>
<p>values for all the samples.11 The H-score obtained for our problem is
</p>
<p>13.038.
</p>
<p>�
k
</p>
<p>c�1
</p>
<p> 
��Nc
</p>
<p>i�1
</p>
<p> Ri�2
Nc
</p>
<p>H � 	� 12N(N � 1)� ��kc�1 ��
Nc
</p>
<p>i�1
</p>
<p> Ri�2
Nc �
� 3(N � 1)
</p>
<p>11Most statistical computing packages provide an alternative calculation that adjusts for
</p>
<p>ties. In practice, the differences between using this correction procedure and perform-
</p>
<p>ing the unadjusted test are generally small. For our example, where there are a large
</p>
<p>number of ties relative to the sample size (14/30), the difference in the observed sig-
</p>
<p>nificance level is only 0.0001.
</p>
<p>336</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S U M M A R Y
</p>
<p>The Decision As with the F-test, our score exceeds the critical value
</p>
<p>needed to reject the null hypothesis of no difference. The observed sig-
</p>
<p>nificance level of our test is thus less than the criterion significance level
</p>
<p>we set at the outset (p � 0.05). From the Kruskal-Wallis test, we can
</p>
<p>again conclude that there is a statistically significant relationship between
</p>
<p>type of white-collar crime and age of offender. This time, however, we
</p>
<p>can have more confidence in our conclusion because the assumptions of
</p>
<p>the test are met more strictly.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>ANOVA is a parametric test of statistical significance that allows a researcher
</p>
<p>to compare means across more than two groups. It takes into account not
</p>
<p>only variability between groups but also variability within groups. The larger
</p>
<p>the differences between the groups relative to the variability within them,
</p>
<p>the more confidence the researcher can have in a conclusion that differ-
</p>
<p>ences exist among the population means. Between-group variability is mea-
</p>
<p>sured by the between sum of squares (or explained sum of squares).
</p>
<p>Within-group variability is measured by the within sum of squares (or un-
</p>
<p>explained sum of squares). The total sum of squares is equal to the
</p>
<p>sum of the between and within sums of squares. To develop estimates of
</p>
<p>population variances, the sums of squares are divided by the appropriate
</p>
<p>degrees of freedom. ANOVA requires the following assumptions: interval
</p>
<p>scales, normal population distributions, independent random sampling, and
</p>
<p>homoscedasticity. The sampling distribution for ANOVA is denoted as F.
</p>
<p>W orking It Out
</p>
<p> � 13.038
</p>
<p> � � 12930�8,217.95 � 3(31)
 � � 1230(31)��(78.5)
</p>
<p>2
</p>
<p>10
 � 
</p>
<p>(167.5)2
</p>
<p>10
 � 
</p>
<p>(219)2
</p>
<p>10 � � 3(31)
</p>
<p> H � 	� 12N(N � 1)���kc�1 ��
Nc
</p>
<p>i�1
</p>
<p> Ri�2
Nc �
 � 3(N � 1)
</p>
<p>337</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>The F-value needed to reject the null hypothesis gets smaller as the
</p>
<p>within-group degrees of freedom grows. The F-statistic is calculated by
</p>
<p>dividing the between-group variance by the within-group variance.
</p>
<p>The strength of the relationship observed is measured by the statistic
</p>
<p>eta squared, or percent of variance explained. Eta squared is the
</p>
<p>ratio of the between sum of squares to the total sum of squares. An eta
</p>
<p>squared value of 0 indicates that there is no relationship between the
</p>
<p>nominal and interval variables (i.e., the means are the same). An eta
</p>
<p>squared value of 1 represents a perfect relationship between the nominal
</p>
<p>A researcher who wishes to compare means of pairs of specific sam-
</p>
<p>ples within a larger test makes a pairwise comparison. Running a se-
</p>
<p>ries of two-sample t-tests, however, unfairly increases the odds of getting
</p>
<p>a statistically significant result. The honestly significant difference
</p>
<p>(HSD) test is a pairwise comparison test that corrects for this bias.
</p>
<p>When the assumptions underlying ANOVA are difficult to meet, the
</p>
<p>researcher may choose a nonparametric alternative&mdash;the Kruskal-Wallis
</p>
<p>test. This test does not require an assumption of normal population dis-
</p>
<p>tributions or homoscedasticity. As a rank-order test, however, it does
</p>
<p>not use all of the information available from interval-level data.
</p>
<p>K e y  T e r m s
</p>
<p>analysis of variance (ANOVA) A para-
</p>
<p>metric test of statistical significance that
</p>
<p>assesses whether differences in the means
</p>
<p>of several samples (groups) can lead the
</p>
<p>researcher to reject the null hypothesis
</p>
<p>that the means of the populations from
</p>
<p>which the samples are drawn are the
</p>
<p>same.
</p>
<p>between sum of squares (BSS) A mea-
</p>
<p>sure of the variability between samples
</p>
<p>(groups). The between sum of squares is
</p>
<p>calculated by taking the sum of the
</p>
<p>squared deviation of each sample mean
</p>
<p>from the grand mean multiplied by the
</p>
<p>number of cases in that sample.
</p>
<p>correlation A measure of the strength of a
</p>
<p>relationship between two variables.
</p>
<p>eta A measure of the degree of correlation
</p>
<p>between an interval-level and a nominal-
</p>
<p>level variable.
</p>
<p>eta squared The proportion of the total
</p>
<p>sum of squares that is accounted for by the
</p>
<p>between sum of squares. Eta squared is
</p>
<p>sometimes referred to as the percent of
</p>
<p>variance explained.
</p>
<p>explained sum of squares (ESS) Another
</p>
<p>name for the between sum of squares. The
</p>
<p>explained sum of squares is the part of the
</p>
<p>total variability that can be explained by
</p>
<p>visible differences between the groups.
</p>
<p>grand mean The overall mean of every
</p>
<p>single case across all of the samples.
</p>
<p>honestly significant difference (HSD)
</p>
<p>test A parametric test of statistical signifi-
</p>
<p>338
</p>
<p>and interval variables. The correlation coefficient, or eta, is obtained 
</p>
<p>by taking the square root of eta squared.</p>
<p/>
</div>
<div class="page"><p/>
<p>S Y M B O L S A N D F O R M U L A S
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>Xi Individual subject or score
</p>
<p>Sample or category mean
</p>
<p>Grand or overall mean
</p>
<p>Nc Number of cases in each sample
</p>
<p>k Number of categories or samples
</p>
<p>� Correlation coefficient eta
</p>
<p>�2 Percent of variance explained; eta squared
</p>
<p>Pcrit Critical value for HSD test
</p>
<p>Ri Individual rank of score
</p>
<p>To calculate the between sum of squares:
</p>
<p>BSS � �
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2]
</p>
<p>Xg
</p>
<p>Xc
</p>
<p>cance, adjusted for making pairwise compar-
</p>
<p>isons. The HSD test defines the difference
</p>
<p>between the pairwise comparisons required
</p>
<p>to reject the null hypothesis.
</p>
<p>Kruskal-Wallis test A nonparametric test
</p>
<p>of statistical significance for multiple
</p>
<p>groups, requiring at least an ordinal scale
</p>
<p>of measurement.
</p>
<p>overall mean See grand mean.
</p>
<p>pairwise comparisons Comparisons
</p>
<p>made between two sample means ex-
</p>
<p>tracted from a larger statistical analysis.
</p>
<p>percent of variance explained The pro-
</p>
<p>portion of the total sum of squares that is
</p>
<p>accounted for by the explained sum of
</p>
<p>squares; eta squared.
</p>
<p>rank-order test A test of statistical
</p>
<p>significance that uses information relating to
</p>
<p>the relative order, or rank, of variable scores.
</p>
<p>sum of squares The sum of squared devia-
</p>
<p>tions of scores from a mean or set of means.
</p>
<p>total sum of squares (TSS) A measure of
</p>
<p>the total amount of variability across all of
</p>
<p>the groups examined. The total sum of
</p>
<p>squares is calculated by summing the
</p>
<p>squared deviation of each score from the
</p>
<p>grand mean.
</p>
<p>unexplained sum of squares (USS) An-
</p>
<p>other name for the within sum of squares.
</p>
<p>The unexplained sum of squares is the part
</p>
<p>of the total variability that cannot be ex-
</p>
<p>plained by visible differences between the
</p>
<p>groups.
</p>
<p>within sum of squares (WSS) A measure
</p>
<p>of the variability within samples (groups).
</p>
<p>The within sum of squares is calculated by
</p>
<p>summing the squared deviation of each
</p>
<p>score from its sample mean.
</p>
<p>339</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>To calculate the within sum of squares:
</p>
<p>To calculate the total sum of squares:
</p>
<p>To partition the total sum of squares:
</p>
<p>Total sum of squares � between sum of squares 
</p>
<p>� within sum of squares
</p>
<p>TSS � BSS � WSS
</p>
<p>To estimate between-group variance:
</p>
<p>To estimate within-group variance:
</p>
<p>To calculate F:
</p>
<p>To calculate eta:
</p>
<p>To calculate eta squared:
</p>
<p>To perform the HSD test:
</p>
<p>To perform the Kruskal-Wallis test:
</p>
<p>H � 	� 12N(N � 1)���kc�1 ��
Nc
</p>
<p>i�1
</p>
<p> Ri�2
Nc �
 � 3(N � 1)
</p>
<p>HSD � Pcrit � �̂
2
wg
</p>
<p>Nc
</p>
<p>�2 � 
BSS
TSS
</p>
<p>� � �BSSTSS
</p>
<p>F � 
between-group variance
</p>
<p>within-group variance
</p>
<p>�̂2wg � 
�
N
</p>
<p>i�1
</p>
<p>(Xi � Xc)
2
</p>
<p>N � k
</p>
<p>�̂2bg � 
�
k
</p>
<p>c�1
</p>
<p>[Nc(Xc � Xg)
2
</p>
<p>k � 1
</p>
<p>TSS � �
N
</p>
<p>i�1
</p>
<p>(Xi � Xg)
2
</p>
<p>WSS � �
N
</p>
<p>i�1
</p>
<p>(Xi � Xc)
2
</p>
<p>340</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S
</p>
<p>E x e r c i s e s
</p>
<p>12.1 Dawn, a criminal justice researcher, gives 125 pretrial defendants
scores based on a questionnaire that assesses their ability to under-
stand the court process. The defendants were selected from five sepa-
rate counties. Dawn took an independent random sample of 25 defen-
dants from each county. The scores for the five populations are
normally distributed. Dawn runs an ANOVA test on her results, which
produces a test statistic of 3.35.
</p>
<p>a. Would Dawn be able to reject her null hypothesis that there is no
difference between the populations in their ability to comprehend
the court process if she were to set a 5% significance level?
</p>
<p>b. Would she be able to reject the null hypothesis using a 1% signifi-
cance level?
</p>
<p>c. Would your answer to either part a or part b be different if Dawn&rsquo;s
sample had consisted of five equally sized groups of 200 subjects
each?
</p>
<p>12.2 Random samples of individuals were drawn from three neighborhoods
by a policing research foundation to study the level of public support
for the local police department. The research foundation constructed a
complicated interval-level measure of police support, in which higher
values indicated more support. The researchers found the following
pattern across the three neighborhoods: The mean level of support in
neighborhood A was 3.1 (N � 15); in neighborhood B, it was 5.6 
(N � 17); and in neighborhood C, 4.2 (N � 11). The measure of be-
tween-group variance was 4.7, and the measure of within-group vari-
ance was 1.1.
</p>
<p>a. If the significance level is 0.05, can the research foundation con-
clude that there are different levels of support for the police depart-
ment across neighborhoods? Write out all of the steps of a test of
statistical significance, including any violations of assumptions.
</p>
<p>b. What if the significance level is 0.01?
</p>
<p>12.3 Random sampling of individuals with four different majors at a univer-
sity found the following grade point averages (GPAs) for the four
groups:
</p>
<p>Major A: GPA � 3.23 (N � 178)
</p>
<p>Major B: GPA � 2.76 (N � 64)
</p>
<p>Major C: GPA � 2.18 (N � 99)
</p>
<p>Major D: GPA � 3.54 (N � 121)
</p>
<p>If the between-group variance is 5.7 and the within-group variance is
1.5, are the GPAs different for the four majors? Use a significance level
of 0.01, and write out all of the steps of a test of statistical significance,
including any violations of assumptions.
</p>
<p>341</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>12.4 Random sampling offenders convicted of minor drug possession in
Border County found the average jail sentence for white offenders
to be 86 days (N � 15), for African American offenders to be 99
days (N � 10), and for Hispanic offenders to be 72 days (N � 7).
Further analysis of jail sentence lengths by race found the between
sum of squares to be 250 and the within sum of squares to be
1,300. Are the jail sentence lengths significantly different across 
race categories?
</p>
<p>a. Use a significance level of 0.05. Write out all of the steps of a 
test of statistical significance, including any violations of
assumptions.
</p>
<p>b. Would the conclusion be any different if the significance level had
been set at 0.01?
</p>
<p>12.5 Listed below is a set of data identifying previous convictions for any
</p>
<p>Robbery Rape Murder Drug Dealing
</p>
<p>1 1 0 5
</p>
<p>0 1 0 3
</p>
<p>2 1 0 7
</p>
<p>6 0 6 4
</p>
<p>4 0 2 8
</p>
<p>5 2 7 0
</p>
<p>3 2 1 6
</p>
<p>1 1 4 2
</p>
<p>5 0 2 1
</p>
<p>3 2 3 4
</p>
<p>Calculate the following values:
</p>
<p>a.
</p>
<p>b. df for between-group variance
</p>
<p>c. df for within-group variance
</p>
<p>d. the four values of 
</p>
<p>e. the total sum of squares
</p>
<p>f. the between sum of squares
</p>
<p>g. the within sum of squares
</p>
<p>12.6 Convicted drug dealers held in Grimsville Prison are placed in cell
block A, B, or C according to their city of origin. Danny (who has little
knowledge of statistics) was once an inmate in the prison. Now re-
</p>
<p>Xc
</p>
<p>Xg
</p>
<p>342
</p>
<p>murder, and drug dealing.
offense of 40 inmates serving prison sentences for robbery, rape, </p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S
</p>
<p>leased, he still bears a grudge against the prison authorities. Danny
wishes to make up a series of statistics to show that the convicts in the
various blocks are treated differently. According to his fictitious sam-
ple, the mean number of hours of exercise per week given to the in-
mates is 10 hours for block A offenders, 20 hours for block B offend-
ers, and 30 hours for block C offenders. Shown below are two
fictitious sets of results.
</p>
<p>Fictitious study 1:
</p>
<p>Block A Block B Block C
</p>
<p>9 21 30
</p>
<p>10 19 29
</p>
<p>9 20 31
</p>
<p>11 19 29
</p>
<p>11 21 31
</p>
<p>Fictitious study 2:
</p>
<p>Block A Block B Block C
</p>
<p>18 16 37
</p>
<p>16 18 36
</p>
<p>10 2 7
</p>
<p>2 31 41
</p>
<p>4 33 29
</p>
<p>a. From simply looking at the numbers, without running any
statistical tests, which of the two fictitious studies would you
expect to provide stronger backing for Danny&rsquo;s claim? Explain
your answer.
</p>
<p>b. Calculate the between sum of squares and the within sum of
squares for study 1.
</p>
<p>c. Calculate the between sum of squares and the within sum of
squares for study 2.
</p>
<p>d. Calculate the value of eta for each study. How do you account for
the difference?
</p>
<p>12.7 A researcher takes three independent random samples of young pick-
pockets and asks them how old they were when they first committed
the offense. The researcher wishes to determine whether there are any
differences among the three populations from which the samples were
drawn&mdash;those who have no siblings, those who have one or two sib-
lings, and those with three or more siblings.
</p>
<p>X � 30X � 20X � 10
</p>
<p>X � 30X � 20X � 10
</p>
<p>343</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S
</p>
<p>Age at first theft:
</p>
<p>0 Siblings 1 or 2 Siblings 3� Siblings
</p>
<p>10 14 15
</p>
<p>8 15 15
</p>
<p>16 15 10
</p>
<p>14 13 13
</p>
<p>7 12 16
</p>
<p>8 9 15
</p>
<p>a. Show that the total sum of squares is equal to the between sum of
squares plus the within sum of squares.
</p>
<p>b. What is the value of eta?
</p>
<p>c. Can the researcher reject the null hypothesis on the basis of the dif-
ferences observed? Run an F-test using a 5% significance level. Re-
member to outline all of the steps of a test of statistical significance,
including any violations of assumptions.
</p>
<p>12.8 Using independent random sampling, Sophie draws samples from
three different populations: psychologists, police officers, and factory
workers. She gives each subject a hypothetical case study of a drug
dealer who has been found guilty and awaits sentencing. The subjects
are then asked to suggest how many years the drug dealer should
serve in prison. The results are presented below:
</p>
<p>Psychologists Police Factory Workers
</p>
<p>2 3 5
</p>
<p>1 2 6
</p>
<p>0 3 4
</p>
<p>0 3 8
</p>
<p>1 4 7
</p>
<p>2.5 1 7
</p>
<p>2 1.5 6
</p>
<p>1.5 0 2
</p>
<p>4 0.5 3
</p>
<p>1 7 2
</p>
<p>a. Can Sophie conclude that the three populations are different in
terms of their attitudes toward punishing convicted drug dealers?
Run an F-test using a 5% significance level. Remember to outline all
of the steps of a test of statistical significance, including any viola-
tions of assumptions.
</p>
<p>b. Would Sophie&rsquo;s decision be any different if she chose a 1% or a
0.1% level of significance?
</p>
<p>344</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S
</p>
<p>c. Calculate the value of eta for the results above. Is the relationship a
strong one?
</p>
<p>12.9 For the data in Exercise 12.4, run a Kruskal-Wallis test using a 5%
level of statistical significance. Remember to outline all of the steps of
a test of statistical significance, including any violations of assump-
tions. Are you able to reject the null hypothesis?
</p>
<p>345
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>In the following discussion for estimating one-way analysis of variance models 
</p>
<p>in SPSS and Stata, you may find it useful to open the data file presented in Table 
</p>
<p>12.4 in the text (ex12_1.sav or ex12_1.dta) or the corresponding syntax file 
</p>
<p>(Chapter_12.sps or Chapter_12.do).
</p>
<p>SPSS
</p>
<p>ANOVA
</p>
<p>To compute a one-way ANOVA model in SPSS, you will use the ONEWAY 
</p>
<p>command:
</p>
<p>The output will present the ANOVA table (discussed in the box on p. 325). If 
</p>
<p>you run this command using the data from Table 12.4 (i.e., ex12_1.sav), it would 
</p>
<p>be
</p>
<p>Tukey&rsquo;s HSD
</p>
<p>The ONEWAY command will perform a wide range of additional calculations 
</p>
<p>on a data file, including Tukey&rsquo;s HSD statistic. To obtain Tukey&rsquo;s HSD statistic, 
</p>
<p>execute the ONEWAY command with the /POSTHOC = TUKEY option:
</p>
<p>The value of the F-test reported by SPSS matches that reported on page 321.
</p>
<p>ANOVA results can also be obtained with the MEANS command:
</p>
<p>where the /CELLS option will generate a table of results that lists the mean, 
</p>
<p>number of cases, and standard deviation for the variable(s) of interest by group 
</p>
<p>category. The /STATISTICS option with ANOVA will generate the ANOVA 
</p>
<p>table produced by ONEWAY (and discussed on p. 325). The ANOVA option 
</p>
<p>will also compute the value of Eta and Eta-squared, which are not available in 
</p>
<p>the ONEWAY command.
</p>
<p>ONEWAY variable_name(s) BY grouping_variable.
</p>
<p>ONEWAY age BY crime.
</p>
<p>MEANS TABLES = variable_name(s) BY grouping_variable
</p>
<p>/CELLS MEAN COUNT STDDEV
</p>
<p>/STATISTICS ANOVA.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S346
</p>
<p>The output presented will contain the ANOVA table that you have already seen 
</p>
<p>and an additional table that lists all possible comparisons of group means.
</p>
<p>If  you run this command using the data from Table 12.4, the command  
</p>
<p>would be
</p>
<p>Executing this command will reproduce the results from Table 12.10. The three 
</p>
<p>major rows in this table represent the three samples of offenders. Within each 
</p>
<p>major row are two smaller rows that represent contrasts between the groups. So, 
</p>
<p>for example, in the first major row (the embezzlement sample), there are calcula-
</p>
<p>tions for the mean of this group minus the mean of the second group (the bribery 
</p>
<p>sample) in the first line, followed by calculations for the mean of the first group 
</p>
<p>minus the mean of the third group (the antitrust sample) in the second line. The 
</p>
<p>values for Tukey&rsquo;s HSD reported in the first major row match those reported 
</p>
<p>in Table 12.10. In the second major row (the bribery sample), the second line 
</p>
<p> represents the difference between this group&rsquo;s mean and the mean for the third 
</p>
<p>group (the antitrust sample), and the value for Tukey&rsquo;s HSD again matches that 
</p>
<p>reported in Table 12.10.
</p>
<p>Sometimes the labels in the table of results for Tukey&rsquo;s HSD can be 
</p>
<p> confusing, so you will need to pay attention to the lines you are working with. 
</p>
<p>Keep in mind that the variable listed in the first column of each major row has 
</p>
<p>the mean for every other group (listed in the second column) subtracted  
</p>
<p>from its mean.
</p>
<p>Kruskal&ndash;Wallis Test
</p>
<p>The Kruskal&ndash;Wallis test is available in SPSS through the use of the NPAR  
</p>
<p>command with the /K-W option:
</p>
<p>ONEWAY variable_name BY grouping_variable
</p>
<p>/POSTHOC = TUKEY.
</p>
<p>ONEWAY age BY crime
</p>
<p>/POSTHOC = TUKEY.
</p>
<p>Note the parentheses following the name of the grouping variable. The values to 
</p>
<p>be included here are the values representing the minimum (minrange) and maxi-
</p>
<p>mum (maxrange) for the grouping variable. Returning to the use of the data in 
</p>
<p>Table 12.4, we would enter the following command:
</p>
<p>where 1 and 3 represent the bounds of the grouping variable.
</p>
<p>NPAR TESTS
</p>
<p>/K-W = variable_name BY grouping_variable (minrange, maxrange).
</p>
<p>NPAR TESTS
</p>
<p>/K-W = age BY crime (1,3).</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 347
</p>
<p>The output generated by the NPAR TESTS command will contain two small 
</p>
<p>tables. The first table lists each group or category and its average rank. The sec-
</p>
<p>ond table presents the results for the Kruskal&ndash;Wallis test. Note that the value of 
</p>
<p>the test statistic reported by SPSS differs slightly from that reported in the text 
</p>
<p>(SPSS: 13.073; text: 13.038). The reason for this difference was noted in footnote 
</p>
<p>11: SPSS corrects the calculation of the test statistic by adjusting for ties in rank, 
</p>
<p>and the formula in the text does not make such a correction.
</p>
<p>Stata
</p>
<p>ANOVA
</p>
<p>Similar to many other statistical packages, there are multiple ways of obtaining 
</p>
<p>one-way ANOVA results in Stata. The two most direct commands are oneway 
</p>
<p>and anova. The oneway command provides sufficient information for most  
</p>
<p>purposes. If you are interested in obtaining Tukey&rsquo;s HSD, however, you will need 
</p>
<p>to use the anova command. To compute a one-way ANOVA model in Stata, 
</p>
<p>you will use the oneway command:
</p>
<p>The output will present the ANOVA table. If the tabulate option is included 
</p>
<p>on the command line, Stata will generate group means, standard deviations, 
</p>
<p>and counts for the number of cases on the variable of interest. If you omit the 
</p>
<p>tabulate option, oneway will simply generate an ANOVA table.
</p>
<p>To run this command using the data from Table 12.4 (i.e., ex12_1.dta) enter 
</p>
<p>the following:
</p>
<p>The value of the F-test reported by SPSS matches that reported on page 321.
</p>
<p>The format for the anova command is identical:
</p>
<p>Not surprisingly, the output from this command will be the ANOVA table.
</p>
<p>Tukey&rsquo;s HSD
</p>
<p>Tukey&rsquo;s HSD statistic is not an option in any of the Stata ANOVA commands. 
</p>
<p>To obtain Tukey&rsquo;s HSD statistic, we must first install a pair of user-written pro-
</p>
<p>cedures that will use the results from an anova command and then compute 
</p>
<p>Tukey&rsquo;s HSD.
</p>
<p>To install these user-written procedures, enter the following two commands 
</p>
<p>(one time only for each one):
</p>
<p>oneway variable_name grouping_variable, tabulate
</p>
<p>oneway age crime, tabulate
</p>
<p>anova variable_name grouping_variable
</p>
<p>net install tukeyhsd, from (http://www.ats.ucla.edu/stat/
</p>
<p>stata/ado/analysis)
</p>
<p>net install sg101, from (http://www.stata.com/stb/stb47)</p>
<p/>
<div class="annotation"><a href="http://www.ats.ucla.edu/stat/stata/ado/analysis">http://www.ats.ucla.edu/stat/stata/ado/analysis</a></div>
<div class="annotation"><a href="http://www.ats.ucla.edu/stat/stata/ado/analysis">http://www.ats.ucla.edu/stat/stata/ado/analysis</a></div>
<div class="annotation"><a href="http://www.stata.com/stb/stb47">http://www.stata.com/stb/stb47</a></div>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S348
</p>
<p>The tukeyhsd command will be the procedure we use, but its calculations are 
</p>
<p>based on the other procedure installed (sg101).
</p>
<p>To obtain Tukey&rsquo;s HSD, we first run a one-way ANOVA using the anova 
</p>
<p>command, followed by the tukeyhsd command:
</p>
<p>For the data in Table 12.4 that we have been working with, the two commands 
</p>
<p>would be
</p>
<p>The output from the running of the tukeyhsd command will show only three 
</p>
<p>comparisons, 1 v. 2, 1 v. 3, and 2 v. 3, making the interpretation of the output 
</p>
<p>somewhat simpler than in SPSS, where all possible comparisons are presented. 
</p>
<p>The results are identical to the results from Table 12.10.
</p>
<p>Kruskal&ndash;Wallis Test
</p>
<p>The Kruskal&ndash;Wallis test is available in Stata with the kwallis command:
</p>
<p>Returning to the use of  the data in Table 12.4, we would run the following 
</p>
<p> command:
</p>
<p>The output generated by the kwallis command will contain one small table 
</p>
<p>listing the group, number of cases in that group, and rank sum. Below the table, 
</p>
<p>there are chi-square test statistics for both of the methods we have noted: with 
</p>
<p>and without a correction for ties. Consequently, Stata reproduces the value in the 
</p>
<p>text (13.038) that does not correct for ties and the value also estimated by SPSS 
</p>
<p>(13.073) that does correct for ties.
</p>
<p>Problems
</p>
<p> 1. Input the data from Table 12.6 as two variables: bail amount and race 
</p>
<p>(use 1 = non-Hispanic white, 2 = non-Hispanic African American, and 
</p>
<p>3 = Hispanic of  any race).
</p>
<p>a. Reproduce the ANOVA results in the text.
</p>
<p>b. Compute the HSD for these data. What can you conclude about the 
</p>
<p>pairwise comparisons across race categories?
</p>
<p>c. Perform the Kruskal&ndash;Wallis test. How do the results from the Kruskal&ndash;
</p>
<p>Wallis test compare to the ANOVA results in part a? Do the results 
</p>
<p>from the Kruskal&ndash;Wallis test alter the conclusions obtained through the 
</p>
<p>use of  ANOVA?
</p>
<p>anova variable_name grouping_variable
</p>
<p>tukeyhsd grouping_variable
</p>
<p>anova age crime
</p>
<p>tukeyhsd crime
</p>
<p>kwallis variable_name, by(grouping_variable)
</p>
<p>kwallis age, by(crime)</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 349
</p>
<p> 2. Enter the data from Exercise 12.5. Use one of  the ANOVA commands to 
</p>
<p>test for differences in group means.
</p>
<p>a. Write out the assumptions of  the test, critical value of  the test statistic, 
</p>
<p>value of  the computed test statistic, and decision regarding the null 
</p>
<p>hypothesis.
</p>
<p>b. Compute the HSD for each of  the group comparisons. What can you 
</p>
<p>conclude about pairwise comparisons for each group?
</p>
<p>c. Use the Kruskal&ndash;Wallis test to test for differences in rank order across 
</p>
<p>groups. Write out the assumptions of  the test, critical value of  the test 
</p>
<p>statistic, value of  the computed test statistic, and decision regarding the 
</p>
<p>null hypothesis.
</p>
<p> 3. Enter the data from Exercise 12.6. Use one of  the ANOVA commands to 
</p>
<p>test for differences in group means.
</p>
<p>a. Write out the assumptions of  the test, critical value of  the test statistic, 
</p>
<p>value of  the computed test statistic, and decision regarding the null 
</p>
<p>hypothesis.
</p>
<p>b. Compute the HSD for each of  the group comparisons. What can you 
</p>
<p>conclude about pairwise comparisons for each group?
</p>
<p>c. Use the Kruskal&ndash;Wallis test to test for differences in rank order across 
</p>
<p>groups. Write out the assumptions of  the test, critical value of  the test 
</p>
<p>statistic, value of  the computed test statistic, and decision regarding the 
</p>
<p>null hypothesis.
</p>
<p> 4. Open the NYS data file (nys_1.sav, nys_1_student.sav, or nys_1.dta). Carry 
</p>
<p>out the following statistical analyses for each of  the research questions in 
</p>
<p>parts a through e:
</p>
<p>a. Does the mean number of  thefts valued at $5&ndash;$50 vary across academic 
</p>
<p>ability?
</p>
<p>b. Does the mean number of  times drunk vary across race?
</p>
<p>c. Does the level of  marijuana use vary across amount of  contact with 
</p>
<p> delinquent peers?
</p>
<p>d. Does the mean number of  attacks on other students vary across victim-
</p>
<p>ization experience?
</p>
<p>e. Does the mean number of  times cheating on schoolwork vary across 
</p>
<p>grade point average?
</p>
<p>Use ANOVA to test for differences in group means. For each 
</p>
<p>hypothesis test, write out the assumptions of  the test, critical value 
</p>
<p>of  the test statistic, value of  the computed test statistic, and decision 
</p>
<p>regarding the null hypothesis.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T W E L V E :  M O R E T H A N T W O S A M P L E S350
</p>
<p>Compute the HSD for each of  the pairwise comparisons. What can 
</p>
<p>you conclude about pairwise comparisons for each research question?
</p>
<p>Use the Kruskal&ndash;Wallis test to test for differences in rank order 
</p>
<p>across groups. For each hypothesis test, write out the assumptions 
</p>
<p>of  the test, critical value of  the test statistic, value of  the computed 
</p>
<p>test statistic, and decision regarding the null hypothesis.
</p>
<p> 5. Open the Pennsylvania Sentencing data file (pcs_ 98.sav or pcs_98.dta). 
</p>
<p>Carry out the following statistical analyses for each of  the research  
</p>
<p>questions in parts a through c:
</p>
<p>a. Does the length of  incarceration sentence vary across race?
</p>
<p>b. Does the length of  incarceration sentence vary across method of  conviction?
</p>
<p>c. Does the length of  incarceration sentence vary by type of  conviction offense?
</p>
<p>&#149; Use ANOVA to test for differences in group means. For each 
</p>
<p> hypothesis test, write out the assumptions of  the test, critical value 
</p>
<p>of  the test statistic, value of  the computed test statistic, and decision 
</p>
<p>regarding the null hypothesis.
</p>
<p>&#149; Compute the HSD for each of  the pairwise comparisons. What can 
</p>
<p>you conclude about pairwise comparisons for each research question?
</p>
<p>&#149; Use the Kruskal&ndash;Wallis test to test for differences in rank order 
</p>
<p>across groups. For each hypothesis test, write out the assumptions 
</p>
<p>of  the test, critical value of  the test statistic, value of  the computed 
</p>
<p>test statistic, and decision regarding the null hypothesis.</p>
<p/>
</div>
<div class="page"><p/>
<p>Measures of Association 
</p>
<p>for Nominal and Ordinal Variables
</p>
<p>What Do Nominal Measures of Association Describe?
</p>
<p>What Do Ordinal Measures of Association Describe?
</p>
<p>C h a p t e r  t h i r t e e n
</p>
<p>M e a s u r i n g  t h e  s t r e n g t h  o f  a  r e l a t i o n s h i p
</p>
<p>M e a s u r e s  o f  a s s o c i a t i o n  f o r  n o m i n a l  v a r i a b l e s
</p>
<p>M e a s u r e s  o f  a s s o c i a t i o n  f o r  o r d i n a l  v a r i a b l e s
</p>
<p>When are These Measures Used?
</p>
<p>What is the Test of Statistical Significance for These Measures?
</p>
<p>When are These Measures Used?
</p>
<p>What is the Test of Statistical Significance for These Measures?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_13,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER 12 INTRODUCED eta (�) and the more general concept of mea-
sures of association. Eta is a descriptive statistic that allows us to define
</p>
<p>how strongly the categorical variable or sample in an analysis of variance
</p>
<p>is related to the interval-level variable or trait we examined across the
</p>
<p>samples. But there are many other useful measures of association that
</p>
<p>allow us to define relationships among variables. Over the next few
</p>
<p>chapters, we will focus on some of these that are particularly useful in
</p>
<p>studying criminal justice. We will still be concerned with statistical signif-
</p>
<p>icance in these chapters, but we will examine not only whether a mea-
</p>
<p>sure is statistically significant but also how strong the relationship is.
</p>
<p>In this chapter, our focus is on nominal- and ordinal-level measures of
</p>
<p>association. We begin with a discussion of why it is important to distin-
</p>
<p>guish between statistical significance and strength of association. While
</p>
<p>statistical significance can tell us whether we can make reliable state-
</p>
<p>ments about differences in a population from observations made from
</p>
<p>samples, it does not define the size of the relationship observed. It is im-
</p>
<p>portant to define the strength of the relationship between variables being
</p>
<p>D i s t i n g u i s h i n g  S t a t i s t i c a l  S i g n i f i c a n c e  a n d  S t r e n g t h  
o f  R e l a t i o n s h i p :  T h e  E x a m p l e  o f  t h e  C h i - S q u a r e  S t a t i s t i c
</p>
<p>In Chapter 9, we explored the chi-square statistic as a way to determine
</p>
<p>whether there was a statistically significant relationship between two
</p>
<p>nominal-level variables. The chi-square statistic is useful as a way of test-
</p>
<p>ing for such a relationship, but it is not meant to provide a measure of the
</p>
<p>strength of the relationship between the variables. It is tempting to look at
</p>
<p>the value of the chi-square statistic and the observed significance level as-
</p>
<p>sociated with a particular chi-square value and infer from these statistics
</p>
<p>the strength of the relationship between the two variables. If we follow
</p>
<p>such an approach, however, we run the risk of an interpretive error.
</p>
<p>results that are statistically significant are also substantively important.
</p>
<p>examined because that puts us in a better position to decide whether
</p>
<p>352</p>
<p/>
</div>
<div class="page"><p/>
<p>S T A T I S T I C A L S I G N I F I C A N C E A N D S T R E N G T H
</p>
<p>The problem with using the chi-square statistic&mdash;or outcomes of other
</p>
<p>tests of statistical significance&mdash;in this way is that the size of the test sta-
</p>
<p>tistic is influenced not only by the nature of the relationship observed
</p>
<p>but also by the number of cases in the samples examined. As we have
</p>
<p>noted a number of times in the text, this makes good sense. Larger sam-
</p>
<p>ples, all else being equal, are likely to be more trustworthy. Just as we
</p>
<p>feel more confident in drawing inferences from a sample of 10 or 20
</p>
<p>coin tosses than from a sample of 2 or 3 tosses, our confidence in mak-
</p>
<p>ing a decision about the null hypothesis grows as the sizes of the sam-
</p>
<p>ples examined using a chi-square statistic increase.
</p>
<p>The following example will help to illustrate this problem. Suppose we
</p>
<p>have a sample of 200 cases that cross-tabulate experimental condition with
</p>
<p>an outcome measure, as shown in Table 13.1. We see that 60% of those in
</p>
<p>the treatment group have an outcome classified as a success, while only
</p>
<p>40% of those in the control group have an outcome classified as a success.
</p>
<p>Our calculated value of chi-square for these data is 8.00 with df � 1, which
</p>
<p>has an observed significance level less than 0.01 (see Appendix 2). See
</p>
<p>Table 13.2 for detailed calculations for obtaining the chi-square statistic.
</p>
<p>Observed Frequencies ( fo) and Expected Frequencies ( fe) for Two
Outcomes of an Experimental Condition with 200 Cases
</p>
<p>OUTCOME
</p>
<p>EXPERIMENTAL
</p>
<p>CONDITION Failure Success Total
</p>
<p>Treatment f � 40 fo � 60 100
fe � 50 fe � 50
</p>
<p>Control fo � 60 fo � 40 100
fe � 50 fe � 50
</p>
<p>Total 100 100 200
</p>
<p>Table 13.1
</p>
<p>Calculations for Obtaining Chi-Square 
Statistic for the Example in Table 13.1
</p>
<p>EXPERIMENTAL
</p>
<p>CONDITION OUTCOME f fe fo � fe (fo � fe)
2
</p>
<p>Treatment Failure 40 50 �10 100 2
Treatment Success 60 50 10 100 2
Control Failure 60 50 10 100 2
Control Success 40 50 �10 100 2
</p>
<p>� � 8.0
</p>
<p>(fo � fe)
2
</p>
<p>fe
</p>
<p>Table 13.2
</p>
<p>353
</p>
<p>o
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>Without changing the proportional distribution of cases for this exam-
</p>
<p>ple&mdash;keeping success at 60% for the treatment group and 40% for the
</p>
<p>control group&mdash;suppose we multiply the number of cases by 10. We
</p>
<p>now have 2,000 total observations, as shown in Table 13.3, but the rela-
</p>
<p>tionship between experimental condition and outcome is the same. Our
</p>
<p>calculated chi-square statistic, however, now has a value of 80.00 (see
</p>
<p>Table 13.4) with df � 1, and the observed significance level is less than
</p>
<p>0.0001. So, simply by increasing the size of the sample, we increase the
</p>
<p>value of chi-square and decrease the corresponding observed signifi-
</p>
<p>cance level.
</p>
<p>This feature of the chi-square statistic applies to all tests of statistical
</p>
<p>significance. Irrespective of the observed relationship between measures,
</p>
<p>as the sample size increases, the observed significance level associated
</p>
<p>The rule does not raise any new questions regarding the meaning of sta-
</p>
<p>tistical significance. It simply reminds us that, all else being equal, we can
</p>
<p>be more confident in making statistical inferences from larger samples. It
</p>
<p>also emphasizes the importance of distinguishing between statistical sig-
</p>
<p>nificance and the size or strength of a relationship between variables.
</p>
<p>To allow researchers to define the strength of a relationship among
</p>
<p>nominal-level or ordinal-level variables, statisticians have developed a vari-
</p>
<p>ety of measures of association. Some of these measures are based on the
</p>
<p>value of the chi-square statistic; others are based on unique transforma-
</p>
<p>tions of the counts or distributions of cases within a table. All the measures
</p>
<p>of association that we discuss share a standardized scale: A value of 0 is
</p>
<p>interpreted as no relationship, and a value of 1.0 (or, in the case of ordinal
</p>
<p>scales, �1 or �1) is interpreted as a perfect relationship between the two
</p>
<p>variables. The discussion that follows describes some of the more fre-
</p>
<p>quently used measures of association for nominal and ordinal variables.
</p>
<p>Observed Frequencies ( fo) and Expected Frequencies ( fe) for Two
Outcomes of an Experimental Condition with 2,000 Cases
</p>
<p>OUTCOME
</p>
<p>EXPERIMENTAL
</p>
<p>CONDITION Failure Success Total
</p>
<p>Treatment fo � 400 fo � 600 1,000
fe � 500 fe � 500
</p>
<p>Control fo � 600 fo � 400 1,000
fe � 500 fe � 500
</p>
<p>Total 1,000 1,000 2,000
</p>
<p>Table 13.3
</p>
<p>relationship between statistical significance and sample size will be exam-
</p>
<p>with that relationship will also increase. This simple rule regarding the 
</p>
<p>354
</p>
<p>ined in more detail in the discussion of statistical power in Chapter 21.</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R N O M I N A L V A R I A B L E S
</p>
<p>M e a s u r e s  o f  A s s o c i a t i o n  f o r  N o m i n a l  V a r i a b l e s
</p>
<p>Measures of Association Based on the Chi-Square Statistic
</p>
<p>The preceding example illustrated how the chi-square statistic is affected
</p>
<p>by sample size. With a 2 � 2 table (i.e., two rows and two columns),
</p>
<p>one straightforward way of measuring the strength of a relationship be-
</p>
<p>tween two variables that adjusts for the influence of sample size is to
</p>
<p>transform the value of the chi-square statistic by adjusting for the total
</p>
<p>number of observations. One measure of association that does this is
</p>
<p>phi (�). Phi is obtained simply by dividing the chi-square statistic by the
</p>
<p>total number of observations (N) and taking the square root of this value
</p>
<p>(see Equation 13.1).
</p>
<p>Equation 13.1
</p>
<p>Phi will have a value of 0 if the value of the chi-square statistic is 0 and
</p>
<p>there is no relationship between the two variables. Phi will have a value
</p>
<p>of 1 if the chi-square statistic takes on a value equal to the sample size,
</p>
<p>which can occur only when there is a perfect relationship between two
</p>
<p>categorical variables. It is important to note that phi is appropriate only
</p>
<p>eliminating the possibility of any kind of meaningful interpretation.
</p>
<p>Consider the two chi-square statistics that we calculated above for the
</p>
<p>data in Tables 13.1 and 13.3: 8.00 and 80.00, respectively. If we insert
</p>
<p>these values for chi-square and the sample size, we find that the value of
</p>
<p>phi for both tables is 0.20.
</p>
<p>� � ��
2
</p>
<p>N
</p>
<p>Calculations for Obtaining Chi-Square 
Statistic for the Example in Table 13.3
</p>
<p>EXPERIMENTAL
</p>
<p>CONDITION OUTCOME fo fe fo � fe (fo � fe)
2
</p>
<p>Treatment Failure 400 500 �100 10,000 20
Treatment Success 600 500 100 10,000 20
Control Failure 600 500 100 10,000 20
Control Success 400 500 �100 10,000 20
</p>
<p>� � 80.0
</p>
<p>(fo � fe)
2
</p>
<p>fe
</p>
<p>Table 13.4
</p>
<p>355
</p>
<p>exceeds two, then it is possible for phi to take on values greater than 1.0,
</p>
<p>for analyses that use a 2 � 2 table. If the number of rows or columns </p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>We now have a measure of association that is not influenced by sample
</p>
<p>size. For both of our examples, in which the proportion of cases in each
</p>
<p>group was similar, we have the same phi statistic. However, is the relation-
</p>
<p>ship large or small? As noted in Chapter 12, defining &ldquo;large&rdquo; and &ldquo;small&rdquo; is
</p>
<p>a matter of judgment and not statistics. In judging the importance of a re-
</p>
<p>sult, researchers can compare it with other findings from prior studies. Or
</p>
<p>they may examine the importance of the policy implications that could be
</p>
<p>drawn from the result. For example, a very small change in rates of heart
</p>
<p>attacks in the population could save many lives, and thus a small relation-
</p>
<p>ship may still be important. According to a standard measure of effect size
</p>
<p>suggested by Jacob Cohen, a phi of 0.10 is considered to indicate a small
</p>
<p>relationship, one of 0.30 a medium relationship, and one of 0.50 a large
</p>
<p>relationship.1
</p>
<p>Our examples suggest why we might be misled if we used the chi-
</p>
<p>square statistic and its corresponding significance level as an indicator of
</p>
<p>the strength of the relationship between two variables. If we had tried to
</p>
<p>infer the strength of the relationship between experimental condition
</p>
<p>and outcome from the value of the chi-square statistic, we would have
</p>
<p>been tempted to conclude that Table 13.3 showed a stronger relationship
</p>
<p>than Table 13.1. However, once we take into account the size of the
</p>
<p>sample, we see that the two tables reflect the same relationship between
</p>
<p>the two variables. The data in Table 13.3 lead to a higher observed sig-
</p>
<p>nificance level because the samples examined are larger. However, the
</p>
<p>strength of the relationship observed in the two tables is the same.
</p>
<p>For tables with more than two rows or two columns, we cannot use
</p>
<p>phi. Instead, we use a measure of association known as Cramer&rsquo;s V,
</p>
<p>which is also based on the value of the chi-square statistic but makes an
</p>
<p>adjustment for the number of categories in each variable. Equation 13.2
</p>
<p>presents the formula for calculating Cramer&rsquo;s V.
</p>
<p>Equation 13.2
</p>
<p>In Equation 13.2, the chi-square statistic (�2) is divided by the product of
</p>
<p>the total number of observations (N ), and the smaller of two numbers, 
</p>
<p>V � � �
2
</p>
<p>N � min(r � 1, c � 1)
</p>
<p>1See Jacob Cohen, Statistical Power Analysis for the Behavioral Sciences (Hillsdale, NJ:
</p>
<p>Lawrence Erlbaum, 1988), pp. 215&ndash;271.
</p>
<p>W orking It Out
</p>
<p>and � � �80.002,000 � 0.20� � �8.00200  � 0.20
</p>
<p>356</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R N O M I N A L V A R I A B L E S
</p>
<p>r � 1 or c � 1 (i.e., the minimum of these two values), where r is the
</p>
<p>number of rows in the table and c is the number of columns. For exam-
</p>
<p>ple, if we had a table with two rows and three columns, we would have
</p>
<p>r � 1 � 2 � 1 � 1 and c � 1 � 3 � 1 � 2. The value for r � 1 is the
</p>
<p>smaller of these two numbers, so we would use that value (1) for 
</p>
<p>min(r � 1, c � 1) in the denominator of the formula. If we were work-
</p>
<p>ing with a larger table with, say, five rows and four columns, we would
</p>
<p>have r � 1 � 5 � 1 � 4 and c � 1 � 4 � 1 � 3. Since 3 is less than 4,
</p>
<p>we would use the value 3 for min(r � 1, c � 1) in the denominator.
</p>
<p>Let&rsquo;s consider an example. Table 13.5 reproduces the data from Table
</p>
<p>9.9 on cell-block assignment and race of prisoner. Recall from Chapter 9
</p>
<p>that the chi-square statistic for this cross-tabulation was 88.2895, and
</p>
<p>with df � 6, the observed significance level was less than 0.001. We can
</p>
<p>use the data in this table to illustrate the calculation of V. The table has
</p>
<p>seven rows (r � 7) and two columns (c � 2), meaning that r � 1 � 7 �
</p>
<p>1 � 6 and c � 1 � 2 � 1 � 1. The smaller of these two values is 1,
</p>
<p>which we substitute for min(r � 1, c � 1) in the denominator of the for-
</p>
<p>mula for V. After inserting the other values into Equation 13.2, we find
</p>
<p>that V � 0.2708.
</p>
<p>Assignment of Non-Hispanic White 
and Nonwhite Prisoners in Seven Prison Cell Blocks
</p>
<p>NON-HISPANIC
</p>
<p>CELL BLOCK WHITES NONWHITES
</p>
<p>C 48 208 1,256
D 17 37 1, 54
E 28 84 1,112
F 32 79 1,111
G 37 266 1,303
H 34 22 1, 56
I 44 268 1,312
</p>
<p>240 964 1,204
</p>
<p>Table 13.5
</p>
<p>W orking It Out
</p>
<p> � 0.2708
</p>
<p> � � 88.2895(1,204)(1)
</p>
<p> V � � �
2
</p>
<p>N � min(r � 1, c � 1)
</p>
<p>357
</p>
<p>Column total
</p>
<p>ROW TOTAL</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>Proportional Reduction in Error Measures: Tau and Lambda
</p>
<p>Some measures of association that are appropriate for nominal-level vari-
</p>
<p>ables are based on the idea of proportional reduction in error, or
</p>
<p>PRE. Such measures indicate how much knowledge of one variable
</p>
<p>helps to reduce the error we make in defining the values of a second
</p>
<p>variable. If we make about the same number of errors when we know
</p>
<p>the value of the first variable as when we don&rsquo;t, then we can conclude
</p>
<p>that the PRE is low and the variables are not strongly related. However,
</p>
<p>if knowledge of one variable helps us to develop much better predic-
</p>
<p>tions of the second variable, then we have a high PRE and the variables
</p>
<p>may be assumed to be strongly related.
</p>
<p>Two of the more common measures of association between nominal
</p>
<p>variables, Goodman and Kruskal&rsquo;s tau (�) and lambda (�) are both
</p>
<p>PRE measures. Both of these measures require that we identify at the
</p>
<p>outset which variable is the dependent variable and which variable is
</p>
<p>the independent variable. A dependent variable is an outcome vari-
</p>
<p>able&mdash;it represents the phenomenon that we are interested in explaining.
</p>
<p>It is &ldquo;dependent&rdquo; on other variables, meaning that it is influenced&mdash;or we
</p>
<p>expect it to be influenced&mdash;by other variables. Any variable that affects,
</p>
<p>or influences, the dependent variable is referred to as an independent
</p>
<p>variable. The values of Goodman and Kruskal&rsquo;s tau (�) and lambda (	)
</p>
<p>For most research projects, a body of prior research and/or theory
</p>
<p>will indicate which variables are dependent and which are independent.
</p>
<p>For example, for the study in Table 13.1, the independent variable is the
</p>
<p>experimental condition: the treatment or the control group. Whether the
</p>
<p>person participated in the treatment or the control group is generally
</p>
<p>theorized to influence outcome success or failure, which is the depen-
</p>
<p>dent variable. In other words, the experiment tests whether success or
</p>
<p>failure is due, at least in part, to participation in some kind of treatment.
</p>
<p>PRE measures of association, such as tau and lambda, require the use
</p>
<p>of two decision rules. The first decision rule&mdash;the naive decision rule&mdash;
</p>
<p>involves making guesses about the value of the dependent variable with-
</p>
<p>out using any information about the independent variable. The second
</p>
<p>decision rule&mdash;the informed decision rule&mdash;involves using information
</p>
<p>about how the cases are distributed within levels or categories of the in-
</p>
<p>dependent variable. The question becomes &ldquo;Can we make better predic-
</p>
<p>tions about the value of the dependent variable by using information
</p>
<p>about the independent variable?&rdquo; Will the informed decision rule provide
</p>
<p>358
</p>
<p>relationship between cell-block assignment and race of prisoner.
</p>
<p>The value of Cramer&rsquo;s V may be interpreted in the same way as that 
</p>
<p>of phi. Accordingly, a value for V of 0.2708 is suggestive of a moderate 
</p>
<p>dependent variable and which as the independent variable.
</p>
<p>will generally differ depending on which variable is identified as the </p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R N O M I N A L V A R I A B L E S
</p>
<p>better predictions than the naive decision rule? PRE measures of associa-
</p>
<p>tion have a value of 0 when there is no relationship between the two
</p>
<p>variables and a value of 1 when there is a perfect relationship between
</p>
<p>the two variables. Table 13.6 presents two hypothetical distributions il-
</p>
<p>lustrating PRE measures showing no relationship (part a) and a perfect
</p>
<p>relationship (part b). In part a, we see that knowledge of one variable
</p>
<p>does not help us make predictions about the second variable, since the
</p>
<p>A key advantage to PRE measures of association is the interpretation of
</p>
<p>values between 0 and 1. Any value greater than 0 may be interpreted as a
</p>
<p>proportionate reduction in error achieved by using information on the in-
</p>
<p>dependent variable. Alternatively, we can multiply the PRE measure by
</p>
<p>100 and interpret the value as the percent reduction in errors. For exam-
</p>
<p>For an illustration of the calculation of tau and lambda, consider the
</p>
<p>data presented in Table 13.7. These data come from responses to a sur-
</p>
<p>Hypothetical Distribution of 200 Cases for Two Nominal Variables
</p>
<p>(a) PRE Measure of Association � 0.0
</p>
<p>VARIABLE 2
</p>
<p>VARIABLE 1 Category 1 Category 2
</p>
<p>Category 1 50 50 100
Category 2 50 50 100
</p>
<p>100 100 200
</p>
<p>(b) PRE Measure of Association � 1.0
</p>
<p>VARIABLE 2
</p>
<p>VARIABLE 1 Category 1 Category 2
</p>
<p>Category 1 0 100 100
Category 2 100 0 100
</p>
<p>100 100 200
</p>
<p>Table 13.6
</p>
<p>2For a description of the study, see Chester L. Britt, &ldquo;Health Consequences of Criminal
</p>
<p>Victimization,&rdquo; International Review of Victimology, 8 (2001): 63&ndash;73.
</p>
<p>359
</p>
<p>errors of 50% when information about the independent variable is used.
</p>
<p>ple, a PRE measure of 0.50 indicates a percent reduction in prediction 
</p>
<p>Row total
</p>
<p>Column total
</p>
<p>Row total
</p>
<p>Column total
</p>
<p>knowledge of one variable determines, without error, the value of the
</p>
<p>cases are evenly distributed across all possible cells of the table (e.g., of
</p>
<p>the 100 cases in Category 1 of Variable 1, exactly 50 cases each fall into
</p>
<p>2 of Variable 2).
</p>
<p>Category 1 and 2 of Variable 2).  In the perfect relationship shown in part b, 
</p>
<p>second variable (e.g., all cases in Category 1 of Variable 1 fall into Category 
</p>
<p>vey by adult residents of the state of Illinois.2 Respondents who reported</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>questions about the most recent event. Two of these questions addressed
</p>
<p>the relationship between the victim and the offender and the location of
</p>
<p>the assault. Here we have classified the victim-offender relationship into
</p>
<p>four categories: stranger, acquaintance/friend, partner (includes spouse
</p>
<p>and boyfriend or girlfriend), and relative. Location of the assault is also
</p>
<p>classified into four categories: home, neighborhood, work, and someplace
</p>
<p>else. For this analysis, we assume that the victim-offender relationship is
</p>
<p>the independent variable and the location of the assault is the dependent
</p>
<p>variable. Our research question is &ldquo;What is the strength of the relationship
</p>
<p>between victim-offender relationship and location of an assault?&rdquo;
</p>
<p>Goodman and Kruskal&rsquo;s tau uses information about the marginal dis-
</p>
<p>tributions of the two variables to test whether knowledge of the inde-
</p>
<p>pendent variable reduces prediction errors for the dependent variable.
</p>
<p>The first step in computing this statistic is to ask how many errors we
</p>
<p>would expect to make, on average, if we did not have knowledge about
</p>
<p>the victim-offender relationship. This is our naive decision rule, where
</p>
<p>we are effectively trying to guess what category of the dependent vari-
</p>
<p>able an observation might belong to, without using any information
</p>
<p>about the independent variable. For our example, we begin by looking
</p>
<p>at the column totals in Table 13.7, which reflect the categories of the de-
</p>
<p>pendent variable. Of the 410 assaults, we see that 139 occurred in the
</p>
<p>home, 77 in the neighborhood, 30 at work, and 164 someplace else. We
</p>
<p>use these column totals to help us determine the average number of er-
</p>
<p>rors we would expect to make if we assigned cases without any informa-
</p>
<p>tion about the victim-offender relationship.
</p>
<p>Let&rsquo;s begin with assaults in the home. Of the 410 total assaults, 139
</p>
<p>belong in the assaulted-in-the-home category, while 271 do not belong
</p>
<p>in this category (i.e., the assault occurred elsewhere). Proportionally,
</p>
<p>0.6610 (271 of 410) of the cases do not belong in the assaulted-in-the-
</p>
<p>Data on Victim-Offender Relationship and Location of Assault
</p>
<p>VICTIM- LOCATION OF ASSAULT
</p>
<p>OFFENDER
</p>
<p>RELATIONSHIP Home Neighborhood Work Someplace Else
</p>
<p>Stranger 10 49 18 89 166
Acquaintance/friend 21 22 7 46 96
Partner 77 5 3 19 104
Relative 31 1 2 10 44
</p>
<p>139 77 30 164 410
</p>
<p>Table 13.7
</p>
<p>360
</p>
<p>assaulted-in-the-home category, we would expect 0.6610 of these 139
</p>
<p>home category. If we randomly assigned 139 of the 410 cases to the 
</p>
<p>that they had experienced an assault were asked a series of follow-up
</p>
<p>Row total
</p>
<p>Column total</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R N O M I N A L V A R I A B L E S
</p>
<p>incorrectly&mdash;the number of prediction errors&mdash;we multiply the propor-
</p>
<p>tion of cases not in the category by the number of cases assigned to that
</p>
<p>category. For assaulted in the home, this is (0.6610 � 139) � 92. The
</p>
<p>value 92 represents the number of prediction errors we would expect to
</p>
<p>make, on average, in assigning cases to the assaulted-in-the-home cate-
</p>
<p>gory without any knowledge of the victim-offender relationship.3
</p>
<p>Turning to assaults in the neighborhood, we see that 77 cases belong in
</p>
<p>this category, and the remaining 333 do not belong in this category. As a
</p>
<p>proportion, 0.8122 of the cases (333 of 410) do not belong in the as-
</p>
<p>saulted-in-the-neighborhood category. This means that we would expect
</p>
<p>to make 0.8122 � 77 � 63 prediction errors, on average, in assigning
</p>
<p>cases to this category without any knowledge of the victim-offender rela-
</p>
<p>tionship. For assaults at work, 30 cases belong in this category and 380 do
</p>
<p>not, meaning that we would expect to make (380/410) � 30 � 28 predic-
</p>
<p>tion errors, on average, in assigning cases to the assaults-at-work category
</p>
<p>without any information about the victim-offender relationship. There are
</p>
<p>164 cases that belong to the assaulted-someplace-else category, meaning
</p>
<p>that 246 cases do not belong in this category. We would expect to make
</p>
<p>(246/410) � 164 � 98 prediction errors, on average, in assigning cases to
</p>
<p>this category without any information about the victim-offender relation-
</p>
<p>ship. To determine the total number of prediction errors we would make
</p>
<p>without any knowledge of the victim-offender relationship, we add these
</p>
<p>four values together: 92 � 63 � 28 � 98 � 281 total prediction errors.
</p>
<p>If we then use information about the victim-offender relationship&mdash;
</p>
<p>whether the victim and offender were strangers, acquaintances/friends,
</p>
<p>partners, or relatives&mdash;we can test whether this information improves our
</p>
<p>ability to predict the location of the assault. This reflects the use of our
</p>
<p>informed decision rule: Does our assignment of cases to categories of
</p>
<p>the dependent variable improve when we use information about the cat-
</p>
<p>egory of the independent variable? In other words, does knowing the
</p>
<p>category of the independent variable (victim-offender relationship) re-
</p>
<p>duce the number of prediction errors we make about the category of the
</p>
<p>dependent variable (location of assault)? To the extent that the indepen-
</p>
<p>dent variable has a relationship with the dependent variable, the number
</p>
<p>of prediction errors should decrease.
</p>
<p>The logic behind calculating the prediction errors is the same as before,
</p>
<p>except that we focus on the row totals in the table, rather than the total
</p>
<p>number of cases in each category of the dependent variable. We start with
</p>
<p>the first category of the independent variable (i.e., the first row of Table
</p>
<p>13.7) and note that 166 cases involved offenders who were strangers to
</p>
<p>3For all calculations of prediction errors, we have rounded the result to the nearest
</p>
<p>integer.
</p>
<p>361
</p>
<p>cases to be assigned incorrectly. To obtain the number of cases assigned</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>the victim. In a process similar to our earlier analysis, we begin by noting
</p>
<p>the placement of cases within this row: 10 assaults occurred at home, 49 in
</p>
<p>the neighborhood, 18 at work, and 89 someplace else. Starting with the as-
</p>
<p>signment of cases to assaulted-in-the-home, we note that 10 cases belong
</p>
<p>in this category and 156 do not belong in this category. As a proportion,
</p>
<p>0.9398 of the cases (156 of 166) do not belong in the assaulted-in-the-
</p>
<p>gory, we would expect to make 0.9398 � 10 � 9 prediction errors, on
</p>
<p>average. Turning to the assaulted-in-the-neighborhood category, we note
</p>
<p>that 49 cases belong in this category and 117 do not belong in this cate-
</p>
<p>gory, which means that we would expect to make (117/166) � 49 � 35
</p>
<p>prediction errors. For the assaulted-at-work category, we would expect to
</p>
<p>make (148/166) � 18 � 16 prediction errors, and for the assaulted-
</p>
<p>someplace-else category, we would expect to make (77/166) � 89 � 41
</p>
<p>prediction errors. The total number of prediction errors in assigning cases
</p>
<p>involving offenders who were strangers is 101 (that is, 9 � 35 � 16 � 41).
</p>
<p>To determine the prediction errors for each of the remaining cate-
</p>
<p>gories of the independent variable (assaults involving offenders who
</p>
<p>were acquaintances/friends, partners, or relatives), we use the same ap-
</p>
<p>proach with the three remaining rows of Table 13.7. Table 13.8 presents
</p>
<p>all the calculations of prediction errors necessary for obtaining tau.
</p>
<p>We obtain the total number of prediction errors made using informa-
</p>
<p>tion about the victim-offender relationship by summing the errors across
</p>
<p>each category of relationship. For cases involving an offender who was a
</p>
<p>tion errors; for cases involving partners, 44 prediction errors; and for
</p>
<p>cases involving a relative, 20 prediction errors (see the bottom row of
</p>
<p>Table 13.8). Altogether, we would expect to make 228 (that is, 101 � 63 �
</p>
<p>Calculations of Prediction Errors for Obtaining Tau for a Relationship
Between Victim-Offender Relationship and Location of Assault
</p>
<p>PREDICTION  PREDICTION PREDICTION PREDICTION PREDICTION
</p>
<p>ERRORS: ERRORS: ERRORS: ERRORS: ERRORS:
</p>
<p>No Knowledge of Offender Offender Was an Offender Offender 
</p>
<p>LOCATION OF Victim-Offender Was a Acquaintance Was a Was a
</p>
<p>ASSAULT Relationship Stranger or a Friend Partner Relative
</p>
<p>Home 139(271/410) � 92 10(156/166) � 9 21(75/96) � 16 77(27/104) � 20 31(13/44) � 9
Neighborhood 77(333/410) � 63 49(117/166) � 35 22(74/96) � 17 5(99/104) � 5 1(43/44) � 1
Work 30(380/410) � 28 18(148/166) � 16 7(89/96) � 6 3(101/104) � 3 2(42/44) � 2
Someplace else 164(246/410) � 98 89(77/166) � 41 46(50/96) � 24 19(85/104) � 16 10(34/44) � 8
Total � � 281 � � 101 � � 63 � � 44 � � 20
</p>
<p>Table 13.8
</p>
<p>362
</p>
<p>assigned 10 of the 166 cases in this row to the assaulted-in-the-home cate-
</p>
<p>home category when the offender is a stranger. Thus, if we randomly 
</p>
<p>involving an acquaintance or friend, we would expect to make 63 predic-
</p>
<p>stranger, we would expect to make 101 prediction errors; for cases </p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R N O M I N A L V A R I A B L E S
</p>
<p>Goodman and Kruskal&rsquo;s tau is a measure of the reduction in predic-
</p>
<p>tion errors achieved by using knowledge of the independent variable&mdash;
</p>
<p>which, again, in our example is the victim-offender relationship. Equa-
</p>
<p>tion 13.3 presents the general formula for calculating tau.
</p>
<p>Equation 13.3
</p>
<p>For our example, tau is equal to 0.1886. If we multiply this proportion by
</p>
<p>100%, we can discern that knowledge of the victim-offender relationship
</p>
<p>reduced our prediction errors by 18.86%, which implies a weak to mod-
</p>
<p>erate relationship between victim-offender relationship and location of
</p>
<p>assault.
</p>
<p>� � 
</p>
<p>�number of errorswithout knowledge ofindependent variable � � �
number of errors
with knowledge of
independent variable�
</p>
<p>number of errors without knowledge of independent variable
</p>
<p>W orking It Out
</p>
<p>� � 
281 � 228
</p>
<p>281
 � 0.1886
</p>
<p>Lambda (	) is a measure of association that is conceptually very simi-
</p>
<p>lar to Goodman and Kruskal&rsquo;s tau in that it is a PRE measure. However,
</p>
<p>rather than using the proportional distribution of cases to determine pre-
</p>
<p>diction errors, lambda uses the mode of the dependent variable. We
</p>
<p>begin with the naive decision rule, placing all possible observations in
</p>
<p>the modal category of the dependent variable and counting as errors the
</p>
<p>number of cases that do not belong in that modal category. We then use
</p>
<p>information about the value of the independent variable (the informed
</p>
<p>decision rule), making assignments of cases based on the mode of the
</p>
<p>dependent variable within each category of the independent variable.
</p>
<p>Equation 13.4 shows that lambda is calculated in a manner similar to
</p>
<p>that used to calculate tau.
</p>
<p> � 
</p>
<p>�number of errorsdependent variable� � �
number of errors
using mode of
dependent variable
by level of
independent variable
</p>
<p>�
</p>
<p>363
</p>
<p>relationship to predict location of assault.
</p>
<p>44 � 20) prediction errors using information about the victim-offender 
</p>
<p>Equation 13.4
</p>
<p>using mode of
</p>
<p>number of errors using mode of dependent variable
	</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>The calculation of lambda is less tedious, since we use only informa-
</p>
<p>tion on the modal category overall and then within each level of the
</p>
<p>independent variable. Without knowledge of the victim-offender rela-
</p>
<p>tionship, we would assign all 410 cases to the assaulted-someplace-
</p>
<p>else category, resulting in 410 � 164 � 246 classification errors. 
</p>
<p>What about the number of classification errors when we use knowl-
</p>
<p>someplace-else category, resulting in 166 � 89 � 77 classification
</p>
<p>errors. For assaults where the offender was an acquaintance or friend,
</p>
<p>we would assign all 96 cases to the assaulted-someplace-else category,
</p>
<p>resulting in 96 � 46 � 50 classification errors. All 104 partner offend-
</p>
<p>ers and 44 relative offenders would both be assigned to the home cat-
</p>
<p>use knowledge of the victim-offender relationship, compared to 246
</p>
<p>prediction errors made without any knowledge of the victim-offender
</p>
<p>relationship. The value of lambda is 0.3211, meaning that knowledge
</p>
<p>of the modal location of assault for each type of victim-offender rela-
</p>
<p>tionship reduces our errors in predicting location of assault by 32.11%.
</p>
<p>W orking It Out
</p>
<p>	 � 
246 � 167
</p>
<p>246
 � 0.3211
</p>
<p>As can be seen from our example, different measures of association
</p>
<p>may lead to somewhat different interpretations of the relationship be-
</p>
<p>tween two variables. This occurs because different measures use differ-
</p>
<p>ent strategies in coming to a conclusion about that relationship. Which
</p>
<p>is the best measure of association for assessing the strength of the rela-
</p>
<p>tionship between two nominal-level variables? Researchers often prefer
</p>
<p>the two PRE measures&mdash;tau and lambda&mdash;over phi and V, since PRE
</p>
<p>measures have direct interpretations of values that fall between 0 and
</p>
<p>1. However, to use PRE measures, a researcher must assume that one
</p>
<p>measure (the independent variable) affects a second (the dependent
</p>
<p>variable). Of tau and lambda, tau is often defined as the better measure
</p>
<p>able, lambda will have a value of 0, implying that there is no relation-
</p>
<p>ship between the two variables. Since tau relies on the marginal
</p>
<p>distributions of observations both overall and within each category of
</p>
<p>364
</p>
<p>offender was a stranger, we would assign all 166 cases to the assaulted-
</p>
<p>edge of the victim-offender relationship? For assaults where the 
</p>
<p>errors, respectively. We have a sum of 167 prediction errors when we
</p>
<p>egory, resulting in 104 � 77 � 27 and 44 � 31 � 13 classification 
</p>
<p>dependent variable is the same for all categories of the independent vari-
</p>
<p>of association for two reasons. First, if the modal category of the </p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R N O M I N A L V A R I A B L E S
</p>
<p>the independent variable, tau can still detect a relationship between the
</p>
<p>marginal totals (i.e., row or column totals). When row or column totals
</p>
<p>are not approximately equal, the value of lambda may be artificially
</p>
<p>high or low. The reliance on marginal distributions in the calculation of
</p>
<p>tau allows that measure of association to account for the size of the
</p>
<p>marginal totals directly and causes it not to be as sensitive to differ-
</p>
<p>ences in marginal totals.
</p>
<p>Statistical Significance of Measures of Association for Nominal Variables
</p>
<p>The statistical significance of each of the nominal measures of associa-
</p>
<p>tion just discussed can be assessed with the results of a chi-square test.
</p>
<p>When the chi-square statistic has a value of 0, each of the four coeffi-
</p>
<p>cients will also have a value of 0. The null hypothesis for each of the
</p>
<p>four coefficients is simply that the coefficient is equal to 0. The research
</p>
<p>hypothesis is simply that the coefficient is not equal to 0.
</p>
<p>We illustrate the steps of a hypothesis test for tau and lambda, using
</p>
<p>the data on victim-offender relationship and location of assault.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Nominal scale.
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: Adults aged 18 years and older in the state of 
</p>
<p>Illinois.
</p>
<p>Hypotheses:
</p>
<p>H0: There is no association between victim-offender relationship and
</p>
<p>location of assault (�p � 0).
</p>
<p>H1: There is an association between victim-offender relationship and
</p>
<p>location of assault (�p � 0).
</p>
<p>or
</p>
<p>H0: There is no association between victim-offender relationship and
</p>
<p>location of assault (	p � 0).
</p>
<p>H1: There is an association between victim-offender relationship and
</p>
<p>location of assault (	p � 0).
</p>
<p>The Sampling Distribution Since we are testing for a relationship be-
</p>
<p>tween two nominal-level variables, we use the chi-square distribution,
</p>
<p>where degrees of freedom � (r � 1)(c � 1) � (4 � 1)(4 � 1) � 9.
</p>
<p>365
</p>
<p>related to the marginal distributions, the value of lambda is sensitive to
</p>
<p>independent and the dependent variables. Second, and this is again </p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>Observed Frequencies and Expected Frequencies 
for Victim-Offender Relationship and Location of Assault
</p>
<p>LOCATION OF ASSAULT
</p>
<p>VICTIM-
</p>
<p>OFFENDER Someplace 
</p>
<p>RELATIONSHIP Home Neighborhood Work Else
</p>
<p>Stranger fo � 10 fo � 49 fo � 18 fo � 89 166
fe � 56.2780 fe � 31.1756 fe � 12.1463 fe � 66.4000
</p>
<p>Acquaintance/ fo � 21 fo � 22 fo � 7 fo � 46 96
friend fe � 32.5463 fe � 18.0293 fe � 7.0244 fe � 38.4000
</p>
<p>Partner fo � 77 fo � 5 fo � 3 fo � 19 104
fe � 35.2585 fe � 19.5317 fe � 7.6098 fe � 41.6000
</p>
<p>Relative fo � 31 fo � 1 fo � 2 fo � 10 44
fe � 14.9171 fe � 8.2634 fe � 3.2195 fe � 17.6000
</p>
<p>139 77 30 164 410
</p>
<p>Table 13.9
</p>
<p>Calculations of Chi-Square for Victim-Offender 
Relationship and Location of Assault
</p>
<p>VICTIM-
</p>
<p>OFFENDER LOCATION
</p>
<p>RELATIONSHIP OF ASSAULT fo fe fo � fe (fo � fe)
2
</p>
<p>Stranger Home 10 56.2780 �46.2780 2141.6578 38.0549
Stranger Neighborhood 49 31.1756 17.8244 317.7089 10.1909
Stranger Work 18 12.1463 5.8537 34.2653 2.8210
Stranger Someplace else 89 66.4000 22.6000 510.7600 7.6922
Friend Home 21 32.5463 �11.5463 133.3180 4.0963
Friend Neighborhood 22 18.0293 3.9707 15.7667 0.8745
Friend Work 7 7.0244 �0.0244 0.0006 0.0001
Friend Someplace else 46 38.4000 7.6000 57.7600 1.5042
Partner Home 77 35.2585 41.7415 1742.3498 49.4164
Partner Neighborhood 5 19.5317 �14.5317 211.1705 10.8117
Partner Work 3 7.6098 �4.6098 21.2499 2.7924
Partner Someplace else 19 41.6000 �22.6000 510.7600 12.2779
Other relative Home 31 14.9171 16.0829 258.6605 17.3399
Other relative Neighborhood 1 8.2634 �7.2634 52.7572 6.3844
Other relative Work 2 3.2195 �1.2195 1.4872 0.4619
Other relative Someplace else 10 17.6000 �7.6000 57.7600 3.2818
</p>
<p>� � 168.0005
</p>
<p>(fo � fe)
2
</p>
<p>fe
</p>
<p>Table 13.10
</p>
<p>366
</p>
<p>Row total
</p>
<p>Column total</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>Significance Level and Rejection Region We use the conventional 5% sig-
</p>
<p>nificance level for this example. From Appendix 2, we see that the criti-
</p>
<p>cal value of chi-square associated with a significance level of 5% and 
</p>
<p>df � 9 is 16.919. If the calculated chi-square statistic is greater than
</p>
<p>16.919, we will reject the null hypotheses and conclude that the associa-
</p>
<p>tion between victim-offender relationship and location of assault is statis-
</p>
<p>tically significant.
</p>
<p>The Test Statistic The chi-square statistic for the data in Table 13.7 is
</p>
<p>168.001. See Table 13.9 for the expected and observed frequencies and
</p>
<p>Table 13.10 for the detailed calculations.
</p>
<p>The Decision Since our calculated chi-square statistic of 168.001 is much
</p>
<p>M e a s u r e s  o f  A s s o c i a t i o n  f o r  O r d i n a l - L e v e l  V a r i a b l e s
</p>
<p>The preceding discussion described several measures of association for
</p>
<p>nominal variables, where there is no rank ordering of the categories of
</p>
<p>each variable. With ordinal-level variables, we can use the ordering of the
</p>
<p>categories to measure whether there is a positive or a negative relation-
</p>
<p>ship between two variables. A positive relationship would be indicated
</p>
<p>by higher ranks on one variable corresponding to higher ranks on a sec-
</p>
<p>ond variable. A negative relationship would be indicated by higher ranks
</p>
<p>on one variable corresponding to lower ranks on a second variable. The
</p>
<p>measures of association for ordinal-level variables all have values that
</p>
<p>value of 0.0 indicates no relationship between the two variables.
</p>
<p>Table 13.11 illustrates these variations in the strength of the relation-
</p>
<p>ship between two ordinal variables with a hypothetical distribution of
</p>
<p>450 cases. Part a presents a pattern of no association between the two
</p>
<p>variables. Since the cases are evenly distributed across all the cells of the
</p>
<p>table, knowledge of the level of one ordinal variable does not provide
</p>
<p>any information about the level of the second ordinal variable. Parts b
</p>
<p>and c show perfect negative and positive relationships, respectively,
</p>
<p>where knowledge of the level of one ordinal variable determines, with-
</p>
<p>out error, the level of the second ordinal variable.
</p>
<p>The calculation of ordinal measures of association is tedious to perform
</p>
<p>by hand. When doing data analysis, you would likely rely on a statistical
</p>
<p>367
</p>
<p>larger than our critical chi-square of 16.919, we reject the null hypotheses
</p>
<p>and conclude that there is a statistically significant relationship between
</p>
<p>victim-offender relationship and location of assault.
</p>
<p>relationship, a value of �1.0 indicates a perfect positive relationship, and a
</p>
<p>range from �1.0 to �1.0. A value of �1.0 indicates a perfect negative </p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>software package to perform the calculations for you. Most common statis-
</p>
<p>tical software packages will compute the measures of association for ordi-
</p>
<p>nal variables described here (see, for example, the computer exercises at
</p>
<p>the end of this chapter). The following discussion is intended to help you
</p>
<p>understand how these various measures are calculated.
</p>
<p>There are four common measures of association for ordinal variables:
</p>
<p>gamma (�), Kendall&rsquo;s b, Kendall&rsquo;s c, and Somers&rsquo; d. Common to all
</p>
<p>four is the use of concordant pairs and discordant pairs of observa-
</p>
<p>tions. The logic behind using concordant and discordant pairs of obser-
</p>
<p>vations is that we take each possible pair of observations in a data set
</p>
<p>and compare the relative ranks of the two observations on the two vari-
</p>
<p>ables examined. Concordant pairs are those pairs of observations for
</p>
<p>which the rankings are consistent: One observation is ranked high on
</p>
<p>both variables, while the other observation is ranked low on both vari-
</p>
<p>Hypothetical Distribution of 450 Cases for Two Ordinal Variables
</p>
<p>(a) Measure of Association � 0.0
</p>
<p>VARIABLE 2
</p>
<p>VARIABLE 1 Low Medium High
</p>
<p>Low 50 50 50 150
Medium 50 50 50 150
High 50 50 50 150
</p>
<p>150 150 150 450
</p>
<p>(b) Measure of Association � �1.0
</p>
<p>VARIABLE 2
</p>
<p>VARIABLE 1 Low Medium High
</p>
<p>Low 0 0 150 150
Medium 0 150 0 150
High 150 0 0 150
</p>
<p>150 150 150 450
</p>
<p>(c) Measure of Association � �1.0
</p>
<p>VARIABLE 2
</p>
<p>VARIABLE 1 Low Medium High
</p>
<p>Low 150 0 0 150
Medium 0 150 0 150
High 0 0 150 150
</p>
<p>150 150 150 450
</p>
<p>Table 13.11
</p>
<p>368
</p>
<p>ables. For example, one observation is ranked 1 (of five ranked categories)
</p>
<p>Row total
</p>
<p>Column total
</p>
<p>Column total
</p>
<p>Row total
</p>
<p>Row total
</p>
<p>Column total
</p>
<p>� �</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>ond variable, while the other observation is ranked 4 on the first variable
</p>
<p>and 3 on the second variable. Discordant pairs refer to those pairs of ob-
</p>
<p>servations for which the rankings are inconsistent: One observation is
</p>
<p>ranked high on the first variable and low on the second variable, while
</p>
<p>the other observation is ranked low on the first variable and high on the
</p>
<p>second variable. For example, one observation is ranked 1 on the first
</p>
<p>variable and 5 on the second variable, while the other observation is
</p>
<p>called a tied pair of observations (tie).4
</p>
<p>How do we decide whether a pair of observations is a concordant
</p>
<p>pair, a discordant pair, or a tied pair? Let&rsquo;s look at the determination of
</p>
<p>concordant, discordant, and tied pairs for the data presented in Table
</p>
<p>13.12, which represents a cross-tabulation of two ordinal variables, each
</p>
<p>with three categories: low, medium, and high.
</p>
<p>4All the measures of association for ordinal variables that we discuss here are for
</p>
<p>grouped data that can be represented in the form of a table. In Chapter 14, we discuss
</p>
<p>another measure of association for ordinal variables&mdash;Spearman&rsquo;s r (rs)&mdash;that is most
</p>
<p>useful in working with ungrouped data, such as information on individuals. The diffi-
</p>
<p>culty we confront when using Spearman&rsquo;s r on grouped data is that the large number
</p>
<p>of tied pairs of observations complicates the calculation of this measure of association.
</p>
<p>Spearman&rsquo;s r is a more appropriate measure of association when we have ordinal
</p>
<p>variables with a large number of ranked categories for individual cases or when we
</p>
<p>take an interval-level variable and rank order the observations (see Chapter 14).
</p>
<p>Cross-Tabulation of Two Ordinal Variables
</p>
<p>DEPENDENT VARIABLE
</p>
<p>INDEPENDENT
</p>
<p>VARIABLE Low Medium High
</p>
<p>Low Cell A Cell B Cell C
12 4 3
</p>
<p>Medium Cell D Cell E Cell F
5 10 6
</p>
<p>High Cell G Cell H Cell I
3 5 14
</p>
<p>Table 13.12
</p>
<p>on the first variable and 2 (of five ranked categories) on the sec-
</p>
<p>369
</p>
<p>ranked 4 on the first variable and 2 on the second variable. A pair of 
</p>
<p>observations that has the same rank on one or both of the variables is
</p>
<p>the dependent and the independent variables is required. The value of d
</p>
<p>four measures of association for ordinal variables for which specification of
</p>
<p>Somers&rsquo; d is the only one of the 
</p>
<p>depending on which variable is specified as the dependent variable. To simp-
</p>
<p>lify the following discussion, the examples we present in the next section de-
</p>
<p>fine one variable as the dependent and the other as the independent variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>We begin by determining the concordant pairs&mdash;those pairs of observa-
</p>
<p>since the cases in the same row or column will have the same ranking on
</p>
<p>the independent and dependent variables, respectively, and thus represent
</p>
<p>ties. We then look for cases located below and to the right of the cell of in-
</p>
<p>terest. For Cell A, the cells we will use to determine concordant pairs are
</p>
<p>of observations that are concordant for observations in Cell A, we begin
</p>
<p>by summing the number of observations in Cells E, F, G, and I: 10 � 6 �
</p>
<p>5 � 14 � 35. This tells us that for a single observation in Cell A, there are
</p>
<p>35 concordant pairs of observations. Since there are 12 observations in
</p>
<p>Cell A, we multiply the number of cases in Cell A (12) by the sum of the
</p>
<p>cases in Cells E, F, H, and I. For Cell A, there are 420 concordant pairs.
</p>
<p>W orking It Out
</p>
<p>12(10 � 6 � 5 � 14) � 420
</p>
<p>Continuing to work across the first row of Table 13.12, we move to Cell
</p>
<p>B. The cells located below and to the right of Cell B are Cells F and I, so
</p>
<p>the number of concordant pairs is 4(6 � 14) � 80. When we move to Cell
</p>
<p>C, we see there are no cells below and to the right, so we drop down to
</p>
<p>the next row and start with Cell D. The cells located below and to the right
</p>
<p>of Cell D are Cells H and I, so the number of concordant pairs is 5(5 � 14)
</p>
<p>� 95. Moving to Cell E, we see that only Cell I is below and to the right, so
</p>
<p>the number of concordant pairs is 10(14) � 140. The remaining cells in the
</p>
<p>table&mdash;F, G, H, and I&mdash;have no other cells located below and to the right,
</p>
<p>so they are not used in the calculation of concordant pairs. After calculating
</p>
<p>concordant pairs for all cells in the table, we sum these values to get the
</p>
<p>number of concordant pairs for the table. For Table 13.12, the total number
</p>
<p>of concordant pairs is 735 (that is, 420 � 80 � 95 � 140).
</p>
<p>W orking It Out
</p>
<p>Sum � 420 � 80 � 95 � 140 � 735
</p>
<p>Cell A:
Cell B:
Cell D:
Cell E:
</p>
<p> 12(10 � 6 � 5 � 14) � 420
4(6 � 14) �  80
5(5 � 14) �  95
</p>
<p>10(14) � 140
</p>
<p>370
</p>
<p>tions that have consistent relative rankings. Let&rsquo;s start with Cell A. We 
</p>
<p>Cells E, F, H, and I, since the ranks are consistently lower on both the 
</p>
<p>independent and the dependent variables. To determine the number of pairs
</p>
<p>remove from consideration the row and column that Cell A is located in,</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>To calculate discordant cells, we begin in the upper right corner of
</p>
<p>Table 13.12 (Cell C), locate cells that are positioned below and to the
</p>
<p>left of the cell of interest, and perform calculations similar to those for
</p>
<p>concordant pairs. Beginning with Cell C, we multiply the number of
</p>
<p>cases in Cell C by the sum of cases in Cells D, E, G, and H, which are
</p>
<p>located below and to the left of Cell C. The number of discordant pairs
</p>
<p>for Cell C is 69.
</p>
<p>W orking It Out
</p>
<p>3(5 � 10 � 3 � 5) � 69
</p>
<p>Moving from right to left in the top row of Table 13.12, we shift our
</p>
<p>attention to Cell B. The discordant pairs for Cell B are calculated by mul-
</p>
<p>tiplying the number of cases in Cell B by the sum of cases in Cells D and
</p>
<p>G. We find the number of discordant pairs for Cell B to be 4(5 � 3) �
</p>
<p>32. Since there are no cells located below and to the left of Cell A, it is
</p>
<p>not used to calculate discordant pairs, and we move on to Cell F. The
</p>
<p>cells located below and to the left of Cell F are Cells G and H, so the
</p>
<p>is 10(3) � 30. There are no cells located below and to the left of Cells D,
</p>
<p>G, H, and I, so no further calculations are performed. As with the con-
</p>
<p>cordant pairs, we sum our discordant pairs for Table 13.12 and find the
</p>
<p>sum to be 179 (that is, 69 � 32 � 48 � 30).
</p>
<p>W orking It Out
</p>
<p>Sum � 69 � 32 � 48 � 30 � 179
</p>
<p>Cell C:
Cell B:
Cell F:
Cell E:
</p>
<p>3(5 � 10 � 3 � 5) � 69
4(5 � 3) � 32
6(3 � 5) � 48
</p>
<p>10(3) � 30
</p>
<p>To calculate ties in rank for pairs of observations, we have to con-
</p>
<p>sider the independent and dependent variables separately. We denote
</p>
<p>ties on the independent variable as TX and ties on the dependent vari-
</p>
<p>able as TY. Since the independent variable is represented by the rows
</p>
<p>371
</p>
<p>number of discordant pairs is 6(3 � 5) � 48. For Cell E, the only cell 
</p>
<p>located below and to the left is Cell G, so the number of discordant pairs</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>in Table 13.12, the pairs of observations that will be defined as ties on 
</p>
<p>the independent variable will be those cases located in the same row
</p>
<p>of Table 13.12. To calculate the number of ties in each row, we use
</p>
<p>Equation 13.5.
</p>
<p>Equation 13.5
</p>
<p>where TX is the number of ties on the independent variable and Nrow is
</p>
<p>the row total. Equation 13.5 tells us to calculate the product of the num-
</p>
<p>ber of observations in a row and the number of observations in a row
</p>
<p>minus 1 for all rows. We then sum the products calculated for each row
</p>
<p>and multiply the sum by .
</p>
<p>For Table 13.12, the three row totals are 19 (row 1), 21 (row 2), and
</p>
<p>22 (row 3). When we insert these values into Equation 13.5, we find the
</p>
<p>number of ties on the independent variable to be 612.
</p>
<p>1
\2
</p>
<p>TX � 
1
2
</p>
<p> � Nrow(Nrow � 1)
</p>
<p>W orking It Out
</p>
<p> � 612
</p>
<p> � 
1
2
</p>
<p> (342 � 420 � 462) � 
1
2
</p>
<p> (1,224)
</p>
<p> � 
1
2
</p>
<p> [(19)(18) � (21)(20) � (22)(21)]
</p>
<p> � 
1
2
</p>
<p> [(19)(19 � 1) � (21)(21 � 1) � (22)(22 � 1)]
</p>
<p> TX � 
1
2
</p>
<p> � Nrow(Nrow � 1)
</p>
<p>The ties on the dependent variable are found in a similar manner. Since
</p>
<p>the dependent variable is represented in the columns, we perform the
</p>
<p>same type of calculation, but using column totals rather than row totals.
</p>
<p>Equation 13.6 presents the formula for calculating ties on the dependent
</p>
<p>variable.
</p>
<p>Equation 13.6
</p>
<p>In Equation 13.6, TY is the number of ties on the dependent variable and
</p>
<p>Ncol is the total number of observations in the column.
</p>
<p>TY � 
1
2
</p>
<p> � Ncol(Ncol � 1)
</p>
<p>372</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>Gamma
</p>
<p>Once we have calculated the numbers of concordant pairs and discor-
</p>
<p>dant pairs, gamma (
) is the simplest of the ordinal measures of associa-
</p>
<p>tion to calculate, since it does not use information about ties in rank.
</p>
<p>Gamma has possible values that range from �1.0 to �1.0. Gamma may
</p>
<p>also be interpreted as a PRE measure: We can interpret the value of
</p>
<p>gamma as indicating the proportional reduction in errors in predicting
</p>
<p>the dependent variable, based on information about the independent
</p>
<p>variable.
</p>
<p>Equation 13.7 presents the formula for calculating gamma. Gamma is
</p>
<p>the difference between the number of concordant (C ) and discordant
</p>
<p>(D) pairs, (C � D), divided by the sum of the concordant and discordant
</p>
<p>pairs, (C � D).
</p>
<p>Equation 13.7
</p>
<p>For the data in Table 13.12, gamma is equal to 0.6083. The positive value
</p>
<p>of gamma tells us that as we move from lower ranked to higher ranked
</p>
<p>categories on the independent variable, the category of the dependent
</p>
<p>variable also tends to increase. In regard to the relative strength of the
</p>
<p>relationship, a value of 0.6083 suggests a strong relationship between the
</p>
<p>independent and dependent variables, since knowledge of the indepen-
</p>
<p>dent variable reduces our errors in predicting the dependent variable by
</p>
<p>60.83%.
</p>
<p>
 � 
C � D
C � D
</p>
<p>W orking It Out
</p>
<p> � 614
</p>
<p> � 
1
2
</p>
<p> (380 � 342 � 506) � 
1
2
</p>
<p> (1,228)
</p>
<p> � 
1
2
</p>
<p> [(20)(19) � (19)(18) � (23)(22)]
</p>
<p> � 
1
2
</p>
<p> [(20)(20 � 1) � (19)(19 � 1) � (23)(23 � 1)]
</p>
<p> TY � 
1
2
</p>
<p> � Ncol(Ncol � 1)
</p>
<p>In Table 13.12, the column totals are 20 (column 1), 19 (column 2),
</p>
<p>and 23 (column 3). After inserting these values into Equation 13.6, we
</p>
<p>find the number of ties on the dependent variable to be 614.
</p>
<p>373</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>Kendall&rsquo;s �b and �c
Kendall&rsquo;s tau measures&mdash;�b and �c&mdash;also assess the strength of associa-
</p>
<p>tion between two ordinal variables.5 The two measures are conceptually
</p>
<p>very similar in that they use information about concordant and discor-
</p>
<p>dant pairs of observations. But they also utilize information about tied
</p>
<p>pairs on both the independent and the dependent variables. Both tau
</p>
<p>measures have possible values ranging from �1.0 to �1.0. There are two
</p>
<p>important differences between �b and �c: First, �b should be applied only
</p>
<p>to a table where the number of rows is equal to the number of columns;
</p>
<p>�c should be applied to a table where the number of rows is not equal to
</p>
<p>the number of columns. When the number of rows is equal to the num-
</p>
<p>ber of columns, �c will have a value close to that of �b. Second, �b may
</p>
<p>be interpreted as a PRE measure, but �c may not. The differences in the
</p>
<p>application and interpretation of each measure suggest that knowing the
</p>
<p>dimensions of the table is important in deciding which measure is most
</p>
<p>appropriate.
</p>
<p>Equations 13.8 and 13.9 present the formulas for calculating �b and �c,
</p>
<p>respectively.
</p>
<p>Equation 13.8
</p>
<p>In Equation 13.8, C and D represent the concordant and the discordant
</p>
<p>pairs, respectively; N represents the total number of cases; TX represents
</p>
<p>the number of ties on the independent variable; and TY represents the
</p>
<p>number of ties on the dependent variable.
</p>
<p>�b � 
C � D
</p>
<p>�[N(N � 1)/2 � TX][N (N � 1)/2 � TY]
</p>
<p>W orking It Out
</p>
<p> � 0.6083
</p>
<p> � 
556
914
</p>
<p> � 
735 � 179
735 � 179
</p>
<p> 
 � 
C � D
C � D
</p>
<p>5These two tau measures are different from Goodman and Kruskal&rsquo;s tau, which mea-
</p>
<p>sures the strength of association between two nominal variables.
</p>
<p>374</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>Let&rsquo;s return to the data presented in Table 13.12. We have already
</p>
<p>calculated the number of concordant pairs to be 735, the number of
</p>
<p>discordant pairs to be 179, the total number of cases to be 62, the
</p>
<p>number of ties on the independent variable to be 612, and the number
</p>
<p>of ties on the dependent variable to be 614. After inserting these val-
</p>
<p>ues into Equation 13.6, we find �b to be 0.4351. This indicates that
</p>
<p>knowledge of the independent variable reduces our prediction errors
</p>
<p>by 43.51%.
</p>
<p>W orking It Out
</p>
<p> � 0.4351
</p>
<p> � 
556
</p>
<p>�(1,279)(1,277)
</p>
<p> � 
556
</p>
<p>�[1,891 � 612][1,891 � 614]
</p>
<p> � 
735 � 179
</p>
<p>�[62(62 � 1)/2 � 612][62(62 � 1)/2 � 614]
</p>
<p> �b � 
C � D
</p>
<p>�[N (N � 1)/2 � TX ][N (N � 1)/2 � TY ]
</p>
<p>Equation 13.9 presents the formula for calculating �c. We do not calcu-
</p>
<p>late �c for Table 13.12, since the number of rows is equal to the number
</p>
<p>of columns. We do, however, illustrate its calculation below with another
</p>
<p>example.
</p>
<p>Equation 13.9
</p>
<p>In Equation 13.9, C and D represent the concordant and the discordant
</p>
<p>pairs, respectively; N represents the total number of cases; and m is the
</p>
<p>smaller of the number of rows (r) and the number of columns (c). Sup-
</p>
<p>pose, for example, that we had a table with five rows (r � 5) and four
</p>
<p>columns (c � 4). The number of columns is smaller than the number of
</p>
<p>rows, so m would be 4.
</p>
<p> where m � min(r, c)
</p>
<p> �c � 
C � D
</p>
<p>1
2
</p>
<p> N 2[(m � 1)/m]
</p>
<p>375</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>Somers&rsquo; d
</p>
<p>The fourth measure of association for ordinal variables that we present
</p>
<p>ables, Somers&rsquo; d uses information on ties on only the independent vari-
</p>
<p>able. It is important to remember that the statistic you get for Somers&rsquo; d
</p>
<p>may vary, depending on which variable is defined as the dependent vari-
</p>
<p>able. The formula for calculating Somers&rsquo; d is given in Equation 13.10.
</p>
<p>Equation 13.10
</p>
<p>In Equation 13.10, where C, D, N, and TX represent the concordant pairs,
</p>
<p>the discordant pairs, the total number of cases, and the number of ties
</p>
<p>For Table 13.12, we have already calculated values for C, D, N, and
</p>
<p>TX. After inserting these values into Equation 13.10, we find Somers&rsquo; d to
</p>
<p>be 0.4347.
</p>
<p>dYX � 
C � D
</p>
<p>N (N � 1)/2 � TX
</p>
<p>W orking It Out
</p>
<p> � 0.4347
</p>
<p> � 
556
</p>
<p>1,279
</p>
<p> � 
556
</p>
<p>1,891 � 612
</p>
<p> � 
735 � 179
</p>
<p>62(62 � 1)/2 � 612
</p>
<p> dYX � 
C � D
</p>
<p>N (N � 1)/2 � TX
</p>
<p>A Substantive Example: Affectional Identification 
</p>
<p>with Father and Level of Delinquency
</p>
<p>Table 9.14 presented a cross-tabulation of two ordinal variables: affec-
</p>
<p>much they wanted to grow up and be like their fathers. The responses
</p>
<p>were classified into five ordered categories: in every way, in most
</p>
<p>376
</p>
<p>information about ties on both the independent and the dependent vari-
</p>
<p>here&mdash;Somers&rsquo; d&mdash;is similar to the tau measures, but instead of using 
</p>
<p>on the independent variable, respectively. The subscript YX on d
</p>
<p>denotes the dependent and the independent variables, in order.
</p>
<p>father was determined by the youth&rsquo;s responses to a question about how
</p>
<p>tional identification with father and delinquency. Identification with </p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>ways, in some ways, in just a few ways, and not at all. Delinquent acts
</p>
<p>were classified into three ordered categories: none, one, and two or
</p>
<p>more. The data came from the Richmond Youth Survey report, and the
</p>
<p>distribution of cases presented refers only to the white males who re-
</p>
<p>sponded to the survey.6 We reproduce this cross-tabulation in Table
</p>
<p>13.13.
</p>
<p>In our earlier analysis of the data in this table (see Chapter 9), we
</p>
<p>found a statistically significant relationship between affectional identifica-
</p>
<p>tion with father and delinquency. However, the chi-square statistic told
</p>
<p>us nothing about the direction of the effect or the strength of the rela-
</p>
<p>tionship between these two variables. We can use the measures of asso-
</p>
<p>We begin by calculating the numbers of concordant pairs, discordant
</p>
<p>pairs, and tied pairs of observations. The number of concordant pairs of
</p>
<p>observations is 201,575; the number of discordant pairs is 125,748; the
</p>
<p>number of pairs tied on the independent variable is 187,516; and the
</p>
<p>number of pairs tied on the dependent variable is 315,072.
</p>
<p>Affectional Identification with Father by Number of Delinquent Acts
</p>
<p>AFFECTIONAL
</p>
<p>IDENTIFICATION NUMBER OF DELINQUENT ACTS
</p>
<p>WITH FATHER None One Two or more
</p>
<p>In every way Cell A Cell B Cell C 121
77 25 19
</p>
<p>In most ways Cell D Cell E Cell F 404
263 97 44
</p>
<p>In some ways Cell G Cell H Cell I 387
224 97 66
</p>
<p>In just a few ways Cell J Cell K Cell L 172
82 52 38
</p>
<p>Not at all Cell M Cell N Cell O 138
56 30 52
</p>
<p>702 301 219 1,222
</p>
<p>Table 13.13
</p>
<p>6David F. Greenberg, &ldquo;The Weak Strength of Social Control Theory,&rdquo; Crime and Delin-
</p>
<p>quency 45:1 (1999): 66&ndash;81.
</p>
<p>377
</p>
<p>between identification with father and level of delinquency. 
</p>
<p>ciation for ordinal variables to test the strength of the relationship 
</p>
<p>Row total
</p>
<p>Column total</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>W orking It Out
</p>
<p>Concordant Pairs:
</p>
<p>Cell A: 77(97 � 44 � 97 � 66 � 52 � 38 � 30 � 52) � 36,652
</p>
<p>Cell B: 25(44 � 66 � 38 � 52) � 5,000
</p>
<p>Cell D: 263(97 � 66 � 52 � 38 � 30 � 52) � 88,105
</p>
<p>Cell E: 97(66 � 38 + 52) � 15,132
</p>
<p>Cell G: 224(52 � 38 � 30 � 52) � 38,528
</p>
<p>Cell H: 97(38 � 52) � 8,730
</p>
<p>Cell J: 82(30 � 52) � 6,724
</p>
<p>Cell K: 52(52) � 2,704
</p>
<p>Sum � 36,652 � 5,000 � 88,105 � 15,132 � 38,528 � 8,730 
</p>
<p>� 6,724 � 2,704
</p>
<p>� 201,575
</p>
<p>Discordant Pairs:
</p>
<p>Cell C: 19(263 � 97 � 224 � 97 � 82 � 52 � 56 � 30) � 17,119
</p>
<p>Cell B: 25(263 � 224 � 82 � 56) � 15,625
</p>
<p>Cell F: 44(224 � 97 � 82 � 52 � 56 � 30) � 23,804
</p>
<p>Cell E: 97(224 � 82 � 56) � 35,114
</p>
<p>Cell I: 66(82 � 52 � 56 � 30) � 14,520
</p>
<p>Cell H: 97(82 � 56) � 13,386
</p>
<p>Cell L: 38(56 � 30) � 3,268
</p>
<p>Cell K: 52(56) � 2,912
</p>
<p>Sum � 17,119 � 15,625 � 23,804 � 35,114 � 14,520 � 13,386 
</p>
<p>� 3,268 + 2,912
</p>
<p>� 125,748
</p>
<p>378</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>After calculating the concordant pairs, discordant pairs, and pairs
</p>
<p>tied on the independent and dependent variables, we can calculate
</p>
<p>the measures of association for ordinal variables. We find the value of
</p>
<p>gamma to be 0.2317. Don&rsquo;t be confused by the fact that for affectional
</p>
<p>identification movement from lower to higher ordered categories rep-
</p>
<p>resents movement from more to less identification with the father.
</p>
<p>Substantively, what this value of gamma tells us is that as the level of
</p>
<p>affectional identification with father decreases (i.e., as we move down
</p>
<p>the rows of the table), the youth are likely to report higher levels of
</p>
<p>delinquency. The value of gamma also indicates that we reduce our
</p>
<p>prediction errors about level of delinquency by 23.17% when we use
</p>
<p>information about the level of affectional identification with father. If
</p>
<p>affectional identification in this example had been measured from less
</p>
<p>to more identification with father (rather than more to less identifica-
</p>
<p>tion), gamma would have been negative. As a general rule, it is impor-
</p>
<p>tant to look carefully at the ordering of the categories of your measure
</p>
<p>in order to make a substantive interpretation of your result.
</p>
<p>Pairs Tied on the Independent Variable:
</p>
<p>Pairs Tied on the Dependent Variable:
</p>
<p> � 315,072
</p>
<p> � (1\2)(630,144)
</p>
<p> � (1\2)(492,102 � 90,300 � 47,742)
</p>
<p> TY � (
1
\2)[(702)(701) � (301)(300) � (219)(218)]
</p>
<p> � 187,516
</p>
<p> � (1\2)(375,032)
</p>
<p> � (1\2)(14,520 � 162,812 � 149,382 � 29,412 � 18,906)
</p>
<p>  �� (172)(171) � (138)(137)]
 TX � (
</p>
<p>1
\2)[(121)(120) � (404)(403) � (387)(386)
</p>
<p>379</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>Recall that there are two tau measures: �b and �c. If the number of
</p>
<p>rows were equal to the number of columns, then we would use �b. Since
</p>
<p>the number of rows is different from the number of columns in Table
</p>
<p>13.13, we use �c. For the data presented in Table 13.13, �c has a value of
</p>
<p>0.1523, meaning that as the level of affectional identification with father
</p>
<p>decreases, the level of delinquency increases. However, since �c
</p>
<p>W orking It Out
</p>
<p> � 0.2317
</p>
<p> � 
75,827
</p>
<p>327,323
</p>
<p> � 
201,575 � 125,748
</p>
<p>201,575 � 125,748
</p>
<p> 
 � 
C � D
C � D
</p>
<p>W orking It Out
</p>
<p> � 0.1523
</p>
<p> � 
75,827
</p>
<p>497,761.3333
</p>
<p> � 
201,575 � 125,748
</p>
<p>1
2
</p>
<p> (1,222)2 �3 � 13 �
</p>
<p> �c � 
C � D
</p>
<p>1
2
</p>
<p> N 2�m � 1m �
, � where m � min(r, c) � min(5, 3) � 3
</p>
<p>Our third measure of association for ordinal variables, Somers&rsquo; d, has
</p>
<p>a value of 0.1358. The interpretation is the same as that for gamma and
</p>
<p>�c: Lower levels of affectional identification with father are associated
</p>
<p>level of delinquency by 13.58%.
</p>
<p>380
</p>
<p>is not 
</p>
<p>reduction in error.
</p>
<p>a PRE measure, we cannot interpret this result in terms of proportional 
</p>
<p>affectional identification with father reduces our prediction errors about
</p>
<p>with higher levels of delinquency. In this case, knowledge of level of </p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>Note on the Use of Measures of Association for Ordinal Variables
</p>
<p>As illustrated in our example, the values for gamma, Kendall&rsquo;s tau mea-
</p>
<p>sures, and Somers&rsquo; d will generally not be the same. The difference in val-
</p>
<p>ues can be attributed primarily to whether the measure accounts for tied
</p>
<p>pairs of observations. Gamma does not account for tied pairs of observa-
</p>
<p>pairs of observations tied on the independent variable, while Kendall&rsquo;s tau
</p>
<p>measures account for tied pairs of observations on both variables.
</p>
<p>Which of these measures is best to use in which situations? As in
</p>
<p>our discussion of measures of association for nominal variables, to
</p>
<p>begin to address this question, we need to consider the dimensions of
</p>
<p>the table and our desire for a PRE measure. If the number of rows is
</p>
<p>equal to the number of columns, then �b is likely the best overall mea-
</p>
<p>sure of association for two reasons: First, it has a PRE interpretation,
</p>
<p>meaning that values falling between 0 and 1 have direct interpreta-
</p>
<p>tions in terms of reduction of error. Second, since �b accounts for pairs
</p>
<p>of observations tied on both the independent and the dependent vari-
</p>
<p>ables, it will provide a more conservative estimate than gamma. If the
</p>
<p>number of rows is not equal to the number of columns, Somers&rsquo; d is
</p>
<p>sometimes considered a better measure of association than �c, since it
</p>
<p>has a PRE interpretation and �c does not. Somers&rsquo; d offers the addi-
</p>
<p>tional advantage of being an appropriate measure of association for
</p>
<p>Statistical Significance of Measures of Association for Ordinal Variables
</p>
<p>Each of the four measures of association for ordinal variables can 
</p>
<p>be tested for statistical significance with a z-test. The general formula
</p>
<p>for calculating the z-score is given in Equation 13.11, where we divide
</p>
<p>W orking It Out
</p>
<p> � 0.1358
</p>
<p> � 
75,827
</p>
<p>558,515
</p>
<p> � 
201,575 � 125,748
</p>
<p>[(1,222)(1,222 � 1)/2] � 187,516
</p>
<p> dYX � 
C � D
</p>
<p>N (N � 1)/2 � TX
</p>
<p>381
</p>
<p>association between two ordinal variables. Somers&rsquo; d accounts for only the
</p>
<p>tions and thus is sometimes criticized for overestimating the strength of 
</p>
<p>dependent variables.
</p>
<p>those situations where we have clearly defined independent and </p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>the measure of association by the standard error of the measure of
</p>
<p>association.
</p>
<p>Equation 13.11
</p>
<p>What will differ for each of the measures of association for ordinal vari-
</p>
<p>ables is the calculation of the standard error. Equations 13.12, 13.13, and
</p>
<p>13.14 present approximate standard errors for gamma, Kendall&rsquo;s tau
</p>
<p>measures, and Somers&rsquo; d, respectively.7
</p>
<p>In all three equations, N is the total number of observations, r is the
</p>
<p>number of rows, and c is the number of columns in the table.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Ordinal scale.
</p>
<p>Population Distribution: Normal distribution for the relationship exam-
</p>
<p>ined (relaxed because N is large).
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: High school&ndash;age white males in Richmond, California,
</p>
<p>in 1965.
</p>
<p>Hypotheses:
</p>
<p>H0: There is no association between affectional identification with father
</p>
<p>and delinquency (
p � 0).
</p>
<p>H1: There is an association between affectional identification with father
</p>
<p>and delinquency (
p � 0).
</p>
<p>Equation 13.14
</p>
<p>Approximate Standard Error for
</p>
<p>Somers&rsquo; d
</p>
<p>�̂d � �4(r
2 � 1)(c � 1)
</p>
<p>9Nr  2(c � 1)
</p>
<p>Equation 13.13
</p>
<p>Approximate Standard Error for
</p>
<p>Kendall&rsquo;s Tau Measures
</p>
<p>�̂� � �4(r � 1)(c � 1)9Nrc
</p>
<p>Equation 13.12
</p>
<p>Approximate Standard Error for
</p>
<p>Gamma
</p>
<p>�̂
 � � 4(r � 1)(c � 1)9N (r � 1)(c � 1)
</p>
<p>z � 
measure of association
</p>
<p>standard error of measure of association
</p>
<p>7For a more detailed discussion of these issues, see Jean Dickson Gibbons, Nonpara-
</p>
<p>metric Measures of Association (Newbury Park, CA: Sage, 1993).
</p>
<p>382</p>
<p/>
</div>
<div class="page"><p/>
<p>M E A S U R E S O F A S S O C I A T I O N F O R O R D I N A L - L E V E L V A R I A B L E S
</p>
<p>or
</p>
<p>H0: There is no association between affectional identification with father
</p>
<p>and delinquency (�c (p) � 0).
</p>
<p>H1: There is an association between affectional identification with father
</p>
<p>and delinquency (�c (p) � 0).
</p>
<p>or
</p>
<p>H0: There is no association between affectional identification with father
</p>
<p>and delinquency (dp � 0).
</p>
<p>H1: There is an association between affectional identification with father
</p>
<p>and delinquency (dp � 0).
</p>
<p>The Sampling Distribution We use the normal distribution to test
</p>
<p>whether the measures of ordinal association differ significantly from 0.
</p>
<p>As with our earlier examples using a normal sampling distribution, the N
</p>
<p>of cases must be large in order for us to relax the normality assumption.
</p>
<p>When examining the relationship between two ordinal-level variables,
</p>
<p>we recommend a sample of at least 60 cases.
</p>
<p>Significance Level and Rejection Region We use the conventional 5% sig-
</p>
<p>nificance level for our example. From Appendix 3, we can determine
</p>
<p>that the critical values for z are �1.96. If the calculated z-score is greater
</p>
<p>than 1.96 or less than �1.96, we will reject the null hypotheses and con-
</p>
<p>clude that the measure of association between affectional identification
</p>
<p>with father and delinquency is significantly different from 0.
</p>
<p>The Test Statistic Since we have three different measures of associa-
</p>
<p>tion&mdash;
, �c, and d&mdash;we need to calculate three separate test statistics. We
</p>
<p>first need to calculate the approximate standard error for gamma, using
</p>
<p>Equation 13.12. We find the standard error for gamma to be 0.0330.
</p>
<p>W orking It Out
</p>
<p> � 0.0330
</p>
<p> � �0.00109
</p>
<p> � � 9687,984
</p>
<p> � � 4(5 � 1)(3 � 1)(9)(1,222)(5 � 1)(3 � 1)
</p>
<p> �̂
 � � 4(r � 1)(c � 1)9N (r � 1)(c � 1)
</p>
<p>383</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>Using the standard error for gamma, we then calculate the z-score
</p>
<p>using Equation 13.11. In our example, we find the z-score for gamma to
</p>
<p>be 7.0212.
</p>
<p>W orking It Out
</p>
<p> � 7.0212
</p>
<p> � 
0.2317
0.0330
</p>
<p> z � 


</p>
<p>�̂

</p>
<p>Turning to �c, we calculate the standard error using Equation 13.13.
</p>
<p>For our example, the standard error for �c is 0.0241.
</p>
<p>W orking It Out
</p>
<p> � 0.0241
</p>
<p> � �0.00058
</p>
<p> � � 96164,970
</p>
<p> � �4(5 � 1)(3 � 1)(9)(1,222)(5)(3)
</p>
<p> �̂� � �4(r � 1)(c � 1)9Nrc
</p>
<p>Using the standard error for �c
</p>
<p>W orking It Out
</p>
<p> � 6.3195
</p>
<p> � 
0.1523
</p>
<p>0.0241
</p>
<p> z � 
�c
</p>
<p>�̂�
</p>
<p>384
</p>
<p>to be 6.3195.
</p>
<p>and Equation 13.11, we find the z-score </p>
<p/>
</div>
<div class="page"><p/>
<p>C H O O S I N G T H E B E S T M E A S U R E O F A S S O C I A T I O N
</p>
<p>For Somers&rsquo; d, we follow the same process, calculating the standard
</p>
<p>error for d and then using the standard error to calculate the z-score for
</p>
<p>d. For our example, the standard error for d is 0.0264 and the corre-
</p>
<p>sponding z-score is 5.1439.
</p>
<p>W orking It Out
</p>
<p> � 0.0264
</p>
<p> � �0.00070
</p>
<p> � � 384549,900
</p>
<p> � � 4(5
2 � 1)(3 � 1)
</p>
<p>(9)(1,222)(52)(3 � 1)
</p>
<p> �̂d � �4(r 
2 � 1)(c � 1)
</p>
<p>9Nr 2(c � 1)
</p>
<p>W orking It Out
</p>
<p> � 5.1439
</p>
<p> � 
0.1358
</p>
<p>0.0264
</p>
<p> z � 
d
</p>
<p>�̂d
</p>
<p>The Decision All three of the calculated z-scores are greater than 1.96,
</p>
<p>meaning that we reject the null hypotheses and conclude in the case of
</p>
<p>each test that there is a statistically significant relationship between affec-
</p>
<p>tional identification with father and delinquency.
</p>
<p>C h o o s i n g  t h e  B e s t  M e a s u r e  o f  A s s o c i a t i o n  
f o r  N o m i n a l -  a n d  O r d i n a l - L e v e l  V a r i a b l e s  
</p>
<p>Because we have covered so many different measures in this chapter,
</p>
<p>we thought it would be useful to recap them in a simple table that can
</p>
<p>be used in deciding which measure of association is appropriate 
</p>
<p>385</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>for which specific research problem. Table 13.14 presents summary
</p>
<p>information on the measures of association for nominal and ordinal
</p>
<p>variables discussed in this chapter. The first column of Table 13.14
</p>
<p>gives the measure of association, the second column notes the appro-
</p>
<p>priate level of measurement for the two variables, the third column
</p>
<p>tells whether the measure of association is also a PRE measure, and the
</p>
<p>fourth column lists any restrictions on the size of the table used in the
</p>
<p>analysis. Thus, for any given pair of nominal or ordinal variables, you
</p>
<p>should be able to determine which measure of association best suits
</p>
<p>your needs.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>Measures of association for nominal and ordinal variables allow
</p>
<p>researchers to go beyond a simple chi-square test for independence
</p>
<p>between two variables and assess the strength of the relationship. 
</p>
<p>The measures of association discussed in this chapter are the most
</p>
<p>commonly used measures of association for nominal and ordinal
</p>
<p>variables.
</p>
<p>Two of the measures of association for nominal variables are based
</p>
<p>on the value of the chi-square statistic. Phi (�) adjusts the value of chi-
</p>
<p>square by taking into account the size of the sample, but is useful only
</p>
<p>for 2 � 2 tables. Cramer&rsquo;s V is also based on the value of the chi-square
</p>
<p>statistic, but makes an additional adjustment for the numbers of rows
</p>
<p>and columns in the table. One of the difficulties with the interpretation
</p>
<p>Summary of Measures of Association 
for Nominal and Ordinal Variables
</p>
<p>MEASURE OF LEVEL OF DIMENSIONS OF TABLE
</p>
<p>ASSOCIATION MEASUREMENT PRE MEASURE? (ROWS BY COLUMNS)
</p>
<p>� Nominal No 2 � 2
V Nominal No Any size
	 Nominal Yes Any size
� Nominal Yes Any size

 Ordinal Yes Any size
�b Ordinal Yes Number of rows �
</p>
<p>Number of columns
�c Ordinal No Number of rows �
</p>
<p>Number of columns
d Ordinal Yes Any size
</p>
<p>Table 13.14
</p>
<p>386</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S U M M A R Y
</p>
<p>of phi and V is that a value that falls between 0 and 1 does not have a
</p>
<p>precise interpretation. We can infer that as values approach 0, there is a
</p>
<p>weak relationship between the two variables. Similarly, as values ap-
</p>
<p>proach 1, there is a strong (or near perfect) relationship between the two
</p>
<p>variables.
</p>
<p>Goodman and Kruskal&rsquo;s tau and lambda are measures of associa-
</p>
<p>tion that are not based on the value of the chi-square statistic and instead
</p>
<p>use different decision rules for classifying cases. Tau relies on the pro-
</p>
<p>portional distribution of cases in a table, while lambda relies on the
</p>
<p>modal values of the dependent variable overall and within each level or
</p>
<p>category of the independent variable. Tau and lambda offer an improve-
</p>
<p>ment over phi and V in that a value between 0 and 1 can be interpreted
</p>
<p>directly as the proportional reduction in errors made by using informa-
</p>
<p>tion about the independent variable. More generally, this characteristic is
</p>
<p>called proportional reduction in error, or PRE. PRE measures tell us
</p>
<p>how much knowledge of one measure helps to reduce the errors we
</p>
<p>make in defining the values of a second measure. Both measures require
</p>
<p>that we define at the outset which variable is the dependent variable
</p>
<p>and which variable is the independent variable. The dependent vari-
</p>
<p>able is the outcome variable&mdash;the phenomenon that we are interested in
</p>
<p>explaining. As it is dependent on other variables, it is influenced&mdash;or we
</p>
<p>expect it to be influenced&mdash;by other variables. The variables that affect,
</p>
<p>or influence, the dependent variable are referred to as the independent
</p>
<p>variables.
</p>
<p>There are four common measures of association for ordinal vari-
</p>
<p>ables: gamma (�), Kendall&rsquo;s �b and �c, and Somers&rsquo; d. Measures of
</p>
<p>association for ordinal variables are all based on concordant pairs
</p>
<p>and discordant pairs of observations. Concordant pairs are pairs of
</p>
<p>observations that have consistent rankings on the two variables (e.g.,
</p>
<p>high on both variables or low on both variables), while discordant
</p>
<p>pairs are those pairs of observations that have inconsistent rankings on
</p>
<p>the two variables (e.g., high on one variable and low on the other vari-
</p>
<p>able). Gamma uses information only on the concordant and discordant
</p>
<p>pairs of observations. The remaining measures of association&mdash;
</p>
<p>Kendall&rsquo;s tau measures and Somers&rsquo; d&mdash;use information about pairs of
</p>
<p>observations that have tied rankings. All four of the measures of associ-
</p>
<p>ation for ordinal variables discussed in this chapter have values ranging
</p>
<p>from �1.0 to 1.0, where a value of �1.0 indicates a perfect negative re-
</p>
<p>lationship (i.e., as we increase the value of one variable, the other vari-
</p>
<p>able decreases), a value of 1.0 indicates a perfect positive relationship
</p>
<p>(i.e., as we increase the value of one variable, the other variable also
</p>
<p>increases), and a value of 0.0 indicates no relationship between the
</p>
<p>two variables. Gamma (
), Kendall&rsquo;s �b, and Somers&rsquo; d all have PRE
</p>
<p>interpretations.
</p>
<p>387</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>K e y  T e r m s
</p>
<p>concordant pairs of observations Pairs
</p>
<p>of observations that have consistent rank-
</p>
<p>ings on two ordinal variables.
</p>
<p>Cramer&rsquo;s V A measure of association for
</p>
<p>two nominal variables that adjusts the chi-
</p>
<p>square statistic by the sample size. V is ap-
</p>
<p>propriate when at least one of the nominal
</p>
<p>variables has more than two categories.
</p>
<p>dependent variable The outcome vari-
</p>
<p>able; the phenomenon that we are inter-
</p>
<p>ested in explaining. It is dependent on
</p>
<p>other variables in the sense that it is influ-
</p>
<p>enced&mdash;or we expect it to be influenced&mdash;
</p>
<p>by other variables.
</p>
<p>discordant pairs of observations Pairs
</p>
<p>of observations that have inconsistent rank-
</p>
<p>ings on two ordinal variables.
</p>
<p>gamma (�) PRE measure of association 
</p>
<p>for two ordinal variables that uses informa-
</p>
<p>tion about concordant and discordant 
</p>
<p>pairs of observations within a table.
</p>
<p>Gamma has a standardized scale ranging
</p>
<p>from �1.0 to 1.0.
</p>
<p>Goodman and Kruskal&rsquo;s tau (�) PRE
</p>
<p>measure of association for two nominal
</p>
<p>variables that uses information about the
</p>
<p>proportional distribution of cases within a
</p>
<p>table. Tau has a standardized scale ranging
</p>
<p>from 0 to 1.0. For this measure, the re-
</p>
<p>searcher must define the independent and
</p>
<p>dependent variables.
</p>
<p>independent variable A variable as-
</p>
<p>sumed by the researcher to affect or influ-
</p>
<p>ence the dependent variable.
</p>
<p>Kendall&rsquo;s �b PRE measure of association
</p>
<p>for two ordinal variables that uses informa-
</p>
<p>tion about concordant pairs, discordant
</p>
<p>pairs, and pairs of observations tied on
</p>
<p>both variables examined. �b has a standard-
</p>
<p>ized scale ranging from �1.0 to 1.0 and is
</p>
<p>appropriate only when the number of rows
</p>
<p>equals the number of columns in a table.
</p>
<p>Kendall&rsquo;s �c A measure of association for
</p>
<p>two ordinal variables that uses information
</p>
<p>about concordant pairs, discordant pairs,
</p>
<p>and pairs of observations tied on both vari-
</p>
<p>ables examined. �c has a standardized scale
</p>
<p>ranging from �1.0 to 1.0 and is appropriate
</p>
<p>when the number of rows is not equal to
</p>
<p>the number of columns in a table.
</p>
<p>lambda (�) PRE measure of association
</p>
<p>for two nominal variables that uses infor-
</p>
<p>independent variable. Lambda has a stan-
</p>
<p>dardized scale ranging from 0 to 1.0.
</p>
<p>phi (�) A measure of association for two
</p>
<p>nominal variables that adjusts the chi-
</p>
<p>square statistic by the sample size. Phi is
</p>
<p>appropriate only for nominal variables that
</p>
<p>each have two categories.
</p>
<p>proportional reduction in error (PRE)
</p>
<p>The proportional reduction in errors made
</p>
<p>when the value of one measure is pre-
</p>
<p>dicted using information about the second
</p>
<p>measure.
</p>
<p>Somers&rsquo; d PRE measure of association for
</p>
<p>two ordinal variables that uses information
</p>
<p>about concordant pairs, discordant pairs,
</p>
<p>and pairs of observations tied on the inde-
</p>
<p>pendent variable. Somers&rsquo; d has a standard-
</p>
<p>ized scale ranging from �1.0 to 1.0.
</p>
<p>tied pairs of observations (ties) Pairs of
</p>
<p>observation that have the same ranking on
</p>
<p>two ordinal variables.
</p>
<p>388
</p>
<p>dependent variable for each category of the
</p>
<p>mation about the modal category of the </p>
<p/>
</div>
<div class="page"><p/>
<p>S Y M B O L S A N D F O R M U L A S
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>C Number of concordant pairs of observations
</p>
<p>D Number of discordant pairs of observations
</p>
<p>Nrow Total number of observations for each row
</p>
<p>Ncol Total number of observations for each column
</p>
<p>TX Number of pairs of observations tied on the independent 
</p>
<p>variable
</p>
<p>TY Number of pairs of observations tied on the dependent 
</p>
<p>variable
</p>
<p>� Phi; measure of association for nominal variables
</p>
<p>V Cramer&rsquo;s V; measure of association for nominal variables
</p>
<p>	 Lambda; measure of association for nominal variables
</p>
<p>� Goodman and Kruskal&rsquo;s tau; measure of association for nominal
</p>
<p>variables
</p>
<p>
 gamma; measure of association for ordinal variables
</p>
<p>�b Kendall&rsquo;s �b; measure of association for ordinal variables
</p>
<p>�c Kendall&rsquo;s �c; measure of association for ordinal variables
</p>
<p>d Somers&rsquo; d; measure of association for ordinal variables
</p>
<p>To calculate phi (�):
</p>
<p>To calculate Cramer&rsquo;s V:
</p>
<p>V � � �
2
</p>
<p>N � min(r � 1, c � 1)
</p>
<p>� � ��
2
</p>
<p>N
</p>
<p>389</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>To calculate Goodman and Kruskal&rsquo;s tau:
</p>
<p>To calculate lambda:
</p>
<p>To calculate the number of tied pairs of observations on the
</p>
<p>independent variable:
</p>
<p>To calculate the number of tied pairs of observations on the dependent
</p>
<p>variable:
</p>
<p>To calculate gamma:
</p>
<p>To calculate �b:
</p>
<p>To calculate �c:
</p>
<p>To calculate Somers&rsquo; d:
</p>
<p>To calculate the z-score:
</p>
<p>z � 
measure of association
</p>
<p>standard error of measure of association
</p>
<p>dYX � 
C � D
</p>
<p>N (N � 1)/2 � TX
</p>
<p> �c � 
C � D
</p>
<p>1
2
</p>
<p> N 2[(m � 1)/m]
, where m � min(r, c)
</p>
<p>�b � 
C � D
</p>
<p>�[N (N � 1)/2 � TX][N (N � 1)/2 � TY ]
</p>
<p>
 � 
C � D
C � D
</p>
<p>TY � 
1
2
 � Ncol(Ncol � 1)
</p>
<p>TX � 
1
2
 � Nrow(Nrow � 1)
</p>
<p> � 
</p>
<p>�number of errorsdependent variable� � �
number of errors
using mode of
dependent variable
by level of
independent variable
</p>
<p>�
</p>
<p>� � 
</p>
<p>�number of errorswithout knowledge ofindependent variable � � �
number of errors
with knowledge of
independent variable�
</p>
<p>number of errors without knowledge of independent variable
</p>
<p>390
</p>
<p>	
</p>
<p>using mode of
</p>
<p>number of errors using mode of dependent variable</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S
</p>
<p>To calculate the standard error for gamma:
</p>
<p>To calculate the standard error for Kendall&rsquo;s tau measures:
</p>
<p>To calculate the standard error for Somers&rsquo; d:
</p>
<p>E x e r c i s e s
</p>
<p>13.1 A researcher studies the link between race of offender and death sen-
tence decision in a state by selecting a random sample of death
penalty cases over a 20-year period. The researcher finds the follow-
ing distribution of death sentence decisions by race:
</p>
<p>Race Sentenced to Death Not Sentenced to Death
</p>
<p>White 8 73
</p>
<p>African American 16 52
</p>
<p>a. Calculate phi for these data.
</p>
<p>b. Calculate Goodman and Kruskal&rsquo;s tau for these data.
</p>
<p>c. Using the values that you calculated for phi and tau, how strongly
related are the race of the offender and receiving a death sentence?
</p>
<p>13.2 Silver Bullet Treatment Services claims to have an effective system for
treating criminal offenders. As evidence for the effectiveness of its pro-
gram, a spokesperson from the organization presents information on
rearrest within one year for 100 individuals randomly assigned to the
treatment program and for 100 individuals randomly assigned to a
control group. The distribution of cases follows:
</p>
<p>Experimental Condition Not Rearrested Rearrested
</p>
<p>Treatment group 75 25
</p>
<p>Control group 40 60
</p>
<p>a. Calculate phi for these data.
</p>
<p>b. Calculate Goodman and Kruskal&rsquo;s tau for these data.
</p>
<p>�̂d � �4(r 
2 � 1)(c � 1)
</p>
<p>9Nr 2(c � 1)
</p>
<p>�̂� � �4(r � 1)(c � 1)9Nrc
</p>
<p>�̂
 � � 4(r � 1)(c � 1)9N (r � 1)(c � 1)
</p>
<p>391</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>c. Calculate lambda for these data.
</p>
<p>d. Based on these three measures of association, what can you con-
clude about the strength of the relationship between the treatment
and rearrest?
</p>
<p>13.3 A graduate student is interested in the relationship between the gen-
der of a violent crime victim and the victim&rsquo;s relationship to the of-
fender. To study this relationship, the student analyzes survey data
collected on a random sample of adults. Among those persons who
had been victims of violent crimes, the student finds the following dis-
tribution of cases by gender:
</p>
<p>Relationship of Offender to Victim
</p>
<p>Gender Stranger Friend Partner
</p>
<p>Male 96 84 21
</p>
<p>Female 55 61 103
</p>
<p>a. Calculate V for these data.
</p>
<p>b. Calculate Goodman and Kruskal&rsquo;s tau for these data.
</p>
<p>c. Calculate lambda for these data.
</p>
<p>d. Based on these three measures of association, what can you con-
clude about the strength of the relationship between gender and
the victim&rsquo;s relationship to a violent offender?
</p>
<p>13.4 In an attempt to explore the relationship between type of legal repre-
sentation and method of case disposition, a student working on a re-
search project randomly selects a small sample of cases from the local
court. The student finds the following distribution of cases:
</p>
<p>Method of Case Disposition
</p>
<p>Type of Legal Convicted by Convicted by Guilty
</p>
<p>Representation Trial Plea Acquitted
</p>
<p>Privately retained 10 6 4
</p>
<p>Public defender 3 17 2
</p>
<p>Legal aid 3 13 1
</p>
<p>a. Calculate V for these data.
</p>
<p>b. Calculate Goodman and Kruskal&rsquo;s tau for these data.
</p>
<p>c. Calculate lambda for these data.
</p>
<p>d. Based on these three measures of association, what should the stu-
dent conclude about the relationship between type of legal repre-
sentation and method of case disposition?
</p>
<p>392</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S
</p>
<p>13.5 A researcher interested in the link between attacking other students
and being bullied by other students at school used data from a self-
report survey administered to a random sample of teenagers. The dis-
tribution of responses was as follows:
</p>
<p>Attacked Another Student
</p>
<p>Bullied Never Once Two or More Times
</p>
<p>Never 59 22 19
</p>
<p>Once 31 44 52
</p>
<p>Two or more times 25 29 61
</p>
<p>a. Calculate gamma for these data.
</p>
<p>b. Calculate �b for these data.
</p>
<p>c. Calculate Somers&rsquo; d for these data.
</p>
<p>d. Interpret each of the three measures of association. What can you
conclude about the relationship between being bullied and attack-
ing other students?
</p>
<p>13.6 In response to an increasing reluctance of individuals to serve on ju-
ries, a study is commissioned to investigate what might account for
the public&rsquo;s change of heart. Wondering whether prior jury experience
has any effect on how favorably the jury system is viewed, a re-
searcher constructs the following table:
</p>
<p>&ldquo;How would you rate the current jury system?&rdquo;
</p>
<p>Very Very 
</p>
<p>Served on a jury Unfavorable Unfavorable Favorable Favorable
</p>
<p>Never 22 20 21 26
</p>
<p>Once 11 19 12 13
</p>
<p>Two or three times 18 23 9 6
</p>
<p>Four or more times 21 15 7 4
</p>
<p>a. Calculate gamma for these data.
</p>
<p>b. Calculate �b for these data.
</p>
<p>c. Calculate Somers&rsquo; d for these data.
</p>
<p>d. Interpret each of the three measures of association. What can you
conclude about the relationship between serving on a jury and atti-
tudes about the jury system?
</p>
<p>13.7 A researcher interested in the relationship between attitudes about
school and drug use analyzed data from a delinquency survey admin-
istered to a random sample of high school youth. The researcher was
</p>
<p>393</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S
</p>
<p>particularly interested in how well the youth liked school and their
use of marijuana. A cross-tabulation of responses revealed the follow-
ing distribution of cases:
</p>
<p>Smoked Marijuana in the Last Year
</p>
<p>Once or Three or
</p>
<p>I Like School Never Twice More Times
</p>
<p>Strongly agree 52 20 12
</p>
<p>Agree 48 26 20
</p>
<p>Disagree 31 32 33
</p>
<p>Strongly disagree 35 45 50
</p>
<p>a. Calculate gamma for these data.
</p>
<p>b. Calculate �c for these data. Explain why �b is not appropriate for
these data.
</p>
<p>c. Calculate Somers&rsquo; d for these data.
</p>
<p>d. Interpret each of the three measures of association. What can you
conclude about the relationship between liking school and smoking
marijuana?
</p>
<p>13.8 A public opinion poll asked respondents whether punishments for
</p>
<p>whether their political views were liberal, moderate, or conservative. A
cross-tabulation of the responses to these two questions shows the fol-
lowing distribution of cases:
</p>
<p>Criminal punishments should be . . .
</p>
<p>Political
</p>
<p>Views More Severe About the Same Less Severe
</p>
<p>Liberal 8 54 79
</p>
<p>Moderate 35 41 37
</p>
<p>Conservative 66 38 12
</p>
<p>a. Calculate gamma for these data.
</p>
<p>b. Calculate �c for these data. Explain why �b is not appropriate for
these data.
</p>
<p>c. Calculate Somers&rsquo; d for these data.
</p>
<p>d. Interpret each of the three measures of association. What can you
conclude about the relationship between views about politics and
attitudes about criminal punishments?
</p>
<p>394
</p>
<p>convicted criminals should be made more severe, made less severe,
or kept about the same. The respondents were also asked to state</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 395
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>Many of the measures of association discussed in this chapter are available in 
</p>
<p> common statistical packages. There are variations in coverage, however, as we 
</p>
<p>note below. There are also sample files containing examples of syntax for both 
</p>
<p>SPSS (Chapter_13.sps) and Stata (Chapter_13.do).
</p>
<p>SPSS
</p>
<p>Each measure of  association discussed in this chapter is available in SPSS  
</p>
<p>with the CROSSTABS command discussed in Chapter 9. Recall from that  
</p>
<p>discussion that the computation of  the chi-square statistic was obtained 
</p>
<p>through the /STATISTICS= option. To obtain all the nominal and ordinal 
</p>
<p>measures of  association we have discussed in this chapter, we would simply 
</p>
<p>add to the list of  association measures:
</p>
<p>where CHISQ is the chi-square, D is Somers&rsquo; d, and BTAU and CTAU are 
</p>
<p>Kendall&rsquo;s Tau-b and Tau-c, respectively. PHI, LAMBDA, and GAMMA are 
</p>
<p>self-explanatory. Although it is not listed in the command line, Goodman and 
</p>
<p>Kruskal&rsquo;s tau is obtained with the LAMBDA option. Note that you will not need 
</p>
<p>all of these measures for every comparison, and you should pay some attention 
</p>
<p>to the level of measurement and select only those measures that make sense  
</p>
<p>for your data.
</p>
<p>In the output generated by this command, you will be presented with the 
</p>
<p>cross-tabulation of the two variables, followed by additional tables that give  
</p>
<p>the various measures of association. Depending on which measures you  
</p>
<p>have requested, you may have three measures of lambda and two measures of 
</p>
<p>Goodman and Kruskal&rsquo;s tau reported. The key to reading the correct values for 
</p>
<p>lambda and tau is to know which variable is the dependent (or outcome) variable.
</p>
<p>To illustrate this process, enter the data from Table 13.5 on race and cell 
</p>
<p>block (follow the same process used in the exercises in Chapter 9). Recall in 
</p>
<p>the discussion of the cell block assignment data that we treated cell block as 
</p>
<p>the dependent variable. The value reported for lambda in the line for cell block 
</p>
<p>as the dependent variable will match the value reported in the text. The value 
</p>
<p>reported for Goodman and Kruskal&rsquo;s tau in the line for cell block as the  
</p>
<p>dependent variable will differ slightly from that reported in the text because  
</p>
<p>SPSS does not round the prediction errors to the nearest integer; instead, it 
</p>
<p>records prediction errors with digits after the decimal.
</p>
<p>CROSSTABS
</p>
<p>/TABLES = row_variable BY column_variable
</p>
<p>/STATISTICS = CHISQ PHI LAMBDA GAMMA D BTAU CTAU.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R T H I R T E E N :  N O M I N A L A N D O R D I N A L V A R I A B L E S396
</p>
<p>Stata
</p>
<p>Measures of association are available through the use of the tabulate  
</p>
<p>command by adding either a list of association measures desired or using all. 
</p>
<p>Unfortunately, Stata does not compute a wide range of association measures  
</p>
<p>for nominal and ordinal measures.
</p>
<p>The command for computing all of the possible (in Stata) measures of  
</p>
<p>association is
</p>
<p>The output will contain chi-square, Cramer&rsquo;s V, gamma, and Kendall&rsquo;s Tau-b. 
</p>
<p>Cramer&rsquo;s V will apply to nominal data, while gamma and Kendall&rsquo;s Tau-b will 
</p>
<p>apply to ordinal data.
</p>
<p>Problems
</p>
<p>The first four problems are likely done more effectively in SPSS, since Stata has 
</p>
<p>limited abilities to compute these measures of association.
</p>
<p> 2. Enter the data from Table 13.13 into SPSS. Compute the values of  
</p>
<p>gamma, τ
c
, and Somers&rsquo; d for these data. How do the values of  these 
</p>
<p>measures of  association compare to those reported in the text? Test the 
</p>
<p>statistical significance of  each of  the measures of  association.
</p>
<p> 3. Enter the data from Exercise 13.2 into SPSS. Compute the values of  phi, 
</p>
<p>tau, and lambda. How do these measures of  association compare to the 
</p>
<p>values that you calculated for this exercise? Test the statistical significance 
</p>
<p>of  each of  the measures of  association.
</p>
<p> 4. Enter the data from Exercise 13.6 into SPSS. Compute the values of  
</p>
<p>gamma, τ
c
, and Somers&rsquo; d for these data. How do these measures of  asso-
</p>
<p>ciation compare to the values that you calculated for this exercise?
</p>
<p> 5. Open the NYS data file (nys_1.sav, nys_1_student.sav, or nys_1.dta). Each 
</p>
<p>pair of  variables listed below was tested for a relationship using the  
</p>
<p>chi-square test in the computer exercises at the end of  Chapter 9. For each 
</p>
<p>pair of  variables, determine the level of  measurement (nominal or ordinal) 
</p>
<p>and the dependent and the independent variables; then compute appropri-
</p>
<p>ate measures of  association to the extent that you are able. Interpret each 
</p>
<p>of  the measures of  association that you have computed. Test the statistical 
</p>
<p>significance of  each of  the measures of  association. What can you con-
</p>
<p>clude about the relationship between each pair of  variables?
</p>
<p>a. What is the relationship between ethnicity and grade point average?
</p>
<p>b. What is the relationship between marijuana use among friends and the 
</p>
<p>youth&rsquo;s attitudes about marijuana use?
</p>
<p>tabulate row_variable column_variable, all
</p>
<p> 1. Enter the data from Table 13.7 into SPSS. Compute the values of  
</p>
<p>Cramer&rsquo;s V, tau, and lambda for these data. How do the values of  these 
</p>
<p>measures of  association compare to those reported in the text?</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 397
</p>
<p>c. What is the relationship between the importance of  going to college 
</p>
<p>and the importance of  having a job?
</p>
<p>d. What is the relationship between grade point average and the impor-
</p>
<p>tance of  having a job?
</p>
<p>e. What is the relationship between the youth&rsquo;s sex and the importance of  
</p>
<p>having friends?
</p>
<p>f. What is the relationship between the importance of  having a job and 
</p>
<p>the youth&rsquo;s attitudes about having a job?
</p>
<p> 6. Open the Pennsylvania Sentencing data file (pcs_98.sav or pcs_98.dta). 
</p>
<p>Each pair of  variables listed below was tested for a relationship using the 
</p>
<p>chi-square test in the computer exercises at the end of  Chapter 9. For each 
</p>
<p>pair of  variables, determine the level of  measurement (nominal or ordinal) 
</p>
<p>and the dependent and the independent variables; then compute appropri-
</p>
<p>ate measures of  association. Interpret each of  the measures of  association 
</p>
<p>that you have computed. Test the statistical significance of  each of  the 
</p>
<p>measures of  association. What can you conclude about the relationship 
</p>
<p>between each pair of  variables?
</p>
<p>a. Is the sex of  the offender related to the method of  conviction?
</p>
<p>b. Is the race&ndash;ethnicity of  the offender related to whether the offender 
</p>
<p>was incarcerated or not?
</p>
<p>c. Is the method of  conviction related to the type of  punishment  
</p>
<p>received?
</p>
<p>d. Is the type of  conviction offense related to the method of  conviction?</p>
<p/>
</div>
<div class="page"><p/>
<p>Measuring Association for Interval-Level
</p>
<p>Data: Pearson&rsquo;s Correlation Coefficient
</p>
<p>What Does a Correlation Coefficient Describe?
</p>
<p>When Might Pearson&rsquo;s r Provide Misleading Results?
</p>
<p>C h a p t e r  f o u r t e e n
</p>
<p>T h e  l i n e a r  c o r r e l a t i o n  c o e f f i c i e n t
</p>
<p>T e s t i n g  f o r  s t a t i s t i c a l  s i g n i f i c a n c e
</p>
<p>What are the Characteristics of Pearson&rsquo;s r?
</p>
<p>What are the Characteristics of Spearman&rsquo;s r?
</p>
<p>What is the Test of Statistical Significance for Pearson&rsquo;s r?
</p>
<p>What is the Test of Statistical Significance for Spearman&rsquo;s r?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_14,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>THIS CHAPTER INTRODUCES the linear correlation coefficient, a widely
used descriptive statistic that enables the researcher to describe the rela-
</p>
<p>tionship between two interval-level measures. This situation is encoun-
</p>
<p>tered often in criminal justice research. For example, researchers may
</p>
<p>want to establish whether number of prior arrests is related to age, edu-
</p>
<p>cation, or monthly income. Similarly, it is common in criminal justice re-
</p>
<p>search to ask whether the severity of a sanction measured on an interval
</p>
<p>scale (e.g., number of years sentenced to imprisonment or amount of a
</p>
<p>fine) is related to such variables as the amount stolen in an offense or
</p>
<p>the number of prior arrests or convictions of a defendant. We also exam-
</p>
<p>ine an alternative rank-order measure of association that may be used
</p>
<p>when the linear correlation coefficient will lead to misleading results.
</p>
<p>M e a s u r i n g  A s s o c i a t i o n  
B e t w e e n  T w o  I n t e r v a l - L e v e l  V a r i a b l e s
</p>
<p>example, that we are presented with the data in Table 14.1. Can we find a
</p>
<p>simple way of expressing the relationship between these two variables?
</p>
<p>For each of the 15 young offenders in our sample, we have informa-
</p>
<p>tion regarding age and number of arrests over the last year. The mean
</p>
<p>age of the sample overall is 17.1 years. The mean number of arrests is
</p>
<p>4.9. These statistics describe the characteristics of our sample overall,
</p>
<p>One way to understand this relationship is to change one of these
</p>
<p>measures into a categorical variable. For example, we might divide the
</p>
<p>399
</p>
<p>of examining a new statistic to describe the relationship between two 
</p>
<p>It may not be intuitively obvious why we need to go to the trouble
</p>
<p>interval-level measures. Why can&rsquo;t we just use the means, as we did when 
</p>
<p>we examined interval-level measures in Chapters 11 and 12? Suppose, for
</p>
<p>but, importantly, they do not help us to understand the relationship
</p>
<p>between age and arrests in the study.</p>
<p/>
</div>
<div class="page"><p/>
<p>offenders into two groups&mdash;one consisting of offenders under age 18
</p>
<p>and the other of offenders 18 and older. Then we could use the same
</p>
<p>approach taken in earlier chapters and simply compare the means for
</p>
<p>the younger and older groups, as shown in Table 14.2. On average, the
</p>
<p>older offenders appear to have more arrests than the younger offenders
</p>
<p>( versus ).
</p>
<p>Similarly, we could divide arrests into categories and compare the
</p>
<p>mean age of offenders in each category. For example, Table 14.3 divides
</p>
<p>arrests into three categories: low number of arrests (less than 3), moder-
</p>
<p>ate number of arrests (3&ndash;8), and high number of arrests (9 and above).
</p>
<p>This table again shows that, on average, older offenders have more ar-
</p>
<p>rests than younger ones. In this case, the mean age for the high-arrest
</p>
<p>X � 2.500X � 7.571
</p>
<p>Age and Number of Arrests over the Last Year for 15 Young Offenders
</p>
<p>SUBJECT NUMBER OF ARRESTS AGE
</p>
<p>1 0 14
2 1 13
3 1 15
4 2 13
5 2 14
6 3 14
7 3 17
8 4 19
9 4 21
</p>
<p>10 6 19
11 8 16
12 9 18
13 9 20
14 10 21
15 11 22
</p>
<p>X � 17.0667X � 4.8667
</p>
<p>Table 14.1
</p>
<p>Mean Numbers of Arrests for Offenders 
Under Age 18 versus Those Age 18 and Older
</p>
<p>NUMBER OF ARRESTS NUMBER OF ARRESTS
</p>
<p>(UNDER AGE 18) (AGE 18 AND OLDER)
</p>
<p>0 4
1 4
1 6
2 9
2 9
3 10
3 11
8
</p>
<p>X � 7.5714X � 2.5000
</p>
<p>Table 14.2
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A400</p>
<p/>
</div>
<div class="page"><p/>
<p>group was 20.3 and those for the moderate- and low-arrest groups were
</p>
<p>17.7 and 13.8, respectively.
</p>
<p>Although this approach allowed us to come to a general conclusion
</p>
<p>regarding the relationship between age and arrests in our sample, it
</p>
<p>forced us to convert one measure from an interval- to a nominal-level
</p>
<p>variable. In each example, we had to take a step down the ladder of
</p>
<p>measurement, which means that we did not use all of the information
</p>
<p>provided by our data. This, of course, violates one of the general princi-
</p>
<p>ples stated earlier in the text: Statistics based on more information are
</p>
<p>generally preferred over those based on less information.
</p>
<p>But how can we describe the relationship between two interval-level
</p>
<p>variables without converting one to a nominal scale? A logical solution to
</p>
<p>this dilemma is provided by a coefficient named after Karl Pearson, a
</p>
<p>noted British statistician who died in 1936. Pearson&rsquo;s r estimates the
</p>
<p>correlation, or relationship, between two measures by comparing how
</p>
<p>specific individuals stand relative to the mean of each measure. Pear-
</p>
<p>son&rsquo;s correlation coefficient (r) has become one of the most widely
</p>
<p>used measures of association in the social sciences.
</p>
<p>P e a r s o n &rsquo; s  C o r r e l a t i o n  C o e f f i c i e n t
</p>
<p>Pearson&rsquo;s r is based on a very simple idea. If we use the mean of each
</p>
<p>distribution as a starting point, we can then see how specific individuals
</p>
<p>in the sample stand on each measure relative to its mean. If, in general,
</p>
<p>people who are above average on one trait are also above average on
</p>
<p>another, we can say that there is a generally positive relationship be-
</p>
<p>tween the two traits. That is, being high, on average, on one trait is re-
</p>
<p>lated to being high, on average, on the other. If, in contrast, people who
</p>
<p>are higher, on average, on one trait tend to be low, on average, on the
</p>
<p>Mean Ages for Offenders with Low, Moderate, 
and High Numbers of Arrests
</p>
<p>LOW NUMBER OF ARRESTS MODERATE NUMBER OF ARRESTS HIGH NUMBER OF ARRESTS
</p>
<p>(0&ndash;2) (3&ndash;8) (9�)
</p>
<p>14 14 18
13 17 20
15 19 21
13 21 22
14 19
</p>
<p>16
X � 20.2500X � 17.6667X � 13.8000
</p>
<p>Table 14.3
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 401</p>
<p/>
</div>
<div class="page"><p/>
<p>other, then we conclude that there is a negative relationship between
</p>
<p>those traits.
</p>
<p>To illustrate these relationships, let&rsquo;s use the data presented in Table
</p>
<p>14.1. If we put a plus next to each subject whose average age or number
</p>
<p>of arrests is above the mean for the sample overall and a minus next to
</p>
<p>those whose average is below the mean, a pattern begins to emerge (see
</p>
<p>Table 14.4). When a subject is above average in number of arrests, the
</p>
<p>subject is also generally above average in age. This is true for five of the
</p>
<p>six subjects above average in number of arrests (subjects 10, 12, 13, 14,
</p>
<p>and 15). Conversely, when a subject is below average in number of ar-
</p>
<p>rests, the subject is generally below the mean age for the sample. This is
</p>
<p>true for seven of the nine subjects below average in number of arrests
</p>
<p>(subjects 1 through 7).
</p>
<p>Accordingly, for this sample, subjects generally tend to stand in the
</p>
<p>same relative position to the mean for both age and arrests. When indi-
</p>
<p>viduals in the sample have a relatively high number of arrests, they also
</p>
<p>tend to be relatively older. When they have fewer arrests, they tend to
</p>
<p>be younger than average for the sample. A simple mathematical way to
</p>
<p>express this relationship is to take the product of the signs. By doing
</p>
<p>this, we find that for 12 of the 15 subjects, the result is a positive value
</p>
<p>(see Table 14.4). Put simply, 12 of the cases move in the same direction
</p>
<p>relative to the mean. The relationship observed in this case is generally
</p>
<p>positive.
</p>
<p>A Positive Relationship Between Age and Number 
of Arrests for 15 Young Offenders Relative to the Means
</p>
<p>ABOVE OR ABOVE OR
</p>
<p>NUMBER OF BELOW THE BELOW THE PRODUCT OF
</p>
<p>SUBJECT ARRESTS MEAN? AGE MEAN? THE SIGNS
</p>
<p>1 0 � 14 � �
2 1 � 13 � �
3 1 � 15 � �
4 2 � 13 � �
5 2 � 14 � �
6 3 � 14 � �
7 3 � 17 � �
8 4 � 19 � �
9 4 � 21 � �
</p>
<p>10 6 � 19 � �
11 8 � 16 � �
12 9 � 18 � �
13 9 � 20 � �
14 10 � 21 � �
15 11 � 22 � �
</p>
<p>X � 17.0667X � 4.8667
</p>
<p>Table 14.4
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A402</p>
<p/>
</div>
<div class="page"><p/>
<p>A generally negative relationship can be illustrated by reversing the
</p>
<p>scores for arrests in Table 14.4. That is, the first subject does not have 0
</p>
<p>arrests, but 11; the second does not have 1 arrest, but 10; and so forth. If
</p>
<p>we now indicate each subject&rsquo;s placement relative to the mean, we ob-
</p>
<p>tain the set of relationships listed in Table 14.5. In this table, subjects
</p>
<p>who are above average in number of arrests are generally below average
</p>
<p>in age, and subjects who are below average in number of arrests are
</p>
<p>generally above average in age. The products of these signs are mostly
</p>
<p>negative. Put differently, the scores generally move in opposite direc-
</p>
<p>tions relative to the mean. There is still a relationship between age and
</p>
<p>number of arrests, but in this case the relationship is negative.
</p>
<p>This is the basic logic that underlies Pearson&rsquo;s r. However, we need to
</p>
<p>take into account two additional pieces of information to develop this
</p>
<p>correlation coefficient. The first is the values of scores. Using plus (�)
</p>
<p>and minus (�) divides the scores into categories and thus does not take
</p>
<p>full advantage of the information provided by interval-level measures.
</p>
<p>Accordingly, instead of taking the product of the signs, we take the
</p>
<p>product of the difference between the actual scores and the sample
</p>
<p>means. This measure is termed the covariation of scores and is ex-
</p>
<p>pressed mathematically in Equation 14.1.
</p>
<p>Equation 14.1Covariation of scores � �
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>A Negative Relationship Between Age and Number 
of Arrests for 15 Young Offenders Relative to the Means
</p>
<p>ABOVE OR ABOVE OR
</p>
<p>NUMBER OF BELOW THE BELOW THE PRODUCT OF
</p>
<p>SUBJECT ARRESTS MEAN? AGE MEAN? THE SIGNS
</p>
<p>1 11 � 14 � �
2 10 � 13 � �
3 9 � 15 � �
4 9 � 13 � �
5 8 � 14 � �
6 6 � 14 � �
7 4 � 17 � �
8 4 � 19 � �
9 3 � 21 � �
</p>
<p>10 3 � 19 � �
11 2 � 16 � �
12 2 � 18 � �
13 1 � 20 � �
14 1 � 21 � �
15 0 � 22 � �
</p>
<p>X � 17.0667X � 4.8667
</p>
<p>Table 14.5
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 403</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 14.6 illustrates what we gain by including the values of the
</p>
<p>scores. We now have not only a measure of the subjects&rsquo; placement on
</p>
<p>both variables relative to the mean&mdash;the sign of the relationship&mdash; but
</p>
<p>also an estimate of how strongly the scores vary from the mean. In
</p>
<p>general, for this distribution, the stronger the deviation from the mean
</p>
<p>on one variable, the stronger the deviation on the second variable. For
</p>
<p>example, if we look at the scores most distant in value from the mean
</p>
<p>in terms of number of arrests over the last year, we also find the scores
</p>
<p>most distant in terms of age. Those subjects with either zero or one ar-
</p>
<p>rest are not just younger, on average, than other subjects; they are
</p>
<p>among the youngest offenders overall in the sample. Similarly, those
</p>
<p>with the most arrests (10 or 11) are also the oldest members of the
</p>
<p>sample (ages 21 and 22).
</p>
<p>The covariation of scores provides an important piece of information
</p>
<p>for defining Pearson&rsquo;s r. However, the size of the covariation between
</p>
<p>two measures depends on the units of measurement used. To permit
</p>
<p>comparison of covariation across variables with different units of
</p>
<p>measurement, we must standardize the covariation between the two
</p>
<p>Covariation of Number of Arrests (X1) 
and Age (X2) for 15 Young Offenders
</p>
<p>NUMBER 
</p>
<p>OF ARRESTS AGE
</p>
<p>SUBJECT X1 X2
</p>
<p>1 0 �4.8667 14 �3.0667 14.9247
</p>
<p>2 1 �3,8667 13 �4.0667 15.7247
</p>
<p>3 1 �3.8667 15 �2.0667 7.9913
</p>
<p>4 2 �2.8667 13 �4.0667 11.6580
</p>
<p>5 2 �2.8667 14 �3.0667 8.7913
</p>
<p>6 3 �1.8667 14 �3.0667 5.7246
</p>
<p>7 3 �1.8667 17 �0.0667 0.1245
</p>
<p>8 4 �0.8667 19 1.9333 �1.6756
</p>
<p>9 4 �0.8667 21 3.9333 �3.4090
</p>
<p>10 6 1.1333 19 1.9333 2.1910
</p>
<p>11 8 3.1333 16 �1.0667 �3.3423
</p>
<p>12 9 4.1333 18 0.9333 3.8576
</p>
<p>13 9 4.1333 20 2.9333 12.1242
</p>
<p>14 10 5.1333 21 3.9333 20.1908
</p>
<p>15 11 6.1333 22 4.9333 30.2574
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2) � 125.1332X2 � 17.0667X1 � 4.8667
</p>
<p>(X1i � X1)(X 2i � X 2)X 2i � X 2X1i � X1
</p>
<p>Table 14.6
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A404</p>
<p/>
</div>
<div class="page"><p/>
<p>variables according to the variability within each. This is done by taking
</p>
<p>the square root of the product of the sums of the squared deviations
</p>
<p>from the mean for the two variables. Pearson&rsquo;s r is then the ratio be-
</p>
<p>tween the covariation of scores and this value (see Equation 14.2). The
</p>
<p>numerator of the equation is the covariation of the two variables. The
</p>
<p>Equation 14.2
</p>
<p>ables is negative (i.e., when subjects&rsquo; scores vary in opposite directions
</p>
<p>relative to the mean). The ratio will be largest when there is a good deal
</p>
<p>of covariation of the variables and when the variability of scores around
</p>
<p>each mean is small. The ratio will be smallest when there is little covaria-
</p>
<p>tion and a good deal of variability in the measures. The range of possible
</p>
<p>values of r is between �1 and �1.
</p>
<p>The Calculation
</p>
<p>Calculating Pearson&rsquo;s r by hand takes a good deal of work. For that rea-
</p>
<p>son, in the future you will probably enter the data into a computer and
</p>
<p>then use a packaged statistical program to calculate correlation coeffi-
</p>
<p>cients. But it will help you to understand r better if we take the time to
</p>
<p>calculate an actual example. We will use the data on number of arrests
</p>
<p>and age presented in Table 14.1. The calculations needed for Pearson&rsquo;s r
</p>
<p>are shown in Table 14.7.
</p>
<p>To calculate the numerator of Equation 14.2, we must first take the
</p>
<p>simple deviation of each subject&rsquo;s score from the mean number of arrests
</p>
<p>(Table 14.7, column 3) and multiply it by the deviation of the subject&rsquo;s
</p>
<p>age from the mean age of the sample (column 6). The result, the covari-
</p>
<p>ation between the measures, is presented in column 8. So, for the first
</p>
<p>subject, the product of the deviations from the means is 14.9247; for the
</p>
<p>second, it is 15.7247; and so on. The covariation for our problem,
</p>
<p>125.1332, is gained by summing these 15 products.
</p>
<p>To obtain the denominator of the equation, we again begin with the
</p>
<p>deviations of subjects&rsquo; scores from the mean. However, in this case we
</p>
<p>Pearson&rsquo;s r � 
�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>���
N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2���N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2�
</p>
<p>denominator of the equation standardizes this outcome according to
</p>
<p>the square root of the product of the variability found in each of the
</p>
<p>two distributions, again summed across all subjects.
</p>
<p>This ratio will be positive when the covariation between the variables 
</p>
<p>is positive (i.e., when subjects&rsquo; scores vary in the same direction relative 
</p>
<p>to the mean). It will be negative when the covariation between the vari-
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 405</p>
<p/>
</div>
<div class="page"><p/>
<p>do not multiply the two scores for each subject. Rather, we first square
</p>
<p>the deviations from each mean (columns 4 and 7) and then sum the
</p>
<p>squared deviations for each variable. The sum of the squared deviations
</p>
<p>of each score from the mean number of arrests is equal to 187.7333; the
</p>
<p>sum of the squared deviations of each score from the mean age is equal
</p>
<p>to 138.9326. Next we take the product of those deviations, and finally we
</p>
<p>take the square root of that product.
</p>
<p>W orking It Out
</p>
<p> � 161.5001
</p>
<p> � �26,082.2755
</p>
<p> ���
N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2���N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2� � �(187.7333)(138.9326)
</p>
<p>Calculations for the Correlation of Number 
of Arrests (X1) and Age (X2) for 15 Young Offenders
</p>
<p>NUMBER OF ARRESTS AGE
</p>
<p>SUBJECT X1 X2
</p>
<p>(1) (2) (3) (4) (5) (6) (7) (8)
</p>
<p>1 0 �4.8667 23.6848 14 �3.0667 9.4046 14.9247
2 1 �3.8667 14.9514 13 �4.0667 16.5380 15.7247
3 1 �3.8667 14.9514 15 �2.0667 4.2712 7.9913
4 2 �2.8667 8.2180 13 �4.0667 16.5380 11.6580
5 2 �2.8667 8.2180 14 �3.0667 9.4046 8.7913
6 3 �1.8667 3.4846 14 �3.0667 9.4046 5.7246
7 3 �1.8667 3.4846 17 �0.0667 0.0044 0.1245
8 4 �0.8667 0.7512 19 1.9333 3.7376 �1.6756
9 4 �0.8667 0.7512 21 3.9333 15.4708 �3.4090
</p>
<p>10 6 1.1333 1.2844 19 1.9333 3.7376 2.1910
11 8 3.1333 9.8176 16 �1.0667 1.1378 �3.3423
12 9 4.1333 17.0842 18 0.9333 0.8710 3.8576
13 9 4.1333 17.0842 20 2.9333 8.6042 12.1242
14 10 5.1333 26.3508 21 3.9333 15.4708 20.1908
15 11 6.1333 37.6174 22 4.9333 24.3374 30.2574
</p>
<p>X2 � 17.0667X1 � 4.8667
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2) � 125.1332�
N
</p>
<p>i�1
</p>
<p>(X2i � X 2)
2 � 138.9326�
</p>
<p>N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2 � 187.7333
</p>
<p>(X1i � X1)(X 2i � X 2)(X2i � X2)
2X 2i � X 2(X1i � X1)
</p>
<p>2X1i � X1
</p>
<p>Table 14.7
</p>
<p>This leaves us with a value of 161.5001 for the denominator of our
</p>
<p>equation.
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A406</p>
<p/>
</div>
<div class="page"><p/>
<p>Our correlation is about 0.77, meaning that the correlation between age
</p>
<p>and number of arrests is a positive one. As number of arrests increases,
</p>
<p>so does the average age of the offenders in our sample. But what is the
</p>
<p>strength of this relationship? Is it large or small? As discussed in Chapter
</p>
<p>12 when we examined the correlation coefficient eta, whether something
</p>
<p>is large or small is in good measure a value judgment. The answer de-
</p>
<p>pends in part on how the result compares to other research in the same
</p>
<p>area of criminal justice. For example, if other studies produced correla-
</p>
<p>tions that were generally much smaller, we might conclude that the rela-
</p>
<p>tionship in our sample was a very strong one. Jacob Cohen suggests that
</p>
<p>a correlation of 0.10 may be defined as a small relationship; a correlation
</p>
<p>of 0.30, a moderate relationship; and a correlation of 0.50, a large rela-
</p>
<p>tionship.1 On this yardstick, the relationship observed in our example is
</p>
<p>a very strong one.
</p>
<p>A Substantive Example: Crime and Unemployment in California
</p>
<p>W orking It Out
</p>
<p> � 0.7748
</p>
<p> � 
125.1332
</p>
<p>161.5001
</p>
<p> � 
125.1332
</p>
<p>�(187.7333)(138.9326)
</p>
<p> Pearson&rsquo;s r � 
�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>���
N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2���N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2�
</p>
<p>We are now ready to calculate Pearson&rsquo;s r for our example. We sim-
</p>
<p>ply take the covariation of 125.1332 and divide it by 161.5001, to get
</p>
<p>0.7748:
</p>
<p>1See Jacob Cohen, Statistical Power Analysis for the Behavioral Sciences (Hillsdale, NJ:
</p>
<p>statisticians develop standardized estimates of &ldquo;effect size.&rdquo;
</p>
<p>An area of study that has received extensive attention from criminologists
</p>
<p>is the relationship between crime rates and other social or economic
</p>
<p>indicators, such as unemployment. An example of such data is provided
</p>
<p>in Table 14.8, which presents the burglary rate and the unemployment
</p>
<p>Lawrence Erlbaum, 1988), pp. 79&ndash;80. In Chapter 21, we discuss in greater detail how
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 407</p>
<p/>
</div>
<div class="page"><p/>
<p>Unemployment Rate and Burglary Rate 
for 58 California Counties in 1999
</p>
<p>UNEMPLOYMENT RATE BURGLARY RATE (PER 100,000)
</p>
<p>COUNTY (X1) (X2)
</p>
<p>Alameda 3.5 837.89
Alpine 9.1 2,037.49
Amador 4.6 818.55
Butte 6.8 865.04
Calaveras 6.9 989.76
Colusa 15.9 520.06
Contra Costa 3.0 664.73
Del Norte 8.0 1,200.91
El Dorado 3.9 509.87
Fresno 13.4 924.10
Glenn 11.2 845.29
Humboldt 6.4 1,027.79
Imperial 23.4 1,526.40
Inyo 5.7 511.12
Kern 11.4 960.18
Kings 13.1 649.22
Lake 7.7 1,333.21
Lassen 7.0 361.24
Los Angeles 5.9 610.28
Madera 11.5 929.32
Marin 1.9 526.98
Mariposa 7.4 775.92
Mendocino 6.7 843.92
Merced 13.3 1,214.69
Modoc 8.5 325.08
Mono 6.7 957.95
Monterey 9.6 570.14
Napa 3.3 477.54
Nevada 4.1 455.37
Orange 2.6 464.52
Placer 3.2 646.12
Plumas 9.0 1,030.58
Riverside 5.4 1,049.18
Sacramento 4.2 925.61
San Benito 8.0 845.75
San Bernadino 4.8 883.02
San Diego 3.1 539.82
San Francisco 3.0 744.81
San Joaquin 8.8 896.85
San Luis Obispo 3.2 540.79
San Mateo 2.0 355.82
Santa Barbara 3.9 444.07
Santa Clara 3.0 347.57
Santa Cruz 6.3 647.73
Shasta 7.0 823.95
Sierra 9.2 699.71
Siskiyou 10.3 575.09
Solano 4.6 769.30
Sonoma 2.7 555.44
Stanislaus 10.5 1,057.99
Sutter 13.0 859.11
Tehama 6.7 816.55
Trinity 11.5 676.23
Tulare 16.5 1,047.32
Tuolumne 6.5 908.79
Ventura 4.8 491.86
Yolo 4.3 591.28
Yuba 11.6 1,366.76
</p>
<p>Table 14.8</p>
<p/>
</div>
<div class="page"><p/>
<p>Calculations for the Correlation of Unemployment Rate (X1) 
and Burglary Rate (X2) for 58 California Counties
</p>
<p>UNEMPLOYMENT RATE BURGLARY RATE
</p>
<p>X1i X2i
</p>
<p>3.5 15.2639 837.89 2,208.9342 �183.6216
9.1 2.8666 2,037.49 1,554,009.8148 2,110.6173
4.6 7.8787 818.55 765.0369 �77.6369
6.8 0.3683 865.04 5,498.1187 �45.0012
6.9 0.2569 989.76 39,548.9985 �100.8068
</p>
<p>15.9 72.1327 520.06 73,349.2681 �2,300.1922
3.0 19.4208 664.73 15,916.5222 555.9776
8.0 0.3518 1,200.91 168,115.8264 243.1824
3.9 12.2983 509.87 78,972.6338 985.5115
</p>
<p>13.4 35.9172 924.10 17,744.7176 798.3367
11.2 14.3876 845.29 2,959.2838 206.3420
</p>
<p>6.4 1.0138 1,027.79 56,121.2783 �238.5339
23.4 255.7792 1,526.40 540,973.9304 11,763.0738
</p>
<p>5.7 2.9135 511.12 78,271.6446 477.5406
11.4 15.9448 960.18 28,658.8671 675.9891
13.1 32.4114 649.22 20,070.5872 �806.5455
</p>
<p>7.7 0.0859 1,333.21 294,110.2232 158.9538
7.0 0.1656 361.24 184,599.7240 174.8249
5.9 2.2707 610.28 32,620.2250 272.1623
</p>
<p>11.5 16.7535 929.32 19,162.6711 566.6050
1.9 30.3259 526.98 69,648.8576 1,453.3298
7.4 0.0000 775.92 224.1219 0.1033
6.7 0.4997 843.92 2,812.1067 �37.4864
</p>
<p>13.3 34.7286 1,214.69 179,605.8467 2,497.4917
8.5 1.1949 325.08 216,979.6082 �509.1777
6.7 0.4997 957.95 27,908.8097 �118.0942
9.6 4.8097 570.14 48,730.8716 �484.1284
3.3 16.8666 477.54 98,188.6612 1,286.9000
4.1 10.9356 455.37 112,574.1401 1,109.5334
2.6 23.1063 464.52 106,517.8338 1,568.8313
3.2 17.6980 646.12 20,958.5556 609.0359
9.0 2.5380 1,030.58 57,450.9605 381.8490
5.4 4.0276 1,049.18 66,713.3625 �518.3608
4.2 10.2842 925.61 18,149.2898 �432.0313
8.0 0.3518 845.75 3,009.5428 32.5371
4.8 6.7959 883.02 8,487.8079 �240.1719
3.1 18.5494 539.82 63,036.4964 1,081.3364
3.0 19.4208 744.81 2,123.4309 203.0730
8.8 1.9407 896.85 11,227.3733 147.6119
3.2 17.6980 540.79 62,550.3601 1,052.1486
2.0 29.2346 355.82 189,286.5140 2,352.3838
3.9 12.2983 444.07 120,284.5979 1,216.2655
3.0 19.4208 347.57 196,533.2430 1,953.6700
6.3 1.2252 647.73 20,494.9860 158.4646
7.0 0.1656 823.95 1,092.9173 �13.4518
9.2 3.2152 699.71 8,313.9201 �163.4961
</p>
<p>10.3 8.3700 575.09 46,569.9421 �624.3330
4.6 7.8787 769.30 466.1583 60.6029
2.7 22.1549 555.44 55,437.0321 1,108.2429
</p>
<p>10.5 9.5673 1,057.99 71,342.0361 826.1648
13.0 31.2828 859.11 4,653.8729 381.5574
</p>
<p>(X1i � X1)(X 2i � X 2)(X2i � X2)
2(X1i � X1)
</p>
<p>2
</p>
<p>Table 14.9
</p>
<p>(continued on next page)
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 409</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 14.9 presents the detailed calculations for the unemployment
</p>
<p>and burglary data from California. The sum of the covariations is
</p>
<p>37,128.9297, the sum of the squared deviations for the unemployment
</p>
<p>rate is 1,010.3570, and the sum of the squared deviations for the bur-
</p>
<p>glary rate is 5,659,402.5114. After inserting these values into Equation
</p>
<p>14.2, we find that r � 0.4910. The positive correlation between the un-
</p>
<p>employment rate and the burglary rate means that counties with higher
</p>
<p>rates of unemployment also tended to have higher rates of burglary,
</p>
<p>while counties with lower rates of unemployment tended to have lower
</p>
<p>rates of burglary.
</p>
<p>W orking It Out
</p>
<p> � 0.4910
</p>
<p> � 
37,128,9297
</p>
<p>�(1,010.3570)(5,659,402.5114)
</p>
<p> r � 
�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>�� �
N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2��  �
</p>
<p>N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2�
</p>
<p>Calculations for the Correlation of Unemployment Rate (X1) 
and Burglary Rate (X2) for 58 California Counties (Continued)
</p>
<p>UNEMPLOYMENT RATE BURGLARY RATE
</p>
<p>X1i X2i
</p>
<p>6.7 0.4997 816.55 658.3997 �18.1386
11.5 16.7535 676.23 13,147.0761 �469.3177
16.5 82.6845 1,047.32 65,755.9859 2,331.7373
</p>
<p>6.5 0.8225 908.79 13,900.2449 �106.9229
4.8 6.7959 491.86 89,419.3595 779.5431
4.3 9.6528 591.28 39,844.4316 620.1705
</p>
<p>11.6 17.5821 1,366.76 331,625.4507 2,414.6776
</p>
<p>X2 � 790.8907X1 � 7.4069
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> � 37,128.9297�
N
</p>
<p>i�1
</p>
<p> � 5,659,402.5114�
N
</p>
<p>i�1
</p>
<p> � 1,010.3570
</p>
<p>(X1i � X1)(X 2i � X 2)(X2i � X2)
2(X1i � X1)
</p>
<p>2
</p>
<p>Table 14.9
</p>
<p>rate for all 58 counties in California in 1999. The burglary rate represents 
</p>
<p>the number of burglaries per 100,000 population, and the unemployment
</p>
<p>rate represents the percentage of persons actively looking for work who
</p>
<p>have not been able to find employment.
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A410</p>
<p/>
</div>
<div class="page"><p/>
<p>Nonlinear Relationships and Pearson&rsquo;s r
</p>
<p>Pearson&rsquo;s r allows us to assess the correlation between two interval-level
</p>
<p>measures, taking into account the full amount of information that these
</p>
<p>measures provide. However, it assesses the strength of only a linear re-
</p>
<p>lationship. If the correlation between two variables is not linear, then
</p>
<p>Pearson&rsquo;s r will give very misleading results.
</p>
<p>A simple way to see this is to look at scatterplots, or scatter dia-
</p>
<p>grams, representing different types of relationships. A scatterplot posi-
</p>
<p>tions subjects according to their scores on both variables being examined.
</p>
<p>Figure 14.1 represents the subjects in our example concerning age and
</p>
<p>number of arrests. The first case (age � 14, arrests � 0) is represented
</p>
<p>by the dot with a 1 next to it. The overall relationship in this example is
</p>
<p>basically linear and positive. That is, the dots move together in a positive
</p>
<p>direction (as age increases, so too do arrests). A scatterplot of the data in
</p>
<p>Table 14.5, where the highest number of arrests is found among younger
</p>
<p>rather than older subjects, is presented in Figure 14.2. In this case, the
</p>
<p>scatterplot shows a negative relationship (as age increases, number of ar-
</p>
<p>rests decreases).
</p>
<p>But what would happen if there were a curvilinear relationship be-
</p>
<p>tween age and number of arrests? That is, what if the number of arrests
</p>
<p>for both younger and older subjects was high, and the number for those
</p>
<p>of average age was low? This relationship is illustrated in Figure 14.3. In
</p>
<p>Scatterplot Showing a Positive Relationship Between Age 
</p>
<p>and Number of Arrests for 15 Subjects
Figure 14.1
</p>
<p>0
</p>
<p>12
</p>
<p>10
</p>
<p>8
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>12 14 16 18 20 22 24
</p>
<p>Age
</p>
<p>N
u
</p>
<p>m
b
</p>
<p>er
 o
</p>
<p>f 
A
</p>
<p>rr
es
</p>
<p>ts
</p>
<p>1
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 411</p>
<p/>
</div>
<div class="page"><p/>
<p>Scatterplot Showing a Negative Relationship 
</p>
<p>Between Age and Number of Arrests for 15 Subjects
Figure 14.2
</p>
<p>0
</p>
<p>12
</p>
<p>10
</p>
<p>8
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>12 14 16 18 20 22 24
</p>
<p>Age
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>A
rr
</p>
<p>es
ts
</p>
<p>Scatterplot Showing a Curvilinear Relationship 
</p>
<p>Between Age and Number of Arrests for 15 Subjects
Figure 14.3
</p>
<p>0
</p>
<p>12
</p>
<p>10
</p>
<p>8
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>12 14 16 18 20 22 24
</p>
<p>Age
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>A
rr
</p>
<p>es
ts
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A412</p>
<p/>
</div>
<div class="page"><p/>
<p>this scatterplot, the relationship is clear: The number of arrests declines
</p>
<p>until age 17 and then increases. However, Pearson&rsquo;s r for these data is
</p>
<p>close to 0. If there is a relationship, why does this happen? Table 14.10
</p>
<p>shows why. Subjects who are either much above or much below the
</p>
<p>mean for age have large numbers of arrests. The covariance for these
</p>
<p>subjects is accordingly very high. However, for those below the mean in
</p>
<p>age, the covariance is negative, and for those above the mean, the co-
</p>
<p>variance is positive. If we add these scores together, they cancel each
</p>
<p>other out. As a result, Pearson&rsquo;s r for this example is close to 0.
</p>
<p>W orking It Out
</p>
<p> � 0.0066
</p>
<p> � 
0.8668
</p>
<p>�(123.7338)(138.9326)
</p>
<p> Pearson&rsquo;s r � 
�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>���
N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2���N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2�
</p>
<p>Curvilinear Relatonship: Calculations for the Correlation 
of Number of Arrests (X1) and Age (X2) for 15 Young Offenders
</p>
<p>NUMBER OF ARRESTS AGE
</p>
<p>SUBJECT X1 X2
</p>
<p>1 9 1.8667 3.4846 14 �3.0667 9.4046 �5.7246
2 11 3.8667 14.9514 13 �4.0667 16.5380 �15.7247
3 6 �1.1333 1.2844 15 �2.0667 4.2712 2.3422
4 10 2.8667 8.2180 13 �4.0667 16.5380 �11.6580
5 8 0.8667 0.7512 14 �3.0667 9.4046 �2.6579
6 7 �0.1333 0.0178 14 �3.0667 9.4046 0.4088
7 2 �5.1333 26.3508 17 �0.0667 0.0044 0.3424
8 5 �2.1333 4.5510 19 1.9333 3.7376 �4.1243
9 9 1.8667 3.4846 21 3.9333 15.4708 7.3423
</p>
<p>10 6 �1.1333 1.2844 19 1.9333 3.7376 �2.1910
11 2 �5.1333 26.3508 16 �1.0667 1.1378 5.4757
12 4 �3.1333 9.8176 18 0.9333 0.8710 �2.9243
13 7 �0.1333 0.0178 20 2.9333 8.6042 �0.3910
14 10 2.8667 8.2180 21 3.9333 15.4708 11.2756
15 11 3.8667 14.9514 22 4.9333 24.3374 19.0756
</p>
<p>X2 � 17.0667X1 � 7.1333
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)� 0.8668�
N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2 � 138.9326�
</p>
<p>N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2 � 123.7338
</p>
<p>(X1i � X1)(X 2i � X 2)(X2i � X2)
2X 2i � X 2(X1i � X1)
</p>
<p>2X1i � X1
</p>
<p>Table 14.10
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 413</p>
<p/>
</div>
<div class="page"><p/>
<p>Pearson&rsquo;s r will provide a good estimate of correlation when the
</p>
<p>relationship between two variables is approximately linear. However,
</p>
<p>a strong nonlinear relationship will lead to a misleading correlation
</p>
<p>coefficient. Figure 14.4 provides examples of a number of nonlinear
</p>
<p>relationships. These examples illustrate why it is important to look at
</p>
<p>the scatterplot of the relationship between two interval-level measures
</p>
<p>to establish that it is linear before calculating Pearson&rsquo;s correlation
</p>
<p>coefficient. Linear relationships are much more common in criminal
</p>
<p>justice than nonlinear ones. But you would not want to conclude,
</p>
<p>based on r, that there was a very small relationship between two vari-
</p>
<p>ables when in fact there was a very strong nonlinear correlation be-
</p>
<p>tween them.
</p>
<p>What can you do if the relationship is nonlinear? Sometimes the so-
</p>
<p>lution is simply to break up the distribution of scores. For example,
</p>
<p>Figure 14.3 shows a nonlinear relationship that results in an r of 0.007.
</p>
<p>If we break this distribution at the point where it changes direction,
</p>
<p>we can calculate two separate Pearson&rsquo;s correlation coefficients, 
</p>
<p>each for a linear relationship. The first would provide an estimate 
</p>
<p>of the relationship for younger offenders (which is negative), and the
</p>
<p>second an estimate of the relationship for older offenders (which is
</p>
<p>positive).
</p>
<p>For some nonlinear relationships, you may want to consider using al-
</p>
<p>ternative statistics. For example, it may be worthwhile to break up your
</p>
<p>Examples of Nonlinear RelationshipsFigure 14.4
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A414</p>
<p/>
</div>
<div class="page"><p/>
<p>sample into a number of groups, or categories, and then look at the
</p>
<p>means for each. In some cases, it may be possible to change the form of
</p>
<p>the variables and, in doing so, increase the linearity of the relationship
</p>
<p>being examined. Although such transformations are beyond the scope of
</p>
<p>this text, you should be aware that they provide one solution to prob-
</p>
<p>lems of nonlinearity.2
</p>
<p>Beware of Outliers
</p>
<p>For Pearson&rsquo;s r, as for other statistics based on deviations from the mean,
</p>
<p>outliers can have a strong impact on results. For example, suppose we
</p>
<p>add to our study of age and number of arrests (from Table 14.1) one
</p>
<p>subject who was very young (12) but nonetheless had an extremely large
</p>
<p>number of arrests over the last year (25), as shown in the scatterplot in
</p>
<p>Figure 14.5. If we take the covariation for this one relationship (see sub-
</p>
<p>2For a discussion of this issue, see. J. Fox, Linear Statistical Models and Related Meth-
</p>
<p>ods (New York: Wiley, 1994).
</p>
<p>Scatterplot Showing the Relationship Between Age and Number of Arrests for 16 Subjects,
</p>
<p>Including an Outlier
Figure 14.5
</p>
<p>0
</p>
<p>30
</p>
<p>20
</p>
<p>10
</p>
<p>10 12 14 16 18 20 22 24
</p>
<p>Age
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>A
rr
</p>
<p>es
ts
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 415</p>
<p/>
</div>
<div class="page"><p/>
<p>ject 16 in Table 14.11), we see that it is very large relative to that of other
</p>
<p>subjects in our analysis. Because it is negative, it cancels out the positive
</p>
<p>covariation produced by the other subjects in the sample. Indeed, with
</p>
<p>this subject included, the correlation decreases from 0.77 to 0.10.
</p>
<p>Calculations for the Correlation of Number 
of Arrests (X1) and Age (X2) for 16 Young Offenders
</p>
<p>NUMBER OF ARRESTS AGE
</p>
<p>SUBJECT X1 X2
</p>
<p>1 0 �6.1250 37.5156 14 �2.7500 7.5625 16.84375
</p>
<p>2 1 �5.1250 26.2656 13 �3.7500 14.0625 19.21875
</p>
<p>3 1 �5.1250 26.2656 15 �1.7500 3.0625 8.96875
</p>
<p>4 2 �4.1250 17.0156 13 �3.7500 14.0625 15.46875
</p>
<p>5 2 �4.1250 17.0156 14 �2.7500 7.5625 11.34375
</p>
<p>6 3 �3.1250 9.7656 14 �2.7500 7.5625 8.59375
</p>
<p>7 3 �3.1250 9.7656 17 0.2500 0.0625 �0.78125
</p>
<p>8 4 �2.1250 4.5156 19 2.2500 5.0625 �4.78125
</p>
<p>9 4 �2.1250 4.5156 21 4.2500 18.0625 �9.03125
</p>
<p>10 6 �0.1250 0.0156 19 2.2500 5.0625 �0.28125
</p>
<p>11 8 1.8750 3.5156 16 �0.7500 0.5625 �1.40625
</p>
<p>12 9 2.8750 8.2656 18 1.2500 1.5625 3.59375
</p>
<p>13 9 2.8750 8.2656 20 3.2500 10.5625 9.34375
</p>
<p>14 10 3.8750 15.0156 21 4.2500 18.0625 16.46875
</p>
<p>15 11 4.8750 23.7656 22 5.2500 27.5625 25.59375
</p>
<p>16 25 18.8750 356.2656 12 �4.7500 22.5625 �89.65625
</p>
<p>� 567.7496 � 163.000 � 29.5000
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)�
N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2X2 � 16.7500�
</p>
<p>N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2X1 � 6.1250
</p>
<p>(X1i � X1)(X 2i � X 2)(X2i � X2)
2X 2i � X 2(X1i � X1)
</p>
<p>2X1i � X1
</p>
<p>Table 14.11
</p>
<p>W orking It Out
</p>
<p> � 0.0970
</p>
<p> � 
29.5000
</p>
<p>�(567.7496)(163)
</p>
<p> Pearson&rsquo;s r � 
�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>���
N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2���N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2�
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A416</p>
<p/>
</div>
<div class="page"><p/>
<p>What should you do when faced with outliers? When you have just a
</p>
<p>few deviant cases in your sample, the best decision may be to exclude
</p>
<p>them from your analysis. If you take this approach, it is important to
</p>
<p>clearly state that certain cases have been excluded and to explain why.
</p>
<p>Before excluding outliers, however, you should compare the correlations
</p>
<p>with and without them. When samples are large, deviant cases may have
</p>
<p>a relatively small impact; thus, including them may not lead to mislead-
</p>
<p>ing results.
</p>
<p>When there are a relatively large number of outliers that follow the
</p>
<p>general pattern of relationships in your data, it may be better to choose
</p>
<p>an alternative correlation coefficient rather than exclude such cases. For
</p>
<p>example, suppose we add to our study of age and arrests three different
</p>
<p>subjects for whom the relationships are similar to those noted previously,
</p>
<p>but the number of arrests and the average age are much higher (see sub-
</p>
<p>jects 16, 17, and 18 in Table 14.12). These data are depicted in the scat-
</p>
<p>Calculations for the Correlation of Number 
of Arrests (X1) and Age (X2) for 18 Young Offenders
</p>
<p>NUMBER OF ARRESTS AGE
</p>
<p>SUBJECT X1 X2
</p>
<p>1 0 �11.2778 127.1888 14 �8.5556 73.1983 96.4883
</p>
<p>2 1 �10.2778 105.6332 13 �9.5556 91.3095 98.2105
</p>
<p>3 1 �10.2778 105.6332 15 �7.5556 57.0871 77.6549
</p>
<p>4 2 �9.2778 86.0776 13 �9.5556 91.3095 88.6549
</p>
<p>5 2 �9.2778 86.0776 14 �8.5556 73.1983 79.3771
</p>
<p>6 3 �8.2778 68.5220 14 �8.5556 73.1983 70.8215
</p>
<p>7 3 �8.2778 68.5220 17 �5.5556 30,8647 45.9881
</p>
<p>8 4 �7.2778 52.9664 19 �3.5556 12.6423 25.8769
</p>
<p>9 4 �7.2778 52.9664 21 �1.5556 2.4199 11.3213
</p>
<p>10 6 �5.2778 27.8552 19 �3.5556 12.6423 18.7675
</p>
<p>11 8 �3.2778 10.7440 16 �6.556 42.9759 21.4879
</p>
<p>12 9 �2.2778 5.1884 18 �4.5556 20.7535 10.3767
</p>
<p>13 9 �2.2778 5.1884 20 �2.5556 6.5311 5.8211
</p>
<p>14 10 �1.2778 1.6328 21 �1.5556 2.4199 1.9877
</p>
<p>15 11 �0.2778 0.0772 22 �0.5556 0.3087 0.1543
</p>
<p>16 36 24.7222 611.1872 40 17.4444 304.3071 431.2639
</p>
<p>17 40 28.7222 824.9648 50 27.4444 753.1951 788.2635
</p>
<p>18 54 42.7222 1,825.1864 60 37.4444 1,402.0831 1,5999.7071
</p>
<p>� 4,065.6116 � 3,050.4446 � 3,472.2214
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)�
N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2X2 � 22.5556�
</p>
<p>N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2X1 � 11.2778
</p>
<p>(X1i � X1)(X 2i � X 2)(X2i � X2)
2X 2i � X 2(X1i � X1)
</p>
<p>2X1i � X1
</p>
<p>Table 14.12
</p>
<p>P E A R S O N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 417</p>
<p/>
</div>
<div class="page"><p/>
<p>terplot in Figure 14.6. In such situations, Pearson&rsquo;s r is likely to give a
</p>
<p>misleading view of the relationship between the two variables. For our
</p>
<p>example, the correlation changes from 0.77 to 0.99.
</p>
<p>Scatterplot Showing the Relationship Between Age and Number of Arrests 
</p>
<p>for 18 Subjects, Including Three Outliers Who Follow the General Pattern
</p>
<p>Figure 14.6
</p>
<p>0
</p>
<p>60
</p>
<p>40
</p>
<p>20
</p>
<p>50
</p>
<p>30
</p>
<p>10
</p>
<p>10 20 30 40 50 60 70
</p>
<p>Age
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>A
rr
</p>
<p>es
ts
</p>
<p>W orking It Out
</p>
<p> � 0.9859
</p>
<p> � 
3,472.2214
</p>
<p>�(4,065.6116)(3,050.4446)
</p>
<p> Pearson&rsquo;s r � 
�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>���
N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2���N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2�
</p>
<p>In such situations, you may want to use a rank-order correlation coef-
</p>
<p>ficient called Spearman&rsquo;s r. Pearson&rsquo;s r is generally more appropriate
</p>
<p>for interval-level data. However, where a number of outliers are found in
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A418</p>
<p/>
</div>
<div class="page"><p/>
<p>the distribution, Spearman&rsquo;s correlation coefficient can provide a use-
</p>
<p>ful alternative.
</p>
<p>S p e a r m a n &rsquo; s  C o r r e l a t i o n  C o e f f i c i e n t
</p>
<p>Spearman&rsquo;s r compares the rank order of subjects on each measure
</p>
<p>rather than the relative position of each subject to the mean. Like Pear-
</p>
<p>son&rsquo;s r, its range of possible values is between �1 and �1. It is calcu-
</p>
<p>lated using Equation 14.3.
</p>
<p>Equation 14.3
</p>
<p>Let&rsquo;s calculate rs for our original example with 15 cases (from Table
</p>
<p>14.1), and for the example with the additional three outliers (from Table
</p>
<p>14.12). To carry out the calculation, we must first rank order the cases, as
</p>
<p>rs � 1 � 
</p>
<p>6�
N
</p>
<p>i�1
</p>
<p>D 2i
</p>
<p>N  (N 2 � 1)
</p>
<p>Calculation of Difference in Rank (D) 
for Spearman&rsquo;s r for 15 Young Offenders
</p>
<p>NUMBER OF RANK ARRESTS RANK AGE D
</p>
<p>SUBJECT ARRESTS Rk1 AGE Rk2 (Rk1 � Rk2) D
2
</p>
<p>1 0 1 14 4 �3 9
</p>
<p>2 1 2.5 13 1.5 1 1
</p>
<p>3 1 2.5 15 6 �3.5 12.25
</p>
<p>4 2 4.5 13 1.5 3 9
</p>
<p>5 2 4.5 14 4 0.5 0.25
</p>
<p>6 3 6.5 14 4 2.5 6.25
</p>
<p>7 3 6.5 17 8 �1.5 2.25
</p>
<p>8 4 8.5 19 10.5 �2 4
</p>
<p>9 4 8.5 21 13.5 �5 25
</p>
<p>10 6 10 19 10.5 �0.5 0.25
</p>
<p>11 8 11 16 7 4 16
</p>
<p>12 9 12.5 18 9 3.5 12.25
</p>
<p>13 9 12.5 20 12 0.5 0.25
</p>
<p>14 10 14 21 13.5 0.5 0.25
</p>
<p>15 11 15 22 15 0 0
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>D 2i  � 98X2 � 17.0667X1 � 4.8667
</p>
<p>Table 14.13
</p>
<p>S P E A R M A N &rsquo; S C O R R E L A T I O N C O E F F I C I E N T 419</p>
<p/>
</div>
<div class="page"><p/>
<p>shown in Tables 14.13 and 14.14. We then take the squared difference in
</p>
<p>ranks for each subject on the two measures and sum it across all the
</p>
<p>cases in our example. This value is multiplied by 6, and then divided by
</p>
<p>N(N 2 � 1). The final figure is then subtracted from 1.
</p>
<p>Calculation of Difference in Rank (D) 
for Spearman&rsquo;s r for 18 Young Offenders
</p>
<p>NUMBER OF RANK ARRESTS RANK AGE D
</p>
<p>SUBJECT ARRESTS Rk1 AGE Rk2 (Rk1 � Rk2) D
2
</p>
<p>1 0 1 14 4 �3 9
</p>
<p>2 1 2.5 13 1.5 1 1
</p>
<p>3 1 2.5 15 6 �3.5 12.25
</p>
<p>4 2 4.5 13 1.5 3 9
</p>
<p>5 2 4.5 14 4 0.5 0.25
</p>
<p>6 3 6.5 14 4 2.5 6.25
</p>
<p>7 3 6.5 17 8 �1.5 2.25
</p>
<p>8 4 8.5 19 10.5 �2 4
</p>
<p>9 4 8.5 21 13.5 �5 25
</p>
<p>10 6 10 19 10.5 �0.5 0.25
</p>
<p>11 8 11 16 7 4 16
</p>
<p>12 9 12.5 18 9 3.5 12.25
</p>
<p>13 9 12.5 20 12 0.5 0.25
</p>
<p>14 10 14 21 13.5 0.5 0.25
</p>
<p>15 11 15 22 15 0 0
</p>
<p>16 36 16 40 16 0 0
</p>
<p>17 40 17 50 17 0 0
</p>
<p>18 54 18 60 18 0 0
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>D 2i  � 98X2 � 22.5556X1 � 11.2778
</p>
<p>Table 14.14
</p>
<p>W orking It Out No Outliers
</p>
<p> � 0.8250
</p>
<p> � 1 � 0.1750
</p>
<p> � 1 � 
588
</p>
<p>3360
</p>
<p> � 1 � 
(6)(98)
</p>
<p>(15)(224)
</p>
<p>rs  � 1 � 
</p>
<p>6�
N
</p>
<p>i�1
</p>
<p>D 2i
</p>
<p>N(N 2 � 1)
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A420</p>
<p/>
</div>
<div class="page"><p/>
<p>The correlation coefficients for our two distributions (without outliers
</p>
<p>and with outliers) are similar. In the case without the outliers, rs � 0.83;
</p>
<p>in the case with them, rs � 0.90. The outliers here do not have as much
</p>
<p>of an impact because Spearman&rsquo;s correlation coefficient does not take
</p>
<p>into account the actual values of the scores, but only their ranks in the
</p>
<p>distribution. Note that the correlation coefficient obtained here for the 15
</p>
<p>cases, rs � 0.83, is a bit larger than, although similar to, r � 0.77. Which
</p>
<p>is the better estimate of the correlation between these two variables? In
</p>
<p>the case without the outliers, Pearson&rsquo;s r would be preferred because it
</p>
<p>takes into account more information (order as well as value). In the case
</p>
<p>with the outliers, however, Spearman&rsquo;s r would be preferred because it
</p>
<p>is not affected by the extreme values of the three outliers, but only by
</p>
<p>their relative positions in the distributions.
</p>
<p>T e s t i n g  t h e  S t a t i s t i c a l  S i g n i f i c a n c e  o f  P e a r s o n &rsquo; s  r
</p>
<p>As in Chapter 13, our emphasis in this chapter has been not on statistical
</p>
<p>inference but rather on statistical description. Our concern has been to
</p>
<p>describe the strength or nature of the relationship between two interval-
</p>
<p>level variables. Nonetheless, it is important here, as before, to define
</p>
<p>whether the differences observed in our samples can be inferred to the
</p>
<p>populations from which they were drawn.
</p>
<p>Statistical Significance of r : The Case of Age and Number of Arrests
</p>
<p>We can use the t-distribution introduced in Chapter 10 to test for the sig-
</p>
<p>nificance of Pearson&rsquo;s r. We begin by conducting a test of statistical sig-
</p>
<p>nificance for our example of the correlation between age and number of
</p>
<p>arrests.
</p>
<p>W orking It Out With Outliers
</p>
<p> � 0.8989
</p>
<p> � 1 � 0.1011
</p>
<p> � 1 � 
588
5814
</p>
<p> � 1 � 
(6)(98)
</p>
<p>(18)(323)
</p>
<p> rs � 1 � 
</p>
<p>6�
N
</p>
<p>i�1
</p>
<p>D 2i
</p>
<p>N (N 2 � 1)
</p>
<p>S T A T I S T I C A L S I G N I F I C A N C E O F P E A R S O N &rsquo; S r 421</p>
<p/>
</div>
<div class="page"><p/>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution of Y around each value of
</p>
<p>X (must be assumed because N is not large).
</p>
<p>Homoscedasticity.
</p>
<p>Linearity.
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: Youth in one U.S. city.
</p>
<p>Hypotheses:
</p>
<p>H0: There is no linear relationship between age and number of arrests in
</p>
<p>the population of young offenders (rp � 0).
</p>
<p>H1: There is a linear relationship between age and number of arrests in
</p>
<p>the population of young offenders (rp � 0).
</p>
<p>The t-test for Pearson&rsquo;s r assumes that the variables examined are
</p>
<p>measured on an interval scale. In practice, researchers sometimes use
</p>
<p>ordinal-scale measures for calculating these coefficients, particularly
</p>
<p>when an interval-level measure is related to an ordinal-scale variable.
</p>
<p>There is no simple answer to the question of which statistic is most ap-
</p>
<p>propriate in such cases, and Pearson&rsquo;s r is often considered a good solu-
</p>
<p>tion. Nonetheless, you should keep in mind that Pearson&rsquo;s r, like other
</p>
<p>statistics that require an interval level of measurement, assumes that the
</p>
<p>categories are not only ranked but also equal to one another. When
</p>
<p>there are clear differences between the categories, the meaning of r be-
</p>
<p>comes ambiguous. For example, suppose you were interested in the re-
</p>
<p>lationship between amount stolen in robberies and age, where amount
</p>
<p>stolen in robberies was measured on an ordinal scale, with the first cat-
</p>
<p>egory as $1&ndash;50, the second as $51&ndash;200, and subsequent intervals also of
</p>
<p>unequal size. If the real relationship between amount stolen and age
</p>
<p>was truly linear, with every year of age related to a measured increase
</p>
<p>in amount stolen, you would likely get a misleading correlation coeffi-
</p>
<p>cient. In this case, an interval-scale measurement would allow you to
</p>
<p>represent the linear relationship between amount stolen and age. The
</p>
<p>ordinal scale we have described might mask or misrepresent that rela-
</p>
<p>tionship. In practice, you should also be wary of using Pearson&rsquo;s corre-
</p>
<p>lation coefficient when the number of categories is small (for example,
</p>
<p>less than 5). While r is sometimes used when the researcher wants to
</p>
<p>represent the relationship between an ordinal-scale measure and an in-
</p>
<p>terval-level variable, it should not be used to define the relationship be-
</p>
<p>tween two ordinal-level measures or when nominal-scale measurement
</p>
<p>is involved. As noted in earlier chapters, other statistics are more appro-
</p>
<p>priate for measuring such relationships.
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A422</p>
<p/>
</div>
<div class="page"><p/>
<p>Because our test of the statistical significance of r is a parametric test
</p>
<p>of significance, we must also make assumptions regarding the popula-
</p>
<p>tion distribution. For tests of statistical significance with r, we must
</p>
<p>assume a normal distribution. However, in this case, it is useful to think
</p>
<p>of this distribution in terms of the joint distribution of scores between 
</p>
<p>X1 and X2. Assume that the scatterplot in Figure 14.7 represents the
</p>
<p>relationship between age and number of arrests for ages 16&ndash;19 for the
</p>
<p>population of scores. The relationship, as in our sample, is linear. Notice
</p>
<p>how the points in the scatterplot are distributed. Suppose we put an
</p>
<p>imaginary line through the scatter of points (shown as a real line in Fig-
</p>
<p>ure 14.7). There is a clustering of points close to the line, in the middle
</p>
<p>Scatterplot Showing Normal Distribution and HomoscedasticityFigure 14.7
</p>
<p>16 17 18 19
</p>
<p>Age
</p>
<p>A
</p>
<p>Cross Sections:
</p>
<p>Arrests
</p>
<p>Age = 16
A
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>A
rr
</p>
<p>es
ts
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>1 case
5 cases
10 cases
25 cases
50 cases
100 cases
</p>
<p>B
</p>
<p>C
</p>
<p>D
</p>
<p>Arrests
</p>
<p>Age = 17
B
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Arrests
</p>
<p>Age = 18
C
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Arrests
</p>
<p>Age = 19
D
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>S T A T I S T I C A L S I G N I F I C A N C E O F P E A R S O N &rsquo; S r 423</p>
<p/>
</div>
<div class="page"><p/>
<p>of the distribution. As we move away from the center of the distribution
</p>
<p>of number of arrests for each age (represented by the imaginary line),
</p>
<p>there are fewer points. The distribution of number of arrests for each
</p>
<p>value of age is basically normal in form, as illustrated in the cross section
</p>
<p>for each of the four ages examined. This imaginary population distribu-
</p>
<p>tion meets the normality assumption of our test.
</p>
<p>One problem in drawing conclusions about our assumptions is that
</p>
<p>they relate to the population and not to the sample. Because the popula-
</p>
<p>tion distribution is usually unknown, we generally cannot come to solid
</p>
<p>conclusions regarding our assumptions about the population. In the case
</p>
<p>of an assumption of normality, the researcher is most often aided by the
</p>
<p>central limit theorem. When the number of cases in a sample is greater
</p>
<p>than 30, the central limit theorem can be safely invoked. For our exam-
</p>
<p>ple, we cannot invoke the central limit theorem. Accordingly, our test re-
</p>
<p>sults cannot be relied on unless the assumption of a normal distribution
</p>
<p>is true for the population to which we infer.
</p>
<p>For our t-test, we must also assume that the variances of the joint dis-
</p>
<p>tribution of scores are equal. In our example, this means that the spread
</p>
<p>of number of arrests around each value of age should be about the
</p>
<p>same. This is the assumption of homoscedasticity. To visualize this as-
</p>
<p>sumption, it is useful to look again at the scatterplot in Figure 14.7.
</p>
<p>We can see that for each age examined, the variance in the distribu-
</p>
<p>tion of scores for number of arrests is about equal. That is, the spread of
</p>
<p>the scores around our imaginary line for each value of age in this popu-
</p>
<p>lation distribution is about equal, whether we look at the cases associ-
</p>
<p>ated with the youngest subjects (on the left side of the scatterplot), those
</p>
<p>associated with average-age subjects (in the middle of the scatterplot), or
</p>
<p>those associated with the oldest offenders (on the right side of the scat-
</p>
<p>terplot). With regard to the assumption of homoscedasticity, researchers
</p>
<p>generally use the scatterplot of sample cases as an indication of the form
</p>
<p>of the population distribution. As with analysis of variance, we are gen-
</p>
<p>erally concerned with only marked violations of the homoscedasticity as-
</p>
<p>sumption. Given the small number of cases in our sample distribution of
</p>
<p>scores, it is very difficult to examine the assumption of homoscedasticity.
</p>
<p>Nonetheless, if you look back at Figure 14.1, it seems reasonable to con-
</p>
<p>clude that there are no major violations of this assumption.
</p>
<p>What would a joint distribution of the relationship between arrests
</p>
<p>and age look like if both the normality and the homoscedasticity as-
</p>
<p>the points for each age category are not clustered in the center of the
</p>
<p>distribution of scores for number of arrests. Indeed, they form a type of
</p>
<p>bimodal distribution, with peaks above and below our imaginary line
</p>
<p>(see the cross section for each age group). Heteroscedasticity (or un-
</p>
<p>equal variances), rather than homoscedasticity, is also represented in
</p>
<p>the scatterplot in Figure 14.8. For subjects aged 17 and 19, the scores
</p>
<p>sumption were violated? In the scatterplot in Figure 14.8 (page 425),
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A424</p>
<p/>
</div>
<div class="page"><p/>
<p>are scattered widely. For subjects aged 16 and 18, however, the scores
</p>
<p>are tightly clustered around the imaginary line. If this were the popula-
</p>
<p>tion distribution of the variables under study, you would want to be very
</p>
<p>cautious in applying the t-test to your correlation coefficient.
</p>
<p>As with other tests of statistical significance, we assume independent
</p>
<p>random sampling. Pearson&rsquo;s correlation coefficient adds one new as-
</p>
<p>sumption to our test&mdash;that of linearity. Our null hypothesis is simply that
</p>
<p>there is no linear relationship between age and number of arrests, or that
</p>
<p>the population correlation coefficient (rp) is equal to 0. The research hy-
</p>
<p>pothesis is nondirectional; that is, we simply test for a linear relationship
</p>
<p>Scatterplot Showing Nonnormal Distribution and HeteroscedasticityFigure 14.8
</p>
<p>16 17 18 19
</p>
<p>Age
</p>
<p>A
</p>
<p>Cross Sections:
</p>
<p>Arrests
</p>
<p>Age = 16
A
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>A
rr
</p>
<p>es
ts
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>100 cases
50 cases
25 cases
10 cases
5 cases
1 case B C
</p>
<p>D
</p>
<p>Arrests
</p>
<p>Age = 17
B
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Arrests
</p>
<p>Age = 18
C
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Arrests
</p>
<p>Age = 19
D
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>S T A T I S T I C A L S I G N I F I C A N C E O F P E A R S O N &rsquo; S r 425</p>
<p/>
</div>
<div class="page"><p/>
<p>between age and number of arrests (i.e., rp � 0). However, we might
</p>
<p>have proposed a directional research hypothesis&mdash;for example, that
</p>
<p>there is a positive relationship between age and number of arrests in the
</p>
<p>population, or that rp � 0.
</p>
<p>The Sampling Distribution The sampling distribution is t, with N � 2 de-
</p>
<p>grees of freedom. For our example, df � 15 � 2 � 13.
</p>
<p>Significance Level and Rejection Region With a two-tailed 0.05 signifi-
</p>
<p>cance threshold, the critical value for the t-test (with 13 degrees of free-
</p>
<p>dom) is 2.160 (see Appendix 4). We will reject the null hypothesis if the
</p>
<p>t-score is greater than 2.160 or less than �2.160. In these cases, the ob-
</p>
<p>served significance level of the test will be less than the criterion signifi-
</p>
<p>cance level we have chosen.
</p>
<p>The Test Statistic It is important to note that there is more than one way
</p>
<p>to test statistical significance for r. Equation 14.4 provides a straightfor-
</p>
<p>ward estimate of t, based on our calculation of r.
</p>
<p>Equation 14.4
</p>
<p>Inserting our sample estimates, we calculate that the t-statistic for r is
</p>
<p>4.4188:
</p>
<p>t � r �N � 21 � r 2
</p>
<p>W orking It Out
</p>
<p> � 4.4188
</p>
<p> � 0.7748�32.5256
</p>
<p> � 0.7748� 15 � 21 � (0.7748)2
 t � r �N � 21 � r 2
</p>
<p>The Decision Since 4.419 is greater than our critical value of t (2.160),
</p>
<p>we reject the null hypothesis and conclude that there is a statistically sig-
</p>
<p>nificant relationship between age and number of arrests. However, be-
</p>
<p>cause we cannot strongly support the assumption of normality in this test
</p>
<p>or relax that assumption because N is large, we cannot place strong re-
</p>
<p>liance on our test result.
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A426</p>
<p/>
</div>
<div class="page"><p/>
<p>Statistical Significance of r : 
</p>
<p>Unemployment and Crime in California
</p>
<p>In our analysis of unemployment and burglary rates in California coun-
</p>
<p>ties, we found r � 0.4910. We can test the statistical significance of this
</p>
<p>result by following the same approach we used in the previous example.
</p>
<p>We start by outlining our assumptions and hypotheses.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution of Y (i.e., burglary rate)
</p>
<p>around each value of X (i.e., unemployment rate) (relaxed because N is
</p>
<p>large).
</p>
<p>Homoscedasticity.
</p>
<p>Sampling Method: Independent random sampling (the 58 counties repre-
</p>
<p>sent all counties in California in 1999).
</p>
<p>Sampling Frame: California counties.
</p>
<p>Linearity.3
</p>
<p>Hypotheses:
</p>
<p>H0: There is no linear relationship between unemployment rate and bur-
</p>
<p>glary rate (rp � 0).
</p>
<p>H1: There is a linear relationship between unemployment rate and bur-
</p>
<p>glary rate (rp � 0).
</p>
<p>might question why we would choose to conduct a statistical test of sig-
</p>
<p>nificance. Why do we need to make inferences? We already have the
</p>
<p>population of scores. One reason might be that we want to look at data
</p>
<p>for the year observed as a sample of the relationships that occur over a
</p>
<p>number of years. Similarly, we might want to use the data in California
</p>
<p>to represent other states. For either of these inferences, we would need
</p>
<p>to explain why this sample was representative of the population. An-
</p>
<p>other reason we might choose to conduct a statistical test of significance
</p>
<p>is to see whether the correlation observed is likely to be a chance occur-
</p>
<p>rence. We would expect differences across the counties simply as a
</p>
<p>product of the natural fluctuations that occur in statistics. A significance
</p>
<p>test in this case can tell us whether the relationship observed is likely to
</p>
<p>be the result of such chance fluctuations or whether it is likely to repre-
</p>
<p>sent a real relationship between the measures examined.
</p>
<p>3It is good practice to examine the sample scatterplot of scores to assess whether this
</p>
<p>assumption is likely to be violated. We find no reason to suspect a violation of the as-
</p>
<p>sumption when we examine this scatterplot (see Chapter 15, Figure 15.2).
</p>
<p>Since we have data for all counties in California for a given year, you
</p>
<p>S T A T I S T I C A L S I G N I F I C A N C E O F P E A R S O N &rsquo; S r 427</p>
<p/>
</div>
<div class="page"><p/>
<p>The Sampling Distribution The sampling distribution is t, with N � 2 de-
</p>
<p>grees of freedom. For this example, df � 58 � 2 � 56.
</p>
<p>Significance Level and Rejection Region With a two-tailed 0.05 signifi-
</p>
<p>cance level and 56 degrees of freedom, interpolation yields estimates of
</p>
<p>�2.003 and �2.003 for the critical values of the t-test.4
</p>
<p>The Test Statistic We again use Equation 14.4 to calculate a t-value to
</p>
<p>test the significance of r. Inserting the values for our data, we find the
</p>
<p>value of the t-statistic to be 4.2177:
</p>
<p>The Decision Since 4.218 is greater than our critical value of 2.003, we
</p>
<p>reject the null hypothesis and conclude that there is a statistically sig-
</p>
<p>nificant relationship between the unemployment rate and the burglary
</p>
<p>rate.
</p>
<p>T e s t i n g  t h e  S t a t i s t i c a l  S i g n i f i c a n c e  o f  S p e a r m a n &rsquo; s  r
</p>
<p>For Spearman&rsquo;s r, we use a nonparametric statistical test. With N 	 30,
</p>
<p>we use an exact probability distribution constructed for the distribution
</p>
<p>of differences between ranked pairs (see Appendix 7). For larger sam-
</p>
<p>ples, a normal approximation of this test is appropriate. It is con-
</p>
<p>structed by taking the difference between the observed value of rs and
</p>
<p>the parameter value under the null hypothesis (rs(p)). This value is then
</p>
<p>divided by 1 divided by the square root of N � 1, as shown in Equa-
</p>
<p>tion 14.5.
</p>
<p>Equation 14.5z � 
rs � rs (p)
</p>
<p>1
</p>
<p>�N � 1
</p>
<p> � 4.2177
</p>
<p> � 0.491�73.789165
</p>
<p> � 0.491� 58 � 21 � (0.491)2
</p>
<p> t � r �N � 21 � r 2
</p>
<p>4The table does not list a t-value for df � 56. We therefore interpolate from the values
</p>
<p>of df � 55 (2.004) and df � 60 (2.000).
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A428</p>
<p/>
</div>
<div class="page"><p/>
<p>Because we are examining less than 15 cases, we will use the exact
</p>
<p>probability table presented in Appendix 7.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Ordinal scale.
</p>
<p>Population Distribution: No assumption made.
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: Youth in one U.S. city.
</p>
<p>Hypotheses:
</p>
<p>H0: There is no linear relationship between the rank order of scores in
</p>
<p>the population (rs(p) � 0).
</p>
<p>H1: There is a linear relationship between the rank order of scores in the
</p>
<p>population (rs(p) � 0).
</p>
<p>The Sampling Distribution Because N is small, we use the exact probabil-
</p>
<p>ity distribution constructed for Spearman&rsquo;s r in Appendix 7.
</p>
<p>Significance Level and Critical Region As earlier, we use the conventional
</p>
<p>0.05 significance threshold. Since our research hypothesis is not direc-
</p>
<p>tional, we use a two-tailed rejection region. From Appendix 7, under a
</p>
<p>two-tailed 0.05 probability value and an N of 15, we find that an rs
greater than or equal to 0.525 or less than or equal to �0.525 is needed
</p>
<p>to reject the null hypothesis.
</p>
<p>The Test Statistic In the case of the exact probability distribution, the
</p>
<p>test statistic is simply the value of rs. As calculated earlier in this chapter
</p>
<p>s equals 0.825.
</p>
<p>The Decision Because the observed rs is larger than 0.525, we reject the
</p>
<p>null hypothesis and conclude that there is a statistically significant linear
</p>
<p>relationship between ranks of age and number of arrests in the popula-
</p>
<p>tion. The observed significance level of our test is less than the criterion
</p>
<p>significance level we set at the outset (p � 0.05).
</p>
<p>Because we use a nonparametric test, we do not need to make
</p>
<p>assumptions regarding the population distribution. The null hypothesis
</p>
<p>is the same as for the correlation coefficient r ; however, it is concerned
</p>
<p>with ranks rather than raw scores.
</p>
<p>(see page 420), r
</p>
<p>S T A T I S T I C A L S I G N I F I C A N C E O F S P E A R M A N &rsquo; S r 429</p>
<p/>
</div>
<div class="page"><p/>
<p>C h a p t e r  S u m m a r y
</p>
<p>Linear correlation coefficients describe the relationship between two
</p>
<p>interval-level measures, telling us how strongly the two are associated.
</p>
<p>Pearson&rsquo;s r is a widely used linear correlation coefficient. It examines
</p>
<p>the placement of subjects on both variables relative to the mean and
</p>
<p>estimates how strongly the scores move together or in opposite direc-
</p>
<p>tions relative to the mean. The covariation, which is the numerator of
</p>
<p>the Pearson&rsquo;s r equation, is positive when both scores vary in the
</p>
<p>same direction relative to the mean and negative when they vary in
</p>
<p>opposite directions. Dividing the covariation by the denominator of
</p>
<p>the Pearson&rsquo;s r equation serves to standardize the coefficient so that it
</p>
<p>varies between �1 and �1. Pearson&rsquo;s r will produce a misleading cor-
</p>
<p>relation coefficient if there is a nonlinear relationship between the
</p>
<p>variables.
</p>
<p>Outliers have a strong impact on Pearson&rsquo;s r. If there are several out-
</p>
<p>liers that follow the general pattern of relationships in the data, Spear-
</p>
<p>man&rsquo;s r may provide less misleading results. Spearman&rsquo;s r also varies
</p>
<p>between �1 and �1. It compares the rank order of subjects on each
</p>
<p>measure.
</p>
<p>The t distribution may be used to test significance for Pearson&rsquo;s
</p>
<p>correlation coefficient. It is assumed that the variables examined 
</p>
<p>are measured on an interval scale. There is also an assumption of
</p>
<p>normality and a requirement of homoscedasticity. These assumptions re-
</p>
<p>late to the joint distribution of X1 and X2. The researcher must also as-
</p>
<p>sume linearity. For Spearman&rsquo;s r, a nonparametric test of statistical signif-
</p>
<p>icance is used.
</p>
<p>K e y  T e r m s
</p>
<p>covariation A measure of the extent to
</p>
<p>which two variables vary together relative
</p>
<p>to their respective means. The covariation
</p>
<p>between the two variables serves as the
</p>
<p>numerator for the equation to calculate
</p>
<p>Pearson&rsquo;s r.
</p>
<p>curvilinear relationship An association
</p>
<p>between two variables whose values may
</p>
<p>be represented as a curved line when plot-
</p>
<p>ted on a scatter diagram.
</p>
<p>heteroscedasticity A situation in which
</p>
<p>the variances of scores on two or more
</p>
<p>variables are not equal. Heteroscedasticity
</p>
<p>violates one of the assumptions of the
</p>
<p>parametric test of statistical significance for
</p>
<p>the correlation coefficient.
</p>
<p>linear relationship An association be-
</p>
<p>tween two variables whose joint distribu-
</p>
<p>tion may be represented in linear form
</p>
<p>when plotted on a scatter diagram.
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A430</p>
<p/>
</div>
<div class="page"><p/>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>r Pearson&rsquo;s correlation coefficient
</p>
<p>rs Spearman&rsquo;s correlation coefficient
</p>
<p>D Difference in rank of a subject on two variables
</p>
<p>To calculate the covariation of scores for two variables:
</p>
<p>To calculate Pearson&rsquo;s correlation coefficient:
</p>
<p>To calculate Spearman&rsquo;s correlation coefficient:
</p>
<p>To test statistical significance for Pearson&rsquo;s r :
</p>
<p>To test statistical significance for Spearman&rsquo;s r where N is large:
</p>
<p>z � 
rs � rs (p)
</p>
<p>1
</p>
<p>�N � 1
</p>
<p>t � r �N � 21 � r 2
</p>
<p>rs � 1 � 
</p>
<p>6�
N
</p>
<p>i�1
</p>
<p>D 2i
</p>
<p>N (N 2 � 1)
</p>
<p>Pearson&rsquo;s r � 
�
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>���
N
</p>
<p>i�1
</p>
<p>(X1i � X1)
2���N
</p>
<p>i�1
</p>
<p>(X2i � X2)
2�
</p>
<p>Covariation of scores � �
N
</p>
<p>i�1
</p>
<p>(X1i � X1)(X2i � X2)
</p>
<p>Pearson&rsquo;s correlation coefficient See
</p>
<p>Pearson&rsquo;s r.
</p>
<p>Pearson&rsquo;s r A commonly used measure of
</p>
<p>association between two variables. Pear-
</p>
<p>son&rsquo;s r measures the strength and direction
</p>
<p>of linear relationships on a standardized
</p>
<p>scale from �1.0 to 1.0.
</p>
<p>scatter diagram See scatterplot.
</p>
<p>scatterplot A graph whose two axes are
</p>
<p>defined by two variables and upon which a
</p>
<p>point is plotted for each subject in a sam-
</p>
<p>ple according to its score on the two vari-
</p>
<p>ables.
</p>
<p>Spearman&rsquo;s correlation coefficient See
</p>
<p>Spearman&rsquo;s r.
</p>
<p>Spearman&rsquo;s r (rs) A measure of associa-
</p>
<p>tion between two rank-ordered variables.
</p>
<p>Spearman&rsquo;s r measures the strength and di-
</p>
<p>rection of linear relationships on a stan-
</p>
<p>dardized scale between �1.0 and 1.0.
</p>
<p>S Y M B O L S A N D F O R M U L A S 431</p>
<p/>
</div>
<div class="page"><p/>
<p>E x e r c i s e s
</p>
<p>14.1 A researcher draws four random samples of ten offenders, aged be-
tween 30 and 35 years, all of whom are currently serving out a term of
imprisonment and all of whom have been in prison before. For each
sample, she compares the subjects on the following pairs of variables:
</p>
<p>Sample 1 1 2 3 4 5 6 7 8 9 10
</p>
<p>X1: Number of convictions 3 5 1 7 6 2 4 9 10 8
</p>
<p>X2: Average sentence 2 2.5 0.5 3 3 1 2 4.5 5 3.5
</p>
<p>Sample 2 1 2 3 4 5 6 7 8 9 10
</p>
<p>X1: Years of education 9 12 17 16 9 14 10 17 17 9
</p>
<p>X2: Age at first offense 14 17 14 16 10 17 16 10 12 12
</p>
<p>Sample 3 1 2 3 4 5 6 7 8 9 10
</p>
<p>X1: Age at first offense 13 17 10 16 14 11 18 19 15 12
</p>
<p>X2: Number of convictions 7 3 10 4 6 9 1 1 6 8
</p>
<p>Sample 4 1 2 3 4 5 6 7 8 9 10
</p>
<p>X1: Age at first offense 11 16 18 12 15 17 13 20 20 13
</p>
<p>X2: Average sentence 3 5 1.5 1 1 4 4.5 5 3 2.5
</p>
<p>a. Calculate the mean scores of both variables for Samples 1, 2, 3, 
and 4.
</p>
<p>b. Display the data for the four samples in four frequency distribution
tables. For each score, add a positive or negative sign to indicate
the direction in which the score differs from the mean (as done in
Tables 14.4 and 14.5). Add an extra column in which you record a
plus or a minus for the product of the two signs.
</p>
<p>c. Draw four scatterplots, one for each sample distribution. State
whether each scatterplot shows a positive relationship, a negative
relationship, a curvilinear relationship, or no relationship between
the two variables.
</p>
<p>d. Would you advise against using Pearson&rsquo;s correlation coefficient as
a measure of association for any of the four samples? Explain your
answer.
</p>
<p>14.2 Jeremy, a police researcher, is concerned that police officers may not be
assigned to areas where they are needed. He wishes to find out whether
there is a connection between the number of police officers assigned to
a particular block and the number of violent incidents reported on that
block during the preceding week. For ten different blocks (designated A
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A432</p>
<p/>
</div>
<div class="page"><p/>
<p>through J), the number of patrolling officers assigned and the number of
prior violent incidents reported are as follows:
</p>
<p>A B C D E F G H I J
</p>
<p>X1: Violent incidents 7 10 3 9 8 0 4 4 2 8
</p>
<p>X2: Officers assigned 6 9 3 10 8 1 4 5 2 7
</p>
<p>a. Calculate the covariance for the data recorded above.
</p>
<p>b. Calculate the value of Pearson&rsquo;s r for the data recorded above.
</p>
<p>c. On an 11th block&mdash;block K&mdash;there are no police officers patrolling,
yet in the previous week 11 violent incidents were reported there.
What effect would it have on Pearson&rsquo;s r if Jeremy included block
K in his calculations?
</p>
<p>d. How do you explain this difference?
</p>
<p>14.3 Seven subjects of different ages are asked to complete a questionnaire
measuring attitudes about criminal behavior. Their answers are coded
into an index, with scores ranging from 1 to 15. The subjects&rsquo; scores
are as follows:
</p>
<p>X1: Age 12 22 10 14 18 20 16
</p>
<p>X2: Score 6 3 3 9 9 6 13
</p>
<p>a. Calculate Pearson&rsquo;s correlation coefficient for the two variables
listed above.
</p>
<p>b. Illustrate the sample distribution on a scatterplot.
</p>
<p>c. Divide the scatterplot into two sections, and calculate the value of
Pearson&rsquo;s r for each section.
</p>
<p>d. Explain the difference between the r values you obtained in parts a
and c.
</p>
<p>14.4 Eight homeowners in the inner-city neighborhood of Moss Tide are
asked how long they have been living in the neighborhood and how
many times during that period their house has been burglarized. The
results for the eight subjects are listed below:
</p>
<p>X1: Years in neighborhood 2 1.5 3.5 28 1 5 20 3
</p>
<p>X2: Number of burglaries 2 1 5 55 0 4 10 3
</p>
<p>a. Calculate Pearson&rsquo;s r for the two variables recorded above.
</p>
<p>b. Calculate Spearman&rsquo;s r for the same data.
</p>
<p>c. Illustrate the sample distribution on a scatterplot.
</p>
<p>d. Which of the two correlation coefficients is more appropriate, in
your opinion, for this case? Refer to the scatterplot in explaining
your answer.
</p>
<p>E X E R C I S E S 433</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Eleven defendants arrested for violent offenses were all required to
post bail. The judge said that the amount of bail assigned was related
to the total number of prior arrests. The results for the 11 defendants
are as follows:
</p>
<p>X1: Number of 
</p>
<p>prior arrests 0 3 9 13 2 7 1 4 7 20 5
</p>
<p>X2: Amount of
</p>
<p>bail assigned 100 500 2,500 10,000 1,000 10,000 100 7,500 5,000 100,000 4,000
</p>
<p>a. Calculate Pearson&rsquo;s r for the two variables recorded above.
</p>
<p>b. Calculate Spearman&rsquo;s r for the same data.
</p>
<p>c. Illustrate the sample distribution on a scatterplot.
</p>
<p>d. Which of the two correlation coefficients is more appropriate, in
your opinion, for this case? Refer to the scatterplot in explaining
your answer.
</p>
<p>14.6 Researchers looking at age and lifetime assault victimization inter-
viewed nine adults and found the following values:
</p>
<p>X1: Age 18 20 19 25 44 23 67 51 33
</p>
<p>X2: Number of times
</p>
<p>assaulted in lifetime 1 4 8 0 6 2 9 3 10
</p>
<p>a. Calculate Pearson&rsquo;s r for the two variables recorded above.
</p>
<p>b. Calculate Spearman&rsquo;s r for the same data.
</p>
<p>c. Illustrate the sample distribution on a scatterplot.
</p>
<p>d. Which of the two correlation coefficients is more appropriate, in
your opinion, for this case? Refer to the scatterplot in explaining
your answer.
</p>
<p>14.7 In a study looking at the relationship between truancy and theft, a
sample of ten youth were asked how many times in the last year they
had skipped school and how many times they had stolen something
worth $20 or less. Their responses were
</p>
<p>X1: Number of times 
</p>
<p>skipped school 9 2 4 0 0 10 6 5 3 1
</p>
<p>X2: Number of thefts
</p>
<p>valued at $20 or less 25 10 13 0 2 24 31 20 1 7
</p>
<p>a. Calculate Pearson&rsquo;s r for the two variables recorded above.
</p>
<p>b. Use a 5% level of significance and outline each of the steps re-
quired in a test of statistical significance of r.
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A434</p>
<p/>
</div>
<div class="page"><p/>
<p>14.8 A study investigating the link between child poverty and property
crime rates gathered information on a random sample of 13 counties.
The values for the percentage of children under 18 living in poverty
and property crime rates (given per 100,000) are
</p>
<p>a. Calculate Pearson&rsquo;s r for the two variables recorded above.
</p>
<p>b. Use a 5% level of significance and outline each of the steps re-
quired in a test of statistical significance of r.
</p>
<p>X1: Percentage of 
</p>
<p>children living 
</p>
<p>in poverty 10 8 43 11 27 18 15 22 17 17 20 25 35
</p>
<p>X2: Property crime 
</p>
<p>rate 1,000 2,000 7,000 4,000 3,000 4,500 2,100 1,600 2,700 1,400 3,200 4,800 6,300
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>The process for obtaining correlation coefficients and scatterplots is similar in 
</p>
<p>SPSS and Stata. Similar to other chapters, we have made available example  
</p>
<p> syntax files that illustrate the commands below for both SPSS (Chapter_14.sps) 
</p>
<p>and Stata (Chapter_14.do).
</p>
<p>SPSS
</p>
<p>Correlation Coefficients
</p>
<p>Both correlation coefficients discussed in this chapter&mdash;Pearson&rsquo;s r and 
</p>
<p>Spearman&rsquo;s r&mdash;can be obtained with similar commands. To obtain Pearson&rsquo;s r, 
</p>
<p>use the command CORRELATIONS:
</p>
<p>The /STATISTICS DESCRIPTIVES line is optional and would simply 
</p>
<p>generate a table of descriptive statistics, much like that produced with the 
</p>
<p>DESCRIPTIVES command in Chapter 4. Although it is much more efficient  
</p>
<p>to have SPSS compute all the correlations simultaneously for however many  
</p>
<p>variables you have included in the command, rather than selectively picking  
</p>
<p>out specific pairs that you might be most interested in, bear in mind that 
</p>
<p>the output may be rather unwieldy if too many variables are listed in the
</p>
<p>The output from the CORRELATIONS command will consist of a 
</p>
<p>matrix (grid) of correlations for all the variables whose names appear in the
</p>
<p>C O M P U T E R E X E R C I S E S 435
</p>
<p>/VARIABLES= line.
</p>
<p>/VARIABLES= line. You should also note that this matrix of correlations is sym-
</p>
<p>CORRELATIONS
</p>
<p>/VARIABLES = list_of_variable_names
</p>
<p>/STATISTICS DESCRIPTIVES.</p>
<p/>
</div>
<div class="page"><p/>
<p>metric; running from the upper left to the lower right corner of the matrix is a 
</p>
<p>diagonal that is made up of 1s (the correlation of the variable with itself). The cor-
</p>
<p>relations that appear above the diagonal will be a mirror image of the correlations 
</p>
<p>that appear below the diagonal. Thus, to locate the value of the correlation coeffi-
</p>
<p>cient you are most interested in, you simply find the row that corresponds to one 
</p>
<p>of the variables and the column that corresponds to the other variable. It does not 
</p>
<p>matter which variable you select for the row or for the column; the correlation 
</p>
<p>coefficient reported in the matrix will be the same.
</p>
<p>The command for obtaining Spearman&rsquo;s r is NONPAR CORR  
</p>
<p>(nonparametric correlation in SPSS):
</p>
<p>The output will again be a matrix of correlations for the variables listed  
</p>
<p>on the /VARIABLES= line of the command. The /PRINT = SPEARMAN 
</p>
<p>option ensures that SPSS will compute the correct correlation.
</p>
<p>Scatterplots
</p>
<p>The Computer Exercises section of Chapter 3 illustrated a wide range of graphics 
</p>
<p>commands available in SPSS. One of the simple, but powerful, graphs that SPSS 
</p>
<p>can produce is a scatterplot, which is obtained with the GRAPH command  
</p>
<p>and /SCATTERPLOT(BIVAR) option:
</p>
<p>The graph produced by this command will be a simple scatterplot that can be 
</p>
<p>edited to suit your needs. As we noted in Chapter 3, the interactive graphical 
</p>
<p>options in SPSS are extensive, and we encourage you to explore some of the 
</p>
<p>additional possibilities for creating novel and informative scatterplots.
</p>
<p>Stata
</p>
<p>Correlation Coefficients
</p>
<p>Obtaining Pearson&rsquo;s r in Stata is also simple using the pwcorr command 
</p>
<p> (pairwise correlation in Stata):
</p>
<p>where the options obs and sig will print in the correlation matrix the number  
</p>
<p>of observations used in the calculation of each correlation and the observed 
</p>
<p>significance level, making the output comparable to that in SPSS. The pwcorr 
</p>
<p>command does not require these options; they are included here simply to  
</p>
<p>provide more comprehensive and informative output.
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A436
</p>
<p>(nonparametric correlation in SPSS):
</p>
<p>NONPAR CORR
</p>
<p>/VARIABLES = list_of_variable_names
</p>
<p>/PRINT = SPEARMAN.
</p>
<p>GRAPH
</p>
<p>/SCATTERPLOT(BIVAR) = x_axis_var WITH y_axis_var.
</p>
<p>pwcorr list_of_variable_names, obs sig</p>
<p/>
</div>
<div class="page"><p/>
<p>There is one notable difference in the Stata output compared to that in 
</p>
<p>SPSS. Rather than printing a full symmetric matrix of correlations (see previous 
</p>
<p> discussion of SPSS output), Stata prints only the diagonal (where r = 1.0 and the 
</p>
<p>variable is correlated with itself) and the correlations appearing below the  
</p>
<p>diagonal (i.e., lower left half of the correlation matrix). This makes the output 
</p>
<p>less overwhelming and easier to sort through a large number of correlations.
</p>
<p>Spearman&rsquo;s r is obtained with the spearman command:
</p>
<p>where the stats(rho obs p) option will print the correlation (rho), the  
</p>
<p>number of observations used in calculating the correlation (obs), and the 
</p>
<p>observed  significance level of the correlation (p). The default is to simply print 
</p>
<p>the value of Spearman&rsquo;s r without information about the number of observations 
</p>
<p>or observed significance level. The pw option ensures that Stata uses all avail-
</p>
<p>able cases in computing the correlation rather than deleting an observation if it is 
</p>
<p>missing information on any one of the variables included in the list of variables.
</p>
<p>The output will be a matrix of Spearman&rsquo;s r correlations.
</p>
<p>Scatterplots
</p>
<p>In Chapter 3&rsquo;s Computer Exercises section we illustrated the use of the  twoway 
</p>
<p>command for the creation of other graphs. It can also be used for the creation of 
</p>
<p>simple scatterplots by changing the graph-type option to scatter:
</p>
<p>As with all other graphs in Stata, this scatterplot may be edited and customized 
</p>
<p>to suit your purpose.
</p>
<p>Problems
</p>
<p> 1. Open the California UCR data file (caucr_ 99.sav or caucr_99.dta). These 
</p>
<p>are the data presented in Table 14.8. Compute Pearson&rsquo;s r for these data 
</p>
<p>and note that it matches the value reported in the text.
</p>
<p>a. Compute Spearman&rsquo;s r for these data.
</p>
<p>b. How does the value of  Pearson&rsquo;s r compare to that for Spearman&rsquo;s r? 
</p>
<p>What might account for this?
</p>
<p>c. Generate a scatterplot for these data.
</p>
<p> 2. Enter the data from Exercise 14.2.
</p>
<p>a. Compute both Pearson&rsquo;s r and Spearman&rsquo;s r for these data.
</p>
<p>b. Add the extra case presented in part c of  Exercise 14.2. Recompute 
</p>
<p> Pearson&rsquo;s r and Spearman&rsquo;s r for these data.
</p>
<p>c. Compare your answers from parts a and b. How do you explain this 
</p>
<p> pattern?
</p>
<p>d. Generate a scatterplot for the 11 cases.
</p>
<p>C O M P U T E R E X E R C I S E S 437
</p>
<p>spearman list_of_variable_names, stats(rho obs p) pw
</p>
<p>twoway (scatter y_var_name x_var_name)</p>
<p/>
</div>
<div class="page"><p/>
<p> 3. Enter the data from Exercise 14.8.
</p>
<p>a. Compute Spearman&rsquo;s r for these data.
</p>
<p>b. How does the value of  Pearson&rsquo;s r compare to that for Spearman&rsquo;s r ? 
</p>
<p>What might account for this?
</p>
<p>c. Generate a scatterplot for these data.
</p>
<p> 4. Open the NYS data file (nys_1.sav, nys_1_student.sav, or nys_1.dta). Use a 
</p>
<p>5% level of  significance, and outline each of  the steps required in a test of  
</p>
<p>statistical significance for each of  the following relationships:
</p>
<p>a. Age and number of  thefts valued at less than $5 in the last year.
</p>
<p>b. Number of  times drunk and number of  thefts valued at $5 to $50 in 
</p>
<p>the last year.
</p>
<p>c. Frequency of  marijuana use and number of  times the youth has hit 
</p>
<p>other students in the last year.
</p>
<p>d. Number of  times the youth has hit a parent and number of  thefts val-
</p>
<p>ued at more than $50 in the last year.
</p>
<p>e. Number of  times the youth has been beaten up by parent and number 
</p>
<p>of  times the youth has hit a teacher in the last year.
</p>
<p> 5. Generate a scatterplot for each pair of  variables listed in Computer  
</p>
<p>Exercise 4.
</p>
<p> 6. Open the Pennsylvania Sentencing data file (pcs_ 98.sav or pcs_98.dta). 
</p>
<p>Use a 5% level of  significance, and outline each of  the steps required in a 
</p>
<p>test of  statistical significance for each of  the following relationships:
</p>
<p>a. Age of  offender and length of  incarceration sentence.
</p>
<p>b. Severity of  conviction offense and length of  incarceration sentence.
</p>
<p>c. Prior criminal history score and age.
</p>
<p>d. Length of  incarceration sentence and prior criminal history score.
</p>
<p> 7. Generate a scatterplot for each pair of  variables listed in Computer  
</p>
<p>Exercise 6.
</p>
<p>C H A P T E R F O U R T E E N :  I N T E R V A L - L E V E L D A T A438</p>
<p/>
</div>
<div class="page"><p/>
<p>An Introduction to Bivariate Regression
</p>
<p>What Does a Regression Coefficient Describe?
</p>
<p>How Can We Predict the Value of Y ?
</p>
<p>2 Statistic?
</p>
<p>Coefficient b?
</p>
<p>C h a p t e r  f i f t e e n
</p>
<p>T h e  r e g r e s s i o n  c o e f f i c i e n t
</p>
<p>P r e d i c t i o n :  b u i l d i n g  t h e  r e g r e s s i o n  l i n e
</p>
<p>E v a l u a t i n g  t h e  r e g r e s s i o n  l i n e
</p>
<p>S t a t i s t i c a l  i n f e r e n c e  i n  r e g r e s s i o n
</p>
<p>How is the Regression Coefficient b Expressed?
</p>
<p>How is the Regression Line Built?
</p>
<p>What are the Limits of Prediction?
</p>
<p>What is the R
</p>
<p>What is the Test of Statistical Significance for the Regression 
</p>
<p>What is the F-Test for the Overall Regression?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_15,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>IN THE PREVIOUS CHAPTER, we asked how strongly two interval-level vari-
ables were correlated. In this chapter, we ask a related question that is
</p>
<p>central to much statistical analysis in criminology and criminal justice: Can
</p>
<p>we predict the level of one interval-level variable from the value of an-
</p>
<p>other? As in the description of specific measures of association for nominal
</p>
<p>and ordinal variables in Chapter 13, in regression analysis we must define
</p>
<p>which variable is the independent variable, or predictor variable, and
</p>
<p>which variable is the dependent variable, or the variable being predicted.
</p>
<p>We also introduce the idea of regression modeling in this chapter. Our dis-
</p>
<p>cussion focuses on how regression analysis is used to create a prediction
</p>
<p>model and the statistics that researchers use to evaluate such models.
</p>
<p>E s t i m a t i n g  t h e  I n f l u e n c e  o f  O n e  V a r i a b l e  
o n  A n o t h e r :  T h e  R e g r e s s i o n  C o e f f i c i e n t
</p>
<p>By making only a slight change to the equation for Pearson&rsquo;s correlation
</p>
<p>coefficient, we can construct the regression coefficient b&mdash;a statistic that
</p>
<p>estimates how much one variable influences another. As with Goodman
</p>
<p>and Kruskal&rsquo;s tau, lambda, and Somers&rsquo; d (see Chapter 13), in developing
</p>
<p>this measure we again make a very important assumption about the rela-
</p>
<p>tionship between the variables examined. We assume that the indepen-
</p>
<p>dent variable (X) influences or predicts the dependent variable (Y).
</p>
<p>The regression coefficient b asks how much impact one variable (the
</p>
<p>independent variable) has on another (the dependent variable). It an-
</p>
<p>swers this question not in standardized units, but in the specific units of
</p>
<p>the variables examined. The specific interpretation of a regression coeffi-
</p>
<p>cient will depend on the units of measurement used. Nonetheless, b has
</p>
<p>a general interpretation in terms of X, the symbol for the independent
</p>
<p>variable, and Y, the symbol for the dependent variable:
</p>
<p>A change of one unit in X produces a change of b units in the estimated
</p>
<p>value of Y.
</p>
<p>440</p>
<p/>
</div>
<div class="page"><p/>
<p>E S T I M A T I N G T H E I N F L U E N C E O F O N E V A R I A B L E O N A N O T H E R
</p>
<p>Let&rsquo;s take a concrete example. Suppose that you are studying the rela-
</p>
<p>tionship between education and reoffending. You assume that education
</p>
<p>has an impact on reoffending and thus define years of education as the
</p>
<p>independent variable, X, and number of arrests as the dependent vari-
</p>
<p>able, Y. You calculate the regression coefficient b and find that it has a
</p>
<p>value of �2. You can interpret this coefficient as meaning that a one-
</p>
<p>year change (or increase) in education produces, on average, a two-unit
</p>
<p>change&mdash;in this case, reduction&mdash;in number of rearrests. If b had been
</p>
<p>positive, we would have concluded that a one-year increase in education
</p>
<p>produces, on average, an increase of two arrests.
</p>
<p>This interpretation of the regression coefficient reminds us that we are
</p>
<p>once again concerned with linear relationships. If we say that a unit
</p>
<p>change in X results in b units of change in Y, we are also saying that 
</p>
<p>You can see from this example why regression analysis is such a
</p>
<p>widely used tool in criminal justice and criminology. Contrary to what
</p>
<p>students often fear, regression coefficients are very easy to understand
</p>
<p>and make good intuitive sense. The regression coefficient in this case
</p>
<p>Hypothetical Regression Line of the Relationship 
</p>
<p>Between Number of Arrests and Years of Education
Figure 15.1
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>A
rr
</p>
<p>es
ts
</p>
<p>20
</p>
<p>15
</p>
<p>10
</p>
<p>5
</p>
<p>0
</p>
<p>Years of Education 
</p>
<p>       8     9     10     11    12   13    14    15    16     
</p>
<p>441
</p>
<p>In our example, the regression coefficient predicts that a one-year increase
</p>
<p>in education produces a reduction of two arrests, irrespective of how 
</p>
<p>education an offender already has. If the change is constant, then the
</p>
<p>result is a linear relationship. This is illustrated in Figure 15.1. Importantly,
</p>
<p>linearity is a central assumption of regression analysis. As with linear
</p>
<p>correlation coefficients, you should always examine the scatterplot of 
</p>
<p>the relationship between the variables to be sure that the relationship
</p>
<p>you are estimating is a linear one.
</p>
<p>the change produced by X is constant. By &ldquo;constant&rdquo; we mean that the
</p>
<p>change produced in Y is always the same for each unit change in X.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>tells us that education reduces reoffending. It also tells us by how much.
</p>
<p>For each year of education, there is a reduction of about two arrests.
</p>
<p>Often, criminologists use regression coefficients to estimate how much
</p>
<p>1
</p>
<p>Calculating the Regression Coefficient
</p>
<p>The calculation for the regression coefficient b (Equation 15.1) is very
</p>
<p>similar to that for the correlation coefficient r (Equation 14.2). Once
</p>
<p>again, the covariation between the two variables examined is in the nu-
</p>
<p>merator, although the two variables are expressed as X and Y rather than
</p>
<p>X1 and X2. The difference is in the denominator of the equation. Instead
</p>
<p>of standardizing this value according to the variability found in both
</p>
<p>measures, we now contrast the covariation of the relationship of X and Y
</p>
<p>with the variability found only in X.
</p>
<p>Equation 15.1 
</p>
<p>By taking the example of age and number of arrests over the last year
</p>
<p>from Table 14.1, we can see how a regression coefficient is calculated in
</p>
<p>practice (see Table 15.1). Our first step, as noted above, is to examine
</p>
<p>the scatterplot of points to make sure that the relationship we are esti-
</p>
<p>mating is linear. This was done in Chapter 14, so we will not repeat that
</p>
<p>(Y ). Our assumption here is that age influences the number of arrests on
</p>
<p>an offender&rsquo;s record over the previous year. As with the correlation coef-
</p>
<p>ficient, we first calculate the covariation between age (X ) and number of
</p>
<p>arrests (Y ), as shown in Equation 15.2.
</p>
<p>Covariation of X and Y � Equation 15.2�
N
</p>
<p>i�1
</p>
<p> (Xi � X)(Yi � Y)
</p>
<p>b � 
�
N
</p>
<p>i�1
</p>
<p> (Xi � X)(Yi � Y)
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2
</p>
<p>1Jihong Zhao and Quint Thurman, &ldquo;A National Evaluation of the Effect of COPS Grants
</p>
<p>on Crime from 1994&ndash;1999,&rdquo; unpublished manuscript, University of Nebraska, Decem-
</p>
<p>ber, 2001.
</p>
<p>442
</p>
<p>crime-control benefit is gained from a particular strategy or from the
</p>
<p>step here. The next step is to define X and Y. In this case, age is the inde-
</p>
<p>pendent variable (X ), and number of arrests is the dependent variable
</p>
<p>addition of criminal justice resources. In a report to the federal govern-
</p>
<p>ment, for example, a group of researchers from the University of Nebraska
</p>
<p>used regression coefficients to estimate how much each dollar spent on
</p>
<p>additional police in the United States during the 1990s influenced crime
</p>
<p>rates in American cities. Clearly, a statistic that can tell us how much 
</p>
<p>change in one variable influences another can be very useful in deciding 
</p>
<p>on criminal justice strategies or policies.</p>
<p/>
</div>
<div class="page"><p/>
<p>E S T I M A T I N G T H E I N F L U E N C E O F O N E V A R I A B L E O N A N O T H E R
</p>
<p>Our result (calculated in column 6 in Table 15.1) is the same as the
</p>
<p>simply take the sum of the squared deviations of each subject&rsquo;s age from
</p>
<p>the mean for age. The result, calculated in column 3 of Table 15.1, is
</p>
<p>138.9326. Our regression coefficient is obtained by dividing these two
</p>
<p>sums.
</p>
<p>Calculations for the Regression Coefficient 
of Age and Number of Arrests for 15 Young Offenders
</p>
<p>AGE ARRESTS
</p>
<p>X Xi � (Xi � )
2 Y Yi � (Xi � )(Yi � )
</p>
<p>(1) (2) (3) (4) (5) (6)
</p>
<p>14 �3.0667 9.4046 0 �4.8667 14.9247
13 �4.0667 16.5380 1 �3.8667 15.7247
15 �2.0667 4.2712 1 �3.8667 7.9913
13 �4.0667 16.5380 2 �2.8667 11.6580
14 �3.0667 9.4046 2 �2.8667 8.7913
14 �3.0667 9.4046 3 �1.8667 5.7246
17 �0.0667 0.0044 3 �1.8667 0.1245
19 1.9333 3.7376 4 �0.8667 �1.6756
21 3.9333 15.4708 4 �0.8667 �3.4090
19 1.9333 3.7376 6 1.1333 2.1910
16 �1.0667 1.1378 8 3.1333 �3.3423
18 0.9333 0.8710 9 4.1333 3.8576
20 2.9333 8.6042 9 4.1333 12.1242
21 3.9333 15.4708 10 5.1333 20.1908
22 4.9333 24.3374 11 6.1333 30.2574
</p>
<p>� 125.1332�
N
</p>
<p>i�1
 (Xi � X)(Yi � Y)�
</p>
<p>N
</p>
<p>i�1
</p>
<p> (Xi � X)
2 � 138.9326
</p>
<p>Y � 4.8667X � 17.0667
</p>
<p>YXYXX
</p>
<p>Table 15.1
</p>
<p>W orking It Out
</p>
<p> � 0.9007
</p>
<p> � 
125.1332
</p>
<p>138.9326
</p>
<p> b � 
�
N
</p>
<p>i�1
</p>
<p> (Xi � X)(Yi � Y)
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2
</p>
<p>This result of 0.9007 can be interpreted as meaning that a one-year in-
</p>
<p>crease in age produces, on average, a 0.90 increase in number of arrests.
</p>
<p>443
</p>
<p>denominator, however, involves less work than before. In this case, we
</p>
<p>result for the correlation coefficient: 125.1332. The calculation of the </p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>A Substantive Example: Unemployment and Burglary in California
</p>
<p>In Chapter 14, in an analysis of unemployment and burglary rates for
</p>
<p>counties in the state of California in 1999, we reported that the correla-
</p>
<p>tion between unemployment rates and burglary rates was r � 0.4910.
</p>
<p>We can continue to work with these data to provide another illustration
</p>
<p>practice in regression to start by examining a scatterplot (see Figure
</p>
<p>15.2). Our interest is in confirming that the relationship between unem-
</p>
<p>ployment rates and burglary rates in California counties is linear. The
</p>
<p>scatterplot in Figure 15.2 provides no strong evidence to suggest that the
</p>
<p>relationship is not linear.
</p>
<p>In Table 14.9, we reported the two results needed to calculate the re-
</p>
<p>gression coefficient b. The covariation between the unemployment rate
</p>
<p>and the burglary rate is 37,128.9297, and the variability of the unemploy-
</p>
<p>ment rate is 1,010.3570 (see the bottom of Table 14.9). Inserting these
</p>
<p>values into Equation 15.1, we find that b � 36.7483.
</p>
<p> � 36.7483
</p>
<p> � 
37128.9297
1010.3570
</p>
<p> b � 
�
N
</p>
<p>i�1
</p>
<p> (Xi � X)(Yi � Y)
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X)
2
</p>
<p>Scatterplot of Unemployment Rates and Burglary Rates in California, 1999Figure 15.2
</p>
<p>B
u
</p>
<p>rg
la
</p>
<p>ry
 R
</p>
<p>at
e
</p>
<p>3,000
</p>
<p>2,000
</p>
<p>1,000
</p>
<p>0
</p>
<p>Unemployment Rate 
</p>
<p>       0                   10              20                      30
</p>
<p>444
</p>
<p>relationship between unemployment rates and burglary rates, it is good
</p>
<p>of regression analysis. While the correlation of 0.49 suggests a linear </p>
<p/>
</div>
<div class="page"><p/>
<p>P R E D I C T I O N I N R E G R E S S I O N
</p>
<p>This coefficient tells us that, on average, for every one-unit increase in
</p>
<p>the unemployment rate, the burglary rate increases by about 36.75
</p>
<p>crimes (per 100,000).
</p>
<p>P r e d i c t i o n  i n  R e g r e s s i o n :  B u i l d i n g  t h e  R e g r e s s i o n  L i n e
</p>
<p>The regression coefficient provides a method for estimating how
</p>
<p>change in an independent variable influences change in a dependent
</p>
<p>variable. However, by itself, it does not allow the researcher to predict
</p>
<p>the actual values of the dependent variable. For example, we found in
</p>
<p>our examination of age and number of arrests that a one-year increase
</p>
<p>in age in our sample of young offenders was associated with a 0.9007
</p>
<p>increase in number of arrests over the last year. Accordingly, based on
</p>
<p>our analysis, we would predict that a 14-year-old would have about
</p>
<p>0.9 more arrests than a 13-year-old. Someone 15 years old would be
</p>
<p>expected to have about 1.8 more arrests over the last year than some-
</p>
<p>one 13 years old. This is because our regression coefficient suggests
</p>
<p>expect a county with an unemployment rate of 5.0 to have a burglary
</p>
<p>rate that was about 36.75 (per 100,000) higher than that of a county
</p>
<p>with an employment rate of 4.0. A county with an unemployment rate
</p>
<p>of 6.0 would be expected to have a burglary rate that was about 36.75
</p>
<p>(per 100,000) higher than that of the county with an unemployment
</p>
<p>rate of 5.0.
</p>
<p>But this still does not tell us how many arrests overall a person 13, 14,
</p>
<p>or 15 years old would be expected to have over the last year. Nor can
</p>
<p>we define the rate of burglary in a county with an unemployment rate of
</p>
<p>4.0, 5.0, or 6.0. To answer these questions, we need another piece of in-
</p>
<p>formation: a starting point from which to calculate change. That starting
</p>
<p>point is provided by a statistic called the Y-intercept.
</p>
<p>The Y-Intercept
</p>
<p>The Y-intercept, or b0, is the expected value of Y when X equals 0.
2 It is
</p>
<p>calculated by taking the product of b and the mean of X, and subtracting
</p>
<p>it from the mean of Y (Equation 15.3).
</p>
<p>Equation 15.3b0 � Y � bX
</p>
<p>2Note that there is no single accepted convention for representing the Y-intercept.
</p>
<p>Some researchers use the symbol � (alpha), while others prefer to use a.
</p>
<p>445
</p>
<p>that for each year increase in age, we can expect about 0.9 more arrests. 
</p>
<p>Similarly, for the unemployment rate and burglary rate analysis, we would</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>Let&rsquo;s begin with our example of age and number of arrests. We get 
</p>
<p>the value of b0 by first taking the product of b (0.9007) and the mean
</p>
<p>for age (17.0667; see Table 15.1) and then subtracting that value
</p>
<p>(15.3720) from 4.8667, the mean for arrests in the sample. The result is
</p>
<p>�10.5053.
</p>
<p>W orking It Out
</p>
<p> � �10.5053
</p>
<p> � 4.8667 � 15.3720
</p>
<p> � 4.8667 � (0.9007)(17.0667)
</p>
<p> b0 � Y � bX
</p>
<p>The Regression Line
</p>
<p>By looking at a scatterplot (see Figure 15.3), we can see how the Y-
</p>
<p>intercept helps in developing predictions of number of arrests (Y ) from
</p>
<p>age (X). If we put the value �10.5053 on the line where the value of age
</p>
<p>is 0, we can then use the regression coefficient b to draw a line of pre-
</p>
<p>Scatterplot and Regression Line Showing the Relationship 
</p>
<p>Between Age and Number of Arrests for 15 Subjects
Figure 15.3
</p>
<p>&ndash;10
</p>
<p>20
</p>
<p>0,0
</p>
<p>10
</p>
<p>10 20 30
</p>
<p>Age
</p>
<p>N
u
</p>
<p>m
b
</p>
<p>er
 o
</p>
<p>f 
A
</p>
<p>rr
es
</p>
<p>ts
</p>
<p>Y-intercept
</p>
<p>446</p>
<p/>
</div>
<div class="page"><p/>
<p>P R E D I C T I O N I N R E G R E S S I O N
</p>
<p>0 � 0.9007, or �10.5053 � 0.9007 � �9.6046. For 
</p>
<p>0 � 0.9007 � 0.9007, and
</p>
<p>so forth. By plotting these values, we can draw the regression line for
</p>
<p>our example. This is done in Figure 15.3, which also includes the scatter-
</p>
<p>plot of the 15 sample scores (from Table 15.1).
</p>
<p>The predicted values of Y, designated , can also be found through a
</p>
<p>simple equation. In this case, is equivalent to the Y-intercept plus the
</p>
<p>regression coefficient times the value of X (Equation 15.4).
</p>
<p>Equation 15.4
</p>
<p>For our example, this equation may be represented as shown in Equa-
</p>
<p>tion 15.5.
</p>
<p>Predicted number of arrests � �10.5053 � (0.9007)(age) Equation 15.5
</p>
<p>We now have a method for predicting the number of arrests based on
</p>
<p>the age of subjects in our sample. For example, looking at our regression
</p>
<p>line, we would predict that someone 13 years old would have had about
</p>
<p>one arrest over the last year. To obtain the exact prediction, we simply
</p>
<p>insert age 13 in Equation 15.5. The result is 1.2038:
</p>
<p>Ŷ � b0 � bX
</p>
<p>Ŷ
</p>
<p>Ŷ
</p>
<p>W orking It Out
</p>
<p> � 1.2038
</p>
<p> � �10.5053 � (0.9007)(13)
</p>
<p> Predicted number of arrests � �10.5053 � (0.9007)(age)
</p>
<p>W orking It Out
</p>
<p> � 3.9059
</p>
<p> � �10.5053 � (0.9007)(16)
</p>
<p> Predicted number of arrests � �10.5053 � (0.9007)(age)
</p>
<p>For someone 16 years old, we would predict about four arrests.
</p>
<p>447
</p>
<p>us that for each increase of one year in age, there is a corresponding 
</p>
<p>diction, called the regression line. The regression coefficient tells 
</p>
<p>bof arrests should be  
</p>
<p>2 years of age, the number of arrests should be b
</p>
<p>increase of 0.9007 arrest. This means that when age is about 1, number </p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>Predictions Beyond the Distribution Observed in a Sample
</p>
<p>Are our predictions good ones? We will turn a bit later in the chapter to
</p>
<p>methods of evaluating how well the regression line predicts or fits the
</p>
<p>data in our sample. Nonetheless, at this point it is useful to look at Fig-
</p>
<p>ure 15.3 and evaluate on your own how well we have done. I think you
</p>
<p>will agree that we have done a fairly good job of drawing a line through
</p>
<p>the data points in our sample. For the most part, the points are close to
</p>
<p>the line, meaning that our prediction of number of arrests based on age
</p>
<p>of the offender is fairly close to the actual scores in our sample. But what
</p>
<p>about predicting beyond our sample scores? Can we use our regression
</p>
<p>line to predict number of arrests for those 30 or 40 years old? What about
</p>
<p>for offenders younger than 13?
</p>
<p>This approach should be used with caution. In order to predict be-
</p>
<p>yond the data points in your sample, you must assume that the relation-
</p>
<p>ships are similar for offenders your study has not examined. As is always
</p>
<p>the case when you try to make inferences beyond your sampling frame,
</p>
<p>example, we would have to explain why we think that the relationship
</p>
<p>3
</p>
<p>W orking It Out
</p>
<p> � 7.5087
</p>
<p> � �10.5053 � (0.9007)(20)
</p>
<p> Predicted number of arrests � �10.5053 � (0.9007)(age)
</p>
<p>For someone 20 years old, we would predict between seven and eight
</p>
<p>arrests.
</p>
<p>3For a discussion of the relationship between age and crime, see David F. Farrington,
</p>
<p>&ldquo;Age and Crime,&rdquo; in Crime and Justice: An Annual Review of Research, Vol. 7
</p>
<p>(Chicago: University of Chicago Press, 1986), pp. 189&ndash;250.
</p>
<p>448
</p>
<p>your conclusions cannot be justified directly from the statistics you develop.
</p>
<p>You must explain why you expect the relationships observed to con-
</p>
<p>tinue for populations that have not been represented. In our case, for
</p>
<p>between age and number of arrests continues for both older and youn-
</p>
<p>ger offenders. What we know about offending in criminology suggests
</p>
<p>that this approach would not be a good one to use for older offenders.
</p>
<p>A number of studies have shown that offending rates often decrease
</p>
<p>as offenders get older. Certainly our predictions of number of arrests
</p>
<p>for those younger than age 12 are to be viewed with caution. Our</p>
<p/>
</div>
<div class="page"><p/>
<p>P R E D I C T I O N I N R E G R E S S I O N
</p>
<p>regression line predicts negative values of arrests for these young offend-
</p>
<p>ers. This example illustrates a very important limitation of regression
</p>
<p>analysis. You should be very cautious about predicting beyond the distri-
</p>
<p>bution observed in your sample. As a general rule, regression can pro-
</p>
<p>vide good estimates of predictions for the range of offenders represented
</p>
<p>by the data; however, it often does not provide a solid basis for predic-
</p>
<p>tions beyond that range.
</p>
<p>Predicting Burglary Rates from Unemployment Rates in California
</p>
<p>What would our prediction model look like for the case of the unemploy-
</p>
<p>ment rate and burglary rate data from counties in California? First, we
</p>
<p>need to define the Y-intercept by using Equation 15.3. The value of b is
</p>
<p>36.7483, the mean unemployment rate is 7.4069, and the mean burglary
</p>
<p>rate is 790.8907 (see Table 14.9). The Y-intercept has a value of 518.6997.
</p>
<p>W orking It Out
</p>
<p> � 518.6997
</p>
<p> � 790.8907 � (36.7483)(7.4069)
</p>
<p> b0 � Y � bX
</p>
<p>Once we know the Y-intercept, we can develop the regression line. For
</p>
<p>our unemployment and burglary example, we write the regression equa-
</p>
<p>tion as shown in Equation 15.6.
</p>
<p>Burglary rate � 518.6997 � 36.7483(unemployment rate) Equation 15.6
</p>
<p>Equation 15.6 can be used to calculate the predicted value of the bur-
</p>
<p>glary rate, given a value for the unemployment rate. For example, if a
</p>
<p>county had an unemployment rate of 2.5, we would predict from our
</p>
<p>model that the burglary rate would be about 610.57.
</p>
<p>W orking It Out
</p>
<p> � 610.5705
</p>
<p> � 518.6997 � (36.7483)(2.5)
</p>
<p> Burglary rate � 518.6997 � 36.7483(unemployment rate)
</p>
<p>449</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>For a county with an unemployment rate of 4.2, we would predict a bur-
</p>
<p>glary rate of about 673.04.
</p>
<p>W orking It Out
</p>
<p> � 673.0426
</p>
<p> � 518.6997 � (36.7483)(4.2)
</p>
<p> Burglary rate � 518.6997 � 36.7483(unemployment rate)
</p>
<p>And for a county that had an unemployment rate of 7.5, we would pre-
</p>
<p>dict a burglary rate of about 794.31.
</p>
<p>W orking It Out
</p>
<p> � 794.3120
</p>
<p> � 518.6997 � (36.7483)(7.5)
</p>
<p> Burglary rate � 518.6997 � 36.7483(unemployment rate)
</p>
<p>How well our regression line fits the data is illustrated in Figure 15.4,
</p>
<p>which presents the regression line in the scatterplot of the data. Overall,
</p>
<p>it looks as though the regression line captures the relationship between
</p>
<p>unemployment rates and burglary rates reasonably well&mdash;the values for
</p>
<p>burglary rate tend to cluster near the regression line.
</p>
<p>Scatterplot and Regression Line Showing the Relationship Between 
</p>
<p>Unemployment Rate and Burglary Rate for California Counties, 1999
Figure 15.4
</p>
<p>B
u
</p>
<p>rg
la
</p>
<p>ry
 R
</p>
<p>at
e
</p>
<p>3,000
</p>
<p>2,000
</p>
<p>1,000
</p>
<p>0
</p>
<p>Unemployment Rate 
</p>
<p>       0                   10              20                      30
</p>
<p>450</p>
<p/>
</div>
<div class="page"><p/>
<p>P R E D I C T I O N I N R E G R E S S I O N
</p>
<p>Choosing the Best Line of Prediction Based on Regression Error
</p>
<p>One question we might ask is whether the regression line we identify is
</p>
<p>the best one that could be drawn, given the data available to us. In order
</p>
<p>to answer this question, we must first decide on the criteria that we will
</p>
<p>use for defining the best line. In regression, as in many of the statistical
</p>
<p>techniques we have examined, we use the criterion of minimizing error.
</p>
<p>Regression error (e) is defined as the difference between the actual
</p>
<p>values of Y and the predicted values of Y, or , as shown in Equation 15.7.
</p>
<p>Equation 15.7
</p>
<p>In Table 15.2, the actual values of Y and the predicted values ( ) are
</p>
<p>predicted value for arrests based on our regression equation is 6.6080,
</p>
<p>so the error in this case is �2.6080. In other words, the actual value of
</p>
<p>Y is made up of both our prediction and some amount of error. The
</p>
<p>equation form of this relationship gives the basic regression model for
</p>
<p>our example. This model may be expressed either in terms of a theoret-
</p>
<p>ical model describing the relationships in the population or in terms of
</p>
<p>the observed relationships found in our sample. For a population
</p>
<p>Y � �0 � �(age) � �
</p>
<p>Ŷ
</p>
<p>e � Y � Ŷ
</p>
<p>Ŷ
</p>
<p>Contrast of the Predicted Values 
for Y, or , and the Actual Values for Y
</p>
<p>NUMBER OF
</p>
<p>AGE ARRESTS
</p>
<p>SUBJECT X Y Yi �
</p>
<p>1 14 0 2.1046 �2.1046
2 13 1 1.2039 �0.2039
3 15 1 3.0053 �2.0053
4 13 2 1.2039 0.7961
5 14 2 2.1046 �0.1046
6 14 3 2.1046 0.8954
7 17 3 4.8066 �1.8066
8 19 4 6.6080 �2.6080
9 21 4 8.4093 �4.4093
</p>
<p>10 19 6 6.6080 �0.6080
11 16 8 3.9060 4.0940
12 18 9 5.7073 3.2927
13 20 9 7.5086 1.4914
14 21 10 8.4093 1.5907
15 22 11 9.3100 1.6900
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi �Ŷi ) � 0.0000
</p>
<p>ŶiŶ
</p>
<p>Ŷ
Table 15.2
</p>
<p>451
</p>
<p>arrests. Subject 8, for example, had four arrests over the last year. The
</p>
<p>contrasted for the 15 subjects in our example of age and number of 
</p>
<p>model, we use Greek letters to represent the parameters. So for our 
</p>
<p>example, the population model would be</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>As we see from Table 15.2, this subject&rsquo;s actual value for Y is also 1 ar-
</p>
<p>rest over the last year.
</p>
<p>In using error as a criterion for choosing the best line, we are forced
</p>
<p>to base our decision not on the sum of errors in our equation, but on the
</p>
<p>sum of the squared errors. This is because the deviations above and
</p>
<p>below the regression line cancel each other out (see Table 15.2). In re-
</p>
<p>gression, as in deviations from the mean, the sum of the deviations of 
</p>
<p>from Y are always equal to 0.
</p>
<p>Squaring the deviations of from Y provides estimates with only pos-
</p>
<p>itive signs and allows us to assess the amount of error found. The regres-
</p>
<p>sion line we have constructed is the best line in terms of the criteria of
</p>
<p>squared deviations of from Y. In mathematical language, the regression
</p>
<p>line is the line for which the sum of the squared errors is a minimum
</p>
<p>(Equation 15.8).
</p>
<p>Equation 15.8
</p>
<p>For this reason, we call this approach ordinary least squares regres-
</p>
<p>sion analysis, or OLS regression. We hope you remember that we first
</p>
<p>introduced the concept of the least squares property when we discussed
</p>
<p>the mean in Chapter 4. As is often the case in statistics, ideas learned early
</p>
<p>on continue to be important in understanding more complex statistics.
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Ŷ
 )2 � minimum
</p>
<p>Ŷ
</p>
<p>Ŷ
</p>
<p>Ŷ
</p>
<p>W orking It Out
</p>
<p> � 1
</p>
<p> � �10.5053 � 13.5105 � (�2.0053)
</p>
<p> � �10.5053 � 0.9007(15) � (�2.0053)
</p>
<p> Y � b0 � b(age) � e
</p>
<p>The model for the example would be expressed as
</p>
<p>Y � b0 � b(age) � e
</p>
<p>By looking at one of our 15 subjects, we can see this relationship in
</p>
<p>practice. Subject 3&rsquo;s age is 15. The difference between the predicted
</p>
<p>value for arrests and the actual number of arrests, or the error (e), for
</p>
<p>Subject 3 is �2.0053. If we add the Y-intercept, the subject&rsquo;s age times
</p>
<p>the coefficient b, and the error, we obtain a value of 1.
</p>
<p>452</p>
<p/>
</div>
<div class="page"><p/>
<p>E V A L U A T I N G T H E R E G R E S S I O N M O D E L
</p>
<p>E v a l u a t i n g  t h e  R e g r e s s i o n  M o d e l
</p>
<p>Having noted that OLS regression provides the best line in terms of the
</p>
<p>least squares criteria for error, we may still ask how well this line pre-
</p>
<p>dicts the dependent variable. Does the regression model add to our abil-
</p>
<p>ity to predict number of arrests in our sample? Researchers commonly
</p>
<p>use a measure called the percent of variance explained to answer this
</p>
<p>question.
</p>
<p>Percent of Variance Explained
</p>
<p>The percent of variance explained, or R2, in regression is analogous to
</p>
<p>eta squared in analysis of variance. With eta squared, we examine the
</p>
<p>proportion of the total sum of squares accounted for by the between (or
</p>
<p>explained) sum of squares. In the case of regression, the explained sum
</p>
<p>of squares (ESS) is calculated from the difference between the predicted
</p>
<p>value of Y, or , and the mean of Y, or : ( i � )
2. The total sum of
</p>
<p>squares (TSS) is represented by the difference between Y and the mean of
</p>
<p>Y: (Yi � )
2. R 2 for regression, like eta squared for analysis of variance, is
</p>
<p>the ratio of the explained to the total sum of squares (Equation 15.9).
</p>
<p>Equation 15.9
</p>
<p>Why do we define the explained and total sums of squares in terms
</p>
<p>of the mean? If we did not have our regression model, but had only the
</p>
<p>raw data in our sample, our best single prediction of Y would be the
</p>
<p>mean of Y. The question answered by R 2 is &ldquo;How much additional
</p>
<p>)
</p>
<p>).
</p>
<p>The explained deviation thus represents the improvement in predicting
</p>
<p>Y that the regression line provides over the mean.
</p>
<p>To calculate the explained sum of squares for our example, we sim-
</p>
<p>ply take the difference between the predicted score for arrests and the
</p>
<p>mean for arrests, square that value, and sum the outcomes across the 15
</p>
<p>subjects. This is done in column 5 of Table 15.3, where our final result is
</p>
<p>112.7041. For the total sum of squares, we subtract the mean of arrests
</p>
<p>for the sample from each subject&rsquo;s actual number of arrests. This value is
</p>
<p>Y
</p>
<p>ˆ
</p>
<p>R 2 � 
ESS
TSS
</p>
<p> � 
�
N
</p>
<p>i�1
</p>
<p> (Ŷi � Y )
2
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Y )
2
</p>
<p>Y
</p>
<p>YŶYŶ
</p>
<p>453
</p>
<p>knowledge have we gained by developing the regression line?&rdquo; This is 
</p>
<p>illustrated in Figure 15.5, where we take one subject from our analysis 
</p>
<p>of the effect of age on number of arrests, Subject 13, and plot that subject&rsquo;s
</p>
<p>. The distance from the mean of Y to the explained deviation, (Y � Y
</p>
<p>actual score for the subject is the total deviation from the mean, (Y �
</p>
<p>between the predicted value of Y and the mean of Y represents the
</p>
<p>score relative to the regression line and the mean of Y. The distance 
</p>
<p>i
</p>
<p>i</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>squared, and the 15 outcomes are added together (see Table 15.3, col-
</p>
<p>umn 7). The total sum of squares for our example is 187.7333.
</p>
<p>To obtain the percent of variance explained, or R 2, we take the ratio
</p>
<p>of these two values. The percent of variance explained beyond the mean
</p>
<p>in our regression model is 0.60.4
</p>
<p>The Explained, Unexplained, and Total Deviations 
</p>
<p>from the Mean for Subject 13 (Age � 20; Arrests � 9)
Figure 15.5
</p>
<p>12
</p>
<p>10
</p>
<p>8
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>0
</p>
<p>12               14                16               18              20               22              24
</p>
<p>Unexplained Variation
</p>
<p>Explained Variation
Total Variation
</p>
<p>Age
</p>
<p>N
u
</p>
<p>m
b
er
</p>
<p> o
f 
</p>
<p>A
rr
</p>
<p>es
ts
</p>
<p>X13
</p>
<p>Y13
</p>
<p>Y &ndash; Y
</p>
<p>Y &ndash; Y
</p>
<p>Y &ndash; Y
</p>
<p>Y
</p>
<p>Y
</p>
<p>4In the case of multivariate regression analysis (described in detail in Chapter 16),
</p>
<p>where there are multiple independent variables, some statisticians advise using an
</p>
<p>adjusted measure of R2. Commonly referred to as adjusted R2, it is routinely provided
</p>
<p>by most statistical software programs. Adjusted R2 is calculated using the following
</p>
<p>formula:
</p>
<p>Adjusted R2 � 1 � (1 � R2)
</p>
<p>where k equals the number of independent variables and N is the number of observa-
</p>
<p>tions in the sample.
</p>
<p>Adjused R2 can prevent a misleading interpretation of the strength of prediction of a
</p>
<p>model, because it offsets the artificial inflation in the statistic that is created with every
</p>
<p>additional variable added to a regression model. Nonetheless, the adjustment repre-
</p>
<p>sents a transformation of the R2 value and thus alters the simple meaning of the statis-
</p>
<p>tic. We do not advise using the adjusted R2 value unless it differs substantially from
</p>
<p>the simple R2 value.
</p>
<p>� N � 1N � k � 1�
</p>
<p>454
</p>
<p>i
</p>
<p>i
</p>
<p>i
</p>
<p>i</p>
<p/>
</div>
<div class="page"><p/>
<p>E V A L U A T I N G T H E R E G R E S S I O N M O D E L
</p>
<p>Is an R 2 of 0.6003 large or small? As noted in earlier chapters, deter-
</p>
<p>mining whether a relationship is large or small inevitably involves a
</p>
<p>value judgment. In deciding on the strength of your prediction, you
</p>
<p>would likely compare your results to those of other research on the
</p>
<p>same or related topics. As a general rule in criminal justice, regression
</p>
<p>models seldom result in R 2 values greater than 0.40. If your R 2 is larger
</p>
<p>than 0.40, you can usually assume that your prediction model is a pow-
</p>
<p>erful one. The percent of variance explained in our model accordingly
</p>
<p>suggests a very high level of prediction. Conversely, when the percent of
</p>
<p>variance explained is less than 0.15 or 0.20, the model is likely to be
</p>
<p>viewed as relatively weak in terms of prediction.
</p>
<p>W orking It Out
</p>
<p> � 0.6003
</p>
<p> � 
112.7041
187.7333
</p>
<p> R 2 � 
ESS
TSS
</p>
<p> � 
�
N
</p>
<p>i�1
</p>
<p> (Ŷi � Y )
2
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Y )
2
</p>
<p>Calculations for R2 Values for 15 Young Offenders
</p>
<p>X Y i � ( i � )
2 Yi � (Yi � )
</p>
<p>2
</p>
<p>(1) (2) (3) (4) (5) (6) (7)
</p>
<p>14 0 2.1046 �2.7621 7.6292 �4.8667 23.6848
13 1 1.2039 �3.6628 13.4158 �3.8667 14.9514
15 1 3.0053 �1.8614 3.4649 �3.8667 14.9514
13 2 1.2039 �3.6628 13.4158 �2.8667 8.2180
14 2 2.1046 �2.7621 7.6292 �2.8667 8.2180
14 3 2.1046 �2.7621 7.6292 �1.8667 3.4846
17 3 4.8066 �0.0601 0.0036 �1.8667 3.4846
19 4 6.6080 1.7413 3.0320 �0.8667 0.7512
21 4 8.4093 3.5426 12.5501 �0.8667 0.7512
19 6 6.6080 1.7413 3.0320 1.1333 1.2844
16 8 3.9060 �0.9607 0.9230 3.1333 9.8176
18 9 5.7073 0.8406 0.7066 4.1333 17.0842
20 9 7.5086 2.6419 6.9798 4.1333 17.0842
21 10 8.4093 3.5426 12.5501 5.1333 26.3508
22 11 9.3100 4.4433 19.7427 6.1333 37.6174
</p>
<p>� 17.0667 � 112.7041 � 187.7333�
N
</p>
<p>i�1
</p>
<p> (Yi � Y )
2�
</p>
<p>N
</p>
<p>i�1
</p>
<p> (Ŷi � Y )
2X
</p>
<p>YYYŶYŶŶ
</p>
<p>Table 15.3
</p>
<p>455</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>Percent of Variance Explained: 
</p>
<p>Unemployment Rates and Burglary Rates in California
</p>
<p>Returning to our example of unemployment rates and burglary rates, we
</p>
<p>can similarly examine how well unemployment rates help to explain
</p>
<p>burglary rates in California counties. The two key pieces of information
</p>
<p>we need are the explained sum of squares and the total sum of squares.
</p>
<p>The first column of Table 15.4 presents the burglary rates, and the sec-
</p>
<p>ond column presents the predicted value of the burglary rate for each
</p>
<p>county. The third and fourth columns present the calculations to obtain
</p>
<p>the explained sum of squares. Column 3 displays the difference be-
</p>
<p>tween the predicted value of the burglary rate ( ) and the mean of the
</p>
<p>burglary rate ( ), while column 4 presents the squared difference be-
</p>
<p>tween the predicted value and the mean. The explained sum of squares,
</p>
<p>which has a value of 1,364,425.7358 (see the bottom of column 4), is
</p>
<p>the sum of the squared differences between the predicted value of the
</p>
<p>burglary rate and the mean of the burglary rate. The calculations for the
</p>
<p>total sum of squares are presented in columns 5 and 6. The fifth column
</p>
<p>represents the difference between the observed burglary rate (Yi ) and
</p>
<p>the mean of the burglary rate ( ), while the sixth column contains the
</p>
<p>squared differences between the observed burglary rate and the mean
</p>
<p>of the burglary rate. The total sum of squares, which has a value of
</p>
<p>5,659,404.5114 (located at the bottom of column 6), is the sum of the
</p>
<p>squared differences between the observed burglary rate and the mean
</p>
<p>of the burglary rate.
</p>
<p>To obtain the value of R 2, we again use Equation 15.9, inserting our
</p>
<p>values for the explained and total sums of squares.
</p>
<p>Y
</p>
<p>Y
</p>
<p>Ŷi
</p>
<p>W orking It Out
</p>
<p> � 0.2411
</p>
<p> � 
1,364,425.7358
</p>
<p>5,659,404.5114
</p>
<p> R 2 � 
ESS
TSS
</p>
<p> � 
�
N
</p>
<p>i�1
</p>
<p> (Ŷi � Y  )
2
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Y )
2
</p>
<p>Our R 2 of 0.2411 indicates that about 24% of the variance in bur-
</p>
<p>glary rates in California counties is explained by the unemployment
</p>
<p>rate.
</p>
<p>456</p>
<p/>
</div>
<div class="page"><p/>
<p>Calculations for R2 Values for Unemployment 
Rates and Burglary Rates in California Counties
</p>
<p>Y i � ( i � )
2 Yi � (Yi � )
</p>
<p>2
</p>
<p>(1) (2) (3) (4) (5) (6)
</p>
<p>837.89 647.3188 �143.5720 20,612.9048 46.9993 2,208.9342
2,037.49 853.1092 62.2185 3,871.1455 1,246.5993 1,554,009.8148
</p>
<p>818.55 687.7419 �103.1488 10,639.6791 27.6593 765.0369
865.04 768.5881 �22.3026 497.4042 74.1493 5,498.1187
989.76 772.2630 �18.6277 346.9923 198.8693 39,548.9985
520.06 1,102.9977 312.1070 97,410.7607 �270.8307 73,349.2681
664.73 628.9446 �161.9461 26,226.5393 �126.1607 15,916.5222
</p>
<p>1,200.91 812.6861 21.7954 475.0395 410.0193 168,115.8264
509.87 662.0181 �128.8726 16,608.1548 �281.0207 78,972.6338
924.10 1,011.1269 220.2362 48,503.9926 133.2093 17,744.7176
845.29 930.2807 139.3900 19,429.5609 54.3993 2,959.2838
</p>
<p>1,027.79 753.8888 �37.0019 1,369.1391 236.8993 56,121.2783
1,526.40 1,378.6099 587.7192 345,413.8816 735.5093 540,973.9304
</p>
<p>511.12 728.1650 �62.7257 3,934.5122 �279.7707 78,271.6446
960.18 937.6303 146.7396 21,532.5161 169.2893 28,658.8671
649.22 1,000.1024 209.2117 43,769.5480 �141.6707 20,070.5872
</p>
<p>1,333.21 801.6616 10.7709 116.0125 542.3193 294,110.2232
361.24 775.9378 �14.9529 223.5892 �429.6507 184,599.7240
610.28 735.5147 �55.3760 3,066.5047 �180.6107 32,620.2250
929.32 941.3052 150.4145 22,624.5068 138.4293 19,162.6711
526.98 588.5215 �202.3692 40,953.3053 �263.9107 69,648.8576
775.92 790.6371 �0.2536 0.0643 �14.9707 224.1219
843.92 764.9133 �25.9774 674.8248 53.0293 2,812.1067
</p>
<p>1,214.69 1,007.4521 216.5614 46,898.8356 423.7993 179,605.8467
325.08 831.0603 40.1695 1,613.5927 �465.8107 216,979.6082
957.95 764.9133 �25.9774 674.8248 167.0593 27,908.8097
570.14 871.4834 80.5927 6,495.1801 �220.7507 48,730.8716
477.54 639.9691 �150.9216 22,777.3324 �313.3507 98,188.6612
455.37 669.3677 �121.5230 14,767.8322 �335.5207 112,574.1401
464.52 614.2453 �176.6454 31,203.6044 �326.3707 106,517.8338
646.12 636.2943 �154.5964 23,900.0593 �144.7707 20,958.5556
</p>
<p>1,030.58 849.4344 58.5437 3,427.3648 239.6893 57,450.9605
1,049.18 717.1405 �73.7502 5,439.0891 258.2893 66,713.3625
</p>
<p>925.61 673.0426 �117.8481 13,888.1841 134.7193 18,149.2898
845.75 812.6861 21.7954 475.0395 54.8593 3,009.5428
883.02 695.0915 �95.7992 9,177.4791 92.1293 8,487.8079
539.82 632.6194 �158.2713 25,049.7949 �251.0707 63,036.4964
744.81 628.9446 �161.9461 26,226.5393 �46.0807 2,123.4309
896.85 842.0847 51.1940 2,620.8297 105.9593 11,227.3733
540.79 636.2943 �154.5964 23,900.0593 �250.1007 62,550.3601
355.82 592.1963 �198.6944 39,479.4646 �435.0707 189,286.5140
444.07 662.0181 �128.8726 16,608.1548 �346.8207 120,284.5979
347.57 628.9446 �161.9461 26,226.5393 �443.3207 196,533.2430
647.73 750.2140 �40.6767 1,654.5947 �143.1607 20,494.9860
823.95 775.9378 �14.9529 223.5892 33.0593 1,092.9173
699.71 856.7841 65.8934 4,341.9349 �91.1807 8,313.9201
575.09 897.2072 106.3165 11,303.1960 �215.8007 46,569.9421
769.30 687.7419 �103.1488 10,639.6791 �21.5907 466.1583
555.44 617.9201 �172.9706 29,918.8250 �235.4507 55,437.0321
</p>
<p>1,057.99 904.5569 113.6661 12,919.9937 267.0993 71,342.0361
859.11 996.4276 205.5369 42,245.4173 68.2193 4,653.8729
816.55 764.9133 �25.9774 674.8248 25.6593 658.3997
676.23 941.3052 150.4145 22,624.5068 �114.6607 13,147.0761
</p>
<p>1,047.32 1,125.0467 334.1560 111,660.1989 256.4293 65,755.9859
908.79 757.5637 �33.3270 1,110.6923 117.8993 13,900.2449
491.86 695.0915 �95.7992 9,177.4791 �299.0307 89,419.3595
591.28 676.7174 �114.1733 13,035.5447 �199.6107 39,844.4316
</p>
<p>1,366.76 944.9800 154.0893 23,743.5062 575.8693 331,625.4507
</p>
<p>� 1,364,425.7358 � 5,659,404.5114�
N
</p>
<p>i�1
</p>
<p> (Yi � Y )
2�
</p>
<p>N
</p>
<p>i�1
</p>
<p> (Ŷi � Y )
2
</p>
<p>YYYŶYŶŶ
</p>
<p>Table 15.4</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>Statistical Significance of the Regression Coefficient: 
</p>
<p>The Case of Age and Number of Arrests
</p>
<p>In assessing the statistical significance of the regression coefficient, we
</p>
<p>once again use the t distribution, introduced in Chapter 10.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution of Y around each value of
</p>
<p>X (must be assumed because N is not large).
</p>
<p>Homoscedasticity.
</p>
<p>Linearity.
</p>
<p>Sampling Method: Independent random sampling.
</p>
<p>Sampling Frame: Youth in one U.S. city.
</p>
<p>Hypotheses:
</p>
<p>H0: Age does not predict the number of arrests over the last year in the
</p>
<p>population of young offenders ( � � 0).
</p>
<p>H1: Age does predict the number of arrests over the last year in the pop-
</p>
<p>ulation of young offenders ( � � 0).
</p>
<p>To test the significance of a regression coefficient, we assume that
</p>
<p>both variables are measured at the interval level. Because this is a para-
</p>
<p>metric test of significance, we must also make assumptions regarding
</p>
<p>the population distribution. For tests of statistical significance with b,
</p>
<p>we must assume that for each value of X the scores of Y are normally
</p>
<p>distributed around the regression line. We must also assume that the
</p>
<p>variances of the distribution of Y scores around each value of X are
</p>
<p>equal. This is the assumption of homoscedasticity, described in Chapter
</p>
<p>14. Researchers generally rely on the central limit theorem to relax as-
</p>
<p>sumptions of normality. As with analysis of variance and correlation,
</p>
<p>To visualize these assumptions in the case of regression, it is once
</p>
<p>again useful to look at a scatterplot. Suppose Figure 15.6 represents the
</p>
<p>scatterplot of the population of scores for age and number of arrests in
</p>
<p>joint distribution between X and Y that both is normal in form and meets
</p>
<p>the assumption of homoscedasticity. But here, our imaginary line is actu-
</p>
<p>ally the regression line. When we examine points above and below the
</p>
<p>regression line, we see that there is a clustering of points close to the
</p>
<p>458
</p>
<p>we are generally concerned only with marked violations of the homos-
</p>
<p>cedasticity assumption.
</p>
<p>assumptions in terms of the correlation coefficient, this figure illustrates a
</p>
<p>the city examined. Like Figure 14.7, which we used to examine these 
</p>
<p>line. Farther from the line, there are fewer points. This distribution is 
</p>
<p>basically normal in that the scores of Y for every value of X form a bell</p>
<p/>
</div>
<div class="page"><p/>
<p>E V A L U A T I N G T H E R E G R E S S I O N M O D E L
</p>
<p>shape that is highest on the regression line and then slopes down in nor-
</p>
<p>mal form away from the line. This is illustrated in the cross section for
</p>
<p>each age group.
</p>
<p>Also, the points around the regression line have about equal variances
</p>
<p>(homoscedasticity). That is, the spread of the Y scores around each X in
</p>
<p>this distribution is about equal, whether we look at the cases associated
</p>
<p>with the youngest subjects (on the left side of the scatterplot), those asso-
</p>
<p>ciated with average-age subjects (in the middle of the scatterplot), or those
</p>
<p>associated with the oldest offenders (on the right side of the scatterplot).
</p>
<p>Scatterplot Showing Normal Distribution and HomoscedasticityFigure 15.6
</p>
<p>16 17 18 19
</p>
<p>Age
</p>
<p>A
</p>
<p>Cross Sections:
</p>
<p>Arrests
</p>
<p>Age = 16
A
</p>
<p>N
u
</p>
<p>m
b
</p>
<p>er
 o
</p>
<p>f 
A
</p>
<p>rr
es
</p>
<p>ts
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>1 case
5 cases
10 cases
25 cases
50 cases
100 cases
</p>
<p>B
</p>
<p>C
</p>
<p>D
</p>
<p>Arrests
</p>
<p>Age = 17
B
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Arrests
</p>
<p>Age = 18
C
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Arrests
</p>
<p>Age = 19
D
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Regression Line
</p>
<p>459</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>In contrast, the scatterplot in Figure 15.7 shows a case that violates&mdash;
</p>
<p>rather than meets&mdash;the assumptions for our t-test. In this scatterplot, the
</p>
<p>points are not normally distributed around the regression line. Indeed,
</p>
<p>they form a type of bimodal distribution with peaks above and below
</p>
<p>the regression line (see the cross section for each age group). As
</p>
<p>discussed in Chapter 14, heteroscedasticity refers to unequal variances
</p>
<p>around the regression line. The scatterplot in Figure 15.7 also shows a
</p>
<p>distribution with unequal variances. For subjects aged 17 and 19, the
</p>
<p>scores of Y are scattered widely around the regression line. For subjects
</p>
<p>Scatterplot Showing Nonnormal Distribution and HeteroscedasticityFigure 15.7
</p>
<p>16 17 18 19
</p>
<p>Age
</p>
<p>A
</p>
<p>Cross Sections:
</p>
<p>Arrests
</p>
<p>Age = 16
A
</p>
<p>N
u
</p>
<p>m
b
</p>
<p>er
 o
</p>
<p>f 
A
</p>
<p>rr
es
</p>
<p>ts
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>100 cases
50 cases
25 cases
10 cases
5 cases
1 case B C
</p>
<p>D
</p>
<p>Arrests
</p>
<p>Age = 17
B
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Arrests
</p>
<p>Age = 18
C
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Arrests
</p>
<p>Age = 19
D
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>en
cy
</p>
<p>Regression Line
</p>
<p>460</p>
<p/>
</div>
<div class="page"><p/>
<p>E V A L U A T I N G T H E R E G R E S S I O N M O D E L
</p>
<p>aged 16 and 18, however, the scores of Y are tightly clustered around
</p>
<p>the regression line.
</p>
<p>In our example, we have only 15 cases, and thus we cannot invoke
</p>
<p>the central limit theorem. As in the case of Pearson&rsquo;s correlation coeffi-
</p>
<p>cient, we recommend that the N of cases be at least 30 before this is
</p>
<p>We also must assume linearity, which we examined in our scatterplot
</p>
<p>of the relationship between age and number of arrests . Finally, our null
</p>
<p>hypothesis is that age does not influence number of arrests over the last
</p>
<p>year for the population of young offenders from which our sample was
</p>
<p>drawn ( � � 0). Our research hypothesis is that age does influence num-
</p>
<p>ber of arrests in that population ( � � 0).
</p>
<p>The Sampling Distribution The sampling distribution is t, with N � 2 de-
</p>
<p>grees of freedom. For our example, df � 15 � 2 � 13.
</p>
<p>Significance Level and Rejection Region With a two-tailed 0.05 signifi-
</p>
<p>cance threshold, the critical value for the t-test (with 13 degrees of free-
</p>
<p>dom) is 2.160 (see Appendix 4). We will reject the null hypothesis if the
</p>
<p>t-score is greater than 2.160 or less than �2.160.
</p>
<p>The Test Statistic The t-test for the significance of the regression coeffi-
</p>
<p>cient is performed by taking the difference between the observed value
</p>
<p>of b and the hypothesized value � and then dividing that result by the
</p>
<p>standard error of b, or . The formula for the t-statistic is shown in
</p>
<p>Equation 15.10.
</p>
<p>Equation 15.10
</p>
<p>where b is the estimated regression coefficient, � is the hypothesized
</p>
<p>population value, and is the standard error of b. In practice, this for-
</p>
<p>mula can simply be written as b divided by the standard error of b, since
</p>
<p>the null hypothesis for � ordinarily is that it is equal to 0.
</p>
<p>�̂b
</p>
<p>t � 
b � �
</p>
<p>�̂b
</p>
<p>�̂b
</p>
<p>461
</p>
<p>done. Accordingly, our test results cannot be relied on unless the assump-
</p>
<p>tion of a normal distribution of Y around each value of X is true for the
</p>
<p>population to which we infer. Of course, we cannot make this assump-
</p>
<p>indication of the form of the population distribution. Based on the scat- 
</p>
<p>terplot shown in Figure 14.1, we concluded earlier that there was not
</p>
<p>evidence of serious violations of this assumption.
</p>
<p>tion without some prior knowledge of the population distribution of 
</p>
<p>scores. With regard to the assumption of homoscedasticity, as we noted in 
</p>
<p>Chapter 14, researchers generally use the scatterplot of sample cases as an </p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>To determine the standard error of b in a bivariate regression
</p>
<p>model (one with one dependent and one independent variable), we use
</p>
<p>Equation 15.11.5
</p>
<p>Equation 15.11
</p>
<p>i
</p>
<p>Table 15.5 presents the values for age and number of arrests for 15 of-
</p>
<p>fenders and illustrates the calculations necessary for obtaining the standard
</p>
<p>error of b. In the fourth column, we have calculated the difference between
</p>
<p>the observed value for number of arrests (Yi ) and the predicted value ( ).Ŷi
</p>
<p>ˆ
i
</p>
<p>�̂b � ��
N
</p>
<p>i�1
</p>
<p> (Yi � Ŷi )
2/(N � 2)
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X  )
2
</p>
<p>Calculations for the Standard Error of b for Age 
and Number of Arrests for 15 Young Offenders
</p>
<p>X Y Yi � i (Yi � i)
2 Xi � (Xi � )
</p>
<p>2
</p>
<p>(1) (2) (3) (4) (5) (6) (7)
</p>
<p>14 0 2.1046 �2.1046 4.4294 �3.0667 9.4046
13 1 1.2039 �0.2039 0.0416 �4.0667 16.5380
15 1 3.0053 �2.0053 4.0211 �2.0667 4.2712
13 2 1.2039 0.7961 0.6337 �4.0667 16.5380
14 2 2.1046 �0.1046 0.0109 �3.0667 9.4046
14 3 2.1046 0.8954 0.8017 �3.0667 9.4046
17 3 4.8066 �1.8066 3.2639 �0.0667 0.0044
19 4 6.6080 �2.6080 6.8015 1.9333 3.7376
21 4 8.4093 �4.4093 19.4420 3.9333 15.4708
19 6 6.6080 �0.6080 0.3696 1.9333 3.7376
16 8 3.9060 4.0940 16.7612 �1.0667 1.1378
18 9 5.7073 3.2927 10.8419 0.9333 0.8710
20 9 7.5086 1.4914 2.2242 2.9333 8.6042
21 10 8.4093 1.5907 2.5303 3.9333 15.4708
22 11 9.3100 1.6900 2.8562 4.9333 24.3374
</p>
<p>� 17.0667 � 75.0293 � 138.9326�
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2�
</p>
<p>N
</p>
<p>i�1
</p>
<p> (Yi � Ŷi)
2X
</p>
<p>XXŶŶŶ
</p>
<p>Table 15.5
</p>
<p>5It is important to note that this equation for the standard error of b is appropriate
</p>
<p>only if we have a bivariate regression model. If we have two or more independent
</p>
<p>variables, then a modified equation is necessary to calculate the standard error of the
</p>
<p>regression coefficients.
</p>
<p>462
</p>
<p>the observed value of the dependent variable (Y ) and the predicted
</p>
<p>value of the dependent variable (Y ) divided by the number of observa-
</p>
<p>tions (N) minus 2. The denominator is the measure of variability for
</p>
<p>the independent variable, X.
</p>
<p>The numerator represents the sum of the squared differences between</p>
<p/>
</div>
<div class="page"><p/>
<p>E V A L U A T I N G T H E R E G R E S S I O N M O D E L
</p>
<p>Once we have calculated the standard error of b, we can return to Equa-
</p>
<p>tion 15.10 and calculate a t-score for b to obtain our test statistic. The test
</p>
<p>statistic is 4.4195 for our example.6
</p>
<p>W orking It Out
</p>
<p> � 0.2038
</p>
<p> � �75.0293/(15 � 2)138.9326
</p>
<p> �̂b � ��
N
</p>
<p>i �1
</p>
<p> (Yi � Yi
ˆ )2/(N � 2)
</p>
<p>�
N
</p>
<p>i �1
</p>
<p> (Xi � X )
2
</p>
<p>Column 5 shows the squares of the differences, which are then summed at
</p>
<p>i
</p>
<p>value (
</p>
<p>appear in column 7. The sum of the squared deviations between age and
</p>
<p>mean age is 138.9326 (see the bottom of column 7). We insert these values
</p>
<p>into Equation 15.11 to calculate the standard error for b, which has a value
</p>
<p>of 0.2038.
</p>
<p>Ŷi
</p>
<p>W orking It Out
</p>
<p> � 4.4195
</p>
<p> � 
0.9007
0.2038
</p>
<p> t � 
b � �
</p>
<p>�̂b
</p>
<p>6Except for rounding error, this result is the same as the one we obtained in testing
</p>
<p>the significance of the correlation coefficient for this relationship (4.4195 vs. 4.4188).
</p>
<p>In practice, you could use the correlation coefficient significance test result for defin-
</p>
<p>ing the statistical significance of the regression coefficient. Indeed, in many texts only
</p>
<p>one formula is provided for both coefficients.
</p>
<p>463
</p>
<p>the bottom of the column. We find the sum of the squared deviations bet-
</p>
<p>ween the observed value for number of arrests (Y ) and the predicted
</p>
<p>appear in column 6, while the squared deviations between age and mean age
</p>
<p>) to be 75.0293. The deviations between age and mean age </p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>The Decision As 4.4195 is greater than our critical value of t (2.160), we
</p>
<p>reject the null hypothesis that age does not predict number of arrests.
</p>
<p>We conclude that there is a statistically significant relationship between
</p>
<p>age and number of arrests in the population of young offenders. How-
</p>
<p>ever, because we cannot strongly support the assumption of normality in
</p>
<p>this test or relax that assumption because N is large, we cannot place
</p>
<p>strong reliance on our test result.
</p>
<p>Testing the Statistical Significance of the Regression Coefficient for
</p>
<p>Unemployment Rates and Burglary Rates in California
</p>
<p>Let&rsquo;s assess the statistical significance of the regression coefficient in our
</p>
<p>example concerning unemployment rates and burglary rates from Cali-
</p>
<p>fornia counties. We again begin by stating the assumptions and hypothe-
</p>
<p>ses of our test.
</p>
<p>Assumptions:
</p>
<p>Level of Measurement: Interval scale.
</p>
<p>Population Distribution: Normal distribution of Y around each value of
</p>
<p>X (relaxed because N is large).
</p>
<p>Homoscedasticity.
</p>
<p>Linearity.
</p>
<p>Sampling Method: Independent random sampling (all counties are in-
</p>
<p>cluded in one year).
</p>
<p>Sampling Frame: Counties in California.
</p>
<p>Hypotheses:
</p>
<p>H0: Unemployment rates do not influence burglary rates in California
</p>
<p>counties ( � � 0).
</p>
<p>H1: Unemployment rates do influence burglary rates in California coun-
</p>
<p>ties ( � � 0).
</p>
<p>The Sampling Distribution The sampling distribution is the t distribution,
</p>
<p>with df � 58 � 2 � 56.
</p>
<p>Significance Level and Rejection Region Since the research hypothesis is
</p>
<p>nondirectional, we use a two-tailed test of statistical significance and set the
</p>
<p>significance level at 0.05. The critical values for the t-test with 56 degrees of
</p>
<p>freedom are about �2.003 and 2.003, meaning that we will reject the null
</p>
<p>hypothesis if the test statistic is less than �2.003 or greater than 2.003.
</p>
<p>The Test Statistic To test the statistical significance of our regression
</p>
<p>coefficient b, we again use Equation 15.11 to determine the standard
</p>
<p>error of b and Equation 15.10 to calculate the t-score. Table 15.6
</p>
<p>464</p>
<p/>
</div>
<div class="page"><p/>
<p>Calculations for the Standard Error of b for 
Unemployment Rates and Burglary Rates for 58 California Counties
</p>
<p>X Y Yi � i (Yi � i)
2 Xi � (Xi � )
</p>
<p>2
</p>
<p>(1) (2) (3) (4) (5) (6) (7)
</p>
<p>3.5 837.89 647.3188 190.5712 36,317.4013 �3.9069 15.2639
9.1 2,037.49 853.1092 1,184.3808 1,402,757.8083 1.6931 2.8666
4.6 818.55 687.7419 130.8081 17,110.7643 �2.8069 7.8787
6.8 865.04 768.5881 96.4519 9,302.9613 �0.6069 0.3683
6.9 989.76 772.2630 217.4970 47,304.9581 �0.5069 0.2569
</p>
<p>15.9 520.06 1,102.9977 �582.9377 339,816.3271 8.4931 72.1327
3.0 664.73 628.9446 35.7854 1,280.5949 �4.4069 19.4208
8.0 1,200.91 812.6861 388.2239 150,717.7965 0.5931 0.3518
3.9 509.87 662.0181 �152.1481 23,149.0352 �3.5069 12.2983
</p>
<p>13.4 924.10 1,011.1269 �87.0269 7,573.6848 5.9931 35.9172
11.2 845.29 930.2807 �84.9907 7,223.4123 3.7931 14.3876
</p>
<p>6.4 1,027.79 753.8888 273.9012 75,021.8564 �1.0069 1.0138
23.4 1,526.40 1,378.6099 147.7901 21,841.9077 15.9931 255.7792
</p>
<p>5.7 511.12 728.1650 �217.0450 47,108.5364 �1.7069 2.9135
11.4 960.18 937.6303 22.5497 508.4881 3.9931 15.9448
13.1 649.22 1,000.1024 �350.8824 123,118.4797 5.6931 32.4114
</p>
<p>7.7 1,333.21 801.6616 531.5484 282,543.6909 0.2931 0.0859
7.0 361.24 775.9378 �414.6978 171,974.2653 �0.4069 0.1656
5.9 610.28 735.5147 �125.2347 15,683.7226 �1.5069 2.2707
</p>
<p>11.5 929.32 941.3052 �11.9851 143.6438 4.0931 16.7535
1.9 526.98 588.5215 �61.5415 3,787.3525 �5.5069 30.3259
7.4 775.92 790.6371 �14.7171 216.5936 �0.0069 0.0000
6.7 843.92 764.9133 79.0067 6,242.0571 �0.7069 0.4997
</p>
<p>13.3 1,214.69 1,007.4521 207.2379 42,947.5513 5.8931 34.7286
8.5 325.08 831.0603 �505.9803 256,016.0134 1.0931 1.1949
6.7 957.95 764.9133 193.0367 37,263.1637 �0.7069 0.4997
9.6 570.14 871.4834 �301.3434 90,807.8327 2.1931 4.8097
3.3 477.54 639.9691 �162.4291 26,383.2093 �4.1069 16.8666
4.1 455.37 669.3677 �213.9977 45,795.0284 �3.3069 10.9356
2.6 464.52 614.2453 �149.7253 22,417.6595 �4.8069 23.1063
3.2 646.12 636.2943 9.8257 96.5452 �4.2069 17.6980
9.0 1,030.58 849.4344 181.1456 32,813.7284 1.5931 2.5380
5.4 1,049.18 717.1405 332.0395 110,250.2163 �2.0069 4.0276
4.2 925.61 673.0426 252.5674 63,790.3117 �3.2069 10.2842
8.0 845.75 812.6861 33.0639 1,093.2215 0.5931 0.3518
4.8 883.02 695.0915 187.9285 35,317.1061 �2.6069 6.7959
3.1 539.82 632.6194 �92.7994 8,611.7342 �4.3069 18.5494
3.0 744.81 628.9446 115.8654 13,424.7909 �4.4069 19.4208
8.8 896.85 842.0847 54.7653 2,999.2337 1.3931 1.9407
3.2 540.79 636.2943 �95.5043 9,121.0637 �4.2069 17.6980
2.0 355.82 592.1963 �236.3763 55,873.7552 �5.4069 29.2346
3.9 444.07 662.0181 �217.9481 47,501.3612 �3.5069 12.2983
3.0 347.57 628.9446 �281.3746 79,171.6655 �4.4069 19.4208
6.3 647.73 750.2140 �102.4840 10,502.9682 �1.1069 1.2252
7.0 823.95 775.9378 48.0122 2,305.1713 �0.4069 0.1656
9.2 699.71 856.7841 �157.0741 24,672.2603 1.7931 3.2152
</p>
<p>10.3 575.09 897.2072 �322.1172 103,759.4841 2.8931 8.3700
4.6 769.30 687.7419 81.5581 6,651.7269 �2.8069 7.8787
2.7 555.44 617.9201 �62.4801 3,903.7641 �4.7069 22.1549
</p>
<p>10.5 1,057.99 904.5569 153.4332 23,541.7315 3.0931 9.5673
13.0 859.11 996.4276 �137.3176 18,856.1233 5.5931 31.2828
</p>
<p>6.7 816.55 764.9133 51.6367 2,666.3478 �0.7069 0.4997
11.5 676.23 941.3052 �265.0752 70,264.8351 4.0931 16.7535
16.5 1,047.32 1,125.0467 �77.7267 6,041.4321 9.0931 82.6845
</p>
<p>6.5 908.79 757.5637 151.2263 22,869.4089 �0.9069 0.8225
4.8 491.86 695.0915 �203.2315 41,303.0589 �2.6069 6.7959
4.3 591.28 676.7174 �85.4374 7,299.5476 �3.1069 9.6528
</p>
<p>11.6 1,366.76 944.9800 421.7800 177,898.3853 4.1931 17.5821
</p>
<p>� 7.4069 � 4,294,978.7756 � 1,010.3570�
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2�
</p>
<p>N
</p>
<p>i�1
</p>
<p> (Yi � Ŷi)
2X
</p>
<p>XXŶŶŶ
</p>
<p>Table 15.6</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>presents the calculations necessary for calculating the standard error
</p>
<p>of b. Column 4 of Table 15.6 provides the difference between the ob-
</p>
<p>served burglary rate (Yi) and the predicted burglary rate ( i). Column
</p>
<p>5 shows the square of each of the differences; the squares are then
</p>
<p>summed at the bottom of the column. We find the sum of the squared
</p>
<p>deviations between the observed burglary rate and the predicted bur-
</p>
<p>glary rate to be 4,294,978.7756. The deviations between observed un-
</p>
<p>employment (Xi ) and mean unemployment appear in Column 6, while
</p>
<p>the squared deviations between unemployment and mean unemploy-
</p>
<p>ment appear in Column 7. The sum of the squared deviations between
</p>
<p>age and mean age is 1,010.3570 (see the bottom of Column 7). After
</p>
<p>inserting these values into Equation 15.11, we calculate a value of
</p>
<p>8.7126 for the standard error for b.
</p>
<p>Ŷ
</p>
<p>W orking It Out
</p>
<p> � 8.7126
</p>
<p> � �4,294,978.7756/(58 � 2)1,010.3570
</p>
<p> �̂b � ��
N
</p>
<p>i�1
</p>
<p> (Yi � Ŷi )
2/(N � 2)
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2
</p>
<p>The t-statistic is then calculated by inserting our values for b and the
</p>
<p>standard error of b into Equation 15.10. For our example, the test statistic
</p>
<p>is 4.2178.
</p>
<p>W orking It Out
</p>
<p> � 4.2178
</p>
<p> � 
36.7483 � 0
</p>
<p>8.7126
</p>
<p> t � 
b � �
</p>
<p>�̂b
</p>
<p>466</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E F - T E S T F O R T H E O V E R A L L R E G R E S S I O N
</p>
<p>The Decision Our test statistic of 4.2178 is greater than our critical 
</p>
<p>t-value of 2.003, leading us to reject the null hypothesis that unemploy-
</p>
<p>T h e  F - T e s t  f o r  t h e  O v e r a l l  R e g r e s s i o n
</p>
<p>In regression analysis, we can carry out a second type of test of statistical
</p>
<p>significance to evaluate whether the overall regression model contributes
</p>
<p>particularly useful when we have more than one independent variable in
</p>
<p>our model, a situation we will examine in Chapter 16. For this second
</p>
<p>test of statistical significance, we draw on the logic we used in develop-
</p>
<p>ing the measure of percent of variance explained, or R 2.
</p>
<p>Percent of variance explained tells how much our model improves
</p>
<p>our predictions beyond what can be learned from the mean. But another
</p>
<p>question is whether we can conclude from our sample R 2 that R 2 is in
</p>
<p>fact different from 0 in the population. This is the test of statistical signif-
</p>
<p>icance for the regression model overall. The assumptions for this test are
</p>
<p>the same as those described in the previous section.
</p>
<p>To test this hypothesis, we use analysis of variance, which was intro-
</p>
<p>Equation 15.12
</p>
<p>The explained sum of squares was discussed above. The unexplained
</p>
<p>sum of squares is simply the sum of the squared errors of the regression:
</p>
<p>Age and Number of Arrests
</p>
<p>The total sum of squares can be partitioned into its explained and unex-
</p>
<p>plained components. The total sum of squares for our example of age
</p>
<p>USS � �
N
</p>
<p>i �1
</p>
<p> (Yi � Y i
ˆ  )2
</p>
<p>F � 
ESS/df
USS/df
</p>
<p>467
</p>
<p>conclude that there is a statistically significant relationship between 
</p>
<p>unemployment rates and burglary rates.
</p>
<p>ment rates do not influence burglary rates in California counties and 
</p>
<p>duced in Chapter 12. Again, the F-test is based on a ratio of the explained
</p>
<p>variance to the unexplained variance. The explained and unexplained
</p>
<p>variance estimates are obtained by dividing the explained sum of squares
</p>
<p>(ESS) and unexplained sum of squares (USS) by their appropriate degrees
</p>
<p>of freedom, as shown in Equation 15.12.
</p>
<p>7 This test issignificantly to our understanding of the dependent variable  .
</p>
<p>7
</p>
<p>discussed in the prior section. We assume an interval scale, a normal distribution (relaxed
</p>
<p>While we do not state the assumption of the tests below formally, they follow those
</p>
<p>when N is large), homoscedasticity, linearity, and independent random sampling.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>sum of squares (112.7041) plus the unexplained sum of squares, or error
</p>
<p>sum of squares of the regression (75.0292).
</p>
<p>The number of degrees of freedom for the ESS (df1) is k, or the num-
</p>
<p>ber of variables in the regression. In our example, the regression in-
</p>
<p>cludes only one independent variable&mdash;age&mdash;and thus the number of
</p>
<p>degrees of freedom for the ESS is 1. For the USS, the number of degrees
</p>
<p>of freedom (df2) is equal to N � k � 1. In the case of our example of
</p>
<p>number of arrests and age, it is equal to 15 � 1 � 1, or 13. The F-statistic
</p>
<p>for our regression is thus calculated by dividing the ratio of the ex-
</p>
<p>plained variance (112.7041/1) by that of the unexplained variance
</p>
<p>(75.0292/13), obtaining F � 19.53.
</p>
<p>W orking It Out
</p>
<p> � 19.5278
</p>
<p> � 
112.7041
5.7715
</p>
<p> � 
112.7041/1
75.0292/13
</p>
<p> F � 
ESS/df
USS/df
</p>
<p>Setting a 5% significance threshold, we can see in the F table in Ap-
</p>
<p>pendix 5 that the critical value associated with 1 and 13 degrees of free-
</p>
<p>dom is 4.67. If our F-statistic is larger than this value, then our observed
</p>
<p>significance level is less than the criterion significance level we set for
</p>
<p>our test. Our test statistic of 19.53 is much larger than this value, and
</p>
<p>W orking It Out
</p>
<p> 187.7333 � 112.7041 � 75.0292
</p>
<p> �
N
</p>
<p>i�1
</p>
<p> (Yi � Y
 )2 � �
</p>
<p>N
</p>
<p>i�1
</p>
<p>(Yi
ˆ � Y )2 � �
</p>
<p>N
</p>
<p>i�1
</p>
<p>(Yi � Ŷ
 
</p>
<p>i)
2
</p>
<p> TSS � ESS � USS
</p>
<p>468
</p>
<p>and number of arrests is 187.7333, which is equivalent to the explained</p>
<p/>
</div>
<div class="page"><p/>
<p>T H E F - T E S T F O R T H E O V E R A L L R E G R E S S I O N
</p>
<p>plained by the regression line in the population to which we infer is 0.
</p>
<p>Unemployment Rates and Burglary Rates in California
</p>
<p>The F-test may also be used to assess the overall regression for the rela-
</p>
<p>tionship between unemployment rates and burglary rates in California.
</p>
<p>We reported in our calculations of R 2 that the value for the total sum of
</p>
<p>squares is 5,659,404.5114 and the value for the explained sum of squares
</p>
<p>is 1,364,425.7358, which means that the value for the unexplained sum
</p>
<p>of squares is 4,294,978.7756.
</p>
<p>We have likely violated the normality assumption of our test because we do not 
</p>
<p>have knowledge about the shape of the joint distribution of age and number of arrests
</p>
<p>in the population and N � 15 cases is not enough to safely invoke the central limit
</p>
<p>theorem.
</p>
<p>W orking It Out
</p>
<p> 5,659,404.5114 � 1,364,425.7358 � 4,294,978.7756
</p>
<p> TSS � ESS � USS
</p>
<p>The number of degrees of freedom for the ESS (df1) is k � 1, and the
</p>
<p>number of degrees of freedom for the USS (df2) is N � k � 1, which is
</p>
<p>58 � 1 � 1 � 56. The F-statistic for our regression of burglary rates on
</p>
<p>unemployment rates is calculated by dividing the ratio of the explained
</p>
<p>variance (1,364,425.7358/1) by that of the unexplained variance
</p>
<p>(4,294,978.7756/56), which gives us F � 17.7900.
</p>
<p>W orking It Out
</p>
<p> � 17.7900
</p>
<p> � 
1,364,425.7358
</p>
<p>76,696.0496
</p>
<p> � 
1,364,425.7358/1
</p>
<p>4,294,978.7756/56
</p>
<p> F � 
ESS/df
USS/df
</p>
<p>469
</p>
<p>thus we would reject the null hypothesis that the percent of variance ex-
8
</p>
<p>8</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>If we set a 5% significance threshold, we see in the F table (Appendix 5)
</p>
<p>that the critical value associated with 1 and 56 degrees of freedom is not
</p>
<p>given in the table. Interpolating from the values for 40 degrees of free-
</p>
<p>dom (4.008) and 60 degrees of freedom (4.000) for df2, we estimate a
</p>
<p>critical value of 4.002. Our test statistic of 17.79 is much larger than the
</p>
<p>critical value, so we reject the null hypothesis that the percent of vari-
</p>
<p>ance explained by the regression line in the population to which we
</p>
<p>infer is 0.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>The regression coefficient b tells us how much one variable (the in-
</p>
<p>dependent variable, X) influences another variable (the dependent
</p>
<p>variable, Y ). The regression coefficient is expressed in specific units of
</p>
<p>the dependent variable and is interpreted as follows: A change of one
</p>
<p>unit in X produces a change of b units in the estimated value of Y.
</p>
<p>A researcher cannot predict values of Y using the regression coeffi-
</p>
<p>cient alone. The additional piece of information required is the Y-
</p>
<p>intercept (b0). The b0 coefficient may be interpreted as the expected
</p>
<p>value of Y when X � 0. The predicted value of Y for other values of 
</p>
<p>X can be calculated by adding b0 to the product of the regression
</p>
<p>coefficient and X. Regression error is the difference between the
</p>
<p>predicted and actual values of Y. The regression line is the line 
</p>
<p>for which the sum of the squared errors is at a minimum&mdash;hence 
</p>
<p>the name ordinary least squares regression (OLS). OLS regression
</p>
<p>is a solid basis for prediction within, but not beyond, the sample
</p>
<p>range.
</p>
<p>The R2 statistic is the proportion of the total sum of squares (Y � )2
</p>
<p>accounted for by the explained sum of squares . This propor-
</p>
<p>tion represents the improvement in predicting Y that the regression line
</p>
<p>provides over the mean of Y.
</p>
<p>sured on an interval scale. There is also an assumption of normality and
</p>
<p>a requirement of homoscedasticity. These assumptions relate to the dis-
</p>
<p>tribution of Y around each value of X. The researcher must also assume
</p>
<p>2 that R 2
</p>
<p>(Ŷ � Y   )2
Y
</p>
<p>470
</p>
<p>regression coefficient b. It is assumed that the variables examined are mea-
</p>
<p>linearity. The F-test for the overall regression determines whether the
</p>
<p>The t distribution may be used to test statistical significance for the
</p>
<p>in the population.
</p>
<p>researcher can conclude from the sample R is different from 0 
</p>
<p>i
</p>
<p>i</p>
<p/>
</div>
<div class="page"><p/>
<p>S Y M B O L S A N D F O R M U L A S
</p>
<p>K e y  T e r m s
</p>
<p>bivariate regression A technique for pre-
</p>
<p>dicting change in a dependent variable
</p>
<p>using one independent variable.
</p>
<p>dependent variable (Y) The variable as-
</p>
<p>sumed by the researcher to be influenced
</p>
<p>by one or more independent variables.
</p>
<p>independent variable (X) A variable
</p>
<p>assumed by the researcher to have an impact
</p>
<p>on the value of the dependent variable, Y.
</p>
<p>OLS regression See ordinary least squares
</p>
<p>regression analysis.
</p>
<p>ordinary least squares regression
</p>
<p>analysis A type of regression analysis in
</p>
<p>which the sum of squared errors from the
</p>
<p>regression line is minimized.
</p>
<p>percent of variance explained (R2) A
</p>
<p>measure for evaluating how well the re-
</p>
<p>gression model predicts values of Y. It rep-
</p>
<p>resents the improvement in predicting Y
</p>
<p>that the regression line provides over the
</p>
<p>mean of Y.
</p>
<p>regression coefficient b A statistic used
</p>
<p>to assess the influence of an independent
</p>
<p>variable, X, on a dependent variable, Y.
</p>
<p>The regression coefficient b is interpreted
</p>
<p>as the estimated change in Y that is associ-
</p>
<p>ated with a one-unit change in X.
</p>
<p>regression error (e) The difference be-
</p>
<p>tween the predicted value of Y and the 
</p>
<p>regression line The line predicting values
</p>
<p>of Y. The line is plotted from knowledge of
</p>
<p>the Y-intercept and the regression coeffi-
</p>
<p>cient.
</p>
<p>regression model The hypothesized
</p>
<p>statement by the researcher of the factor or
</p>
<p>factors that define the value of the depen-
</p>
<p>dent variable, Y. The model is normally 
</p>
<p>Y-intercept (b0) The expected value of Y
</p>
<p>when X � 0. The Y-intercept is used in
</p>
<p>predicting values of Y.
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>b0 Y-intercept
</p>
<p>�0 Y-intercept for population model
</p>
<p>b Regression coefficient
</p>
<p>� Regression coefficient for the population
</p>
<p>X Independent variable
</p>
<p>Y Dependent variable
</p>
<p>e Error
</p>
<p>� Error for population model
</p>
<p>ESS Explained sum of squares
</p>
<p>471
</p>
<p>actual value of Y.
</p>
<p>expressed in equation form.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>USS Unexplained sum of squares
</p>
<p>TSS Total sum of squares
</p>
<p>k Number of variables in the overall regression model
</p>
<p>Standard error of the regression coefficient
</p>
<p>To determine the value of the Y-intercept:
</p>
<p>b0 �
</p>
<p>To predict values of the dependent variable, Y:
</p>
<p>To identify the regression error:
</p>
<p>e � Y �
</p>
<p>To show that the sum of squared error in an OLS regression line is a
</p>
<p>minimum:
</p>
<p>� minimum
</p>
<p>A bivariate regression model for a sample:
</p>
<p>Y � b0 � bX � e
</p>
<p>A bivariate regression model for a population:
</p>
<p>Y � �0 � �X � �
</p>
<p>To calculate the percent of explained variance:
</p>
<p>R 2 �
</p>
<p>To calculate the value of t for the regression coefficient b:
</p>
<p>t �
</p>
<p>To calculate the standard error of the regression coefficient:
</p>
<p>�̂b � ��
N
</p>
<p>i�1
</p>
<p> (Yi � Ŷi )
2/(N � 2)
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2
</p>
<p>b � �
</p>
<p>�̂b
</p>
<p>ESS
TSS
</p>
<p> � 
�
N
</p>
<p>i�1
</p>
<p> (Ŷi � Y )
2
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Y )
2
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Ŷi)
2
</p>
<p>Ŷ
</p>
<p>Ŷ � b0 � bX
</p>
<p>Y � bX
</p>
<p>�̂b
</p>
<p>472</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S
</p>
<p>To calculate the value of F for the overall regression:
</p>
<p>F �
</p>
<p>E x e r c i s e s
</p>
<p>15.1 A researcher carries out a series of regression analyses for different
studies. The results of three of the studies are given below. In each
case, explain what the results mean in plain English.
</p>
<p>a. X � number of prior driving offenses; Y � fine in dollars imposed
by magistrate; b � 72.
</p>
<p>b. X � number of times a household has been broken into prior 
to purchase of first burglar alarm; Y � amount of money 
in dollars spent by homeowner on first burglar alarm; 
b � 226.
</p>
<p>c. X � number of times subject has been involved in a car accident;
Y � estimated average speed of subject when driving on a freeway
in miles per hour; b � �8.5.
</p>
<p>15.2 Nine adolescents are interviewed about the number of hours 
per week they work and the number of times they have 
smoked marijuana in the last year. The results are recorded 
as follows:
</p>
<p>X: Number of hours worked per week 0 10 10 15 5 30 20 40 15
</p>
<p>Y: Number of times smoked marijuana 
in the last year 1 3 2 5 0 13 10 20 25
</p>
<p>a. Calculate the regression coefficient b, and explain what it means in
plain English.
</p>
<p>b. Calculate the value of the Y-intercept, b0.
</p>
<p>c. Calculate a test of statistical significance for the regression coeffi-
cient b.
</p>
<p>15.3 A study of sentencing decisions hypothesized that judges would
become increasingly lenient with drug offenders as they accumulated
more years of experience. To test this hypothesis, researchers
gathered data on a sample of 12 drug offenders. The data included
information on number of years on the bench for the judge and 
</p>
<p>ESS/df
USS/df
</p>
<p> � 
�
N
</p>
<p>i�1
</p>
<p> (Ŷi � Y )
2/k
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Ŷi
 )2/(N � k � 1)
</p>
<p>473</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>length of the sentence (in months). The results were recorded as
follows:
</p>
<p>X: Number of years as a judge 3 1 0 1 2 5 9 13 17 0 6 2
</p>
<p>Y: Length of sentence (months) 14 22 24 20 15 12 3 6 18 18 10 18
</p>
<p>a. Calculate the regression coefficient b, and explain what it means in
plain English.
</p>
<p>b. Calculate the value of the Y-intercept, b0.
</p>
<p>c. Calculate a test of statistical significance for the regression coeffi-
cient b.
</p>
<p>15.4 Ten police officers are asked how many promotions they have re-
ceived and how many years they have served on the force. The results
are recorded below:
</p>
<p>X: Years on the force 7 1 5 3 12 2 4 1 9 6
</p>
<p>Y: Number of promotions 5 1 3 1 8 1 2 0 7 2
</p>
<p>a. Calculate the regression coefficient b.
</p>
<p>b. Calculate the value of the Y-intercept, b0.
</p>
<p>c. How many promotions would you predict for an officer who had
served 10 years on the force?
</p>
<p>d. What is the regression error in predicting the number of
promotions for an officer who has served 12 years on the 
force?
</p>
<p>15.5 Ten prosecutors were asked what percentage of their cases ended in
guilty pleas and how many years of experience each had as a prose-
cutor. The results were recorded as
</p>
<p>X: Number of years of experience 
as a prosecutor 10 12 8 1 0 2 7 20 5
</p>
<p>Y: Percentage of cases resulting in 
a guilty plea 93 90 87 72 70 70 82 97 94
</p>
<p>a. Calculate the regression coefficient b, and explain what it means in
plain English.
</p>
<p>b. Calculate the value of the Y-intercept, b0.
</p>
<p>c. Calculate a test of statistical significance for the regression coeffi-
cient b.
</p>
<p>d. If a prosecutor had six years of experience, what would be the pre-
dicted percentage of guilty pleas?
</p>
<p>15.6 A study exploring the link between aggression and crime reported ag-
gression scores and number of arrests for six individuals who partici-
</p>
<p>474</p>
<p/>
</div>
<div class="page"><p/>
<p>E X E R C I S E S
</p>
<p>pated in the study. The values for aggression and number of arrests
are as follows:
</p>
<p>X: Aggression score 92 63 77 29 51 10
</p>
<p>Y: Number of arrests 6 2 3 1 2 0
</p>
<p>a. Calculate the regression coefficient b, and explain what it means in
plain English.
</p>
<p>b. Calculate the value of the Y-intercept, b0.
</p>
<p>c. Calculate a test of statistical significance for the regression
coefficient b.
</p>
<p>d. What would be the predicted number of arrests for a person with
an aggression score of 75?
</p>
<p>e. What would be the predicted number of arrests for a person with
an aggression score of 25?
</p>
<p>15.7 For the last ten convicts released from Wilmslow Prison, Joan
recorded the percentage of their initial sentence from which they were
excused. She also recorded the number of times each convict was
called before a disciplinary committee over the course of his sentence.
The scores of each subject on these two variables are listed below:
</p>
<p>X: Number of disciplinary hearings 0 5 2 1 6 4 4 0 5 3
</p>
<p>Y: Percentage of sentence not served 33 5 18 32 0 10 5 30 0 17
</p>
<p>a. Calculate the regression coefficient b.
</p>
<p>b. Calculate the value of the Y-intercept, b0.
</p>
<p>c. Using the data provided, show that the sum of the error on either
side of the regression line equals 0:
</p>
<p>d. Using the data provided, show that
</p>
<p>is less than
</p>
<p>e. Explain in plain English the meaning of what you showed in part d
for the regression model.
</p>
<p>15.8 In running a small pilot study for a large-scale research project,
George gathers data on the average number of homicides monthly for
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Y )
2
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Ŷ
 )2
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Yi � Y ̂) � 0
</p>
<p>475</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N
</p>
<p>five U.S. cities. While he is looking for a good predictor of the differ-
ent homicide rates, he stumbles across the following set of data on the
number of theaters in each of the cities:
</p>
<p>X: Number of theaters 1 3 6 7 8
</p>
<p>Y: Homicides monthly 10 14 23 26 32
</p>
<p>a. Calculate the regression coefficient b.
</p>
<p>b. Calculate the value of the Y-intercept, b0.
</p>
<p>c. According to this regression model, how many homicides would a
city with ten theaters expect per month?
</p>
<p>d. Why is this model misleading?
</p>
<p>15.9 Lee is investigating six recent cases of vandalism in the local shopping
mall. She compares the amount of damage done in each case with the
number of vandals involved in each incident. Her findings are as follows:
</p>
<p>X: Number of vandals 3 2 6 4 1 2
</p>
<p>Y: Damage done ($) 1,100 1,850 3,800 3,200 250 1,200
</p>
<p>a. Calculate the regression coefficient b.
</p>
<p>b. Calculate the value of the Y-intercept, b0.
</p>
<p>c. Plot the scores on a scatterplot, and draw the regression line where
you think it should go.
</p>
<p>d. Calculate the value of R 2. What does this tell you about the model?
</p>
<p>15.10 Refer to the data from Exercise 15.2.
</p>
<p>a. Calculate the value of R 2. What does it tell you about the model?
</p>
<p>b. Run an F-test for the overall regression. Remember to outline all of
the steps required in a test of statistical significance, including any
violations of your assumptions. Can you conclude that the percent
of explained variance (R 2) is different from 0 for the population?
</p>
<p>15.11 Refer to the data from Exercise 15.5.
</p>
<p>a. Calculate the value of R 2. What does it tell you about the model?
</p>
<p>b. Run an F-test for the overall regression. Remember to outline all of
the steps required in a test of statistical significance, including any
violations of your assumptions. Can you conclude that the percent
of explained variance (R 2) is different from 0 for the population?
</p>
<p>15.12 Refer to the data from Exercise 15.9.
</p>
<p>a. Calculate the value of R 2. What does it tell you about the model?
</p>
<p>b. Run an F-test for the overall regression. Remember to outline all of
the steps required in a test of statistical significance, including any
violations of your assumptions. Can you conclude that the percent
of explained variance (R 2) is different from 0 for the population?
</p>
<p>476</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 477
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>Basic OLS regression analysis is the core of many statistical packages, and SPSS 
</p>
<p>and Stata have features that make obtaining basic results straightforward but have 
</p>
<p>also added a number of options and features for obtaining much more complex 
</p>
<p>results. We discuss some of these features and options in subsequent chapters. 
</p>
<p>As before, there are example syntax files that illustrate the commands below for 
</p>
<p>both SPSS (Chapter_15.sps) and Stata (Chapter_15.do).
</p>
<p>SPSS
</p>
<p>Ordinary least squares regression analyses are performed with the 
</p>
<p>REGRESSION command and is generally self-explanatory:
</p>
<p>The output from executing this command will present three tables of results that 
</p>
<p>contain the statistics discussed in this chapter. The first table of results, labeled 
</p>
<p>&ldquo;Model Summary,&rdquo; presents the value for R2. The second table of results, labeled 
</p>
<p>&ldquo;ANOVA,&rdquo; contains the ANOVA table that was originally discussed in Chapter 
</p>
<p>12. In this table, you will find the values for the explained, unexplained, and 
</p>
<p>total sums of squares; the F-statistic; and the observed significance level of F. 
</p>
<p>Please note, however, that the labels for the sums of squares are different. The 
</p>
<p>explained sum of squares is labeled &ldquo;Regression,&rdquo; and the unexplained sum of 
</p>
<p>squares is labeled &ldquo;Residual.&rdquo; The third table of results presents the regression 
</p>
<p>coefficients, standard errors of the coefficients, t-statistic for each coefficient, 
</p>
<p>and observed significance level for each regression coefficient. Note, too, that 
</p>
<p>there is a distinction between unstandardized and standardized regression coef-
</p>
<p>ficients. The unstandardized coefficients are the regression coefficients presented 
</p>
<p>in this chapter. We discuss standardized regression coefficients in Chapter 16.
</p>
<p>It is also possible to have SPSS compute predicted values and residuals for 
</p>
<p>each observation in the data file. To obtain one or both of these values, you will 
</p>
<p>need to add a /SAVE line to the command line:
</p>
<p>where /SAVE PRED RESID will save the predicted values (PRED) and the 
</p>
<p>residuals (RESID) to the data file, where they will appear as new variables  
</p>
<p>(i.e., additional columns of data on the far right of the spreadsheet). The default 
</p>
<p>REGRESSION
</p>
<p>/DEPENDENT dep_var_name
</p>
<p>/METHOD = ENTER indep_var_name.
</p>
<p>REGRESSION
</p>
<p>/DEPENDENT dep_var_name
</p>
<p>/METHOD = ENTER indep_var_name
</p>
<p>/SAVE PRED RESID.</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N478
</p>
<p>variable name for the predicted values is PRE_1, and default variable name  
</p>
<p>for the residuals is RES_1. We will return to this command in the computer 
</p>
<p>exercises for Chapter 16.
</p>
<p>Stata
</p>
<p>The estimation of OLS regression models uses the regress command:
</p>
<p>The output generated by the regress command is compact but includes an 
</p>
<p>ANOVA table, a listing of model summary statistics (e.g., R2), and a table of 
</p>
<p>results presenting the coefficients, standard errors, t-statistics for the coeffi-
</p>
<p>cients, an observed significance level, and something labeled a &ldquo;95 % Confidence 
</p>
<p>Interval.&rdquo; We discuss confidence intervals in Chapter 20.
</p>
<p>To compute predicted values and residuals in Stata, you will use the predict 
</p>
<p>command immediately following a regress command. The default output for pre-
</p>
<p>dict is to compute the predicted value; the residuals are designated with an option.
</p>
<p>To request predicted values:
</p>
<p>In contrast to SPSS, which will automatically generate a new variable name to 
</p>
<p>contain the predicted values or residuals, Stata requires that the user enter a new 
</p>
<p>variable name. If you wanted to use the same convention as SPSS, this command 
</p>
<p>would appear as
</p>
<p>To request residuals, run another predict command immediately following the 
</p>
<p>previous line for predicted values:
</p>
<p>The option r that appears after the comma indicates to Stata to compute the 
</p>
<p>residuals rather than some other quantity. Again, if you wanted to use the same 
</p>
<p>convention as in SPSS for naming these variables, this command would look like
</p>
<p>It is important to bear in mind that if you run several different models, you will 
</p>
<p>need to continually modify the variable names for the predicted values and  
</p>
<p>residuals. Stata will not allow you to overwrite existing variables.
</p>
<p>Problems
</p>
<p> 1. Open the California UCR data file (caucr_99.sav or caucr_99.dta),  
</p>
<p>which contains the data presented in Table 14.8. Run the linear regres-
</p>
<p>sion command in SPSS, using burglary rate as the dependent variable 
</p>
<p>and unemployment rate as the independent variable. Note that the values 
</p>
<p>reported in the three tables of  results match those reported in the text.
</p>
<p>regress dep_var_name indep_var_name
</p>
<p>predict new_variable_name
</p>
<p>predict PRE_1
</p>
<p>predict new_variable_name, r
</p>
<p>predict RES_1, r</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R E X E R C I S E S 479
</p>
<p> 2. Enter the data from Exercise 15.2.
</p>
<p>a. Run the linear regression command, using number of  times smoked 
</p>
<p>marijuana as the dependent variable and number of  hours worked per 
</p>
<p>week as the independent variable.
</p>
<p>b. Compare your answers to Exercises 15.2 and 15.10 with the results 
</p>
<p>produced by SPSS.
</p>
<p> 3. Enter the data from Exercise 15.3.
</p>
<p>a. Run the linear regression command, using sentence length as the 
</p>
<p>dependent variable and number of  years as a judge as the independent 
</p>
<p>variable.
</p>
<p>b. What is the value of  R2?
</p>
<p>c. Perform an F-test for the overall regression. Outline all of  the steps 
</p>
<p>your assumptions. Can you conclude that the percent of  explained  
</p>
<p>variance (R2) is different from 0 for the population?
</p>
<p> 4. Enter the data from Exercise 15.7.
</p>
<p>a. Run the linear regression command, using percentage of  sentence not 
</p>
<p>served as the dependent variable and number of  disciplinary hearings as 
</p>
<p>the independent variable.
</p>
<p>b. What is the value of  R2?
</p>
<p>c. Perform an F-test for the overall regression. Outline all of  the steps 
</p>
<p>your assumptions. Can you conclude that the percent of  explained  
</p>
<p>variance (R2) is different from 0 for the population?
</p>
<p> 5. Open the NYS data file (nys_1.sav, nys_1_student.sav, or nys_1.dta). Use 
</p>
<p>the linear regression command, and run regression analyses for the pairs 
</p>
<p>of  variables listed below. Do the following for each pair of  variables:
</p>
<p>Perform a t
</p>
<p>Report the value of  R2.
</p>
<p>Perform an F-test for the overall regression.
</p>
<p>a. Age (X) and number of  thefts valued at less than $5 in the last year (Y).
</p>
<p>b. Number of  times drunk (X) and number of  thefts valued at $5&ndash;$50 
</p>
<p>in the last year (Y).
</p>
<p>c. Frequency of  marijuana use (X) and number of  times the youth has 
</p>
<p>hit other students in the last year (Y).</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R F I F T E E N :  B I V A R I A T E R E G R E S S I O N480
</p>
<p>d. Number of  times the youth has hit a parent (X) and number of  
</p>
<p>thefts valued at more than $50 in the last year (Y).
</p>
<p>e. Number of  times the youth has been beaten up by a parent (X) and 
</p>
<p>number of  times the youth has hit a teacher in the last year (Y).
</p>
<p> 6. Open the Pennsylvania Sentencing data file (pcs_98.sav or pcs_98.dta). 
</p>
<p>Use the linear regression command, and run regression analyses for the 
</p>
<p>pairs of  variables listed below. Do the following for each pair of  variables:
</p>
<p>Perform a t
</p>
<p>Report the value of  R2.
</p>
<p>Perform an F-test for the overall regression.
</p>
<p>a. Age (X) and length of  incarceration sentence (Y).
</p>
<p>b. Prior criminal history score (X) and length of  incarceration  
</p>
<p>sentence (Y).
</p>
<p>c. Offense severity score (X) and length of  incarceration sentence (Y).</p>
<p/>
</div>
<div class="page"><p/>
<p>Multivariate Regression
</p>
<p>How Does the Researcher Try to Correctly Specify the Model?
</p>
<p>How Can You Include Nominal and Ordinal Variables in a Regression Model?
</p>
<p>How Do You Compare Regression Coefficients?
</p>
<p>C h a p t e r  s i x t e e n
</p>
<p>S p e c i f i c a t i o n :  b u i l d i n g  a  m u l t i v a r i a t e  m o d e l
</p>
<p>M o d e l  b u i l d i n g
</p>
<p>What is a Correctly Specified Model?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_16,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>ONE OF THE STATISTICAL TOOLS most commonly used in criminal justice
and criminology is regression modeling. A regression model allows the
</p>
<p>researcher to take a broad approach to criminological research problems.
</p>
<p>It is based not simply on understanding the relationships among vari-
</p>
<p>ables, but on specifying why changes occur and what factors are directly
</p>
<p>responsible for these changes. In a regression model, the researcher tries
</p>
<p>to disentangle the various potential factors that have an impact on the
</p>
<p>dependent variable, in order to provide an accurate picture of which
</p>
<p>variables are in fact most important in causing change.
</p>
<p>In this chapter, we discuss why it is generally necessary to take into
</p>
<p>account more than just one independent variable in building a regres-
</p>
<p>sion model. Previous chapters have focused on bivariate statistical analy-
</p>
<p>sis, in which we relate two variables&mdash;nominal, ordinal, or interval&mdash;to
</p>
<p>T h e  I m p o r t a n c e  o f  C o r r e c t  M o d e l  S p e c i f i c a t i o n s
</p>
<p>The most important assumption we make in regression modeling is that the
</p>
<p>model we have estimated is specified correctly. A correctly specified re-
</p>
<p>gression model is one in which the researcher has taken into account all
</p>
<p>of the relevant predictors of the dependent variable and has measured
</p>
<p>them accurately. This requirement of regression modeling is the most diffi-
</p>
<p>cult one that researchers face. Its importance is linked both to prediction of
</p>
<p>the dependent variable and to correct estimation of regression coefficients.
</p>
<p>Errors in Prediction
</p>
<p>Predictions of Y in regression are based on the factors that are included
</p>
<p>in a regression model. So far, we have examined bivariate regression
</p>
<p>models, in which one independent variable is used to predict values of
</p>
<p>Y. But in the real world it is unlikely that only one variable will influence
</p>
<p>the dependent measure you are examining. Most often, it will be neces-
</p>
<p>sary to take into account a number of independent variables. Regression
</p>
<p>statistical model.
</p>
<p>researcher takes into account a series of independent variables within one
</p>
<p>each other. This chapter introduces multivariate analysis, in which the 
</p>
<p>482</p>
<p/>
</div>
<div class="page"><p/>
<p>analysis that takes into account more than one independent variable is
</p>
<p>called multivariate regression analysis. The regression model we have
</p>
<p>discussed so far can be extended to the multivariate case simply by
</p>
<p>adding a term for each new variable. For example, to include years of
</p>
<p>education in the model predicting number of arrests presented earlier,
</p>
<p>we would express our regression equation as follows:
</p>
<p>Yarrests � b0 � b1(age) � b2(education) � e
</p>
<p>The population model for this equation would be written as
</p>
<p>Yarrests � �0 � �1(age) � �2(education) � �
</p>
<p>Sometimes, when examining multiple independent variables, re-
</p>
<p>searchers find it tedious to include the names of the variables in sub-
</p>
<p>scripts. Accordingly, they will often use a general form of the regression
</p>
<p>equation and then define each variable in a table or in their description
</p>
<p>of results. For example, the above equation could be expressed in terms
</p>
<p>of the population parameters as 
</p>
<p>Model 1: Y � �0 � �1X1 � �2X2 � �
</p>
<p>where Y � arrests
</p>
<p>X1 � age
</p>
<p>X2 � years of education
</p>
<p>In theory, you could define all relevant predictors of Y and include them
</p>
<p>all in your regression model. This correctly specified model would also
</p>
<p>provide the most accurate predictions of Y. Conversely, a misspecified
</p>
<p>model, or one that does not include all relevant predictors, will provide
</p>
<p>biased predictions of Y.
</p>
<p>Let&rsquo;s say, for example, that family median income is also an important
</p>
<p>predictor of arrests. In this case, the corrected population regression
</p>
<p>equation would be written as follows:
</p>
<p>Model 2: Y � �0 � �1X1 � �2X2 � �3X3 � �
</p>
<p>where Y � arrests
</p>
<p>X1 � age
</p>
<p>X2 � years of education
</p>
<p>X3 � family median income
</p>
<p>By adding this additional variable, we improve our predictions of Y over
</p>
<p>those provided by Model 1. Because we have taken into account the in-
</p>
<p>fluence of family income on arrests, we have added to our ability to cor-
</p>
<p>rectly predict the dependent variable. By implication, our predictions of
</p>
<p>Y will be less trustworthy when we do not include a factor that influ-
</p>
<p>ences the dependent variable.
</p>
<p>T H E I M P O R T A N C E O F C O R R E C T M O D E L S P E C I F I C A T I O N S 483</p>
<p/>
</div>
<div class="page"><p/>
<p>Sometimes, statisticians express this fact in terms of an assumption
</p>
<p>about the error term in the population regression model. The error term,
</p>
<p>�, should represent only random fluctuations that are related to the out-
</p>
<p>residuals, since they are in theory what is left over once you have taken
</p>
<p>into account all systematic causes of Y. However, if you fail to include an
</p>
<p>important predictor of Y as an independent variable, then by implication
</p>
<p>it moves to your error term. The error term now is not made up only of
</p>
<p>random&mdash;or what statisticians sometimes call stochastic&mdash;variation in Y,
</p>
<p>but rather includes the systematic variation that can be attributed to the
</p>
<p>excluded variable. For example, in Model 1, the effect of median family
</p>
<p>income is not taken into account, and thus the systematic relationship
</p>
<p>between median family income and number of arrests is found in the
</p>
<p>error term for that regression equation. When a model is not correctly
</p>
<p>specified, the error term will not represent only random or stochastic
</p>
<p>variation, as is required by the assumptions of regression analysis; it will
</p>
<p>be systematically related to the dependent variable.
</p>
<p>Correctly Estimating the Effect of b
</p>
<p>dent variables. Suppose, for example, that a bivariate regression is de-
</p>
<p>fined in which number of years in prison is identified as influencing
</p>
<p>number of arrests after prison:
</p>
<p>Yrearrests � b0 � b1(years in prison) � e
</p>
<p>In estimating this relationship from the data presented in Table 16.1, we
</p>
<p>find that the regression coefficient based on this model is 1.709. That is,
</p>
<p>every additional year of imprisonment produces about a 1.709 increase
</p>
<p>in our prediction of number of subsequent arrests.
</p>
<p>W orking It Out
</p>
<p> � 1.7089
</p>
<p> � 
31.7
18.55
</p>
<p> b � 
�
N
</p>
<p>i�1
</p>
<p> (Xi � X )(Yi � Y )
</p>
<p>�
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2
</p>
<p>Our model for subsequent arrests states that the only causal factor in-
</p>
<p>fluencing arrests is years of imprisonment. This, of course, is a question-
</p>
<p>able statement, because common sense tells us that this model is not
</p>
<p>researcher to present biased estimates of the effects of specific indepen-
</p>
<p>Failure to correctly specify a regression model may also lead the 
</p>
<p>comes (Y ) that you are examining. For this reason, we also call the errors
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N484</p>
<p/>
</div>
<div class="page"><p/>
<p>correctly specified. There are certainly other factors that influence arrests.
</p>
<p>Some of those factors, in turn, may also be related to the number of
</p>
<p>years that an offender serves in prison. If this is true&mdash;that relevant fac-
</p>
<p>tors related to years of imprisonment have been omitted from the
</p>
<p>model&mdash;then the regression coefficient may provide a very misleading
</p>
<p>estimate of the effect of imprisonment on arrests.
</p>
<p>Judges, for example, are likely to impose longer prison sentences on
</p>
<p>offenders with more serious prior records. Using the sample data in
</p>
<p>Table 16.2, we can look at the correlations among these three variables
</p>
<p>(see Table 16.3). The number of prior arrests is strongly related (r �
</p>
<p>0.63) to the length of prison term served. Prior arrests are even more
</p>
<p>strongly related to subsequent arrests (r � 0.76). This suggests, first of
</p>
<p>all, that prior record is a relevant factor that should be included if our
</p>
<p>model is to be correctly specified. But it also raises a very important con-
</p>
<p>cern: How do we know that our finding that &ldquo;years in prison&rdquo; increases
</p>
<p>reoffending is not simply a result of the fact that those who serve longer
</p>
<p>prison terms generally have more serious prior records of offending?
</p>
<p>Number of Rearrests (Y ) and Years 
Spent in Prison (X ) for 20 Former Inmates
</p>
<p>YEARS SPENT
</p>
<p>REARRESTS IN PRISON
</p>
<p>SUBJECT Y Yi � X Xi � (Xi � )
2 (Xi � )(Yi � )
</p>
<p>1 0 �3.1 2 �1.15 1.3225 3.565
2 0 �3.1 3 �0.15 0.0225 0.465
3 1 �2.1 1 �2.15 4.6225 4.515
4 1 �2.1 2 �1.15 1.3225 2.415
5 1 �2.1 3 �0.15 0.0225 0.315
6 1 �2.1 3 �0.15 0.0225 0.315
7 2 �1.1 4 0.85 0.7225 �0.935
8 2 �1.1 2 �1.15 1.3225 1.265
9 2 �1.1 2 �1.15 1.3225 1.265
</p>
<p>10 3 �0.1 3 �0.15 0.0225 0.015
11 3 �0.1 3 �0.15 0.0225 0.015
12 3 �0.1 3 �0.15 0.0225 0.015
13 4 0.9 3 �0.15 0.0225 �0.135
14 4 0.9 4 0.85 0.7225 0.765
15 4 0.9 4 0.85 0.7225 0.765
16 4 0.9 4 0.85 0.7225 0.765
17 5 1.9 4 0.85 0.7225 1.615
18 6 2.9 4 0.85 0.7225 2.465
19 7 3.9 5 1.85 3.4225 7.215
20 9 5.9 4 0.85 0.7225 5.015
</p>
<p>� 18.55
</p>
<p>Bivariate Regression Model:
Dependent Variable: Subsequent Rearrests
Independent Variable: Years in Prison
Regression Coefficient: b (years in prison) � 31.7/18.55 � 1.7089
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(Xi � X)(Yi � Y )�
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2X � 3.15Y � 3.1
</p>
<p>YXXXY
</p>
<p>Table 16.1
</p>
<p>� 31.7
</p>
<p>485T H E I M P O R T A N C E O F C O R R E C T M O D E L S P E C I F I C A T I O N S</p>
<p/>
</div>
<div class="page"><p/>
<p>would be made with subjects who were otherwise similar. That is, we
</p>
<p>would want to be sure that the offenders with longer and shorter prison
</p>
<p>sentences were comparable on other characteristics, such as the serious-
</p>
<p>ness of prior records. In this case, there would be no relationship be-
</p>
<p>tween prior arrests and length of imprisonment, and thus we would not
</p>
<p>have to be concerned with the possibility that the effect of length of im-
</p>
<p>In criminal justice, this approach is taken in the development of
</p>
<p>randomized experiments.1 A randomized study of the impact of
</p>
<p>Number of Rearrests, Years Spent in Prison, 
and Number of Prior Arrests for 20 Former Inmates
</p>
<p>SUBJECT REARRESTS YEARS IN PRISON PRIOR ARRESTS
</p>
<p>1 0 2 4
2 0 3 2
3 1 1 2
4 1 2 3
5 1 3 3
6 1 3 2
7 2 4 3
8 2 2 3
9 2 2 1
</p>
<p>10 3 3 2
11 3 3 3
12 3 3 3
13 4 3 4
14 4 4 3
15 4 4 4
16 4 4 5
17 5 4 4
18 6 4 5
19 7 5 5
20 9 4 6
</p>
<p>= 3.10 = 3.15 = 3.35
s � 2.300 s � 0.9631 s � 1.2360
</p>
<p>XXY
</p>
<p>Table 16.2
</p>
<p>Correlation Coefficients for the Variables 
Years in Prison, Prior Arrests, and Subsequent 
Rearrests Based on Data from 20 Former Inmates
</p>
<p>YEARS IN PRISON PRIOR ARRESTS
</p>
<p>Prior Arrests r � 0.6280
Subsequent Rearrests r � 0.7156 r � 0.7616
</p>
<p>Table 16.3
</p>
<p>1
</p>
<p>Wadsworth, 1995). For a comparison of experimental and nonexperimental methods,
</p>
<p>see D. Weisburd, C. Lum, and A. Petrosino, &ldquo;Does Research Design Affect Study Out-
</p>
<p>comes in Criminal Justice?&rdquo; The Annals 578 (2001): 50&ndash;70.
</p>
<p>In an ideal world, our comparisons of the impact of imprisonment
</p>
<p>M. Maxfield, The Practice of Social Research in Criminal Justice (Belmont, CA:
</p>
<p>prisonment actually reflects the influence of prior arrests on reoffending.
</p>
<p>For a discussion of experimental methods in criminal justice, see E. Babbie and 
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N486</p>
<p/>
</div>
<div class="page"><p/>
<p>487T H E I M P O R T A N C E O F C O R R E C T M O D E L S P E C I F I C A T I O N S
</p>
<p>length of imprisonment on reoffending would be one in which the re-
</p>
<p>searcher took a sample of offenders and assigned them to treatment
</p>
<p>and control conditions at random. For example, the researcher might
</p>
<p>define a sentence of 6 months as a control condition and a sentence of
</p>
<p>1 year as an experimental condition. In this case, the researcher could
</p>
<p>of subjects to treatment and control conditions allows the researcher
</p>
<p>to assume that other traits, such as prior record, are randomly scattered
</p>
<p>across the treatment and control conditions. Our problem in criminal
</p>
<p>justice is that it is often impractical to develop experimental research
</p>
<p>designs. For example, it is highly unlikely that judges would allow a
</p>
<p>researcher to randomly allocate prison sanctions. The same is true for 
</p>
<p>many other research problems relating to crime and justice.
</p>
<p>Fortunately for criminal justice researchers, a correctly specified re-
</p>
<p>gression model will take into account and control for relationships that
</p>
<p>exist among the independent variables included in the model. So, for
</p>
<p>example, the inclusion of both length of imprisonment and prior ar-
</p>
<p>rests in one regression model will provide regression coefficients that
</p>
<p>reflect the specific impact of each variable, once the impact of the
</p>
<p>other has been taken into account. This is illustrated in Equation 16.1,
</p>
<p>which describes the calculation of a multivariate regression coefficient
</p>
<p>in the case of two independent variables (X1 and X2). Equation 16.2
</p>
<p>applies Equation 16.1 to the specific regression model including both
</p>
<p>length of imprisonment and prior arrests. The model can be described
</p>
<p>as follows:
</p>
<p>Y � b0 � b1X1 � b2X2 � e
</p>
<p>where Y � subsequent rearrests
</p>
<p>X1 � years in prison
</p>
<p>X2 � prior arrests
</p>
<p>Here we calculate the multivariate regression coefficient b1 for length of
</p>
<p>imprisonment.
</p>
<p>Equation 16.1
</p>
<p>Equation 16.2
</p>
<p>In Equations 16.1 and 16.2, the bivariate correlations among the
</p>
<p>three measures examined, as well as the standard deviations of years in
</p>
<p>bX1 � �rY,YP � (rY,PArYP,PA)1 � r 2YP,PA ��
sY
s �
</p>
<p>bX1 � �rY,X1 � (rY,X2rX1,X2)1 � r 2X1,X2 ��
sY
s �
</p>
<p>rearrests without concern about the confounding influences of other vari-
</p>
<p>examine the effects of a longer versus a shorter prison sentence on
</p>
<p>YP
</p>
<p>X1
</p>
<p>ables. In Chapter 21, we focus more directly on the analysis of experi-
</p>
<p>mental data. But it is important to note here that random allocation </p>
<p/>
</div>
<div class="page"><p/>
<p>prison and rearrests, are used to calculate the multivariate regression
</p>
<p>coefficients. The three correlations for our specific example are (1) rY,YP,
</p>
<p>or the correlation between subsequent rearrests and years in prison; 
</p>
<p>(2) rY,PA, or the correlation between subsequent rearrests and prior ar-
</p>
<p>rests; and (3) rYP,PA, or the correlation between years in prison and
</p>
<p>prior arrests.
</p>
<p>What is most important to note in Equation 16.2 is that the numer-
</p>
<p>ator (in the first part) takes into account the product of the relation-
</p>
<p>ship between prior arrests and subsequent rearrests and that of prior
</p>
<p>arrests and years in prison. This relationship is subtracted from the
</p>
<p>simple correlation between years in prison and subsequent arrests. In
</p>
<p>this way, multivariate regression provides an estimate of b that takes
</p>
<p>into account that some of the impact of years in prison may be due to
</p>
<p>the fact that longer prison terms are associated with more serious
</p>
<p>prior records. This estimate is now purged of the bias that was intro-
</p>
<p>duced when prior record was not included in the regression model.
</p>
<p>The multivariate regression coefficient for years in prison when prior
</p>
<p>record is included in the regression model (0.936) is considerably
</p>
<p>smaller than the estimate calculated earlier in the bivariate regression
</p>
<p>(1.709).
</p>
<p>W orking It Out
</p>
<p> � 0.9358
</p>
<p> � �0.23731520.605616 �(2.388122)
</p>
<p> � �0.7156 � (0.7616)(0.6280)1 � (0.6280)2 �� 2.3000.9631�
</p>
<p> bX1 � �rY,YP � (rY,PArYP,PA)1 � r 2YP,PA ��
sY
sYP�
</p>
<p>With the same information, we can calculate the multivariate regres-
</p>
<p>sion coefficient for prior arrests. We find the value for b2 to be 0.9593 (see
</p>
<p>sults. As you can see, the value of b when we take into account years in
</p>
<p>prison (0.96) is much smaller than that in the bivariate case (1.4).
</p>
<p>working it out, page 490). The bivariate regression coefficient for prior ar-
</p>
<p>rests is calculated in the box on page 489 so that you can compare the re-
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N488</p>
<p/>
</div>
<div class="page"><p/>
<p>PRIOR
</p>
<p>REARRESTS ARRESTS
</p>
<p>SUBJECT Y Yi � X Xi � (Xi � )
2 (Xi � )(Yi � )
</p>
<p>1 0 �3.1 4 0.65 0.4225 �2.015
</p>
<p>2 0 �3.1 2 �1.35 1.8225 4.185
</p>
<p>3 1 �2.1 2 �1.35 1.8225 2.835
</p>
<p>4 1 �2.1 3 �0.35 0.1225 0.735
</p>
<p>5 1 �2.1 3 �0.35 0.1225 0.735
</p>
<p>6 1 �2.1 2 �1.35 1.8225 2.835
</p>
<p>7 2 �1.1 3 �0.35 0.1225 0.385
</p>
<p>8 2 �1.1 3 �0.35 0.1225 0.385
</p>
<p>9 2 �1.1 1 �2.35 5.5225 2.585
</p>
<p>10 3 �0.1 2 �1.35 1.8225 0.135
</p>
<p>11 3 �0.1 3 �0.35 0.1225 0.035
</p>
<p>12 3 �0.1 3 �0.35 0.1225 0.035
</p>
<p>13 4 0.9 4 0.65 0.4225 0.585
</p>
<p>14 4 0.9 3 �0.35 0.1225 �0.315
</p>
<p>15 4 0.9 4 0.65 0.4225 0.585
</p>
<p>16 4 0.9 5 1.65 2.7225 1.485
</p>
<p>17 5 1.9 4 0.65 0.4225 1.235
</p>
<p>18 6 2.9 5 1.65 2.7225 4.785
</p>
<p>19 7 3.9 5 1.65 2.7225 6.435
</p>
<p>20 9 5.9 6 2.65 7.0225 15.635
</p>
<p>= 3.10 = 3.35
</p>
<p>� 30.55 � 43.30
</p>
<p>Bivariate Regression Model:
Dependent Variable: Subsequent Rearrests
Independent Variable: Prior Arrests
Regression Coefficient: b (Prior Arrests) � 43.30/30.55 � 1.4173
</p>
<p>�
N
</p>
<p>i�1
</p>
<p>(Xi � X )(Yi � Y )�
N
</p>
<p>i�1
</p>
<p> (Xi � X )
2XY
</p>
<p>YXXXY
</p>
<p>Calculation of Bivariate Regression 
Coefficient for Number of Rearrests (Y) 
and Number of Prior Arrests (X) for 20 Former Inmates</p>
<p/>
</div>
<div class="page"><p/>
<p>The fact that the results are different when we examine the effects of
</p>
<p>years in prison and prior arrests in the multivariate regression model
</p>
<p>shows that the bivariate regression coefficients were indeed biased. In
</p>
<p>both cases, the estimate of the effect of b provided by the bivariate re-
</p>
<p>gression coefficient was much too high. These differences also reflect a
</p>
<p>difference in interpretation between the multivariate regression coeffi-
</p>
<p>duced by a one-unit change in X. In the multivariate case, b represents
</p>
<p>the estimated change in Y associated with a one-unit change in X when
</p>
<p>all other independent variables in the model are held constant. Holding
</p>
<p>prior arrests constant leads to a reduction in the impact of years in
</p>
<p>prison. Holding years in prison constant leads to a reduction in the esti-
</p>
<p>mate of the effect of prior arrests. These differences may be seen as the
</p>
<p>bias introduced by misspecifying the regression model through the ex-
</p>
<p>clusion of prior arrests.
</p>
<p>We can also identify this bias in terms of assumptions related to the
</p>
<p>error term in regression. It is assumed not only that the errors in the re-
</p>
<p>gression are stochastic, but also that there is no specific systematic rela-
</p>
<p>tionship between the error term and the independent variables included
</p>
<p>in the regression. If there is such a relationship, the regression coefficient
</p>
<p>will be biased. While this may seem like a new concept, it is really a re-
</p>
<p>statement of what you learned above.
</p>
<p>Let&rsquo;s use our model predicting rearrest as a substantive example. We
</p>
<p>saw that if we estimated the regression coefficient for years in prison
</p>
<p>without taking into account prior arrests, the regression coefficient
</p>
<p>would be biased&mdash;in this case, overestimated. What happens in theory to
</p>
<p>the error term in this case? As we discussed earlier in the chapter, when
</p>
<p>we exclude an independent variable, the effect of that variable moves to
</p>
<p>W orking It Out
</p>
<p> � 0.9593
</p>
<p> � �0.31220320.605616 �(1.8608)
 � �0.7616 � (0.7156)(0.6280)1 � (0.6280)2 �� 2.3001.2360�
</p>
<p> bX2 � �rY,PA � (rY,YPrYP,PA)1 � r 2YP,PA ��
sY
sPA�
</p>
<p>regression coefficient represents the estimated change in Y that is pro-
</p>
<p>cient and the bivariate regression coefficient. In the bivariate case, the
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N490</p>
<p/>
</div>
<div class="page"><p/>
<p>the error term. In our case, the population model including both inde-
</p>
<p>pendent variables may be stated as follows:
</p>
<p>Y � �0 � �1X1 � �2X2 � �
</p>
<p>where Y � subsequent rearrests
</p>
<p>X1 � years in prison
</p>
<p>X2 � prior arrests
</p>
<p>When we take into account only one independent variable, the model
</p>
<p>includes only the term X1:
</p>
<p>Y � �0 � �1X1 � �
</p>
<p>where Y � subsequent rearrests
</p>
<p>X1 � years in prison
</p>
<p>In the latter model, number of prior arrests is included by implication in
</p>
<p>the error term. But what does this mean regarding the relationship in this
</p>
<p>model between the error term and years in prison? Our sample data sug-
</p>
<p>gest that the number of prior arrests is related to years in prison (as was
</p>
<p>By looking at bias in terms of the error term, we can also specify when
</p>
<p>excluding an independent variable will not lead to bias in our estimates
</p>
<p>of the regression coefficients of other variables. If the excluded variable is
</p>
<p>unrelated to other variables included in the regression, it will not cause
</p>
<p>bias in estimates of b for those specific variables. This is the case because
</p>
<p>when there is no systematic relationship between the excluded variable
</p>
<p>and the included variable of interest, its exclusion does not lead to a sys-
</p>
<p>tematic relationship between the error term and the variable of interest.
</p>
<p>For example, if years in prison and prior arrests were not systemati-
</p>
<p>cally related (e.g., the correlation between the variables was 0), it would
</p>
<p>not matter whether we took into account prior arrests in estimating the re-
</p>
<p>gression coefficient for years in prison.2 In this case, the exclusion of prior
</p>
<p>arrests would not lead to a systematic relationship between the error term
</p>
<p>2It is important to note that bias can be caused by a nonlinear relationship between
</p>
<p>the excluded and the included variable. The assumption is that there is no systematic
</p>
<p>relationship of any form.
</p>
<p>shown in Table 16.3). By implication, since number of prior arrests is 
</p>
<p>related to years in prison as well. Accordingly, if we leave prior arrests
</p>
<p>out of our equation, then we violate the assumption that there is no
</p>
<p>varibles in the equation.
</p>
<p>now found in the error term, the error term can now be assumed to be
</p>
<p>systematic relationship between the error term and the independent
</p>
<p>491T H E I M P O R T A N C E O F C O R R E C T M O D E L S P E C I F I C A T I O N S</p>
<p/>
</div>
<div class="page"><p/>
<p>and years in prison, because there is no systematic relationship between
</p>
<p>prior arrests and years in prison. However, it is important to remember
</p>
<p>that the exclusion of prior arrests will still cause bias in our estimate of Y.
</p>
<p>In this situation, we continue to violate the assumption that the error term
</p>
<p>is stochastic. It now includes a systematic predictor of Y, prior arrests.
</p>
<p>Comparing Regression Coefficients Within a Single 
</p>
<p>Model: The Standardized Regression Coefficient
</p>
<p>A multivariate regression model allows us to specify the impact of a spe-
</p>
<p>cific independent variable while holding constant the impact of other in-
</p>
<p>dependent variables. This is a very important advantage of multivariate
</p>
<p>regression analysis over bivariate regression analysis. However, when we
</p>
<p>include multiple variables in the same model, it is natural to want to
</p>
<p>compare the impact of the different variables examined. For example, in
</p>
<p>our case, does years in prison have a stronger effect on subsequent rear-
</p>
<p>rests than number of prior arrests does? Or does number of prior arrests
</p>
<p>have a stronger effect than years in prison? The ordinary regression coef-
</p>
<p>ficient b does not allow us to answer this question, since it reports the
</p>
<p>effect of a variable in its original units of measurement. Accordingly, the
</p>
<p>regression coefficient for years in prison reports the predicted change in
</p>
<p>subsequent rearrests for each year change in years in prison. The regres-
</p>
<p>sion coefficient for number of prior arrests reports the predicted change
</p>
<p>in subsequent rearrests for each change in number of prior arrests.
</p>
<p>Though the interpretation of the regression coefficients in these cases is
</p>
<p>straightforward, we cannot directly compare them.
</p>
<p>Another statistic, called the standardized regression coefficient or
</p>
<p>Beta, allows us to make direct comparisons. Beta weights take the re-
</p>
<p>gression coefficients in an equation and standardize them according to
</p>
<p>the ratio of the standard deviation of the variable examined to the stan-
</p>
<p>dard deviation of the dependent variable. Beta is expressed mathemati-
</p>
<p>cally in Equation 16.3:
</p>
<p>Equation 16.3
</p>
<p>The interpretation of the standardized coefficient is similar to that of b
</p>
<p>(the unstandardized coefficient), except that we change the units. We in-
</p>
<p>terpret Beta as the expected amount of change in the standard deviation
</p>
<p>of the dependent variable, given a one-unit change in the standard devi-
</p>
<p>ation of the independent variable.
</p>
<p>For years in prison in our example, we take the regression coefficient
</p>
<p>of 0.9358 and multiply it by the ratio of the standard deviation of years in
</p>
<p>prison (0.9631) and subsequent rearrests (2.3000). The result is 0.3919,
</p>
<p>Beta � b�sXsY�
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N492</p>
<p/>
</div>
<div class="page"><p/>
<p>which tells us that an increase of one standard deviation in years in
</p>
<p>prison is expected to result in an increase of 0.392 standard deviation in
</p>
<p>rearrests.
</p>
<p>W orking It Out
</p>
<p> � 0.3919
</p>
<p> � 0.9358�0.96312.3000�
</p>
<p> Beta � b�sXsY�
</p>
<p>For prior arrests, we begin with our regression coefficient of 0.9593.
</p>
<p>Again, we standardize our estimate by taking the ratio of the standard
</p>
<p>deviation of prior arrests (1.2360) and subsequent rearrests (2.3000). Our
</p>
<p>estimate of Beta here is 0.5155, which indicates that an increase of one
</p>
<p>standard deviation in prior arrests is expected to result in an increase of
</p>
<p>0.516 standard deviation in rearrests.
</p>
<p>W orking It Out
</p>
<p> � 0.5155
</p>
<p> � 0.9593�1.23602.3000�
</p>
<p> Beta � b�sXsY�
</p>
<p>In our example, the Beta weight for prior arrests is larger than that for
</p>
<p>years in prison. According to this estimate, the number of prior arrests
</p>
<p>has a greater impact on subsequent rearrests than the number of years in
</p>
<p>prison does. The standardized regression coefficient thus provides us
</p>
<p>with an answer to our original question regarding which of the indepen-
</p>
<p>dent variables examined has the most influence on the dependent vari-
</p>
<p>able. As you can see, the standardized regression coefficient is a useful
</p>
<p>tool for comparing the effects of variables measured differently within a
</p>
<p>493T H E I M P O R T A N C E O F C O R R E C T M O D E L S P E C I F I C A T I O N S</p>
<p/>
</div>
<div class="page"><p/>
<p>single regression model. However, because standardized regression coef-
</p>
<p>ficients are based on the standard deviations of observed samples, they
</p>
<p>are generally considered inappropriate for making comparisons across
</p>
<p>samples.
</p>
<p>C o r r e c t l y  S p e c i f y i n g  t h e  R e g r e s s i o n  M o d e l
</p>
<p>The previous section illustrated the importance of a correctly specified
</p>
<p>regression model. If a regression model is not correctly specified, then
</p>
<p>the predictions that are made and the coefficients that are estimated may
</p>
<p>provide misleading results. This raises important theoretical as well as
</p>
<p>practical questions for criminal justice research.
</p>
<p>In criminal justice research, we can seldom say with assurance that
</p>
<p>the models we develop include all relevant predictors of the dependent
</p>
<p>variables examined. The problem is often that our theories are not pow-
</p>
<p>erful enough to clearly define the factors that influence criminal justice
</p>
<p>questions. Criminal justice is still a young science, and our theories for
</p>
<p>explaining crime and justice issues often are not well specified. This fact
</p>
<p>has important implications for the use of criminal justice research in de-
</p>
<p>veloping public policy. When our predictions are weak, they do not
</p>
<p>form a solid basis on which to inform criminal justice policies.3
</p>
<p>One implication of our failure to develop strongly predictive models
</p>
<p>in criminal justice is that our estimates of variable effects likely include
</p>
<p>some degree of bias. We have stressed in this chapter the importance of
</p>
<p>controlling for relevant predictors in regression modeling. The cost of
</p>
<p>leaving out important causes is not just weaker prediction but also esti-
</p>
<p>mates of variable effects that include potentially spurious components.
</p>
<p>This fact should make you cautious in reporting regression analyses and
</p>
<p>critical in evaluating the research of others. Just because regression coef-
</p>
<p>ficients are reported to the fifth decimal place on a computer printout
</p>
<p>does not mean that the estimates so obtained are solid ones.
</p>
<p>The fact that regression models often include some degree of mis-
</p>
<p>specification, however, should not lead you to conclude that the regres-
</p>
<p>3Mark Moore of Harvard University has argued, for example, that legal and ethical
</p>
<p>dilemmas make it difficult to base criminal justice policies about crime control on
</p>
<p>models that still include a substantial degree of statistical error. See M. Moore, &ldquo;Pur-
</p>
<p>blind Justice: Normative Issues in the Use of Prediction in the Criminal Justice Sys-
</p>
<p>tem,&rdquo; in A. Blumstein, J. Cohen, A. Roth, and C. A. Visher (eds.), Criminal Careers
</p>
<p>and &ldquo;Career Criminals,&rdquo; Vol. 2 (Washington, DC: National Academy Press, 1986).
</p>
<p>sion approach is not useful for criminal justice researchers. As in any 
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N494</p>
<p/>
</div>
<div class="page"><p/>
<p>available. The researcher&rsquo;s task in developing regression models is to im-
</p>
<p>prove on models that were developed before. With each improvement,
</p>
<p>the results we gain provide a more solid basis for making decisions
</p>
<p>about criminal justice theory and policy. This, of course, makes the prac-
</p>
<p>tical task of defining the correct model for the problem you are examin-
</p>
<p>ing extremely important. How then should you begin?
</p>
<p>Defining Relevant Independent Variables
</p>
<p>Importantly, model specification does not begin with your data. Rather,
</p>
<p>it starts with theory and a visit to the library or other information sys-
</p>
<p>tems. To build a regression model, you should first identify what is al-
</p>
<p>ready known about the dependent variable you have chosen to study. If
</p>
<p>your interest, for example, is in the factors that influence involvement in
</p>
<p>criminality, you will need to carefully research what others have said and
</p>
<p>found regarding the causes of criminality. Your regression model should
</p>
<p>take into account the main theories and perspectives that have been
</p>
<p>raised by others.
</p>
<p>If you do not take prior research and theory into account, then those
</p>
<p>reviewing your work will argue that your predictions and your estimates
</p>
<p>of variable effects are biased in one way or another. Just as the exclusion
</p>
<p>of prior record from our example led to a misleading estimate of its im-
</p>
<p>pact on length of imprisonment, so too the exclusion of relevant causal
</p>
<p>factors in other models may lead to bias. The only way to refute this po-
</p>
<p>tential criticism is to include such variables in your regression model.
</p>
<p>Taking into account the theories and perspectives of others is the first
</p>
<p>step in building a correctly specified regression model. However, in most
</p>
<p>research we seek to add something new to existing knowledge. In regres-
</p>
<p>sion modeling, this usually involves the addition of new variables. Some-
</p>
<p>times, such new variables are drawn from an innovative change in theory.
</p>
<p>Other times, they involve improvements in measurement. Often, the find-
</p>
<p>ing that these new or transformed variables have an independent impact
</p>
<p>above and beyond those of variables traditionally examined by researchers
</p>
<p>leads to important advances in criminal justice theory and policy.
</p>
<p>Taking into Account Ordinal- and Nominal-Scale 
</p>
<p>Measures in a Multivariate Regression
</p>
<p>Until now, we have assumed that ordinary least squares regression
</p>
<p>analysis requires an interval level of measurement, both for the depen-
</p>
<p>dent and for the independent variables. However, criminal justice re-
</p>
<p>searchers will sometimes use this approach with ordinal-level dependent
</p>
<p>variables when there are a number of categories and there is good rea-
</p>
<p>son to assume that the intervals for the categories are generally similar.
</p>
<p>In practice, you should be cautious in using OLS regression when your
</p>
<p>science, the task is to continue to build on the knowledge that is presently
</p>
<p>C O R R E C T L Y S P E C I F Y I N G T H E R E G R E S S I O N M O D E L 495</p>
<p/>
</div>
<div class="page"><p/>
<p>What about the inclusion of non&ndash;interval-level independent variables?
</p>
<p>Such variables often are important in explaining criminal justice out-
</p>
<p>comes. If we are required to include all relevant causes of Y in order to
</p>
<p>correctly specify our model, how can we exclude ordinal- and nominal-
</p>
<p>level measures? Fortunately, we do not have to. In multivariate regres-
</p>
<p>sion, it is acceptable to include ordinal- and nominal-level independent
</p>
<p>variables as long as there is at least one interval-level independent vari-
</p>
<p>able also included in the analysis.
</p>
<p>But even though you can include ordinal- and nominal-level variables,
</p>
<p>you need to take into account the specific interpretation used by regres-
</p>
<p>sion analysis for interpreting the effects of one variable on another. In-
</p>
<p>cluding an ordinal-level measure in a multivariate regression is relatively
</p>
<p>straightforward. This is done in Table 16.4, which presents a standard
</p>
<p>SPSS printout for a regression analysis. The data used are drawn from a
</p>
<p>national sample of police officers developed by the Police Foundation.4
</p>
<p>The dependent variable in this analysis is hours worked per week. There
</p>
<p>are two independent variables. One, years with the department, is mea-
</p>
<p>sured at the interval level. The second, level of education, is on an ordi-
</p>
<p>SPSS Printout for Regression Analysis of the Police Officer Example
</p>
<p>Coefficients
</p>
<p>Unstandardized Standardized
Coefficients Coefficients t Sig.
</p>
<p>Model B Std. Error Beta
</p>
<p>1 (Constant) 44.968 .749 60.031 .000
</p>
<p>YEARS WITH �7.354E-02 .026 �.092 �2.816 .005
</p>
<p>DEPARTMENT
</p>
<p>LEVEL OF .456 .173 .086 2.636 .009
EDUCATION
</p>
<p>a Dependent Variable: HOURS PER WEEK WORKED
</p>
<p>Table 16.4
</p>
<p>4David Weisburd et al., The Abuse of Authority: A National Study of Police Officers&rsquo; At-
</p>
<p>titudes (Washington, DC: The Police Foundation, 2001).
</p>
<p>nal scale with eight levels, ranging from some high school to doctoral
</p>
<p>regression cannot be met in the case of ordinal dependent variables, you
</p>
<p>dependent variable is not interval level. When the assumptions of OLS 
</p>
<p>should use ordinal regression (see Chapter 19). As will be explained
in Chapter 18, the use of OLS regression in the case of a binary dependent
</p>
<p>variable is inappropriate.
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N496</p>
<p/>
</div>
<div class="page"><p/>
<p>SPSS table) are less than the conventionally applied significance level of
</p>
<p>0.05 we would likely use in this case. This result, as in most statistical
</p>
<p>packages, is calculated for a two-tailed test of statistical significance (gen-
</p>
<p>erally the default option). For years with the department, we can see that
</p>
<p>the impact is negative. When we control for the impact of level of educa-
</p>
<p>tion, each year with the department is associated with an average de-
</p>
<p>But what is the meaning of the effect of level of education? Here, what
</p>
<p>we have is not an interval scale but a group of ordered categories. For the
</p>
<p>regression, this ordinal-level scale is treated simply as an interval-level
</p>
<p>scale. It is assumed that the categories must be roughly similar in value, or
</p>
<p>that each level increase in that scale is related in a linear manner to the de-
</p>
<p>pendent variable. Thus, our interpretation of this regression coefficient is
</p>
<p>that for every one-level increase in education level, there is, on average, a
</p>
<p>0.456 increase in the number of hours worked (once we have taken into
</p>
<p>account years with the department). In this case, the standardized regres-
</p>
<p>sion coefficient is very useful. It appears from the size of the coefficients
</p>
<p>This example illustrates how we can include an ordinal-level variable
</p>
<p>in a multivariate regression. The inclusion of an ordinal variable is
</p>
<p>straightforward, and its interpretation follows that of an interval-level in-
</p>
<p>dependent variable. But when we include a nominal-level variable, we
</p>
<p>have to adjust our interpretation of the regression coefficient.
</p>
<p>SPSS Printout for Regression Analysis 
with an Interval-Level and Nominal-Level Variable
</p>
<p>Coefficients
</p>
<p>Unstandardized Standardized
Coefficients Coefficients t Sig.
</p>
<p>Model B Std. Error Beta
</p>
<p>1 (Constant) 48.550 .977 49.672 .000
</p>
<p>YEARS WITH �7.737E-02 .026 �.097 �2.943 .003
</p>
<p>DEPARTMENT
</p>
<p>RESPONDENT �1.669 .803 �.068 �2.077 .038
GENDER
</p>
<p>a Dependent Variable: HOURS PER WEEK WORKED
</p>
<p>Table 16.5
</p>
<p>hours worked per week&mdash;the observed significance levels (&ldquo;Sig.&rdquo; in the
</p>
<p>degree. We can see that both of these variables have a significant impact on
</p>
<p>crease of about 0.074 hours in number of hours worked each week.
</p>
<p>that the overall effect of years with the department is much less than that 
</p>
<p>of level of education. However, the standardized regression coefficients 
</p>
<p>(represented by Beta) show that the difference between the two variables
</p>
<p>is not large.
</p>
<p>C O R R E C T L Y S P E C I F Y I N G T H E R E G R E S S I O N M O D E L 497</p>
<p/>
</div>
<div class="page"><p/>
<p>dependent variable and data examined in our prior example. To make
</p>
<p>our example easier, we include only two measures in predicting number
</p>
<p>of hours worked. Again, we have an interval-level measure, years with
</p>
<p>the department. A binary independent variable, gender, is also included.
</p>
<p>In regression analysis, a binary nominal-level independent variable is
</p>
<p>generally called a dummy variable. Our first problem is to give num-
</p>
<p>bers to this dummy variable. Multivariate regression analysis does not
</p>
<p>recognize qualitative categories. By convention, we give one category a
</p>
<p>value of 0 and the other a value of 1. It is generally good practice to give
</p>
<p>the category with the largest number of cases a value of 0 because, as
</p>
<p>we will illustrate in a moment, that category becomes the reference cate-
</p>
<p>gory. Since this sample included many more men than women, we as-
</p>
<p>signed men the value 0 and women the value 1.
</p>
<p>Again, we can see that both variables have a statistically significant im-
</p>
<p>pact on hours worked per week. The observed significance level for years
</p>
<p>with the department is 0.003, and that for gender is 0.038. But how can
</p>
<p>we interpret the dummy variable regression coefficient of �1.669? One
</p>
<p>way to gain a better understanding of the interpretation of dummy vari-
</p>
<p>able regression coefficients is to see how they affect our regression equa-
</p>
<p>tion. Let&rsquo;s begin by writing out the regression equation for our example:
</p>
<p>Y � b0 � b1X1 � b2X2
</p>
<p>where Y � hours worked per week
</p>
<p>X1 � years with the department
</p>
<p>X2 � gender of officer
</p>
<p>As a second step, let&rsquo;s insert the coefficients gained in our regression
</p>
<p>analysis:
</p>
<p>Y � 48.550 � (�0.077)X1 � (�1.669)X2
</p>
<p>or
</p>
<p>Y � 48.550 � 0.077X1 � 1.669X2
</p>
<p>What happens if we try to write out the regression equations for men
</p>
<p>and women separately? For men, the regression equation is
</p>
<p>Y � 48.550 � 0.077X1 � 1.669(0)
</p>
<p>Table 16.5 reports the results of a regression with a single interval-
</p>
<p>level variable and a binary nominal-level variable. We use the same
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N498</p>
<p/>
</div>
<div class="page"><p/>
<p>Because men are coded as 0, the second term of the equation falls out.
</p>
<p>But what about for women? The second term in the equation is a con-
</p>
<p>stant because all of the women have a value of 1. If we write it out, we
</p>
<p>have the following result:
</p>
<p>Y � 48.550 � 0.077X1 � 1.669(1) or
</p>
<p>Y � 48.550 � 0.077X1 � 1.669
</p>
<p>We can simplify this formula even more, because the two constants at
</p>
<p>the beginning and the end of the equation can be added together:
</p>
<p>Y � 46.881 � 0.077X1
</p>
<p>What then is the difference between the regression equations for men
</p>
<p>and women? In both cases, the slope of the regression line is given by
</p>
<p>the term �0.077X1. The difference between the two equations lies in the
</p>
<p>Y-intercept, as illustrated in Figure 16.1. As you can see, men and
</p>
<p>women have parallel regression lines. However, the women&rsquo;s line inter-
</p>
<p>sects the Y-axis about 1.7 hours lower than the men&rsquo;s line. This provides
</p>
<p>us with the interpretation of our coefficient. Women police officers, on
</p>
<p>Regression Lines for Men and Women Police OfficersFigure 16.1
</p>
<p>H
ou
</p>
<p>rs
 W
</p>
<p>or
ke
</p>
<p>d
 p
</p>
<p>er
 W
</p>
<p>ee
k
</p>
<p>50
</p>
<p>49.5
</p>
<p>49
</p>
<p>48.5
</p>
<p>48
</p>
<p>47.5
</p>
<p>47
</p>
<p>46.5
</p>
<p>46
</p>
<p>Years with Department (X1)
</p>
<p>0      2      4      6      8     10    12    14    16    18    20   
</p>
<p>Males
</p>
<p>Females
</p>
<p>Y � 48.550 � 0.077X1
</p>
<p>or
</p>
<p>C O R R E C T L Y S P E C I F Y I N G T H E R E G R E S S I O N M O D E L 499</p>
<p/>
</div>
<div class="page"><p/>
<p>average, work about 1.669 hours a week less than men police officers,
</p>
<p>taking into account years with the department.
</p>
<p>This example also suggests why it is generally recommended that you
</p>
<p>place the category with the largest number of cases as the 0 category of
</p>
<p>a binary dummy variable. The category men, in this case, is the reference
</p>
<p>category, meaning that the coefficient for gender gives us the estimate of
</p>
<p>the female category in reference to the male category. We want our ref-
</p>
<p>erence category to be as stable as possible, and a large number of cases
</p>
<p>makes this category more stable.
</p>
<p>But how can we assess the impact of a nominal variable that has
</p>
<p>multiple categories? In fact, multiple-category nominal variables create
</p>
<p>a good deal more complexity for the researcher than do ordinal or
</p>
<p>binary nominal variables. In this case, you must create a separate
</p>
<p>variable for each category in your analysis. For example, the Police
</p>
<p>Foundation study divided the United States into four regions: North
</p>
<p>Central, Northeast, South, and West. In practice, you would need to
</p>
<p>create a separate variable for each of these regions. In other words,
</p>
<p>you would define a variable North Central, which you would code 1
</p>
<p>for all those officers in the North Central region and 0 for all other of-
</p>
<p>ficers. You would repeat this process for each of the other regional
</p>
<p>categories.
</p>
<p>As with the binary independent variable, you must choose one of
</p>
<p>the categories to be a reference category. In this case, however, the
</p>
<p>reference category is excluded from the regression. Again, it is gener-
</p>
<p>ally recommended that you choose as the reference category the cate-
</p>
<p>gory with the largest number of cases.5 In our example, the largest
</p>
<p>number of officers is drawn from the South. Suppose that we include
</p>
<p>only one interval-level variable in our equation: years with the depart-
</p>
<p>ment. Table 16.6 presents the results from an analysis in which years
</p>
<p>with the department and region are used to predict number of hours
</p>
<p>worked.
</p>
<p>In this example, we included in the regression a single interval-level
</p>
<p>variable and three region measures: North Central, Northeast, and West.
</p>
<p>5There may be times when you want to choose a category that does not include the
</p>
<p>largest number of cases as the reference. For example, if you wanted to compare a se-
</p>
<p>ries of treatments to a no-treatment, or control, condition, it would make sense to
</p>
<p>have the control condition as the excluded category, even if it did not include the
</p>
<p>largest N. However, if the excluded category has a small number of cases, it may lead
</p>
<p>to instability in the regression estimates.
</p>
<p>While South is not included as a variable, it is in fact the reference 
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N500</p>
<p/>
</div>
<div class="page"><p/>
<p>Y � b0 � b1X1 � b2X2 � b3X3 � b4X4
</p>
<p>where Y � hours worked per week
</p>
<p>X1 � years with department
</p>
<p>X2 � North Central
</p>
<p>X3 � Northeast
</p>
<p>X4 � West
</p>
<p>Using the results for this model presented in Table 16.6, we can write
</p>
<p>the results in equation form as follows:
</p>
<p>Y � 47.654 � 0.061X1 � 2.335X2 � 1.758X3 � 0.846X4
</p>
<p>In this case, we can also write out a separate regression equation for
</p>
<p>each of the four regions. Since the South is our reference category,
</p>
<p>those from the South are coded 0 on the three included variables.
</p>
<p>Thus, our equation is simply the Y-intercept and the variable years
</p>
<p>with the department. For officers from the North Central region, 
</p>
<p>the equation includes the Y-intercept, b1X1, and b2X2. The other para-
</p>
<p>meters are set to 0, since those in the North Central region have 0 val-
</p>
<p>ues on X3 and X4. Similarly, for both the Northeast and the West, only
</p>
<p>SPSS Printout for Regression Analysis 
with Multiple-Category Nominal Variable
</p>
<p>Coefficients
</p>
<p>Unstandardized Standardized
Coefficients Coefficients t Sig.
</p>
<p>Model B Std. Error Beta
</p>
<p>1 (Constant) 47.654 .470 101.435 .000
</p>
<p>YEARS WITH �6.138E-02 .026 �.077 �2.352 .019
</p>
<p>DEPARTMENT
</p>
<p>NORTH CENTRAL �2.335 .610 �.141 �3.825 .000
</p>
<p>NORTHEAST �1.758 .573 �.114 �3.067 .002
</p>
<p>WEST �.846 .616 �.050 �1.372 .170
</p>
<p>a Dependent Variable: HOURS PER WEEK WORKED
</p>
<p>Table 16.6
</p>
<p>category. If we again write out our regression equation, we can see why 
</p>
<p>this is the case:
</p>
<p>C O R R E C T L Y S P E C I F Y I N G T H E R E G R E S S I O N M O D E L 501</p>
<p/>
</div>
<div class="page"><p/>
<p>one of the three dummy variables is included. For each equation, 
</p>
<p>we can once again add the constant for the dummy variable to the 
</p>
<p>Y-intercept:
</p>
<p>Officers from the South:
</p>
<p>Y � 47.654 � 0.061X1
</p>
<p>Officers from the North Central:
</p>
<p>Y � 47.654 � 0.061X1 � 2.335X2 or Y � 45.319 � 0.061X1
</p>
<p>Officers from the Northeast:
</p>
<p>Y � 47.654 � 0.061X1 � 1.758X3 or Y � 45.896 � 0.061X1
</p>
<p>Officers from the West:
</p>
<p>Y � 47.654 � 0.061X1 � 0.846X4 or Y � 46.808 � 0.061X1
</p>
<p>Once again, we can gain a better conceptual understanding of our re-
</p>
<p>sults if we plot them, as in Figure 16.2. In this case, each of the included
</p>
<p>categories is found to have a Y-intercept lower than that of the excluded
</p>
<p>Plot of Hours Worked and Years with Department, by RegionFigure 16.2
</p>
<p>H
ou
</p>
<p>rs
 W
</p>
<p>or
ke
</p>
<p>d
 p
</p>
<p>er
 W
</p>
<p>ee
k
</p>
<p>48
</p>
<p>47.5
</p>
<p>47
</p>
<p>46.5
</p>
<p>46
</p>
<p>45.5
</p>
<p>45
</p>
<p>Years with Department (X1)
</p>
<p>0      2      4      6      8     10    12    14    16    18    20   
</p>
<p>South
</p>
<p>West
</p>
<p>Northeast
</p>
<p>North Central
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N502</p>
<p/>
</div>
<div class="page"><p/>
<p>category, the South. This means that, on average, officers work fewer
</p>
<p>hours in all of the other regions. The least number of hours worked per
</p>
<p>week is found in the North Central region. Here, officers work, on aver-
</p>
<p>age, about 2.335 hours less than they do in the South, once we have
</p>
<p>taken into account years with the department. In the Northeast, officers
</p>
<p>work about 1.758 hours less and in the West about 0.846 hour less a
</p>
<p>week.
</p>
<p>Are these differences statistically significant? It is important to note
</p>
<p>that the significance statistic reported for each coefficient tells us only
</p>
<p>whether the category is significantly different from the reference cate-
</p>
<p>gory. This is one reason it is so important to be clear about the definition
</p>
<p>of the reference category. In our example, the North Central and North-
</p>
<p>east regions are significantly different from the South, using a 5% signifi-
</p>
<p>cance threshold and a two-tailed significance test (the default option in
</p>
<p>SPSS). The West, however, is not significantly different from the South,
</p>
<p>using this threshold.
</p>
<p>If you wanted to determine whether region overall as a variable
</p>
<p>had a statistically significant impact on hours worked per week, you
</p>
<p>would have to run an additional significance test based on the F-test
</p>
<p>for the regression model, introduced in Chapter 15. The F-test for mul-
</p>
<p>tiple-category dummy variables in regression compares the R 2 statistic
</p>
<p>gained with the dummy variables included in the regression to the R 2
</p>
<p>statistic gained without those variables. In practice, you must run two
</p>
<p>separate regressions, although most computer programs now provide
</p>
<p>this statistic directly. First, you calculate the regression without the
</p>
<p>new dummy variable categories (referred to as the reduced model)
</p>
<p>and identify its R 2. In our case, the regression without the dummy
</p>
<p>variables produces an R 2 of only 0.008. You then compute the regres-
</p>
<p>sion with the dummy variables, as we did earlier (referred to as the
</p>
<p>full model). In this case, R 2 is 0.023. The F-test formula is presented in
</p>
<p>Equation 16.4.
</p>
<p>Equation 16.4
</p>
<p>To apply Equation 16.4 to our example, we first subtract the R 2 of the
</p>
<p>reduced model from the R 2 of the full model and then divide
</p>
<p>this quantity by the number of variables in the full model (k fm) minus the
</p>
<p>number of variables in the reduced model (k rm), which is 3. The denom-
</p>
<p>inator is found by subtracting the R 2 of the full model from 1, and then
</p>
<p>dividing this quantity by N � k fm � 1. For this sample, N is 923 and k fm
is 4. Our final result is F � 4.55. Looking at the F-distribution (see
</p>
<p>(R 2fm)(R
2
rm)
</p>
<p>F � 
(R 2f m � R
</p>
<p>2
rm)/(k fm � k rm)
</p>
<p>(1 � R 2fm)/(N � k fm � 1)
</p>
<p>C O R R E C T L Y S P E C I F Y I N G T H E R E G R E S S I O N M O D E L 503</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 5) with 3 and 918 degrees of freedom, we can see that our re-
</p>
<p>sult is statistically significant at the 0.05 level.
</p>
<p>W orking It Out
</p>
<p> � 4.5455
</p>
<p> � 
(0.015/3)
</p>
<p>(0.977/918)
 � 
</p>
<p>0.005
0.0011
</p>
<p> � 
(0.023 � 0.008)/(4 � 1)
</p>
<p>(1 � 0.023)/(923 � 4 � 1)
</p>
<p> F � 
(R 2fm � R
</p>
<p>2
rm)/(k fm � k rm)
</p>
<p>(1 � R 2fm)/(N � k fm � 1)
</p>
<p>One final question we might ask is whether we can use the standard-
</p>
<p>ized regression coefficient to compare dummy variables to ordinal- and
</p>
<p>interval-level measures. In general, statisticians discourage such use of
</p>
<p>standardized regression coefficients, since they are based on standard de-
</p>
<p>viations and the standard deviation is not an appropriate statistic for a
</p>
<p>nominal-level variable. Additionally, for a nominal-level variable, the
</p>
<p>standardized regression coefficient refers only to the difference between
</p>
<p>the reference category and the dummy variable category examined. This
</p>
<p>may sometimes make sense in the case of a binary dummy variable, since
</p>
<p>we can say that one category is Beta standard deviations higher or lower
</p>
<p>on the dependent variable. But it can be extremely misleading in the case
</p>
<p>The size of the standardized regression coefficient, like the size of the co-
</p>
<p>efficient itself, will depend on which category is excluded. In general,
</p>
<p>you should exercise caution when interpreting standardized regression
</p>
<p>coefficients for dummy variables in a multivariate regression analysis.
</p>
<p>of multi-category nominal-level variables, such as region in our example.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>In a bivariate regression model, there is only one independent vari-
</p>
<p>able, and it must be an interval-level measure. Importantly, the re-
</p>
<p>searcher can rarely be sure that the change observed in the dependent
</p>
<p>variable is due to one independent variable alone. And if variables that
</p>
<p>have an impact on the dependent measure are excluded, the predictions
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N504</p>
<p/>
</div>
<div class="page"><p/>
<p>related to the included factor, then the estimate of b for the included fac-
</p>
<p>tor will also be biased. Randomized experiments, which scatter differ-
</p>
<p>ent traits at random, offer a solution to the latter problem, but they are
</p>
<p>often impractical in criminal justice research. A statistical solution that
</p>
<p>enables us to correct for both types of bias is to create a multivariate
</p>
<p>regression model.
</p>
<p>In a multivariate regression model, there may be several independent
</p>
<p>variables, only one of which needs to be interval level. Such a model
</p>
<p>considers the effect of each independent variable, while holding all the
</p>
<p>other variables constant. A binary nominal-level variable included in a
</p>
<p>regression model is called a dummy variable. Regression coefficients
</p>
<p>measured using different scales may be compared with a standardized
</p>
<p>regression coefficient (Beta). A regression model is correctly speci-
</p>
<p>fied if the researcher has taken into account and correctly measured all
</p>
<p>of the relevant predictors of the dependent variable. Existing literature
</p>
<p>and prior research are suitable places to start.
</p>
<p>K e y  T e r m s
</p>
<p>biased Describing a statistic when its esti-
</p>
<p>mate of a population parameter does not
</p>
<p>center on the true value. In regression
</p>
<p>analysis, the omission of relevant indepen-
</p>
<p>dent variables will lead to bias in the esti-
</p>
<p>mate of Y. When relevant independent vari-
</p>
<p>ables are omitted and those measures are
</p>
<p>related to an independent variable included
</p>
<p>in regression analysis, then the estimate of
</p>
<p>the effect of that variable will also be biased.
</p>
<p>correctly specified regression model A
</p>
<p>regression model in which the researcher
</p>
<p>has taken into account all of the relevant
</p>
<p>predictors of the dependent variable and
</p>
<p>has measured them correctly.
</p>
<p>dummy variable A binary nominal-level
</p>
<p>variable that is included in a multivariate
</p>
<p>regression model.
</p>
<p>multivariate regression A technique for
</p>
<p>predicting change in a dependent variable,
</p>
<p>using more than one independent variable.
</p>
<p>randomized experiment A type of study
</p>
<p>in which the effect of one variable can be
</p>
<p>examined in isolation through random allo-
</p>
<p>cation of subjects to treatment and control,
</p>
<p>or comparison, groups.
</p>
<p>standardized regression coefficient
</p>
<p>(Beta) Weighted or standardized estimate
</p>
<p>of b that takes into account the standard
</p>
<p>deviation of the independent and the de-
</p>
<p>pendent variables. The standardized regres-
</p>
<p>sion coefficient is used to compare the ef-
</p>
<p>fects of independent variables measured on
</p>
<p>different scales in a multivariate regression
</p>
<p>analysis.
</p>
<p>of Y gained in a regression will be biased. If the excluded variables are
</p>
<p>K E Y  T E R M S 505</p>
<p/>
</div>
<div class="page"><p/>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>k Number of independent variables in the overall regression model
</p>
<p>Correlation coefficient for Y and X1
</p>
<p>Correlation coefficient for Y and X2
</p>
<p>Correlation coefficient for X1 and X2
</p>
<p>sY Standard deviation for Y
</p>
<p>Standard deviation for X1
</p>
<p>R 2 obtained for the full regression model
</p>
<p>R 2 obtained for the reduced regression model
</p>
<p>k fm Number of independent variables in the full regression model
</p>
<p>k rm Number of independent variables in the reduced regression model
</p>
<p>To calculate a multivariate regression coefficient for two independent
</p>
<p>variables:
</p>
<p>and
</p>
<p>A sample multivariate regression model with three independent variables:
</p>
<p>0 � b1X1 � b2X2 � b3X3 � e
</p>
<p>A population multivariate regression model with three independent
</p>
<p>variables:
</p>
<p>Y � �0 � �1X1 � �2X2 � �3X3 � �
</p>
<p>To calculate the standardized coefficient (Beta):
</p>
<p>Beta � b�sXsY�
</p>
<p>b  � �r )1 � ��
sY
s �
</p>
<p>b
1
 � �rY,X1 � (rY,X2rX1,X2)1 � r 2X1,X2 ��
</p>
<p>sY �
</p>
<p>R  2rm
</p>
<p>R  2fm
</p>
<p>sX1
</p>
<p>rX1,X2
</p>
<p>rY,X2
</p>
<p>rY,X1
</p>
<p>Y,
</p>
<p>Y � b
</p>
<p>Y,
</p>
<p>x
</p>
<p>x 2
</p>
<p>�  (r r, X1,X2
</p>
<p>r 2X1,X2
</p>
<p>s
</p>
<p>X1
</p>
<p>2X
</p>
<p>X1
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N506</p>
<p/>
</div>
<div class="page"><p/>
<p>To calculate an F-test on a subset of variables in a regression model:
</p>
<p>E x e r c i s e s
</p>
<p>16.1 Consider the following regression model, which purports to predict
the length of sentence given to convicted thieves:
</p>
<p>Y � b0 � bX � e
</p>
<p>where Y � length of sentence
</p>
<p>X � number of prior sentences
</p>
<p>a. List the variables you might wish to include in a more comprehen-
sive model. Include a brief statement about why each additional
variable should be included.
</p>
<p>b. Present your model in equation form.
</p>
<p>16.2 In an article in the newspaper, a researcher claims that low self-
esteem is the cause of crime. Upon closer inspection of the results in
the paper, you learn that the researcher has computed a bivariate
model using self-reported theft as the dependent variable (Y ) and self-
esteem as the one independent variable (X ).
</p>
<p>a. List the variables you might wish to include in a more comprehen-
sive model. Include a brief statement about why each additional
variable should be included.
</p>
<p>b. Present your model in equation form.
</p>
<p>16.3 A researcher has built a multivariate regression model to predict the
effect of prior offenses and years of education on the length of sen-
tence received by 100 convicted burglars. He feeds the data into a
computer package and obtains the following printout:
</p>
<p>Dependent Variable (Y): Length of Sentence (months)
</p>
<p>Independent Variable (X1 ): Number of Prior Offenses
</p>
<p>Independent Variable (X2 ): Years of Education
</p>
<p>F sig � 0.018
</p>
<p>R Square � 0.16
</p>
<p>X1 : b � �0.4 Sig t � 0.023
</p>
<p>X2 : b � �0.3 Sig t � 0.310
</p>
<p>Evaluate the results, taking care to explain the meaning of each of the
statistics produced by the computer.
</p>
<p>F � 
(R 2fm � R
</p>
<p>2
rm)/(k fm � k rm)
</p>
<p>(1 � R 2fm)/(N � k fm � 1)
</p>
<p>E X E R C I S E S 507</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 An analysis of the predictors of physical violence at school produced
the following results:
</p>
<p>Independent Variable b Beta
</p>
<p>Age (Years) 0.21 0.05
</p>
<p>Sex (Female � 1, Male � 0) �3.78 0.07
</p>
<p>�1.34 0.06
</p>
<p>1.96 0.33
</p>
<p>3.19 0.24
</p>
<p>2.05 0.27
</p>
<p>Explain what each regression coefficient (b) and standardized regres-
sion coefficient (Beta) means in plain English.
</p>
<p>16.5
</p>
<p>explain variations in the amount of drugs seized per month and
runs a regression analysis to check the effect of his independent
variable&mdash;the total number of customs officers on duty for each
</p>
<p>coefficient is �4.02. Danny is worried, however, that his bivariate
model might not be correctly specified, and he decides to add
another variable&mdash;the number of ships that arrive at the port each
month. He calculates the correlations between the three pairs of
variables, and the results are as follows:
</p>
<p>Y (drugs seized), X1 (customs officers): �0.55
</p>
<p>Y (drugs seized), X2 (ships arriving): �0.60
</p>
<p>X1 (customs officers), X2 (ships arriving): �0.80
</p>
<p>The standard deviations for the three variables are 20 kg (quantity of
drugs seized per month), 1.6 (number of customs officers on duty),
and 22.5 (number of ships arriving).
</p>
<p>a. Calculate the regression coefficient for customs officers.
</p>
<p>b. Calculate the regression coefficient for ships arriving.
</p>
<p>c. How do you account for the difference between your answer to
part a and the regression coefficient of �4.02 that Danny obtained
earlier?
</p>
<p>16.6 A study of prison violence examined the effects of two independent
variables&mdash;percent of inmates sentenced for a violent crime (X1) and
average amount of space per inmate (X2)&mdash;on the average number of
violent acts per day (Y ). All variables were measured for a random
</p>
<p>Race (White � 1, Non-white � 0)
</p>
<p>month&mdash;on the quantity of drugs seized. The resulting regression
</p>
<p>Number of Friends Arrested
</p>
<p>Danny has obtained figures on the amount of drugs seized per
</p>
<p>Number of Times Attacked by Others
</p>
<p>month at a seaport over the course of two years. He wishes to
</p>
<p>Number of Times Hit by Parents
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N508</p>
<p/>
</div>
<div class="page"><p/>
<p>selection of cell blocks in three prisons. The researcher reported the
following results:
</p>
<p>a. Calculate the regression coefficients for the effects of X1 and X2 on
Y. Explain what these coefficients mean in plain English.
</p>
<p>b. Calculate the standardized regression coefficients for the effects 
of X1 and X2 on Y. Explain what these coefficients mean in plain
English.
</p>
<p>c. Which one of the variables has the largest effect on prison vio-
</p>
<p>16.7 A study of recidivism classified offenders by type of punishment re-
ceived: prison, jail, probation, fine, or community service. A re-
searcher interested in the effects of these different punishments ana-
lyzes data on a sample of 967 offenders. She computes two regression
models. In the first, she includes variables for age, sex, race, number
of prior arrests, severity of the last conviction offense, and length of
punishment. The R 2 for this model is 0.27. In the second model, she
adds four dummy variables for jail, probation, fine, and community
service, using prison as the reference category. The R 2 for this model
is 0.35. Explain whether the type of punishment had an effect on re-
cidivism (assume a 5% significance level).
</p>
<p>16.8 A public opinion poll of 471 randomly selected adult respondents
asked about their views on the treatment of offenders by the courts.
Expecting race/ethnicity to be related to views about the courts, a re-
searcher classifies respondents as African American, Hispanic, and
white. To test for the effect of race/ethnicity, he computes one regres-
sion using information about the age, sex, income, and education of
the respondents and finds the R 2 for this model to be 0.11. In a sec-
ond regression, he adds two dummy variables for African American
and Hispanic, using white as the reference category. The R 2 for this
second model is 0.16. Explain whether the race/ethnicity of the re-
spondent had a statistically significant effect on views about the courts
(assume a 5% significance level).
</p>
<p> sX2 � 2.64
</p>
<p> sX1 � 10.52
</p>
<p> sY � 0.35
</p>
<p> rX1,X2
</p>
<p> rY,X2
</p>
<p> rY,X1 � 0.20
</p>
<p> � 0.20
</p>
<p>lence? Explain why.
</p>
<p> � 0.20
</p>
<p>E X E R C I S E S 509</p>
<p/>
</div>
<div class="page"><p/>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>In Chapter 15, we explored the basic features of the regression commands in 
</p>
<p>SPSS and Stata in the computation of a bivariate regression model. To compute 
</p>
<p>a multivariate regression model, we simply add additional independent variable 
</p>
<p>names to the list of independent variables on the command line. The following 
</p>
<p>exercises illustrate some of the additional features of the regression command. 
</p>
<p>Please see the appropriate SPSS (Chapter_16.sps) or Stata (Chapter_16.do)  
</p>
<p>syntax file for specific examples.
</p>
<p>SPSS
</p>
<p>Standardized Regression Coefficients (Betas)
</p>
<p>The standardized regression coefficients (Betas) are part of the standard  
</p>
<p>output for SPSS&rsquo;s linear regression command. In the table of results presenting 
</p>
<p>the coefficients, the standardized coefficients are located in the column  
</p>
<p>following those presenting the values for the regression coefficients (b) and the 
</p>
<p>standard errors of b. Nothing else is required to obtain the standardized  
</p>
<p>coefficients.
</p>
<p>F-Test for a Subset of Variables
</p>
<p>The computation of an F-test for a subset of variables requires a little  
</p>
<p>planning in setting up a multivariate linear regression model. When think-
</p>
<p>ing about your regression model and a test of one or more subsets of 
</p>
<p>variables, you will need to enter these independent variables on separate
</p>
<p>interested in, we would use the  following syntax:
</p>
<p>The trick here is to keep track of all the independent variables in your regression 
</p>
<p>model and determine whether they belong to the first or the second group&mdash;the 
</p>
<p>second group would be the subset of interest. For example, suppose we had an 
</p>
<p>interest in looking at whether demographic characteristics of offenders affected 
</p>
<p>punishment severity. In this case, we would then list the demographic character-
</p>
<p>istics (however measured) in the second block (i.e., /METHOD = ENTER line).
</p>
<p>We have also added the /STATISTICS option line to the REGRESSION 
</p>
<p>command. The reason for this is to force SPSS to compute the F-test on the 
</p>
<p>subset of variables and to simultaneously report all of the other results in a 
</p>
<p>linear regression analysis that it usually reports. Specifically, the items on the
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N510
</p>
<p>/METHOD = ENTER lines. In general, if we have one subset that we are  
</p>
<p>/STATISTICS line request the coefficient table (COEFF), model summary (R), 
</p>
<p>REGRESSION
</p>
<p>/STATISTICS COEFF R ANOVA CHANGE
</p>
<p>/DEPENDENT dep_var_name
</p>
<p>/METHOD = ENTER list_of_variables_NOT_in_subset
</p>
<p>/METHOD = ENTER list_of_variables_in_subset.</p>
<p/>
</div>
<div class="page"><p/>
<p>ANOVA table (ANOVA), and change in R2 when the second block of variables 
</p>
<p>is added to the regression model (CHANGE). The F-test on the subset of  
</p>
<p>variables is produced with the CHANGE option.
</p>
<p>The output from running this command is nearly identical to what you have 
</p>
<p>viewed previously. The major difference is that there will be two major rows of 
</p>
<p>results for all of the tables viewed in the output before&mdash;one row will be labeled 
</p>
<p>Model 1 and the other row Model 2. In other words, there will be a row for the 
</p>
<p>&ldquo;reduced&rdquo; model (Model 1 in SPSS) that contains only those variables included 
</p>
<p>in the first block of variables and a second row for the &ldquo;full&rdquo; model that includes 
</p>
<p>all variables (Model 2 in SPSS).
</p>
<p>The F-test for the subset of  variables can be found in the &ldquo;Model Summary&rdquo; 
</p>
<p>table of  results under the columns labeled &ldquo;Change Statistics.&rdquo; For Model 2, the 
</p>
<p>F-statistic for the subset of  variables appears in the column labeled &ldquo;F Change.&rdquo; 
</p>
<p>the next column, providing you with all the information you need to test whether 
</p>
<p>the subset of  variables makes a statistically significant contribution to the overall 
</p>
<p>regression model.
</p>
<p>C O M P U T E R  E X E R C I S E S 511
</p>
<p>Since the description of  the various pieces may be confusing, we encourage  
</p>
<p>you to open and run the accompanying SPSS syntax file for this chapter 
</p>
<p>(Chapter_16.sps).
</p>
<p>Residual Plot
</p>
<p>It is also possible with the regression command to analyze residuals in ways ranging 
</p>
<p>from simple to complex. Perhaps the most straightforward way of  analyzing residu-
</p>
<p>als is graphically, through the use of  a residual plot that SPSS can produce. There 
</p>
<p>are many different kinds of  residual plots that SPSS could create&mdash;we highlight 
</p>
<p>only one simple example here. A histogram of  the residuals from a regression analy-
</p>
<p>sis with a normal curve overlaid on the histogram is obtained as follows:
</p>
<p>where the /RESIDUALS line will request a plot of  residuals&mdash;the 
</p>
<p>HISTOGRAM(ZRESID) option specifies a histogram of  what are known as &ldquo;stand-
</p>
<p>ardized residuals.&rdquo; A histogram of  the residuals with the overlaid normal curve will 
</p>
<p>give you some idea of  how closely the residuals approximate a normal distribution 
</p>
<p>(which is what is to be expected). If  the residuals do not resemble a normal distribu-
</p>
<p>tion, this is often an indication of  a problem with the regression model, such as one 
</p>
<p>or more relevant independent variables having been omitted from the analysis.
</p>
<p>REGRESSION
</p>
<p>/DEPENDENT dep_var_name
</p>
<p>/METHOD = ENTER list_of_indep_vars
</p>
<p>/RESIDUALS HISTOGRAM(ZRESID).
</p>
<p>The value for the numerator degrees of  freedom (df
1
) will appear in the next 
</p>
<p>column to the right and will equal the number of  independent variables included 
</p>
<p>in the subset. The value for the denominator degrees of  freedom (df
2
) appears in </p>
<p/>
</div>
<div class="page"><p/>
<p>Stata
</p>
<p>Standardized Regression Coefficients (Betas)
</p>
<p>To request the standardized regression coefficients (Betas) in a multivariate 
</p>
<p>linear regression model in Stata, you will need to add the option b to a regress 
</p>
<p>command:
</p>
<p>The last column of output in the coefficient table will then report the  
</p>
<p>standardized coefficients.
</p>
<p>F-Test for a Subset of Variables
</p>
<p>In contrast to the cumbersome syntax in SPSS for testing a subset of  
</p>
<p>variables, the syntax required in Stata involves two steps: (1) estimate the full 
</p>
<p>regression model with the regress command and (2) test the subset of variables 
</p>
<p>using the testparm command. The form of the testparm command is simply
</p>
<p>C H A P T E R S I X T E E N :  M U L T I V A R I A T E R E G R E S S I O N512
</p>
<p>The output from running this command is an F-test on the set of variables listed 
</p>
<p>on the testparm command line.
</p>
<p>Residual Plot
</p>
<p>We illustrated at the end of Chapter 15 the process for computing residuals from 
</p>
<p>a linear regression analysis. To create a histogram of the residuals, we would use 
</p>
<p>the histogram command (discussed at the end of Chapter 3), and if we wanted 
</p>
<p>to overlay a normal curve, we use the normal option:
</p>
<p>Problems
</p>
<p> 1. Enter the data from Table 16.2. Run the regression command to  
</p>
<p>reproduce the unstandardized and standardized regression coefficients 
</p>
<p>presented in this chapter.
</p>
<p>a. Compute two bivariate regression models, using years in prison as the 
</p>
<p> independent variable in one regression and prior arrests as the inde-
</p>
<p>pendent variable in the second regression. Generate a histogram of  the 
</p>
<p> residuals for each regression model. What does the pattern of  results in 
</p>
<p>this plot suggest to you about the distribution of  error terms?
</p>
<p>b. Compute the multivariate model, and generate a histogram of  the 
</p>
<p> residuals for this regression model. How has the pattern of  error terms 
</p>
<p>changed relative to the two histograms produced in part a?
</p>
<p>to do Exercises 2 through 5.
</p>
<p>regress dep_var_name indep_var_names, b
</p>
<p>testparm list_of_subset_variables
</p>
<p>regress dep_var_name indep_var_names
</p>
<p>predict RES_1, r
</p>
<p>histogram RES_1, normal</p>
<p/>
</div>
<div class="page"><p/>
<p> 2. Compute a multivariate regression model, using number of  times the 
</p>
<p>student hit other students as the dependent variable. From the variables 
</p>
<p>included in the data file, select at least five independent variables that you 
</p>
<p>think have some relationship to hitting other students.
</p>
<p>-
</p>
<p>b. Generate a histogram of  the residuals for this regression model. What 
</p>
<p>does the pattern of  results in this plot suggest to you about the distri-
</p>
<p>bution of  error terms?
</p>
<p> 3. Compute a multivariate regression model, using number of  times  
</p>
<p>something worth $5 or less has been stolen as the dependent variable. 
</p>
<p>From the variables included in the data file, select at least five independ-
</p>
<p>ent variables that you think have some relationship to stealing something 
</p>
<p>worth $5 or less.
</p>
<p> 
</p>
<p>b. Generate a histogram of  the residuals for this regression model.  
</p>
<p>What does the pattern of  results in this plot suggest to you about the 
</p>
<p>distribution of  error terms?
</p>
<p> 4. Compute a multivariate regression model, using number of  times the 
</p>
<p>student cheated on exams as the dependent variable. From the variables 
</p>
<p>included in the data file, select at least five independent variables that you 
</p>
<p>think have some relationship to cheating on exams.
</p>
<p> 
</p>
<p>b. Generate a histogram of  the residuals for this regression model. What 
</p>
<p>does the pattern of  results in this plot suggest to you about the  
</p>
<p>distribution of  error terms?
</p>
<p> 5. Compute a multivariate regression model, using number of  times drunk as 
</p>
<p>the dependent variable. Use age, sex, race, employment status, hours spent 
</p>
<p>studying per week, grade point average, and number of  friends who use 
</p>
<p>alcohol as the independent variables.
</p>
<p>a. Use an F-test to test whether demographic characteristics&mdash;age, sex, 
</p>
<p>and race&mdash;affect drinking behavior.
</p>
<p>b. Use an F-test to test whether academic characteristics&mdash;hours spent 
</p>
<p>studying per week and grade point average&mdash;affect drinking behavior.
</p>
<p>C O M P U T E R  E X E R C I S E S 513</p>
<p/>
</div>
<div class="page"><p/>
<p>494 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>C h a p t e r  s e v e n t e e n
</p>
<p>Multivariate Regression:
</p>
<p>Additional Topics
</p>
<p>N o n - l i n e a r  r e l a t i o n s h i p s
</p>
<p>What are They?
</p>
<p>How are They Included in Regression Models?
</p>
<p>How are They Interpreted?
</p>
<p>I n t e r a c t i o n  e f f e c t s
</p>
<p>What are They?
</p>
<p>How are They Included in Regression Models?
</p>
<p>How are They Interpreted?
</p>
<p>M u l t i c o l l i n e a r i t y
</p>
<p>When Does It Arise?
</p>
<p>How is It Diagnosed?
</p>
<p>How is It Treated?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_17,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>IN THE PREVIOUS CHAPTER we extended the bivariate regression approach
by showing how we could include multiple independent variables simulta-
</p>
<p>neously in a single model. We illustrated how we incorporate variables
</p>
<p>measured not only at the interval level of measurement, but also nominal
</p>
<p>and ordinal independent variables, into a regression model. While the mod-
</p>
<p>els we have examined so far allow us to approximate linear relationships
</p>
<p>between various independent variables and the dependent variable, in the
</p>
<p>real world we are sometimes confronted with more complex research ques-
</p>
<p>tions that require us to make additional modifications to our model.
</p>
<p>For example, in the OLS regression model, our interpretation of the coef-
</p>
<p>ficients is based on the notion that there is a linear relationship between the
</p>
<p>independent and the dependent variable. But what if we find evidence of
</p>
<p>a curvilinear relationship? Or, theory suggests that there may be a non-linear
</p>
<p>relationship between two variables? Although the OLS regression model is
</p>
<p>based on the assumption of a linear relationship between the dependent
</p>
<p>and each of the independent variables, non-linear relationships can be
</p>
<p>incorporated into an OLS regression model in a straightforward manner.
</p>
<p>Another issue in the application of OLS regression is that the interpre-
</p>
<p>tation of the coefficients is based on the idea that each independent vari-
</p>
<p>able has a constant effect irrespective of the levels of other independent
</p>
<p>variables. For example, if we include a dummy variable we assume that
</p>
<p>the effect of every other independent variable is the same for men and
</p>
<p>women. But what if there was a good theoretical or policy reason to sus-
</p>
<p>pect that the effect of some variable was different for men and women?
</p>
<p>How would we incorporate that into our model? In the statistical litera-
</p>
<p>ture, these are known as interaction effects, and they allow us to test
</p>
<p>whether the effects of specific independent variables in a regression model
</p>
<p>vary by the level of other independent variables.
</p>
<p>In this chapter we also introduce an important problem that
</p>
<p>researchers sometimes face when estimating multivariate regression mod-
</p>
<p>els. We have emphasized so far that researchers must include all relevant
</p>
<p>independent variables in a model if it is to be correctly specified. Correct
</p>
<p>515</p>
<p/>
</div>
<div class="page"><p/>
<p>model specification, in turn, is necessary to avoid bias in regression models.
</p>
<p>But sometimes the inclusion of multiple independent variables can lead
</p>
<p>to a problem we term multicollinearity which is likely to lead to esti-
</p>
<p>mation of unstable regression coefficients. Multicollinearity refers to the
</p>
<p>situation where independent variables are very highly correlated with
</p>
<p>each other, which then makes it very difficult for OLS regression to deter-
</p>
<p>mine the unique effects of each independent variable.
</p>
<p>N o n - l i n e a r  R e l a t i o n s h i p s
</p>
<p>Policy-oriented research focused on the severity of punishment for convict-
</p>
<p>ed offenders illustrates that as the severity of an offender&rsquo;s prior record and
</p>
<p>the severity of the conviction offense increase, the severity of the punish-
</p>
<p>ment tends to increase.1 In the interpretation of OLS regression results, we
</p>
<p>would say something, for example, about how each one unit increase in
</p>
<p>the severity of an offender&rsquo;s prior record results in the length of a sentence
</p>
<p>increasing by some fixed time period (e.g., 8 months). Key to the interpre-
</p>
<p>tation of OLS regression coefficients is the idea that the level of the inde-
</p>
<p>pendent variable does not matter &ndash; each unit change is expected to result
</p>
<p>in the same change in the dependent variable regardless of whether we are
</p>
<p>looking at small or large values on the independent variable. What if this is
</p>
<p>not always the case?
</p>
<p>For example, an assumption often made in research on sentencing out-
</p>
<p>comes, but rarely examined, is the idea that first-time offenders (i.e., those
</p>
<p>with no prior record) or those offenders convicted of relatively minor
</p>
<p>forms of crime will be punished much more leniently than other offenders.
</p>
<p>Then, as the severity of prior record or of conviction offense increase,
</p>
<p>there is an expectation of an increasingly punitive response by the crimi-
</p>
<p>nal justice system. Put another way, there is an expectation of a non-linear
</p>
<p>relationship between the severity of the conviction offense or the offender&rsquo;s
</p>
<p>prior record and the severity of punishment &ndash; changes in the level of the
</p>
<p>dependent variable may vary by the level of the independent variable.
</p>
<p>Figure 17.1 presents a hypothetical plot for punishment severity and
</p>
<p>prior criminal history that reflects increasingly harsher punishments for
</p>
<p>offenders with more extensive criminal records.
</p>
<p>As can be seen in the figure, there is a gradual increase in the severi-
</p>
<p>ty of the punishment as the severity of the prior record increases. Then,
</p>
<p>the increases in the severity of the punishment become larger for the same
</p>
<p>unit increase in prior criminal history.
</p>
<p>The range of potential non-linear relationships is limitless and is
</p>
<p>bounded only by the imagination and creativity of the researcher and the
</p>
<p>theoretical basis for conducting the research. Yet, while there may be a
</p>
<p>wide range of possible non-linear relationships, most researchers will con-
</p>
<p>fine their analyses to a relatively limited group of non-linear possibilities,
</p>
<p>C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>1 Michael H. Tonry, Sentencing Matters (New York, Oxford University Press, 1996).
</p>
<p>516</p>
<p/>
</div>
<div class="page"><p/>
<p>N O N - L I N E A R R E L A T I O N S H I P S
</p>
<p>some of which are displayed in Figure 17.2. Panel (a) presents what is
</p>
<p>referred to as a quadratic equation. All that this means is that a squared
</p>
<p>term has been added to the equation to give it a form such as Y = X + X2.
</p>
<p>The quadratic equation is one of the more commonly used transforma-
</p>
<p>tions in criminology and criminal justice, and has had frequent applica-
</p>
<p>tion in the study of age-related behavior.
</p>
<p>Panel (b) presents an inverse function of the form Y = 1/X. This kind
</p>
<p>of transformation helps to capture relationships where there is a decreasing
</p>
<p>negative effect of the independent variable on the dependent variable.
</p>
<p>Prior Criminal History
</p>
<p>P
u
</p>
<p>n
is
</p>
<p>h
m
</p>
<p>e
n
</p>
<p>t 
S
</p>
<p>e
v
e
ri
</p>
<p>ty
</p>
<p>Figure 17.1 Hypothetical Non-linear Relationship Between Punishment Severity and Prior Criminal
History
</p>
<p>X
</p>
<p>Y
</p>
<p>Figure 17.2 Common Non-linear Relationships Used in Criminal Justice Research
</p>
<p>(a) Quadratic
</p>
<p>517</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>Panel (c) presents a square root transformation of the form Y = X. This
</p>
<p>kind of transformation is useful when there is a diminishing positive
</p>
<p>impact of the independent variable on the dependent variable.
</p>
<p>Finding a Non-linear Relationship: Graphical Assessment
</p>
<p>Perhaps the most straightforward way of exploring data for a non-linear
</p>
<p>relationship is to use a line graph (see Chapter 3). A simple scatterplot 
</p>
<p>(discussed in Chapter 14) will often contain so many data points that it is
</p>
<p>difficult, if not impossible, to discern any pattern in the data. A line graph
</p>
<p>that plots the mean of the dependent variable against the value of the
</p>
<p>X
</p>
<p>Y
</p>
<p>(b) Inverse
</p>
<p>Figure 17.2 Continued
</p>
<p>X
</p>
<p>Y
</p>
<p>(c) Square Root
</p>
<p>518</p>
<p/>
</div>
<div class="page"><p/>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>100
</p>
<p>120
</p>
<p>140
</p>
<p>160
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
</p>
<p>Offense Severity
</p>
<p>L
e
</p>
<p>n
g
</p>
<p>th
 o
</p>
<p>f 
S
</p>
<p>e
n
</p>
<p>te
n
</p>
<p>c
e
</p>
<p>Mean Length of Sentence OLS Regression Line
</p>
<p>Figure 17.3 Plot for Mean Length of Sentence by Offense Severity for Offenders in Pennsylvania
</p>
<p>independent variable will likely provide a rough indication of the nature of
</p>
<p>the bivariate relationship between the two variables. For example, Figure 17.3
</p>
<p>presents the mean for length of sentence against the severity of the convic-
</p>
<p>tion offense for over 20,000 offenders sentenced in Pennsylvania in 1998.2
</p>
<p>As you look at Figure 17.3, you can see that there is a gradual, linear
</p>
<p>increase in length of sentence as offense severity increases to about level
</p>
<p>6 to 7. At that point, the increases in sentence length become larger for each
</p>
<p>additional increase in offense severity. To highlight the curvilinear nature of
</p>
<p>the relationship between offense severity and length of punishment, the
</p>
<p>OLS regression line for these data is overlayed in Figure 17.3, indicating that
</p>
<p>the straight-line relationship does not capture the relationship between
</p>
<p>length of sentence and severity of offense particularly well.
</p>
<p>Incorporating Non-linear Relationships into an OLS Model
</p>
<p>Assuming that we have good reason for assuming that a non-linear
</p>
<p>relationship exists between the dependent variable and one or more of
</p>
<p>the independent variables, how do we incorporate this information into
</p>
<p>2 These data are available through the National Archive of Criminal Justice Data and can
be accessed at http://www.icpsr.umich.edu/NACJD
</p>
<p>519N O N - L I N E A R R E L A T I O N S H I P S</p>
<p/>
<div class="annotation"><a href="http://www.icpsr.umich.edu/NACJD">http://www.icpsr.umich.edu/NACJD</a></div>
</div>
<div class="page"><p/>
<p>the OLS regression model? The first step, as noted above, is to try and
</p>
<p>gain a sense of the relationship graphically. In most circumstances, if the-
</p>
<p>ory suggests or if we find evidence of a curvilinear relationship, the most
</p>
<p>straightforward approach is to add a quadratic term &ndash; the squared value of
</p>
<p>the independent variable &ndash; such as that in Panel (a) of Figure 17.2. More
</p>
<p>formally, a quadratic regression equation would have the following form:
</p>
<p>Y = b
0
</p>
<p>+ b
1
X
</p>
<p>1
+ b
</p>
<p>2
X
</p>
<p>1
2
</p>
<p>where Y represents the dependent variable
</p>
<p>X
1
</p>
<p>represents the independent variable
</p>
<p>In our example presented in Figure 17.3, we have evidence of a curvi-
</p>
<p>linear relationship that might be accounted for by adding a squared term
</p>
<p>for offense severity to a regression equation. We begin by noting that the
</p>
<p>OLS regression line portrayed in Figure 17.3 is
</p>
<p>Y = &minus;9.85 + 4.62 X
1
</p>
<p>where Y represents length of sentence (in months)
</p>
<p>X
1
</p>
<p>represents offense severity
</p>
<p>To incorporate a non-linear relationship, we begin by transforming the
</p>
<p>variable &ndash; in this case offense severity &ndash; and then add this transformed
</p>
<p>variable to the regression equation. In most statistical software packages
</p>
<p>this would simply involve the creation of a new variable which represents
</p>
<p>the original variable squared. When we square offense severity and add
</p>
<p>it to the regression equation, we obtain the following equation:
</p>
<p>Y = b
0
</p>
<p>+ b
1
X
</p>
<p>1
+ b
</p>
<p>2
(X
</p>
<p>1
* X
</p>
<p>1
) = b
</p>
<p>0
+ b
</p>
<p>1
X
</p>
<p>1
+ b
</p>
<p>2
X
</p>
<p>1
2.
</p>
<p>If we then estimate this new regression equation that includes both the
</p>
<p>original measure of offense severity and the squared value of offense
</p>
<p>severity, we obtain the following results:
</p>
<p>Y = 7.07 &minus; 3.99 X
1
</p>
<p>+ 0.77 X
1
2
</p>
<p>Substantively, this regression equation captures the curvilinear rela-
</p>
<p>tionship between offense severity and sentence length much better than
</p>
<p>a straight-line relationship, since there are increasingly larger increases in
</p>
<p>sentence length for each unit change in offense severity. Figure 17.4
</p>
<p>presents the mean sentence length by offense severity (similar to that in
</p>
<p>Figure 17.3) along with the new regression line based on including the
</p>
<p>quadratic term.
</p>
<p>Interpreting Non-linear Coefficients
</p>
<p>In many practical applications of adding non-linear terms to OLS regres-
</p>
<p>sion models, there is often less emphasis on the interpretation of the indi-
</p>
<p>vidual coefficients that represent transformed variables. The reason for
</p>
<p>520 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>this is the difficulty in making sense of individual coefficients. For example,
</p>
<p>in the example using the data on offenders sentenced in Pennsylvania, the
</p>
<p>coefficient for the squared value of offense severity is given as 0.77. Like
</p>
<p>any other OLS coefficient, this coefficient can be interpreted in the con-
</p>
<p>text of one unit changes of the transformed variable:
</p>
<p>For each one unit increase in the squared value of offense severity, the length
</p>
<p>of sentence is expected to increase by 0.77 months.
</p>
<p>Unfortunately, this kind of interpretation tends not to have much intu-
</p>
<p>itive meaning for most people &ndash; researchers and others. Consequently, the
</p>
<p>description of results using non-linear transformations in OLS regression
</p>
<p>models will often focus on the general pattern of results, rather than on
</p>
<p>the specific coefficients. This is not entirely satisfactory, however, because
</p>
<p>it still leaves the reader wondering whether the transformation added
</p>
<p>much to the analysis. Our suggestion is to use graphs, such as that pre-
</p>
<p>sented in Figure 17.4, which do provide an effective way to convey evi-
</p>
<p>dence of a non-linear relationship between the dependent and inde-
</p>
<p>pendent variables. What makes this kind of plot particularly useful is that
</p>
<p>it conveys both the pattern in the observed data and the predicted values
</p>
<p>based on the estimated regression model.
</p>
<p>Note on Statistical Significance
</p>
<p>Estimating statistical significance for a non-linear term does not present any
</p>
<p>new problem to our understanding of multivariate regression. The statisti-
</p>
<p>cal significance of both the individual coefficients and the overall model 
</p>
<p>in an OLS regression model incorporating non-linear terms is determined
</p>
<p>521
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>100
</p>
<p>120
</p>
<p>140
</p>
<p>160
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
</p>
<p>Offense Severity
</p>
<p>L
e
</p>
<p>n
g
</p>
<p>th
 o
</p>
<p>f 
S
</p>
<p>e
n
</p>
<p>te
n
</p>
<p>c
e
</p>
<p>Mean Length of Sentence Regression Line (Quadratic)
</p>
<p>Figure 17.4 Plot for Mean Length of Sentence by Offense Severity for Offenders in Pennsylvania with
Quadratic Regression Line
</p>
<p>N O N - L I N E A R R E L A T I O N S H I P S</p>
<p/>
</div>
<div class="page"><p/>
<p>522 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>in the same way as for any other OLS regression model. For individual
</p>
<p>coefficients, we use the t-test and for the overall model, we use the F-test.
</p>
<p>Summary
</p>
<p>How does one know whether to include a non-linear term in a regression
</p>
<p>model? In light of the many different non-linear relationships that are pos-
</p>
<p>sible &ndash; we could transform any number of our independent variables in
</p>
<p>an OLS regression model &ndash; how do we settle on an appropriate model?
</p>
<p>The single best guide for the researcher is prior theory and research. If a
</p>
<p>theoretical perspective claims a non-linear relationship or prior research
</p>
<p>has established a non-linear relationship between two variables, then the
</p>
<p>researcher may want to examine a non-linear relationship in the regres-
</p>
<p>sion analysis. Without the guidance of theory and prior research, the
</p>
<p>researcher is better off using an OLS model without any non-linear 
</p>
<p>relationships included. If subsequent analyses, such as a residual analysis
</p>
<p>(discussed in Chapter 16) indicate a non-linear relationship, then some
</p>
<p>kind of transformation of an independent variable may be in order.
</p>
<p>I n t e r a c t i o n  E f f e c t s
</p>
<p>A number of different theories of crime and delinquency make statements
</p>
<p>about how the effect of one variable will vary by the level of some other
</p>
<p>variable. A perspective known as general strain theory hypothesizes that
</p>
<p>the effects of psychological strain (e.g., having one&rsquo;s parents file for
</p>
<p>divorce) on delinquency will vary by the ability of a youth to adapt to
</p>
<p>strain.3 For example, if an individual characteristic, such as self-esteem,
</p>
<p>helps individuals to adapt to various forms of strain, then the effect of that
</p>
<p>strain may vary by the level of self-esteem: as the level of self-esteem
</p>
<p>increases, the effect of strain on the chances of delinquency may become
</p>
<p>smaller. Alternatively, research on criminal justice decision-making has
</p>
<p>suggested that the effects of offender characteristics, such as the offend-
</p>
<p>er&rsquo;s age, may differentially affect the severity of punishment across differ-
</p>
<p>ent racial or ethnic categories.4
</p>
<p>Assuming that we have a rationale for including an interactions effect,
</p>
<p>how do we incorporate it into our regression model? Let us begin with a
</p>
<p>simple regression model that has two independent variables X
1
</p>
<p>and X
2
.
</p>
<p>Y = b
0
</p>
<p>+ b
1
X
</p>
<p>1
+ b
</p>
<p>2
X
</p>
<p>2
+ e
</p>
<p>3 R. Agnew, 1992, Foundation for a general strain theory of crime and delinquency,
Criminology, 30, 47-87.
4 D. Steffensmeier, J. Kramer, and J. Ulmer, 1995, Age differences in sentencing,
Justice Quarterly, 12, 583-602.</p>
<p/>
</div>
<div class="page"><p/>
<p>To add an interaction effect to a regression model, all that we need to
</p>
<p>do is to compute the product of the two variables: X
1
*X
</p>
<p>2
= X
</p>
<p>3
. We then
</p>
<p>add this term to the regression equation:
</p>
<p>Y = b
0
</p>
<p>+ b
1
X
</p>
<p>1
+ b
</p>
<p>2
X
</p>
<p>2
+ b
</p>
<p>3
X
</p>
<p>3
+ e
</p>
<p>Where X
3
</p>
<p>= X
1
*X
</p>
<p>2
</p>
<p>Note that we now have an additional regression coefficient (b
3
) in the
</p>
<p>model that represents the interaction of the variables X
1
and X
</p>
<p>2
, which we
</p>
<p>will need to interpret. The interpretation of interaction effects can be quite
</p>
<p>complicated, with the degree of complexity based on the level of meas-
</p>
<p>urement of the two variables.
</p>
<p>Interaction of a Dummy Variable and Interval-Level Variable
</p>
<p>To illustrate the process of interpreting interaction effects, it is useful to
</p>
<p>begin with a relatively simple case: the interaction of a dummy variable with
</p>
<p>a variable measured at the interval level of measurement. In the regression
</p>
<p>model above, let us assume that X
2
</p>
<p>is a dummy variable, where the
</p>
<p>two categories are coded as either 0 or 1.
</p>
<p>It is now possible to work through a series of regression equations,
</p>
<p>much like we did in the previous chapter in our discussion of dummy
</p>
<p>variables, by inserting different values for X
2
. Note that the key difference
</p>
<p>is we now have more than one place where we need to insert values for
</p>
<p>the dummy variable.
</p>
<p>If we set the value for X
2
= 1, we have the following regression equation:
</p>
<p>Y = b
0
</p>
<p>+ b
1
X
</p>
<p>1
+ b
</p>
<p>2
X
</p>
<p>2
+ b
</p>
<p>3
(X
</p>
<p>1
*X
</p>
<p>2
) = b
</p>
<p>0
+ b
</p>
<p>1
X
</p>
<p>1
+ b
</p>
<p>2
(1) + b
</p>
<p>3
(X
</p>
<p>1
*1)
</p>
<p>Which reduces to:
</p>
<p>Y = b
0
</p>
<p>+ b
1
X
</p>
<p>1
+ b
</p>
<p>2
+ b
</p>
<p>3
X
</p>
<p>1
</p>
<p>By rearranging our terms, we can rewrite the regression equation as:
</p>
<p>Y = (b
0
</p>
<p>+ b
2
) + b
</p>
<p>1
X
</p>
<p>1
+ b
</p>
<p>3
X
</p>
<p>1
= (b
</p>
<p>0
+ b
</p>
<p>2
) + (b
</p>
<p>1
+ b
</p>
<p>3
)X
</p>
<p>1
</p>
<p>As in the previous chapter, we see that when we focus our attention
</p>
<p>on the category with the value 1, the model intercept changes by the
</p>
<p>value of the coefficient for that variable (i.e., b
2
). What is different in the
</p>
<p>above equation is that the effect of variable X
1
is now the sum of two dif-
</p>
<p>ferent regression coefficients: the original coefficient for X
1
</p>
<p>(i.e., b
1
) and
</p>
<p>the coefficient for the interaction term (i.e., b
3
).
</p>
<p>How do we now interpret the effect of X
1
? After summing the two
</p>
<p>regression coefficients b
1
</p>
<p>and b
3
, we would say that for cases that had a
</p>
<p>value of 1 on X
2
</p>
<p>(i.e., the cases were in Group 1), for each one unit
</p>
<p>increase in X
1
, Y is expected to change by b
</p>
<p>1
+ b
</p>
<p>3
units.
</p>
<p>I N T E R A C T I O N E F F E C T S 523</p>
<p/>
</div>
<div class="page"><p/>
<p>When we set the value for X
2
= 0 (the reference category for our dummy
</p>
<p>variable &ndash; Group 0), we now have the following regression equation:
</p>
<p>Y = b
0
</p>
<p>+ b
1
X
</p>
<p>1
+ b
</p>
<p>2
(0) + b
</p>
<p>3
(X
</p>
<p>1
*0)
</p>
<p>Which reduces to:
</p>
<p>Y = b
0
</p>
<p>+ b
1
X
</p>
<p>1
</p>
<p>This indicates that the model intercept (b
0
) and coefficient for X
</p>
<p>1
(b
</p>
<p>1
)
</p>
<p>represent the intercept for the reference category on X
2
</p>
<p>and the effect of
</p>
<p>X
1
</p>
<p>for cases in the reference category, respectively.
</p>
<p>To make the example more concrete, suppose that after estimating this
</p>
<p>regression equation, we find the following results:
</p>
<p>b
0
</p>
<p>= 2.5
</p>
<p>b
1
</p>
<p>= 3.2
</p>
<p>b
2
</p>
<p>= 1.9
</p>
<p>b
3
</p>
<p>= 1.3
</p>
<p>By inserting the values for the regression coefficients into the regres-
</p>
<p>sion equation, we have the following:
</p>
<p>Y = 2.5 + 3.2 X
1
</p>
<p>+ 1.9 X
2
</p>
<p>+ 1.3 (X
1
*X
</p>
<p>2
)
</p>
<p>For X
2
</p>
<p>= 1, we have the following:
</p>
<p>Y = 2.5 + 3.2 X
1
</p>
<p>+ 1.9 (1) + 1.3 (X
1
*1) 
</p>
<p>= (2.5 + 1.9) + (3.2 + 1.3) X
1
</p>
<p>= 4.4 + 4.5 X
1
</p>
<p>And for X
2
</p>
<p>= 0, we have:
</p>
<p>Y = 2.5 + 3.2 X
1
</p>
<p>+ 1.9 (0) + 1.3 (X
1
*0) = 2.5 + 3.2 X
</p>
<p>1
</p>
<p>The interpretation of the effect of X
1
is straightforward, but we need to
</p>
<p>make sure that we are clear about the group for which we are interpret-
</p>
<p>ing the effect of X
1
. Thus, for X
</p>
<p>2
= 1, for each one unit increase in X
</p>
<p>1
, we
</p>
<p>expect Y to increase by 4.5 units. When X
2
= 0, for each one unit increase
</p>
<p>in X
1
, Y is expected to increase by 3.2 units. Substantively, this type of
</p>
<p>result would allow a researcher to say that the effect of X
1
</p>
<p>varied across
</p>
<p>the groups measured in X
2
. As a visual aid to understanding these results,
</p>
<p>X
2
</p>
<p>= 0 is represented by the dashed line and group X
2
</p>
<p>= 1 is represented
</p>
<p>by the solid line.
</p>
<p>Up to this point, we have assumed that all of the coefficients are positive.
</p>
<p>Figure 17.6 presents several additional possibilities for various combinations
</p>
<p>524 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>we have presented the two regression lines in Figure 17.5, where group</p>
<p/>
</div>
<div class="page"><p/>
<p>A N E X A M P L E : R A C E A N D P U N I S H M E N T S E V E R I T Y 525
</p>
<p>X1
</p>
<p>Y
</p>
<p>X2 = 0
</p>
<p>X2 = 1
</p>
<p>Figure 17.5 Regression Lines for the Interaction of X
1
</p>
<p>and X
2
</p>
<p>of positive and negative values for b
1
</p>
<p>and b
3
</p>
<p>(we assumed that b
0
</p>
<p>and b
2
</p>
<p>were positive in each plot). (Keep in mind that b
1
represents the effect of X
</p>
<p>1
</p>
<p>for the reference category of the dummy variable and b
3
represents the value
</p>
<p>of the interaction effect.) Panel (a) is comparable to the preceding example,
</p>
<p>where b
1
and b
</p>
<p>3
are both positive. Panel (b) illustrates a hypothetical exam-
</p>
<p>ple when b
1
</p>
<p>is positive and b
3
</p>
<p>is negative. Panels (c) and (d) illustrate pos-
</p>
<p>sible patterns when b
1
</p>
<p>is negative and b
3
</p>
<p>is positive (panel (c)) or negative
</p>
<p>(panel (d)). Clearly, there are many other possibilities, but we wanted to pro-
</p>
<p>vide a few illustrations for different patterns that researchers have had to
</p>
<p>address in their analyses.
</p>
<p>A n  E x a m p l e :  R a c e  a n d  P u n i s h m e n t  S e v e r i t y
</p>
<p>Suppose that we are interested in testing whether the severity of a crimi-
</p>
<p>nal offense differentially affects the length of time offenders are sentenced
</p>
<p>to prison by race. Put another way, does the severity of the conviction
</p>
<p>offense affect the severity of punishment in the same way for offenders
</p>
<p>of different races? We again use data on the sentences of over 20,000
</p>
<p>offenders sentenced to prison in Pennsylvania in 1998 to illustrate the
</p>
<p>test for an interaction effect between severity of offense and race of
</p>
<p>offender. To simplify our model here, we measure race as a dummy vari-
</p>
<p>able (0 = white, 1 = African American). Offense severity is scored by the
</p>
<p>Pennsylvania Sentencing Commission and has values ranging from 1 to 14</p>
<p/>
</div>
<div class="page"><p/>
<p>526 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>X1
</p>
<p>Y
</p>
<p>X2=0
</p>
<p>X2=1
</p>
<p>Figure 17.6 Hypothetical Interaction Effects for Different Combinations of Positive and Negative
Interaction Effects
</p>
<p>(a) b
1
</p>
<p>and b
3
</p>
<p>are both positive 
</p>
<p>(b) b
1
</p>
<p>is positive and b
3
</p>
<p>is negative
</p>
<p>X1
</p>
<p>Y
</p>
<p>X2=0
</p>
<p>X2=1</p>
<p/>
</div>
<div class="page"><p/>
<p>527
</p>
<p>Figure 17.6 Continued
</p>
<p>1
</p>
<p>Y
</p>
<p>X2=0
</p>
<p>X2=1
</p>
<p>X1
</p>
<p>Y
</p>
<p>X2=0
</p>
<p>X2=1
</p>
<p>(c) b
1
</p>
<p>is negative and b
3
</p>
<p>is positive
</p>
<p>(d) b
1
</p>
<p>and b
3
</p>
<p>are both negative
</p>
<p>X
</p>
<p>A N E X A M P L E : R A C E A N D P U N I S H M E N T S E V E R I T Y</p>
<p/>
</div>
<div class="page"><p/>
<p>and sentence length is measured in months sentenced to prison. The
</p>
<p>regression model we set out to test can be written as:
</p>
<p>Y = b
0
</p>
<p>+ b
1
</p>
<p>X
1
</p>
<p>+ b
2
</p>
<p>X
2
</p>
<p>+ b
3
</p>
<p>X
1
*X
</p>
<p>2
</p>
<p>where Y represents length of sentence (in months)
</p>
<p>X
1
</p>
<p>represents offense severity
</p>
<p>X
2
</p>
<p>represents race
</p>
<p>When we estimate this regression model, we produce the following set
</p>
<p>of results:
</p>
<p>Y = &minus;8.14 + 4.12 X
1
</p>
<p>&minus; 5.41 X
2
</p>
<p>+ 1.31 X
1
*X
</p>
<p>2
</p>
<p>Using the same approach as above, we begin by focusing on African
</p>
<p>Americans (X
2
</p>
<p>= 1):
</p>
<p>Y = &minus;8.14 + 4.12 X
1
</p>
<p>&minus; 5.41 (1) + 1.31 (X
1
*1)
</p>
<p>= (&minus;8.14 &minus; 5.41) + (4.12 + 1.31) X
1
</p>
<p>= &minus;13.55 + 5.43 X
1
</p>
<p>For whites, the equation is:
</p>
<p>Y = &minus;8.14 + 4.12 X
1
</p>
<p>&minus; 5.41 (0) + 1.31 (X
1
*0)
</p>
<p>= &minus;8.14 + 4.12 X
1
</p>
<p>Substantively, we can now directly interpret the effect of offense sever-
</p>
<p>ity for white and African American offenders separately. Among white
</p>
<p>offenders, each one unit increase in offense severity, is expected to
</p>
<p>increase sentence length by 4.12 months, while for African American
</p>
<p>offenders, each one unit increase in offense severity is expected to
</p>
<p>increase sentence length by 5.43 months. More succinctly, these results
</p>
<p>suggest that the effect of offense severity on punishment severity is greater
</p>
<p>for African American offenders than for white offenders. These results are
</p>
<p>presented graphically in Figure 17.7. The dashed line reflects the slope for
</p>
<p>offense severity on sentence length for white offenders, while the solid line
</p>
<p>reflects the effect for African American offenders. As one moves further out
</p>
<p>the x-axis to greater values for offense severity, there is an increasingly
</p>
<p>greater effect for African American offenders compared to white offenders.
</p>
<p>Interaction Effects between Two Interval-level Variables
</p>
<p>Up to this point, our attention has been focused on interaction effects
</p>
<p>involving one variable measured at the interval level of measurement and
</p>
<p>one dummy variable measured at the nominal level of measurement. 
</p>
<p>The inclusion of an interaction effect between two interval-level variables
</p>
<p>in a regression model is done in exactly the same way &ndash; we compute a
</p>
<p>product of the two variables and add the product to the regression equa-
</p>
<p>tion. The interpretation of the interaction effect is much more complex,
</p>
<p>528 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>however, since we are no longer able to simplify the regression equation
</p>
<p>to represent the effect of one variable for two different groups.
</p>
<p>In some cases we may not be concerned with a specific interpretation
</p>
<p>of the interaction term. For example, we may want to simply identify
</p>
<p>whether the interaction between two measures is statistically significant.
</p>
<p>A statistically significant interaction term between two measures would
</p>
<p>suggest that their effect cannot be measured only by the additive effects
</p>
<p>of each measure in the model, but rather there is an additional effect that
</p>
<p>is measured by the interaction term.
</p>
<p>It may help to conceptualize this issue if we turn to a substantive exam-
</p>
<p>ple. Many sociologists have suggested that extra-legal variables such as
</p>
<p>income and social status impact upon sentencing outcomes.5 In a simple
</p>
<p>additive model each of these factors would have some defined independ-
</p>
<p>ent effect on the severity of a sentence measured in months of imprison-
</p>
<p>ment. This model is illustrated in equation form below.
</p>
<p>Y = b
0
</p>
<p>+ b
1
</p>
<p>X
1
</p>
<p>+ b
2
</p>
<p>X
2
</p>
<p>where Y represents length of sentence (in months)
</p>
<p>X
1
</p>
<p>represents income
</p>
<p>X
2
</p>
<p>represents social status
</p>
<p>529
</p>
<p>&minus;20
</p>
<p>&minus;10
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>50
</p>
<p>60
</p>
<p>70
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
</p>
<p>Offense Severity
</p>
<p>L
e
n
</p>
<p>g
th
</p>
<p> o
f 
</p>
<p>S
e
n
</p>
<p>te
n
</p>
<p>c
e
 (
</p>
<p>M
o
</p>
<p>n
th
</p>
<p>s
)
</p>
<p>White African American
</p>
<p>Figure 17.7 Regression Lines for the Effect of Offense Severity on Sentence Length by Race of
Offender
</p>
<p>5 Donald J Black, The Behavior of Law (New York; Academic Press, 1976).
</p>
<p>A N E X A M P L E : R A C E A N D P U N I S H M E N T S E V E R I T Y</p>
<p/>
</div>
<div class="page"><p/>
<p>530 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>But what if the researcher believed that the effect of income and social sta-
</p>
<p>tus was not simply additive but also multiplicative, meaning that there was
</p>
<p>an added effect that was due to the interaction between the two. This the-
</p>
<p>ory is represented in the equation below:
</p>
<p>Y = b
0
</p>
<p>+ b
1
</p>
<p>X
1
</p>
<p>+ b
2
</p>
<p>X
2
</p>
<p>+ b
3
</p>
<p>X
1
* X
</p>
<p>2
</p>
<p>where Y represents length of sentence (in months)
</p>
<p>X
1
</p>
<p>represents income
</p>
<p>X
2
</p>
<p>represents social status
</p>
<p>In this case the researcher is hypothesizing that there is not only the inde-
</p>
<p>pendent effect of income and of social class, but that there is an additional
</p>
<p>interaction effect that is measured by multiplying income by social class.
</p>
<p>What if this effect is statistically significant? What interpretation can the
</p>
<p>researcher draw? To illustrate this we take a hypothetical example of regres-
</p>
<p>sion results as reported below:
</p>
<p>Y = 7.2 &minus; 2.4 X
1
</p>
<p>&minus; 1.6 X
2
</p>
<p>&minus; 1.1 X
1
* X
</p>
<p>2
</p>
<p>The additional interaction term in this case suggests that there is an addi-
</p>
<p>tional benefit beyond that of the additive independent effects of income
</p>
<p>and social status that must be taken into account. In a very simple inter-
</p>
<p>pretation, we can say that not only does a high income high status indi-
</p>
<p>vidual receive a benefit from their income and status, but when the
</p>
<p>individual is high on both measures simultaneously they gain an added
</p>
<p>benefit above and beyond that provided by each characteristic on its own.
</p>
<p>While models specified in this way can help us to identify additional
</p>
<p>impacts that come from the interactions between interval level variables,
</p>
<p>it is very hard to develop interpretations beyond what we have noted
</p>
<p>above. But the researcher can adjust such models to develop a more easily
</p>
<p>interpretable understanding of interaction terms.
</p>
<p>Conceptually, when we have an interaction between two interval-
</p>
<p>level variables, we are testing the idea that the effect of one interval-level
</p>
<p>variable varies by the level of the second interval-level variable. For
</p>
<p>example, in the example noted above from general strain theory, the
</p>
<p>hypothesis is that the effect of strain varies by the level of self-esteem. In
</p>
<p>practice, the difficulty we often have in the interpretation of interaction
</p>
<p>effects between two interval-level variables is in choosing values for one
</p>
<p>variable to represent the effect of the other variable. In effect, we have
</p>
<p>already done this in our example of an interaction between the dummy
</p>
<p>variable and the interval-level variable. Recall that when we include an
</p>
<p>interaction effect between a dummy variable and an interval-level variable,
</p>
<p>we set the value of the dummy variable to either 0 or 1 and then inter-
</p>
<p>pret the effect of the interval-level variable for each group represented in
</p>
<p>the dummy variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>In trying to determine how to interpret the interaction between two
</p>
<p>interval-level variables, we would encourage you to first consider which
</p>
<p>variable is of key importance for a study. The second variable would then
</p>
<p>be set at a limited number of values, which allows the researcher to see
</p>
<p>how the effect of the key variable changes across levels of the second
</p>
<p>variable. For example, if we again refer to the interaction between strain
</p>
<p>and self-esteem, the key theoretical variable is strain. Following these
</p>
<p>guidelines, we would then want to interpret the effect of strain for several
</p>
<p>designated values of self-esteem. Clearly, we could interpret the interaction
</p>
<p>effect the other way: the effect of self-esteem for specified levels of strain,
</p>
<p>but this is not a key piece of the theory.
</p>
<p>What values do we use for the second interval-level variable? For any
</p>
<p>given interval-level variable, there may be hundreds or thousands of real-
</p>
<p>istic possible values that we could use. We think that a useful place to start
</p>
<p>is to use the mean, one standard deviation above and below the mean,
</p>
<p>and two standard deviations above and below the mean. This will cover
</p>
<p>a wide range of possible values of the variable we are using and should
</p>
<p>be ample for understanding how our key variable changes across values
</p>
<p>of the second variable. In other cases, where there may be meaningful
</p>
<p>values on the second independent variable that have more intuitive mean-
</p>
<p>ing to the reader, these values should be used. For example, if we were
</p>
<p>to fix years of education, we might use 8, 12, and 16 to reflect the com-
</p>
<p>pletion of junior high school, high school, and undergraduate collegiate
</p>
<p>education, respectively.
</p>
<p>For example, suppose that we have estimated a regression model with
</p>
<p>two interval-level variables X
1
</p>
<p>and X
2
</p>
<p>and the interaction of X
1
</p>
<p>and X
2
:
</p>
<p>Y = 2.3 + 1.7 X
1
</p>
<p>+ 2.0 X
2
</p>
<p>+ 0.5 (X
1
* X
</p>
<p>2
)
</p>
<p>For the purpose of this example, we will consider X
1
</p>
<p>the key variable.
</p>
<p>We find the mean and standard deviation of X
2
</p>
<p>to be 3.2 and 1.2,
</p>
<p>respectively.
</p>
<p>The values that are one or two standard deviations above and below
</p>
<p>the mean of X
2
</p>
<p>are:
</p>
<p>531
</p>
<p>Two standard deviations above: 3.2 + 2 * 1.2 = 3.2 + 2.4 = 5.6
</p>
<p>One standard deviation above: 3.2 + 1.2 = 4.4
</p>
<p>One standard deviation below: 3.2 &minus; 1.2 = 2.0
</p>
<p>Two standard deviations below: 3.2 &minus; 2 * 1.2 = 3.2 &minus; 2.4 = 0.8
</p>
<p>We can now input these values for X
2
</p>
<p>to determine the effect of X
1
</p>
<p>on Y:
</p>
<p>Effect of X
1
</p>
<p>at the mean of X
2
:
</p>
<p>Y = 2.3 + 1.7 X
1
</p>
<p>+ 2.0 * (3.2) + 0.5 (X
1
* 3.2)
</p>
<p>= 2.3 + 1.7 X
1
</p>
<p>+ 6.4 + 1.6 X
1
</p>
<p>= (2.3 + 6.2) + (1.7 + 1.6) X
1
</p>
<p>= 8.5 + 3.3 X
1
</p>
<p>A N E X A M P L E : R A C E A N D P U N I S H M E N T S E V E R I T Y</p>
<p/>
</div>
<div class="page"><p/>
<p>If we wanted to interpret the effect of X
1
</p>
<p>directly, then we would state
</p>
<p>that at the mean for X
2
, each one-unit increase in X
</p>
<p>1
is expected to
</p>
<p>increase Y by 3.3 units.
</p>
<p>Effect of X
1
</p>
<p>at one standard deviation above the mean of X
2
:
</p>
<p>Y = 2.3 + 1.7 X
1
</p>
<p>+ 2.0 * (4.4) + 0.5 (X
1
* 4.4)
</p>
<p>= 2.3 + 1.7 X
1
</p>
<p>+ 8.8 + 2.2 X
1
</p>
<p>= (2.3 + 8.8) + (1.7 + 2.2) X
1
</p>
<p>= 11.1 + 3.9 X
1
</p>
<p>If we wanted to interpret the effect of X
1
</p>
<p>directly, then we would state
</p>
<p>that at one standard deviation above the mean for X
2
, each one-unit
</p>
<p>increase in X
1
</p>
<p>is expected to increase Y by 3.9 units.
</p>
<p>Effect of X
1
</p>
<p>at one standard deviation below the mean of X
2
:
</p>
<p>Y = 2.3 + 1.7 X
1
</p>
<p>+ 2.0 * (2.0) + 0.5 (X
1
* 2.0)
</p>
<p>= 2.3 + 1.7 X
1
</p>
<p>+ 4.0 + 1.0 X
1
</p>
<p>= (2.3 + 4.0) + (1.7 + 1.0) X
1
</p>
<p>= 6.3 + 2.7 X
1
</p>
<p>If we wanted to interpret the effect of X
1
</p>
<p>directly, then we would state
</p>
<p>that at one standard deviation below the mean for X
2
, each one-unit
</p>
<p>increase in X
1
</p>
<p>is expected to increase Y by 2.7 units.
</p>
<p>Effect of X
1
</p>
<p>at two standard deviations above the mean of X
2
:
</p>
<p>Y = 2.3 + 1.7 X
1
</p>
<p>+ 2.0 * (5.6) + 0.5 (X
1
* 5.6)
</p>
<p>= 2.3 + 1.7 X
1
</p>
<p>+ 11.2 + 2.8 X
1
</p>
<p>= (2.3 + 11.2) + (1.7 + 2.8) X
1
</p>
<p>= 13.5 + 4.5 X
1
</p>
<p>If we wanted to interpret the effect of X
1
</p>
<p>directly, then we would state
</p>
<p>that at two standard deviations above the mean for X
2
, each one-unit
</p>
<p>increase in X
1
</p>
<p>is expected to increase Y by 4.5 units.
</p>
<p>Effect of X
1
</p>
<p>at two standard deviations below the mean of X
2
:
</p>
<p>Y = 2.3 + 1.7 X
1
</p>
<p>+ 2.0 * (0.8) + 0.5 (X
1
* 0.8)
</p>
<p>= 2.3 + 1.7 X
1
</p>
<p>+ 1.6 + 0.4 X
1
</p>
<p>= (2.3 + 1.6) + (1.7 + 0.4) X
1
</p>
<p>= 3.9 + 2.1 X
1
</p>
<p>If we wanted to interpret the effect of X
1
</p>
<p>directly, then we would state
</p>
<p>that at two standard deviations below the mean for X
2
, each one-unit
</p>
<p>increase in X
1
</p>
<p>is expected to increase Y by 2.1 units.
</p>
<p>Aside from making direct interpretations of the effect of X
1
at these five
</p>
<p>values of X
2
, what we see is that as the value of X
</p>
<p>2
increases, the effect
</p>
<p>of X
1
</p>
<p>on Y increases.
</p>
<p>532 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>A n  E x a m p l e :  P u n i s h m e n t  S e v e r i t y
</p>
<p>We again use the data on the sentencing of offenders in Pennsylvania in
</p>
<p>1998 and modify our regression model slightly. We continue to use length
</p>
<p>of sentence as the dependent variable and severity of the offense as an
</p>
<p>independent variable. Our second independent variable is a prior crimi-
</p>
<p>nal history score that is computed by the Pennsylvania Sentencing
</p>
<p>Commission and can take on values ranging from 0 to 8; larger values for
</p>
<p>prior criminal history reflect both a greater number of prior offenses as
</p>
<p>well as more serious prior offenses. For the purposes of this example, we
</p>
<p>have added an interaction effect between severity of the offense and prior
</p>
<p>criminal history and are interested in how the effect of offense severity
</p>
<p>varies across levels of prior criminal history.
</p>
<p>After estimating this model, we obtain the following regression equation:
</p>
<p>Y = &minus;7.83 + 3.58 X
1
</p>
<p>&minus; 1.21 X
2
</p>
<p>+ 0.62 X
1
* X
</p>
<p>2
</p>
<p>Where Y represents length of prison sentence (in months)
</p>
<p>X
1
</p>
<p>represents offense severity
</p>
<p>X
2
</p>
<p>represents prior criminal history
</p>
<p>Since we are primarily interested in the effect of offense severity across
</p>
<p>levels of criminal history, we have computed the mean and standard devi-
</p>
<p>ation of the criminal history variable to be 1.51 and 1.97, respectively.
</p>
<p>Following the same procedure as in our hypothetical example, we calcu-
</p>
<p>late the effect of offense severity at the mean of prior criminal history.
</p>
<p>Effect of offense severity at the mean of prior criminal history:
</p>
<p>Y = &minus;7.83 + 3.58 X
1
</p>
<p>&ndash; 1.21 (1.51) + 0.62 (X
1
* 1.51)
</p>
<p>= &minus;7.83 + 3.58 X
1
</p>
<p>&ndash; 1.83 + 0.94 X
1
</p>
<p>= (&minus;7.83 &ndash; 1.83) + (3.58 + 0.94) X
1
</p>
<p>= &minus;9.66 + 4.52 X
1
</p>
<p>If we wanted to interpret the effect of offense severity directly, then we
</p>
<p>would state that at the mean for prior criminal history, each one-unit
</p>
<p>increase in offense severity is expected to increase sentence length by
</p>
<p>4.52 months.
</p>
<p>Since the standard deviation of prior criminal history is greater than the
</p>
<p>mean, we cannot sensibly compute values at 1 or two standard deviations
</p>
<p>below the mean, since the values would be negative, and prior record
</p>
<p>score is constrained to have a value ranging from 0 to 8. In such a case,
</p>
<p>we can choose other values for the independent variable that carry sub-
</p>
<p>stantive meaning. For example, a prior criminal history score of 0 implies
</p>
<p>little or no prior criminal activity, while a prior criminal history score of 8
</p>
<p>implies an extensive history of serious and likely violent offending. By
</p>
<p>using the minimum and maximum values for prior criminal history, along
</p>
<p>A N E X A M P L E : P U N I S H M E N T S E V E R I T Y 533</p>
<p/>
</div>
<div class="page"><p/>
<p>with the mean, we can gain a good sense of how the effect of offense
</p>
<p>severity varies by level of prior criminal history.
</p>
<p>Effect of offense severity at a prior criminal history score of 0:
</p>
<p>Y = &ndash;7.83 + 3.58 X
1
</p>
<p>&ndash; 1.21 (0) + 0.62 (X
1
* 0)
</p>
<p>= &ndash;7.83 + 3.58 X
1
</p>
<p>A direct interpretation of the effect of offense severity at a prior criminal
</p>
<p>history score of 0 indicates that each one-unit increase in offense severity
</p>
<p>is expected to increase sentence length by 3.58 months.
</p>
<p>Similarly, if we take the maximum value for prior criminal history score
</p>
<p>of 8:
</p>
<p>Y = &ndash;7.83 + 3.58 X
1
</p>
<p>&ndash; 1.21 (8) + 0.62 (X
1
* 8)
</p>
<p>= &ndash;7.83 + 3.58 X
1
</p>
<p>&ndash; 9.68 + 4.96 X
1
</p>
<p>= (&ndash;7.83 &ndash; 9.68) + (3.58 + 4.96) X
1
</p>
<p>= &ndash;17.51 + 8.54 X
1
</p>
<p>Thus, among the historically most serious offenders, these results suggest
</p>
<p>that each one-unit increase in offense severity is expected to increase sen-
</p>
<p>tence length by 8.54 months.
</p>
<p>Much like the hypothetical example earlier, we see that as the value
</p>
<p>for prior criminal history increases, the effect of offense severity increas-
</p>
<p>es. To appreciate the change in the effect of offense severity, we have
</p>
<p>plotted these three regression equations in Figure 17.8. We see that at a
</p>
<p>prior criminal history score of 0, the regression line is positive, but shows
</p>
<p>modest increases over the range of offense severity. As we move to the
</p>
<p>mean of prior criminal history and then to the maximum value for prior
</p>
<p>criminal history, we see the slope of the regression line become steeper,
</p>
<p>reflecting increasingly greater effects on sentence length for any given
</p>
<p>increase in prior criminal history.
</p>
<p>T h e  P r o b l e m  o f  M u l t i c o l l i n e a r i t y
</p>
<p>The use of interaction terms is very likely to create a problem in regres-
</p>
<p>sion analyses that can lead to difficulty in estimation of the regression
</p>
<p>equation. This is because we are likely to be interested not only in the
</p>
<p>interaction between two variables, but also the simple additive effects of
</p>
<p>each independently. When we include the original variables as well as the
</p>
<p>term for their interaction, the three terms are likely to be very highly cor-
</p>
<p>related. Even though multivariate regression was developed in part to take
</p>
<p>into account the interrelationships among variables that predict Y, when
</p>
<p>independent variables in a regression model are too strongly related to
</p>
<p>one another, regression estimates become unstable. This problem is called
</p>
<p>Multicollinearity.
</p>
<p>534 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>In criminal justice, the independent variables examined are generally
</p>
<p>multicollinear, or correlated with one another. Indeed, this correlation is
</p>
<p>one reason it is so important to use multivariate techniques in criminal jus-
</p>
<p>tice research. When variables are intercorrelated, as in the case of our
</p>
<p>example of years in prison and prior arrests discussed in Chapter 16, it is
</p>
<p>important to control for the potential confounding influences of one vari-
</p>
<p>able on the other. Failure to do so is likely to lead to bias in our estimates
</p>
<p>of the effects of specific regression coefficients. However, the irony of mul-
</p>
<p>ticollinearity is that when variables become too correlated, or highly mul-
</p>
<p>ticollinear, the regression estimates become unreliable. This may happen
</p>
<p>when models do not include interaction terms, but is a particularly serious
</p>
<p>concern when interactions between variables are specified in a model.
</p>
<p>Multicollinearity can be identified in one of two ways. A common
</p>
<p>method is to look at the intercorrelations among the independent variables
</p>
<p>included in your model. Very high correlations between independent 
</p>
<p>variables are likely to lead to multicollinearity problems. What is considered
</p>
<p>a very high correlation? As with many other definitions in statistics, there
</p>
<p>is no absolute number at which multicollinearity is considered serious. As
</p>
<p>a general rule, a correlation between two independent variables of greater
</p>
<p>than 0.80 should be seen as a warning that serious multicollinearity may
</p>
<p>be evident in your model.
</p>
<p>Multicollinearity between two variables occurs less often than multi-
</p>
<p>collinearity across a series of variables. To diagnose this type of multicolli-
</p>
<p>nearity, we use a statistic that is usually defined as tolerance. Tolerance
</p>
<p>T H E P R O B L E M O F M U L T I C O L L I N E A R I T Y 535
</p>
<p>&minus;20
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>100
</p>
<p>120
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
</p>
<p>Offense Severity
</p>
<p>L
e
n
</p>
<p>g
th
</p>
<p> o
f 
</p>
<p>S
e
n
</p>
<p>te
n
</p>
<p>c
e
 (
</p>
<p>M
o
</p>
<p>n
th
</p>
<p>s
)
</p>
<p>Prior Criminal History = 0 Mean of Prior Criminal History
</p>
<p>Prior Criminal History = 8
</p>
<p>Figure 17.8 Regression Lines for the Effect of Offense Severity on Sentence Length by Level of Prior
Criminal History Score</p>
<p/>
</div>
<div class="page"><p/>
<p>measures the extent of the intercorrelations of each independent variable
</p>
<p>with all other independent variables. It is defined as 1 minus the percent
</p>
<p>of variance in X explained by the other independent variable examined
</p>
<p>(Equation 17.1).
</p>
<p>Tolerance = 1 &minus; R
X
</p>
<p>2
</p>
<p>Calculation of tolerance is generally provided as an option in standard sta-
</p>
<p>tistical computing packages, but it also can be calculated by taking each
</p>
<p>independent variable as the dependent variable in a regression that
</p>
<p>includes all other independent variables. This value is then subtracted
</p>
<p>from 1. For example, let&rsquo;s say we defined a model for explaining sentence
</p>
<p>length among offenders in Pennsylvania that included three independent
</p>
<p>variables:
</p>
<p>Y
length
</p>
<p>= b
0
</p>
<p>+ b
1
</p>
<p>(age) + b
2
</p>
<p>(offense severity) 
</p>
<p>+ b
3
</p>
<p>(prior  criminal  history) + e
</p>
<p>The R2
X
</p>
<p>for age would be estimated by calculating a regression in which
</p>
<p>age was the dependent variable and offense severity and prior criminal
</p>
<p>history were the independent variables. You would then take this R2 and
</p>
<p>subtract it from 1. Similarly, to get R2
X
</p>
<p>for offense severity, you would
</p>
<p>regress age and prior criminal history on offense severity and then sub-
</p>
<p>tract the resulting R2 from 1. Table 17.1 presents the Tolerance statistics
</p>
<p>for this regression model.
</p>
<p>How do we know if multicollinearity is negatively effecting our model
</p>
<p>estimates based on the tolerance statistics? A very small tolerance statistic
</p>
<p>suggests that the model is likely to include a high level of multicollinear-
</p>
<p>ity. Again, there is no clear yardstick for defining a level of tolerance that
</p>
<p>is likely to lead to estimation problems. In general, however, a tolerance
</p>
<p>level of less than 0.20 should be taken as a warning that serious multi-
</p>
<p>collinearity may exist in your model. We see from the results in Table 17.1
</p>
<p>that the smallest tolerance statistic has a value of 0.94, which does not
</p>
<p>indicate any serious multicollinearity in this regression model.
</p>
<p>Beyond these diagnostic procedures for multicollinearity, there are
</p>
<p>warning signs that can be observed in the regressions that are estimated.
</p>
<p>Sometimes when multicollinearity is present, the percent of explained
</p>
<p>536 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>Table 17.1 Tolerance Statistics for Regression of Sentence Length for Pennsylvania
</p>
<p>Offenders
</p>
<p>INDEPENDENT VARIABLE TOLERANCE
</p>
<p>Age 0.940
Offense Severity 0.931
Prior Criminal History 0.975
</p>
<p>Equation 17.1</p>
<p/>
</div>
<div class="page"><p/>
<p>variance in a model is high, but the regression coefficients overall fail to
</p>
<p>reach conventional thresholds of statistical significance. Sometimes multi-
</p>
<p>collinearity inflates coefficients to unrealistic sizes or produces coefficients
</p>
<p>in a direction contrary to conventional wisdom. One problem in diagnos-
</p>
<p>ing multicollinearity is that it may have such varied effects in your model
</p>
<p>that you may have difficulty distinguishing a misleading result that is due
</p>
<p>to multicollinearity from one that represents a new and interesting finding.
</p>
<p>When there are indications of serious multicollinearity, you can take a
</p>
<p>number of alternative corrective measures. The simplest is to exclude the
</p>
<p>variable or variables that are contributing most to multicollinearity. In the
</p>
<p>case of interaction terms this might require that one of the terms included
</p>
<p>in the interaction be dropped from the model. In the case where a small
</p>
<p>group of measures are highly collinear, you might choose to exclude the
</p>
<p>one variable that appears to present the most serious problem (i.e., that has
</p>
<p>the lowest tolerance value). The drawback of this approach is that the
</p>
<p>exclusion of such measures is likely to lead to model misspecification and
</p>
<p>may result in biased estimates of other regression coefficients that remain
</p>
<p>in the model. This approach makes sense only when other variables that
</p>
<p>remain in the model measure the same concept or theory. An approach that
</p>
<p>achieves a similar result, without excluding specific measures, is to create
</p>
<p>new indices from clusters of variables that are multicollinear. For example,
</p>
<p>if a group of measures all relating to social status are multicollinear, you
</p>
<p>may decide to create a new composite measure defined as social status and
</p>
<p>use it as an independent variable in subsequent regressions.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>Non-linear relationships refer to the effect of the independent variable
</p>
<p>on the dependent variable not being a straight-line (linear) relationship. A
</p>
<p>linear relationship implies that each one unit increase in the independent
</p>
<p>variable will result in the dependent variable increasing or decreasing by
</p>
<p>some fixed amount, regardless of the level of the independent variable.
</p>
<p>A non-linear relationship implies that each one unit increase in the inde-
</p>
<p>pendent variable does not result in the same amount of change in the
</p>
<p>dependent variable &ndash; it may be larger or smaller and will vary by the level
</p>
<p>of the independent variable. A non-linear relationship can be incorporated
</p>
<p>into an OLS regression equation by transforming the independent variable.
</p>
<p>Interaction effects reflect the varying effect of one independent vari-
</p>
<p>able on the dependent variable across the levels or values of a second
</p>
<p>independent variable. When we have an interaction effect between a
</p>
<p>dummy variable and an interval-level variable, we can directly interpret the
</p>
<p>effect of the interval-level variable on the dependent variable for each
</p>
<p>group measured by the dummy variable. Interpretation of an interaction of
</p>
<p>two interval level independent variables is much more difficult. One way
</p>
<p>C H A P T E R S U M M A R Y 537</p>
<p/>
</div>
<div class="page"><p/>
<p>538 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>of simplifying interpretation is to designate values for one variable, such as
</p>
<p>the mean, one standard deviation above/below the mean, two standard
</p>
<p>deviations above/below the mean, and so on, as fixed points to compute
</p>
<p>the effect of the other interval-level variable on the dependent variable.
</p>
<p>Multicollinearity occurs when independent variables in a regression
</p>
<p>model are too strongly related. It leads to unstable results. The problem
</p>
<p>may be diagnosed by checking the bivariate correlations between the
</p>
<p>variables and by measuring tolerance. Multicollinearity may be dealt with
</p>
<p>either by excluding specific variables altogether or by merging several
</p>
<p>similar variables into one composite index.
</p>
<p>K e y  T e r m s
</p>
<p>interaction effect An interaction effect is
present when the effect of one independent
variable on the dependent variable is 
conditional on the level of a second 
independent variable.
</p>
<p>multicollinearity Condition in a multi-
variate regression model in which 
independent variables examined are very 
strongly intercorrelated. Multicollinearity 
leads to unstable regression coefficients.
</p>
<p>non-linear relationship Relationship
between the dependent and the independent
variable that is not captured by a straight line
(linear) relationship.
</p>
<p>tolerance A measure of the extent of the
intercorrelations of each independent vari-
able with all other independent variables.
Tolerance may be used to test for 
multicollinearity in a multivariate regression
model.
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>R 2
X
</p>
<p>R2 obtained when an independent variable is treated as a dependent 
variable in a test for tolerance
</p>
<p>To calculate tolerance:
</p>
<p>Tolerance = 1 &ndash; R 2
X
</p>
<p>E x e r c i s e s
</p>
<p>17.1 An analysis of shoplifting frequency among youth and young
adults included a quadratic term for age of the individual and 
produced the following results:
</p>
<p>INDEPENDENT VARIABLE B
</p>
<p>Age (in years) 0.35
Age2 (in years2) &minus;0.01
</p>
<p>Interpret the effect of age on the frequency of shoplifting.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 An analysis linking level of informal social control to frequency of
delinquency produced the following results:
</p>
<p>INDEPENDENT VARIABLE B
</p>
<p>Age (in years) &minus;0.12
Sex (1=Female, 0=Male) &minus;1.50
Race (1=White, 0=Non-white) 0.27
Informal Social Control (1=Low, 10=High) &minus;0.83
</p>
<p>After plotting the mean level of delinquency by level of informal social con-
</p>
<p>trol, the researcher observed what appeared to be an inverse relationship
</p>
<p>(1/X) between delinquency and informal social control. After transforming
</p>
<p>the measure of informal social control, the researcher estimated a new
</p>
<p>regression and produced the following results:
</p>
<p>INDEPENDENT VARIABLE B
</p>
<p>Age (in years) &minus;0.11
Sex (1=Female, 0=Male) &minus;1.61
Race (1=White, 0=Non-white) 0.32
Inverse of Informal Social Control (1=Low, 10=High) 2.45
</p>
<p>a. Interpret the effect of the inverse of informal social control.
</p>
<p>b. Sketch the relationship between delinquency and informal
social control using the coefficient for the inverse of 
informal social control.
</p>
<p>17.3 A researcher wanted to test the hypothesis that adolescent females
were more affected by parental supervision than adolescent males.
In a regression analysis incorporating an interaction effect between
sex and supervision, the researcher produced the following set of
results:
</p>
<p>INDEPENDENT VARIABLE B
</p>
<p>Sex (1=Female, 0=Male) &minus;2.7
Supervision (1=Low, 10=High) &minus;1.3
Sex * Supervision &minus;0.5
</p>
<p>Interpret the effect of supervision for adolescent females and males.
</p>
<p>17.4 A study of attitudes about punishment used a scale of punitiveness
ranging in value from 1 (Low) to 100 (High). The researcher was
particularly interested in whether there was an interaction effect
between age and political conservatism. A regression analysis pro-
duced the following results:
</p>
<p>INDEPENDENT VARIABLE B MEAN
</p>
<p>Age (years) 1.67 44.95
Political Conservatism (1=Low, 10=High) 0.92 6.5
Age * Political Conservatism 0.56
</p>
<p>a. What is the effect of political conservatism at the mean age of the
sample? Interpret this effect.
</p>
<p>b. What is the effect of age at the mean level of political conser-
vatism for the sample? Interpret this effect.
</p>
<p>E X E R C I S E S 539</p>
<p/>
</div>
<div class="page"><p/>
<p>540 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>c. What is the effect of political conservatism at each of the 
following ages?
</p>
<p>&ndash; 20
&ndash; 30
&ndash; 50
&ndash; 60
</p>
<p>Describe how the effect of political conservatism changes as age
</p>
<p>increases.
</p>
<p>d. What is the effect of age at each of the following values of
political conservatism?
</p>
<p>&ndash; 0
&ndash; 2
&ndash; 5
&ndash; 8
&ndash; 10
</p>
<p>Describe how the effect of age changes as the level of political con-
</p>
<p>servatism increases.
</p>
<p>17.5 A study of violence in prison cell blocks was concerned about the
amount of space available to each inmate and the proportion of
inmates identified as gang members who had been identified as
gang members. The researcher tested the hypothesis of an interac-
tion effect between space available and the proportion of inmates
identified as gang members. A regression analysis produced the
following results:
</p>
<p>INDEPENDENT VARIABLE B MEAN
</p>
<p>Space available (square feet per inmate) &minus;0.25 10.0
Proportion gang members 0.77 0.77
Space available * Proportion gang members &minus;0.05
</p>
<p>a. What is the effect of space available at the mean proportion of
gang members for the sample of cell blocks? Interpret this effect.
</p>
<p>b. What is the effect of proportion of gang members at the mean
level of space available for the sample of cell blocks? Interpret
this effect.
</p>
<p>c. What is the effect of space available at each of the following
proportions of gang membership?
</p>
<p>&ndash; 0.2
&ndash; 0.4
&ndash; 0.6
&ndash; 0.8
</p>
<p>Describe how the effect of space available changes as proportion of
</p>
<p>gang membership increases.
</p>
<p>d. What is the effect of proportion of gang membership at each of
the following values of space available?</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; 3
&ndash; 6
&ndash; 12
&ndash; 15
</p>
<p>Describe how the effect of proportion of gang membership changes as
</p>
<p>the level of space available increases.
</p>
<p>17.6 Rachel collects police data on a series of burglaries and wishes to
determine the factors that influence the amount of property stolen
in each case. She creates a multivariate regression model and runs
a test of tolerance for each of the independent variables. Her
results are as follows, where Y = Amount of property stolen ($):
</p>
<p>INDEPENDENT VARIABLE SCALE TOLERANCE
</p>
<p>X
1
: Time of robbery (AM or PM) Nominal 0.98
</p>
<p>X
2
: Accessibility of property Ordinal 0.94
</p>
<p>X
3
: Number of rooms in house Interval 0.12
</p>
<p>X
4
: Size of house Interval 0.12
</p>
<p>X
5
: Joint income of family Interval 0.46
</p>
<p>Would you advise Rachel to make any changes to her model? Explain
</p>
<p>your answer.
</p>
<p>17.7 A researcher examining neighborhood crime rates computes a
regression model using the following variables:
</p>
<p>Y = crime rate (per 100,000)
</p>
<p>X
1
</p>
<p>= percent living in poverty
</p>
<p>X
2
</p>
<p>= percent unemployed
</p>
<p>X
3
</p>
<p>= median income
</p>
<p>X
4
</p>
<p>= percent of homes being rented
</p>
<p>The researcher finds the F-statistic for the overall model to be statistically
</p>
<p>significant (with α = 0.05), but the results for each variable are as follows:
</p>
<p>INDEPENDENT VARIABLE B SIG. TOLERANCE
</p>
<p>X
1
: Percent living in poverty 52.13 0.17 0.15
</p>
<p>X
2
: Percent unemployed 39.95 0.23 0.07
</p>
<p>X
3
: Median income 22.64 0.12 0.19
</p>
<p>X
4
: Percent of homes being rented 27.89 0.33 0.05
</p>
<p>a. Explain why the researcher found a statistically significant
regression model, but no significant regression coefficients
</p>
<p>b. What would you recommend the researcher do in this case?
</p>
<p>C O M P U T E R E X E R C I S E S 541
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>In Chapter 16, we illustrated the use of the regression command in SPSS and 
</p>
<p>Stata to estimate multivariate regression models. The analyses described in this 
</p>
<p>chapter&mdash;nonlinear terms, interaction effects, and a test for multicollinearity&mdash;are 
</p>
<p>accomplished with the same regression command in each program. The  </p>
<p/>
</div>
<div class="page"><p/>
<p>542 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>following exercises should help to illustrate how to perform these analyses, as 
</p>
<p>will the sample syntax files for SPSS (Chapter_17.sps) and Stata (Chapter_17.do).
</p>
<p>SPSS
</p>
<p>Computing Nonlinear and Interaction Terms
</p>
<p>To include a nonlinear or an interaction term in a multivariate regression model, 
</p>
<p>it is necessary to first compute the nonlinear or the interaction term. This com-
</p>
<p>putation is done with the COMPUTE command that we briefly discussed in the 
</p>
<p>Computer Exercises in Chapter 4. The general format for the COMPUTE com-
</p>
<p>mand is
</p>
<p>In SPSS, the double asterisk (**) indicates that we want to take a variable to a 
</p>
<p>power. In this example, we want to square AGE, so we add the value 2. If, for 
</p>
<p>some reason, we had wanted to cube AGE, then we would have typed AGE**3. 
</p>
<p>Also, recall that the addition of the EXECUTE command forces SPSS to per-
</p>
<p>form this calculation immediately.
</p>
<p>An alternative that accomplishes the same thing is
</p>
<p>where we simply multiply the variable AGE by itself.
</p>
<p>The calculation can be a function of one or more variables, which we illustrate 
</p>
<p>below.
</p>
<p>Nonlinear Terms
</p>
<p>Suppose we wanted to compute a squared term for a variable AGE. We might 
</p>
<p>name this new variable AGE_SQ. The COMPUTE command would look like
</p>
<p>In either case, once the COMPUTE command has been executed, the new 
</p>
<p>variable will appear in the data file.
</p>
<p>Interaction Terms
</p>
<p>The computation of an interaction term is as direct as the equations given in this 
</p>
<p>chapter. We again use the COMPUTE command, but our calculation involves 
</p>
<p>multiplying the two variables of interest. We have found that it is often helpful to 
</p>
<p>make the name of the new variable representing an interaction term a combination 
</p>
<p>of fragments from both of the original variables being used in the calculation.
</p>
<p>For example, suppose that we want to create an interaction term for two 
</p>
<p>variables that we have named EDUCATION and INCOME. We might call the 
</p>
<p>interaction variable EDUC_INC:
</p>
<p>COMPUTE new_var_name = calculation
</p>
<p>COMPUTE AGE_SQ = AGE**2.
</p>
<p>EXECUTE.
</p>
<p>COMPUTE AGE_SQ = AGE * AGE.
</p>
<p>EXECUTE.
</p>
<p>COMPUTE EDUC_INC = EDUCATION * INCOME.
</p>
<p>EXECUTE.</p>
<p/>
</div>
<div class="page"><p/>
<p>543
</p>
<p>Estimating the Regression Model
</p>
<p>model&mdash;identical to how we presented these terms in the discussion in this  
</p>
<p>chapter. For situations where we are using nonlinear terms, we may need to drop 
</p>
<p>the original variable. Prior research and theory indicating that a nonlinear term 
</p>
<p>was appropriate will often be the best guide on what the regression equation 
</p>
<p>should look like. In the case of  an interaction term, keep in mind that we must 
</p>
<p>include both of  the original variables and the interaction term; otherwise, it will be 
</p>
<p>nearly impossible to interpret the coefficients that we do obtain from a regression 
</p>
<p>C O M P U T E R E X E R C I S E S
</p>
<p>After computing the nonlinear or the interaction term, we then simply treat the 
</p>
<p>created variable as an additional variable added to our multivariate regression 
</p>
<p>analysis.
</p>
<p>Collinearity Diagnostics
</p>
<p>SPSS&rsquo;s regression command will produce a wide assortment of collinearity  
</p>
<p>statistics, including the tolerance statistic discussed above. To obtain the  
</p>
<p>collinearity diagnostics, we include the /STATISTICS option in our 
</p>
<p>REGRESSION command:
</p>
<p>where TOL requests the tolerance statistics for each independent variable and 
</p>
<p>COLLIN requests all other collinearity measures. The tolerance statistics are  
</p>
<p>presented in the coefficients table, where you will also find the regression  
</p>
<p>coefficients. Recall from the discussion in the chapter that a tolerance of less 
</p>
<p>than about 0.20 is indicative of collinearity problems in a regression model.
</p>
<p>Stata
</p>
<p>Computing Nonlinear and Interaction Terms
</p>
<p>To include a nonlinear or an interaction term in a multivariate regression model, 
</p>
<p>it is necessary to first compute the nonlinear or the interaction term. This com-
</p>
<p>putation is done with the gen command (short for generate) that we briefly 
</p>
<p>discussed in the Computer Exercises in Chapter 4. The general format for the 
</p>
<p>gen command is
</p>
<p>The calculation can be a function of one or more variables, which we illustrate 
</p>
<p>below. (Note that the structure of the discussion is nearly identical to that in the 
</p>
<p>SPSS section, with slight changes for the Stata syntax.)
</p>
<p>Nonlinear Terms
</p>
<p>Suppose we wanted to compute a squared term for a variable AGE. We might 
</p>
<p>name this new variable AGE_SQ. The gen command would look like
</p>
<p>REGRESSION
</p>
<p>/STATISTICS COEFF R ANOVA COLLIN TOL
</p>
<p>/DEPENDENT dep_var_name
</p>
<p>/METHOD = ENTER list_of_indep_vars.
</p>
<p>gen new_var_name = calculation
</p>
<p>gen AGE_SQ = AGE^2</p>
<p/>
</div>
<div class="page"><p/>
<p>544 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>In Stata, the upward pointing arrow (^) indicates that we want to take a vari-
</p>
<p>able to a power. In this example, we want to square AGE, so we add the value 
</p>
<p>2. If, for some reason, we had wanted to cube AGE, then we would have typed 
</p>
<p>AGE^3.
An alternative that accomplishes the same thing is
</p>
<p>where we simply multiply the variable AGE by itself.
</p>
<p>In either case, once the gen command has been executed, the new variable 
</p>
<p>will appear in the data file.
</p>
<p>Interaction Terms
</p>
<p>The computation of an interaction term is as direct as the equations given in this 
</p>
<p>chapter. We again use the gen command, but our calculation involves multiply-
</p>
<p>ing the two variables of interest. We have found that it is often helpful to make 
</p>
<p>the name of the new variable representing an interaction term a combination of 
</p>
<p>fragments from both of the original variables being used in the calculation.
</p>
<p>For example, suppose that we want to create an interaction term for two 
</p>
<p>variables that we have named EDUCATION and INCOME. We might call the 
</p>
<p>interaction variable EDUC_INC:
</p>
<p>Estimating the Regression Model
</p>
<p>After computing the nonlinear or the interaction term, we then simply treat the 
</p>
<p>created variable as an additional variable added to our multivariate regression 
</p>
<p>model&mdash;identical to how we presented these terms in the discussion in this  
</p>
<p>chapter. For situations where we are using nonlinear terms, we may need to drop 
</p>
<p>the original variable. Prior research and theory indicating that a nonlinear term  
</p>
<p>was appropriate will often be the best guide on what the regression equation 
</p>
<p>should look like. In the case of an interaction term, keep in mind that we must 
</p>
<p>include both of the original variables and the interaction term; otherwise,  
</p>
<p>it will be nearly impossible to interpret the coefficients that we do obtain  
</p>
<p>from a regression analysis.
</p>
<p>Collinearity Diagnostics
</p>
<p>Within Stata, only the VIF is directly available, but there is a user-written pack-
</p>
<p>age&mdash;collin&mdash;that will compute the full set of collinearity statistics, including the 
</p>
<p>tolerance. To install collin on your system, run the following command (one time 
</p>
<p>only&mdash;see Chapter_17.do):
</p>
<p>This will make the collin command available for use.
</p>
<p>In order to use the collin command, you must first run the regress  
</p>
<p>command. Immediately following the regress command, run the collin using 
</p>
<p>the same set of independent variables:
</p>
<p>gen AGE_SQ = AGE * AGE
</p>
<p>gen EDUC_INC = EDUCATION * INCOME
</p>
<p>net install collin, from(http://www.ats.ucla.edu/stat/stata/
</p>
<p>ado/analysis)</p>
<p/>
<div class="annotation"><a href="http://www.ats.ucla.edu/stat/stata/ado/analysis">http://www.ats.ucla.edu/stat/stata/ado/analysis</a></div>
<div class="annotation"><a href="http://www.ats.ucla.edu/stat/stata/ado/analysis">http://www.ats.ucla.edu/stat/stata/ado/analysis</a></div>
</div>
<div class="page"><p/>
<p>545
</p>
<p>The output will be similar to that in SPSS and will produce a wide assortment 
</p>
<p>of collinearity statistics, including the tolerance statistic discussed above. Each 
</p>
<p> measure of collinearity is presented in a table that lists each independent variable 
</p>
<p>(by row) and collinearity statistic across columns.
</p>
<p>Problems
</p>
<p>Open the NYS data file (nys_1.sav, nys_1_student.sav, or nys_1.dta) to complete 
</p>
<p>Exercises 1 through 4.
</p>
<p>C O M P U T E R E X E R C I S E S
</p>
<p>regress dep_var list_of_independent_variables
</p>
<p>collin list_of_independent_variables
</p>
<p> 1. Using one of  the line graph commands (described in Chapter 3), generate 
</p>
<p>a plot for the mean of  a delinquency measure by age.
</p>
<p>In SPSS, this should look something like
</p>
<p> 2. Compute a squared term for the age variable as described above. Estimate 
</p>
<p>a multivariate regression model using one of  the measures of  delinquency 
</p>
<p>as the dependent variable. Use age and age squared as the independent 
</p>
<p>variables. As noted earlier in the chapter, this will provide a quadratic 
</p>
<p>equation that will test for a nonlinear relationship between age and the 
</p>
<p>measure of  delinquency that you have selected.
</p>
<p>Report the values of  the regression coefficients for age and age 
</p>
<p>squared and whether or not the coefficients are statistically significant 
</p>
<p>(with α = 0.05). Interpret the effect of  age on this measure of  delinquency.
</p>
<p>GRAPH /LINE(SIMPLE) = MEAN(delinquency_variable)  
</p>
<p>BY age .
</p>
<p>In Stata, the corresponding syntax is a bit more complicated, but use the  
</p>
<p>following lines to accomplish a similar graph as in SPSS:
</p>
<p>egen mean_delinq1 = mean(delinquency_variable), by(age)
</p>
<p>sort age
</p>
<p>twoway (line mn_delinq1 age, connect(ascending))
</p>
<p>The resulting line graphs from the syntax given above will plot the mean 
</p>
<p>level of  delinquency&mdash;for the variable you picked&mdash;for each age recorded 
</p>
<p>in the NYS sample. Try this command with other measures of  delinquency 
</p>
<p>and see if  you notice any variations in the shapes of  the lines. (NOTE: If  
</p>
<p>you are using Stata, you will need to change the output variable name in the 
</p>
<p>egen command line, perhaps most simply by changing the number at the  
</p>
<p>end.)
</p>
<p>Do they imply a linear relationship between age and delinquency? A 
</p>
<p>nonlinear relationship? If  nonlinear, what kind of  nonlinear relationship? 
</p>
<p>(You may want to refer back to Figure 17.2 for some approximations of  
</p>
<p>different curves.)</p>
<p/>
</div>
<div class="page"><p/>
<p>546 C H A P T E R S E V E N T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p> 3. Compute a multivariate regression model using number of  times drunk as 
</p>
<p>the dependent variable. Include the measures for age, sex, race (recoded 
</p>
<p>as a dummy variable), and at least two other variables that you think are 
</p>
<p>related to the number of  times drunk. This will be the &ldquo;baseline model&rdquo; in 
</p>
<p>the following questions.
</p>
<p>a. Compute the tolerance statistic for each of  the independent variables in 
</p>
<p>the baseline model. Does it appear that there is a problem with  
</p>
<p>collinearity in the regression model? Explain why.
</p>
<p>b. Compute an interaction term for sex with age. Add this term to the 
</p>
<p>baseline model, and rerun the regression command. Is the effect of  age 
</p>
<p>effect of  age on the number of  times drunk for males and females.
</p>
<p>i. Compute the tolerance statistic for each of  the independent variables 
</p>
<p>in this model. Does it appear that there is a problem with collinearity 
</p>
<p>in this model? Explain why.
</p>
<p>c. Compute an interaction term for race (which should be coded as a 
</p>
<p>dummy variable) and age. Add this term to the baseline model, and 
</p>
<p>rerun the regression command. (The interaction effect from part 
</p>
<p>(a) should no longer be included in the analysis.) Is the effect of  age 
</p>
<p>groups? If  so, interpret the effect of  age on the number of  times drunk 
</p>
<p>for each race group.
</p>
<p>i. Compute the tolerance statistic for each of  the independent variables 
</p>
<p>in this model. Does it appear that there is a problem with collinearity 
</p>
<p>in this model? Explain why.
</p>
<p>d. If  one of  the additional variables that you have added to your 
</p>
<p> regression model is measured at the interval level of  measurement, 
</p>
<p>compute an interaction term between this variable and either the sex 
</p>
<p>or the race variable. Add this term to the baseline model (there should 
</p>
<p>be no other interaction terms included in this analysis), and rerun the 
</p>
<p>regression command. Is the effect of  this variable on number of  times 
</p>
<p> 
</p>
<p>effect of  this variable for each group.
</p>
<p>i. Compute the tolerance statistic for each of  the independent variables 
</p>
<p>in this model. Does it appear that there is a problem with collinearity 
</p>
<p>in this model? Explain why.
</p>
<p> 4. Compute a multivariate regression model using number of  times cheated 
</p>
<p>at school as the dependent variable. Include the measures for age, sex, 
</p>
<p>race (recoded as a dummy variable), grade point average, and amount of  
</p>
<p>time spent studying as the independent variables. This will be the baseline 
</p>
<p>model in the following questions.</p>
<p/>
</div>
<div class="page"><p/>
<p>547
</p>
<p>a. Compute an interaction term for grade point average and time spent 
</p>
<p>studying and add this term to the regression model. Prior to rerunning 
</p>
<p>the regression command, check the item for descriptive statistics  
</p>
<p>available through the regression command window. Report the  
</p>
<p>b. Compute the tolerance statistic for each of  the independent variables 
</p>
<p>in this model. Does it appear that there is a problem with collinearity in 
</p>
<p>this model? Explain why.
</p>
<p>c. What is the effect of  grade point average on number of  times cheated 
</p>
<p>at the mean level of  time spent studying? (You will need to use the 
</p>
<p>mean reported in the results for part (a).) Interpret this effect.
</p>
<p>i. How does the effect of  grade point average change as the value for 
</p>
<p>time spent studying increases or decreases?
</p>
<p>d. What is the effect of  time spent studying on number of  times cheated 
</p>
<p>at the mean grade point average? Interpret this effect.
</p>
<p>i. How does the effect of  time spent studying change as the value for 
</p>
<p>grade point average increases or decreases?
</p>
<p>C O M P U T E R E X E R C I S E S</p>
<p/>
</div>
<div class="page"><p/>
<p>Logistic Regression
</p>
<p>f o r  e x a m i n i n g  a  d i c h o t o m o u s  d e p e n d e n t  v a r i a b l e
</p>
<p>Dependent Variable?
</p>
<p>c o e f f i c i e n t s  w i t h i n  a  s i n g l e  m o d e l
</p>
<p>Regression Coefficients?
</p>
<p>Interpreted?
</p>
<p>2
</p>
<p>Model?
</p>
<p>C h a p t e r  e i g h t e e n
</p>
<p>L o g i s t i c  r e g r e s s i o n  a s  a  t o o l  
</p>
<p>I n t e r p r e t i n g  l o g i s t i c  r e g r e s s i o n  c o e f f i c i e n t s
</p>
<p>C o m p a r i n g  l o g i s t i c  r e g r e s s i o n  
</p>
<p>E v a l u a t i n g  t h e  l o g i s t i c  r e g r e s s i o n  m o d e l
</p>
<p>T e s t i n g  f o r  s t a t i s t i c a l  s i g n i f i c a n c e
</p>
<p>Why is It Inappropriate to Use OLS Regression for a Dichotomous 
</p>
<p>How is the Outcome Altered in a Logistic Regression Model?
</p>
<p>Why is It Difficult to Interpret the Logistic Regression Coefficient?
</p>
<p>How Can Probability Estimates be Used to Compare the Strength of Logistic
</p>
<p>What is the Standardized Logistic Regression Coefficient and How is It
</p>
<p>How is the Percent of Correct Predictions Interpreted?
</p>
<p>What is Pseudo R and How is It Interpreted?
</p>
<p>What is the Test of Statistical Significance for the Overall Logistic Regression
</p>
<p>What is the Test of Statistical Significance for the Logistic Regression Coefficient?
</p>
<p>What is the Test of Statistical Significance for a Multicategory Nominal Variable?
</p>
<p>What is an Odds Ratio and How is It Interpreted?
</p>
<p>What Shape Does the Logistic Model Curve Take?
</p>
<p>What is the Derivative at Mean and How is It Interpreted?
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_18,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>ORDINARY LEAST SQUARES REGRESSION is a very useful tool for identi-
fying how one or a series of independent variables affects an interval-
</p>
<p>level dependent variable. As noted in Chapter 16, this method may
</p>
<p>also be used&mdash;though with caution&mdash;to explain dependent variables
</p>
<p>that are measured at an ordinal level. But what should the researcher
</p>
<p>do when faced with a binary or dichotomous dependent variable?
</p>
<p>Such situations are common in criminology and criminal justice. For
</p>
<p>example, in examining sentencing practices, the researcher may want
</p>
<p>to explain why certain defendants get a prison sentence while others
</p>
<p>do not. In assessing the success of a drug treatment program, the re-
</p>
<p>searcher may be interested in whether offenders failed a drug test or
</p>
<p>whether they returned to prison within a fixed follow-up period. In
</p>
<p>each of these examples, the variable that the researcher seeks to ex-
</p>
<p>plain is a simple binary outcome. It is not appropriate to examine bi-
</p>
<p>nary dependent variables using the regression methods that we have
</p>
<p>reviewed thus far.
</p>
<p>This chapter introduces a type of regression analysis that allows us to
</p>
<p>examine a dichotomous dependent variable. Called logistic regression
</p>
<p>analysis, it has become one of the analysis tools most frequently used in
</p>
<p>crime and justice research. We begin the chapter by explaining why the
</p>
<p>OLS regression approach described in Chapters 15 and 16 is not appro-
</p>
<p>trate the interpretation of logistic regression statistics in the context of a
</p>
<p>substantive criminal justice research example. In this chapter, as in
</p>
<p>Chapter 16, our focus will be more on explaining how logistic regres-
</p>
<p>sion can be used in research than on describing the mathematical prop-
</p>
<p>erties that underlie the computations used to develop logistic regression
</p>
<p>statistics.
</p>
<p>logistic regression approach and the logic underlying it. Finally, we illus-
</p>
<p>priate when the dependent variable is binary. We then describe the
</p>
<p>549</p>
<p/>
</div>
<div class="page"><p/>
<p>f o r  a  D i c h o t o m o u s  D e p e n d e n t  V a r i a b l e ?
</p>
<p>In Chapter 16, you saw that we could use not only interval-level vari-
</p>
<p>ables, but also ordinal- and nominal-level variables, as independent
</p>
<p>measures in a multivariate ordinary least squares regression. While 
</p>
<p>we emphasized that the assumptions regarding measurement of the
</p>
<p>dependent variable are much more stringent in OLS regression, even 
</p>
<p>in the case of the dependent variable the researcher may sometimes
</p>
<p>decide to use ordinal- as well as interval-level variables. But applying
</p>
<p>the OLS regression approach is inappropriate when the dependent
</p>
<p>variable is nominal, as is the case with a binary or dichotomous depen-
</p>
<p>dent variable.
</p>
<p>Why do we state this rule so unequivocally? One reason is that the
</p>
<p>logic underlying our explanation of a dichotomous dependent variable is
</p>
<p>at odds with the models that we build using the OLS regression ap-
</p>
<p>proach. In order to expand on this idea, we need to return to how pre-
</p>
<p>dictions are developed using OLS regression. In the simple linear
</p>
<p>model&mdash;the OLS model&mdash;we predict the value of Y based on an equation
</p>
<p>that takes into account the values of a Y-intercept (b0) and one or a se-
</p>
<p>ries of independent variables (e.g., b1X1). This model is represented
</p>
<p>below for a bivariate regression example in which we seek to explain
</p>
<p>the yearly budget of police departments based on the number of officers
</p>
<p>employed.
</p>
<p>where
</p>
<p>This is an additive model, in which we predict the value of Y&mdash;in this
</p>
<p>case, the yearly police department budget in dollars&mdash;by adding the
</p>
<p>value of the Y-intercept to the value of the regression coefficient times
</p>
<p>the value of X1 (the number of sworn officers in a department).
</p>
<p>Let&rsquo;s say that a representative sample of police agencies were sur-
</p>
<p>veyed and analysis of the responses yielded the following regression
</p>
<p>equation:
</p>
<p>Y � 100,000 � 100,000X1
</p>
<p>This equation suggests that for each additional officer employed, the de-
</p>
<p>partment budget is expected to increase by $100,000. For a police
</p>
<p>agency with 100 officers, we would expect a budget of about
</p>
<p>$10,100,000:
</p>
<p> X1 � number of sworn officers
</p>
<p> Y � yearly police department budget in dollars
</p>
<p> Y � b0 � b1X1
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N550
</p>
<p>W h y  i s  I t  I n a p p r o p r i a t e  t o  U s e  O L S  R e g r e s s i o n  </p>
<p/>
</div>
<div class="page"><p/>
<p>For a police agency with 1,000 officers, we would expect a budget of
</p>
<p>about $100,100,000:
</p>
<p>W orking It Out
</p>
<p> � 10,100,000
</p>
<p> � 100,000 � 10,000,000
</p>
<p> � 100,000 � 100,000(100)
</p>
<p> Y � 100,000 � 100,000X1
</p>
<p>W orking It Out
</p>
<p> � 100,100,000
</p>
<p> � 100,000 � 100,000,000
</p>
<p> � 100,000 � 100,000(1,000)
</p>
<p> Y � 100,000 � 100,000X1
</p>
<p>This model, like other OLS regression models, assumes that there is
</p>
<p>no real limit to the value that the dependent variable can attain. With
</p>
<p>each additional officer comes an expected increase in the departmental
</p>
<p>budget. Our model suggests that the increase is about $100,000 for each
</p>
<p>additional officer. While this logic makes very good sense when we are
</p>
<p>speaking about interval-scale measures, such as the budget of a police
</p>
<p>agency, does it make sense when we are dealing with a dichotomous
</p>
<p>dependent variable, such as whether a parolee has failed a drug test?
</p>
<p>Let&rsquo;s say that we surveyed 30 parolees who had been tested for drug
</p>
<p>use. Our independent variable is prior drug arrests. The data in Table
</p>
<p>ber of prior drug arrests for each parolee. We have coded a failed drug
</p>
<p>gression results for our example. The OLS regression suggests a very
</p>
<p>strong relationship between prior drug arrests and failing the drug test.
</p>
<p>But if we look more closely at the regression model, we can see that this
</p>
<p>approach may lead to outcomes that are not consistent with the
</p>
<p>processes we seek to understand.
</p>
<p>18.1 report whether a parolee failed the drug test and also give the num-
</p>
<p>test as 1 and a successful drug test as 0. Table 18.2 provides the OLS re-
</p>
<p>Figure 18.1 shows the data points for our example in a scatterplot, as
</p>
<p>well as the regression line drawn from the outcomes in Table 18.2. It is
</p>
<p>I S I T I N A P P R O P R I A T E T O U S E O L S R E G R E S S I O N ?W H Y 551</p>
<p/>
</div>
<div class="page"><p/>
<p>Drug Testing Results and Prior Drug Arrests for 30 Parolees
</p>
<p>DRUG TEST RESULT DRUG TEST SCORE NUMBER OF DRUG ARRESTS
</p>
<p>Pass 0 0
Pass 0 0
Pass 0 0
Pass 0 0
Pass 0 0
Fail 1 0
Pass 0 1
Pass 0 1
Pass 0 1
Pass 0 1
Pass 0 1
Pass 0 1
Fail 1 2
Fail 1 2
Fail 1 2
Fail 1 2
Fail 1 2
Fail 1 3
Fail 1 3
Pass 0 3
Fail 1 4
Fail 1 4
Fail 1 4
Fail 1 5
Fail 1 5
Fail 1 5
Fail 1 6
Fail 1 6
Fail 1 7
Fail 1 8
</p>
<p>OLS Regression Results for Example 
of Drug Testing and Prior Drug Arrests
</p>
<p>Unstandardized Standardized
Coefficients Coefficients t Sig.
</p>
<p>Model B Std. Error Beta
</p>
<p>1 (Constant) .211 .104 2.025 .052
</p>
<p>Drug Arrests 0.148 .030 .681 4.921 .000
</p>
<p>a Dependent Variable: FAILURE ON DRUG TEST
</p>
<p>Table 18.1
</p>
<p>Table 18.2
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N552</p>
<p/>
</div>
<div class="page"><p/>
<p>clear that the OLS regression approach leads to predicted outcomes that
</p>
<p>are not possible, given our dependent variable. For example, for a parolee
</p>
<p>with six drug arrests, our model predicts that Y will have a value of 1.099:
</p>
<p>Scatterplot of Example of Drug Testing with the Predicted Regression Line
</p>
<p>D
ru
</p>
<p>g 
T
</p>
<p>es
t 
</p>
<p>Fa
il
</p>
<p>u
re
</p>
<p>s
</p>
<p>2.0
</p>
<p>1.8
</p>
<p>1.6
</p>
<p>1.4
</p>
<p>1.2
</p>
<p>1.0
</p>
<p>0.8
</p>
<p>0.6
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>0.0
</p>
<p>&ndash;0.2
</p>
<p>Number of Drug Arrests
</p>
<p>&ndash;2        0         2        4         6         8        10    
</p>
<p>W orking It Out
</p>
<p> � 1.099
</p>
<p> � 0.211 � 0.888
</p>
<p> � 0.211 � 0.148(6)
</p>
<p> Y � 0.211 � 0.148X
</p>
<p>For a parolee with eight drug arrests, our model predicts that Y will have
</p>
<p>a value of 1.395:
</p>
<p>W orking It Out
</p>
<p> � 1.395
</p>
<p> � 0.211 � 1.184
</p>
<p> � 0.211 � 0.148(8)
</p>
<p> Y � 0.211 � 0.148X
</p>
<p>Note: In this scatterplot (produced using SPSS), the points sometimes represent more
than one observation.
</p>
<p>Figure 18.1
</p>
<p>553I S I T I N A P P R O P R I A T E T O U S E O L S R E G R E S S I O N ?W H Y</p>
<p/>
</div>
<div class="page"><p/>
<p>But in our example, the predicted value of Y should logically be no
</p>
<p>greater than 1 or no less than 0. A value of 1 means that the parolee
</p>
<p>failed the drug test, and a value of 0 means that the parolee passed the
</p>
<p>drug test. Predicting values greater than 1 or less than 0 just does not
</p>
<p>make sense given the possible outcomes of a binary dependent variable.
</p>
<p>This example, then, illustrates a logical problem in using OLS meth-
</p>
<p>ods to gain estimates for cases where the dependent variable is dichoto-
</p>
<p>mous. The OLS approach assumes that there is no limit to the predicted
</p>
<p>value of the dependent variable. But in the case of a dichotomous de-
</p>
<p>pendent variable, there are limits&mdash;the values 0 and 1. While this as-
</p>
<p>sumption of predictions within the limits of the possible outcomes of the
</p>
<p>dependent variable is also violated when OLS regression is used for an
</p>
<p>ordinal-level dependent variable and sometimes when it is applied to
</p>
<p>specific interval-level measures, the violation is most extreme in the case
</p>
<p>of a binary dependent variable, such as drug testing failures. It does not
</p>
<p>make sense to analyze such situations with a model that allows the value
</p>
<p>of Y to increase at a constant rate for each change in the value of X. For
</p>
<p>our analysis to be consistent with the problem we are examining, it must
</p>
<p>provide predictions that are constrained to values between 0 and 1.
</p>
<p>method in a case where the dependent variable is dichotomous. In our
</p>
<p>discussion of excluded variables in Chapter 16, we noted that a central
</p>
<p>assumption of the regression approach is that there is no systematic rela-
</p>
<p>tionship between the error term and the independent variables included
</p>
<p>in the regression. When a systematic relationship exists, estimates of the
</p>
<p>regression coefficients are likely to be biased. But if you look at parolees
</p>
<p>for whom the value of prior drug arrests is greater than 5 (see Figure
</p>
<p>gression error and the independent variable. Because the actual value of
</p>
<p>Y cannot be greater than 1, and the predicted values continue to increase
</p>
<p>in a linear fashion (as evidenced by the regression line), the regression
</p>
<p>error increases in the negative direction as the number of prior drug ar-
</p>
<p>rests increases. This means that as the number of prior drug arrests gets
</p>
<p>larger and larger, we will make larger and larger negative errors in pre-
</p>
<p>diction. When OLS regression is used to examine a binary dependent
</p>
<p>variable, we are very likely to have a systematic relationship between the
</p>
<p>independent variable and the errors we make in predicting Y, because 
</p>
<p>Y-values are constrained to 0 and 1 and predicted values of Y have no
</p>
<p>limit.
</p>
<p>pendent variable, we violate assumptions important to making statistical
</p>
<p>inferences with OLS regression. We noted in Chapter 15 that two para-
</p>
<p>metric assumptions of our tests of statistical significance in regression are
</p>
<p>that the values of X are normally distributed around the regression line
</p>
<p>Figure 18.1 illustrates additional problems that arise in using the OLS
</p>
<p>18.1), you can see that there is a consistent relationship between the re-
</p>
<p>Figure 18.1 also illustrates why, when we examine a dichotomous de-
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N554</p>
<p/>
</div>
<div class="page"><p/>
<p>and that they meet an assumption of homoscedasticity (equal variances
</p>
<p>around the regression line). The normality assumption is clearly violated
</p>
<p>the shape of the distribution of X around Y will be bimodal because the
</p>
<p>observed values of our dependent variable are constrained in practice to
</p>
<p>0 and 1. However, if our sample is large enough, we generally allow vio-
</p>
<p>lations of this assumption. Our problem in regard to homoscedasticity is
</p>
<p>more serious. We noted in Chapter 15 that violations of the assumption
</p>
<p>of homoscedasticity must be large before they become a concern. In the
</p>
<p>case of a binary dependent variable, heteroscedasticity (violation of the
</p>
<p>homoscedasticity assumption) is likely to be large. As shown in Figure
</p>
<p>scores of X around the regression line is likely to vary widely in form,
</p>
<p>depending on the scores of the independent variable.
</p>
<p>L o g i s t i c  R e g r e s s i o n
</p>
<p>While the application of OLS regression methods to a dichotomous de-
</p>
<p>pendent variable raises a number of substantive and statistical concerns,
</p>
<p>there are many advantages to the basic form of the regression approach
</p>
<p>introduced in previous chapters. For example, the effect of each b was
</p>
<p>constant. That is, we could define a single constant effect for each vari-
</p>
<p>able in the regression model. That effect took into account the other
</p>
<p>variables in the model. And we could add all of these effects and the Y-
</p>
<p>intercept to get a predicted value for Y. Because of the utility of the re-
</p>
<p>gression approach, statisticians have developed alternative methods for
</p>
<p>conducting regression analysis with dichotomous dependent variables
</p>
<p>that do not violate basic assumptions but allow us to continue to use the
</p>
<p>overall regression approach. Perhaps the most widely used of these
</p>
<p>methods is logistic regression analysis.1 Logistic regression analysis is
</p>
<p>1A method called generalized least squares might also be used to deal with violations
</p>
<p>of our assumptions, though logistic regression analysis is generally the preferred
</p>
<p>method. See E. A. Hanushek and J. E. Jackson, Statistical Methods for Social Scientists
</p>
<p>(New York: Academic Press, 1977) for a comparison of these approaches. See also
</p>
<p>David W. Hosmer and Stanley Lemeshow, Applied Regression Analysis, 2nd ed. (New
</p>
<p>York: Wiley, 2000). Another method, probit regression analysis, is very similar to that
</p>
<p>presented here, though it is based on the standard normal distribution rather than the
</p>
<p>logistic model curve. The estimates gained from probit regression are likely to be very
</p>
<p>similar to those gained from logistic regression. Because logistic regression analysis
</p>
<p>has become much more widely used and is available in most statistical software pack-
</p>
<p>ages, we focus on logistic regression in this chapter.
</p>
<p>when we have a dichotomous dependent variable. As Figure 18.1 shows,
</p>
<p>18.1 for our distribution of drug testing failures, the distribution of the
</p>
<p>L O G I S T I C  R E G R E S S I O N 555</p>
<p/>
</div>
<div class="page"><p/>
<p>based on a transformation of the regression model that allows the out-
</p>
<p>comes of the regression equation to vary without limit, but constrains the
</p>
<p>predictions of the dependent variable to values between 0 and 1. At the
</p>
<p>same time, the inferential statistics used in logistic regression do not rely
</p>
<p>on assumptions regarding the population distribution of scores.
</p>
<p>In fitting the data that are analyzed, logistic regression analysis uses
</p>
<p>the logistic model curve for the probability that Y � 1. While the logis-
</p>
<p>tic regression curve follows the linear model in the middle of its distribu-
</p>
<p>tion, it does not allow values below 0 or above 1. Indeed, as the logistic
</p>
<p>curve approaches 0 or 1, it begins to flatten, so it keeps coming closer
</p>
<p>to&mdash;but never actually reaches&mdash;either of these two values. The logistic
</p>
<p>curve thus satisfies our primary objection to the linear OLS regression
</p>
<p>method. That is, it does not allow predictions greater than or less than the
</p>
<p>actual values of the distribution of scores that we are trying to predict.
</p>
<p>The logistic model curve provides a solution to the problem of pre-
</p>
<p>dictions beyond the observed distribution. However, in order to gain the
</p>
<p>desired property of outcomes between 0 and 1, we have to alter the
</p>
<p>form of our regression equation. Using OLS regression, we represent our
</p>
<p>equation for the prediction of Y with one independent variable as
</p>
<p>follows:
</p>
<p>Y � b0 � b1X1
</p>
<p>As noted earlier, this approach may yield values that are greater than 1
</p>
<p>or less than 0, as was the case in our drug testing example.
</p>
<p>The Logistic Model Curve
</p>
<p>P
ro
</p>
<p>b
ab
</p>
<p>il
it
</p>
<p>y 
(Y
</p>
<p> =
 1
</p>
<p>)
</p>
<p>1.0
</p>
<p>0.8
</p>
<p>0.6
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>0
</p>
<p>Xb
</p>
<p>&ndash;5    &ndash;4   &ndash;3    &ndash;2    &ndash;1     0      1      2      3      4      5 
</p>
<p>the logic of a curve rather than that of a straight line. Figure 18.2 shows
</p>
<p>Figure 18.2
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N556</p>
<p/>
</div>
<div class="page"><p/>
<p>In logistic regression, we alter the form of what we are trying to pre-
</p>
<p>dict. Rather than predicting Y, as in OLS regression, we now predict the
</p>
<p>natural logarithm (ln) of the odds of getting a 1 on the dependent vari-
</p>
<p>able. Although this sounds very imposing, it is simply a transformation of
</p>
<p>equation for a bivariate logistic regression:
</p>
<p>There is no change on the right side of this equation. We have the
</p>
<p>constant b0 and the regression coefficient b1 that reflects a constant effect
</p>
<p>for the independent variable examined. Moreover, the outcome of this
</p>
<p>formula has no limit. But on the left side of the equation, we now have
</p>
<p>the natural logarithm of the odds of Y, or what statisticians call the
</p>
<p>logit of Y. A logarithm, or log, is the exponent of the power to which
</p>
<p>a fixed number (called a base) must be raised to produce another num-
</p>
<p>ber. So, for example, if the base is 10, the logarithm of 100 is 2. That is, if
</p>
<p>we take 10 to the 2nd power (102), we get 100. In logistic regression, we
</p>
<p>do not use a base of 10, which is associated with what is called the com-
</p>
<p>mon logarithm; rather, we use a base of 2.71828, which is associated
</p>
<p>with what is called the natural logarithm and is represented in symbol
</p>
<p>form as ln. The base of the natural logarithm, 2.71828, is also known as
</p>
<p>Euler&rsquo;s constant and is denoted by the symbol e. What this means is that
</p>
<p>ln(x) is the power to which e must be raised to get x.
</p>
<p>This represents the odds of getting an outcome of 1, rather than 0, on
</p>
<p>the dependent variable. The odds are determined by dividing the proba-
</p>
<p>bility of getting a 1 [P(Y � 1)] by the probability of not getting a 1 [1 �
</p>
<p>P(Y � 1)]. In our drug testing example, this would be the odds of failing
</p>
<p>a drug test divided by those of not failing the test. If an individual had an
</p>
<p>80% predicted likelihood of failing the drug test, then the odds would be
</p>
<p>0.80/0.20, or 4 to 1.
</p>
<p>ln � P(Y � 1)1 � P(Y � 1)� � ln �P(Y � 1)P(Y � 0)� � b0 � b1X1
</p>
<p>W orking It Out
</p>
<p> � 4.0
</p>
<p> � 
0.80
0.20
</p>
<p> Odds � 
P(Y � 1)
</p>
<p>1 � P(Y � 1)
</p>
<p>the equation presented above. Equation 18.1 represents the prediction
</p>
<p>Equation 18.1
</p>
<p>What about the notation P(Y � 1)/[1�P(Y � 1)] in Equation 18.1?
</p>
<p>557L O G I S T I C  R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>If we transform this equation further, we see that it gives us the prop-
</p>
<p>erty we are looking for. That is, the predicted values of Y produced by
</p>
<p>our regression equation will vary between 0 and 1, despite the fact that
</p>
<p>the outcomes in our regression equation can reach any value between
</p>
<p>plus and minus infinity. In the box above, we show how to transform
</p>
<p>the equation so that the outcome is the probability that Y will be 1. The
</p>
<p>We begin with the specification of the logistic regression model:
</p>
<p>To simplify, we let
</p>
<p>and
</p>
<p>Using these simplifications, we can rewrite the logistic regression equa-
</p>
<p>tion as
</p>
<p>If we exponentiate both sides of the equation (i.e., take the value of e to
</p>
<p>the power of both sides of the equation), we obtain
</p>
<p>Then, ln is the power to which we must raise e to get ; that is,
</p>
<p>e ln[P/(1�P)] � 
P
</p>
<p>1 �P
</p>
<p>P
1 � P� P1 � P�
</p>
<p>e ln[P/(1�P)] � e Xb
</p>
<p>ln� P1 � P� � Xb
</p>
<p>P � P (Y � 1) &rArr; 1 � P (Y � 1) � 1 � P
</p>
<p>Xb � b0 � b1X1
</p>
<p>ln� P (Y � 1)1 � P (Y � 1)� � b0 � b1X1
</p>
<p>Derivation of P (Y � 1) 
from the Cumulative Logistic Probability Function</p>
<p/>
</div>
<div class="page"><p/>
<p>end result is a simple equation that can be calculated on a hand calcula-
</p>
<p>tor with a natural log function. This equation is often called the cumula-
</p>
<p>tive logistic probability function.
</p>
<p>P(Y � 1) � 
1
</p>
<p>1 � e�(b0�b1X1)
</p>
<p>This leads to rewriting the logistic regression equation as
</p>
<p>We multiply both sides of the equation by (1 � P):
</p>
<p>Then we add PeXb to both sides of the equation:
</p>
<p>Next we rewrite the equation to pull out the common factor, P :
</p>
<p>Now we divide both sides of the equation by (1 + e Xb) to solve for P :
</p>
<p>Since, as noted above, P � P (Y � 1),
</p>
<p>P (Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> � 
1
</p>
<p>1 � e�Xb
</p>
<p> P � 
e Xb
</p>
<p>1 � e Xb
 � 
</p>
<p>1
</p>
<p>�1 � e Xbe Xb �
 � 
</p>
<p>1
</p>
<p>� 1e Xb � 1�
</p>
<p>P (1 � e Xb) � e Xb
</p>
<p>P � Pe Xb � e Xb
</p>
<p>P � e Xb(1 � P) � e Xb � Pe Xb
</p>
<p>P
</p>
<p>1 � P
 � e Xb
</p>
<p>Equation 18.2</p>
<p/>
</div>
<div class="page"><p/>
<p>By using the term Xb to represent the right side of the regression equa-
</p>
<p>any number of independent variables:
</p>
<p>What this equation does is divide 1 by the sum of 1 and e (the value
</p>
<p>2.71828) taken to the �Xb power. The process of taking a number to
</p>
<p>some power is referred to as exponentiation. Here we exponentiate e to
</p>
<p>the power �Xb. Exponentiation may also be familiar to you as the an-
</p>
<p>tilog or inverse log.2
</p>
<p>P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p>Illustration of the Fact That 
P(Y � 1) Will Not Exceed 1 
or Be Less Than 0
</p>
<p>Xb P (Y � 1)
</p>
<p>�25 0.000000000014
�20 0.000000002061
�15 0.000000305902
�10 0.000045397669
�5 0.006692850924
�4 0.017986209962
�3 0.047425873178
�2 0.119202922022
�1 0.268941421370
</p>
<p>0 0.500000000000
1 0.731058578630
2 0.880797077978
3 0.952574126822
4 0.982013790038
5 0.993307149076
</p>
<p>10 0.999954602131
15 0.999999694098
20 0.999999997939
25 0.999999999986
</p>
<p>2Your calculator likely has a button labeled &ldquo;e x,&rdquo; which performs this operation. If
</p>
<p>there is no ex button, then you should be able to locate a button labeled &ldquo;INV&rdquo; and
</p>
<p>another for the natural logarithm, ln. By pushing &ldquo;INV&rdquo; and then &ldquo;ln&rdquo; (the inverse or
</p>
<p>antilog), you will be able to perform this operation.
</p>
<p>Equation 18.3
</p>
<p>tion, we may write Equation 18.2 more generally to take into account
</p>
<p>Table 18.3
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N560</p>
<p/>
</div>
<div class="page"><p/>
<p>Importantly, whatever the value associated with Xb, the value of 
</p>
<p>P(Y � 1) will always be between 0 and 1. The value of P(Y � 1) can get
</p>
<p>closer and closer to 1 or to 0, but it will never exceed that number. This
</p>
<p>tive values of Xb. As the values get very large, the gain for each increase
</p>
<p>in Xb becomes smaller and smaller. Logistic regression, then, allows us
</p>
<p>to use the traditional regression format in which the outcome, Xb, can
</p>
<p>achieve any size without limit. However, since we have converted what
</p>
<p>we are predicting to the logit of Y, our predictions of Y are bounded by
</p>
<p>0 and 1.
</p>
<p>comes of Y are constrained between 0 and 1. But what does a prediction
</p>
<p>between the values 0 and 1 mean? As we have already noted, the ob-
</p>
<p>served outcomes for a dichotomous dependent variable have a score of
</p>
<p>either 0 or 1. For example, in our drug testing example, either parolees
</p>
<p>failed the drug test (coded as 1) or they passed the test (coded as 0).
</p>
<p>When we examined the regression approach in previous chapters, the
</p>
<p>pendent variable, our interpretation must be different. The predicted
</p>
<p>value of Y in this case is the predicted probability of getting an outcome
</p>
<p>of 1. So, for example, a value of 0.50 in our drug testing example would
</p>
<p>mean that, according to our model, an individual was predicted to have
</p>
<p>about an equal chance of failing and not failing drug testing. A value of
</p>
<p>0.90 would suggest that an individual was highly likely to have a drug
</p>
<p>testing failure.
</p>
<p>To estimate the coefficients of a logistic regression, we use a much
</p>
<p>more complex mathematical process than was used in OLS regression.
</p>
<p>It is based on maximum likelihood estimation (MLE) techniques.
</p>
<p>Using these techniques, we try to maximize the probability that our re-
</p>
<p>gression estimates will produce a distribution similar to that of the ob-
</p>
<p>served data. With this approach, we do not simply derive a single
</p>
<p>mathematical solution for obtaining the regression estimates.3 Rather,
</p>
<p>we begin by identifying a tentative solution, which we then try to im-
</p>
<p>prove upon. Our criterion for improvement is termed a likelihood func-
</p>
<p>tion. A likelihood function measures the probability of observing the
</p>
<p>3It should be noted, however, that maximum likelihood techniques do not always re-
</p>
<p>quire an iterative process.
</p>
<p>is illustrated in Table 18.3, where we take very large negative and posi-
</p>
<p>allowed us to develop a regression model in which the predicted out-
</p>
<p>Use of the natural logarithm of the odds of Y, or the logit of Y, has
</p>
<p>bution of scores on our interval-level measure. With a dichotomous de-
</p>
<p>predicted value of Y was simply one of the possible values in the distri-
</p>
<p>561L O G I S T I C  R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>results in the sample, given the coefficient estimates in our model. By
</p>
<p>convention in logistic regression, we use �2 times the natural loga-
</p>
<p>rithm of the likelihood function (or �2LL), which is defined as the log
</p>
<p>likelihood function. We repeat this process again and again, until the
</p>
<p>change in the likelihood function is considered negligible. Each time
</p>
<p>we repeat the process and reestimate our coefficients is called an itera-
</p>
<p>tion. Logistic regression is said to be an iterative procedure, because it
</p>
<p>tries a number of solutions before arriving at a final result&mdash;or, in statis-
</p>
<p>tical terms, converging.
</p>
<p>Most packaged statistical programs set a default limit on the num-
</p>
<p>ber of iterations that can be tried. In SPSS, that limit is 20 iterations.
</p>
<p>Lack of convergence in a standard number of iterations may indi-
</p>
<p>cate some type of problem in the regression model. Often, it occurs
</p>
<p>when the number of variables examined is large relative to the
</p>
<p>number of cases in the study. John Tukey, a noted statistician who
</p>
<p>taught at Princeton University, has suggested a rule for logistic regres-
</p>
<p>sion: that there be at least five cases and preferably at least ten in 
</p>
<p>the smaller category of the dependent variable for each independent
</p>
<p>variable examined.4 Whatever the cause, if you receive a message
</p>
<p>from a packaged statistical analysis program that your regression has
</p>
<p>failed to converge, you should look carefully at your model and your
</p>
<p>measures.
</p>
<p>We have now looked at the basic logic of the logistic regression
</p>
<p>model. While the logistic regression model differs from the OLS re-
</p>
<p>gression model in the outcome predicted, the basic form of the addi-
</p>
<p>tive linear model has been maintained. The right side of the equation
</p>
<p>remains an additive function of the Y-intercept and the independent
</p>
<p>variables (multiplied by their associated regression coefficients). The
</p>
<p>effect of each independent variable remains its independent effect,
</p>
<p>with the other variables in the model controlled. We also continue 
</p>
<p>to be constrained by the same regression assumptions regarding cor-
</p>
<p>rect model specification. Excluding variables from the regression will
</p>
<p>lead to bias, either in our prediction of Y or in our estimates of
</p>
<p>specific regression coefficients. The models are also sensitive to prob-
</p>
<p>lems of multicollinearity. These concepts were reviewed in Chapter
</p>
<p>4See John Tukey, Report to the Special Master, p. 5; Report to the New Jersey Supreme
</p>
<p>Court 27 (1997).
</p>
<p>logistic regression.
</p>
<p>16, but you should remember that they apply to our discussion of
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N562</p>
<p/>
</div>
<div class="page"><p/>
<p>A Substantive Example: Adoption of Compstat in U.S. Police Agencies
</p>
<p>Application of logistic regression to a substantive problem will help you
</p>
<p>to understand the use of logistic regression, as well as the different coef-
</p>
<p>ficients associated with the technique. The example we use is drawn
</p>
<p>from a Police Foundation survey of U.S. police agencies, begun in 1999
5
</p>
<p>stat, a management system first developed in New York City in order to
</p>
<p>reduce crime and improve quality of life, had been widely adopted in
</p>
<p>some form by other U.S. police agencies. It was theorized that Compstat
</p>
<p>would be much more likely to be adopted by larger police agencies.
</p>
<p>Using logistic regression analysis, we will examine whether this hypothe-
</p>
<p>sis is supported by the Police Foundation data.
</p>
<p>The dependent variable in our analysis is dichotomous, measuring
</p>
<p>whether the department claimed to have adopted a &ldquo;Compstat-like pro-
</p>
<p>gram.&rdquo; The main independent variable is the number of sworn officers
</p>
<p>serving in the department at the time of survey.6 We also include, as a
</p>
<p>second independent variable, region, which divides the country into four
</p>
<p>regions: South, West, North Central, and Northeast. For this multicate-
</p>
<p>gory nominal variable, we use three dummy variables to represent re-
</p>
<p>gion and define the North Central region as the reference, or excluded,
</p>
<p> X4 � West
</p>
<p> X3 � South
</p>
<p> X2 � Northeast
</p>
<p> where X1 � number of sworn officers
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X3 � b4X4
</p>
<p>5For a description of this study, see David Weisburd, Stephen Mastrofski, Ann Marie
</p>
<p>McNally, and Rosann Greenspan, Compstat and Organizational Change (Washington,
</p>
<p>DC: The Police Foundation, 2001).
6Departments with 1,300 or more officers were coded in our example as 1,300 officers.
</p>
<p>This transformation was used in order to take into account the fact that only 5% of the
</p>
<p>departments surveyed had more than this number of officers and their totals varied very
</p>
<p>widely relative to the overall distribution. Another solution that could be used to address
</p>
<p>the problem of outliers is to define the measure as the logarithm of the number of sworn
</p>
<p>officers, rather than the raw scores. We relied on the former solution for our example be-
</p>
<p>cause interpretation of the coefficients is more straightforward. In an analysis of this
</p>
<p>problem, a researcher would ordinarily want to compare different transformations of the
</p>
<p>dependent variable in order to define the one that best fit the data being examined.
</p>
<p>police agencies with more than 100 sworn officers (N � 515) and got a 
</p>
<p>The Police Foundation surveyed all and completed in the year 2000.
</p>
<p>response rate of 86%. A main concern of the survey was whether Comp-
</p>
<p>category. Our regression model (Xb) is represented in Equation 18.4:
</p>
<p>Equation 18.4
</p>
<p>563L O G I S T I C  R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>Summary of the Logistic Regression Coefficients 
Using SPSS&rsquo;s Logistic Regression Program
</p>
<p>Variables in the Equation
</p>
<p>B S.E. Wald df Sig. Exp(B)
</p>
<p>Step 1 NORTHEAST .359 .372 .931 1 .335 1.432
</p>
<p>SOUTH .805 .332 5.883 1 .015 2.237
</p>
<p>WEST .428 .367 1.360 1 .244 1.534
</p>
<p>#SWORN .002 .000 24.842 1 .000 1.002
</p>
<p>Constant �1.795 .311 33.378 1 .000 .166
</p>
<p>a Variable(s) entered on step 1: NORTHEAST, SOUTH, WEST, #SWORN.
</p>
<p>As you can see, it took only three iterations to achieve convergence. The
</p>
<p>convergence criterion used in this SPSS run was that the log likelihood
</p>
<p>function declined by less than 0.010%. As noted earlier, we use �2 times
</p>
<p>the natural logarithm of the likelihood function (�2LL) to define the log
</p>
<p>likelihood function. The final coefficients listed in this table are the same
</p>
<p>as the regression coefficients (B) reported in the summary of the regres-
</p>
<p>Iteration History Using SPSS&rsquo;s Logistic Regression Program
</p>
<p>Iteration History
</p>
<p>�2 Log Coefficients
likelihood
</p>
<p>Iteration Constant NOREAST SOUTH WEST NMSWORN
</p>
<p>Step 1 1 493.418 �1.555 .258 .629 .308 .001
</p>
<p>2 492.515 �1.783 .351 .795 .419 .002
</p>
<p>3 492.513 �1.795 .359 .805 .428 .002
</p>
<p>a Method: Enter
b Constant is included in the model.
c Initial �2 Log Likelihood: 528.171
d Estimation terminated at iteration number 3 because log likelihood decreased by less than .010 percent.
</p>
<p>Table 18.4
</p>
<p>Table 18.4 shows the iteration history for estimating this regression.
</p>
<p>Table 18.5
</p>
<p>sion results provided in Table 18.5.
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N564</p>
<p/>
</div>
<div class="page"><p/>
<p>We can now express our regression equation in terms of the outcomes
</p>
<p>Inserting the values from our regression analysis, we can express the
</p>
<p>equation as follows (note that the constant in the SPSS printout is the 
</p>
<p>Y-intercept, b0):
</p>
<p>Xb � �1.75 � 0.002X1 � 0.359X2 � 0.805X 3 � 0.428X 4
</p>
<p>We can also develop predictions of Y from this model, as in the case
</p>
<p>of the OLS model. However, as explained above, our predictions of Y
</p>
<p>are not the direct outcome of our additive regression model. Rather, the
</p>
<p>lative logistic probability function:
</p>
<p>For example, let&rsquo;s say that we want to predict the probability of 
</p>
<p>Because North Central is the reference category, the equation contains
</p>
<p>only the Y-intercept and the effect of the number of sworn officers:
</p>
<p>P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> X 4 � West
</p>
<p> X 3 � South
</p>
<p> X2 � Northeast
</p>
<p> where X1 � number of sworn officers
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X 3 � b4X 4
</p>
<p>W orking It Out
</p>
<p> X 4 � West
</p>
<p> X 3 � South
</p>
<p> X2 � Northeast
</p>
<p> where X1 � number of sworn officers
</p>
<p> � 0.205
</p>
<p> � �1.795 � 2
</p>
<p> � �1.795 � 0.002(1000)
</p>
<p> � �1.795 � 0.002(1000) � 0.359(0) � 0.805(0) � 0.428(0)
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X 3 � b4X4
</p>
<p>of our analysis. Above, we defined our regression model using the term Xb :
</p>
<p>probability of Y occurring was expressed in Equation 18.3 for the cumu-
</p>
<p>a Compstat-like program in a department with 1,000 officers in the
</p>
<p>North Central region. Our first task is to define the value of Xb. We do
</p>
<p>that by applying coefficients gained in our logistic regression analysis.
</p>
<p>565L O G I S T I C  R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>For a police department in the South with 1,000 officers, the predicted
</p>
<p>probability of having a Compstat-like program is fully 73%:
</p>
<p>W orking It Out
</p>
<p>And,
</p>
<p> � 0.7330
</p>
<p> � 
1
</p>
<p>1 � 0.3642
</p>
<p> � 
1
</p>
<p>1 � e�1.01
</p>
<p> P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> � 1.01
</p>
<p> � �1.795 � 2 � 0.805
</p>
<p> � �1.795 � 0.002(1000) � 0.805(1)
</p>
<p> � �1.795 � 0.002(1000) � 0.359(0) � 0.805(1) � 0.428(0)
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X 3 � b4X 4
</p>
<p>Applying this result to our equation, we see that, according to our re-
</p>
<p>gression model, the probability of having a Compstat-like program in
</p>
<p>such a department is about 55%.
</p>
<p>W orking It Out
</p>
<p> � 0.55
</p>
<p> � 
1
</p>
<p>1 � 0.8146
</p>
<p> � 
1
</p>
<p>1 � e�0.205
</p>
<p>P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N566</p>
<p/>
</div>
<div class="page"><p/>
<p>I n t e r p r e t i n g  L o g i s t i c  R e g r e s s i o n  C o e f f i c i e n t s
</p>
<p>in our model. In this case, the logistic regression coefficients are listed in
</p>
<p>the column labeled B. As expected, the coefficient for the number of
</p>
<p>sworn officers (#SWORN) is positive; that is, as the number of sworn offi-
</p>
<p>cers increases, the likelihood of having a Compstat-like program also in-
</p>
<p>category. This means that in the Police Foundation sample, police depart-
</p>
<p>ments in the Northeast, West, and South regions were more likely to claim
</p>
<p>to have a Compstat-like program than those in the North Central region.
</p>
<p>What about the exact interpretation of the logistic regression coefficient?
</p>
<p>Here, we can see the price we pay for developing a regression model in
</p>
<p>W orking It Out
</p>
<p>And,
</p>
<p> � 0.3122
</p>
<p> � 
1
</p>
<p>1 � 2.2034
</p>
<p> � 
1
</p>
<p>1 � e�(�0.79)
</p>
<p> P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> � �0.79
</p>
<p> � �1.795 � 0.2 � 0.805
</p>
<p> � �1.795 � 0.002(100) � 0.805(1)
</p>
<p> � �1.795 � 0.002(100) � 0.359(0) � 0.805(1) � 0.428(0)
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X 3 � b4X 4
</p>
<p>If we apply our prediction model to smaller departments, we see that
</p>
<p>they are less likely, according to our estimates, to have a Compstat-like
</p>
<p>program. For example, our model suggests that a police agency with
</p>
<p>only 100 officers from the South would have a probability of only 31% of
</p>
<p>having a Compstat-like program:
</p>
<p>increases. The three dummy variables included for region also have a 
</p>
<p>positive impact relative to the North Central region, which is the excluded
</p>
<p>which the predictions of the probability of Y are constrained between 
</p>
<p>Using Table 18.5, we can also define the specific effects of the variables
</p>
<p>I N T E R P R E T I N G  L O G I S T I C  R E G R E S S I O N  C O E F F I C I E N T S 567</p>
<p/>
</div>
<div class="page"><p/>
<p>increase in Y (all other included independent variables held constant).
</p>
<p>The interpretation of the logistic regression coefficient is not as
</p>
<p>straightforward. Our regression equation is predicting not Y, but the loga-
</p>
<p>rithm of the odds of getting a 1&mdash;or, in our example, the log of the odds of
</p>
<p>having a Compstat-like program. In a multivariate logistic regression, b rep-
</p>
<p>resents the estimated change in the log of the odds of Y occurring when all
</p>
<p>other independent variables are held constant. The coefficient for number
</p>
<p>of sworn officers is 0.002, meaning that each additional officer increases by
</p>
<p>0.002 the log of the odds of having a Compstat-like program. While some
</p>
<p>researchers may have an intuitive understanding of the change in the log of
</p>
<p>the odds, the transformation of the outcome measure in the logistic regres-
</p>
<p>sion model has made the regression coefficients very difficult to explain or
</p>
<p>interpret in a way that nonstatisticians will understand.
</p>
<p>The Odds Ratio
</p>
<p>To make results easier to understand, statisticians have developed other
</p>
<p>methods of interpreting logistic regression coefficients. An approach
</p>
<p>commonly used is to report the regression coefficient in terms of its odds
</p>
<p>ratio. The odds ratio, sometimes called the exponent of B, is reported
</p>
<p>the impact of a one-unit change in X on the ratio of the probability of an
</p>
<p>event occurring to the probability of the event not occurring. Equation
</p>
<p>events separated by a change of one unit in X:
</p>
<p>An odds ratio greater than 1 indicates that the odds of getting a 1 on
</p>
<p>the dependent variable increase when the independent variable increases.
</p>
<p>An odds ratio less than 1 indicates that the odds of getting a 1 on the de-
</p>
<p>pendent variable decreases when the independent variable increases. For
</p>
<p>our example, an odds ratio greater than 1 indicates that as the indepen-
</p>
<p>dent variable increases, the odds of having a Compstat-like program also
</p>
<p>increase. If the odds ratio were 3, for example, then a one-unit change in
</p>
<p>  where P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> Odds ratio � 
	 P(Y � 1)1 � P(Y � 1)
X
	 P(Y � 1)1 � P(Y � 1)
X � 1
</p>
<p>0 and 1. In the OLS regression case, the interpretation of b is in reference 
</p>
<p>to units of measurement of Y. In the multivariate case, b represents the 
</p>
<p>estimated change in Y associated with a unit change in X, when all other 
</p>
<p>independent variables in the model are held constant. So a b of 2 in an OLS 
</p>
<p>regression suggests that a unit increase in X is associated with a two-unit
</p>
<p>as Exp(B) in the SPSS printout in Table 18.5. The odds ratio represents
</p>
<p>18.5 defines the odds ratio in terms of the calculation of the odds for two
</p>
<p>Equation 18.5
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N568</p>
<p/>
</div>
<div class="page"><p/>
<p>X would make the event Y about three times as likely to occur. An odds
</p>
<p>ratio less than 1 would suggest that the likelihood of having a Compstat-
</p>
<p>like program decreased as the independent variable increased.
</p>
<p>To calculate the odds ratio, we need to define the probability of get-
</p>
<p>ting a 1 [i.e., P(Y � 1)] on our dependent variable at two values, X and 
</p>
<p>X � 1. We can choose any two consecutive values of the independent
</p>
<p>variable; our odds ratio will be the same, no matter what consecutive val-
</p>
<p>ues we choose. Let&rsquo;s start with number of sworn officers. For simplicity,
</p>
<p>we will take 101 and 100 as X and X � 1 and we will calculate the proba-
</p>
<p>bilities when a department is in the North Central region. We first need to
</p>
<p>work out the odds of getting a Compstat-like program from our model for
</p>
<p>the case of 101 sworn officers. As shown below, the result is 0.2034.
</p>
<p>W orking It Out Number of Sworn Officers � 101
</p>
<p>Step 1: Defining the probability of Y � 1.
</p>
<p>Step 2: Defining P(Y � 1).
</p>
<p>Step 3: Defining the odds.
</p>
<p> � 0.2034
</p>
<p> 	 P(Y � 1)1 � P(Y � 1)
X � 
0.1690
0.8310
</p>
<p> � 0.1690
</p>
<p> � 
1
</p>
<p>1 � 4.9185
</p>
<p> � 
1
</p>
<p>1 � e�(�1.593)
</p>
<p> P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> X 4 � West
</p>
<p> X 3 � South
</p>
<p> X2 � Northeast
</p>
<p> where X1 � number of sworn officers
</p>
<p> � �1.593
</p>
<p> � �1.795 � 0.202
</p>
<p> � �1.795 � 0.002(101)
</p>
<p> � �1.795 � 0.002(101) � 0.359(0) � 0.805(0) � 0.428(0)
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X 3 � b4X 4
</p>
<p>569I N T E R P R E T I N G  L O G I S T I C  R E G R E S S I O N  C O E F F I C I E N T S</p>
<p/>
</div>
<div class="page"><p/>
<p>We then need to follow the same procedure for 100 sworn officers. As
</p>
<p>shown below, the odds in the case of 100 sworn officers in the North
</p>
<p>Central region are 0.2029.
</p>
<p>W orking It Out Number of Sworn Officers � 100
</p>
<p>Step 1: Defining the probability of Y � 1.
</p>
<p>Step 2: Defining P (Y � 1).
</p>
<p>Step 3: Defining the odds.
</p>
<p> � 0.2029
</p>
<p> 	 P(Y � 1)1 � P(Y � 1)
X�1 � 
0.1687
0.8313
</p>
<p> � 0.1687
</p>
<p>  � 
1
</p>
<p>1 � 4.9283
</p>
<p> � 
1
</p>
<p>1 � e�(�1.595)
</p>
<p> P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> X 4 � West
</p>
<p> X 3 � South
</p>
<p> X2 � Northeast
</p>
<p> where X1 � number of sworn officers
</p>
<p> � �1.595
</p>
<p> � �1.795 � 0.2
</p>
<p> � �1.795 � 0.002(100)
</p>
<p> � �1.795 � 0.002(100) � 0.359(0) � 0.805(0) � 0.428(0)
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X 3 � b4X 4
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N570</p>
<p/>
</div>
<div class="page"><p/>
<p>Finally, using these two odds, we can estimate our odds ratio, which is
</p>
<p>simply the ratio of these two numbers, or 1.002:
</p>
<p>W orking It Out
</p>
<p> � 1.002
</p>
<p> � 
0.2034
0.2029
</p>
<p> Odds ratio � 
	 P(Y � 1)1 � P(Y � 1)
X
	 P(Y � 1)1 � P(Y � 1)
X�1
</p>
<p>Of course, it would be a lot easier to just look at your computer print-
</p>
<p>out, which provides the same outcome. You will probably not calculate
</p>
<p>odds ratios by hand outside your statistics class. But working out the
</p>
<p>odds ratio gives you a better understanding of what it is and where it
</p>
<p>comes from.
</p>
<p>ber of sworn officers. Instead of working through the three steps
</p>
<p>above, we can move directly from the logistic regression coefficient to
</p>
<p>the odds ratio by exponentiating the value of the coefficient b. As
</p>
<p>noted earlier in the chapter, when we exponentiate the value of the co-
</p>
<p>efficient b, we take e&mdash;the value 2.71828&mdash;to the power of the coeffi-
</p>
<p>cient b. For number of sworn officers, it is e (0.002) � 1.002. What this
</p>
<p>means is that for any logistic regression analysis, all we need to do is
</p>
<p>exponentiate the logistic regression coefficient to calculate the odds
</p>
<p>ratio. SPSS, like most other statistical software, will automatically report
</p>
<p>the odds ratios for each of the independent variables included in the
</p>
<p>analysis.
</p>
<p>It is important to keep in mind that the odds ratio provides an esti-
</p>
<p>mate for only a single one-unit increase in the independent variable. The
</p>
<p>odds ratio is not a linear function of the coefficients; thus, we cannot say
</p>
<p>that for each one-unit increase in the independent variable, the odds in-
</p>
<p>crease by some amount. If we are interested in a change of more than
</p>
<p>one unit in our independent variable&mdash;say 2, 5, 10, or 100 units&mdash; we
</p>
<p>multiply that number by our coefficient b and then exponentiate that
</p>
<p>value. For example, returning to the number of sworn officers, suppose
</p>
<p>we are interested in the odds of adopting a Compstat-like program for a
</p>
<p>In Table 18.5, we see that b � 0.002 and Exp(B) � 1.002 for num-
</p>
<p>571I N T E R P R E T I N G  L O G I S T I C  R E G R E S S I O N  C O E F F I C I E N T S</p>
<p/>
</div>
<div class="page"><p/>
<p>department that added 100 officers. We multiply our coefficient of 0.002
</p>
<p>by 100, getting a value of 0.2, and then take e to the power of 0.2, which
</p>
<p>gives us a value of 1.2214.
</p>
<p>W orking It Out
</p>
<p>Odds ratio � e (0.002)(100) � e (0.2) � 1.2214
</p>
<p>This odds ratio tells us that the odds of adopting a Compstat-like
</p>
<p>program increase by a factor of 1.22 for a department with 100 addi-
</p>
<p>tional officers. As an exercise, take the odds of adopting a Compstat-
</p>
<p>like program for a department with 100 officers in the North Central
</p>
<p>with 200 officers. Then take the ratio of these two odds&mdash;it will equal
</p>
<p>1.2214.
</p>
<p>Our focus on the number of sworn officers illustrates another fea-
</p>
<p>ture of logistic regression coefficients that is easily overlooked. There
</p>
<p>are times&mdash;usually for an interval-level independent variable&mdash; when
</p>
<p>the logistic regression coefficient will appear to have a small value.
</p>
<p>Yet, when we begin to account for the range of the independent vari-
</p>
<p>able and start to look at increases of 10, 100, or even 1,000 in the inde-
</p>
<p>pendent variable, we may find that the odds increase by a substantial
</p>
<p>amount.
</p>
<p>For our regional dummy variables, it should be remembered that the
</p>
<p>three measures are compared to the reference category, the North Cen-
</p>
<p>tral region. Because working out the odds ratio is tedious, we will carry
</p>
<p>out the calculations only for the South. According to the results pre-
</p>
<p>meaning that being in the South region of the country, as opposed to the
</p>
<p>North Central region, more than doubles the odds of having a Compstat-
</p>
<p>like program. As with our number of sworn officers coefficient, we get a
</p>
<p>value of 2.2367 by taking e to the power of 0.805, which is the logistic
</p>
<p>regression coefficient for the South region.
</p>
<p>W orking It Out
</p>
<p>Odds ratio � e (0.805) � 2.2367
</p>
<p>sented in Table 18.5, the South has an associated odds ratio of 2.237,
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N
</p>
<p>region (0.2029; see page 570) and calculate the odds for a department
</p>
<p>572</p>
<p/>
</div>
<div class="page"><p/>
<p>Alternatively, we can work through the calculation of the odds ratio
</p>
<p>to arrive at the same conclusion. Setting the number of sworn officers at
</p>
<p>100, we will calculate the odds ratio of a Compstat-like program for the
</p>
<p>case where a department is in the South versus the case where it is in
</p>
<p>the North Central region.
</p>
<p>W orking It Out Departments in the South
</p>
<p>Step 1: Defining the probability of Y � 1.
</p>
<p>Step 2: Defining P(Y � 1).
</p>
<p>Step 3: Defining the odds.
</p>
<p> � 0.4539
</p>
<p> 	 P(Y � 1)1 � P(Y � 1)
X � 
0.3122
</p>
<p>0.6878
</p>
<p> � 0.3122
</p>
<p> � 
1
</p>
<p>1 � 2.2033
</p>
<p> � 
1
</p>
<p>1 � e�(�0.790)
</p>
<p> P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> X 4 � West
</p>
<p> X 3 � South
</p>
<p> X2 � Northeast
</p>
<p> where X1 � number of sworn officers
</p>
<p> � �0.790
</p>
<p> � �1.795 � 0.200 � 0.805
</p>
<p> � �1.795 � 0.002(100) � 0.805(1)
</p>
<p> � �1.795 � 0.002(100) � 0.359(0) � 0.805(1) � 0.428(0)
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X 3 � b4X 4
</p>
<p>573I N T E R P R E T I N G  L O G I S T I C  R E G R E S S I O N  C O E F F I C I E N T S</p>
<p/>
</div>
<div class="page"><p/>
<p>W orking It Out Departments in the North Central Region
</p>
<p>Step 1: Defining the probability of Y � 1.
</p>
<p>Step 2: Defining P (Y � 1).
</p>
<p>Step 3: Defining the odds.
</p>
<p> � 0.2029
</p>
<p> 	 P(Y � 1)1 � P(Y � 1)
X�1 � 
0.1687
0.8313
</p>
<p> � 0.1687
</p>
<p> � 
1
</p>
<p>1 � 4.9283
</p>
<p> � 
1
</p>
<p>1 � e�(�1.595)
</p>
<p>P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p> X 4 � West
</p>
<p> X 3 � South
</p>
<p> X2 � Northeast
</p>
<p> where X1 � number of sworn officers
</p>
<p> � �1.595
</p>
<p> � �1.795 � 0.2
</p>
<p> � �1.795 � 0.002(100)
</p>
<p> � �1.795 � 0.002(100) � 0.359(0) � 0.805(0) � 0.428(0)
</p>
<p> Xb � b0 � b1X1 � b2X2 � b3X 3 � b4X 4
</p>
<p>W orking It Out
</p>
<p> � 2.2367
</p>
<p> � 
0.4539
0.2029
</p>
<p> Odds ratio � 
	 P(Y �1)1 �P (Y �1)
X
	 P (Y � 1)1 � P(Y � 1)
X�1
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N574</p>
<p/>
</div>
<div class="page"><p/>
<p>Turning to the odds comparing the West and Northeast regions with
</p>
<p>the North Central region, we can see that the differences are smaller (see
</p>
<p>meaning that departments in these regions have a higher likelihood of
</p>
<p>reporting a Compstat-like program. The odds ratios for both regions are
</p>
<p>about 1.5. A police department in these regions is about 1.5 times as
</p>
<p>likely to have a Compstat-like program as a department in the North
</p>
<p>Central region.
</p>
<p>The Derivative at Mean
</p>
<p>tic regression coefficient into a simple linear regression coefficient.
</p>
<p>Accordingly, it has the advantage of having the same interpretation as
</p>
<p>the result would have had if OLS regression had been appropriate to the
</p>
<p>problem. The disadvantage of the derivative at mean is that it calculates
</p>
<p>the regression coefficient as if it had a constant effect over the entire dis-
</p>
<p>tribution of predicted values of Y, based on the change observed when
</p>
<p>the predicted value of Y is at its mean. In fact, the logistic curve in Figure
</p>
<p>derivative at mean will be largest when the mean of the dependent vari-
</p>
<p>able is close to the middle of the logistic curve. As the mean of the distri-
</p>
<p>The interpretation of the derivative at mean is similar to that of the
</p>
<p>OLS regression coefficient. The derivative at mean may be defined as the
</p>
<p>change in Y associated with a unit change in X at the mean value of 
</p>
<p>the dependent variable. The derivative at mean is defined mathemati-
</p>
<p>where is the mean of the dependent variable (i.e., the proportion of
</p>
<p>cases having a value of 1 for the dependent variable).
</p>
<p>in our regression model. Since about 33% of the sample claimed to have
</p>
<p>implemented a Compstat-like program, the derivative at mean is calcu-
</p>
<p>lated for a mean of Y of 0.33. If we look at the derivative at mean for 
</p>
<p>the dummy variables associated with region, we can see the advantage
</p>
<p>of this approach. Taking the South region, for which the difference 
</p>
<p>Y
</p>
<p>DM � Y (1 � Y )bi
</p>
<p>regression coefficient (but that is not reported in SPSS) is the derivative
</p>
<p>at mean (DM). The derivative at mean converts the nonlinear logis-
</p>
<p>18.2 shows that the impact of the parameters will change in absolute
</p>
<p>cally in Equation 18.6:
</p>
<p>Table 18.6 provides the derivative at mean for each of the coefficients
</p>
<p>Equation 18.6
</p>
<p>terms, depending on where in the distribution they are calculated. The
</p>
<p>bution moves closer to the tails of the logistic curve, the derivative will
</p>
<p>be smaller.
</p>
<p>Table 18.5). Like the South region statistic, these coefficients are positive,
</p>
<p>Another measure that sometimes makes it easier to understand the logistic
</p>
<p>575I N T E R P R E T I N G  L O G I S T I C  R E G R E S S I O N  C O E F F I C I E N T S</p>
<p/>
</div>
<div class="page"><p/>
<p>from the excluded category is largest, we calculate the derivative at
</p>
<p>mean below:
</p>
<p>W orking It Out
</p>
<p> � 0.1778
</p>
<p> � (0.33)(0.67)(0.805)
</p>
<p> � (0.33)(1 � 0.33)(0.805)
</p>
<p> DM � Y (1 � Y )bi
</p>
<p>We can interpret this coefficient much as we interpreted the dummy
</p>
<p>variable regression coefficients in Chapter 16. If a police department is
</p>
<p>located in the South as opposed to the North Central region, its outcome
</p>
<p>on the dependent variable is about 0.1778 unit higher. Since the depen-
</p>
<p>dent variable has values ranging between 0 and 1, we can interpret this
</p>
<p>coefficient in terms of percentages. Departments in the South have, on
</p>
<p>average, about an 18 percentage-point higher chance of claiming to have
</p>
<p>a Compstat-like program when Y is at its mean.
</p>
<p>The derivative at mean for number of sworn officers is about 0.0004.
</p>
<p>This suggests that for each additional officer, there is a 0.0004 increase in
</p>
<p>the value of Y. According to the derivative at mean, an increase in 100
</p>
<p>officers would lead to a 4 percentage-point increase in the likelihood of
</p>
<p>having a Compstat-like program. An increase of 1,000 officers would
</p>
<p>lead to a 40 percentage-point increase.
</p>
<p>W orking It Out
</p>
<p> � 0.0004
</p>
<p> � (0.33)(0.67)(0.002)
</p>
<p> � (0.33)(1 � 0.33)(0.002)
</p>
<p> DM � Y (1 � Y )bi
</p>
<p>Derivative at Mean for Each of the Regression 
Coefficients in the Compstat Example
</p>
<p>VARIABLE b
</p>
<p>Northeast 0.359 (0.33)(0.67)(0.359) � 0.0794
South 0.805 (0.33)(0.67)(0.805) � 0.1778
West 0.428 (0.33)(0.67)(0.428) � 0.0946
Number of Sworn Officers 0.002 (0.33)(0.67)(0.002) � 0.0004
</p>
<p>DM � Y (1 � Y )bi
</p>
<p>Table 18.6
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N576</p>
<p/>
</div>
<div class="page"><p/>
<p>C o m p a r i n g  L o g i s t i c  R e g r e s s i o n  C o e f f i c i e n t s
</p>
<p>In Chapter 16, you saw how standardized regression coefficients could
</p>
<p>be used to compare the magnitude of regression coefficients measured
</p>
<p>on different scales. There is no widely accepted method for comparing
</p>
<p>the magnitude of the coefficients in logistic regression. When variables
</p>
<p>are measured on the same scale, we can rely on comparisons of the sta-
</p>
<p>tistics we have reviewed so far. For example, if our model includes two
</p>
<p>binary dummy variables, we can easily gain a sense of the impact of
</p>
<p>each variable by comparing the size of each odds ratio.
</p>
<p>Let&rsquo;s say that we are interested in predicting the likelihood of getting a
</p>
<p>prison sentence for a sample of convicted burglars. We include two bi-
</p>
<p>nary dummy variables in our analysis. The odds ratio for the first variable,
</p>
<p>gender (0 � female; 1 � male), is 1.5. The odds ratio for the second,
</p>
<p>whether a gun was used in the burglary (0 � no; 1 � yes), is 2.0. In this
</p>
<p>case, we could say that use of a weapon has a larger effect on the likeli-
</p>
<p>hood of getting a prison sentence than does gender. In the case of gen-
</p>
<p>der, being a male as opposed to a female increases the odds of getting a
</p>
<p>prison sentence by about 50%. However, according to these estimates,
</p>
<p>using a gun in the burglary doubles the odds of getting a prison sentence.
</p>
<p>Using Probability Estimates to Compare Coefficients
</p>
<p>If variables are measured on very different scales, comparing the magni-
</p>
<p>tude of effects from one variable to another is often difficult. One easily
</p>
<p>understood and transparent method for doing this is to rely on the pre-
</p>
<p>dicted probabilities of Y. In a study using logistic regression, Wheeler,
</p>
<p>Weisburd, and Bode were confronted with a large number of statistically
</p>
<p>significant independent variables measured on very different scales.7
</p>
<p>They decided to calculate probability estimates for measures at selected
</p>
<p>intervals when the scores of all other predictors were held at their mean.
</p>
<p>They also calculated a range of predictions computed from the 5th to
</p>
<p>95th percentile scores for the measure of interest. The table they devel-
</p>
<p>The study examined factors that explained whether or not white-
</p>
<p>collar offenders convicted in federal courts were sentenced to prison.
</p>
<p>The table gives the reader a sense of how changes in the independent
</p>
<p>variable affect changes in the dependent variable, as well as a general
</p>
<p>idea (using the range) of the overall influence of the measure examined.
</p>
<p>For example, the amount of &ldquo;dollar victimization&rdquo; in an offense (variable
</p>
<p>2) and &ldquo;role in offense&rdquo; (variable 13) are both ordinal-level variables but
</p>
<p>are measured with a different number of categories. Looking at the table,
</p>
<p>7See Stanton Wheeler, David Weisburd, and Nancy Bode, &ldquo;Sentencing the White Col-
</p>
<p>lar Offender: Rhetoric and Reality,&rdquo; American Sociological Review 47 (1982): 641&ndash;659.
</p>
<p>oped is reproduced in Table 18.7.
</p>
<p>C O M P A R I N G  L O G I S T I C  R E G R E S S I O N  C O E F F I C I E N T S 577</p>
<p/>
</div>
<div class="page"><p/>
<p>Selected Probability Estimates and Calculated Range 
for Significant Variables in Wheeler, Weisburd, 
and Bode&rsquo;s Study of White-Collar-Crime Sentencing
</p>
<p>PROBABILITY PROBABILTY
</p>
<p>INDEPENDENT VARIABLES ESTIMATESa RANGEb INDEPENDENT VARIABLES ESTIMATESa RANGEb
</p>
<p>I. Act-Related Variables
1) Maximum Exposure to
</p>
<p>Prison 44
1 day&ndash;1 year 32
1 year &amp; 1 day&ndash;2 years 35
4 years &amp; 1 day&ndash;5 years 45
14 years &amp; 1 day&ndash;15 years 76
</p>
<p>2) Dollar Victimization 41
$101&ndash;$500 27
$2,501&ndash;$5,000 38
$10,001&ndash;$25,000 47
$25,001&ndash;$100,000 51
over $2,500,000 68
</p>
<p>3) Complexity/Sophistication 27
4 32
6 38
8 45
</p>
<p>10 52
12 59
</p>
<p>4) Spread of Illegality 21
Individual 40
Local 47
Regional 54
National/International 61
</p>
<p>II. Actor-Related Variables
7) Social Background: 
</p>
<p>Duncan S.E.I. 29
15.1 28
49.4 41
62.0 47
66.1 49
84.0 57
</p>
<p>8) Social Background: 
Impeccability 17
</p>
<p>7 54
11 49
14 45
17 42
21 37
</p>
<p>9) Criminal Background:
Number of Arrests 22
</p>
<p>0 37
1 43
2 45
5 51
9 59
</p>
<p>10) Criminal Background:
Most Serious Prior 
Conviction 20
</p>
<p>None 37
Minor Offense 46
Low Felony 52
Moderate Felony 57
</p>
<p>13) Role in Offense 24
Minor 26
Missing 33
Single/Primary 50
</p>
<p>III. Legal Process Variables
16) Statutory Offense Category 39
</p>
<p>Antitrust Violations 28
Bribery 30
Bank Embezzlement 36
False Claims 36
Postal Fraud 38
Lending/Credit Fraud 45
SEC Violations 65
Tax Violations 69
</p>
<p>IV. Other Variables 30
17) Sex
</p>
<p>Male 50
Female 20
</p>
<p>18) Age &mdash;&mdash;c
</p>
<p>22 42
30 48
39 50
48 46
61 32
</p>
<p>21) District 28
Northern Georgia 34
Southern New York 34
Central California 43
Western Washington 43
Maryland 50
Northern Illinois 53
Northern Texas 62
</p>
<p>aEstimated likelihood of imprisonment when scores on all other variables are held at their mean.
bRange computed from 5th to 95th percentile score.
cBecause of the curvilinear effect measured here, the range is not relevant.
</p>
<p>Table 18.7
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N578</p>
<p/>
</div>
<div class="page"><p/>
<p>we can see that a person playing a minor role in an offense had a pre-
</p>
<p>dicted probability of imprisonment of about 26%, while someone playing
</p>
<p>a primary role had a 50% likelihood of imprisonment, according to the
</p>
<p>model estimated (and holding all other independent variables constant at
</p>
<p>their mean). A crime involving less than $500 in victimization led to an
</p>
<p>estimated likelihood of imprisonment of 27%. A crime netting over
</p>
<p>$2,500,000 led to an estimated likelihood of imprisonment of 68%. If we
</p>
<p>compare the range of predicted values between the 5th and 95th per-
</p>
<p>centile scores for each variable, our calculation suggests that dollar vic-
</p>
<p>timization (with a range of 41%) has a much larger impact than role in
</p>
<p>an offense (with a range of 24%). Of course, the choice of the 5th and
</p>
<p>95th percentiles is arbitrary. And this method also arbitrarily holds every
</p>
<p>other independent variable to its mean. Nonetheless, the advantage of
</p>
<p>this approach is that it provides a method of comparison that is straight-
</p>
<p>forward and easy for the nonstatistician to understand.
</p>
<p>To apply this method to our data, we need information on the mean
</p>
<p>for each independent variable. For our data, the means are
</p>
<p>Northeast: 0.225
</p>
<p>South: 0.373
</p>
<p>West: 0.229
</p>
<p>Number of sworn officers: 334.784
</p>
<p>scribes the results. Using this table, we can see that there are very large
</p>
<p>differences in the predicted probabilities of a Compstat-like program for
</p>
<p>departments of varying size. This illustrates a point made earlier, when
</p>
<p>we noted that the odds ratio for each change in number of sworn officers
</p>
<p>was small. Though the change per unit change in X is small in this case
</p>
<p>(because departments differ widely in size), the predicted change can be
</p>
<p>very large. Under this approach, the range variable suggests a larger im-
</p>
<p>pact for number of sworn officers than for region of country.
</p>
<p>Table of Selected Probability Estimates 
and Range for the Compstat Model
</p>
<p>VARIABLE PROBABILITY ESTIMATE RANGE
</p>
<p>Number of sworn officers:
100 (5th percentile) 0.2468 0.53
500 0.4217
1,300 (95th percentile) 0.7831
</p>
<p>Northeast 0.4090 &mdash;&mdash;
</p>
<p>South 0.4647 &mdash;&mdash;
</p>
<p>West 0.4216 &mdash;&mdash;
</p>
<p>Table 18.8
</p>
<p>579C O M P A R I N G  L O G I S T I C  R E G R E S S I O N  C O E F F I C I E N T S
</p>
<p>to the method employed by Wheeler, Weisburd, and Bode. Table 18.8 de-
</p>
<p>In the box on pages 580 581, the calculations are carried out according  and </p>
<p/>
</div>
<div class="page"><p/>
<p>&ldquo;Standardized&rdquo; Logistic Regression Coefficients
</p>
<p>Some statistical software programs list the standardized logistic regres-
</p>
<p>sion coefficient Beta, which is analogous to the standardized regression
</p>
<p>coefficient. Like the standardized regression coefficient, the standardized
</p>
<p>logistic regression coefficient can be interpreted relative to changes
</p>
<p>For all of the following calculations,
</p>
<p>P (Y � 1) � 
1
</p>
<p>1 � e�[�1.3159 � (0.002)(1300)]
 � 
</p>
<p>1
</p>
<p>1 � e�(1.2841)
 � 0.7831
</p>
<p>P (Y � 1) � 
1
</p>
<p>1 � e�[�1.3159 � (0.002)(500)]
 � 
</p>
<p>1
</p>
<p>1 � e�(�0.3159)
 � 0.4217
</p>
<p>P (Y � 1) � 
1
</p>
<p>1 � e�[�1.3159 � (0.002)(100)]
 � 
</p>
<p>1
</p>
<p>1 � e�(�1.1159)
 � 0.2468
</p>
<p>P (Y � 1) � 
1
</p>
<p>1 � e�(�1.3159�0.002X1)
</p>
<p> � �1.3159 � 0.002X1
</p>
<p> � �1.795 � 0.002X1 � 0.0808 � 0.3003 � 0.0980
</p>
<p> � �1.795 � 0.002X1 � 0.359(0.225) � 0.805(0.373) � 0.428(0.229)
</p>
<p> Xb � b0 � b1 X1 � b2 X2 � b3 X3 � b4 X4
</p>
<p> X 4 � West
</p>
<p> X 3 � South
</p>
<p> X2 � Northeast
</p>
<p> X1 � number of sworn officers
</p>
<p>Calculating Selected Probability Estimates 
and Range for the Compstat Model
</p>
<p>Probability estimate for number of sworn officers:
</p>
<p>P (Y � 1) for 100 officers:
</p>
<p>P (Y � 1) for number of sworn officers:
</p>
<p>P (Y � 1) for 500 officers:
</p>
<p>P (Y � 1) for 1,300 officers:</p>
<p/>
</div>
<div class="page"><p/>
<p>(measured in standard deviation units) in the independent variable. The
</p>
<p>magnitude of the standardized logistic regression coefficient allows us to
</p>
<p>compare the relative influence of the independent variables, since a
</p>
<p>larger value for the standardized coefficient means that a greater change
</p>
<p>in the log of the odds is expected. In contrast to the standardized regres-
</p>
<p>Probability estimate for Northeast:
</p>
<p>Probability estimate for South:
</p>
<p>Probability estimate for West:
</p>
<p>P (Y � 1) � 
1
</p>
<p>1 � e�(�0.7443 � 0.428X4)
 � 
</p>
<p>1
</p>
<p>1 � e 0.3163
 � 0.4216
</p>
<p>P (Y � 1) for West:
</p>
<p> � �0.7443 � 0.428X4
</p>
<p> � �1.795 � 0.6696 � 0.0808 � 0.3003 � 0.428X4
</p>
<p> � �1.795 � 0.002(334.784) � 0.359(0.225) � 0.805(0.373) � 0.428X4
</p>
<p> Xb � b0 � b1X1 � b2 X2 � b3 X 3 � b4 X 4
</p>
<p>P (Y � 1) � 
1
</p>
<p>1 � e�(�0.9466 � 0.805X3)
 � 
</p>
<p>1
</p>
<p>1 � e 0.1416
 � 0.4647
</p>
<p>P (Y � 1) for South:
</p>
<p> � �0.9466 � 0.805X 3
</p>
<p> � �1.795 � 0.6696 � 0.0808 � 0.805X3 � 0.0980
</p>
<p> � �1.795 � 0.002(334.784) � 0.359(0.225) � 0.805X3 � 0.428(0.229)
</p>
<p> Xb � b0 � b1X1 � b2 X2 � b3 X 3 � b4 X 4
</p>
<p>P (Y � 1) � 
1
</p>
<p>1 � e�(�0.7271 � 0.359X2)
  � 
</p>
<p>1
</p>
<p>1 �e 0.3681
 � 0.4090
</p>
<p>P (Y � 1) for Northeast:
</p>
<p> � �0.7271 � 0.359X2
</p>
<p> � �1.795 � 0.6696 � 0.359X2 � 0.3003 � 0.0980
</p>
<p> � �1.795 � 0.002(334.784) � 0.359X2 � 0.805(0.373) � 0.428(0.229)
</p>
<p> Xb � b0 � b1X1 � b2 X2 � b3 X3 � b4X 4</p>
<p/>
</div>
<div class="page"><p/>
<p>gistic regression models does not fall between 0 and 1, but can take on
</p>
<p>any value.8 Some statisticians warn that such coefficients should be inter-
</p>
<p>preted with caution.9 Nonetheless, they can provide a method for gain-
</p>
<p>ing a general sense of the strength of coefficients in logistic regression.
</p>
<p>The standardized logistic regression coefficient is calculated using
</p>
<p>where bi is the unstandardized coefficient for variable i from the origi-
</p>
<p>nal logistic regression model and si is the standard deviation for vari-
</p>
<p>able i. We interpret Beta as the change in the log of the odds of 
</p>
<p>P (Y � 1) relative to changes (measured in standard deviation units) 
</p>
<p>in the independent variable. For example, a Beta of 0.4 implies that
</p>
<p>for a one-standard-deviation change in the independent variable, 
</p>
<p>the log of the odds is expected to increase by 0.4. Alternatively, if 
</p>
<p>Beta � �0.9, a one-standard-deviation change in the independent
</p>
<p>variable is expected to result in a decrease of 0.9 in the log of the
</p>
<p>odds that P (Y � 1).
</p>
<p>Returning to our example using the Compstat data, we find the stan-
</p>
<p>dardized coefficient for number of sworn officers to be 0.6616.
</p>
<p>Betai � bisi
</p>
<p>8Some researchers have proposed alternative ways of calculating standardized logistic re-
</p>
<p>gression coefficients that allow for interpretations related to changes in probabilities. See,
</p>
<p>for example, Robert L. Kaufman, &ldquo;Comparing Effects in Dichotomous Logistic Regression:
</p>
<p>A Variety of Standardized Coefficients,&rdquo; Social Science Quarterly 77 (1996): 90&ndash;109.
9For example, see Andy Field, Discovering Statistics Using SPSS for Windows (London:
</p>
<p>Sage Publications, 2000).
</p>
<p>W orking It Out
</p>
<p> � 0.6616
</p>
<p> � (0.002)(330.797)
</p>
<p>Beta � bisi
</p>
<p>model and present the accompanying unstandardized logistic regression
</p>
<p>coefficients. Though the unstandardized logistic regression coefficient for
</p>
<p>the South (0.805) seems very large relative to that for number of sworn
</p>
<p>Equation 18.7:
</p>
<p>Equation 18.7
</p>
<p>In Table 18.9, we calculate Beta for all four of the coefficients in the
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N
</p>
<p>sion coefficients for linear regression models, the Beta calculated for log-
</p>
<p>582</p>
<p/>
</div>
<div class="page"><p/>
<p>officers (0.002), the reverse relationship is found when we look at the
</p>
<p>standardized coefficients. The standardized coefficient for number of
</p>
<p>sworn officers is 0.662, while that for the South is 0.390. This is consis-
</p>
<p>tent with our analysis of the probability estimates. Both of these methods
</p>
<p>take into account the fact that the scales of measurement for these mea-
</p>
<p>sures differ widely. While you should use caution in relying on standard-
</p>
<p>coefficients within regression models.10
</p>
<p>E v a l u a t i n g  t h e  L o g i s t i c  R e g r e s s i o n  M o d e l
</p>
<p>In OLS regression, to assess how well our model explains the data, we
</p>
<p>use a straightforward measure of the percent of variance explained be-
2
</p>
<p>Nonetheless, a number of measures have been proposed for assessing
</p>
<p>how well a model predicts the data.
</p>
<p>Percent of Correct Predictions
</p>
<p>One widely accepted method for assessing how well a logistic regression
</p>
<p>model predicts the dependent variable is to compare the values of Y pre-
</p>
<p>dicted by the model to those that would be obtained simply by taking
</p>
<p>the observed distribution of the dependent variable. This statistic is com-
</p>
<p>Beta and Associated Logistic Regression 
Coefficients for the Compstat Model
</p>
<p>STANDARD
</p>
<p>VARIABLE DEVIATION b BETA
</p>
<p>Number of sworn officers 330.797 0.002 (330.797)(0.002) � 0.6616
</p>
<p>Northeast 0.416 0.359 (0.416)(0.359) � 0.1493
</p>
<p>South 0.484 0.805 (0.484)(0.805) � 0.3896
</p>
<p>West 0.421 0.428 (0.421)(0.428) � 0.1802
</p>
<p>10As with standardized regression coefficients in OLS regression, you should not com-
</p>
<p>pare standardized logistic regression coefficients across models. Moreover, while we
</p>
<p>report standardized regression coefficients for the dummy variables included in the
</p>
<p>model, you should use caution in interpreting standardized coefficients for dummy
</p>
<p>Table 18.9
</p>
<p>provide a general yardstick for comparing the relative strength of
</p>
<p>ized logistic regression coefficients, here, as in other cases, they can 
</p>
<p>). There is no equivalent statistic in logistic regression.yond the mean (R
</p>
<p>monly described as the percent of correct predictions. Table 18.10
</p>
<p>E V A L U A T I N G  T H E  L O G I S T I C  R E G R E S S I O N  M O D E L
</p>
<p>variables. See Chapter 16, pages 493&ndash;494, for a discussion of this problem.
</p>
<p>583</p>
<p/>
</div>
<div class="page"><p/>
<p>shows the percent of correct predictions for our Compstat example. The
</p>
<p>Percent of correct predictions �
</p>
<p>The observed predictions in our example represent the observed pro-
</p>
<p>portion of departments that report having a Compstat-like program. As
</p>
<p>we noted before, this number is about 0.33 (or 33%). We add the 106
</p>
<p>vide this number by the total number of cases in the analysis (N � 419).
</p>
<p>order to compare these predicted values with the observed values, we
</p>
<p>must assign each case a 0 or a 1. The decision as to whether to define
</p>
<p>the predicted value as a 1 or a 0 is based on a set cut-off point. Herein
</p>
<p>lies the main drawback of this approach: The point at which you deter-
</p>
<p>mine that the prediction is a 1 is arbitrary. In SPSS, as in other standard
</p>
<p>software packages, 0.50 is used as a natural cut-off point. That is, if we
</p>
<p>get a predicted probability of 0.50 or greater for a case in our study, it
</p>
<p>will be counted as a prediction of 1. Remember that a 1 in our case
</p>
<p>means that the department has a Compstat-like program. In this analysis,
</p>
<p>if the prediction is 0.495, the case is given a 0. Clearly, by using a single
</p>
<p>and arbitrary cut-off point, we are losing a good deal of information
</p>
<p>about how well the model fits the data.
</p>
<p>The proportion of correct predictions is worked out below, using
</p>
<p>for which the actual and predicted values are the same. In 264 cases, the
</p>
<p>�Ncorrrect predictionsNtotal � � 100
</p>
<p>Percent of Correct Predictions 
for the Logistic Regression of the Compstat Data
</p>
<p>Classification Table
</p>
<p>Predicted
</p>
<p>COMPSTAT Percentage
Correct
</p>
<p>Observed .00 1.00
</p>
<p>Step 1 COMPSTAT .00 264 19 93.3
</p>
<p>1.00 106 30 22.1
</p>
<p>Overall 70.2
</p>
<p>a The cut value is .500.
</p>
<p>Table 18.10
</p>
<p>formula for percent of correct predictions is presented in Equation 18.8.
</p>
<p>Equation 18.8
</p>
<p>and 30 cases in Table 18.7 where the observed value is 1 and then di-
</p>
<p>The predicted values are drawn from Equation 18.3. But, importantly, in
</p>
<p>Equation 18.8. The N of correct predictions is found by taking each case
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N584</p>
<p/>
</div>
<div class="page"><p/>
<p>actual and predicted values are 0. In only 30 cases are the actual and
</p>
<p>predicted values equal to 1. These are the correct predictions in our
</p>
<p>analysis, so the total number of correct predictions is 264 � 30 � 294.
</p>
<p>The percent of correct predictions is 70.2. This seems like a very high
</p>
<p>level of prediction. However, to interpret this statistic, we must compare
</p>
<p>it with the level we would have reached if we had not used our regres-
</p>
<p>sion model. In that case, we would have had information only on the
</p>
<p>split in the dependent variable. As noted earlier, 33% of the departments
</p>
<p>claim to have implemented a Compstat-like program. Knowing only this,
</p>
<p>our best bet would have been to predict that every department did not
</p>
<p>have a Compstat-like program. If we did this, we would be correct about
</p>
<p>67% of the time. Thus, our model did not improve our prediction very
</p>
<p>much over what we would have predicted with knowledge of only the
</p>
<p>outcomes of the dependent variable.
</p>
<p>W orking It Out
</p>
<p> � 70.17
</p>
<p> � (0.7017) � 100
</p>
<p> � �294419� � 100
 Percent of correct predictions � �Ncorrect predictionsNtotal � � 100
</p>
<p>Pseudo R 2
</p>
<p>While there is no direct R2 measure for logistic regression, a number of
</p>
<p>what may be termed pseudo R2 measures have been proposed. Like stan-
</p>
<p>dardized logistic regression coefficients, these measures are not well ac-
</p>
<p>cepted and must be used with caution. Nonetheless, by providing a general
</p>
<p>sense of the prediction level of a model, they can add information to other
</p>
<p>statistics, such as the percent of correct predictions. A commonly used
</p>
<p>pseudo R2 measure is Cox and Snell&rsquo;s R2.11 As with other pseudo R2 sta-
</p>
<p>tistics, a main component of this measure is the log likelihood function
</p>
<p>(�2LL). It makes good sense to rely on the log likelihood function, since it
</p>
<p>measures the degree to which a proposed model predicts the data exam-
</p>
<p>ined. In this case, we compare the difference between the �2LL estimate
</p>
<p>11D. R. Cox and E. J. Snell, The Analysis of Binary Data, 2nd ed. (London: Chapman
</p>
<p>and Hall, 1989).
</p>
<p>585E V A L U A T I N G  T H E  L O G I S T I C  R E G R E S S I O N  M O D E L</p>
<p/>
</div>
<div class="page"><p/>
<p>obtained when no independent variables are included (the null model) and
</p>
<p>the �2LL estimate obtained when all the independent variables are in-
</p>
<p>cluded (the full model). The �2LL value for the null model (528.171) is
</p>
<p>vides the method of calculation for Cox and Snell&rsquo;s R2.
</p>
<p>While this equation looks intimidating, it can be solved in two easy
</p>
<p>steps. First, we calculate the number that appears above e, or the expo-
</p>
<p>nent of the natural log:
</p>
<p>R 2 � 1 � e�[(�2LLnull model) � (�2LLfull model)]/N
</p>
<p>Pseudo R2 Statistics as Reported in SPSS
</p>
<p>Model Summary
</p>
<p>Step �2 Log Cox &amp; Snell R Nagelkerke R
likelihood Square Square
</p>
<p>1 492.513 .082 .114
</p>
<p>W orking It Out Step 1
</p>
<p>  � �0.085
</p>
<p>  � �35.658/419
</p>
<p>  � �[(528.171) � (492.513)]/419
</p>
<p>�[(�2LLnull model) � (�2LLfull model)]/N
</p>
<p>We then take e to the power of �0.085, which, as we noted earlier, can
</p>
<p>be done simply on a basic scientific calculator. We next subtract this
</p>
<p>number from 1:
</p>
<p>W orking It Out Step 2
</p>
<p> � 0.0816
</p>
<p> � 1 � 0.9185
</p>
<p> Cox and Snell&rsquo;s R 2 � 1 � e�0.085
</p>
<p>Table 18.11
</p>
<p>given in Table 18.4. The �2LL value for the full model (492.513) is given in
</p>
<p>the model summary statistics provided in Table 18.11. Equation 18.9 pro-
</p>
<p>Equation 18.9
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N586</p>
<p/>
</div>
<div class="page"><p/>
<p>Rounding 0.0816 to three decimal places gives a result of 0.082, which is
</p>
<p>identical to that produced in the SPSS printout.
</p>
<p>Like the percent of correct predictions, Cox and Snells R 2 suggests
</p>
<p>that our model does not provide for a very strong level of prediction.
</p>
<p>SPSS produces another R 2 statistic: the Nagelkerke R 2. This statistic cor-
</p>
<p>rects for the fact that Cox and Snell&rsquo;s estimate, as well as many other
</p>
<p>pseudo R 2 statistics, often have a maximum value of less than 1 (which
</p>
<p>2 2
</p>
<p>12 Other pseudo 
</p>
<p>R 2
</p>
<p>these values should be seen as an exact representation of the percent of
</p>
<p>variance explained in your model. But they can give you a rough sense
</p>
<p>of how well your model predicts the outcome measure.
</p>
<p>S t a t i s t i c a l  S i g n i f i c a n c e  i n  L o g i s t i c  R e g r e s s i o n
</p>
<p>Statistical significance for a logistic regression can be interpreted in much
</p>
<p>the same way as it was for the regression models discussed in Chapters
</p>
<p>15 and 16. However, a chi-square distribution is used, and thus we do
</p>
<p>not have to be concerned with assumptions regarding the population
</p>
<p>distribution in our tests. For the overall model, there is a general test,
</p>
<p>based on the difference between the �2LL statistics for the full and null
</p>
<p>models. The chi-square formula for the overall model in logistic regres-
</p>
<p>Model chi-square � (�2LLnull model) � (�2LLfull model)
</p>
<p>next page). The number of degrees of freedom is determined by the
</p>
<p>number of independent variables included in the model estimated. In
</p>
<p>our case, there are three regression coefficients for the variable region
</p>
<p>and the measure number of sworn officers. The number of degrees of
</p>
<p>freedom thus equals 4. Looking at Appendix 2, we can see that a chi-
</p>
<p>square statistic of greater than 18.465 is needed for a statistically signifi-
</p>
<p>cant result at the 0.001 level. Because our chi-square statistic is much
</p>
<p>larger than this, our observed significance level is less than 0.001. Using
</p>
<p>12See N. J. D. Nagelkerke, &ldquo;A Note on a General Definition of the Coefficient of Deter-
</p>
<p>mination, Biometrika 78 (1991): 691&ndash;692.
</p>
<p>erke&rsquo;s R is thus generally larger than Cox and Snell&rsquo;s R , which&mdash;
</p>
<p>especially with large values&mdash;will be too conservative.
</p>
<p>explained by the independent variables included in the model). Nagelk-
</p>
<p>would indicate that all of the variance in the dependent variable was 
</p>
<p>statistics will give estimates similar to those produced here. None of
</p>
<p>sion is represented in Equation 18.10.
</p>
<p>Equation 18.10
</p>
<p>S T A T I S T I C A L  S I G N I F I C A N C E  I N  L O G I S T I C  R E G R E S S I O N  587
</p>
<p>For our example, the model chi-square is 35.658 (see working it out,
</p>
<p>conventional significance criteria, we would reject the null hypothesis</p>
<p/>
</div>
<div class="page"><p/>
<p>and conclude that the model estimated provides significant improvement
</p>
<p>over that without any independent variables.
</p>
<p>W orking It Out
</p>
<p> � 35.658
</p>
<p> � 528.171 � 492.513
</p>
<p> Model chi-square � (�2LLnull model) � (�2LLfull model)
</p>
<p>In testing the statistical significance of individual parameters, statistical
</p>
<p>This statistic
</p>
<p>also has a chi-square distribution, and so the statistical significance of a
</p>
<p>result may be checked in a chi-square table. The Wald statistic takes the
</p>
<p>ratio of the logistic regression coefficient to its standard error (see Equa-
</p>
<p>the South and North Central regions (the latter being the excluded cate-
</p>
<p>gory), we take the logistic regression coefficient of 0.805 and divide it by
</p>
<p>the reported standard error of 0.332. To get the Wald statistic, we square
</p>
<p>this number. The result is 5.879.
</p>
<p>W 2 � � bSEb�
2
</p>
<p>13
</p>
<p>W orking It Out South Region Measure
</p>
<p> � 5.879
</p>
<p> � �0.8050.332�
2
</p>
<p> W 2 � � bSEb�
2
</p>
<p>tion 18.11). The standard error of the logistic regression coefficient is
</p>
<p>provided in the SPSS printout (see Table 18.12). For the comparison of
</p>
<p>Equation 18.11
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N
</p>
<p>13software packages ordinarily provide the Wald statistic.
</p>
<p>14
</p>
<p>error.
</p>
<p>We discuss the Wald statistic in detail here, because it is the most common test of 
</p>
<p>14
</p>
<p>chapter and in more detail in Chapter 19 offers an alternative test for statistical significance 
</p>
<p>for Categorical and Limited Dependent Variables (Thousand Oaks, CA, Sage, 1997).
that is appropriate to both small and large samples (see J. Scott Long, Regression Models 
</p>
<p>The difference between our result and that shown in Table 18.12 is due to rounding
</p>
<p>statistical significance reported in many statistical software applications. it should be noted,
</p>
<p>however, that some researchers have noted that the Wald statistic is sensitive to small
sample sizes (e.g., less than 100 cases). The likelihood-ratio test discussed later in this
</p>
<p>588</p>
<p/>
</div>
<div class="page"><p/>
<p>To determine whether this coefficient is statistically significant, we can
</p>
<p>refer to the chi-square table for 1 degree of freedom. The number of de-
</p>
<p>grees of freedom for an individual variable in a logistic regression will al-
</p>
<p>ways be 1. Looking at Appendix 2, we see that a chi-square of 10.827 is
</p>
<p>required for a result to be statistically significant at the 0.001 level. A chi-
</p>
<p>square of 6.635 is required at the 0.01 level, and a chi-square of 5.412 at
</p>
<p>the 0.02 level. Our observed significance level can therefore be defined
</p>
<p>as falling between 0.01 and 0.02. The SPSS printout gives the exact ob-
</p>
<p>served significance level as 0.015. Using conventional levels of statistical
</p>
<p>significance, we would conclude that we can reject the null hypothesis
</p>
<p>that there is no difference in the reported implementation of Compstat-
</p>
<p>like programs in the South versus the North Central region.
</p>
<p>that the number of sworn officers is also statistically significant&mdash;in this
</p>
<p>case, at greater than the 0.001 level. It is important to note that the statistics
</p>
<p>reported in this table, as well as in most statistical software, are for two-
</p>
<p>tailed significance tests. We mentioned at the outset that there was a strong
</p>
<p>hypothesis that larger departments are more likely to report a Compstat-like
</p>
<p>program. If we wanted to use a directional test of statistical significance, we
</p>
<p>would simply divide the observed significance level in our test by 2.
</p>
<p>Looking at the other region dummy variables, we can see that there is
</p>
<p>not a statistically significant difference between the Northeast and North
</p>
<p>Central regions or between the West and North Central regions. But, as
</p>
<p>noted in Chapter 16, it is important to ask whether the variable region
</p>
<p>overall contributes significantly to the regression. To test this hypothesis,
</p>
<p>we can conduct a likelihood ratio chi-square test, which compares the
</p>
<p>SPSS Printout with B, SE of B, and Wald Statistics
</p>
<p>Variables in the equation
</p>
<p>B S.E. Wald df Sig. Exp(B)
</p>
<p>Step 1 NORTHEAST .359 .372 .931 1 .335 1.432
</p>
<p>SOUTH .805 .332 5.883 1 .015 2.237
</p>
<p>WEST .428 .367 1.360 1 .244 1.534
</p>
<p>#SWORN .002 .000 24.842 1 .000 1.002
</p>
<p>Constant �1.795 .311 33.378 1 .000 .166
</p>
<p>a Variable(s) entered on step 1: NORTHEAST, SOUTH, WEST, #SWORN.
</p>
<p>Looking at the significance statistics column in Table 18.12, we can see
</p>
<p>This printout is identical to that in Table 18.5. It is reproduced here for easy reference
</p>
<p>as you work through the computations presented in this section.
</p>
<p>Table 18.12
</p>
<p>589
</p>
<p>15
</p>
<p>15
</p>
<p>S T A T I S T I C A L  S I G N I F I C A N C E  I N  L O G I S T I C  R E G R E S S I O N  </p>
<p/>
</div>
<div class="page"><p/>
<p>log likelihood function of the model with the multicategory nominal vari-
</p>
<p>able (the full model) with the log likelihood function of the model without
</p>
<p>details the likelihood ratio chi-square test. The number of degrees of free-
</p>
<p>dom is defined as the number of dummy variables added by the multicate-
</p>
<p>gory nominal variable. In our case, it is 3 for the three included regions.
</p>
<p>Likelihood ratio chi-square test � (�2LLreduced model) � (�2LLfull model)
</p>
<p>We can get the statistics for the test by running two separate regres-
</p>
<p>sions. The reduced model regression excludes the dummy variables as-
</p>
<p>sociated with region. The �2LL for this model is shown in the model
</p>
<p>the full model&mdash;the model we have been using throughout the chapter,
</p>
<p>with the region dummy variables included. The model statistics were re-
</p>
<p>Below, we work out the likelihood ratio chi-square using these two
</p>
<p>estimates. The likelihood ratio chi-square for the region variable is 6.934,
</p>
<p>Compstat-like program at the 0.05 significance threshold. Because our
</p>
<p>chi-square statistic is smaller than this number, we cannot conclude that
</p>
<p>there is overall a statistically significant relationship between region and
</p>
<p>claimed development of a Compstat-like program.
</p>
<p>W orking It Out
</p>
<p> � 6.934
</p>
<p> � 499.447 � 492.513
</p>
<p> Likelihood ratio chi-square test � (�2LLreduced model) � (�2LLfull model)
</p>
<p>Model Summary for the Reduced Model
</p>
<p>Model Summary
</p>
<p>Step �2 Log Cox &amp; Snell R Nagelkerke R
likelihood Square Square
</p>
<p>1 499.447 .066 .092
</p>
<p>Table 18.13
</p>
<p>the multicategory nominal variable (the reduced model). Equation 18.12
</p>
<p>Equation 18.12
</p>
<p>summary from an SPSS printout in Table 18.13. The second regression is
</p>
<p>ported in Table 18.11.
</p>
<p>with 3 degrees of freedom (the number of dummy variable measures 
</p>
<p>included in the model). Looking at Appendix 2, we can see that with 3 
</p>
<p>degrees of freedom, a chi-square of 7.815 would be needed to reject 
</p>
<p>the null hypothesis of no relationship between region and a reported
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N590</p>
<p/>
</div>
<div class="page"><p/>
<p>C h a p t e r  S u m m a r y
</p>
<p>Ordinary least squares regression is not an appropriate tool for analyzing
</p>
<p>a problem in which the dependent variable is dichotomous. In such
</p>
<p>cases, OLS regression is likely to predict values that are greater than 1
</p>
<p>and less than 0 and thus outside the observed distribution of Y. Using
</p>
<p>the OLS approach in this case will also lead to violations of parametric
</p>
<p>assumptions required for associated statistical tests. Logistic regression
</p>
<p>analysis uses a logistic model curve, rather than a straight line, to pre-
</p>
<p>dict outcomes for Y in the case of a dichotomous dependent variable.
</p>
<p>This constrains predictions to between 0 and 1.
</p>
<p>While the logistic model curve provides a solution to predictions be-
</p>
<p>yond the observed distribution, the outcome variable is transformed into
</p>
<p>the natural logarithm of the odds of Y, or the logit of Y. Through use
</p>
<p>of the cumulative logistic probability function, the logistic regression
</p>
<p>equation may be used to predict the likelihood of Y occurring. Maxi-
</p>
<p>mum likelihood techniques are used to estimate the coefficients in a
</p>
<p>logistic regression analysis. In this approach, we begin by identifying a
</p>
<p>tentative solution, which we then try to improve upon. Our criterion for
</p>
<p>improvement is termed the log likelihood function (�2LL). We repeat
</p>
<p>this process again and again until the change in the likelihood function
</p>
<p>is considered negligible. Each time we repeat the process and reestimate
</p>
<p>our coefficients is called an iteration. Lack of convergence in a stan-
</p>
<p>dard number of iterations indicates some type of problem in the regres-
</p>
<p>sion model that is being estimated.
</p>
<p>The multivariate logistic regression coefficient, b, may be inter-
</p>
<p>preted as the increase in the log of the odds of Y associated with a one-
</p>
<p>unit increase in X (with all other independent variables in the model
</p>
<p>held constant). The odds ratio, or Exp(B), and the derivative at
</p>
<p>mean, DM, provide more easily interpreted representations of the logis-
</p>
<p>change in Y associated with a unit change in X. The DM will change de-
</p>
<p>pending on the mean value of Y in the problem examined.
</p>
<p>There is no widely accepted method for comparing logistic regression
</p>
<p>coefficients measured on different scales. One method is to compare
</p>
<p>probability estimates at selected intervals. Standardized regression coeffi-
</p>
<p>cients have been suggested for logistic regression, though they should be
</p>
<p>interpreted with caution. There is no single widely accepted statistic for
</p>
<p>assessing how well the logistic regression model predicts the observed
</p>
<p>data. An approach commonly used is to calculate the percent of cor-
</p>
<p>rect predictions. This method establishes an arbitrary decision point
</p>
<p>regression coefficient, the derivative at mean may be interpreted as the
</p>
<p>tic regression coefficient. The odds ratio represents the impact of a one-
</p>
<p>unit change in X on the ratio of the probability of Y. Like an ordinary
</p>
<p>C H A P T E R  S U M M A R Y 591</p>
<p/>
</div>
<div class="page"><p/>
<p>(usually 0.50) for deciding when a predicted value should be set at 1.
</p>
<p>These predictions are then compared to the observed data. Pseudo R2
</p>
<p>statistics have also been developed, though they remain a subject of
</p>
<p>debate.
</p>
<p>Statistical significance for the overall logistic regression model is as-
</p>
<p>sessed through computation of the model chi-square. Statistical signifi-
</p>
<p>cance for individual regression coefficients is evaluated with the Wald
</p>
<p>statistic. A likelihood ratio chi-square test can be used to calculate
</p>
<p>the statistical significance of a multicategory nominal variable.
</p>
<p>K e y  T e r m s
</p>
<p>Cox and Snell&rsquo;s R2 A commonly used
</p>
<p>pseudo R 2 measure whose main compo-
</p>
<p>nent, as in other pseudo R2 statistics, is the
</p>
<p>log likelihood function (�2LL).
</p>
<p>cumulative logistic probability func-
</p>
<p>tion A transformation of the logistic
</p>
<p>probability function that allows
</p>
<p>computation of the probability that 
</p>
<p>Y will occur, given a certain combination
</p>
<p>of characteristics of the independent
</p>
<p>variables.
</p>
<p>derivative at mean (DM) A measure that
</p>
<p>converts the nonlinear logistic regression
</p>
<p>coefficient to a simple linear regression co-
</p>
<p>efficient, which may be interpreted as the
</p>
<p>change in Y associated with a unit change
</p>
<p>in X.
</p>
<p>iteration Each time we identify another
</p>
<p>tentative solution and reestimate our logis-
</p>
<p>tic regression coefficients.
</p>
<p>lack of convergence Failure of a logistic
</p>
<p>regression analysis to reach a result that
</p>
<p>meets the criterion of reduction in the log
</p>
<p>likelihood function.
</p>
<p>likelihood ratio chi-square test A test
</p>
<p>for statistical significance that allows the re-
</p>
<p>searcher to examine whether a subset of
</p>
<p>independent variables in a logistic regres-
</p>
<p>sion is statistically significant. It compares
</p>
<p>�2LL for a full model to �2LL for a re-
</p>
<p>duced model.
</p>
<p>log likelihood function A measure of the
</p>
<p>probability of observing the results in the
</p>
<p>sample, given the coefficient estimates in
</p>
<p>the model. In logistic regression, the log
</p>
<p>likelihood function (�2LL) is defined as �2
</p>
<p>times the natural logarithm of the likeli-
</p>
<p>hood function.
</p>
<p>logarithm The power to which a fixed
</p>
<p>number (the base) must be raised to pro-
</p>
<p>duce another number.
</p>
<p>logistic model curve The form of the pre-
</p>
<p>dicted outcomes of a logistic regression
</p>
<p>analysis. Shaped like an S, the logistic
</p>
<p>curve begins to flatten as it approaches 
</p>
<p>never actually reaches&mdash;either of these two
</p>
<p>values.
</p>
<p>logistic regression analysis A type of re-
</p>
<p>gression analysis that allows the researcher
</p>
<p>to make predictions about dichotomous de-
</p>
<p>pendent variables in terms of the log of the
</p>
<p>odds of Y.
</p>
<p>logistic regression coefficient The coef-
</p>
<p>ficient b produced in a logistic regression
</p>
<p>0 or 1, so it keeps coming closer to&mdash;but
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N
</p>
<p>analysis. It may be interpreted as the 
</p>
<p>592</p>
<p/>
</div>
<div class="page"><p/>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>e Base of the natural logarithm
</p>
<p>ln Natural logarithm
</p>
<p>�2LL �2 times the log likelihood function
</p>
<p>The natural logarithm of the odds of P(Y � 1) to P(Y � 0):
</p>
<p>ln � P(Y � 1)1 � P(Y � 1)� � ln �P(Y � 1)P(Y � 0)� � b0 � b1X1
</p>
<p>ated with a one-unit increase in X.
</p>
<p>maximum likelihood estimation A tech-
</p>
<p>nique for estimating the parameters or co-
</p>
<p>efficients of a model that maximizes the
</p>
<p>probability that the estimates obtained will
</p>
<p>produce a distribution similar to that of the
</p>
<p>observed data.
</p>
<p>model chi-square The statistical test used
</p>
<p>to assess the statistical significance of the
</p>
<p>overall logistic regression model. It com-
</p>
<p>pares the �2LL for the full model with the
</p>
<p>�2LL calculated without any independent
</p>
<p>variables included.
</p>
<p>Nagelkerke R2 A pseudo R 2 statistic that
</p>
<p>corrects for the fact that Cox and Snell&rsquo;s es-
</p>
<p>timates, as well as many other pseudo R 2
</p>
<p>statistics, often have a maximum value of
</p>
<p>less than 1.
</p>
<p>natural logarithm of the odds of Y (logit
</p>
<p>of Y) The outcome predicted in a logistic
</p>
<p>regression analysis.
</p>
<p>odds ratio [Exp(B)] A statistic used to in-
</p>
<p>terpret the logistic regression coefficient. It
</p>
<p>represents the impact of a one-unit change
</p>
<p>in X on the ratio of the probability of Y.
</p>
<p>percent of correct predictions A 
</p>
<p>statistic used to assess how well a logistic
</p>
<p>regression model explains the observed
</p>
<p>data. An arbitrary decision point (usually
</p>
<p>0.50) is established for deciding when a
</p>
<p>predicted value should be set at 1, and
</p>
<p>then the predictions are compared to the
</p>
<p>observed data.
</p>
<p>pseudo R2 The term generally used for a
</p>
<p>group of measures used in logistic regres-
</p>
<p>sion to create an approximation of the OLS
</p>
<p>regression R 2. They are generally based on
</p>
<p>comparisons of �2LL for a full model and
</p>
<p>a null model (without any independent
</p>
<p>variables).
</p>
<p>standardized logistic regression
</p>
<p>coefficient A statistic used to compare
</p>
<p>logistic regression coefficients that use
</p>
<p>different scales of measurement. It is 
</p>
<p>meant to approximate Beta, the
</p>
<p>standardized regression coefficient in 
</p>
<p>OLS regression.
</p>
<p>Wald statistic A statistic used to assess the
</p>
<p>statistical significance of coefficients in a 
</p>
<p>logistic regression model.
</p>
<p>S Y M B O L S  A N D  F O R M U L A S 593
</p>
<p>change in the log of the odds of Y associ-</p>
<p/>
</div>
<div class="page"><p/>
<p>To calculate the probability that Y � 1:
</p>
<p>To calculate the odds ratio for P(Y � 1), given a one-unit change in the
</p>
<p>independent variable X:
</p>
<p>� Exp(B)
</p>
<p>To calculate the derivative at mean:
</p>
<p>To calculate the standardized logistic regression coefficient:
</p>
<p>Betai = bisi
</p>
<p>To calculate the percent of correct predictions:
</p>
<p>Percent correct �
</p>
<p>To calculate Cox and Snell&rsquo;s R 2:
</p>
<p>To calculate the model chi-square:
</p>
<p>Model chi-square � (�2LLnull model) � (�2LLfull model)
</p>
<p>To calculate the Wald statistic:
</p>
<p>To calculate the likelihood ratio chi-square statistic for a subset of
</p>
<p>independent variables:
</p>
<p>Likelihood ratio chi-square test � (�2LLreduced model) � (�2LLfull model)
</p>
<p> W 2 � � bSEb�
2
</p>
<p>R 2 � 1 � e�[(�2LLnull model) � (�2LLfull model)]/N
</p>
<p>�Ncorrrect predictionsNtotal � � 100
</p>
<p>DM � Y (1 � Y )bi
</p>
<p>Odds ratio � 
	 P(Y � 1)1 � P(Y � 1)
X
	 P(Y � 1)1 � P(Y � 1)
X�1
</p>
<p>P(Y � 1) � 
1
</p>
<p>1 � e�Xb
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N594</p>
<p/>
</div>
<div class="page"><p/>
<p>E x e r c i s e s
</p>
<p>As part of a research project for a class, a student analyzed data on a
sample of adults who had been asked about their decision to report
being assaulted to the police. Their decision was coded as 1 � assault
</p>
<p>age (in years), sex (0 � male, 1 � female), and race (0 � white, 
1 � nonwhite). The student reported the regression results as
</p>
<p>Variable b
</p>
<p>Age 0.01
</p>
<p>Sex 0.5
</p>
<p>Race �0.2
</p>
<p>Constant �0.1
</p>
<p>a. Calculate the predicted values for each of the following persons:
</p>
<p>&mdash; A 65-year-old white female
</p>
<p>&mdash; A 25-year-old nonwhite male
</p>
<p>&mdash; A 40-year-old white male
</p>
<p>&mdash; A 30-year-old nonwhite female
</p>
<p>b. Should any of the student&rsquo;s predicted values lead the student to
question the use of ordinary least squares regression? Explain why.
</p>
<p>A research institute concerned with raising public attention about the use
of force by school children calculates the following effects on the likeli-
hood of hitting another child at school, using logistic regression analysis:
</p>
<p>Variable b
</p>
<p>Sex (0 � girl, 1 � boy) 0.7
</p>
<p>Grade in school �0.1
</p>
<p>Constant �0.4
</p>
<p>Hitting another child was coded as 1; no hitting was coded as 0.
</p>
<p>a. Interpret the effects of sex and grade in school on the log of the
odds that P(Y � 1).
</p>
<p>b. Calculate and interpret the odds ratios for the effects of sex and
grade in school on use of force.
</p>
<p>Supervision of defendants on pretrial release is thought to reduce the
chance that defendants will flee the community. A government agency
funds a small study to examine whether supervision affects pretrial
</p>
<p>E X E R C I S E S
</p>
<p>student used ordinary least squares regression to estimate the effects of
reported to the police, 0 � assault not reported to the police. The 
</p>
<p>595
</p>
<p>18.1
</p>
<p>18.2
</p>
<p>18.3</p>
<p/>
</div>
<div class="page"><p/>
<p>flight (flight � 1, no flight � 0) and reports the following logistic
regression results:
</p>
<p>Variable b Standard Error of b
</p>
<p>Age (years) �0.01 0.02
</p>
<p>Sex ( 1 � male, 0.67 0.25
</p>
<p>0 � female)
</p>
<p>Severity of offense 0.21 0.03
</p>
<p>scale (0 to 10)
</p>
<p>Number of prior 0.35 0.09
</p>
<p>felony convictions
</p>
<p>Number of contacts with �0.13 0.03
</p>
<p>supervision caseworker
</p>
<p>Constant �0.52
</p>
<p>a. Calculate and interpret the odds ratio for each of the independent
variables.
</p>
<p>b. Can the government agency conclude that supervision in the form
of contact with a caseworker has a statistically significant effect on
pretrial flight? Explain why.
</p>
<p>c. If the agency reports that the �2LLnull model is 653.2 and the 
�2LLfull model is 597.6, can it conclude that the model is statistically
significant? Explain why.
</p>
<p>A survey of adolescents indicated that 17% had used marijuana in the
last year. In addition to standard demographic predictors of drug use,
a researcher expects that school performance also affects the likeli-
hood of marijuana use. The researcher&rsquo;s table of results follows.
</p>
<p>Standard Standard 
</p>
<p>Variable Mean Deviation b Error of b
</p>
<p>Age (years) 14.6 3.1 0.07 0.03
</p>
<p>Sex (1 � male, 0 � female) 0.55 0.50 0.36 0.15
</p>
<p>Race (1 � white, 0 � nonwhite) 0.75 0.43 �0.42 0.30
</p>
<p>Grade point average 2.76 1.98 �0.89 0.24
</p>
<p>Think of self as a good 0.59 0.49 �0.65 0.33
</p>
<p>student (1 � yes, 0 � no)
</p>
<p>Constant �0.87
</p>
<p>a. Calculate the predicted probability of marijuana use for each of the
following persons:
</p>
<p>&mdash; A 14-year-old white male who does not think of himself as a
good student and has a GPA of 3.07.
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N
</p>
<p>18.4
</p>
<p>596</p>
<p/>
</div>
<div class="page"><p/>
<p>&mdash; A 17-year-old nonwhite female who thinks of herself as a good
student and has a GPA of 3.22.
</p>
<p>&mdash; A 15-year-old white female who thinks of herself as a good stu-
dent and has a GPA of 2.53.
</p>
<p>b. Calculate the standardized coefficient for each of the independent
variables in the model. Which variable appears to have the largest
effect on marijuana use?
</p>
<p>c. Calculate the derivative at mean for each of the independent vari-
</p>
<p>d. Compare your answers for parts b and c. How do you explain this
pattern?
</p>
<p>After losing a court battle over a requirement that it reduce its jail
population, a county conducted an analysis to predict which offenders
would pose the greatest threat of committing a violent offense if re-
leased early. A random sample of 500 inmates released from the jail in
the last three years was analyzed to see what factors predicted arrest
for a violent crime in the 12 months after release. For the final model,
which included five predictors of violent arrest, the county reported
the following statistics:
</p>
<p>Predicted Predicted 
</p>
<p>No Violent Arrest Violent Arrest
</p>
<p>Observed No Violent Arrest 439 19
</p>
<p>Observed Violent Arrest 27 15
</p>
<p>a. Calculate the percent correctly predicted for this model. What does
this statistic indicate about the county&rsquo;s prediction model?
</p>
<p>b. Calculate the model chi-square for this model. Interpret this
statistic.
</p>
<p>c. Calculate Cox and Snell&rsquo;s R 2 for this model. Interpret this statistic.
</p>
<p>d. How do you explain the difference in the results for parts a
through c?
</p>
<p>Hopeful that media attention to wrongful convictions has increased
public opinion in favor of abolishing the death penalty, an abolitionist
organization conducts a study to assess public support for abolishing
the death penalty. Overall, the organization finds that 35% would sup-
port abolishing the death penalty if offenders could be sentenced to
life without the option of parole (coded as 1 � abolish the death
penalty, 0 � do not abolish the death penalty). In a logistic regression
</p>
<p> �2LLfull model � 861.3
</p>
<p> �2LLnull model � 876.5
</p>
<p>effect on marijuana use?
ables in the model. Which variable appears to have the largest 
</p>
<p>597
</p>
<p>18.5
</p>
<p>18.6
</p>
<p>E X E R C I S E S</p>
<p/>
</div>
<div class="page"><p/>
<p>model examining the effects of respondent characteristics on support,
the organization finds the following:
</p>
<p>Standard Standard 
</p>
<p>Variable Mean Deviation b Error of b
</p>
<p>Age (years) 41.2 15.4 �0.01 0.01
</p>
<p>Sex (1 � male, 0 � female) 0.44 0.50 �0.42 0.19
</p>
<p>Race (1 � white, 0 � nonwhite) 0.76 0.43 �0.24 0.09
</p>
<p>Political conservative 0.33 0.47 �1.12 0.22
</p>
<p>(1 � yes, 0 � no)
</p>
<p>Region of Country:
</p>
<p>South 0.23 0.42 �0.19 0.11
</p>
<p>West 0.31 0.46 �0.09 0.04
</p>
<p>North 0.27 0.44 0.27 0.12
</p>
<p>(omitted � Central)
</p>
<p>Constant 0.11
</p>
<p>a. Which variable has a relatively greater impact on support for abol-
ishing the death penalty? Explain why.
</p>
<p>b. If �2LLreduced model � 376.19 and �2LLfull model � 364.72 when region
variables are omitted from the analysis, do the region variables
have a statistically significant effect on support for abolishing the
death penalty?
</p>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>In both SPSS and Stata, the general format for many of the multivariate  statistical 
</p>
<p>models is much the same. So, while the models may become increasingly  
</p>
<p>complex, the syntax necessary to run these models is oftentimes very  
</p>
<p>similar. We have included examples in both the SPSS (Chapter_18.sps) and Stata 
</p>
<p>(Chapter_18.do) syntax files.
</p>
<p>SPSS
</p>
<p>Logistic regression analyses are performed in SPSS with the LOGISTIC 
</p>
<p>REGRESSION command:
</p>
<p>The structure to this command is identical to the REGRESSION command 
</p>
<p>discussed in previous chapters. Much of the output from running this command 
</p>
<p>has been discussed in this chapter. The difference between the linear regression 
</p>
<p>command and the logistic regression command is that the output from a logistic 
</p>
<p>regression presents information for the model that includes only the intercept 
</p>
<p>and is labeled &ldquo;Block 0&rdquo; in the SPSS output. The next section of output is 
</p>
<p>598
</p>
<p>LOGISTIC REGRESSION VARIABLES dep_var_name
</p>
<p>/METHOD = ENTER list_of_indep_vars.</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R  E X E R C I S E S 599
</p>
<p>labeled &ldquo;Block 1,&rdquo; and it contains the results discussed above: Omnibus Tests of 
</p>
<p>Model Coefficients, Model Summary, Classification Table, and Variables in the 
</p>
<p>Equation.
</p>
<p>It is possible to have SPSS calculate the predicted probability and residual for 
</p>
<p>each observation in the data file. To obtain one or both of these values, insert 
</p>
<p>the /SAVE PRED RESID line, just as in the linear regression command&mdash;the 
</p>
<p>naming convention in the LOGISTIC REGRESSION is the same as that used 
</p>
<p>in the REGRESSION command.
</p>
<p>Stata
</p>
<p>Logistic regression analyses are performed in Stata with the logit command:
</p>
<p>The output from running the logit command parallels that in Stata&rsquo;s regress 
</p>
<p>command. The model summary results appear at the top of the output, followed 
</p>
<p>by a table that presents the coefficients, their standard errors, z-scores, and  
</p>
<p>confidence intervals (we discuss these in Chapter 20). Note that rather than the 
</p>
<p>Wald statistic for each variable, Stata computes the z-score. To obtain the Wald 
</p>
<p>statistic, simply square the z-score:
</p>
<p>which will result in the same substantive conclusion (i.e., a significant Wald statis-
</p>
<p>tic will also be a significant z-score).
</p>
<p>To obtain the odds ratios, we need to add the option or to the logit  
</p>
<p>command:
</p>
<p>Note that the table of coefficients will report the odds ratios rather than the 
</p>
<p>coefficients interpretable as the log of the odds.
</p>
<p>In the model summary statistics, Stata reports the test for the overall model 
</p>
<p>as &ldquo;LR chi2(#)&rdquo; and what is labeled as &ldquo;Pseudo R2.&rdquo; Recall that in a chi-square 
</p>
<p>test in Stata, the degrees of freedom associated with a test are included within the 
</p>
<p>parentheses following &ldquo;chi2.&rdquo; The &ldquo;Pseudo R2&rdquo; value is the Cox and Snell R2.
</p>
<p>Saving predicted values and residuals following the logit command is  
</p>
<p>identical to saving these same values after running the regress command:
</p>
<p>which will then add two additional variables to the working data file.
</p>
<p>Problems
</p>
<p> 1. Open the Compstat data file (compstat.sav or compstat.dta). These are 
</p>
<p>the data analyzed in this chapter. Use one of  the binary logistic regres-
</p>
<p>sion commands with Compstat as the dependent variable and number of  
</p>
<p>logit dev_var list_of_indep_vars
</p>
<p>logit dev_var list_of_indep_vars, or
</p>
<p>predict PRE_1
</p>
<p>predict RES_1, r
</p>
<p>Wald = z2</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R  E I G H T E E N :  L O G I S T I C  R E G R E S S I O N
</p>
<p>sworn officers, Northeast, South, and West as the independent variables. 
</p>
<p>Note that the values reported in the software output match those reported 
</p>
<p>in the text in Tables 18.4, 18.5, 18.10, and 18.11.
</p>
<p> 2. Open the Pennsylvania Sentencing data file (pcs_98.sav or pcs_98.dta). 
</p>
<p>Use one of  the binary logistic regression commands with incarceration as 
</p>
<p>the dependent variable and age, race, sex, offense severity score, and prior 
</p>
<p>criminal history score as the independent variables.
</p>
<p>b. Explain the odds ratios for each independent variable in plain English.
</p>
<p>c. Interpret the results of  the Wald statistic for each of  the logistic regres-
</p>
<p>d. Interpret the value of  Cox and Snell&rsquo;s R2.
</p>
<p>e. Perform a chi-square test for the overall regression model.
</p>
<p> 3. Open the NYS data file (nys_1.sav, nys_1_student.sav, or nys_1.dta). Using 
</p>
<p>one of  the binary logistic regression commands, run an analysis for each 
</p>
<p>of  the measures of  delinquency below. As in the Computer Exercises in 
</p>
<p>Chapters 16 and 17, you will need to select a set of  independent variables 
</p>
<p>that you think are related to the dependent variable. Note that each of  the 
</p>
<p>delinquency items will need to be recoded as 0 or 1 to represent whether 
</p>
<p>or not the act was committed (see Chapter_18.sps and Chapter_18.do for 
</p>
<p>examples). Do the following for each analysis:
</p>
<p>Explain the odds ratios in plain English.
</p>
<p>Interpret the results of  the Wald statistic for each of  the logistic 
</p>
<p>Interpret the value of  Cox and Snell&rsquo;s R2.
</p>
<p>Perform a chi-square test for the overall regression model.
</p>
<p>a.   Number of  thefts valued at less than $5 in the last year; convert to 
</p>
<p>any thefts in the last year.
</p>
<p>b.  Number of  times drunk in the last year; convert to any times drunk 
</p>
<p>in the last year.
</p>
<p>c.  Number of  times the youth has hit other students in the last year; 
</p>
<p>convert to any times the youth has hit other students in the last year.
</p>
<p>d.  Number of  times the youth has hit a parent in the last year; convert 
</p>
<p>to any times the youth has hit a parent in the last year.
</p>
<p>600</p>
<p/>
</div>
<div class="page"><p/>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_19,  
</p>
<p>C h a p t e r  n i n e t e e n
</p>
<p>Multivariate Regression with Multiple
Category Nominal or Ordinal Measures:
Extending the Basic Logistic Regression
Model
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>H o w  d o  w e  a n a l y z e  a  d e p e n d e n t  v a r i a b l e  w i t h
</p>
<p>m o r e  t h a n  t w o  c a t e g o r i e s ?
</p>
<p>M u l t i n o m i a l  l o g i s t i c  r e g r e s s i o n
</p>
<p>How Do We Interpret the Multinomial Logistic Regression Model?
</p>
<p>Does the Reference Category Make a Difference?
</p>
<p>What is the Test of Statistical Significance for Single Coefficients?
</p>
<p>What is the Test of Statistical Significance for Multiple Coefficients?
</p>
<p>What are the Practical Limits of the Multinomial Logistic Regression
</p>
<p>Model?
</p>
<p>O r d i n a l  l o g i s t i c  r e g r e s s i o n
</p>
<p>How Do We Interpret the Ordinal Logistic Regression Model?
</p>
<p>How Do We Interpret Cumulative Probabilities?
</p>
<p>How are Cumulative Probabilities Related to Odds Ratios?
</p>
<p>How Do We Interpret Ordinal Logistic Coefficients?
</p>
<p>What is the Test of Statistical Significance for Coefficients?
</p>
<p>What are the Parallel Slopes Tests?
</p>
<p>How is the Score Test Computed?
</p>
<p>How is the Brant Test Computed?
</p>
<p>What is the Partial Proportional Odds Model? 
</p>
<p>How Do We Interpret the Results from the Partial Proportional Odds Model?</p>
<p/>
</div>
<div class="page"><p/>
<p>IN THE PREVIOUS CHAPTER, we examined how to analyze data in a binary
logistic regression model that included a dependent variable with two cat-
</p>
<p>egories. This allowed us to overcome problems associated with using
</p>
<p>Ordinary Least Squares Regression in cases where the variable that is
</p>
<p>being explained is measured as a simple dichotomy. Accordingly, we
</p>
<p>have now described tools that allow the researcher to develop explana-
</p>
<p>tory models with either an interval dependent variable or a dichotomous
</p>
<p>dependent variable.
</p>
<p>But there are many situations in which researchers are faced with ana-
</p>
<p>lyzing dependent variables that include more than two categories, or that
</p>
<p>are measured on an ordinal scale. For example, we may want to identify
</p>
<p>the factors that lead to a dismissal, a guilty plea conviction, or a trial con-
</p>
<p>viction in court. The methods we have covered so far do not allow us to
</p>
<p>examine this problem in a single multivariate statistical model. We have
</p>
<p>also not discussed how a researcher should deal with dependent vari-
</p>
<p>ables such as fear of crime, which are measured on an ordinal scale. As
</p>
<p>we noted in Chapter 16, the assumptions of Ordinary Least Squares
</p>
<p>variables.
</p>
<p>regression are not likely to be met when using ordinal scale dependent
</p>
<p>602
</p>
<p>Fortunately, we can extend our discussion of the logistic regression
</p>
<p>model to consider such dependent variables. However, such logistic
</p>
<p>regression models need to be modified to take into account these new sit-
</p>
<p>uations. In this chapter we provide an introduction to multinomial and
</p>
<p>ordinal logistic regression. Though these topics are generally not included 
</p>
<p>in an introductory statistics text, we think the problems that these 
</p>
<p>approaches address are becoming very common in criminal justice 
</p>
<p>research and even for a basic understanding of statistical methods in 
</p>
<p>criminal justice it is important to be able to understand and apply them.</p>
<p/>
</div>
<div class="page"><p/>
<p>M U L T I N O M I A L L O G I S T I C R E G R E S S I O N
</p>
<p>M u l t i n o m i a l  L o g i s t i c  R e g r e s s i o n
</p>
<p>Multinomial logistic regression is used to examine problems where
</p>
<p>there are more than two nominal categories in the dependent variable. We
</p>
<p>have already mentioned the case where a researcher wants to explain
</p>
<p>why convicted offenders are sentenced to prison, probation or fines, but
</p>
<p>there are many situations in criminal justice in which dependent variables
</p>
<p>include multiple nominal categories. For example, a researcher may want
</p>
<p>to explain why certain offenders tend to specialize in either violent crime,
</p>
<p>property crime, or white collar crime. Multinomial regression is particu-
</p>
<p>larly useful when researchers create categorizations for groups of offenders
</p>
<p>and then want to explain why certain people fall into those groups. This
</p>
<p>is common, for example, in recent studies in developmental criminology
</p>
<p>where offenders are placed into a small number of groups that evidence
</p>
<p>different crime trajectories.1 It is then natural to ask why offenders fall
</p>
<p>into those groups. Multinomial regression provides a very useful tool for
</p>
<p>conducting multivariate analyses in such situations.
</p>
<p>Multinomial regression is conceptually a straightforward extension of
</p>
<p>the binary logistic regression model that we discussed in the previous
</p>
<p>chapter. Recall that in the binary logistic regression model, we designated
</p>
<p>one of the two outcome categories as the presence of a given trait and
</p>
<p>the second as the absence of that trait. For example, we compared police
</p>
<p>departments that had adopted Compstat (Y=1) versus those that did not
</p>
<p>adopt the Compstat program (Y=0). In logistic regression the left side of
</p>
<p>the regression equation is the natural logarithm (ln) of the odds of hav-
</p>
<p>ing a 1 on the dependent variable (Y=1) as opposed to having a 0 (Y=0).
</p>
<p>This transformation, illustrated in Equation 19.1, allowed us to develop a
</p>
<p>prediction model in which the predictions of the regression equation are
</p>
<p>constrained to fall between 0 and 1. We called this transformation the
</p>
<p>logit of Y:
</p>
<p>( )
( )
</p>
<p>ln
P Y
</p>
<p>P Y
b b X
</p>
<p>0
1
</p>
<p>0 1 1
=
</p>
<p>=
= +e o Equation 19.1
</p>
<p>603
</p>
<p>What happens when the outcome variable has more than two cate-
</p>
<p>Y in Equation 19.1 requires that
</p>
<p>there be only the absence (Y=0) or the presence (Y=1) of a trait. This sit-
</p>
<p>uation is not appropriate when you have the possibility of the presence of
</p>
<p>more than one positive outcome (e.g. dismissal, a guilty plea conviction,
</p>
<p>gories? The problem here is that we do not have a simple change in the odds 
</p>
<p>for one outcome, as we did with the example of Compstat in the prior 
</p>
<p>chapter. Here we have to take into account changes in the odds in relation 
</p>
<p>to more than two categories. The logit of 
</p>
<p>1See, for example, D. Nagin, Group-based Modeling of Development (Cambridge,
MA: Harvard University Press, 2005).</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>or a trial conviction) and you want to distinguish among them. Of course,
</p>
<p>you could simply argue that you are only interested for example, in
</p>
<p>whether individuals received a dismissal. However, you would not have
</p>
<p>the possibility in the simple logistic model to predict why they received
</p>
<p>alternatively a guilty plea conviction, or a trial conviction. This is the prob-
</p>
<p>lem that multinomial regression seeks to solve.
</p>
<p>Suppose that our outcome variable has three categories (C1, C2, and
</p>
<p>C3) with the number of observations in each being represented by N
C1
</p>
<p>,
</p>
<p>N
C2
</p>
<p>, and N
C3
</p>
<p>. We could begin by estimating three binary logistic regression
</p>
<p>models that would allow for all possible comparisons of the outcome cate-
</p>
<p>gories &ndash; the logits for C1 and C2, C2 and C3, and C1 and C3. The logit of
</p>
<p>Y for each regression could be written simply as:
</p>
<p>( )
( )
</p>
<p>,
( )
( )
</p>
<p>,
( )
( )
</p>
<p>ln ln ln
P Y C
</p>
<p>P Y C
</p>
<p>P Y C
</p>
<p>P Y C
</p>
<p>P Y C
</p>
<p>P Y C
and
</p>
<p>2
1
</p>
<p>3
2
</p>
<p>3
1
</p>
<p>=
</p>
<p>=
</p>
<p>=
</p>
<p>=
</p>
<p>=
</p>
<p>=e e eo o o for each comparison,
respectively.
</p>
<p>Interestingly, these three logits can be linked in what can be defined as
</p>
<p>an identity equation that illustrates how knowledge of any two logits can
</p>
<p>produce the values of the third. The identity equation2 can be stated as:
</p>
<p>( )
( )
</p>
<p>( )
( )
</p>
<p>( )
( )
</p>
<p>ln ln ln
P Y C
</p>
<p>P Y C
</p>
<p>P Y C
</p>
<p>P Y C
</p>
<p>P Y C
</p>
<p>P Y C
</p>
<p>2
1
</p>
<p>3
2
</p>
<p>3
1
</p>
<p>=
</p>
<p>=
+
</p>
<p>=
</p>
<p>=
=
</p>
<p>=
</p>
<p>=e e eo o o
</p>
<p>If we were to estimate these three separate logits, the coefficients would
</p>
<p>be interpreted in the same way as we described in Chapter 18. While this
</p>
<p>approach would allow us to make comparisons of the likelihood of sub-
</p>
<p>jects falling in each of the three categories examined as compared to each
</p>
<p>2You can verify this identity by using the fact that the logarithm of a fraction is equal
to the logarithm of the numerator minus the logarithm of the denominator: ln(x/y) =
ln(x) &minus; ln(y). Specifically, for this equality, we note that
</p>
<p>( )
</p>
<p>( )
( ( ) ) ( ( ) )ln ln ln
</p>
<p>P Y C
</p>
<p>P Y C
P Y C P Y C
</p>
<p>2
</p>
<p>1
1 2
</p>
<p>=
</p>
<p>=
= = - =e o
</p>
<p>and
</p>
<p>( )
</p>
<p>( )
( ( ) ) ( ( ) )ln ln ln
</p>
<p>P Y C
</p>
<p>P Y C
P Y C P Y C
</p>
<p>3
</p>
<p>2
2 3
</p>
<p>=
</p>
<p>=
= = - =e o .
</p>
<p>When we put these two pieces together in a single equation, we have
</p>
<p>[ ( ( ) ) ( ( ) )] [ ( ( ) ) ( ( ) )]
</p>
<p>( ( ) ) ( ( ) ) ( ( ) ) ( ( ) )
</p>
<p>( ( ) ) ( ( ) )
</p>
<p>( ( ) )
</p>
<p>( ( ) )
</p>
<p>ln ln ln ln
</p>
<p>ln ln ln ln
</p>
<p>ln ln
</p>
<p>ln
ln
</p>
<p>ln
</p>
<p>P Y C P Y C P Y C P Y C
</p>
<p>P Y C P Y C P Y C P Y C
</p>
<p>P Y C P Y C
</p>
<p>P Y C
</p>
<p>P Y C
</p>
<p>1 2 2 3
</p>
<p>1 2 2 3
</p>
<p>1 3
</p>
<p>3
</p>
<p>1
</p>
<p>= - = + = - =
</p>
<p>= = - = + = - =
</p>
<p>= = - =
</p>
<p>=
=
</p>
<p>=e o
Which establishes the equality. We explain the practical implication of this equality
below in our discussion of the interpretation of the coefficients from a multinomial
logistic regression model.
</p>
<p>604</p>
<p/>
</div>
<div class="page"><p/>
<p>other, it would require us to run three separate regressions. Moreover, and
</p>
<p>more importantly from a statistical point of view, we would likely be
</p>
<p>working with three completely different samples in each of the three
</p>
<p>analyses: N
C1
</p>
<p>+ N
C2
</p>
<p>, N
C2
</p>
<p>+ N
C3
</p>
<p>, and N
C1
</p>
<p>+ N
C3
</p>
<p>. This is because the cases on
</p>
<p>the dependent variable are unlikely to be distributed evenly. For exam-
</p>
<p>ple, we would not expect sentences for 300 offenders to be distributed
</p>
<p>with exactly one hundred in each group (e.g. dismissal, a guilty plea con-
</p>
<p>viction, or a trial conviction). Given this, each of our comparisons would
</p>
<p>be based on different samples. In comparing N
C 1
</p>
<p>to N
C2
</p>
<p>we would have
</p>
<p>only defendants who had outcomes C1 and C2. Defendants that had out-
</p>
<p>come C3 would not be included in that comparison. But what we are
</p>
<p>really interested in is the choice among the three outcomes and how this
</p>
<p>choice is distributed in our entire sample. The statistical problem here is
</p>
<p>that the varying sample sizes would then result in incorrect standard
</p>
<p>errors for the coefficients, leading to inaccurate tests of statistical signifi-
</p>
<p>cance. The multinomial logistic regression model simultaneously accounts
</p>
<p>for these different sample sizes, ensuring a more valid estimate of signif-
</p>
<p>icance levels. It also has the benefit of allowing us to conduct our analysis
</p>
<p>using only one regression model.
</p>
<p>A Substantive Example: Case Dispositions in California
</p>
<p>The State Court Processing Statistics database includes information on ran-
</p>
<p>dom samples of individuals arrested for felony offenses in the largest court
</p>
<p>districts in the United States. To illustrate the application of the multino-
</p>
<p>mial logistic regression model, we focus on a random sample of 10,230
</p>
<p>felony arrestees in California in the 1990s. A question of both policy and
</p>
<p>theoretical relevance is the study of the factors that affect the type of case
</p>
<p>disposition (outcome) &ndash; whether a dismissal, a guilty plea conviction, or a
</p>
<p>trial conviction.3
</p>
<p>A first step in a multinomial regression is to define a &ldquo;reference cate-
</p>
<p>gory.&rdquo; This is necessary because we need to decide which category we
</p>
<p>want to use as a baseline. It is an arbitrary decision about which category
</p>
<p>is designated the reference category, but to the extent that we can make a
</p>
<p>choice that has some theoretical relevance or makes the interpretation of
</p>
<p>the results simpler, that would be the preferred choice. For case disposi-
</p>
<p>tion, suppose that we choose dismissal as the reference category, which
</p>
<p>then allows us to make two comparisons between a type of conviction &ndash;
</p>
<p>guilty plea or trial &ndash; and dismissal. More directly, our multinomial logistic
</p>
<p>3While it may appear odd at first glance that we have not included those individuals
who were acquitted at a trial, there were very few individuals who fell into this cate-
gory. Like most jurisdictions, courts in California acquit relatively few individuals
through a trial &ndash; it was about 1% of all cases in the 1990s. What this means is that once
the prosecutor has filed charges against a defendant, rather than dismiss the case, it
will likely result in the conviction of the defendant through either a guilty plea or a
trial conviction. This also implies that a dismissal of the case functions much like an
acquittal, but one made by the prosecuting attorney rather than a judge or jury.
</p>
<p>605M U L T I N O M I A L L O G I S T I C R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>regression results will indicate (1) the relative likelihood of a guilty plea
</p>
<p>conviction compared to a dismissal and (2) the relative likelihood of a trial
</p>
<p>conviction compared to a dismissal. The one comparison we did not men-
</p>
<p>tion was the relative likelihood of a guilty plea conviction compared to a
</p>
<p>trial conviction. In the multinomial logistic regression model, this compar-
</p>
<p>ison is not directly estimated, but as we illustrate shortly, the results can be
</p>
<p>obtained very simply from the results for the comparison of each convic-
</p>
<p>tion type to a dismissal.
</p>
<p>In equation form, the multinomial model can be written as either a
</p>
<p>probability model or an odds ratio model. In Equation 19.2 we provide
</p>
<p>an example of the probability equation.
</p>
<p>Probability Equation
</p>
<p>)j
</p>
<p>( )
(
</p>
<p>( )
</p>
<p>exp
</p>
<p>exp
P Y m
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>j
</p>
<p>J
</p>
<p>m
</p>
<p>1
</p>
<p>= =
</p>
<p>=
/
</p>
<p>In this equation, m refers to the outcome category of interest and has val-
</p>
<p>ues ranging from 1 to J (the last category). The numerator to the equa-
</p>
<p>tion tells us to exponentiate the value of Xb for category m. The
</p>
<p>denominator, in turn, tells us that we need to exponentiate the value of
</p>
<p>Xb for all categories and then sum these values together. Since there is a
</p>
<p>redundancy built into the values of the coefficients in a multinomial
</p>
<p>logistic model, the values for one set of coefficients are set at 0 (e.g., 
</p>
<p>b
1
= 0). This is the reference category and leads to the estimation of 
</p>
<p>coefficients for the total number of categories minus 1 ( i.e., J &minus; 1).
</p>
<p>For our three-category case disposition variable, m = 1, 2, or 3. Writing
</p>
<p>out the probability equations for each outcome leads to the following for-
</p>
<p>mulations of the probability of each of the three outcomes in our example.
</p>
<p>For m = 1, 
</p>
<p>( )
( ) ( ) ( )
</p>
<p>( )
</p>
<p>( ) ( )exp exp exp
</p>
<p>exp
</p>
<p>exp exp
P Y
</p>
<p>X Xb Xb
</p>
<p>X
</p>
<p>Xb Xb
1
</p>
<p>0
</p>
<p>0
</p>
<p>1
1
</p>
<p>2 3 2 3
</p>
<p>= =
+ +
</p>
<p>=
+ +
</p>
<p>For m = 2 and m = 3, we have
</p>
<p>( )
( ) ( )
</p>
<p>( )
</p>
<p>( )
( ) ( )
</p>
<p>( )
</p>
<p>exp exp
</p>
<p>exp
</p>
<p>exp exp
</p>
<p>exp
</p>
<p>P Y
Xb Xb
</p>
<p>Xb
</p>
<p>P Y
Xb Xb
</p>
<p>Xb
</p>
<p>2
1
</p>
<p>3
1
</p>
<p>2 3
</p>
<p>2
</p>
<p>2 3
</p>
<p>3
</p>
<p>= =
+ +
</p>
<p>= =
+ +
</p>
<p>Odds Ratio Equation
</p>
<p>)
</p>
<p>)
</p>
<p>)
</p>
<p>( )
( )
</p>
<p>(
</p>
<p>( )
</p>
<p>(
</p>
<p>(
</p>
<p>( )
</p>
<p>( )
</p>
<p>exp
</p>
<p>exp
</p>
<p>exp
</p>
<p>exp
</p>
<p>exp
</p>
<p>exp
OR
</p>
<p>P Y n
</p>
<p>P Y m
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>Xb
m n
</p>
<p>jj
</p>
<p>J
</p>
<p>n
</p>
<p>jj
</p>
<p>J
</p>
<p>m
</p>
<p>n
</p>
<p>m
</p>
<p>1
</p>
<p>1
=
</p>
<p>=
</p>
<p>=
= =e
</p>
<p>=
</p>
<p>=
</p>
<p>/
</p>
<p>/
</p>
<p>Equation 19.2
</p>
<p>Equation 19.3
</p>
<p>606
</p>
<p>= 0 b1</p>
<p/>
</div>
<div class="page"><p/>
<p>We provide the odds ratio equation in Equation 19.3. Although this equa-
</p>
<p>tion may look much more complicated, it uses the information in Equation
</p>
<p>19.2 for the probabilities of each category. The equation reduces to some-
</p>
<p>thing less complex, because the denominators in the fraction in the mid-
</p>
<p>dle of the equation cancel each other out. The only notable difference here
</p>
<p>is the notation of a second category by the subscript n. Thus, for any odds
</p>
<p>ratio that we may be interested in between categories m and n, Equation
</p>
<p>19.3 illustrates that it can be obtained from the respective probabilities.
</p>
<p>If we are interested in computing the odds ratio for a comparison
</p>
<p>between any category and the reference category (m = 1), where b
1
</p>
<p>= 0,
</p>
<p>we obtain
</p>
<p>( )
( )
</p>
<p>( )
</p>
<p>( )
( )
</p>
<p>exp
</p>
<p>exp
expOR
</p>
<p>P Y
</p>
<p>P Y m
</p>
<p>X1 0/m n
m
</p>
<p>m=
=
</p>
<p>=
= =
</p>
<p>Xb
Xb
</p>
<p>This last result confirms how we are then to interpret the coefficients from
</p>
<p>the multinomial logistic regression model. Since the coefficients for the ref-
</p>
<p>erence category have been fixed at 0, the coefficients for each of the
</p>
<p>remaining outcome categories will compare the relative likelihood of that
</p>
<p>category compared to the reference category.4
</p>
<p>In practice, what these equations tell us is that we will have J &minus; 1 sets
</p>
<p>of coefficients from a multinomial logistic regression model that can be
</p>
<p>interpreted in the same way as binary logistic coefficients, where we com-
</p>
<p>pare each outcome (m) to the reference category (m = 1) for the outcome
</p>
<p>variable. In our example for case disposition, where we have designated
</p>
<p>dismissal as the reference category, one set of coefficients will give us the
</p>
<p>log of the odds or the odds ratios comparing the likelihood of a guilty
</p>
<p>plea conviction relative to a dismissal, while the second set of coefficients
</p>
<p>will give us the log of the odds or the odds ratios comparing the likelihood
</p>
<p>of a trial conviction relative to a dismissal.
</p>
<p>Table 19.1 presents the results from our application of the multinomial
</p>
<p>logistic regression model. We have kept the multivariate model simple and
</p>
<p>used age, sex (males = 1, females = 0), race (non-white = 1, white = 0),
</p>
<p>Table 19.1 Multinomial Logistic Regression Coefficients
</p>
<p>INDEPENDENT TRIAL CONVICTION V. GUILTY PLEA CONVICTION V. 
VARIABLES DISMISSAL DISMISSAL
</p>
<p>Age .016 .004
Male 1.123 &minus;.013
Non-white .043 &minus;.266
Violent Offense .657 &minus;.525
Number of Charges .325 .192
Intercept &minus;4.767 1.318
</p>
<p>4It is worth pointing out that the binary logistic regression model presented in Chapter
18 is a special case of the multinomial logistic regression model, where m = 2. If you
work through both Equations 19.2 and 19.3 above assuming that m = 2, you will be
able to replicate the equations in the previous chapter.
</p>
<p>607M U L T I N O M I A L L O G I S T I C R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>type of crime (violent =1, non-violent=0), and total number of charges as
</p>
<p>predictors of the type of case disposition for the sample of 10,230 arrestees
</p>
<p>in California in the 1990s. The first column lists all the independent vari-
</p>
<p>ables, while the second and third columns present the coefficients for each
</p>
<p>of the two comparisons: Column 2 presents the comparison of trial con-
</p>
<p>viction to dismissal, while Column 3 presents the comparison of guilty plea
</p>
<p>conviction to dismissal.
</p>
<p>The results in Column 2 show that as age and the number of charges
</p>
<p>increase, the likelihood of a trial conviction increases relative to a dis-
</p>
<p>missal. Similarly, defendants who are male, non-white, and charged with
</p>
<p>a violent offense are also more likely to be convicted at trial than to have
</p>
<p>their case dismissed. As in the previous chapter, we can also interpret
</p>
<p>each of these coefficients more directly as odds ratios. (Recall from the
</p>
<p>previous chapter that the exponentiation of the coefficient provides us
</p>
<p>with the odds ratio given a one-unit change in the independent variable.)
</p>
<p>● If age is increased by one year, the odds of a trial conviction versus a
</p>
<p>dismissal increase by a factor of exp(.016) = 1.016, controlling for all
</p>
<p>other variables in the model.
● The odds of a trial conviction versus a dismissal are exp(1.123) = 3.074
</p>
<p>times higher for male than for female defendants, controlling for all
</p>
<p>other variables in the model.
● The odds of a trial conviction versus a dismissal are exp(.043) = 1.044
</p>
<p>times higher for non-white than white defendants, controlling for all
</p>
<p>other variables in the model.
● The odds of a trial conviction versus a dismissal are exp(.657) = 1.929
</p>
<p>times higher for defendants charged with a violent rather than a non-
</p>
<p>violent offense, controlling for all other variables in the model.
● If the number of charges is increased by one, the odds of a trial con-
</p>
<p>viction versus a dismissal increase by a factor of exp(.325) = 1.384, con-
</p>
<p>trolling for all other variables in the model.
</p>
<p>We can similarly interpret the results in Column 3, which show that as
</p>
<p>age and number of charges increase, the likelihood of a guilty plea con-
</p>
<p>viction relative to a dismissal increases. We also see from these results that
</p>
<p>defendants who are male, non-white and charged with a violent offense
</p>
<p>will be less likely to be convicted with a guilty plea than to have their
</p>
<p>cases dismissed. Again, the direct interpretations of the coefficients would
</p>
<p>be the following:
</p>
<p>● If age is increased by one year, the odds of a guilty plea conviction ver-
</p>
<p>sus a dismissal increase by a factor of exp(.004) = 1.004, controlling for
</p>
<p>all other variables in the model.
● The odds of a guilty plea conviction versus a dismissal are exp(&minus;.013)
</p>
<p>= .987 times smaller for male than for female defendants, controlling for
</p>
<p>all other variables in the model.
</p>
<p>608</p>
<p/>
</div>
<div class="page"><p/>
<p>● The odds of a guilty plea conviction versus a dismissal are exp(&minus;.266)
</p>
<p>= .766 times smaller for non-white than white defendants, controlling
</p>
<p>for all other variables in the model.
● The odds of a guilty plea conviction versus a dismissal are exp(&minus;.525)
</p>
<p>= .592 times smaller for defendants charged with a violent rather than a
</p>
<p>non-violent offense, controlling for all other variables in the model.
● If the number of charges is increased by one, the odds of a guilty plea
</p>
<p>conviction versus a dismissal increase by a factor of exp(.192) = 1.212,
</p>
<p>controlling for all other variables in the model.
</p>
<p>The Missing Set of Coefficients
</p>
<p>As we noted earlier, when we estimate a multinomial logistic regression
</p>
<p>model, we obtain coefficients for all contrasts but one. In the example of
</p>
<p>case disposition, we are missing the contrast between guilty plea convic-
</p>
<p>tion and trial conviction. Based on the identity relationship of multiple
</p>
<p>logits that we described earlier in the chapter (see, also, footnote 2), for
</p>
<p>all possible comparisons of the outcome categories, the most direct way
</p>
<p>of obtaining the missing coefficients is to simply subtract one set of coef-
</p>
<p>ficients from another set of coefficients. In Table 19.1, the results in
</p>
<p>Column 2 represent the logit for Trial Conviction and Dismissal, while
</p>
<p>those in Column 3 represent the logit for Guilty Plea Conviction and
</p>
<p>Dismissal.
</p>
<p>Since the logarithm of a fraction can be rewritten as the subtraction of
</p>
<p>the logarithm of the denominator from the logarithm of the numerator, the
</p>
<p>logits can be rewritten as
</p>
<p>( )
( )
</p>
<p>( ( ))
</p>
<p>( ( ))
</p>
<p>ln ln
</p>
<p>ln
</p>
<p>P Y Dismissal
</p>
<p>P Y Trial Conviction
P Y Trial Conviction
</p>
<p>P Y Dismissal
</p>
<p>=
</p>
<p>=
= =
</p>
<p>- =
</p>
<p>e o
</p>
<p>and
</p>
<p>( )
</p>
<p>( )
( ( ))
</p>
<p>( ( ))
</p>
<p>ln ln
</p>
<p>ln
</p>
<p>P Y Dismissal
</p>
<p>P Y Guilty Plea Conviction
P Y Guilty Plea Conviction
</p>
<p>P Y Dismissal
</p>
<p>=
</p>
<p>=
= =
</p>
<p>- =
</p>
<p>e o
</p>
<p>By performing simple subtractions of the logits, we can generate addi-
</p>
<p>tional contrasts between the outcome categories. To obtain the missing
</p>
<p>coefficients for the comparison of Guilty Plea Conviction to Trial
</p>
<p>Conviction, we subtract the logit for Trial Conviction and Dismissal from the
</p>
<p>logit for Guilty Plea Conviction and Dismissal:
</p>
<p>( )
</p>
<p>( )
</p>
<p>( )
( )
</p>
<p>( ( )) ( ( ))
</p>
<p>( ( )) ( ( ))
</p>
<p>ln ln
</p>
<p>ln ln
</p>
<p>ln ln
</p>
<p>P Y Dismissal
</p>
<p>P Y Guilty Plea Conviction
</p>
<p>P Y Dismissal
</p>
<p>P Y Trial Conviction
</p>
<p>P Y Guilty Plea Conviction P Y Dismissal
</p>
<p>P Y Trial Conviction P Y Dismissal
</p>
<p>=
</p>
<p>=
-
</p>
<p>=
</p>
<p>=
</p>
<p>= = - =
</p>
<p>- = - =
</p>
<p>e eo o
7
7
</p>
<p>A
A
</p>
<p>( ( )) ( ( ))ln lnP Y Guilty Plea Conviction P Y Trial Conviction= = - =
</p>
<p>609M U L T I N O M I A L L O G I S T I C R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>( )
</p>
<p>( )
ln
</p>
<p>P Y Trial Conviction
</p>
<p>P Y Guilty Plea Conviction
=
</p>
<p>=
</p>
<p>=e o
In other words, what this algebraic manipulation of logits shows us is
</p>
<p>that we can obtain the coefficients for the omitted contrast simply by sub-
</p>
<p>tracting one set of coefficients from another set of coefficients.
</p>
<p>When applied to our case disposition coefficients, we obtain the results
</p>
<p>presented in Table 19.2. Here, we see that as age and the number of
</p>
<p>charges increase, the likelihood of a guilty plea conviction relative to a trial
</p>
<p>conviction decrease. Similarly, defendants who are male, non-white and
</p>
<p>charged with a violent offense will be less likely to be convicted through a
</p>
<p>guilty plea than through a trial.
</p>
<p>In regard to the direct interpretation of the coefficients, we have the
</p>
<p>following:
</p>
<p>● If age is increased by one year, the odds of a guilty plea conviction ver-
</p>
<p>sus a trial conviction decrease by a factor of exp(&minus;.012) = .988, con-
</p>
<p>trolling for all other variables in the model.
● The odds of a guilty plea conviction versus a trial conviction are 
</p>
<p>exp(&minus;1.136) = .321 times smaller for male than for female defendants, con-
</p>
<p>trolling for all other variables in the model.
● The odds of a guilty plea conviction versus a trial conviction are 
</p>
<p>exp(&minus;.309) = .734 times smaller for non-white than white defendants,
</p>
<p>controlling for all other variables in the model.
● The odds of a guilty plea conviction versus a trial conviction are 
</p>
<p>exp(&minus;1.182) = .307 times smaller for defendants charged with a violent
</p>
<p>rather than a non-violent offense, controlling for all other variables in
</p>
<p>the model.
● If the number of charges is increased by one, the odds of a guilty plea
</p>
<p>conviction versus a trial conviction increase by a factor of exp(&minus;.133) =
</p>
<p>875, controlling for all other variables in the model.
</p>
<p>A second way to obtain the coefficients for the comparison of guilty
</p>
<p>plea conviction to trial conviction would be to simply redefine our statis-
</p>
<p>tical model so that trial conviction was chosen as the reference category
</p>
<p>and re-estimate our multinomial model. Upon rerunning the multinomial
</p>
<p>logistic regression model, we obtain the results presented in Table 19.3.
</p>
<p>Table 19.2 Coefficients for the Omitted Contrast of Guilty Plea Conviction v.Trial
</p>
<p>Conviction through Subtraction of Coefficients
</p>
<p>INDEPENDENT VARIABLES GUILTY PLEA CONVICTION V. TRIAL CONVICTION
</p>
<p>Age .004 &ndash;.016 = &minus;.012
Male &minus;.013 &ndash; 1.123 = &minus;1.136
Non-white &minus;.266 &ndash;.043 = &minus;.309
Violent Offense &minus;.525 &ndash;.657 = &minus;1.182
Number of Charges .192 &ndash;.325 = &minus;.133
Intercept 1.318 &ndash; (&minus;4.767) = 6.085
</p>
<p>610</p>
<p/>
</div>
<div class="page"><p/>
<p>Note that Column 2 contains the coefficients for the contrast between dis-
</p>
<p>missal v. trial conviction, which substantively gets at the same compari-
</p>
<p>son that appears in Table 19.1, Column 2, except for the order of the
</p>
<p>comparison (trial conviction v. dismissal). The only difference between
</p>
<p>the results presented in Column 2 of both Tables 19.1 and 19.3 are the
</p>
<p>signs of the coefficients, reflecting the order of comparison of the out-
</p>
<p>come categories. Again, what this indicates to us is that the selection of
</p>
<p>reference categories is arbitrary and that we will obtain the same sub-
</p>
<p>stantive results, regardless of which category is selected. At the same time,
</p>
<p>we need to be aware of the selection of categories so that we correctly
</p>
<p>interpret our results.
</p>
<p>Column 3 presents the contrast for guilty plea conviction relative to trial
</p>
<p>conviction. The results in this column are identical to those appearing in
</p>
<p>column 2 of Table 19.2, which were based on simply subtracting one set of
</p>
<p>coefficients from another.
</p>
<p>Had we been interested in the contrast of Trial Conviction relative to Guilty
</p>
<p>Plea Conviction, we would have reversed the original order of subtraction
</p>
<p>(i.e., the coefficients in Column 3 would have been subtracted from the 
</p>
<p>coefficients in Column 2). Then, the only difference that we would have seen
</p>
<p>in Table 19.2 is that the signs of the coefficients would have been reversed.
</p>
<p>Otherwise, the substantive interpretation of the results would be identical.
</p>
<p>Statistical Inference
</p>
<p>Single Coefficients
</p>
<p>The results from a multinomial logistic regression analysis complicate
</p>
<p>slightly tests of statistical significance. Since we now have multiple coef-
</p>
<p>ficients for each independent variable, there are questions about how to
</p>
<p>discern whether an independent variable has an effect on the dependent
</p>
<p>variable. Specifically, there are two issues of statistical inference that are
</p>
<p>important for interpreting the results from a multinomial logistic regres-
</p>
<p>sion analysis. For each coefficient we can estimate the statistical signifi-
</p>
<p>cance of each category compared to the reference category. But we also
</p>
<p>can estimate the overall significance of the independent variable in 
</p>
<p>predicting the multi-category dependent variable.
</p>
<p>Table 19.3 Multinomial Logistic Regression Coefficients Using Trial Conviction as the
</p>
<p>Reference Category from Re-estimated Model
</p>
<p>GUILTY PLEA CONVICTION 
INDEPENDENT VARIABLES DISMISSAL V. TRIAL CONVICTION V. TRIAL CONVICTION
</p>
<p>Age &minus;.016 &minus;.012
Male &minus;1.123 &minus;1.136
Non-white &minus;.043 &minus;.309
Violent Offense &minus;.657 &minus;1.182
Number of Charges &minus;.325 &minus;.133
Intercept 4.767 6.085
</p>
<p>611M U L T I N O M I A L L O G I S T I C R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>To test the effect of each individual coefficient in comparison to the
</p>
<p>reference category, we would again use the Wald statistic described in
</p>
<p>Chapter 18, with degrees of freedom equal to 1. As noted in Chapter 18,
</p>
<p>the Wald statistic has a chi-square distribution, and so the statistical sig-
</p>
<p>nificance of a result may be checked in the chi&ndash;square table. Table 19.4
</p>
<p>presents the Multinomial Logistic Coefficients from the original model
</p>
<p>along with the standard errors (se) of the coefficients and value of the
</p>
<p>Wald statistic (W).
</p>
<p>If we set the significance level at 5%, the critical value of the Wald sta-
</p>
<p>tistic with df = 1, is 3.841 (see Appendix 2). We can see that the violent
</p>
<p>offense charge and number of charges have statistically significant effects
</p>
<p>for both pairs of outcomes. The demographic characteristics have varying
</p>
<p>effects, where age and sex have statistically significant effects on the like-
</p>
<p>lihood of a trial conviction compared to a dismissal, but race has a statis-
</p>
<p>tically significant effect on the likelihood of a guilty plea conviction
</p>
<p>compared to a dismissal.
</p>
<p>Multiple Coefficients
</p>
<p>Note in Table 19.4 that there are two coefficients for each independent
</p>
<p>variable. As we noted above, the number of coefficients from a multino-
</p>
<p>mial logistic regression model for each independent variable will be one less
</p>
<p>than the number of categories on the dependent variable (i.e., J &ndash; 1). How
</p>
<p>do we assess the overall effect of each independent variable on the depend-
</p>
<p>ent variable? There are two key ways of doing this &ndash; one is a likelihood
</p>
<p>ratio test similar to the test we discussed in the previous chapter on binary
</p>
<p>logistic regression. The other test is an extension of the Wald test we have
</p>
<p>also already used. Regardless of the statistical software package one uses
</p>
<p>to estimate a multinomial logistic regression model, one of these two meth-
</p>
<p>ods will be reported to test the overall effect of each independent variable.
</p>
<p>The likelihood ratio test involves estimating the full multinomial logistic
</p>
<p>regression equation with all variables and then estimating reduced models
</p>
<p>that eliminate one independent variable from each analysis. The difference
</p>
<p>in the &minus;2 log-likelihood function for each equation will then allow for the
</p>
<p>test of each independent variable.
</p>
<p>Table 19.4 Multinomial Logistic Regression Coefficients, Standard Errors, and Wald Test
</p>
<p>Results
</p>
<p>GUILTY PLEA CONVICTION V. 
TRIAL CONVICTION V. DISMISSAL DISMISSAL
</p>
<p>INDEPENDENT 
VARIABLES B SE W B SE W
</p>
<p>Age .016 .008 4.181 .004 .003 2.110
Male 1.123 .373 11.350 &minus;.013 .071 .032
Non-white .043 .158 .072 &minus;.266 .053 24.827
Violent Offense .657 .160 16.834 &minus;.525 .060 76.287
Number of Charges .325 .043 58.524 .192 .021 82.642
Intercept &minus;4.767 .438 118.726 1.318 .124 112.754
</p>
<p>612</p>
<p/>
</div>
<div class="page"><p/>
<p>For example, in the case dismissal analysis, the value of the &minus;2 log-like-
</p>
<p>lihood for the full model is 3625.670. When we estimate the same model,
</p>
<p>tions is 3653.501 &ndash; 3625.670 = 27.831. By eliminating the variable for
</p>
<p>non-white, we have removed two coefficients from the analysis. If you
</p>
<p>refer again to Table 19.4, you will see that each independent variable
</p>
<p>appears twice to indicate its overall effect on case disposition &ndash; once to
</p>
<p>represent the effect on trial conviction versus dismissal and once more
</p>
<p>to represent the effect on guilty plea conviction versus dismissal. The degrees
</p>
<p>of freedom for the test will be df = 2 to reflect the removal of the two coef-
</p>
<p>ficients. At a significance level of 5%, the critical value of the chi-square is
</p>
<p>5.991. This means that we would conclude that the race of the defendant
</p>
<p>has a statistically significant effect on the type of case disposition. Table
</p>
<p>19.5 presents the vales of the &minus;2 log-likelihood function for each of
</p>
<p>the reduced models and the value of the likelihood ratio test for each inde-
</p>
<p>pendent variable. Based on the critical value of the chi-square of 5.991, we
</p>
<p>see that race and sex of defendant, violent offense charge, and number of
</p>
<p>charges all have statistically significant effects on type of case disposition,
</p>
<p>while age of defendant does not have a statistically significant effect.
</p>
<p>An alternative test of each independent variable is to use the Wald sta-
</p>
<p>tistic. Up to this point, we have used the Wald statistic to test the statistical
</p>
<p>significance of a single coefficient, but it can also be used to test the group
</p>
<p>of coefficients representing the effect of any given independent variable.
</p>
<p>Recall that the Wald test statistic for a single coefficient is computed by
</p>
<p>dividing the coefficient by its standard error and then squaring this value.
</p>
<p>The Wald statistic for a group of coefficients involves an analogous calcu-
</p>
<p>lation, but requires the use of matrix algebra &ndash; a topic beyond the scope
</p>
<p>of our text. That said, many statistical software packages will generate the
</p>
<p>results for the Wald test as part of the standard output, and our attention
</p>
<p>here is focused more on illustrating the interpretation of the results, rather
</p>
<p>than the actual calculation. In most applications, the value of the Wald
</p>
<p>statistic will be very similar to the value of the LR test.5
</p>
<p>Table 19.5 Likelihood Ratio and Wald Statistic Results for the Overall Effect of Each
</p>
<p>Independent Variable
</p>
<p>&minus;2 LOG-LIKELIHOOD FOR 
INDEPENDENT VARIABLE DF THE REDUCED MODEL LR TEST STATISTIC WALD
</p>
<p>Age 2 3630.528 4.859 4.94
Male 2 3642.342 16.672 11.98
Non-white 2 3653.501 27.831 27.77
Violent Offense 2 3747.168 121.498 125.95
Number of Charges 2 3737.877 112.207 99.08
</p>
<p>5Recall from footnote # 13 in Chapter 18 that the Wald statistic is sensitive to small
samples (e.g., less than 100), while the LR test is not.
</p>
<p>613M U L T I N O M I A L L O G I S T I C R E G R E S S I O N
</p>
<p>but eliminate the variable non-white from the analysis, the value of the &minus;2
</p>
<p>log-likelihood is 3653.501. The difference of the two log-likelihood func-</p>
<p/>
</div>
<div class="page"><p/>
<p>To test the overall effect of an independent variable with the Wald sta-
</p>
<p>tistic, we continue to use a chi-square distribution with degrees of free-
</p>
<p>dom equal to the number of coefficients being tested &ndash; the number of
</p>
<p>outcome categories minus 1 (i.e., df = J &ndash; 1).
</p>
<p>In our case disposition example, we have three outcome categories
</p>
<p>( J = 3), so the degrees of freedom will be 3 &ndash; 1 = 2, which again corre-
</p>
<p>sponds to the number of sets of coefficients that have been estimated. The
</p>
<p>values of the Wald test for each of the independent variables included in
</p>
<p>our analysis are presented in Table 19.5.
</p>
<p>Using a significance level of 5%, we see from Appendix 2 that the crit-
</p>
<p>ical chi-square statistic has a value of 5.991. Based on this value, we see
</p>
<p>that all the independent variables, except for age of defendant, have sta-
</p>
<p>tistically significant effects on type of case disposition. The substance of
</p>
<p>these results is identical to that using the LR test.
</p>
<p>How should we address mixed results? For example, it is not uncom-
</p>
<p>mon for a researcher to find that the overall effect of an independent
</p>
<p>variable is not statistically significant, but one of the individual coeffi-
</p>
<p>cients does have a significant effect on a comparison of two outcome 
</p>
<p>categories. Alternatively, the Wald test for the overall effect of an inde-
</p>
<p>pendent variable may show it to have a statistically significant effect, but
</p>
<p>there may be individual coefficients representing the effect of that inde-
</p>
<p>pendent variable on a specific comparison that are not statistically 
</p>
<p>significant.
</p>
<p>This kind of difficulty is illustrated in the results presented in Tables
</p>
<p>19.4 and 19.5. Age of defendant does not have a statistically significant
</p>
<p>effect on case disposition overall, yet age does have a statistically signifi-
</p>
<p>cant effect on the comparison of trial conviction to dismissal. In such a
</p>
<p>case, the researcher should carefully note the pattern of results, explain-
</p>
<p>ing that overall age does not affect case disposition but that there appears
</p>
<p>to be a statistically significant impact of age on gaining a trial conviction
</p>
<p>as compared to a dismissal.
</p>
<p>Alternatively, we also see that the overall effect of race is statistically
</p>
<p>significant, but the individual coefficient for race on the comparison
</p>
<p>between trial conviction and dismissal is not statistically significant. The
</p>
<p>safest approach for the researcher in this type of situation is to note the
</p>
<p>significance of the overall effect of the independent variable, but again
</p>
<p>to clearly explain the pattern of results for the individual coefficients. In
</p>
<p>this case, our model suggests that race has an overall impact on case
</p>
<p>disposition but our data do not allow us to conclude, despite this, that
</p>
<p>race has a significant effect on gaining a trial conviction as opposed to a
</p>
<p>dismissal.
</p>
<p>Our suggestion is to use caution in interpreting the results and to be
</p>
<p>as clear as possible in explaining the nature and type of effect that is sta-
</p>
<p>tistically significant. In multinomial regression, a number of statistical out-
</p>
<p>comes are included and the researcher should be careful not to draw
</p>
<p>selectively from the results gained.
</p>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N614</p>
<p/>
</div>
<div class="page"><p/>
<p>O R D I N A L L O G I S T I C R E G R E S S I O N
</p>
<p>Overall Model
</p>
<p>In addition to testing the statistical significance of the individual coeffi-
</p>
<p>cients, we are also often interested in assessing the statistical significance
</p>
<p>of the overall model. To assess the statistical significance of the full multi-
</p>
<p>nomial regression model, we compute a model chi&ndash;square statistic that is
</p>
<p>identical in form to that used for the binary logistic regression model dis-
</p>
<p>cussed in Chapter 18. Recall that the model chi&ndash;square is computed as:
</p>
<p>Model chi&ndash;square = (&minus;2LLnull model) &ndash; (&minus;2LLfull model)
</p>
<p>For our case disposition analysis, the &minus;2LL
null model
</p>
<p>= 3923.540 and the 
</p>
<p>&minus;2LL
full model
</p>
<p>= 3625.670, resulting in a model chi-square of 3923.540 &ndash;
</p>
<p>3625.670 = 297.870. We have already noted that a total of 10 coefficients
</p>
<p>have been estimated (2 for each of the 5 independent variables), which
</p>
<p>gives us a degrees of freedom value for this test equal to 10. Looking at
</p>
<p>Appendix 2, we see that at a significance level of 5%, we see that a chi&ndash;
</p>
<p>square statistic greater than 18.307 is needed to reject the null hypothesis
</p>
<p>that the model has no statistically significant effect on case disposition.
</p>
<p>Since our model chi-square is larger than the critical value of the chi&ndash;
</p>
<p>square, we conclude that the overall model has a statistically significant
</p>
<p>effect on case disposition.
</p>
<p>A Concluding Observation about Multinomial Logistic Regression Models
</p>
<p>In our substantive example, we selected a dependent variable with only
</p>
<p>three categories. Realistic applications of multinomial logistic regression
</p>
<p>models with more than three categories can quickly become unwieldy in
</p>
<p>regard to the number of contrasts that are being analyzed. For example, if
</p>
<p>we had a dependent variable with four categories, we would have three
</p>
<p>sets of coefficients to represent a total of six different contrasts (C1 and C2,
</p>
<p>C1 and C3, C1 and C4, C2 and C3, C2 and C4, and C3 and C4). If we
</p>
<p>increased the number of outcome categories to five, we would have four
</p>
<p>sets of coefficients to represent a total of ten different contrasts, at which
</p>
<p>point the results from a multinomial logistic regression analysis likely
</p>
<p>become too difficult for most researchers to summarize in a coherent and
</p>
<p>concise way.
</p>
<p>O r d i n a l  L o g i s t i c  R e g r e s s i o n
</p>
<p>Multinomial regression provides a solution to the important problem of
</p>
<p>predicting multiple nominal category dependent variables. But in our dis-
</p>
<p>cussions so far we have not examined how to analyze ordinal level
</p>
<p>dependent variables. For many years researchers simply used Ordinary
</p>
<p>Least Squares Regression to deal with this type of analysis. There are times
</p>
<p>when this approach makes sense, and is unlikely to lead to estimation
</p>
<p>615</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>problems that are important. For example, if a researcher is examining an
</p>
<p>ordinal level variable that is measured as ten ascending categories, and
</p>
<p>can easily assume that the interval each category represents is the same
</p>
<p>as the prior interval, OLS estimates are likely to be satisfactory from a 
</p>
<p>statistical perspective. In this case, the ordinal level variable of interest can
</p>
<p>be assumed to have characteristics close to that of an interval level
</p>
<p>dependent variable.
</p>
<p>However, because until recently other estimation approaches for ordi-
</p>
<p>nal dependent variables were not easily accessible, researchers often
</p>
<p>made assumptions regarding an ordinal dependent variable that was
</p>
<p>examined that were clearly not appropriate. For example, when we exam-
</p>
<p>ine fear of crime measured as a series of categories from &ldquo;very fearful&rdquo; to
</p>
<p>&ldquo;not fearful at all&rdquo; it is very hard to assume that there are equal intervals
</p>
<p>between these qualitative responses. Today, with the availability of statis-
</p>
<p>tical software packages that will estimate ordinal regression models,
</p>
<p>researchers should be cautious in applying OLS regression to ordinal level
</p>
<p>measures. In this section, we present the ordinal logistic regression
</p>
<p>model that explicitly takes into account an ordered categorical dependent
</p>
<p>variable.
</p>
<p>In order to set up the application and interpretation of the ordinal
</p>
<p>logistic model, we need to reconsider what a variable measured at the
</p>
<p>ordinal level tells us. Recall from Chapter 2 that an ordinal variable has
</p>
<p>ranked categories that are assumed to represent an underlying continuum.
</p>
<p>For example, when respondents to a survey are presented with a state-
</p>
<p>ment that has as response choices Strongly Agree, Agree, Disagree, and
</p>
<p>Strongly Disagree, the variable is assumed to represent an underlying con-
</p>
<p>tinuum of agreement-disagreement with some issue. Yet, we know that
</p>
<p>however an individual responds to the question, any two individuals
</p>
<p>falling in the same category may not mean exactly the same thing. For
</p>
<p>example, if we randomly selected two individuals who had responded
</p>
<p>Strongly Disagree with a policy statement and we were able to ask more
</p>
<p>in-depth follow-up questions, we would likely discover that there were
</p>
<p>degrees of how strongly each disagreed.
</p>
<p>If we assume that an ordinal variable&rsquo;s categories represent an under-
</p>
<p>lying continuum, we can think of thresholds as those points where an
</p>
<p>individual may move from one ordinal category to another (adjacent) cat-
</p>
<p>egory. In the example above, we could make note of the thresholds
</p>
<p>between Strongly Agree and Agree, Agree and Disagree, and Disagree
</p>
<p>and Strongly Disagree. Figure 19.1 illustrates the link between the
</p>
<p>underlying continuum and the variable measured at the ordinal level. In
</p>
<p>Figure 19.1, each dot represents the &ldquo;true value&rdquo; for an individual&rsquo;s attitudes
</p>
<p>about a given issue &ndash; but this true value cannot be measured directly, and
</p>
<p>we are left with the four response choices indicating degree of agreement
</p>
<p>or disagreement. Each of the vertical lines marks the point between one
</p>
<p>of the possible response choices and indicates the threshold for each
</p>
<p>response category.
</p>
<p>616</p>
<p/>
</div>
<div class="page"><p/>
<p>The ordinal logistic regression model represents something of a hybrid
</p>
<p>of the binary logistic and multinomial logistic regression models. Similar
</p>
<p>to the multinomial logistic regression model&rsquo;s estimation of multiple
</p>
<p>model intercepts, the ordinal logistic model estimates multiple intercepts
</p>
<p>that represent the values of the thresholds. Comparable to the binary
</p>
<p>logistic model, the ordinal logistic model estimates one coefficient for the
</p>
<p>effect of each independent variable on the dependent variable. In part,
</p>
<p>this is due to the added information contained in an ordinal variable,
</p>
<p>rather than a multi-category nominal variable. The interpretation of the
</p>
<p>results from the ordinal logistic model is also potentially much simpler
</p>
<p>than the results from the multinomial logistic model.
</p>
<p>One of the key differences between the ordinal logistic model and
</p>
<p>other logistic models is that rather than estimating the probability of a sin-
</p>
<p>gle category as in the binary and multinomial logistic models, the ordinal
</p>
<p>logistic model estimates a cumulative probability &ndash; the probability that
</p>
<p>the outcome is equal to or less than the category of interest. In equation
</p>
<p>format:
</p>
<p>Figure 19.1 Hypothetical Ordinal Variable and Underlying Continuum
</p>
<p>617O R D I N A L L O G I S T I C R E G R E S S I O N</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>Cumulative Probability
</p>
<p>( ) ( )P Y m P Y j
j
</p>
<p>m
</p>
<p>1
</p>
<p># = =
</p>
<p>=
</p>
<p>/
</p>
<p>In Equation 19.4, m is the category of interest and can take on values rang-
</p>
<p>ing from 1 to J &minus; 1, while j denotes each individual category. The summa-
</p>
<p>tion sign tells us that we are to add the probabilities for each individual
</p>
<p>outcome from the first category (i.e., Y = 1) to the last category of interest
</p>
<p>(i.e., Y = m). For example, using the four response categories above would
</p>
<p>mean that J = 4, and we could compute a total of J &minus; 1 = 4 &minus; 1 = 3 cumu-
</p>
<p>lative probabilities. If we define a Strongly Agree response as 1 and
</p>
<p>Strongly Disagree response as 4, we could then compute probabilities for
</p>
<p>P(Y &le; 1), P(Y &le; 2), and P(Y &le; 3), representing P(Y &le; Strongly Agree),
</p>
<p>P(Y &le; Agree), and P(Y &le; Disagree), respectively. We would not include the
</p>
<p>final category (i.e., P(Y &le; 4) or P(Y &le; Strongly Disagree)), since it would
</p>
<p>have to be equal to 1 (or 100%) &ndash; all possible values have to fall in one of
</p>
<p>the four response categories.
</p>
<p>Using the cumulative probabilities, we can then compute odds ratios to
</p>
<p>represent the effects of the independent variables on the dependent vari-
</p>
<p>able. Again, there is a slight variation to the construction and interpretation
</p>
<p>of the odds ratios, since we are using cumulative probabilities:
</p>
<p>Odds Ratio Using Cumulative Probabilities
</p>
<p>( )
( )
</p>
<p>( &gt; )
( )
</p>
<p>OR
P Y m
</p>
<p>P Y m
</p>
<p>P Y m
</p>
<p>P Y m
</p>
<p>1m #
# #
</p>
<p>=
-
</p>
<p>=
</p>
<p>Substantively, the odds ratio presented in Equation 19.5 indicates the odds
</p>
<p>of an outcome less than or equal to category m versus the odds of a cat-
</p>
<p>egory greater than m. In the context of our four response choices, the three
</p>
<p>odds ratios that we could make reference to would be the following:
</p>
<p>● Odds of a Strongly Agree response versus the combined outcomes of
</p>
<p>Agree, Disagree, and Strongly Disagree.
● Odds of the combined outcomes of Strongly Agree and Agree response
</p>
<p>versus the combined outcomes of Disagree and Strongly Disagree.
● Odds of the combined outcomes of Strongly Agree, Agree, and Disagree
</p>
<p>versus Strongly Disagree.
</p>
<p>We can take Equation 19.5 for the odds ratio and rewrite it as a linear
</p>
<p>model, similar to that for the binary and multinomial logistic models.
</p>
<p>( &gt; )
( )
</p>
<p>( )expOR
P Y m
</p>
<p>P Y m
Xbm m
</p>
<p>#
= = -x .
</p>
<p>The general form for this equation is very similar to that for either the binary
</p>
<p>or multinomial logistic model, except that we have introduced a new term
</p>
<p>(τ
m
) and now have a negative sign to the left of Xb. The (τ
</p>
<p>m
) represent the
</p>
<p>Equation 19.4
</p>
<p>Equation 19.5
</p>
<p>618</p>
<p/>
</div>
<div class="page"><p/>
<p>threshold parameters, which function as intercepts in the model and will
</p>
<p>take on values for j = 1 to J &minus; 1.
</p>
<p>By taking the natural logarithm of the odds ratio equation, we produce
</p>
<p>the logit Equation 19.6:
</p>
<p>Ordinal Logit Equation
</p>
<p>( &gt; )
( )
</p>
<p>[ ( )]ln ln exp
P Y m
</p>
<p>P Y m
Xb Xbm m
</p>
<p>#
= - = -x xe o .
</p>
<p>Equation 19.6 forms the basis for estimating ordinal logistic regression
</p>
<p>models.
</p>
<p>Interpretation of Ordinal Logistic Regression Coefficients
</p>
<p>In our discussion of the binary logistic regression model, we illustrated
</p>
<p>how a one-unit increase in the independent variable would modify the
</p>
<p>odds of the different outcomes by a factor of exp(b). Since the form of
</p>
<p>the ordinal logistic equation is slightly different, we cannot simply expo-
</p>
<p>nentiate b to obtain the effect of the independent variable.
</p>
<p>Thus, to interpret the effect of a one-unit change in the independent vari-
</p>
<p>able in an ordinal logistic regression model, we will need to exponentiate
</p>
<p>the negative value of the estimated coefficient. We can then interpret the
</p>
<p>coefficient as indicating the odds of an outcome less than or equal to cate-
</p>
<p>gory m versus the odds of a category greater than m.
</p>
<p>S u b s t a n t i v e  E x a m p l e :  S e v e r i t y  o f  P u n i s h m e n t
D e c i s i o n s
</p>
<p>Using the State Court Processing data for California in the 1990s resulted
</p>
<p>in a sample of 8,197 individuals being convicted for some type of crime.
</p>
<p>The primary punishment outcomes &ndash; community-based, jail, and prison &ndash;
</p>
<p>represent a continuum of punishment severity with prison the most severe
</p>
<p>sentence. Again, if we keep our set of independent variables limited by
</p>
<p>using the same variables as in our case disposition example above, we
</p>
<p>have measures of age, sex (males = 1, females = 0), race (non-white = 1,
</p>
<p>white = 0), type of crime (violent =1, non-violent=0), and total number of
</p>
<p>charges as predictors of severity of punishment. Table 19.6 presents the
</p>
<p>results of our ordinal logistic regression model. You will notice that ordi-
</p>
<p>nal regression, like multinomial regression uses the Wald statistic to assess
</p>
<p>the statistical significance of individual parameters.
</p>
<p>S U B S T A N T I V E E X A M P L E : S E V E R I T Y O F P U N I S H M E N T D E C I S I O N S
</p>
<p>Equation 19.6
</p>
<p>619
</p>
<p>To illustrate the modification, suppose we have two values of X: X and
</p>
<p>+ 1. The odds ratio for X and X + 1 would be
</p>
<p>OR X+1
</p>
<p>OR X
=
exp X+1 b
</p>
<p>exp Xb
=exp X X+1 b =m
</p>
<p>m
</p>
<p>m
</p>
<p>m
</p>
<p>( )
( )
</p>
<p>( )( )
( )
</p>
<p>( )&eacute;&euml; &ugrave;&ucirc;( )
t
</p>
<p>t
eexp b .( )
</p>
<p>-
</p>
<p>-
- -
</p>
<p>X </p>
<p/>
</div>
<div class="page"><p/>
<p>Interpreting the Coefficients
</p>
<p>While ordinal regression accounts for the fact that the categories in the
</p>
<p>dependent variable are ranked, for example in our case from less to more
</p>
<p>severe sanctions, the interpretation of the coefficients is similar to that
</p>
<p>used in multinomial regression. In this case, we can compare lower cate-
</p>
<p>gories to the categories ranked above them. For example, in the case of
</p>
<p>sentences, we can compare either community-based punishment to jail
</p>
<p>and prison sentences, or community-based punishment and jail sentences
</p>
<p>to prison sentences. In both these cases, the exponent of the negative of
</p>
<p>the coefficient provides the odds ratio for change. Since age, for example,
</p>
<p>is measured at the interval level of measurement, we would note that
</p>
<p>for a one year increase in age, the odds ratio changes by a factor of
</p>
<p>exp(&minus;.004) = .996, controlling for the other variables in the model. We can
</p>
<p>write out the interpretations as follows:
</p>
<p>● The odds of receiving a community-based punishment versus a jail and
</p>
<p>prison punishment decrease by a factor of .996 for a one year increase
</p>
<p>in age, controlling for all other variables in the model.
● The odds of receiving a community-based and a jail punishment versus
</p>
<p>a prison punishment decrease by a factor of .996 for a one year increase
</p>
<p>in age, controlling for all other variables in the model.
</p>
<p>We see that the coefficient for male is .821. By exponentiating the negative
</p>
<p>of .821 (exp(&minus;.821) = .440), we see that males are likely to receive more
</p>
<p>severe punishments than females, controlling for the other independent
</p>
<p>variables in the model. More concretely, we can state the following about
</p>
<p>the punishment of male offenders:
</p>
<p>● The odds of receiving a community-based punishment versus a jail and
</p>
<p>prison punishment are .440 times smaller for males than for females,
</p>
<p>controlling for all other variables in the model.
● The odds of receiving a community-based and jail punishment versus a
</p>
<p>prison punishment are .440 times smaller for males than for females,
</p>
<p>controlling for all other variables in the model.
</p>
<p>The effect of race on punishment severity is exp(&minus;.166) = .847. Writing out
</p>
<p>direct interpretations of this coefficient leads to the following statements:
</p>
<p>C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>Table 19.6 Ordinal Logistic Regression Results for Severity of Punishment
</p>
<p>INDEPENDENT VARIABLE B SE WALD
</p>
<p>Age .004 .002 4.000
Male .821 .058 202.991
Non-white .166 .043 14.862
Violent Offense .328 .053 38.312
Number of Charges .014 .014 1.000
τ
</p>
<p>1
&minus;.881 .099 79.175
</p>
<p>τ
2
</p>
<p>1.720 .101 291.980
</p>
<p>620</p>
<p/>
</div>
<div class="page"><p/>
<p>S U B S T A N T I V E E X A M P L E : S E V E R I T Y O F P U N I S H M E N T D E C I S I O N S
</p>
<p>● The odds of receiving a community-based punishment versus a jail and
</p>
<p>prison punishment are .847 times smaller for non-whites than for
</p>
<p>whites, controlling for all other variables in the model.
● The odds of receiving a community-based and jail punishment versus a
</p>
<p>prison punishment are .847 times smaller for non-whites than for
</p>
<p>whites, controlling for all other variables in the model.
</p>
<p>The effect of a violent offense charge is exp(&minus;.328) = .720, indicating that
</p>
<p>a violent offense is likely to result in more severe forms of punishment
</p>
<p>(as we would expect):
</p>
<p>● The odds of receiving a community-based punishment versus a jail and
</p>
<p>prison punishment are .720 times smaller for individuals charged with a
</p>
<p>violent offense rather than a non-violent offense, controlling for all
</p>
<p>other variables in the model.
● The odds of receiving a community-based and jail punishment versus a
</p>
<p>prison punishment are .720 times smaller for individuals charged with a
</p>
<p>violent offense rather than a non-violent offense, controlling for all
</p>
<p>other variables in the model.
</p>
<p>Finally, the effect of a one unit increase in the number of charges is exp
</p>
<p>(&minus;.014) = .986. In practice, we would not spend much time interpreting this
</p>
<p>coefficient, since the Wald statistic indicates it is not significantly different
</p>
<p>from 0.6 However, as another illustration for how to interpret coefficients
</p>
<p>from an ordinal logistic regression model, it is useful to write out the inter-
</p>
<p>pretations of this coefficient:
</p>
<p>● The odds of receiving a community-based punishment versus a jail and
</p>
<p>prison punishment decrease by a factor of .986 for a one unit increase
</p>
<p>in the number of charges, controlling for all other variables in the model.
● The odds of receiving a community-based and a jail punishment versus
</p>
<p>a prison punishment decrease by a factor of .986 for a one unit increase
</p>
<p>in the number of charges, controlling for all other variables in the model.
</p>
<p>Note, too, that there are two threshold parameters representing the thresh-
</p>
<p>old points between each of the ordered categories (i.e., community-based
</p>
<p>and jail punishments and then between jail and prison punishments).
</p>
<p>Statistical Significance
</p>
<p>As we noted above, the test of statistical significance for each individual
</p>
<p>coefficient is a Wald statistic that is computed and is interpreted in exactly
</p>
<p>the same way as the Wald statistic for binary logistic and multinomial logistic
</p>
<p>regression models. Table 19.6 reports the values of the Wald statistic for
</p>
<p>6You should verify the statistical significance of each coefficient presented in Table 19.6
using a Wald test statistic with df = 1.
</p>
<p>621</p>
<p/>
</div>
<div class="page"><p/>
<p>622 C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>each independent variable. The statistical significance of the overall model
</p>
<p>is based on a model chi-square statistic that is also computed and inter-
</p>
<p>preted in exactly the same way as for the binary logistic and multinomial
</p>
<p>logistic regression models. In our punishment example, the &minus;2LL
null model
</p>
<p>=
</p>
<p>5883.113 and the &minus;2LL
full model
</p>
<p>= 5601.386, resulting in a model chi&ndash;square
</p>
<p>of 5883.113 &ndash; 5601.386 = 281.727. Since a total of 5 coefficients have been
</p>
<p>estimated (one for each independent variable), the degrees of freedom
</p>
<p>value for this test is equal to 5. Looking at Appendix 2, we see that at a
</p>
<p>significance level of 5%, a chi&ndash;square statistic greater than 11.070 is needed
</p>
<p>to reject the null hypothesis that the model had no effect on punishment
</p>
<p>severity. Since our model chi&ndash;square is larger than the critical value of 
</p>
<p>the chi&ndash;square, we conclude that the overall model had a statistically 
</p>
<p>significant effect on punishment severity.
</p>
<p>Parallel Slopes Tests
</p>
<p>As we noted earlier, the proportional odds model assumes that the effects of the 
</p>
<p>independent variables are constant across all categories of the dependent variable, 
</p>
<p>which is analogous to our interpretation of coefficients in a multivariate linear 
</p>
<p>regression model. Regardless of the level (or category) of the dependent variable, 
</p>
<p>we expect the independent variable to exert a constant (i.e., proportional) effect 
</p>
<p>on the dependent variable. The constant effect of each independent variable should 
</p>
<p>have also been clear in the direct interpretations of the coefficients noted in the pre-
</p>
<p>vious section. This is known more generally as the parallel slopes assumption. 
</p>
<p>Most statistical packages include a score test of this assumption that informs the 
</p>
<p>user of the appropriateness of the ordinal logistic model. Somewhat less common 
</p>
<p>is the Brant test, which tests for parallel slopes in the overall model and in each 
</p>
<p>independent variable.
</p>
<p>Score Test
</p>
<p>Conceptually, the parallel slopes score test is based on the idea that we could esti-
</p>
<p>mate a series of  J &ndash; 1 binary logistic regression models (i.e., one model less than the 
</p>
<p>number of  ordered categories in the dependent variable) of  the form P(Y &le; m) that 
</p>
<p>allowed the effects for all K independent variables to vary by outcome category on 
</p>
<p>the dependent variable. The test would then focus on whether a single coefficient 
</p>
<p>or multiple coefficients best represented the effect of  the independent variables 
</p>
<p>on the dependent variable. Technically, the score test uses information about the 
</p>
<p>log-likelihood for the ordinal logistic regression model and assesses how much it 
</p>
<p>would change by allowing the coefficients for all the independent variables to vary 
</p>
<p>by the outcome category on the dependent variable. The degree of  change in the 
</p>
<p> likelihood function then indicates whether the parallel slopes assumption is met. 
</p>
<p>The null hypothesis of  the score test is parallel (equal) slopes. The research hypoth-
</p>
<p>esis is that the slopes are not parallel (equal). The value of  the score test (reported 
</p>
<p>by most statistical software) is distributed as a chi-square with K( J &ndash; 2) degrees of  
</p>
<p>freedom.</p>
<p/>
</div>
<div class="page"><p/>
<p>S U B S T A N T I V E E X A M P L E : S E V E R I T Y O F P U N I S H M E N T D E C I S I O N S 623
</p>
<p>For our severity of  punishment example, we have K = 5 (i.e., five independent 
</p>
<p>variables) and J = 3 (i.e., three outcome categories on the dependent variable). The cor-
</p>
<p>critical values for the chi-square reported in Appendix 2, the critical chi-square for a 
</p>
<p>significance level of  5% is 11.070. The value of  the score test for our model is 57.890, 
</p>
<p>which indicates that we should reject our null hypothesis of  parallel slopes and con-
</p>
<p>clude that our model does not meet the parallel slopes assumption.
</p>
<p>Brant Test
</p>
<p>Similar to the score test, the Brant test  is a Wald test that assesses whether all the 
</p>
<p>coefficients in a proportional odds model satisfy the parallel slopes assumption. 
</p>
<p>The computation of the Brant test is based on the values of the coefficients and 
</p>
<p>their respective variances. In addition to providing an overall test for the parallel 
</p>
<p>slopes assumption, the Brant test can be decomposed into values for each of the 
</p>
<p>independent variables in the ordinal logistic regression model to test whether each 
</p>
<p>independent variable meets the parallel slopes assumption.
</p>
<p>The Brant test for the overall model is distributed as a chi-square with K( J-2) 
</p>
<p>degrees of freedom (same as in the score test). Each independent variable&rsquo;s test 
</p>
<p>statistic is distributed as a chi-square with J &ndash; 2 degrees of freedom.
</p>
<p>The results of the Brant test for our severity of punishment example appear 
</p>
<p>in Table 19.7. As expected, the overall test again indicates that the parallel slopes 
</p>
<p>assumption is violated for our model. The chi-square is computed as 65.34, and 
</p>
<p>with df = 5 and a critical chi-square of 11.070, we reject the null hypothesis of 
</p>
<p>parallel slopes for the full model. For each independent variable, the critical chi-
</p>
<p>3.841. We see that age (chi-square = 40.25), male (chi-square = 4.72), and violent 
</p>
<p>offense charge (chi-square = 15.82) would lead us to reject the null hypothesis of 
</p>
<p>parallel slopes for each of these variables, since all have chi-square values greater 
</p>
<p>than 3.841. This means that the effects of these three independent variables are 
</p>
<p>not proportional (constant) across the levels of severity of punishment. In contrast, 
</p>
<p>the effects of non-white (chi-square = 2.28) and total number of charges (chi-
</p>
<p>square = 2.14) have chi-square values less than 3.841, leading us to fail to reject the 
</p>
<p>parallel slopes assumption and conclude that the effects of these two independent 
</p>
<p>variables are proportional across the levels of severity of punishment.
</p>
<p> Brant, Rollin. 1990. &ldquo;Assessing proportionality in the proportional odds model for ordinal 
</p>
<p>logistic regression.&rdquo; Biometrics 46: 1171&ndash;1178.
</p>
<p>Brant Test Results for Severity of Punishment
</p>
<p>VARIABLE χ2 df p
</p>
<p>Overall 65.34 5 0.000
Age 40.25 1 0.000
Male  4.72 1 0.030
Non-white  2.28 1 0.131
Violent 15.82 1 0.000
Total number of charges  2.14 1 0.143
</p>
<p>Table 19.7
</p>
<p>7
</p>
<p>7
</p>
<p>responding degrees of  freedom for our score test is equal to 5(3 &ndash; 2) = 5. Based on the </p>
<p/>
</div>
<div class="page"><p/>
<p>624 C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>Partial Proportional Odds
</p>
<p>In much of the research in criminology and criminal justice, it is quite common for 
</p>
<p>the parallel slopes assumption not to be met in practice. Historically, when researchers 
</p>
<p>have been confronted with results from the score test indicating that the model failed 
</p>
<p>to satisfy the parallel slopes assumption, they were left with a choice of fitting the 
</p>
<p>proportional odds model and violating a key assumption of the model or of fitting a 
</p>
<p>multinomial logistic regression model and ignoring the ordinal nature of the dependent 
</p>
<p>variable, complicating the interpretation of the results through the increased number 
</p>
<p>of coefficients. Recently, a class of models referred to as partial proportional odds or 
</p>
<p>generalized ordinal logistic regression models has received increasing attention.  The 
</p>
<p>logic to the partial proportional odds model is to allow some or all of the coefficients 
</p>
<p>of the independent variables to vary by the level of the dependent variable, much like 
</p>
<p>we see in the application of multinomial logistic regression, but to constrain other coef-
</p>
<p>ficients to have a single value, as in the proportional odds model.
</p>
<p>We obtain the partial proportional odds model by generalizing the proportional 
</p>
<p>odds equation from Section 4.1 to allow the coefficients (the b
m
</p>
<p> ) to vary by level 
</p>
<p>of the dependent variable (m):
</p>
<p>Without any further constraints on the coefficients, the total number of coef-
</p>
<p>ficients estimated will be identical to that obtained from a multinomial logistic 
</p>
<p>regression analysis. It is important to note, however, that the coefficients do not 
</p>
<p>mean the same thing. Recall from our discussion above that multinomial logistic 
</p>
<p>regression coefficients refer to comparisons between a given category and the 
</p>
<p>reference category. As noted in the equation above, the logit in the partial propor-
</p>
<p>tional odds model is identical to that in the proportional odds model and refers to 
</p>
<p>the odds of a category less than or equal to m versus a category greater than m.
</p>
<p>Due to the potentially large number of coefficients in a fully generalized ordi-
</p>
<p>nal logit model, most researchers will want to limit the number of variables with 
</p>
<p>nonconstant effects. The results from the Brant test are useful for determining 
</p>
<p>which independent variables, if any, appear to have varying effects on the differ-
</p>
<p>ent categories of the dependent variable (i.e., the slopes are not parallel). If the 
</p>
<p>overall Brant test result is not statistically significant, it implies that the parallel 
</p>
<p>slopes assumption is met for the full model. In this case, there is likely little to be 
</p>
<p>gained by relaxing the parallel slopes assumption for a single variable&mdash;the results 
</p>
<p>become unnecessarily complicated and will not add much statistically to the model. 
</p>
<p> There are a number of sources interested readers can consult, although most of these are much more technical than the 
</p>
<p>material presented in this text. See, for example, Fu, Vincent. 1998. &ldquo;Estimating generalized ordered logit models.&rdquo; Stata 
</p>
<p>Technical Bulletin 8:160&ndash;164. Lall, R., Walters, S.J., Morgan, K., and MRC CFAS Co-operative Institute of Public Health. 
</p>
<p>2002. &ldquo;A review of ordinal regression models applied on health-related quality of life assessments.&rdquo; Statistical Methods 
</p>
<p>in Medical Research 11:49&ndash;67. O&rsquo;Connell, Ann A. 2006. Logistic Regression Models for Ordinal Response Variables. 
</p>
<p>Thousand Oaks, CA: Sage. Peterson, Bercedis and Harrell, Jr, Frank E. 1990. &ldquo;Partial proportional odds models for ordi-
</p>
<p>nal response variables.&rdquo; Applied Statistics 39: 205&ndash;217. Williams, Richard. 2006. &ldquo;Generalized Ordered Logit/ Partial 
</p>
<p>Proportional Odds Models for Ordinal Dependent Variables.&rdquo; The Stata Journal 6(1):58&ndash;82.
</p>
<p>8
</p>
<p>8
</p>
<p>ln ln exp .
P Y
</p>
<p>P Y
Xb Xb
</p>
<p>&pound;( )
&gt;( )
</p>
<p>&aelig;
</p>
<p>&egrave;
&ccedil;&ccedil;
</p>
<p>&ouml;
</p>
<p>&oslash;
&divide;&divide; = ( )&eacute;&euml; &ugrave;&ucirc; =
</p>
<p>m
</p>
<p>m
m m m mt t- -</p>
<p/>
</div>
<div class="page"><p/>
<p>S U B S T A N T I V E E X A M P L E : S E V E R I T Y O F P U N I S H M E N T D E C I S I O N S 625
</p>
<p>In those cases where the overall Brant test result is statistically significant, then 
</p>
<p>the Brant test results for individual variables will point to those variables with the 
</p>
<p>greatest divergence from the parallel slopes assumption and the best candidates for 
</p>
<p>allowing the effects to vary across the different ordinal logits.
</p>
<p>All other features of the partial proportional odds model&mdash;tests for statistical 
</p>
<p>significance, interpretation of the coefficients, and the like&mdash;are the same as found 
</p>
<p>in the proportional odds model.
</p>
<p>Severity of Punishment Example
</p>
<p>In our application of the proportional odds model to the severity of punishment 
</p>
<p>data from California, we also noted that the parallel slopes assumption was not 
</p>
<p>satisfied for the overall model. In particular, the effects of age, male, and violent 
</p>
<p>offense charge violated the parallel slopes assumption, while those of non-white 
</p>
<p>and total number of charges did not (see Table 19.9).
</p>
<p>To illustrate the application and interpretation of the partial proportional odds 
</p>
<p>model, we begin by allowing all five independent variables to have different effects 
</p>
<p>on the two ordinal logits:
</p>
<p>ln
1
</p>
<p>&gt; 1
= ln
</p>
<p>= Probation
</p>
<p> = Jail or Prison
</p>
<p>P Y
</p>
<p>P Y
</p>
<p>P Y
</p>
<p>P Y
</p>
<p>&pound;( )
( )
</p>
<p>&aelig;
</p>
<p>&egrave;
&ccedil;&ccedil;
</p>
<p>&ouml;
</p>
<p>&oslash;
&divide;&divide;
</p>
<p>( )
( )
</p>
<p>&aelig;
</p>
<p>&egrave;&egrave;
&ccedil;
&ccedil;
</p>
<p>&ouml;
</p>
<p>&oslash;
&divide;
&divide;
</p>
<p>= 1 1t X b
</p>
<p>and
</p>
<p>ln
ln Pr
</p>
<p>Pr
</p>
<p>P Y
</p>
<p>P Y
</p>
<p>P Y
</p>
<p>P Y
</p>
<p>&pound;( )
&gt;( )
</p>
<p>&aelig;
</p>
<p>&egrave;
&ccedil;&ccedil;
</p>
<p>&ouml;
</p>
<p>&oslash;
&divide;&divide; =
</p>
<p>=( )
=( )
</p>
<p>&aelig;
</p>
<p>&egrave;
</p>
<p>2
</p>
<p>2
</p>
<p>obation or Jail
</p>
<p>ison
&ccedil;&ccedil;
&ccedil;
</p>
<p>&ouml;
</p>
<p>&oslash;
&divide;
&divide;
= -t 2 2Xb .
</p>
<p>Since there are two different ordinal logits being estimated, there are two full 
</p>
<p>sets of unique coefficients to interpret that illustrate the different effects the inde-
</p>
<p>pendent variables have on the two different ordered logits. These coefficients are 
</p>
<p>presented in Table 19.8.
</p>
<p>Some highlights found in the results are presented in Table 19.8:
</p>
<p>Age:
</p>
<p>Table 19.8 Partial Proportional Odds Model for Severity of Punishment&mdash;All Coefficients 
</p>
<p>Allowed to Vary
</p>
<p>VARIABLE
</p>
<p>PROBATION VS. JAIL  
AND/OR PRISON
</p>
<p>PROBATION AND/
OR JAIL VS. PRISON
</p>
<p>COEFFICIENT (SE) COEFFICIENT (SE)
</p>
<p>Age &minus;0.010(0.003) 0.011(0.003)
Male  0.728(0.073) 0.916(0.074)
Non-white  0.103(0.064) 0.190(0.048)
Violent offense  0.101(0.081) 0.410(0.057)
Total number of charges &minus;0.006(0.020) 0.021(0.015)
Constant (τ) &minus;1.496(0.133) 2.073(0.116)
</p>
<p>-
</p>
<p>&ndash;  The odds of  probation versus jail or prison increase by a factor of  exp 
</p>
<p>other variables in the model. For a ten-year increase in age, the odds of  </p>
<p/>
</div>
<div class="page"><p/>
<p>626 C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p> Steffensmeier, D., Ulmer, J. and Kramer, J. (1998), The Interaction of Race, Gender, and Age in 
</p>
<p>Criminal Sentencing: The Punishment Cost of Being Young, Black, and Male. Criminology, 36: 
</p>
<p>763&ndash;798.
</p>
<p>Partial Proportional Odds Model for Severity of Punishment&mdash; Selected  
</p>
<p>Coefficients Allowed to Vary
</p>
<p>Table 19.9
</p>
<p>9
</p>
<p>Substantively, these results indicate that offenders charged with a violent offense 
</p>
<p>are less likely to be treated leniently in the form of receiving either a probation or 
</p>
<p>a jail sentence and are more likely to receive a prison sentence. The results for age 
</p>
<p>suggest that while older offenders are more likely to receive a probation sentence 
</p>
<p>rather than a jail or a prison sentence, they are less likely to receive a probation or 
</p>
<p>a jail sentence rather than a prison sentence. These results may seem contradictory, 
</p>
<p>but one way of interpreting the pattern is that older offenders are more likely to 
</p>
<p>be sentenced to probation or prison, depending on crime, criminal history, and so 
</p>
<p>on, but less likely to receive jail sentences. This kind of variability in the effect of 
</p>
<p>age on sentencing has also been found in prior research.
</p>
<p>Since the results of the Brant test indicated that there were only three of the 
</p>
<p>five independent variables that did not satisfy the parallel slopes assumption, we 
</p>
<p>have rerun the partial proportional odds model allowing only the effects for age, 
</p>
<p>male, and violent offense charge to vary. Table 19.9 presents these results. Since 
</p>
<p>the results presented here are nearly identical to those presented in Table 19.8 for 
</p>
<p>the coefficients allowed to vary, we limit our discussion here to the effects of male 
</p>
<p>(variable) and non-white (constrained):
</p>
<p>9
</p>
<p>0.010) = 1.105, controlling for all other variables in the model.
</p>
<p>(0.011)) = 0.989 for a one-unit increase in age, controlling for all other vari-
</p>
<p>ables in the model. For a ten-year increase in age, the odds of  probation or 
</p>
<p>controlling for all other variables in the model.
</p>
<p>&#149; Violent offense charge:
</p>
<p>smaller for offenders charged with a violent offense than for offenders charged 
</p>
<p>with a nonviolent offense, controlling for all other variables in the model.
</p>
<p>smaller for offenders charged with a violent offense than for offenders charged 
</p>
<p>with a miscellaneous offense, controlling for all other variables in the model.
</p>
<p>VARIABLE
</p>
<p>CONSTRAINED
PROBATION VS. JAIL  
AND/OR PRISON
</p>
<p>PROBATION AND/
OR JAIL VS. PRISON
</p>
<p>COEFFICIENT (SE) COEFFICIENT (SE) COEFFICIENT (SE)
</p>
<p>Age 0.011 (0.003)
Male 0.916 (0.074)
Non-white 0.164 (0.043)
Violent offense 0.415 (0.057)
Total number of charges 0.013 (0.014)
Constant (τ) 2.043 (0.115)
</p>
<p>&minus;0.010 (0.003)
  0.731 (0.073)
</p>
<p>  0.096 (0.081)
</p>
<p>&minus;1.430 (0.128)</p>
<p/>
</div>
<div class="page"><p/>
<p>627
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>In this chapter, we have examined two different multivariate statistical 
</p>
<p>models to be used when we are confronted with a categorical dependent 
</p>
<p>variable that has more than two categories. When the dependent variable 
</p>
<p>has three or more categories, we can use the multinomial logistic 
</p>
<p>regression model. The multinomial logistic regression model allows for 
</p>
<p>the computation of probabilities and odds ratios that indicate the effects 
</p>
<p>of the independent variables on the relative likelihood of the different 
</p>
<p>outcome categories.
</p>
<p>Since the multinomial logistic regression model estimates a set of coef-
</p>
<p>ficients for each independent variable, we have two issues of statistical 
</p>
<p>significance to assess: the individual coefficients and the overall effect of 
</p>
<p>the independent variable on the dependent variable. For the individual 
</p>
<p>coefficients, we continue to use the Wald statistic, which is distributed as 
</p>
<p>a chi-square statistic with one degree of freedom. For the overall effect of 
</p>
<p>the independent variable on the dependent variable, where we are testing 
</p>
<p>multiple coefficients, we can use the likelihood ratio (LR) test or the Wald 
</p>
<p>statistic. Both test statistics are distributed as a chi-square with J
</p>
<p>of freedom.
</p>
<p>When the dependent variable is measured at the ordinal level of meas-
</p>
<p>urement, we can use the ordinal logistic regression model (or 
</p>
<p>proportional odds model). The ordinal logistic regression model also 
</p>
<p>C H A P T E R S U M M A R Y
</p>
<p>&#149; Male:
</p>
<p>smaller for male offenders than for female offenders, controlling for all oth-
</p>
<p>er variables in the model.
</p>
<p>smaller for male offenders than for female offenders, controlling for all oth-
</p>
<p>er variables in the model.
</p>
<p>&#149; Non-white:
</p>
<p>smaller for non-white offenders than for white offenders, controlling for all 
</p>
<p>other variables in the model.
</p>
<p>smaller for non-white offenders than for white offenders, controlling for all 
</p>
<p>other variables in the model.
</p>
<p>Substantively, these results indicate that male and non-white offenders are less 
</p>
<p>likely to receive more lenient punishments (either probation or jail) and more likely 
</p>
<p>to receive a prison sentence. As noted above, this kind of pattern is consistent with 
</p>
<p>much of the prior research on punishment severity.</p>
<p/>
</div>
<div class="page"><p/>
<p>628 C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>allows for the computation of probabilities and odds ratios, but the focus 
</p>
<p>is on the likelihood of increasing or decreasing categories on the ordered 
</p>
<p>dependent variable. The ordinal logistic model assumes that the effects of 
</p>
<p>the independent variables are constant across the categories of the depend-
</p>
<p>ent variable, which can be tested with the parallel slopes test that is 
</p>
<p>commonly reported in the output of most statistical programs. The parallel 
</p>
<p>slopes test statistic is distributed as a chi-square with K( J
</p>
<p>freedom. The null hypothesis in such a test is that the slopes are parallel, 
</p>
<p>while the research hypothesis is that the slopes are not parallel.
</p>
<p>When there is evidence that the parallel slopes assumption is not satis-
</p>
<p>fied, we can use the partial proportional odds model that allows one 
</p>
<p>or more of the effects of the independent variable to vary across the levels 
</p>
<p>of the ordinal dependent variable. The interpretation of the results and the 
</p>
<p>tests for statistical significance work the same way in the partial proportional 
</p>
<p>odds model as they do in the ordinal logistic regression model.
</p>
<p>K e y  T e r m s
</p>
<p>multinomial logistic regression A statistical 
</p>
<p>technique to predict the value of a dependent vari-
</p>
<p>able with three or more categories measured at the 
</p>
<p>nominal level of measurement.
</p>
<p>ordinal logistic regression (proportional 
</p>
<p>odds model) A statistical technique to predict the 
</p>
<p>value of a dependent variable with three or more cat-
</p>
<p>egories measured at the ordinal level of measurement.
</p>
<p>parallel slopes assumption In an ordinal 
</p>
<p>logistic regression model, the effect of each inde-
</p>
<p>pendent variable is assumed to be constant across 
</p>
<p>all categories of the dependent variable.
</p>
<p>partial proportional odds model An ordi-
</p>
<p>nal logistic regression model that allows the effects 
</p>
<p>of one or more of the independent variables to 
</p>
<p>vary across the levels of the ordinal dependent 
</p>
<p>variable. Useful when the parallel slopes assump-
</p>
<p>tion is violated.
</p>
<p>thresholds Points that mark the limits of the under-
</p>
<p>lying continuum measured by an ordinal variable.
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>To calculate the probability that Y =m:
</p>
<p>exp
</p>
<p>exp
P Y m
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>jj
</p>
<p>J
</p>
<p>m
</p>
<p>1
</p>
<p>= =
</p>
<p>=
/
</p>
<p>_ `
_i j
</p>
<p>i
</p>
<p>To calculate the odds ratio in a multinomial logistic regression model for
</p>
<p>P (Y =m) relative to P (Y =n), given a one-unit change in an independent
</p>
<p>variable:
</p>
<p>exp
</p>
<p>exp
</p>
<p>exp
</p>
<p>exp
</p>
<p>exp
</p>
<p>exp
OR
</p>
<p>P Y n
</p>
<p>P Y m
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>Xb
</p>
<p>Xb
|m n
</p>
<p>jj
</p>
<p>J
</p>
<p>n
</p>
<p>jj
</p>
<p>J
</p>
<p>m
</p>
<p>n
</p>
<p>m
</p>
<p>1
</p>
<p>1
=
</p>
<p>=
</p>
<p>=
= =
</p>
<p>=
</p>
<p>=
</p>
<p>/
</p>
<p>/
_
_
</p>
<p>`
_
</p>
<p>`
_
</p>
<p>_
_
</p>
<p>i
i
</p>
<p>j
i
</p>
<p>j
i
</p>
<p>i
i</p>
<p/>
</div>
<div class="page"><p/>
<p>629
</p>
<p>To calculate the cumulative probability of P (Y &le; m):
</p>
<p>P Y m P Y j
j
</p>
<p>m
</p>
<p>1
</p>
<p># = =
</p>
<p>=
</p>
<p>/_ _i i
To calculate the odds ratio in an ordinal logistic regression model using
</p>
<p>cumulative probabilities:
</p>
<p>&gt;
OR
</p>
<p>P Y m
</p>
<p>P Y m
</p>
<p>P Y m
</p>
<p>P Y m
</p>
<p>1m #
</p>
<p># #
=
</p>
<p>-
=_
</p>
<p>_
_
_
</p>
<p>i
i
</p>
<p>i
i
</p>
<p>Ordinal logit equation:
</p>
<p>&gt;
ln ln exp
</p>
<p>P Y m
</p>
<p>P Y m
Xb Xbm m
</p>
<p>#
= - = -x x
</p>
<p>J
</p>
<p>L
</p>
<p>K
K _
</p>
<p>_ _
N
</p>
<p>P
</p>
<p>O
Oi
</p>
<p>i i8 B
</p>
<p>E x e r c i s e s
</p>
<p>19.1 A large survey of adults asked about violent victimization experi-
</p>
<p>ences. A question of particular interest to one researcher was the
</p>
<p>location of the victimization event &ndash; home, work, or elsewhere.
</p>
<p>She computed a multinomial logistic regression model that pro-
</p>
<p>duced the following results:
</p>
<p>INDEPENDENT VARIABLE HOME V. WORK ELSEWHERE V. WORK
</p>
<p>Age (years) 0.01 0.05
Sex (1=Male, 0=Female) &minus;0.19 0.22
Married (1=Yes, 0=No) 0.37 &minus;0.13
Number of nights out per week for leisure 0.07 0.16
</p>
<p>a. Calculate the odds ratio for each coefficient and explain what
</p>
<p>each odds ratio means.
</p>
<p>b. Calculate the coefficients and the odds ratios for the omitted
</p>
<p>comparison and explain what each odds ratio means.
</p>
<p>19.2 In an attempt to better understand how non-incarcerative punish-
</p>
<p>ments were being used by judges, Blue State funded an evaluation
</p>
<p>study of misdemeanor punishment decisions. The evaluators classi-
</p>
<p>fied non-incarcerative sentences in the following four categories:
</p>
<p>fine, restitution, community service, and electronic monitoring. The
</p>
<p>researchers&rsquo; final analysis produced the following results:
</p>
<p>COMMUNITY 
FINE V. RESTITUTION V. SERVICE V. 
ELECTRONIC ELECTRONIC ELECTRONIC 
</p>
<p>INDEPENDENT VARIABLE MONITORING MONITORING MONITORING
</p>
<p>Any prior criminal record (1=Yes, 0=No) &minus;0.06 &minus;0.07 &minus;0.10
Severity of offense &minus;0.10 &minus;0.12 &minus;0.14
Employed (1=Yes, 0=No) 0.25 0.23 0.36
</p>
<p>E X E R C I S E S</p>
<p/>
</div>
<div class="page"><p/>
<p>630 C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>a. Calculate the odds ratios for the effect of any prior record and
</p>
<p>explain what each odds ratio means.
</p>
<p>b. Calculate the odds ratios for the effect of severity of offense and
</p>
<p>explain what each odds ratio means.
</p>
<p>c. Calculate the odds ratios for the effect of employed and explain
</p>
<p>what each odds ratio means.
</p>
<p>d. Calculate the coefficients and the odds ratios for the comparison
</p>
<p>between Fine and Community Service. Explain what each odds
</p>
<p>ratio means.
</p>
<p>19.3 Criminological theory has attempted to explain both the fre-
</p>
<p>quency of delinquency as well as the type of delinquency an
</p>
<p>individual is likely to commit. A longitudinal study of adolescents
</p>
<p>tested for the effects of several background characteristics on the
</p>
<p>likelihood an individual would commit a drug, property, violent,
</p>
<p>or public order offense. The researchers used a multinomial
</p>
<p>logistic regression model and found the value of the &minus;2 log-
</p>
<p>likelihood for the full model to be 5263.1. The values for the &minus;2
</p>
<p>log-likelihood for each of the independent variables was 
</p>
<p>reported as:
</p>
<p>INDEPENDENT VARIABLE &minus;2 LOG-LIKELIHOOD
</p>
<p>Age 5264.7
Sex 5322.5
Race 5271.1
Grade point average 5267.9
Employment status 5414.6
Parental supervision 5272.3
Number of friends who had been arrested 5459.4
</p>
<p>Calculate the LR Test statistic for each independent variable and
</p>
<p>state whether this variable has a statistically significant effect on
</p>
<p>type of crime (assume α = 0.05).
</p>
<p>19.4 In response to public perceptions that the police in Riverside City
</p>
<p>were too prone to use physical force on suspects, a study was
</p>
<p>commissioned to examine the factors related to when police did
</p>
<p>use physical force. After a year of data collection, the researchers
</p>
<p>classified police use of force into the following three categories:
</p>
<p>regression model of only demographic characteristics produced the
</p>
<p>following results:
</p>
<p>INDEPENDENT VARIABLE B
</p>
<p>Age of officer (years) &minus;0.02
Sex of officer (1 = Male, 0 = Female) 0.18
Race of officer (1 = White, 0 = Non-white) 0.13
Age of suspect (years) &minus;0.03
Sex of suspect (1 = Male, 0 = Female) 0.33
Race of suspect (1 = White, 0 = Non-white) &minus;0.11
</p>
<p>Calculate the odds ratio for each coefficient and explain what each
</p>
<p>odds ratio means.
</p>
<p>None, Mild restraint, and Complete restraint. The ordinal logistic</p>
<p/>
</div>
<div class="page"><p/>
<p>631
</p>
<p>19.5 A survey of adults in the US asked a series of questions about sup-
</p>
<p>port for various policies related to the treatment of criminal
</p>
<p>offenders. One question focused on the level of support for the
</p>
<p>use of the death penalty &ndash; whether the respondent was opposed
</p>
<p>to its use, neutral, or favored its use. An ordinal logistic regression
</p>
<p>model that included age (years), sex (1 = male, 0 = female), race
</p>
<p>(1 = African American, 0 = white), education (number of years
</p>
<p>completed), and degree of political liberalism (1 = low, 10 = High) 
</p>
<p>produced the following results:
</p>
<p>19.6 In a study of community perceptions of the local police department, indi-
</p>
<p>viduals were asked a series of questions about their perceptions of police 
</p>
<p>behavior when interacting with local residents. Of particular interest to 
</p>
<p>the researchers was a question about trust that residents had in the police: 
</p>
<p>&ldquo;Would you say that your level of trust in the police is &hellip;&rdquo; The responses 
</p>
<p>were limited to Very Low, Low, Moderate, High, and Very High. The 
</p>
<p>researchers estimated an ordinal logistic regression model and Brant test 
</p>
<p>and found the following:
</p>
<p>Independent Variable B Brant Test
</p>
<p>Age (in years)  0.02  4.372
</p>
<p>Sex (1 = male, 0 = female) &minus;0.38  8.914
</p>
<p>Race (1 = non-white, 0 = white) &minus;0.42 12.695
</p>
<p>Ever arrested? (1 = yes, 0 = no) &minus;0.67  2.720
</p>
<p>Ever reported a crime to the police? (1 = yes, 0 = no) &minus;0.26  5.661
</p>
<p>Total 34.362
</p>
<p>b. Test the parallel slopes assumption for the full model and each coef-
</p>
<p>recommendation be to the researchers about the use of  the ordinal 
</p>
<p>19.7 A longitudinal study of delinquent and criminal behavior classified a 
</p>
<p>cohort of males (all the same race&ndash;ethnicity) into one of the three cat-
</p>
<p>egories based on patterns of illegal behavior throughout adolescence: 
</p>
<p>Nondelinquent, Low-rate delinquent, and High-rate delinquent. On the 
</p>
<p>basis of Brant test results, the researchers estimated a partial proportional 
</p>
<p>odds model using a small subset of background characteristics to predict 
</p>
<p>delinquent group:
</p>
<p>Independent variable
</p>
<p>Nondelinquent vs. 
low and high rate
</p>
<p>Nondelinquent and/or 
low rate vs. high rate
</p>
<p>Coefficient Coefficient
</p>
<p>Academic performance (1 = low to 10 = high) &minus;0.12 &minus;0.08
</p>
<p>Risk scale (1 = low to 20 = high)  0.23  0.33
</p>
<p>Parent arrested? (1 = yes, 0 = no)  0.47  0.85
</p>
<p>Number of friends arrested  0.17  0.29
</p>
<p>Level of parental supervision (1 = low, 10 = high) &minus;0.09 &minus;0.11
</p>
<p>Interpret and explain these results.
</p>
<p>E X E R C I S E S</p>
<p/>
</div>
<div class="page"><p/>
<p>632 C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>We will rely on two new commands to estimate multinomial logistic regression 
</p>
<p>estimate OLS regression models and binary logistic regression models. The data 
</p>
<p>file used to illustrate the application of the multinomial and ordinal models in 
</p>
<p>this chapter can be found in either SPSS (ca_scps9098.sav) or Stata (ca_scps9098.
</p>
<p>dta) format. The illustration of the commands below assumes that you have 
</p>
<p>opened one of these files into SPSS or Stata and can also be found in the sample 
</p>
<p>syntax files in both SPSS (Chapter_19.sps) and Stata (Chapter_19.do) format.
</p>
<p>SPSS
</p>
<p>Multinomial Logistic Regression
</p>
<p>To estimate a multinomial logistic regression model, you will need to use the 
</p>
<p>NOMREG command:
</p>
<p>As in previous illustrations of SPSS commands, everything can be issued in upper 
</p>
<p>or lowercase, but we have used uppercase lettering to highlight the key components 
</p>
<p>of the command. The /PRINT= option forces SPSS to print all of the model and 
</p>
<p>individual coefficient results. Also take note that the default reference category in 
</p>
<p>SPSS is the category with the highest number (i.e., category value). To force a par-
</p>
<p>ticular category as the reference, use the &ldquo;base&rdquo; option in parentheses.
</p>
<p>To reproduce our results in Table 19.1, enter the following command:
</p>
<p>Similarly, to reproduce the results in Table 19.3, where &ldquo;Trial&rdquo; was used as the 
</p>
<p>reference category, use the following command:
</p>
<p>Note that the only difference between these two commands is changing the base 
</p>
<p>from 1 to 3.
</p>
<p>Ordinal Logistic Regression
</p>
<p>To estimate an ordinal logistic regression model in SPSS, use the PLUM 
</p>
<p> command:
</p>
<p>and ordinal logistic regression models in both SPSS and Stata, but the basic format 
</p>
<p>of each of the new commands is nearly identical to previous commands used to 
</p>
<p>NOMREG depvar (BASE = #) WITH indepvars
</p>
<p>/PRINT = PARAMETER SUMMARY LRT CPS MFI.
</p>
<p>NOMREG casedisp (BASE = 1) WITH age male nonwhite violent
</p>
<p>total_charges
</p>
<p>/PRINT = PARAMETER SUMMARY LRT CPS MFI.
</p>
<p>NOMREG casedisp (BASE = 3) WITH age male nonwhite violent
</p>
<p>total_charges
</p>
<p>/PRINT = PARAMETER SUMMARY LRT CPS MFI.
</p>
<p>PLUM depvar WITH indepvars
</p>
<p>/LINK = LOGIT
</p>
<p>/PRINT = FIT PARAMETER SUMMARY TPARALLEL.</p>
<p/>
</div>
<div class="page"><p/>
<p>633
</p>
<p>Since there are other types of models for ordinal regression, the /LINK= option 
</p>
<p>forces SPSS to estimate an ordinal logistic regression model. The /PRINT= 
</p>
<p>option forces SPSS to generate a full set of output that is consistent with the 
</p>
<p>items we have discussed in this chapter.
</p>
<p>To reproduce the results in Table 19.6, enter the following command:
</p>
<p>At the time of this writing, SPSS does not have the option of computing the 
</p>
<p>Brant test or estimating partial proportional odds models.
</p>
<p>Stata
</p>
<p>Multinomial Logistic Regression
</p>
<p>To estimate a multinomial logistic regression model in Stata, we use the mlogit 
</p>
<p>command:
</p>
<p>where baseoutcome refers to the category that should be used as the reference 
</p>
<p>category. The default in Stata is to use the category with the greatest number of 
</p>
<p>cases.
</p>
<p>To reproduce our results in Table 19.1, enter the following command:
</p>
<p>Note that the baseoutcome(#) option was used to force Stata into using &ldquo;dis-
</p>
<p>missal&rdquo; as the reference category. If this option had been omitted, &ldquo;plea&rdquo; would 
</p>
<p>have been used as the reference category.
</p>
<p>Similarly, to reproduce the results in Table 19.3, the command would be
</p>
<p>Note, too, that the output provided in Stata in regard to statistical significance 
</p>
<p>is a z-score rather than a Wald statistic. This is not problematic, since there is a 
</p>
<p>simple relationship between the Wald and z-score:
</p>
<p>Consequently, if you square the values of the z-scores in the Stata output, it 
</p>
<p>will reproduce the Wald statistics reported in the text (with some rounding error).
</p>
<p>Ordinal Logistic Regression
</p>
<p>To estimate an ordinal logistic regression model in Stata, use the ologit command:
</p>
<p>To reproduce the results in Table 19.6, enter the following command:
</p>
<p>C O M P U T E R E X E R C I S E S
</p>
<p>PLUM ord_punishmentWITH age total_charges nonwhite male
</p>
<p>violent
</p>
<p>/LINK = LOGIT
</p>
<p>/PRINT = FIT PARAMETER SUMMARY TPARALLEL.
</p>
<p>mlogit depvar indepvars, baseoutcome(#)
</p>
<p>mlogit casedisp age male nonwhite violent total_charges, 
</p>
<p>baseoutcome(1)
</p>
<p>mlogit casedisp age male nonwhite violent total_charges, 
</p>
<p>baseoutcome(3)
</p>
<p>ologit depvar indepvars
</p>
<p>ologit ord_punishment age male nonwhite violent total_charges
</p>
<p>Wald=z2.</p>
<p/>
</div>
<div class="page"><p/>
<p>634 C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>The Brant test requires downloading and installing a set of commands written by 
</p>
<p>Long and Freese.  To install this package of user-written commands, enter the 
</p>
<p>following command (one time, only):
</p>
<p>After this command has been run, the Brant test will be available and only 
</p>
<p>requires entering the command brant in the line after running the ologit  
</p>
<p>command. The following commands will reproduce the results for Tables 19.6 
</p>
<p>and then 19.7:
</p>
<p>Partial Proportional Odds
</p>
<p>Before running a partial proportional odds model, we again need to download 
</p>
<p>and install a user-written procedure called gologit2. The process here is the 
</p>
<p>same as before. Enter the following command (one time only):
</p>
<p>This command will install gologit2 from an archive of procedures housed and 
</p>
<p>maintained by Stata. Once this command has been run, the basic structure of the 
</p>
<p>gologit2 command is
</p>
<p> Long, J.S. and J. Freese, 2006, Regression Models for Categorical Dependent Variables 
</p>
<p>Using Stata, 2 ed., College Station, TX: Stata Press.
</p>
<p>This will estimate a fully unconstrained model, where all of the independ-
</p>
<p>ent variables are allowed to have variable effects across the levels of the ordinal 
</p>
<p>dependent variable.
</p>
<p>To estimate a partial proportional odds model that constrains some independ-
</p>
<p>ent variables to have the same effect and allows others to have variable effects, 
</p>
<p>use the pl(constrained_indepvars) option:
</p>
<p>The variable names listed inside the parentheses with the pl option will be 
</p>
<p>constrained. Any other independent variable listed in the gologit2 command 
</p>
<p>line will be allowed to have variable effects.
</p>
<p>To reproduce the results in Table 19.8, enter the following command:
</p>
<p>To reproduce the results in Table 19.9, which constrains the effects of non-white 
</p>
<p>and number of charges, enter the following command:
</p>
<p>10
</p>
<p>10
</p>
<p>net install spost9_ado,
</p>
<p>from(http://www.indiana.edu/~jslsoc/stata)
</p>
<p>ologit ord_punishment age male nonwhite violent total_charges
</p>
<p>brant
</p>
<p>ssc install gologit2
</p>
<p>gologit2 depvar indepvars
</p>
<p>gologit2 depvar indepvars, pl(constrained_indepvars)
</p>
<p>gologit2 ord_punishment age male nonwhite violent  
</p>
<p>total_charges
</p>
<p>gologit2 ord_punishment age male nonwhite violent total_charges, 
</p>
<p>pl(nonwhite total_charges)</p>
<p/>
</div>
<div class="page"><p/>
<p>635
</p>
<p>Problems
</p>
<p>C O M P U T E R E X E R C I S E S
</p>
<p> 1. As a first step in working with these two commands, open either the SPSS 
</p>
<p>(Chapter_19.sps) or the Stata (Chapter_19.do) files that will run all of  
</p>
<p>the commands described above. If  you prefer to work directly with the 
</p>
<p>California data file, then open either the SPSS (ca_scps9098.sav) or the 
</p>
<p>Stata version (ca_scps9098.dta). The syntax and data files contain the fel-
</p>
<p>ony arrest cases used in the examples in this chapter and will allow you to 
</p>
<p>reproduce the results in this chapter. Follow the commands as described 
</p>
<p>above or run the appropriate syntax file.
</p>
<p> 2. Compute a multinomial logistic regression model using employment status 
</p>
<p>(full-time, part-time, and not employed) as the dependent variable. From 
</p>
<p>the remaining variables included in the NYS data file, select at least five 
</p>
<p>variables that you think might have some relationship to an adolescent&rsquo;s 
</p>
<p>employment status. Calculate the odds ratio for each coefficient, and 
</p>
<p>explain what each odds ratio means in plain English.
</p>
<p> 3. Compute an ordinal logistic regression model using the same set of  
</p>
<p>dependent and independent variables that you used in Question 2.
</p>
<p>odds ratio means in plain English.
</p>
<p>b. Test the parallel slopes assumption. If  you have access to Stata, use the 
</p>
<p>c. If  you estimate a different model, report the results, and explain what 
</p>
<p> 4. Select one of  the measures of  delinquency and recode it into three cat-
</p>
<p>egories representing no delinquency (a score of  0), one delinquent act, and 
</p>
<p>two or more delinquent acts. Compute an ordinal logistic regression model 
</p>
<p>using this recoded measure of  delinquency as the dependent variable. 
</p>
<p>From the remaining variables in the data file, select at least five variables 
</p>
<p>that you think might have some relationship with this measure of   
</p>
<p>delinquency.
</p>
<p>ratio means in plain English.
</p>
<p> 
</p>
<p>(Use the Brant test, if  available.)
</p>
<p>c. If  you have access to Stata, estimate a partial proportional odds model 
</p>
<p>using the same dependent and independent variables that takes into  
</p>
<p>account your results from part (b). If  you only have access to SPSS, 
</p>
<p>estimate a multinomial logistic regression model using the same  
</p>
<p>dependent and independent variables. Calculate the odds ratio for each 
</p>
<p>Questions 2 through 4 use the NYS data (nys_1.sav or 
nys_1_student.sav for SPSS and nys_1.dta for Stata).</p>
<p/>
</div>
<div class="page"><p/>
<p>636 C H A P T E R N I N E T E E N : M U L T I V A R I A T E R E G R E S S I O N
</p>
<p>d. How does the substance of  the results from the initial ordinal  
</p>
<p>logistic regression model compare to the substance of  the results from 
</p>
<p>either the partial proportional odds model or the multinomial logistic </p>
<p/>
</div>
<div class="page"><p/>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_20,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>C h a p t e r  t w e n t y
</p>
<p>Multilevel Regression Models
</p>
<p>H o w  d o  w e  a n a l y z e  c l u s t e r e d  o r  m u l t i l e v e l  d a t a ?
</p>
<p>V a r i a n c e  c o m p o n e n t s  m o d e l
</p>
<p>What are Fixed and Random Effects?
</p>
<p>How Do We Interpret the Results from a Variance Components Model?
</p>
<p>What is the Intraclass Correlation and How is it Interpreted?
</p>
<p>R a n d o m  i n t e r c e p t  m o d e l
</p>
<p>How is the Random Intercept Model Different from the Variance Components 
</p>
<p>Model?
</p>
<p>How is Explained Variance Computed and Interpreted?
</p>
<p>What are Between and Within Effects?
</p>
<p>How Do We Test for Differences in Between and Within Effects?
</p>
<p>R a n d o m  c o e f f i c i e n t  m o d e l
</p>
<p>How is the Random Coefficient Model Different from the Random Intercept 
</p>
<p>Model?
</p>
<p>How Do We Test for Differences Between the Random Coefficient Model and 
</p>
<p>the Random Intercept Model?
</p>
<p>A d d i n g  c l u s t e r  ( l e v e l - 2 )  c h a r a c t e r i s t i c s
</p>
<p>How are Cluster (Level-2) Characteristics Added to the Random Coefficient 
</p>
<p>Model?
</p>
<p>How Do We Interpret the Effects of Cluster (Level-2) Characteristics?</p>
<p/>
</div>
<div class="page"><p/>
<p>happens if there are clusters of cases in our data? For example, it is common in 
</p>
<p>community-based surveys to first select a set of neighborhoods from a larger 
</p>
<p>population of neighborhoods. Then, within these neighborhoods, a sample of 
</p>
<p>individuals is selected to respond to the survey questions. A second example may 
</p>
<p>involve researchers collecting data from a survey of youth by first sampling schools 
</p>
<p>and then administering the survey to students within the selected schools, which 
</p>
<p>may be further complicated by selecting a limited number of classrooms within 
</p>
<p>each school. A third example might involve an experiment with multiple treat-
</p>
<p>ment and control sites, but the statistical analysis only distinguishes treatment from 
</p>
<p>control group, ignoring information about site. This kind of design, where there 
</p>
<p>are different levels of sampling, has the potential to allow the researcher to look at 
</p>
<p>the effects of the larger sampling unit (e.g., classroom, neighborhood, school, or 
</p>
<p>research site) on individual responses or outcomes.
</p>
<p>Clustered data is also conceptualized as multilevel data (and hence, the 
</p>
<p>name of the statistical models we discuss in this chapter). In thinking about clus-
</p>
<p>tered data as multilevel data, we would define the cluster&mdash;neighborhood, school, 
</p>
<p>treatment site above&mdash;as the level 2 data. The unique observations&mdash;typically, 
</p>
<p>individual cases&mdash;would be defined as the level 1 data. We are also not limited to 
</p>
<p>thinking of our data as having only two levels and could conceivably work with 
</p>
<p>data that have three or more levels. An example of three-level data might involve 
</p>
<p>a study that begins with a sample of schools (level 3), followed by a sample of 
</p>
<p>classrooms within each of those schools (level 2), and then the individual students 
</p>
<p>within each of the selected classrooms (level 1). Put in terms of clustered data, we 
</p>
<p>have students clustered within classrooms that are clustered within schools. The 
</p>
<p>more general point to describing clustered data as multilevel data is that the lowest 
</p>
<p>level of data will represent the total number of observations in our data&mdash;whatever 
</p>
<p>these observations happen to represent. Each additional level of clustering then 
</p>
<p>reflects a higher level of data.
</p>
<p>Why does the clustering of  data matter? There are both statistical and theoretical 
</p>
<p>reasons for why we may want to pay attention to clustered data. Statistically, obser-
</p>
<p>vations within a cluster will tend to be more similar to each other than to observa-
</p>
<p>638
</p>
<p>hroughout  the  discussion  of  regression  models&mdash;both  OLS  and  logistic&mdash;T
we have assumed a single sample of cases that represents a single sample frame. What </p>
<p/>
</div>
<div class="page"><p/>
<p>tions from different clusters. For example, survey respondents within a neighbor-
</p>
<p>hood will tend to be more alike on key individual characteristics when compared to 
</p>
<p>survey respondents from another neighborhood, regardless of  whether that other 
</p>
<p>neighborhood is across town or across the nation. The increased similarity of  cases 
</p>
<p>within a cluster has consequences for our statistical tests, making it more likely that 
</p>
<p>we will find statistically significant results, since cases within a cluster will tend to 
</p>
<p>exhibit a similar pattern of  association and consequently smaller standard errors.1
</p>
<p>Theoretically, we may also have an interest in the multilevel structure of the 
</p>
<p>data that points to important effects of the cluster on relationships observed at 
</p>
<p>the individual level. For example, how might characteristics of a neighborhood&mdash;
</p>
<p>such as poverty rate or unemployment rate&mdash;affect the relationship we might 
</p>
<p>observe between a respondent&rsquo;s gender and fear of crime? If we find that female 
</p>
<p>respondents express higher levels of fear of crime, then we could ask the ques-
</p>
<p>tion about whether this statistical relationship is the same across neighborhoods. 
</p>
<p>Does the effect of gender on fear of crime change across neighborhood? If the 
</p>
<p>effect is essentially the same across neighborhood, it tells us that neighborhood 
</p>
<p>may be unimportant for understanding fear of crime. In contrast, if we find the 
</p>
<p>effect of gender does vary across neighborhood, we may then want to investigate 
</p>
<p>why the effect varies. Is it due to other characteristics of the neighborhood, such 
</p>
<p>as poverty, unemployment, vacant houses, and the like? Multilevel data are struc-
</p>
<p>tured in such a way that the clustering of cases presents both a challenge and an 
</p>
<p>opportunity to test for the effects of different independent variables measured for 
</p>
<p>different units of analysis.
</p>
<p>In this chapter, we provide a brief introduction to what are known as multilevel 
</p>
<p>models2 that account for the clustering of cases&mdash;the multilevel structure of the 
</p>
<p>data&mdash;and can tell us interesting things about the nature of the statistical relation-
</p>
<p>ships we are studying. We take as a given that there is something informative 
</p>
<p>or interesting about the multilevel structure of the data&mdash;that the clustering of 
</p>
<p>observations is not simply a statistical nuisance to be corrected.  In the discussion 
</p>
<p>that follows, we restrict our attention to the analysis of dependent variables meas-
</p>
<p>ured at the interval or ratio level of measurement. We also limit our discussion to 
</p>
<p>two-level models: we have individual-level data (level 1) nested within one set of 
</p>
<p>clusters (level 2). There is an extensive and growing literature on increasingly more 
</p>
<p>sophisticated multilevel models that account for dependent variables measured 
</p>
<p>at the nominal and the ordinal levels of measurement as well as multilevel data 
</p>
<p>with three or more levels.3 These models are too complex to examine in this brief 
</p>
<p>introductory treatment of multilevel models.
</p>
<p>1If  our concern is primarily in statistically accounting for clustered observations, we can use 
</p>
<p>what are referred to as robust standard errors and is available as an option in most statistical 
</p>
<p>packages. We do not discuss these standard errors in this chapter, but encourage curious read-
</p>
<p>ers to consult Angrist, J.D., &amp; Pischke, J. (2009). Mostly harmless econometrics: An empiri-
</p>
<p>cist&rsquo;s companion. Princeton, NJ: Princeton University Press.
2 These models are also known as mixed models, random effects models, and hierarchical linear 
</p>
<p>models. Since these phrases all take on different meanings across the social and behavioral sci-
</p>
<p>ences, we use multilevel models, since that phrase seems to hold less potential for confusion 
</p>
<p>across disciplinary boundaries.
3 See, for example, Raudenbush, S., &amp; Bryk, A. (2002). Hierarchical linear models: 
</p>
<p>Applications and data analysis methods, 2nd edn. Thousand Oaks, CA: Sage.
</p>
<p>639M U L T I L E V E L  R E G R E S S I O N  M O D E L S</p>
<p/>
</div>
<div class="page"><p/>
<p> 
</p>
<p>V a r i a n c e  C o m p o n e n t s  M o d e l
</p>
<p>We begin our discussion of multilevel models by starting with the simplest case, 
</p>
<p>that of assessing how much cluster (i.e., group) means vary from each other. We 
</p>
<p>find that the most straightforward building block for accomplishing this is the sim-
</p>
<p>ple one-way analysis of variance model. Recall from our discussion of analysis of 
</p>
<p>variance in Chap. 12 that our presentation emphasized how the decomposition of 
</p>
<p>the dependent variable&rsquo;s total variance into two parts&mdash;between-group and within-
</p>
<p>group&mdash;could be used to test whether group had any ability to &ldquo;explain&rdquo; (statisti-
</p>
<p>cally) the total variation in the dependent variable. Fundamentally, we were trying 
</p>
<p>to assess whether the group means were significantly different from each other.
</p>
<p>As a linear statistical model similar to OLS regression, we note that a one-way 
</p>
<p>analysis of variance can be written in equation form analogous to an OLS regres-
</p>
<p>sion equation: 
</p>
<p>Y
ij
 = b
</p>
<p>0
 + b
</p>
<p>j
X
</p>
<p>ij
 + ϵ
</p>
<p>ij
,
</p>
<p>where Y
ij
 is the value of  the dependent variable for individual i in group j, X
</p>
<p>ij
 rep-
</p>
<p>resents the group (j) that an individual (i) belongs to, b
0
 is a mean of  the dependent 
</p>
<p>variable, b
j
 a measure of  the distance between each group and b
</p>
<p>0 ij
 is the indi-
</p>
<p>vidual residual. The specific meaning of  b
0
 and each b
</p>
<p>j
 depends on how the X
</p>
<p>ij
 have 
</p>
<p>been coded, which we explain below. As in the OLS regression model, the error term 
</p>
<p>(
ij
) is assumed to have a normal distribution with a mean of  0 and variance of  
</p>
<p>e
2. 
</p>
<p>There are two primary ways that the X
ij
 are typically coded to estimate this 
</p>
<p>model&mdash;regression coding and contrast coding. Regression coding refers to creat-
</p>
<p>ing a series of dummy variables coded 0&ndash;1 for all groups except one, which is used 
</p>
<p>as the reference category. This procedure is identical to that discussed in Chap. 16 
</p>
<p>on the use of multiple dummy variables to represent a multi-category nominal 
</p>
<p>independent variable in OLS regression analysis. If regression coding is used, we 
</p>
<p>could rewrite the above equation as 
</p>
<p>Y
ij
 = b
</p>
<p>0
 + b
</p>
<p>1
D
</p>
<p>1i
 + b
</p>
<p>2
D
</p>
<p>2i
 + . . . + b
</p>
<p>(j&minus;1)
D
</p>
<p>(j&minus;1)i
 + ϵ
</p>
<p>ij
,
</p>
<p>where each D
j
 represents a dummy variable indicator for up to j j groups 
</p>
<p>(since one group does not have an indicator variable and functions as the refer-
</p>
<p>ence category). In this model, b
0
 represents the mean for the omitted group and 
</p>
<p>each b
j
 measures the difference between the mean for group j and the mean for 
</p>
<p>the omitted group.
</p>
<p>Contrast coding works in a similar way to dummy variable coding, with the 
</p>
<p>difference being the reference category in the regression coding scheme (i.e., the 
</p>
<p>Contrast coding ensures that the sum of all the estimated effects (i.e., the b
j
) is 0, 
</p>
<p>meaning that we can always determine the value for the reference category. We 
</p>
<p>could rewrite the one-way ANOVA equation using contrast coding as 
</p>
<p>Y
ij
 = b
</p>
<p>0
 + b
</p>
<p>1
C
</p>
<p>1i
 + b
</p>
<p>2
C
</p>
<p>2i
 + . . . + b
</p>
<p>(j&minus;1)
C
</p>
<p>(j&minus;1)i
 + ϵ
</p>
<p>ij
, 
</p>
<p> C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S640
</p>
<p>ϵ
</p>
<p>ϵ
</p>
<p>A Regression Approach to Analysis of Variance</p>
<p/>
</div>
<div class="page"><p/>
<p>C
1
 + C
</p>
<p>2
). Suppose that we esti-
</p>
<p>mated this model and found the following:
</p>
<p>b
0
 = 2. 3, C
</p>
<p>1
 = 0. 3, C
</p>
<p>2
 = 0. 6:
</p>
<p>Mean for Treatment 1: 2. 3 + 0. 3 = 2. 6
</p>
<p>Mean for Treatment 2: 2. 3 + 0. 6 = 2. 9
</p>
<p>Several decades of research have shown that defendants who have been released 
</p>
<p>during the pretrial period&mdash;the time between arrest and disposition (conclusion) 
</p>
<p>of a case&mdash;will tend to receive more lenient punishments if convicted. Those 
</p>
<p>defendants who remain in jail during the pretrial period, due to the judge deny-
</p>
<p>ing release altogether or to the judge requiring a bail amount the defendant could 
</p>
<p>not pay, will typically be more likely to go to prison and to receive slightly longer 
</p>
<p>sentences if sentenced to prison. Importantly, these effects hold, even after tak-
</p>
<p>ing into account other characteristics of the defendant, such as prior record and 
</p>
<p>severity of the offense.
</p>
<p>As part of a larger project exploring judicial decision-making in the bail and 
</p>
<p>release decision, John Goldkamp and Michael Gottfredson conducted two stud-
</p>
<p>ies in Philadelphia&mdash;the first a pilot study to examine the factors that influenced 
</p>
<p>the level of bail judges required and the second a test of whether the use of what 
</p>
<p>were called bail guidelines made the decision-making more consistent and equita-
</p>
<p>ble across defendants.  In this chapter, we focus our attention on data from the 
</p>
<p>first study. Goldkamp and Gottfredson selected a random sample of 20 judges to 
</p>
<p>where each C
j
 represents the contrast coded indicator for up to j j 
</p>
<p>groups.  In this model, b
0
 represents the overall sample mean and the b
</p>
<p>j
 measure of 
</p>
<p>the distance between each group mean and the overall sample mean. The distance 
</p>
<p>negative of the sum of the other effects.
</p>
<p>For example, suppose we have an experiment with three conditions: Treatment 
</p>
<p>1, Treatment 2, and Control Group. If we designate the Control Group as the ref-
</p>
<p>erence category, the contrast coding scheme would look like the following:
</p>
<p>C
1
</p>
<p>C
2
</p>
<p>Treatment 1
Treatment 2
Control Group
</p>
<p>1
0
</p>
<p>&minus;1
</p>
<p>0
1
</p>
<p>&minus;1
</p>
<p>Policy guidelines for bail: An experiment in 
</p>
<p>court reform. Phildelphia, PA: Temple University Press.
</p>
<p>641
</p>
<p>A Substantive Example: Bail Decision-Making Study
</p>
<p>One of  the most important decisions in the criminal process is the bail and 
</p>
<p>release decision made by the judge shortly after the arrest of  most individuals. 
</p>
<p>V A R I A N C E  C O M P O N E N T S  M O D E L  
</p>
<p>C
j
 is simply the </p>
<p/>
</div>
<div class="page"><p/>
<p>cases clustered evenly across the 20 judges. Put in the terminology of levels of 
</p>
<p>level 2 data.
</p>
<p>We can consider each judge as a separate experimental condition&mdash;cases were 
</p>
<p>randomly assigned to each judge, ensuring broad similarity of the cases and thereby 
</p>
<p>creating the opportunity to assess how similarly or differently judges would process 
</p>
<p>these cases. Our attention in the example that follows is the bail decision for each 
</p>
<p>case that was indicated by the dollar amount the judge set for the person&rsquo;s release.  
</p>
<p>Our dependent variable is the common logarithm of bail amount. Of the original 
</p>
<p>sample for the following analyses.
</p>
<p>Table 20.1 presents the means and standard deviations for bail amount (in dol-
</p>
<p>lars) and logged bail amount for each of the 20 judges. The average bail amount 
</p>
<p>required by each judge varies considerably. For example, the average bail amount 
</p>
<p>values for logged bail are much smaller and have a more limited range. This is due 
</p>
<p>to the fact that the logarithm used here&mdash;base 10&mdash;reflects the exponent for the 
</p>
<p>number of times 10 would be multiplied by itself to reproduce the bail amount 
</p>
<p>(e.g., log(100) = 2, log(1000) = 3, and so on). Consequently, a difference of 1.0 on 
</p>
<p>the logarithmic scale used here is equivalent to a ten-fold increase in bail amount.
</p>
<p>To help establish a baseline for the multilevel models discussed in the remain-
</p>
<p>der of this chapter, it will be useful to present the results from a one-way ANOVA, 
</p>
<p>where we treat each judge as a type of experimental condition and test for differ-
</p>
<p>ences in mean bail amounts. Table 20.2 presents the results for logged bail amount 
</p>
<p>(since this will be the outcome measure we rely on in subsequent analyses in this 
</p>
<p>chapter). Note that for the coefficients reported in Table 20.2, we have used 
</p>
<p>dummy variable coding, using Judge 1 as the reference category. Consequently, the 
</p>
<p>model intercept represents the mean for Judge 1 and each of the reported coef-
</p>
<p>ficients represents the difference between that judge&rsquo;s mean and that for Judge 1. 
</p>
<p>6 The test of the null hypothesis of equality of means 
</p>
<p>across judge gives us an F-test value of F df
1
 = 19, df
</p>
<p>2
</p>
<p>p &lt; 0. 001. We then conclude that the mean logged bail amount across this sample 
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S642
</p>
<p>of 20 judges is significantly different.
</p>
<p>Fixed and Random Effects
</p>
<p>In the analysis of variance model as we have presented it above, the b
j
 are referred 
</p>
<p>to as fixed effects, meaning that they represent a constant effect of the group 
</p>
<p>for all of the cases within that group. In experimental research, this implies that the 
</p>
<p>treatment received by each individual assigned to a particular  condition is assumed 
</p>
<p> There was a 10 % rule in effect in Philadelphia at the time of  the study, meaning that defend-
</p>
<p>ants would only need to post 10 % of  the dollar amount requested by the judge in order to 
</p>
<p>ensure their freedom during the pretrial period.
6 Due to rounding, some of  the judge means estimated with the coefficients in Table 20.2 will 
</p>
<p>differ at the second decimal when compared to the means reported in Table 20.1.</p>
<p/>
</div>
<div class="page"><p/>
<p> 
</p>
<p>to be the same. However, this assumption ignores the fact that there are often 
</p>
<p>differences in the kind of treatment each case within a particular condition may 
</p>
<p>receive&mdash;known as treatment effect heterogeneity. Although the lead researcher 
</p>
<p>may have designed a protocol that minimizes variations in the treatments received 
</p>
<p>by participants, the requirement for many treatments to be administered by anoth-
</p>
<p>er human being introduces the possibility of differences in multiple administrations 
</p>
<p>of the treatment. For example, an individual police officer may think that he/she is 
</p>
<p>meeting the expectations of the researcher, but events and circumstances unique to 
</p>
<p>that officer, that day, that site, may result in slight differences in how a treatment 
</p>
<p>is administered.
</p>
<p>If we assume that there are no systematic differences in how an individual 
</p>
<p>administers a treatment within a group, then the analysis of variance model can 
</p>
<p>be modified to incorporate random effects. These random effects allow for 
</p>
<p> Table 20.1 Means and Standard Deviations of Bail Amounts by Judge in Philadelphia
</p>
<p>BAIL(DOLLARS) BAIL(LOGGED) BAIL(DOLLARS) BAIL(LOGGED)
</p>
<p>JUDGE MEAN SD MEAN SD JUDGE MEAN SD MEAN SD
</p>
<p>1
2
3
4
5
6
7
8
9
10
</p>
<p>2076.30
4784.88
1901.68
1830.43
2204.58
</p>
<p>17486.78
1842.76
2117.76
1652.86
3627.04
</p>
<p>4513.85
8522.53
3414.13
2808.85
3890.20
</p>
<p>60861.05
3320.16
3583.45
2099.00
</p>
<p>10960.82
</p>
<p>3.04
3.28
3.05
2.97
2.98
3.51
3.00
3.01
2.95
2.96
</p>
<p>0.45
0.56
0.41
0.46
0.50
0.73
0.42
0.47
0.46
0.57
</p>
<p>11
12
13
14
15
16
17
18
19
20
</p>
<p>4576.19
7299.55
3385.96
7945.31
4944.00
8747.40
2184.62
4158.82
1956.30
6246.90
</p>
<p>7170.58
16270.59
6310.05
</p>
<p>24293.52
11726.94
19082.44
3182.93
7765.01
3053.54
</p>
<p>23285.99
</p>
<p>3.38
3.34
3.21
3.32
3.28
3.57
3.11
3.24
3.07
3.23
</p>
<p>0.47
0.63
0.50
0.65
0.51
0.50
0.41
0.51
0.38
0.58
</p>
<p>JUDGE COEFFICIENT (b
J
) STD. ERROR
</p>
<p>2
3
4
5
6
7
8
9
</p>
<p>10
11
12
13
14
15
16
17
18
19
20
Intercept
</p>
<p>0.24
0.01
</p>
<p>&minus;0.08
&minus;0.06
</p>
<p>0.47
&minus;0.05
&minus;0.04
&minus;0.09
&minus;0.08
</p>
<p>0.33
0.30
0.16
0.28
0.24
0.53
0.07
0.19
0.03
0.18
3.04
</p>
<p>0.07
0.06
0.07
0.06
0.06
0.06
0.07
0.06
0.07
0.07
0.07
0.07
0.06
0.06
0.07
0.07
0.07
0.06
0.06
0.04
</p>
<p> Table 20.2 Analysis of Variance of Logged Bail Amount
</p>
<p>643V A R I A N C E  C O M P O N E N T S  M O D E L  </p>
<p/>
</div>
<div class="page"><p/>
<p>variation within a group or condition, which acknowledges that there will be 
</p>
<p> differences in the treatments individuals in each group or condition receive. A par-
</p>
<p>whatever the group represents, as a sample of all possible conditions within the 
</p>
<p>group.
</p>
<p>The random effects model can be written as: 
</p>
<p>Y bij j ij= + +0 z ,
</p>
<p>where Y
ij
, b
</p>
<p>0
, and 
</p>
<p>ij j
 (Greek letter zeta) are the 
</p>
<p>random effects and represent the difference in mean for group j (as sampled) and 
</p>
<p>the overall sample mean b
0 j
</p>
<p> are assumed to have a normal distribution with 
</p>
<p>a mean of 0 and variance of 
z
</p>
<p>2.
</p>
<p>Var( )Y = +s z e
2 2s
</p>
<p>These are what are known as the variance components that can be used to 
</p>
<p>assess whether there is variation in the dependent variable across the group means.
</p>
<p>How do we know when to choose a fixed or random effects model? Of pri-
</p>
<p>mary consideration is whether the effect of the group is viewed as being consistent 
</p>
<p>across all cases within the group or whether the effect of the group represents a 
</p>
<p>sampling of all possible effects of the group. To the extent the effect of the group 
</p>
<p>is viewed as consistent across cases, then a fixed effects model is the optimal 
</p>
<p>choice and we would estimate a standard analysis of variance model. Alternatively, 
</p>
<p>if the effect of the group is expected to vary among cases within that group, then 
</p>
<p>a random effects model is the more appropriate choice.
</p>
<p>From a practical standpoint, there are no firm rules about the sample sizes 
</p>
<p>needed to estimate models with fixed and random effects. The total sample size (N) 
</p>
<p>is used to estimate the fixed effects and much like estimating any linear regression 
</p>
<p>model, relatively modest sample sizes (100&ndash;200 cases) are often adequate. That 
</p>
<p>same guideline holds for multilevel models. Since the random effects are estimated 
</p>
<p>at the level of the cluster, it is unclear just how many clusters are necessary to esti-
</p>
<p>mate a multilevel model, although 10&ndash;20 clusters provide a lower bound .
</p>
<p>Intraclass Correlation and Explained Variance
</p>
<p>Given the two measures of variance&mdash;
z
</p>
<p>2 and 
e
2&mdash;we can compute a measure of 
</p>
<p>explained variance (ρ): 
</p>
<p>r
s
</p>
<p>s s
=
</p>
<p>+
z
</p>
<p>z e
</p>
<p>2
</p>
<p>2 2( )
,
</p>
<p> 
</p>
<p> 
</p>
<p> Rabe-Hesketh, S., &amp; Skrondal, A. (2012). Multilevel and longitudinal modeling using 
</p>
<p>stata,  volume I: Continuous responses, 3rd edn. College Station, TXL: Stata Press.
</p>
<p>  C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S644
</p>
<p>Equation 20.1
</p>
<p>Equation 20.2
</p>
<p>Equation 20.3
</p>
<p>Note that we now have two measures of variance&mdash;
z
</p>
<p>2 and 
e
2&mdash;that reflect 
</p>
<p>variation between groups (
z
2  and within groups (
</p>
<p>e
2) and combined represents the 
</p>
<p>total variation in the dependent variable: 
</p>
<p>)
</p>
<p>allel way of considering random effects is to think of the conditions of the group, 
</p>
<p>ϵ
</p>
<p>ϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>where ρ has values ranging from 0 to 1 and measures the proportion of total 
</p>
<p> variation in the dependent variable that is due to the group. At ρ = 0, the group 
</p>
<p>explains none of the variation in the dependent variable, while at ρ = 1, the group 
</p>
<p>explains all of the variation in the dependent variable.
</p>
<p>An alternative interpretation of  ρ is as the intraclass correlation, which 
</p>
<p>indicates the level of  absolute agreement of  values within each group. By absolute 
</p>
<p>agreement, we&rsquo;re trying to assess the extent to which the values within a group 
</p>
<p>correlation coefficients, we were assessing relative agreement in cases. For the 
</p>
<p>Pearson correlation, it was the relative agreement in values of  two variables, while 
</p>
<p>for the Spearman correlation, it was the relative agreement in ranks of  values of  
</p>
<p>two variables.
</p>
<p>The intraclass correlation provides a measure that can be viewed in two differ-
</p>
<p>ent ways. In part, it provides a measure of intergroup heterogeneity by measuring 
</p>
<p>how much of the total variation is due to the group. At the same time, it provides 
</p>
<p>a measure of within group homogeneity by measuring how similar the values are 
</p>
<p>within each group.
</p>
<p>Statistical Significance
</p>
<p>A natural question to arise in the application of random effects models is whether 
</p>
<p>the random effect&mdash;the estimate of variance 
z
</p>
<p>2&mdash;is statistically significant. 
</p>
<p>Substantively, this is a question about whether allowing for random variation 
</p>
<p>around the overall mean adds anything statistically to the model over and above a 
</p>
<p>fixed effects model.
</p>
<p>To test the statistical significance of 
z
</p>
<p>2, we rely on a likelihood-ratio test, 
</p>
<p>similar to that used in previous chapters. To compute the LR test for the vari-
</p>
<p>ance component, we need two values of the log-likelihood: (1) log-likelihood for 
</p>
<p>the ANOVA model and (2) log-likelihood for the random effects model (REM). 
</p>
<p>The LR test is computed as 
</p>
<p>c 2 2= - -( ( ) ( )).LL LLANOVA REM
</p>
<p>The likelihood-ratio test statistic has a 2 sampling distribution with 1 degree of 
</p>
<p>freedom. We then divide the observed level of statistical significance for the com-
</p>
<p>puted 2 by 2, since it is a test of variances, which can only take on positive values 
</p>
<p>and effectively truncates the sampling distribution to positive values.
</p>
<p>Bail Decision-Making Study
</p>
<p>We return to our example from the Bail Decision-Making Study and present the 
</p>
<p>results for a variance components model in Table 20.3. The model intercept is 
</p>
<p>variance of the groups (
z
</p>
<p>2) is estimated to be 0.031, indicating the degree to which 
</p>
<p>the group (i.e., judge) means vary around the full sample mean. The unexplained 
</p>
<p>error variance (
e
2
</p>
<p>To what extent does the judge making the decision about bail affect the 
</p>
<p>required amount? The intraclass correlation provides an indicator of the influence 
</p>
<p>645V A R I A N C E  C O M P O N E N T S  M O D E L  
</p>
<p>Equation 20.4</p>
<p/>
</div>
<div class="page"><p/>
<p>of the judge and is estimated to be 0.10 for logged bail. The intraclass correlation 
</p>
<p>can also be obtained from the two variance components estimates: 
</p>
<p>r
s
</p>
<p>s s
=
</p>
<p>+
=
</p>
<p>+
=z
</p>
<p>z e
</p>
<p>2
</p>
<p>2 2
</p>
<p>0 03
</p>
<p>0 03 0 27
0 10
</p>
<p>( )
</p>
<p>.
</p>
<p>. .
. .
</p>
<p>The value of the intraclass correlation suggests that the decision-making judge only 
</p>
<p>accounts for about 10 % of the variation in the logged bail amounts.
</p>
<p>In regard to statistical significance, we find that the log-likelihood for the 
</p>
<p>c 2 2 1871 73 1777 35 188 76= - - - =(( . ) ( . )) .
</p>
<p>Based on 1 degree of freedom, we find the critical 2, assuming a p
2 p
</p>
<p>before dividing it by 2, meaning the variance components model represents a sig-
</p>
<p>nificant improvement over the standard one-way ANOVA model. Substantively, 
</p>
<p>these results indicate that the decision-making judge is important to understanding 
</p>
<p>bail amount requested.
</p>
<p>R a n d o m  I n t e r c e p t  M o d e l
</p>
<p>We can extend the basic variance components model to include independent 
</p>
<p>variables to estimate what is known as a random intercept model (RIM). 
</p>
<p>Alternatively, we could start with an OLS regression model and allow the intercept to 
</p>
<p>vary randomly across cluster. Either way, we estimate a model that takes on the form 
</p>
<p>Y b b Xij ij j= + + +0 1 1 z
</p>
<p>where Y and X
1
 represent the dependent and independent variables, respectively, 
</p>
<p>b
0
 and b
</p>
<p>1
 represent the model intercept and the effect of X
</p>
<p>1 j
 represents the 
</p>
<p>ij
 represents the random 
</p>
<p>error term for each individual observation.
</p>
<p>In the random intercept model, the regression coefficients are interpreted in 
</p>
<p>the same way as discussed previously&mdash;a unit change in the independent variable 
</p>
<p>VARIABLE COEFFICIENT se z-SCORE
</p>
<p>Fixed Effect:
Intercept 3.17 0.04 77.73
</p>
<p>Random Effects:
</p>
<p>Intercept (
z
</p>
<p>2)
Error (
</p>
<p>e
2)
</p>
<p>ρ
</p>
<p>0.031
0.270
0.104
</p>
<p> Table 20.3 Variance Components Results for Logged Bail Amount
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S646
</p>
<p>Equation 20.5
</p>
<p>ϵ
</p>
<p>ijϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>   
</p>
<p>is expected to result in a change in the dependent variable equal to b
1
 and the 
</p>
<p>intercept is the expected value of the dependent variable if X
1
 has a value of 0.
</p>
<p>Explained Variance
</p>
<p>With a random intercept model, we have three variations on explained variance 
</p>
<p>that help us to understand the patterns of association in our multilevel data. Recall 
</p>
<p>that the total variance in the dependent variable is Var(Y ) =
z
</p>
<p>2 +
e
2. Following the 
</p>
<p>estimation of a random intercept model, we can compute the explained variance 
</p>
<p>(R2) of the dependent variable with the following equation: 
</p>
<p>R z e z e
</p>
<p>z e
</p>
<p>2 0
2
</p>
<p>0
2
</p>
<p>1
2
</p>
<p>1
2
</p>
<p>0
2
</p>
<p>0
2
</p>
<p>=
+ - +
</p>
<p>+
</p>
<p>( ) ( )
,
</p>
<p>s s s s
s s
</p>
<p>where 
z0
</p>
<p>2 and 
z1
</p>
<p>2 represent the variance of the random effects for the intercept 
</p>
<p>in the variance components model (subscripted with a 0) and the random intercept 
</p>
<p>model (subscripted with a 1), respectively. The error variance (unexplained vari-
</p>
<p>ance) in the variance components model and the random intercept are indicated by 
</p>
<p>e0
2 and 
</p>
<p>e1
2, respectively. Consistent with previous interpretations of R2, a value of 
</p>
<p>0 indicates none of the variance in the dependent variable was explained, while a 
</p>
<p>value of 1 would indicate that all of the variance was explained.
</p>
<p>We can further decompose the total explained variance into each of the two 
</p>
<p>levels of data: (a) explained variance at the level of the cluster (level 2) and (b) 
</p>
<p>unexplained variance at the level of the individual observations (level 1). The 
</p>
<p>explained variance at level 2 is: 
</p>
<p>Rz
z z
</p>
<p>z
</p>
<p>2 0
2
</p>
<p>1
2
</p>
<p>0
2
</p>
<p>=
-s s
</p>
<p>s
.
</p>
<p> The explained variance at level 1 is: 
</p>
<p>Re
e e
</p>
<p>e
</p>
<p>2 0
2
</p>
<p>1
2
</p>
<p>0
2
</p>
<p>=
-s s
s
</p>
<p>.
</p>
<p>What do these level-specific measures of explained variance tell us? The level 2 
</p>
<p>explained variance (R
z
</p>
<p>2) informs us how much of the random variation in cluster 
</p>
<p>means found in the variance components model is due to the individual level char-
</p>
<p>acteristics of the observations (i.e., the set of independent variables we included in 
</p>
<p>the random intercept model). A value of 0 indicates that none of the cluster-level 
</p>
<p>variation was explained by the characteristics of the individuals included in the 
</p>
<p>data. Conversely, a value of 1 indicates that all of the cluster-level variation was due 
</p>
<p>to the characteristics of the individuals included in the data. Generally, the level 2 
</p>
<p>explained variance will tell us how much of the observed cluster-level variance is 
</p>
<p>due to the composition of the clusters.
</p>
<p>The level 1 explained variance (R
e
2) tells us how much of the error (residual) 
</p>
<p>variance&mdash;the &ldquo;unexplained&rdquo; variance&mdash;was reduced by adding in a set of inde-
</p>
<p>pendent variables thought to be related to the dependent variable. A value of 
</p>
<p>0 indicates that none of the error variance was explained by the inclusion of 
</p>
<p>the independent variables, while a value of 1 would indicate that all of the error 
</p>
<p>R A N D O M  I N T E R C E P T  M O D E L  647
</p>
<p>Equation 20.6
</p>
<p>Equation 20.7
</p>
<p>Equation 20.8</p>
<p/>
</div>
<div class="page"><p/>
<p>variance was explained by the independent variables. The explained variance at 
</p>
<p>level 1 is directly analogous to our discussion of R2 in linear regression models in 
</p>
<p>statistically explains the values on the dependent variable.
</p>
<p>Statistical Significance
</p>
<p>The results from a random intercept model will lead to testing the statistical signifi-
</p>
<p>cance of  both the effects of  the independent variables included in the model and 
</p>
<p>the use of  the random intercept model over an OLS linear regression model. In 
</p>
<p>regard to testing for the statistical significance of  the effects of  the independent var-
</p>
<p>iables (e.g., b
1
 in the equation above), we would use the following familiar equation: 
</p>
<p>z
b
</p>
<p>se b
=
</p>
<p>( )
,
</p>
<p>where se(b) is the standard error of the coefficient and z the test statistic assumed 
</p>
<p>to have a normal distribution. Similar to our discussion of testing the statistical 
</p>
<p>and 19, the coefficients in a random intercept model are also expected to be nor-
</p>
<p>mally distributed. The maximum likelihood estimation procedure for the random 
</p>
<p>intercept model estimates standard errors that are adjusted for the clustered nature 
</p>
<p>of the data. Depending on the particular data we are working with, the standard 
</p>
<p>errors for the coefficients in a random intercept model will always be at least as 
</p>
<p>large, but more likely larger, than those estimated in an OLS linear regression 
</p>
<p>model with the same variables, since the clustered nature of the data has been 
</p>
<p>taken into account during the estimation process.
</p>
<p>The test of the random intercept model against the OLS regression model 
</p>
<p>involves the use of the same type of likelihood-ratio test that we used in the test 
</p>
<p>of the variance components model against a one-way ANOVA model. The goal of 
</p>
<p>this test is to assess whether allowing the model intercept to vary randomly across 
</p>
<p>clusters improves the fit of the statistical model. The test is: 
</p>
<p>c 2 2= - -( ( ) ( )).LL LLOLS RIM
</p>
<p>As before, the likelihood-ratio test statistic has a 2 sampling distribution with 1 
</p>
<p>degree of freedom for the one variance estimate of the random effects. We then 
</p>
<p>divide the observed level of statistical significance by 2, since it is a test of vari-
</p>
<p>ances, which can only take on positive values and effectively truncates the sampling 
</p>
<p>distribution to positive values. Substantively, this test will indicate whether the 
</p>
<p>addition of a random intercept to a linear regression model makes a statistically 
</p>
<p>significant contribution.
</p>
<p>Centering Independent Variables
</p>
<p>There are many instances where the interpretation of the model intercept is 
</p>
<p>important to understand the implications of the estimated results. When we are 
</p>
<p>confronted with an independent variable that has no meaningful zero-point, the 
</p>
<p>meaning of the intercept is difficult to explain. One of the straightforward ways of 
</p>
<p>dealing with this issue is to center the independent variable. What do we mean by 
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S648
</p>
<p>Equation 20.9
</p>
<p>Equation 20.10</p>
<p/>
</div>
<div class="page"><p/>
<p>center a variable? In general, centering refers to subtracting a measure of central 
</p>
<p>tendency (e.g., a mean) from each raw score.
</p>
<p>The two types of centering of independent variable that are often most useful 
</p>
<p>for estimating multilevel models are (1) grand or overall mean centering and (2) 
</p>
<p>cluster or group-based centering. In grand-mean centering, we subtract the overall 
</p>
<p>sample mean of the independent variable from each observation 
</p>
<p>X Xij - ..
</p>
<p>Note that the two periods in the subscript of X  indicate that the mean is the same 
</p>
<p>for each individual observation i and group j.
</p>
<p>In cluster-based centering, we subtract the relevant cluster (group) mean from 
</p>
<p>each observation 
</p>
<p>X Xij j- .
</p>
<p>Note that there is a period in place of the i subscript to indicate the value of X j.  is 
</p>
<p>the same for each observation i in a given group j.
</p>
<p>How does the inclusion of a centered variable, instead of the original 
</p>
<p> independent variable, affect the interpretation of the results? The regression coef-
</p>
<p>ficient for the centered value of the independent variable is interpreted in exactly 
</p>
<p>the same way: a unit change in X is expected to result in a change of b units in the 
</p>
<p>dependent variable. Keep in mind that all we have done by centering a variable is 
</p>
<p>shifted its location; its scale is unchanged and so the slope is unchanged (this was 
</p>
<p>The difference in interpretation that comes from using centered variables is in 
</p>
<p>the model&rsquo;s intercept. In the grand-mean centering case, the model intercept now 
</p>
<p>represents the expected mean for a case that has a value equal to the overall mean 
</p>
<p>in the sample for X (which is equal to a value of zero on a grand-mean centered 
</p>
<p>variable). For the cluster-based centering, the model intercept now represents a 
</p>
<p>weighted average of the cluster means.
</p>
<p>The other implication for interpreting the results is focused on the variance 
</p>
<p>component 
z
</p>
<p>2. When grand-mean centering has been used, 
z
</p>
<p>2 represents varia-
</p>
<p>tion in the group means around the overall mean, identical to the case where no 
</p>
<p>centering has been used. With cluster-based centering, 
z
</p>
<p>2 is interpreted as the vari-
</p>
<p>ation of group means around the weighted average of cluster means.
</p>
<p>When should centering be used? In general, centering an independent variable 
</p>
<p>measured at the interval or ratio level of measurement aids in the estimation of 
</p>
<p>multilevel models, particularly some of the more complex models that involve esti-
</p>
<p>mating interaction effects across levels of data (which we discuss below). Centering 
</p>
<p>should not change the substance of the statistical results, since all that centering 
</p>
<p>accomplishes is a shifting of the independent variable so that a value of 0 repre-
</p>
<p>sents either the overall mean for the sample or the cluster mean for each group.
</p>
<p>Dummy variables may also be centered. In this case, the overall or group means 
</p>
<p>simply represent the proportion of cases in the full sample or the cluster that has 
</p>
<p>the characteristic measured by the dummy variable.
</p>
<p>  R A N D O M  I N T E R C E P T  M O D E L  649</p>
<p/>
</div>
<div class="page"><p/>
<p>Bail Decision-Making Study
</p>
<p>To illustrate the application of random intercept models and the use of center-
</p>
<p>ing, we return to the Bail Decision-Making Study. In the interest of keeping the 
</p>
<p>statistical model simple, we include only one independent variable predicting bail 
</p>
<p>amount. Clearly, there are numerous characteristics of defendants and their cases 
</p>
<p>that affect the bail decision. Our goal here is to illustrate how one would go about 
</p>
<p>estimating and interpreting a random intercept model with and without a centered 
</p>
<p>independent variable. We will develop a more complex model in a later section.
</p>
<p>In making assessments about bail, the judge is expected to consider both the 
</p>
<p>chances of the defendant fleeing the community and the potential threat to public 
</p>
<p>safety. One indicator of a defendant&rsquo;s overall risk is the number of prior drug 
</p>
<p>offenses in the person&rsquo;s criminal history record. In general, the greater the evidence 
</p>
<p>of prior drug offending, the higher the perceived risk of some kind of pretrial mis-
</p>
<p>conduct and consequently higher bail amounts being requested from defendants.
</p>
<p>model (RIM) with no centering, RIM with grand mean centering, and RIM with 
</p>
<p>cluster mean centering. The OLS model ignores the clustering of cases by judge 
</p>
<p>and simply reports the effect of number of prior drug offenses on the amount of 
</p>
<p>bail requested. We see from the results in Column 1 that a one-unit increase in the 
</p>
<p>number of prior drug offenses increases logged bail by 0.12 units. Since talking 
</p>
<p>about changes in logged units likely makes little intuitive sense, a more meaningful 
</p>
<p>interpretation of this coefficient is in terms of percentage change in the logged out-
</p>
<p>come variable. Our observed coefficient of 0.12 for number of prior drug offenses 
</p>
<p>can alternatively be interpreted as expecting bail amount to increase by 12 % for 
</p>
<p>each additional prior drug offense.
</p>
<p>respectively) as does the effect of number of prior drug offenses (0.12 v. 0.11, 
</p>
<p>respectively). For the two RIMs with centering, the estimates for the intercept and 
</p>
<p>the effect of number of prior drug offenses are the same through two significant 
</p>
<p>digits (but not beyond). Note that the estimate of the intercept is the overall mean 
</p>
<p> 
</p>
<p> Table 20.4
</p>
<p>  C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S650
</p>
<p>RIM RIM
</p>
<p>VARIABLE OLS RIM GRAND MEAN CENTERING CLUSTER CENTERING
</p>
<p>Fixed Effects:
</p>
<p>Intercept
(se)
(t or z-score)
</p>
<p>2.69
(0.019)
</p>
<p>(141.22)
</p>
<p>2.71
(0.040)
</p>
<p>(67.11)
</p>
<p>3.17
(0.037)
</p>
<p>(85.18)
</p>
<p>3.17
(0.041)
</p>
<p>(77.62)
</p>
<p>Number of Prior
</p>
<p>Drug Offenses
(se)
(t or z-score)
</p>
<p>0.12
(0.004)
</p>
<p>(29.87)
</p>
<p>0.11
(0.004)
</p>
<p>(29.87)
</p>
<p>0.11
(0.004)
</p>
<p>(29.87)
</p>
<p>0.11
(0.004)
</p>
<p>(29.80)
</p>
<p>Random Effects:
</p>
<p>Intercept (
z
</p>
<p>2)
Intercept (
</p>
<p>e
2)
</p>
<p>0.026
0.192
</p>
<p>0.026
0.192
</p>
<p>0.031
0.192
</p>
<p>Regression Results for Logged Bail Amount on Number of Prior Drug Offenses</p>
<p/>
</div>
<div class="page"><p/>
<p>for logged bail amount reported earlier, which is expected under the use of grand 
</p>
<p>mean centering. The use of cluster mean centering estimates a model intercept that 
</p>
<p>is the weighted average of the group means. The reason the estimates for the inter-
</p>
<p>cept are so similar in the two models using different types of centering is an artifact 
</p>
<p>of the study design that used a balanced approach to select the same number of 
</p>
<p>cases for each judge. The small variation in the number of cases per judge used in 
</p>
<p>these analyses accounts for the minor differences that do appear in the intercept 
</p>
<p>and the effect of number of prior drug offenses.
</p>
<p>In regard to the effect of number of prior drug offenses, please note that the 
</p>
<p>effect is the same for all three RIMs, regardless of whether no centering (Column 
</p>
<p>used. This is to be expected, as centering an independent variable only shifts the 
</p>
<p>distribution of cases and does not alter anything else about the values, meaning 
</p>
<p>that the coefficient representing the effect of the independent variable should stay 
</p>
<p>the same.
</p>
<p>As we did with the variance components model, we can test whether the RIM 
</p>
<p>offers a statistically significant improvement over the OLS regression model. This 
</p>
<p>again requires a chi-square test relying on the difference in the log-likelihood 
</p>
<p>values for the OLS model and the RIM. The log-likelihood for the RIM model 
</p>
<p>c 2 2 1513 37 1400 06 226 62= - - - - =(( . ) ( . )) .
 
</p>
<p>Based on 1 degree of freedom, we find the critical 2
</p>
<p>2 of 226.62 has a p
</p>
<p>regardless of whether it is divided by 2. This result indicates that the RIM offers a 
</p>
<p>substantial improvement in the statistical model over the traditional linear regres-
</p>
<p>sion model.
</p>
<p>The explained variance for the RIM is based on variance estimates presented in 
</p>
<p>R2 to be: 
</p>
<p>R 2
0 031 0 270 0 026 0 192
</p>
<p>0 031 0 270
0 276=
</p>
<p>+ - +
+
</p>
<p>=
( . . ) ( . . )
</p>
<p>. .
. .
</p>
<p>This shows that the inclusion of a single independent variable&mdash;number of prior 
</p>
<p>bail.
</p>
<p>When we decompose R2 by level of data, we find the explained variance at level 
</p>
<p>2 (the judge) to be: 
</p>
<p>Rz
2 0 031 0 026
</p>
<p>0 031
0 161=
</p>
<p>&minus;
=
</p>
<p>. .
</p>
<p>.
. .
</p>
<p>Meaning that 16.1 % of the variation across judges is due to the composition of 
</p>
<p>the cases in their courtroom. Put another way, about 16 % of the variation across 
</p>
<p>judges is due to the number of drug offenses that defendants in their courtroom 
</p>
<p>possess prior to the current arrest.
</p>
<p>  R A N D O M  I N T E R C E P T  M O D E L  651</p>
<p/>
</div>
<div class="page"><p/>
<p>Although we have only added a single independent variable, we find the 
</p>
<p>explained variance at level 1 (error variance) to be: 
</p>
<p>Re
2 0 270 0 192
</p>
<p>0 270
0 289=
</p>
<p>&minus;
=
</p>
<p>. .
</p>
<p>.
. .
</p>
<p>What this means is a single independent variable&mdash;number of prior drug 
</p>
<p> offenses&mdash;explains nearly 29 % of the variation in the error variance.
</p>
<p>What we have not yet addressed is how to determine which type of centering 
</p>
<p>to use, or whether to use any centering at all. We turn our attention to answering 
</p>
<p>this question in the next section.
</p>
<p>Between and Within Effects
</p>
<p>In our discussion of centering, we noted that centering variables can assist in 
</p>
<p>estimating multilevel models. What this means is that the algorithms used by vari-
</p>
<p>ous statistical packages to estimate multilevel models perform better when using 
</p>
<p>centered independent variables. Although the explanation for how these algo-
</p>
<p>rithms work goes beyond the focus of our text, we note that statistical  packages 
</p>
<p>that  estimate multilevel models often require multiple iterations to come to a 
</p>
<p> solution&mdash;the estimates of the intercept and the other coefficients.
</p>
<p>We are still left with the question, then, of which method of centering to use. 
</p>
<p>How do we make this determination? One of the issues that naturally arises in the 
</p>
<p>study of clustered or multilevel data is whether the effects of the independent vari-
</p>
<p>ables are the same across group as they are within group. For example, in the analy-
</p>
<p>sis of judicial bail decision-making, we might wonder whether the effect of number 
</p>
<p>of prior drug offenses across judge&mdash;what we will call a between effect&mdash;is the 
</p>
<p>same as the effect of number of drug offenses processed by each judge&mdash;what 
</p>
<p>we will call a within effect. Conceptually, what we are attempting to get at is 
</p>
<p>whether a regression model for each judge (the within regression for each judge) is 
</p>
<p>parallel to a single regression line based on the means for each judge (the between 
</p>
<p>regression for all judges). To the extent the slopes (coefficients) are parallel, there 
</p>
<p>are similar between and within effects, meaning that each judge uses information 
</p>
<p>on number of prior drug offenses in approximately the same way. To the extent 
</p>
<p>the slopes differ, there are different between and within effects, indicating that 
</p>
<p>judges weight information about number of prior drug offenses differently.
</p>
<p>Figures 20.1 and 20.2 present a way of thinking hypothetically about similar 
</p>
<p>and different effects. In each figure, the solid line represents the overall regression 
</p>
<p>slope for the effect of X
1
 on Y . The dashed lines represent the regression lines 
</p>
<p>within each of the five clusters plotted. In Fig. 20.1, the between and within effects 
</p>
<p>are parallel to each other. The different placement of the dashed lines represents 
</p>
<p>j
), where two clusters have positive random 
</p>
<p>effects (and appear above the solid line), and three clusters have negative random 
</p>
<p>effects (and appear below the solid line). In Fig. 20.2, the between and within 
</p>
<p>effects are different&mdash;one slope is positive, one slope is essentially flat, while the 
</p>
<p>remaining three slopes are negative. Although these figures are informative in 
</p>
<p>highlighting similarities and differences in the between and within effects, it is 
</p>
<p>impractical to plot out regression lines for many groups, and will instead rely on a 
</p>
<p>statistical test for differences in the between and within effects.
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S652</p>
<p/>
</div>
<div class="page"><p/>
<p>Testing for Between and Within Effects
</p>
<p>The most direct way of testing for a difference in the between and the within 
</p>
<p>effects is with the addition of the cluster mean for each independent variable 
</p>
<p>already included in the random intercept model. There are two equivalent ways 
</p>
<p>of testing for differences in the between and the within effects&mdash;both include an 
</p>
<p>estimate for the cluster mean, but differ in whether the original raw score or the 
</p>
<p>centered variable is included in the model. The following discussion illustrates the 
</p>
<p>differences and equivalences between the two approaches.
</p>
<p>First, we start with a simple random intercept model that has only a single 
</p>
<p>independent variable: 
</p>
<p>Y b b Xij ij j= + + +0 1 1 z
</p>
<p>We then add the cluster mean for X
1
 and estimate the following equation: 
</p>
<p>Y b b X b Xij a ij b j j= + + + +0 1 1 1 1. z
</p>
<p> Fig. 20.1 Parallel Between and Within Effects
</p>
<p> Fig. 20.2 Different Between and Within Effects
</p>
<p>  R A N D O M  I N T E R C E P T  M O D E L  653
</p>
<p>Y
</p>
<p>X
</p>
<p>Y
</p>
<p>X
</p>
<p>Equation 20.11
</p>
<p>ij .ϵ
</p>
<p>ij .ϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>The coefficient b
1b
</p>
<p> estimates the magnitude of difference in the between and the 
</p>
<p>within effects and captures any possible divergence in slope, such as that portrayed 
</p>
<p>in Fig. 20.2. If b
1b
</p>
<p> is not significantly different from zero, then the between and 
</p>
<p>the within effects are the same, and the slopes for each cluster parallel those of 
</p>
<p>the overall effect. If b
1b
</p>
<p> is significantly different from zero, then the between and 
</p>
<p>within effects diverge and the slopes are not parallel.
</p>
<p>In the alternative, but fully equivalent, approach, we estimate a model that 
</p>
<p>includes the cluster mean centered value for X
1
 and the cluster mean for X
</p>
<p>1
 as two 
</p>
<p>separate independent variables: 
</p>
<p>Y b b X X b Xij a ij j b j j= + - + + +0 1 1 1 1 1( ). . z
</p>
<p>log( ) .BAIL b b DRUGOFF b DRUGOFFij a ij b j j= + + + +0 1 1 1 1 z  
</p>
<p>number of drug offenses, which represents the difference of the within and the 
</p>
<p>between effects. The within effects estimate has a value of b = 0. 11. The estimate 
</p>
<p>of the difference in the two effects has a value of b
</p>
<p>effect is greater than the within effect, which confirms that the between and within 
</p>
<p>effects of number of drug offenses are significantly different from each other. 
</p>
<p>More importantly, in the context of understanding judicial decision-making, these 
</p>
<p>results imply that the 20 judges in this study differentially weight information about 
</p>
<p>number of prior drug offenses.
</p>
<p>If  we were interested in estimating both the within and between effects directly, we 
</p>
<p>could use the second approach described above and estimate the following equation: 
</p>
<p> 
log( ) ( ). .BAIL b b DRUGOFF DRUGOFF b DRUGOFFij a ij j b j j= + - + +0 1 1 1 1 1 z ++  
</p>
<p> 
</p>
<p>VARIABLE ESTIMATE se z-SCORE
</p>
<p>Intercept
Number of Drug Offenses
Number of Drug Offenses
(Cluster means)
</p>
<p>1.29
0.11
0.35
</p>
<p>0.523
0.004
0.128
</p>
<p> 2.47
29.80
 2.72
</p>
<p> Table 20.5 Test of Between and Within Effects Similarity
</p>
<p>  C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S654
</p>
<p>In this model, the coefficient b
1a
</p>
<p> directly estimates the within effect and the coef-
</p>
<p>ficient b
1b
</p>
<p> directly estimates the between effect of X
1
. To obtain the difference in 
</p>
<p>between and within effects, we would simply subtract b
1a
</p>
<p> from b
1b
</p>
<p>.
</p>
<p>Bail Decision-Making Study
</p>
<p>In the Bail Decision-Making Study, one of the key areas of attention was a ques-
</p>
<p>tion about whether judges weighted information about defendants in similar or 
</p>
<p>different ways. A direct test of this is provided by a test for similarity of between 
</p>
<p>and within effects. If we continue the example started previously using number of 
</p>
<p>drug offenses (DRUGOFF ) as the independent variable, we estimate the following 
</p>
<p>model: 
</p>
<p>Equation 20.12
</p>
<p>ij .ϵ
</p>
<p>.ijϵ
</p>
<p>ij .ϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 20.6 presents the results from this analysis.
</p>
<p>As expected, given the previous set of  results, the between effect of  number 
</p>
<p>of  drug offenses is greater than the within effect of  number of  drug offenses, 
</p>
<p>again confirming that these 20 judges differentially used information about 
</p>
<p>drug offending when making bail decisions. There are two additional findings in 
</p>
<p>Table 20.6 worth noting. First, the effect of  cluster deviations is the same as the 
</p>
<p>since this is just the effect for the cluster-mean centered number of  drug offenses. 
</p>
<p>which is the estimate for the difference obtained directly using the first method 
</p>
<p>R a n d o m  C o e f f i c i e n t  M o d e l
</p>
<p>A straightforward extension of the random intercept model involves thinking 
</p>
<p>about the effects of one or more of the independent variables in a multilevel model 
</p>
<p>also varying across cluster. Put another way, we may have justification, based on 
</p>
<p>prior research and theory, to expect the slope coefficients for a key variable to vary 
</p>
<p>across cluster. For example, in a study of fear of crime across neighborhoods, we 
</p>
<p>might expect the effect of gender to vary by neighborhood. Similarly, in our study 
</p>
<p>of judicial decision-making, we might expect judges to weight information about 
</p>
<p>cases and defendants differently, suggesting that we will find different slopes for 
</p>
<p>key predictors of the outcome variable.
</p>
<p>The development of  the random coefficient model (RCM) begins with 
</p>
<p>the random intercept model (here we have included only a single independent  
</p>
<p>variable X
1
): 
</p>
<p>Y b b Xij ij j= + + +0 1 1 0z
</p>
<p>where all terms are defined as above, except that there is now a 0 included in the 
</p>
<p>b
0
). 
</p>
<p>For a random coefficient model, we add a random effect for the slope coefficient 
</p>
<p>in question. In our example, to estimate a model in which b
1
 is allowed to vary 
</p>
<p>1j
: 
</p>
<p>Y b b X Xij ij j j ij= + + + +0 1 1 0 1 1z z
</p>
<p>VARIABLE ESTIMATE se z-SCORE
</p>
<p>Intercept
Number of Drug Offenses
(Cluster deviations)
Number of Drug Offenses
(Cluster means)
</p>
<p>1.29
0.11
</p>
<p>0.46
</p>
<p>0.523
0.004
</p>
<p>0.128
</p>
<p> 2.47
29.80
</p>
<p> 3.61
</p>
<p> Table 20.6 Test of Between and Within Effects Similarity
</p>
<p>R A N D O M  C O E F F I C I E N T  M O D E L  655
</p>
<p>Equation 20.13ij ,ϵ
</p>
<p>ij ,ϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S656
</p>
<p>We can rewrite this equation to more directly link the random effects with the 
</p>
<p>proper slope coefficient: 
</p>
<p>Y b b X X
</p>
<p>b b X
</p>
<p>ij j ij j ij
</p>
<p>j j ij
</p>
<p>= + + + +
</p>
<p>= + + + +
</p>
<p>( ) ( )
</p>
<p>( ) ( )
</p>
<p>0 0 1 1 1 1
</p>
<p>0 0 1 1 1
</p>
<p>z z
</p>
<p>z z
 
</p>
<p>In this model, we estimate fixed effects for the model intercept (b
0
) and the slope 
</p>
<p>coefficient for X
1
 (b
</p>
<p>1
) and simultaneously estimate random effects for both the 
</p>
<p>0j 1j
).
</p>
<p>Conceptually, what the random coefficient model does is analogous to estimat-
</p>
<p>ing a regression model for each cluster and then examining whether the intercepts 
</p>
<p>and slope coefficients vary in any meaningful way across the clusters. We now turn 
</p>
<p>to a more formal examination of variance estimates from the random coefficient 
</p>
<p>model.
</p>
<p>Variance Estimates
</p>
<p>Similar to the variance components and random intercept models, the level 1 error 
</p>
<p>variance continues to be represented by 
e
2. The variance of the random effects 
</p>
<p>Variance of the intercept across cluster: 
z00
2
</p>
<p>Variance of the slope coefficient across cluster: 
z11
2
</p>
<p>We are now confronted with another choice in regard to the estimation of  the 
</p>
<p>estimate the covariance of  the random effects for the intercept and the slope, 
</p>
<p>which we can label either 
z01
2  or 
</p>
<p>z10
2 . What does this estimate of  the covari-
</p>
<p>0 1
 assess? In general, it will indicate whether the magnitude of  
</p>
<p>the random effect for the intercept covaries with the magnitude of  the random 
</p>
<p>effect for the slope coefficient. More directly, a positive value of 2
z01
</p>
<p> indicates that 
</p>
<p>clusters with larger intercepts will tend to have larger values for the slope coeffi-
</p>
<p>cient. Conversely, a negative covariance would suggest that smaller values of  the 
</p>
<p>intercept are associated with larger values of  the slope coefficient, and vice versa.
</p>
<p>It is important to note that we cannot make direct comparisons of the variance 
</p>
<p>components 2
z00
</p>
<p>, 2
z11
</p>
<p>, and 2
z01
</p>
<p>. Since the variance estimates reflect the different 
</p>
<p>metrics of the variables being analyzed, simply by changing the scale of one or 
</p>
<p>another variable, we could drastically alter the variance or covariance estimate. For 
</p>
<p>example, by expanding the scale of a variable, say from (0,1) to (0,10), it would 
</p>
<p>inflate the values of each variance estimate without changing the substantive inter-
</p>
<p>pretation of the results.
</p>
<p>Note on Explained Variance
</p>
<p>In contrast to the variance components model and the random intercept model, 
</p>
<p>we are no longer able to compute estimates of explained variance in the random 
</p>
<p>coefficient model. The reason for this is the distribution of the residuals is heter-
</p>
<p>Specifically, the total residual variance is now proportional to the value of the 
</p>
<p>independent variable with the random effect. For example, if we use the equation 
</p>
<p>ijϵ
</p>
<p>ijj .ϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>0j
), 
</p>
<p>ij
), and the product of X
</p>
<p>1ij
 and the random effect 
</p>
<p>for X
1 1j
</p>
<p>): 
</p>
<p>z z0 1 1j j ijX+ +  
</p>
<p>It is the product of X
1ij 1j
</p>
<p> that creates the heteroskedastic error variance, since 
</p>
<p>the total residual will depend on the magnitude of X
1ij
</p>
<p>. In short, as X
1ij
</p>
<p> increases or 
</p>
<p>decreases, the magnitude of the residual will change.
</p>
<p>Bail Decision-Making Study
</p>
<p>To gain an appreciation of the random coefficient model, we begin our analysis of 
</p>
<p>the bail decision-making data by estimating regression equations for each of the 
</p>
<p>20 judges included in the sample. We continue to rely on the same simple model 
</p>
<p>of logged bail as the dependent variable and number of prior drug offenses as the 
</p>
<p>only independent variable in our model: 
</p>
<p>log( ) ( )BAIL b b DRUGOFFij j j ij= + +0 1 1  
</p>
<p>Note from the subscripts to the intercept (b
0
) and the slope coefficient (b
</p>
<p>1
) that 
</p>
<p>there will be unique estimates for each value across the judges/clusters (j). Rather 
</p>
<p>than present the results from 20 regression analyses in a table, the results are pre-
</p>
<p>sented graphically in Fig. 20.3, where each line represents the regression line for a 
</p>
<p>single judge. Clearly, the results show variation in the intercept across judge&mdash;note 
</p>
<p>the vertical placement of each regression line that reflects larger or smaller values 
</p>
<p>of the judge-specific intercept. We also note that there is variation in the regres-
</p>
<p>sion slopes across the 20 judges&mdash;some of the slopes are steeper, some are flatter.
</p>
<p>One way of starting to assess how similar or different the intercepts and 
</p>
<p>slope coefficients are for each judge can be viewed in a scatterplot of the inter-
</p>
<p>graph represents one of the 20 judges included in the analysis. The spread of cases 
</p>
<p> Fig. 20.3 Judge-specific Regression Lines
</p>
<p>R A N D O M  C O E F F I C I E N T  M O D E L  657
</p>
<p>2
.5
</p>
<p>3
3
.5
</p>
<p>4
4
.5
</p>
<p>5
</p>
<p>L
o
</p>
<p>g
(B
</p>
<p>a
il)
</p>
<p>0 5 10 15 20
</p>
<p>Number of Prior Drug Offenses
</p>
<p>ij .ϵ
</p>
<p>.ijϵ</p>
<p/>
</div>
<div class="page"><p/>
<p> Fig. 20.4 Scatterplot of Judge-specific Intercepts and Slope Coefficients
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S658
</p>
<p>.0
5
</p>
<p>.1
.1
</p>
<p>5
.2
</p>
<p>S
lo
</p>
<p>p
e
</p>
<p>2.4 2.6 2.8 3
</p>
<p>Intercept
</p>
<p>across both axes confirms what we viewed in Fig. 20.3&mdash;there is variation in the 
</p>
<p>a negative association: larger values of the intercept tend to have smaller slope 
</p>
<p>coefficients, while smaller values of the intercept tend to have larger slope coef-
</p>
<p>-
</p>
<p>tern suggests that judges with large intercepts&mdash;their cases receive relatively larger 
</p>
<p>bail amounts on average&mdash;tend to place less weight on the defendant&rsquo;s number 
</p>
<p>of prior drug offenses. In contrast, for judges with smaller intercepts&mdash;their cases 
</p>
<p>tend to receive relatively lower bail amounts on average&mdash;place greater weight on 
</p>
<p>the defendant&rsquo;s number of prior drug offenses. 
</p>
<p>The use of the random coefficient model offers a more efficient way of assess-
</p>
<p>ing the similarities and differences across the judges, but most importantly, will 
</p>
<p>allow us to determine whether the variations in the traditional regression model 
</p>
<p>are statistically meaningful or reflect random variation in the values for each judge. 
</p>
<p>log( ) ( + )+( + X )+
</p>
<p>( + )+(
</p>
<p>0 0 1 1 1 1
</p>
<p>0 0
</p>
<p>Bail b b DRUGOFF
</p>
<p>b
</p>
<p>ij j ij j ij
</p>
<p>j
</p>
<p>=
</p>
<p>=
</p>
<p>z z
</p>
<p>z bb DRUGOFFj ij1 1 1+ ) +z  
</p>
<p>to provide a ready point of comparison. Column 2 displays the results for the ran-
</p>
<p>results using grand mean centering and cluster mean centering, respectively. The 
</p>
<p>estimates of the intercept and the slope coefficient in each of the columns are iden-
</p>
<p>( 2
z11
</p>
<p>) and the covariance of the intercept and slope random effects ( 2
z01
</p>
<p>). As 
</p>
<p>before, we can test whether the addition of these random effects represents an 
</p>
<p>improvement in the statistical model over the previous model.
</p>
<p>Recall from above that the test of statistical significance for the addition of a 
</p>
<p>random effect is a chi-square test that compares the log-likelihood values from two 
</p>
<p>ij
</p>
<p>ij .
</p>
<p>ϵ
</p>
<p>ϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>different models. In the present case, we can make two comparisons: (1) RIM v. 
</p>
<p>RCM with random effect for slope coefficient and (2) RCM with random effects 
</p>
<p>for the intercept and the slope v. RCM with the additional covariance of the ran-
</p>
<p>dom effects. The first comparison assesses whether the basic RCM that adds a 
</p>
<p>random effect for number of drug offenses is an improvement over the RIM. The 
</p>
<p>second comparison tests whether the addition of the covariance of the random 
</p>
<p>effects adds to the model&rsquo;s improvement over and above the basic RCM.
</p>
<p>c 2 2 1391 74 1391 67 0 14= - - - =(( . ) ( . )) .
 
</p>
<p>Based on 1 degree of freedom, we find the critical 2, assuming a p
2 p
</p>
<p>of whether it is divided by 2. This result indicates that the RCM with the covari-
</p>
<p>ance of the random effects does not improve the statistical model and could be 
</p>
<p>dropped from the analysis.
</p>
<p>What do the results of these two comparisons mean? By finding the RCM 
</p>
<p>makes a significant improvement in the statistical model, we know that the 
</p>
<p> Table 20.7
</p>
<p>R A N D O M  C O E F F I C I E N T  M O D E L  659
</p>
<p>Regression Results for Logged Bail Amount on Number of Prior Drug Offenses
</p>
<p>RIM RIM
</p>
<p>VARIABLE OLS RIM GRAND MEAN CENTERING CLUSTER CENTERING
</p>
<p>Fixed Effects:
</p>
<p>Intercept
(se)
(t or z-score)
</p>
<p>2.69
(0.019)
</p>
<p>(141.22)
</p>
<p>2.71
(0.037)
</p>
<p>(72.95)
</p>
<p>3.17
(0.037)
</p>
<p>(85.54)
</p>
<p>3.17
(0.041)
</p>
<p>(77.72)
</p>
<p>Number of Prior
</p>
<p>Drug Offenses
(se)
(t or z-score)
</p>
<p>0.12
(0.004)
</p>
<p>(29.87)
</p>
<p>0.11
(0.006)
</p>
<p>(18.37)
</p>
<p>0.11
(0.006)
</p>
<p>(18.37)
</p>
<p>0.11
(0.006)
</p>
<p>(18.27)
</p>
<p>Random Effects:
</p>
<p>Intercept ( 2
z00
</p>
<p>)
Drug Offenses ( 2
</p>
<p>z11
)
</p>
<p>Covariance ( 2
z01
</p>
<p>)
</p>
<p>0.0209
0.0005
</p>
<p>&minus;0.0004
</p>
<p>0.0258
0.0005
0.0016
</p>
<p>0.0317
0.0005
0.0015
</p>
<p>Comparison 1:
</p>
<p>c 2 2 1400 06 1391 74 16 64= - - - =(( . ) ( . )) .
  
</p>
<p>Based on 1 degree of freedom, we find the critical 2, assuming a p
2 p
</p>
<p>before dividing it by 2. This result indicates that the RCM with a random effect 
</p>
<p>for the slope coefficient (number of prior drug offenses) represents a significant 
</p>
<p>improvement in the statistical model. 
</p>
<p>Comparison 2:</p>
<p/>
</div>
<div class="page"><p/>
<p> intercepts and the effects of number of prior drug offenses vary across the 20 
</p>
<p>judges in making bail decisions. The finding that including the covariances of the 
</p>
<p>random effects does not improve the model means the judge-specific intercepts 
</p>
<p>and coefficients for number of prior drug offenses are not correlated with each 
</p>
<p>between judge-specific intercepts and coefficients, there was no statistical evidence 
</p>
<p>of such a relationship once we more formally tested the model.
</p>
<p>A d d i n g  C l u s t e r  ( L e v e l  2 )  C h a r a c t e r i s t i c s
</p>
<p>Thus far in our discussion of multilevel models, we have focused strictly on char-
</p>
<p>acteristics of the individual observations in the data&mdash;the level 1 characteristics. 
</p>
<p>One of the great strengths of multilevel models is the ability to include cluster-level 
</p>
<p>characteristics that will indicate how the effects of the independent variables may 
</p>
<p>vary across levels of a cluster characteristic. In the example we have used thus far 
</p>
<p>regarding judges and bail decision-making, we might hypothesize that characteris-
</p>
<p>tics of judges would affect how each would weigh information about defendants in 
</p>
<p>making bail decisions. For example, gender of judge may alter the relationship that 
</p>
<p>we have observed between number of prior drug offenses and bail amount. Or, 
</p>
<p>years of service as a judge may affect the observed relationship between number 
</p>
<p>of prior drug offenses and bail amount. These are the kinds of questions to which 
</p>
<p>we now turn.
</p>
<p>When considering adding cluster-level characteristics to a multilevel analy-
</p>
<p>sis, the researcher is confronted with two important questions about a cluster 
</p>
<p> characteristic:
</p>
<p> 1. Is there an expectation that the cluster characteristic will directly affect the 
dependent variable?
</p>
<p> 2. Is there an expectation that the effect of  an independent variable will vary by 
</p>
<p>the level of  the cluster characteristic?
</p>
<p>Both of these questions force us to consider prior theory and research in thought-
</p>
<p>fully developing our multilevel model. The first question is the more straightfor-
</p>
<p>ward of the two questions and will often be a reflection of prior research showing 
</p>
<p>that the cluster characteristic is likely important to the dependent variable being 
</p>
<p>analyzed. For example, there is research indicating that gender of judge affects 
</p>
<p>how criminal defendants and offenders are treated. We would have justification for 
</p>
<p>hypothesizing that gender of judge would affect bail amount. Similarly, if we were 
</p>
<p>studying fear of crime across a large sample of different neighborhoods, we would 
</p>
<p>have justification for hypothesizing that official crime rates in the neighborhoods 
</p>
<p>may have a direct affect on an individual&rsquo;s fear of crime. 
</p>
<p>The second question requires considerable care in developing the model, 
</p>
<p>especially since there is likely to be less evidence and/or theory on which to base 
</p>
<p>a hypothesis of an independent variable&rsquo;s (level 1) effect varying by the level of 
</p>
<p>the cluster characteristic. In the literature on multilevel models, this kind of rela-
</p>
<p>tionship is often referred to as a cross-level interaction, since they imply the 
</p>
<p>effect of one variable (the level 1 independent variable) changes across the levels of 
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S660</p>
<p/>
</div>
<div class="page"><p/>
<p>another variable (the level 2 cluster characteristic). For example, in considering fear 
</p>
<p>of crime, we may hypothesize that official crime rates may interact with the effect 
</p>
<p>of age of a resident on fear of crime. Older individuals are more fearful of being 
</p>
<p>crime victims in general, and we could hypothesize that as the neighborhood crime 
</p>
<p>rate increased, there was a multiplier effect on the fear of crime among elderly 
</p>
<p>residents. At the same time, neighborhood crime rates may not affect the level of 
</p>
<p>fear of younger individuals.
</p>
<p>Although the interpretation of cluster-level characteristics in a multilevel model 
</p>
<p>may become complicated, their inclusion in the statistical model is not compli-
</p>
<p>cated. For the situation where we expect the cluster characteristic to have a direct 
</p>
<p>effect on the dependent variable, we simply include it as an additional independent 
</p>
<p>variable (denoted with a W in the following discussion) in our random intercept or 
</p>
<p>random coefficient model. In the form of a simple random coefficient model, we 
</p>
<p>would estimate the following model: 
</p>
<p>Y b b X bW Xij ij j j j ij= + + + + +0 1 1 3 1 0 1 1z z  
</p>
<p>Note that our cluster characteristic W
1j
 has only a j subscript, indicating that the 
</p>
<p>values of W
1
 vary by cluster j but will be the same for all cases within that cluster. 
</p>
<p>Like any other variable we might include in a linear regression model, the cluster 
</p>
<p>characteristic can be a dummy variable or an interval level variable. The interpre-
</p>
<p>tation of the cluster characteristic&rsquo;s effect (b
3
) is no different than that for other 
</p>
<p>independent variables included in the model: a unit change in W
1
 is expected to 
</p>
<p>change the value of the dependent variable by b
3
.
</p>
<p>If we expect the effect of one of our level 1 independent variables to vary by 
</p>
<p>level of a cluster characteristic, we include an interaction term between the two 
</p>
<p>variables&mdash;our cross-level interaction&mdash;and add it to the model. Continuing the 
</p>
<p>same random coefficient model from above, we would include an interaction 
</p>
<p>between X
1
 and W
</p>
<p>1
: 
</p>
<p>Y b b X bW b X W Xij ij j ij j j j ij= + + + + + +0 1 1 3 1 4 1 1 0 1 1z z  
</p>
<p>How do you interpret these results and make sense of the interaction effect? In 
</p>
<p>-
</p>
<p>ferent ways: we fix the value of one variable and assess the effect of the second 
</p>
<p>variable. In a multilevel model, we will always fix the level (value) of the cluster 
</p>
<p>characteristic first and then interpret the effect of the level 1 independent variable. 
</p>
<p>In the equation above, the effect of X
1
 can be written as 
</p>
<p>b X b X Wij ij j1 1 4 1 1+ .  
</p>
<p>If W
1
 is a dummy variable, then for W
</p>
<p>1
 = 0, we have 
</p>
<p>b X b X b Xij ij ij1 1 4 1 1 10+ =( ) ,  
</p>
<p>meaning the effect of X
1
 at W
</p>
<p>1
 = 0 is simply b
</p>
<p>1
. In contrast, if W
</p>
<p>1
 = 1, we have 
</p>
<p>b X b X b X b X b b Xij ij ij ij ij1 1 4 1 1 1 4 1 1 4 11+ = + = +( ) ( ) ,  
</p>
<p>A D D I N G  C L U S T E R  ( L E V E L  2 )  C H A R A C T E R I S T I C S  661
</p>
<p>Equation 20.14
</p>
<p>Equation 20.15
</p>
<p>ij .ϵ
</p>
<p>ij .ϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>meaning the effect of X
1
 at W
</p>
<p>1
 = 1 is b
</p>
<p>1
 + b . For cluster-level characteristics 
</p>
<p>measured on an interval scale of measurement, we would typically pick out some 
</p>
<p>meaningful values and highlight the effect of X
1
 at those values.
</p>
<p>A Substantive Example: Race and Sentencing Across Pennsylvania Counties
</p>
<p>In an analysis of sentencing decisions in Pennsylvania in the 1990s, Britt used 
</p>
<p>a multilevel model to assess the effects of various social, economic, and crime 
</p>
<p>measures on punishment severity decisions for offenders . Of particular interest 
</p>
<p>in Britt&rsquo;s analysis was the effect of these kinds of community characteristics on 
</p>
<p>the effect of offender&rsquo;s race on punishment severity. For example, were black 
</p>
<p>offenders punished more severely in those counties with higher rates of crime? 
</p>
<p>Alternatively, were black offenders punished less severely in those counties with 
</p>
<p>proportionally larger black populations? The theoretical rationale for these differ-
</p>
<p>ent hypotheses is presented in the original paper.
</p>
<p>In what follows, we highlight a few of his key findings as examples of the power 
</p>
<p>-
</p>
<p>In the context of a multilevel model, the sentence length decisions represent the 
</p>
<p>individual-level data (i.e., level 1) and the counties in which the sentences were 
</p>
<p>given represent the cluster-level data (level 2). We focus our discussion on the 
</p>
<p>following variables:
</p>
<p>Since our intention here is to illustrate the use and interpretation of multilevel 
</p>
<p>showing only those elements focused on the effect of county characteristics on 
</p>
<p>overall sentence length and county characteristics interacting with race of the 
</p>
<p>offender. Omitted from the table are numerous case and offender characteristics 
</p>
<p>relevant to predicting punishment severity, such as offense severity, criminal his-
</p>
<p>tory, plea bargaining, and the like.
</p>
<p>To begin the interpretation of the results, note that percentage of the popula-
</p>
<p>tion classified as black, difference in white and black per capita income, percent-
</p>
<p>age living in urban areas, and trend in unemployment all have direct effects on 
</p>
<p> Britt, C.L. (2000). Social context and racial disparities in punishment decisions. Justice 
</p>
<p>Quarterly
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S662
</p>
<p>Individual-level (Level 1)
</p>
<p>Sentence length: Months sentenced to jail or prison.
</p>
<p>Black: Coded as 1 if  offender was black, 0 if  offender was white (all other 
</p>
<p>cases were excluded from analysis).
</p>
<p>County-level (Level 2)
</p>
<p>Percentage of  population living in an urban area.
</p>
<p>Average violent crime rate.</p>
<p/>
</div>
<div class="page"><p/>
<p> Table 20.8
</p>
<p>A D D I N G  C L U S T E R  ( L E V E L  2 )  C H A R A C T E R I S T I C S  663
</p>
<p>black offender and the interaction terms for black offender with percentage of the 
</p>
<p>population classified as black and the average violent crime rate. The interpretation 
</p>
<p>of the results at this point is no different than the interpretations we made in the 
</p>
<p>linear regression model. For the direct effects on sentence length: 
</p>
<p>As the percentage of  blacks in a county increases, the average sentence length 
</p>
<p>decreases.
</p>
<p>As the percentage of  a county&rsquo;s population living in an urban area increases, 
</p>
<p>the average sentence length increases.
</p>
<p>As the county-level unemployment rate increased over time, the average 
</p>
<p> sentence length increases.
</p>
<p>For the cross-level interaction effects of offender race with percentage of black 
</p>
<p>residents in a county and average crime rate, the interpretations follow the logic 
</p>
<p>to any other interaction effect. In general, what we find is that as the percentage 
</p>
<p>of a county&rsquo;s population classified as black increases, the effect of being black 
</p>
<p>decreases, meaning that black offenders received significantly shorter sentences 
</p>
<p>than white offenders overall, but the magnitude of this difference increases as the 
</p>
<p>percentage of blacks in a county increases. Conversely, in counties where the aver-
</p>
<p>age violent crime rate was higher, the effect of being black increases, meaning 
</p>
<p>that the punishments received by black offenders were more severe than those for 
</p>
<p>white offenders in counties with higher violent crime rates.
</p>
<p>The following hypotheticals will help to illustrate these patterns. Suppose that 
</p>
<p>we have four different counties with the following characteristics: 
</p>
<p>County A: Percentage Black = 10, Violent Crime Rate = 100
</p>
<p>County B: Percentage Black = 20, Violent Crime Rate = 100
</p>
<p>County C: Percentage Black = 10, Violent Crime Rate = 200
</p>
<p>County D: Percentage Black = 20, Violent Crime Rate = 200
</p>
<p>The equation for the effect of being a black offender on sentence length is: 
</p>
<p> 
- - &acute; + &acute;2 277 0 315 0 009. . .Black Black PercentBlack Black ViolentCrimeeRate.  
</p>
<p>The effect of being a black offender in County A: 
</p>
<p>&minus; &minus; &times; &times; + &times; &times; = &minus;2 277 0 315 10 0 009 100 4 527. . . . .Black Black Black Black  
</p>
<p>VARIABLE ESTIMATE se z-SCORE
</p>
<p>Intercept
Percentage Black
Percentage Urban
Trend in Unemployment
</p>
<p>Black
Black &times; Percentage Black
Black &times; Violent Crime Rate
</p>
<p>13.968
&minus;0.161
</p>
<p>0.026
0.820
</p>
<p>&minus;2.277
&minus;0.315
</p>
<p>0.009
</p>
<p>0.574
0.021
0.007
0.228
</p>
<p>0.618
0.092
0.003
</p>
<p>24.334
&minus;7.750
3.910
3.596
</p>
<p>&minus;3.684
&minus;3.292
3.000
</p>
<p>Multilevel Regression Results for Sentence Length</p>
<p/>
</div>
<div class="page"><p/>
<p>The effect of being a black offender in County B: 
</p>
<p>&minus; &minus; &times; &times; + &times; &times; = &minus;2 277 0 315 20 0 009 100 7 677. . . . .Black Black Black Black  
</p>
<p>The effect of being a black offender in County C: 
</p>
<p>&minus; &minus; &times; &times; + &times; &times; = &minus;2 277 0 315 10 0 009 200 3 627. . . . .Black Black Black Black  
</p>
<p>The effect of being a black offender in County D: 
</p>
<p>&minus; &minus; &times; &times; + &times; &times; = &minus;2 277 0 315 20 0 009 200 6 777. . . . .Black Black Black Black  
</p>
<p>In all cases, the effect of being a black offender resulted in a shorter sentence, 
9 For the two pairs coun-
</p>
<p>ties with matching percentage black populations, the increase in the violent crime 
</p>
<p>rate resulted in a shrinking of the effect of being black and moving the coefficient 
</p>
<p>closer to 0, where there is no difference between black and white offenders. For 
</p>
<p>the two pairs of counties with matching violent crime rates, as the percentage of 
</p>
<p>the black population increased, the sentence disparity increased further, with black 
</p>
<p>offenders receiving even more lenient sentences.
</p>
<p>    C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S664
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>9 While this finding often strikes many criminal justice students as counterintuitive, it is  
</p>
<p>consistent with much of  the research done on the race effects on sentencing outcomes. What 
</p>
<p>is not shown here, but is included in Britt&rsquo;s article, is the effect of  race on the likelihood of  
</p>
<p>being incarcerated, where black offenders were much more likely than white offenders to be 
</p>
<p>sentenced to prison. The findings here just highlight that once sentenced to incarceration, the 
</p>
<p>length of  time is shorter for black offenders compared to white offenders. In Pennsylvania, this 
</p>
<p>has typically taken the form of  more black offenders being sentenced to local jails for relatively 
</p>
<p>short periods of  time, while white offenders who have received incarceration sentences will be 
</p>
<p>sent to state prisons for relatively longer stays.
</p>
<p>Multilevel models offer an important extension to traditional linear regres-
</p>
<p>sion models by statistically accounting for possible clustering in a sample of  data. 
</p>
<p>Observations that come from the same cluster (e.g., multiple survey respondents 
</p>
<p>within the same neighborhood, multiple cases processed by a judge or prosector, 
</p>
<p>and so on) will tend to be more similar to each other than to observations from 
</p>
<p>different clusters. This results in an increased likelihood of  finding statistically 
</p>
<p>significant effects, since many of  the cases within a cluster will exhibit a similar 
</p>
<p>pattern of  association. Multilevel models account for clustering by allowing for 
</p>
<p>random variation in the intercepts and possibly the coefficients of  the independ-
</p>
<p>ent variables. Models that allow for variation in the model intercept are referred to 
</p>
<p>as random intercept models (RIMs), while models that contain a random 
</p>
<p>intercept and at least one random slope coefficient are referred to as random 
</p>
<p>coefficient models (RCMs).
</p>
<p>Variation in both the intercept and the coefficient for an independent vari-
</p>
<p>able are measured with what are called variance components&mdash;measures of 
</p>
<p>how much the intercept and slope may vary across cluster. These are also called </p>
<p/>
</div>
<div class="page"><p/>
<p>665K E Y  T E R M S
</p>
<p>the random effects. We test the significance of the variance estimates with a 
</p>
<p>chi-square test that compares the model with the random effects against a linear 
</p>
<p>regression model without any random effects.
</p>
<p>In estimating multilevel models, we may also center the values of  the independ-
</p>
<p>ent variables. Centering can take on two forms: grand-mean or cluster-mean center-
</p>
<p>ing. Centering has no effect on the interpretation of  the slope coefficients, but will 
</p>
<p>alter the substantive meaning of  the model intercept. In grand-mean centering, 
</p>
<p>the model intercept represents the overall sample mean for the dependent variable. 
</p>
<p>In cluster-mean centering, the model intercept represents a weighted sample 
</p>
<p>mean for the dependent variable that is conditioned on the number of  cases per 
</p>
<p>cluster. The more balanced the size of  the clusters, the more similar the two esti-
</p>
<p>mates of  the model intercept will be. In general, centering the values of  the inde-
</p>
<p>pendent variables will tend to simplify the estimation of  the overall multilevel model.
</p>
<p>Centering also allows for the testing of different between and within 
</p>
<p>group effects of the independent variables. Much of the research in criminology 
</p>
<p>and criminal justice assumes the between and within effects are the same without 
</p>
<p>ever testing for similarity. By estimating models that include the cluster means as 
</p>
<p>independent variables, it is possible to assess directly how or whether the between 
</p>
<p>cluster effects are the same as the within cluster effects. If the results of these 
</p>
<p>tests indicate the effects are the same, then grand-mean centering is appropriate. 
</p>
<p>Alternatively, if the between and within effects are different, then cluster-mean 
</p>
<p>centering is a more appropriate technique.
</p>
<p>K e y  T e r m s
</p>
<p>between effect Effect of  an independent  
</p>
<p>variable on the dependent variable using the  
</p>
<p>cluster as the unit of  analysis&mdash;a regression of  
</p>
<p>cluster-level averages across all the clusters  
</p>
<p>included in the analysis.
</p>
<p>cluster-mean centering Computed difference 
</p>
<p>between the observed raw score on some variable 
</p>
<p>for each observation in the sample and the cluster 
</p>
<p>mean for that variable.
</p>
<p>contrast coding A method for recoding a  
</p>
<p>multi-category nominal variable into multiple 
</p>
<p>indicator variables (one less than the total number 
</p>
<p>of  categories), where the indicator category is 
</p>
<p>and all other categories are coded as 0. Contrast 
</p>
<p>coding ensures that the sum of  all the estimated 
</p>
<p>effects for the indicator variable is equal to 0.
</p>
<p>cross-level interaction An interaction effect 
</p>
<p>included in a multilevel model between a level 1 
</p>
<p>independent variable and a level 2 cluster  
</p>
<p>characteristic.
</p>
<p>fixed effects A descriptive label for the  
</p>
<p>regression coefficients (b
k
) estimated in a model 
</p>
<p>with random effects. Fixed effects represent the 
</p>
<p>average effects of  the independent variables on 
</p>
<p>the dependent variable across all individuals and 
</p>
<p>clusters in a multilevel model.
</p>
<p>grand-mean centering Computed difference 
</p>
<p>between the observed raw score on some variable 
</p>
<p>for each observation in the sample and the overall 
</p>
<p>sample mean for that variable.
</p>
<p>intraclass correlation A measure of   
</p>
<p>association that measures the level of  absolute 
</p>
<p>agreement of  values within each cluster.
</p>
<p>multilevel data Sample data where individual 
</p>
<p>observations (level 1 data) are clustered within a 
</p>
<p>higher-level sampling unit (level 2 data).</p>
<p/>
</div>
<div class="page"><p/>
<p>    C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S666
</p>
<p>random coefficient model A linear regres-
</p>
<p>sion model that allows the intercept and the effect 
</p>
<p>of  at least one independent variable to vary  
</p>
<p>randomly across cluster&mdash;random effects are 
</p>
<p>included for the model intercept and at least one 
</p>
<p>independent variable.
</p>
<p>random effects A descriptive label for the  
</p>
<p>random error terms included in a multilevel  
</p>
<p>model that allow for variation across cluster  
</p>
<p>from the sample average estimated in the  
</p>
<p>fixed effects. Random effects are assumed  
</p>
<p>to be normally distributed in most multilevel  
</p>
<p>models.
</p>
<p>random intercept model A linear regression 
</p>
<p>model that allows the intercept to vary randomly 
</p>
<p>across cluster&mdash;random effects are included for 
</p>
<p>the model intercept.
</p>
<p>regression coding A method for recoding a 
</p>
<p>multi-category nominal variable into multiple  
</p>
<p>indicator dummy variables (one less than the  
</p>
<p>total number of  categories), where the indicator 
</p>
<p>category is coded as 1 and all other categories  
</p>
<p>are coded as 0. The reference category does not 
</p>
<p>have an indicator variable and is coded as a 0 on 
</p>
<p>all the indicator dummy variables.
</p>
<p>variance components model A one-way anal-
</p>
<p>ysis of  variance model that includes random effects 
</p>
<p>for each cluster that assesses whether there is  
</p>
<p>random variation in the mean of  the dependent 
</p>
<p>variable across the clusters included in the analysis.
</p>
<p>within effect Effect of  an independent  
</p>
<p>variable on the dependent variable within each 
</p>
<p>cluster and then averaged across all clusters or 
</p>
<p>groups included in the analysis.
</p>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>0j
 Random effect for the model intercept b
</p>
<p>0
</p>
<p>kj
  Random effect for the regression coefficient for the independent variable k 
</p>
<p>(bk)
</p>
<p>2
z0
</p>
<p>  variance of the random effect for the model intercept in a variance compo-
</p>
<p>nents model
</p>
<p>2
z1
</p>
<p>  variance of the random effect for the model intercept in a random intercept 
</p>
<p>model
</p>
<p>2
e0
</p>
<p> error variance (unexplained variance) in a variance components model
</p>
<p>2
z1
</p>
<p> error variance (unexplained variance) in a random intercept model
</p>
<p>2
z00
</p>
<p>  Variance of the random effect for the model intercept in a random coef-
</p>
<p>ficient model
</p>
<p>2
zkk
</p>
<p>  Variance of the random effect for the independent variable k in a random 
</p>
<p>coefficient model
</p>
<p>2
z0k
</p>
<p>  Covariance of the random effects for the model intercept and for the inde-
</p>
<p>pendent variable k in a random coefficient model 
</p>
<p>General equation for the variance components model: 
</p>
<p>Y bij j= + +0 z  
</p>
<p>General equation for the random intercept model with one independent variable: 
</p>
<p>Y b b Xij ij j= + + +0 1 1 0z  
</p>
<p>ijϵ
</p>
<p>ijϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>General equation for the random coefficient model with one independent variable: 
</p>
<p>Y b b Xij j j ij= + + + +( ) ( )0 0 1 1 1z z  
</p>
<p>Likelihood-ratio test for variance components: 
</p>
<p>c 2 2 1 2= - -( ( ) ( ))LL LLModel Model
 
</p>
<p>Equation for computing the intraclass correlation (ρ): 
</p>
<p>r
s
</p>
<p>s s
=
</p>
<p>+
z
</p>
<p>z e
</p>
<p>2
</p>
<p>2 2( )
,
</p>
<p> 
</p>
<p>Equation for grand-mean centering: 
</p>
<p>X Xij - ..  
</p>
<p>Equation for cluster-mean centering: 
</p>
<p>X Xij j&minus; .  
</p>
<p>Equation for explained variance in a random intercept model: 
</p>
<p>R z e z e
</p>
<p>z e
</p>
<p>2 0
2
</p>
<p>0
2
</p>
<p>1
2
</p>
<p>1
2
</p>
<p>0
2
</p>
<p>0
2
</p>
<p>=
+ - +
</p>
<p>+
</p>
<p>( ) ( )s s s s
s s
</p>
<p> 
</p>
<p>Equation for the explained variance at level 2 of a random intercept model: 
</p>
<p>Rz
z z
</p>
<p>z
</p>
<p>2 0
2
</p>
<p>1
2
</p>
<p>0
2
</p>
<p>=
-s s
</p>
<p>s
 
</p>
<p>Equation for the explained variance at level 1 of a random intercept model: 
</p>
<p>Re
e e
</p>
<p>e
</p>
<p>2 0
2
</p>
<p>1
2
</p>
<p>0
2
</p>
<p>=
-s s
s
</p>
<p> 
</p>
<p>E x e r c i s e s
</p>
<p> 1. Researchers interested in the possible effects of  neighborhood poverty on 
</p>
<p>patterns of  intimate partner violence (IPV) gathered interview data from 
</p>
<p>-
</p>
<p>lishing neighborhood variability in IPV, the researchers estimated a vari-
</p>
<p>ance components model and obtained the following results: 
</p>
<p>- = -2 2000LL( )ANOVA   
</p>
<p>- = -2 1900LL( )REM   
</p>
<p>E X E R C I S E S  667
</p>
<p>ijϵ</p>
<p/>
</div>
<div class="page"><p/>
<p>s z
2 0 10= .
</p>
<p>  
</p>
<p>s e
2 0 40= .
</p>
<p> 
</p>
<p> b. To what extent does neighborhood affect the prevalence of  IPV? Cal-
</p>
<p>meaning.
</p>
<p>self-reported delinquent behavior, which was measured with a scale that 
</p>
<p>researchers were particularly interested in the effects of  academic perfor-
</p>
<p>mance on delinquent behavior and estimated a random intercept model, 
</p>
<p>obtaining the following results:
</p>
<p> 
</p>
<p>VARIABLE COEFFICIENT
</p>
<p>Intercept
GPA
Educational Aspirations
Father&rsquo;s Education
Mother&rsquo;s Education
</p>
<p>0.50
&minus; 0. 30
&minus; 0. 05
&minus; 0. 20
&minus; 0. 35
</p>
<p>2
z
2
e
 
</p>
<p>0.15
0.95
</p>
<p> 
</p>
<p>components model and explain its substantive meaning.
</p>
<p>model and explain its substantive meaning.
</p>
<p>between the variance components and random intercept models.
</p>
<p>covariates were added to the model? Describe how the statistical  
</p>
<p> 3. A study of  anti-social behavior among children collected information on 
</p>
<p>study. Based on prior research, the investigators expected the  
</p>
<p>within-family and between-family effects of  parental attachment to be  
</p>
<p>different. The investigators found the following effects:
</p>
<p>b
Attachment
</p>
<p>se
</p>
<p>b
ClusterMeanofAttachment
</p>
<p>se = 0. 03
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S668</p>
<p/>
</div>
<div class="page"><p/>
<p> a. Explain whether the investigators found evidence of  different within-
</p>
<p>family and between-family effects of  parental attachment.
</p>
<p> b. Which type of  centering would be most appropriate for these data if  
</p>
<p>the investigators simply want to estimate a single effect for parental at-
</p>
<p>tachment that ignores the between and the within effects?
</p>
<p>-
</p>
<p>istics on individuals&rsquo; perceptions of  fear of  crime victimization selected a 
</p>
<p>within each neighborhood. Using a fear of  crime scale as the dependent 
</p>
<p>variable and demographic characteristics as covariates, the researchers 
</p>
<p>estimated a series of  regression models to test for random effects across 
</p>
<p>neighborhood. The results appear in the following table (assume all fixed 
</p>
<p>effects are statistically significant with p
</p>
<p>result.
</p>
<p>a random sample of  200 cases among those involving sentences to jail or 
</p>
<p>prison. Sentence length was measured as the number of  months sentenced 
</p>
<p>to incarceration. To assess the impact of  legal characteristics on sentence 
</p>
<p>length decisions, the researchers developed measures of  severity of  the 
</p>
<p>conviction crime and of  criminal history. After establishing that the inter-
</p>
<p>cept varied across judge, the researchers investigated a series of  random 
</p>
<p>coefficient models that examined whether there were correlations of  the 
</p>
<p>random effects for the intercept and the two covariates. The following 
</p>
<p>table presents their results:
</p>
<p>VARIABLE OLS
RANDOM INTERCEPT  
MODEL
</p>
<p>RANDOM COEFFICIENT  
MODEL
</p>
<p>Fixed Effects:
</p>
<p>Intercept
Age
Female
Black
</p>
<p>4.50
0.07
0.78
0.64
</p>
<p>4.40
0.06
0.80
0.52
</p>
<p>4.50
0.05
0.90
0.48
</p>
<p>Random Effects:
</p>
<p>Intercept (
00
2 )
</p>
<p>Age (
11
2 )
</p>
<p>Female (
22
2 )
</p>
<p>Black (
33
2 )
</p>
<p>0.03 0.03
0.01
0.12
0.05
</p>
<p>Model Information:
</p>
<p>&minus; 2L L &minus;3176 &minus;2273 &minus;2095
</p>
<p>E X E R C I S E S  669</p>
<p/>
</div>
<div class="page"><p/>
<p>your result.
</p>
<p> b. Test whether the addition of  the random effect covariances is statisti-
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>We have noted throughout this chapter that multilevel models can become com-
</p>
<p>plex very quickly and so we have tried to keep our focus on the basic elements 
</p>
<p>of  multilevel models. The essential syntax required to estimate these models is 
</p>
<p>generally straightforward and does not become complicated until we start cus-
</p>
<p>tomizing the statistical model. The examples below and in the accompanying 
</p>
<p>syntax files for SPSS (Chapter_20.sps) and Stata (Chapter_20.do) illustrate key 
</p>
<p>components to the multilevel commands without getting bogged down in too 
</p>
<p>many of  the options and details.
</p>
<p>SPSS
</p>
<p>In SPSS, random intercept models and random coefficient models are both esti-
</p>
<p>mated with the MIXED command:
</p>
<p>where the first line has the same structure as many of  the other commands in SPSS. 
</p>
<p>The /PRINT=SOLUTION TESTCOV option requests SPSS to print out the 
</p>
<p>coefficient table (SOLUTION) and to test the random effect variances and covari-
</p>
<p>ances for statistical significance (TESTCOV). The /METHOD=ML option forces 
</p>
<p>SPSS to estimate the models with maximum likelihood, while the FIXED = option 
</p>
<p>VARIABLE RIM RCM 1 RCM 2
</p>
<p>Fixed Effects:
</p>
<p>Intercept
Severity of Offense
Criminal History
</p>
<p>5.19
2.72
5.31
</p>
<p>5.23
2.65
5.62
</p>
<p>7.98
1.95
4.97
</p>
<p>Random Effects:
</p>
<p>Intercept (
00
2 )
</p>
<p>Severity of Offense (
11
2 )
</p>
<p>Criminal History (
22
2 )
</p>
<p>Intercept-Severity (
01
2 )
</p>
<p>Intercept-History (
02
2 )
</p>
<p>Severity-History (
12
2 )
</p>
<p>0.29 0.26
0.16
0.21
</p>
<p>0.21
0.12
0.18
0.06
0.04
0.11
</p>
<p>Model Information:
</p>
<p>&minus; 2LL &minus;2984 &minus;2781 &minus;2779
</p>
<p>    C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S670
</p>
<p>MIXED dep_var WITH list_of_indep_vars
</p>
<p>/PRINT = SOLUTION TESTCOV
</p>
<p>/FIXED = INTERCEPT list_of_indep_vars
</p>
<p>/RANDOM = INTERCEPT random_indep_var(s) |
</p>
<p>SUBJECT(cluster_variable) COVTYPE(UN) . </p>
<p/>
</div>
<div class="page"><p/>
<p>line should include INTERCEPT and all of  the independent variables included in 
</p>
<p>the model. The /RANDOM = option will then determine whether a RIM or RCM 
</p>
<p>will be estimated. If  a RIM is to be estimated, the /RANDOM line simplifies to: 
</p>
<p>For an RCM that estimates random effects covariances and variances you will 
</p>
<p>need to add the COVTYPE(UN) option to the /RANDOM line, since the 
</p>
<p>default in SPSS is to estimate an RCM without random effect covariances: 
</p>
<p>The output from running any of these commands is the same and includes 
</p>
<p>tables of coefficients (the fixed effects), of covariances and variances for the ran-
</p>
<p>dom effects, and of model summary statistics. 
</p>
<p>Stata 
</p>
<p>Random Intercept Models 
</p>
<p>To estimate random coefficient models in Stata, you will have access to two dif-
</p>
<p>ferent commands: xtreg and xtmixed. The structure to the xtreg command is: 
</p>
<p>The basic format is similar to the regress command&mdash;the difference follows the 
</p>
<p>comma, where the i() option indicates which variable provides information on 
</p>
<p>the cluster. In this chapter, the cluster was the judge identifier. The var option 
</p>
<p>is included to force Stata to estimate the variance of the random intercept&mdash;the 
</p>
<p>default output in Stata is to report the square root of the variance (i.e., the standard 
</p>
<p>deviation of the random effect). Finally, the mle option forces Stata to compute 
</p>
<p>the maximum likelihood estimates for the RIM. 
</p>
<p>The use of the xtmixed command is similar: 
</p>
<p>where instead of a comma immediately following the list of independent variables, 
</p>
<p>we have two vertical bars (||) that are followed by the cluster variable with a colon 
</p>
<p>(:) appended, and then the comma and request for maximum likelihood estimates. 
</p>
<p>As we explain in the next section regarding the estimation of random coefficient 
</p>
<p>models, the vertical bars will be useful for designating random coefficients. 
</p>
<p>Random Coefficient Models 
</p>
<p>The xtreg command cannot be used for random coefficient models, while 
</p>
<p>xtmixed can be used. To use xtmixed for an RCM, we simply add the name of 
</p>
<p>the independent variable(s) after the : that we want to estimate random effects for: 
</p>
<p>C O M P U T E R  E X E R C I S E S  671
</p>
<p>/RANDOM = INTERCEPT | SUBJECT(cluster_variable) . 
</p>
<p>For an RCM that only estimates random effects variances: 
</p>
<p>/RANDOM = INTERCEPT random_indep_var(s) |
</p>
<p>SUBJECT(cluster_variable) . 
</p>
<p>/RANDOM = INTERCEPT random_indep_var(s) |
</p>
<p>SUBJECT(cluster_variable) COVTYPE(UN) . 
</p>
<p>xtreg depvar list_of_indep_vars, i(cluster_variable) mle var 
</p>
<p>xtmixed depvar list_of_indep_vars || cluster_variable:, 
</p>
<p>mle var </p>
<p/>
</div>
<div class="page"><p/>
<p> 
</p>
<p>The structure to the rest of the command is the same&mdash;all we do is include one or 
</p>
<p>more independent variable names after the : and before the comma. Note that the 
</p>
<p>default RCM in Stata is to estimate a model with no covariances of the random 
</p>
<p>effects. If we want to estimate the covariances of the random effects, we add the 
</p>
<p>option cov(unstructured) to the command line: 
</p>
<p>The output will then contain the variances for the intercept, any independent vari-
</p>
<p>ables with effects allowed to vary across cluster, and all possible covariances of the 
</p>
<p>random effects.
</p>
<p>Problems
</p>
<p> 1. Open the Bail Decision-Making data file (bail-data-example.sav or bail-
</p>
<p>data-example.dta). The sample syntax files for this chapter include the 
</p>
<p>syntax required to reproduce most of  the tables in this chapter. Work your 
</p>
<p>way through one of  the syntax files and make sure you understand how it 
</p>
<p>works.
</p>
<p> 2. Using the Bail Decision-Making data file, use logbail as the dependent 
</p>
<p>-
</p>
<p>ing models:
</p>
<p> a. Random intercept model:
</p>
<p>estimate the covariances of  the random effects.
</p>
<p>    i. Did anything change in regard to the effects of  these independent 
</p>
<p>-
</p>
<p>mate the random effect covariances.
</p>
<p>    i. Did anything change in regard to the effects of  these independent 
</p>
<p>   C H A P T E R  T W E N T Y :  M U L T I L E V E L  R E G R E S S I O N  M O D E L S672
</p>
<p>xtmixed depvar list_of_indep_vars || cluster_variable:  
</p>
<p>random_indep_var(s), mle var 
</p>
<p>xtmixed depvar list_of_indep_vars || cluster_variable:  
</p>
<p>random_indep_var(s), mle var cov(unstructured)</p>
<p/>
</div>
<div class="page"><p/>
<p> 3. Continue to use the Bail Decision-Making data file, change the dependent 
</p>
<p>variable to the bail amount requested and estimate the same set of  models 
</p>
<p>with the same independent variables as in Question 2.
</p>
<p> a. Explain how the results are similar. Different. Focus on the values  
</p>
<p>effects.
</p>
<p> b. What might account for these differences? (Hint: You may want to 
</p>
<p>generate histograms for both dependent variables as a starting point.)
</p>
<p>C O M P U T E R  E X E R C I S E S  673</p>
<p/>
</div>
<div class="page"><p/>
<p>   
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_21,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>C h a p t e r  t w e n t y  o n e
</p>
<p>Special Topics: Randomized Experiments
</p>
<p>What does a randomized experiment look like?
</p>
<p>What are the advantages of randomized experiments? 
</p>
<p>How do randomized experiments maximize internal validity?
</p>
<p>What is a block randomized trial?
</p>
<p>How does block randomization help increase statistical power?
</p>
<p>How can covariates be used to help increase statistical power? </p>
<p/>
</div>
<div class="page"><p/>
<p>Many of the descriptive and inferential statistical approaches we have 
examined in this text are appropriate for analyzing randomized experiments. 
</p>
<p>For example, in Chapter 11 we used the two-sample test of proportions to ana-
</p>
<p>lyze data from the Maricopa County Drug Testing Experiment. In this chapter 
</p>
<p>we want to focus more specifically on randomized studies. We begin the chapter 
</p>
<p>by showing why randomized experiments provide a very strong ability to make 
</p>
<p>causal inferences without concern for confounding. Indeed, many scholars have 
</p>
<p>taken the position that only randomized experiments can provide valid conclu-
</p>
<p>sions regarding the impacts of treatments and programs.1 Joan McCord argued, 
</p>
<p>for example, that crime and justice evaluations should employ random assign-
</p>
<p>ment &ldquo;whenever possible.&rdquo; 2
</p>
<p>Once we have established why randomized experiments provide distinct advan-
</p>
<p>tages, we will focus on some specific approaches for strengthening analysis of 
</p>
<p>1 See Robert Boruch, Brooke Snyder, and Dorothy DeMoya, &ldquo;The Importance of  Randomized 
</p>
<p>Field Trials,&rdquo; Crime &amp; Delinquency 46 (2000):156&ndash;180; Donald Campbell and Robert 
</p>
<p>F. Boruch, &ldquo;Making the Case for Randomized Assignment to Treatments by Considering 
</p>
<p>the Alternatives: Six Ways in Which Quasi-Experimental Evaluations in Compensatory 
</p>
<p>Education Tend to Underestimate Effects,&rdquo; In Carl A. Bennett and Arthur A. Lumsdaine 
</p>
<p>(eds.), Evaluation and Experiment: Some Critical Issues in Assessing Social Programs, 
</p>
<p>(New York: Academic Press, 1975: 195&ndash;296); Thomas Cook and Donald Campbell, Quasi-
</p>
<p>Experimentation: Design and Analysis Issues for Field Settings, (Boston, MA: Houghton 
</p>
<p>Mifflin Harcourt, 1979); Lynette Feder, Annette Jolin, and William Feyerherm, &ldquo;Lessons 
</p>
<p>from Two Randomized Experiments in Criminal Justice Settings,&rdquo; Crime &amp; Delinquency 46 
</p>
<p>(2000): 380&ndash;400; Brian R. Flay and J. Allen Best, &ldquo;Overcoming Design Problems in Evaluating 
</p>
<p>Health Behavior Programs,&rdquo; Evaluation and the Health Professions, 5 (1982): 43&ndash;69; David 
</p>
<p>Weisburd, &ldquo;Randomized Experiments in Criminal Justice Policy: Prospects and Problems,&rdquo; 
</p>
<p>Crime &amp; Delinquency, 46 (2000): 181&ndash;193; David Weisburd, Cynthia Lum, and Anthony 
</p>
<p>Petrosino, &ldquo;Does Study Design Affect Research Outcomes in Criminal Justice?&rdquo; The Annals 
</p>
<p>of the American Academy of Political and Social Sciences 578 (2001): 50&ndash;70; Leland 
</p>
<p>Wilkinson and Task Force on Statistical Inference, APA Board of  Scientific Affairs, &ldquo;Statistical 
</p>
<p>Methods in Psychology Journals: Guidelines and Explanations,&rdquo; American Psychologist 54 
</p>
<p>(1999): 594&ndash;604.
2 See Joan McCord, &ldquo;Cures that Harm: Unanticipated Outcomes of  Crime Prevention 
</p>
<p>Programs,&rdquo; Annals of the American Academy of Political and Social Science 587 (2003): 
</p>
<p>16&ndash;30, p. 29.
</p>
<p>675</p>
<p/>
</div>
<div class="page"><p/>
<p>randomized studies. While the basic tools we have already described for analyzing 
</p>
<p>data are also appropriate for experimental data, there are specific problems that 
</p>
<p>researchers might encounter in experimental research. One of these derives from 
</p>
<p>the fact that experiments are &ldquo;carried out&rdquo; by the researcher and practitioner in 
</p>
<p>the field. Unlike &ldquo;observational&rdquo; research studies that observe programs and draw 
</p>
<p>data from them, in a randomized experiment the researcher makes the scene by 
</p>
<p>randomly allocating subjects or places to treatment and control conditions. This 
</p>
<p>means that it is often difficult to gain enough cases for a statistically powerful 
</p>
<p>research design (see Chapter 23). In this chapter we examine two methods for 
</p>
<p>increasing the statistical power of experimental research programs. We also dis-
</p>
<p>cuss a related problem that develops from the focus of experimental research on 
</p>
<p>specific research problems. Some scholars have criticized randomized experiments 
</p>
<p>T h e  S t r u c t u r e  o f  a  R a n d o m i z e d  E x p e r i m e n t 4
</p>
<p>The general structure of experiments in criminology is usually similar in design 
</p>
<p>regardless of the area or the question of interest. Generally, experiments in crimi-
</p>
<p>nology start with an eligibility pool, randomization, group allocation, and posttest 
</p>
<p>measures relevant to the dependent variable of interest (Figure 21.1).
</p>
<p>The eligibility pool is made up of those participants or units that are eligible for the 
</p>
<p>experiment. Units of analysis can be individuals or aggregated groups or other enti-
</p>
<p>ties that often are found in clusters. For example, in an experiment that evaluates the 
</p>
<p>impact of increased foot patrol on crime rates, the eligibility pool may be individual 
</p>
<p>years of experience. Or the unit of analysis could be the geographic area or the &ldquo;beat&rdquo; 
</p>
<p>that will be assigned to different conditions. The eligibility pool thus comprises those 
</p>
<p>patrol officers (or patrol &ldquo;beats&rdquo;) that meet the criteria for inclusion in the study.
</p>
<p>Next, researchers randomly assign members from this pool of eligible partici-
</p>
<p>pants or units to the study conditions&mdash;often a treatment group and a control or 
</p>
<p>a comparison group. Historically, randomization was carried out through a simple 
</p>
<p>coin flip. There are now many ways to randomize subjects, but most often research-
</p>
<p>ers rely on computerized statistical software to carry out randomization. Some 
</p>
<p>researchers may simply use the rule of odds and evens&mdash;that is, assigning every 
</p>
<p>other case to one particular group. This is often referred to as alternation and is 
</p>
<p>3 James J. Heckman and Jeffrey A. Smith, &ldquo;Assessing the Case for Social Experimentation,&rdquo; 
</p>
<p>Journal of Economic Perspectives, 9 (1995): 85&ndash;110.
4 For this section and the section on internal validity we draw heavily from David Weisburd, 
</p>
<p>Anthony Petrosino, Trevor Fronius (2013). Randomized Experiments in Criminology 
</p>
<p>and Criminal Justice. In David Weisburd and Gerben Bruinsma (Eds.). Encyclopedia of 
</p>
<p>Criminology and Criminal Justice. New York: Springer Verlaag.
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S676
</p>
<p>because they generally do not take into account &ldquo;interactions&rdquo; between the variable 
</p>
<p>or the treatment of interest and other relevant variables.3 For example, in assess-
</p>
<p>ing a rehabilitation program in prison we may be interested not only in the general 
</p>
<p>impacts of the program but also the differential impacts across men and women, 
</p>
<p>or older or younger offenders.  We will discuss how a researcher can do this in the 
</p>
<p>context of an experimental research model. 
</p>
<p>officers who &ldquo;walk the beat&rdquo; in a specific area, and who have a specific number of </p>
<p/>
</div>
<div class="page"><p/>
<p> 
</p>
<p>considered quasi-random assignment as the assignment of the numbers used is not 
</p>
<p>actually random. The most critical factor in randomization, however, is that each 
</p>
<p>case has the same probability or likelihood of being selected for the control group 
</p>
<p>as the experimental group and that the assignment is based purely on chance.
</p>
<p>In the usual criminological experiment, eligible cases are randomly assigned to 
</p>
<p>one of the two groups&mdash;treatment or control. Experiments in criminology may 
</p>
<p>have more than two groups. But typically, an experiment comprises a group that 
</p>
<p>receives the treatment or the intervention and a control or a comparison group that 
</p>
<p>does not. It is also quite common in a criminological experiment for the control 
</p>
<p>group to actually receive something rather than nothing. For example, in a foot 
</p>
<p>patrol experiment, the control group may receive treatment as usual or the same 
</p>
<p>number of foot patrol officers as typically employed.
</p>
<p>Experimental designs can include any number of outcomes. If the randomiza-
</p>
<p>tion was implemented with fidelity, it should produce two equivalent groups on 
</p>
<p>the pretest or baseline measures related to the outcome of interest. The researcher 
</p>
<p>then conducts analyses to determine if the intervention had any impact on the 
</p>
<p>posttest or follow-up measures of the outcomes of interest.
</p>
<p>T h e  M a i n  A d v a n t a g e  o f  E x p e r i m e n t s :  
I s o l a t i n g  C a u s a l  E f f e c t s
</p>
<p>In identifying whether a variable has a causal influence, or evaluating the out-
</p>
<p>comes of treatments or programs, the key issue for researchers is getting an 
</p>
<p>unbiased estimate of the treatment or the intervention effect. Without that any 
</p>
<p>other considerations, such as the ability to generalize results, are superfluous. For 
</p>
<p>example, suppose an evaluator was asked to assess whether an intervention for 
</p>
<p>drug involved offenders provided an effective deterrent to future offending. In the 
</p>
<p>study employed, the treatment group was found to be half as likely to recidivate as 
</p>
<p>the control condition not receiving the intervention. This would ordinarily lead the 
</p>
<p>evaluator to report that the intervention was a success. But what if it was difficult 
</p>
<p>to &ldquo;believe&rdquo; the result that was gained in the study? Suppose that the design of the 
</p>
<p>study did not allow the evaluator to assume that the observed effect was actually 
</p>
<p>the result of the intervention. In this scenario, the evaluator could not be sure that 
</p>
<p>Diagram of the Typical Criminological ExperimentFigure 21.1
</p>
<p>    T H E  M A I N  A D V A N T A G E  O F  E X P E R I M E N T S 677
</p>
<p>Cases Then
</p>
<p>Followed to
</p>
<p>Determine Impact 
</p>
<p>Experimental Outcome
</p>
<p>Measures
Eligibility Pool 
</p>
<p>Created
</p>
<p>Eligible Cases Randomized
Control </p>
<p/>
</div>
<div class="page"><p/>
<p>it was the intervention that caused the change or something else that was common 
</p>
<p>to the treatment group but not to the control group. In such a situation, it does not 
</p>
<p>do much good to argue about whether the results can be generalized to a specific 
</p>
<p>population of interest. The results themselves are not believable.
</p>
<p>The main problem researchers face in producing believable results is that treat-
</p>
<p>ments are often confounded with other factors. For example, suppose that the 
</p>
<p>reason for the outcome observed above was that the evaluator had not taken into 
</p>
<p>account the fact that the treated drug offenders were volunteers. Volunteers in turn 
</p>
<p>are more likely to be highly motivated to succeed in such programs than individu-
</p>
<p>als who are not volunteers.5 This is often termed &ldquo;creaming&rdquo; in the identification 
</p>
<p>of the subjects in the treatment condition. The reason why the intervention group 
</p>
<p>had lower recidivism rates in this case could easily be explained by the fact that 
</p>
<p>they were on average more motivated to be rehabilitated than drug offenders in 
</p>
<p>the control condition.
</p>
<p>All research studies that seek to establish causation between a specific variable 
</p>
<p>or treatment and an outcome must deal with this problem of confounding, and 
</p>
<p>it stands as the major barrier to drawing believable conclusions in criminological 
</p>
<p>studies. Non-experimental methods, such as regression techniques using obser-
</p>
<p>vational data that we reviewed in Chapters 15&ndash;20, and quasi-experiments using 
</p>
<p>approaches such as matching of subjects rely on a similar logic to solve the prob-
</p>
<p>lem of confounding. The logic is easily stated: if we know what the factors are that 
</p>
<p>confound treatment (or the variable of interest) we can take them into account. 
</p>
<p>In other words, non-experimental methods, as we noted in Chapter 16, rely on a 
</p>
<p>&ldquo;knowledge solution&rdquo; to the problem of confounding.
</p>
<p>But how does knowledge solve the problem of confounding? Let us take the 
</p>
<p>example of regression analyses as described in Chapter 16 using observational data 
</p>
<p>examining the question of the effect of a drug intervention program on recidivism. 
</p>
<p>Figure 21.2a shows the effect of the intervention on recidivism using a standard-
</p>
<p>ized regression coefficient approach (see pages 492&ndash;493). Here we have the simple 
</p>
<p>result suggesting that the intervention decreases recidivism. However, when we 
</p>
<p>include in our analysis the &ldquo;confounding&rdquo; factor&mdash;level of motivation&mdash;the rela-
</p>
<p>tionship between treatment and recidivism changes (see Figure 21.2b). Taking into 
</p>
<p>account the effect of level of motivation, the effect of the intervention becomes 0 
</p>
<p>in this illustration. The observed effect was not due to the intervention but rather 
</p>
<p>due to the confounding of the intervention with motivation of offenders.
</p>
<p>Notice that two extra coefficients are included in Figure 21.2b. The first repre-
</p>
<p>sents the relationship between treatment (the variable of interest) and motivation 
</p>
<p>(the confounding variable). This standardized coefficient is .50 and represents the 
</p>
<p>extent to which treatment and motivation are related or confounded. The  second 
</p>
<p>5 See George De Leon, Gerald Melnick, George Thomas David Kressel, and Harry K. Wexler, 
</p>
<p>&ldquo;Motivation for Treatment in a Prison-Based Therapeutic Community,&rdquo; American Journal  
</p>
<p>of Drug and Alcohol Abuse, 26 (2000): 33&ndash;46.; Faye Taxman, Reducing Recidivism through 
</p>
<p>a Seamless System of Care: Components of Effective Treatment, Supervision, and 
</p>
<p>Transition Services in the Community, (Washington, DC: Office of  National Drug  
</p>
<p>Control Policy, 1998).; Robert Rosenthal, &ldquo;The Volunteer Subject,&rdquo; Human Relations  
</p>
<p>18 (1965): 389&ndash;406.
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S678</p>
<p/>
</div>
<div class="page"><p/>
<p>and recidivism. Together these two relationships detail the extent of confound-
</p>
<p>ing that is clouding our ability to estimate the treatment or the program effect. 
</p>
<p>Confounding in this context takes into account the extent to which the confound-
</p>
<p>ing factor is related to the outcome of interest, and the degree to which it is related 
</p>
<p>to or confounded with the treatment variable. And indeed, we can estimate the 
</p>
<p>value of the simple relationship observed in Figure 21.2a. This can be defined as 
</p>
<p>the degree of bias. In this case, the degree of bias is equal to the observed effect. 
</p>
<p>Had we not taken motivation into account we would have erroneously concluded 
</p>
<p>that the treatment leads to lower rates of recidivism, when in fact it is the motiva-
</p>
<p>The way in which multivariate regression approaches allow us to control for 
</p>
<p>confounding is illustrated in Equation 21.1 and described in Chapter 16. Here we 
</p>
<p>show the computation of the regression coefficient b for a treatment variable (Tr) 
</p>
<p>controlling for a confounding factor (CC, in this case motivation):
</p>
<p>Equation 21.1
</p>
<p>Example of the Bias in the Estimate of a Treatment Effect Caused by the Exclusion of an 
</p>
<p>Unknown or an Unmeasured Factor (X
j
). (a) Estimate of B
</p>
<p>1
 in the Case Where the Factor 
</p>
<p>(X
j
) Is Unmeasured and Excluded from the Model. Estimate of B
</p>
<p>1
 is &minus;.25. (b) Estimate of 
</p>
<p>B
1
 in the Case Where the Factor (X
</p>
<p>j
) Is Included in the Model. Estimate of B
</p>
<p>1
 is .00.
</p>
<p>Figure 21.2
</p>
<p>      T H E  M A I N  A D V A N T A G E  O F  E X P E R I M E N T S 679
</p>
<p>b =
r r r
</p>
<p>r
</p>
<p>S
</p>
<p>S
Tr
</p>
<p>Y Tr Y CC TrCC
</p>
<p>TrCC
</p>
<p>Y
</p>
<p>Tr
</p>
<p>1 1
</p>
<p>1
2
</p>
<p>&minus; ( )
&minus;
</p>
<p>
</p>
<p>


</p>
<p>
</p>
<p>



</p>
<p>

</p>
<p>
 
</p>
<p>a
</p>
<p>b
</p>
<p>Treatment
</p>
<p>Outcome
</p>
<p>Xj
</p>
<p>B = .50
</p>
<p>B1  = .00
</p>
<p>B2  = -.50
</p>
<p>B1 = -.25
</p>
<p>Treatment
</p>
<p>tion of offenders (represented by ) which is responsible for this result.Xj</p>
<p/>
</div>
<div class="page"><p/>
<p>The key part of the equation for our interest is the numerator in the first part of the 
</p>
<p>equation: r
Y1Tr
</p>
<p>r
Y1CC
</p>
<p>r
TrCC
</p>
<p>). Note that it includes the simple correlation between 
</p>
<p>the treatment variable and the outcome measure (r
Y1Tr
</p>
<p>). Subtracted from that is 
</p>
<p>the product of the correlation between the outcome measure and the confound-
</p>
<p>ing variable (r
Y1CC
</p>
<p>) and the treatment and confounding variable (r
TrCC
</p>
<p>)&mdash;the two 
</p>
<p>components of confounding we have just described. In this context we can statis-
</p>
<p>tically &ldquo;un-confound&rdquo; our estimate of the treatment if we have knowledge of the 
</p>
<p>This solution is also key to the myriad of approaches that have been developed 
</p>
<p>for other types of non-experimental approaches. All of them rely on knowledge 
</p>
<p>about confounding. For example, matching of subjects on known characteristics, 
</p>
<p>or the more sophisticated propensity score matching approach,6 begins with the 
</p>
<p>basic assumption that we have enough knowledge to create equivalence of units in 
</p>
<p>the treatment and control conditions. Because the subjects in the matched groups 
</p>
<p>are assumed to be alike, we make an assumption that confounders are not influ-
</p>
<p>encing our observations of a treatment effect. Note that in this case we are trying 
</p>
<p>to statistically control for such confounding factors, by making the treatment and 
</p>
<p>control groups alike on these factors. In such a case, the correlation between treat-
</p>
<p>ment and confounding variables is assumed to be 0. When it is, as illustrated in 
</p>
<p>Equation 21.2 for the bivariate regression coefficient (see Chapter 15), the effect 
</p>
<p>of the treatment breaks down to the simple correlation between treatment and 
</p>
<p>outcome:
</p>
<p>b = r
S
</p>
<p>S
Tr Y Tr
</p>
<p>Y
</p>
<p>Tr
1
</p>
<p>( )

</p>
<p>
</p>
<p>
 Equation 21.2
</p>
<p>The problem with these methods is that if we want to get an unbiased estimate of 
</p>
<p>treatment we would in theory have to identify all &ldquo;confounding causes.&rdquo; Using the 
</p>
<p>regression approach, which in some sense provides the most transparent form of non-
</p>
<p>experimental methods, we would need to identify all confounding factors that also 
</p>
<p>have meaningful impacts on the outcome measure and include them in the regres-
</p>
<p>sion. This would mean both that we would have to have knowledge about all such 
</p>
<p>confounding factors and that we would be able to measure them in a research study.
</p>
<p>Randomized experiments start with a different logic. If we cannot control out 
</p>
<p>for confounding, we can make it irrelevant for the problem at hand. This is done 
</p>
<p>through the process of randomizing treatment. If treatment is randomized then 
</p>
<p>there is no reason to suspect systematic biases. This can be illustrated by return-
</p>
<p>ing to the simple path diagrams we used earlier. In Figure 21.3a we show the 
</p>
<p>simple relationship between a treatment and outcome. In Figure 21.3b we include 
</p>
<p>a potential confounding variable. Note that the confounding factor has a strong 
</p>
<p>6 See Paul R. Rosenbaum and Dennis R. Rubin, &ldquo;The Central Role of  the Propensity Score in 
</p>
<p>Observational Studies for Causal Effects,&rdquo; Biometrika 70 (1983): 41&ndash;55.; Paul R. Rosenbaum 
</p>
<p>and Dennis R. Rubin, &ldquo;Reducing Bias in Observational Studies Using Subclassification on the 
</p>
<p>Propensity Score,&rdquo; Journal of the American Statistical Association 79 (1984): 516&ndash;524.
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S680</p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 21.3
Example of the Lack of Confounding in the Treatment Effect When the Treatment (V
</p>
<p>1
) 
</p>
<p>and Potential Confounder (V
2
) Have No Relationship. (a) The Model Excluding a Potential 
</p>
<p>Confounder, V
2
. (b) The Model Including a Potential Confounder (V
</p>
<p>2
) but No Relationship 
</p>
<p>Between the Treatment (V
1
) and the Confounder Because of Randomization. If V
</p>
<p>2
 is 
</p>
<p>Excluded, the Bias = B * B
2
 = .00 * .50 = .00
</p>
<p>   T H E  M A I N  A D V A N T A G E  O F  E X P E R I M E N T S 681
</p>
<p>standardized relationship with the outcome variable (B = .50). However, using the 
</p>
<p>theory of randomization we can assume that there is no systematic relationship 
</p>
<p>between the treatment and the confounder. This is the case because treatment has 
</p>
<p>been randomly allocated. In theory it is not going to be related systematically to 
</p>
<p>other factors such as gender, race, age, and attitudes.
</p>
<p>What this means is that the relationship between any potential confounder 
</p>
<p>and the treatment can be assumed to be 0. By chance, fluctuations will occur, 
</p>
<p>and there will be systematic relationships observed, but these can in this case be 
</p>
<p>assumed to balance out in the long run. Or at least there is no reason to assume 
</p>
<p>that they will not. And if  the relationship between the confounder and the treat-
</p>
<p>ment is 0, then when we multiply this by the large relationship between the 
</p>
<p>confounder and the outcome we will also gain 0. The effect of  treatment is not 
</p>
<p>confounded. This is also illustrated in Equation 21.2 presented earlier, though in 
</p>
<p>this case the assumption that treatment and the confounder have a 0 correlation 
</p>
<p>is more believable.
</p>
<p>a
</p>
<p>b
</p>
<p>Y
</p>
<p>V1
</p>
<p>V2
</p>
<p>B = .00
</p>
<p>B1  = .50
</p>
<p>B2  = .50
</p>
<p>V1
</p>
<p>Y
</p>
<p>B1 = .50</p>
<p/>
</div>
<div class="page"><p/>
<p>I n t e r n a l  V a l i d i t y
</p>
<p>Our discussion so far is often subsumed under the heading of &ldquo;internal validity&rdquo; 
</p>
<p>in methodological texts in criminology. A research design in which the impact of 
</p>
<p>the intervention can be clearly distinguished from other observed factors is known 
</p>
<p>as having high internal validity. If there are confounding factors involved in the 
</p>
<p>impact of the intervention, then the evaluation design is considered to have low 
</p>
<p>internal validity. Shadish, Cook, and Campbell, among others, have identified the 
</p>
<p>most common threats to internal validity7:
</p>
<p> 1. Selection: The preexisting differences between treatment and control subject 
</p>
<p>or units.
</p>
<p> 2. History: An external event occurring at the same time of  the study that may 
</p>
<p>influence impact.
</p>
<p> 3. Maturation: Changes in subjects or units between measurements of  the 
</p>
<p>dependent variable. These changes may be of  natural evolution (e.g., aging) or 
</p>
<p>due to time-specific incidences (e.g., fatigue, illness).
</p>
<p> 4. Testing: Measurement at pretest impacts measurement at posttest.
</p>
<p> 5. Instrumentation: Changes to the instrument or the method of  measure-
</p>
<p>ment in posttest measures.
</p>
<p> 6. Regression to the mean: Natural trends may cause extreme subjects or units 
</p>
<p>who score extremely high or low during the pretest to score closer to the 
</p>
<p>mean at posttest.
</p>
<p> 7. Differential attrition: The differential loss of  subjects or units from the 
</p>
<p>treatment group compared to the control group.
</p>
<p> 8. Causal order: The certainty that the intervention did in fact precede the out-
</p>
<p>come of  interest.8
</p>
<p>To further illustrate the importance of internal validity, let us suppose a researcher 
</p>
<p>internal validity is considered high if, at the end of the evaluation, the researcher 
</p>
<p>can show that the change in juvenile recidivism among the intervention group is 
</p>
<p>due only to the intervention (i.e., youth court) and no other confounding factors 
</p>
<p>were at play. The researcher must show through either research design or analytical 
</p>
<p>procedures that all confounding factors are accounted for in the measurement of 
</p>
<p>outcomes. If the researcher is unable to account for other factors such as serious-
</p>
<p>ness of first offense or the maturation of the study population, he or she must 
</p>
<p>note that the observed effects may be due to other factors. If threats to validity (or 
</p>
<p>potential confounding factors) are not accounted for, the internal validity of the 
</p>
<p>study would be considered low.
</p>
<p>7 See William R. Shadish, Thomas D. Cook, and Donald T. Campbell, Experimental and 
</p>
<p>Quasi-Experimental Designs for Generalized Causal Inference, (Boston: Houghton-
</p>
<p>Mifflin, 2002)
8David P. Farrington and Brandon C. Welsh, &ldquo;Randomized Experiments in Criminal Justice: 
</p>
<p>What Have we Learned in the Past Two Decades?&rdquo; Journal of Experimental Criminology  
</p>
<p>1 (2005): 9&ndash;38.
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S682
</p>
<p>is interested in evaluating the impact of a youth court on juvenile recidivism. The </p>
<p/>
</div>
<div class="page"><p/>
<p>Generally speaking, a randomized experiment has the highest possible internal 
</p>
<p>validity, because as we illustrated above, this approach allows the researcher to assume 
</p>
<p>that other confounding causes of  the outcome of  interest, known and unknown, are 
</p>
<p>not systematically influencing the study results. High internal validity in randomized 
</p>
<p>experiments is gained through the process of  randomly allocating the treatment or 
</p>
<p>the intervention to the experimental and control or comparison groups. Through 
</p>
<p>random assignment, the researcher is not just randomizing the treatment. He or she 
</p>
<p>is randomizing all other factors that may influence the outcome of  the treatment. 
</p>
<p>Thus, there is no systematic bias that increases the odds of  one unit&rsquo;s assignment to 
</p>
<p>the treatment group and another unit&rsquo;s assignment to the control or the comparison 
</p>
<p>group. This is not to imply that the groups are the same on every characteristic&mdash;it is 
</p>
<p>very possible that differences may occur; however, these differences can be assumed 
</p>
<p>to be randomly distributed and are accounted for in the probability distributions that 
</p>
<p>underlie statistical tests of  significance. Regardless, neither the treatment group nor 
</p>
<p>the control group should have an advantage over the other on the basis of  known 
</p>
<p>or unknown variables. Thus, randomized experiments are the only design that allows 
</p>
<p>the researcher to assume statistically unbiased effects.9
</p>
<p>S A M P L E  S I Z E ,  E Q U I V A L E N C E ,  A N D  S T A T I S T I C A L  P O W E R  683
</p>
<p>The goal of most randomized experiments in criminology and criminal justice, 
</p>
<p>as in other social science fields, is to disentangle the impact of the treatment or 
</p>
<p>the intervention from the impact of other factors on the outcomes that are to be 
</p>
<p>tested. A randomized experiment allows the researcher to attribute differences 
</p>
<p>between the groups from pretest to posttest to the treatments or the interventions 
</p>
<p>that are applied. At the conclusion of the study the researcher is able to assert, with 
</p>
<p>confidence, that the differences are likely a result of the treatment and not due to 
</p>
<p>other confounding factors. It is more difficult for non-randomized studies, even 
</p>
<p>a high-quality quasi-experimental design, to make this assertion. This advantage is 
</p>
<p>underscored by Farrington10:
</p>
<p>The unique advantage of randomized experiments over other methods is high internal 
validity. There are many threats to internal validity that are eliminated in randomized 
experiments but are serious in non-experimental research. In particular, selection effects, 
owing to differences between the kinds of persons in one condition and those in another, 
are eliminated.
</p>
<p>S a m p l e  S i z e ,  E q u i v a l e n c e ,  a n d  S t a t i s t i c a l  P o w e r 1 1
</p>
<p>Despite the distinct advantages of randomized studies, it is often difficult to gain a 
</p>
<p>large number of cases in a randomized experiment. Sometimes this is true because 
</p>
<p>it is difficult to identify a large number of subjects who can be made eligible for 
</p>
<p>9 See Robert F. Boruch, Randomized Experiments for Planning and Evaluation:  
</p>
<p>A Practical Guide, (Thousand Oaks, CA: Sage, 1997).
10 See David P. Farrington &ldquo;Randomized Experiments on Crime and Justice,&rdquo; In Michael Tonry 
</p>
<p>(ed.), Crime and Justice: A Review of Research, 4 (1983): 257&ndash;308, p. 260.
11 Our discussion in this section relies heavily on David Weisburd and Charlotte Gill, &ldquo;Block 
</p>
<p>Randomized Trials at Places: Rethinking the Limitations of  Small N Experiments,&rdquo; Journal of 
</p>
<p>Quantitative Criminology (2013).</p>
<p/>
</div>
<div class="page"><p/>
<p>randomization into treatment and control conditions. Sometimes this is the case 
</p>
<p>because treatment conditions or data collection are expensive, and each new case 
</p>
<p>will increase the cost of the study. These problems are particularly acute in place-
</p>
<p>based randomized trials since the number of places with a specific crime problem 
</p>
<p>is generally limited.12 Moreover, place-based trials ordinarily demand significant 
</p>
<p>treatment resources per site, and accordingly it is expensive for agencies to &ldquo;treat&rdquo; 
</p>
<p>a large number of sites at one time.13
</p>
<p>Farrington14 argues that small N studies are not likely to achieve realistic pretest 
</p>
<p>balance across measured and unmeasured covariates.15 This of course undermines 
</p>
<p>the main advantage of experimental studies&mdash;that the control and treatment 
</p>
<p>groups can be assumed to be equivalent and differ only in the receipt or the non-
</p>
<p>receipt of treatment.
</p>
<p>While Farrington and colleagues have focused primarily on the problem of 
</p>
<p>equivalence, a related criticism of the small N sizes of many randomized experi-
</p>
<p>ments can also be raised. If the sample sizes used for such studies are small, 
</p>
<p>then their statistical power under traditional assumptions is also likely to be low. 
</p>
<p>Statistical power (see Chapter 23) is a particularly important component of evalu-
</p>
<p>ation studies, because it assesses whether the study will provide a &ldquo;fair test&rdquo; of 
</p>
<p>the interventions examined. Sample size is one of the key elements of statistical 
</p>
<p>power, and experiments with very small samples are also likely to have low statisti-
</p>
<p>cal power.
</p>
<p>A design approach called &ldquo;block randomization&rdquo; provides a potential solution 
</p>
<p>to the risk of unbalanced samples in small N studies as well as a valid method for 
</p>
<p>increasing the statistical power of such studies. Block randomized experiments take 
</p>
<p>12 See Anthony A. Braga, David L. Weisburd, Elin J. Waring, Lorraine Green Mazerolle, 
</p>
<p>William Spelman, and Francis Gajewski, Problem-Oriented Policing in Violent Crime Places: 
</p>
<p>A Randomized Controlled Experiment,&rdquo; Criminology 37 (1999): 541&ndash;580; Lawrence W. 
</p>
<p>Sherman, Patrick R. Gartin, and Michael E. Buerger, &ldquo;Hot Spots of  Predatory Crime: Routine 
</p>
<p>Activities and the Criminology of  Place,&rdquo; Criminology 27 (1989): 27&ndash;56.
13 See Robert Boruch, Henry May, Herbert Turner, Julia Lavenberg, Anthony Petrosino, 
</p>
<p>Dorothy De Moya, Jeremy Grimshaw, and Ellen Foley, &ldquo;Estimating the Effects of  
</p>
<p>Interventions that are Deployed in Many Places: Place-Randomized Trials,&rdquo; American 
</p>
<p>Behavioral Scientist 47 (2004): 608&ndash;633; David Weisburd, &ldquo;Hot Spots Policing Experiments 
</p>
<p>and Criminal Justice Research: Lessons from the Field,&rdquo; Annals of the American Academy 
</p>
<p>of Political and Social Science 599 (2005): 220&ndash;245.
14See note 8; See also David P. Farrington, Lloyd E. Ohlin, and James Q. Wilson, 
</p>
<p>Understanding and Controlling Crime: Toward a New Research Strategy, (New York: 
</p>
<p>Springer-Verlag, 1986); David P. Farrington and Maria M. Ttofi, &ldquo;School-Based Programs 
</p>
<p>to Reduce Bullying and Victimization,&rdquo; Campbell Systematic Reviews, 6(6), 2009; Darrick 
</p>
<p>Jolliffe and David P. Farrington, A Rapid Evidence Assessment of the Impact of Mentoring 
</p>
<p>on Reoffending, (London: Home Office Online Report, 2007).
15 Farrington notes in this regard, &ldquo;(t)o understand why randomization ensures closer equiva-
</p>
<p>lence with larger samples, imagine drawing samples of  10, 100, or 1,000 unbiased coins. With 
</p>
<p>10 coins, just over 10 percent of  the samples would include 2 or less, or 8 or more, heads.  
</p>
<p>With 100 coins, just over 10 percent of  the samples would include 41 or less, or 59 or more, 
</p>
<p>heads. With 1,000 coins, just over 10 percent of  the samples would include 474 or less,  
</p>
<p>or 526 or more, heads. It can be seen that, as the sample size increases, the proportion of  heads 
</p>
<p>in it fluctuates in a narrower and narrower band around the mean figure of  50 percent&rdquo;  
</p>
<p>(see note 8 p. 263n).
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S684</p>
<p/>
</div>
<div class="page"><p/>
<p>advantage of prior knowledge about the distribution of units in an experimental 
</p>
<p>study to maximize equivalence of treatment and control conditions.
</p>
<p>We begin our discussion below by focusing on statistical theory, detailing how 
</p>
<p>block randomized trials maximize equivalence of experimental studies and pro-
</p>
<p>vide valid methods for increasing statistical power in small sample experiments. 
</p>
<p>We then turn to an empirical illustration of these arguments, drawing from data 
</p>
<p>used in the Jersey City Drug Market Analysis Experiment (Jersey City Experiment 
</p>
<p>(JCE)).16 Using data from the JCE and simulation methods, we illustrate the advan-
</p>
<p>tages of block randomization approaches over simple or &ldquo;na&iuml;ve&rdquo; randomization in 
</p>
<p>developing equivalent groups. We then illustrate the overall increase in statistical 
</p>
<p>power provided by the block randomized statistical modeling approach.
</p>
<p>Statistical Foundations for Block Randomization
</p>
<p>While criminological researchers are often unfamiliar with block randomized 
</p>
<p>studies, the first randomized experiment in crime and justice, the Cambridge-
</p>
<p>Somerville Youth Study, used a complete or a fully blocked randomized design.17 
</p>
<p>In that study, problem youths were paired on age, social background, biological 
</p>
<p>somatotype, and temperament. The researchers used this paired or fully blocked 
</p>
<p>design because the experimental treatment was lengthy and complex, so they 
</p>
<p>sought to maximize the equivalence of the comparisons they could make. Their 
</p>
<p>design is illustrated in Figure 21.4. In practice, the researchers matched the youths 
</p>
<p>into pairs on these characteristics and then randomly allocated them within the 
</p>
<p>pairs into treatment and control conditions. In the fully blocked randomized 
</p>
<p>design, each subject in a pair has an equal probability of being assigned to treat-
</p>
<p>ment or control conditions, but randomization is restricted in that one subject 
</p>
<p>from each pair must be assigned to treatment and one subject to control.
</p>
<p>16 See David Weisburd and Lorraine Green, &ldquo;Policing Drug Hot Spots: The Jersey City Drug 
</p>
<p>Market Analysis Experiment,&rdquo; Justice Quarterly 12 (1995): 711&ndash;735.
17 See Edwin Powers and Helen Witmer, An Experiment in the Prevention of Delinquency,  
</p>
<p>(New York: Columbia University Press, 1951).
</p>
<p> Figure 21.4 Fully Blocked (Matched Pairs) Random Assignment
</p>
<p>      S A M P L E  S I Z E ,  E Q U I V A L E N C E ,  A N D  S T A T I S T I C A L  P O W E R  685
</p>
<p>Sample of units (100)
</p>
<p>Units paired by similarity on relevant
</p>
<p>characteristics
</p>
<p>Pair 1
</p>
<p>Treatment (1) Control (1)
</p>
<p>Pair 2
</p>
<p>Treatment (1) Control (1)
</p>
<p>. . . Pair 50
</p>
<p>Treatment (1) Control (1)</p>
<p/>
</div>
<div class="page"><p/>
<p>What advantage does this approach give over na&iuml;ve or simple randomization? 
</p>
<p>Na&iuml;ve randomization (illustrated in Figure 21.5), which is the most common 
</p>
<p>approach in crime and justice experiments, assigns the total sample under study to 
</p>
<p>treatment or control conditions without restrictions. Every subject in this case has 
</p>
<p>an equal probability of being assigned to either the treatment or the control condi-
</p>
<p>tion. Na&iuml;ve randomization relies on the assumption that there are no systematic 
</p>
<p>reasons for the treatment and control subjects to differ (since every subject had an 
</p>
<p>mental studies in the first place. But it does not guarantee equivalence, simply that 
</p>
<p>there is no reason for non-equivalence. When samples are large, this assumption 
</p>
<p>is reasonable because large differences between the groups are unlikely by chance.
</p>
<p>subjects or the units in an experiment that can help us create equivalence on fac-
</p>
<p>tors that are related to the outcomes observed. Age and social background were 
</p>
<p>considered key predictors of delinquency by the Cambridge-Somerville research-
</p>
<p>ers, and their introduction as factors to match the youths in the study was seen as 
</p>
<p>a direct way of making sure that the treatment and control conditions were similar 
</p>
<p>on important influences of treatment success.
</p>
<p>However, the benefit of equivalence gained through fully blocked randomiza-
</p>
<p>tion comes at a statistical price. For each limitation on randomization the study 
</p>
<p>must &ldquo;pay a fine&rdquo; in terms of degrees of freedom. For example, in the Cambridge-
</p>
<p>Somerville Youth Study 650 boys were matched into pairs. In a na&iuml;ve randomiza-
</p>
<p>tion design with 325 cases per group, the study would have had 648 degrees of 
</p>
<p>freedom for statistical tests of significance (N
1
</p>
<p>N
2
</p>
<p>design, the degrees of freedom of the tests declined to 324 (N
pairs
</p>
<p>The loss of degrees of freedom is meaningful because it changes the distribu-
</p>
<p>tion of the test statistic. For example, as illustrated by  Equation 21.3 (dependent 
</p>
<p>samples) and 21.4 (independent samples) below, in a t-test the  estimated standard 
</p>
<p> Figure 21.5 Na&iuml;ve (Balanced) Random Assignment
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S686
</p>
<p>Sample of units (100)
</p>
<p>Treatment (50) Control (50)
</p>
<p>equal probability of assignment to each condition), a key raison d&rsquo;etre for experi-
</p>
<p>But why shouldn&rsquo;t we increase equivalence if we can, especially in small studies 
</p>
<p>where chance differences between control and treatment groups might be large in 
</p>
<p>the case of na&iuml;ve randomization? Fully blocked randomized designs like the 
</p>
<p>Cambridge-Somerville Youth Study assume that we have knowledge about the </p>
<p/>
</div>
<div class="page"><p/>
<p>deviations are divided by the degrees of freedom. This means that as the degrees 
</p>
<p>of freedom of a test get smaller the t-value observed also gets smaller:
</p>
<p>t =
X -
</p>
<p>s
</p>
<p>df
</p>
<p>d d
</p>
<p>d
</p>
<p>2
</p>
<p>m
 Equation 21.3
</p>
<p>t =
X - X - -
</p>
<p>N s +N s
</p>
<p>df
</p>
<p>N +N
</p>
<p>N N
</p>
<p>1 2 1 2
</p>
<p>2
</p>
<p>2 2
</p>
<p>2
</p>
<p>1 2
</p>
<p>1 2
</p>
<p>1 1
</p>
<p>( ) ( )m m
 Equation 21.4
</p>
<p>In turn, the value of the statistic needed to achieve statistical significance will be 
</p>
<p>larger as the degrees of freedom for a test get smaller (see the t distribution exam-
</p>
<p>ple in Table 21.1 below). This difference is not meaningful in the case of relatively 
</p>
<p>large sample studies. For example, in the Cambridge-Somerville experiment, with 
</p>
<p>324 degrees of freedom, the critical value of the t-test (with standard criteria of 
</p>
<p>p &lt; .05 and a nondirectional test) is about 1.967, almost the same as the 1.960 in 
</p>
<p>the z normal distribution without adjustment. But when the degrees of freedom 
</p>
<p>are reduced to 100, the critical value for the t-test becomes 1.984 and at 50 degrees 
</p>
<p>of freedom, 2.009.
</p>
<p>The balance between loss of degrees of freedom and greater equivalence is 
</p>
<p>weighted toward the goal of equivalence in disciplines where the causal processes 
</p>
<p>underlying the impacts of treatment are well understood. This is the case because 
</p>
<p>the benefits of the complete or the fully blocked randomized design are greatest 
</p>
<p>when each loss of degrees of freedom is accompanied by a gain in the equivalence 
</p>
<p>of the treatment and control conditions on factors that are related to treatment 
</p>
<p>outcomes. If treatment outcomes are conditioned by such factors, then blocking 
</p>
<p>will decrease the heterogeneity of outcomes in the study. Looking at Equation 21.3 
</p>
<p>and Equation 21.4 above, this would mean that the numerators of the standard 
</p>
<p>errors are made smaller and accordingly the t-values observed are larger. This 
</p>
<p>makes intuitive sense because if the groups are more similar in terms of what 
</p>
<p>would have been expected absent treatment, then it should be easier to identify a 
</p>
<p>treatment outcome. In statistical terms, there is likely to be less noise in identifying 
</p>
<p>that outcome. In the case of a fully blocked design in which treatment outcomes 
</p>
<p>DEGREES OF FREEDOM CRITICAL VALUE
</p>
<p> 10
 20
 50
100
200
324
500
648
</p>
<p>2.228
2.086
2.009
1.984
1.972
1.967
1.965
1.964
</p>
<p> Table 21.1 Critical Values for the t Distribution (Two-Tailed, α = .05)
</p>
<p>S A M P L E  S I Z E ,  E Q U I V A L E N C E ,  A N D  S T A T I S T I C A L  P O W E R  687</p>
<p/>
</div>
<div class="page"><p/>
<p>were not related to the blocking factors, the standard deviations would remain the 
</p>
<p>same as in a na&iuml;ve design, while there would be a substantial loss of degrees of 
</p>
<p>freedom. This would mean that a large price was paid for the fully blocked design 
</p>
<p>without a corresponding benefit.
</p>
<p>And here lies the primary argument against the use of fully blocked randomized 
</p>
<p>designs in crime and justice. The level of knowledge of the causal processes under-
</p>
<p>lying crime and justice research simply does not allow us to parse randomization 
</p>
<p>with sufficient distinctions to allow us to gain a benefit from a fully blocked rand-
</p>
<p>omized design. This is one of the main reasons that matched pair designs are not 
</p>
<p>common more generally in criminology, though we suspect that criminologists are 
</p>
<p>often uninformed about the benefits of fully blocked randomized designs.
</p>
<p>To say that criminologists do not have a full understanding of  causal processes 
</p>
<p>in place-based studies does not mean that they do not have sufficient knowledge to 
</p>
<p>improve experimental designs using block randomization methods. A compromise 
</p>
<p>approach between the fully blocked and na&iuml;ve randomization approach is a  partially 
</p>
<p>blocked or termed here simply, block randomized design. A block randomized 
</p>
<p>design makes no assumptions regarding the number of  blocks or groups identi-
</p>
<p>fied at the outset. Rather, the number of  blocks is determined by the researcher&rsquo;s 
</p>
<p>assessment of  the ability of  known data to group units in ways that maximize 
</p>
<p>their similarities on key causal variables. Cases are placed within the specified ran-
</p>
<p>domization blocks and then randomized within those blocks (see Figure 21.6). The 
</p>
<p>blocks do not have to be of  equal size, but the number of  cases in each block must 
</p>
<p>be even to allow for equal randomization and balance within blocks.
</p>
<p>An Example: Jersey City Experiment
</p>
<p>At this juncture it is worthwhile to introduce the substantive example we will 
</p>
<p>draw from for illustrating the benefits of  a block randomized design for place-
</p>
<p>based studies, the Jersey City Drug Market Analysis Experiment. The JCE evalu-
</p>
<p>ated an innovative drug enforcement strategy involving police crackdowns along 
</p>
<p>with citizen and local business engagement in controlling crime at drug markets. 
</p>
<p>A total of  56 high drug activity hot spots were randomly assigned in equal num-
</p>
<p>bers to receive either the experimental program or regular, unsystematic enforce-
</p>
<p>ment on an ad hoc basis. Most of  the drug market hot spots included fewer than 
</p>
<p>four street segments and intersections, though two places included more than 
</p>
<p>ten street segments. Police emergency calls for service for a variety of  crime and 
</p>
<p> Figure 21.6 Partially Blocked Random Assignment (Jersey City Drug Market Analysis Experiment)
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S688
</p>
<p>Hot spots (56)
</p>
<p>Very High
</p>
<p>(5 sets of matched pairs)
</p>
<p>T (5) C (5)
</p>
<p>High (8)
</p>
<p>T (4) C (4)
</p>
<p>Moderate (26)
</p>
<p>T (13) C (13)
</p>
<p>Low (12)
</p>
<p>T (6) C (6)</p>
<p/>
</div>
<div class="page"><p/>
<p>disorder-related issues were measured for 7-month pre- and post-intervention 
</p>
<p>periods. We focus below on three main outcome measures for disorder measured 
</p>
<p>in the study: suspicious persons, public morals, and police assistance.
</p>
<p>Knowing that there was considerable variation in criminal activity even across 
</p>
<p>the sample of hot spots, the study authors were concerned at the outset that the 
</p>
<p>prior level of crime would influence the effect of treatment. Given the small sam-
</p>
<p>ple of drug hot spots that could be identified in Jersey City, the authors were also 
</p>
<p>concerned that na&iuml;ve randomization might lead to non-equivalent groups. At the 
</p>
<p>same time, there was concern that each loss of degrees of freedom in the experi-
</p>
<p>ment would substantially impact the results, since the total N of available cases 
</p>
<p>was only 56. The solution in the JCE was to examine the distribution of both 
</p>
<p>emergency calls for service and arrests and then to identify natural cutting points.
</p>
<p>In this way the researchers believed that they could gain greater equivalence 
</p>
<p>between the groups without a large loss of degrees of freedom (28) that would 
</p>
<p>have ensued if the fully blocked randomized design was adopted. The assumption 
</p>
<p>here was that prior crime and disorder would have a general impact on the effects 
</p>
<p>of treatment but would not be specific enough to distinguish sites in a way that 
</p>
<p>would justify a complete randomized block design. The researchers identified eight 
</p>
<p>statistical blocks for randomization. The ten highest activity hot spots were rand-
</p>
<p>omized in pairs because of large gaps between them; these five pairs represented 
</p>
<p>the five &ldquo;very-high-activity&rdquo; statistical blocks. Of the rest of the sample of hot 
</p>
<p>spots, 8 were grouped into a &ldquo;high-activity&rdquo; block, 26 hot spots were classified as 
</p>
<p>a medium-activity block, and 12 were classified as a low-activity block.
</p>
<p>The Benefits of Block Randomized Trials
</p>
<p>One approach to examining the contribution of statistical blocking to equivalence 
</p>
<p>in the Jersey City Drug Market Analysis Experiment is to compare the equivalence 
</p>
<p>gained between the treatment and control conditions on key baseline (pretest) 
</p>
<p>measures. However, the Jersey City study is only one specific draw of randomi-
</p>
<p>zation. By definition any specific draw of a sample is going to be different from 
</p>
<p>another draw. The statistical concern is whether on average, a draw using the 
</p>
<p>block randomization procedure is likely to produce a more equivalent outcome 
</p>
<p>than a draw using a simple randomization procedure. To examine this question we 
</p>
<p>develop 10,000 simulations of both na&iuml;ve randomization and block randomization 
</p>
<p>using the Jersey City data.18 We focus on baseline calls for service for the three key 
</p>
<p>disorder outcomes in the study (suspicious persons, public morals, and assistance). 
</p>
<p>Table 21.2 reports the baseline information from the original study, the simula-
</p>
<p>tion results for the blocking approach, and the simulation outcomes of a na&iuml;ve 
</p>
<p>18 Stata programs were developed to run a randomization sequence (blocked or na&iuml;ve) on the 
</p>
<p>JCE dataset and then run a t-test comparing the treatment and control group means at baseline 
</p>
<p>on the three outcomes of  interest. Stata&rsquo;s simulation function was then used to run each  
</p>
<p>program 10,000 times and create a dataset containing the group means, t-values, p-values, an 
</p>
<p>indicator showing whether or not the two groups were significantly different at baseline for 
</p>
<p>each iteration, and the absolute average mean group difference across all iterations. We are 
</p>
<p>grateful to David B. Wilson for developing the programs and simulation syntax.
</p>
<p>S A M P L E  S I Z E ,  E Q U I V A L E N C E ,  A N D  S T A T I S T I C A L  P O W E R  689</p>
<p/>
</div>
<div class="page"><p/>
<p> Table 21.2
</p>
<p>19 Of  course, this is about what we would have expected given a .10 significance threshold 
</p>
<p>and a fair randomization procedure. But the important point is that the block randomization 
</p>
<p>approach allows us to do better.
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S690
</p>
<p>Calls for Service at Baseline in Block and Na&iuml;ve Randomizations of JCE Data
</p>
<p>SUSPICIOUS PERSONS PUBLIC MORALS ASSISTANCE
</p>
<p>Original JCE block randomized data
</p>
<p> Treatment mean (SD)
 Control mean (SD)
 Absolute mean difference (SE)
</p>
<p>17.00 (16.15)
17.93 (21.16)
</p>
<p>.93 (5.03)
</p>
<p>9.32 (10.58)
9.11 (13.36)
</p>
<p>.21 (3.22)
</p>
<p>43.86 (43.40)
42.11 (43.05)
1.75 (11.55)
</p>
<p>10,000 simulations Block randomized data (N = 56)
</p>
<p> Average absolute mean difference across all samples (SD)
  Number of samples with significant difference  
</p>
<p>at baseline (p &le; 0.10)
</p>
<p>2.67 (1.95)
93
</p>
<p>1.29 (0.94)
4
</p>
<p>7.11 (5.05)
218
</p>
<p>10,000 simulations Na&iuml;ve randomized data (N = 56)
</p>
<p> Average absolute mean difference across all samples (SD)
  Number of samples with significant difference  
</p>
<p>at baseline (p &le; 0.10)
</p>
<p>3.98 (2.89)
955
</p>
<p>2.56 (1.86)
989
</p>
<p>9.13 (6.71)
966
</p>
<p>randomization approach. In the case of the simulations, we report the number of 
</p>
<p>samples that have significant differences at baseline and the overall absolute mean 
</p>
<p>difference found in the 10,000 simulation samples.
</p>
<p>Table 21.2 suggests the importance of the simulation approach. The specific 
</p>
<p>draw in the Jersey City study produced unusually equivalent groups on the three 
</p>
<p>baseline measures examined. The absolute mean difference for all three measures 
</p>
<p>is substantially lower than the average absolute mean difference produced in our 
</p>
<p>simulations. This does not mean that the Jersey City randomization was flawed but 
</p>
<p>rather that the investigators by chance gained one of the more equivalent randomi-
</p>
<p>zations from the sampling distribution of all possible randomizations.
</p>
<p>But despite the fact that the Jersey City randomization was a relatively &ldquo;lucky 
</p>
<p>draw,&rdquo; it is clear from Table 21.2 that the procedure used was likely to produce 
</p>
<p>much more equivalent groups than a simple randomization procedure. In the 
</p>
<p>10,000 simulations of the JCE block randomization procedure only 93 simulations 
</p>
<p>produced significantly different outcomes (p &lt; .10) for treatment and control con-
</p>
<p>ditions at baseline for suspicious persons calls, 4 for public moral calls, and 218 
</p>
<p>for assistance calls. In contrast, using the simple randomization approach on the 
</p>
<p>same 56 cases, 955 samples produced significant differences for suspicious person 
</p>
<p>calls, 989 for public moral calls, and 966 for assistance calls.19 These differences 
</p>
<p>are of substantial magnitude and are also reflected in the average absolute mean 
</p>
<p>difference across all of the simulation samples. For suspicious persons the mean 
</p>
<p>differences were almost 50 % larger in the na&iuml;ve randomization sample, for public 
</p>
<p>morals about twice as large, and for assistance almost a third larger.</p>
<p/>
</div>
<div class="page"><p/>
<p>S t a t i s t i c a l  P o w e r  a n d  B l o c k  R a n d o m i z a t i o n
</p>
<p>As we noted earlier, the benefits of a block randomized design are dependent 
</p>
<p>on the assumption that the blocking factors are related to the study outcomes. 
</p>
<p>This cannot be assessed ordinarily because knowledge about treatment outcomes 
</p>
<p>is unknown until the experiment is complete. However, we are able to exam-
</p>
<p>ine this assumption using post-experiment data from Jersey City. For all three 
</p>
<p> outcomes of interest discussed here, the correlations between pre- and posttest 
</p>
<p>outcomes were significant and had greater than a moderate size coefficient.20 
</p>
<p>For suspicious persons the correlation was .44 (p &lt; .10),21 for public morals .52 
</p>
<p>(p &lt; .01), and for assistance .63 (p &lt; .001). These results suggest that Weisburd and 
</p>
<p>Green&rsquo;s assumption that there would be a strong relationship between the blocking 
</p>
<p>factors and the final study outcomes was correct.
</p>
<p>In turn, the statistical model benefits of the identification of block variability 
</p>
<p>can be observed directly in these data. A simple or a na&iuml;ve randomized experi-
</p>
<p>ment presents a model for understanding outcomes where systematic variation is 
</p>
<p>determined only by treatment. Accordingly, the model can be expressed in terms 
</p>
<p>of sums of squares (SS; see Chapter 12) by Equation 21.5:
</p>
<p>SS SS +SS +SS
total intercept group error
</p>
<p>=  Equation 21.5
</p>
<p>The total variability of the study in this case is broken down to the influence of the 
</p>
<p>treatment (SS
group
</p>
<p>) and the overall variability in the data (SS
error
</p>
<p>), with the intercept 
</p>
<p>(SS
intercept
</p>
<p>) completing the linear model. Remember that the statistical denominator 
</p>
<p>for the statistical significance test is going to be the error term, meaning that as the 
</p>
<p>error term gets smaller the significance of the test will get larger.
</p>
<p>With the introduction of a blocking factor, an additional source of variability is 
</p>
<p>taken into account in the model&mdash;SS
block
</p>
<p>&mdash;as illustrated in Equation 21.6:
</p>
<p>SS = SS +SS +SS +SS
total intercept group block error  Equation 21.6
</p>
<p>would expect the overall size of SS
error
</p>
<p> to decline. This is the case because the block 
</p>
<p>randomized model limits any relationship between SS
group
</p>
<p> (i.e., the treatment com-
</p>
<p>ponent of the model) and SS
block
</p>
<p>, meaning that the two components of variability 
</p>
<p>are constrained to be independent (because they are independent, the inclusion of 
</p>
<p>the blocking factor in the model will not impact the size of the treatment effect). 
</p>
<p>Accordingly, any SS
block
</p>
<p> effect will be drawn out of the error term for the model 
</p>
<p>(SS
total
</p>
<p> is a fixed quantity irrespective of the model defined). Since SS
error
</p>
<p> is a key 
</p>
<p>21 We calculated the correlation between the blocking factor and the three disorder outcome 
</p>
<p>measures by running a GLM with only the blocking factor included. The correlation is based 
</p>
<p>on taking the square root of  the overall R2 of  the model. We use a one-tailed test of   
</p>
<p>significance following the assumption that the correlation between the blocking factor  
</p>
<p>and the outcome is positive.
</p>
<p>20 See Jacob Cohen, Statistical Power Analysis for the Behavioral Sciences, 2nd ed., 
</p>
<p>(Hillsdale, NJ: Lawrence Erlbaum, 1988).
</p>
<p>S T A T I S T I C A L  P O W E R  A N D  B L O C K  R A N D O M I Z A T I O N  691
</p>
<p>When the blocking factor (SS
block
</p>
<p>) is strongly related to the outcome (SS
total
</p>
<p>) we </p>
<p/>
</div>
<div class="page"><p/>
<p> Table 21.3
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S692
</p>
<p>element of the denominator of the test statistic (the degrees of freedom being a 
</p>
<p>second key element), its reduction without a proportionate reduction in  SS
group
</p>
<p> 
</p>
<p>(and a large decrease in degrees of freedom) will lead to a more significant outcome 
</p>
<p>(i.e., a more powerful statistical outcome) than a na&iuml;ve model.
</p>
<p>As an illustration of these assumptions, we estimated univariate ANOVA 
</p>
<p>models using just the treatment factor, and separately with the treatment and 
</p>
<p>blocking factors as fixed effects (Table 21.3).22 In order to simplify our example, 
</p>
<p>we do not estimate the full model which could include a treatment by block inter-
</p>
<p>action (see later).23 Following our assumptions, SS
total
</p>
<p>  and SS
group
</p>
<p> are the same in 
</p>
<p>both models. The total variability in the model is constant irrespective of model 
</p>
<p>specification, and the effect of treatment is not influenced because of the balanced 
</p>
<p>randomization of cases within blocks. However, SS
error
</p>
<p> declines in the analyses that 
</p>
<p>include blocking as a factor. For suspicious persons the decline is from 6,373.643 
</p>
<p>to 5,021.566, for public morals from 2,884.107 to 2,080.761, and for assistance 
</p>
<p>from 14,875.821 to 8,788.761. Note as well that there is a corresponding decrease 
</p>
<p>in the degrees of freedom of SS
error
</p>
<p> in the block randomized design (from 54 to 
</p>
<p>47 in all the models), reflecting the &ldquo;price&rdquo; of this approach. But, following our 
</p>
<p>examination of the correlation between the blocking factor and the outcomes, the 
</p>
<p>loss of statistical power generated by the reduction of degrees of freedom of the 
</p>
<p>22 In this case the effects are fixed because we are assuming analysis of  specific categories and 
</p>
<p>do not assume that those categories are representative of  the population of  cases. For example, 
</p>
<p>the analysis looks at the specific blocks of  hot spots in the experiment; it does not assume that 
</p>
<p>we have a representative sample of  all possible &ldquo;blocks&rdquo; of  hot spots. If  the effects were ran-
</p>
<p>dom, the blocks observed would be seen as a representative sample of  blocks of  hot spots.
23 Where the interaction between treatment and block is significant, Fleiss recommends including 
</p>
<p>an interaction term in the model. When the blocking factor represents a substantively important 
</p>
<p>variable, the introduction of  a block by treatment interaction can also add knowledge about 
</p>
<p>the differential effects of  treatment across values of  the blocking variable. See Joseph L. Fleiss, 
</p>
<p>The Design and Analysis of Clinical Experiments, (New York: John Wiley and Sons, 1986); 
</p>
<p>David Weisburd and Faye Taxman, &ldquo;Developing a Multicenter Randomized Trial in Criminology: 
</p>
<p>The Case of  HIDTA,&rdquo; Journal of Quantitative Criminology 16 (200): 315&ndash;340.
</p>
<p>UNIVARIATE ANALYSIS  
OF VARIANCE MODELS SUSPICIOUS PERSONS PUBLIC MORALS ASSISTANCE
</p>
<p>Treatment-only model
</p>
<p>SS
intercept
</p>
<p> (df)
SS
</p>
<p>group
 (df)
</p>
<p>SS
error
</p>
<p> (df)
SS
</p>
<p>TOTAL
 (df)
</p>
<p>F (p) for group effect
</p>
<p>480.286 (1)
516.071 (1)**
</p>
<p>6,373.643 (54)
7,370.000 (56)
</p>
<p>4.372 (.041**)
</p>
<p>21.875 (1)
129.018 (1)
</p>
<p>2,884.107 (54)
3,035.000 (56)
</p>
<p>2.416 (.126)
</p>
<p>1,512.161 (1)
355.018 (1)
</p>
<p>14,875.821 (54)
16,743.000 (56)
</p>
<p>1.289 (.261)
</p>
<p>Treatment and block model
</p>
<p>SS
intercept
</p>
<p> (df)
SS
</p>
<p>group
 (df)
</p>
<p>SS
block
</p>
<p> (df)
SS
</p>
<p>error
 (df)
</p>
<p>SS
TOTAL
</p>
<p> (df)
F (p) for group effect
</p>
<p>510.998 (1)
516.071 (1)**
</p>
<p>1,352.076 (7)
5,021.566 (47)
7,370.000 (56)
</p>
<p>4.830 (.033**)
</p>
<p>145.484 (1)
129.018 (1)*
803.346 (7)
</p>
<p>2,080.761 (47)
3,035.000 (56)
</p>
<p>2.914 (.094*)
</p>
<p>3,250.491 (1)
355.018 (1)
</p>
<p>6,087.060 (7)
8,788.761 (47)
</p>
<p>16,743.000 (56)
1.899 (.175)
</p>
<p>Notes: *p &lt; .10 **p &lt; .05.
</p>
<p>Univariate Analysis of Variance for Treatment and Treatment&ndash;Block Effects (JCE)</p>
<p/>
</div>
<div class="page"><p/>
<p>U S I N G  C O V A R I A T E S  T O  I N C R E A S E  S T A T I S T I C A L  P O W E R 693
</p>
<p>error term is less than the gain from the inclusion of the blocking term. When we 
</p>
<p>combine treatment and block effects in the model, all three comparisons show 
</p>
<p>larger F-statistics. The observed p-value for suspicious persons declines from .041 
</p>
<p>to .033, for public morals from .126 to .094, and for assistance from .261 to .175.
</p>
<p>U s i n g  C o v a r i a t e s  t o  I n c r e a s e  S t a t i s t i c a l  P o w e r  
i n  E x p e r i m e n t a l  S t u d i e s
</p>
<p>Another technique for increasing statistical power in experimental studies follows 
</p>
<p>the statistical logic of block randomization but does not balance the blocking 
</p>
<p>characteristics at the outset. It relies heavily on the logic of randomization that we 
</p>
<p>have already described. As noted earlier, if the cases are randomized to treatment 
</p>
<p>and control conditions then we can assume that there is no correlation between 
</p>
<p>treatment and possible confounding factors (see Figure 21.2). That means that 
</p>
<p>the inclusion of additional covariates in an analysis will not, in theory, affect the 
</p>
<p>estimate of the treatment effect. Since that is the case, we should be able to include 
</p>
<p>covariates without creating any bias in our assessment of the influence of the 
</p>
<p>experimental variable.
</p>
<p>However, we do gain a direct benefit in calculating the statistical significance 
</p>
<p>of the test. Let us use again the approach of examining the sums of squares of our 
</p>
<p>equation. Suppose we convert the JCE to a simple na&iuml;ve randomization sequence. 
</p>
<p>In this case our model includes treatment and error as the only variables (see 
</p>
<p>Equation 21.5).
</p>
<p>If we add covariates the error term for the model should decline, because as we 
</p>
<p>noted earlier we have no reason to expect that the effect of treatment (i.e., group) 
</p>
<p>will change (see Equation 21.7). As an example, let us add as covariates variables 
</p>
<p>that should be related to the dependent variables: the pre-experiment calls for ser-
</p>
<p>vice for robbery and aggravated assault (collectively the baseline violent crime calls 
</p>
<p>for service) and the baseline calls for service for each respective outcome measure. 
</p>
<p>Thus, for each outcome, we include three covariates: robbery at baseline, aggra-
</p>
<p>vated assault at baseline, and the outcome at baseline (e.g., suspicious person calls 
</p>
<p>at baseline for the suspicious person outcome):
</p>
<p>SS
total 
</p>
<p>= SS
intercept 
</p>
<p>+ SS
group 
</p>
<p>+ SS
covariates 
</p>
<p>+ SS
error
</p>
<p> Equation 21.7
</p>
<p>In Table 21.4 we show the results using the simple na&iuml;ve design as well as 
</p>
<p>the results we would gain for taking into account the three covariates for each 
</p>
<p>outcome. As can be seen from the table, the statistical significance of the results 
</p>
<p>including the covariates is considerably lower than when no covariates are includ-
</p>
<p>ed. For public morals, for example, the p-value for the group effect has dropped 
</p>
<p>from a non-statistically significant .126 in the na&iuml;ve example to a significant .041 
</p>
<p>when including the covariates. For all three outcomes, we have substantially low-
</p>
<p>ered the SS
error
</p>
<p> by adding the covariates. Even though we paid a price in degrees of 
</p>
<p>freedom for using three covariates, the benefit in terms of reducing the error and 
</p>
<p>increasing the significance of our group findings outweighs the cost. We should </p>
<p/>
</div>
<div class="page"><p/>
<p> Table 21.4
</p>
<p>       C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S694
</p>
<p>be cautious in including these covariates, however. Note that we would expect the 
</p>
<p>effect of treatment to remain similar between the simple model and the model 
</p>
<p>with covariates in terms of the sums of squares explained. This is largely true for 
</p>
<p>suspicious persons and public morals where the SS
group
</p>
<p> remains fairly similar in 
</p>
<p>both sections of Table 21.4. For assistance, however, there is a large increase in 
</p>
<p>the SS
group
</p>
<p> potentially suggesting that we may have introduced some level of bias 
</p>
<p>into the model with our choice of covariates.
</p>
<p>Importantly, if we had the data available, we could add many different covari-
</p>
<p>ates to the models. Those covariates could be related to the characteristics of the 
</p>
<p>units of randomization, for example the social characteristics of the hot spots. 
</p>
<p>In a study of individuals we might include gender or age or race to the analysis. 
</p>
<p>In some sense, the larger the group of covariates that are related to the outcome 
</p>
<p>measure that are included, the greater the benefit. This is because each additional 
</p>
<p>variable included that is relevant to the prediction of the outcome will decrease the 
</p>
<p>error term for the significance test. The rule does not apply if there is no relation-
</p>
<p>ship between the covariate and the outcome. In that case, the researcher will lose 
</p>
<p>a degree of freedom (for every additional variable or parameter) in the analysis 
</p>
<p>without an additional benefit in reduction of the error variance.
</p>
<p>As is apparent, there is much to be gained by including covariates in an experi-
</p>
<p>mental analysis. However, as in other statistical procedures, covariates can be 
</p>
<p>manipulated in ways that affect the validity of your results. Randomization allows 
</p>
<p>us to assume that there is no relationship between the covariate and the treatment 
</p>
<p>or the variable interest. But this does not mean that there is not in the sample 
</p>
<p>of interest a spurious relationship that is observed. In any randomization there 
</p>
<p>are likely to be some measures that by chance are related to the treatment. What 
</p>
<p>randomization guarantees is that such bias will be random, and it is likely in the 
</p>
<p>long run that whenever there is a spurious correlation it is likely to be balanced 
</p>
<p>off with another correlation in the opposite direction. But what if the researcher 
</p>
<p>UNIVARIATE ANALYSIS  
OF VARIANCE MODELS SUSPICIOUS PERSONS PUBLIC MORALS ASSISTANCE
</p>
<p>Treatment-only model
</p>
<p>SS
intercept
</p>
<p> (df)
</p>
<p>SS
group
</p>
<p> (df)
</p>
<p>SS
error
</p>
<p> (df)
</p>
<p>SS
TOTAL
</p>
<p> (df)
</p>
<p>F (p) for group effect
</p>
<p>480.286 (1)
</p>
<p>516.071 (1)**
</p>
<p>6,373.643 (54)
</p>
<p>7,370.000 (56)
</p>
<p>4.372 (.041**)
</p>
<p>Treatment and covariate model
</p>
<p>SS
intercept
</p>
<p> (df)
</p>
<p>SS
group
</p>
<p> (df)
</p>
<p>SS
covariate (pre - outcome)
</p>
<p> (df)
</p>
<p>SS
covariate (pre - robbery)
</p>
<p> (df)
</p>
<p>SS
covariate (pre - assault)
</p>
<p> (df)
</p>
<p>SS
error
</p>
<p> (df)
</p>
<p>SS
TOTAL
</p>
<p> (df)
</p>
<p>F (p) for group effect
</p>
<p>188.025 (1)
</p>
<p>559.666 (1)**
</p>
<p>678.443 (1)**
</p>
<p>752.979 (1)**
</p>
<p>55.281 (1)
</p>
<p>5,334.804 (51)
</p>
<p>7,370.000 (56)
</p>
<p>5.350 (.025**)
</p>
<p>Notes: *p &lt; .10 **p &lt; .05.
</p>
<p>Univariate Analysis of Variance for Treatment and Treatment&ndash;Covariate Effects (JCE)
</p>
<p>21.875 (1)
</p>
<p>129.018 (1)
</p>
<p>2,884.107 (54)
</p>
<p>3,035.000 (56)
</p>
<p>2.416 (.126)
</p>
<p>1,512.161 (1)
</p>
<p>355.018 (1)
</p>
<p>14,875.821 (54)
</p>
<p>16,743.000 (56)
</p>
<p>1.289 (.261)
</p>
<p>34.122 (1)
</p>
<p>162.019 (1)**
</p>
<p>751.986 (1)**
</p>
<p>160.373 (1)**
</p>
<p>15.301 (1)
</p>
<p>1,875.149 (51)
</p>
<p>3,035.000 (56)
</p>
<p>4.407 (.041**)
</p>
<p>84.815 (1)
</p>
<p>638.146 (1)*
</p>
<p>305.731 (1)
</p>
<p>2403.850 (1)**
</p>
<p>405.114 (1)
</p>
<p>9596.472 (51)
</p>
<p>16,743.000 (56)
</p>
<p>3.391 (.071*)</p>
<p/>
</div>
<div class="page"><p/>
<p>chooses specific variables that have relationships in the dataset with the treatment 
</p>
<p>or the variable of interest? In this case, the error term will be reduced if these 
</p>
<p>measures are correlated with the outcome, but so will the estimate of the treatment 
</p>
<p>effect. Again, here as in regression analyses more generally, the researcher can &ldquo;go 
</p>
<p> fishing&rdquo; until the result they are looking for is gained.
</p>
<p>The dangers of influencing the validity of the treatment effect should lead to 
</p>
<p>caution in using covariates to reduce error variance in the analysis of experimental 
</p>
<p>studies. A general rule that will protect you from the danger of manipulation of 
</p>
<p>results is for the researcher to define at the outset which covariates will be used 
</p>
<p>in analyzing the outcomes. In this way, the researcher cannot manage results post 
</p>
<p>facto on the basis of knowledge of sample characteristics. Clearly, one should not 
</p>
<p>run a large number of regressions with different covariates included until a &ldquo;good&rdquo; 
</p>
<p>result is gained. The process of selecting variables before the results of an experi-
</p>
<p>ment are known is in our view a good rule to follow. But more generally, if an 
</p>
<p>experiment has sufficient statistical power, the researcher should use the simple 
</p>
<p>analysis approach, in which covariates are not included. This is the only way to 
</p>
<p>guarantee that the results are not being manipulated in a way that might lead to 
</p>
<p>spurious findings.
</p>
<p>E x a m i n i n g  I n t e r a c t i o n  T e r m s  i n  E x p e r i m e n t a l  R e s e a r c h
</p>
<p>The best way to examine interactions in an experimental study is to define the 
</p>
<p>contextual factor at the outset. For example, in the JCE, the block randomiza-
</p>
<p>tion procedure was based on the level of crime observed in the baseline year. For 
</p>
<p>illustration purposes we use four blocks in our analysis: very high, high, moderate, 
</p>
<p>and low. One might ask whether the level of crime was related to the effective-
</p>
<p>ness of treatment. Importantly, because of the use of block randomization the 
</p>
<p>cases are equally divided between the blocks, meaning that the design of the study 
</p>
<p>ensures that there is no relationship between block and treatment. There are an 
</p>
<p>equal number of very high, high, moderate, and low baseline crime hot spots in 
</p>
<p>the experimental and control groups. We add to our equation an additional term 
</p>
<p>beyond treatment or group and block&mdash;that of the interaction between treatment 
</p>
<p>and block as illustrated in Equation 21.8:
</p>
<p>SS
total
</p>
<p>=SS
intercept
</p>
<p>+SS
group
</p>
<p>+SS
block
</p>
<p>+SS
group&times;block
</p>
<p>+SS
error
</p>
<p> Equation 21.8
</p>
<p>       E X A M I N I N G  I N T E R A C T I O N  T E R M S  I N  E X P E R I M E N T A L  R E S E A R C H 695
</p>
<p>Some scholars have criticized experimental  studies because in their simplest form 
</p>
<p>they do not allow us to examine contextual factors that might influence treatments 
</p>
<p>or outcomes. For example, the average treatment effect observed in an experiment 
</p>
<p>will tell you whether the treatment is effective or not for the entire sample. But 
</p>
<p>what if we are interested in understanding whether the treatment effect differs for 
</p>
<p>men and women or younger versus older subjects. Most experimental studies in 
</p>
<p>criminology have not looked at these &ldquo;interaction effects&rdquo; between treatment and 
</p>
<p>other factors. But this does not mean that interactions cannot be observed in a 
</p>
<p>valid way in experimental studies.</p>
<p/>
</div>
<div class="page"><p/>
<p> Table 21.5  
</p>
<p> Table 21.6
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S696
</p>
<p>In Table 21.5 we report the results using this approach. The addition of  
</p>
<p>another term has again reduced the error variance. But more importantly, given 
</p>
<p>our discussion, we now have an answer to the question of  whether the treatment 
</p>
<p>differs in effect across the groups. The significance statistic for the interaction 
</p>
<p>between group and block is significant only for suspicious persons. Accordingly, 
</p>
<p>we now have an experimental result that tells us that the effect of  treatment on 
</p>
<p>suspicious person calls for service is conditioned by the level of  crime in the 
</p>
<p>baseline year (i.e., the blocking factor). Table 21.6 shows these results producing 
</p>
<p>the effect of  treatment for each of  the four blocks for suspicious persons. The 
</p>
<p>table suggests that the effect is consistent in three of  the four blocks represent-
</p>
<p>ing higher crime rates and that the overall effect of  the treatment is much larger 
</p>
<p>in the highest rate blocks. However, in the low rate blocks, which represented 
</p>
<p>much less serious crime areas, the observed treatment effect is in the opposite 
</p>
<p>direction. These results would likely lead the researchers to recommend the hot 
</p>
<p>spot policing strategy tested in the Jersey City study for higher rate drug market 
</p>
<p>hot spots, at least in regard to influencing suspicious persons. It might also raise 
</p>
<p>questions regarding why the treatment did not have the desired effect in the lower 
</p>
<p>crime areas.
</p>
<p>BLOCK TREATMENT MEAN (SD) CONTROL MEAN (SD)
</p>
<p>Very high (n = 5)
High (n = 4)
Moderate (n = 13)
Low (n = 6)
Total (n = 28)
</p>
<p>&minus;4.20 (6.979)
&minus;12.00 (8.246)
</p>
<p>3.15 (9.353)
4.17 (6.145)
&minus;.11 (9.689)
</p>
<p>18.40 (19.995)
8.25 (5.188)
5.69 (5.991)
</p>
<p>&minus;5.33 (5.785)
5.96 (11.924)
</p>
<p>Mean Change in Suspicious Person Calls for Service by Block and Group (JCE)
</p>
<p>UNIVARIATE ANALYSIS  
OF VARIANCE MODEL SUSPICIOUS PERSONS PUBLIC MORALS ASSISTANCE
</p>
<p>Treatment, block, and group &times; block model
</p>
<p>SS
intercept
</p>
<p> (df)
</p>
<p>SS
group
</p>
<p> (df)
</p>
<p>SS
block
</p>
<p> (df)
</p>
<p>SS
group &times; block
</p>
<p> (df)
</p>
<p>SS
error
</p>
<p> (df)
</p>
<p>SS
TOTAL
</p>
<p> (df)
</p>
<p>F (p) for group x block effect
</p>
<p>236.940 (1)
</p>
<p>928.490 (1)**
</p>
<p>564.676 (3)*
</p>
<p>1,893.588 (3)**
</p>
<p>3,915.378 (48)
</p>
<p>7,370.000 (56)
</p>
<p>7.738 (&lt;.001**)
</p>
<p>55.582 (1)
</p>
<p>150.470 (1)*
</p>
<p>581.946 (3)**
</p>
<p>88.137 (3)
</p>
<p>2,214.024 (48)
</p>
<p>3,035.000 (56)
</p>
<p>0.637 (.595)
</p>
<p>1,729.492 (1)
</p>
<p>693.174 (1)
</p>
<p>1,865.060 (3)*
</p>
<p>559.706 (3)
</p>
<p>12,451.055 (48)
</p>
<p>16,743.000 (56)
</p>
<p>0.719 (.545)
</p>
<p>Notes: * p &lt; .10 ** p &lt; .05.
</p>
<p>Univariate Analysis of Variance for Treatment, Block, and Group by Block 
</p>
<p>Effects (JCE)</p>
<p/>
</div>
<div class="page"><p/>
<p>Some scholars suggest that interaction terms can be added without a block 
</p>
<p> randomized design.24 This approach is based on the assumption we noted earlier 
</p>
<p>that randomization is likely to lead to balance between the groups on  characteristics 
</p>
<p>that might be measured. The problem of course is, as we noted earlier, that there 
</p>
<p>may be chance relationships between variables in your observed data and this 
</p>
<p>might lead to spurious results. It seems reasonable to us that the inclusion of 
</p>
<p>interaction terms without block randomization requires very large samples. Only 
</p>
<p>in such cases can you rely on randomization providing equivalence across a large 
</p>
<p>number of measures. We recommend samples larger than 300 per group before 
</p>
<p>examining interactions in the data without block randomization. Moreover, the 
</p>
<p>researchers should examine whether the data are balanced (and thus there is no 
</p>
<p>relationship between treatment and covariate) in each specific case. With smaller 
</p>
<p>samples, this approach should be carried out with caution, and the interactions 
</p>
<p>observed should be identified at the outset. In any event, only a discrete number 
</p>
<p>of interactions should be observed.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>Randomized experiments provide higher levels of internal validity than observa-
</p>
<p>tional studies in terms of determining the impacts of a treatment or an interven-
</p>
<p>tion. Through the process of determining an eligibility pool, randomizing the 
</p>
<p>eligible participants or units, and assigning them to treatment or control groups, 
</p>
<p>researchers can better deal with the problem of confounding in posttest measures 
</p>
<p>relevant to the dependent variable of interest. Randomized experiments have the 
</p>
<p>highest possible internal validity as they allow us to assume that confounding 
</p>
<p>causes of the dependent variable are not a concern&mdash;this since treatment has been 
</p>
<p>allocated randomly and we can assume that possible confounding factors are not 
</p>
<p>systematically related to treatment.
</p>
<p>For concerns of statistical power related to smaller sample experiments, 
</p>
<p>blocked randomized trials can maximize equivalence of experimental studies. 
</p>
<p>Another way of increasing the statistical power in experimental studies is with the 
</p>
<p>inclusion of covariates; however, covariates should be used cautiously as they can 
</p>
<p>allow the researcher to manipulate the results of the study.
</p>
<p>One criticism of experimental research is that in its simplest form it is unable to 
</p>
<p>examine contextual factors that may influence treatments or outcomes. Although 
</p>
<p>many experimental studies in criminology have not looked at interaction effects 
</p>
<p>between treatments and other factors that should not be taken to mean that inter-
</p>
<p>actions cannot be observed with experimental designs. The best way to examine 
</p>
<p>such interactions is to define them at the outset and use block randomization 
</p>
<p>techniques.
</p>
<p>24 See Barak Ariel and David Farrington, &ldquo;Block Randomized Trials,&rdquo; In Alex R. Piquero and 
</p>
<p>David Weisburd (eds.), Handbook of Quantitative Criminology, (New York: Springer, 2010: 
</p>
<p>437&ndash;454).
</p>
<p>C H A P T E R  S U M M A R Y 697</p>
<p/>
</div>
<div class="page"><p/>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>To compute the regression coefficient b for a treatment variable (Tr) controlling 
</p>
<p>for a confounding factor (CC):
</p>
<p>b =
r - r r
</p>
<p>- r
</p>
<p>S
</p>
<p>S
Tr
</p>
<p>Y Tr Y CC TrCC
</p>
<p>2
</p>
<p>TrCC
</p>
<p>Y
</p>
<p>Tr
</p>
<p>1 1
</p>
<p>1
</p>
<p>( )



</p>
<p>
</p>
<p>



</p>
<p>

</p>
<p>
</p>
<p>With the basic assumption that we have enough knowledge to create equivalence 
</p>
<p>of units in the treatment and control conditions:
</p>
<p>b = r
S
</p>
<p>S
Tr Y Tr
</p>
<p>Y
</p>
<p>Tr
1
</p>
<p>( )

</p>
<p>
</p>
<p>
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S698
</p>
<p>K e y  T e r m s
</p>
<p>block randomization A type of  
</p>
<p>randomization whereby cases are first sorted  
</p>
<p>into like groups and then afterwards randomly 
</p>
<p>allocated into treatment and control  
</p>
<p>conditions.
</p>
<p>group allocation In criminological experiments, 
</p>
<p>eligible cases are randomly assigned to two or 
</p>
<p>more groups&mdash;typically treatment or control.
</p>
<p>treatment group One group that eligible cases 
</p>
<p>are randomly assigned to which receives the treat-
</p>
<p>ment or the intervention being evaluated.
</p>
<p>control group The group that eligible cases 
</p>
<p>are randomly assigned to which does not receive 
</p>
<p>the treatment or the intervention being evaluated. 
</p>
<p>In many criminological experiments the control 
</p>
<p>group may receive existing interventions in  
</p>
<p>contrast to the innovative treatment.
</p>
<p>posttest measure Analyses conducted by the 
</p>
<p>researcher to determine if the intervention had any 
</p>
<p>impact on the outcome measures of interest.
</p>
<p>internal validity Whether the research design 
</p>
<p>has allowed for the impact of  the intervention 
</p>
<p>or the treatment to be clearly distinguished from 
</p>
<p>other factors.
</p>
<p>alternation A type of quasi-random assignment 
</p>
<p>in which researchers assign every other case to 
</p>
<p>one particular group.
</p>
<p>confounding factors Variables associated  
</p>
<p>with treatments and/or outcomes that can  
</p>
<p>bias overall results if not controlled for  
</p>
<p>statistically.
</p>
<p>eligibility pool Participants or units that are 
</p>
<p>eligible for an experiment.
</p>
<p>randomization The process of randomly 
</p>
<p>assigning members from the pool of eligible  
</p>
<p>participants or units to the study conditions&mdash;
</p>
<p>often a treatment group and a control or a  
</p>
<p>comparison group.</p>
<p/>
</div>
<div class="page"><p/>
<p>T-value for dependent samples:
</p>
<p>t =
X
</p>
<p>s
</p>
<p>df
</p>
<p>d d
</p>
<p>d
</p>
<p>- m
2
</p>
<p>T-value for independent samples:
</p>
<p>t =
X - X - -
</p>
<p>N s +N s
</p>
<p>df
</p>
<p>N +N
</p>
<p>N N
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1
</p>
<p>2 2
</p>
<p>1
</p>
<p>2
</p>
<p>2 2
</p>
<p>2
</p>
<p>2
</p>
<p>2
</p>
<p>( ) ( )m m
</p>
<p>Sum of squares for a simple or a na&iuml;ve randomized experiment:
</p>
<p>SS
total 
</p>
<p>= SS
intercept 
</p>
<p>+ SS
group 
</p>
<p>+ SS
error
</p>
<p>With the introduction of a blocking factor:
</p>
<p>SS
total
</p>
<p>=SS
intercept
</p>
<p>+SS
group
</p>
<p>+SS
block
</p>
<p>+SS
error
</p>
<p>With the introduction of covariates:
</p>
<p>SS
total 
</p>
<p>= SS
intercept 
</p>
<p>+ SS
group 
</p>
<p>+ SS
covariates 
</p>
<p>+ SS
error
</p>
<p>Accounting for interaction between the treatment and block:
</p>
<p>SS
total 
</p>
<p>= SS
intercept 
</p>
<p>+ SS
group 
</p>
<p>+ SS
block 
</p>
<p>+ SS
group&times;block 
</p>
<p>+ SS
error
</p>
<p>E x e r c i s e s
</p>
<p>21.1 Danny randomly allocates 30 students to either a treatment group that 
</p>
<p>receives new instructional program or a control group that receives 
</p>
<p> standard instruction. After randomization, he compares the characteristics 
</p>
<p>of  the treatment and control groups and finds that the treatment group  
</p>
<p>is significantly different from the control group in two characteristics  
</p>
<p>(age, reading level).
</p>
<p>group indicate that randomization failed?
</p>
<p>21.2 Mark finds in a bivariate regression analysis that a drug treatment  program 
</p>
<p>has a significant impact on reducing the likelihood that patients will 
</p>
<p>     E X E R C I S E S  699</p>
<p/>
</div>
<div class="page"><p/>
<p>the drug treatment program is effective. Brent, however, argues that 
</p>
<p>Mark&rsquo;s results are confounded because he did not account for patients&rsquo; 
</p>
<p>level of  motivation. Brent notes that motivation and likelihood of  relapse 
</p>
<p>are highly related (r = 0.50). He reruns the regression results controlling 
</p>
<p>for the level of  motivation and finds that the impact of  treatment has 
</p>
<p> a. Diagram the impact of  treatment on likelihood of  relapse based on 
</p>
<p>Mark&rsquo;s initial result.
</p>
<p> b. Diagram the impact of  treatment on likelihood of  relapse using Brent&rsquo;s 
</p>
<p>analyses.
</p>
<p> c. What is the level of  bias Mark has introduced by not including this 
</p>
<p>confounder? What is the estimated r between the level of  motivation 
</p>
<p>and treatment?
</p>
<p>21.3 Darcy wants to test the effectiveness of  a new police training program on 
</p>
<p>domestic violence. She identifies the officers with the least knowledge of  
</p>
<p>domestic violence and administers the training to this group because she 
</p>
<p>believes that it will be most worthwhile since they have the most to learn. 
</p>
<p>She tests this group on domestic violence knowledge before and after the 
</p>
<p>training. She also tests a comparison group of  officers who did not receive 
</p>
<p>the training. She finds a major jump in knowledge in the trained officers 
</p>
<p>compared to the non-trained officers and concludes that her training  
</p>
<p>program was effective.
</p>
<p> a. Are Darcy&rsquo;s conclusions warranted? Are there any threats to internal 
</p>
<p>validity in her research design?
</p>
<p> b. Design an alternative study to test the effectiveness of  the training 
</p>
<p>program that has a higher level of  internal validity than Darcy&rsquo;s study.
</p>
<p>21.4 Adrian is designing a randomized trial to examine the effectiveness of  a 
</p>
<p>program designed to reduce recidivism in offenders. He has a sample of  
</p>
<p>200 prisoners that will all be released from prison on the same day and 
</p>
<p>can be randomly allocated to a treatment group receiving the program or a 
</p>
<p>control group that does not receive the program.
</p>
<p> a. If  Adrian uses a na&iuml;ve randomization procedure, how many prisoners 
</p>
<p>will be in each group? What will be the total degrees of  freedom for the 
</p>
<p>research design?
</p>
<p> b. If  Adrian uses a fully blocked randomization procedure, how many 
</p>
<p>pairs of  prisoners will be randomized? What will be the total degrees of  
</p>
<p>freedom for the research design?
</p>
<p> c. If  Adrian wants to use a partially blocked randomization procedure, 
</p>
<p>what might be one prisoner characteristic he uses to create statistical 
</p>
<p>blocks? What are the statistical consequences if  this prisoner character-
</p>
<p>istic does not end up being related to the effectiveness of  the program?
</p>
<p>21.5 Logan is reexamining data from a policing experiment to assess whether 
</p>
<p>using blocking provided a statistical benefit. Results from a &ldquo;treatment-
</p>
<p>only&rdquo; model that did not include the blocking factor and a &ldquo;treatment and 
</p>
<p>block model&rdquo; that did include the blocking factor are provided below.
</p>
<p>  C H A P T E R  T W E N T Y  O N E :  S P E C I A L  T O P I C S700</p>
<p/>
</div>
<div class="page"><p/>
<p> a. What is the total sum of  squares in each model? Are these values the 
</p>
<p>same? Why or why not?
</p>
<p> b. Why is the  SS
group
</p>
<p> the same in both models?
</p>
<p> c. How many total blocks were used in the treatment and block model?
</p>
<p>21.6 Sharon is analyzing data from a large randomized trial of  the impact 
</p>
<p>of  after-school programs on juvenile delinquency. After completing the 
</p>
<p>experiment she has been considering adding a number of  different  
</p>
<p>covariates to her overall analysis to minimize the error and improve her 
</p>
<p>ability to identify a treatment effect.
</p>
<p> a. Do you have any concerns about the approach Sharon is taking to  
</p>
<p>analyzing the experimental data? If  so, what would be a better  
</p>
<p>approach?
</p>
<p> b. If  Sharon has chosen good covariates, what should happen to the  
</p>
<p>SS
total
</p>
<p> in the model? What should happen to the SS
error
</p>
<p>? What should 
</p>
<p>happen to the SS
group
</p>
<p>?
</p>
<p> c. With the large sample size, the statistical power in Sharon&rsquo;s experiment 
</p>
<p>is estimated to be about 0.9. Does this affect whether she should  
</p>
<p>consider using covariates?
</p>
<p>21.7 Refer to Table 21.6 that provides the mean change in suspicious person 
</p>
<p>calls for service by block and group.
</p>
<p> a. This chapter described what the differences by block suggested for the 
</p>
<p>effectiveness of  the treatment across the statistical blocks. What do the 
</p>
<p>results for the control group show?
</p>
<p> b. What does the total change versus the change in each block suggest 
</p>
<p>about the importance of  block by group interaction effects?
</p>
<p>E X E R C I S E S  701
</p>
<p>UNIVARIATE ANALYSIS OF VARIANCE MODELS CALLS FOR SERVICE
</p>
<p>TREATMENT-ONLY MODEL
</p>
<p>SS
intercept
</p>
<p> (df)
</p>
<p>SS
group
</p>
<p> (df)
</p>
<p>SS
error
</p>
<p> (df)
</p>
<p>p-value for group effect
</p>
<p>260.50 (1)
</p>
<p>300.65 (1) *
</p>
<p>10,875.75 (80)
</p>
<p>.038*
</p>
<p>Treatment and block model
</p>
<p>SS
intercept
</p>
<p>(df)
</p>
<p>SS
group
</p>
<p> (df)
</p>
<p>SS
block
</p>
<p> (df)
</p>
<p>SS
error
</p>
<p> (df)
</p>
<p>p-value for group effect
</p>
<p>500.25 (1)
</p>
<p>300.65 (1) *
</p>
<p>3,614.44 (5)*
</p>
<p>7,021.56 (75)
</p>
<p>.022*
</p>
<p>*p &lt; .05</p>
<p/>
</div>
<div class="page"><p/>
<p>Special Topics: Confidence Intervals
</p>
<p>f o r  d i f f e r e n t  s t a t i s t i c s
</p>
<p>Coefficient Calculated?
</p>
<p>C h a p t e r  t w e n t y  t w o
</p>
<p>D e s c r i b i n g  a  c o n f i d e n c e  i n t e r v a l
</p>
<p>D e f i n i n g  c o n f i d e n c e  i n t e r v a l s  
</p>
<p>What is a Confidence Interval?
</p>
<p>How is It Constructed?
</p>
<p>How is the Confidence Interval for Sample Means Calculated?
</p>
<p>How is the Confidence Interval for Sample Proportions Calculated?
</p>
<p>How is the Confidence Interval for a Difference of Means Calculated?
</p>
<p>How is the Confidence Interval for a Correlation Coefficient Calculated?
</p>
<p>How is the Confidence Interval for a Regression Coefficient Calculated?
</p>
<p>How is the Confidence Interval for a Logistic Regression 
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_22,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 </p>
<p/>
</div>
<div class="page"><p/>
<p>ONE OF THE MAIN CONCERNS of this text has been to define how we
make inferences from samples to populations. This is also one of the
</p>
<p>main concerns of researchers, since in most cases they must make deci-
</p>
<p>sions about population parameters on the basis of sample statistics. Our
</p>
<p>approach has been to use the logic of statistical inference, which begins
</p>
<p>with the creation of a null hypothesis. Importantly, the logic of statistical
</p>
<p>inference we have reviewed so far is concerned primarily with the deci-
</p>
<p>sion as to whether to reject or fail to reject the null hypothesis. This
</p>
<p>means in practice that we have relied on a logic that allows us to make a
</p>
<p>statement about where the population parameter is not.
</p>
<p>This approach has enabled us to come to concrete decisions about
</p>
<p>population parameters on the basis of sample statistics. When we reject
</p>
<p>the null hypothesis on the basis of a statistical test, we conclude that the
</p>
<p>relationship we have examined is statistically significant. For example,
</p>
<p>when we reject the null hypothesis on the basis of our sample statistics
</p>
<p>in a statistical test of the difference of means, we have confidence that
</p>
<p>there is a difference between the means of the two populations. When
</p>
<p>we reject the null hypothesis that there is not a linear correlation be-
</p>
<p>tween two variables, we have confidence that there is in fact a linear
</p>
<p>correlation between these two variables in the population. But the logic
</p>
<p>we have used so far does not allow us to zero in on the value of the
</p>
<p>population parameter. When we find that the relationship between two
</p>
<p>variables in a sample is statistically significant, we conclude that there is
</p>
<p>likely to be a relationship in the population from which the sample was
</p>
<p>drawn. But this decision does not provide us with an estimate of the size
</p>
<p>of that relationship in the population.
</p>
<p>In this chapter, we turn to an approach to statistical inference that
</p>
<p>leads us to make specific statements about population parameters from
</p>
<p>sample statistics. The logic used in this approach is very similar to that
</p>
<p>described in earlier chapters. However, we do not make a single deci-
</p>
<p>sion about the null hypothesis. Rather, we create an interval of values
</p>
<p>within which we can be fairly confident that the true parameter lies. Of
</p>
<p>703</p>
<p/>
</div>
<div class="page"><p/>
<p>course, without data on the population itself, we can never be certain of
</p>
<p>the value of the population parameter. This interval is generally called a
</p>
<p>confidence interval. In this chapter, we begin by explaining the logic be-
</p>
<p>hind confidence intervals and how they are used. We then illustrate how
</p>
<p>confidence intervals are constructed for the main statistics reviewed in
</p>
<p>this text.
</p>
<p>C o n f i d e n c e  I n t e r v a l s
</p>
<p>In the statistical tests presented in earlier chapters, we began by setting a
</p>
<p>null hypothesis. Our null hypothesis made a statement about the value
</p>
<p>of the population parameter. In practice, the null hypothesis generally
</p>
<p>stated that a statistic had a value of 0. For example, for the difference of
</p>
<p>means test, the null hypothesis generally stated that the difference be-
</p>
<p>tween two population means was 0; for the correlation coefficient, that
</p>
<p>the population correlation had a value of 0; or for the regression coeffi-
</p>
<p>cient, that the population regression coefficient had a value of 0. When
</p>
<p>the results of our statistical test indicated that we should reject the null
</p>
<p>hypothesis, we concluded that it was unlikely that the population para-
</p>
<p>meter had the value stated in the null hypothesis. Since the null hypothe-
</p>
<p>sis was generally 0 or no difference, we rejected the hypothesis that the
</p>
<p>population parameter had this null value.
</p>
<p>We can use similar logic to make a very different statement about
</p>
<p>population parameters. In this case, we ask where the population para-
</p>
<p>meters are likely to be found. In statistics, the interval of values around
</p>
<p>the sample statistic within which we can be fairly confident that the
</p>
<p>true parameter lies is called a confidence interval. A confidence in-
</p>
<p>terval makes it possible for us to state where we think the population
</p>
<p>parameter is likely to be&mdash;that is, the range of values within which we
</p>
<p>feel statistically confident that the true population parameter is likely to
</p>
<p>be found. Importantly, the fact that we are confident does not mean
</p>
<p>that the population parameter actually lies in that range of values. As 
</p>
<p>in tests of statistical significance, we rely on probabilities in making 
</p>
<p>our decisions.
</p>
<p>One common illustration of confidence intervals comes from newspa-
</p>
<p>results typically make reference to a range of values. For example, a poll
</p>
<p>might indicate that 60% of adults in the United States favor using the
</p>
<p>death penalty for convicted murderers, �4% (plus or minus 4 percent).
</p>
<p>per articles and television news programs reporting the results from public
</p>
<p> opinion polls. In addition to stating that some percentage of the population
</p>
<p> supports a particular political candidate in an upcoming election or a 
</p>
<p>particular policy, more thorough accounts of these kinds of survey
</p>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S704</p>
<p/>
</div>
<div class="page"><p/>
<p>The value of 60% is often described in statistics as a point estimate. Ab-
</p>
<p>sent knowledge of the population parameter, the statistic we obtain for
</p>
<p>our sample is generally used as an estimate&mdash;in statistical terms, a point
</p>
<p>estimate&mdash;of the population parameter. The range of values represented
</p>
<p>by �4% is sometimes described as the margin of error of a poll. In sta-
</p>
<p>tistics, we prefer to call this margin of error a confidence interval. Based
</p>
<p>on a very specific set of statistical assumptions, it is the interval within
</p>
<p>which the true population value is expected to fall.
</p>
<p>Confidence intervals are based on the same statistical logic as tests of
</p>
<p>statistical significance. It will be easier to understand the relationship be-
</p>
<p>tween tests of statistical significance and the construction of confidence
</p>
<p>intervals if we start with an example that&mdash;although it is very unusual&mdash;
</p>
<p>allows us to make a straightforward link between these two concepts.
</p>
<p>Let&rsquo;s say that we gather data on attitudes toward the death penalty using
</p>
<p>an interval-scale measure that has both positive values, indicating sup-
</p>
<p>port for the death penalty, and negative values, indicating opposition to
</p>
<p>the death penalty. We use an independent random sampling method to
</p>
<p>draw our sample from the population of all adult Americans. After com-
</p>
<p>pleting our study, we find that the mean score for attitudes toward the
</p>
<p>death penalty is 0.
</p>
<p>In determining a confidence interval, we rely on the same basic as-
</p>
<p>sumptions that we use for tests of statistical significance. If we were
</p>
<p>going to compare the mean in our sample to some hypothesized popula-
</p>
<p>tion mean, we would use a t-test as our test of statistical significance.
</p>
<p>This means that the t-test also underlies our confidence interval. Accord-
</p>
<p>ingly, we have to assume an interval level of measurement and make
</p>
<p>parametric assumptions regarding the population distribution. Let&rsquo;s as-
</p>
<p>sume that our sample is very large, so we can relax the assumption of a
</p>
<p>normal population distribution. We have already noted that the sampling
</p>
<p>method meets the requirements of a t-test.
</p>
<p>If we intended to conduct a test of statistical significance for this ex-
</p>
<p>ample, we would have stated a null hypothesis and an alternative hy-
</p>
<p>pothesis and set a level of statistical significance. Let&rsquo;s say that the null
</p>
<p>hypothesis is that Americans are neutral regarding the death penalty.
</p>
<p>This means that H0 for our example will be 0.0, as the scale is divided
</p>
<p>for our sample that is greater than 1.96 or less than �1.96. Since our ob-
</p>
<p>served value of the measure is 0, the value of t will also be 0. Clearly, we
</p>
<p>would not reject the null hypothesis in this case.
</p>
<p>between positive attitudes greater than 0 and negative attitudes less than 0.
</p>
<p>There is no reason to posit a directional research hypothesis, so our test
</p>
<p>will be two-tailed. We will use a standard 0.05 significance level.
</p>
<p>Figure 22.1 illustrates the t-test for this example. The rejection region
</p>
<p>begins at a t- value of 1.96 either above or below the null hypothesis of 0.
</p>
<p>In order to reject the null hypothesis, we need an observed value of t
</p>
<p>C O N F I D E N C E I N T E R V A L S 705</p>
<p/>
</div>
<div class="page"><p/>
<p>But what would a confidence interval for this example look like?
</p>
<p>With a confidence interval, we are not concerned about whether the
</p>
<p>population parameter is not at a specific value (for example, the null
</p>
<p>hypothesis); rather, we are concerned about specifying a range of val-
</p>
<p>ues within which we can be fairly confident (though not certain) that
</p>
<p>the population parameter lies. How do we choose this range of values?
</p>
<p>Clearly, we want to make the interval large enough that, given the ob-
</p>
<p>served statistic in our sample, the population parameter is unlikely to
</p>
<p>lie outside it. As in tests of statistical significance, our choice is based
</p>
<p>on convention. With a test of statistical significance, it is common to set
</p>
<p>a threshold of 5% for the risk we are willing to take of falsely rejecting
</p>
<p>the null hypothesis. With confidence intervals, we define the width of
</p>
<p>the interval so that we can be very confident that the true population
</p>
<p>value lies within it. The confidence interval most commonly used is a
</p>
<p>terval for our example. As you can see, the confidence interval extends
</p>
<p>until the rejection region begins. It is, in this sense, the flip side of the
</p>
<p>rejection region.
</p>
<p>The 5% Rejection Region and 95% Confidence Interval 
</p>
<p>on a Normal Frequency Distribution (where and H0 � 0)X
</p>
<p>95% Confidence Interval
</p>
<p>Rejection
Region
0.025
</p>
<p>Rejection
Region
0.025
</p>
<p>Rejection Region for Two-Tailed Test
   = 0.05
</p>
<p>t = &ndash;1.96 t = +1.960
</p>
<p>α
</p>
<p>Figure 22.1
</p>
<p>95% confidence interval. Figure 22.1 illustrates the 95% confidence in-
</p>
<p>Thus, a 95% confidence interval and a 5% significance level are directly
</p>
<p> related. In our example, the 5% significance rejection region represents
</p>
<p> values far enough away from the null hypothesis that we are confident
</p>
<p> in rejecting it. The 95% confidence interval represents values close enough
</p>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S706</p>
<p/>
</div>
<div class="page"><p/>
<p>Of course, in practical examples it is very unlikely that our observed
</p>
<p>sample statistic will be the same as the population parameter hypothe-
</p>
<p>sized by the null hypothesis. A more common situation is that of the
</p>
<p>opinion poll described earlier. What would the confidence interval look
</p>
<p>like for our opinion poll? We have all the information we need to illus-
</p>
<p>trate that example, except that the level of confidence of the interval was
</p>
<p>not specified. Let&rsquo;s assume that a 95% confidence interval was used in ar-
</p>
<p>riving at the margin of error. The observed statistic, or point estimate, of
</p>
<p>60% will be the mean of the distribution. We would use a z-test rather
</p>
<p>than a t-test because we are concerned with only a single proportion.
</p>
<p>Let&rsquo;s assume that the other assumptions of the test were met. The margin
</p>
<p>shows the confidence interval relative to the z distribution. As you can
</p>
<p>see, the interval ranges between 56% and 64%.
</p>
<p>But how does this confidence interval relate to a test of statistical sig-
</p>
<p>nificance? First, we need to identify a null hypothesis. Suppose we make
</p>
<p>the null hypothesis for our test that the population is evenly divided in
</p>
<p>their attitudes toward the death penalty. In this case, the H0 takes on a
</p>
<p>value of 0.50, meaning that about half of the population to which the
</p>
<p>sample infers are for and half against the use of the death penalty for
</p>
<p>95% Confidence Interval for the Public Opinion Poll Example
</p>
<p>Test of Statistical
Significance for
Opinion Poll
</p>
<p>95% Confidence
for
</p>
<p>Opinion Poll
</p>
<p>95%
Confidence
</p>
<p>Interval
</p>
<p>Rejection
Region 
0.025
</p>
<p>Rejection
Region 
0.025
</p>
<p>P = 0.50
H0
</p>
<p>p = 0.600.56 0.64
&ndash;1.96 1.96
</p>
<p>to our observed statistic, or point estimate, that we are confident that the
</p>
<p>population parameter lies within that interval.
</p>
<p>of error of the test, or size of the confidence interval, is 4%. Figure 22.2
</p>
<p>Figure 22.2
</p>
<p>C O N F I D E N C E I N T E R V A L S 707</p>
<p/>
</div>
<div class="page"><p/>
<p>convicted murderers. Note that this value is very far outside the confi-
</p>
<p>dence interval that we have defined for our example.
</p>
<p>can see, our point estimate of 0.60 falls much to the right of the critical
</p>
<p>value (t � �1.96) of our test of statistical significance. As a general rule,
</p>
<p>if the null hypothesis for a test of statistical significance lies outside the
</p>
<p>confidence interval for the statistic (and the confidence interval and the
</p>
<p>significance level represent opposite parts of the same criterion&mdash;for ex-
</p>
<p>ample, 0.95 and 0.05; 0.99 and 0.01), then you may assume that the re-
</p>
<p>sult is statistically significant. This again points to the close relationship
</p>
<p>between tests of statistical significance and confidence intervals.
</p>
<p>While we use the logic of confidence intervals to define where a pop-
</p>
<p>ulation parameter is likely to be found, the confidence interval has a
</p>
<p>very specific statistical interpretation. Were we to draw repeated samples
</p>
<p>of a specific sample size from the same population, using a 95% confi-
</p>
<p>dence interval, we would expect that in 95% of these samples the confi-
</p>
<p>dence interval would include the population parameter. That is, we
</p>
<p>C o n s t r u c t i n g  C o n f i d e n c e  I n t e r v a l s
</p>
<p>Confidence intervals for many different sample statistics can be constructed
</p>
<p>using the same basic equation. To illustrate how we construct a confidence
</p>
<p>interval, we use the example of a t-statistic. The most general equation for
</p>
<p>calculating a t-statistic for a sample statistic is written as follows:
</p>
<p>To construct a confidence interval, we adjust this equation so that we
</p>
<p>can solve it for the population parameter. We can do this through simple
</p>
<p>algebra. Solving for the population parameter produces the following
</p>
<p>equation:
</p>
<p>Population parameter � �samplestatistic� � t � standard error ofsampling distribution�
</p>
<p>t � 
sample statistic � population parameter
</p>
<p>standard error of sampling distribution
</p>
<p>Figure 22.2 shows the sampling distribution for our example. As you
</p>
<p>would expect that in only 5 out of every 100 samples would the parameter
</p>
<p>lie outside the confidence interval. As in tests of statistical significance,
</p>
<p>we must be aware at the outset that we are only making an informed
</p>
<p>decision about the value of the population parameter. Using a 95%
</p>
<p>confidence interval, we will make the wrong decision about 5 in a 100
</p>
<p>times.
</p>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S708</p>
<p/>
</div>
<div class="page"><p/>
<p>In setting the boundaries for the confidence interval, we will use the
</p>
<p>positive and negative values associated with a two-tailed t-test to provide
</p>
<p>account for the positive and negative t-values, our confidence interval is
</p>
<p>The t-value in the equation coincides with the level of confidence we re-
</p>
<p>quire (i.e., the critical t-value). Following our earlier logic, this t-value is
</p>
<p>the flip side of the significance threshold. For a 95% confidence interval,
</p>
<p>we use a t-value associated with a two-tailed 0.05 significance level. For
</p>
<p>a 99% confidence interval, we use a t-value associated with a two-tailed
</p>
<p>0.01 significance level. In general, if � is our significance level for a 
</p>
<p>two-tailed test, then we can construct a confidence interval for 100 �
</p>
<p>(1 � �) using the same critical t-values.
</p>
<p>Confidence Intervals for Sample Means
</p>
<p>Let&rsquo;s start by constructing a confidence interval for a sample mean ( ). If
</p>
<p>where is the sample mean, s is the sample standard deviation, N is the
</p>
<p>sample size, and t is the critical t-value associated with a given signifi-
</p>
<p>cance level. To determine our critical t, we use df � N � 1, as in the
</p>
<p>single-sample t-test (see Chapter 10).
</p>
<p>For an illustration of the calculation of a confidence interval for a
</p>
<p>sample mean, consider a recent study of fear of crime among Korean
</p>
<p>Americans living in the Chicago area.1 The investigators constructed a
</p>
<p>fear of crime instrument that was measured on an interval scale and
</p>
<p>ranged in value from 11.00 to 110.00. The mean fear of crime score for
</p>
<p>the 721 respondents was 81.05, with a standard deviation of 23.41.
</p>
<p>To calculate a 99% confidence interval for the fear of crime instrument,
</p>
<p>we use the t-value associated with a 0.01 significance level and 720 de-
</p>
<p>grees of freedom (df � 721 � 1). Using the last line of the t distribution
</p>
<p>table in Appendix 4, we find the corresponding critical t-value to be 2.576.
</p>
<p>X
</p>
<p>Confidence limit � X � t � s�N � 1�
</p>
<p>X
</p>
<p>Confidence limit � �samplestatistic� � t � standard error ofsampling distribution�
</p>
<p>1Min Sik Lee and Jeffery T. Ulmer, &ldquo;Fear of Crime Among Korean Americans in
</p>
<p>Chicago Communities,&rdquo; Criminology 38:4 (2000): 1173&ndash;1206.
</p>
<p>the upper and lower boundaries, respectively (see Figure 22.1). After we
</p>
<p>given by Equation 22.1.
</p>
<p>Equation 22.1
</p>
<p>we rewrite Equation 22.1 to replace the general terms with the mean and
</p>
<p>the standard error, we have Equation 22.2.
</p>
<p>Equation 22.2
</p>
<p>C O N S T R U C T I N G C O N F I D E N C E I N T E R V A L S 709</p>
<p/>
</div>
<div class="page"><p/>
<p>W orking It Out
</p>
<p> � 81.05 � 2.25
</p>
<p> � 81.05 � 2.576� 23.41�721 � 1�
</p>
<p> Confidence limit � X � t � s�N � 1�
</p>
<p>The result of �2.25 indicates that the 99% confidence interval includes
</p>
<p>values ranging from a low of 78.80 (81.05 � 2.25) to a high of 83.30
</p>
<p>(81.05 � 2.25). By using a 99% confidence interval, we can be very con-
</p>
<p>fident that the population mean lies somewhere between 78.80 and
</p>
<p>83.30. In statistical terms, if we were to observe repeated samples of this
</p>
<p>size drawn from this population and calculate a confidence interval for
</p>
<p>each of them, only about 1 in 100 would fail to include the true popula-
</p>
<p>tion parameter.
</p>
<p>Confidence Intervals for Sample Proportions We can apply the same type of
</p>
<p>logic to calculating a confidence interval for a sample proportion, modi-
</p>
<p>value. To calculate a confidence interval for a sample proportion, we use
</p>
<p>where p is the sample proportion, q is 1 � p, N is the sample size, and
</p>
<p>z is the critical z-value associated with a given significance level.
</p>
<p>In their study of fear of crime among Korean Americans in the
</p>
<p>Chicago area, the investigators also included a question about the re-
</p>
<p>spondent&rsquo;s victimization experiences. Specifically, respondents were
</p>
<p>asked whether they had experienced any kind of victimization in the
</p>
<p>past three years. Included in this global indicator of victimization were
</p>
<p>violent as well as property crime victimizations. The investigators re-
</p>
<p>ported that 27% of the 721 respondents had experienced some form of
</p>
<p>victimization during this time period.
</p>
<p>Confidence limit � p � z��pqN �
</p>
<p>A 99% confidence interval for the fear of crime instrument has the
</p>
<p>following values:
</p>
<p>fying Equation 22.1 by replacing the critical t-value with the critical z-
</p>
<p>Equation 22.3:
</p>
<p>Equation 22.3
</p>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S710</p>
<p/>
</div>
<div class="page"><p/>
<p>Knowing that the sample proportion is 0.27 and the z-score is 1.96,
</p>
<p>we can calculate a 95% confidence interval for this proportion. We insert
</p>
<p>W orking It Out
</p>
<p> � 0.27 � 0.03
</p>
<p> � 0.27 � 1.960(0.0165)
</p>
<p> � 0.27 � 1.960�(0.27)(1 � 0.27)721
</p>
<p> Confidence limit � p � z �pqN
</p>
<p>The 95% confidence interval is �3% around the sample mean of 27%. It
</p>
<p>suggests that we can be confident that the percentage of Korean Ameri-
</p>
<p>cans living in and around Chicago who experienced some form of crimi-
</p>
<p>nal victimization within the three-year period lies between 24% and 30%.
</p>
<p>Confidence Intervals for a Difference of Sample Means In Chapter 11, we
</p>
<p>discussed calculating t-statistics to test for significant differences between
</p>
<p>two sample means. Another way of calculating a confidence interval for
</p>
<p>place the sample mean with the difference of sample means and insert
</p>
<p>the appropriate standard error for the difference of two sample means.
</p>
<p>Recall from Chapter 11, however, that there are two methods for calcu-
</p>
<p>lating the standard error of the sampling distribution: the separate vari-
</p>
<p>interval for a difference of two sample means, using either the separate
</p>
<p>variance method or the pooled variance method.
</p>
<p>Confidence limit � (X1 � X2) � t �� N1s 21 � N2s 22N1 � N2 � 2 �N1 � N2N1N2 �
</p>
<p>Confidence limit � (X1 � X2) � t � s
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s 22
N2 � 1
</p>
<p>our values for p, q, and N into Equation 22.3.
</p>
<p>Equations 22.4a and 22.4b present formulas for calculating a confidence
</p>
<p>Equation 22.4a Separate Variance Method
</p>
<p>Equation 22.4b Pooled Variance Method
</p>
<p>the difference of two sample means is by modifying Equation 22.1 to re-
</p>
<p>ance method and the pooled variance method (see pages 274&ndash;279).
</p>
<p>C O N S T R U C T I N G C O N F I D E N C E I N T E R V A L S 711</p>
<p/>
</div>
<div class="page"><p/>
<p>In both equations, represent the two sample means, 
</p>
<p>represent the two sample variances, N1 and N2 represent the two sample
</p>
<p>sizes, and t is the critical t-value associated with a given significance level.
</p>
<p>As with the two-sample t-test (see Chapter 11), the number of degrees of
</p>
<p>freedom for determining the critical t-value will be df � N1 � N2 � 2.
</p>
<p>Chapter 11 presented a test for differences in bail amounts required of
</p>
<p>African American and Hispanic defendants in Los Angeles County. A
</p>
<p>sample of 1,121 African Americans were required to post a mean bail
</p>
<p>amount of $50,841 (s � 115,565), while a sample of 1,798 Hispanics
</p>
<p>were required to post a mean bail amount of $66,552 (s � 190,801). The
</p>
<p>difference in the two sample means is $15,711, where Hispanics were re-
</p>
<p>quired to post higher bail amounts, on average.
</p>
<p>interval for this difference of sample means. For both equations, we use
</p>
<p>the t-value associated with a 0.05 significance level and 2,917 degrees of
</p>
<p>freedom. From the last line of the t distribution table in Appendix 4, we
</p>
<p>find that critical t � 1.960.
</p>
<p>s 21 and s
2
2X1 and X2
</p>
<p>W orking It Out Separate Variance Method
</p>
<p> � �15,711 � 11,119.12
</p>
<p> � �15,711 � 1.960(5,673.02)
</p>
<p> � (50,841 � 66,552) � 1.960 � 115,565
2
</p>
<p>1,121 � 1
 � 
</p>
<p>190,8012
</p>
<p>1,798 � 1
</p>
<p> Confidence limit � (X1 � X2) � t � s
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s 22
N2 � 1
</p>
<p>W orking It Out Pooled Variance Method
</p>
<p> � �15,711 � 12,385.38
</p>
<p> � �15,711 � 1.960(6,319.07)
</p>
<p> � 1.960 ��(1,121)(115,5652) � (1,798)(190,8012)1,121 � 1,798 � 2  �1,121 � 1,798(1,121)(1,798)�
 � (50,841 � 66,552)
</p>
<p> Confidence limit � (X1 � X2) � t�� N1s 21
 
</p>
<p> � N2s
2
2
</p>
<p>N1 � N2 � 2
 �N1 � N2N1N2 �
</p>
<p>Using Equations 22.4a and 22.4b, we can calculate a 95% confidence
</p>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S712</p>
<p/>
</div>
<div class="page"><p/>
<p>Using the separate variance method, we find that the confidence in-
</p>
<p>terval is �11,119.12 around the difference of sample means of 15,711.
</p>
<p>This interval suggests that we can be confident, based on our sample
</p>
<p>findings, that the average bail amounts posted in Los Angeles by African
</p>
<p>Americans were from $4,591.88 to $26,830.12 less than the average bail
</p>
<p>amounts required of Hispanics. The pooled variance method provides
</p>
<p>very similar results, indicating that the confidence interval is �12,385.38
</p>
<p>around the difference of sample means. Again, this interval suggests that
</p>
<p>we can be fairly confident that African Americans were required to post
</p>
<p>average bail amounts from $3,325.62 to $28,096.38 less than those re-
</p>
<p>quired of Hispanics.
</p>
<p>Confidence Intervals for Pearson&rsquo;s Correlation Coefficient, r
</p>
<p>The calculation of confidence intervals for Pearson&rsquo;s correlation coeffi-
</p>
<p>cient, r, relies on a similar logic, but requires an additional step. In
</p>
<p>contrast to that for sample means, sample proportions, or differences
</p>
<p>of means, the sampling distribution for Pearson&rsquo;s r is not normal or
</p>
<p>even approximately normal.2 Consequently, we need to convert r into
</p>
<p>3
</p>
<p>equation, we take the natural logarithm of 1 plus r divided by 1 minus r
</p>
<p>and multiply this value by .
</p>
<p>Values for Z* for correlation coefficients ranging in value from 0.000 to
</p>
<p>1.000 are given in Appendix 8. Note that the correlations given in the ap-
</p>
<p>pendix are all positive. If r is negative, then Z* will also be negative.
</p>
<p>In Chapter 14, we reported that the correlation between unemploy-
</p>
<p>ment rates and burglary rates for 58 counties in California was 0.491. If
</p>
<p>Z * � 
1
2
</p>
<p> � ln �1 � r1 � r�
</p>
<p>1
\2
</p>
<p>2The sampling distribution for r will generally be normal and symmetric only for the
</p>
<p>case where r � 0, which is what allowed us to use the t distribution to test whether 
</p>
<p>rp � 0 (i.e., the null hypothesis) in Chapter 14. When r � 0, the sampling distribution
</p>
<p>is not symmetric around r, so we cannot calculate a confidence interval for r in the
</p>
<p>same way we did for sample means or the difference of sample means.
3Ronald A. Fisher, Statistical Methods for Research Workers, 14th ed. (New York:
</p>
<p>Hafner, 1970).
</p>
<p>another statistic, Z *, that does have a normal distribution. The conversion
</p>
<p>After calculating of r is known as the Fisher r-to-Z * transformation.
</p>
<p>  a confidence interval for Z *. Since the values for Z * are not directly
</p>
<p> interpretable, we will then convert the confidence limits back into 
</p>
<p>values of r.
</p>
<p>The Fisher r-to-Z* transformation is given in Equation 22.5. In this
</p>
<p> the standard error for Z *, we can then modify Equation 22.1 to calculate
</p>
<p>Equation 22.5
</p>
<p>C O N S T R U C T I N G C O N F I D E N C E I N T E R V A L S 713</p>
<p/>
</div>
<div class="page"><p/>
<p>we locate r � 0.491 in Appendix 8, we find Z* to be 0.5374. We obtain
</p>
<p>W orking It Out
</p>
<p> � 0.5374
</p>
<p> � 
1
2
</p>
<p> � ln �1 � 0.4911 � 0.491�
</p>
<p> Z * � 
1
2
</p>
<p> � ln �1 � r1 � r�
</p>
<p>The standard error of Z*, which is based on the size of the sample
</p>
<p>In our example concerning unemployment rates and burglary rates for
</p>
<p>California counties, we have 58 observations, so the standard error of Z*
</p>
<p>is 0.1348.
</p>
<p>�sd(Z*) � 
1
</p>
<p>�N � 3
</p>
<p>W orking It Out
</p>
<p> � 0.1348
</p>
<p> � 
1
</p>
<p>�58 � 3
</p>
<p> �sd(Z*) � 
1
</p>
<p>�N � 3
</p>
<p>tistic, a critical z-value (since Z* is approximately normally distributed),
</p>
<p>and the equation for the standard error of Z*. The formula for the confi-
</p>
<p>Confidence limit � Z * � z � 1�N � 3�
</p>
<p>the same value for Z* if we use Equation 22.5.
</p>
<p>Equation 22.6
</p>
<p>(N), is presented in Equation 22.6.
</p>
<p>dence interval for Z* is given in Equation 22.7.
</p>
<p>Equation 22.7
</p>
<p>We can now modify Equation 22.1 by inserting Z* as the sample sta-
</p>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S714</p>
<p/>
</div>
<div class="page"><p/>
<p>where Z* is based on the Fisher r -to-Z* transformation, N is the sample
</p>
<p>size, and z is the critical z-value associated with a given significance
</p>
<p>level.
</p>
<p>Continuing our example for Z* � 0.5374 and N � 58, we calculate a
</p>
<p>95% confidence interval for Z* by using critical z � 1.960 and inserting
</p>
<p>W orking It Out
</p>
<p> � 0.5374 � 0.2642
</p>
<p> � 0.5374 � 1.960 � 1�58 �3�
</p>
<p> Confidence limit � Z * � z � 1�N � 3�
</p>
<p>The confidence interval is �0.2642 around Z* � 0.5374, indicating
</p>
<p>that the range for Z* is 0.2732 to 0.8016. Since we are unable to directly
</p>
<p>interpret values of Z*, we should convert the values of Z* back to values
</p>
<p>of r, using Appendix 8. The conversion of Z* back to r will provide us
</p>
<p>with the confidence interval for r. For Z* � 0.2732, we find that r �
</p>
<p>0.267. For Z* � 0.8016, we find that r � 0.665. For both values of Z*, we
</p>
<p>used the closest Z*-value reported in Appendix 8 to determine the val-
</p>
<p>ues for r, since an exact match could not be found. These results suggest
</p>
<p>that we can be confident that the population value for the correlation co-
</p>
<p>efficient falls between 0.267 and 0.665. Note that the upper and lower
</p>
<p>confidence limits are not symmetric around r &mdash;the lower limit is farther
</p>
<p>away from r � 0.491 than is the upper limit.
</p>
<p>Confidence Intervals for Regression Coefficients
</p>
<p>Confidence intervals for regression coefficients are nearly identical in form
</p>
<p>to confidence intervals for sample means. The formula for calculating con-
</p>
<p>where b is the regression coefficient, is the standard error of b, and t
</p>
<p>is the critical t -value associated with a given level of significance. The
</p>
<p>number of degrees of freedom for the critical t will be equal to N � k �
</p>
<p>1, where N is the sample size and k is the number of independent vari-
</p>
<p>ables in the regression model.
</p>
<p>�̂b
</p>
<p>ˆ
b
</p>
<p>the values into Equation 22.7.
</p>
<p>fidence intervals for regression coefficients is given in Equation 22.8.
</p>
<p>Equation 22.8Confidence limit � b �   � �t �
</p>
<p>C O N S T R U C T I N G C O N F I D E N C E I N T E R V A L S 715</p>
<p/>
</div>
<div class="page"><p/>
<p>In Chapter 15, we reported that the regression coefficient representing
</p>
<p>the effect of unemployment rates on burglary rates in California was
</p>
<p>calculate a 99% confidence interval, the number of degrees of freedom
</p>
<p>will be 56 (df � 58 � 1 � 1 � 56), so the critical t we will use is 2.669
</p>
<p>(see Appendix 4).
</p>
<p>W orking It Out
</p>
<p> � 36.7483 � 23.2539
</p>
<p> � 36.7483 � 2.669(8.7126)
</p>
<p> Confidence limit � b �  t  �̂b
</p>
<p>The result of �23.2539 indicates that the 99% confidence interval in-
</p>
<p>cludes values ranging from a low of 13.4944 to a high of 60.0022. The
</p>
<p>99% confidence interval suggests that we can be very confident that the
</p>
<p>population value of the regression coefficient representing the effect of
</p>
<p>unemployment rates on burglary rates lies somewhere between 13.4944
</p>
<p>and 60.0022.
</p>
<p>Confidence Intervals for Logistic Regression Coefficients and Odds Ratios
</p>
<p>Confidence intervals for logistic regression coefficients are calculated in
</p>
<p>The number of degrees of freedom for determining the critical t-value
</p>
<p>also equals N � k � 1.
</p>
<p>In addition to being able to calculate confidence intervals for the orig-
</p>
<p>formed coefficients, which are difficult to interpret.
</p>
<p>An illustration of the use of confidence intervals for logistic regression
</p>
<p>coefficients is provided by a recent study examining the link between
</p>
<p>mental disorders and violent victimization for a sample of 747 adults.4
</p>
<p>The dependent variable measured whether the individual had reported a
</p>
<p>4 Eric Silver, &ldquo;Mental Disorder and Violent Victimization: The Mediating Role of In-
</p>
<p>volvement in Conflicted Social Relationships,&rdquo; Criminology 40 (2002): 191&ndash;212.
</p>
<p>exactly the same way as logistic regression coefficients (Equation 22.8).
</p>
<p>inal logistic regression coefficients, we can also refer to confidence inter-
</p>
<p>regression coefficients into odds ratios by exponentiating the coefficient b. 
</p>
<p>This means that we can take the lower and upper limits of our confi-
</p>
<p>dence interval for b and convert them to odds ratios. We can then 
</p>
<p>discuss the confidence interval relative to the odds, rather than the untrans-
</p>
<p>36.7483 and the standard error for b was 8.7126 (see page 466). If we
</p>
<p>vals for odds ratios. As noted in Chapter 18, we can convert our logistic
</p>
<p>� �
</p>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S716</p>
<p/>
</div>
<div class="page"><p/>
<p>violent victimization in the preceding ten weeks. One of the nine inde-
</p>
<p>pendent variables used by the researcher was the level of neighborhood
</p>
<p>disadvantage, which was an interval-level instrument that combined eco-
</p>
<p>nomic indicators, such as poverty rate, unemployment rate, and income.
</p>
<p>The effect of neighborhood disadvantage was positive (b � 0.33), mean-
</p>
<p>ing the greater the level of neighborhood disadvantage, the more likely
</p>
<p>the individual was to have experienced a violent victimization. The stan-
</p>
<p>dard error for b was reported to be 0.09.
</p>
<p>To calculate a 99% confidence interval for b, we use critical t � 2.576,
</p>
<p>W orking It Out
</p>
<p> � 0.33 � 0.23
</p>
<p> � 0.33 � 2.576(0.09)
</p>
<p> Confidence limit � b �  t  �̂b
</p>
<p>The result of �0.23 tells us that the 99% confidence interval includes
</p>
<p>values ranging from a low of 0.10 to a high of 0.56. If we exponentiate
</p>
<p>The lower limit of the confidence interval for the odds ratio is 1.105
</p>
<p>[Exp(0.10)], and the upper limit of the confidence interval for the odds
</p>
<p>ratio is 1.751 [Exp(0.56)]. These results suggest that we can be very confi-
</p>
<p>dent that the population value of the odds ratio lies somewhere between
</p>
<p>1.105 and 1.751. If we took repeated random samples of the size exam-
</p>
<p>ined here and calculated a confidence interval for each, then in only
</p>
<p>about 1 in 100 cases would that interval fail to include the true odds
</p>
<p>ratio.
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>In tests of statistical significance, we make a statement about where the
</p>
<p>population parameter is not. In this chapter, we turned to an approach
</p>
<p>to statistical inference that leads us to make a very different type of state-
</p>
<p>ment about population parameters. The logic used in this approach is
</p>
<p>similar to that described in earlier chapters. However, we do not make a
</p>
<p>single decision about the null hypothesis. Rather, we create an interval
</p>
<p>since df � 747 � 9 � 1 � 737, and insert the values into Equation 22.8.
</p>
<p>the lower and upper limits of the confidence interval for b, we will have the
</p>
<p>lower and upper limits of the confidence interval for the odds ratio.
</p>
<p>� �
</p>
<p>C H A P T E R  S U M M A R Y 717</p>
<p/>
</div>
<div class="page"><p/>
<p>of values within which we can be fairly confident that the true parameter
</p>
<p>lies&mdash;although, without data on the population itself, we can never be
</p>
<p>certain of the value of the population parameter. This interval is gener-
</p>
<p>ally called a confidence interval.
</p>
<p>A confidence interval makes it possible for us to say where we
</p>
<p>think the population parameter is likely to be&mdash;that is, the range of
</p>
<p>values within which we feel statistically confident that the true popula-
</p>
<p>tion parameter is likely to be found. A confidence interval is generally
</p>
<p>constructed around the observed statistic of interest, commonly called
</p>
<p>a point estimate. Absent knowledge of the population parameter, the
</p>
<p>statistic we obtain for our sample is generally used as an estimate&mdash;in
</p>
<p>statistical terms, a point estimate&mdash;of the population parameter. The
</p>
<p>size of the confidence interval is often referred to as the margin of
</p>
<p>error.
</p>
<p>Confidence intervals may be constructed at any level of confidence.
</p>
<p>By convention, we use 95% and 99% confidence levels, which are
</p>
<p>based on 5% and 1% significance thresholds. While it is commonly
</p>
<p>said, when using a confidence interval, that the researcher is confident
</p>
<p>that the true parameter lies in the interval defined, confidence inter-
</p>
<p>vals have a specific statistical interpretation. Suppose we find, using a
</p>
<p>95% or 99% criterion, that a confidence interval is of a certain size. If
</p>
<p>we were to draw repeated samples of the same size, using the same
</p>
<p>methods, and calculate a confidence interval for each sample, then in
</p>
<p>only 5 in 100 (for a 95% interval) or 1 in 100 (for a 99% interval) of
</p>
<p>these samples would the interval fail to include the true population
</p>
<p>parameter.
</p>
<p>K e y  T e r m s
</p>
<p>confidence interval An interval of values
</p>
<p>around a statistic (usually a point estimate).
</p>
<p>If we were to draw repeated samples and
</p>
<p>calculate a 95% confidence interval for each,
</p>
<p>then in only 5 in 100 of these samples would
</p>
<p>the interval fail to include the true popula-
</p>
<p>tion parameter. In the case of a 99% confi-
</p>
<p>dence interval, only 1 in 100 samples would
</p>
<p>fail to include the true population parameter.
</p>
<p>margin of error The size of the confi-
</p>
<p>dence interval for a test. A margin of error
</p>
<p>of �3% in an opinion poll means that the
</p>
<p>confidence interval ranged between 3%
</p>
<p>above and 3% below the point estimate or
</p>
<p>observed statistic.
</p>
<p>point estimate An estimate of the
</p>
<p>population parameter. Absent knowledge
</p>
<p>of the population parameter, the statistic
</p>
<p>we obtain for a sample is generally used
</p>
<p>as an estimate&mdash;or, in statistical terms, 
</p>
<p>a point estimate&mdash;of the population
</p>
<p>parameter.
</p>
<p>718 C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S</p>
<p/>
</div>
<div class="page"><p/>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>To calculate the confidence interval for a sample mean:
</p>
<p>To calculate the confidence interval for a sample proportion:
</p>
<p>To calculate the confidence interval for a difference of sample means,
</p>
<p>using the separate variance method:
</p>
<p>To calculate the confidence interval for a difference of sample means,
</p>
<p>using the pooled variance method:
</p>
<p>To convert r to Z* (Fisher r -to-Z* transformation):
</p>
<p>To calculate the confidence interval for Z*:
</p>
<p>To calculate the confidence interval for a regression or logistic
</p>
<p>regression coefficient:
</p>
<p>E x e r c i s e s
</p>
<p>In a study of self-reported marijuana use, a sample of high school stu-
dents were asked how many times they had smoked marijuana in the
last month. Researchers reported that the average for the sample was
</p>
<p> Confidence limit � b �  t �̂b
</p>
<p>Confidence limit � Z * � z � 1�N � 3�
</p>
<p>Z * � 
1
2
</p>
<p> � ln �1 � r1 � r�
</p>
<p>Confidence limit � (X1 � X2) � t �� N1s
2
1 � N2s
</p>
<p>2
2
</p>
<p>N1 � N2 � 2�
N1 � N2
</p>
<p> N1N2 �
</p>
<p>Confidence limit � (X1 � X2) � t� s
2
1
</p>
<p>N1 � 1
 � 
</p>
<p>s 22
N2 � 1
</p>
<p>Confidence limit � p � z ��pqN �
</p>
<p>Confidence limit � X � t � s�N � 1�
</p>
<p>22.1
</p>
<p>� �
</p>
<p>E X E R C I S E S 719</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 times, with a 95% confidence interval of �1.3. Explain what this
result means in plain English.
</p>
<p>Following a revolution, the new leadership of the nation of Kippax
intends to hold a national referendum on whether the practice of
capital punishment should be introduced. In the buildup to the refer-
endum, a leading army general wishes to gauge how the people are
likely to vote so that he can make a public statement in line with
popular feeling on the issue. He commissions Greg, a statistician, to
carry out a secret poll of how people plan to vote. The results of
Greg&rsquo;s poll are as follows: The sample proportion in favor of intro-
ducing capital punishment is 52%; the sample has a 95% confidence
interval of �10%. How should Greg explain these results to the army
general?
</p>
<p>Concerned that taxpayers were not reporting incomes honestly, a state
department of revenue commissioned an independent study to esti-
mate the number of times people had cheated on their tax returns in
the last five years. The researchers interviewed a random sample of
121 adults and found that the mean number of times they had cheated
on their income taxes in the last five years was 2.7, with a standard
deviation of 1.1.
</p>
<p>a. Calculate a 95% confidence interval for this sample mean.
</p>
<p>b. Explain what this result means.
</p>
<p>The country of Mifflin is preparing for an upcoming presidential elec-
tion. A random sample of 200 likely voters in Mifflin indicates that
57% are going to vote for the Hawk Party candidate, while the remain-
ing 43% are planning on voting for the Gopher Party candidate.
</p>
<p>a. Calculate a 95% confidence interval for the proportion voting for
the Hawk Party candidate.
</p>
<p>b. Calculate a 99% confidence interval for the proportion voting for
the Hawk Party candidate.
</p>
<p>c. Which of the two confidence intervals provides a better indicator 
</p>
<p>election?
</p>
<p>A long-running disagreement between science and humanities profes-
sors at Big Time University focuses on which department has the
</p>
<p>grade point average for a random sample of 322 recent science gradu-
ates was 3.51 (s � 1.2). Asserting that there is no meaningful differ-
ence, a history professor shows that the mean grade point average for
a sample of 485 recent humanities graduates was 3.36 (s � 1.6). Con-
</p>
<p>22.2
</p>
<p>22.3
</p>
<p>22.4
</p>
<p>22.5
</p>
<p>of who will win the election? Who do you predict will win the
</p>
<p>smarter students. As evidence supportive of the contention that 
</p>
<p>explain which professor appears to be more correct.
</p>
<p>science students are smarter, a physics professor shows that the mean
</p>
<p>struct a 99% confidence interval for this difference of means, and 
</p>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S720</p>
<p/>
</div>
<div class="page"><p/>
<p>Interested in the effects of income and poverty on robbery rates, a stu-
dent selected a random sample of 125 cities and correlated average in-
come and percentage of persons living in poverty with the robbery
rate. She reported the following correlations:
</p>
<p>Income and robbery: r � �0.215
</p>
<p>Poverty and robbery: r � 0.478
</p>
<p>a. Calculate a 95% confidence interval for each correlation.
</p>
<p>b. Explain what these results mean.
</p>
<p>adolescents about their behavior. The researchers estimated a regres-
</p>
<p>Variable b Standard Error
</p>
<p>Intercept �0.21 0.15
</p>
<p>Age �0.02 0.01
</p>
<p>Number of friends arrested 2.56 0.73
</p>
<p>Number of hours per week studying �0.17 0.08
</p>
<p>Number of hours per week working 0.09 0.03
</p>
<p>Self-esteem �1.05 0.51
</p>
<p>a. Calculate a 95% confidence interval for each of the independent
variable regression coefficients.
</p>
<p>b. Explain what these results mean.
</p>
<p>searchers recoded delinquency as 0 � no delinquency and 1 � one or
more delinquent acts. They estimated a logistic regression model and
found the following:
</p>
<p>Variable b Standard Error
</p>
<p>Intercept 0.05 0.04
</p>
<p>Age �0.12 0.05
</p>
<p>Number of friends arrested 1.86 0.57
</p>
<p>Number of hours per week studying �0.23 0.09
</p>
<p>Number of hours per week working 0.44 0.17
</p>
<p>Self-esteem �0.79 0.38
</p>
<p>a. Calculate a 95% confidence interval for each of the independent
variable regression coefficients.
</p>
<p>b. Explain what these results mean.
</p>
<p>22.6
</p>
<p>22.7
</p>
<p>22.8 In a follow-up to the analysis reported in Exercise 22.7, the re-
</p>
<p>dependent variable. The table of results follows:
</p>
<p>Delinquency researchers at DP Institute interviewed a sample of 96
</p>
<p>sion model, using number of delinquent acts in the last year as the 
</p>
<p>E X E R C I S E S 721</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S722
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>All of the statistical packages that we are familiar with allow for the  
</p>
<p>straightforward computation of confidence intervals. SPSS and Stata both 
</p>
<p>allow for easy reporting of confidence intervals, as we illustrate below. Four 
</p>
<p>of the confidence intervals discussed in this chapter&mdash;sample mean, difference 
</p>
<p>of means, regression, and logistic regression&mdash;are the focus of our discussion. 
</p>
<p>There are no options in either program for computing confidence intervals for 
</p>
<p>Pearson&rsquo;s r. Sample syntax in both SPSS (Chapter_22.sps) and Stata  
</p>
<p>(Chapter_22.do) illustrate each of the following commands.
</p>
<p>SPSS
</p>
<p>To obtain the confidence interval for a sample mean, use the T-TEST  
</p>
<p>command, but use the /TESTVAL option to test the value specified in the  
</p>
<p>null hypothesis (e.g., 0):
</p>
<p>T-TEST
</p>
<p>/TESTVAL = 0
</p>
<p>/VARIABLES = variable_name
</p>
<p>/CRITERIA = CI(.95).
</p>
<p>where the /TESTVAL = 0 implements the null hypothesis (H
0
</p>
<p>T-TEST command will test whether the sample mean is different from 0.  
</p>
<p>The /CRITERIA = CI(.95) requests a 95 % confidence interval. If the  
</p>
<p>/CRITERIA line is omitted from the command, the default output will still  
</p>
<p>contain the 95 % confidence interval. Where you may want to include the  
</p>
<p>/CRITERIA line is in the situation where you are interested in a different  
</p>
<p>confidence interval. For example, a 90 % confidence interval would be requested 
</p>
<p>with /CRITERIA = CI(.90).
</p>
<p>In Chapter 11, we discussed how to compute an independent samples t-test 
</p>
<p>in SPSS with the T-TEST command. Recall the basic format for the command is
</p>
<p>T-TEST GROUPS = grouping_variable(category_1 category_2)
</p>
<p>/VARIABLES = variable_name.
</p>
<p>Similar to the one-sample t-test, the default output from executing the T-TEST 
</p>
<p>for independent samples will include the 95% confidence interval. Should  
</p>
<p>you be interested in a confidence interval of a different size, insert the  
</p>
<p>/CRITERIA = CI(.##) option as explained above.
</p>
<p>We discussed various features and option of the linear regression command 
</p>
<p>in SPSS (REGRESSION) in Chapters 15 through 17. Confidence intervals 
</p>
<p>are obtained by adding the option CI(##) to the /STATISTICS line in the 
</p>
<p>REGRESSION command:</p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R  E X E R C I S E S 723
</p>
<p>REGRESSION
</p>
<p>/STATISTICS COEFF CI(95) R ANOVA
</p>
<p>/DEPENDENT dep_var
</p>
<p>/METHOD = ENTER list_of_indep_vars.
</p>
<p>where we have inserted a request for a 95 % confidence interval by adding 
</p>
<p>CI(95) to the /STATISTICS line. Should you want a different confidence inter-
</p>
<p>val, simply change the values inside the parentheses. As we noted in previous 
</p>
<p>discussions of the REGRESSION command, when you desire some additional 
</p>
<p>output, it is necessary to also request all of the standard output from the com-
</p>
<p>mand (i.e., COEFF R ANOVA). Note that the specification of the confidence 
</p>
<p>interval in the REGRESSION command does not require a decimal point in the 
</p>
<p>CI(##) option.
</p>
<p>Chapter 18&rsquo;s Computer Exercises focused on the use of the LOGISTIC 
</p>
<p>REGRESSION command. To obtain the confidence intervals for the estimated 
</p>
<p>coefficients, we need to add the /PRINT = CI(##) option line to the command:
</p>
<p>LOGISTIC REGRESSION VARIABLES dep_var
</p>
<p>/METHOD = ENTER list_of_indep_vars
</p>
<p>/PRINT = CI(95).
</p>
<p>The confidence intervals will appear as the far right columns in the table of 
</p>
<p>coefficients. Should you want a different confidence interval, all you need to 
</p>
<p>do is change the 95 in /PRINT = CI(95) to the value of interest. NOTE: The 
</p>
<p>confidence intervals computed by SPSS in the LOGISTIC REGRESSION com-
</p>
<p>mand are for the odds ratios [Exp(B)], not the original coefficients (B). Similar 
</p>
<p>to requesting confidence intervals in REGRESSION, no decimal point is used in 
</p>
<p>the CI(##) option in the LOGISTIC REGRESSION command.
</p>
<p>Stata
</p>
<p>The ttest command is used to compute a one-sample ttest, where we compare a 
</p>
<p>sample mean to a hypothesized value:
</p>
<p>ttest variable_name == hypothesized_value
</p>
<p>In most cases, the hypothesized value will be 0. By default, the ttest command 
</p>
<p>will compute 95% confidence intervals. If you are interested in a different value 
</p>
<p>for the confidence interval, then add the level(##) option to the command line:
</p>
<p>ttest variable_name == hypothesized_value, level(##)
</p>
<p>For example, if we were interested in testing the hypothesis that GPA in the 
</p>
<p>NYS data was equal to 0 and use 90% confidence intervals, we would enter the 
</p>
<p>following command:</p>
<p/>
</div>
<div class="page"><p/>
<p>C H A P T E R  T W E N T Y  T W O :  S P E C I A L  T O P I C S724
</p>
<p>ttest gpa == 0, level(90)
</p>
<p>We discussed the independent samples t-test in Chapter 11&rsquo;s Computer Exercises 
</p>
<p>section using the ttest command. The default output from the ttest command 
</p>
<p>is a 95% confidence interval. Should we be interested in a different range, we 
</p>
<p>would add the level(##) option:
</p>
<p>ttest variable_name, by(grouping_variable) level(##)
</p>
<p>In previous chapters, we have discussed the use of both the regress and logit 
</p>
<p>commands to estimate linear regression and binary logistic regression models, 
</p>
<p>respectively. Both of those commands report 95% confidence intervals in the 
</p>
<p>table of coefficients output by default. To request a different confidence interval, 
</p>
<p>add the level(##) option to the end of the command line (following a comma):
</p>
<p>regress dep_var list_of_indep_vars, level(##)
</p>
<p>and
</p>
<p>logit dep_var list_of_indep_vars, level(##)
</p>
<p>Consistent with all other output, the confidence intervals appear in the far right 
</p>
<p>column of the coefficients table.
</p>
<p>Problems
</p>
<p>Open the NY data file (nys_1.sav, nys _1_student.sav, or nys_1.dta) to answer 
</p>
<p>questions 1 through 4.
</p>
<p> 1. For each of  the following measures of  delinquency, compute a 95%  
</p>
<p>confidence interval and explain what it means.
</p>
<p>a. Number of  times the youth has stolen something valued at less than $5.
</p>
<p>b. Number of  times the youth has cheated on exams at school.
</p>
<p>c. Number of  times the youth has been drunk.
</p>
<p> 2. For each of  the following difference of  means tests, compute a 95% con-
</p>
<p>fidence interval and explain what it means.
</p>
<p>a. Does the number of  times the youth has taken something valued at less 
</p>
<p>b. Does the number of  times the youth has hit his or her parents differ for 
</p>
<p>c. Does the number of  times the youth has cheated on exams differ for </p>
<p/>
</div>
<div class="page"><p/>
<p>C O M P U T E R  E X E R C I S E S 725
</p>
<p> 3. Rerun two of  the regression models you estimated in the Computer 
</p>
<p>Exercises in Chapter 16. For each model, compute 95% confidence inter-
</p>
<p>vals for the regression coefficients and explain what each result means. 
</p>
<p> 4. Rerun two of  the logistic regression models you estimated in the 
</p>
<p>Computer Exercises in Chapter 18. For each model, compute 95% con-
</p>
<p>fidence intervals for the odds ratios and explain what each result means. 
</p>
<p>Open the Pennsylvania Sentencing data file (pcs_98.sav or pcs_98.dta) 
</p>
<p>to answer questions 5 and 6.
</p>
<p> 5. Run a regression model using length of  incarceration sentence as the 
</p>
<p>dependent variable and age, race, sex, offense severity score, and prior 
</p>
<p>criminal history score as the independent variables. Compute 99% confi-
</p>
<p>dence intervals for the regression coefficients and explain what each result 
</p>
<p>means.
</p>
<p>Run a binary logistic regression model using incarceration as the 
</p>
<p>dependent variable and age, race, sex, offense severity score, and prior 
</p>
<p>criminal history score as the independent variables (this is the model you 
</p>
<p>estimated in the Computer Exercises in Chapter 18). Compute 99%  
</p>
<p>confidence intervals for the odds ratios and explain what each result 
</p>
<p>means.</p>
<p/>
</div>
<div class="page"><p/>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5_23,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>Special Topics: Statistical Power
</p>
<p>C h a p t e r  t w e n t y  t h r e e
</p>
<p>A s s e s s i n g  t h e  r i s k  o f  T y p e  I I  e r r o r
</p>
<p>How is Statistical Power Defined?
</p>
<p>How Do Significance Criteria Influence Statistical Power?
</p>
<p>How Does Effect Size Influence Statistical Power?
</p>
<p>How Does Sample Size Influence Statistical Power?
</p>
<p>E s t i m a t i n g  s t a t i s t i c a l  p o w e r
</p>
<p>How Do We Define the Significance Criteria and Effect Size in a  
</p>
<p>Statistical Power Analysis?
</p>
<p>How Do We Determine the Sample Size Needed to Ensure a Statistically 
</p>
<p>Powerful Study?</p>
<p/>
</div>
<div class="page"><p/>
<p>As we have seen in earlier chapters, criminal justice researchers place a 
 premium on statistical inference and its use in making decisions about population 
</p>
<p>parameters from sample statistics. In assessing statistical significance, the focus 
</p>
<p>hypothesis. Paying attention to the statistical significance of a finding should keep 
</p>
<p>researchers honest, because it provides a systematic approach for deciding when 
</p>
<p>the observed statistics are convincing enough for the researcher to state that they 
</p>
<p>reflect broader processes or relationships in the general population from which the 
</p>
<p>sample was drawn. If the threshold of statistical significance is not met, then the 
</p>
<p>researcher cannot reject the null hypothesis and cannot conclude that a relation-
</p>
<p>ship exists.
</p>
<p>Another type of error that most criminal justice researchers are aware of, but 
</p>
<p>pay relatively little attention to, is Type II, or beta (
</p>
<p>failing to reject the null hypothesis that we originally introduced in Chapter 6. 
</p>
<p>A study that has a high risk of Type II error is likely to mistakenly conclude that 
</p>
<p>treatments are not worthwhile or that a relationship does not exist when in fact it 
</p>
<p>does. Understanding the risk of a Type II error is crucial to the development of a 
</p>
<p>research design that will give the researcher a good chance of finding a treatment 
</p>
<p>effect or a statistical relationship, if those effects and relationships exist in the 
</p>
<p>population. This is fundamentally what we mean by statistical power&mdash;given the 
</p>
<p>-
</p>
<p>cally significant effects and relationships?
</p>
<p>Although researchers in criminal justice have placed much more emphasis 
</p>
<p>on the statistical significance than on the statistical power of a study, research in 
</p>
<p>fields such as medicine and psychology routinely reports estimates of statistical 
</p>
<p>power.1
</p>
<p>require research proposals to estimate how powerful the proposed research design 
</p>
<p>will be. The purpose of this chapter is to present an introductory discussion of 
</p>
<p>the key components in an assessment of the statistical power of a research design 
</p>
<p>727
</p>
<p>1 See, for example, S. E. Maxwell, K. Kelley, and J. R. Rausch &ldquo;Sample Size Planning for 
</p>
<p>Accuracy in Parameter Estimation,&rdquo; Annual Review of Psychology</p>
<p/>
</div>
<div class="page"><p/>
<p>728 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>and to explain why it is important for criminal justice researchers to have a basic 
</p>
<p>understanding of the importance of statistical power in designing and evaluating 
</p>
<p>criminal justice research.
</p>
<p>S t a t i s t i c a l  P o w e r
</p>
<p>Statistical power measures the probability of rejecting the null hypothesis when it 
</p>
<p>is false, but it cannot be measured directly. Rather, statistical power is calculated 
</p>
<p>by subtracting the probability of a Type II error&mdash;the probability of falsely failing 
</p>
<p>.   
</p>
<p>For many sample statistics, the Type II error can be estimated directly from the 
</p>
<p>sampling distributions commonly assumed for most test statistics. In contrast to a 
</p>
<p>traditional test of statistical significance, which identifies for the researcher the risk 
</p>
<p>-
</p>
<p>cal power measures how often one would fail to identify a relationship that in fact 
</p>
<p>does exist in the population. For example, a study with a statistical power level 
</p>
<p>of 0.90 has only a 10% probability of falsely failing to reject the null hypothesis. 
</p>
<p>Alternatively, a study with a statistical power estimate of 0.40 has a 60% probability 
</p>
<p>of falsely failing to reject the null hypothesis. Generally as the statistical power of a 
</p>
<p>proposed study increases, the risk of making a Type II error decreases.
</p>
<p>graphically. Suppose that we are interested in a difference in group means, say 
</p>
<p>between a control and treatment group in a criminal justice experiment, and 
</p>
<p>based on prior research and theory, we expect to find a positive difference in the 
</p>
<p>outcome measure. We would test for a difference in the group means by using 
</p>
<p>a one-tailed t-test. If we have 100 cases in each group, then the critical t-value 
</p>
<p>by the solid line represents the t-distribution&mdash;the sampling distribution&mdash;under 
</p>
<p>t-value. 
</p>
<p> Figure 23.1  Graphical Representation of Type I and Type II Errors in a Difference of Means Test  
</p>
<p>(100 Cases Per Sample)
</p>
<p>&minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>αβ
</p>
<p>Critical t=1.653</p>
<p/>
</div>
<div class="page"><p/>
<p> S T A T I S T I C A L  P O W E R  729
</p>
<p>represents the hypothesized sampling distribution based on prior research and 
</p>
<p>theory and our expectations for the expected differences in the two group means. 
</p>
<p>The hypothesized sampling distribution is also a t-distribution, but it is known as 
</p>
<p>a non-central t-distribution&mdash;we illustrate below how this distribution is used to 
</p>
<p>compute statistical power. The probability of making a Type II error (
</p>
<p>in the figure and is the cumulative probability in the distribution on the right up to 
</p>
<p>the critical t-value (i.e., t
</p>
<p>test is represented in the figure by the area under the dashed line that falls to the 
</p>
<p>right of the critical value&mdash;the difference between 1 and &mdash;and represents t-values 
</p>
<p>the null hypothesis.
</p>
<p>It is important to note that our estimate of  is fully dependent on our estimate 
</p>
<p>illustrates the differences for two alternative effect sizes while assuming that the 
</p>
<p>sample sizes remain fixed at 100 cases per group. For example, if we expect the dif-
</p>
<p>ference of means to be smaller, we would shift the hypothesized sampling distribu-
</p>
<p>expect a larger difference, we would shift the hypothesized sampling distribution 
</p>
<p>to the right, reducing the estimate of 
</p>
<p>If  the statistical power of  a research design is high and the null hypothesis is false 
</p>
<p>for the population under study, then it is very likely that the researcher will reject 
</p>
<p>the null hypothesis and conclude that there is a statistically significant finding. If  the 
</p>
<p>statistical power of  a research design is low, it is unlikely to yield a statistically sig-
</p>
<p>nificant finding, even if  the research hypothesis is in fact true. Studies with very low 
</p>
<p>statistical power are sometimes described as being &ldquo;designed for failure,&rdquo; because a 
</p>
<p>study that is underpowered is unlikely to yield a statistically significant result, even 
</p>
<p>when the outcomes observed are consistent with the research hypothesis. 2
</p>
<p> Figure 23.2  Graphical Representation of Type I and Type II Errors in a Difference of Means 
</p>
<p> Test&mdash;Changing the Difference in Mean Values. (a) Smaller Difference in Means&mdash;Fixed 
</p>
<p>Sample Size. (b) Larger Difference in Means&mdash;Fixed Sample Size
</p>
<p>2 For an extended discussion of  this, see D. Weisburd &ldquo;Design Sensitivity in Criminal Justice 
</p>
<p>Experiments,&rdquo; Crime and Justice
</p>
<p>&minus;4 &minus;2 0 2 4
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>α
β
</p>
<p>Critical t=1.653
a b
</p>
<p>&minus;4 &minus;2 0 2 4 6 8
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>αβ
</p>
<p>Critical t=1.653</p>
<p/>
</div>
<div class="page"><p/>
<p>730 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>Consider the implications for theory and practice in criminal justice of a study 
</p>
<p>that has low statistical power. Suppose that a promising new program has been 
</p>
<p>developed for dealing with spouse assault. If that program is evaluated with a study 
</p>
<p>that has low statistical power, then the research team will likely fail to reject the null 
</p>
<p>hypothesis based on the sample statistics, even if the program does indeed have the 
</p>
<p>potential for affecting spouse assault. Although the research team is likely to say 
</p>
<p>that the program does not have a statistically significant impact on spouse assault, 
</p>
<p>this is not because the program is not an effective one, but because the research 
</p>
<p>team designed the study in such a way that it was unlikely to be able to identify 
</p>
<p>program success. Conceptually, this same problem occurs in the analysis of other 
</p>
<p>types of data when trying to establish whether a relationship exists between two 
</p>
<p>theoretically important variables. The relationship may exist in the population of 
</p>
<p>interest, but a study with low statistical power will be unlikely to conclude that the 
</p>
<p>relationship is statistically significant.
</p>
<p>One might assume that researchers in criminal justice would work hard to 
</p>
<p>develop statistically powerful studies, because such studies are more likely to sup-
</p>
<p>port the research hypothesis proposed by the investigators. Unfortunately, statisti-
</p>
<p>cal power is often ignored altogether by criminal justice researchers, which results 
</p>
<p>in many criminal justice studies having a low level of statistical power.
</p>
<p>Setting the Level of Statistical Power
</p>
<p>What is a desirable level of statistical power? There is no single correct answer to 
</p>
<p>this question, since it depends on the relative importance of Type I and Type II 
</p>
<p>errors for the researcher. That said, one of the more common suggestions in the 
</p>
<p>statistical power literature has been that studies should attempt to achieve a power 
</p>
<p>level of 0.80, meaning that the chances of a Type II error are  = 0.20. There are 
</p>
<p>many ways in which this is an arbitrary threshold. At the same time, it implies a 
</p>
<p>straightforward gauge for the relative importance of both types of error. If we use 
</p>
<p>If the target level of statistical power is 0.90, then  = 0.10, and the ratio of prob-
</p>
<p>abilities decreases to 0.10/0.05 = 2.0. What this means is that for a fixed level 
</p>
<p>chances of a Type II error (
</p>
<p>to 1.0, where the chances of both types of error are viewed as equally important.
</p>
<p>What happens if  we reduce the desired level of  statistical significance? For 
</p>
<p>example, suppose we were particularly concerned about our chances of  mak-
</p>
<p> See S. E. Brown &ldquo;Statistical Power and Criminal Justice Research,&rdquo; Journal of Criminal 
</p>
<p>Justice
</p>
<p>Experiments,&rdquo; Crime and Justice</p>
<p/>
</div>
<div class="page"><p/>
<p>of  0.80, this would imply that we are willing to accept a probability of  making a 
</p>
<p>Type II error that is 20 times greater than the probability of  a Type I error. If  
</p>
<p>we simultaneously increase the level of  statistical power to 0.90 at the same time 
</p>
<p>we reduce the significance level, the 
</p>
<p>a much greater likelihood of  a Type II error. If  we wanted to keep the ratio 
</p>
<p>of  error probabilities at 4.0, we would need a study with a power level of  0.96 
</p>
<p>-
</p>
<p>neously increase our chances of  failing to reject a false null hypothesis unless we 
</p>
<p>have a more powerful study.
</p>
<p>C o m p o n e n t s  o f  S t a t i s t i c a l  P o w e r
</p>
<p>The level of statistical power associated with any given test of a sample statistic is 
</p>
<p>&#149; 
</p>
<p>&#149; Sample size
</p>
<p>&#149; Effect size
</p>
<p>The level of statistical significance and sample size are assumed to be within the 
</p>
<p>control of the researcher, while the estimated effect size is not. The following 
</p>
<p> discussion briefly describes the links between each element and the statistical 
</p>
<p>power of any given test.
</p>
<p>Statistical Significance and Statistical Power
</p>
<p>The most straightforward way to increase the statistical power of a test is to change 
</p>
<p>the significance level used. As we reduce the chances of making a Type I error by 
</p>
<p>reducing the level of statistical significance from 0.10 to 0.05 to 0.01, it becomes 
</p>
<p>increasingly difficult to reject the null hypothesis. Simultaneously, the power of the 
</p>
<p>test is reduced. A significance level of 0.05 results in a more powerful test than a 
</p>
<p>significance level of 0.01, because it is easier to reject the null hypothesis using the 
</p>
<p>more lenient significance criteria. Conversely, a 0.10 level of significance would 
</p>
<p>make it even easier to reject the null hypothesis.
</p>
<p>z-scores required to reject the 
</p>
<p>null hypothesis for several levels of statistical significance using a two-tailed test. 
</p>
<p>It would take a z
</p>
<p>hypothesis with p = 0.10, a z
</p>
<p>p = 0.05, and a z p = 0.01. Clearly, 
</p>
<p>α 0.20 0.10 0.05 0.01 0.001
</p>
<p>z-score &plusmn;1.282 &plusmn;1.645 &plusmn;1.960 2.576 3.291
</p>
<p> Table 23.1  z-Scores Needed to Reject the Null Hypothesis in a Two-Tailed Test of Statistical 
</p>
<p>Significance by Level of α
</p>
<p> C O M P O N E N T S  O F  S T A T I S T I C A L  P O W E R  731</p>
<p/>
</div>
<div class="page"><p/>
<p>732 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>it is much easier to reject the null hypothesis with a 0.10 significance threshold than 
</p>
<p>with a 0.01 significance threshold.
</p>
<p>This method for increasing statistical power is direct, but it means that any 
</p>
<p>benefit we gain in reducing the risk of a Type II error is offset by an increase in 
</p>
<p>the risk of a Type I error. By setting a more lenient significance threshold, we 
</p>
<p>do indeed gain a more statistically powerful research study. However, the level 
</p>
<p>of statistical significance of our test also declines. Since a 0.05 significance level 
</p>
<p>has become the convention in much of the research in criminology and criminal 
</p>
<p>statistical significance is used.
</p>
<p>Directional Hypotheses
</p>
<p>A related method for increasing the statistical power of a study is to limit the 
</p>
<p>direction of the research hypothesis to either a positive or a negative outcome, 
</p>
<p>which implies the use of a one-tailed statistical test. A one-tailed test will provide 
</p>
<p>greater statistical power than a two-tailed test for the same reason that a less strin-
</p>
<p>gent level of statistical significance provides more power than a more stringent 
</p>
<p>one. By choosing a one-tailed test, the researcher reduces the absolute value of the 
</p>
<p>test statistic needed to reject the null hypothesis by placing all of the probability of 
</p>
<p>making a Type I error in a single tail of the distribution.
</p>
<p>z-scores 
</p>
<p>needed to reject the null hypothesis in one- and two-tailed tests for five different 
</p>
<p>levels of statistical significance. (For the sake of simplicity, we assume in the one-
</p>
<p>tests, the test statistic required to reject the null hypothesis is smaller in the case 
</p>
<p>of a one-tailed test. For example, at p = 0.05, a z-score greater than or equal to 
</p>
<p>two-tailed test. In the one-tailed test, the z-score needs only to be greater than or 
</p>
<p>equal to 1.645. When we reduce the significance level to p = 0.01, a z-score greater 
</p>
<p>hypothesis in the two-tailed test, but in the one-tailed test, the z-score needs only 
</p>
<p>Although the researcher can increase the statistical power of a study by using 
</p>
<p>a directional, as opposed to a nondirectional, research hypothesis, there is a price 
</p>
<p>for shifting the rejection region to one side of the sampling distribution. Once a 
</p>
<p>one-directional test is defined, a finding in the direction opposite to that originally 
</p>
<p>predicted cannot be recognized. To do otherwise would bring into question the 
</p>
<p>integrity of the assumptions of the statistical test used in the analysis.
</p>
<p>0.20 0.10 0.05 0.01 0.001
</p>
<p>z-score (one-tail test)
z-score (two-tail test)
</p>
<p>&minus;0.842 or 0.842
&plusmn;1.282
</p>
<p>&minus;1.282 or 1.282
&plusmn;1.645
</p>
<p>&minus;1.645 or 1.645
&plusmn;1.960
</p>
<p>&minus;2.326 or 2.326
2.576
</p>
<p>&minus;3.090 or 3.090
3.291
</p>
<p> Table 23.2  z-Scores Needed to Reject the Null Hypothesis in One-Tailed and Two-Tailed 
</p>
<p>Tests of Statistical Significance</p>
<p/>
</div>
<div class="page"><p/>
<p>Sample Size and Statistical Power
</p>
<p>The method used most often to change the level of statistical power in social 
</p>
<p> science research is to vary the size of the sample. Similar to specifying the level of 
</p>
<p>statistical significance, sample size can be controlled by the researcher. Modifying 
</p>
<p>the size of the sample is typically a more attractive option for increasing statistical 
</p>
<p>power than modifying the level of statistical significance, since the risk of a Type I 
</p>
<p>error remains fixed&mdash;presumably at the conventional p = 0.05.
</p>
<p>The relationship between statistical power and sample size is straightforward. 
</p>
<p>All else being equal, larger samples provide more stable estimates of the popula-
</p>
<p>tion parameters than do smaller samples. Assuming that we are analyzing data 
</p>
<p>from random samples of a population, the larger sample will have smaller standard 
</p>
<p>errors of the coefficients than will the smaller sample. As the number of cases in 
</p>
<p>a sample increases, the standard error of the sampling distribution (for any given 
</p>
<p>-
</p>
<p>ard error for a single-sample t-test is computed as
</p>
<p>s
s
</p>
<p>se
N
</p>
<p>=
-1
</p>
<p>.
</p>
<p>As N
</p>
<p>se
</p>
<p>decreases, the likelihood of achieving statistical significance grows, because the 
</p>
<p>test statistic for a test of statistical significance is calculated by taking the ratio of 
</p>
<p>the difference between the observed statistic and the value proposed in the null 
</p>
<p>is held constant, then as the sample size increases, the standard error decreases, 
</p>
<p>and a larger test statistic is computed, making it easier to reject the null hypothesis.
</p>
<p>The effect of sample size on statistical power for a t-test of the difference of 
</p>
<p>100 two-sample t-tests in which a mean difference of two arrests between groups 
</p>
<p>hypothesis changes substantially with each increase in sample size, even though all 
</p>
<p>SCENARIO SAMPLE SIZE(PER GROUP) μ1 &minus; μ2 σ EXPECTED SIGNIFICANT OUTCOMES
</p>
<p>1
2
3
4
</p>
<p>35
100
200
</p>
<p>1,000
</p>
<p>0.2
0.2
0.2
0.2
</p>
<p>1
1
1
1
</p>
<p>13
29
51
99
</p>
<p> Table 23.3  Number of Statistically Significant Outcomes Expected in 100 Two-Sample 
</p>
<p>t-Tests for Four Scenarios
</p>
<p> 733C O M P O N E N T S  O F  S T A T I S T I C A L  P O W E R</p>
<p/>
</div>
<div class="page"><p/>
<p>734 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>other characteristics are held constant across the four scenarios. Under the first 
</p>
<p>tests. In the second scenario, 29 significant outcomes would be expected and in 
</p>
<p>the third, 51. In the final scenario of samples of 1,000, nearly every test (99 out of 
</p>
<p>statistical significance of a study.
</p>
<p>In most cases, researchers maximize the statistical power of  a study by increas-
</p>
<p>ing sample size. The concern with sample size is also reflected in the number of  
</p>
<p>publications focused on advising researchers in all behavioral and social science 
</p>
<p>fields on how to determine the appropriate sample size for a proposed research 
</p>
<p>study.4
</p>
<p>Although sample size should be under the control of the researcher, it is 
</p>
<p>important to be aware of the unanticipated consequences of simply increasing 
</p>
<p>sample size may have on other factors that influence statistical power, particularly 
</p>
<p>in evaluation research.5 For example, suppose a researcher has developed a com-
</p>
<p>plex and intensive method for intervening with high-risk youth. The impact of the 
</p>
<p>treatment is dependent on each subject receiving the &ldquo;full dosage&rdquo; of the treatment 
</p>
<p>for a six-month period. If the researcher were to increase the sample size of this 
</p>
<p>study, it might become more difficult to deliver the treatments in the way that was 
</p>
<p>originally intended by the researcher. More generally, increasing the sample size 
</p>
<p>of a study can decrease the integrity or the dosage of the interventions that are 
</p>
<p>applied and result in the study showing no effect of the treatment. Increasing the 
</p>
<p>size of a sample may also affect the variability of study estimates in other ways. For 
</p>
<p>example, it may become more difficult to monitor implementation of treatments as 
</p>
<p>a study grows. It is one thing to make sure that 100 subjects receive a certain inter-
</p>
<p>vention but quite another to ensure consistency of interventions across hundreds 
</p>
<p>or thousands of subjects. Also, studies are likely to include more heterogeneous 
</p>
<p>groups of subjects as sample size increases. For example, in a study of intensive 
</p>
<p>probation, eligibility requirements were continually relaxed in order to meet project 
</p>
<p>goals regarding the number of participants.6 As noted earlier, as the heterogeneity 
</p>
<p>of treatments or subjects in a study grows, it is likely that the standard deviations 
</p>
<p>of the outcomes examined will also get larger. This, in turn, leads to a smaller effect 
</p>
<p>size for the study and thus a lower level of statistical power.
</p>
<p>4 For a range of  examples, see P. Dattalo Determining Sample Size
</p>
<p>How Many Subjects: Statistical 
</p>
<p>Power Analysis in Research
</p>
<p>Myors Statistical Power Analysis
5 D. Weisburd &ldquo;Design Sensitivity in Criminal Justice Experiments,&rdquo; Crime and Justice
</p>
<p>6
</p>
<p>Evaluation Review</p>
<p/>
</div>
<div class="page"><p/>
<p>Effect Size and Statistical Power
</p>
<p>for statistical significance used in a test. Effect size measures the difference 
</p>
<p>between the actual parameters in the population and those hypothesized in the null 
</p>
<p>hypothesis. In computing effect size, it is important to take into account both the 
</p>
<p>raw differences between scores and the degree of variability found in the measures 
</p>
<p>examined. Taking into account variability in effect size is a method of standardiza-
</p>
<p>tion that allows for the comparison of effects across studies that may have used 
</p>
<p>different scales or slightly different types of measures. It has also allowed for the 
</p>
<p>standardization of estimates of statistical power across a wide range of studies and 
</p>
<p>types of analyses.
</p>
<p>Generally, effect size is defined as
</p>
<p>ES
Parameter H
</p>
<p>=
-
</p>
<p>.0
s
</p>
<p> Equation 23.1
</p>
<p>The relationship between effect size and statistical power should be clear. When 
</p>
<p>the standardized population parameters differ substantially from those proposed 
</p>
<p>in the null hypothesis, the researcher should be more likely to observe a significant 
</p>
<p>examined. Effect size will increase when the difference between the population 
</p>
<p>parameter and the hypothesized parameter increases and the standard error is 
</p>
<p>held constant or when the difference is held constant and the standard error is 
</p>
<p>decreased, perhaps through the use of a larger sample of cases.
</p>
<p>A difference of means test for two independent samples provides a simple 
</p>
<p>illustration for these relationships. In the difference of means test, effect size 
</p>
<p>would be calculated by first subtracting the population difference as stated in the 
</p>
<p>null hypothesis (H
0
μ
</p>
<p>1
 &ndash; H
</p>
<p>0
μ
</p>
<p>2
</p>
<p>population (μ
1
 &ndash; μ
</p>
<p>2
</p>
<p>as the pooled or the common standard deviation of the outcome measures in the 
</p>
<p>ES =
-( )- -( )m m m m
</p>
<p>s
1 2 0 1 0 2H H .  Equation 23.2
</p>
<p> Effect size can also be calculated for observed differences in a study. This is a common 
</p>
<p>approach in meta-analysis, where a large group of  studies are summarized in a single analysis. 
</p>
<p>For example, in calculating effect size for a randomized experiment with one treatment and 
</p>
<p>one control group, the researcher would substitute the outcome scores for both groups in the 
</p>
<p>numerator of  the ES equation and the pooled standard deviation for the two outcome  
</p>
<p>measures in the denominator. For a more detailed discussion of  effect size and its use generally 
</p>
<p>for comparing effects across different studies, see M. Lipsey and D. Wilson Practical  
</p>
<p>Meta-Analysis Meta-Analytic 
</p>
<p>Procedures for Social Research
</p>
<p>  735C O M P O N E N T S  O F  S T A T I S T I C A L  P O W E R</p>
<p/>
</div>
<div class="page"><p/>
<p>736 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>Since the null hypothesis for a difference of means test is ordinarily that the two 
</p>
<p>population means are equal (i.e., H
0
μ
</p>
<p>1
H
</p>
<p>0
μ
</p>
<p>2
</p>
<p>ES =
-( )m m
s
</p>
<p>1 2
.  Equation 23.3
</p>
<p>Thus, the ES for a difference of means test may be defined simply as the raw 
</p>
<p>difference between the two population parameters, divided by their common 
</p>
<p>standard deviation. To reiterate an earlier comment, when the difference between 
</p>
<p>the population means is greater, the ES for the difference of means will be larger. 
</p>
<p>Also, as the variability of the scores of the parameters grows, as represented by the 
</p>
<p>standard deviation of the estimates, the ES will get smaller.
</p>
<p>number of statistically significant outcomes expected in 100 t-tests (using a 0.05 
</p>
<p>significance threshold and a nondirectional research hypothesis, resulting in a two-
</p>
<p>In the first three scenarios, the mean differences between the two populations 
</p>
<p>are varied and the standard deviations for the populations are held constant. In 
</p>
<p>the last three scenarios, the mean differences are held constant and the standard 
</p>
<p>deviations differ.
</p>
<p>is expected in either the comparisons with the largest differences between mean 
</p>
<p>scores or the comparisons with the smallest standard deviations. As the differ-
</p>
<p>likelihood of obtaining a statistically significant result. Conversely, as the popula-
</p>
<p>expected number of significant outcomes decreases.
</p>
<p>As this exercise illustrates, there is a direct relationship between the two com-
</p>
<p>ponents of effect size and statistical power. Studies that examine populations in 
</p>
<p>which there is a larger effect size will, all else being equal, have a higher level of 
</p>
<p>statistical power. Importantly, the relationship between effect size and statistical 
</p>
<p>power is unrelated to the significance criteria we use in a test. In this sense, effect 
</p>
<p>SCENARIO μ1 μ2 σ EXPECTED SIGNIFICANT OUTCOMES
</p>
<p>(a) Means differ; standard deviations constant
</p>
<p>1
2
3
</p>
<p>0.3
0.3
0.3
</p>
<p>0.5
0.9
1.3
</p>
<p>2
2
2
</p>
<p>10
56
94
</p>
<p>(b) Means constant; standard deviations differ
</p>
<p>4
5
6
</p>
<p>0.3
0.3
0.3
</p>
<p>0.5
0.5
0.5
</p>
<p>0.5
1
2
</p>
<p>80
29
10
</p>
<p> Table 23.4  Number of Statistically Significant Outcomes Expected in 100 Two-Sample 
</p>
<p>t-Tests for Six Different Scenarios (100 Cases in Each Sample)</p>
<p/>
</div>
<div class="page"><p/>
<p>size allows for increasing the statistical power of a study (and thus reducing the risk 
</p>
<p>-
</p>
<p>Although effect size is often considered the most important component of 
</p>
<p>statistical power, it is generally very difficult for the researcher to manipulate in 
</p>
<p>a specific study.8 Ordinarily, a study is initiated in order to determine the type 
</p>
<p>and magnitude of a relationship that exists in a population. In many cases, the 
</p>
<p>researcher has no influence at all over the raw differences or the variability of the 
</p>
<p>scores on the measures examined. For example, a researcher who is interested in 
</p>
<p>identifying whether male and female police officers have different attitudes toward 
</p>
<p>corruption may have no idea prior to the execution of a study the nature of these 
</p>
<p>attitudes or their variability. It is then not possible for the researcher to estimate 
</p>
<p>the nature of the effect size prior to collecting and analyzing data&mdash;the effect size 
</p>
<p>may be large or small, but it is not a factor that the researcher is able to influence.
</p>
<p>In contrast, evaluation research&mdash;in which a study attempts to assess a specific 
</p>
<p>program or intervention&mdash;the researcher may have the ability to influence the 
</p>
<p>effect size of a study and thus minimize the risk of making a Type II error. There is 
</p>
<p>growing recognition, for example, of the importance of ensuring the strength and 
</p>
<p>integrity of criminal justice interventions.9 Moreover, many criminal justice evalu-
</p>
<p>ations fail to show a statistically significant result simply because the interventions 
</p>
<p>are too weak to have the desired impact or the outcomes are too variable to allow 
</p>
<p>a statistically significant finding.10
</p>
<p>Statistical power suggests that researchers should be concerned with the effect 
</p>
<p>size of their evaluation studies if they want to develop a fair test of the research 
</p>
<p>hypothesis. First, the interventions should be strong enough to lead to the expect-
</p>
<p>ed differences in the populations under study. Of course, the larger the differences 
</p>
<p>expected, the greater the statistical power of an investigation. Second, interven-
</p>
<p>tions should be administered in ways that maximize the homogeneity of outcomes. 
</p>
<p>For example, interventions applied differently to each subject will likely increase 
</p>
<p>the variability of outcomes and thus the standard deviation of those scores. Finally, 
</p>
<p>researchers should recognize that the heterogeneity of the subjects studied (and 
</p>
<p>the statistical power of their tests. Different types of people are likely to respond 
</p>
<p>in different ways to treatment or interventions. If they do respond differently, the 
</p>
<p>variability of outcomes will be larger, and thus the likelihood of making a Type II 
</p>
<p>error will increase.
</p>
<p>As a caution, we note that a wide range of research in criminology and crimi-
</p>
<p>nal justice has increasingly made use of archival data sets that result in research-
</p>
<p>ers analyzing populations rather than samples. Examples of this would include 
</p>
<p>studies that rely on archival data on all punishment decisions made in the US 
</p>
<p>8 M. Lipsey Design Sensitivity: Statistical Power for Experimental Research,  
</p>
<p>9
</p>
<p>Evaluation Review
10 D. Weisburd &ldquo;Design Sensitivity in Criminal Justice Experiments,&rdquo; Crime and Justice  
</p>
<p>  737C O M P O N E N T S  O F  S T A T I S T I C A L  P O W E R</p>
<p/>
</div>
<div class="page"><p/>
<p>738 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>Federal District Courts or census data on all prisoners in a state on a specific date.  
</p>
<p>which populations are analyzed calls into question many of the assumptions about 
</p>
<p>performing tests for statistical significance.11 Put simply, the analysis of popula-
</p>
<p>tion data implies no need for statistical significance testing, since the researcher is 
</p>
<p>not trying to generalize from a sample to a population. Clearly, issues of statistical 
</p>
<p>makes little sense, the number of cases in the data set is as large as it possibly can 
</p>
<p>E s t i m a t i n g  S t a t i s t i c a l  P o w e r  a n d  S a m p l e  S i z e  
f o r  a  S t a t i s t i c a l l y  P o w e r f u l  S t u d y
</p>
<p>A number of texts have been written that provide detailed tables for defining the 
</p>
<p>statistical power of a study.12 All of these texts also provide a means for computing 
</p>
<p>the size of the sample needed to achieve a given level of statistical power. In both 
</p>
<p>cases&mdash;the estimation of statistical power or the estimation of necessary sample 
</p>
<p>size&mdash;assumptions will need to be made about effect size and level of statistical 
</p>
<p>significance desired. The following discussion provides a basic illustration for 
</p>
<p>how to compute estimates of statistical power. (The computations reported in the 
</p>
<p>following discussion have been performed with a variety of statistical software 
</p>
<p>tools, several of which are freely available. More detail on several easily accessible 
</p>
<p>resources to compute power estimates is provided in the computer problems sec-
</p>
<p>The most common application of statistical power analysis in criminology and 
</p>
<p>criminal justice research has been to compute the sample size needed to achieve a 
</p>
<p>to be cautious about simply increasing the size of the sample, since a larger sample 
</p>
<p>can affect other important features of statistical power. Thus, in using increased 
</p>
<p>sample size to minimize Type II error, we must consider the potential conse-
</p>
<p>quences that larger samples might have on the nature of interventions or subjects 
</p>
<p>studied, particularly in evaluation research. Nonetheless, sample size remains the 
</p>
<p>tool most frequently used for adjusting the power of studies, because it can be 
</p>
<p>manipulated by the researcher and does not require changes in the significance 
</p>
<p>criteria of a test.
</p>
<p>To define how many cases should be included in a study, we must conduct 
</p>
<p>power analyses before the study is begun, generally referred to as prospective or  
</p>
<p>11  
</p>
<p>Journal of Research in Crime and Delinquency
12 Among some of  the more widely used examples are J. Cohen Statistical Power Analysis 
</p>
<p>for the Behavioral Sciences
</p>
<p>and S. Thiemann How Many Subjects: Statistical Power Analysis in Research, (Newbury 
</p>
<p>Design Sensitivity: Statistical Power for Experimental 
</p>
<p>Research Statistical 
</p>
<p>Power Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>a priori power analysis, and where our attention has been focused thus far in this 
</p>
<p>chapter. Some authors have advocated the use of power analysis to evaluate wheth-
</p>
<p>er studies already conducted have acceptable levels of statistical power, based 
</p>
<p>on the sample statistics, referred to as retrospective or post hoc power analy-
</p>
<p>sis. Although there is much agreement about the utility of prospective power 
</p>
<p>analysis, there is little consensus about the appropriateness of retrospective power 
</p>
<p>analysis.  The widespread use of secondary data sources in the study of crime and 
</p>
<p>criminal justice further complicates the interpretation of results from a statistical 
</p>
<p>power analysis. Since it is not possible for researchers to augment the original 
</p>
<p>that the results will indicate to the researchers using these data sources what the 
</p>
<p>archived data set can and cannot tell them about the statistical relationships they 
</p>
<p>may be most interested in.
</p>
<p>To define the sample size needed for a powerful study, we must first clearly 
</p>
<p>define each of the components of statistical power other than sample size. These 
</p>
<p> 1. The statistical test
</p>
<p> 2. The significance level
</p>
<p> 4. The effect size
</p>
<p>The first three of these elements should be familiar, since they are based on com-
</p>
<p>mon assumptions made in developing any statistical test. The statistical test is cho-
</p>
<p>sen based on the type of measurement and the extent to which the study can meet 
</p>
<p>certain assumptions. For example, if we want to compare three sample means, we 
</p>
<p>will likely use analysis of variance as our test. If we are comparing means from two 
</p>
<p>samples, we will likely use a two-sample t-test. If we are interested in the unique 
</p>
<p>effects of a number of independent variables on a single interval-level dependent 
</p>
<p>variable, we will likely use OLS regression and rely on t-tests for the individual 
</p>
<p>coefficients and F-tests for either the full regression model or a subset of variables 
</p>
<p>from the full model.
</p>
<p>To calculate statistical power, we must also define the significance level of a 
</p>
<p>test and its research hypothesis. By convention, we generally use a 0.05 significance 
</p>
<p>threshold, and thus we are likely to compute statistical power estimates based 
</p>
<p>on this criterion. The research hypothesis defines whether a test is directional or 
</p>
<p>nondirectional. When the statistical test allows for it, we will typically choose a 
</p>
<p>nondirectional test to take into account the different types of outcomes that can 
</p>
<p>be found in a study.14 If we were evaluating an existing study, we would use the 
</p>
<p> For an example, see the exchange between J. P. Hayes and R. J. Steidl &ldquo;Statistical Power 
</p>
<p>Analysis and Amphibian Population Trends,&rdquo; Conservation Biology  
</p>
<p>and L. Thomas &ldquo;Retrospective Power Analysis,&rdquo; Conservation Biology
14 J. Cohen Statistical Power Analysis for the Behavioral Sciences  
</p>
<p> E S T I M A T I N G  S T A T I S T I C A L  P O W E R  A N D  S A M P L E  S I Z E  739</p>
<p/>
</div>
<div class="page"><p/>
<p>740 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>The fourth element, defining effect size, is perhaps the most difficult 
</p>
<p> component. If we are trying to estimate the magnitude of a relationship in the 
</p>
<p>population that has not been well examined in the past, how can we estimate the 
</p>
<p>effect size in the population? It may be useful to reframe this criterion. The pur-
</p>
<p>pose of a power analysis is to see whether our study is likely to detect an effect 
</p>
<p>of a certain size. Usually, we define that effect in terms of what is a meaningful 
</p>
<p>outcome in a study. A power analysis, then, tells us whether our study is designed 
</p>
<p>in a way that is likely to detect that outcome (i.e., reject the null hypothesis on the 
</p>
<p>sometimes defined as design sensitivity.15 It assesses whether our study is designed 
</p>
<p>with enough sensitivity to be likely to reject the null hypothesis if an effect of a 
</p>
<p>certain size exists in the population under study.
</p>
<p>The task of defining effect size has been made easier by identifying broad 
</p>
<p>have been the most widely adopted by other researchers and simply refer to classi-
</p>
<p>fying effect sizes as small, medium, and large.16 The numeric value associated with 
</p>
<p>an effect size classified as small, medium, or large is contingent on the specific sta-
</p>
<p>tistical test being considered. For example, if our focus is on a difference of means 
</p>
<p>test for two independent samples, the standardized effect size estimate is known 
</p>
<p>as d
</p>
<p>effect if it is 0.5, and a large effect if it is 0.8. In contrast, if we are considering the 
</p>
<p>statistical power of an OLS regression model, the standardized effect size estimate 
</p>
<p>is known as f2 and is considered to be a small effect if it is 0.02, a medium effect 
</p>
<p>attempted to define similar types of standardized effects for more complex statisti-
</p>
<p>The following illustration turns to a discussion of the computation of statistical 
</p>
<p>difference of means test, ANOVA, correlation, and OLS regression&mdash;all of which 
</p>
<p>have been the focus of previous chapters.
</p>
<p>The computation of statistical power estimates requires the comparison of a 
</p>
<p>sampling distribution under the null hypothesis with a sampling distribution under 
</p>
<p>sampling distribution under the research hypothesis is referred to as a non-central 
</p>
<p>-
</p>
<p>tion under the null hypothesis is the t-distribution, while the sampling distribution 
</p>
<p>under the research hypothesis is the non-central t-distribution.
</p>
<p>The non-central sampling distribution is computed based on a &ldquo;non-centrality&rdquo; 
</p>
<p>parameter, which in all cases is a function of the standardized effect for the statisti-
</p>
<p>cal test under consideration. For each of the statistical tests discussed below, we 
</p>
<p>describe both the standardized effect and the non-centrality parameter and explain 
</p>
<p>15 M. Lipsey Design Sensitivity: Statistical Power for Experimental Research, (Newbury 
</p>
<p>16 J. Cohen Statistical Power Analysis for the Behavioral Sciences</p>
<p/>
</div>
<div class="page"><p/>
<p>how to use these values to estimate the statistical power of a sample as well as the 
</p>
<p>size of sample needed to meet a target level of statistical power.
</p>
<p>Difference of Means Test
</p>
<p>Throughout this chapter, we have pointed to the difference of means test as an 
</p>
<p>example for many of the points we wanted to make about statistical power. More 
</p>
<p>directly, the standardized effect size d is
</p>
<p>d =
-m m
s
</p>
<p>1 2 ,
 
</p>
<p>which is identical to the equation noted earlier for computing a standardized dif-
</p>
<p>ference of means for two independent samples. Recall that  represents the pooled, 
</p>
<p>or common, standard deviation for the difference of means.
</p>
<p>The non-centrality parameter  for the t-distribution is
</p>
<p>d = d
N
</p>
<p>4
,  Equation 23.4
</p>
<p>where N = n
1
 + n
</p>
<p>2
 when there are equal numbers of cases in each group (i.e., 
</p>
<p>n
1
 = n
</p>
<p>2
n
</p>
<p>1
n
</p>
<p>2
, the non-centrality parameter  is
</p>
<p>d = =
+
</p>
<p>d
N
</p>
<p>where N
n n
</p>
<p>n n
</p>
<p>H
</p>
<p>H
2
</p>
<p>2 1 2
</p>
<p>1 2
</p>
<p>, .  Equation 23.5
</p>
<p>To illustrate the computation of a statistical power estimate, suppose that we want 
</p>
<p>to assess the effectiveness of a treatment program for drug offenders. Our design 
</p>
<p>calls for random assignment of 100 cases to each group. We expect the program 
</p>
<p>to be effective at reducing recidivism in the treatment group and so can assume a 
</p>
<p>one-tailed t-test with a significance level of 5%. What is the statistical power of our 
</p>
<p>design for detecting standardized effects at the small (d d
</p>
<p>and large (d
</p>
<p>For all three scenarios, the critical t
</p>
<p>test with a significance level of 0.05 and df = N &ndash; 2 = 198. For a small effect, 
</p>
<p>an estimate for risk of making a Type II error of 
</p>
<p>hypothesis when it is false.  The corresponding estimate of statistical power is 
</p>
<p>in each group, our probability of rejecting the null hypothesis when it is false is 
</p>
<p> &lt; 0.0001, and power &gt; 0.9999. 
</p>
<p> It is not possible to include copies of  non-central t-distribution tables in the same way that 
</p>
<p>t-distribution in Appendix 4. We will illustrate in the Computer 
</p>
<p>Exercises at the end of  this chapter how to work with the non-centrality parameter to obtain 
</p>
<p>estimates of   from various statistical packages.
</p>
<p> E S T I M A T I N G  S T A T I S T I C A L  P O W E R  A N D  S A M P L E  S I Z E  741</p>
<p/>
</div>
<div class="page"><p/>
<p>742 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>Putting these results together indicates that our design with 100 cases assigned to 
</p>
<p>each group provides a high level of statistical power for detecting medium effects 
</p>
<p>and larger but an inadequate level of power for detecting small effects.
</p>
<p>Alternatively, we may be interested in determining the sample size needed to 
</p>
<p>provide us with a statistical power estimate of 80% for each of the three effect 
</p>
<p>to reject the null hypothesis when it is false about 80% of the time. To achieve a 
</p>
<p>power estimate of 80% for a medium effect, we only need 102 cases (51 in each 
</p>
<p>ANOVA
</p>
<p>For a simple ANOVA, where we are looking only at fixed effects and assume equal 
</p>
<p>sample sizes across groups, the standardized effect size f is defined as
</p>
<p>f m=
s
s
</p>
<p>,  Equation 23.6
</p>
<p>where sm
i
</p>
<p>k
im m
</p>
<p>k
=
</p>
<p>-( )
=
&aring;
</p>
<p>1
</p>
<p>2
</p>
<p>, k is the number of groups, m is the grand mean, and 
</p>
<p>m
i
 represents each of the group means with n
</p>
<p>1
 = n
</p>
<p>2
 = &hellip; = n
</p>
<p>k
.
</p>
<p>The non-centrality parameter  for the F-distribution is
</p>
<p>=f 2N, Equation 23.7
</p>
<p>where f2 refers to the square of the standardized effect size (f N refers to the 
</p>
<p>total sample size.
</p>
<p>As an illustration of the calculation of statistical power estimates for a fixed-
</p>
<p>effects ANOVA model, assume that we have three groups, each with 100 cases 
</p>
<p>participating in an experiment aimed at reducing recidivism among violent offend-
</p>
<p>significance level has been set at 5%. What is the level of statistical power of our 
</p>
<p>design for detecting standardized effects at the small (f f
</p>
<p>and large (f
</p>
<p>For each of the three scenarios, the critical value of the F
</p>
<p>(df
1
 = 2, df
</p>
<p>2
</p>
<p>of  = 0.681, suggesting that we have a probability of 68.1% of making a Type II 
</p>
<p>error and fail to reject the null hypothesis when it is false. The corresponding esti-
</p>
<p>chance of rejecting the null hypothesis when it is false. This result is presented 
</p>
<p>F-distribution, and the dashed 
</p>
<p>line the non-central F-distribution. Below the two curves, represented by two dif-
</p>
<p>ferent shades of grey, alpha is indicated by the darker shading in the right tail of </p>
<p/>
</div>
<div class="page"><p/>
<p>the F-distribution beyond the critical value, and beta is represented by the lighter 
</p>
<p>shaded area to the left of the critical value and under the non-central F-distribution.
</p>
<p>For the medium and large effect size analyses, the F-distribution remains the 
</p>
<p>same, but the non-central F-distribution is shifted further to the right. For the 
</p>
<p>medium effect size, 
</p>
<p>has  = 48,  &lt; 0.0001, and power &gt; 0.9999. Similar to the previous analysis compar-
</p>
<p>ing the means for only two groups, our research design with 100 cases assigned 
</p>
<p>to each of the three groups provides a high level of statistical power for detecting 
</p>
<p>medium and large effects but an inadequate level of power for detecting small 
</p>
<p>effects.
</p>
<p>If our concern is focused on the size of the sample needed for a power level of 
</p>
<p>80% for each of the three effect sizes&mdash;small, medium, and large&mdash;then we would 
</p>
<p>again proceed in the same way as in the two-sample t-test. To have an 80% chance 
</p>
<p>of detecting a small effect (f
</p>
<p>Correlation
</p>
<p>To test the statistical power of a correlation coefficient, we can use either the 
</p>
<p>correlation coefficient (r r-to-Z transformation of the correlation 
</p>
<p>coefficient (r
Z
</p>
<p>power will not be identical, they will tend to be very close, typically differing only 
</p>
<p>at the second or the third decimal.
</p>
<p>d =
-
</p>
<p>&acute;
r
</p>
<p>r
N
</p>
<p>2
</p>
<p>21
,  Equation 23.8
</p>
<p>where r is either the sample correlation coefficient (r
</p>
<p>(r
Z
</p>
<p>N is the sample size.
</p>
<p>We can again illustrate the calculation of statistical power for correlations by 
</p>
<p>assuming that we have 100 observations that would allow us to compute a correla-
</p>
<p>tion between two variables. For example, suppose we interview a random sample 
</p>
<p>of police officers and are interested in the correlation between the number of 
</p>
<p> Figure 23.3 Graphical Representation for Power Analysis in a One-Way ANOVA
</p>
<p> E S T I M A T I N G  S T A T I S T I C A L  P O W E R  A N D  S A M P L E  S I Z E  743
</p>
<p>0 1 2 3 4 5 6 7
0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>α
β
</p>
<p>Critical F=3.026</p>
<p/>
</div>
<div class="page"><p/>
<p>744 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>years on the police force and a scale that measured hostility toward judges. We 
</p>
<p>might expect that more years on the police force will have a positive correlation 
</p>
<p>with hostility toward judges, implying that we can conduct a one-tailed t-test of 
</p>
<p>statistical significance. As with the preceding examples, assume that the level of 
</p>
<p>statistical significance is 5%. What is the level of statistical power of our design 
</p>
<p>for detecting standardized effects at the small (r r
</p>
<p>(r
</p>
<p>The critical t-value for all three scenarios is 1.661, based on df = N
</p>
<p>a small effect size (r
</p>
<p>us with an estimate for risk of making a Type II error of 
</p>
<p>the null hypothesis when it is false. The corresponding estimate of statistical power 
</p>
<p>is 0.259, indicating that we would only reject the null hypothesis when it was false 
</p>
<p>-
</p>
<p>cal power analysis of the medium effect indicates that 
</p>
<p>where  &lt; 0.0001, and power &gt; 0.9999.
</p>
<p>The sample size required to detect each of the three effect sizes&mdash;small, 
</p>
<p>medium, and large&mdash;with a statistical power of 80% again requires the use of the 
</p>
<p>t-distribution. To achieve a power level of 80% for a small effect (r -
</p>
<p>ple of 614 cases would be needed. For the medium effect (r
</p>
<p>number of cases drops to 64, while for the large effect (r
</p>
<p>required to have an 80% chance of rejecting the null hypothesis when it is false.
</p>
<p>Least-Squares Regression
</p>
<p>The statistical power analysis of least-squares regression can take two different, but 
</p>
<p>related, forms. One question asks about the ability to detect whether a regression 
</p>
<p>model&mdash;a single dependent variable and two or more independent variables&mdash;has 
</p>
<p>a statistically significant effect on the dependent variable. This means that the null 
</p>
<p>hypothesis is focused on whether the regression model in its entirety has an effect 
</p>
<p>on the dependent variable. A second question asks about the ability to detect the 
</p>
<p>effect of a single variable or a subset of variables added to a regression model. 
</p>
<p>This addresses the more common substantive question in much of the published 
</p>
<p> Figure 23.4 Graphical Representation for Power Analysis of a Correlation
</p>
<p>&minus;4 &minus;2 0 2 4
0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>αβ
</p>
<p>Critical t=1.661</p>
<p/>
</div>
<div class="page"><p/>
<p>taken into account statistically, does variable X add anything to the overall model?
</p>
<p>Whether we are analyzing the full model or a subset of the full model, the stand-
</p>
<p>ardized effect size (denoted as f 2 R2 for the full model or 
</p>
<p>the partial R2 for the subset of variables we are interested in analyzing. Specifically,
</p>
<p>R
f
</p>
<p>f
</p>
<p>2
2
</p>
<p>21
=
</p>
<p>+
.  Equation 23.9
</p>
<p>To provide some context to these values, an f 2 value of 0.02 corresponds to an R2  
</p>
<p>of 0.02, while f 2 = 0.15 implies that R2 f 2 R2 = 0.26. 
</p>
<p>Statistical power analysis for least-squares regression uses the F-distribution.
</p>
<p>As noted in the discussion of statistical power analysis for ANOVA models, the 
</p>
<p>non-centrality parameter  for the F-distribution is
</p>
<p> = f 2N.
</p>
<p>To assess the statistical power for the full regression model consider the follow-
</p>
<p>ing simple example. Suppose that we are interested in the effects of various case 
</p>
<p>and defendant characteristics on the amount of bail required by a court. Typical 
</p>
<p>analyses of bail decisions would consider some of the following characteristics (as 
</p>
<p>-
</p>
<p>whether the defendant was under criminal justice supervision at the time of the 
</p>
<p>of the defendant. This provides us with a regression model with eight independent 
</p>
<p>variables.
</p>
<p>As a point of illustration, we may want to estimate the statistical power of the 
</p>
<p>regression model assuming that we have a sample of only 100 cases and have set 
</p>
<p>a significance level of 5%, giving us a crucial F-value of 2.024. For the small effect 
</p>
<p>size ( f 2
</p>
<p>find 
</p>
<p>a Type II error of just under 88%. Alternatively, the estimate of statistical power 
</p>
<p>is 0.124, meaning that we have a probability of only 12.4% of rejecting the null 
</p>
<p>hypothesis when it is false. The results for the medium effect ( f 2
</p>
<p> = 15.0, 
</p>
<p>an inadequate level of power but is much closer to the target of 80%. For the large 
</p>
<p>effect (f 2
</p>
<p>desired level of 80%.
</p>
<p>For a regression model with eight independent variables, what sample size is 
</p>
<p>required to achieve a statistical power level of  80% for detecting effects at the small 
</p>
<p>(f2 f2 f2
</p>
<p>-
</p>
<p>um and large effects, we would require samples of  109 and 52 cases, respectively. 
</p>
<p> E S T I M A T I N G  S T A T I S T I C A L  P O W E R  A N D  S A M P L E  S I Z E  745</p>
<p/>
</div>
<div class="page"><p/>
<p>746 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>The number of  cases required to detect a statistically significant effect at either the 
</p>
<p>medium or the large effect level may strike many readers as small. It is important to 
</p>
<p>keep in mind that we have only been assessing the full model&mdash;the number of  cases 
</p>
<p>required for detecting individual effects will tend to be different than the number of  
</p>
<p>cases required for detecting whether the full model is significant.
</p>
<p>The assessment of statistical power for a single independent variable or a small 
</p>
<p>subset of independent variables proceeds in much the same way as the analysis for 
</p>
<p>the full model. The key difference is in the degrees of freedom required for the 
</p>
<p>F-distribution. In the case of a single independent variable, the numerator df = 1, 
</p>
<p>while the denominator df remains the same as in the full model. For a subset of 
</p>
<p>independent variables, the numerator df = the number of variables in the subset 
</p>
<p>(the denominator df
</p>
<p>If we return to the bail example above, the analysis of statistical power for 
</p>
<p>any one of the independent variables will be identical. We continue to keep the 
</p>
<p>sample size at 100 cases, the level of statistical significance at 5%, and the defini-
</p>
<p>tion of small, medium, and large effects the same as before. For the small effect 
</p>
<p>( f 2  = 2.0, 
</p>
<p>able to reject the null hypothesis of no relationship between the independent and 
</p>
<p>dependent variables about 28.8% of the time. For the medium effect ( f 2
</p>
<p> = 15.0, f 2
</p>
<p> &lt; 0.0001, and power &gt; 0.9999.
</p>
<p>Similarly, we may be interested in assessing the statistical power of a subset 
</p>
<p>of variables. For example, in the bail example, the subset of demographic char-
</p>
<p>theory predicting differential treatment of defendants within the courts. We find 
</p>
<p>a similar pattern to the results. For the small effect ( f 2  = 2.0,  = 0.814, 
</p>
<p>and power = 0.186, again indicating a low level of statistical power for detecting a 
</p>
<p>statistically significant relationship between demographic characteristics and bail 
</p>
<p>amount. For the medium effect ( f 2  = 15.0,  = 0.095, and power = 0.905, 
</p>
<p>while for the large effect ( f 2  = 0.001, and power = 0.999.
</p>
<p>Sample size calculations work in the same way as for the full model. If we 
</p>
<p>hope to achieve a power level of 80%, what size sample is necessary to detect 
</p>
<p> Figure 23.5  Graphical Representation for Power Analysis of a Regression Model (with Eight Independent 
</p>
<p>Variables)
</p>
<p>0 1 2 3 4 5
0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>αβ
</p>
<p>Critical F=2.042</p>
<p/>
</div>
<div class="page"><p/>
<p>small, medium, and large effects for either single variables or subsets of variables? 
</p>
<p>Continuing the bail example, we assume that there are eight independent variables. 
</p>
<p>For the single variable, the number of cases required to detect a small effect with 
</p>
<p>effect requires only 26 cases. It is worth noting that sample size calculations for 
</p>
<p>single variable effects are not affected by the number of variables included in the 
</p>
<p>full regression model.
</p>
<p>In practice, many of the individual effects that researchers are trying to assess in 
</p>
<p>their multivariate models will tend toward the small effect size. For example, much 
</p>
<p>survey research aimed at trying to explain attitudes toward a particular topic will 
</p>
<p>often incorporate 10&ndash;20 independent variables and have a full model R2  typically 
</p>
<p>between 0.15 and 0.20. This implies that many of the effects of individual variables 
</p>
<p>will tend to be quite small in magnitude. In order for an analysis to detect a statisti-
</p>
<p>cally significant relationship, a much large sample becomes necessary.
</p>
<p>S u m m i n g  U p :  A v o i d i n g  S t u d i e s  D e s i g n e d  f o r  F a i l u r e
</p>
<p>The statistical power of a test can be compared to the sensitivity of a radiation 
</p>
<p>meter. A very sensitive meter will be able to identify even the smallest deposits of 
</p>
<p>radioactivity. A meter that is not very sensitive will often miss such small deposits, 
</p>
<p>although it likely will detect very large radiation signals from areas rich in radio-
</p>
<p>activity. Similarly, a statistically sensitive study will be able to identify even small 
</p>
<p>effects. This is usually because the researcher has increased the sample size of 
</p>
<p>the study to make it more statistically powerful. Conversely, a study that has little 
</p>
<p>sensitivity is unlikely to yield a statistically significant result even when relatively 
</p>
<p>large differences or program impacts are observed. Such studies may be seen as 
</p>
<p>&ldquo;designed for failure,&rdquo; not because of inadequacies in the theories or the programs 
</p>
<p>evaluated, but because the investigator failed to consider statistical power at the 
</p>
<p>outset of the study.
</p>
<p>You might question why we would even bother to define the size of the sam-
</p>
<p>ple needed for statistically powerful studies. Why not just collect 1,000 or more 
</p>
<p>cases in every study and be almost assured of a statistically powerful result? The 
</p>
<p>simple answer is that although you should try to sample as many cases as you can 
</p>
<p>in a study, there are generally constraints in developing samples. These constraints 
</p>
<p>may be monetary, related to time, or associated with access to subjects. It is often 
</p>
<p>important to know the minimum number of cases needed to achieve a certain 
</p>
<p>threshold of statistical power so that you can try, within the constraints of the 
</p>
<p>research setting, to reach an adequate level of statistical power in your study. It is 
</p>
<p>also important to be able to assess whether studies that you read or evaluate were 
</p>
<p>designed in such a way that they are reasonable tests of the hypotheses presented. 
</p>
<p>If such studies are strongly underpowered, then you should have much less confi-
</p>
<p>dence in findings that do not support the research hypothesis.
</p>
<p> S U M M I N G  U P :  A V O I D I N G  S T U D I E S  D E S I G N E D  F O R  F A I L U R E  747</p>
<p/>
</div>
<div class="page"><p/>
<p>748 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>C h a p t e r  S u m m a r y
</p>
<p>A statistically powerful test is one for which there is a low risk of making a Type II 
</p>
<p>error. Statistical power can be defined as 1 minus the probability of falsely accept-
</p>
<p>ing the null hypothesis. A test with a statistical power of 0.90 is one for which 
</p>
<p>there is only a 10% probability of making a Type II error. If the power of a test is 
</p>
<p>0.10, the probability of a Type II error is 90%. A minimum statistical power level 
</p>
<p>of at least 0.50 is recommended. However, it is generally accepted that in better 
</p>
<p>studies, the level of statistical power will be at least 0.80. A study with a low level 
</p>
<p>of statistical power can be described as &ldquo;designed for failure,&rdquo; as it is unlikely to 
</p>
<p>produce a statistically significant result even if the expected effect exists in the 
</p>
<p>population under study.
</p>
<p>There are several ways in which statistical power can be maximized. First, we 
</p>
<p>may raise the significance threshold. Doing so, however, also increases the risk of 
</p>
<p>a Type I error. Second, we may limit the direction of the research hypothesis and 
</p>
<p>conduct a one-tailed test. Doing so, though, will necessarily ignore outcomes in the 
</p>
<p>opposite direction. Third, we may try to maximize the effect size. The greater the 
</p>
<p>differences betwen the populations and the smaller the variability of those differ-
</p>
<p>ences, the larger the population effect size will be. Effect size, however, is usually 
</p>
<p>beyond the control of the researcher. Fourth, we may increase the sample size. A 
</p>
<p>larger sample produces a smaller standard error for the sampling distribution and 
</p>
<p>a larger test statistic. The larger the sample, all else being equal, the greater the 
</p>
<p>chance of rejecting the null hypothesis.
</p>
<p>Sample size is generally the most useful tool for maximizing statistical power. 
</p>
<p>A power analysis before a study is begun will define the number of cases needed 
</p>
<p>to identify a particular size effect&mdash;small, medium, or large. A power analysis of an 
</p>
<p>existing study will help to identify whether it was well designed to assess the ques-
</p>
<p>tions that were examined. To identify a small effect size, the overall sample must 
</p>
<p>be very large. For a large effect size, a much smaller sample will suffice.
</p>
<p>K e y  T e r m s
</p>
<p>design sensitivity The statistical power  
</p>
<p>of a research study. In a sensitive study design, 
</p>
<p>statistical power will be maximized, and the  
</p>
<p>statistical test employed will be more capable  
</p>
<p>of identifying an effect.
</p>
<p>effect size (ES) A standardized measure 
</p>
<p>derived by taking the effect size (e.g., the  
</p>
<p> 
</p>
<p>measured in the raw units of the outcome  
</p>
<p>measure examined, and dividing it by the  
</p>
<p>pooled or common standard deviation of the  
</p>
<p>outcome measure.
</p>
<p>statistical power One minus the probability of 
</p>
<p>a Type II error. The greater the statistical power 
</p>
<p>of a test, the less chance there is that a researcher 
</p>
<p>will mistakenly fail to reject the null hypothesis.</p>
<p/>
</div>
<div class="page"><p/>
<p>S y m b o l s  a n d  F o r m u l a s
</p>
<p>D
</p>
<p>F
</p>
<p>N
</p>
<p>n
i
</p>
<p>i
</p>
<p>t-distribution
</p>
<p>F-distribution (used for ANOVA and OLS 
</p>
<p>ES
Parameter H
</p>
<p>=
- 0
</p>
<p>s
.
 
</p>
<p>ES
H H
</p>
<p>=
-( )- -( )m m m m
</p>
<p>s
1 2 0 1 0 2
</p>
<p>, which simplifies to ES =
-( )m m
s
</p>
<p>1 2 .
</p>
<p>To calculate the non-centrality parameter for the t-distribution for difference  
</p>
<p>d = d
N
</p>
<p>4
.
</p>
<p>d = d
NH
2
</p>
<p>, where  N
n n
</p>
<p>n n
H = +
</p>
<p>2 1 2
</p>
<p>1 2
</p>
<p> 
</p>
<p>f m=
s
s
</p>
<p>, where sm
i
</p>
<p>k
im m
</p>
<p>k
=
</p>
<p>-( )
=
&aring;
</p>
<p>1
</p>
<p>2
</p>
<p>, k is the number of groups, m is the 
</p>
<p>grand mean, and m
i
 represents each of the group means with n
</p>
<p>1
 = n
</p>
<p>2
 = ⋯ = n
</p>
<p>k
.
</p>
<p>To calculate the non-centrality parameter  for the F
</p>
<p>l = f N2 .
</p>
<p>To calculate the non-centrality parameter for the t-distribution for correlation 
</p>
<p>d =
-
</p>
<p>&acute;
r
</p>
<p>r
N
</p>
<p>2
</p>
<p>21
.
</p>
<p> S Y M B O L S  A N D  F O R M U L A S  749</p>
<p/>
</div>
<div class="page"><p/>
<p>750 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>To calculate R2 for OLS regression using the standardized effect size f as defined 
</p>
<p>R f f2 2 21= +( ).
</p>
<p>C o m p u t e r  E x e r c i s e s
</p>
<p>In contrast to many of the other computer exercises in this text, the computation 
</p>
<p>of statistical power estimates is not easily performed in any of the large stand-alone 
</p>
<p>statistical packages. There are a variety of software packages available for comput-
</p>
<p>ing statistical power as well as a number of websites that host power calculators for 
</p>
<p>a wide range of statistical tests. All of the analyses presented in this chapter were 
</p>
<p>-
</p>
<p>load from the Institut fur Experimentelle Psychologie at Universitat Dusseldorf 
</p>
<p>is a specialized package devoted to statistical power estimation and offers a wide 
</p>
<p>creation of powerful graphs that will plot power estimates across a range of sample 
</p>
<p>sizes, effect sizes, and statistical significance levels. The figures presented in this 
</p>
<p>18.
</p>
<p>Power and Precision v. 2.0 is a commercially available software package 
</p>
<p>designed to compute power estimates for a wide range of statistical models in a 
</p>
<p>user-friendly environment.19 As a commercial software package, its range of capa-
</p>
<p>all of the output&mdash;text and graphs&mdash;can be easily exported to other programs.
</p>
<p>In the case that one simply wants to compute a small number of power esti-
</p>
<p>mates without bothering to learn a new software package, a reasonably compre-
</p>
<p>org/#Power. The list of websites hosting power calculators is categorized by the 
</p>
<p>type of statistical test that the user is searching for&mdash;one-sample t-test, two-sample 
</p>
<p>t-test, correlation, regression, and so on.
</p>
<p>On a technical note, it is worth highlighting that there will be slight differences 
</p>
<p>across statistical software packages and power calculators in the estimated sample 
</p>
<p>sizes needed to achieve a given level of statistical power. The primary reason for 
</p>
<p>this appears to be focused on rounding the estimated sample size to an integer, 
</p>
<p>since we cannot sample a fraction of a case in any research study. Some packages 
</p>
<p>round up so that the estimated statistical power as always at least as great as the 
</p>
<p>target entered into the computation. Other packages and calculators will round 
</p>
<p>estimate of statistical power may be slightly less than the initial target.
</p>
<p>18
</p>
<p>Analysis Program for the Social, Behavioral, and Biomedical Sciences,&rdquo; Behavior Research 
</p>
<p>Methods
19 M. Borenstein, H. Rothstein, and J. Cohen Power and Precision</p>
<p/>
</div>
<div class="page"><p/>
<p>Stata
</p>
<p>Two-Sample Difference of Means Test
</p>
<p>In Stata, one- and two-sample difference of means tests are performed with the 
</p>
<p>sampsi
</p>
<p>sampsi Mean1 Mean2 , sd1(#) sd2(#) n1(#) n2(#) power(#) 
</p>
<p>onesided
</p>
<p>where Mean1 and Mean2 refer to the expected population means for the two 
</p>
<p>samples being compared, sd1(#) and sd2(#) refer to the expected standard 
</p>
<p>n1(#) and 
</p>
<p>n2(#)
</p>
<p>in each sample, power(#) is a designated level of power for sample size  
</p>
<p>onesided indicates that a 
</p>
<p>we are trying to estimate power and assume constant standard deviations and 
</p>
<p>sample sizes across the two samples, this can be simplified to
</p>
<p>sampsi Mean1 Mean2, sd(#) n(#)
</p>
<p>Upon entering the command, the output will list all of the assumptions (alpha 
</p>
<p>cases each, a difference of population means of 0.2, a common standard devia-
</p>
<p>sampsi command, we would 
</p>
<p>sampsi 0 0.2 , sd(1) n(35)
</p>
<p>The use of 0 and 0.2 for the two sample means is a convenient way to represent 
</p>
<p>the difference. It would make no difference what two numbers we inserted here 
</p>
<p> C O M P U T E R  E X E R C I S E S  751</p>
<p/>
</div>
<div class="page"><p/>
<p>752 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>respectively.
</p>
<p>If our interest is in estimating the sample size required to achieve a given 
</p>
<p>level of statistical power, we would alter the sampsi command by omitting the 
</p>
<p>sample size values (n(#)
</p>
<p>difference of means power analysis, we assumed a small standardized effect (i.e., 
</p>
<p>d
</p>
<p>sampsi 0 .2 , sd(1) power(.8) onesided
</p>
<p>sample size estimates for the medium and large effects, simply increase the value 
</p>
<p>of the second mean from 0.2 to 0.5 and 0.8.
</p>
<p>ANOVA
</p>
<p>Unfortunately, there is no built-in command in Stata to compute power in a 
</p>
<p>simple one-way ANOVA. Although there are several user-written commands 
</p>
<p>that can be installed and used, it is often difficult to obtain the kind of  infor-
</p>
<p>mation we may be most interested in gleaning from a power analysis. Based on 
</p>
<p>existing procedures in Stata, as well as other statistical packages, we have written 
</p>
<p>anova_pwr, which is a modest Stata procedure that you can use to replicate 
</p>
<p>the results in this chapter and perform other simple one-way ANOVA estimates </p>
<p/>
</div>
<div class="page"><p/>
<p>of  power and sample size. To install this command on your copy of  Stata, type 
</p>
<p>net install anova_pwr,
</p>
<p>from(http://myfiles.neu.edu/c.britt/stata/ado/power)
</p>
<p>The basic components to the anova_pwr
</p>
<p>anova_pwr, ngp(#) f(#) min(#) max(#)
</p>
<p>where ngp(#) represents the number of  groups to be compared, f (#)  
</p>
<p>represents the standardized effect size (the default is f min(#) is the  
</p>
<p>max(#) is the  
</p>
<p>byvalue(#)  
</p>
<p>represents a way to control how much output is generated by the command 
</p>
<p>For our example above, we computed the power of a one-way ANOVA 
</p>
<p>design with three groups (k
</p>
<p>effect sizes f
</p>
<p>The anova_pwr command to compute the power estimate for the small 
</p>
<p>anova_pwr, ngp(3) f(.1) min(100) max(100)
</p>
<p>Since we specified the minimum and maximum group sizes to be the same (100 
</p>
<p>If you rerun this command, but change the value for f to reflect the  
</p>
<p>medium and strong effect sizes, the power estimates reported above will also  
</p>
<p>be reproduced.
</p>
<p>It is not possible to estimate directly the sample size required for a designated 
</p>
<p>level of statistical power in an ANOVA using Stata. The anova_pwr command 
</p>
<p>can be used to represent a range of group sizes through the use of the min(#) 
</p>
<p>and max(#) options that will estimate the power associated with a given effect 
</p>
<p>size. If our goal is to achieve a power of 0.80, then we might start by estimating 
</p>
<p>the power for a wide range of sample sizes and then narrowing down the range 
</p>
<p>on a second run. For example, if we are interested in determining the sample size 
</p>
<p>required to detect a small effect (f
</p>
<p>enter the following command using 100 cases as the minimum, since we already 
</p>
<p>anova_pwr, ngp(3) f(.1) min(100) max(500) byvalue(10)
</p>
<p>  753C O M P U T E R  E X E R C I S E S  </p>
<p/>
</div>
<div class="page"><p/>
<p>754 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>The output from this run is</p>
<p/>
</div>
<div class="page"><p/>
<p>As we move through the values in the output, we see that a power level of 0.8 
</p>
<p>would be required to achieve a minimum power of 0.80. The table of results also 
</p>
<p>illustrates how sample size estimates may vary across programs to compute sta-
</p>
<p>0.80, which some programs would round to 0.80. At the same time, it is  
</p>
<p>that threshold.
</p>
<p>There is one user-written procedure that we are aware of for computing power 
</p>
<p>estimates of correlation coefficients in Stata. The command is sampsi_rho, 
</p>
<p>which bases power calculations on converting the correlation coefficient with  
</p>
<p>the Fisher z formula and then using the normal distribution (instead of a  
</p>
<p>t
</p>
<p>ssc install sampsi_rho
</p>
<p>The basic structure of the sampsi_rho command is
</p>
<p>sampsi_rho , null(#) alt(#) n(#) power(#) solve() alpha(#) 
</p>
<p>onesided
</p>
<p> 755C O M P U T E R  E X E R C I S E S  
</p>
<p>Correlation</p>
<p/>
</div>
<div class="page"><p/>
<p>756 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p>where null(#) specifies the value of the correlation for the null hypothesis 
</p>
<p>alt(#) specifies the alternative hypothesis value of the correlation 
</p>
<p>n(#) power(#) indi-
</p>
<p>solve() notes whether to solve 
</p>
<p>alpha(#) specifies the alpha 
</p>
<p>To replicate the values above in our analysis of power estimates for correla-
</p>
<p>tion coefficients for a sample size of 100, we would enter the following com-
</p>
<p>sampsi_rho, solve(power) n(100) alt(0.1) onesided
</p>
<p>The estimated power is 0.260, very nearly the same as the estimate produced 
</p>
<p>using the correlation coefficient and the t-distribution. If you were interested in 
</p>
<p>reproducing the power estimates for the medium and strong effects, you would 
</p>
<p>just need to change the value of alt(#) to alt(0.3) and alt(0.5), respectively.
</p>
<p>In a similar way, we can estimate the sample size needed to achieve a desig-
</p>
<p>nated level of statistical power for a hypothesized effect size by making just a 
</p>
<p>few changes to the sampsi_rho command. For example, if we wanted to esti-
</p>
<p>mate the sample size needed to detect a medium correlation (i.e., r
</p>
<p>power level of 0.80, we would omit the sample size and solve options but insert 
</p>
<p>power(0.8)
</p>
<p>sampsi_rho, alt(0.3) power(0.8) onesided
</p>
<p>-
</p>
<p>tion of a case, we would typically round up to 68 in this case. The rationale, as 
</p>
<p>we noted above, in rounding up is to ensure that a power level of no less than 
</p>
<p>entirely due to the use of the Fisher-transformed value of the correlation and use 
</p>
<p>of the normal distribution and is to be expected.
</p>
<p>OLS Regression
</p>
<p>Similar to computing power with ANOVA in Stata, it is necessary to rely on the 
</p>
<p>user-written command powerreg
</p>
<p>net install powerreg, from(http://www.ats.ucla.edu/stat/
</p>
<p>stata/ado/analysis)
</p>
<p>The basic structure to the powerreg command is
</p>
<p>powerreg, r2f(value) r2r(value) nvar(#) ntest(#) alpha(value)
</p>
<p>where r2f(value) is the hypothesized value of R2 expected, r2r(value) is the 
</p>
<p>R2 nvar(#) refers to the total number of inde-
</p>
<p>pendent variables included in the regression model, and ntest(#) refers to the 
</p>
<p>number of independent variables being tested. Alpha is assumed to be 0.05, and 
</p>
<p>nvar and ntest are both set at a default of 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>To reproduce the results reported above for power in OLS regression for a 
</p>
<p>weak effect (i.e., R2
</p>
<p>powerreg, r2f(.02) r2r(0) n(100) nvar(8) ntest(8)
</p>
<p>Note that the value for r2r is entered as 0&mdash;this is the expected value of R2 
</p>
<p>without any of the independent variables included in the analysis. The power 
</p>
<p>above. Results for the moderate (R2 R2
</p>
<p>are obtained by simply altering the value of r2f in the powerreg command. 
</p>
<p>Note that the power estimates reported by Stata vary slightly from those reported 
</p>
<p>To compute the sample size needed to achieve a designated level of statistical 
</p>
<p>power, we would omit the n(#) option but insert an option for power(#)
</p>
<p>powerreg, r2f(.02) r2r(0) power(0.8) nvar(8) ntest(8)
</p>
<p>We find that the estimated sample size needed to detect a weak effect (R2
</p>
<p>the calculation of the standardized effect (f2
</p>
<p>effect size20&mdash;the values for the medium and large effect sizes are nearly identical 
</p>
<p>and differ by only 1 case.21
</p>
<p>Problems
</p>
<p> 1. Compute the estimates of  statistical power for each of  the four  
</p>
<p>scenarios in Exercise 21.1. Which scenario has the highest level of   
</p>
<p>statistical power? Explain why.
</p>
<p> 2. Compute the estimates of  statistical power for each of  the four scenarios 
</p>
<p>Explain why.
</p>
<p>in Exercise 21.6. Was this study designed to have a high level of  statistical 
</p>
<p>power to identify small and medium effects? Explain why.
</p>
<p> 4. Compute the estimates of  statistical power for each of  the following  
</p>
<p>one-way ANOVA studies. (For all scenarios, assume that the researcher is 
</p>
<p>&#149; 
</p>
<p>&#149; 
</p>
<p>&#149; 
</p>
<p>Which scenario would have the highest level of statistical power? 
</p>
<p>Explain why.
</p>
<p>20 If  the value of  r2r(#) in the command is changed to r2r(0.0196), the resulting estimate 
</p>
<p>21 The reason for this difference is that the powerreg command computes sample size esti-
</p>
<p>sample size that ensures that the power level is at least 0.80 and so the estimates reported above 
</p>
<p>are greater by 1.
</p>
<p> 757C O M P U T E R  E X E R C I S E S  </p>
<p/>
</div>
<div class="page"><p/>
<p>758 C H A P T E R  T W E N T Y  T H R E E :  S P E C I A L  T O P I C S
</p>
<p> 5. In attempting to design a correlation study looking at academic  
</p>
<p>performance and delinquency, a researcher expects a small-to-moderate 
</p>
<p>correlation among a population of  adolescents he or she will sample from.
</p>
<p> a. If  he or she computes estimates of  statistical power assuming a  
</p>
<p>two-tail test, what size sample would he or she need to detect a small 
</p>
<p>correlation? Medium correlation?
</p>
<p> b. Do you think he or she could justify a one-tail test of  the correlation? 
</p>
<p>If  a one-tail test was used, how does the estimated sample size change 
</p>
<p>for both the small and medium correlations?
</p>
<p> 6. A research team is preparing to launch a statewide survey to gauge public 
</p>
<p>sentiment about the incarceration of  juvenile offenders, focusing primarily 
</p>
<p>on support for more lenient punishments. Consistent with much public 
</p>
<p>opinion research, expectations are that a combination of  ten independent
</p>
<p>variables is likely to explain about 15% of  the variation in views about 
</p>
<p>juvenile punishment.
</p>
<p> a. What size sample would the researchers need to have to achieve a 
</p>
<p>power of  0.80? 0.90?
</p>
<p> b. Of  particular interest to the researchers is the effect of  three different 
</p>
<p>measures of  experience with the justice system, but their expectation is 
</p>
<p>that the overall effect of  these three measures will be small. What size 
</p>
<p>sample would the researchers need to achieve a power of  0.80? 0.90?
</p>
<p> c. What size sample should the researchers try to obtain? Explain why.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 1
Factorials
</p>
<p>0! � 1
</p>
<p>1! � 1
</p>
<p>2! � 2
</p>
<p>3! � 6
</p>
<p>4! � 24
</p>
<p>5! � 120
</p>
<p>6! � 720
</p>
<p>7! � 5,040
</p>
<p>8! � 40,320
</p>
<p>9! � 362,880
</p>
<p>10! � 3,628,800
</p>
<p>11! � 39,916,800
</p>
<p>12! � 479,001,600
</p>
<p>13! � 6,227,020,800
</p>
<p>14! � 87,178,291,200
</p>
<p>15! � 1,307,674,368,000
</p>
<p>16! � 20,922,789,888,000
</p>
<p>17! � 355,687,428,096,000
</p>
<p>18! � 6,402,373,705,728,000
</p>
<p>19! � 121,645,100,408,832,000
</p>
<p>20! � 2,432,902,008,176,640,000
</p>
<p>21! � 51,090,942,171,709,440,000
</p>
<p>22! � 1,124,000,727,777,607,680,000
</p>
<p>23! � 25,852,016,738,884,976,640,000
</p>
<p>24! � 620,448,401,733,239,439,360,000
</p>
<p>25! � 15,511,210,043,330,985,984,000,000
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>759</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 2
Critical Values of �2 Distribution
</p>
<p>�
</p>
<p>df 0.20 0.10 0.05 0.02 0.01 0.001
</p>
<p>1 1.642 2.706 3.841 5.412 6.635 10.827
2 3.219 4.605 5.991 7.824 9.210 13.815
3 4.642 6.251 7.815 9.837 11.341 16.268
4 5.989 7.779 9.488 11.668 13.277 18.465
5 7.289 9.236 11.070 13.388 15.086 20.517
6 8.558 10.645 12.592 15.033 16.812 22.457
7 9.803 12.017 14.067 16.622 18.475 24.322
8 11.030 13.362 15.507 18.168 20.090 26.125
9 12.242 14.684 16.919 19.679 21.666 27.877
</p>
<p>10 13.442 15.987 18.307 21.161 23.209 29.588
11 14.631 17.275 19.675 22.618 24.725 31.264
12 15.812 18.549 21.026 24.054 26.217 32.909
13 16.985 19.812 22.362 25.472 27.688 34.528
14 18.151 21.064 23.685 26.873 29.141 36.123
15 19.311 22.307 24.996 28.259 30.578 37.697
16 20.465 23.542 26.296 29.633 32.000 39.252
17 21.615 24.769 27.587 30.995 33.409 40.790
18 22.760 25.989 28.869 32.346 34.805 42.312
19 23.900 27.204 30.144 33.687 36.191 43.820
20 25.038 28.412 31.410 35.020 37.566 45.315
21 26.171 29.615 32.671 36.343 38.932 46.797
22 27.301 30.813 33.924 37.659 40.289 48.268
23 28.429 32.007 35.172 38.968 41.638 49.728
24 29.553 33.196 36.415 40.270 42.980 51.179
25 30.675 34.382 37.652 41.566 44.314 52.620
26 31.795 35.563 38.885 42.856 45.642 54.052
27 32.912 36.741 40.113 44.140 46.963 55.476
28 34.027 37.916 41.337 45.419 48.278 56.893
29 35.139 39.087 42.557 46.693 49.588 58.302
30 36.250 40.256 43.773 47.962 50.892 59.703
</p>
<p>Source: From Table IV of R. A. Fisher and F. Yates, Statistical Tables for Biological, Agricultural and Medical Research
(London: Longman Group Ltd., 1974). (Previously published by Oliver &amp; Boyd, Edinburgh.) Reprinted by permission of
Pearson Education Ltd.
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>760</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 3
The entries in this table are the proportion of the cases in a standard
</p>
<p>normal distribution that lie between 0 and z.
</p>
<p>SECOND DECIMAL PLACE IN z
</p>
<p>z 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
</p>
<p>0.0 0.0000 0.0040 0.0080 0.0120 0.0160 0.0199 0.0239 0.0279 0.0319 0.0359
0.1 0.0398 0.0438 0.0478 0.0517 0.0557 0.0596 0.0636 0.0675 0.0714 0.0753
0.2 0.0793 0.0832 0.0871 0.0910 0.0948 0.0987 0.1026 0.1064 0.1103 0.1141
0.3 0.1179 0.1217 0.1255 0.1293 0.1331 0.1368 0.1406 0.1443 0.1480 0.1517
0.4 0.1554 0.1591 0.1628 0.1664 0.1700 0.1736 0.1772 0.1808 0.1844 0.1879
</p>
<p>0.5 0.1915 0.1950 0.1985 0.2019 0.2054 0.2088 0.2123 0.2157 0.2190 0.2224
0.6 0.2257 0.2291 0.2324 0.2357 0.2389 0.2422 0.2454 0.2486 0.2517 0.2549
0.7 0.2580 0.2611 0.2642 0.2673 0.2704 0.2734 0.2764 0.2794 0.2823 0.2852
0.8 0.2881 0.2910 0.2939 0.2967 0.2995 0.3023 0.3051 0.3078 0.3106 0.3133
0.9 0.3159 0.3186 0.3212 0.3238 0.3264 0.3289 0.3315 0.3340 0.3365 0.3389
</p>
<p>1.0 0.3413 0.3438 0.3461 0.3485 0.3508 0.3531 0.3554 0.3577 0.3599 0.3621
1.1 0.3643 0.3665 0.3686 0.3708 0.3729 0.3749 0.3770 0.3790 0.3810 0.3830
1.2 0.3849 0.3869 0.3888 0.3907 0.3925 0.3944 0.3962 0.3980 0.3997 0.4015
1.3 0.4032 0.4049 0.4066 0.4082 0.4099 0.4115 0.4131 0.4147 0.4162 0.4177
1.4 0.4192 0.4207 0.4222 0.4236 0.4251 0.4265 0.4279 0.4292 0.4306 0.4319
</p>
<p>1.5 0.4332 0.4345 0.4357 0.4370 0.4382 0.4394 0.4406 0.4418 0.4429 0.4441
1.6 0.4452 0.4463 0.4474 0.4484 0.4495 0.4505 0.4515 0.4525 0.4535 0.4545
1.7 0.4554 0.4564 0.4573 0.4582 0.4591 0.4599 0.4608 0.4616 0.4625 0.4633
1.8 0.4641 0.4649 0.4656 0.4664 0.4671 0.4678 0.4686 0.4693 0.4699 0.4706
1.9 0.4713 0.4719 0.4726 0.4732 0.4738 0.4744 0.4750 0.4756 0.4761 0.4767
</p>
<p>2.0 0.4772 0.4778 0.4783 0.4788 0.4793 0.4798 0.4803 0.4808 0.4812 0.4817
2.1 0.4821 0.4826 0.4830 0.4834 0.4838 0.4842 0.4846 0.4850 0.4854 0.4857
2.2 0.4861 0.4864 0.4868 0.4871 0.4875 0.4878 0.4881 0.4884 0.4887 0.4890
2.3 0.4893 0.4896 0.4898 0.4901 0.4904 0.4906 0.4909 0.4911 0.4913 0.4916
2.4 0.4918 0.4920 0.4922 0.4925 0.4927 0.4929 0.4931 0.4932 0.4934 0.4936
</p>
<p>2.5 0.4938 0.4940 0.4941 0.4943 0.4945 0.4946 0.4948 0.4949 0.4951 0.4952
2.6 0.4953 0.4955 0.4956 0.4957 0.4959 0.4960 0.4961 0.4962 0.4963 0.4964
2.7 0.4965 0.4966 0.4967 0.4968 0.4969 0.4970 0.4971 0.4972 0.4973 0.4974
2.8 0.4974 0.4975 0.4976 0.4977 0.4977 0.4978 0.4979 0.4979 0.4980 0.4981
2.9 0.4981 0.4982 0.4982 0.4983 0.4984 0.4984 0.4985 0.4985 0.4986 0.4986
</p>
<p>3.0 0.4987 0.4987 0.4987 0.4988 0.4988 0.4989 0.4989 0.4989 0.4990 0.4990
3.1 0.4990 0.4991 0.4991 0.4991 0.4992 0.4992 0.4992 0.4992 0.4993 0.4993
3.2 0.4993 0.4993 0.4994 0.4994 0.4994 0.4994 0.4994 0.4995 0.4995 0.4995
3.3 0.4995 0.4995 0.4995 0.4996 0.4996 0.4996 0.4996 0.4996 0.4996 0.4997
3.4 0.4997 0.4997 0.4997 0.4997 0.4997 0.4997 0.4997 0.4997 0.4997 0.4998
</p>
<p>3.5 0.4998 0.4998 0.4998 0.4998 0.4998 0.4998 0.4998 0.4998 0.4998 0.4998
3.6 0.4998 0.4998 0.4999 0.4999 0.4999 0.4999 0.4999 0.4999 0.4999 0.4999
3.7 0.4999
4.0 0.49997
4.5 0.499997
5.0 0.4999997
</p>
<p>Source: R. Johnson, Elementary Statistics (Belmont, CA: Duxbury Press, 1996).
</p>
<p>Areas of the Standard 
Normal Distribution
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>761</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 4
ONE-TAILED VALUE
</p>
<p>Degrees 0.25 0.10 0.05 0.025 0.01 0.005
</p>
<p>of TWO-TAILED VALUE
</p>
<p>Freedom 0.50 0.20 0.10 0.05 0.02 0.01
</p>
<p>1 1.000 3.078 6.314 12.706 31.821 63.657
2 0.816 1.886 2.920 4.303 6.965 9.925
3 0.765 1.638 2.353 3.182 4.541 5.841
4 0.741 1.533 2.132 2.776 3.747 4.604
5 0.727 1.476 2.015 2.571 3.365 4.032
6 0.718 1.440 1.943 2.447 3.143 3.707
7 0.711 1.415 1.895 2.365 2.998 3.499
8 0.706 1.397 1.860 2.306 2.896 3.355
9 0.703 1.383 1.833 2.262 2.821 3.250
</p>
<p>10 0.700 1.372 1.812 2.228 2.764 3.169
11 0.697 1.363 1.796 2.201 2.718 3.106
12 0.695 1.356 1.782 2.179 2.681 3.055
13 0.694 1.350 1.771 2.160 2.650 3.012
14 0.692 1.345 1.761 2.145 2.626 2.977
15 0.691 1.341 1.753 2.131 2.602 2.947
16 0.690 1.337 1.746 2.120 2.583 2.921
17 0.689 1.333 1.740 2.110 2.567 2.898
18 0.688 1.330 1.734 2.101 2.552 2.878
19 0.688 1.328 1.729 2.093 2.539 2.861
20 0.687 1.325 1.725 2.086 2.528 2.845
21 0.686 1.323 1.721 2.080 2.518 2.831
22 0.686 1.321 1.717 2.074 2.508 2.819
23 0.685 1.319 1.714 2.069 2.500 2.807
24 0.685 1.318 1.711 2.064 2.492 2.797
25 0.684 1.316 1.708 2.060 2.485 2.787
26 0.684 1.315 1.706 2.056 2.479 2.779
27 0.684 1.314 1.703 2.052 2.473 2.771
28 0.683 1.313 1.701 2.048 2.467 2.763
29 0.683 1.311 1.699 2.045 2.462 2.756
30 0.683 1.310 1.697 2.042 2.457 2.750
31 0.682 1.309 1.696 2.040 2.453 2.744
32 0.682 1.309 1.694 2.037 2.449 2.739
33 0.682 1.308 1.692 2.035 2.445 2.733
34 0.682 1.307 1.691 2.032 2.441 2.728
35 0.682 1.306 1.690 2.030 2.438 2.724
40 0.681 1.303 1.684 2.021 2.423 2.704
45 0.680 1.301 1.680 2.014 2.412 2.690
50 0.680 1.299 1.676 2.008 2.403 2.678
55 0.679 1.297 1.673 2.004 2.396 2.669
60 0.679 1.296 1.671 2.000 2.390 2.660
70 0.678 1.294 1.667 1.994 2.381 2.648
80 0.678 1.293 1.665 1.989 2.374 2.638
90 0.678 1.291 1.662 1.986 2.368 2.631
</p>
<p>100 0.677 1.290 1.661 1.982 2.364 2.625
120 0.677 1.289 1.658 1.980 2.358 2.617
</p>
<p>�500 0.674 1.282 1.645 1.960 2.326 2.576
</p>
<p>Source: &ldquo;Table D, The t Table&rdquo; adapted from SCIENTIFIC TABLES, published by Ciba-Geigy, in WAYS AND MEANS OF
STATISTICS by Leonard Tashman and Kathleen Lamborn, Copyright � 1979 by Harcourt Brace &amp; Company, reprinted
by permission of Harcourt Brace &amp; Company.
</p>
<p>Critical Values 
of Student&rsquo;s t Distribution
</p>
<p>One-tailed value
</p>
<p>Two-tailed value
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>762</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 5
Critical Values of the F-Statistic
</p>
<p>(� � 0.05)
</p>
<p>NUMERATOR DEGREES OF FREEDOM
</p>
<p>df1
df2 1 2 3 4 5 6 8 12 24 �
</p>
<p>1 161.4 199.5 215.7 224.6 230.2 234.0 238.9 243.9 249.0 254.3
</p>
<p>2 18.51 19.00 19.16 19.25 19.30 19.33 19.37 19.41 19.45 19.50
</p>
<p>3 10.13 9.55 9.28 9.12 9.01 8.94 8.84 8.74 8.64 8.53
</p>
<p>4 7.71 6.94 6.59 6.39 6.26 6.16 6.04 5.91 5.77 5.63
</p>
<p>5 6.61 5.79 5.41 5.19 5.05 4.95 4.82 4.68 4.53 4.36
</p>
<p>6 5.99 5.14 4.76 4.53 4.39 4.28 4.15 4.00 3.84 3.67
</p>
<p>7 5.59 4.74 4.35 4.12 3.97 3.87 3.73 3.57 3.41 3.23
</p>
<p>8 5.32 4.46 4.07 3.84 3.69 3.58 3.44 3.28 3.12 2.93
</p>
<p>9 5.12 4.26 3.86 3.63 3.48 3.37 3.23 3.07 2.90 2.71
</p>
<p>10 4.96 4.10 3.71 3.48 3.33 3.22 3.07 2.91 2.74 2.54
</p>
<p>11 4.84 3.98 3.59 3.36 3.20 3.09 2.95 2.79 2.61 2.40
</p>
<p>12 4.75 3.88 3.49 3.26 3.11 3.00 2.85 2.69 2.50 2.30
</p>
<p>13 4.67 3.80 3.41 3.18 3.02 2.92 2.77 2.60 2.42 2.21
</p>
<p>14 4.60 3.74 3.34 3.11 2.96 2.85 2.70 2.53 2.35 2.13
</p>
<p>15 4.54 3.68 3.29 3.06 2.90 2.79 2.64 2.48 2.29 2.07
</p>
<p>16 4.49 3.63 3.24 3.01 2.85 2.74 2.59 2.42 2.24 2.01
</p>
<p>17 4.45 3.59 3.20 2.96 2.81 2.70 2.55 2.38 2.19 1.96
</p>
<p>18 4.41 3.55 3.16 2.93 2.77 2.66 2.51 2.34 2.15 1.92
</p>
<p>19 4.38 3.52 3.13 2.90 2.74 2.63 2.48 2.31 2.11 1.88
</p>
<p>20 4.35 3.49 3.10 2.87 2.71 2.60 2.45 2.28 2.08 1.84
</p>
<p>21 4.32 3.47 3.07 2.84 2.68 2.57 2.42 2.25 2.05 1.81
</p>
<p>22 4.30 3.44 3.05 2.82 2.66 2.55 2.40 2.23 2.03 1.78
</p>
<p>23 4.28 3.42 3.03 2.80 2.64 2.53 2.38 2.20 2.00 1.76
</p>
<p>24 4.26 3.40 3.01 2.78 2.62 2.51 2.36 2.18 1.98 1.73
</p>
<p>25 4.24 3.38 2.99 2.76 2.60 2.49 2.34 2.16 1.96 1.71
</p>
<p>26 4.22 3.37 2.98 2.74 2.59 2.47 2.32 2.15 1.95 1.69
</p>
<p>27 4.21 3.35 2.96 2.73 2.57 2.46 2.30 2.13 1.93 1.67
</p>
<p>28 4.20 3.34 2.95 2.71 2.56 2.44 2.29 2.12 1.91 1.65
</p>
<p>29 4.18 3.33 2.93 2.70 2.54 2.43 2.28 2.10 1.90 1.64
</p>
<p>30 4.17 3.32 2.92 2.69 2.53 2.42 2.27 2.09 1.89 1.62
</p>
<p>40 4.08 3.23 2.84 2.61 2.45 2.34 2.18 2.00 1.79 1.51
</p>
<p>60 4.00 3.15 2.76 2.52 2.37 2.25 2.10 1.92 1.70 1.39
</p>
<p>120 3.92 3.07 2.68 2.45 2.29 2.17 2.02 1.83 1.61 1.25
</p>
<p>�500 3.84 2.99 2.60 2.37 2.21 2.09 1.94 1.75 1.52 1.00
</p>
<p>D
</p>
<p>E
</p>
<p>N
</p>
<p>O
</p>
<p>M
</p>
<p>I
</p>
<p>N
</p>
<p>A
</p>
<p>T
</p>
<p>O
</p>
<p>R
</p>
<p>D
</p>
<p>E
</p>
<p>G
</p>
<p>R
</p>
<p>E
</p>
<p>E
</p>
<p>S
</p>
<p>O
</p>
<p>F
</p>
<p>F
</p>
<p>R
</p>
<p>E
</p>
<p>E
</p>
<p>D
</p>
<p>O
</p>
<p>M
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>763</p>
<p/>
</div>
<div class="page"><p/>
<p>(� � 0.01)
</p>
<p>NUMERATOR DEGREES OF FREEDOM
</p>
<p>df1
df2 1 2 3 4 5 6 8 12 24 �
</p>
<p>1 4052 4999 5403 5625 5764 5859 5981 6106 6234 6366
</p>
<p>2 98.49 99.01 99.17 99.25 99.30 99.33 99.36 99.42 99.46 99.50
</p>
<p>3 34.12 30.81 29.46 28.71 28.24 27.91 27.49 27.05 26.60 26.12
</p>
<p>4 21.20 18.00 16.69 15.98 15.52 15.21 14.80 14.37 13.93 13.46
</p>
<p>5 16.26 13.27 12.06 11.39 10.97 10.67 10.27 9.89 9.47 9.02
</p>
<p>6 13.74 10.92 9.78 9.15 8.75 8.47 8.10 7.72 7.31 6.88
</p>
<p>7 12.25 9.55 8.45 7.85 7.46 7.19 6.84 6.47 6.07 5.65
</p>
<p>8 11.26 8.65 7.59 7.01 6.63 6.37 6.03 5.67 5.28 4.86
</p>
<p>9 10.56 8.02 6.99 6.42 6.06 5.80 5.47 5.11 4.73 4.31
</p>
<p>10 10.04 7.56 6.55 5.99 5.64 5.39 5.06 4.71 4.33 3.91
</p>
<p>11 9.65 7.20 6.22 5.67 5.32 5.07 4.74 4.40 4.02 3.60
</p>
<p>12 9.33 6.93 5.95 5.41 5.06 4.82 4.50 4.16 3.78 3.36
</p>
<p>13 9.07 6.70 5.74 5.20 4.86 4.62 4.30 3.96 3.59 3.16
</p>
<p>14 8.86 6.51 5.56 5.03 4.69 4.46 4.14 3.80 3.43 3.00
</p>
<p>15 8.68 6.36 5.42 4.89 4.56 4.32 4.00 3.67 3.29 2.87
</p>
<p>16 8.53 6.23 5.29 4.77 4.44 4.20 3.89 3.55 3.18 2.75
</p>
<p>17 8.40 6.11 5.18 4.67 4.34 4.10 3.79 3.45 3.08 2.65
</p>
<p>18 8.28 6.01 5.09 4.58 4.25 4.01 3.71 3.37 3.00 2.57
</p>
<p>19 8.18 5.93 5.01 4.50 4.17 3.94 3.63 3.30 2.92 2.49
</p>
<p>20 8.10 5.85 4.94 4.43 4.10 3.87 3.56 3.23 2.86 2.42
</p>
<p>21 8.02 5.78 4.87 4.37 4.04 3.81 3.51 3.17 2.80 2.36
</p>
<p>22 7.94 5.72 4.82 4.31 3.99 3.76 3.45 3.12 2.75 2.31
</p>
<p>23 7.88 5.66 4.76 4.26 3.94 3.71 3.41 3.07 2.70 2.26
</p>
<p>24 7.82 5.61 4.72 4.22 3.90 3.67 3.36 3.03 2.66 2.21
</p>
<p>25 7.77 5.57 4.68 4.18 3.86 3.63 3.32 2.99 2.62 2.17
</p>
<p>26 7.72 5.53 4.64 4.14 3.82 3.59 3.29 2.96 2.58 2.13
</p>
<p>27 7.68 5.49 4.60 4.11 3.78 3.56 3.26 2.93 2.55 2.10
</p>
<p>28 7.64 5.45 4.57 4.07 3.75 3.53 3.23 2.90 2.52 2.06
</p>
<p>29 7.60 5.42 4.54 4.04 3.73 3.50 3.20 2.87 2.49 2.03
</p>
<p>30 7.56 5.39 4.51 4.02 3.70 3.47 3.17 2.84 2.47 2.01
</p>
<p>40 7.31 5.18 4.31 3.83 3.51 3.29 2.99 2.66 2.29 1.80
</p>
<p>60 7.08 4.98 4.13 3.65 3.34 3.12 2.82 2.50 2.12 1.60
</p>
<p>120 6.85 4.79 3.95 3.48 3.17 2.96 2.66 2.34 1.95 1.38
</p>
<p>�500 6.64 4.60 3.78 3.32 3.02 2.80 2.51 2.18 1.79 1.00
</p>
<p>D
</p>
<p>E
</p>
<p>N
</p>
<p>O
</p>
<p>M
</p>
<p>I
</p>
<p>N
</p>
<p>A
</p>
<p>T
</p>
<p>O
</p>
<p>R
</p>
<p>D
</p>
<p>E
</p>
<p>G
</p>
<p>R
</p>
<p>E
</p>
<p>E
</p>
<p>S
</p>
<p>O
</p>
<p>F
</p>
<p>F
</p>
<p>R
</p>
<p>E
</p>
<p>E
</p>
<p>D
</p>
<p>O
</p>
<p>M
</p>
<p>764 A P P E N D I C E S</p>
<p/>
</div>
<div class="page"><p/>
<p>(� � 0.001)
</p>
<p>NUMERATOR DEGREES OF FREEDOM
</p>
<p>df1
df2 1 2 3 4 5 6 8 12 24 �
</p>
<p>1 405284 500000 540379 562500 576405 585937 598144 610667 623497 636619
</p>
<p>2 998.5 999.0 999.2 999.2 999.3 999.3 999.4 999.4 999.5 999.5
</p>
<p>3 167.5 148.5 141.1 137.1 134.6 132.8 130.6 128.3 125.9 123.5
</p>
<p>4 74.14 61.25 56.18 53.44 51.71 50.53 49.00 47.41 45.77 44.05
</p>
<p>5 47.04 36.61 33.20 31.09 29.75 28.84 27.64 26.42 25.14 23.78
</p>
<p>6 35.51 27.00 23.70 21.90 20.81 20.03 19.03 17.99 16.89 15.75
</p>
<p>7 29.22 21.69 18.77 17.19 16.21 15.52 14.63 13.71 12.73 11.69
</p>
<p>8 25.42 18.49 15.83 14.39 13.49 12.86 12.04 11.19 10.30 9.34
</p>
<p>9 22.86 16.39 13.90 12.56 11.71 11.13 10.37 9.57 8.72 7.81
</p>
<p>10 21.04 14.91 12.55 11.28 10.48 9.92 9.20 8.45 7.64 6.76
</p>
<p>11 19.69 13.81 11.56 10.35 9.58 9.05 8.35 7.63 6.85 6.00
</p>
<p>12 18.64 12.97 10.80 9.63 8.89 8.38 7.71 7.00 6.25 5.42
</p>
<p>13 17.81 12.31 10.21 9.07 8.35 7.86 7.21 6.52 5.78 4.97
</p>
<p>14 17.14 11.78 9.73 8.62 7.92 7.43 6.80 6.13 5.41 4.60
</p>
<p>15 16.59 11.34 9.34 8.25 7.57 7.09 6.47 5.81 5.10 4.31
</p>
<p>16 16.12 10.97 9.00 7.94 7.27 6.81 6.19 5.55 4.85 4.06
</p>
<p>17 15.72 10.66 8.73 7.68 7.02 6.56 5.96 5.32 4.63 3.85
</p>
<p>18 15.38 10.39 8.49 7.46 6.81 6.35 5.76 5.13 4.45 3.67
</p>
<p>19 15.08 10.16 8.28 7.26 6.61 6.18 5.59 4.97 4.29 3.52
</p>
<p>20 14.82 9.95 8.10 7.10 6.46 6.02 5.44 4.82 4.15 3.38
</p>
<p>21 14.59 9.77 7.94 6.95 6.32 5.88 5.31 4.70 4.03 3.26
</p>
<p>22 14.38 9.61 7.80 6.81 6.19 5.76 5.19 4.58 3.92 3.15
</p>
<p>23 14.19 9.47 7.67 6.69 6.08 5.65 5.09 4.48 3.82 3.05
</p>
<p>24 14.03 9.34 7.55 6.59 5.98 5.55 4.99 4.39 3.74 2.97
</p>
<p>25 13.88 9.22 7.45 6.49 5.88 5.46 4.91 4.31 3.66 2.89
</p>
<p>26 13.74 9.12 7.36 6.41 5.80 5.38 4.83 4.24 3.59 2.82
</p>
<p>27 13.61 9.02 7.27 6.33 5.73 5.31 4.76 4.17 3.52 2.75
</p>
<p>28 13.50 8.93 7.19 6.25 5.66 5.24 4.69 4.11 3.46 2.70
</p>
<p>29 13.39 8.85 7.12 6.19 5.59 5.18 4.64 4.05 3.41 2.64
</p>
<p>30 13.29 8.77 7.05 6.12 5.53 5.12 4.58 4.00 3.36 2.59
</p>
<p>40 12.61 8.25 6.60 5.70 5.13 4.73 4.21 3.64 3.01 2.23
</p>
<p>60 11.97 7.76 6.17 5.31 4.76 4.37 3.87 3.31 2.69 1.90
</p>
<p>120 11.38 7.31 5.79 4.95 4.42 4.04 3.55 3.02 2.40 1.56
</p>
<p>�500 10.83 6.91 5.42 4.62 4.10 3.74 3.27 2.74 2.13 1.00
</p>
<p>Source: From Table IV of R. A. Fisher and F. Yates, Statistical Tables for Biological, Agricultural and Medical Research (London: Longman Group Ltd.,
1974). (Previously published by Oliver &amp; Boyd, Edinburgh.) Reprinted by permission of Pearson Education Ltd.
</p>
<p>D
</p>
<p>E
</p>
<p>N
</p>
<p>O
</p>
<p>M
</p>
<p>I
</p>
<p>N
</p>
<p>A
</p>
<p>T
</p>
<p>O
</p>
<p>R
</p>
<p>D
</p>
<p>E
</p>
<p>G
</p>
<p>R
</p>
<p>E
</p>
<p>E
</p>
<p>S
</p>
<p>O
</p>
<p>F
</p>
<p>F
</p>
<p>R
</p>
<p>E
</p>
<p>E
</p>
<p>D
</p>
<p>O
</p>
<p>M
</p>
<p>A P P E N D I C E S 765</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 6
Critical Value for P (Pcrit), Tukey&rsquo;s HSD Test
</p>
<p>LEVEL OF SIGNIFICANCE (� � 0.05)
</p>
<p>k � THE NUMBER OF MEANS OR
</p>
<p>NUMBER OF STEPS BETWEEN ORDERED MEANS
</p>
<p>dfw 2 3 4 5 6 7 8 9 10 12 15 20
</p>
<p>1 17.97 26.98 32.82 37.08 40.41 43.12 45.40 47.36 49.07 51.96 55.36 59.56
</p>
<p>2 6.08 8.33 9.80 10.88 11.74 12.44 13.03 13.54 13.99 14.75 15.65 16.77
</p>
<p>3 4.50 5.91 6.82 7.50 8.04 8.48 8.85 9.18 9.46 9.95 10.52 11.24
</p>
<p>4 3.93 5.04 5.76 6.29 6.71 7.05 7.35 7.60 7.83 8.21 8.66 9.23
</p>
<p>5 3.64 4.60 5.22 5.67 6.03 6.33 6.58 6.80 6.99 7.32 7.72 8.21
</p>
<p>6 3.46 4.34 4.90 5.30 5.63 5.90 6.12 6.32 6.49 6.79 7.14 7.59
</p>
<p>7 3.34 4.16 4.68 5.06 5.36 5.61 5.82 6.00 6.16 6.43 6.76 7.17
</p>
<p>8 3.26 4.04 4.53 4.89 5.17 5.40 5.60 5.77 5.92 6.18 6.48 6.87
</p>
<p>9 3.20 3.95 4.41 4.76 5.02 5.24 5.43 5.59 5.74 5.98 6.28 6.64
</p>
<p>10 3.15 3.88 4.33 4.65 4.91 5.12 5.30 5.46 5.60 5.83 6.11 6.47
</p>
<p>11 3.11 3.82 4.26 4.57 4.82 5.03 5.20 5.35 5.49 5.71 5.98 6.33
</p>
<p>12 3.08 3.77 4.20 4.51 4.75 4.95 5.12 5.27 5.39 5.61 5.88 6.21
</p>
<p>13 3.06 3.73 4.15 4.45 4.69 4.88 5.05 5.19 5.32 5.53 5.79 6.11
</p>
<p>14 3.03 3.70 4.11 4.41 4.64 4.83 4.99 5.13 5.25 5.46 5.71 6.03
</p>
<p>15 3.01 3.67 4.08 4.37 4.59 4.78 4.94 5.08 5.20 5.40 5.65 5.96
</p>
<p>16 3.00 3.65 4.05 4.33 4.56 4.74 4.90 5.03 5.15 5.35 5.59 5.90
</p>
<p>17 2.98 3.63 4.02 4.30 4.52 4.70 4.86 4.99 5.11 5.31 5.54 5.84
</p>
<p>18 2.97 3.61 4.00 4.28 4.49 4.67 4.82 4.96 5.07 5.27 5.50 5.79
</p>
<p>19 2.96 3.59 3.98 4.25 4.47 4.65 4.79 4.92 5.04 5.23 5.46 5.75
</p>
<p>20 2.95 3.58 3.96 4.23 4.45 4.62 4.77 4.90 5.01 5.20 5.43 5.71
</p>
<p>24 2.92 3.53 3.90 4.17 4.37 4.54 4.68 4.81 4.92 5.10 5.32 5.59
</p>
<p>30 2.89 3.49 3.85 4.10 4.30 4.46 4.60 4.72 4.82 5.00 5.21 5.47
</p>
<p>40 2.86 3.44 3.79 4.04 4.23 4.39 4.52 4.63 4.73 4.90 5.11 5.36
</p>
<p>60 2.83 3.40 3.74 3.98 4.16 4.31 4.44 4.55 4.65 4.81 5.00 5.24
</p>
<p>120 2.80 3.36 3.68 3.92 4.10 4.24 4.36 4.47 4.56 4.71 4.90 5.13
</p>
<p>� 2.77 3.31 3.63 3.86 4.03 4.17 4.29 4.39 4.47 4.62 4.80 5.01
</p>
<p>Source: From Comprehending Behavioral Statistics, by R. T. Hurlburt, Copyright � 1994, Brooks/Cole Publishing Company, Pacific Grove, CA 93950, a
division of International Thomson Publishing Inc. By permission of the publisher. Adapted from Biometrika Tables for Statisticians, vol. 1, 3rd ed., E. S.
Pearson and H. O. Hartley (eds.). Copyright � 1966, Cambridge University Press for Biometrika Trust. By permission of the Biometrika Trust.
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>766</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 7
Critical Values for Spearman&rsquo;s 
Rank-Order Correlation Coefficient
</p>
<p>LEVEL OF SIGNIFICANCE (�) FOR ONE-TAILED TEST
</p>
<p>0.05 0.025 0.01 0.005
</p>
<p>LEVEL OF SIGNIFICANCE (�) FOR TWO-TAILED TEST
</p>
<p>n 0.10 0.05 0.02 0.01
</p>
<p>5 0.900 &mdash; &mdash; &mdash;
6 0.829 0.886 0.943 &mdash;
7 0.714 0.786 0.893 0.929
8 0.643 0.738 0.833 0.881
9 0.600 0.700 0.783 0.833
</p>
<p>10 0.564 0.648 0.745 0.794
11 0.536 0.618 0.709 0.818
12 0.497 0.591 0.703 0.780
13 0.475 0.566 0.673 0.745
14 0.457 0.545 0.646 0.716
15 0.441 0.525 0.623 0.689
16 0.425 0.507 0.601 0.666
17 0.412 0.490 0.582 0.645
18 0.399 0.476 0.564 0.625
19 0.388 0.462 0.549 0.608
20 0.377 0.450 0.534 0.591
21 0.368 0.438 0.521 0.576
22 0.359 0.428 0.508 0.562
23 0.351 0.418 0.496 0.549
24 0.343 0.409 0.485 0.537
25 0.336 0.400 0.475 0.526
26 0.329 0.392 0.465 0.515
27 0.323 0.385 0.456 0.505
28 0.317 0.377 0.448 0.496
29 0.311 0.370 0.440 0.487
30 0.305 0.364 0.432 0.478
</p>
<p>Source: From Comprehending Behavioral Statistics, by R. T. Hurlburt, Copyright � 1994, Brooks/Cole Publishing
Company, Pacific Grove, CA 93950, a division of International Thomson Publishing Inc. By permission of the publisher.
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>767</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 8 Fisher r-to-Z* Transformation
The entries in this table are the Z-values for correlations ranging from 0.000 to 0.999.
</p>
<p>THIRD DECIMAL PLACE IN r
</p>
<p>r 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009
</p>
<p>0.00 0.0000 0.0010 0.0020 0.0030 0.0040 0.0050 0.0060 0.0070 0.0080 0.0090
0.01 0.0100 0.0110 0.0120 0.0130 0.0140 0.0150 0.0160 0.0170 0.0180 0.0190
0.02 0.0200 0.0210 0.0220 0.0230 0.0240 0.0250 0.0260 0.0270 0.0280 0.0290
0.03 0.0300 0.0310 0.0320 0.0330 0.0340 0.0350 0.0360 0.0370 0.0380 0.0390
0.04 0.0400 0.0410 0.0420 0.0430 0.0440 0.0450 0.0460 0.0470 0.0480 0.0490
0.05 0.0500 0.0510 0.0520 0.0530 0.0541 0.0551 0.0561 0.0571 0.0581 0.0591
0.06 0.0601 0.0611 0.0621 0.0631 0.0641 0.0651 0.0661 0.0671 0.0681 0.0691
0.07 0.0701 0.0711 0.0721 0.0731 0.0741 0.0751 0.0761 0.0772 0.0782 0.0792
0.08 0.0802 0.0812 0.0822 0.0832 0.0842 0.0852 0.0862 0.0872 0.0882 0.0892
0.09 0.0902 0.0913 0.0923 0.0933 0.0943 0.0953 0.0963 0.0973 0.0983 0.0993
0.10 0.1003 0.1013 0.1024 0.1034 0.1044 0.1054 0.1064 0.1074 0.1084 0.1094
0.11 0.1104 0.1115 0.1125 0.1135 0.1145 0.1155 0.1165 0.1175 0.1186 0.1196
0.12 0.1206 0.1216 0.1226 0.1236 0.1246 0.1257 0.1267 0.1277 0.1287 0.1297
0.13 0.1307 0.1318 0.1328 0.1338 0.1348 0.1358 0.1368 0.1379 0.1389 0.1399
0.14 0.1409 0.1419 0.1430 0.1440 0.1450 0.1460 0.1471 0.1481 0.1491 0.1501
0.15 0.1511 0.1522 0.1532 0.1542 0.1552 0.1563 0.1573 0.1583 0.1593 0.1604
0.16 0.1614 0.1624 0.1634 0.1645 0.1655 0.1665 0.1676 0.1686 0.1696 0.1706
0.17 0.1717 0.1727 0.1737 0.1748 0.1758 0.1768 0.1779 0.1789 0.1799 0.1809
0.18 0.1820 0.1830 0.1841 0.1851 0.1861 0.1872 0.1882 0.1892 0.1903 0.1913
0.19 0.1923 0.1934 0.1944 0.1955 0.1965 0.1975 0.1986 0.1996 0.2007 0.2017
0.20 0.2027 0.2038 0.2048 0.2059 0.2069 0.2079 0.2090 0.2100 0.2111 0.2121
0.21 0.2132 0.2142 0.2153 0.2163 0.2174 0.2184 0.2195 0.2205 0.2216 0.2226
0.22 0.2237 0.2247 0.2258 0.2268 0.2279 0.2289 0.2300 0.2310 0.2321 0.2331
0.23 0.2342 0.2352 0.2363 0.2374 0.2384 0.2395 0.2405 0.2416 0.2427 0.2437
0.24 0.2448 0.2458 0.2469 0.2480 0.2490 0.2501 0.2512 0.2522 0.2533 0.2543
0.25 0.2554 0.2565 0.2575 0.2586 0.2597 0.2608 0.2618 0.2629 0.2640 0.2650
0.26 0.2661 0.2672 0.2683 0.2693 0.2704 0.2715 0.2726 0.2736 0.2747 0.2758
0.27 0.2769 0.2779 0.2790 0.2801 0.2812 0.2823 0.2833 0.2844 0.2855 0.2866
0.28 0.2877 0.2888 0.2899 0.2909 0.2920 0.2931 0.2942 0.2953 0.2964 0.2975
0.29 0.2986 0.2997 0.3008 0.3018 0.3029 0.3040 0.3051 0.3062 0.3073 0.3084
0.30 0.3095 0.3106 0.3117 0.3128 0.3139 0.3150 0.3161 0.3172 0.3183 0.3194
0.31 0.3205 0.3217 0.3228 0.3239 0.3250 0.3261 0.3272 0.3283 0.3294 0.3305
0.32 0.3316 0.3328 0.3339 0.3350 0.3361 0.3372 0.3383 0.3395 0.3406 0.3417
0.33 0.3428 0.3440 0.3451 0.3462 0.3473 0.3484 0.3496 0.3507 0.3518 0.3530
0.34 0.3541 0.3552 0.3564 0.3575 0.3586 0.3598 0.3609 0.3620 0.3632 0.3643
0.35 0.3654 0.3666 0.3677 0.3689 0.3700 0.3712 0.3723 0.3734 0.3746 0.3757
0.36 0.3769 0.3780 0.3792 0.3803 0.3815 0.3826 0.3838 0.3850 0.3861 0.3873
0.37 0.3884 0.3896 0.3907 0.3919 0.3931 0.3942 0.3954 0.3966 0.3977 0.3989
0.38 0.4001 0.4012 0.4024 0.4036 0.4047 0.4059 0.4071 0.4083 0.4094 0.4106
0.39 0.4118 0.4130 0.4142 0.4153 0.4165 0.4177 0.4189 0.4201 0.4213 0.4225
0.40 0.4236 0.4248 0.4260 0.4272 0.4284 0.4296 0.4308 0.4320 0.4332 0.4344
0.41 0.4356 0.4368 0.4380 0.4392 0.4404 0.4416 0.4428 0.4441 0.4453 0.4465
0.42 0.4477 0.4489 0.4501 0.4513 0.4526 0.4538 0.4550 0.4562 0.4574 0.4587
0.43 0.4599 0.4611 0.4624 0.4636 0.4648 0.4660 0.4673 0.4685 0.4698 0.4710
0.44 0.4722 0.4735 0.4747 0.4760 0.4772 0.4784 0.4797 0.4809 0.4822 0.4834
0.45 0.4847 0.4860 0.4872 0.4885 0.4897 0.4910 0.4922 0.4935 0.4948 0.4960
0.46 0.4973 0.4986 0.4999 0.5011 0.5024 0.5037 0.5049 0.5062 0.5075 0.5088
0.47 0.5101 0.5114 0.5126 0.5139 0.5152 0.5165 0.5178 0.5191 0.5204 0.5217
0.48 0.5230 0.5243 0.5256 0.5269 0.5282 0.5295 0.5308 0.5321 0.5334 0.5347
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>768</p>
<p/>
</div>
<div class="page"><p/>
<p>THIRD DECIMAL PLACE IN r
</p>
<p>r 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009
</p>
<p>0.49 0.5361 0.5374 0.5387 0.5400 0.5413 0.5427 0.5440 0.5453 0.5466 0.5480
0.50 0.5493 0.5506 0.5520 0.5533 0.5547 0.5560 0.5573 0.5587 0.5600 0.5614
0.51 0.5627 0.5641 0.5654 0.5668 0.5682 0.5695 0.5709 0.5722 0.5736 0.5750
0.52 0.5763 0.5777 0.5791 0.5805 0.5818 0.5832 0.5846 0.5860 0.5874 0.5888
0.53 0.5901 0.5915 0.5929 0.5943 0.5957 0.5971 0.5985 0.5999 0.6013 0.6027
0.54 0.6042 0.6056 0.6070 0.6084 0.6098 0.6112 0.6127 0.6141 0.6155 0.6169
0.55 0.6184 0.6198 0.6213 0.6227 0.6241 0.6256 0.6270 0.6285 0.6299 0.6314
0.56 0.6328 0.6343 0.6358 0.6372 0.6387 0.6401 0.6416 0.6431 0.6446 0.6460
0.57 0.6475 0.6490 0.6505 0.6520 0.6535 0.6550 0.6565 0.6580 0.6595 0.6610
0.58 0.6625 0.6640 0.6655 0.6670 0.6685 0.6700 0.6716 0.6731 0.6746 0.6761
0.59 0.6777 0.6792 0.6807 0.6823 0.6838 0.6854 0.6869 0.6885 0.6900 0.6916
0.60 0.6931 0.6947 0.6963 0.6978 0.6994 0.7010 0.7026 0.7042 0.7057 0.7073
0.61 0.7089 0.7105 0.7121 0.7137 0.7153 0.7169 0.7185 0.7201 0.7218 0.7234
0.62 0.7250 0.7266 0.7283 0.7299 0.7315 0.7332 0.7348 0.7365 0.7381 0.7398
0.63 0.7414 0.7431 0.7447 0.7464 0.7481 0.7498 0.7514 0.7531 0.7548 0.7565
0.64 0.7582 0.7599 0.7616 0.7633 0.7650 0.7667 0.7684 0.7701 0.7718 0.7736
0.65 0.7753 0.7770 0.7788 0.7805 0.7823 0.7840 0.7858 0.7875 0.7893 0.7910
0.66 0.7928 0.7946 0.7964 0.7981 0.7999 0.8017 0.8035 0.8053 0.8071 0.8089
0.67 0.8107 0.8126 0.8144 0.8162 0.8180 0.8199 0.8217 0.8236 0.8254 0.8273
0.68 0.8291 0.8310 0.8328 0.8347 0.8366 0.8385 0.8404 0.8423 0.8441 0.8460
0.69 0.8480 0.8499 0.8518 0.8537 0.8556 0.8576 0.8595 0.8614 0.8634 0.8653
0.70 0.8673 0.8693 0.8712 0.8732 0.8752 0.8772 0.8792 0.8812 0.8832 0.8852
0.71 0.8872 0.8892 0.8912 0.8933 0.8953 0.8973 0.8994 0.9014 0.9035 0.9056
0.72 0.9076 0.9097 0.9118 0.9139 0.9160 0.9181 0.9202 0.9223 0.9245 0.9266
0.73 0.9287 0.9309 0.9330 0.9352 0.9373 0.9395 0.9417 0.9439 0.9461 0.9483
0.74 0.9505 0.9527 0.9549 0.9571 0.9594 0.9616 0.9639 0.9661 0.9684 0.9707
0.75 0.9730 0.9752 0.9775 0.9798 0.9822 0.9845 0.9868 0.9892 0.9915 0.9939
0.76 0.9962 0.9986 1.0010 1.0034 1.0058 1.0082 1.0106 1.0130 1.0154 1.0179
0.77 1.0203 1.0228 1.0253 1.0277 1.0302 1.0327 1.0352 1.0378 1.0403 1.0428
0.78 1.0454 1.0479 1.0505 1.0531 1.0557 1.0583 1.0609 1.0635 1.0661 1.0688
0.79 1.0714 1.0741 1.0768 1.0795 1.0822 1.0849 1.0876 1.0903 1.0931 1.0958
0.80 1.0986 1.1014 1.1042 1.1070 1.1098 1.1127 1.1155 1.1184 1.1212 1.1241
0.81 1.1270 1.1299 1.1329 1.1358 1.1388 1.1417 1.1447 1.1477 1.1507 1.1538
0.82 1.1568 1.1599 1.1630 1.1660 1.1692 1.1723 1.1754 1.1786 1.1817 1.1849
0.83 1.1881 1.1914 1.1946 1.1979 1.2011 1.2044 1.2077 1.2111 1.2144 1.2178
0.84 1.2212 1.2246 1.2280 1.2315 1.2349 1.2384 1.2419 1.2454 1.2490 1.2526
0.85 1.2562 1.2598 1.2634 1.2671 1.2707 1.2745 1.2782 1.2819 1.2857 1.2895
0.86 1.2933 1.2972 1.3011 1.3050 1.3089 1.3129 1.3169 1.3209 1.3249 1.3290
0.87 1.3331 1.3372 1.3414 1.3456 1.3498 1.3540 1.3583 1.3626 1.3670 1.3714
0.88 1.3758 1.3802 1.3847 1.3892 1.3938 1.3984 1.4030 1.4077 1.4124 1.4171
0.89 1.4219 1.4268 1.4316 1.4365 1.4415 1.4465 1.4516 1.4566 1.4618 1.4670
0.90 1.4722 1.4775 1.4828 1.4882 1.4937 1.4992 1.5047 1.5103 1.5160 1.5217
0.91 1.5275 1.5334 1.5393 1.5453 1.5513 1.5574 1.5636 1.5698 1.5762 1.5826
0.92 1.5890 1.5956 1.6022 1.6089 1.6157 1.6226 1.6296 1.6366 1.6438 1.6510
0.93 1.6584 1.6658 1.6734 1.6811 1.6888 1.6967 1.7047 1.7129 1.7211 1.7295
0.94 1.7380 1.7467 1.7555 1.7645 1.7736 1.7828 1.7923 1.8019 1.8117 1.8216
0.95 1.8318 1.8421 1.8527 1.8635 1.8745 1.8857 1.8972 1.9090 1.9210 1.9333
0.96 1.9459 1.9588 1.9721 1.9857 1.9996 2.0139 2.0287 2.0439 2.0595 2.0756
0.97 2.0923 2.1095 2.1273 2.1457 2.1649 2.1847 2.2054 2.2269 2.2494 2.2729
0.98 2.2976 2.3235 2.3507 2.3796 2.4101 2.4427 2.4774 2.5147 2.5550 2.5987
</p>
<p>0.99 2.6467 2.6996 2.7587 2.8257 2.9031 2.9945 3.1063 3.2504 3.4534 3.8002
</p>
<p>Note: Values were computed using the equation for the Fisher r-to-Z* transformation.
</p>
<p>A P P E N D I C E S 769</p>
<p/>
</div>
<div class="page"><p/>
<p>Glossary
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>770
</p>
<p>analysis of variance (ANOVA) A parametric test
of statistical significance that assesses whether
differences in the means of several samples
(groups) can lead the researcher to reject the
null hypothesis that the means of the popula-
tions from which the samples are drawn are the
same.
</p>
<p>arrangements The different ways events can be
ordered and yet result in a single outcome. For
example, there is only one arrangement for
gaining the outcome of ten heads in ten tosses
of a coin. There are, however, ten different
arrangements for gaining the outcome of nine
heads in ten tosses of a coin.
</p>
<p>assumptions Statements that identify the require-
ments and characteristics of a test of statistical
significance. These are the foundations on
which the rest of the test is built.
</p>
<p>bar chart A graph in which bars represent frequen-
cies, percentages, or proportions for the cate-
gories or values of a variable.
</p>
<p>between sum of squares (BSS) A measure of the
variability between samples (groups). The be-
tween sum of squares is calculated by taking the
sum of the squared deviation of each sample
mean from the grand mean multiplied by the
number of cases in that sample.
</p>
<p>biased Describing a statistic when its estimate of
a population parameter does not center on the
true value. In regression analysis, the omission
of relevant independent variables will lead to
bias in the estimate of Y. When relevant inde-
pendent variables are omitted and those mea-
sures are related to an independent variable
included in regression analysis, then the esti-
mate of the effect of that variable will also be
biased.
</p>
<p>binomial distribution The probability or sampling
distribution for an event that has only two possi-
ble outcomes.
</p>
<p>binomial formula The means of determining the
probability that a given set of binomial events
will occur in all its possible arrangements.
</p>
<p>bivariate regression A technique for predicting
change in a dependent variable using one inde-
pendent variable.
</p>
<p>cells The various entries in a table, each of which 
is identified by a particular row and column.
When we use a table to compare two variables,
it is convenient to refer to each combination of
categories as a cell.
</p>
<p>central limit theorem A theorem that states: &ldquo;If re-
peated independent random samples of size N
are drawn from a population, as N grows large,
the sampling distribution of sample means will
be approximately normal.&rdquo; The central limit the-
orem enables the researcher to make inferences
about an unknown population using a normal
sampling distribution.
</p>
<p>chi-square distribution A sampling distribution
that is used to conduct tests of statistical signifi-
cance with binary or multicategory nominal vari-
ables. The distribution is nonsymmetrical and
varies according to degrees of freedom. All the
values in the distribution are positive.
</p>
<p>chi-square statistic The test statistic resulting from
applying the chi-square formula to the observed
and expected frequencies for each cell. This statis-
tic tells us how much the observed distribution dif-
fers from that expected under the null hypothesis.
</p>
<p>classification The process whereby data are orga-
nized into categories or groups.
</p>
<p>coefficient of relative variation A measure of dis-
persion calculated by dividing the standard devi-
ation by the mean.
</p>
<p>concordant pairs of observations Pairs of obser-
vations that have consistent rankings on two or-
dinal variables.
</p>
<p>confidence interval An interval of values around a
statistic (usually a point estimate). If we were to
draw repeated samples and calculate a 95% con-
fidence interval for each, then in only 5 in 100
of these samples would the interval fail to in-
clude the true population parameter. In the case
of a 99% confidence interval, only 1 in 100 sam-
ples would fail to include the true population
parameter.
</p>
<p>alternation A type of quasi-random assignment in 
which researchers assign every other case to one 
particular group.
</p>
<p>between effect Effect of an independent variable 
on the dependent variable using the cluster as 
the unit of analysis&mdash;a regression of cluster-level 
averages across all the clusters included in the 
analysis.
</p>
<p>cluster-mean centering Computed difference 
between the observed raw score on some vari-
able for each observation in the sample and the 
cluster mean for that variable.
</p>
<p>block randomization A type of randomization 
whereby cases are first sorted into like groups 
and then afterwards randomly allocated into 
treatment and control conditions.
</p>
<p>confounding factors Variables associated with 
treatments and/or outcomes that can bias overall 
results if not controlled for statistically.
</p>
<p>contrast coding A method for recoding a multi-
category nominal variable into multiple indicator </p>
<p/>
</div>
<div class="page"><p/>
<p>G L O S S A R Y 771
</p>
<p>Cramer&rsquo;s V A measure of association for two nomi-
nal variables that adjusts the chi-square statistic
by the sample size. V is appropriate when at
least one of the nominal variables has more than
two categories.
</p>
<p>critical value The point at which the rejection re-
gion begins.
</p>
<p>cumulative logistic probability function A trans-
formation of the logistic probability function that
allows computation of the probability that Y will
occur, given a certain combination of character-
istics of the independent variables.
</p>
<p>curvilinear relationship An association between
two variables whose values may be represented
as a curved line when plotted on a scatter dia-
gram.
</p>
<p>data Information used to answer a research ques-
tion.
</p>
<p>degrees of freedom A mathematical index that
places a value on the extent to which a particu-
lar operation is free to vary after certain limita-
</p>
<p>determines which chi-square probability distrib-
ution we use.
</p>
<p>distribution-free tests Another name for nonpara-
metric tests.
</p>
<p>dummy variable A binary nominal-level variable
that is included in a multivariate regression
model.
</p>
<p>effect size (ES) A standardized measure derived by
taking the effect size (e.g., the difference be-
tween two populations), measured in the raw
units of the outcome measure examined, and di-
viding it by the pooled or common standard de-
viation of the outcome measure.
</p>
<p>eta A measure of the degree of correlation between
an interval-level and a nominal-level variable.
</p>
<p>eta squared The proportion of the total sum of
squares that is accounted for by the between
sum of squares. Eta squared is sometimes re-
ferred to as the percent of variance explained.
</p>
<p>expected frequency The number of observations
one would predict for a cell if the null hypothe-
sis were true.
</p>
<p>explained sum of squares (ESS) Another name
for the between sum of squares. The explained
sum of squares is the part of the total variability
that can be explained by visible differences be-
tween the groups.
</p>
<p>tions have been imposed. Calculating the
degrees of freedom for a chi-square test
</p>
<p>cross-level interaction An interaction effect 
included in a multilevel model between a level 
1 independent variable and a level 2 cluster 
characteristic. 
</p>
<p>convenience sample A sample chosen not at ran-
dom, but according to criteria of expedience or
accessibility to the researcher.
</p>
<p>correctly specified regression model A regres-
sion model in which the researcher has taken
into account all of the relevant predictors of the
dependent variable and has measured them cor-
rectly.
</p>
<p>correlation A measure of the strength of a relation-
ship between two variables.
</p>
<p>covariation A measure of the extent to which two
variables vary together relative to their respec-
tive means. The covariation between the two
variables serves as the numerator for the equa-
tion to calculate Pearson&rsquo;s r.
</p>
<p>Cox and Snell&rsquo;s R2 A commonly used pseudo R 2
</p>
<p>measure whose main component, as in other
pseudo R 2 statistics, is the log likelihood func-
tion (�2LL).
</p>
<p>variables (one less than the total number of cate-
gories), where the indicator category is coded as 
1, the reference category is coded as &minus; 1, and all 
other categories are coded as 0. Contrast coding 
ensures that the sum of all the estimated effects 
for the indicator variable is equal to 0.
</p>
<p>control group The group that eligible cases are 
randomly assigned to which does not receive 
the treatment or the intervention being evalu-
ated. In many criminological experiments the 
control group may receive existing interventions 
in contrast to the innovative treatment.
</p>
<p>eligibility pool Participants or units that are eligible 
for an experiment.
</p>
<p>dependent variable (Y) The variable assumed by
the researcher to be influenced by one or more
independent variables; the outcome variable;
the phenomenon that we are interested in ex-
plaining. It is dependent on other variables in
the sense that it is influenced&mdash;or we expect it
to be influenced&mdash;by other variables.
</p>
<p>derivative at mean (DM) A measure that converts
the nonlinear logistic regression coefficient to a
simple linear regression coefficient, which may
be interpreted as the change in Y associated
with a unit change in X.
</p>
<p>descriptive statistics A broad area of statistics that
is concerned with summarizing large amounts of
information in an efficient manner. Descriptive
statistics are used to describe or represent in
summary form the characteristics of a sample or
population.
</p>
<p>design sensitivity The statistical power of a re-
search study. In a sensitive study design, statisti-
cal power will be maximized, and the statistical
test employed will be more capable of identify-
ing an effect.
</p>
<p>deviation from the mean The extent to which
each individual score differs from the mean of
all the scores.
</p>
<p>directional hypothesis A research hypothesis that
indicates a specific type of outcome by specify-
ing the nature of the relationship that is ex-
pected.
</p>
<p>discordant pairs of observations Pairs of obser-
vations that have inconsistent rankings on two
ordinal variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>G L O S S A R Y772
</p>
<p>external validity The extent to which a study sam-
ple is reflective of the population from which it
is drawn. A study is said to have high external
validity when the sample used is representative
of the population to which inferences are made.
</p>
<p>factorial The product of a number and all the posi-
tive whole numbers lower than it.
</p>
<p>frequency The number of times that a score or
value occurs.
</p>
<p>frequency distribution An arrangement of scores
in order from the lowest to the highest, accompa-
nied by the number of times each score occurs.
</p>
<p>gamma (�) PRE measure of association for two or-
dinal variables that uses information about con-
cordant and discordant pairs of observations
within a table. Gamma has a standardized scale
ranging from �1.0 to 1.0.
</p>
<p>Goodman and Kruskal&rsquo;s tau (�) PRE measure of
association for two nominal variables that uses
information about the proportional distribution
of cases within a table. Tau has a standardized
scale ranging from 0 to 1.0. For this measure,
the researcher must define the independent and
dependent variables.
</p>
<p>grand mean The overall mean of every single case
across all of the samples.
</p>
<p>heteroscedasticity A situation in which the vari-
ances of scores on two or more variables are
not equal. Heteroscedasticity violates one of the
assumptions of the parametric test of statistical
significance for the correlation coefficient.
</p>
<p>histogram A bar graph used to represent a fre-
quency distribution.
</p>
<p>fixed effects A descriptive label for the regres-
sion coefficients (b
</p>
<p>k
) estimated in a model with 
</p>
<p>random effects. Fixed effects represent the aver-
age effects of the independent variables on the 
dependent variable across all individuals and 
clusters in a multilevel model.
</p>
<p>grand-mean centering Computed difference 
between the observed raw score on some vari-
able for each observation in the  sample and the 
overall sample mean for that  variable.
</p>
<p>group allocation In criminological experiments, 
eligible cases are randomly assigned to two or 
more groups&mdash;typically treatment or control.
</p>
<p>homoscedasticity A statement that the variances
and standard deviations of two or more popula-
tions are the same.
</p>
<p>honestly significant difference (HSD) test A
parametric test of statistical significance, ad-
justed for making pairwise comparisons. The
HSD test defines the difference between the
pairwise comparisons required to reject the null
hypothesis.
</p>
<p>independent Describing two events when the oc-
currence of one does not affect the occurrence
of the other.
</p>
<p>independent random sampling A form of ran-
dom sampling in which the fact that one subject
is drawn from a population in no way affects
the probability of drawing any other subject
from that population.
</p>
<p>independent variable (X) A variable assumed by
the researcher to have an impact on or influence
the value of the dependent variable, Y.
</p>
<p>index of qualitative variation A measure of dis-
persion calculated by dividing the sum of the
possible pairs of observed scores by the sum of
the possible pairs of expected scores (when
</p>
<p>inferential, or inductive, statistics A broad area
of statistics that provides the researcher with
tools for making statements about populations
on the basis of knowledge about samples. Infer-
ential statistics allow the researcher to make in-
ferences regarding populations from information
gained in samples.
</p>
<p>interval scale A scale of measurement that uses a
common and standard unit and enables the re-
searcher to calculate exact differences between
scores, in addition to categorizing and ordering
data.
</p>
<p>iteration Each time we identify another tentative
solution and reestimate our logistic regression
coefficients.
</p>
<p>Kendall&rsquo;s �b PRE measure of association for two or-
dinal variables that uses information about con-
</p>
<p>b
</p>
<p>has a standardized scale ranging from �1.0 to
1.0 and is appropriate only when the number of
rows equals the number of columns in a table.
</p>
<p>Kendall&rsquo;s �c A measure of association for two ordi-
nal variables that uses information about con-
</p>
<p>c
</p>
<p>has a standardized scale ranging from �1.0 to
1.0 and is appropriate when the number of rows
is not equal to the number of columns in a
table.
</p>
<p>cordant pairs, discordant pairs, and pairs of 
observations tied on both variables examined. �
</p>
<p>cordant pairs, discordant pairs, and pairs of 
observations tied on both variables examined. �
</p>
<p>variable on the dependent variable is conditional
on the level of a second independent variable.
</p>
<p>cases are equally distributed across categories).
</p>
<p>sent when the effect of one independent 
interaction effect An interaction effect is pre-
</p>
<p>internal validity Whether the research design has 
allowed for the impact of the intervention or the 
treatment to be clearly distinguished from other 
factors.
</p>
<p>intraclass correlation A measure of association 
that measures the level of absolute agreement of 
values within each cluster.
</p>
<p>Kruskal-Wallis test A nonparametric test of statisti-
cal significance for multiple groups, requiring at
least an ordinal scale of measurement.</p>
<p/>
</div>
<div class="page"><p/>
<p>G L O S S A R Y 773
</p>
<p>lack of convergence Failure of a logistic regression
analysis to reach a result that meets the criterion
of reduction in the log likelihood function.
</p>
<p>lambda (�) PRE measure of association for two
nominal variables that uses information about
</p>
<p>least squares property A characteristic of the
mean whereby the sum of all the squared devia-
tions from the mean is a minimum&mdash;it is lower
than the sum of the squared deviations from any
other fixed point.
</p>
<p>levels of measurement Types of measurement
that make use of progressively larger amounts
of information.
</p>
<p>likelihood ratio chi-square test A test for statisti-
</p>
<p>ables in a logistic regression is statistically
significant. It compares �2LL for a full model to
�2LL for a reduced model.
</p>
<p>linear relationship An association between two
variables whose joint distribution may be repre-
sented in linear form when plotted on a scatter
diagram.
</p>
<p>log likelihood function A measure of the proba-
bility of observing the results in the sample,
given the coefficient estimates in the model. In
logistic regression, the log likelihood function
(�2LL) is defined as �2 times the natural loga-
rithm of the likelihood function.
</p>
<p>logarithm The power to which a fixed number
(the base) must be raised to produce another
number.
</p>
<p>logistic model curve The form of the predicted
outcomes of a logistic regression analysis.
Shaped like an S, the logistic curve begins to
flatten as it approaches 0 or 1, so it keeps com-
ing closer to&mdash;but never actually reaches&mdash;either
of these two values.
</p>
<p>logistic regression analysis A type of regression
analysis that allows the researcher to make pre-
dictions about dichotomous dependent variables
in terms of the log of the odds of Y.
</p>
<p>logistic regression coefficient The coefficient b
produced in a logistic regression analysis. It
may be interpreted as the increase in the log of
the odds of Y associated with a one-unit in-
</p>
<p>margin of error The size of the confidence inter-
val for a test. A margin of error of �3% in an
opinion poll means that the confidence interval
ranged between 3% above and 3% below the
point estimate or observed statistic. 
</p>
<p>cal significance that allows the researcher to
examine whether a subset of independent vari-
</p>
<p>crease in X. 
</p>
<p>the modal category of the dependent variable for 
each category of the independent variable. Lambda
has a standardized scale ranging from 0 to 1.0.
</p>
<p>maximum likelihood estimation A technique for
estimating the parameters or coefficients of a
model that maximizes the probability that the
</p>
<p>estimates obtained will produce a distribution
similar to that of the observed data. 
</p>
<p>mean A measure of central tendency calculated by
dividing the sum of the scores by the number of
cases. 
</p>
<p>mean deviation A measure of dispersion calcu-
lated by adding the absolute deviation of each
score from the mean and then dividing the sum
by the number of cases. 
</p>
<p>measurement The assignment of numerical values
to objects, characteristics, or events in a system-
atic manner. 
</p>
<p>measures of central tendency Descriptive statis-
tics that allow us to identify the typical case in a
sample or population. Measures of central ten-
dency are measures of typicality. 
</p>
<p>measures of dispersion Descriptive statistics that
tell us how tightly clustered or dispersed the
cases in a sample or population are. They answer
the question &ldquo;How typical is the typical case?&rdquo; 
</p>
<p>median A measure of central tendency calculated
by identifying the value or category of the score
that occupies the middle position in the distribu-
tion of scores. 
</p>
<p>mode A measure of central tendency calculated by
identifying the score or category that occurs
most frequently. 
</p>
<p>model chi-square The statistical test used to assess
the statistical significance of the overall logistic
regression model. It compares the �2LL for the
full model with the �2LL calculated without any
independent variables included. 
</p>
<p>multicollinearity Condition in a multivariate re-
gression model in which independent variables
examined are very strongly intercorrelated. Mul-
ticollinearity leads to unstable regression coeffi-
cients. 
</p>
<p>marginal The value in the margin of a table that to-
tals the scores in the appropriate column or row. 
</p>
<p>multilevel data Sample data where individual 
observations (level 1 data) are clustered within a 
higher-level sampling unit (level 2 data).
</p>
<p>probability that a series of events will jointly occur. 
</p>
<p>multivariate regression A technique for predict-
ing change in a dependent variable, using more
than one independent variable. 
</p>
<p>multivariate statistics Statistics that examine the
relationships among variables while taking into
account the possible influences of other con-
founding factors. Multivariate statistics allow the
researcher to isolate the impact of one variable
from others that may distort his or her results.
</p>
<p> 
</p>
<p>multiplication rule The means for determining the
</p>
<p>multinomial logistic regression A statistical te-
</p>
<p>variable with three or more categories measured
chnique to predict the value of a dependent
</p>
<p>at the nominal level of measurement.
</p>
<p>Nagelkerke R2 A pseudo R 2 statistic that corrects
for the fact that Cox and Snell&rsquo;s estimates, as
well as many other pseudo R 2 statistics, often
have a maximum value of less than 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>G L O S S A R Y774
</p>
<p>natural logarithm of the odds of Y (logit of Y)
The outcome predicted in a logistic regression
analysis.
</p>
<p>nominal scale A scale of measurement that assigns
each piece of information to an appropriate cat-
egory without suggesting any order for the cate-
gories created.
</p>
<p>nondirectional hypothesis A research hypothesis
that does not indicate a specific type of out-
come, stating only that there is a relationship or
a difference.
</p>
<p>nonparametric tests Tests of statistical signifi-
cance that make no assumptions as to the shape
of the population distribution.
</p>
<p>normal curve A normal frequency distribution rep-
resented on a graph by a continuous line.
</p>
<p>normal frequency distribution A bell-shaped fre-
quency distribution, symmetrical in form. Its
mean, mode, and median are always the same.
The percentage of cases between the mean and
points at a measured distance from the mean is
fixed.
</p>
<p>null hypothesis A statement that reduces the re-
search question to a simple assertion to be
tested by the researcher. The null hypothesis
normally suggests that there is no relationship or
no difference.
</p>
<p>observed frequency The observed result of the
study, recorded in a cell.
</p>
<p>observed significance level The risk of Type I
error associated with a specific sample statistic
in a test. When the observed significance level is
less than the criterion significance level in a test
of statistical significance, the researcher will re-
ject the null hypothesis.
</p>
<p>odds ratio [Exp(B)] A statistic used to interpret the
logistic regression coefficient. It represents the
impact of a one-unit change in X on the ratio of
the probability of Y.
</p>
<p>OLS regression See ordinary least squares regres-
sion analysis.
</p>
<p>one-tailed test of significance A test of statistical
significance in which the region for rejecting the
null hypothesis falls on only one side of the
sampling distribution. One-tailed tests are based
on directional research hypotheses.
</p>
<p>dependent and the independent variable that is
</p>
<p>ordinal logistic regression A statistical technique
</p>
<p>level of measurement.
</p>
<p>non-linear relationship Relationship between the
</p>
<p>not captured by a straight line (linear) relationship.
</p>
<p>three or more categories measured at the ordinal
to predict the value of a dependent variable with
</p>
<p>ordinal scale A scale of measurement that catego-
rizes information and assigns it an order of mag-
nitude without using a standard scale of equal
intervals.
</p>
<p>parameter A characteristic of the population&mdash;for
example, the mean number of previous convic-
tions for all U.S. prisoners.
</p>
<p>parametric tests Tests of statistical significance
that make assumptions as to the shape of the
population distribution.
</p>
<p>Pearson&rsquo;s correlation coefficient See Pearson&rsquo;s r.
</p>
<p>tionships on a standardized scale from �1.0 to
1.0.
</p>
<p>percent of correct predictions A statistic used to
assess how well a logistic regression model ex-
plains the observed data. An arbitrary decision
point (usually 0.50) is set for deciding when a
predicted value should be set at 1, and then the
predictions are compared to the observed data.
</p>
<p>percent of variance explained (1) R 2, a measure
for evaluating how well the regression model
predicts values of Y; it represents the improve-
ment in predicting Y that the regression line
provides over the mean of Y. (2) �2, the propor-
tion of the total sum of squares that is ac-
counted for by the explained sum of squares.
</p>
<p>percentage A relation between two numbers in
which the whole is accorded a value of 100 and
the other number is given a numerical value
corresponding to its share of the whole.
</p>
<p>phi (�) A measure of association for two nominal
variables that adjusts the chi-square statistic by
the sample size. Phi is appropriate only for
nominal variables that each have two categories.
</p>
<p>pie chart A graph in which a circle (called a pie) is
cut into wedges to represent the relative size of
each category&rsquo;s frequency count.
</p>
<p>point estimate An estimate of the population para-
meter. Absent knowledge of the population pa-
rameter, the statistic we obtain for a sample is
</p>
<p>r
Pearson&rsquo;s r A commonly used measure of asso-
</p>
<p>ciation between two variables. Pearson&rsquo;s 
measures the strength and direction of linear rela-
</p>
<p>parallel slopes assumption In an ordinal logistic
</p>
<p>ordinary least squares regression analysis A
type of regression analysis in which the sum of
squared errors from the regression line is mini-
mized.
</p>
<p>outlier(s) A single or small number of exceptional
cases that substantially deviate from the general
pattern of scores.
</p>
<p>overall mean See grand mean.
pairwise comparisons Comparisons made be-
</p>
<p>tween two sample means extracted from a
larger statistical analysis.
</p>
<p>categories of the dependent variable.
</p>
<p>regression model, be effect of each independent
variable is assumed to be constant across all
</p>
<p>generally used as an estimate&mdash;or, in statistical
terms, a point estimate&mdash;of the population para-
meter.</p>
<p/>
</div>
<div class="page"><p/>
<p>G L O S S A R Y 775
</p>
<p>the other number is given a numerical value
corresponding to its share of the whole.
</p>
<p>proportional reduction in error (PRE) The pro-
portional reduction in errors made when the
value of one measure is predicted using infor-
mation about the second measure.
</p>
<p>pseudo R2 The term generally used for a group of
measures used in logistic regression to create an
approximation of the OLS regression R 2. They
are generally based on comparisons of �2LL for
a full model and a null model (without any in-
dependent variables).
</p>
<p>random sampling Drawing samples from the pop-
ulation in a manner that ensures every individ-
ual in that population an equal chance of being
selected.
</p>
<p>randomized experiment A type of study in which
the effect of one variable can be examined in
isolation through random allocation of subjects
to treatment and control, or comparison, groups.
</p>
<p>pooled variance A method of obtaining the standard
error of the sampling distribution for a difference
of means test. The pooled variance method re-
quires an assumption of homoscedasticity.
</p>
<p>population The universe of cases that the re-
searcher seeks to study. The population of cases
is fixed at a particular time (e.g., the population
of the United States). However, populations
usually change across time.
</p>
<p>population distribution The frequency distribu-
tion of a particular variable within a population.
</p>
<p>probability distribution A theoretical distribution
consisting of the probabilities expected in the
long run for all possible outcomes of an event.
</p>
<p>proportion A relation between two numbers in
which the whole is accorded a value of 1 and
</p>
<p>posttest measure Analyses conducted by the 
researcher to determine if the intervention had 
any impact on the outcome measures of interest.
</p>
<p>random coefficient model A linear regression 
model that allows the intercept and the effect 
of at least one independent variable to vary 
randomly across cluster&mdash;random effects are 
included for the model intercept and at least one 
independent variable.
</p>
<p>random effects A descriptive label for the random 
error terms included in a multilevel model that 
allow for variation across cluster from the sample 
average estimated in the fixed effects. Random 
effects are assumed to be normally distributed in 
most multilevel models.
</p>
<p>random intercept model A linear regression 
model that allows the intercept to vary randomly 
across cluster&mdash;random effects are included for 
the model intercept.
</p>
<p>randomization The process of  randomly assigning 
members from the pool of  eligible participants or 
units to the study  conditions&mdash;often a treatment 
group and a  control or a comparison group.
</p>
<p>range A measure of dispersion calculated by sub-
tracting the smallest score from the largest score.
The range may also be calculated from specific
points in a distribution, such as the 5th and 95th
percentile scores.
</p>
<p>rank-order test A test of statistical significance that
uses information relating to the relative order, or
rank, of variable scores.
</p>
<p>ratio scale A scale of measurement identical to an
interval scale in every respect except that, in ad-
dition, a value of zero on the scale represents
the absence of the phenomenon.
</p>
<p>regression coefficient b A statistic used to assess
the influence of an independent variable, X, on a
dependent variable, Y. The regression coefficient
b is interpreted as the estimated change in Y that
is associated with a one-unit change in X.
</p>
<p>regression error (e) The difference between the
predicted value of Y and the actual value of Y.
</p>
<p>regression coding A method for recoding a multi-
category nominal variable into multiple indicator 
dummy variables (one less than the total number 
of categories), where the indicator category is 
coded as 1 and all other categories are coded 
as 0. The reference category does not have an 
indicator variable and is coded as a 0 on all the 
indicator dummy variables.
</p>
<p>representative sample A sample that reflects the
population from which it is drawn.
</p>
<p>research hypothesis The antithesis of the null hy-
pothesis. The statement normally answers the
initial research question by suggesting that there
is a relationship or a difference.
</p>
<p>research question The question the researcher
hopes to be able to answer by means of a study.
</p>
<p>sample A set of actual observations or cases drawn
from a population.
</p>
<p>sample distribution The frequency distribution of
a particular variable within a sample drawn from
a population.
</p>
<p>sample statistic A characteristic of a sample&mdash;for
example, the mean number of previous convic-
tions in a random sample of 1,000 prisoners.
</p>
<p>regression line The line predicting values of Y.
The line is plotted from knowledge of the 
Y-intercept and the regression coefficient.
</p>
<p>regression model The hypothesized statement by
the researcher of the factor or factors that define
the value of the dependent variable, Y. The
model is normally expressed in equation form.
</p>
<p>rejection region The area of a sampling distribu-
tion containing the test statistic values that will
</p>
<p>relaxing an assumption Deciding that we need
not be concerned with that assumption. For ex-
ample, the assumption that a population is nor-
mal may be relaxed if the sample size is suffi-
ciently large to invoke the central limit theorem.
</p>
<p>reliability The extent to which a measure provides
consistent results across subjects or units of
study.
</p>
<p>cause the researcher to reject the null hypothesis.</p>
<p/>
</div>
<div class="page"><p/>
<p>G L O S S A R Y776
</p>
<p>sampling distribution A distribution of all the re-
sults of a very large number of samples, each
one of the same size and drawn from the same
population under the same conditions. Ordinar-
ily, sampling distributions are derived using
probability theory and are based on probability
distributions.
</p>
<p>sampling frame The universe of eligible cases
from which a sample is drawn.
</p>
<p>sampling with replacement A sampling method
in which individuals in a sample are returned to
the sampling frame after they have been se-
lected. This raises the possibility that certain in-
</p>
<p>scale of measurement Type of categorization
used to arrange or assign values to data.
</p>
<p>scatter diagram See scatterplot.
scatterplot A graph whose two axes are defined by
</p>
<p>two variables and upon which a point is plotted
for each subject in a sample according to its
score on the two variables.
</p>
<p>separate variance A method of obtaining the stan-
dard error of the sampling distribution for a dif-
</p>
<p>or&mdash;as in the case of a proportion&mdash;is defined
by the null hypothesis.
</p>
<p>skewed Describing a spread of scores that is clearly
weighted to one side.
</p>
<p>Somers&rsquo; d PRE measure of association for two ordi-
nal variables that uses information about con-
</p>
<p>Somers&rsquo; d has a standardized scale ranging from
�1.0 to 1.0.
</p>
<p>Spearman&rsquo;s correlation coefficient See Spear-
man&rsquo;s r.
</p>
<p>Spearman&rsquo;s r (rs) A measure of association be-
tween two rank-ordered variables. Spearman&rsquo;s r
measures the strength and direction of linear re-
lationships on a standardized scale between
�1.0 and 1.0.
</p>
<p>standard deviation A measure of dispersion calcu-
lated by taking the square root of the variance.
</p>
<p>standard deviation unit A unit of measurement
used to describe the deviation of a specific score
or value from the mean in a z distribution.
</p>
<p>standard error The standard deviation of a sam-
pling distribution.
</p>
<p>standard normal distribution A normal fre-
quency distribution with a mean of 0 and a stan-
dard deviation of 1. Any normal frequency dis-
tribution can be transformed into the standard
normal distribution by using the z formula.
</p>
<p>standardized logistic regression coefficient A
statistic used to compare logistic regression coef-
ficients that use different scales of measurement.
It is meant to approximate Beta, the standardized
regression coefficient in OLS regression.
</p>
<p>standardized regression coefficient (Beta)
Weighted or standardized estimate of b that
takes into account the standard deviation of the
independent and the dependent variables. The
standardized regression coefficient is used to
compare the effects of independent variables
</p>
<p>dividuals in a population may appear in a 
sample more than once.
</p>
<p>observations tied on the independent variable.
cordant pairs, discordant pairs, and pairs of 
</p>
<p>ference of means test. The separate variance
method does not require an assumption of ho-
moscedasticity.
</p>
<p>significance level The level of Type I error a re-
searcher is willing to risk in a test of statistical
significance.
</p>
<p>single-sample t-test A test of statistical significance
that is used to examine whether a sample is
drawn from a specific population with a known
or hypothesized mean. In a t-test, the standard
deviation of the population to which the sample
is being compared is unknown.
</p>
<p>single-sample z-test A test of statistical signifi-
cance that is used to examine whether a sample
is drawn from a specific population with a
known or hypothesized mean. In a z-test, the
standard deviation of the population to which
the sample is being compared either is known tails of the distribution The extremes on the sides
</p>
<p>of a sampling distribution. The events repre-
sented by the tails of a sampling distribution are
those deemed least likely to occur if the null hy-
pothesis is true for the population.
</p>
<p>test of statistical significance A test in which a
researcher makes a decision to reject or to fail to
reject the null hypothesis on the basis of a sam-
ple statistic.
</p>
<p>test statistic The outcome of the study, expressed
in units of the sampling distribution. A test sta-
tistic that falls within the rejection region will
lead the researcher to reject the null hypothesis.
</p>
<p>tied pairs of observations (ties) Pairs of observa-
tions that have the same ranking on two ordinal
variables.
</p>
<p>tresholds Points that mark the limits of the under
</p>
<p>measured on different scales in a multivariate re-
gression analysis.
</p>
<p>statistical inference The process of making gener-
alizations from sample statistics to population
parameters.
</p>
<p>statistical power One minus the probability of a
Type II error. The greater the statistical power
of a test, the less chance there is that a re-
searcher will mistakenly fail to reject the null
hypothesis.
</p>
<p>statistically significant Describing a test statistic
that falls within the rejection region defined by
the researcher. When this occurs, the researcher
is prepared to reject the null hypothesis and
state that the outcome or relationship is statisti-
cally significant.
</p>
<p>sum of squares The sum of squared deviations of
scores from a mean or set of means.
</p>
<p>lying continuum measured by an ordinal variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>G L O S S A R Y 777
</p>
<p>time series data Repeated measures of the same
</p>
<p>time series plot A line graph that connects re-
peated measures of the same variable over some
regularly occurring time period, such as days,
months, or years.
</p>
<p>tolerance A measure of the extent of the intercor-
relations of each independent variable with all
other independent variables. Tolerance may be
used to test for multicollinearity in a multivariate
regression model.
</p>
<p>total sum of squares (TSS) A measure of the total
amount of variability across all of the groups ex-
amined. The total sum of squares is calculated
by summing the squared deviation of each score
from the grand mean.
</p>
<p>t-test for dependent samples A test of statistical
significance that is used when two samples are
not independent.
</p>
<p>two-sample t-test A test of statistical significance
that examines the difference observed between
the means or proportions of two samples.
</p>
<p>two-tailed test of significance A test of statistical
significance in which the region for rejecting the
null hypothesis falls on both sides of the sam-
pling distribution. Two-tailed tests are based on
nondirectional research hypotheses.
</p>
<p>Type I error Also known as alpha error. The mis-
take made when a researcher rejects the null hy-
pothesis on the basis of a sample statistic (i.e.,
claiming that there is a relationship) when in
fact the null hypothesis is true (i.e., there is ac-
tually no such relationship in the population).
</p>
<p>Type II error Also known as beta error. The mis-
take made when a researcher fails to reject the
null hypothesis on the basis of a sample statistic
(i.e., failing to claim that there is a relationship)
when in fact the null hypothesis is false (i.e.,
there actually is a relationship).
</p>
<p>unexplained sum of squares (USS) Another
name for the within sum of squares. The unex-
plained sum of squares is the part of the total
variability that cannot be explained by visible
differences between the groups.
</p>
<p>universe The total population of cases.
validity The extent to which a variable accurately
</p>
<p>reflects the concept being measured.
variable A trait, characteristic, or attribute of a per-
</p>
<p>son/object/event that can be measured at least
at the nominal-scale level.
</p>
<p>variance (s2) A measure of dispersion calculated
by adding together the squared deviation of
each score from the mean and then dividing the
sum by the number of cases.
</p>
<p>variation ratio A measure of dispersion calculated
by subtracting the proportion of cases in the
modal category from 1.
</p>
<p>Wald statistic A statistic used to assess the statisti-
cal significance of coefficients in a logistic re-
gression model.
</p>
<p>within sum of squares (WSS) A measure of the vari-
ability within samples (groups). The within sum of
squares is calculated by summing the squared de-
viation of each score from its sample mean.
</p>
<p>Y-intercept (b0) The expected value of Y when 
X � 0. The Y-intercept is used in predicting
values of Y.
</p>
<p>z-score Score that represents standard deviation
units for a standard normal distribution.
</p>
<p>period, such as days, months, or years.
variable over some regularly occurring  time
</p>
<p>treatment group One group that eligible cases are 
randomly assigned to which receives the treat-
ment or the intervention being evaluated.
</p>
<p>variance components model A one-way analysis 
of variance model that includes random effects 
for each cluster that assesses whether there is 
random variation in the mean of the depend-
ent variable across the clusters included in the 
analysis.
</p>
<p>within effect Effect of an independent variable 
on the dependent variable within each cluster 
and then averaged across all clusters or groups 
included in the analysis.</p>
<p/>
</div>
<div class="page"><p/>
<p>I n d e x
</p>
<p>D. Weisburd and C. Britt, Statistics in Criminal Justice, DOI 10.1007/978-1-4614-9170-5,  
</p>
<p>&copy; Springer Science+Business Media New York 2014 
</p>
<p>778
</p>
<p>A
</p>
<p>alpha error. See Type I error
</p>
<p>analysis of  variance (ANOVA), 307&ndash;337
</p>
<p>arrangements, 152&ndash;147, 163
</p>
<p>equation for, 152
</p>
<p>association, measures of, 354&ndash;355, 359, 363&ndash;364, 
</p>
<p>367, 369, 382&ndash;383, 386&ndash;388
</p>
<p>Cramer&rsquo;s V, 356, 358, 386, 388&ndash;389
</p>
<p>387&ndash;388
</p>
<p>b c
), 368, 374, 381, 
</p>
<p>387&ndash;388, 390&ndash;391
</p>
<p>lambda ( ), 358, 387&ndash;388, 390
</p>
<p>nominal measures of  statistical significance, 365
</p>
<p>ordinal measures of  statistical significance, 
</p>
<p>367&ndash;368
</p>
<p>phi ( ), 364, 388&ndash;389
</p>
<p>Somers&rsquo; d, 365, 368&ndash;369, 376, 380, 387&ndash;388
</p>
<p>assumptions, for statistical inference, 174&ndash;180, 214
</p>
<p>B
</p>
<p>b (regression coefficient), 440&ndash;447, 458, 461,  
</p>
<p>464, 624
</p>
<p>calculation of, 442
</p>
<p>statistical significance of, 458&ndash;467
</p>
<p>b
0
(Y-intercept), 445&ndash;447, 449, 470&ndash;472
</p>
<p>equation for, 445, 447
</p>
<p>bar charts, 43&ndash;44, 55&ndash;56
</p>
<p>horizontal, 43&ndash;45
</p>
<p>for nominal and ordinal data, 50
</p>
<p>vertical, 43&ndash;44
</p>
<p>beta error. See Type II error
</p>
<p>between effect, 652
</p>
<p>testing of, 654
</p>
<p>between sum of  squares (BSS), 310&ndash;311, 313&ndash;315, 
</p>
<p>320, 329&ndash;330, 337&ndash;338
</p>
<p>equation for, 310, 339
</p>
<p>binomial distribution, 155&ndash;164
</p>
<p>equation for, 156, 164
</p>
<p>bivariate regression, 439&ndash;473
</p>
<p>block randomization, 685&ndash;688
</p>
<p>benefits of, 689&ndash;690
</p>
<p>statistical power, 691&ndash;693
</p>
<p>Brant test, 623
</p>
<p>BSS. See between sum of  squares
</p>
<p>C
</p>
<p>cells, 209, 223
</p>
<p>centering, 648&ndash;649
</p>
<p>central limit theorem, 251&ndash;252, 262
</p>
<p>central tendency, 8, 66, 73
</p>
<p>chi-square ( 2) distribution, 199&ndash;202, 223, 612, 614, 
</p>
<p>622, 623, 628
</p>
<p>chi-square ( 2 ) statistic, 201&ndash;203, 223, 614, 627
</p>
<p>for relating two measures, 210
</p>
<p>chi-square ( 2) table, 202&ndash;203, 612
</p>
<p>chi-square ( 2 ) test, 203&ndash;205
</p>
<p>with multicategory variables, 212&ndash;217
</p>
<p>with ordinal variables, 217&ndash;222
</p>
<p>with small samples, 222
</p>
<p>classification, 14&ndash;15, 26
</p>
<p>clustered data, 638&ndash;639
</p>
<p>coefficient of  relative variation (CRV), 111&ndash;112
</p>
<p>equation for, 113, 118
</p>
<p>coin toss, fair, 147&ndash;151
</p>
<p>column marginals, 209
</p>
<p>confidence intervals, 702&ndash;718
</p>
<p>compared to rejection region, 706&ndash;707
</p>
<p>for logistic regression coefficient, 716, 719
</p>
<p>for Pearson&rsquo;s r, 713
</p>
<p>for regression coefficient, 716, 719
</p>
<p>for sample mean, 711&ndash;713
</p>
<p>for sample proportion, 710&ndash;713
</p>
<p>confidence limit equations, 709&ndash;711, 714&ndash;715, 719
</p>
<p>convenience samples, 175, 189
</p>
<p>correctly specified model, 483&ndash;484
</p>
<p>correlation, 361 (see also eta ( ), Pearson&rsquo;s r)
</p>
<p>equation for, 403, 431
</p>
<p>Spearman&rsquo;s r covariation, 403&ndash;404
</p>
<p>correct model specification, 482&ndash;484
</p>
<p>Cramer&rsquo;s V, 354, 386
</p>
<p>statistical significance of, 363&ndash;364
</p>
<p>critical region. See rejection region
</p>
<p>critical value, 182, 189, 612, 613, 615, 622, 623
</p>
<p>cross-level interaction, 660&ndash;662
</p>
<p>CRV. See coefficient of  relative variation
</p>
<p>curvilinear relationship, 43, 411&ndash;412, 430</p>
<p/>
</div>
<div class="page"><p/>
<p>I N D E X 779
</p>
<p>D
</p>
<p>data, 22&ndash;23, 26, 602, 614, 619, 625. See also decisions 
</p>
<p>about data
</p>
<p>by comparing sample means, 270&ndash;279
</p>
<p>clustered, 638&ndash;639
</p>
<p>with chi-square test, 199&ndash;202
</p>
<p>collection of, 18
</p>
<p>multilevel, 638&ndash;639
</p>
<p>with chi-square test, 201&ndash;203
</p>
<p>by comparing sample means, 270&ndash;275, 278
</p>
<p>decisions about data, making, 180
</p>
<p>making analysis of, 5&ndash;11
</p>
<p>with Kruskal-Wallis test, 334&ndash;335
</p>
<p>with logistic regression coefficients, 567&ndash;576
</p>
<p>making analysis of, 6&ndash;11, 367&ndash;368
</p>
<p>with Pearson&rsquo;s r, 401, 403&ndash;405, 407
</p>
<p>with regression coefficients, 456&ndash;462
</p>
<p>with single-sample t test, 257&ndash;261
</p>
<p>with Spearman&rsquo;s r, 420&ndash;421
</p>
<p>with t-test for dependent samples, 289&ndash;291, 
</p>
<p>294&ndash;295
</p>
<p>with z-test for proportions, 252&ndash;257
</p>
<p>degrees of  freedom (df), 199&ndash;200, 202&ndash;203, 205, 209, 
</p>
<p>321, 365, 426, 428, 461, 612&ndash;615, 622, 623
</p>
<p>dependent variable (Y), 440, 462, 603, 605, 611, 612, 
</p>
<p>615&ndash;618, 622&ndash;624, 627, 628
</p>
<p>derivative at mean, 575&ndash;576, 591, 594
</p>
<p>descriptive statistics, 7&ndash;8, 11
</p>
<p>design sensitivity, 740, 748
</p>
<p>deviation from the mean, 81, 85&ndash;86, 104&ndash;105,  
</p>
<p>114&ndash;115, 310, 395, 452
</p>
<p>equation for, 80&ndash;82, 254
</p>
<p>differenes from the mean. See deviation  
</p>
<p>from the mean
</p>
<p>directional hypotheses, 179, 185
</p>
<p>dispersion measures, 8
</p>
<p>distribution
</p>
<p>binomial, 156&ndash;158, 192&ndash;193, 199
</p>
<p>normal, 235&ndash;241
</p>
<p>normal frequency, 236&ndash;237, 244, 246, 262&ndash;263
</p>
<p>predictions beyond observed, 448&ndash;449, 551
</p>
<p>probability, 147&ndash;149, 158, 163&ndash;164
</p>
<p>rejection region of, 182&ndash;183
</p>
<p>sampling, 146&ndash;148, 164, 174, 180&ndash;183
</p>
<p>skewed, 83
</p>
<p>standard normal, 239&ndash;241, 245, 256, 262&ndash;263
</p>
<p>tails of, 183&ndash;184, 189, 244&ndash;245
</p>
<p>types of, 126&ndash;127, 138
</p>
<p>distribution-free tests. See nonparametric tests
</p>
<p>E
</p>
<p>e (regression error), 451&ndash;453, 470
</p>
<p>effect size (ES), 735&ndash;745
</p>
<p>error
</p>
<p>in prediction, 440
</p>
<p>reduction of, 5, 381
</p>
<p>risk of  in hypothesis testing, 133&ndash;135
</p>
<p>standard, 243&ndash;245, 605, 612, 613
</p>
<p>Type I, 134&ndash;135, 139&ndash;140
</p>
<p>Type II, 134&ndash;135, 137&ndash;140
</p>
<p>ESS, 313, 338
</p>
<p>equation for, 329&ndash;330, 338
2), 329&ndash;330, 338. See also percent  
</p>
<p>of  variance explained
</p>
<p>equation for, 329&ndash;330, 338
</p>
<p>expected frequency, 201, 205, 211, 212, 216
</p>
<p>experiments
</p>
<p>advantages of, 677&ndash;681
</p>
<p>interactions and, 695&ndash;697
</p>
<p>internal validity of, 682&ndash;683
</p>
<p>explained sum of  squares (ESS), 313, 329, 337&ndash;338
</p>
<p>explained variance, 313&ndash;314, 453&ndash;455, 647&ndash;648
</p>
<p>external validity, 175, 178, 188
</p>
<p>F
</p>
<p>failure, studies designed for, 747
</p>
<p>F equation, 319, 337
</p>
<p>factorials (!), 153
</p>
<p>frequency distribution, 37&ndash;38, 56
</p>
<p>frequency, expected, 201, 205, 222, 223
</p>
<p>studies designed for, 747&ndash;748
</p>
<p>-test for overall regression, 467&ndash;470
</p>
<p>fixed effects, 641&ndash;644
</p>
<p>G
</p>
<p>statistical significance of, 381
</p>
<p>387&ndash;388
</p>
<p>grand mean, 309&ndash;310, 333
</p>
<p>statistical significance of, 365
</p>
<p>group variability, 309&ndash;310, 313, 337
</p>
<p>H
</p>
<p>H
0
 (null hypothesis), 131&ndash;133
</p>
<p>H
1
 (research hypothesis), 129&ndash;131
</p>
<p>heteroscedasticity, 426&ndash;427, 430
</p>
<p>histogram, 39, 43, 56</p>
<p/>
</div>
<div class="page"><p/>
<p>I N D E X780
</p>
<p>homoscedasticity, 274, 295
</p>
<p>honestly significant difference (HSD), 332&ndash;334, 338
</p>
<p>equation for, 332, 340
</p>
<p>hypotheses, 129&ndash;133, 622, 628
</p>
<p>and risk of  error, 133&ndash;136
</p>
<p>I
</p>
<p>independence, defined, 164
</p>
<p>independent random sampling, 176&ndash;178, 189
</p>
<p>independent variable (X ), 358&ndash;365, 388, 390, 608, 
</p>
<p>611&ndash;615, 617&ndash;620, 622&ndash;628
</p>
<p>index of  qualitative variation (IQV), 100&ndash;102, 116
</p>
<p>equation for, 100, 117
</p>
<p>inferential statistics, 9&ndash;10
</p>
<p>interaction effects, 522&ndash;525, 528&ndash;532, 537&ndash;538
</p>
<p>cross-level, 660&ndash;662
</p>
<p>interpretation of  dummy variable
</p>
<p>and interval-level variable, 498&ndash;499
</p>
<p>interpretation of  two interval-level variables, 530&ndash;532
</p>
<p>interval scales, 19&ndash;21, 26
</p>
<p>measuring dispersion in, 102&ndash;111
</p>
<p>intraclass correlation, 644&ndash;645
</p>
<p>IQV. See index of  qualitative variation
</p>
<p>K
</p>
<p>b c  
), 368, 374, 381, 
</p>
<p>386&ndash;387, 390&ndash;391
</p>
<p>statistical significance of, 381&ndash;383
</p>
<p>Kruskal-Wallis test, 334&ndash;337
</p>
<p>equation for, 336, 339&ndash;340
</p>
<p>L
</p>
<p>statistical significance of, 363
</p>
<p>level-2 characteristics, 660&ndash;662
</p>
<p>levels of  measurement, 15&ndash;21, 26
</p>
<p>likelihood ratio chi-square test, 589&ndash;590, 592, 594, 613
</p>
<p>linear relationship, 411, 430
</p>
<p>logistic model curve, 556, 592
</p>
<p>logistic regression, 549&ndash;594, 602&ndash;607, 609&ndash;612, 615, 
</p>
<p>617, 619, 621&ndash;624, 627&ndash;629
</p>
<p>compared to OLS regression, 550&ndash;555
</p>
<p>estimating coefficients for, 550, 554
</p>
<p>statistical significance for, 555&ndash;558
</p>
<p>logistic regression coefficients, 567&ndash;568, 572,  
</p>
<p>580&ndash;583, 592, 594, 624
</p>
<p>comparing, 577&ndash;583
</p>
<p>confidence interval for, 582
</p>
<p>statistical significance of, 587&ndash;590, 613
</p>
<p>M
</p>
<p>marginals, 209, 223
</p>
<p>maximum likelihood estimation (MLE), 561, 593
</p>
<p>mean, 74&ndash;81, 85&ndash;86
</p>
<p>comparing to median, 77&ndash;80
</p>
<p>computing, 90&ndash;91
</p>
<p>confidence interval for, 711
</p>
<p>equation for, 74, 87
</p>
<p>grand, 307
</p>
<p>for noninterval scales, 82
</p>
<p>overall, 309
</p>
<p>mean deviation, 113&ndash;115
</p>
<p>measurement
</p>
<p>concept of, 14&ndash;15, 26
</p>
<p>ladder of, 16
</p>
<p>levels of, 15&ndash;16, 26
</p>
<p>scales of, 16
</p>
<p>measures
</p>
<p>of  association, 354&ndash;385
</p>
<p>of  central tendency, 8, 66&ndash;77
</p>
<p>of  dispersion, 9, 95&ndash;111
</p>
<p>median, 66&ndash;74, 86
</p>
<p>comparing to mean, 77&ndash;80
</p>
<p>mode, 66&ndash;68, 86
</p>
<p>multicollinearity, 516, 534&ndash;538
</p>
<p>multilevel data, 638&ndash;639
</p>
<p>multiplication rule, 149&ndash;151, 164
</p>
<p>multivariate regression, 481&ndash;507
</p>
<p>multivariate statistics, 7, 10&ndash;12
</p>
<p>N
</p>
<p>National Archive of  Criminal Justice Data  
</p>
<p>(NACJD), 31
</p>
<p>nominal-level data, 16&ndash;18
</p>
<p>nominal-level variable, 17
</p>
<p>nominal scales
</p>
<p>bar charts with, 50
</p>
<p>central tendency in, 66&ndash;68
</p>
<p>chi-square test with, 198&ndash;202
</p>
<p>measures of  association for, 355&ndash;365
</p>
<p>nondirectional hypotheses, 179
</p>
<p>relating to other scales, 22&ndash;23
</p>
<p>nondirectional hypotheses, 179
</p>
<p>non-linear relationship, 516&ndash;522
</p>
<p>coefficients, 519&ndash;521
</p>
<p>detecting with line graphs, 517&ndash;519
</p>
<p>statistical significance, 522
</p>
<p>nonparametric tests, 175, 179. See also  
</p>
<p>Kruskal-Wallis test</p>
<p/>
</div>
<div class="page"><p/>
<p>I N D E X 781
</p>
<p>normal curve, 235&ndash;247, 262
</p>
<p>normal frequency distribution, 235&ndash;247, 262
</p>
<p>null hypothesis (H
0
), 131&ndash;133
</p>
<p>O
</p>
<p>observed frequencies, 201&ndash;202, 216
</p>
<p>odds ratio, 568&ndash;569, 571&ndash;572, 607, 608, 618, 619, 
</p>
<p>620, 627&ndash;629, 716&ndash;717
</p>
<p>OLS regression, 452, 470
</p>
<p>one-tailed rejection region, 183
</p>
<p>one-tailed test of  significance, 183, 186, 188, 287
</p>
<p>ordinal-level data, 18&ndash;19
</p>
<p>ordinal logistic regression, 602, 616, 617, 619, 620, 
</p>
<p>622&ndash;624, 627, 628
</p>
<p>parallel slopes assumption, 622&ndash;627
</p>
<p>partial proportional odds, 624&ndash;625
</p>
<p>ordinal scales, 18&ndash;20, 26, 602
</p>
<p>bar charts with, 50
</p>
<p>measures of  association for, 367&ndash;369, 381
</p>
<p>ordinary least squares regression analysis, 452, 495&ndash;496
</p>
<p>outlier, 6, 79&ndash;80, 83&ndash;86
</p>
<p>overall mean, 309, 338
</p>
<p>P
</p>
<p>pairwise comparisons, 331&ndash;334, 339
</p>
<p>between groups studied, 331&ndash;334
</p>
<p>parallel slopes test, 622&ndash;623
</p>
<p>Brant test, 623
</p>
<p>score test, 622&ndash;623
</p>
<p>parameters, 127, 139&ndash;140, 619, 621
</p>
<p>parametric tests, 174, 235
</p>
<p>partial proportional odds, 624&ndash;625
</p>
<p>Pearson, Karl, 401
</p>
<p>Pearson&rsquo;s correlation coefficient. See Pearson&rsquo;s r
</p>
<p>Pearson&rsquo;s r, 401&ndash;405, 430&ndash;431
</p>
<p>calculation of, 405&ndash;407
</p>
<p>confidence interval for, 713&ndash;715
</p>
<p>equation for, 440&ndash;442
</p>
<p>for nonlinear relationships, 411&ndash;415
</p>
<p>statistical significance of, 421&ndash;426
</p>
<p>percentage, 48&ndash;49, 56
</p>
<p>equation for, 48&ndash;49, 57
</p>
<p>percent of  variance explained (R2), 453&ndash;455, 467, 
</p>
<p>470. See also eta squared ( 2 ) equation for
</p>
<p>phi ( ), 353, 386
</p>
<p>pie charts, 51&ndash;52, 55&ndash;56
</p>
<p>pooled variance method, 274, 276&ndash;277, 282, 288, 
</p>
<p>294&ndash;295
</p>
<p>equation for, 282
</p>
<p>population, 140
</p>
<p>population distribution, 126, 140
</p>
<p>positivism, 15
</p>
<p>probability distributions, 147&ndash;149, 164
</p>
<p>problem-oriented policing (POP) approach, 173
</p>
<p>proportion, 48, 56
</p>
<p>confidence interval for, 710&ndash;711
</p>
<p>equation for, 48, 56
</p>
<p>pseudo R2, 585&ndash;587, 592
</p>
<p>R
</p>
<p>r. See Pearson&rsquo;s r
</p>
<p>r
s
. See Spearman&rsquo;s r
</p>
<p>R2. See percent of  variance explained
</p>
<p>random coefficient model, 655&ndash;660
</p>
<p>level-2 characteristics, 660&ndash;662
</p>
<p>statistical significance, 636&ndash;637
</p>
<p>random effects, 642&ndash;644
</p>
<p>model, 642&ndash;644
</p>
<p>randomized experiments, 486, 505
</p>
<p>random intercept model, 646&ndash;653
</p>
<p>statistical significance, 648&ndash;649
</p>
<p>random sampling, 175, 189
</p>
<p>range, 102&ndash;103, 116
</p>
<p>rank order test, 334, 338, 339. See also  
</p>
<p>nonparametric tests
</p>
<p>ratio scales, 19&ndash;21, 26
</p>
<p>regression
</p>
<p>approach to ANOVA, 640&ndash;641
</p>
<p>bivariate, 439, 462, 471&ndash;472, 482, 484, 485, 488
</p>
<p>F-test, 467&ndash;470
</p>
<p>regression coefficient (b), 440&ndash;444, 464, 470, 471, 624
</p>
<p>calculation of, 440&ndash;443
</p>
<p>confidence interval for, 715
</p>
<p>correctly estimating, 482&ndash;492
</p>
<p>statistical significance of, 458&ndash;461
</p>
<p>regression error (e), 451&ndash;452, 471
</p>
<p>regression line, 446&ndash;450
</p>
<p>regression model, 449&ndash;450, 469
</p>
<p>multivariate, 482, 490, 492, 494, 505, 605, 622
</p>
<p>predictions in, 445&ndash;449, 603
</p>
<p>Reiss, Albert J., Jr., 6
</p>
<p>rejection region, 182&ndash;186, 188
</p>
<p>in ANOVA, 319&ndash;321
</p>
<p>with chi-square test, 202&ndash;203
</p>
<p>compared to confidence interval, 706&ndash;707
</p>
<p>with Kruskal-Wallis test, 334
</p>
<p>for Pearson&rsquo;s r, 421&ndash;426
</p>
<p>for regression coefficients, 464</p>
<p/>
</div>
<div class="page"><p/>
<p>I N D E X782
</p>
<p>for Spearman&rsquo;s r, 428
</p>
<p>with t-test for dependent samples, 289&ndash;291
</p>
<p>with two-sample t-test, 277&ndash;278
</p>
<p>with z-test for proportions, 252&ndash;257
</p>
<p>relationship
</p>
<p>defining strength of, 328&ndash;331
</p>
<p>linear, 411, 414
</p>
<p>relaxing assumptions, 259, 263
</p>
<p>relevant independent variables, defining, 495
</p>
<p>reliability, 24&ndash;25
</p>
<p>representative sample, 175, 189
</p>
<p>research
</p>
<p>measurement in, 14&ndash;15
</p>
<p>purpose of, 15
</p>
<p>research hypothesis (H
1
), 129&ndash;131, 140,  
</p>
<p>622, 628
</p>
<p>research questions, 129, 139&ndash;140
</p>
<p>equation for, 449, 470
</p>
<p>row marginals, 209
</p>
<p>S
</p>
<p>s. See standard deviation
</p>
<p>sample distribution, 126, 140
</p>
<p>sample statistics, 127&ndash;129, 140
</p>
<p>sampling, 175&ndash;178
</p>
<p>random, 175&ndash;176
</p>
<p>with replacement, 176, 189
</p>
<p>sampling distribution, 147&ndash;149, 164
</p>
<p>for chi-square test, 208&ndash;209
</p>
<p>for comparing sample means, 273&ndash;277
</p>
<p>for Kruskal-Wallis test, 336
</p>
<p>for nonnormal populations, 247&ndash;252
</p>
<p>for proportion, 252&ndash;254
</p>
<p>selecting, 180&ndash;181
</p>
<p>for single-sample t-test, 258&ndash;260
</p>
<p>for Spearman&rsquo;s r, 429
</p>
<p>for t-test for dependent samples, 291
</p>
<p>sampling frame, 176, 189
</p>
<p>scales, measurement, 16, 26
</p>
<p>interval, 19&ndash;21, 26
</p>
<p>mean and, 82
</p>
<p>nominal, 16&ndash;18, 26
</p>
<p>ordinal, 18&ndash;19, 26, 602
</p>
<p>ratio, 19&ndash;21, 26
</p>
<p>scatter diagrams, 411&ndash;415, 431
</p>
<p>scatterplots, 411&ndash;415, 431
</p>
<p>science and measurement, 14&ndash;15
</p>
<p>score test, 622&ndash;623
</p>
<p>separate variance method, 67&ndash;278, 295
</p>
<p>significance criteria, 137&ndash;138
</p>
<p>significance level
</p>
<p>in ANOVA, 319&ndash;320
</p>
<p>with chi-square test, 202&ndash;203, 205
</p>
<p>with Kruskal-Wallis test, 336
</p>
<p>for logistic regression coefficients, 557&ndash;558
</p>
<p>for nominal measures of  association, 365&ndash;367
</p>
<p>for ordinal measures of  association, 381&ndash;383
</p>
<p>for Pearson&rsquo;s r, 426
</p>
<p>for regression coefficients, 462
</p>
<p>and rejection region, 182&ndash;183
</p>
<p>for Spearman&rsquo;s r, 429
</p>
<p>with t-test for dependent samples, 291
</p>
<p>with two-sample t-test, 277&ndash;278
</p>
<p>with z-test for proportions, 244
</p>
<p>skewed, 83&ndash;86, 222
</p>
<p>Somers&rsquo; d, 368&ndash;369, 376, 380, 385, 387&ndash;391
</p>
<p>statistical significance of, 381&ndash;382
</p>
<p>Spearman&rsquo;s correlation coefficient. See Spearman&rsquo;s r
</p>
<p>Spearman&rsquo;s r, 369, 418&ndash;421, 430&ndash;431
</p>
<p>equation for, 419, 431
</p>
<p>statistical significance of, 428&ndash;429
</p>
<p>SPSS, 31&ndash;33
</p>
<p>standard deviation (s), 107&ndash;111, 116
</p>
<p>equation for, 107, 117
</p>
<p>estimate of, 258, 264
</p>
<p>of  population distribution, 239&ndash;243
</p>
<p>of  sampling distribution of  a proportion, 252&ndash;254
</p>
<p>standard deviation sample, 140
</p>
<p>standard deviation unit, 238&ndash;240, 263
</p>
<p>standard error, 243, 252, 263, 605, 612, 613
</p>
<p>standard normal distribution, 239, 263
</p>
<p>Stata, 33&ndash;35
</p>
<p>statistical error, 134&ndash;138
</p>
<p>statistical inference, 129&ndash;138, 611&ndash;615
</p>
<p>statistical power, 728&ndash;730, 748
</p>
<p>components of, 726
</p>
<p>use of  covariates, 693&ndash;695
</p>
<p>statistical significance, 135, 140, 605, 611&ndash;613, 615, 
</p>
<p>619, 621&ndash;622, 625, 627
</p>
<p>test of, 135, 140
</p>
<p>statistics
</p>
<p>descriptive, 7&ndash;9
</p>
<p>faulty, 7
</p>
<p>fear of, 5
</p>
<p>inductive, 7
</p>
<p>inferential, 9&ndash;10
</p>
<p>multivariate, 10&ndash;12
</p>
<p>problem solving with, 4&ndash;5
</p>
<p>purpose of, 3&ndash;4
</p>
<p>techniques of, 5&ndash;6</p>
<p/>
</div>
<div class="page"><p/>
<p>I N D E X 783
</p>
<p>Student&rsquo;s. See t distribution
</p>
<p>studies designed for failure, 729, 747
</p>
<p>sum of  deviations from the mean, equation for, 80, 86
</p>
<p>sums of  squares, 312&ndash;313, 337
</p>
<p>partitioning of, 312&ndash;313
</p>
<p>T
</p>
<p>t distribution, 258&ndash;260, 281, 283, 286&ndash;287, 291, 307, 
</p>
<p>421, 423, 430, 458, 464, 470, 709
</p>
<p>test statistic, 187&ndash;189, 205, 210, 215&ndash;216, 220, 245, 
</p>
<p>257, 260, 281, 627, 628
</p>
<p>in ANOVA, 319, 324
</p>
<p>for chi-square test, 201
</p>
<p>for comparing sample means, 278, 287
</p>
<p>for Kruskal-Wallis test, 335, 337&ndash;340, 345&ndash;346
</p>
<p>for logistic regression coefficients, 564, 567&ndash;568, 
</p>
<p>572, 580, 582&ndash;583, 585, 591, 716
</p>
<p>for Pearson&rsquo;s r, 405, 431
</p>
<p>for regression coefficients, 477
</p>
<p>for sample proportions, 287
</p>
<p>for Spearman&rsquo;s r, 428
</p>
<p>for t-test for dependent samples, 288&ndash;290, 
</p>
<p>294&ndash;295
</p>
<p>for z-test for proportions, 252
</p>
<p>time series data, 52&ndash;53, 56
</p>
<p>time series plot, 53&ndash;56
</p>
<p>tolerance
</p>
<p>equation for, 536, 538
</p>
<p>total sum of  squares ( TSS), 312, 339, 453
</p>
<p>equation for, 312&ndash;314, 322, 330, 340, 468
</p>
<p>t-test
</p>
<p>for dependent samples, 288&ndash;291, 294&ndash;296
</p>
<p>for means, 271, 280&ndash;281, 294, 296
</p>
<p>for ordinal scales, 294
</p>
<p>tails of  the distribution, 183&ndash;184, 189, 244
</p>
<p>two-sample t-test, 271, 277, 280, 284, 285, 294, 295
</p>
<p>equations for, 277, 295, 296
</p>
<p>two-tailed rejection region, 181
</p>
<p>two-tailed test of  significance, 183&ndash;185, 429
</p>
<p>Type I error, 134&ndash;140
</p>
<p>Type II error 1, 134&ndash;140
</p>
<p>U
</p>
<p>unbiased estimate of  standard error equation, 259, 
</p>
<p>277, 295
</p>
<p>unexplained sum of  squares (USS), 313, 337, 339, 
</p>
<p>467&ndash;469, 472
</p>
<p>universe, 127, 140, 176, 189
</p>
<p>unknown population, 252, 254, 259, 262
</p>
<p>V
</p>
<p>Validity, 23&ndash;27, 32, 175&ndash;176, 178, 188&ndash;189,  
</p>
<p>273, 331
</p>
<p>Variable, 16&ndash;17, 19&ndash;22, 31&ndash;32, 613, 631
</p>
<p>Variance, 104&ndash;107, 116
</p>
<p>equation for, 104, 117
</p>
<p>between and within groups, 313
</p>
<p>pooled, 272, 274&ndash;275, 278, 294
</p>
<p>separate, 274, 295
</p>
<p>variation ratio, 98&ndash;100, 115&ndash;117
</p>
<p>variance components, 640&ndash;646
</p>
<p>statistical significance of, 645
</p>
<p>W
</p>
<p>Wald statistic, 588, 589, 612&ndash;614, 619,  
</p>
<p>621, 627
</p>
<p>within effect, 652
</p>
<p>testing of, 653&ndash;654
</p>
<p>within group variance equation, 315, 340
</p>
<p>equation for, 315
</p>
<p>within sum of  squares (WSS), 311, 337, 339
</p>
<p>equation for, 311&ndash;313, 340
</p>
<p>equation for, 313
</p>
<p>X
</p>
<p>X (independent variable), 440&ndash;441, 471
</p>
<p>Y
</p>
<p>Y (dependent variable), 440&ndash;441, 471
</p>
<p>Z
</p>
<p>z-score, 239&ndash;241
</p>
<p>z-test for proportions, 252&ndash;257</p>
<p/>
</div>
<ul>	<li>Contents</li>
	<li>Preface</li>
	<li>A c k n o w l e d g m e n t s</li>
	<li>A b o u t t h e A u t h o r s</li>
	<li>C h a p t e r o n e</li>
<ul>	<li>Introduction: Statistics as a Research Tool</li>
<ul>	<li>T h e P u r p o s e o f S t a t i s t i c s I s t o C l a r i f y</li>
	<li>S t a t i s t i c s A r e U s e d t o S o l v e P r o b l e m s</li>
	<li>B a s i c P r i n c i p l e s A p p l y A c r o s s S t a t i s t i c a l T e c h n i q u e s</li>
<ul>	<li>Descriptive Statistics</li>
</ul>
	<li>T h e U s e s o f S t a t i s t i c s</li>
<ul>	<li>Inferential Statistics</li>
	<li>Taking into Account Competing Explanations: Multivariate Statistics</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
</ul>
</ul>
	<li>C h a p t e r t w o</li>
<ul>	<li>Measurement: The Basic Building Block of Research</li>
	<li>S c i e n c e a n d M e a s u r e m e n t : C l a s s i f i c a t i o n a s a F i r s t S t e p i n R e s e a r c h</li>
	<li>L e v e l s o f M e a s u r e m e n t</li>
<ul>	<li>Nominal Scales</li>
	<li>Ordinal Scales</li>
	<li>Interval and Ratio Scales</li>
</ul>
	<li>R e l a t i n g I n t e r v a l , O r d i n a l , a n d N o m i n a l S c a l e s : T h e I m p o r t a n c e o f C o l l e c t </li>
	<li>W h a t I s a G o o d M e a s u r e ?</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r t h r e e</li>
<ul>	<li>Representing and Displaying Data</li>
<ul>	<li>W h a t A r e F r e q u e n c y D i s t r i b u t i o n s a n d H i s t o g r a m s ?</li>
	<li>E x t e n d i n g H i s t o g r a m s t o M u l t i p l e G r o u p s : U s i n g B a r C h a r t s</li>
	<li>U s i n g B a r C h a r t s w i t h N o m i n a l o r O r d i n a l D a t a</li>
	<li>P i e C h a r t s</li>
	<li>T i m e S e r i e s D a t a</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
</ul>
	<li>C h a p t e r f o u r</li>
<ul>	<li>Describing the Typical Case: Measures of Central Tendency</li>
	<li>T h e M o d e : C e n t r a l T e n d e n c y i n N o m i n a l S c a l e s</li>
	<li>T h e M e d i a n : T a k i n g i n t o A c c o u n t P o s i t i o n</li>
	<li>T h e M e a n : A d d i n g V a l u e t o P o s i t i o n</li>
<ul>	<li>Comparing Results Gained Using the Mean and Median</li>
	<li>Other Characteristics of the Mean</li>
	<li>Using the Mean for Noninterval Scales</li>
</ul>
	<li>S t a t i s t i c s i n P r a c t i c e : C o m p a r i n g t h e M e d i a n a n d t h e M e a n</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
	<li>Recoding Variables</li>
</ul>
</ul>
	<li>C h a p t e r f i v e</li>
<ul>	<li>How Typical Is the Typical Case?: Measuring Dispersion</li>
	<li>M e a s u r e s o f D i s p e r s i o n f o r N o m i n a l a n d O r d i n a l L e v e l D a t a</li>
<ul>	<li>The Proportion in the Modal Category</li>
	<li>The Percentage in the Modal Category</li>
	<li>The Variation Ratio</li>
	<li>Index of Qualitative Variation</li>
</ul>
	<li>M e a s u r i n g D i s p e r s i o n i n I n t e r v a l S c a l e s : T h e R a n g e , V a r i a n c e , a n d S t a n d a r </li>
<ul>	<li>The Variance</li>
	<li>The Standard Deviation</li>
	<li>The Coefficient of Relative Variation</li>
	<li>A Note on the Mean Deviation</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r s i x</li>
<ul>	<li>The Logic of Statistical Inference: Making Statements About Populations from Sample Statistics</li>
	<li>T h e D i l e m m a : M a k i n g S t a t e m e n t s A b o u t P o p u l a t i o n s f r o m S a m p l e S t a t i s t i c s</li>
	<li>T h e R e s e a r c h H y p o t h e s i s</li>
	<li>T h e N u l l H y p o t h e s i s</li>
	<li>R i s k s o f E r r o r i n H y p o t h e s i s T e s t i n g</li>
	<li>R i s k s o f E r r o r a n d S t a t i s t i c a l L e v e l s o f S i g n i f i c a n c e</li>
	<li>D e p a r t i n g f r o m C o n v e n t i o n a l S i g n i f i c a n c e C r i t e r i a</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
</ul>
	<li>C h a p t e r s e v e n</li>
<ul>	<li>Defining the Observed Significance Level of a Test: A Simple Example Using the Binomial Distribution</li>
	<li>T h e F a i r C o i n T o s s</li>
<ul>	<li>Sampling Distributions and Probability Distributions</li>
	<li>The Multiplication Rule</li>
</ul>
	<li>D i f f e r e n t W a y s o f G e t t i n g S i m i l a r R e s u l t s</li>
	<li>S o l v i n g M o r e C o m p l e x P r o b l e m s</li>
	<li>T h e B i n o m i a l D i s t r i b u t i o n</li>
	<li>U s i n g t h e B i n o m i a l D i s t r i b u t i o n t o E s t i m a t e t h e O b s e r v e d S i g n i f i c a n c e L e v </li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r e i g h t</li>
<ul>	<li>Steps in a Statistical Test: Using the Binomial Distribution to Make Decisions About Hypotheses</li>
	<li>T h e P r o b l e m : T h e I m p a c t o f P r o b l e m O r i e n t e d P o l i c i n g o n D i s o r d e r l y A c t i v i t </li>
	<li>A s s u m p t i o n s : L a y i n g t h e F o u n d a t i o n s f o r S t a t i s t i c a l I n f e r e n c e</li>
<ul>	<li>Level of Measurement</li>
	<li>Shape of the Population Distribution</li>
	<li>Sampling Method</li>
	<li>The Hypotheses</li>
	<li>Stating All of the Assumptions</li>
</ul>
	<li>S e l e c t i n g a S a m p l i n g D i s t r i b u t i o n</li>
	<li>S i g n i f i c a n c e L e v e l a n d R e j e c t i o n R e g i o n</li>
<ul>	<li>Choosing a One-Tailed or a Two-Tailed Rejection Region</li>
</ul>
	<li>T h e T e s t S t a t i s t i c</li>
	<li>M a k i n g a D e c i s i o n</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r n i n e</li>
<ul>	<li>Chi-Square: A Test Commonly Used for Nominal-Level Measures</li>
	<li>T e s t i n g H y p o t h e s e s C o n c e r n i n g t h e R o l l o f a D i e</li>
<ul>	<li>The Chi-Square Distribution</li>
	<li>Calculating the Chi-Square Statistic</li>
	<li>Linking the Chi-Square Statistic to Probabilities: The Chi-Square Table</li>
	<li>A Substantive Example: The Relationship Between Assault Victims and Offenders</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>R e l a t i n g T w o N o m i n a l S c a l e M e a s u r e s i n a C h i S q u a r e T e s t</li>
<ul>	<li>A Substantive Example: Type of Sanction and Recidivism Among Convicted White-Collar Criminals</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>E x t e n d i n g t h e C h i S q u a r e T e s t t o M u l t i c a t e g o r y V a r i a b l e s : T h e E x a m p l e o f C e </li>
<ul>	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>E x t e n d i n g t h e C h i S q u a r e T e s t t o a R e l a t i o n s h i p B e t w e e n T w o O r d i n a l V a r i a b l </li>
<ul>	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>T h e U s e o f C h i S q u a r e W h e n S a m p l e s A r e S m a l l : A F i n a l N o t e</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r t e n</li>
<ul>	<li>The Normal Distribution and Its Application to Tests of Statistical Significance</li>
	<li>T h e N o r m a l F r e q u e n c y D i s t r i b u t i o n , o r N o r m a l C u r v e</li>
<ul>	<li>Characteristics of the Normal Frequency Distribution</li>
	<li>Scores</li>
	<li>Developing Tests of Statistical Significance Based on the Standard Normal Distribution: The Single-Sample</li>
	<li>Test for Known Populations</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>A p p l y i n g N o r m a l S a m p l i n g D i s t r i b u t i o n s t o N o n n o r m a l P o p u l a t i o n s</li>
	<li>C o m p a r i n g a S a m p l e t o a n U n k n o w n P o p u l a t i o n : T h e S i n g l e S a m p l e</li>
	<li>T e s t f o r P r o p o r t i o n s</li>
<ul>	<li>Computing the Mean and Standard Deviation for the Sampling Distribution of a Proportion</li>
	<li>Testing Hypotheses with the Normal Distribution: The Case of a New Prison Program</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>C o m p a r i n g a S a m p l e t o a n U n k n o w n P o p u l a t i o n : T h e S i n g l e S a m p l e</li>
	<li>T e s t f o r M e a n s</li>
<ul>	<li>Testing Hypotheses with the</li>
	<li>Distribution</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
</ul>
	<li>C h a p t e r e l e v e n</li>
<ul>	<li>Comparing Means and Proportions in Two Samples</li>
	<li>C o m p a r i n g S a m p l e M e a n s</li>
<ul>	<li>The Case of Anxiety Among Police Officers and Firefighters</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
	<li>Bail in Los Angeles County: Another Example of the Two-Sample</li>
	<li>Test for Means</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>C o m p a r i n g S a m p l e P r o p o r t i o n s : T h e T w o S a m p l e</li>
	<li>T e s t f o r D i f f e r e n c e s o f P r o p o r t i o n s</li>
<ul>	<li>The Case of Drug Testing and Pretrial Misconduct</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>T h e</li>
	<li>T e s t f o r D e p e n d e n t S a m p l e s</li>
<ul>	<li>The Effect of Police Presence Near High-Crime Addresses</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>A N o t e o n U s i n g t h e</li>
	<li>T e s t f o r O r d i n a l S c a l e s</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r t w e l v e</li>
<ul>	<li>Comparing Means Among More Than Two Samples: Analysis of Variance</li>
	<li>A n a l y s i s o f V a r i a n c e</li>
<ul>	<li>Developing Estimates of Variance Between and Within Groups</li>
	<li>Partitioning Sums of Squares</li>
	<li>Developing Estimates of Population Variances</li>
	<li>A Substantive Example: Age and White-Collar Crimes</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
	<li>Another ANOVA Example: Race and Bail Amounts Among Felony Drug Defendants</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>D e f i n i n g t h e S t r e n g t h o f t h e R e l a t i o n s h i p O b s e r v e d</li>
	<li>M a k i n g P a i r w i s e C o m p a r i s o n s B e t w e e n t h e G r o u p s S t u d i e d</li>
	<li>A N o n p a r a m e t r i c A l t e r n a t i v e : T h e K r u s k a l W a l l i s T e s t</li>
<ul>	<li>Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r t h i r t e e n</li>
<ul>	<li>Measures of Association for Nominal and Ordinal Variables</li>
	<li>D i s t i n g u i s h i n g S t a t i s t i c a l S i g n i f i c a n c e a n d S t r e n g t h o f R e l a t i o n s h i p : T </li>
	<li>M e a s u r e s o f A s s o c i a t i o n f o r N o m i n a l V a r i a b l e s</li>
<ul>	<li>Measures of Association Based on the Chi-Square Statistic</li>
	<li>Proportional Reduction in Error Measures: Tau and Lambda</li>
	<li>Statistical Significance of Measures of Association for Nominal Variables</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>M e a s u r e s o f A s s o c i a t i o n f o r O r d i n a l L e v e l V a r i a b l e s</li>
<ul>	<li>Gamma</li>
	<li>Kendall&rsquo;s</li>
	<li>and</li>
	<li>Somers&rsquo;</li>
	<li>A Substantive Example: Affectional Identification with Father and Level of Delinquency</li>
	<li>Note on the Use of Measures of Association for Ordinal Variables</li>
	<li>Statistical Significance of Measures of Association for Ordinal Variables</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>C h o o s i n g t h e B e s t M e a s u r e o f A s s o c i a t i o n f o r N o m i n a l a n d O r d i n a l L e v e l V a r i </li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r f o u r t e e n</li>
<ul>	<li>Measuring Association for Interval-Level Data: Pearson&rsquo;s Correlation Coefficient</li>
	<li>M e a s u r i n g A s s o c i a t i o n B e t w e e n T w o I n t e r v a l L e v e l V a r i a b l e s</li>
	<li>P e a r s o n &rsquo; s C o r r e l a t i o n C o e f f i c i e n t</li>
<ul>	<li>The Calculation</li>
	<li>A Substantive Example: Crime and Unemployment in California</li>
	<li>Nonlinear Relationships and Pearson&rsquo;s</li>
	<li>Beware of Outliers</li>
</ul>
	<li>S p e a r m a n &rsquo; s C o r r e l a t i o n C o e f f i c i e n t</li>
	<li>T e s t i n g t h e S t a t i s t i c a l S i g n i f i c a n c e o f P e a r s o n &rsquo; s</li>
<ul>	<li>Statistical Significance of</li>
	<li>: The Case of Age and Number of Arrests</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
	<li>Statistical Significance of</li>
	<li>Unemployment and Crime in California</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>T e s t i n g t h e S t a t i s t i c a l S i g n i f i c a n c e o f S p e a r m a n &rsquo; s</li>
<ul>	<li>The Sampling Distribution</li>
	<li>Significance Level and Critical Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r f i f t e e n</li>
<ul>	<li>An Introduction to Bivariate Regression</li>
	<li>E s t i m a t i n g t h e I n f l u e n c e o f O n e V a r i a b l e o n A n o t h e r : T h e R e g r e s s i o n C o e f f i </li>
<ul>	<li>Calculating the Regression Coefficient</li>
	<li>A Substantive Example: Unemployment and Burglary in California</li>
</ul>
	<li>P r e d i c t i o n i n R e g r e s s i o n : B u i l d i n g t h e R e g r e s s i o n L i n e</li>
<ul>	<li>The</li>
	<li>Intercept</li>
	<li>The Regression Line</li>
	<li>Predictions Beyond the Distribution Observed in a Sample</li>
	<li>Predicting Burglary Rates from Unemployment Rates in California</li>
	<li>Choosing the Best Line of Prediction Based on Regression Error</li>
</ul>
	<li>E v a l u a t i n g t h e R e g r e s s i o n M o d e l</li>
<ul>	<li>Percent of Variance Explained</li>
	<li>Percent of Variance Explained: Unemployment Rates and Burglary Rates in California</li>
	<li>Statistical Significance of the Regression Coefficient: The Case of Age and Number of Arrests</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
	<li>Testing the Statistical Significance of the Regression Coefficient for Unemployment Rates and Burglary Rates in California</li>
	<li>The Sampling Distribution</li>
	<li>Significance Level and Rejection Region</li>
	<li>The Test Statistic</li>
	<li>The Decision</li>
</ul>
	<li>T h e</li>
	<li>T e s t f o r t h e O v e r a l l R e g r e s s i o n</li>
<ul>	<li>Age and Number of Arrests</li>
	<li>Unemployment Rates and Burglary Rates in California</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r s i x t e e n</li>
<ul>	<li>Multivariate Regression</li>
	<li>T h e I m p o r t a n c e o f C o r r e c t M o d e l S p e c i f i c a t i o n s</li>
<ul>	<li>Errors in Prediction</li>
	<li>Correctly Estimating the Effect of</li>
	<li>Comparing Regression Coefficients Within a Single Model: The Standardized Regression Coefficient</li>
</ul>
	<li>C o r r e c t l y S p e c i f y i n g t h e R e g r e s s i o n M o d e l</li>
<ul>	<li>Defining Relevant Independent Variables</li>
	<li>Taking into Account Ordinaland Nominal-Scale Measures in a Multivariate Regression</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
	<li>C h a p t e r s e v e n t e e n</li>
<ul>	<li>Multivariate Regression: Additional Topics</li>
<ul>	<li>N o n l i n e a r R e l a t i o n s h i p s</li>
<ul>	<li>Finding a Non-linear Relationship: Graphical Assessment</li>
	<li>Incorporating Non-linear Relationships into an OLS Model</li>
<ul>	<li>Interpreting Non-linear Coefficients</li>
	<li>Note on Statistical Significance</li>
</ul>
	<li>Summary</li>
</ul>
	<li>I n t e r a c t i o n E f f e c t s</li>
<ul>	<li>Interaction of a Dummy Variable and Interval-Level Variable</li>
</ul>
	<li>A n E x a m p l e : R a c e a n d P u n i s h m e n t S e v e r i t y</li>
<ul>	<li>Interaction Effects between Two Interval-level Variables</li>
</ul>
	<li>A n E x a m p l e : P u n i s h m e n t S e v e r i t y</li>
	<li>T h e P r o b l e m o f M u l t i c o l l i n e a r i t y</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y Te r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
</ul>
	<li>C h a p t e r e i g h t e e n</li>
<ul>	<li>Logistic Regression</li>
<ul>	<li>W h y i s I t I n a p p r o p r i a t e t o U s e O L S R e g r e s s i o n f o r a D i c h o t o m o u s D e p e n d e n t V a r i a b l e ?</li>
	<li>L o g i s t i c R e g r e s s i o n</li>
<ul>	<li>A Substantive Example: Adoption of Compstat in U.S. Police Agencies</li>
</ul>
	<li>I n t e r p r e t i n g L o g i s t i c R e g r e s s i o n C o e f f i c i e n t s</li>
<ul>	<li>The Odds Ratio</li>
	<li>The Derivative at Mean</li>
</ul>
	<li>C o m p a r i n g L o g i s t i c R e g r e s s i o n C o e f f i c i e n t s</li>
<ul>	<li>Using Probability Estimates to Compare Coefficients</li>
	<li>&ldquo;Standardized&rdquo; Logistic Regression Coefficients</li>
</ul>
	<li>E v a l u a t i n g t h e L o g i s t i c R e g r e s s i o n M o d e l</li>
<ul>	<li>Percent of Correct Predictions</li>
	<li>Pseudo R2</li>
</ul>
	<li>S t a t i s t i c a l S i g n i f i c a n c e i n L o g i s t i c R e g r e s s i o n</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
</ul>
</ul>
</ul>
	<li>Ch a p t e r n i n e t e e n</li>
<ul>	<li>Multivariate Regression with Multiple Category Nominal or Ordinal Measures: Extending the Basic Logistic Regression Model</li>
	<li>M u l t i n o m i a l L o g i s t i c R e g r e s s i o n</li>
<ul>	<li>A Substantive Example: Case Dispositions in California</li>
	<li>Probability Equation</li>
	<li>Odds Ratio Equation</li>
	<li>The Missing Set of Coefficients</li>
	<li>Statistical Inference</li>
<ul>	<li>Single Coefficients</li>
	<li>Multiple Coefficients</li>
	<li>Overall Model</li>
</ul>
	<li>O r d i n a l L o g i s t i c R e g r e s s i o n</li>
<ul>	<li>Cumulative Probability</li>
	<li>Odds Ratio Using Cumulative Probabilities</li>
	<li>Ordinal Logit Equation</li>
	<li>Interpretation of Ordinal Logistic Regression Coefficients</li>
</ul>
	<li>S u b s t a n t i v e E x a m p l e : S e v e r i t y o f P u n i s h m e n t D e c i s i o n s</li>
<ul>	<li>Interpreting the Coefficients</li>
	<li>Statistical Significance</li>
	<li>Parallel Slopes Tests</li>
	<li>Score Test</li>
	<li>Brant Test</li>
	<li>Partial Proportional Odds</li>
	<li>Severity of Punishment Example</li>
</ul>
	<li>C h a p t e r  S u m m a r y</li>
	<li>K e y Te r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
<ul>	<li>Problems</li>
</ul>
</ul>
</ul>
</ul>
	<li>C h a p t e r t w e n t y</li>
<ul>	<li>Multilevel Regression Models</li>
<ul>	<li>V a r i a n c e C o m p o n e n t s M o d e l</li>
<ul>	<li>A Substantive Example: Bail Decision-Making Study</li>
	<li>Fixed and Random Effects</li>
	<li>Intraclass Correlation and Explained Variance</li>
	<li>Statistical Significance</li>
	<li>Bail Decision-Making Study</li>
</ul>
	<li>R a n d o m I n t e r c e p t M o d e l</li>
<ul>	<li>Explained Variance</li>
	<li>Statistical Significance</li>
	<li>Centering Independent Variables</li>
	<li>Bail Decision-Making Study</li>
	<li>Between and Within Effects</li>
	<li>Testing for Between and Within Effects</li>
</ul>
	<li>R a n d o m C o e f f i c i e n t M o d e l</li>
<ul>	<li>Variance Estimates</li>
	<li>Note on Explained Variance</li>
	<li>Bail Decision-Making Study</li>
</ul>
	<li>A d d i n g C l u s t e r ( L e v e l 2 ) C h a r a c t e r i s t i c s</li>
<ul>	<li>A Substantive Example: Race and Sentencing Across Pennsylvania Counties</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y Te r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
<ul>	<li>Random Intercept Models</li>
	<li>Random Coefficient Models</li>
	<li>Problems</li>
</ul>
</ul>
</ul>
</ul>
	<li>C h a p t e r t w e n t y o n e</li>
<ul>	<li>Special Topics: Randomized Experiments</li>
<ul>	<li>T h e S t r u c t u r e o f a R a n d o m i z e d E x p e r i m e n t</li>
	<li>T h e M a i n A d v a n t a g e o f E x p e r i m e n t s : I s o l a t i n g C a u s a l E f f e c t s</li>
	<li>n t e r n a l V a l i d i t y</li>
	<li>S a m p l e S i z e , E q u i v a l e n c e , a n d S t a t i s t i c a l P o w e r</li>
<ul>	<li>Statistical Foundations for Block Randomization</li>
	<li>An Example: Jersey City Experiment</li>
	<li>The Benefits of Block Randomized Trials</li>
</ul>
	<li>S t a t i s t i c a l P o w e r a n d B l o c k R a n d o m i z a t i o n</li>
	<li>U s i n g C o v a r i a t e s t o I n c r e a s e S t a t i s t i c a l P o w e r i n E x p e r i m e n t a l S t u d i e s</li>
	<li>E x a m i n i n g I n t e r a c t i o n T e r m s i n E x p e r i m e n t a l R e s e a r c h</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y Te r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
</ul>
</ul>
	<li>C h a p t e r t w e n t y t w o</li>
<ul>	<li>Special Topics: Confidence Intervals</li>
<ul>	<li>C o n f i d e n c e I n t e r v a l s</li>
	<li>C o n s t r u c t i n g C o n f i d e n c e I n t e r v a l s</li>
<ul>	<li>Confidence Intervals for Sample Means</li>
	<li>Confidence Intervals for Sample Proportions</li>
	<li>Confidence Intervals for a Difference of Sample Means</li>
	<li>Confidence Intervals for Pearson&rsquo;s Correlation Coefficient, r</li>
	<li>Confidence Intervals for Regression Coefficients</li>
	<li>Confidence Intervals for Logistic Regression Coefficients and Odds Ratios</li>
</ul>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y T e r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>E x e r c i s e s</li>
	<li>Computer Exercises</li>
<ul>	<li>SPSS</li>
	<li>Stata</li>
<ul>	<li>Problems</li>
</ul>
</ul>
</ul>
</ul>
	<li>C h a p t e r t w e n t y t h r e e</li>
<ul>	<li>Special Topics: Statistical Power</li>
<ul>	<li>S t a t i s t i c a l P o w e r</li>
<ul>	<li>Setting the Level of Statistical Power</li>
</ul>
	<li>C o m p o n e n t s o f S t a t i s t i c a l P o w e r</li>
<ul>	<li>Statistical Significance and Statistical Power</li>
<ul>	<li>Directional Hypotheses</li>
</ul>
	<li>Sample Size and Statistical Power</li>
	<li>Effect Size and Statistical Power</li>
	<li>E s t i m a t i n g S t a t i s t i c a l P o w e r a n d S a m p l e S i z e f o r a S t a t i s t i c a l l y P o w e r f u l S t u d y</li>
<ul>	<li>Difference of Means Test</li>
	<li>ANOVA</li>
	<li>Correlation</li>
	<li>Least-Squares Regression</li>
</ul>
	<li>S u m m i n g U p : A v o i d i n g S t u d i e s D e s i g n e d f o r F a i l u r e</li>
	<li>C h a p t e r S u m m a r y</li>
	<li>K e y Te r m s</li>
	<li>S y m b o l s a n d F o r m u l a s</li>
	<li>Computer Exercises</li>
<ul>	<li>Stata</li>
<ul>	<li>Two-Sample Difference of Means Test</li>
	<li>ANOVA</li>
</ul>
	<li>Correlation</li>
<ul>	<li>OLS Regression</li>
	<li>Problems</li>
</ul>
</ul>
</ul>
</ul>
</ul>
	<li>Appendix 1</li>
<ul>	<li>Factorials</li>
</ul>
	<li>Appendix 2</li>
<ul>	<li>Critical Values of X2 Distribution</li>
</ul>
	<li>Appendix 3</li>
<ul>	<li>Areas of the Standard Normal Distribution</li>
</ul>
	<li>Appendix 4</li>
<ul>	<li>Critical Values of Student&rsquo;s t Distribution</li>
</ul>
	<li>Appendix 5</li>
<ul>	<li>Critical Values of the F-Statistic</li>
</ul>
	<li>Appendix 6</li>
<ul>	<li>Critical Value for P (Pcrit), Tukey&rsquo;s HSD Test</li>
</ul>
	<li>Appendix 7</li>
<ul>	<li>Critical Values for Spearman&rsquo;s Rank-Order Correlation Coefficient</li>
</ul>
	<li>Appendix 8</li>
<ul>	<li>Fisher r-to-Z* Transformation</li>
</ul>
	<li>Glossary</li>
	<li>I n d e x</li>
</ul>
</body></html>