<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
</div>
<div class="page"><p/>
<p>Statistical Mechanics for Engineers</p>
<p/>
</div>
<div class="page"><p/>
<p>Isamu Kusaka
</p>
<p>Statistical Mechanics
for Engineers</p>
<p/>
</div>
<div class="page"><p/>
<p>Isamu Kusaka
</p>
<p>Dept. of Chemical and Biomolecular Eng.
</p>
<p>The Ohio State University
</p>
<p>Columbus
</p>
<p>Ohio
</p>
<p>USA
</p>
<p>Additional material to this book can be downloaded from http://extras.springer.com.
</p>
<p>ISBN 978-3-319-13809-1 ISBN 978-3-319-15018-5 (eBook)
</p>
<p>DOI 10.1007/978-3-319-13809-1
</p>
<p>Library of Congress Control Number: 2015930827
</p>
<p>Springer Cham Heidelberg New York Dordrecht London
c&copy; Springer International Publishing Switzerland 2015
</p>
<p>This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.springer.com)</p>
<p/>
</div>
<div class="page"><p/>
<p>To the memory of the late Professor Kazumi
</p>
<p>Nishioka, to whom I owe everything I know
</p>
<p>about thermodynamics.</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>The purpose of writing this book is to explain basic concepts of equilibrium statis-
</p>
<p>tical mechanics to the first year graduate students in engineering departments. Why
</p>
<p>should an engineer care about statistical mechanics?
</p>
<p>Historically, statistical mechanics evolved out of the desire to explain thermo-
</p>
<p>dynamics from fundamental laws of physics governing behavior of atoms and
</p>
<p>molecules. If a microscopic interpretation of the laws of thermodynamics were the
</p>
<p>only outcome of this branch of science, statistical mechanics would not appeal to
</p>
<p>those of us who simply wish to use thermodynamics to perform practical calcula-
</p>
<p>tions. After all, validity of thermodynamics has long been established.
</p>
<p>In thermodynamics, a concept of fundamental equations plays a prominent role.
</p>
<p>From one such equation many profound predictions follow in a completely general
</p>
<p>fashion. However, thermodynamics itself does not predict the explicit form of this
</p>
<p>function. Instead, the fundamental equation must be determined empirically for each
</p>
<p>system of our interest. Being a science built on a set of macroscopic observations,
</p>
<p>thermodynamics does not offer any systematic way of incorporating molecular level
</p>
<p>information, either. Thus, an approach based solely on thermodynamics is not suf-
</p>
<p>ficient if we hope to achieve desired materials properties through manipulation of
</p>
<p>nanoscale features and/or molecular level architecture of materials.
</p>
<p>It is in this context that the method of statistical mechanics becomes important for
</p>
<p>us. Equilibrium statistical mechanics provides a general framework for constructing
</p>
<p>the fundamental equation from a molecular level description of the system of inter-
</p>
<p>est. It can also provide a wealth of molecular level insights that is otherwise inac-
</p>
<p>cessible even experimentally. As such, it is becoming increasingly more relevant to
</p>
<p>engineering problems, requiring majority of engineering students to develop more
</p>
<p>than just a passing acquaintance with the basic results of this subject.
</p>
<p>Because statistical mechanics is built on the basis of classical and quantum
</p>
<p>mechanics, some elementary knowledge of these subjects proves essential in order
</p>
<p>to access the existing textbooks on statistical mechanics in any meaningful manner.
</p>
<p>However, these subjects fall outside the expected background of engineering stu-
</p>
<p>dents. For some, these subjects are entirely foreign. This book is meant to fill in the
</p>
<p>gap felt by such students, who need to efficiently absorb only those essential back-
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Preface
</p>
<p>grounds necessary to understand the basic ideas of statistical mechanics and quickly
</p>
<p>move onto more specific topics of their own interest.
</p>
<p>My intention, therefore, is not to replace many excellent textbooks on statistical
</p>
<p>mechanics that exist today, but to ease the transition into such textbooks. Thus, I did
</p>
<p>not try to showcase various applications of statistical mechanics, of which there are
</p>
<p>many. Instead, the emphasis is on making the basic ideas of statistical mechanics
</p>
<p>accessible to the intended audience. The end result is this book, serving as a gen-
</p>
<p>tle introduction to the subject. By the end of this book, however, you will be well
</p>
<p>positioned to read more advanced textbooks including those with more specialized
</p>
<p>themes, some of which are listed in Appendix E.
</p>
<p>In this book, I have chosen to present classical mechanical formulation of statis-
</p>
<p>tical mechanics. This is somewhat contrary to the prevailing wisdom that favors the
</p>
<p>mathematical simplicity of quantum statistical mechanics: Microstates in quantum
</p>
<p>mechanical systems can be counted, at least in principle. This is not so in clas-
</p>
<p>sical mechanical systems even in principle. However, relevant concepts in quan-
</p>
<p>tum mechanics are far more abstract than those in classical mechanics and a proper
</p>
<p>understanding of the former requires that of the latter. A common compromise is
</p>
<p>to simply accept the discrete energy spectrum of bound quantum states. But, this
</p>
<p>leaves a rather uncomfortable gap in the students&rsquo; knowledge. No less important is
</p>
<p>the fact that many applications of statistical mechanics in engineering problems take
</p>
<p>place within essentially the classical framework even though the fundamental laws
</p>
<p>of physics dictating the behavior of atoms are quantum mechanical in nature.
</p>
<p>It seemed inappropriate to use a symbol that is either very different from estab-
</p>
<p>lished conventions or far detached from the meaning it is supposed to represent. The
</p>
<p>alternative has an unfortunate consequence that multiple meanings had to be given
</p>
<p>to a single symbol on occasion. In such cases, the context should always dictate what
</p>
<p>is meant by the particular symbol in question. In this regard, notation is no different
</p>
<p>from an ordinary language. To minimize a possible confusion, a list of frequently
</p>
<p>used symbols is provided at the end of each chapter.
</p>
<p>Suggestions Before You Start
</p>
<p>A prior exposure to undergraduate level thermodynamics will be very helpful as it
</p>
<p>provides you with a sense of direction throughout our journey ahead. I will also
</p>
<p>assume that you have a working knowledge of calculus. Specifically, you should
</p>
<p>know how to evaluate derivatives and integrals of functions of multiple variables. In
</p>
<p>case you need to regain some of these skills, I tried to include as much calculational
</p>
<p>details as reasonable. However, calculus is a perishable skill and a constant practice
</p>
<p>is essential in maintaining a certain level of proficiency. More importantly, &ldquo;Mathe-
</p>
<p>matics is a language in which the physical world speaks to us.&rdquo;1 That is, you cannot
</p>
<p>expect to understand the subject without penetrating through certain manipulative
</p>
<p>aspects first. I made no attempt to conceal this fact. It is up to you to fill in the
</p>
<p>missing steps of the calculations with a stack of papers and a pencil on your side.</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface ix
</p>
<p>Though this will not change the content of any given equation, it will profoundly
</p>
<p>change your relationship to that equation.
</p>
<p>To help you learn new concepts, exercises are scattered throughout the book.
</p>
<p>Keeping with the above stated goal of this book, most of them require only a fairly
</p>
<p>straightforward (I hope) manipulation of equations and applications of concepts just
</p>
<p>learned. The primary reward for solving these problems is not the final answer per se
</p>
<p>but the perspective you gain from working through them. You are strongly urged to
</p>
<p>attempt as many of them as possible. So that the exercises will not become an undue
</p>
<p>hindrance to your progress, hints are given to a subset of them in Appendix G.
</p>
<p>The materials covered in Chap. 1 through Chap. 5 form the core of this book and
</p>
<p>should be sufficient if you want to transition to more advanced textbooks as quickly
</p>
<p>as possible. Chapters 6 and 7 are concerned with thermodynamics and statistical
</p>
<p>mechanics of inhomogeneous fluids. So that you would not have to feel uncomfort-
</p>
<p>able when consulting existing textbooks on statistical mechanics, Chap. 8 introduces
</p>
<p>key concepts from quantum mechanics and briefly illustrates their application in for-
</p>
<p>mulating statistical mechanics.
</p>
<p>In the main body of the book, you will notice that some section headings bear a
</p>
<p>dagger (&dagger;). These sections are aimed at exploring issues prompted by questions from
students in my graduate level courses on thermodynamics and statistical mechanics.
</p>
<p>Some of them provide detailed derivations of key results that are simply quoted in
</p>
<p>undaggered sections. Sections marked with double dagger (&Dagger;) cover materials that
are considered standard by many, including myself. But, they can be brushed aside
</p>
<p>in view of our immediate goal. These optional sections were retained in the hope that
</p>
<p>they may spice up your journey through the book. Some are retained as a modest
</p>
<p>attempt at completeness. If you are pressed for time, or simply do not want to bother
</p>
<p>with them at this time, you can omit them without any guilt or loss of continuity until
</p>
<p>your curiosity compels you otherwise. On occasion, I do use certain results from
</p>
<p>these optional sections, but only with an explicit reference to the relevant sections.
</p>
<p>You can either choose to read the indicated section at that time, or simply accept the
</p>
<p>results quoted and move on. After all, learning is an iterative process and there is no
</p>
<p>need to absorb everything in your first attempt at the subject. Enjoy!
</p>
<p>October 2014 Isamu Kusaka
</p>
<p>kusaka.2@osu.edu</p>
<p/>
</div>
<div class="page"><p/>
<p>Acknowledgments
</p>
<p>This book grew out of my graduate level courses on thermodynamics and statistical
</p>
<p>mechanics. Despite many inadequacies in earlier versions of the lecture notes, many
</p>
<p>students endured and enjoyed the experience. Intellectual curiosity and a sense of
</p>
<p>excitement they continued to express were the greatest driving force in my effort to
</p>
<p>explain the subject as clearly as I could.
</p>
<p>Prof. Zhen-Gang Wang kindly allowed me to include one of his homework prob-
</p>
<p>lems as an example in this book. The first three sections of Appendix A owe their
</p>
<p>organization to the undergraduate level course on fluid mechanics taught by Prof.
</p>
<p>Martin Feinberg. Ms. Yensil Park, Mr. Nicholas Liesen, and Mr. Robert Gammon
</p>
<p>Pitman, who were among the students in my course, found many persistent typos
</p>
<p>in a later version of the manuscript. Dr. Shunji Egusa read through a large portion
</p>
<p>of the manuscript and made many helpful suggestions to improve its overall read-
</p>
<p>ability. The ultimate responsibility, of course, resides with me. I am also indebted
</p>
<p>to Dr. Kenneth Howell and Ms. Abira Sengupta at Springer for their excellent sup-
</p>
<p>port and strong commitment to this project. On a personal note, my wife gracefully
</p>
<p>endured my long held obsession with this work.
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Classical Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.1 Inertial Frame . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.2 Mechanics of a Single Particle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
</p>
<p>1.2.1 Newton&rsquo;s Equation of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . 2
</p>
<p>1.2.2 Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
</p>
<p>1.2.3 Potential Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
</p>
<p>1.2.4 Kinetic Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
</p>
<p>1.2.5 Conservation of Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
</p>
<p>1.3 Mechanics of Many Particles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
</p>
<p>1.3.1 Mechanical Energy of a Many-Particle System . . . . . . . . . . . 12
</p>
<p>1.4 Center of Mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>1.5 Hamilton&rsquo;s Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
</p>
<p>1.5.1 Lagrangian and Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
</p>
<p>1.5.2 Generalized Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
</p>
<p>1.5.3 Many Mechanical Degrees of Freedom . . . . . . . . . . . . . . . . . . 20
</p>
<p>1.6 Momentum and Energy: Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
</p>
<p>1.7 &dagger;Energy Function and Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
1.8 Conservation Laws and Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
</p>
<p>1.8.1 Conservation of Linear Momentum . . . . . . . . . . . . . . . . . . . . . 29
</p>
<p>1.8.2 &Dagger;Conservation of Angular Momentum . . . . . . . . . . . . . . . . . . . 31
1.8.3 Conservation of Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
</p>
<p>1.9 Hamilton&rsquo;s Equations of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
</p>
<p>1.10 &dagger;Routhian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1.11 Poisson Bracket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
</p>
<p>1.12 Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
</p>
<p>References and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
</p>
<p>2 Thermodynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
</p>
<p>2.1 The First Law of Thermodynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
</p>
<p>2.2 Quantifying Heat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
</p>
<p>2.3 &Dagger;A Typical Expression for d̄W . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>xiii</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Contents
</p>
<p>2.4 The Second Law of Thermodynamics . . . . . . . . . . . . . . . . . . . . . . . . . . 48
</p>
<p>2.5 Equilibrium of an Isolated System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
</p>
<p>2.6 Fundamental Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
</p>
<p>2.6.1 Closed System of Fixed Composition . . . . . . . . . . . . . . . . . . . 51
</p>
<p>2.6.2 Open System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>2.6.3 Heat Capacities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
</p>
<p>2.6.4 &Dagger;Ideal Gas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.6.5 &dagger;Heat Flow into an Open System . . . . . . . . . . . . . . . . . . . . . . . 59
</p>
<p>2.7 Role of Additional Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>2.8 Entropy Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
</p>
<p>2.8.1 Condition of Equilibrium. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
</p>
<p>2.8.2 Equality of Temperature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
</p>
<p>2.8.3 Direction of a Spontaneous Process . . . . . . . . . . . . . . . . . . . . . 66
</p>
<p>2.8.4 &dagger;Very Short Remark on the Stability of Equilibrium . . . . . . . 67
2.9 Energy Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
</p>
<p>2.9.1 Condition of Equilibrium. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
</p>
<p>2.9.2 Reversible Work Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
</p>
<p>2.9.3 &dagger;Condition of Perfect Equilibrium . . . . . . . . . . . . . . . . . . . . . . 72
2.9.4 &dagger;Closed System with a Chemical Reaction . . . . . . . . . . . . . . . 75
2.9.5 &dagger;Maximum Work Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
</p>
<p>2.10 Euler Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
</p>
<p>2.11 Gibbs&ndash;Duhem Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>2.12 &Dagger;Gibbs Phase Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
2.13 Free Energies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
</p>
<p>2.13.1 Fixing Temperature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
</p>
<p>2.13.2 Condition of Equilibrium. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
</p>
<p>2.13.3 Direction of a Spontaneous Process . . . . . . . . . . . . . . . . . . . . . 89
</p>
<p>2.13.4 &dagger;W rev and a Spontaneous Process . . . . . . . . . . . . . . . . . . . . . . . 90
2.13.5 Fundamental Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
</p>
<p>2.13.6 Other Free Energies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
</p>
<p>2.14 &Dagger;Maxwell Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
2.15 &Dagger;Partial Molar Quantities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
2.16 Graphical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
</p>
<p>2.16.1 Pure Systems: F Versus V (Constant T ) . . . . . . . . . . . . . . . . . 101
</p>
<p>2.16.2 Binary Mixtures: G Versus x1 (Constant T and P) . . . . . . . . 103
</p>
<p>2.17 Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
</p>
<p>References and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>3 Classical Statistical Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
</p>
<p>3.1 Macroscopic Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
</p>
<p>3.2 Phase Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
</p>
<p>3.3 Ensemble Average . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
</p>
<p>3.4 Statistical Equilibrium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
</p>
<p>3.5 Statistical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
</p>
<p>3.6 Liouville&rsquo;s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xv
</p>
<p>3.7 Significance of H . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
</p>
<p>3.8 &dagger;The Number of Constants of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.9 Canonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>3.10 Simple Applications of Canonical Ensemble . . . . . . . . . . . . . . . . . . . . 129
</p>
<p>3.10.1 Rectangular Coordinate System . . . . . . . . . . . . . . . . . . . . . . . . 129
</p>
<p>3.10.2 Equipartition Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
</p>
<p>3.10.3 Spherical Coordinate System . . . . . . . . . . . . . . . . . . . . . . . . . . 134
</p>
<p>3.10.4 &dagger;General Equipartition Theorem. . . . . . . . . . . . . . . . . . . . . . . . 137
3.11 Canonical Ensemble and Thermal Contact . . . . . . . . . . . . . . . . . . . . . . 137
</p>
<p>3.12 Corrections from Quantum Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . 141
</p>
<p>3.12.1 A System of Identical Particles . . . . . . . . . . . . . . . . . . . . . . . . . 141
</p>
<p>3.12.2 Implication of the Uncertainty Principle . . . . . . . . . . . . . . . . . 144
</p>
<p>3.12.3 Applicability of Classical Statistical Mechanics . . . . . . . . . . . 147
</p>
<p>3.13 &dagger;A Remark on the Statistical Approach . . . . . . . . . . . . . . . . . . . . . . . . 148
3.14 &Dagger;Expressions for P and &micro; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
</p>
<p>3.14.1 &Dagger;Pressure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
3.14.2 &Dagger;Chemical Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
</p>
<p>3.15 &dagger;Internal Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
3.15.1 &dagger;Equilibrium in Motion? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
3.15.2 &dagger;From a Stationary to a Moving Frame . . . . . . . . . . . . . . . . . . 157
3.15.3 &dagger;Back to the Stationary Frame . . . . . . . . . . . . . . . . . . . . . . . . . 160
</p>
<p>3.16 &dagger;Equilibrium of an Accelerating Body . . . . . . . . . . . . . . . . . . . . . . . . . 162
3.16.1 &dagger;Linear Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
3.16.2 &dagger;Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
</p>
<p>3.17 Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
</p>
<p>References and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
</p>
<p>4 Various Statistical Ensembles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
</p>
<p>4.1 Fluctuations in a Canonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . 169
</p>
<p>4.2 Microcanonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
</p>
<p>4.2.1 Expression for ρ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
4.2.2 Choice of ∆E . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
4.2.3 Isolated System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
</p>
<p>4.3 Phase Integral in Microcanonical Ensemble . . . . . . . . . . . . . . . . . . . . . 175
</p>
<p>4.4 &dagger;Adiabatic Reversible Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.5 Canonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
</p>
<p>4.5.1 Closed System Held at a Constant Temperature . . . . . . . . . . . 181
</p>
<p>4.5.2 Canonical Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
</p>
<p>4.5.3 Classical Canonical Partition Function . . . . . . . . . . . . . . . . . . 189
</p>
<p>4.5.4 Applicability of Canonical Ensemble . . . . . . . . . . . . . . . . . . . . 189
</p>
<p>4.6 &Dagger;Isothermal&ndash;Isobaric Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
4.7 Grand Canonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
</p>
<p>4.8 Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
</p>
<p>References and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Contents
</p>
<p>5 Simple Models of Adsorption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
</p>
<p>5.1 Exact Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
</p>
<p>5.1.1 Single Site . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
</p>
<p>5.1.2 &dagger;Binding Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
5.1.3 Multiple Independent Sites . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
</p>
<p>5.1.4 Four Sites with Interaction Among Particles . . . . . . . . . . . . . . 211
</p>
<p>5.2 Mean-Field Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
</p>
<p>5.2.1 Four Sites with Interaction Among Particles . . . . . . . . . . . . . . 214
</p>
<p>5.2.2 Two-Dimensional Lattice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
</p>
<p>5.3 Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
</p>
<p>Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
</p>
<p>6 Thermodynamics of Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
</p>
<p>6.1 Interfacial Region . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
</p>
<p>6.2 Defining a System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
</p>
<p>6.3 Condition of Equilibrium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
</p>
<p>6.3.1 &dagger;Variations in the State of the System . . . . . . . . . . . . . . . . . . . 233
6.3.2 Fixed System Boundaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
</p>
<p>6.3.3 Reference System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>6.3.4 Movable System Boundaries . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
</p>
<p>6.3.5 Laplace Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>6.4 Euler Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
</p>
<p>6.5 Gibbs&ndash;Adsorption Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
</p>
<p>6.6 Flat Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
</p>
<p>6.7 W rev as a Measure of Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
</p>
<p>6.7.1 Exact Expression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
</p>
<p>6.7.2 &Dagger;Classical Theory Approximations . . . . . . . . . . . . . . . . . . . . . . 247
6.7.3 &dagger;Thermodynamic Degrees of Freedom . . . . . . . . . . . . . . . . . . 248
6.7.4 Small Nucleus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
</p>
<p>6.8 &dagger;Gibbs&ndash;Tolman&ndash;Koenig Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
6.9 &dagger;Interfacial Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
6.10 Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
</p>
<p>References and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
</p>
<p>7 Statistical Mechanics of Inhomogeneous Fluids . . . . . . . . . . . . . . . . . . . . 259
</p>
<p>7.1 Functional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
</p>
<p>7.1.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
</p>
<p>7.1.2 Functional Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
</p>
<p>7.2 Density Functional Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
</p>
<p>7.2.1 Equilibrium Density Profile . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
</p>
<p>7.2.2 Microscopic Definition of Density . . . . . . . . . . . . . . . . . . . . . . 267
</p>
<p>7.2.3 Ω for a Nonequilibrium Density Profile . . . . . . . . . . . . . . . . . 268
7.2.4 &dagger;A Few Remarks on Ω [na,ψb] . . . . . . . . . . . . . . . . . . . . . . . . . 270
</p>
<p>7.3 Formal Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
</p>
<p>7.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xvii
</p>
<p>7.3.2 Key Properties of the Density Functional . . . . . . . . . . . . . . . . 273
</p>
<p>7.3.3 &dagger;Proofs of Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
7.4 Construction of a Density Functional . . . . . . . . . . . . . . . . . . . . . . . . . . 279
</p>
<p>7.4.1 Variation of the External Field . . . . . . . . . . . . . . . . . . . . . . . . . 279
</p>
<p>7.4.2 Variation of the Intermolecular Potential: Case 1 . . . . . . . . . . 280
</p>
<p>7.4.3 Variation of the Intermolecular Potential: Case 2 . . . . . . . . . . 281
</p>
<p>7.4.4 Pair Distribution Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
</p>
<p>7.4.5 Repulsive Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
</p>
<p>7.4.6 Radial Distribution Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
</p>
<p>7.4.7 &dagger;Barker&ndash;Henderson Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
7.5 Hard-Sphere Fluid Under Gravity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
</p>
<p>7.6 Vapor&ndash;Liquid Coexistence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
</p>
<p>7.6.1 Phase Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
</p>
<p>7.6.2 Interfacial Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
</p>
<p>7.7 &Dagger;Equations of State from the Radial Distribution Function . . . . . . . . 304
7.7.1 &Dagger;Compressibility Equation of State . . . . . . . . . . . . . . . . . . . . . 305
7.7.2 &Dagger;Virial Equation of State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
</p>
<p>7.8 Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
</p>
<p>References and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
</p>
<p>8 Quantum Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
</p>
<p>8.1 Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
</p>
<p>8.1.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
</p>
<p>8.1.2 Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
</p>
<p>8.1.3 Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
</p>
<p>8.1.4 &dagger;Proofs of Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
8.1.5 Scalar Product Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
</p>
<p>8.1.6 Orthonormal Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
</p>
<p>8.1.7 Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
</p>
<p>8.1.8 Linear Functional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
</p>
<p>8.1.9 Linear Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
</p>
<p>8.2 Kets, Bras, and Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
</p>
<p>8.2.1 Bra&ndash;Ket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
</p>
<p>8.2.2 Operator and Adjoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
</p>
<p>8.2.3 Addition and Multiplication of Operators . . . . . . . . . . . . . . . . 326
</p>
<p>8.2.4 Unitary Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
</p>
<p>8.2.5 Outer Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
</p>
<p>8.3 Eigenkets and Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
</p>
<p>8.3.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
</p>
<p>8.3.2 Closure relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
</p>
<p>8.3.3 Matrix Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
</p>
<p>8.3.4 Commuting Observables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
</p>
<p>8.3.5 &dagger;Degenerate Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
8.4 Postulates of Quantum Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
</p>
<p>8.5 &Dagger;Uncertainty Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340</p>
<p/>
</div>
<div class="page"><p/>
<p>xviii Contents
</p>
<p>8.6 &Dagger;Operator with Continuous Spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . 342
8.6.1 &Dagger;Position Operator and Position Eigenkets . . . . . . . . . . . . . . . 343
</p>
<p>8.7 &dagger;Linear Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
8.7.1 &dagger;Properties of Linear Translation . . . . . . . . . . . . . . . . . . . . . . . 344
8.7.2 &dagger;Commutation Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
8.7.3 &dagger;Momentum Eigenket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
</p>
<p>8.8 &dagger;Time Evolution Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
8.9 &dagger;Û t is Unitary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
8.10 &Dagger;Formal Solution of the Schrödinger Equation . . . . . . . . . . . . . . . . . . 352
</p>
<p>8.10.1 &Dagger;Time-Independent Ĥ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
8.10.2 &dagger;Time-Dependent Ĥ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
</p>
<p>8.11 &Dagger;Heisenberg&rsquo;s Equation of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
8.11.1 &Dagger;Time-Independent Ĥ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
8.11.2 &dagger;Time-Dependent Ĥ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
</p>
<p>8.12 Eigenstates of Ĥ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
</p>
<p>8.13 &Dagger;Schrödinger Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
8.14 &Dagger;Harmonic Oscillator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
</p>
<p>8.14.1 &Dagger;Operator Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
8.14.2 &Dagger;Energy Eigenfunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
</p>
<p>8.15 &dagger;Ehrenfest&rsquo;s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
8.16 Quantum Statistical Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
</p>
<p>8.16.1 Density Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
</p>
<p>8.16.2 Statistical Equilibrium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
</p>
<p>8.16.3 Liouville&rsquo;s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
</p>
<p>8.16.4 Canonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
</p>
<p>8.16.5 Ideal Gas and Classical Limit . . . . . . . . . . . . . . . . . . . . . . . . . . 383
</p>
<p>8.16.6 Microcanonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
</p>
<p>8.17 Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
</p>
<p>References and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
</p>
<p>A Vectors in Three-Dimensional Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
</p>
<p>A.1 Arrow in Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
</p>
<p>A.2 Components of a Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
</p>
<p>A.3 Dot Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
</p>
<p>A.4 Unit Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
</p>
<p>A.5 &dagger;Schwarz Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
A.6 Cross Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
</p>
<p>B Useful Formulae . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
</p>
<p>B.1 Taylor Series Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
</p>
<p>B.1.1 Function of a Single Variable . . . . . . . . . . . . . . . . . . . . . . . . . . 397
</p>
<p>B.1.2 Function of Multiple Variables . . . . . . . . . . . . . . . . . . . . . . . . . 399
</p>
<p>B.2 Exponential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
</p>
<p>B.3 Summation of a Geometric Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
</p>
<p>B.4 Binomial Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
</p>
<p>B.5 Gibbs&ndash;Bogoliubov Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xix
</p>
<p>C Legendre Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
</p>
<p>C.1 Legendre Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
</p>
<p>C.1.1 Representation of a Curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
</p>
<p>C.1.2 Legendre Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
</p>
<p>C.1.3 Inverse Legendre Transformation . . . . . . . . . . . . . . . . . . . . . . . 406
</p>
<p>D Dirac δ -Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
D.1 Definition of δ (x) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
D.2 Basic Properties of the δ -Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
D.3 Weak Versus Strong Definitions of the δ -Function . . . . . . . . . . . . . . . 415
D.4 Three-Dimensional δ -Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
D.5 &dagger;Representation of the δ -Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
References and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
</p>
<p>E Where to Go from Here . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
</p>
<p>F List of Greek Letters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
</p>
<p>G Hints to Selected Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Classical Mechanics
</p>
<p>A macroscopic object we encounter in our daily life consists of an enormously large
</p>
<p>number of atoms. While the behavior of these atoms is governed by the laws of
</p>
<p>quantum mechanics, it is often acceptable to describe them by means of classi-
</p>
<p>cal mechanics. In this chapter, we familiarize ourselves with the basic concepts of
</p>
<p>classical mechanics. From the outset, we assume that concepts such as mass, time,
</p>
<p>displacement, and force are understood on the basis of our everyday experiences
</p>
<p>without further elaboration. The role of classical mechanics, then is to explore the
</p>
<p>precise relationship among these objects in mathematical terms.
</p>
<p>1.1 Inertial Frame
</p>
<p>When describing physical phenomena, we need a frame of reference from which we
</p>
<p>make our observations. The mathematical expression of the laws of mechanics takes
</p>
<p>the simplest form when the frame is the so-called inertial frame of reference.
</p>
<p>To understand what this is, imagine that you are sitting in front of your desk in
</p>
<p>your office in your building, and suppose that the desk is firmly secured to the floor
</p>
<p>and the surface of the desk is perfectly smooth and horizontal. If you place a billiard
</p>
<p>ball on the desk and remove your hand without giving any nudge to the ball, it will
</p>
<p>just sit there. If you are to give a gentle push, it will start rolling on the desk. It will
</p>
<p>continue to move in a straight line with a constant speed until it falls off at the edge
</p>
<p>of the desk. (We shall formally define the word &ldquo;speed&rdquo; in Sect. 1.2.1.)
</p>
<p>These observations remain valid even if your office is housed in a train moving
</p>
<p>at a constant speed on a perfectly straight and leveled railroad track. A coordinate
</p>
<p>system attached to the desk, secured either to the building or to the train, is an
</p>
<p>example of inertial frames of reference, in which an object when subjected to no net
</p>
<p>force, either stands still or keeps on moving along a straight line at a constant speed.
</p>
<p>However, if the train housing your office is to suddenly change direction, the
</p>
<p>ball, which was previously standing still with respect to the desk, will suddenly start
</p>
<p>moving without you doing anything to it. It is certainly more difficult to describe
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 1
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1 1</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1 Classical Mechanics
</p>
<p>the behavior of the ball in such a frame of reference. In fact, the coordinate sys-
</p>
<p>tem attached to the desk, in this case, is no longer an inertial frame until the train
</p>
<p>resumes its motion along a straight line at a constant speed. Unless explicitly indi-
</p>
<p>cated otherwise, we shall always use an inertial frame of reference.
</p>
<p>1.2 Mechanics of a Single Particle
</p>
<p>Systems of our eventual interest consist of the order of 1024 atoms, if not more.
</p>
<p>Before we tackle the problem of describing their behavior, we shall first learn how
</p>
<p>to describe a motion of a single particle. This allows us to introduce the essential
</p>
<p>concepts and terminologies needed for the studies of many-particle systems.
</p>
<p>1.2.1 Newton&rsquo;s Equation of Motion
</p>
<p>Let us take some macroscopic object such as a ball, and imagine that the size of the
</p>
<p>object becomes smaller and smaller while maintaining its original mass. The limit
</p>
<p>of this imaginary process is a point carrying certain nonzero mass. Such a point is
</p>
<p>referred to as a material point or simply a particle.
</p>
<p>The position r of a material point in space may change with time. This will be
</p>
<p>the case if, for example, you are to throw one such material point, for example, a
</p>
<p>ball. The time dependence of r is determined by Newton&rsquo;s equation of motion. In
</p>
<p>an inertial frame of reference, this equation takes a particularly simple form:
</p>
<p>m
d2r
</p>
<p>dt2
= F , (1.1)
</p>
<p>where t, m, and F denote, respectively, the time, the mass of the material point, and
</p>
<p>the net force acting on it. Conversely, if r of a particle is described by (1.1) in some
</p>
<p>frame of reference, the frame is an inertial frame.
</p>
<p>In a Cartesian coordinate system, (1.1) can be written more explicitly as
</p>
<p>m
d2x
</p>
<p>dt2
= Fx , m
</p>
<p>d2y
</p>
<p>dt2
= Fy , and m
</p>
<p>d2z
</p>
<p>dt2
= Fz , (1.2)
</p>
<p>where x, y, z are, respectively, the x-, y-, and z-components of the vector r. Likewise
</p>
<p>for Fx, Fy, and Fz.
</p>
<p>Example 1.1. Motion under no force: If F is zero at all time, then (1.1) tells us
</p>
<p>that
d2r
</p>
<p>dt2
= 0 , (1.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Mechanics of a Single Particle 3
</p>
<p>which may be integrated to give
</p>
<p>dr
</p>
<p>dt
= c1 and r= c1t + c2 . (1.4)
</p>
<p>When no force is acting on the material point, the vector v := dr/dt, called
the velocity of the material point, is seen to be independent of time. We also
</p>
<p>observe that the material point moves along a straight line, thus confirming
</p>
<p>what was said about inertial frames of reference in Sect. 1.1.
</p>
<p>The constant vectors c1 and c2 may be determined from initial conditions.
</p>
<p>For example, if we know that the material point was at r0 at time t = 0 and that
it was moving with the velocity v0 at t = 0, then substituting this information
into (1.4), we obtain
</p>
<p>c1 = v0 and c2 = r0 , (1.5)
</p>
<p>and hence
dr
</p>
<p>dt
= v0 and r= v0t + r0 . (1.6)
</p>
<p>Example 1.2. Motion under gravity: Let us turn to a slightly more interesting
</p>
<p>problem of throwing a ball. If we ignore the friction between the ball and
</p>
<p>the air, the only force acting on the ball after it left your hand is the force of
</p>
<p>gravity mg, where the gravitational acceleration g is a vector pointing toward
</p>
<p>the center of the earth. In this case, (1.1) becomes
</p>
<p>m
d2r
</p>
<p>dt2
= mg . (1.7)
</p>
<p>Assuming that g is constant, which is an excellent approximation over the time
</p>
<p>and the length scales involved in this example, the equation can be readily
</p>
<p>integrated to give
</p>
<p>v=
dr
</p>
<p>dt
= gt + c1 and r=
</p>
<p>1
</p>
<p>2
gt2 + c1t + c2 , (1.8)
</p>
<p>where c1 and c2 are constant vectors. Unless g is parallel to c1 or c1 = 0, the
trajectory of the particle is not a straight line.
</p>
<p>As before, let r = r0 and v = v0 at t = 0. Then, c1 = v0 and c2 = r0. With
the vectors c1 and c2 so determined, (1.8) completely determines r and v at
</p>
<p>any other value of t.
</p>
<p>The two examples we have just seen point to a general scheme of classical
</p>
<p>mechanics. That is, if F is at most a known function of r, v, and t, then r and v
</p>
<p>of a particle at any time t are completely determined once they are specified at t = 0
or at any other instant of time for that matter. The pair of vectors (r,v) associated</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Classical Mechanics
</p>
<p>with a single particle at some instant of time is referred to as the mechanical state
</p>
<p>of that particle at that instant. It follows that the mechanical state of a particle is
</p>
<p>completely determined for all t if it is specified at some instant, say t = 0.2
</p>
<p>The length ||v|| :=&radic;v &middot; v=
&radic;
</p>
<p>vx2 + vy2 + vz2 of the velocity vector v is called the
speed, where vx, vy, and vz denote, respectively, the x-, y-, and z- components of the
</p>
<p>vector v. We shall often write v instead of ||v||. Finally, d2r/dt2 is the acceleration.
For a brief review of vector algebra, see Appendix A.
</p>
<p>We can now show that a frame of reference moving at a constant velocity with
</p>
<p>respect to an inertial frame is also an inertial frame. To see this, consider motion of
</p>
<p>a particle in two frames O and O &prime;, with respect to which the position vectors of the
particle is r and r&prime;, respectively. Let R denote the position vector of the origin of O &prime;
</p>
<p>with respect to that of O . Then,
</p>
<p>r= r&prime;+R . (1.9)
</p>
<p>If we suppose that O is an inertial frame, (1.1) holds for r. Using (1.9), we find
</p>
<p>m
d2r&prime;
</p>
<p>dt2
= F&minus;md
</p>
<p>2R
</p>
<p>dt2
. (1.10)
</p>
<p>If the velocity dR/dt of O &prime; with respect to O is constant, the second term on the
right-hand side vanishes identically and we obtain
</p>
<p>m
d2r&prime;
</p>
<p>dt2
= F , (1.11)
</p>
<p>indicating that O &prime; is an inertial frame.
If O &prime; is accelerating with respect to O , then d2R/dt2 must be retained in (1.10).
</p>
<p>At this point, however, our definition of inertial frames appears circular. In Newto-
</p>
<p>nian mechanics, a force acting on a particle is defined by means of Newton&rsquo;s equa-
</p>
<p>tion of motion in the form of (1.1). That is, the force is measured by the acceleration
</p>
<p>it produces.3 Therefore, an observer in O &prime; who is oblivious to the fact that his frame
is not an inertial frame would simply compute the force F&prime; acting on the particle
as md2r&prime;/dt2. This vector is, of course, equal to F&minus;md2R/dt2. But, this observer
is under no obligation to separate F&prime; into F and &minus;md2R/dt2. Instead, he can insist
that the frame O &prime; is inertial and that the force acting on the particle is F&prime;, not F. A
tacit assumption made in Newtonian mechanics, therefore, is that forces can arise
</p>
<p>only through an effect a body produces onto another and that we can systematically
</p>
<p>either eliminate or account for such effects.
</p>
<p>1.2.2 Work
</p>
<p>If a particle experiences a displacement ∆r under the influence of a force F and
the force remained constant during the displacement, then the work W done by this</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Mechanics of a Single Particle 5
</p>
<p>r
</p>
<p>r r
</p>
<p>dr1
</p>
<p>dr2
</p>
<p>dr3
</p>
<p>Fig. 1.1 The sum of infinitesimal displacements dri results in a net displacement
&int;
</p>
<p>dr=&sum;ni=1 dri =
∆r.
</p>
<p>force on the particle is, by definition, the dot product of F and ∆r:
</p>
<p>W := F &middot;∆r , (1.12)
</p>
<p>This definition does not preclude the situation in which other forces are also acting
</p>
<p>on the particle. In that case, (1.12) simply gives the work done by this particular
</p>
<p>force F alone and does not include the work done by the other forces.
</p>
<p>It may be that F actually changes during the displacement. To allow for this more
</p>
<p>general situation, we define the work by the equation
</p>
<p>W :=
</p>
<p>&int;
F &middot;dr , (1.13)
</p>
<p>where the integration is along the path taken by the particle. When F remains con-
</p>
<p>stant along the path, (1.13) may be written as
</p>
<p>W = F &middot;
&int;
</p>
<p>dr . (1.14)
</p>
<p>If we regard the integral as a &ldquo;sum&rdquo; of infinitesimal displacements dr1, dr2, dr3,. . . ,
</p>
<p>the integral is equal to ∆r as seen from Fig. 1.1, and (1.14) reduces to (1.12). Thus,
(1.13) contains (1.12) as a special case.
</p>
<p>Let us suppose, as illustrated in Fig. 1.2, that we lift a particle of mass m from
</p>
<p>z = h1 to z = h2 under gravity. In order to do this, we have to apply a force F just
enough to overcome the force exerted on the particle by gravity mg. In terms of com-
</p>
<p>ponents with respect to the coordinate system shown in Fig. 1.2, mg
.
= (0,0,&minus;mg),4
</p>
<p>where the negative sign is needed because the force of gravity points in the negative
</p>
<p>z-direction. Thus, the force needed is F
.
= (0,0,mg+ ε), where ε &gt; 0. Since the
</p>
<p>displacement is in the positive z-direction, dr
.
= (0,0,dz). Combining everything,
</p>
<p>W =
&int;
</p>
<p>F &middot;dr=
&int; h2
</p>
<p>h1
</p>
<p>(mg+ ε)dz = (mg+ ε)(h2 &minus;h1)
</p>
<p>&rarr; mg(h2 &minus;h1) as ε &rarr; 0 . (1.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Classical Mechanics
</p>
<p>z
</p>
<p>F mg 0 0 mg
</p>
<p>gravity: mg 0 0 mg
</p>
<p>h1
</p>
<p>h2
</p>
<p>Fig. 1.2 A particle under gravity.
</p>
<p>We define a new quantity
</p>
<p>ψ(z) := mgz (1.16)
</p>
<p>and call it the potential energy of the particle due to gravity when it is at height z
</p>
<p>above the ground. With this definition, (1.15) becomes
</p>
<p>W = ψ(h2)&minus;ψ(h1) . (1.17)
</p>
<p>We also notice that
</p>
<p>&minus; dψ(z)
dz
</p>
<p>=&minus;mg , (1.18)
</p>
<p>that is, by taking the derivative of ψ with respect to z and putting a negative sign
in front of it, we obtain the z-component of the force of gravity. This is a general
</p>
<p>feature of a potential energy as we shall see in the next subsection.
</p>
<p>1.2.3 Potential Energy
</p>
<p>More generally, potential energy can be introduced as follows. We suppose that the
</p>
<p>force Fc acting on a particle is a function of its position. As was the case with lifting
</p>
<p>of a particle under gravity in the previous section, the force F = &minus;Fc is needed to
counter Fc and move the particle.
</p>
<p>5 If the particle is made to move along a closed
</p>
<p>path, such as the one depicted in Fig. 1.3, the work done by the force F is given by
</p>
<p>W =
</p>
<p>∮
F &middot;dr , (1.19)
</p>
<p>where
∮
</p>
<p>signifies the integration along a closed path. If this quantity is identically
</p>
<p>zero for any closed path, including those that does not necessarily pass through the
</p>
<p>points A, B, C, and D in the figure, then Fc is said to be conservative.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Mechanics of a Single Particle 7
</p>
<p>A
</p>
<p>B
C
</p>
<p>D
</p>
<p>Fig. 1.3 A closed path along which the particle is moved.
</p>
<p>The particular closed path ACBDA shown in Fig. 1.3 can be regarded as consist-
</p>
<p>ing of two parts, the path A &rarr;C &rarr; B and the path B &rarr; D &rarr; A. Thus,
</p>
<p>W =
</p>
<p>&int;
</p>
<p>A&rarr;C&rarr;B
F &middot;dr+
</p>
<p>&int;
</p>
<p>B&rarr;D&rarr;A
F &middot;dr=: WA&rarr;C&rarr;B +WB&rarr;D&rarr;A . (1.20)
</p>
<p>Now, suppose that we reverse the second part of the path and bring the particle
</p>
<p>from A to B through D. The infinitesimal displacement dr on this reversed path
</p>
<p>A &rarr; D &rarr; B will have the opposite sign to that on the original path B &rarr; D &rarr; A. But,
the force Fc(r), and hence F(r), will remain unaffected by the reversal of the path.
Thus,
</p>
<p>WA&rarr;D&rarr;B =&minus;WB&rarr;D&rarr;A , (1.21)
and hence
</p>
<p>W =WA&rarr;C&rarr;B &minus;WA&rarr;D&rarr;B . (1.22)
If Fc is conservative, then W = 0 and we have
</p>
<p>WA&rarr;C&rarr;B =WA&rarr;D&rarr;B . (1.23)
</p>
<p>In other words, the work required to move a particle from point A to point B against
</p>
<p>the conservative force Fc is independent of the actual path taken and depends only
</p>
<p>on the positions of the end points A and B. Thus, the work can be written as
</p>
<p>WA&rarr;B = ψ(rA,rB) , (1.24)
</p>
<p>without the superfluous references to C and D.
</p>
<p>We refer to the quantity ψ(rA,rB) as the potential energy of the particle at rB
with respect to the reference point rA. When ψ(rA,r) is known for all r of interest,
we can calculate the work required to move a particle between any pair of points.
</p>
<p>For example, we have
</p>
<p>WX&rarr;Y = WX&rarr;A&rarr;Y =WX&rarr;A +WA&rarr;Y =&minus;WA&rarr;X +WA&rarr;Y
= &minus;ψ(rA,rX )+ψ(rA,rY ) . (1.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Classical Mechanics
</p>
<p>Note that ψ(rA,r) depends on our choice of rA. However, since WX&rarr;Y depends only
on rX and rY , any rA dependence of ψ must cancel out in (1.25). Thus, as long as
we are interested in computing the work required to move a particle between two
</p>
<p>points, the location of rA is arbitrary.
</p>
<p>Now, let us calculate the work required to move the particle by an infinitesimal
</p>
<p>distance from r to r+dr. According to (1.25), the work is given by
</p>
<p>W = &minus;ψ(rA,r)+ψ(rA,r+dr) = ψ(rA,r+dr)&minus;ψ(rA,r)
= ψ(rA,x+dx,y+dy,z+dz)&minus;ψ(rA,x,y,z) . (1.26)
</p>
<p>Using equations from Appendix B.1, we expand the right-hand side into the Taylor
</p>
<p>series to obtain
</p>
<p>W = dψ+h.o., (1.27)
</p>
<p>where h.o. stands for the second- and higher order terms in dx, dy, and dz, and
</p>
<p>dψ =
&part;ψ
</p>
<p>&part;x
dx+
</p>
<p>&part;ψ
</p>
<p>&part;y
dy+
</p>
<p>&part;ψ
</p>
<p>&part; z
dz =: dr &middot;&nabla;ψ . (1.28)
</p>
<p>In the last step, we introduced a new symbol &nabla;ψ . This vector is known as the gra-
dient of ψ and may also be written as &part;ψ/&part; r. In a Cartesian coordinate system, in
which
</p>
<p>dr
.
= (dx,dy,dz), (1.29)
</p>
<p>we have
</p>
<p>&nabla;ψ &equiv; &part;ψ
&part; r
</p>
<p>.
=
</p>
<p>(
&part;ψ
</p>
<p>&part;x
,
&part;ψ
</p>
<p>&part;y
,
&part;ψ
</p>
<p>&part; z
</p>
<p>)
. (1.30)
</p>
<p>In other coordinate systems, components of dr are not (dx,dy,dz). Accordingly, the
components of &nabla;ψ differ from those displayed here.
</p>
<p>Comparing (1.12) and (1.28), we identify &nabla;ψ as the force F required to move
the particle. It follows that
</p>
<p>Fc =&minus;&nabla;ψ , (1.31)
which should be compared with (1.18).
</p>
<p>In classical mechanics, we take a point of view that the physical reality at some
</p>
<p>instant t concerning a particle is defined completely by the pair of quantities r(t) and
v(t). How these quantities evolve with time is determined by Newton&rsquo;s equation of
motion, and hence by the force acting on the particle and initial conditions. It seems
</p>
<p>natural, then, to expect that the force Fc computed by (1.31) should not in any way
</p>
<p>depend on our arbitrary choice of rA.
</p>
<p>Exercise 1.1. Show that
</p>
<p>&nabla;ψ(rA&prime; ,r) = &nabla;ψ(rA,r) , (1.32)
</p>
<p>that is, the force Fc is indeed independent of the choice of rA. ///
</p>
<p>In view of the indifference of both WX&rarr;Y and Fc to our choice of rA, we shall sup-
press the explicit reference to rA in the function ψ and denote the potential energy</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Mechanics of a Single Particle 9
</p>
<p>v v1 at t t1
</p>
<p>v v2 at t t2
</p>
<p>Fig. 1.4 Accelerating a particle.
</p>
<p>simply as ψ(r) even though its value certainly do depend on rA. From (1.24), we
see that ψ(rA,rA) = 0. After dropping the first reference to rA according to the con-
vention just adopted, we obtain ψ(rA) = 0. We interpret this equation as identifying
rA as the reference point with respect to which ψ(r) is computed.
</p>
<p>1.2.4 Kinetic Energy
</p>
<p>Let us calculate the work required to change the velocity of a particle of mass m
</p>
<p>from v1 at t = t1 to v2 at time t2 as illustrated in Fig. 1.4. The net force F exerted
on it cannot be zero: Newton&rsquo;s equation of motion tells us that v remains constant
</p>
<p>otherwise. What we want to compute is the work done by this force, whether it is
</p>
<p>applied by you, some field of force such as gravity, or both. Using (1.1) in (1.13),
</p>
<p>W =
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
m
</p>
<p>d2r
</p>
<p>dt2
&middot;dr . (1.33)
</p>
<p>Because r is a function of t, we have dr= (dr/dt)dt, which allows us to change the
integration variables from r to t as
</p>
<p>W =
&int; t2
</p>
<p>t1
</p>
<p>m
d2r
</p>
<p>dt2
&middot; dr
</p>
<p>dt
dt =
</p>
<p>1
</p>
<p>2
m
</p>
<p>&int; t2
t1
</p>
<p>d
</p>
<p>dt
</p>
<p>(
dr
</p>
<p>dt
&middot; dr
</p>
<p>dt
</p>
<p>)
dt . (1.34)
</p>
<p>You can verify the second equality by evaluating the last member of the equation
</p>
<p>(with a help of the product rule of differentiation) to restore the second one. Since
</p>
<p>dr/dt = v,
</p>
<p>W =
1
</p>
<p>2
m
</p>
<p>&int; t2
t1
</p>
<p>d
</p>
<p>dt
(v &middot; v)dt = 1
</p>
<p>2
m[v &middot; v]t2t1 =
</p>
<p>1
</p>
<p>2
m
(
v2
</p>
<p>2 &minus; v12
)
. (1.35)
</p>
<p>The resulting expression for W is seen to depend only on the mass of the particle and
</p>
<p>its velocities at t1 and t2. It is independent of the actual path taken by the particle,</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Classical Mechanics
</p>
<p>v2 at t t2
</p>
<p>v1 at t t1
</p>
<p>T2
</p>
<p>T1
</p>
<p>Fig. 1.5 In the case of a particle moving on a circle at a constant speed, the velocity v changes
</p>
<p>its direction without changing its length. A force is still needed to change the direction of v and is
</p>
<p>supplied, for example, by the tension T in a string to which the particle is attached. The work W
</p>
<p>done on the particle by T, however, is zero.
</p>
<p>that is, how r changed with t. In fact, we have made no assumption in this regard.
</p>
<p>In this sense, mv2/2 is a reflection of an intrinsic property m of the particle and its
instantaneous mechanical state, v. The quantity mv2/2 is called the kinetic energy
of the particle.
</p>
<p>From (1.34), we observe that W &equiv; 0 if the acceleration d2r/dt2 is always per-
pendicular to the velocity dr/dt. Equation (1.35) then indicates that the length of
the velocity vector remains constant. Such a motion can be observed directly if you
</p>
<p>attach a particle to one end of a (massless) string and swing it around the other end
</p>
<p>as shown in Fig. 1.5.
</p>
<p>Exercise 1.2. In the derivation of (1.15), we tacitly assumed that the kinetic energy
</p>
<p>acquired by the particle is negligible. Validate this assumption in the limit of ε &rarr; 0.
You may assume that the particle is at rest initially at the height h1. ///
</p>
<p>1.2.5 Conservation of Energy
</p>
<p>Let us consider a particle moving under the influence of a conservative force &minus;&nabla;ψ
and an additional external force Fext. We calculate the work done on the particle by
</p>
<p>Fext. In this case, Newton&rsquo;s equation of motion reads
</p>
<p>m
d2r
</p>
<p>dt2
=&minus;&nabla;ψ(r)+Fext , (1.36)
</p>
<p>from which we have
</p>
<p>W =
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
Fext &middot;dr=
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
</p>
<p>[
m
</p>
<p>d2r
</p>
<p>dt2
+&nabla;ψ(r)
</p>
<p>]
&middot;dr . (1.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Mechanics of Many Particles 11
</p>
<p>Evidently, the two terms in the integrand can be integrated separately. The inte-
</p>
<p>gration of the first term, as before, yields mv2
2/2&minus;mv12/2. The integration of the
</p>
<p>second term is almost immediate if we recall (1.28):
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
&nabla;ψ(r) &middot;dr=
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
dψ = ψ(r2)&minus;ψ(r1) . (1.38)
</p>
<p>Note that we were able to compute the integral without knowing the path taken by
</p>
<p>the particle. This should not surprise you since ψ is a potential energy.
Combining the results, we arrive at the expression
</p>
<p>W =
</p>
<p>[
1
</p>
<p>2
mv2
</p>
<p>2 +ψ(r2)
</p>
<p>]
&minus;
[
</p>
<p>1
</p>
<p>2
mv1
</p>
<p>2 +ψ(r1)
</p>
<p>]
. (1.39)
</p>
<p>The kinetic energy plus the potential energy of a particle, that is,
</p>
<p>E :=
1
</p>
<p>2
mv2 +ψ(r) , (1.40)
</p>
<p>is called the mechanical energy or simply energy of the particle. According to
</p>
<p>(1.39), the change ∆E in the energy of a particle is equal to the work done on the
particle by the external force Fext:
</p>
<p>∆E =W . (1.41)
</p>
<p>From (1.37), we see that W = 0 if Fext &equiv; 0. According to (1.41), this means that
E remains constant. In other words, if a particle moves under the influence of a
</p>
<p>conservative force alone, then the energy of the particle does not change with time
</p>
<p>despite the fact that r and v, upon which E depends, do change with time. Such
</p>
<p>a quantity is called a constant of motion or a conserved quantity. Mechanical
</p>
<p>energy is an example of constants of motion.
</p>
<p>1.3 Mechanics of Many Particles
</p>
<p>So far, we focused on the mechanics of a single particle, and treated a ball as if it is
</p>
<p>a material point. A more satisfactory description of the problem would treat the ball
</p>
<p>as a collection of many material points, each representing perhaps an atom making
</p>
<p>up the ball. These material points may also interact with each other as well as with
</p>
<p>an external field. Newton&rsquo;s equation of motion generalizes quite naturally to this
</p>
<p>case and we have
</p>
<p>mi
d2ri
</p>
<p>dt2
= Fi , i = 1, . . . ,N , (1.42)
</p>
<p>where N is the number of material points. As before, r1, . . . , rN and v1, . . . , vN at any
</p>
<p>time t are determined completely by these equations and initial conditions, that is,
</p>
<p>r1, . . . , rN and v1, . . . , vN at time t = 0. In keeping with the case of a single particle
system, by a mechanical state of a system at some instant t, we shall understand</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1 Classical Mechanics
</p>
<p>the set of vectors (r1, . . . , rN , v1, . . . , vN) referring to the positions and velocities of
</p>
<p>all the particles in the system at that moment. Thus, Newton&rsquo;s equations of motion,
</p>
<p>along with initial conditions, completely specify the mechanical state of a many-
</p>
<p>particle system at any other instant.
</p>
<p>1.3.1 Mechanical Energy of a Many-Particle System
</p>
<p>Let us calculate the work required to bring about a particular change in the mechan-
</p>
<p>ical state of a many-particle system.
</p>
<p>If the system is subject to a conservative field ψ such as gravity, the ith particle
experiences the force given by
</p>
<p>&minus;&nabla;iψ(ri) .=
(
&minus;&part;ψ
&part;xi
</p>
<p>,&minus;&part;ψ
&part;yi
</p>
<p>,&minus;&part;ψ
&part; zi
</p>
<p>)
, i = 1, . . . ,N (1.43)
</p>
<p>with xi, yi, and zi denoting the x-, y-, and z-components of ri, respectively. The
</p>
<p>subscript i on &nabla; indicates that the derivative is with respect to the position vector of
the ith particle.
</p>
<p>We assume that the interaction among the particles is characterized by a conser-
</p>
<p>vative force derivable from a single potential energy function φ(r1, . . . ,rN). That
is
</p>
<p>&minus;&nabla;iφ(r1, . . . ,rN) .=
(
&minus; &part;φ
&part;xi
</p>
<p>,&minus; &part;φ
&part;yi
</p>
<p>,&minus;&part;φ
&part; zi
</p>
<p>)
, i = 1, . . . ,N (1.44)
</p>
<p>gives the force exerted on the ith particle by all the other particles in the system.
</p>
<p>We arbitrarily set φ = 0 when particles are separated sufficiently far away from
each other that there is no interaction among them. If, in addition, the ith particle is
</p>
<p>subject to another external force Fext,i, (1.42) becomes
</p>
<p>mi
d2ri
</p>
<p>dt2
=&minus;&nabla;iψ(ri)&minus;&nabla;iφ(r1, . . . ,rN)+Fext,i , i = 1, . . . ,N . (1.45)
</p>
<p>Thus, the work W done on the many-particle system by the external forces Fext,i
(i = 1, . . . ,N) is given by
</p>
<p>W =
N
</p>
<p>&sum;
i=1
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
Fext,i &middot;dri =
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
</p>
<p>[
mi
</p>
<p>d2ri
</p>
<p>dt2
+&nabla;iψ(ri)+&nabla;iφ(r1, . . . ,rN)
</p>
<p>]
&middot;dri .
</p>
<p>(1.46)
</p>
<p>The first two terms can be integrated as before and yield
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>[
1
</p>
<p>2
mivi
</p>
<p>2 +ψ(ri)
</p>
<p>]
</p>
<p>t2
</p>
<p>&minus;
N
</p>
<p>&sum;
i=1
</p>
<p>[
1
</p>
<p>2
mivi
</p>
<p>2 +ψ(ri)
</p>
<p>]
</p>
<p>t1
</p>
<p>, (1.47)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Center of Mass 13
</p>
<p>where we used the subscripts t1 and t2 to indicate that the expressions in the square
</p>
<p>brackets are to be computed at the initial state (at t = t1) and the final state (at t = t2),
respectively. The last term of (1.46) can be computed by noting that
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>&nabla;iφ(r1, . . . ,rN) &middot;dri =
N
</p>
<p>&sum;
i=1
</p>
<p>(
&part;φ
</p>
<p>&part;xi
dxi +
</p>
<p>&part;φ
</p>
<p>&part;yi
dyi +
</p>
<p>&part;φ
</p>
<p>&part; zi
dzi
</p>
<p>)
= dφ , (1.48)
</p>
<p>where the last step follows from (B.16) generalized to a function of 3N variables.
</p>
<p>Thus,
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
&nabla;iφ(r1, . . . ,rN) &middot;dri =
</p>
<p>&int; (t=t2)
</p>
<p>(t=t1)
dφ = φ2(r1, . . . ,rN)&minus;φ1(r1, . . . ,rN) ,
</p>
<p>(1.49)
</p>
<p>where the subscripts 1 and 2 on φ refer to the initial and the final states, respectively.
Combining everything, we arrive at
</p>
<p>W = ∆E , (1.50)
</p>
<p>where we defined the mechanical energy of the many-particle system by
</p>
<p>E =
N
</p>
<p>&sum;
i=1
</p>
<p>[
1
</p>
<p>2
mivi
</p>
<p>2 +ψ(ri)
</p>
<p>]
+φ(r1, . . . ,rN) , (1.51)
</p>
<p>in which
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mivi
</p>
<p>2 (1.52)
</p>
<p>is the kinetic energy of the many-particle system. If the system evolves under the
</p>
<p>influence of conservative forces (derivable from ψ and φ ) alone, that is, if Fext,i &equiv; 0
for all i = 1, . . . , N, then its mechanical energy is a constant of motion.
</p>
<p>1.4 Center of Mass
</p>
<p>In dealing with a many-particle system, it is often convenient to work with its center
</p>
<p>of mass, which is defined by
</p>
<p>R :=
1
</p>
<p>M
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>miri , (1.53)
</p>
<p>where M := &sum;Ni=1 mi is the total mass of the system.
</p>
<p>Exercise 1.3. Consider a collection of N particles under gravity. If we take the coor-
</p>
<p>dinate system so that the z-axis points vertically upward, then
</p>
<p>ψ(ri) = migzi (1.54)</p>
<p/>
</div>
<div class="page"><p/>
<p>14 1 Classical Mechanics
</p>
<p>and (1.51) reduces to
</p>
<p>E =
N
</p>
<p>&sum;
i=1
</p>
<p>[
1
</p>
<p>2
mivi
</p>
<p>2 +migzi
</p>
<p>]
+φ(r1, . . . ,rN) . (1.55)
</p>
<p>Show that this expression can be written as
</p>
<p>E =
1
</p>
<p>2
MV 2 +MgZ +&sum;
</p>
<p>i
</p>
<p>1
</p>
<p>2
mivi
</p>
<p>&prime;2 +φ(r1
&prime;, . . . ,rN
</p>
<p>&prime;) , (1.56)
</p>
<p>where V := dR/dt is the velocity of the center of mass, V := ||V||, Z is the z-
component of R, ri
</p>
<p>&prime; := ri &minus;R, and vi&prime; := dri&prime;/dt for i = 1, . . . , N. You will need
to use the fact that φ depends on the relative positions of particles, not on their
absolute position in space. ///
</p>
<p>The first two terms of (1.56) is the mechanical energy the collection of N particles
</p>
<p>would have if all the particles were concentrated at R to form a single material
</p>
<p>point of mass M. The remaining term is the mechanical energy of the N particles
</p>
<p>computed in a (generally noninertial) coordinate system whose origin is attached
</p>
<p>to R.
</p>
<p>1.5 Hamilton&rsquo;s Principle
</p>
<p>Mechanical behavior of a many-particle system is determined completely by (1.42)
</p>
<p>and initial conditions. However, for a system consisting of 1024 particles or more,
</p>
<p>which are of our principal interest, the actual solution of the equations of motion
</p>
<p>is practically impossible to obtain. More importantly, the solution contains far more
</p>
<p>detailed information than we possibly need. For example, you probably will not send
</p>
<p>your soup back in a restaurant on the basis that you do not like the position of a par-
</p>
<p>ticular molecule in it. Our primary concern in this case will be in a much less detailed
</p>
<p>description of the system, such as its temperature, volume, and composition. Thus,
</p>
<p>our goal must be to extract such information on a system of many particles without
</p>
<p>ever solving (1.42). Newton&rsquo;s formulation of mechanics is not particularly useful
</p>
<p>for developing such a scheme. For that, we have to reformulate classical mechanics
</p>
<p>first, which is the subject of this section and, in fact, the rest of the chapter.
</p>
<p>1.5.1 Lagrangian and Action
</p>
<p>We define a quantity called Lagrangian L by
</p>
<p>L := (Kinetic Energy)&minus; (Potential Energy) . (1.57)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Hamilton&rsquo;s Principle 15
</p>
<p>Fig. 1.6 The actual path z(t) followed by the mechanical system (thick curve) and a varied path
z&prime;(t) = z(t)+δ z(t) (thin wavy curve). Note that δ z = 0 at t1 and t2.
</p>
<p>For example, the Lagrangian of a particle in a uniform gravitational field is given by
</p>
<p>L =
1
</p>
<p>2
mż2 &minus;mgz , (1.58)
</p>
<p>where we took the z-axis vertically upward. We also introduced a short-hand nota-
</p>
<p>tion ż := dz/dt and assumed that the particle moves only in the z-direction. We see
that, once z is known as a function of time t, we can find L as a function only of
</p>
<p>t. This function can then be integrated with respect to t over some interval of time
</p>
<p>t1 &le; t &le; t2, yielding the quantity called the action integral or action:
</p>
<p>S [z] :=
&int; t2
</p>
<p>t1
</p>
<p>L(z, ż)dt . (1.59)
</p>
<p>The value of the action S depends on the form of the function z(t). We express this
dependence by means of the notation S [z].
</p>
<p>Clearly, the value of S can be calculated not only for the actual path z(t) fol-
lowed by the mechanical system under consideration but also for any other path
</p>
<p>z&prime;(t), for which we obtain S [z&prime;]. (z&prime; is not to be confused with ż.) You might wonder
if computation of S for any path other than the actual one serves any useful pur-
</p>
<p>pose at all. As we shall find out, however, such a computation leads to a useful and
</p>
<p>surprisingly simple (at least conceptually) way to characterize the actual path.
</p>
<p>Now, let z&prime;(t) = z(t) + δ z(t) so that z&prime;(t) differs only infinitesimally from the
actual path z(t) and calculate S [z+δ z]. (See Fig. 1.6.) Hamilton&rsquo;s principle states
that, for any δ z(t),
</p>
<p>S [z+δ z]&minus;S [z] = 0 , (1.60)
to the first order of δ z(t) provided that
</p>
<p>z(t1) = z
&prime;(t1) and z(t2) = z
</p>
<p>&prime;(t2) , (1.61)</p>
<p/>
</div>
<div class="page"><p/>
<p>16 1 Classical Mechanics
</p>
<p>that is, the perturbed path z&prime;(t) coincides with the actual path z(t) at the beginning
and at the end of the time period over which we consider the time evolution of the
</p>
<p>system. This may be expressed as
</p>
<p>δ z(t1) = δ z(t2) = 0 . (1.62)
</p>
<p>The meaning of the phrase to the first order of δ z is explained in Appendix B.1 and
will be made clearer as we explore the consequence of Hamilton&rsquo;s principle.
</p>
<p>First, we evaluate the difference
</p>
<p>S [z+δ z]&minus;S [z] =
&int; t2
</p>
<p>t1
</p>
<p>[
L
</p>
<p>(
z+δ z,
</p>
<p>d
</p>
<p>dt
(z+δ z)
</p>
<p>)
&minus;L(z, ż)
</p>
<p>]
dt . (1.63)
</p>
<p>Recalling the definition of the short-hand notation we introduced earlier, we find
</p>
<p>d
</p>
<p>dt
(z+δ z) =
</p>
<p>dz
</p>
<p>dt
+
</p>
<p>d
</p>
<p>dt
(δ z) = ż+ δ̇ z , (1.64)
</p>
<p>and hence
</p>
<p>S [z+δ z]&minus;S [z] =
&int; t2
</p>
<p>t1
</p>
<p>[
L(z+δ z, ż+ δ̇ z)&minus;L(z, ż)
</p>
<p>]
dt . (1.65)
</p>
<p>To extract the first-order term, we expand the integrand in the Taylor series:
</p>
<p>L(z+δ z, ż+ δ̇ z)&minus;L(z, ż) = &part;L
&part; z
</p>
<p>δ z+
&part;L
</p>
<p>&part; ż
δ̇ z+h.o. , (1.66)
</p>
<p>Bringing (1.66) into (1.65), we find
</p>
<p>S [z+δ z]&minus;S [z] =
&int; t2
</p>
<p>t1
</p>
<p>(
&part;L
</p>
<p>&part; z
δ z+
</p>
<p>&part;L
</p>
<p>&part; ż
δ̇ z
</p>
<p>)
dt +h.o. (1.67)
</p>
<p>The development up to this point might make you a bit uneasy. On the one hand,
</p>
<p>once the entire path z(t) is specified, so is ż(t) = dz/dt. The same remark applies to
the varied path as we have seen in (1.64). On the other hand, in (1.66), we considered
</p>
<p>the partial derivative of L with respect to z taken while holding ż constant. Likewise
</p>
<p>for the partial derivative with respect to ż. That is, we are treating z and ż as if they
</p>
<p>are independent. Is this legitimate? Given a function L that is expressed in terms of
</p>
<p>z and ż, this surely is a well-defined procedure mathematically, and there should be
</p>
<p>no doubt about the validity of (1.66), which is nothing but a Taylor series expansion.
</p>
<p>The real question, then is in what sense z and ż can be regarded as independent. Why
</p>
<p>is L a function of both z and ż and not a function of z alone? The reason is that, for
</p>
<p>a given value of z at any instant t, we can evidently assign different values to ż, thus
</p>
<p>obtaining various different mechanical states. In this sense, z and ż at each instant
</p>
<p>are independent variables necessary to specify the mechanical state of the system at
</p>
<p>that instant. The Lagrangian is a function of the instantaneous mechanical state of
</p>
<p>the system, while the action depends on the entire path taken by the system between
</p>
<p>t1 and t2.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Hamilton&rsquo;s Principle 17
</p>
<p>Applying the product rule of differentiation, we note that
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; ż
δ z
</p>
<p>)
=
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; ż
</p>
<p>)
δ z+
</p>
<p>&part;L
</p>
<p>&part; ż
δ̇ z . (1.68)
</p>
<p>Integrating both sides of the equation with respect to t, we find
</p>
<p>[
&part;L
</p>
<p>&part; ż
δ z
</p>
<p>]t2
</p>
<p>t1
</p>
<p>=
</p>
<p>&int; t2
t1
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; ż
</p>
<p>)
δ zdt +
</p>
<p>&int; t2
t1
</p>
<p>&part;L
</p>
<p>&part; ż
δ̇ zdt , (1.69)
</p>
<p>which is just an integration by parts of the integrand (&part;L/&part; ż)δ̇ z. The left-hand side
of this equation is identically zero because of (1.62). Thus, we may rewrite (1.67)
</p>
<p>as
</p>
<p>S [z+δ z]&minus;S [z] =
&int; t2
</p>
<p>t1
</p>
<p>[
&part;L
</p>
<p>&part; z
&minus; d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; ż
</p>
<p>)]
δ zdt +h.o. (1.70)
</p>
<p>By definition, the variation of S up to the first order of δ z refers to the quantity
</p>
<p>δS :=
&int; t2
</p>
<p>t1
</p>
<p>[
&part;L
</p>
<p>&part; z
&minus; d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; ż
</p>
<p>)]
δ zdt . (1.71)
</p>
<p>Hamilton&rsquo;s principle demands that this expression vanish for any δ z. Hence
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; ż
</p>
<p>)
&minus; &part;L
</p>
<p>&part; z
= 0 for all t1 &le; t &le; t2 . (1.72)
</p>
<p>That (1.72) is sufficient in order for (1.71) to vanish should be obvious. To see that
</p>
<p>(1.72) is also necessary, suppose that (1.72) did not hold at some instant t&lowast;. This
means that the left-hand side of (1.72) is either positive or negative at t&lowast;. Suppose
that it is negative. Then, the integrand occurring in (1.71) will be positive at t&lowast;.
Provided that the integrand is a continuous function of t, it remains positive over
</p>
<p>some interval containing t&lowast;. Then, we could fine-tune δ z over this interval and make
(1.71) false. If we demand that δS vanish for any δ z, nonzero value of the inte-
grand in (1.71) cannot be allowed. Equation (1.72) is called Lagrange&rsquo;s equation
</p>
<p>of motion.
</p>
<p>Exercise 1.4. Show that, for the Lagrangian given by (1.58), (1.72) reduces to New-
</p>
<p>ton&rsquo;s equation of motion. ///
</p>
<p>1.5.2 Generalized Coordinates
</p>
<p>In deriving Lagrange&rsquo;s equation of motion, no use was made of the fact that z is one
</p>
<p>of the Cartesian coordinates. All what we required was that the configuration of the
</p>
<p>system can be specified by z and that L is a (twice) differentiable function of z and
</p>
<p>ż. Any such variable will do.</p>
<p/>
</div>
<div class="page"><p/>
<p>18 1 Classical Mechanics
</p>
<p>Fig. 1.7 A pendulum oscillating in the xy-plane. The length l of the rod is constant.
</p>
<p>As an example, consider a simple pendulum in Fig. 1.7, which consists of a
</p>
<p>massless rigid rod of length l and a particle of mass m attached at the end of the
</p>
<p>rod. For simplicity, we assume that the particle is confined to the xy-plane. Once we
</p>
<p>specify θ , the coordinates (x,y) of the particle are completely determined:
</p>
<p>x = l sinθ and y =&minus;l cosθ . (1.73)
</p>
<p>Thus, we know the time evolution of the pendulum once we find θ as a function of
t.
</p>
<p>How do we find θ(t) then? If we can somehow express L as a function of θ and
θ̇ , we can use this L(θ , θ̇) in (1.59) to find the action:
</p>
<p>S [θ ] :=
&int; t2
</p>
<p>t1
</p>
<p>L(θ , θ̇)dt . (1.74)
</p>
<p>But, this is just (1.59) with z replaced by θ . So, by demanding that δS = 0 with
respect to arbitrary perturbation δθ , we should arrive at
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; θ̇
</p>
<p>)
&minus; &part;L
</p>
<p>&part;θ
= 0 , (1.75)
</p>
<p>which is an equation for θ(t).
We observe that the form of Lagrange&rsquo;s equation of motion as given by (1.72)
</p>
<p>remains unchanged by going from the Cartesian coordinate system to any other
</p>
<p>coordinate system. It should be carefully noted, however, that the functional form of
</p>
<p>L and hence the resulting equation of motion, in which partial derivatives of L are
</p>
<p>carried out explicitly, do depend on the variable used.
</p>
<p>Example 1.3. Simple two-dimensional pendulum: To derive the differential
</p>
<p>equation for θ(t) from (1.75) in the case of the simple pendulum, we need
to find the expressions for the kinetic and the potential energies of the system</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Hamilton&rsquo;s Principle 19
</p>
<p>first. From (1.73), we see that
</p>
<p>ẋ = lθ̇ cosθ and ẏ = lθ̇ sinθ . (1.76)
</p>
<p>So, the kinetic energy of the particle is given by
</p>
<p>1
</p>
<p>2
m
(
ẋ2 + ẏ2
</p>
<p>)
=
</p>
<p>1
</p>
<p>2
ml2θ̇
</p>
<p>2
, (1.77)
</p>
<p>while the potential energy is
</p>
<p>mgy =&minus;mgl cosθ , (1.78)
</p>
<p>and hence
</p>
<p>L =
1
</p>
<p>2
ml2θ̇
</p>
<p>2
+mgl cosθ . (1.79)
</p>
<p>Taking the required partial derivatives, we find
</p>
<p>&part;L
</p>
<p>&part; θ̇
= ml2θ̇ and
</p>
<p>&part;L
</p>
<p>&part;θ
=&minus;mgl sinθ . (1.80)
</p>
<p>Substituting these expressions in (1.75), we arrive at
</p>
<p>ml2θ̈ +mgl sinθ = 0 . (1.81)
</p>
<p>Thus,
</p>
<p>θ̈ =&minus;g
l
</p>
<p>sinθ . (1.82)
</p>
<p>When θ is small, sinθ &asymp; θ and hence
</p>
<p>θ̈ =&minus;g
l
θ , (1.83)
</p>
<p>which is the equation of motion of a harmonic oscillator with the general
</p>
<p>solution given by
</p>
<p>θ(t) = c1 cosωt + c2 sinωt , (1.84)
</p>
<p>where ω =
&radic;
</p>
<p>g/l. Under the initial conditions that θ = θ0 and θ̇ = 0 at t = 0,
we obtain c1 = θ0 and c2 = 0.
</p>
<p>One advantage of formulating mechanics in terms of Lagrange&rsquo;s equation of
</p>
<p>motion rather than Newton&rsquo;s is based on the fact that the choice of coordinates
</p>
<p>is arbitrary as long as they can uniquely specify the configuration of the system. To
</p>
<p>see why this is a good thing, let us try to analyze the same problem using Newton&rsquo;s
</p>
<p>equation of motion.</p>
<p/>
</div>
<div class="page"><p/>
<p>20 1 Classical Mechanics
</p>
<p>Since the pendulum is confined to the xy-plane, we need the x- and y-components
</p>
<p>of the equation:
</p>
<p>mẍ =&minus;T sinθ and mÿ = T cosθ &minus;mg , (1.85)
</p>
<p>the solution of which must satisfy the constraint that the length l of the pendulum
</p>
<p>does not change:
</p>
<p>x2 + y2 = l2 = const. (1.86)
</p>
<p>In (1.85), T = ||T|| is the length of the unknown force vector T required to enforce
the constraint. Thus, T must be found as a part of the solution. In contrast, T sim-
</p>
<p>ply does not arise in Lagrange&rsquo;s equation of motion, because the constraint was
</p>
<p>automatically taken care of by choosing θ as a single variable to specify the config-
uration of the system. That is, x and y given by (1.73) automatically satisfy (1.86).
</p>
<p>It is also worth pointing out that, in Newtonian mechanics, we work directly
</p>
<p>with vectors such as force, position, and velocity. In contrast, we deal primarily
</p>
<p>with scalars, such as kinetic and potential energies and Lagrangian in Lagrangian
</p>
<p>mechanics. This often leads to a simpler treatment of the same problem.
</p>
<p>A variable needed to specify the configuration of the system, such as θ , is called
a generalized coordinate. We refer to its time derivative, θ̇ here, as a generalized
velocity.
</p>
<p>Exercise 1.5. Show that (1.85) reduces to (1.82). ///
</p>
<p>1.5.3 Many Mechanical Degrees of Freedom
</p>
<p>The number of generalized coordinates necessary to specify the configuration of
</p>
<p>a system is called the number of mechanical degrees of freedom. In the above
</p>
<p>example, it was one. A mechanical state of the system having f mechanical degrees
</p>
<p>of freedom is specified by f generalized coordinates, q1, . . . , q f , and f generalized
</p>
<p>velocities, q̇1, . . . , q̇ f . Accordingly, L is a function of these 2 f variables and the
</p>
<p>action is given by
</p>
<p>S [q1, . . . ,q f ] =
&int; t2
</p>
<p>t1
</p>
<p>L(q1, . . . ,q f , q̇1, . . . , q̇ f , t)dt , (1.87)
</p>
<p>where we also allowed for the possibility that L changes even if q1, . . . , q f and q̇1,
</p>
<p>. . . , q̇ f remained constant. Such time dependence is said to be explicit. In contrast,
</p>
<p>the time dependence of L through its dependence on q1, . . . , q f and q̇1, . . . , q̇ f is
</p>
<p>said to be implicit. The explicit time dependence arises if, for example, the external
</p>
<p>field such as an electric field varies with time either according to some prescription
</p>
<p>or stochastically but in a manner that is independent of the mechanical state of the
</p>
<p>system. We exclude a field that changes with time in a manner that depends on the
</p>
<p>state of the system. In this latter case, the field and the system taken together must
</p>
<p>be studied as a single mechanical system.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Hamilton&rsquo;s Principle 21
</p>
<p>. ............. .............
</p>
<p>. ........... ...........
...........
..........
</p>
<p>x
</p>
<p>y
</p>
<p>1 l1
</p>
<p>m1
</p>
<p>g
</p>
<p>2
</p>
<p>l2
</p>
<p>m2
</p>
<p>Fig. 1.8 A coplanar double pendulum oscillating in the xy-plane. Both l1 and l2 are constant.
</p>
<p>The explicit time dependence of L does not play any role when considering the
</p>
<p>variations of q1, . . . , q f at each instant of time between given t1 and t2 because the
</p>
<p>field is common to both actual and varied paths.
</p>
<p>When applying the stationarity condition of S , that is, δS = 0, the variations of
q1, . . . , q f are entirely arbitrary except that the variations must vanish at t = t1 and
t2. As one such variation, we may consider a variation, in which only δqi is nonzero
and δq j &equiv; 0 for j 	= i. Even for this rather special variation, Hamilton&rsquo;s principle
demands that δS = 0. This gives
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; q̇i
</p>
<p>)
&minus; &part;L
</p>
<p>&part;qi
= 0 . (1.88)
</p>
<p>Letting i vary from 1 to f , we arrive at
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; q̇i
</p>
<p>)
&minus; &part;L
</p>
<p>&part;qi
= 0 , i = 1, . . . , f . (1.89)
</p>
<p>What we have shown is that (1.89) is necessary in order for δS to vanish with
respect to arbitrary variations of q1, . . . , q f . Equation (1.89) is also sufficient for
</p>
<p>δS = 0. (Take a moment to think about this.)
In arriving at (1.89), we made use of the fact that qi&rsquo;s are all capable of indepen-
</p>
<p>dent variations. If the number of qi&rsquo;s exceeds f , they are not independent. Thus, by
</p>
<p>demanding that δq j &equiv; 0 for j 	= i, we may be imposing a constraint on the possible
values of δqi. In this case, Hamilton&rsquo;s principle does not necessarily lead to (1.89).
</p>
<p>Example 1.4. Coplanar double pendulum: Let us find a Lagrangian of a copla-
</p>
<p>nar double pendulum placed in a uniform gravitational field as depicted in
</p>
<p>Fig. 1.8.</p>
<p/>
</div>
<div class="page"><p/>
<p>22 1 Classical Mechanics
</p>
<p>The configuration of the system is uniquely determined if we specify the
</p>
<p>position of the particles (x1,y1) and (x2,y2). In terms of these variables, we
have
</p>
<p>L =
1
</p>
<p>2
m1(ẋ1
</p>
<p>2 + ẏ1
2)&minus;m1gy1 +
</p>
<p>1
</p>
<p>2
m2(ẋ2
</p>
<p>2 + ẏ2
2)&minus;m2gy2 . (1.90)
</p>
<p>However, since the particles are attached to the rods, x1, y1, x2, and y2 are not
</p>
<p>independent and we cannot derive Lagrange&rsquo;s equations of motion directly
</p>
<p>from the Lagrangian given by (1.90).
</p>
<p>Instead, we note that x1, y1, x2, and y2 are completely determined by spec-
</p>
<p>ifying two variables φ1 and φ2. In other words, the mechanical degrees of
freedom of the system is two and we should be able to find two Lagrange&rsquo;s
</p>
<p>equations of motion from a Lagrangian expressed in terms of φ1, φ2, φ̇ 1, and
φ̇ 2. From Fig. 1.8, we note the following relations:
</p>
<p>x1 = l1 sinφ1 ,
</p>
<p>y1 = &minus;l1 cosφ1 ,
x2 = l1 sinφ1 + l2 sinφ2 ,
</p>
<p>y2 = &minus;l1 cosφ1 &minus; l2 cosφ2 . (1.91)
</p>
<p>Taking the time derivative,
</p>
<p>ẋ1 = l1φ̇ 1 cosφ1 ,
</p>
<p>ẏ1 = l1φ̇ 1 sinφ1 ,
</p>
<p>ẋ2 = l1φ̇ 1 cosφ1 + l2φ̇ 2 cosφ2 ,
</p>
<p>ẏ2 = l1φ̇ 1 sinφ1 + l2φ̇ 2 sinφ2 . (1.92)
</p>
<p>Using these relations in (1.90), we find
</p>
<p>L =
1
</p>
<p>2
(m1 +m2)l1
</p>
<p>2φ̇ 1
2
+
</p>
<p>1
</p>
<p>2
m2l2
</p>
<p>2φ̇ 2
2
+m2l1l2φ̇ 1φ̇ 2 cos(φ1 &minus;φ2)
</p>
<p>+(m1 +m2)gl1 cosφ1 +m2gl2 cosφ2 , (1.93)
</p>
<p>where we used the identity:
</p>
<p>cos(α+β ) = cosα cosβ &minus; sinα sinβ . (1.94)
</p>
<p>This L can be used in (1.89) to find Lagrange&rsquo;s equations of motion for φ1(t)
and φ2(t).
</p>
<p>Exercise 1.6. Consider a pendulum consisting of a massless harmonic spring and a
</p>
<p>particle of mass m attached at the end of the spring as shown in Fig. 1.9. The spring
</p>
<p>constant and the natural length of the spring are k and l0, respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Hamilton&rsquo;s Principle 23
</p>
<p>Fig. 1.9 A pendulum suspended by a harmonic spring.
</p>
<p>a. Let er denote a unit vector pointing from the origin to the particle. Whether we
</p>
<p>are stretching the spring (r &gt; l0) or compressing it (r &lt; l0), the force exerted by
the spring on the particle is given by
</p>
<p>F=&minus;k(r&minus; l0)er . (1.95)
</p>
<p>(Draw a diagram to convince yourself of this point.) Use this expression to show
</p>
<p>that the potential energy of the harmonic spring is given by
</p>
<p>ψ(r) =
1
</p>
<p>2
k(r&minus; l0)2 . (1.96)
</p>
<p>b. Using θ and r as your generalized coordinates, find the Lagrangian of the system.
You may assume that the motion of the spring and the particle is confined to the
</p>
<p>xy-plane. ///
</p>
<p>Exercise 1.7. A system consists of two particles, one at r1 and the other at r2. They
</p>
<p>interact via a potential energy φ which depends only on their relative position r :=
r1 &minus; r2. Using the position vector R of the center of mass
</p>
<p>R=
m1r1 +m2r2
</p>
<p>m1 +m2
(1.97)
</p>
<p>and r as your generalized coordinates, show that
</p>
<p>L =
1
</p>
<p>2
MṘ
</p>
<p>2
+
</p>
<p>1
</p>
<p>2
&micro; ṙ2 &minus;φ(r) , (1.98)
</p>
<p>where M := m1 +m2, Ṙ
2
</p>
<p>:= ||Ṙ||2 = Ṙ &middot; Ṙ, and likewise for ṙ2. The quantity
</p>
<p>&micro; :=
m1m2
</p>
<p>m1 +m2
(1.99)
</p>
<p>is the so-called reduced mass. Why is it more convenient to work with R and r than
</p>
<p>with r1 and r2? ///</p>
<p/>
</div>
<div class="page"><p/>
<p>24 1 Classical Mechanics
</p>
<p>Given a mechanical system, its Lagrangian is determined only to an additive
</p>
<p>function dF/dt. In fact, if L1 denotes a Lagrangian of a mechanical system with f
degrees of freedom, the new function
</p>
<p>L2 := L1 +
dF(q1, . . . ,q f , t)
</p>
<p>dt
(1.100)
</p>
<p>also serves as a Lagrangian of the same system.
</p>
<p>The validity of this claim is immediately obvious if we integrate (1.100) with
</p>
<p>respect to time. In fact, if
</p>
<p>S i[q1, . . . ,q f ] :=
&int; t2
</p>
<p>t1
</p>
<p>Li(q1, . . . ,q f , q̇1, . . . , q̇ f , t)dt , (1.101)
</p>
<p>then
</p>
<p>S 2[q1, . . . ,q f ] = S 1[q1, . . . ,q f ]+F(q1(t2), . . . ,q f (t2), t2)
</p>
<p>&minus;F(q1(t1), . . . ,q f (t1), t1) . (1.102)
</p>
<p>Since δqi(t1) = δqi(t2) = 0 for all i = 1, . . . , f , we see that δS 2 = δS 1. Thus, if
δS 1 = 0, then δS 2 = 0 for the same set of functions q1(t), . . . , q f (t), which is then
the solution of Lagrange&rsquo;s equations of motion whether they are derived from L1 or
</p>
<p>L2. Alternatively, you can validate the claim through a more explicit computation of
</p>
<p>various derivatives as illustrated in the next exercise.
</p>
<p>Exercise 1.8. Use (1.100) to compute
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L2
&part; q̇i
</p>
<p>)
&minus; &part;L2
</p>
<p>&part;qi
</p>
<p>and show that it is zero if and only if
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L1
&part; q̇i
</p>
<p>)
&minus; &part;L1
</p>
<p>&part;qi
= 0 .
</p>
<p>This being the case for i = 1, . . . , f , the same set of functions q1(t), . . . , q f (t) found
by solving Lagrange&rsquo;s equations of motion obtained from L1 also satisfies those
</p>
<p>derived from L2. ///
</p>
<p>1.6 Momentum and Energy: Definitions
</p>
<p>The quantity
</p>
<p>pi :=
&part;L
</p>
<p>&part; q̇i
(1.103)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.6 Momentum and Energy: Definitions 25
</p>
<p>is called the generalized momentum conjugate to the generalized coordinate qi.
</p>
<p>When qi is one of the Cartesian coordinates, then its conjugate momentum is called
</p>
<p>the linear momentum. Thus, the linear momentum of the ith particle in the system
</p>
<p>is given by
</p>
<p>pi =
&part;L
</p>
<p>&part; ṙi
=
</p>
<p>&part;L
</p>
<p>&part;vi
. (1.104)
</p>
<p>We recall from Sect. 1.2.3 that this is just a compact way of writing
</p>
<p>px =
&part;L
</p>
<p>&part;vx
, py =
</p>
<p>&part;L
</p>
<p>&part;vy
, and pz =
</p>
<p>&part;L
</p>
<p>&part;vz
, (1.105)
</p>
<p>where we omitted the subscript i for brevity. We define the energy function
</p>
<p>h :=
f
</p>
<p>&sum;
i=1
</p>
<p>&part;L
</p>
<p>&part; q̇i
q̇i &minus;L . (1.106)
</p>
<p>As seen in the following example, h is often equal to the mechanical energy, that is,
</p>
<p>the sum of the kinetic and the potential energies.
</p>
<p>Example 1.5. Energy function of a particle: Consider a particle of mass m
</p>
<p>moving in an external field ψ . Using a Cartesian coordinate system, we have
</p>
<p>L =
1
</p>
<p>2
mv2 &minus;ψ(r) . (1.107)
</p>
<p>From (1.104),
</p>
<p>p=
&part;L
</p>
<p>&part;v
= mv . (1.108)
</p>
<p>According to (1.106),
</p>
<p>h = p &middot; v&minus;L = mv2 &minus; 1
2
</p>
<p>mv2 +ψ(r) =
1
</p>
<p>2
mv2 +ψ(r) . (1.109)
</p>
<p>At least in this example, therefore, the energy function h is nothing but the
</p>
<p>mechanical energy defined by (1.40).
</p>
<p>If you prefer, you can work directly with the components of various vec-
</p>
<p>tors. Thus, we start by writing (1.107) as
</p>
<p>L =
1
</p>
<p>2
m
(
vx
</p>
<p>2 + vy
2 + vz
</p>
<p>2
)
&minus;ψ(r) . (1.110)
</p>
<p>From (1.105),
</p>
<p>px =
&part;L
</p>
<p>&part;vx
= mvx , py =
</p>
<p>&part;L
</p>
<p>&part;vy
= mvy , and pz =
</p>
<p>&part;L
</p>
<p>&part;vz
= mvz , (1.111)</p>
<p/>
</div>
<div class="page"><p/>
<p>26 1 Classical Mechanics
</p>
<p>which is just (1.108). According to (1.106),
</p>
<p>h = (pxvx + pyvy + pzvz)&minus;L
</p>
<p>= mvx
2 +mvy
</p>
<p>2 +mvz
2 &minus; 1
</p>
<p>2
m
(
vx
</p>
<p>2 + vy
2 + vz
</p>
<p>2
)
+ψ(r)
</p>
<p>=
1
</p>
<p>2
m
(
vx
</p>
<p>2 + vy
2 + vz
</p>
<p>2
)
+ψ(r) . (1.112)
</p>
<p>So, is h always equal to E? Disappointingly, the answer is no. As an example,
</p>
<p>consider the following exercise.
</p>
<p>Exercise 1.9. For L1 and L2 in (1.100), show that
</p>
<p>h2 = h1 &minus;
&part;F
</p>
<p>&part; t
. (1.113)
</p>
<p>That is, if h1 = E, then h2 	= E unless &part;F/&part; t &equiv; 0. ///
In the next section, marked with a dagger (&dagger;) for an optional reading, we exam-
</p>
<p>ine the condition under which h = E. The conclusion is that, for a broad range of
applications of classical mechanics we are going to encounter, h = E holds. Thus,
unless stated otherwise, we shall assume this equality and speak only of the energy
</p>
<p>E.
</p>
<p>To conclude this section, let us deduce one immediate consequence of (1.106).
</p>
<p>We consider two mechanical systems, A and B, and suppose that they are sufficiently
</p>
<p>far away from each other that they evolve independently. However, there is nothing
</p>
<p>in principle that prevents us from treating them as a single composite system. It is
</p>
<p>clear that Lagrange&rsquo;s equations of motion for the composite system can be derived
</p>
<p>from the Lagrangian L := La +Lb, in which La depends only on qi&rsquo;s and q̇i&rsquo;s per-
taining to system A and does not depend on those pertaining to system B. Similarly
</p>
<p>for Lb. This means that Lagrangian is an additive quantity. From (1.106), it follows
</p>
<p>that energy also is an additive quantity.
</p>
<p>1.7 &dagger;Energy Function and Energy
</p>
<p>Consider a collection of N particles subject to an external field ψ . Using a Cartesian
coordinate system, we have
</p>
<p>E =
N
</p>
<p>&sum;
i=1
</p>
<p>[
1
</p>
<p>2
mivi
</p>
<p>2 +ψ(ri, t)
</p>
<p>]
+φ(r1, . . . ,rN) , (1.114)
</p>
<p>where we allowed for the explicit time dependence of ψ . As in Exercise 1.3, φ is
the potential energy due to the mutual interaction among particles.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.7 &dagger;Energy Function and Energy 27
</p>
<p>Suppose that we introduced a set of generalized coordinates q1, . . . ,q f that are
related to r1, . . . , rN by the following set of equations:
</p>
<p>ri = ri(q1, . . . ,q f , t) , i = 1, . . . ,N . (1.115)
</p>
<p>If there is no constraint on ri&rsquo;s, then f = DN, where D is the dimensionality of the
space in which the particles move. Otherwise, f &lt;DN. In (1.73), for example, f = 1,
D = 2, and N = 1. Following the approach we took in Example 1.3, we proceed to
express L in terms of qi&rsquo;s and q̇i&rsquo;s. First, we note that
</p>
<p>vi =
dri
</p>
<p>dt
=
</p>
<p>f
</p>
<p>&sum;
j=1
</p>
<p>&part; ri
&part;q j
</p>
<p>q̇ j +
&part; ri
&part; t
</p>
<p>. (1.116)
</p>
<p>Thus,
</p>
<p>vi
2 = vi &middot; vi =
</p>
<p>f
</p>
<p>&sum;
j=1
</p>
<p>f
</p>
<p>&sum;
k=1
</p>
<p>A
jk
i q̇ jq̇k +2
</p>
<p>f
</p>
<p>&sum;
j=1
</p>
<p>B
j
i q̇ j +Ci , (1.117)
</p>
<p>where we defined
</p>
<p>A
jk
i :=
</p>
<p>&part; ri
&part;q j
</p>
<p>&middot; &part; ri
&part;qk
</p>
<p>, B ji :=
&part; ri
&part;q j
</p>
<p>&middot; &part; ri
&part; t
</p>
<p>, and Ci :=
&part; ri
&part; t
</p>
<p>&middot; &part; ri
&part; t
</p>
<p>. (1.118)
</p>
<p>Using (1.117), we see that the kinetic energy of the system is given by
</p>
<p>1
</p>
<p>2
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mivi
2 =
</p>
<p>1
</p>
<p>2
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mi
</p>
<p>[
f
</p>
<p>&sum;
j=1
</p>
<p>f
</p>
<p>&sum;
k=1
</p>
<p>A
jk
i q̇ jq̇k +2
</p>
<p>f
</p>
<p>&sum;
j=1
</p>
<p>B
j
i q̇ j +Ci
</p>
<p>]
. (1.119)
</p>
<p>Because of (1.115), ri&rsquo;s in ψ and φ are now functions of q1, . . . ,q f and t. Thus, the
total potential energy may be written as
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ψ(ri, t)+φ(r1, . . . ,rN) =: U(q1, . . . ,q f , t) . (1.120)
</p>
<p>So, the Lagrangian, when expressed in terms of q1, . . . ,q f is
</p>
<p>L =
1
</p>
<p>2
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mi
</p>
<p>[
f
</p>
<p>&sum;
j=1
</p>
<p>f
</p>
<p>&sum;
k=1
</p>
<p>A
jk
i q̇ jq̇k +2
</p>
<p>f
</p>
<p>&sum;
j=1
</p>
<p>B
j
i q̇ j +Ci
</p>
<p>]
&minus;U(q1, . . . ,q f , t) . (1.121)
</p>
<p>It follows that
</p>
<p>&part;L
</p>
<p>&part; q̇n
=
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mi
</p>
<p>[
f
</p>
<p>&sum;
j=1
</p>
<p>A
jn
i q̇ j +B
</p>
<p>n
i
</p>
<p>]
, (1.122)
</p>
<p>where we used the fact that A
jk
i = A
</p>
<p>k j
i . (Take a moment to convince yourself of
</p>
<p>(1.122). You can set N = 1 and f = 2, for example, and compute &part;L/&part; q̇1 explicitly.
This should give you some idea about the essential steps involved.) Using (1.106),</p>
<p/>
</div>
<div class="page"><p/>
<p>28 1 Classical Mechanics
</p>
<p>we arrive at
</p>
<p>h =
f
</p>
<p>&sum;
n=1
</p>
<p>&part;L
</p>
<p>&part; q̇n
q̇n &minus;L =
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mi
</p>
<p>[
f
</p>
<p>&sum;
j=1
</p>
<p>f
</p>
<p>&sum;
n=1
</p>
<p>A
jn
i q̇ jq̇n +
</p>
<p>f
</p>
<p>&sum;
n=1
</p>
<p>Bni q̇n
</p>
<p>]
&minus;L
</p>
<p>=
1
</p>
<p>2
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mi
</p>
<p>[
f
</p>
<p>&sum;
j=1
</p>
<p>f
</p>
<p>&sum;
k=1
</p>
<p>A
jk
i q̇ jq̇k &minus;Ci
</p>
<p>]
+U(q1, . . . ,q f , t) . (1.123)
</p>
<p>But, according to (1.119) and (1.120), the total mechanical energy of the system is
</p>
<p>E =
1
</p>
<p>2
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mi
</p>
<p>[
f
</p>
<p>&sum;
j=1
</p>
<p>f
</p>
<p>&sum;
k=1
</p>
<p>A
jk
i q̇ jq̇k +2
</p>
<p>f
</p>
<p>&sum;
j=1
</p>
<p>B
j
i q̇ j +Ci
</p>
<p>]
+U(q1, . . . ,q f , t) . (1.124)
</p>
<p>Evidently, if B
j
i &equiv; 0 and Ci &equiv; 0 for all i = 1, . . . ,N, then h = E. In view of (1.118),
</p>
<p>this implies that ri&rsquo;s do not depend explicitly on t. So, if the relation between ri&rsquo;s
</p>
<p>and qi&rsquo;s is of the form of
</p>
<p>ri = ri(q1, . . . ,q f ) , i = 1, . . . ,N (1.125)
</p>
<p>in place of (1.115), then the energy function coincides with the mechanical energy
</p>
<p>of the system.
</p>
<p>1.8 Conservation Laws and Symmetry
</p>
<p>As a mechanical system evolves according to the equations of motion, its general-
</p>
<p>ized coordinates and velocities change. If we construct a function of these variables,
</p>
<p>its value will, in general, change with them. However, the value of a certain com-
</p>
<p>bination of the generalized coordinates and velocities may remain constant. Such a
</p>
<p>quantity is called a constant of motion or a conserved quantity. We have already
</p>
<p>encountered one such quantity, that is, energy, in Sects. 1.2.5 and 1.3.1. Constants
</p>
<p>of motions are of great interest since they allow us to extract certain information
</p>
<p>about the mechanical behavior of the system without actually solving the equations
</p>
<p>of motion.
</p>
<p>In general, the Lagrangian of a mechanical system is a function of generalized
</p>
<p>coordinates and generalized velocities. It may also depend explicitly on time. In
</p>
<p>certain problems, however, it may so happen that L remains unchanged when these
</p>
<p>variables undergo certain changes. As we shall see below, this, in turn, points to an
</p>
<p>existence of some conserved quantity.
</p>
<p>Let us start by looking at a somewhat trivial example. We suppose that L is inde-
</p>
<p>pendent of qi. Then,
&part;L
</p>
<p>&part;qi
&equiv; 0 . (1.126)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.8 Conservation Laws and Symmetry 29
</p>
<p>From Lagrange&rsquo;s equation of motion,
</p>
<p>dpi
</p>
<p>dt
=
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; q̇i
</p>
<p>)
=
</p>
<p>&part;L
</p>
<p>&part;qi
&equiv; 0 . (1.127)
</p>
<p>Thus, if L is independent of qi, then, its conjugate momentum pi is a constant of
</p>
<p>motion. Now you are ready to look at something a little more profound.
</p>
<p>1.8.1 Conservation of Linear Momentum
</p>
<p>Let us consider a mechanical system A consisting of N particles. In the absence of
</p>
<p>any constraint that reduces the mechanical degrees of freedom, its Lagrangian is
</p>
<p>given by
</p>
<p>LA = L(r1, . . . ,rN , ṙ1, . . . , ṙN , t) , (1.128)
</p>
<p>Given this system A, imagine that we created its identical copy B, and placed system
</p>
<p>B some distance away from system A. These systems are then related by a linear
</p>
<p>translation, say by ∆r. Thus, the position r&prime;i (with respect to the same origin as
system A) of the ith particle in system B is given by
</p>
<p>r&prime;i = ri +∆r , i = 1, . . . ,N . (1.129)
</p>
<p>By &ldquo;identical copy,&rdquo; we also mean that
</p>
<p>ṙ&prime;i = ṙi , i = 1, . . . ,N , (1.130)
</p>
<p>that is, ṙi&rsquo;s remain unaffected by the linear translation. Now the question is this:
</p>
<p>What is the relationship between the Lagrangians LA and LB of these two systems?
</p>
<p>Here, it should be clearly noted that the comparison is between LA when system A
</p>
<p>alone is present and LB when system B alone is present. Thus, there is no interaction
</p>
<p>between A and B.
</p>
<p>Since these two systems are identical in all respect except for its position in space,
</p>
<p>we have no reason to evaluate LB any differently from LA. That is, we use the same
</p>
<p>function L to evaluate LA and LB, the only difference being in the values assumed
</p>
<p>by its arguments:
</p>
<p>LB = L(r1 +∆r, . . . ,rN +∆r, ṙ1, . . . , ṙN , t) . (1.131)
</p>
<p>If these two systems are far a way from any source of external field, there will be
</p>
<p>nothing that can distinguish the region occupied by system A and that by system B.
</p>
<p>This is referred to as the homogeneity of space. Less formally, one may say that
</p>
<p>&ldquo;here&rdquo; is as good as any other place. Therefore, we should expect that LB = LA. That
is, the value of L remains unaffected by a linear translation by ∆r. Such an L is said
to be translationally invariant.</p>
<p/>
</div>
<div class="page"><p/>
<p>30 1 Classical Mechanics
</p>
<p>As an example, we recall the system considered in Sect. 1.3.1, which has a trans-
</p>
<p>lationally invariant L if ψ &equiv; 0, that is, in the absence of an external field. Clearly, ψ
remains zero even after a displacement by ∆r of the mechanical system. The kinetic
energy of the system depends only on mass and velocity of each particle. So, it
</p>
<p>remains unaffected as well. Likewise for the potential energy φ due to interaction
among particles, which depends only on their relative position. Thus, LA = LB.
</p>
<p>We now explore the consequence of the translational invariance of L. It is suffi-
</p>
<p>cient to restrict ourselves to an infinitesimal translation and replace ∆r by δ r. From
(1.128) and (1.131), we find
</p>
<p>LB &minus;LA =
N
</p>
<p>&sum;
i=1
</p>
<p>(
&part;L
</p>
<p>&part; ri
&middot;δ r
</p>
<p>)
+h.o., (1.132)
</p>
<p>where we recall that
</p>
<p>&part;L
</p>
<p>&part; ri
</p>
<p>.
=
</p>
<p>(
&part;L
</p>
<p>&part;xi
,
&part;L
</p>
<p>&part;yi
,
&part;L
</p>
<p>&part; zi
</p>
<p>)
and δ r
</p>
<p>.
= (δx,δy,δ z) , (1.133)
</p>
<p>and hence
&part;L
</p>
<p>&part; ri
&middot;δ r= &part;L
</p>
<p>&part;xi
δx+
</p>
<p>&part;L
</p>
<p>&part;yi
δy+
</p>
<p>&part;L
</p>
<p>&part; zi
δ z . (1.134)
</p>
<p>From Lagrange&rsquo;s equations of motion and (1.104), we have
</p>
<p>&part;L
</p>
<p>&part; ri
=
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; ṙi
</p>
<p>)
=
</p>
<p>dpi
dt
</p>
<p>. (1.135)
</p>
<p>Thus, the first-order term of (1.132), which we denote by δL, is given by
</p>
<p>δL :=
N
</p>
<p>&sum;
i=1
</p>
<p>(
dpi
dt
</p>
<p>&middot;δ r
)
= δ r &middot;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>dpi
dt
</p>
<p>= δ r &middot; d
dt
</p>
<p>(
N
</p>
<p>&sum;
i=1
</p>
<p>pi
</p>
<p>)
, (1.136)
</p>
<p>where we pulled δ r out of the summation since it is common to all particles and
hence is independent of i.
</p>
<p>If L is translationally invariant, δL must be identically zero for any δ r, from
which we deduce
</p>
<p>d
</p>
<p>dt
</p>
<p>(
N
</p>
<p>&sum;
i=1
</p>
<p>pi
</p>
<p>)
&equiv; 0 . (1.137)
</p>
<p>It follows that the total linear momentum defined by
</p>
<p>P :=
N
</p>
<p>&sum;
i=1
</p>
<p>pi (1.138)
</p>
<p>is a constant of motion. In this way, the conservation of the total linear momentum
</p>
<p>is seen to be a consequence of the translational invariance of L, and hence of the
</p>
<p>homogeneity of space.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.8 Conservation Laws and Symmetry 31
</p>
<p>.
....................
</p>
<p>....................
</p>
<p>.....................
</p>
<p>.....................
</p>
<p>ri
</p>
<p>ri ri ri
</p>
<p>i
</p>
<p>Fig. 1.10 Change in ri upon rotation.
</p>
<p>It may be that L is invariant with respect to translation in a direction parallel to
</p>
<p>the xy-plane, but not in the z-direction. This will be the case, for example, for a
</p>
<p>mechanical system moving in a uniform gravitational field if the z-axis is chosen to
</p>
<p>align with the gravitational acceleration g. By considering the displacement δ r that
is parallel to the xy-plane, we observe that the x- and y- components of P are still
</p>
<p>constants of motion in this case. The z-component, however, is not. You can easily
</p>
<p>confirm this by allowing an object to free fall toward the floor: Pz of the object will
</p>
<p>continue to change with time.
</p>
<p>1.8.2 &Dagger;Conservation of Angular Momentum
</p>
<p>Suppose that system B is related to system A by an infinitesimal rotation around
</p>
<p>some axis by δφ . Again we expect that LA = LB. This is a consequence of the
isotropy of space, that is, all directions in space are equivalent.
</p>
<p>We need to relate ri&rsquo;s and ṙi&rsquo;s of system B to those of system A. From Fig. 1.10,
</p>
<p>we see that
</p>
<p>||δ ri||= ri sinθiδφ , (1.139)
where θi is the angle between the axis of rotation and ri. Let us denote by δφ the
vector of length δφ that points toward the direction a right-hand screw advances
upon rotation by δφ . Note that δφ is not a variation of some vector φ. Rather, it is
a vector of infinitesimal length ||δφ||, where the &ldquo;length&rdquo; is an angle measured in
radian in this case. Since δ ri is perpendicular to the plane including ri and δφ, we
see that δ ri is parallel to the vector δφ&times; ri, where &ldquo;&times;&rdquo; denotes the cross product
of two vectors. (See Appendix A.6 for a brief review.) Combining this observation
</p>
<p>with (1.139), we arrive at
</p>
<p>δ ri = δφ&times; ri , (1.140)
The same argument applies for ṙi as well, leading to
</p>
<p>δ ṙi = δφ&times; ṙi . (1.141)</p>
<p/>
</div>
<div class="page"><p/>
<p>32 1 Classical Mechanics
</p>
<p>Thus, to the first order of δφ , LB &minus;LA is given by
</p>
<p>δL =
N
</p>
<p>&sum;
i=1
</p>
<p>[
&part;L
</p>
<p>&part; r i
&middot;δ ri +
</p>
<p>&part;L
</p>
<p>&part; ṙi
&middot;δ ṙi
</p>
<p>]
=
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>[
&part;L
</p>
<p>&part; ri
&middot;δφ&times; ri +
</p>
<p>&part;L
</p>
<p>&part; ṙi
&middot;δφ&times; ṙi
</p>
<p>]
. (1.142)
</p>
<p>Using (1.89) and (1.104),
</p>
<p>δL =
N
</p>
<p>&sum;
i=1
</p>
<p>[
d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; ṙi
</p>
<p>)
&middot;δφ&times; ri +
</p>
<p>&part;L
</p>
<p>&part; ṙi
&middot;δφ&times; ṙi
</p>
<p>]
=
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>(ṗi &middot;δφ&times; ri +pi &middot;δφ&times; ṙi) .
</p>
<p>(1.143)
</p>
<p>With the help of a vector identity (A.32), we may rewrite this as
</p>
<p>δL = δφ &middot;
N
</p>
<p>&sum;
i=1
</p>
<p>(ri &times; ṗi + ṙi &times;pi) = δφ &middot;
d
</p>
<p>dt
</p>
<p>(
N
</p>
<p>&sum;
i=1
</p>
<p>ri &times;pi
</p>
<p>)
. (1.144)
</p>
<p>Since δL &equiv; 0 for any δφ, we conclude that the total angular momentum defined
by
</p>
<p>M :=
N
</p>
<p>&sum;
i=1
</p>
<p>ri &times;pi (1.145)
</p>
<p>is a constant of motion. We see that the conservation of total angular momentum
</p>
<p>follows from the rotational invariance of L, and hence from the isotropy of space.
</p>
<p>Exercise 1.10. Motion of a particle subject to a central field, that is, one in which the
</p>
<p>potential energy depends only on the distance from some particular point in space,
</p>
<p>is confined to a plane containing that point. Why? ///
</p>
<p>1.8.3 Conservation of Energy
</p>
<p>Returning now to a general mechanical system with f mechanical degrees of free-
</p>
<p>dom, we suppose that its L does not depend explicitly on time:
</p>
<p>L = L(q1, . . . ,q f , q̇1, . . . , q̇ f ) . (1.146)
</p>
<p>Since qi and q̇i are functions of t, L still depends on t. But such a time dependence
</p>
<p>is said to be implicit. Using the chain rule,
</p>
<p>dL
</p>
<p>dt
=
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>&part;L
</p>
<p>&part;qi
q̇i +
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>&part;L
</p>
<p>&part; q̇i
q̈i . (1.147)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.8 Conservation Laws and Symmetry 33
</p>
<p>We can rewrite the first term on the right-hand side using Lagrange&rsquo;s equation of
</p>
<p>motion (1.89) and write
</p>
<p>dL
</p>
<p>dt
=
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>[
d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; q̇i
</p>
<p>)
q̇i +
</p>
<p>&part;L
</p>
<p>&part; q̇i
q̈i
</p>
<p>]
=
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>d
</p>
<p>dt
</p>
<p>(
&part;L
</p>
<p>&part; q̇i
q̇i
</p>
<p>)
, (1.148)
</p>
<p>from which we find
</p>
<p>d
</p>
<p>dt
</p>
<p>(
f
</p>
<p>&sum;
i=1
</p>
<p>&part;L
</p>
<p>&part; q̇i
q̇i &minus;L
</p>
<p>)
= 0 . (1.149)
</p>
<p>Recalling (1.106) and our assumption that h = E, we see that the energy is a con-
served quantity.
</p>
<p>What will happen if we allowed for an explicit time dependence of L? This will
</p>
<p>be the case if there is a time-dependent external field. Carrying out the same analysis
</p>
<p>while retaining &part;L/&part; t, we observe that (1.149) is replaced by
</p>
<p>dE
</p>
<p>dt
=&minus;&part;L
</p>
<p>&part; t
. (1.150)
</p>
<p>Clearly, E is a constant of motion if and only if &part;L/&part; t &equiv; 0.
</p>
<p>Exercise 1.11. Derive (1.150). ///
</p>
<p>The procedure we used to deduce the law of conservation of energy admits the
</p>
<p>following curious interpretation. Suppose you conducted a certain experiment today
</p>
<p>and plan on repeating the identical experiment tomorrow. Within mechanics, this
</p>
<p>means that you start the experiment from the identical initial conditions as specified
</p>
<p>by the generalized coordinates and velocities. You should naturally expect to find
</p>
<p>the identical outcome.6 So, you have no reason to change the form of the function L
</p>
<p>between today (tA) and tomorrow (tB). You would also expect that its value remains
</p>
<p>the same for an identical mechanical state. In other words, each instant of time is
</p>
<p>equivalent to any other. This is referred to as the homogeneity of time. We demand,
</p>
<p>therefore, that the relation
</p>
<p>L(q1, . . . ,q f , q̇1, . . . , q̇ f , t
A) = L(q1, . . . ,q f , q̇1, . . . , q̇ f , t
</p>
<p>B) , (1.151)
</p>
<p>hold for any tA and tB. It follows that
</p>
<p>&part;L
</p>
<p>&part; t
&equiv; 0 . (1.152)
</p>
<p>Equation (1.150) then indicates that E is a constant of motion. Energy is a constant
</p>
<p>of motion because &ldquo;now&rdquo; is as good as any other instant of time.
</p>
<p>One might argue that the outcome of the two experiments should be identical
</p>
<p>even if there is a time-dependent external field. But, this is so only if timing of the
</p>
<p>two experiments is properly chosen to make sure that they are in sync with the time-
</p>
<p>dependent external field in an identical manner. In other words, (1.151) holds only
</p>
<p>for properly chosen pairs of tA and tB. From this, we cannot deduce (1.152).</p>
<p/>
</div>
<div class="page"><p/>
<p>34 1 Classical Mechanics
</p>
<p>Example 1.6. Symmetry and constants of motion: Suppose that a system con-
</p>
<p>sisting of many particles moves in an external field generated by fixed sources
</p>
<p>uniformly distributed on an infinite plane, say, the xy-plane (on which z = 0).
In this case, L is invariant with respect to (1) translation in t, (2) translation
</p>
<p>parallel to the xy-plane, and, if you read Sect. 1.8.2, (3) rotation around any
</p>
<p>axis perpendicular to the xy-plane. So, the energy, the x- and y-components
</p>
<p>of the linear momentum, and the z-component of the angular momentum are
</p>
<p>constants of motion.
</p>
<p>If the source of the field is confined to x &gt; 0, the system is invariant only
with respect to (1) translation in t and (2) translation along the y-axis. So, con-
</p>
<p>stants of motion are the energy and the y-component of the linear momentum.
</p>
<p>1.9 Hamilton&rsquo;s Equations of Motion
</p>
<p>Lagrange&rsquo;s equations of motion are the relationships among the partial derivatives
</p>
<p>of L with respect to its independent variables q1, . . . , q f and q̇1, . . . , q̇ f . When sup-
</p>
<p>plemented with initial conditions, these equations of motion tell us how q1, . . . , q f
and q̇1, . . . , q̇ f evolve with time. Thus, we may say that the function L, if expressed
</p>
<p>in terms of q1, . . . , q f , q̇1, . . . , q̇ f , and t, contains within it the mechanical behavior
</p>
<p>of the system in its entirety.
</p>
<p>Suppose now that, for whatever reason, we wanted to describe mechanics of the
</p>
<p>system using p1, . . . , p f instead of q̇1, . . . , q̇ f as our independent variables. This may
</p>
<p>require a new function to replace L. We would certainly want our new approach to be
</p>
<p>equally useful as the Lagrangian-based approach. We can ensure this by demanding
</p>
<p>that this new function contains the same information as the Lagrangian. In other
</p>
<p>words, we demand not only that we can construct this function from the Lagrangian,
</p>
<p>but also that we can recover the original Lagrangian from it. That way, even in the
</p>
<p>worst case scenario in which equations of motion cannot be derived directly from
</p>
<p>the new function, we can at least derive Lagrange&rsquo;s equations of motion by first
</p>
<p>finding the Lagrangian from our new function.
</p>
<p>Our goal then is to replace q̇i by pi = &part;L/&part; q̇i while preserving the informa-
tion content of L. There is a mathematical procedure called Legendre transforma-
</p>
<p>tion designed just for this purpose. A brief introduction to this method is given in
</p>
<p>Appendix C, in which we present the Legendre transformation of a function of a
</p>
<p>single variable. In contrast, we are now interested in replacing multiple variables,
</p>
<p>q̇1, . . . , q̇ f , by p1, . . . , p f . In the next section, we show in detail how this can be
</p>
<p>accomplished by replacing one variable at a time, thus performing the Legendre
</p>
<p>transformation f times. Here, we take for granted that the transformation described
</p>
<p>in Appendix C generalizes naturally to the case of simultaneous replacement of
</p>
<p>multiple variables and proceed as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>1.9 Hamilton&rsquo;s Equations of Motion 35
</p>
<p>Step 1: Given the Lagrangian
</p>
<p>L(q1, . . . ,q f , q̇1, . . . , q̇ f , t) , (1.153)
</p>
<p>calculate pi by (1.103) for i = 1, . . . , f , thus finding p1, . . . , p f as functions of
q1, . . . , q f and q̇1, . . . , q̇ f :
</p>
<p>pi = pi(q1, . . . ,q f , q̇1, . . . , q̇ f ) , i = 1, . . . , f . (1.154)
</p>
<p>Step 2: Solve these f equations for q̇1, . . . , q̇ f to express them as functions of q1,
</p>
<p>. . . , q f and p1, . . . , p f :
</p>
<p>q̇i = q̇i(q1, . . . ,q f , p1, . . . , p f ) , i = 1, . . . , f . (1.155)
</p>
<p>Step 3: Express
</p>
<p>H :=
f
</p>
<p>&sum;
i=1
</p>
<p>piq̇i &minus;L (1.156)
</p>
<p>as a function of q1, . . . , q f and p1, . . . , p f . This makes H the negative of the
</p>
<p>Legendre transform of L. The function
</p>
<p>H(q1, . . . ,q f , p1, . . . , p f , t) (1.157)
</p>
<p>is called the Hamiltonian of the system. From (1.103) and (1.106), we see that
</p>
<p>Hamiltonian is the energy (function) of the system expressed as a function of q1,
</p>
<p>. . . , q f and p1, . . . , p f .
</p>
<p>By construction, L and H contain the same information. But, before we can
</p>
<p>accept the Hamiltonian as a useful concept, we must be able to derive from H the
</p>
<p>equations of motion that relate q1, . . . , q f and p1, . . . , p f to their time derivatives.
</p>
<p>From (1.156),
</p>
<p>dH =
f
</p>
<p>&sum;
i=1
</p>
<p>(pidq̇i + q̇idpi)&minus;dL . (1.158)
</p>
<p>From (1.153),
</p>
<p>dL =
f
</p>
<p>&sum;
i=1
</p>
<p>(
&part;L
</p>
<p>&part;qi
dqi +
</p>
<p>&part;L
</p>
<p>&part; q̇i
dq̇i
</p>
<p>)
+
</p>
<p>&part;L
</p>
<p>&part; t
dt =
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>(ṗidqi + pidq̇i)+
&part;L
</p>
<p>&part; t
dt , (1.159)
</p>
<p>where we used (1.89) and (1.103). Combining (1.158) and (1.159)
</p>
<p>dH =
f
</p>
<p>&sum;
i=1
</p>
<p>(&minus; ṗidqi + q̇idpi)&minus;
&part;L
</p>
<p>&part; t
dt . (1.160)</p>
<p/>
</div>
<div class="page"><p/>
<p>36 1 Classical Mechanics
</p>
<p>On the other hand, (1.157) indicates that
</p>
<p>dH =
f
</p>
<p>&sum;
i=1
</p>
<p>(
&part;H
</p>
<p>&part;qi
dqi +
</p>
<p>&part;H
</p>
<p>&part; pi
dpi
</p>
<p>)
+
</p>
<p>&part;H
</p>
<p>&part; t
dt . (1.161)
</p>
<p>Comparing (1.160) and (1.161), we conclude that
</p>
<p>&part;H
</p>
<p>&part; pi
= q̇i ,
</p>
<p>&part;H
</p>
<p>&part;qi
=&minus; ṗi , and
</p>
<p>&part;H
</p>
<p>&part; t
=&minus;&part;L
</p>
<p>&part; t
, i = 1, . . . , f . (1.162)
</p>
<p>The first two of these equations, expressing q̇1, . . . , q̇ f and ṗ1, . . . , ṗ f in terms of
</p>
<p>q1, . . . , q f , p1, . . . , p f , and t are called Hamilton&rsquo;s equations of motion. The last
</p>
<p>equation is an identity relating the partial derivatives of H and L with respect to t
</p>
<p>but plays no role in determining the time evolution of q1, . . . , q f and p1, . . . , p f .
</p>
<p>Because H = E, it seems we have two symbols referring to the same quantity.
We emphasize, however, that the Hamiltonian H refers to the energy of a mechani-
</p>
<p>cal system expressed as a function of generalized coordinates q1, . . . , q f and their
</p>
<p>conjugate momenta p1, . . . , p f . In contrast, the energy expressed as a function of
</p>
<p>other variables is not a Hamiltonian. Such a function will be denoted by E. On occa-
</p>
<p>sion, we also use E to refer to the value of H when q1, . . . , q f and p1, . . . , p f assume
</p>
<p>certain values.
</p>
<p>Example 1.7. Particle moving in a conservative field: Consider a particle of
</p>
<p>mass m moving in a conservative potential field ψ(r) with velocity v. The
kinetic energy of the particle is given by
</p>
<p>1
</p>
<p>2
mv &middot; v , (1.163)
</p>
<p>and hence
</p>
<p>L =
1
</p>
<p>2
mv &middot; v&minus;ψ(r) . (1.164)
</p>
<p>By definition, the momentum p that is conjugate to v is
</p>
<p>p=
&part;L
</p>
<p>&part;v
= mv , (1.165)
</p>
<p>and hence
</p>
<p>v=
p
</p>
<p>m
. (1.166)
</p>
<p>The Hamiltonian is obtained as follows:
</p>
<p>H = p &middot; v&minus;L = p &middot; p
m
&minus; 1
</p>
<p>2
m
p
</p>
<p>m
&middot; p
</p>
<p>m
+ψ(r) =
</p>
<p>p &middot;p
2m
</p>
<p>+ψ(r) . (1.167)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.10 &dagger;Routhian 37
</p>
<p>From Hamilton&rsquo;s equations of motion,
</p>
<p>ṙ=
&part;H
</p>
<p>&part;p
=
</p>
<p>p
</p>
<p>m
and ṗ=&minus;&part;H
</p>
<p>&part; r
=&minus;&part;ψ
</p>
<p>&part; r
. (1.168)
</p>
<p>Eliminating p from these two equations, we arrive at
</p>
<p>mr̈=&minus;&part;ψ
&part; r
</p>
<p>, (1.169)
</p>
<p>which is Newton&rsquo;s equation of motion of the particle.
</p>
<p>Exercise 1.12. Translate the above example using components of the vectors
</p>
<p>involved. ///
</p>
<p>Exercise 1.13. By taking the partial derivative of (1.156) with respect to t while
</p>
<p>holding q1, . . . , q f and p1, . . . , p f constant, directly establish the third equation in
</p>
<p>(1.162). ///
</p>
<p>Exercise 1.14. We obtained (1.162) from (1.153), (1.156), (1.157), and Lagrange&rsquo;s
</p>
<p>equations of motion. Since Lagrangian and Hamiltonian are equivalent ways of
</p>
<p>encoding the mechanical behavior of the system, it should be possible to reverse this
</p>
<p>process. Combining (1.162) with (1.153), (1.156), and (1.157), derive Lagrange&rsquo;s
</p>
<p>equations of motion. ///
</p>
<p>1.10 &dagger;Routhian
</p>
<p>Given the Lagrangian as in (1.153), we have
</p>
<p>p1 =
&part;L
</p>
<p>&part; q̇1
= p1(q1, . . . ,q f , q̇1, . . . , q̇ f , t) , (1.170)
</p>
<p>which may be solved for q̇1 to yield
</p>
<p>q̇1 = q̇1(q1, . . . ,q f , p1, q̇2, . . . , q̇ f , t) . (1.171)
</p>
<p>Using this equation, we express a new function
</p>
<p>R1 := L&minus; p1q̇1 (1.172)
</p>
<p>as a function of q1, . . . , q f , p1, q̇2, . . . , q̇ f , and t:
</p>
<p>R1 = R1(q1, . . . ,q f , p1, q̇2, . . . , q̇ f , t) . (1.173)
</p>
<p>A partial Legendre transformation, such as R1, is called a Routhian and is an equally
</p>
<p>valid encoding of the mechanical behavior of the system as L and H.</p>
<p/>
</div>
<div class="page"><p/>
<p>38 1 Classical Mechanics
</p>
<p>Before we can proceed to the next round of Legendre transformation, in which
</p>
<p>we replace q̇2 by p2, we need to evaluate &part;R1/&part; q̇2. From (1.172), we see that
</p>
<p>dR1 = dL&minus; p1dq̇1 &minus; q̇1dp1
</p>
<p>=
f
</p>
<p>&sum;
i=1
</p>
<p>&part;L
</p>
<p>&part;qi
dqi +
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>&part;L
</p>
<p>&part; q̇i
dq̇i +
</p>
<p>&part;L
</p>
<p>&part; t
dt &minus; p1dq̇1 &minus; q̇1dp1
</p>
<p>=
f
</p>
<p>&sum;
i=1
</p>
<p>&part;L
</p>
<p>&part;qi
dqi +
</p>
<p>f
</p>
<p>&sum;
i=2
</p>
<p>&part;L
</p>
<p>&part; q̇i
dq̇i +
</p>
<p>&part;L
</p>
<p>&part; t
dt &minus; q̇1dp1, (1.174)
</p>
<p>where we used (1.103). It follows that
</p>
<p>&part;R1
&part; q̇2
</p>
<p>=
&part;L
</p>
<p>&part; q̇2
= p2. (1.175)
</p>
<p>It is important to note that the partial derivative on the left is evaluated by holding
</p>
<p>q1, . . . , q f , p1, q̇3, . . . , q̇ f , and t constant. In contrast, that on the right is for constant
</p>
<p>q1, . . . , q f , q̇1, q̇3, . . . , q̇ f , and t.
</p>
<p>The left most expression of (1.175) gives p2 as a function of q1, . . . , q f , p1, q̇2,
</p>
<p>. . . , q̇ f , and t. When this is solved for q̇2, we obtain
</p>
<p>q̇2 = q̇2(q1, . . . ,q f , p1, p2, q̇3, . . . , q̇ f , t) . (1.176)
</p>
<p>Using this equation, we now express
</p>
<p>R2 := R1 &minus; p2q̇2 = L&minus; p1q̇1 &minus; p2q̇2 (1.177)
</p>
<p>as a function of q1, . . . , q f , p1, p2, q̇3, . . . , q̇ f , and t. The resulting function is another
</p>
<p>Routhian. Continuing in this way, we finally arrive at
</p>
<p>R f := L&minus;
f
</p>
<p>&sum;
i=1
</p>
<p>piq̇i = R f (q1, . . . ,q f , p1, . . . , p f , t) . (1.178)
</p>
<p>Now that all of q̇1, . . . , q̇ f are replaced by p1, . . . , p f , we do not refer to R f as a
</p>
<p>Routhian. Instead, it is the negative of the Hamiltonian H.
</p>
<p>Exercise 1.15. Derive the following set of equations of motion from the Routhian
</p>
<p>Rn:
&part;Rn
&part;qi
</p>
<p>= ṗi ,
&part;Rn
&part; pi
</p>
<p>=&minus;q̇i , i = 1, . . . ,n , (1.179)
</p>
<p>and
d
</p>
<p>dt
</p>
<p>(
&part;Rn
&part; q̇i
</p>
<p>)
&minus; &part;Rn
</p>
<p>&part;qi
= 0 , i = n+1, . . . , f . (1.180)
</p>
<p>///
</p>
<p>Equations of motion derived from a Routhian find practical applications in an
</p>
<p>analysis of the stability of a steady motion.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.11 Poisson Bracket 39
</p>
<p>1.11 Poisson Bracket
</p>
<p>Quantities such as the energy and the total linear momentum of a system are called
</p>
<p>dynamical variables. More generally, we define a dynamical variable A as a func-
</p>
<p>tion of the instantaneous state of the mechanical system, which is specified by q f
</p>
<p>and p f :
</p>
<p>A(q f , p f , t) . (1.181)
</p>
<p>Here we allowed for an explicit time dependence of A. We also introduced a new
</p>
<p>notation in which q f and p f stand for q1, . . . , q f and p1, . . . , p f , respectively. Note
</p>
<p>carefully the distinction between q f and q f . The former is the collective notation
</p>
<p>just introduced, while the latter refers to the f th generalized coordinate.
</p>
<p>As the coordinates and momenta change in accordance with the laws of mechan-
</p>
<p>ics, the value of A will, in general, change as well. It is straightforward to find the
</p>
<p>expression for the time rate of change of A. Differentiating (1.181) with respect to
</p>
<p>time, we find
</p>
<p>dA
</p>
<p>dt
=
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>(
&part;A
</p>
<p>&part;qi
q̇i +
</p>
<p>&part;A
</p>
<p>&part; pi
ṗi
</p>
<p>)
+
</p>
<p>&part;A
</p>
<p>&part; t
. (1.182)
</p>
<p>Using Hamilton&rsquo;s equations of motion, we may rewrite the right-hand side as
</p>
<p>dA
</p>
<p>dt
=
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>(
&part;A
</p>
<p>&part;qi
</p>
<p>&part;H
</p>
<p>&part; pi
&minus; &part;H
</p>
<p>&part;qi
</p>
<p>&part;A
</p>
<p>&part; pi
</p>
<p>)
+
</p>
<p>&part;A
</p>
<p>&part; t
. (1.183)
</p>
<p>We introduce the Poisson bracket of two dynamical variables A and B, which is
</p>
<p>defined by
</p>
<p>{A,B} :=
f
</p>
<p>&sum;
i=1
</p>
<p>(
&part;A
</p>
<p>&part;qi
</p>
<p>&part;B
</p>
<p>&part; pi
&minus; &part;B
</p>
<p>&part;qi
</p>
<p>&part;A
</p>
<p>&part; pi
</p>
<p>)
, (1.184)
</p>
<p>and write (1.183) more compactly as
</p>
<p>dA
</p>
<p>dt
= {A,H}+ &part;A
</p>
<p>&part; t
. (1.185)
</p>
<p>By definition, A is a constant of motion if dA/dt &equiv; 0. It should be noted that the
definition does not demand &part;A/&part; t to be zero. Thus, a dynamical variable can be a
constant of motion even if it has an explicit time dependence. The Hamiltonian is
</p>
<p>an exception to this rule. In fact, if we set A = H in (1.185),
</p>
<p>dH
</p>
<p>dt
= {H,H}+ &part;H
</p>
<p>&part; t
=
</p>
<p>&part;H
</p>
<p>&part; t
. (1.186)
</p>
<p>So, H is a constant of motion if and only if H does not depend explicitly on time.
</p>
<p>As another example of dynamical variables, let us consider qk. Using (1.185), we
</p>
<p>find
</p>
<p>q̇k = {qk,H}+
&part;qk
&part; t
</p>
<p>. (1.187)</p>
<p/>
</div>
<div class="page"><p/>
<p>40 1 Classical Mechanics
</p>
<p>As in (1.183), the partial derivative with respect to t is taken while holding q f and
</p>
<p>p f constant. Thus, &part;qk/&part; t = 0 and we obtain
</p>
<p>q̇k = {qk,H} . (1.188)
</p>
<p>Similarly, by setting A = pk, we find
</p>
<p>ṗk = {pk,H} . (1.189)
</p>
<p>As seen in Exercise 1.16, (1.188) and (1.189) are nothing but Hamilton&rsquo;s equations
</p>
<p>of motion.
</p>
<p>We note that, since q&rsquo;s and p&rsquo;s are independent variables, we have &part; pk/&part;qi &equiv; 0
and &part;qk/&part; pi &equiv; 0 regardless of the values of i and k. On the other hand, &part;qk/&part;qi is
unity if i = k and zero if i 	= k, that is,
</p>
<p>&part;qk
&part;qi
</p>
<p>= δik and
&part; pk
&part; pi
</p>
<p>= δik , (1.190)
</p>
<p>where δik is the Kronecker delta, which takes the value unity if i = k and zero
otherwise.
</p>
<p>Exercise 1.16. Let A be a dynamical variable of a mechanical system with f degrees
</p>
<p>of freedom, that is, A = A(q f , p f , t). Evaluate {q j,A} and {p j,A}. Then, show that
(1.188) and (1.189) are Hamilton&rsquo;s equations of motion. ///
</p>
<p>Exercise 1.17. Using (1.190), show that
</p>
<p>{qi,q j}= 0 , {qi, p j}= δi j , and {pi, p j}= 0 . (1.191)
</p>
<p>///
</p>
<p>The Poisson bracket satisfies a number of identities:
</p>
<p>{A,A}= 0 , (1.192)
{c,A}= 0 , (1.193)
</p>
<p>{A,B+C}= {A,B}+{A,C} , (1.194)
{A,BC}= B{A,C}+{A,B}C , (1.195)
</p>
<p>where c is a real number. Through a straightforward but lengthy computation, one
</p>
<p>can prove Jacobi&rsquo;s identity:
</p>
<p>{A,{B,C}}+{B,{C,A}}+{C,{A,B}}= 0 . (1.196)
</p>
<p>With the help of this identity, we can show that, if A and B are constants of motion,
</p>
<p>then so is {A,B}. This is known as the Poisson theorem, which, on occasion, allows
us to generate a new constant of motion from known constants of motion.
</p>
<p>Exercise 1.18. Prove the Poisson theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.12 Frequently Used Symbols 41
</p>
<p>a. For simplicity, first assume that neither A nor B depends explicitly on time.
</p>
<p>b. Generalize the theorem when A and B may depend explicitly on time. ///
</p>
<p>Finally, there appears to be no general agreement on the sign of the Poisson
</p>
<p>bracket. For example, Ref. [3] defines it as
</p>
<p>{A,H}=
f
</p>
<p>&sum;
i=1
</p>
<p>(
&part;A
</p>
<p>&part; pi
</p>
<p>&part;H
</p>
<p>&part;qi
&minus; &part;H
</p>
<p>&part; pi
</p>
<p>&part;A
</p>
<p>&part;qi
</p>
<p>)
, (1.197)
</p>
<p>which has the opposite sign to what we defined.
</p>
<p>1.12 Frequently Used Symbols
</p>
<p>A := B , the symbol A is defined by the expression B.
A =: B , the expression A defines the symbol B.
a
.
= (ax,ay,az) , the x-, y-, z-components of the vector a are ax, ay, and az,
</p>
<p>respectively.
</p>
<p>||a|| , length of the vector a.
dφ , the first order term of ∆φ .
∆φ , change in φ including the higher order terms.
</p>
<p>f , the number of mechanical degrees of freedom.
</p>
<p>g , gravitational acceleration.
</p>
<p>h , energy function.
</p>
<p>h.o. , higher order terms of Taylor series expansion, typically second order and
</p>
<p>higher.
</p>
<p>m , mass of a particle.
</p>
<p>pi , generalized momentum conjugate to qi.
</p>
<p>p f , collective notation for p1, . . . , p f .
</p>
<p>pi , linear momentum of the ith particle.
</p>
<p>qi , the ith generalized coordinate.
</p>
<p>q f , collective notation for q1, . . . , q f .
</p>
<p>ri , position vector of the ith particle.
</p>
<p>t , time.
</p>
<p>vi , velocity vector of the ith particle.
</p>
<p>E , mechanical energy.
</p>
<p>F , force.
</p>
<p>H , Hamiltonian.
</p>
<p>L , Lagrangian.
</p>
<p>M , total mass of a many-particle system.
</p>
<p>M , total angular momentum of a many-particle system.
</p>
<p>P , total linear momentum of a many-particle system.</p>
<p/>
</div>
<div class="page"><p/>
<p>42 1 Classical Mechanics
</p>
<p>R , Routhian.
</p>
<p>R , position of the center of mass.
</p>
<p>V , velocity of the center of mass.
</p>
<p>W , work.
</p>
<p>S , action.
</p>
<p>δq(t) , infinitesimal variation of some function q(t) at time t.
δi j , Kronecker delta.
&micro; , reduced mass.
φ , potential energy due to interparticle interactions.
ψ , potential energy due to an external field.
</p>
<p>References and Further Reading
</p>
<p>1. Goldstein H (1980) Classical Mechanics, 2nd edn. Addison-Wesley, Reading, Massachusetts
</p>
<p>The book has much to recommend and it is in fact one of the standard references on classical
</p>
<p>mechanics. For a detailed discussion of what we saw in this chapter, see Chaps. 1, 2, and 8.
</p>
<p>For a discussion on the relation between the energy function and the mechanical energy, see
</p>
<p>pp. 60-61. The corresponding discussion regarding the Hamiltonian and the mechanical energy
</p>
<p>is in pp. 349-351. For Routhian and its application, see Sect. 8.3. Note that his Routhian is the
</p>
<p>negative of ours. Various properties of the Poisson bracket are given in Chap. 9.
</p>
<p>2. Lanczos C (1986) The variational principles of mechanics. Dover, New York
</p>
<p>A very detailed, yet highly accessible and engaging, discussion of the variational principles
</p>
<p>including Hamilton&rsquo;s principle.
</p>
<p>3. Landau L D, Lifshitz E M (1973) Mechanics, 3rd edn. Pergamon Press, New York
</p>
<p>A very short account on classical mechanics. The book is full of unusual insights, though not
</p>
<p>everything they say is immediately obvious. Their Poisson bracket is the negative of ours.
</p>
<p>4. Tolman R C (1979) The principles of statistical mechanics. Dover, New York
</p>
<p>Chapter 2 gives a short review of classical mechanics including topics we have omitted.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>Thermodynamics
</p>
<p>We know from experience that a macroscopic system behaves in a relatively simple
</p>
<p>manner. For example, when liquid water is heated under atmospheric pressure, it
</p>
<p>will boil at 100 ◦C. If the vapor so produced is cooled at the same pressure, it will
condense at 100 ◦C. These statements hold true regardless of the initial conditions
from which the body of water under consideration has evolved. This situation is
</p>
<p>in stark contrast to that in classical mechanics, in which initial conditions play a far
</p>
<p>more prominent role. In fact, our experience tells us that results of measurements we
</p>
<p>make of a macroscopic body are quite insensitive to the detailed microscopic state of
</p>
<p>the body. Thermodynamics is built on this empirical observation and systematically
</p>
<p>elucidates interconnections among these insensitive observations. In this chapter,
</p>
<p>we review the framework of thermodynamics before attempting to interpret it from
</p>
<p>the classical mechanical point of view in Chaps. 3 and 4.
</p>
<p>2.1 The First Law of Thermodynamics
</p>
<p>A macroscopic system, such as a glass of water, consists of many molecules. While
</p>
<p>behavior of molecules is subject to the laws of quantum mechanics rather than those
</p>
<p>of classical mechanics, the latter framework can still provide a useful approximation
</p>
<p>in many situations. On the basis of conservation of energy we saw in the previous
</p>
<p>chapter, we may conclude that the work W &prime; done on the system, which is a collection
of molecules in this case, is equal to the change ∆E in its total energy:
</p>
<p>∆E =W &prime; . (2.1)
</p>
<p>In writing this equation, it is assumed that the number of mechanical degrees of
</p>
<p>freedom remains fixed while the work is performed on the system. In thermody-
</p>
<p>namics, a system that does not exchange particles with the surroundings is called a
</p>
<p>closed system or a body. In contrast, an open system allows for particles to freely
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 43
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1 2</p>
<p/>
</div>
<div class="page"><p/>
<p>44 2 Thermodynamics
</p>
<p>pass through its boundary, thereby changing its number of mechanical degrees of
</p>
<p>freedom. For a moment, we focus only on closed systems.
</p>
<p>Our everyday experience tells us, however, that a change in the apparent state
</p>
<p>of the system can be brought about without exerting any work on it. For example,
</p>
<p>by immersing a piece of heated steel in a glass of cold water, we can cause the
</p>
<p>state of the water to change. It becomes warmer and, if the steel is sufficiently hot,
</p>
<p>the water may even start to boil. From this observation, we infer that its energy
</p>
<p>content has also changed. At the molecular scale, this may be understood as a result
</p>
<p>of kinetic energy being transferred from iron atoms to water molecules through
</p>
<p>collisions among them. The kinetic energy is also constantly redistributed within the
</p>
<p>piece of steel. It appears then that the process we are observing belongs to the realm
</p>
<p>of classical mechanics. While this is true aside from the abovementioned quantum
</p>
<p>nature of atomistic processes, a full description of the process from this mechanistic
</p>
<p>point of view demands that we be capable of knowing the position and velocity
</p>
<p>of each constituent particle in the system (a glass of water in this case) and in the
</p>
<p>surroundings (a block of steel) at some instant of time and that the equations of
</p>
<p>motion can be integrated from the initial conditions so determined. Even if this is
</p>
<p>somehow possible, it is not clear if the microscopic description we would obtain is
</p>
<p>particularly useful.
</p>
<p>Thus, when we perform a macroscopic observation, which does not inquire into
</p>
<p>the molecular-level details of a process, we must acknowledge that there are two
</p>
<p>types of processes. Firstly, there are those processes for which the force and the
</p>
<p>resulting displacement can be identified at a macroscopic level. In this case, we can
</p>
<p>readily compute ∆E by means of (2.1). Then, there are other processes, such as
those we have considered above, for which the relevant forces and displacements
</p>
<p>are detectable only at a microscopic level. In such a situation, we, as a macroscopic
</p>
<p>observer, cannot compute ∆E by means of (2.1).
It is then sensible to use separate notations and words referring to these two
</p>
<p>modes of affecting the energy content of the system. From this point on, we shall
</p>
<p>use the term work to mean the mode of energy transfer into a system in which the
</p>
<p>force and the resulting displacement are measurable at a macroscopic level. We use
</p>
<p>the symbol W to denote the work in this narrower sense of the word. The other
</p>
<p>mode of energy transfer is called heat and is denoted by the symbol Q. Thus, heat
</p>
<p>embraces all modes of energy transfer we cannot express in a form of a macro-
</p>
<p>scopic force and the resulting macroscopic displacement. By means of the newly
</p>
<p>introduced quantities, (2.1) becomes
</p>
<p>∆E =W +Q . (2.2)
</p>
<p>The principle of conservation of energy, when expressed in this form, is called the
</p>
<p>first law of thermodynamics.
</p>
<p>Of course, (2.2) would not be very useful unless we can compute Q for a given
</p>
<p>process despite our inability to identify the relevant forces and displacements at
</p>
<p>the microscopic level. This question of quantifying Q will be taken up in the next
</p>
<p>section.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 The First Law of Thermodynamics 45
</p>
<p>The state of a system defined in terms of the coordinates and momenta of all par-
</p>
<p>ticles in the system is called the &ldquo;microscopic state&rdquo; or microstate of the system. In
</p>
<p>contrast, we use the phrase &ldquo;macroscopic state,&rdquo; or macrostate for short, to refer to
</p>
<p>the state in which the system appears to us when making observations that are insen-
</p>
<p>sitive to the microscopic details. Thus, the notion of heat arises when we describe
</p>
<p>the behavior of the system in terms of macrostates as opposed to microstates.
</p>
<p>In thermodynamics, we are primarily interested in the &ldquo;internal state of the sys-
</p>
<p>tem&rdquo; and not in its macroscopic motion in space or change in its position relative to
</p>
<p>an external field. Accordingly, the kinetic energy due to translational motion of the
</p>
<p>system as a whole and the potential energy due to an external field are subtracted
</p>
<p>off from E, and we focus only on what is left, which is called the internal energy
</p>
<p>of the system and is denoted by U .
</p>
<p>If we consider a system enclosed by rigid walls that are fixed in space, the loca-
</p>
<p>tion of its center of mass fluctuates at a microscopic level as a result of interac-
</p>
<p>tion between the system and its surroundings. Because of this, the procedure just
</p>
<p>described is not well defined if applied within the context of microscopic descrip-
</p>
<p>tion of the system. However, the change in the kinetic or the potential energy due
</p>
<p>to such fluctuation is generally too small to be detectable by macroscopic measure-
</p>
<p>ments. Within the accuracy of such measurements, the separation of E into U and
</p>
<p>the rest will be a well-defined procedure.
</p>
<p>In Sect. 1.6, we saw that E is an additive quantity. Since both the kinetic and
</p>
<p>potential energies we are considering here are additive, U also is an additive quan-
</p>
<p>tity. For a homogeneous body, U is also an extensive quantity. That is, U is pro-
</p>
<p>portional to the size of the system. To see this, we may imagine dividing the homo-
</p>
<p>geneous body into two equal parts. If each part is still macroscopic, the interaction
</p>
<p>between them makes a negligible contribution to U . By the additivity of the internal
</p>
<p>energy, U of the whole system is equal to twice the internal energy of one of the
</p>
<p>parts.
</p>
<p>Provided that neither the heat nor the work affects the translational motion of the
</p>
<p>system as a whole or its position in the external field, we may rewrite (2.2) as
</p>
<p>∆U =W +Q . (2.3)
</p>
<p>Given two macrostate A and B accessible to a system, there are many distinct
</p>
<p>ways of bringing it from one to the other. The amount of work W involved usually
</p>
<p>depends on exactly how the change is brought about. We express this fact by saying
</p>
<p>that W is path dependent. A path-dependent quantity is called a path function.
</p>
<p>Based on the classical mechanical notion of energy, however, we expect that the
</p>
<p>energy of a system has a uniquely determined value for a given macrostate of the
</p>
<p>system. Such a quantity, the value of which is determined only by the macrostate
</p>
<p>in question, is called a state function. Consequently, the energy difference ∆U :=
Ub&minus;Ua between the two states is path independent. According to (2.3), then Q must
be a path function.
</p>
<p>We note that the classical mechanical work W &prime; is, as seen from (2.1), path inde-
pendent. In defining W , we have focused only on macroscopic forces and macro-</p>
<p/>
</div>
<div class="page"><p/>
<p>46 2 Thermodynamics
</p>
<p>scopic displacements and lost track of the processes occurring at the microscopic
</p>
<p>level. This is what makes W a path function.
</p>
<p>We will often be concerned with infinitesimal changes, in which W and Q are
</p>
<p>infinitesimally small. Using d̄W and d̄Q to denote, respectively, the amount of work
</p>
<p>done on and heat added to the system, and replacing ∆U by dU to indicate that only
the leading term of ∆U is retained, we write (2.3) as
</p>
<p>dU = d̄W + d̄Q (2.4)
</p>
<p>for an infinitesimal process. As with W and Q, and in contrast to dU , both d̄W and
</p>
<p>d̄Q are path-dependent quantities. We use the notation d̄ to emphasize this fact.
</p>
<p>Typically, the work term for an infinitesimal change is given by
</p>
<p>d̄W =&minus;PdV , (2.5)
</p>
<p>where P and V are the pressure and volume of the system, respectively. In Sect. 2.3,
</p>
<p>we examine how this expression arises and under what conditions.
</p>
<p>2.2 Quantifying Heat
</p>
<p>The notion of heat was introduced as a result of our inability to track the detailed
</p>
<p>mechanism of energy transfer at the microscopic level. Therefore, it is not clear if
</p>
<p>heat can be quantified at all. How do we compute Q if it is defined as everything
</p>
<p>that cannot be expressed as force times displacement?
</p>
<p>For expediency, we accept the following statement: Using a purely classical
</p>
<p>mechanical device, it is always possible to change the state of a system between
</p>
<p>two distinct macrostates A and B.
</p>
<p>In other words, at least one of the two changes, from A to B or from B to A, can
</p>
<p>always be realized. By a purely classical mechanical device, we imply our ability
</p>
<p>to track all of its generalized coordinates and momenta. As a result, the device does
</p>
<p>not exchange heat with the system, but it can exchange work with the system. This
</p>
<p>means that the difference in energy between any two states can be measured in terms
</p>
<p>of force and the displacement. For example, if the change from A to B is achievable,
</p>
<p>we have
</p>
<p>Ub &minus;Ua =Wcm , (2.6)
where Wcm is the work done on the system by the purely classical mechanical device.
</p>
<p>Having determined the energy difference, we can compute the heat Q received by
</p>
<p>the system during any process that brings the system from A to B. In fact, from
</p>
<p>(2.3),
</p>
<p>Q = (Ub &minus;Ua)&minus;W =Wcm &minus;W , (2.7)
where W is the work done on the system during the process that involves an
</p>
<p>exchange of heat.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 &Dagger;A Typical Expression for d̄W 47
</p>
<p>2.3 &Dagger;A Typical Expression for d̄W
</p>
<p>Suppose that an external body exerts a force tdA on the system through a surface
</p>
<p>element of area dA on the boundary of the system. Further, let dl denote the infinites-
</p>
<p>imal displacement experienced by the surface element. The work done by the force
</p>
<p>tdA is then dl &middot; (tdA). Repeating this computation for each of the surface elements,
into which the system boundary A is divided, and adding together the results, we
</p>
<p>arrive at the total work done on the system:
</p>
<p>d̄W =
</p>
<p>&int;
</p>
<p>A
dl &middot; (tdA) , (2.8)
</p>
<p>where the integral is over the boundary A. If a part of the boundary is held fixed,
</p>
<p>then dl = 0 for that part of the boundary.
The stress vector t is given in terms of the stress tensor T̂ as
</p>
<p>t= n &middot; T̂ , (2.9)
</p>
<p>where n, called the outward unit normal, is a unit vector perpendicular to dA and
</p>
<p>pointing away from the system. By definition,
</p>
<p>T̂ =&minus;PÎ + T̂ v , (2.10)
</p>
<p>where Î is the unit tensor and T̂ v is referred to as the extra (or viscous) stress tensor.
</p>
<p>We recall that T̂ v for Newtonian fluids is proportional to the rate of deformation.
</p>
<p>(The explicit expression for T̂ v is of no importance in the present discussion. For
</p>
<p>further details, consult Ref. [5], for example.) The key observation here is that, if
</p>
<p>any imbalance of force at any point on the system boundary is infinitesimally small,
</p>
<p>then the boundary moves very slowly and T̂ v will be negligibly small. Thus,
</p>
<p>T̂ =&minus;PÎ . (2.11)
</p>
<p>Since a &middot; Î = a for any vector a, we have
</p>
<p>n &middot; T̂ = n &middot; (&minus;PÎ) =&minus;Pn &middot; Î =&minus;Pn . (2.12)
</p>
<p>So,
</p>
<p>d̄W =
</p>
<p>&int;
</p>
<p>A
dl &middot; (&minus;Pn)dA . (2.13)
</p>
<p>If P is uniform over the boundary A of the system, this may be written as
</p>
<p>d̄W =&minus;P
&int;
</p>
<p>A
dl &middot;ndA . (2.14)
</p>
<p>But, as seen from Fig. 2.1, dl &middot;ndA is the volume swept out by the surface element dA
as it moves by dl. When this quantity is added together for all such surface elements,
</p>
<p>the end result is the net change in the volume of the system. In this way, we arrive
</p>
<p>at (2.5).
</p>
<p>We note that solids can sustain nonzero T̂ v even if the boundary does not move at
</p>
<p>all. This is because, according to Hooke&rsquo;s law, which is an excellent approximation</p>
<p/>
</div>
<div class="page"><p/>
<p>48 2 Thermodynamics
</p>
<p>Fig. 2.1 The volume of the column swept out by the surface element dA as it moves by dl is given
</p>
<p>by dl &middot;ndA = dA||dl||cosθ , in which n is the outward unit normal of dA and ||dl||cosθ is the height
of the column.
</p>
<p>for many solids as long as the deformation is sufficiently small, T̂ v in a solid is
</p>
<p>proportional not to the rate of deformation but to the size of deformation itself. We
</p>
<p>can still apply (2.5) to a solid under hydrostatic pressures, for which T̂ v &equiv; 0.
</p>
<p>2.4 The Second Law of Thermodynamics
</p>
<p>The content of the second law of thermodynamics is the following: There is a
</p>
<p>state function of a system called entropy S. The value of S can change as a result of
</p>
<p>both interactions with the surroundings and internal processes. Denoting the change
</p>
<p>associated with these processes by d̄Se and d̄Si, respectively, we have
</p>
<p>dS = d̄Se + d̄Si . (2.15)
</p>
<p>For closed systems, d̄Se is given by
</p>
<p>d̄Se =
d̄Q
</p>
<p>T
(2.16)
</p>
<p>where T is the absolute temperature and is a positive number. Thus, d̄Se can be
</p>
<p>positive, negative, or zero depending on the sign of d̄Q. A process for which d̄Q &equiv; 0
is called an adiabatic process. In contrast,
</p>
<p>d̄Si &ge; 0 (2.17)
</p>
<p>regardless of whether the system is closed or open. For processes occurring in a
</p>
<p>closed system, therefore, we have
</p>
<p>dS &ge; d̄Q
T
</p>
<p>. (2.18)
</p>
<p>As shown more explicitly later in Example 2.1, the equality in (2.17) and (2.18)
</p>
<p>holds only for reversible processes. A process is called reversible if the sequence
</p>
<p>of states visited by the system can be traversed in an opposite direction by an</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Equilibrium of an Isolated System 49
</p>
<p>infinitesimal change in the boundary conditions. As an example, we may think of
</p>
<p>a very slow expansion and compression of a gas enclosed in a thermally insulated
</p>
<p>cylinder fitted with a frictionless piston. The gas will expand if the external pressure
</p>
<p>is infinitesimally smaller than that of the gas. By an infinitesimal increase in the
</p>
<p>external pressure, the process can be reversed. Heat transfer due to an infinitesimal
</p>
<p>temperature difference is another example.
</p>
<p>As is the case with the internal energy, entropy is an additive quantity. To see
</p>
<p>this, we may consider a composite system consisting of subsystems, each at its own
</p>
<p>uniform temperature. For each of them, we can assign a reference state at which its
</p>
<p>entropy is zero. Using a combination of reversible adiabatic processes and reversible
</p>
<p>heat transfer, we can bring a subsystem from its reference state to the actual state
</p>
<p>of interest and compute the change in its entropy by means of (2.18) with equal-
</p>
<p>ity. The entire process in which this is done, one subsystem after another, may be
</p>
<p>regarded as a single process in which the composite system is brought to the state of
</p>
<p>interest from its reference state. As long as the interaction among subsystems can be
</p>
<p>ignored, the resulting change in entropy of the composite system is equal to the sum
</p>
<p>of the entropy change for each of the subsystems. Additivity of S, when applied to
</p>
<p>a homogeneous body implies that entropy of the body is proportional to its size. In
</p>
<p>other word, S is an extensive quantity. This property of S are in stark contrast with
</p>
<p>such quantities as T and P, which are independent of the size of the system and is
</p>
<p>said to be intensive.
</p>
<p>As we shall see later, classification of thermodynamic quantities into extensive
</p>
<p>and intensive variables is of fundamental importance in thermodynamics. Without
</p>
<p>it, one cannot derive important equations such as the Euler relation and the Gibbs&ndash;
</p>
<p>Duhem relation. (See Sects. 2.10, 2.11, 6.4, and 6.5.)
</p>
<p>We will be concerned primarily with the consequence of the second law and the
</p>
<p>properties of entropy as summarized above and will not inquire how the law can be
</p>
<p>established solely on the basis of macroscopic observations. An interested reader
</p>
<p>should consult Ref. [2].
</p>
<p>2.5 Equilibrium of an Isolated System
</p>
<p>When a system is isolated from the surroundings and thus left undisturbed, it even-
</p>
<p>tually reaches a particularly simple state, in which no further change is observed in
</p>
<p>any macroscopic quantities we can measure of the system. This final state is called
</p>
<p>an equilibrium state. The second law of thermodynamics leads to a precise formu-
</p>
<p>lation of the condition of equilibrium of an isolated system in terms of entropy.
</p>
<p>Since d̄Q = 0 for an isolated system, (2.18) reduces to
</p>
<p>dS &ge; 0 . (2.19)
</p>
<p>That is, during a spontaneous internal process that brings the system eventually to
</p>
<p>a state of equilibrium, the entropy of the system does not decrease. At the same
</p>
<p>time, the internal energy of the system remains constant since d̄W is also zero for
</p>
<p>an isolated system.</p>
<p/>
</div>
<div class="page"><p/>
<p>50 2 Thermodynamics
</p>
<p>It seems unlikely that the entropy of a finite isolated system can increase indefi-
</p>
<p>nitely. Instead, we expect the entropy of the system to eventually reach the maximum
</p>
<p>possible value consistent with the given values of U , V , the total mass, and any other
</p>
<p>constraints that might be imposed on the system. Once the system reached this state
</p>
<p>of maximum entropy, any further change would require the entropy to decrease,
</p>
<p>which is impossible for an isolated system. Evidently, the same argument applies
</p>
<p>when S is only a local maximum. The entropy being (local) maximum is thus suffi-
</p>
<p>cient for an isolated system to be in equilibrium.
</p>
<p>To establish its necessity for equilibrium, we may consider a system that is not
</p>
<p>at the state of maximum entropy under a given set of constraints. In this case, those
</p>
<p>processes that result in an increase of S are still possible while the processes in
</p>
<p>the opposite direction are not. Thus, we expect that the state of the system under
</p>
<p>consideration is not one of equilibrium.
</p>
<p>In this way, we are led to accept the following formulation of the condition of
</p>
<p>equilibrium:
</p>
<p>Condition of Equilibrium 1 For the equilibrium of an isolated system, it is
</p>
<p>necessary and sufficient that the entropy of the system is (local) maximum
</p>
<p>under a set of constraints imposed on the system.
</p>
<p>For a more careful discussion aimed at establishing the necessity and the sufficiency
</p>
<p>of the condition, see pp. 58&ndash;61 of Ref. [3].
</p>
<p>According to the second law, processes resulting in a decrease of the entropy are
</p>
<p>impossible for an isolated system. From a point of view of statistical mechanics,
</p>
<p>which provides a microscopic interpretation of the second law, this is not entirely
</p>
<p>accurate. In fact, the probability of finding the system with entropy S&prime; which is less
than the equilibrium value S is proportional to
</p>
<p>e(S
&prime;&minus;S)/kB , (2.20)
</p>
<p>where kB = 1.3806&times;10&minus;23 J/K is the Boltzmann constant. That the entropy of an
isolated system can actually decrease by spontaneous fluctuation is directly respon-
</p>
<p>sible for the initial stage of the so-called first-order phase transition, the examples
</p>
<p>of which include freezing of a supercooled liquid, sudden boiling of superheated
</p>
<p>liquid, and condensation of supersaturated vapor. These phases of temporary exis-
</p>
<p>tence, such as the supercooled liquid, superheated liquid, and supersaturated vapor,
</p>
<p>are said to be metastable and correspond to a local maximum of the entropy.
</p>
<p>2.6 Fundamental Equations
</p>
<p>The concept of fundamental equation is essential to thermodynamics. As we shall
</p>
<p>see, by merely accepting its existence and assuming that it meets some modest math-
</p>
<p>ematical requirements, we can establish surprising interrelations among seemingly
</p>
<p>unrelated quantities almost effortlessly.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Fundamental Equations 51
</p>
<p>2.6.1 Closed System of Fixed Composition
</p>
<p>For a moment, we restrict ourselves to reversible precesses occurring in a closed
</p>
<p>system. For simplicity, we also assume that there is no chemical reaction in the
</p>
<p>system so that the number of moles of species i, to be denoted by Ni, is constant for
</p>
<p>each species (i = 1, . . . ,c). Under these conditions, we may rewrite (2.4) using (2.5)
and (2.18) to obtain
</p>
<p>dU = T dS&minus;PdV . (2.21)
As of now, we have accepted the validity of this equation only for reversible pro-
</p>
<p>cesses during which N1, . . . ,Nc of the system remain constant. We now argue that it
applies to irreversible processes as well.
</p>
<p>The key idea here is that, as long as we limit ourselves to an isolated system that
</p>
<p>is in equilibrium and homogeneous, its entropy S is a function only of U , V , and N1,
</p>
<p>. . . , Nc:
</p>
<p>S = S(U,V,N1, . . . ,Nc) . (2.22)
</p>
<p>This is so because the value of S, in the case of an isolated system in equilibrium,
</p>
<p>is determined by it being the maximum possible value for given values of U , V , and
</p>
<p>N1, . . . , Nc. Equation (2.22) is an example of fundamental equations of the system.
</p>
<p>The restriction on homogeneity stems from our use of (2.5). In the case of an
</p>
<p>inhomogeneous system, there may be no single value of P. Thus, the work done
</p>
<p>on the system by an infinitesimal displacement of an infinitesimal portion of the
</p>
<p>system boundary depends on the position of this surface element. In order to specify
</p>
<p>the state of the system, then, one generally has to specify the shape of the system
</p>
<p>boundary, and not just its volume.
</p>
<p>For the processes under consideration, N1, . . . , Nc are constant. Suppose now that
</p>
<p>we have changed the values of U and V by some infinitesimal amounts dU and dV ,
</p>
<p>respectively. The value of S in this new state is simply
</p>
<p>S(U +dU,V +dV,N1, . . . ,Nc) . (2.23)
</p>
<p>To the first order, therefore, the resulting change in S is given by
</p>
<p>dS =
</p>
<p>(
&part;S
</p>
<p>&part;U
</p>
<p>)
</p>
<p>V,N
</p>
<p>dU +
</p>
<p>(
&part;S
</p>
<p>&part;V
</p>
<p>)
</p>
<p>U,N
</p>
<p>dV , (2.24)
</p>
<p>where we used (B.16) and included N in the subscripts to remind us that the deriva-
</p>
<p>tives are taken while holding all of N1, . . . , Nc constant.
</p>
<p>Note that (2.24) is obtained by applying (2.22) to the states before and after the
</p>
<p>infinitesimal change. Thus, so long as the system is in equilibrium and homogeneous
</p>
<p>both before and after the change and N1, . . . , Nc remain unaffected, (2.24) applies
</p>
<p>to any process including irreversible ones.
</p>
<p>According to (2.21), however,
</p>
<p>dS =
1
</p>
<p>T
dU +
</p>
<p>P
</p>
<p>T
dV (2.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>52 2 Thermodynamics
</p>
<p>for a reversible process. A comparison between (2.24) applied for a reversible pro-
</p>
<p>cess and (2.25) gives
</p>
<p>1
</p>
<p>T
=
</p>
<p>(
&part;S
</p>
<p>&part;U
</p>
<p>)
</p>
<p>V,N
</p>
<p>and
P
</p>
<p>T
=
</p>
<p>(
&part;S
</p>
<p>&part;V
</p>
<p>)
</p>
<p>U,N
</p>
<p>. (2.26)
</p>
<p>The differential coefficients on the right-hand side are obtained by comparing the
</p>
<p>values of S for infinitesimally different two equilibrium states. Since S is a state
</p>
<p>function, the partial derivatives depend only on the values of dU or dV , but not
</p>
<p>on how these changes are brought about. Therefore, (2.26) holds regardless of the
</p>
<p>nature of the process.
</p>
<p>Combining (2.24) and (2.26) and noting that neither depends on the process being
</p>
<p>reversible, we conclude that (2.25), and hence (2.21), hold for any infinitesimal
</p>
<p>change that is occurring in a closed system without affecting N1, . . . , Nc.
</p>
<p>We emphasize that, given a pair of initial and final states, values of d̄Q and d̄W
</p>
<p>do depend on the actual path taken, since they are path functions. In particular,
</p>
<p>d̄Q 	= T dS and d̄W 	=&minus;PdV for irreversible processes. Only the sum of these two,
dU = d̄W + d̄Q, is independent of the path and is given by (2.21). We illustrate
these points with the following example.
</p>
<p>Example 2.1. Gas in a cylinder: Let us consider a gas enclosed in a cylinder
</p>
<p>that is fitted with a frictionless piston. We suppose that the gas is initially in
</p>
<p>equilibrium and has a uniform temperature T and pressure P. Then, we induce
</p>
<p>a change in the state of the gas by changing the external pressure acting on the
</p>
<p>piston to Pe and by changing the wall temperature of the cylinder to Ta.
</p>
<p>The gas will expand if Pe &lt; P while it will be compressed if Pe &gt; P. In both
cases, the work d̄W done on the system by the surroundings is given by
</p>
<p>d̄W =&minus;PedV =&minus;PdV +(P&minus;Pe)dV . (2.27)
</p>
<p>Since temperature varies continuously across interfaces, Ta is also the tem-
</p>
<p>perature of the gas in the immediate vicinity of the wall. According to (2.16),
</p>
<p>the heat d̄Q received by the system during this process is
</p>
<p>d̄Q = Ta d̄Se , (2.28)
</p>
<p>which is positive if Ta &gt; T and negative if Ta &lt; T .
Unless Pe = P and Ta = T , the process will momentarily make the system
</p>
<p>inhomogeneous. Nevertheless, after the process has completed and the system
</p>
<p>is left alone for a while, it will reach a new state of equilibrium and the state
</p>
<p>function S, and hence dS, will have definite values. Let us introduce this dS in
</p>
<p>our expression for d̄Q. Using (2.15),
</p>
<p>d̄Q = T d̄Se +(Ta &minus;T ) d̄Se = T (dS&minus; d̄Si)+(Ta &minus;T ) d̄Se . (2.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Fundamental Equations 53
</p>
<p>We observe that the values of d̄W and d̄Q cannot be determined solely
</p>
<p>by those of T , P, dV , and dS. This is expected: d̄W and d̄Q are path func-
</p>
<p>tions. What we saw in this section is that their sum, d̄W + d̄Q, is nevertheless
completely determined by these and is given by
</p>
<p>d̄W + d̄Q = dU = T dS&minus;PdV . (2.30)
</p>
<p>It follows that
</p>
<p>&minus;PdV +(P&minus;Pe)dV +T (dS&minus; d̄Si)+(Ta &minus;T ) d̄Se = T dS&minus;PdV . (2.31)
</p>
<p>Thus,
</p>
<p>T d̄Si &equiv; (Ta &minus;T ) d̄Se +(P&minus;Pe)dV . (2.32)
This result is entirely consistent with (2.17). If Ta &gt; T , then d̄Se = d̄Q/Ta &gt; 0,
while d̄Se &lt; 0 if Ta &lt; T . Thus, the first term is positive unless T = Ta. Like-
wise, we see that the second term is positive unless P = Pe. The conclusion is
that d̄Si = 0 if and only if T = Ta and Pe = P, that is, if and only if the process
is reversible.
</p>
<p>Finally, we note that (2.21) actually holds even when we allow for chemical
</p>
<p>reactions. This is to be expected since the equation is merely an expression of the
</p>
<p>first law of thermodynamics, that is, conservation of energy but applied to a system
</p>
<p>in equilibrium. An explicit demonstration, however, must wait until our formalism
</p>
<p>is fully developed. We shall visit this issue in Sect. 2.9.4.
</p>
<p>2.6.2 Open System
</p>
<p>Because an open system allows for particles of some of the species to freely pass
</p>
<p>through its boundary, some of N1, . . . , Nc are no longer constant. Thus, the most
</p>
<p>general expression for dS is
</p>
<p>dS =
</p>
<p>(
&part;S
</p>
<p>&part;U
</p>
<p>)
</p>
<p>V,N
</p>
<p>dU +
</p>
<p>(
&part;S
</p>
<p>&part;V
</p>
<p>)
</p>
<p>U,N
</p>
<p>dV +
c
</p>
<p>&sum;
i=1
</p>
<p>(
&part;S
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>U,V,N j 	=i
</p>
<p>dNi , (2.33)
</p>
<p>where the first two partial derivatives are given by (2.26). In the last partial deriva-
</p>
<p>tive, U , V , and all of N1, . . . , Nc except for Ni are held constant. Following the
</p>
<p>convention, we write
</p>
<p>&minus; &micro;i
T
</p>
<p>:=
</p>
<p>(
&part;S
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>U,V,N j 	=i
</p>
<p>(2.34)
</p>
<p>and refer to &micro;i as the chemical potential of species i. The quantity &micro;i was first
introduced by Gibbs, who referred to this quantity simply as potential. Combining</p>
<p/>
</div>
<div class="page"><p/>
<p>54 2 Thermodynamics
</p>
<p>a b
</p>
<p>Fig. 2.2 The function S = S(U), for some fixed values of V , N1, . . . , Nc, can be inverted to give
U = U(S) since 1/T = (&part;S/&part;U)V,N &gt; 0, and hence the function is monotonic as indicated in a.
If 1/T changes its sign as in b, there may be multiple values of U for a given value of S and the
function S = S(U) cannot be inverted.
</p>
<p>everything, we arrive at
</p>
<p>dS =
1
</p>
<p>T
dU +
</p>
<p>P
</p>
<p>T
dV &minus;
</p>
<p>c
</p>
<p>&sum;
i=1
</p>
<p>&micro;i
T
</p>
<p>dNi , (2.35)
</p>
<p>which is often referred to as the fundamental property relation or the fundamental
</p>
<p>equation in differential form.
</p>
<p>Recall that T of a system is a positive quantity. Consequently, S increases mono-
</p>
<p>tonically with increasing U . As indicated in Fig. 2.2, this has an important implica-
</p>
<p>tion that (2.22) can be solved for U , yielding
</p>
<p>U =U(S,V,N1, . . . ,Nc) . (2.36)
</p>
<p>It is graphically clear that the process can be reversed. That is, (2.22) and (2.36) are
</p>
<p>equivalent ways of expressing the same set of information regarding the thermody-
</p>
<p>namic behavior of the system. In view of this equivalence, the function in (2.36) is
</p>
<p>also a fundamental equation.
</p>
<p>Solving (2.35) for dU , we obtain
</p>
<p>dU = T dS&minus;PdV +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;idNi , (2.37)
</p>
<p>which is simply an alternative way of writing the fundamental property relation.
</p>
<p>From this, we obtain
</p>
<p>T =
</p>
<p>(
&part;U
</p>
<p>&part;S
</p>
<p>)
</p>
<p>V,N
</p>
<p>, &minus;P =
(
&part;U
</p>
<p>&part;V
</p>
<p>)
</p>
<p>S,N
</p>
<p>, and &micro;i =
</p>
<p>(
&part;U
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>S,V,N j 	=i
</p>
<p>, i = 1, . . . ,c .
</p>
<p>(2.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Fundamental Equations 55
</p>
<p>Exercise 2.1. A pure gas is well described by the fundamental equation
</p>
<p>S = aN +NR ln
U3/2V
</p>
<p>N5/2
. (2.39)
</p>
<p>a. Find the equation of state of the gas in the form of
</p>
<p>P = P(T,V,N) . (2.40)
</p>
<p>b. Show that
</p>
<p>U =
3
</p>
<p>2
PV . (2.41)
</p>
<p>c. Show that
</p>
<p>PV 5/3 = const. (2.42)
</p>
<p>holds during an adiabatic reversible expansion of the gas.
</p>
<p>d. An isolated cylinder made of adiabatic and rigid wall is divided into two com-
</p>
<p>partments of equal volume by a frictionless piston. One of the compartments
</p>
<p>contains the gas well described by (2.39) and the other compartment is evacu-
</p>
<p>ated. If the piston is linked to an external work source and allowed to move very
</p>
<p>slowly until the gas fills the entire chamber, show that
</p>
<p>U2 = 2
&minus;2/3U1 , (2.43)
</p>
<p>where the subscripts 1 and 2 refer to the initial and the final states, respectively.
</p>
<p>///
</p>
<p>Exercise 2.2. The fundamental equation of a pure system is given by
</p>
<p>S = a(UV N)b , (2.44)
</p>
<p>where a is a positive constant. The system is held at P = 0.3 MPa:
</p>
<p>a. What is the value of b?
</p>
<p>b. Find the value of U/V .
c. Find the value of &micro; in J/mol when N/V = 0.1 (mol/cm3). ///
</p>
<p>2.6.3 Heat Capacities
</p>
<p>The constant volume heat capacity CV is the ratio of the infinitesimal amount of
</p>
<p>heat d̄Q injected into the system to the resulting infinitesimal increase dT in its
</p>
<p>temperature when the system is held at constant volume and N1, . . . , Nc:
</p>
<p>CV :=
d̄Q
</p>
<p>dT
, V , N1, . . . ,Nc const. (2.45)</p>
<p/>
</div>
<div class="page"><p/>
<p>56 2 Thermodynamics
</p>
<p>The heat capacity per mole of the material
</p>
<p>CV :=CV
</p>
<p>/
c
</p>
<p>&sum;
i=1
</p>
<p>Ni (2.46)
</p>
<p>is the constant volume molar heat capacity. Similarly, we define the constant
</p>
<p>pressure heat capacity CP by
</p>
<p>CP :=
d̄Q
</p>
<p>dT
, P, N1, . . . ,Nc const. (2.47)
</p>
<p>and the constant pressure molar heat capacity CP by
</p>
<p>CP :=CP
</p>
<p>/
c
</p>
<p>&sum;
i=1
</p>
<p>Ni (2.48)
</p>
<p>Heat capacity of a unit mass of the material is referred to as the specific heat.
</p>
<p>Exercise 2.3.
</p>
<p>a. Show that
</p>
<p>CV =
</p>
<p>(
&part;U
</p>
<p>&part;T
</p>
<p>)
</p>
<p>V,N
</p>
<p>= T
</p>
<p>(
&part;S
</p>
<p>&part;T
</p>
<p>)
</p>
<p>V,N
</p>
<p>. (2.49)
</p>
<p>b. Show that
</p>
<p>CP =
</p>
<p>(
&part;U
</p>
<p>&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>+P
</p>
<p>(
&part;V
</p>
<p>&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>=
</p>
<p>(
&part;H
</p>
<p>&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>, (2.50)
</p>
<p>in which H := U +PV is the enthalpy. The partial derivative (&part;V/&part;T )P,N is
related to the coefficient of thermal expansion α by
</p>
<p>α :=
1
</p>
<p>V
</p>
<p>(
&part;V
</p>
<p>&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>. (2.51)
</p>
<p>///
</p>
<p>2.6.4 &Dagger;Ideal Gas
</p>
<p>Later in optional sections, we require a few key results regarding properties of an
</p>
<p>ideal gas of fixed N1, . . . , Nc. For convenience, we summarize them here while
</p>
<p>highlighting the key concepts we have introduced so far.
</p>
<p>At the molecular level, an ideal gas is characterized by a lack of interaction
</p>
<p>among molecules. This is, of course, an idealization, which becomes increasingly
</p>
<p>more accurate with decreasing density and increasing temperature. Because a given
</p>
<p>molecule does not feel the presence of others, its energy is independent of density,
</p>
<p>and hence of V . Thus, the internal energy of an ideal gas of fixed N1, . . . , Nc is a</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Fundamental Equations 57
</p>
<p>Fig. 2.3 A thermodynamic path bringing the system from state A to B.
</p>
<p>function only of T . Exercise 2.23 provides a purely thermodynamic justification of
</p>
<p>this statement.
</p>
<p>We wish to compute the changes U and S experience between states A and B that
</p>
<p>are specified by (Ta,Va) and (Tb,Vb), respectively. For simplicity, we assume that
CV is constant in what follows.
</p>
<p>To facilitate the computation, let us imagine bringing the system from A to B
</p>
<p>along the path shown in Fig. 2.3, in which we indicate the intermediate state C at
</p>
<p>(Tb,Va). This does not affect the final answer since U and S are state functions.
For the constant volume process A&rarr;C, we can use (2.49) to write
</p>
<p>dU =CV dT (2.52)
</p>
<p>and hence
</p>
<p>Uc &minus;Ua =CV (Tb &minus;Ta) . (2.53)
Applying (2.25) to a constant volume process,
</p>
<p>dS =
1
</p>
<p>T
dU =
</p>
<p>CV
</p>
<p>T
dT , (2.54)
</p>
<p>yielding
</p>
<p>Sc &minus;Sa =CV ln
Tb
</p>
<p>Ta
. (2.55)
</p>
<p>For the constant temperature process C&rarr;B, U remains constant:
</p>
<p>Ub &minus;Uc = 0 . (2.56)
</p>
<p>Setting dU = 0 in (2.25), we find
</p>
<p>dS =
P
</p>
<p>T
dV =
</p>
<p>NR
</p>
<p>V
dV (2.57)
</p>
<p>and
</p>
<p>Sb &minus;Sc = NR ln
Vb
</p>
<p>Va
, (2.58)</p>
<p/>
</div>
<div class="page"><p/>
<p>58 2 Thermodynamics
</p>
<p>where R = 8.3145 J/mol K is the gas constant. We also used the equation of state,
PV = NRT , of the ideal gas, where N temporarily denotes &sum;ci=1 Ni.
</p>
<p>Combining everything,
</p>
<p>Ub &minus;Ua =CV (Tb &minus;Ta) (2.59)
</p>
<p>and
</p>
<p>Sb &minus;Sa =CV ln
Tb
</p>
<p>Ta
+NR ln
</p>
<p>Vb
</p>
<p>Va
. (2.60)
</p>
<p>We emphasize that (2.59) and (2.60) apply for any pair of states A and B and for
</p>
<p>any process regardless of whether the process goes through state C or not.
</p>
<p>Now, let us suppose that the process A&rarr;B occurred adiabatically and reversibly.
In this case, dS &equiv; 0 and (2.21) reduces to
</p>
<p>dU =&minus;PdV . (2.61)
</p>
<p>When this is combined with (2.52), we find
</p>
<p>CV dT =&minus;
NRT
</p>
<p>V
dV . (2.62)
</p>
<p>Integrating this equation from state A to state B,
</p>
<p>CV ln
Tb
</p>
<p>Ta
=&minus;NR ln Vb
</p>
<p>Va
. (2.63)
</p>
<p>This is nothing but (2.60) applied to an adiabatic reversible process, for which Sb =
Sa. Eliminating temperatures by means of the equation of state, we obtain
</p>
<p>Pb
</p>
<p>Pa
=
</p>
<p>(
Vb
</p>
<p>Va
</p>
<p>)&minus;γ
, (2.64)
</p>
<p>where
</p>
<p>γ := 1+
NR
</p>
<p>CV
. (2.65)
</p>
<p>Exercise 2.4. Show that
</p>
<p>CP =CV +NR (2.66)
</p>
<p>for an ideal gas. It follows that
</p>
<p>γ =CP/CV . (2.67)
</p>
<p>///
</p>
<p>Exercise 2.5. From (2.52) and the ideal gas equation of state, we see that
</p>
<p>U =
CV
</p>
<p>NR
PV + const. (2.68)
</p>
<p>Applying this equation to an adiabatic reversible process, directly establish (2.64).
</p>
<p>///</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Fundamental Equations 59
</p>
<p>2.6.5 &dagger;Heat Flow into an Open System
</p>
<p>Now that we have seen how (2.21) for a closed system generalizes to (2.37) for
</p>
<p>an open system, it is natural to ask how (2.4) and (2.18) written for a closed sys-
</p>
<p>tem should be generalized. There is no fundamental requirement imposing a unique
</p>
<p>solution to this question. Instead, it is a matter of convention. In this section, we
</p>
<p>shall limit ourselves to reversible processes and consider a few such conventions.
</p>
<p>Convention 1: We can take a point of view that adding molecules to or extracting
</p>
<p>them from a system is a form of work and generalize (2.4) to read
</p>
<p>dU = d̄W1 + d̄Q1 + d̄Wmol , (2.69)
</p>
<p>where we use the subscript 1 to indicate the infinitesimal work and heat under
</p>
<p>the current convention. For reversible processes, d̄Wmol is given by
</p>
<p>d̄Wmol =
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;idNi (2.70)
</p>
<p>and is called quasi-static chemical work. Under this convention, we retain the
</p>
<p>original meanings of work and heat, and write
</p>
<p>d̄W1 =&minus;PdV and d̄Q1 = T dS . (2.71)
</p>
<p>Convention 2: We recall that heat was defined as a mode of energy transfer that
</p>
<p>cannot be characterized as a macroscopic force times macroscopic displacement.
</p>
<p>Conduction of heat between bodies occurs when molecules moving about due to
</p>
<p>thermal motion exchange mechanical energy through collisions. Diffusive flux of
</p>
<p>molecules is also driven by thermal motion of molecules. That is, conduction and
</p>
<p>diffusion are both macroscopic manifestations of molecular level processes that
</p>
<p>do not involve macroscopic force or displacement. From this point of view, the
</p>
<p>concept of heat should be enlarged to include both heat conduction and diffusive
</p>
<p>molecular flux. That is, we retain (2.4):
</p>
<p>dU = d̄W2 + d̄Q2 (2.72)
</p>
<p>and write
</p>
<p>d̄W2 =&minus;PdV and d̄Q2 = T dS+
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;idNi . (2.73)
</p>
<p>Convention 3: The second equation in (2.73) can be rewritten in a somewhat more
</p>
<p>illuminating manner using a thermodynamic identity to be established later in
</p>
<p>Exercise 2.24:
</p>
<p>&micro;i = H i &minus;T Si . (2.74)
Here H i and Si are, respectively, the partial molar enthalpy and partial molar
</p>
<p>entropy to be introduced in Sect. 2.15. Using (2.74), we define heat as</p>
<p/>
</div>
<div class="page"><p/>
<p>60 2 Thermodynamics
</p>
<p>d̄Q3 := d̄Q2 &minus;
c
</p>
<p>&sum;
i=1
</p>
<p>H idNi = T dS&minus;T
c
</p>
<p>&sum;
i=1
</p>
<p>SidNi . (2.75)
</p>
<p>With this new definition of heat, (2.72) now reads
</p>
<p>dU = d̄W2 + d̄Q3 +
c
</p>
<p>&sum;
i=1
</p>
<p>H idNi . (2.76)
</p>
<p>Enthalpy is sometimes called the heat function. Thus, the last term of (2.76)
</p>
<p>may be interpreted as the heat brought into the system by the flow of molecules.
</p>
<p>Similarly, when (2.75) is written as
</p>
<p>dS =
d̄Q3
</p>
<p>T
+
</p>
<p>c
</p>
<p>&sum;
i=1
</p>
<p>SidNi , (2.77)
</p>
<p>the last term suggests the flow of entropy associated with the flow of molecules.
</p>
<p>This is a very satisfying state of affair, at least psychologically.
</p>
<p>Convention 1 is used in Ref. [1]. Conventions 3 is often adopted in textbooks on
</p>
<p>transport phenomena. Obviously, it is important to check which convention is being
</p>
<p>used in any given application. Any equation containing heat will have a different
</p>
<p>appearance depending on the convention.
</p>
<p>2.7 Role of Additional Variables
</p>
<p>In Sect. 2.5, we accepted the maximum of S under the existing constraints as both
</p>
<p>necessary and sufficient for equilibrium of an isolated system. Among other things,
</p>
<p>U and V of an isolated system are held constant. If we assume that c species in
</p>
<p>the system do not undergo chemical reactions, then the number of moles of each
</p>
<p>species, that is, all of N1, . . . , Nc, are fixed as well.
</p>
<p>It is at this point that our formulation might appear inconsistent to you. In fact,
</p>
<p>according to (2.22), S is a function of U , V , and N1, . . . , Nc only, all of which
</p>
<p>are fixed in an isolated system without chemical reactions. It is thus legitimate to
</p>
<p>wonder just what is meant by S being maximum when all of its arguments are fixed.
</p>
<p>We need to remember, however, that (2.22) holds only for a homogeneous sys-
</p>
<p>tem in equilibrium. If the system is inhomogeneous or not in equilibrium, specifi-
</p>
<p>cation of its state may require not only those variables already listed but also some
</p>
<p>additional variables. Examples may include those variables characterizing spatial
</p>
<p>variation of composition, density, or temperature. If there is a macroscopic motion
</p>
<p>within the system, the velocity field throughout the system must also be given in
</p>
<p>order to fully specify the state of the system.
</p>
<p>What the condition of equilibrium tells us is that these additional variables, if
</p>
<p>allowed to vary, are determined so as to maximize S. If a chemical reaction does
</p>
<p>take place in the system, then N1, . . . , Nc can change in a manner that is consistent</p>
<p/>
</div>
<div class="page"><p/>
<p>2.7 Role of Additional Variables 61
</p>
<p>A B
</p>
<p>Fig. 2.4 A system consisting of two compartments. The system as a whole is isolated, while the
</p>
<p>properties of the wall separating the compartments can be modified as needed.
</p>
<p>with the stoichiometry of the reaction. At equilibrium, the extent of reaction, once
</p>
<p>again, is determined by the condition that S is maximum.
</p>
<p>To make the role of the additional variables clearer, we consider an example
</p>
<p>shown in Fig. 2.4. The system we consider is isolated from the surroundings by an
</p>
<p>adiabatic and rigid partition impermeable to all species. The interior of the box is
</p>
<p>divided into two compartments by a partition, whose property can be changed as
</p>
<p>needed. Initially, we suppose that the partition is adiabatic, rigid, impermeable to
</p>
<p>all species, and held fixed in place by stops. In this case, each compartment will
</p>
<p>eventually reach a state of equilibrium on its own independent of what is happening
</p>
<p>in the other compartment. When this final state of equilibrium is reached, the entropy
</p>
<p>of each compartment is a function of its U , V , N1, . . . , Nc. Since entropy is an
</p>
<p>additive quantity, the total entropy S of the composite system is given by
</p>
<p>S = Sa(Ua,V a,Na1 , . . . ,N
a
c )+S
</p>
<p>b(Ub,V b,Nb1 , . . . ,N
b
c ) , (2.78)
</p>
<p>where the superscripts a and b label the respective compartment and we assumed
</p>
<p>that each compartment is homogeneous. Since the internal energy, volume, and the
</p>
<p>number of particles are all additive quantities, we have
</p>
<p>Ua +Ub = U , V a +V b =V , and Na1 +N
b
1 = N1 , i = 1, . . . ,c . (2.79)
</p>
<p>Using (2.79) in (2.78),
</p>
<p>S = Sa(Ua,V a,Na1 , . . . ,N
a
c )+S
</p>
<p>b(U &minus;Ua,V &minus;V a,N1 &minus;Na1 , . . . ,Nc &minus;Nac )
= S(U,V,N1, . . . ,Nc;U
</p>
<p>a,V a,Na1 , . . . ,N
a
c ) . (2.80)
</p>
<p>The quantities Ua, V a, and Na1 , . . . , N
a
c are examples of the additional variables. They
</p>
<p>are needed since the composite system taken as a whole, although in equilibrium, is
</p>
<p>not homogeneous.</p>
<p/>
</div>
<div class="page"><p/>
<p>62 2 Thermodynamics
</p>
<p>Now, suppose that the partition suddenly became diathermal, allowing heat to
</p>
<p>pass from one compartment to the other. (The Greek prefix &ldquo;dia&rdquo; means &ldquo;across&rdquo;or
</p>
<p>&ldquo;through.&rdquo;) This implies that Ua is now free to change and, according to (2.80), S can
</p>
<p>change also even though U , V , N1, . . . , Nc, V
a, and Na1 , . . . , N
</p>
<p>a
c are all held constant.
</p>
<p>The content of the second law is that the equilibrium value of Ua is determined to
</p>
<p>maximize S for given values of U , V , N1, . . . , Nc, V
a, and Na1 , . . . , N
</p>
<p>a
c . If the partition
</p>
<p>is made both movable and permeable to all species as well as being diathermal, all
</p>
<p>the quantities with superscript a in (2.80) can change. Their equilibrium values are
</p>
<p>again determined by the condition that S is maximum. (Such a partition is actually
</p>
<p>relevant in applications of thermodynamics. See Sect. 2.12.) The entire argument
</p>
<p>can be generalized to cases with more than two compartments.
</p>
<p>2.8 Entropy Representation
</p>
<p>We are now ready to formulate the condition of equilibrium in quantitative terms.
</p>
<p>The second law also tells us the direction of a spontaneous process as the system
</p>
<p>evolves toward the state of equilibrium.
</p>
<p>2.8.1 Condition of Equilibrium
</p>
<p>To proceed generally, let us suppose that r variables X1, . . . , Xr are required in
</p>
<p>addition to U , V , N1, . . . , Nc to fully specify the state of the system. Of these r
</p>
<p>variables, we assume that only the first m variables are free to change. In (2.80),
</p>
<p>for example, r = c+2. If the partition between the two compartments is diathermal,
rigid, impermeable to all species, and is held fixed in place, then m= 1 and X1 =U
</p>
<p>a.
</p>
<p>On the other hand, if the partition is diathermal, movable, and permeable to all
</p>
<p>species, then m = r = c+2.
If an isolated system is in equilibrium, then its entropy S is maximum for given
</p>
<p>values of U , V , N1, . . . , Nc, and Xm+1, . . . , Xr. This condition, in turn, determines
</p>
<p>the values of X1, . . . , Xm at equilibrium. Thus, if we denote the latter by X
eq
1 , . . . ,
</p>
<p>X
eq
m , then we may express the condition of equilibrium by saying that
</p>
<p>∆S = S(U,V,N1, . . . ,Nc;X
eq
1 +δX1, . . . ,X
</p>
<p>eq
m +δXm;Xm+1, . . . ,Xr)
</p>
<p>&minus; S(U,V,N1, . . . ,Nc;Xeq1 , . . . ,Xeqm ;Xm+1, . . . ,Xr)&le; 0 . (2.81)
</p>
<p>holds for any possible values of δX1, . . . , δXm.
We note that, after the infinitesimal changes denoted by δX1, . . . , δXm are made,
</p>
<p>the system, in general, will no longer be in equilibrium. That is, unless we confine
</p>
<p>X1, . . . , Xm to their new values, they will simply return to their initial values X
eq
1 ,
</p>
<p>. . . , X
eq
m to maximize S. The system may not be even homogeneous both before and
</p>
<p>after the change. This is to be contrasted with the situation considered in (2.35), in</p>
<p/>
</div>
<div class="page"><p/>
<p>2.8 Entropy Representation 63
</p>
<p>Fig. 2.5 Various possible states of equilibrium when X1 is confined to the interval
[
Xa1 ,X
</p>
<p>b
1
</p>
<p>]
. The
</p>
<p>system is in stable (&bull;), metastable (⊙), unstable (▽), and neutral equilibrium (between the two △).
</p>
<p>which the system was supposed to be homogeneous and in equilibrium both before
</p>
<p>and after the infinitesimal change. To make the distinction clear, we used the symbol
</p>
<p>δ for the infinitesimal changes occurring in (2.81) and refer to them as variations.
We shall continue to use d as before to denote infinitesimal changes considered in
</p>
<p>writing (2.35) and refer to them as differentiations.
</p>
<p>In what follows, we retain only up to the first-order terms in (2.81) and adopt the
</p>
<p>following statement as the condition of equilibrium:
</p>
<p>Condition of Equilibrium 2 For the equilibrium of an isolated system, it is
</p>
<p>necessary and sufficient that
</p>
<p>(δS)U,V,N1,...,Nc;Xm+1,...,Xr &le; 0 (2.82)
</p>
<p>for all possible variations of X1, . . . , Xm consistent with given constraints.
</p>
<p>The subscripts U , V , N1, . . . , Nc, Xm+1, . . . , Xr remind us that the variations can-
</p>
<p>not affect the values of these (constrained) variables. However, the set of variables
</p>
<p>showing up in this list must be modified to suite the particular situation at hand.
</p>
<p>For example, if we allow for chemical reactions, N1, . . . , Nc may change, but their
</p>
<p>variations must satisfy a set of equations expressing the constraint on the number of
</p>
<p>atoms of each element.
</p>
<p>In practice, it may be very difficult to explore all possible variations and we will
</p>
<p>have to limit ourselves to a subset of all that are possible. The resulting conditions
</p>
<p>of equilibrium, therefore, will be necessary but not sufficient for equilibrium.
</p>
<p>It is important, at least conceptually, to realize that both (2.81) and (2.82) allow
</p>
<p>for S to exhibit a kink (with a discontinuous first derivative) at the maximum. The
</p>
<p>maximum of S may also occur at an end point of the interval over which X1, . . . , Xm
can vary. In such cases, the system is in equilibrium even though δS 	= 0. According
to (2.82), a minimum entropy state is also an equilibrium state as long as δS = 0.
We illustrate these situations in Fig. 2.5 and classify them according to their stability
</p>
<p>with respect to perturbations.</p>
<p/>
</div>
<div class="page"><p/>
<p>64 2 Thermodynamics
</p>
<p>Such perturbations may result due to outside influences which is so extremely
</p>
<p>minute that the system can otherwise be considered isolated. For example, your &ldquo;iso-
</p>
<p>lated&rdquo; system securely anchored to the floor of your building may experience small
</p>
<p>vibrations from time to time due to traffic outside. Even if the system is perfectly
</p>
<p>isolated from the rest of the universe, small inhomogeneity of density is constantly
</p>
<p>evolving throughout the system as the molecules move about. This is an example of
</p>
<p>spatially varying perturbation, which may be denoted as δXr with r indicating the
position dependence.
</p>
<p>For simplicity, we let m = 1 in Fig. 2.5, that is, only X1 is free to change. Then,
&bull; denotes a stable equilibrium state. This is because the system will return to this
original state even after experiencing perturbations in X1. States marked by ⊙ differ
from the &bull; state in that the system is stable only for small perturbations but not for
sufficiently large perturbation in X1. We call such a state metastable. The states
</p>
<p>marked by ▽ are in equilibrium. But the equilibrium is unstable because the system
</p>
<p>moves away from it as a result of even the slightest perturbation in X1. Over the
</p>
<p>interval between the two △ where S is constant, the system is said to be in neutral
equilibrium. In this case, there is no tendency for the system to return to the original
</p>
<p>state after an infinitesimal perturbation. But, there is not tendency to move away
</p>
<p>from it, either.
</p>
<p>As we shall see shortly, there are other ways to express the condition of equilib-
</p>
<p>rium. The current formulation, in which entropy plays a prominent role, is called
</p>
<p>the entropy representation.
</p>
<p>2.8.2 Equality of Temperature
</p>
<p>To illustrate the use of the condition of equilibrium, let us continue on the exam-
</p>
<p>ple in Sect. 2.7. We assume that the partition separating the two compartments is
</p>
<p>diathermal, rigid, impermeable to all species, and held in place by stops. Equa-
</p>
<p>tions (2.80) and (2.82) provide a practical means of determining the value of the
</p>
<p>additional variable Ua at equilibrium.
</p>
<p>According to (2.78), S = Sa +Sb, and hence
</p>
<p>δS = δSa +δSb . (2.83)
</p>
<p>Using (2.35), we may rewrite this equation as
</p>
<p>δS =
1
</p>
<p>T a
δUa +
</p>
<p>1
</p>
<p>T b
δUb . (2.84)
</p>
<p>Since
</p>
<p>Ua +Ub =U = const. , (2.85)
</p>
<p>and hence δUb =&minus;δUa, (2.84) reduces to
</p>
<p>δS =
</p>
<p>(
1
</p>
<p>T a
&minus; 1
</p>
<p>T b
</p>
<p>)
δUa . (2.86)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.8 Entropy Representation 65
</p>
<p>Now, (2.82) demands that δS &le; 0 for any value of δUa. Provided that δUa can be
either positive or negative, it follows that
</p>
<p>T a = T b , (2.87)
</p>
<p>which is the condition of equilibrium of the composite system.
</p>
<p>From (2.26), we see that T a is a function of Ua, V a, and Na1 , . . . , N
a
c . Likewise
</p>
<p>for T b. Thus, (2.85) and (2.87) serve as the set of equations to determine Ua and
</p>
<p>Ub provided that the value of the constant U is known. Once Ua and Ub are found
</p>
<p>from these equations, then the temperature T a = T b of the composite system in
equilibrium can be computed from (2.26).
</p>
<p>Strictly speaking, (2.87) is necessary for equilibrium but not sufficient since we
</p>
<p>have not considered all possible variations. For example, variations that disturbs
</p>
<p>homogeneity in either of the compartments were not considered. Such a concern is
</p>
<p>best addressed in terms of the stability of the equilibrium.
</p>
<p>Finally, suppose that we remove the stops on the left of the partition but keep
</p>
<p>those on the right. In this case, V a is incapable of a positive variation and δV a &le; 0.
From this, one can only deduce that Pa/T a &ge;Pb/T b at equilibrium. For a diathermal
partition, we have T a = T b and hence Pa &ge; Pb. When this inequality is satisfied,
clearly, the partition will not move to the left. It will not move to the right either
</p>
<p>because of the remaining stops. The composite system is thus in equilibrium even if
</p>
<p>Pa may differ from Pb.
</p>
<p>Exercise 2.6. Suppose that the stops holding the partition are now removed and that
</p>
<p>the partition, besides being diathermal, just became permeable to species 1 while
</p>
<p>still impermeable to other species. Show that T a = T b, Pa = Pb, and &micro;a1 = &micro;
b
1 hold
</p>
<p>at equilibrium. ///
</p>
<p>Exercise 2.7. 7 The fundamental equation of a particular binary mixture is given by
</p>
<p>S = aN +NR ln
U3/2V
</p>
<p>N5/2
&minus;N1R ln
</p>
<p>N1
</p>
<p>N
&minus;N2R ln
</p>
<p>N2
</p>
<p>N
, N := N1 +N2 , (2.88)
</p>
<p>where a is an unspecified constant and R is the gas constant.
</p>
<p>An isolated system is divided into two compartments A and B of equal volume
</p>
<p>by an adiabatic, rigid, and impermeable partition fixed in place by stops. Initially,
</p>
<p>compartment A is filled with N1 = 10 mol of pure species 1 at T
a = 300 K while
</p>
<p>compartment B is filled with N2 = 4 mol of pure species 2 at T
b = 400 K:
</p>
<p>a. The partition suddenly became diathermal, while remaining rigid, impermeable,
</p>
<p>and held fixed in place by stops. After the system reached a new state of equilib-
</p>
<p>rium, what is the temperature in each compartment?
</p>
<p>b. The partition became permeable to species 1 in addition to being diathermal,
</p>
<p>while remaining rigid, impermeable to species 2, and held fixed in place by stops.
</p>
<p>After the system reached a new state of equilibrium, what is the final number of
</p>
<p>moles of species 1 in each compartment? What about the temperature? ///</p>
<p/>
</div>
<div class="page"><p/>
<p>66 2 Thermodynamics
</p>
<p>2.8.3 Direction of a Spontaneous Process
</p>
<p>In addition to identifying the state of equilibrium, the second law also determines
</p>
<p>the direction of a spontaneous process occurring in an isolated system.
</p>
<p>In the example considered in Sect. 2.8.2, let us suppose that T a &gt; T b at the
moment when the adiabatic partition became suddenly diathermal. Then we ask
</p>
<p>which of the two compartments receives heat as the composite system evolves
</p>
<p>toward a new state of equilibrium, at which T a = T b. You know the answer intu-
itively, of course. The purpose of this discussion is to make an argument based on
</p>
<p>the principle of thermodynamics.
</p>
<p>From (2.86), we have
</p>
<p>dS =
</p>
<p>(
1
</p>
<p>T a
&minus; 1
</p>
<p>T b
</p>
<p>)
dUa, (2.89)
</p>
<p>where we use &ldquo;d&rdquo; to indicate an infinitesimal change without implying that the com-
</p>
<p>posite system is in equilibrium either before or after the change. Instead, &ldquo;d&rdquo; in this
</p>
<p>equation denotes the infinitesimal change during a spontaneous process occurring
</p>
<p>in the system as it evolves toward equilibrium. Each compartment, taken separately,
</p>
<p>is assumed to be in equilibrium at all time. This assumption will be satisfactory if
</p>
<p>the change occurs very slowly.
</p>
<p>By assumption, T a &gt; T b initially and the quantity in the brackets is negative. For
a spontaneous process in the composite system that is isolated, dS must be positive.
</p>
<p>According to (2.89), dUa &lt; 0 and hence dUb = &minus;dUa &gt; 0, that is, the heat flows
from compartment A to B.
</p>
<p>Just like a difference in temperature drives the heat flow between the two com-
</p>
<p>partments, a difference in pressure drives the repartitioning of the volume between
</p>
<p>them. Similarly, a difference in the chemical potential of one of the species drives
</p>
<p>the flow of particles of that species. Chemical potentials also play a crucial role
</p>
<p>when chemical reactions can take place. You are invited to explore these remarks in
</p>
<p>detail in the following exercises.
</p>
<p>Exercise 2.8. Consider an isolated system consisting of two compartments sepa-
</p>
<p>rated by a diathermal and rigid partition impermeable to all species and fixed by
</p>
<p>stops. Show that, when the stops are removed, the partition moves from the higher
</p>
<p>pressure compartment to the lower pressure compartment. You may assume that the
</p>
<p>temperatures of the two compartments are equal at all time. ///
</p>
<p>Exercise 2.9. Consider an isolated system consisting of two compartments sepa-
</p>
<p>rated by a diathermal and rigid partition permeable to molecules of species 1 but not
</p>
<p>to the others. The partition is held fixed in place. Show that molecules of species 1
</p>
<p>flow from the higher chemical potential compartment to the lower chemical poten-
</p>
<p>tial compartment. You may assume that the temperatures of the two compartments
</p>
<p>are equal at all time. ///
</p>
<p>Exercise 2.10. Consider an isolated and homogeneous ternary system (c = 3) con-
sisting of species A1, A2, and A3, in which the following chemical reaction can take
</p>
<p>place:
</p>
<p>c1A1 + c2A2 ⇄ c3A3 , (2.90)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.8 Entropy Representation 67
</p>
<p>where c1, c2, and c3 are some positive constants.
</p>
<p>a. Show that
</p>
<p>c1&micro;1 + c2&micro;2 = c3&micro;3 (2.91)
</p>
<p>at equilibrium.
</p>
<p>b. Show that the reaction proceeds from left to right if
</p>
<p>c1&micro;1 + c2&micro;2 &gt; c3&micro;3 . (2.92)
</p>
<p>///
</p>
<p>2.8.4 &dagger;Very Short Remark on the Stability of Equilibrium
</p>
<p>In order for the composite system to eventually come to equilibrium through the
</p>
<p>spontaneous process considered in Sect. 2.8.3, the temperature must increase with
</p>
<p>an inflow of energy and decrease with an outflow of energy. This same condition
</p>
<p>is also necessary for the stability of a body with uniform temperature throughout.
</p>
<p>That is, if, by fluctuation, heat flows from one region to another, the temperature of
</p>
<p>a part receiving the heat must increase while the temperature of a region losing the
</p>
<p>heat must decrease. Otherwise, the ever-increasing inequality of temperature would
</p>
<p>drive the system further away from the original state. Thus, for the stability of any
</p>
<p>system, it is necessary that (
&part;T
</p>
<p>&part;U
</p>
<p>)
</p>
<p>V,N
</p>
<p>&gt; 0 , (2.93)
</p>
<p>which, in view of (2.49), implies that
</p>
<p>CV &gt; 0 . (2.94)
</p>
<p>We can develop a graphical approach to the question of stability. As an example,
</p>
<p>let us take an isolated homogeneous system and define the entropy density s := S/V
and the internal energy density u :=U/V . Since S is an extensive quantity, we have
</p>
<p>s = S(u,1,N1/V, . . . ,Nc/V ) , (2.95)
</p>
<p>which simply states that the system containing U/V of the internal energy and Ni/V
moles of species i in unit volume has 1/V times the entropy of the original system
containing U of internal energy and Ni moles of species i in volume V . It follows
</p>
<p>that, for fixed values of V , and N1, . . . , Nc, the quantity s is a function only of u.
</p>
<p>If s = s(u) is concave up as shown in Fig. 2.6, the system is unstable with respect
to separation into two homogeneous parts with distinct values of the internal energy
</p>
<p>density.
</p>
<p>To see why, let us consider a homogeneous system having the internal energy U0
and consisting of Ni moles of species i in volume V . We denote its entropy density
</p>
<p>by s0 and its internal energy density by u0. Then, the system is represented by the</p>
<p/>
</div>
<div class="page"><p/>
<p>68 2 Thermodynamics
</p>
<p>Fig. 2.6 If the curve s = s(u) for a homogeneous body is concave up, the body is unstable with
respect to the spontaneous separation into two homogeneous parts. Because of how the graph is
</p>
<p>drawn, the system can further increase its entropy by moving the open circles away from each
</p>
<p>other in such a way that the overall energy density remains at u0.
</p>
<p>filled circle in Fig. 2.6. Now, suppose that the system separated into two parts, one
</p>
<p>having the internal energy density u1 and the other u2 such that u1 &lt; u0 &lt; u2 as
represented by the open circles in Fig. 2.6. Then, the entropy density of the system
</p>
<p>at this final state is given by s f in the figure. This is because the entropy density
</p>
<p>at the final state should be somewhere on the straight line connecting the two open
</p>
<p>circles, while the overall internal energy density must be u0 because the system as
</p>
<p>a whole is isolated. If s(u) is concave up, clearly, S f &minus;S0 =V (s f &minus; s0)&gt; 0 and the
process occurs spontaneously.
</p>
<p>Exercise 2.11. Justify the graphical method given above for locating s f through an
</p>
<p>explicit computation. ///
</p>
<p>Exercise 2.12. By considering the sign of the second derivative:
</p>
<p>(
&part; 2S
</p>
<p>&part;U2
</p>
<p>)
</p>
<p>V,N
</p>
<p>, (2.96)
</p>
<p>show that the constant volume heat capacity CV defined by (2.49) is negative if S =
S(U) is concave up. Thus, CV &gt; 0 is necessary for the stability of a thermodynamic
system. ///
</p>
<p>2.9 Energy Representation
</p>
<p>While the second law leads directly to the entropy representation of the condition
</p>
<p>of equilibrium, this is by no means the only possible formulation. Nor is it the most
</p>
<p>convenient. To explore alternatives, we begin by looking into the energy represen-</p>
<p/>
</div>
<div class="page"><p/>
<p>2.9 Energy Representation 69
</p>
<p>Fig. 2.7 Graphical representation of S of the composite system considered in Sect. 2.8.2. In draw-
</p>
<p>ing this graph, all the variables occurring in (2.80), except for U and Ua, are fixed. The equilibrium
</p>
<p>value of Ua, which we denote here by Uaeq, is determined so that S takes the maximum possible
</p>
<p>value consistent with the given value, U0, of the internal energy of the composite system. The
</p>
<p>equilibrium state is then indicated by the filled circle.
</p>
<p>tation, in which the internal energy plays a central role in characterizing a state of
</p>
<p>equilibrium. Other possibilities are considered in Sect. 2.13.
</p>
<p>2.9.1 Condition of Equilibrium
</p>
<p>It is instructive to envision the approach to an equilibrium state graphically. For this
</p>
<p>purpose, let us revisit the example discussed in Sects. 2.8.2 and 2.8.3 and depict S
</p>
<p>of the composite system as a function of U and Ua as shown in Fig. 2.7.
</p>
<p>Because the composite system is isolated, U is fixed at a constant value, say U0.
</p>
<p>As the system evolves toward the equilibrium state, at which T a = T b and Ua =Uaeq,
its representative point moves along the curve defined by the intersection between
</p>
<p>the surface of S and the U = U0 plane until it reaches the maximum S state indi-
cated by &bull; in Fig. 2.7. If this trajectory is projected onto the US-plane as shown in
Fig. 2.8, the representative point is seen to move vertically upward until it meets
</p>
<p>the S versus U curve, at which S = S0. From the latter figure, we observe that the
same equilibrium state, that is, the point (U0,S0) can also be regarded as the state of
minimum internal energy for a given value (S0) of the entropy.
</p>
<p>This suggests the following formulation, called the energy representation, of
</p>
<p>the condition of equilibrium as an alternative to the entropy representation:
</p>
<p>Condition of Equilibrium 3 For the equilibrium of an isolated system, it is
</p>
<p>necessary and sufficient that
</p>
<p>(δU)S,V,N1,...,Nc;Xm+1,...,Xr &ge; 0 (2.97)
</p>
<p>for all possible variations of X1, . . . , Xm consistent with given constraints.</p>
<p/>
</div>
<div class="page"><p/>
<p>70 2 Thermodynamics
</p>
<p>Fig. 2.8 Projection onto the US-plane of the locus of the maximum entropy as a function of U .
</p>
<p>The vertical arrow at U0 indicates the trajectory of the system during evolution toward equilibrium
</p>
<p>as observed in the US-plane. The filled circle indicates the state of equilibrium after the partition
</p>
<p>became diathermal.
</p>
<p>As is the case with Condition of Equilibrium 2, subscripts on δU must be modi-
fied according to the problem at hand. The same remark applies to all criteria of
</p>
<p>equilibrium we encounter later.
</p>
<p>To illustrate the use of Condition of Equilibrium 3 in a more concrete setting, we
</p>
<p>examine the same problem you considered in Exercise 2.6, that is, we deduce the
</p>
<p>equations that must be satisfied at equilibrium when the partition between the two
</p>
<p>subsystems is diathermal, movable, and permeable only to species 1.
</p>
<p>Since the internal energy is an additive quantity, we have
</p>
<p>U =Ua(Sa,V a,Na1 , . . . ,N
a
c )+U
</p>
<p>b(Sb,V b,Nb1 , . . . ,N
b
c ) , (2.98)
</p>
<p>and hence
</p>
<p>δU = δUa +δUb . (2.99)
</p>
<p>Applying (2.37) to each compartment, we can rewrite (2.99) as
</p>
<p>δU = T aδSa &minus;PaδV a +&micro;a1δNa1 +T bδSb &minus;PbδV b +&micro;b1δNb1 , (2.100)
</p>
<p>where we note that δNa2 = &middot; &middot; &middot; = δNbc &equiv; 0 since the partition is impermeable to
species 2, . . . , c.
</p>
<p>Equation (2.97) requires that we minimize U for a given value of S, while allow-
</p>
<p>ing for Sa and Sb to change:
</p>
<p>Sa +Sb = S = const. (2.101)
</p>
<p>Since the composite system is isolated, we also have
</p>
<p>V a +V b =V = const. and Na1 +N
b
1 = N1 = const. (2.102)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.9 Energy Representation 71
</p>
<p>Using (2.101) and (2.102), we rewrite (2.100) as
</p>
<p>δU = (T a &minus;T b)δSa &minus; (Pa &minus;Pb)δV a +(&micro;a1 &minus;&micro;b1 )δNa1 . (2.103)
</p>
<p>For equilibrium of the composite system, it is necessary that this quantity be non-
</p>
<p>negative for all possible values of δSa, δV a, and δNa1 , from which we deduce that
T a = T b, Pa = Pb, and &micro;a1 = &micro;
</p>
<p>b
1 . For example, (2.103) must be nonnegative even
</p>
<p>when δV a and δNa1 are both zero and δS
a takes an arbitrary value, which leads to
</p>
<p>the conclusion that T a = T b.
Suppose that the partition, besides being diathermal and movable, is now made
</p>
<p>permeable to all species. It is straightforward to show that, at equilibrium, T a = T b,
Pa = Pb, and &micro;ai = &micro;
</p>
<p>b
i for i = 1, . . . , c.
</p>
<p>2.9.2 Reversible Work Source
</p>
<p>Looking at (2.97), you may object to the indicated increase of U because the system
</p>
<p>under consideration is isolated. Nevertheless, according to Fig. 2.8, Condition of
</p>
<p>Equilibrium 3 is equivalent to Condition of Equilibrium 2 in Sect. 2.8.1. We can
</p>
<p>give two distinct interpretations of (2.97).
</p>
<p>In one interpretation, the variation is regarded as representing an act of com-
</p>
<p>paring the values of U of many copies of the system, each having the same values
</p>
<p>of S, V , N1, . . . , Nc, and Xm+1, . . . , Xr, but differing by the values of X1, . . . , Xm.
</p>
<p>Equation (2.97) then identifies the system with minimum U as being in equilibrium.
</p>
<p>An alternative, more physical, interpretation is possible. We may suppose that
</p>
<p>the increase in U is brought about by means of a purely classical mechanical device
</p>
<p>which is linked to the otherwise isolated system and induces changes in X1, . . . , Xm.
</p>
<p>As discussed in Sect. 2.2, such a device does not exchange heat with the system.
</p>
<p>Since the variation is taken while maintaining S constant, the process must occur
</p>
<p>reversibly. The mechanical device capable of inducing the variation reversibly is
</p>
<p>called the reversible work source.
</p>
<p>If we take the second view point, then from the conservation of energy, the
</p>
<p>reversible work imparted on the system by the reversible work source is given by
</p>
<p>δW rev = (δU)S,V,N1,...,Nc;Xm+1,...,Xr . (2.104)
</p>
<p>This observation leads to yet another formulation of the condition of equilibrium:
</p>
<p>Condition of Equilibrium 4 For the equilibrium of an isolated system, it is
</p>
<p>necessary and sufficient that
</p>
<p>(δW rev)S,V,N1,...,Nc;Xm+1,...,Xr &ge; 0 (2.105)
</p>
<p>for all possible variations of X1, . . . , Xm consistent with given constraints.</p>
<p/>
</div>
<div class="page"><p/>
<p>72 2 Thermodynamics
</p>
<p>As we shall see later, this formulation applies not only to an isolated system but also
</p>
<p>to other kinds of systems, for example, a system held at a constant temperature by
</p>
<p>virtue of thermal contact with the surroundings.
</p>
<p>Condition of Equilibrium 4 has a direct physical interpretation. To see if a system,
</p>
<p>isolated or otherwise, is in equilibrium, we connect it to a reversible work source and
</p>
<p>let the latter perturb the state of the system by varying the values of X1, . . . , Xm. If, to
</p>
<p>the first order of variation, a nonnegative work is required for any such perturbation,
</p>
<p>then from this observation, we conclude that the initial unperturbed system was in
</p>
<p>equilibrium.8
</p>
<p>We can now establish the equivalence of Conditions of Equilibrium 2 and 3
</p>
<p>without resorting to Fig. 2.8. Suppose that (2.97) holds and that we exerted work
</p>
<p>reversibly on the otherwise isolated system to induce a required change in X1, . . . ,
</p>
<p>Xm. This does not affect S but U may change. According to (2.97), however, U
</p>
<p>cannot decrease. If U remains unchanged, then the variation just considered satis-
</p>
<p>fies (2.82) with equality. If U increases, it can be brought back to its initial value by
</p>
<p>removing the extra energy in the form of heat from the system while holding the val-
</p>
<p>ues of X1, . . . , Xm at those in the varied state, that is, X1 +δX1, . . . , Xm +δXm. The
combined result of the two step process is a decrease of S without affecting U . Thus,
</p>
<p>if (2.97) holds for all possible variations, so does (2.82). To prove the converse, sup-
</p>
<p>pose that (2.97) is violated by some variation, that is, δU &lt; 0 for this variation. The
reversible work source then receives work, which can be converted to heat, using
</p>
<p>friction for example, and injected back into the system. This will restore U to its
</p>
<p>original value but will increase S. The combined result of the two step process is a
</p>
<p>variation that violates (2.82).
</p>
<p>2.9.3 &dagger;Condition of Perfect Equilibrium
</p>
<p>In the absence of any internal constraints Xm+1, . . . , Xr, the condition of equilibrium
</p>
<p>can be formulated as follows:
</p>
<p>Condition of Equilibrium 5 For the equilibrium of an isolated system sub-
</p>
<p>ject to no internal constraint, it is necessary and sufficient that
</p>
<p>δU &ge; TδS&minus;PδV +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδNi (2.106)
</p>
<p>for all possible variations of S, V , N1, . . . , Nc, and X1, . . . , Xm.
</p>
<p>This seems rather surprising since (2.106) looks very much like (2.37), which is
</p>
<p>essentially the first law of thermodynamics. The system we have in mind is a body
</p>
<p>that is homogeneous initially and enclosed in a container without any internal par-
</p>
<p>titions. As an example of variations in X1, . . . , Xm, one may consider fluctuations
</p>
<p>in local density that upset the homogeneity. A state of equilibrium that prevails
</p>
<p>when there is no internal constraints is called the perfect equilibrium state. With a</p>
<p/>
</div>
<div class="page"><p/>
<p>2.9 Energy Representation 73
</p>
<p>slight modification to (2.106), this formulation of the condition of perfect equilib-
</p>
<p>rium becomes an important starting point for thermodynamics of inhomogeneous
</p>
<p>systems as we shall see in Chap. 6.
</p>
<p>To see that (2.106) is sufficient for equilibrium, suppose that (2.106) holds for
</p>
<p>any variations of S, V , N1, . . . , Nc, and X1, . . . , Xm. Then, δU &ge; 0 for any variations
of X1, . . . , Xm that do not affect the values of S, V , and N1, . . . , Nc. This is just (2.97)
</p>
<p>written without Xm+1, . . . , Xr, which are absent for the situation under consideration.
</p>
<p>The necessity of (2.106) takes a little more effort to establish. As in (2.80),
</p>
<p>U =U(S,V,N1, . . . ,Nc;X1, . . . ,Xm) . (2.107)
</p>
<p>in general. Thus, when S, V , N1, . . . , Nc, and X1, . . . , Xm are perturbed by infinitesi-
</p>
<p>mal amounts, the resulting variation in δU is given by
</p>
<p>δU =
</p>
<p>(
&part;U
</p>
<p>&part;S
</p>
<p>)
</p>
<p>V,N,X
</p>
<p>δS+
</p>
<p>(
&part;U
</p>
<p>&part;V
</p>
<p>)
</p>
<p>S,N,X
</p>
<p>δV +
c
</p>
<p>&sum;
i=1
</p>
<p>(
&part;U
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>S,V,N j 	=i,X
δNi
</p>
<p>+
m
</p>
<p>&sum;
i=1
</p>
<p>(
&part;U
</p>
<p>&part;Xi
</p>
<p>)
</p>
<p>S,V,N,X j 	=i
</p>
<p>δXi , (2.108)
</p>
<p>where we abbreviated the subscripts N1, . . . , Nc and X1, . . . , Xm to just N and X ,
</p>
<p>respectively.
</p>
<p>Comparing (2.108) against (2.104), we see immediately that
</p>
<p>δW rev =
m
</p>
<p>&sum;
i=1
</p>
<p>(
&part;U
</p>
<p>&part;Xi
</p>
<p>)
</p>
<p>S,V,N,X j 	=i
</p>
<p>δXi . (2.109)
</p>
<p>If the system is initially in perfect equilibrium, this quantity is nonnegative as
</p>
<p>demanded by (2.105). Therefore, dropping the last term of (2.108), we arrive at
</p>
<p>δU &ge;
(
&part;U
</p>
<p>&part;S
</p>
<p>)
</p>
<p>V,N,X
</p>
<p>δS+
</p>
<p>(
&part;U
</p>
<p>&part;V
</p>
<p>)
</p>
<p>S,N,X
</p>
<p>δV +
c
</p>
<p>&sum;
i=1
</p>
<p>(
&part;U
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>S,V,N j 	=i,X
δNi . (2.110)
</p>
<p>Now, can we say that
</p>
<p>T =
</p>
<p>(
&part;U
</p>
<p>&part;S
</p>
<p>)
</p>
<p>V,N,X
</p>
<p>? (2.111)
</p>
<p>This looks very much like the expression for T in (2.38). In arriving at that expres-
</p>
<p>sion, however, we considered a differentiation, that is, the system was assumed to
</p>
<p>be in equilibrium both before and after the change. In other words, X1, . . . , Xm were
</p>
<p>allowed to adjust themselves in order to maintain the equilibrium. This is why X
</p>
<p>does not show up as a subscript in (2.38). Going back to the definition of partial
</p>
<p>derivative, therefore, we can express T as given by (2.38) as
</p>
<p>T =
</p>
<p>(
&part;U
</p>
<p>&part;S
</p>
<p>)
</p>
<p>V,N
</p>
<p>= lim
∆S&rarr;0
</p>
<p>1
</p>
<p>∆S
[U(S+∆S,V,N1, . . . ,Nc;X1 +∆X1, . . . ,Xm +∆Xm)
</p>
<p>&minus; U(S,V,N1, . . . ,Nc;X1, . . . ,Xm)] , (2.112)</p>
<p/>
</div>
<div class="page"><p/>
<p>74 2 Thermodynamics
</p>
<p>where ∆Xi denotes the adjustment the system makes to Xi in order to maintain the
equilibrium when S is changed by ∆S. Expanding the internal energy difference into
the Taylor series,
</p>
<p>T = lim
∆S&rarr;0
</p>
<p>1
</p>
<p>∆S
</p>
<p>[(
&part;U
</p>
<p>&part;S
</p>
<p>)
</p>
<p>V,N,X
</p>
<p>∆S+
m
</p>
<p>&sum;
i=1
</p>
<p>(
&part;U
</p>
<p>&part;Xi
</p>
<p>)
</p>
<p>S,V,N,X j 	=i
</p>
<p>∆Xi
</p>
<p>]
, (2.113)
</p>
<p>where we dropped the higher order terms that vanish more rapidly than ∆S.
Now, suppose that the system is initially in equilibrium and that each Xi is capable
</p>
<p>of reversible variation, in the sense that both negative and positive values of δXi
are possible for each i. (Our usage of the word &ldquo;reversible&rdquo; in this context is due to
</p>
<p>Gibbs and differs from our earlier usage of the same word pertaining to processes.)
</p>
<p>Then, (2.105) and (2.109) lead to
</p>
<p>(
&part;U
</p>
<p>&part;Xi
</p>
<p>)
</p>
<p>S,V,N,X j 	=i
</p>
<p>= 0 , i = 1, . . . ,m . (2.114)
</p>
<p>This is now introduced into (2.113) to give
</p>
<p>T =
</p>
<p>(
&part;U
</p>
<p>&part;S
</p>
<p>)
</p>
<p>V,N,X
</p>
<p>. (2.115)
</p>
<p>This proof is due to Ref. [4]. Similarly,
</p>
<p>&minus;P =
(
&part;U
</p>
<p>&part;V
</p>
<p>)
</p>
<p>S,N,X
</p>
<p>and &micro;i =
</p>
<p>(
&part;U
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>S,V,N j 	=i,X
, i = 1, . . . ,m . (2.116)
</p>
<p>Because of (2.114), the last term of (2.108) is identically zero and (2.110) holds
</p>
<p>with equality. Therefore, by virtue of (2.115) and (2.116), we obtain
</p>
<p>δU = TδS&minus;PδV +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδNi , (2.117)
</p>
<p>which is a special case of (2.106).
</p>
<p>What if one of ∆X1, . . . , ∆Xm, say ∆X1, cannot take a positive value? Because
the system is initially in equilibrium, the partial derivative
</p>
<p>(
&part;U
</p>
<p>&part;X1
</p>
<p>)
</p>
<p>S,V,N,X j 	=1
</p>
<p>(2.118)
</p>
<p>cannot be positive. In fact, if it were positive, then the system could lower U by
</p>
<p>decreasing X1, in contradiction to the assumption that the system was in equilibrium
</p>
<p>initially. It may be that the partial derivative, which is to be evaluated at the initial
</p>
<p>state, happens to be zero. In this case, we have (2.114) holding for all i including
</p>
<p>i = 1 and we once again obtain (2.117).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.9 Energy Representation 75
</p>
<p>Fig. 2.9 The function U =U(X1). Suppose that X1 cannot exceed X
0
1 and the slope of U at X1 =X
</p>
<p>0
1
</p>
<p>is negative initially as indicated by curve a. As illustrated by curve b, the slope remains negative
</p>
<p>even after an infinitesimal change is made to S. Then, X1 = X
0
1 at equilibrium both before and after
</p>
<p>the change. So, ∆X1 = 0.
</p>
<p>On the other hand, if the partial derivative is negative, ∆X1 occurring in (2.113),
that is, the adjustment the system makes to X1 in response to the change in S must
</p>
<p>be zero. This is because an infinitesimal change in S cannot induce a finite change
</p>
<p>in the value of the partial derivative, which thus remains negative even after the
</p>
<p>change. The underlying assumption here is that the partial derivative is a continuous
</p>
<p>function of S. Figure 2.9 illustrates the situation. In this way, we see that (2.115) and
</p>
<p>(2.116) hold in this case as well. When these equations are substituted into (2.110),
</p>
<p>we arrive at (2.106).
</p>
<p>In this case, however, the inequality sign in (2.106) cannot be dropped. To see
</p>
<p>this, let us consider a variation of X1. By assumption, δX1 can only be negative.
But, its coefficient (&part;U/&part;X1)S,V,N,X j 	=1 is also negative. Thus, the contribution to
δU from any nonzero δX1 is always positive. Note carefully the difference between
∆X1 and δX1 in this discussion. The former is the adjustment the system makes to
maintain equilibrium, while the latter is the change in X1 we impose on the system
</p>
<p>by means of the reversible work source.
</p>
<p>2.9.4 &dagger;Closed System with a Chemical Reaction
</p>
<p>We remarked in Sect. 2.6.1 that (2.21) holds even in the presence of chemical reac-
</p>
<p>tions provided that the system is homogeneous and in equilibrium both before and
</p>
<p>after the infinitesimal change, during which the system remains closed. We are now
</p>
<p>ready to see why this is so.
</p>
<p>For simplicity, we allow only a single chemical reaction:
</p>
<p>c1A1 + c2A2 ⇄ c3A3 (2.119)</p>
<p/>
</div>
<div class="page"><p/>
<p>76 2 Thermodynamics
</p>
<p>among three species A1, A2, and A3. The positive constants c1, c2, and c3 are called
</p>
<p>the stoichiometric coefficients. There may be more species present in the system in
</p>
<p>addition to these three species. But, their numbers of moles are fixed. In contrast,
</p>
<p>N1, N2, and N3 can change, but only in compliance with the stoichiometry of the
</p>
<p>reaction:
</p>
<p>δN1 =&minus;
c1
</p>
<p>c3
δN3 and δN2 =&minus;
</p>
<p>c2
</p>
<p>c3
δN3 (2.120)
</p>
<p>as you saw in Exercise 2.10.
</p>
<p>If the system prior to variation is in equilibrium, (2.106) applies and
</p>
<p>δU &ge; TδS&minus;PδV +
(
&minus;c1
</p>
<p>c3
&micro;1 &minus;
</p>
<p>c2
</p>
<p>c3
&micro;2 +&micro;3
</p>
<p>)
δN3 (2.121)
</p>
<p>must hold true for any possible values of δS, δV , and δN3. In particular, it must
hold even if δS = 0 and δV = 0. Provided that δN3 is capable of assuming both
positive and negative values, we find that
</p>
<p>&minus; c1
c3
</p>
<p>&micro;1 &minus;
c2
</p>
<p>c3
&micro;2 +&micro;3 = 0 . (2.122)
</p>
<p>Thus,
</p>
<p>c1&micro;1 + c2&micro;2 = c3&micro;3 , (2.123)
</p>
<p>which is the condition of equilibrium for the reaction under consideration. Substi-
</p>
<p>tuting (2.123) back into (2.121), we find that
</p>
<p>δU &ge; TδS&minus;PδV . (2.124)
</p>
<p>If we assume that X1, . . . , Xm are all capable of reversible variations in the sense
</p>
<p>defined in the previous subsection, the inequality may be replaced by equality. Fur-
</p>
<p>thermore, if we restrict ourselves to such variations as to leave the system homoge-
</p>
<p>neous and in equilibrium, δ can be replaced by d. Thus we finally arrive at
</p>
<p>dU = T dS&minus;PdV , (2.125)
</p>
<p>which is just (2.21).
</p>
<p>2.9.5 &dagger;Maximum Work Principle
</p>
<p>We can take advantage of the tendency of a system to evolve toward a state of
</p>
<p>equilibrium and extract a useful work. For this purpose, we connect the system to
</p>
<p>an external agency and suppose that the composite system, consisting of the system
</p>
<p>and the external agency attached to it, as a whole, is isolated. We show that, in this
</p>
<p>setting, the maximum amount of work is extracted by the external agency when the
</p>
<p>process is carried out reversibly.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.9 Energy Representation 77
</p>
<p>Fig. 2.10 Graphical illustration of the maximum work principle. The work extracted, Ui &minus;U f , is
maximum when the arrow connecting the initial state to the final state points horizontally to the
</p>
<p>left so that S f &minus;Si = 0. In the absence of the external agency, U f =Ui and the arrow would point
vertically upward as in Fig. 2.8.
</p>
<p>Let us first plot the equilibrium value of S of the system as a function of its U
</p>
<p>as shown in Fig. 2.10. If the system is not in equilibrium, its representative point
</p>
<p>lies below this curve because S of the system is not the maximum possible for a
</p>
<p>given value of U . Let us therefore suppose that the representative point is initially at
</p>
<p>(Ui,Si).
If the system is allowed to relax toward equilibrium in the absence of the external
</p>
<p>agency, then as in Fig. 2.8, the representative point will simply move vertically
</p>
<p>upward until it hits the curve S(U).
We now suppose instead that the system is linked to the external agency. From
</p>
<p>the conservation of energy, the work W extracted by the latter during the approach
</p>
<p>to equilibrium is given by
</p>
<p>W =Ui &minus;U f , (2.126)
where Ui and U f denote the internal energy of the system at the beginning and the
</p>
<p>end of the equilibration process, respectively. If the process is such as not to affect
</p>
<p>the entropy of the external agency, we have
</p>
<p>S f &minus;Si &ge; 0 . (2.127)
</p>
<p>Because the composite system, with the external agency attached, is isolated, this
</p>
<p>means that the process is spontaneous. From Fig. 2.10, it is evident that the maxi-
</p>
<p>mum amount of work is extracted when the equality holds in (2.127), that is if the
</p>
<p>process occurred reversibly. This is the maximum work principle.
</p>
<p>As an example of the external agency whose entropy remains unaffected, we may
</p>
<p>consider a purely classical mechanical device that can exchange energy with the
</p>
<p>system only in the form of work. A so-called cycle, which by definition, repeatedly
</p>
<p>returns to its original state is another example.</p>
<p/>
</div>
<div class="page"><p/>
<p>78 2 Thermodynamics
</p>
<p>Fig. 2.11 The work source extracts work from an adiabatic expansion of an ideal gas. The expan-
</p>
<p>sion is reversible when λ = 1 and irreversible otherwise.
</p>
<p>Example 2.2. Adiabatic expansion of an ideal gas: Suppose that an ideal gas
</p>
<p>is confined to a cylinder made of a rigid and adiabatic wall and is fitted with
</p>
<p>a piston. See Fig. 2.11. The space outside the cylinder is vacuum and our aim
</p>
<p>is to extract work using the tendency of the gas to expand. To do this, we
</p>
<p>connect the work source to the system so that it exerts a pressure λP on the
piston. Here P is the pressure inside the cylinder and λ is a constant. In order
for the expansion to occur, λ &lt; 1.
</p>
<p>For the process considered, d̄Q = 0, and hence
</p>
<p>dU = d̄W =&minus;λPdV . (2.128)
</p>
<p>In order for this quantity to be negative so that we can extract positive work,
</p>
<p>λ &gt; 0 and we assume that this is the case in what follows.
Using the equation of state of an ideal gas and recalling from Sect. 2.6.4
</p>
<p>that dU =CV dT ,
</p>
<p>CV dT =&minus;
λNRT
</p>
<p>V
dV , (2.129)
</p>
<p>which is now integrated to give
</p>
<p>ln
Tf
</p>
<p>Ti
=&minus;λNR
</p>
<p>CV
ln
</p>
<p>Vf
</p>
<p>Vi
. (2.130)
</p>
<p>Applying (2.59), we find that the work extracted from the process is
</p>
<p>Ui &minus;U f =CV Ti
[
</p>
<p>1&minus;
(
</p>
<p>Vi
</p>
<p>Vf
</p>
<p>)λNR/CV ]
, (2.131)
</p>
<p>while it follows from (2.60) that
</p>
<p>S f &minus;Si = (1&minus;λ )NR ln
Vf
</p>
<p>Vi
. (2.132)
</p>
<p>Remembering that Vf /Vi &gt; 1 for an expansion, we see that S f &minus;Si is maximum
at λ = 0 (free expansion) and decreases monotonically with increasing λ until
it reaches the minimum value zero at λ = 1 (a reversible process). In contrast,</p>
<p/>
</div>
<div class="page"><p/>
<p>2.9 Energy Representation 79
</p>
<p>Ui &minus;U f is zero for the free expansion and increases monotonically until it
reaches the maximum value for the reversible expansion.
</p>
<p>We can also extract work from the natural tendency for two bodies in thermal
</p>
<p>contact to reach a common temperature by means of a heat engine. This is a device
</p>
<p>that visits a given set of multiple states in a cyclic fashion and in so doing, it receives
</p>
<p>heat from the higher temperature body, converts a part of the energy into work, and
</p>
<p>expels the rest in the form of heat into the lower temperature body. After completing
</p>
<p>one cycle, the heat engine itself returns to the original state while the temperature
</p>
<p>difference between the bodies decreases. The heat engine is an example of a cycle.
</p>
<p>Analogously, we can also operate what might be called a material engine
</p>
<p>between two bodies having the same temperature but different values of the chemi-
</p>
<p>cal potentials for some of the species, say species 1. In this case, the engine receives
</p>
<p>heat and molecules of species 1 from higher chemical potential body and delivers
</p>
<p>them to the lower chemical potential body while extracting work from the process.
</p>
<p>These possibilities are considered in the following two examples on the ground
</p>
<p>of the general principle. Each example is followed by an exercise, in which you are
</p>
<p>asked to consider the inner working of reversible engines in some detail.
</p>
<p>Example 2.3. Heat engine: Consider two homogeneous bodies H and L at
</p>
<p>thermal contact that are initially at different temperatures Th and Tl , respec-
</p>
<p>tively. To extract work, we break the thermal contact and insert a heat engine
</p>
<p>HE between H and L. The system now consists of H, L, and HE, and our goal
</p>
<p>is to figure out the amount of work this composite system delivers to a work
</p>
<p>source when the latter is linked to HE. We note that the work source is purely
</p>
<p>a classical mechanical device described in Sect. 2.2 and does not exchange
</p>
<p>heat with the composite system.
</p>
<p>We restrict ourselves to processes occurring without affecting the volume
</p>
<p>of H or that of L. We also suppose that only a very small amount of heat
</p>
<p>and work are involved during a single cycle of HE so that Th and Tl may be
</p>
<p>regarded as constant. Then, denoting by d̄Qh the amount per cycle of heat
</p>
<p>transferred from H to HE, we have
</p>
<p>&minus; d̄Qh = dUh = ThdSh , (2.133)
</p>
<p>and hence
</p>
<p>dSh =&minus;
d̄Qh
</p>
<p>Th
. (2.134)
</p>
<p>Similarly, if d̄Ql is the amount per cycle of heat transferred from HE to L,
</p>
<p>d̄Ql = dUl = TldSl , (2.135)</p>
<p/>
</div>
<div class="page"><p/>
<p>80 2 Thermodynamics
</p>
<p>and hence
</p>
<p>dSl =
d̄Ql
</p>
<p>Tl
. (2.136)
</p>
<p>Since HE returns to its original state at the end of one cycle, dShe = 0. So,
for the composite system, we have
</p>
<p>dS = dSh +dSl =&minus;
d̄Qh
</p>
<p>Th
+
</p>
<p>d̄Ql
</p>
<p>Tl
. (2.137)
</p>
<p>According to the first law, the work d̄W delivered to the work source is
</p>
<p>d̄W =&minus;(dUh +dUl) = d̄Qh &minus; d̄Ql , (2.138)
</p>
<p>where we note that dUhe = 0.
Now, let d̄Ql := λ d̄Qh and rewrite these equations as
</p>
<p>dS =
</p>
<p>(
λ
</p>
<p>Tl
&minus; 1
</p>
<p>Th
</p>
<p>)
d̄Qh and d̄W = (1&minus;λ ) d̄Qh . (2.139)
</p>
<p>For spontaneous processes, we have dS &ge; 0. Since we are interested in the
amount of work that can be extracted from such processes, we demand that
</p>
<p>d̄W &ge; 0. These considerations lead to
</p>
<p>Tl
</p>
<p>Th
&le; λ &le; 1 . (2.140)
</p>
<p>When λ = 1, d̄W = 0 and d̄Qh = d̄Ql , and the process is seen to be identical
to the direct heat conduction between H and L through a diathermal wall. In
</p>
<p>contrast, the maximum of d̄W occurs when λ = Tl/Th, and is given by
</p>
<p>d̄W =
</p>
<p>(
1&minus; Tl
</p>
<p>Th
</p>
<p>)
d̄Qh . (2.141)
</p>
<p>In this case, dS = 0 and the process is reversible.
This example demonstrates the existence of an upper limit of the efficiency
</p>
<p>η of any heat engines:
</p>
<p>η :=
d̄W
</p>
<p>d̄Qh
&le; 1&minus; Tl
</p>
<p>Th
. (2.142)
</p>
<p>Exercise 2.13. As an example of reversible heat engines operating between the
</p>
<p>higher temperature body H at Th and the lower temperature one L at Tl , consider
</p>
<p>a Carnot cycle. The engine uses a working fluid confined to a cylinder fitted with a
</p>
<p>frictionless piston. The wall of the cylinder is rigid and impermeable to all species.
</p>
<p>The wall is also adiabatic except that one side of the wall, denoted by ΣT , can be
made either adiabatic or diathermal as needed. The heat engine undergoes the fol-
</p>
<p>lowing sequence of processes:</p>
<p/>
</div>
<div class="page"><p/>
<p>2.9 Energy Representation 81
</p>
<p>a. The engine containing the working fluid at Th is brought to thermal contact with
</p>
<p>H through diathermal ΣT and expands isothermally and reversibly at Th while
extracting heat Qh from H.
</p>
<p>b. The wall ΣT becomes adiabatic and the fluid expands reversibly until its temper-
ature reaches Tl .
</p>
<p>c. The fluid is brought to thermal contact with L through diathermal ΣT and is
compressed isothermally and reversibly while rejecting heat Ql to L.
</p>
<p>d. The wall ΣT becomes adiabatic again and the fluid is compressed reversibly until
its temperature reaches Th.
</p>
<p>Assuming that the working fluid is an ideal gas, show that W = (1&minus; Tl/Th)Q by
means of an explicit computation of work and heat involved in each step. ///
</p>
<p>Example 2.4. Material engine: Consider two homogeneous mixtures H and
</p>
<p>L both at the same temperature T and suppose that the chemical potential
</p>
<p>of species 1 in H and that in L, denoted by &micro;h and &micro;l , respectively, satisfy
&micro;h &gt; &micro;l . If they are brought in contact through a wall permeable to species 1,
molecules of 1 will flow from H to L. Instead of the permeable wall, we will
</p>
<p>insert a material engine ME in between. Once again, we are interested in the
</p>
<p>amount of work the composite system consisting of H, L, and ME delivers to
</p>
<p>the work source.
</p>
<p>We suppose that only a very small amount of heat and material transfer is
</p>
<p>involved during one cycle of the engine, leaving T , &micro;h, and &micro;l constant. For
the heat and material transfer from H to ME, we have
</p>
<p>dUh = T dSh &minus;&micro;hdN , (2.143)
</p>
<p>where dN is the number of moles of species 1 being transferred to ME. Simi-
</p>
<p>larly,
</p>
<p>dUl = T dSl +&micro;ldN . (2.144)
</p>
<p>Note that ME gives up as many molecules of species 1 to L as it has
</p>
<p>received from H. Otherwise, there will be an accumulation or depletion of
</p>
<p>the molecules in ME and it will not qualify as a cycle.
</p>
<p>Combining these equations,
</p>
<p>d̄W =&minus;(dUh +dUl) =&minus;T dS+(&micro;h &minus;&micro;l)dN , (2.145)
</p>
<p>where dS = dSh +dSl is the increment in entropy of the composite system per
single cycle since dSmc = 0 as in the previous example. The maximum of d̄W
occurs when dS = 0 and is given by (&micro;h &minus;&micro;l)dN.
</p>
<p>Exercise 2.14. In analogy to Carnot cycle, let us consider a reversible material
</p>
<p>engine operating between two mixtures H and L. We suppose that they are at the
</p>
<p>same temperature T and denote the chemical potential of species 1 in H and that in
</p>
<p>L by &micro;h and &micro;l , respectively, where &micro;h &gt; &micro;l . As the working fluid, we use a pure</p>
<p/>
</div>
<div class="page"><p/>
<p>82 2 Thermodynamics
</p>
<p>ideal gas of species 1 confined to a cylinder fitted with a frictionless piston. The
</p>
<p>cylinder is made of a rigid wall that is diathermal and impermeable to all species.
</p>
<p>However, one side of the wall, which we call Σ&micro; , can be made either permeable or
impermeable to species 1 as needed. In each of the following steps, expansion or
</p>
<p>compression is performed both isothermally and reversibly:
</p>
<p>a. The engine containing the working fluid at &micro;h is brought to thermal contact with
H through the permeable Σ&micro; and expands while extracting ∆N moles of species
1 from H.
</p>
<p>b. The wall Σ&micro; becomes impermeable and the fluid expands until the chemical
potential of species 1 becomes &micro;l .
</p>
<p>c. The fluid is brought to thermal contact with L through Σ&micro; , which now is made
permeable to species 1. The fluid is compressed while rejecting ∆N moles of
species 1 to L.
</p>
<p>d. The wall Σ&micro; is made impermeable again and the fluid is compressed until the
chemical potential becomes &micro;h.
</p>
<p>Noting that
</p>
<p>&micro;(T,P) = &micro;◦(T )+RT lnP (2.146)
</p>
<p>for a pure ideal gas, show that W = (&micro;h &minus; &micro;l)∆N. Not surprisingly, this is actually
the work extracted from an isothermal and reversible expansion of ∆N moles of an
ideal gas from the pressure Ph, at which &micro; = &micro;h, to Pl at which &micro; = &micro;l . ///
</p>
<p>2.10 Euler Relation
</p>
<p>We note that the variables occurring in (2.36) are all extensive quantities. That is,
</p>
<p>they are all proportional to the &ldquo;size&rdquo; of the system. This can be expressed more
</p>
<p>precisely as
</p>
<p>U(λS,λV,λN1, . . . ,λNc) = λU(S,V,N1, . . . ,Nc) , (2.147)
</p>
<p>where λ is a positive constant. A function that satisfies such a relation is called a
homogeneous function of degree one. This leads to the following important equation
</p>
<p>called the Euler relation:
</p>
<p>U = T S&minus;PV +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iNi . (2.148)
</p>
<p>As we see in Sect. 2.15, (2.148) is a mathematical consequence of (2.147) and can be
</p>
<p>derived without making any further reference to the physical situation beyond what
</p>
<p>has been made so far. Nevertheless, it is of some interest to consider an alternative,
</p>
<p>physically more appealing, derivation of (2.148).
</p>
<p>Let us consider a large homogeneous body. Within this body, we choose a small
</p>
<p>part enclosed by the smaller circle in Fig. 2.12 as our system. Note that this act of
</p>
<p>choosing the system is purely a thought process and does not involve us putting in
</p>
<p>any physical wall. In other words, the partition separating the system from the rest</p>
<p/>
</div>
<div class="page"><p/>
<p>2.10 Euler Relation 83
</p>
<p>....................................................................................................................................................................... ........ ........ ............... . ....... . ....... ........... . ....... . . ....... . ....... . ....... ..... .... . ....... . .
....... . .......
</p>
<p>. ....... .
.......
........
</p>
<p>.......
........
........
...........
.........
........
........
........
.........
.........
........
</p>
<p>........
........
</p>
<p>........
........
</p>
<p>.......
.........
</p>
<p>..............
..............
</p>
<p>...............
</p>
<p>......................................................................................................... ........ ......... ......... ......... ......... ......... .........
........ .
</p>
<p>....... .
........
.........
.........
.........
.........
.........
</p>
<p>.........
........
</p>
<p>..................
........
</p>
<p>Fig. 2.12 A system taken inside a homogeneous body is indicated by the smaller circle. The larger
</p>
<p>circle represents the system after the infinitesimal change we considered in writing (2.150). Note
</p>
<p>that the expansion of the system boundary has no effect on the physical state of the matter anywhere
</p>
<p>in the homogeneous body.
</p>
<p>of the homogeneous body is diathermal, permeable to all species, and has no effect
</p>
<p>either on the state or the property of the homogeneous body.9
</p>
<p>Let u, s, and ni denote, respectively, the densities of internal energy, the entropy,
</p>
<p>and the number of moles of species i in the homogeneous body. Then, U , S, and Ni
of the system are given by
</p>
<p>U = uV , S = sV , and Ni = niV , i = 1, . . . ,c , (2.149)
</p>
<p>where V is the volume of the system.
</p>
<p>Now, let us consider a process in which the partition is enlarged to contain a
</p>
<p>larger part of the homogeneous body. Once again, this is a thought process and has
</p>
<p>no impact on the physical state of the body including the values of u, s, and n1, . . . ,
</p>
<p>nc. But, U , S, and N1, . . . , Nc of the system do change. If we denote by V +dV the
volume of the system after the change, which is indicated in Fig. 2.12 by the larger
</p>
<p>circle, we have
</p>
<p>U +dU = u(V +dV ) , S+dS = s(V +dV ) ,
</p>
<p>Ni +dNi = ni(V +dV ) , i = 1, . . . ,c . (2.150)
</p>
<p>Comparing (2.149) and (2.150), we find
</p>
<p>dU = udV , dS = sdV , and dNi = nidV , i = 1, . . . ,c , (2.151)
</p>
<p>which is now substituted into (2.37) to yield
</p>
<p>udV = T sdV &minus;PdV +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;inidV . (2.152)
</p>
<p>Multiplying both sides by V/dV , we arrive at (2.148).</p>
<p/>
</div>
<div class="page"><p/>
<p>84 2 Thermodynamics
</p>
<p>2.11 Gibbs&ndash;Duhem Relation
</p>
<p>Suppose that the state of the system is altered by infinitesimal changes made to the
</p>
<p>independent variables S, V , and N1, . . . , Nc. In general, this will be accompanied
</p>
<p>by the corresponding changes in the dependent variables U , T , P, and &micro;1, . . . , &micro;c.
If the system is homogeneous and in equilibrium both before and after the change
</p>
<p>is made, (2.148) must hold for both of these states. Thus, applying (2.148) to the
</p>
<p>system after the change, we write
</p>
<p>U +∆U = (T +∆T )(S+dS)&minus; (P+∆P)(V +dV )+
c
</p>
<p>&sum;
i=1
</p>
<p>(&micro;i +∆&micro;i)(Ni +dNi) ,
</p>
<p>(2.153)
</p>
<p>where U is the internal energy of the system before the change, and ∆U is the change
in the internal energy due to the change in the independent variables. (As explained
</p>
<p>in Appendix B.1, ∆U = dU + d2U/2+ d3U/6+ &middot; &middot; &middot;, in which the first-order term
dU dominates the remaining higher order terms.) Using (2.37) and (2.148) applied
</p>
<p>for the initial state, we obtain
</p>
<p>∆U = dU +S∆T &minus;V∆P+
c
</p>
<p>&sum;
i=1
</p>
<p>Ni∆&micro;i +dS∆T &minus;dV∆P+
c
</p>
<p>&sum;
i=1
</p>
<p>dNi∆&micro;i . (2.154)
</p>
<p>Retaining only the first-order terms, that is, ignoring such terms as dS∆T and replac-
ing ∆U , ∆T , ∆P, and ∆&micro;i, by dU , dT , dP, and d&micro;i, respectively, we find
</p>
<p>SdT &minus;V dP+
c
</p>
<p>&sum;
i=1
</p>
<p>Nid&micro;i = 0 , (2.155)
</p>
<p>which is known as the Gibbs&ndash;Duhem relation and indicates that T , P, and &micro;1, . . . ,
&micro;c cannot be varied independently.
</p>
<p>Dividing (2.155) by V ,
</p>
<p>dP = sdT +
c
</p>
<p>&sum;
i=1
</p>
<p>nid&micro;i , (2.156)
</p>
<p>from which we find
(
&part;P
</p>
<p>&part;T
</p>
<p>)
</p>
<p>&micro;
</p>
<p>= s and
</p>
<p>(
&part;P
</p>
<p>&part;&micro;i
</p>
<p>)
</p>
<p>T,&micro; j 	=i
</p>
<p>= ni . (2.157)
</p>
<p>The first equation, for example, is obtained by setting d&micro;1 = &middot; &middot; &middot;= d&micro;c = 0 in (2.156)
and dividing both sides by dT .</p>
<p/>
</div>
<div class="page"><p/>
<p>2.12 &Dagger;Gibbs Phase Rule 85
</p>
<p>2.12 &Dagger;Gibbs Phase Rule
</p>
<p>In thermodynamics, the word phase refers to a homogeneous body without any
</p>
<p>regard to its size. Thermodynamic state of a phase can be specified once the values
</p>
<p>of s and n1, . . . , nc are fixed. Homogeneous bodies with identical values of these
</p>
<p>variables are the same phase regardless of their size. Those with distinct values of
</p>
<p>these variables constitute distinct phases.
</p>
<p>Multiple phases can coexist at equilibrium. By heating water under atmospheric
</p>
<p>pressure, you can achieve liquid&ndash;vapor coexistence of water in a kettle at 100◦C.
If you pour enough sugar into a glass of water, you can observe the coexistence
</p>
<p>between aqueous sugar solution and sugar crystals.
</p>
<p>Experience shows, however, that we cannot maintain the liquid&ndash;vapor coexis-
</p>
<p>tence of water if we fix the pressure and change the temperature from 100◦C. You
can certainly achieve the coexistence at temperatures other than 100◦C, but only by
properly adjusting the pressure. Why should this be?
</p>
<p>The interface between coexisting phases may be regarded as a partition that is
</p>
<p>diathermal, movable, and permeable to all species. Continuing with the example of
</p>
<p>pure water, therefore, we have
</p>
<p>T l = T v , Pl = Pv , and &micro; l = &micro;v (2.158)
</p>
<p>as the condition of equilibrium, where the superscripts l and v refer to the liquid and
</p>
<p>the vapor phases, respectively. Now, suppose that we perturb the state of the system
</p>
<p>by changing its temperature, for example. If the phase coexistence is to subsist,
</p>
<p>(2.158) must hold after the change also:
</p>
<p>T l +dT l = T v +dT v , Pl +dPl = Pv +dPv , and &micro; l +d&micro; l = &micro;v +d&micro;v .
(2.159)
</p>
<p>It follows that
</p>
<p>dT l = dT v , dPl = dPv , and d&micro; l = d&micro;v . (2.160)
</p>
<p>As we have seen in the previous section, these infinitesimal quantities are subject
</p>
<p>to the Gibbs&ndash;Duhem relation:
</p>
<p>sldT &minus;dP+nld&micro; = 0 and svdT &minus;dP+nvd&micro; = 0 , (2.161)
</p>
<p>where we dropped the superscripts from dT , dP, and d&micro; as they are common to both
phases. If we hold P constant as we change T , (2.161) reduces to
</p>
<p>sldT +nld&micro; = 0 and svdT +nvd&micro; = 0 . (2.162)
</p>
<p>Unless the ratio sl/nl happens to be equal to sv/nv, these equations demand that
both dT and d&micro; be zero. That is, if we want to perturb the state of the system
while maintaining the vapor&ndash;liquid coexistence, we cannot possibly hold P constant.
</p>
<p>Instead, (2.161) uniquely determines the necessary change one must make to P for
</p>
<p>a given value of dT . The same set of equations also determine the value of d&micro; .</p>
<p/>
</div>
<div class="page"><p/>
<p>86 2 Thermodynamics
</p>
<p>In the case of sugar crystals in a glass of water, the condition of equilibrium is
</p>
<p>given by
</p>
<p>T c = T l , Pc = Pl , &micro;cs = &micro;
l
s , and &micro;
</p>
<p>c
w &ge; &micro; lw , (2.163)
</p>
<p>where the superscripts c and l refer to the crystal and liquid phases, respectively. The
</p>
<p>subscript s denotes sugar while w refers to water. The inequality in the last relation
</p>
<p>cannot be dropped since the crystal is, by assumption, pure sugar, and the number
</p>
<p>of water molecules in the crystal is incapable of a negative variation. Considering
</p>
<p>an infinitesimal change that maintains the coexistence, we have
</p>
<p>dT c = dT l , dPc = dPl , d&micro;cs = d&micro;
l
s , (2.164)
</p>
<p>while
</p>
<p>&micro;cw +d&micro;
c
w &ge; &micro; lw +d&micro; lw , (2.165)
</p>
<p>which cannot be simplified further. The Gibbs&ndash;Duhem relations for the phases can
</p>
<p>then be written as
</p>
<p>sldT &minus;dP+nlsd&micro;s +nlwd&micro; lw = 0 and scdT &minus;dP+ncsd&micro;s = 0 . (2.166)
</p>
<p>Thus, in order to maintain the coexistence, only two of the four infinitesimal quan-
</p>
<p>tities dT , dP, d&micro;s, and d&micro; lw can be specified independently and only within the con-
fines of the inequality (2.165).
</p>
<p>The above consideration can be generalized for coexistence among M phases of
</p>
<p>c component mixture, for which
</p>
<p>sIdT &minus;dP+
c
</p>
<p>&sum;
i=1
</p>
<p>nIi d&micro;i = 0 , I = 1, . . . ,M , (2.167)
</p>
<p>where the superscripts I labeling the phase should not be confused with exponents.
</p>
<p>We note that (2.167) provides M equations among c+2 infinitesimal quantities dT ,
dP, and d&micro;1, . . . , d&micro;c. Thus, only c+2&minus;M of them can be specified independently.
This result is known as the Gibbs phase rule.
</p>
<p>The examples we considered above are consistent with the predictions of the
</p>
<p>Gibbs phase rule. In the case of vapor&ndash;liquid coexistence of water, M = 2, c= 1, and
hence c+2&minus;M = 1, indicating that only one of dT , dP, and d&micro; can be specified as
we please. For crystal&ndash;solution coexistence, M = 2, c = 2, and hence c+2&minus;M = 2.
</p>
<p>We note that the phase rule is obtained using the Gibbs&ndash;Duhem relation, which
</p>
<p>is a consequence of (2.37). The validity of the latter equation depends, among other
</p>
<p>things, on (2.5). But, this equation does not apply to a solid under a nonhydrostatic
</p>
<p>stress. Thus, when a solid phase is involved, the phase rule we derived applies only
</p>
<p>if the solid is under a hydrostatic pressure. During the initial stage of the first-order
</p>
<p>phase transition, the so-called critical nucleus, a small fragment of the new phase,
</p>
<p>forms in a bulk metastable phase. While the nucleus and the metastable phase are
</p>
<p>in (unstable) equilibrium, the phase rule does not apply here, either. This is because
</p>
<p>the nucleus is inhomogeneous and its fundamental equation cannot be written in the
</p>
<p>form of (2.37). (See Sect. 6.7.3.)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.13 Free Energies 87
</p>
<p>2.13 Free Energies
</p>
<p>While isolated systems are conceptually the simplest to consider, rarely in our every-
</p>
<p>day life do we actually encounter such a system. Far more common is a system
</p>
<p>whose temperature, for example, is actively maintained through an interaction with
</p>
<p>some external body. It is therefore of practical importance to develop thermodynam-
</p>
<p>ics for systems that are not isolated.
</p>
<p>In this section, we present the details only for a closed system held at constant
</p>
<p>temperature and volume. The analysis, however, can be generalized straightfor-
</p>
<p>wardly to other systems and you are invited to explore them in the exercises col-
</p>
<p>lected toward the end of this section.
</p>
<p>2.13.1 Fixing Temperature
</p>
<p>Before we proceed further, we need to know what exactly is meant by temperature
</p>
<p>being fixed. We recall that 1/T = (&part;S/&part;U)V,N , which is a function of U , V , N1,
. . . , Nc. Therefore, once the values of these independent variables are fixed, as is the
</p>
<p>case with any isolated system, the temperature is also fixed. This will be fine as far
</p>
<p>as we are concerned only with a state of equilibrium. In formulating condition of
</p>
<p>equilibrium, however, we must compare two states differing infinitesimally by the
</p>
<p>values of the additional variables as seen from (2.81). When the values of X1, . . . , Xm
are changed from the equilibrium values in an isolated system, there is no guarantee
</p>
<p>that the system temperature remain unchanged. Moreover, when we talk about a
</p>
<p>system held at a given temperature, we presume our ability to set that temperature
</p>
<p>as we please. Thus, we should demand that the temperature occur in our formulation
</p>
<p>not as a dependent variable but as an independent variable.
</p>
<p>One simple way to fix the temperature of a system is to immerse it in a large
</p>
<p>body of liquid with uniform temperature and allow for exchange of heat between
</p>
<p>the two. If the body is much larger than the system itself, the temperature of the
</p>
<p>body will remain unchanged despite the energy exchange with the system. Such a
</p>
<p>body is called a heat bath.
</p>
<p>2.13.2 Condition of Equilibrium
</p>
<p>Because we are interested in the condition of equilibrium of the system but not that
</p>
<p>of the heat bath, we make a few simplifying assumptions about the bath. In addition
</p>
<p>to being sufficiently large, we assume that the relaxation time of the heat bath is
</p>
<p>extremely short. That is, the heat bath reestablishes equilibrium very quickly after
</p>
<p>it is disturbed by exchanging energy with the system. This allows us to assume that
</p>
<p>the heat bath is always in equilibrium.</p>
<p/>
</div>
<div class="page"><p/>
<p>88 2 Thermodynamics
</p>
<p>Now, suppose that the composite system consisting of the heat bath and the sys-
</p>
<p>tem of interest, as a whole, is isolated. If the composite system is in equilibrium,
</p>
<p>then the system is evidently in equilibrium at the temperature T of the heat bath.
</p>
<p>Conversely, if the system is in equilibrium at T , the composite system is also in
</p>
<p>equilibrium by virtue of the assumptions made of the heat bath. Thus, for equilib-
</p>
<p>rium of the system, it is necessary and sufficient that
</p>
<p>δUcomp = δU +δUB &ge; 0 (2.168)
</p>
<p>for any possible variation of the state of the composite system. Here the superscripts
</p>
<p>comp and B denote the composite system and the heat bath, respectively.
</p>
<p>Applying (2.37) to the heat bath,
</p>
<p>δUB = TδSB . (2.169)
</p>
<p>Since Scomp = S+SB is constant as indicated in Condition of Equilibrium 3, δSB =
&minus;δS, and hence
</p>
<p>δUB =&minus;TδS , (2.170)
which is now substituted into (2.168) to give
</p>
<p>δUcomp = δU &minus;TδS = δ (U &minus;T S) = δF , (2.171)
</p>
<p>where we used the fact that the variations considered do not affect T by virtue of the
</p>
<p>first assumptions made of the heat bath. We also introduced a new quantity called
</p>
<p>the Helmholtz free energy through the equation:
</p>
<p>F :=U &minus;T S . (2.172)
</p>
<p>Note that, when considering all possible variations, the system temperature T
</p>
<p>was fixed through exchange of heat with the heat bath. Remembering that V and N1,
</p>
<p>. . . , Nc were also fixed and denoting, as before, any other constraints by means of the
</p>
<p>constant values of Xm+1, . . . , Xr, we arrive at the following condition of equilibrium:
</p>
<p>Condition of Equilibrium 6 For equilibrium of a closed system held at a
</p>
<p>given temperature T and volume V , it is necessary and sufficient that
</p>
<p>(δF)T,V,N1,...,Nc;Xm+1,...,Xr &ge; 0 (2.173)
</p>
<p>holds for any possible variation of the state of the system.
</p>
<p>From (2.171) and the discussion in Sect. 2.9.2, we have
</p>
<p>δF = δUcomp = δW rev . (2.174)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.13 Free Energies 89
</p>
<p>Fig. 2.13 The system of interest in contact with a heat bath. The reversible work source is con-
</p>
<p>nected only to the system and not to the heat bath.
</p>
<p>Thus, Condition of Equilibrium 4 applies also to the current situation provided that
</p>
<p>the subscript S is replaced by T in (2.105). We emphasize that the reversible work
</p>
<p>source is connected only to the system of interest and not to the heat bath as shown
</p>
<p>in Fig. 2.13. This is because the reversible work source does not exchange heat
</p>
<p>with other bodies. Thus, δW rev in (2.174) is the work done on the system when
it is allowed to exchange heat with the heat bath as needed in order to keep its
</p>
<p>temperature constant.
</p>
<p>2.13.3 Direction of a Spontaneous Process
</p>
<p>The Helmholtz free energy cannot increase for spontaneous processes occurring in
</p>
<p>the system. To see this, we start by noting that
</p>
<p>dScomp = dS+dSB = dS+
1
</p>
<p>T
dUB . (2.175)
</p>
<p>Because the composite system is isolated, dUB =&minus;dU :
</p>
<p>dScomp = dS&minus; 1
T
</p>
<p>dU =
d(T S&minus;U)
</p>
<p>T
=&minus;dF
</p>
<p>T
. (2.176)
</p>
<p>For spontaneous processes in the composite system, dScomp &ge; 0, and hence dF &le; 0.
Applying (2.174) for a finite (as opposed to infinitesimal) constant temperature
</p>
<p>process, we have
</p>
<p>∆F =W rev . (2.177)
</p>
<p>Likewise, (2.176) gives
</p>
<p>∆Scomp =&minus;∆F
T
</p>
<p>(2.178)</p>
<p/>
</div>
<div class="page"><p/>
<p>90 2 Thermodynamics
</p>
<p>Fig. 2.14 The entropy change ∆Scomp = Scompb &minus;S
comp
a for the spontaneous process A&rarr;B is related
</p>
<p>to the energy difference W rev =U
comp
c &minus;Ucompa by (2.179).
</p>
<p>for a finite process of constant T . Combining (rather blindly) these equations, we
</p>
<p>obtain
</p>
<p>∆Scomp =&minus;W
rev
</p>
<p>T
. (2.179)
</p>
<p>We examine the validity of this equation closely in the next subsection. Accept-
</p>
<p>ing (2.179) for a moment, suppose that W rev is positive so that ∆Scomp is negative.
Identifying S&prime; &minus; S in (2.20) with ∆Scomp and using (2.179), we conclude that the
probability that the system undergoes the unfavorable finite change as a result of
</p>
<p>spontaneous fluctuation is given by
</p>
<p>e&minus;W
rev/kBT . (2.180)
</p>
<p>You will frequently encounter (2.180) when studying activated processes such as
</p>
<p>nucleation. If W rev is negative, then ∆Scomp is positive and the process occurs spon-
taneously.
</p>
<p>2.13.4 &dagger;W rev and a Spontaneous Process
</p>
<p>At this point, you may be feeling a little uncomfortable with (2.179). In fact, (2.176)
</p>
<p>pertains to a spontaneous process, while (2.174) is written for a process that is car-
</p>
<p>ried out reversibly by means of a reversible work source. So, δF in (2.174) and dF
in (2.176) refer to different processes. Is it really admissible to combine these two
</p>
<p>equations to get (2.179)? How do we know that these two quantities have the same
</p>
<p>value when they refer to different processes?
</p>
<p>Figure 2.14 helps us answer this question. Because of the first assumption regard-
</p>
<p>ing the heat bath, the slope &part;Scomp/&part;Ucomp = 1/T is constant and Scomp is a linear
function of Ucomp.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.13 Free Energies 91
</p>
<p>If we prepare the composite system in state A and simply let it evolve, it will
</p>
<p>eventually reach state B. The resulting change in entropy is
</p>
<p>∆Scomp = Scompb &minus;Scompa &gt; 0 . (2.181)
</p>
<p>Since the slope of the line is 1/T , this same quantity is given by
</p>
<p>∆Scomp =
U
</p>
<p>comp
a &minus;Ucompc
</p>
<p>T
. (2.182)
</p>
<p>But, U
comp
a &minus;Ucompc &gt; 0 is the reversible work we would be able to extract from
</p>
<p>the composite system during the spontaneous process if it were to be carried out
</p>
<p>reversibly, thus leaving the composite system in state C instead of B. Now, W rev
</p>
<p>denotes the work done on the composite system, and hence
</p>
<p>W rev =Ucompc &minus;Ucompa &lt; 0 . (2.183)
</p>
<p>Combining the last two equations, we arrive at (2.179).
</p>
<p>Conversely, if the composite system is initially at state B and a spontaneous fluc-
</p>
<p>tuation brings it to state A,
</p>
<p>∆Scomp = Scompa &minus;S
comp
b &lt; 0 . (2.184)
</p>
<p>Using Fig. 2.14, we find that this same quantity is given by
</p>
<p>∆Scomp =&minus;U
comp
a &minus;Ucompc
</p>
<p>T
, (2.185)
</p>
<p>in which U
comp
a &minus;Ucompc &gt; 0 is the reversible work required to bring the composite
</p>
<p>system to state A but starting from state C instead of B.
</p>
<p>It is worth emphasizing that (2.179) holds only if B and C are equilibrium states
</p>
<p>and state A lies directly below B and directly to the right of C.
</p>
<p>Fluctuations occurring in a stable system or those that initiate the eventual decay
</p>
<p>of a metastable system toward a more stable state are examples of process B&rarr;A.
Using (2.179), we can, if desired, estimate ∆Scomp by means of a thought experiment
in which the composite system is brought from C to A along a reversible path.10
</p>
<p>Equation (2.179), when combined with (2.174) and (2.176), implies that
</p>
<p>Fb &minus;Fa = Fc &minus;Fa , (2.186)
</p>
<p>leading us to expect that any difference between states B and C is confined solely
</p>
<p>to the heat bath. In fact, the composite system can be brought from state B to C
</p>
<p>by removing heat from the heat bath. Since the heat bath is extremely large, this
</p>
<p>has no impact on the state of the system of our interest. Disregarding the difference
</p>
<p>between states B and C, one may say that a spontaneous process proceeds in such a
</p>
<p>direction as to deliver positive work to the reversible work source.11</p>
<p/>
</div>
<div class="page"><p/>
<p>92 2 Thermodynamics
</p>
<p>2.13.5 Fundamental Equation
</p>
<p>We note that when F defined by (2.172) is expressed in terms of T , V , and N1, . . . ,
</p>
<p>Nc, the resulting function
</p>
<p>F = F(T,V,N1, . . . ,Nc) (2.187)
</p>
<p>is the Legendre transform of the internal energy U expressed in terms of S, V , and
</p>
<p>N1, . . . , Nc. Since the latter is a fundamental equation of the system, (2.187) also
</p>
<p>qualifies as a fundamental equation. (When the transformation is used in thermody-
</p>
<p>namics, the condition that d2y/dx2 be of a definite sign in the notation of Sect. C.1
is related to the stability of the system. See Exercises 2.3a and 2.12.)
</p>
<p>To arrive at the differential form of the fundamental equation, consider infinites-
</p>
<p>imal changes in T , V , and N1, . . . , Nc, which induce the corresponding changes in
</p>
<p>the quantities occurring in (2.172):
</p>
<p>F +∆F =U +∆U &minus; (T +dT )(S+∆S) . (2.188)
</p>
<p>Using (2.172), we cancel F on the left-hand side and U &minus;T S on the right to obtain
</p>
<p>∆F = ∆U &minus;T∆S&minus;SdT &minus;dT∆S . (2.189)
</p>
<p>Retaining the first-order terms only,
</p>
<p>dF = dU &minus;T dS&minus;SdT . (2.190)
</p>
<p>Now we use (2.37) and arrive at
</p>
<p>dF =&minus;SdT &minus;PdV +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;idNi , (2.191)
</p>
<p>which is the differential form of the fundamental equation we seek.
</p>
<p>From (2.187), we have
</p>
<p>dF =
</p>
<p>(
&part;F
</p>
<p>&part;T
</p>
<p>)
</p>
<p>V,N
</p>
<p>dT +
</p>
<p>(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>dV +
c
</p>
<p>&sum;
i=1
</p>
<p>(
&part;F
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,V,N j 	=i
</p>
<p>dNi . (2.192)
</p>
<p>Since the last two equations hold for any values of dT , dV , and dN1, . . . , dNc, we
</p>
<p>find
</p>
<p>&minus;S =
(
&part;F
</p>
<p>&part;T
</p>
<p>)
</p>
<p>V,N
</p>
<p>, &minus;P =
(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>,
</p>
<p>and &micro;i =
</p>
<p>(
&part;F
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,V,N j 	=i
</p>
<p>, i = 1, . . . ,c , (2.193)
</p>
<p>which are to be contrasted with (2.38).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.13 Free Energies 93
</p>
<p>Exercise 2.15. Show that
(
&part;F/T
</p>
<p>&part;T
</p>
<p>)
</p>
<p>V,N
</p>
<p>=&minus; U
T 2
</p>
<p>. (2.194)
</p>
<p>This identity is known as the Gibbs&ndash;Helmholtz equation. Following essentially the
</p>
<p>same approach, you can also show that
</p>
<p>(
&part;G/T
</p>
<p>&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>=&minus; H
T 2
</p>
<p>, (2.195)
</p>
<p>where G and H are, respectively, the Gibbs free energy and the enthalpy to be intro-
</p>
<p>duced in Sect. 2.13.6. This is also called the Gibbs&ndash;Helmholtz equation. ///
</p>
<p>2.13.6 Other Free Energies
</p>
<p>The relevant free energy when discussing equilibrium of a closed system held at
</p>
<p>constant temperature and pressure is the Gibbs free energy defined by
</p>
<p>G :=U &minus;T S+PV . (2.196)
</p>
<p>To see why, attempt the following exercise.
</p>
<p>Exercise 2.16. Formulate the condition of equilibrium for a closed system held at
</p>
<p>constant temperature and pressure. Then, using (2.196), obtain the equations corre-
</p>
<p>sponding to (2.176), (2.191), and (2.193). ///
</p>
<p>Using (2.148) in (2.196), we find that
</p>
<p>G =
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iNi . (2.197)
</p>
<p>One of the major goals in mixture thermodynamics is to predict &micro;i as a function of
T , P, and mole fractions x1, . . . , xc&minus;1, where
</p>
<p>xi := Ni
</p>
<p>/
c
</p>
<p>&sum;
i=1
</p>
<p>Ni . (2.198)
</p>
<p>Since
</p>
<p>xc = 1&minus;
c&minus;1
&sum;
i=1
</p>
<p>xi , (2.199)</p>
<p/>
</div>
<div class="page"><p/>
<p>94 2 Thermodynamics
</p>
<p>we do not include xc in the list of independent variables. If functions
</p>
<p>&micro;i = &micro;i(T,P,x1, . . . ,xc&minus;1) , i = 1, . . . ,c (2.200)
</p>
<p>can be constructed for each species in a mixture, then G will be known as a function
</p>
<p>of T , P, and N1, . . . , Nc. But, this function is a fundamental equation of the mixture.
</p>
<p>From this point of view, various activity coefficient models, which are widely used
</p>
<p>for correlating phase equilibria data involving liquid phases, represent an effort to
</p>
<p>construct fundamental equations of mixtures. The following exercise explores the
</p>
<p>simplest example of such model mixtures.
</p>
<p>Exercise 2.17. A mixture in which &micro;i of each species is given by
</p>
<p>&micro;i(T,P,x1, . . . ,xc&minus;1) = &micro;
◦
i (T )+RT (lnP+ lnxi) , i = 1, . . . ,c (2.201)
</p>
<p>is called an ideal gas mixture. Suppose that a composite system made of two com-
</p>
<p>partments, each containing the gas well described by (2.201), is held at some T and
</p>
<p>P by a heat bath and movable pistons.
</p>
<p>a. Initially, compartment A contained 3 mol of species 1 and 1 mol of species 2,
</p>
<p>while compartment B contained no species 1 and 2 mol of species 2. Find the
</p>
<p>number of moles of species 1 in compartment B after they are brought into con-
</p>
<p>tact through the diathermal, rigid, and immovable partition permeable to the first
</p>
<p>but not to the second species and a new state of equilibrium is established.
</p>
<p>b. Do the same when compartment A initially contained 3 mol of species 1 but no
</p>
<p>species 2. The initial numbers of moles in compartment B are the same as in
</p>
<p>part a.
</p>
<p>c. In each case considered above, plot G/RT against the number of moles of species
1 in compartment B. ///
</p>
<p>There are other commonly used free energies. For example, when
</p>
<p>Ω :=U &minus;T S&minus;
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iNi (2.202)
</p>
<p>is expressed as a function of T , V , and &micro;1, . . . , &micro;c, it is another fundamental equation.
This is commonly known as the grand potential. The grand potential is particularly
</p>
<p>useful in describing open systems. If a system is open to species 1 but not to the
</p>
<p>others, the appropriate choice of the free energy is
</p>
<p>χ :=U &minus;T S&minus;&micro;1N1 (2.203)
</p>
<p>to be expressed as a function of T , V , &micro;1, and N2, . . . , Nc. A free energy of this type
is often called a semi-grand potential. Finally, enthalpy, defined by
</p>
<p>H :=U +PV , (2.204)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.13 Free Energies 95
</p>
<p>is also a fundamental equation when it is expressed as a function of S, P, and N1, . . . ,
</p>
<p>Nc. Thus, enthalpy is a free energy characterizing a system held at constant entropy,
</p>
<p>pressure, and the number of moles of each species.
</p>
<p>Exercise 2.18. Show that
(
&part;U
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>S,V,N j 	=i
</p>
<p>=
</p>
<p>(
&part;F
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,V,N j 	=i
</p>
<p>=
</p>
<p>(
&part;G
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,P,N j 	=i
</p>
<p>=
</p>
<p>(
&part;H
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>S,P,N j 	=i
</p>
<p>. (2.205)
</p>
<p>///
</p>
<p>Exercise 2.19. Use (2.202) as a starting point to derive (2.157). ///
</p>
<p>The existence of a multitude of free energies prompts the following question:
</p>
<p>For a system consisting of c components, how many fundamental equations can we
</p>
<p>come up with? Let us start from the internal energy expressed in terms of S, V , and
</p>
<p>N1, . . . , Nc. A new free energy is constructed by replacing some of these variables
</p>
<p>by the corresponding intensive quantities, for example, S by T , V by &minus;P, or, &micro;i by
Ni. (See Sect. 2.15 for an explicit demonstration that &micro;i is an intensive quantity.) For
each of the extensive variables, we may either choose to keep it or replace it. The
</p>
<p>choice we make for a given variable is independent of those we make for the others.
</p>
<p>Apparently, therefore, there are 2c+2 distinct free energies.
</p>
<p>Of these, the choice of replacing every extensive quantity by the corresponding
</p>
<p>intensive variable does not work. The first sign of trouble is that the quantity defined
</p>
<p>by
</p>
<p>ϒ :=U &minus;T S+PV &minus;
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iNi (2.206)
</p>
<p>is, according to the Euler relation, identically zero. There are more convincing rea-
</p>
<p>sons why ϒ does not qualify as a free energy. Firstly, when all the extensive quan-
tities are replaced by the corresponding intensive quantities, information regarding
</p>
<p>the size of the system is permanently lost. Secondly, T , P, &micro;1, . . . , &micro;c, which are to
be used as the independent variables, are not independent of each other as seen from
</p>
<p>the Gibbs&ndash;Duhem relation.
</p>
<p>In this way, we see that the total of 2c+2 &minus; 1 fundamental equations can be
constructed. If we include those functions we obtain by starting from the entropy
</p>
<p>expressed as a function of U , V , and N1, . . . , Nc, we will find the same number of
</p>
<p>fundamental equations, known as Massieu functions, examples of which include
</p>
<p>&minus;F/T , &minus;G/T , and &minus;Ω/T . In total, therefore, there are 2(2c+2 &minus; 1) fundamental
equations all pertaining to the same system. Of course, only a few of them, such as
</p>
<p>those we have mentioned already, will be of any use in practice.
</p>
<p>Exercise 2.20. You might still feel a slight discomfort with the above discussion of
</p>
<p>ϒ . After all, we did follow the well-defined procedure of the Legendre transforma-
tion, did we not? Why should it ever fail? To put your mind at ease, attempt to find
</p>
<p>the Legendre transformation
</p>
<p>ϒ :=Ω +PV (2.207)</p>
<p/>
</div>
<div class="page"><p/>
<p>96 2 Thermodynamics
</p>
<p>of the grand potential
</p>
<p>Ω =Ω(T,V,&micro;1, . . . ,&micro;c) . (2.208)
</p>
<p>Exactly where does the procedure break down? ///
</p>
<p>2.14 &Dagger;Maxwell Relation
</p>
<p>We recall the following result from calculus. Let f = f (x,y) be a scalar-valued
function of two variables x and y. If both
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>(
&part; f
</p>
<p>&part;x
</p>
<p>)
and
</p>
<p>&part;
</p>
<p>&part;x
</p>
<p>(
&part; f
</p>
<p>&part;y
</p>
<p>)
(2.209)
</p>
<p>are continuous in some domain, then
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>(
&part; f
</p>
<p>&part;x
</p>
<p>)
=
</p>
<p>&part;
</p>
<p>&part;x
</p>
<p>(
&part; f
</p>
<p>&part;y
</p>
<p>)
(2.210)
</p>
<p>in that same domain. For example, let f = x2y+ x lny. Then,
</p>
<p>&part; f
</p>
<p>&part;x
= 2xy+ lny and
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>(
&part; f
</p>
<p>&part;x
</p>
<p>)
= 2x+
</p>
<p>1
</p>
<p>y
, (2.211)
</p>
<p>while
&part; f
</p>
<p>&part;y
= x2 +
</p>
<p>x
</p>
<p>y
and
</p>
<p>&part;
</p>
<p>&part;x
</p>
<p>(
&part; f
</p>
<p>&part;y
</p>
<p>)
= 2x+
</p>
<p>1
</p>
<p>y
. (2.212)
</p>
<p>As advertised, we ended up with the same function regardless of the order of taking
</p>
<p>the derivatives.
</p>
<p>By applying this result from calculus to various free energies, we can obtain
</p>
<p>numerous Maxwell relations. Each Maxwell relation predicts an equality between a
</p>
<p>pair of seemingly unrelated partial derivatives, reflecting completely different phys-
</p>
<p>ical situations. From the first two of (2.193), for example, we see that
</p>
<p>&minus;
(
&part;S
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>=
&part; 2F
</p>
<p>&part;T&part;V
=&minus;
</p>
<p>(
&part;P
</p>
<p>&part;T
</p>
<p>)
</p>
<p>V,N
</p>
<p>, (2.213)
</p>
<p>and hence (
&part;S
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>=
</p>
<p>(
&part;P
</p>
<p>&part;T
</p>
<p>)
</p>
<p>V,N
</p>
<p>. (2.214)
</p>
<p>Since T dS= d̄Q for a reversible process, the left-hand side of this equation is related
to the heat absorbed by the system upon expansion at constant T and N1, . . . , Nc.
</p>
<p>On the other hand, the right-hand side is related to the increase in P upon heating
</p>
<p>at constant V and N1, . . . , Nc. It will be difficult to anticipate that these quantities
</p>
<p>pertaining to these distinct physical processes are related in this simple manner. Yet,</p>
<p/>
</div>
<div class="page"><p/>
<p>2.15 &Dagger;Partial Molar Quantities 97
</p>
<p>we have reached this conclusion by admitting only the existing of a fundamental
</p>
<p>equation and some basic mathematical properties about it. In particular, we made
</p>
<p>no reference to any specific material from which the system is made. Here lies the
</p>
<p>power of thermodynamics. Equation (2.214) holds for any homogeneous body in
</p>
<p>equilibrium.
</p>
<p>Exercise 2.21. Derive the following Maxwell relations:
</p>
<p>a.
</p>
<p>&minus;
(
</p>
<p>&part;S
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,V,N j 	=i
</p>
<p>=
</p>
<p>(
&part;&micro;i
&part;T
</p>
<p>)
</p>
<p>V,N
</p>
<p>. (2.215)
</p>
<p>b.
</p>
<p>&minus;
(
</p>
<p>&part;P
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,V,N j 	=i
</p>
<p>=
</p>
<p>(
&part;&micro;i
&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>. (2.216)
</p>
<p>c.
</p>
<p>&minus;
(
&part;S
</p>
<p>&part;P
</p>
<p>)
</p>
<p>T,N
</p>
<p>=
</p>
<p>(
&part;V
</p>
<p>&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>. (2.217)
</p>
<p>d.
</p>
<p>&minus;
(
</p>
<p>&part;S
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,P,N j 	=i
</p>
<p>=
</p>
<p>(
&part;&micro;i
&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>. (2.218)
</p>
<p>e. (
&part;V
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,P,N j 	=i
</p>
<p>=
</p>
<p>(
&part;&micro;i
&part;P
</p>
<p>)
</p>
<p>T,N
</p>
<p>. (2.219)
</p>
<p>///
</p>
<p>Exercise 2.22. For a c component system, how many Maxwell relations are there?
</p>
<p>///
</p>
<p>Exercise 2.23. Using the ideal gas equation of state PV = NRT , show that
</p>
<p>(
&part;U
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>= 0 , (2.220)
</p>
<p>that is U of an ideal gas is independent of its volume. ///
</p>
<p>2.15 &Dagger;Partial Molar Quantities
</p>
<p>Let θ = θ(T,P,N1, . . . ,Nc) be an extensive quantity. That is
</p>
<p>θ(T,P,λN1, . . . ,λNc) = λθ(T,P,N1, . . . ,Nc) , (2.221)</p>
<p/>
</div>
<div class="page"><p/>
<p>98 2 Thermodynamics
</p>
<p>where λ is a positive number. The partial molar quantity θ i is defined by
</p>
<p>θ i(T,P,N1, . . . ,Nc) :=
</p>
<p>(
&part;θ
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,P,N j 	=i
</p>
<p>. (2.222)
</p>
<p>One particularly important example of θ i is the partial molar Gibbs free energy:
</p>
<p>Gi(T,P,N1, . . . ,Nc) =
</p>
<p>(
&part;G
</p>
<p>&part;Ni
</p>
<p>)
</p>
<p>T,P,N j 	=i
</p>
<p>, (2.223)
</p>
<p>which is nothing but the chemical potential of species i. As other examples of partial
</p>
<p>molar quantities, we rewrite the last two equations in Exercise 2.21 as
</p>
<p>Si =&minus;
(
&part;&micro;i
&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>and V i =
</p>
<p>(
&part;&micro;i
&part;P
</p>
<p>)
</p>
<p>T,N
</p>
<p>, (2.224)
</p>
<p>respectively.
</p>
<p>Partial molar quantities are intensive. To see why this might be so, we first recall
</p>
<p>the Gibbs&ndash;Duhem relation, (2.155), which reduces to
</p>
<p>SdT &minus;VdP+Nd&micro; = 0 (2.225)
</p>
<p>for a single component system. Upon division by N, we find
</p>
<p>d&micro; =&minus;SdT +V dP , (2.226)
</p>
<p>where S := S/N and V := V/N. For any changes in T and P, the corresponding
change in &micro; is given by (2.226). Since neither S nor V depends on the size of the
system, it follows that the chemical potential of a single component system is inde-
</p>
<p>pendent of the size of the system. Thus, it is an intensive quantity.
</p>
<p>It seems natural to expect that the same conclusion hold when we move to a mul-
</p>
<p>ticomponent system. But, because chemical potentials are just a particular example
</p>
<p>of partial molar quantities, we are lead to expect that the partial molar quantities are,
</p>
<p>in general, intensive quantities.
</p>
<p>Now we verify this expectation. We start by noting that (2.221) holds for any
</p>
<p>positive value of Ni. Thus, when we take the partial derivative of (2.221) with respect
</p>
<p>to Ni, the equality still holds.
12 Thus,
</p>
<p>&part;θ(T,P,M1, . . . ,Mc)
</p>
<p>&part;Mi
</p>
<p>&part;Mi
&part;Ni
</p>
<p>= λ
&part;θ(T,P,N1, . . . ,Nc)
</p>
<p>&part;Ni
, (2.227)
</p>
<p>where Mi := λNi, and hence &part;Mi/&part;Ni = λ . From (2.222), the partial derivative on
the left-hand side of (2.227) is the partial molar quantity θ i of the system containing
Mi moles of species i. Thus, we have
</p>
<p>θ i(T,P,M1, . . . ,Mc) = θ i(T,P,N1, . . . ,Nc) . (2.228)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.15 &Dagger;Partial Molar Quantities 99
</p>
<p>Since λ is arbitrary, this equation holds even if we set λ = 1/&sum;i Ni:
</p>
<p>θ i(T,P,x1, . . . ,xc&minus;1) = θ i(T,P,N1, . . . ,Nc) , (2.229)
</p>
<p>where xi = Ni/&sum;i Ni is the mole fraction of the ith species. Note that only c&minus; 1 of
the mole fractions are independent because &sum;i xi &equiv; 1. According to (2.229), θ i is an
intensive quantity.
</p>
<p>Next, we consider the partial derivative of (2.221) with respect to λ :
</p>
<p>c
</p>
<p>&sum;
i=1
</p>
<p>&part;θ(T,P,M1, . . . ,Mc)
</p>
<p>&part;Mi
</p>
<p>&part;Mi
&part;λ
</p>
<p>= θ(T,P,N1, . . . ,Nc) . (2.230)
</p>
<p>Since &part;Mi/&part;λ = Ni, we find
</p>
<p>c
</p>
<p>&sum;
i=1
</p>
<p>θ i(T,P,x1, . . . ,xc&minus;1)Ni = θ(T,P,N1, . . . ,Nc) . (2.231)
</p>
<p>Essentially, the identical approach can be used to derive (2.148). In view of this,
</p>
<p>(2.231) is often referred to as the generalized Euler relation .
</p>
<p>If we set θ = G, (2.231) becomes
</p>
<p>G(T,P,N1, . . . ,Nc) =
c
</p>
<p>&sum;
i=1
</p>
<p>Gi(T,P,x1, . . . ,xc&minus;1)Ni , (2.232)
</p>
<p>which is just (2.197) since Gi = &micro;i. To get something new, let θ =V . Then,
</p>
<p>V (T,P,N1, . . . ,Nc) =
c
</p>
<p>&sum;
i=1
</p>
<p>V i(T,P,x1, . . . ,xc&minus;1)Ni , (2.233)
</p>
<p>or dividing by &sum;i Ni,
</p>
<p>V (T,P,N1, . . . ,Nc) =
c
</p>
<p>&sum;
i=1
</p>
<p>V i(T,P,x1, . . . ,xc&minus;1)xi . (2.234)
</p>
<p>Equations like this occur very often in mixture thermodynamics.
</p>
<p>Just like the Gibbs&ndash;Duhem relation followed from the Euler relation in Sect. 2.11,
</p>
<p>we can derive the generalized Gibbs&ndash;Duhem relation from the generalized Euler
</p>
<p>relation using the identical approach. The only difference is in the symbols we use.
</p>
<p>As in Sect. 2.11, suppose that the state of the system has changed as a result of
</p>
<p>infinitesimal changes of T , P, and N1, . . . , Nc. Both θ and θ 1, . . . , θ c will change as
a result. Applying (2.231) to the state after the change, we may write
</p>
<p>θ +∆θ =
c
</p>
<p>&sum;
i=1
</p>
<p>(θ i +∆θ i)(Ni +dNi) . (2.235)</p>
<p/>
</div>
<div class="page"><p/>
<p>100 2 Thermodynamics
</p>
<p>From this, we subtract (2.231) applied to the state prior to the infinitesimal change
</p>
<p>and obtain
</p>
<p>∆θ =
c
</p>
<p>&sum;
i=1
</p>
<p>θ idNi +
c
</p>
<p>&sum;
i=1
</p>
<p>Ni∆θ i +
c
</p>
<p>&sum;
i=1
</p>
<p>∆θ idNi . (2.236)
</p>
<p>Retaining the first-order terms only,
</p>
<p>dθ =
c
</p>
<p>&sum;
i=1
</p>
<p>θ idNi +
c
</p>
<p>&sum;
i=1
</p>
<p>Nidθ i . (2.237)
</p>
<p>But, since θ is a function of T , P, and N1, . . . , Nc,
</p>
<p>dθ = θT dT +θPdP+
c
</p>
<p>&sum;
i=1
</p>
<p>θ idNi , (2.238)
</p>
<p>where
</p>
<p>θT :=
</p>
<p>(
&part;θ
</p>
<p>&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>and θP :=
</p>
<p>(
&part;θ
</p>
<p>&part;P
</p>
<p>)
</p>
<p>T,N
</p>
<p>. (2.239)
</p>
<p>Combining (2.237) and (2.238), we finally arrive at
</p>
<p>0 =&minus;θT dT &minus;θPdP+
c
</p>
<p>&sum;
i=1
</p>
<p>Nidθ i, (2.240)
</p>
<p>which is called the generalized Gibbs&ndash;Duhem relation.
</p>
<p>Once again, let θ = G. Then, (2.240) becomes
</p>
<p>0 =&minus;GT dT &minus;GPdP+
c
</p>
<p>&sum;
i=1
</p>
<p>Nid&micro;i . (2.241)
</p>
<p>But, since
</p>
<p>GT :=
</p>
<p>(
&part;G
</p>
<p>&part;T
</p>
<p>)
</p>
<p>P,N
</p>
<p>=&minus;S and GP :=
(
&part;G
</p>
<p>&part;P
</p>
<p>)
</p>
<p>T,N
</p>
<p>=V , (2.242)
</p>
<p>(2.241) is nothing but the usual Gibbs&ndash;Duhem relation.
</p>
<p>Exercise 2.24. Establish (2.74). ///
</p>
<p>2.16 Graphical Methods
</p>
<p>In this section, we discuss a few graphical methods you will encounter in later chap-
</p>
<p>ters. From a computational point of view, the accuracy expected of such methods is
</p>
<p>not very high. Yet, they do have intuitive appeal and thus help us understand certain
</p>
<p>key concepts in thermodynamics. On occasion, they can also help diagnose difficul-
</p>
<p>ties that may arise in practical computations.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.16 Graphical Methods 101
</p>
<p>2.16.1 Pure Systems: F Versus V (Constant T )
</p>
<p>Recall the definition of the Helmholtz free energy in (2.172). Using (2.148), we find
</p>
<p>F :=U &minus;T S =&minus;PV +&micro;N . (2.243)
</p>
<p>for a single component system, where N is the number of moles in the system.
</p>
<p>Dividing both sides by N, we obtain
</p>
<p>F :=
F
</p>
<p>N
=&minus;PV +&micro; , (2.244)
</p>
<p>where V :=V/N. We now show that
</p>
<p>&minus;P =
(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T
</p>
<p>. (2.245)
</p>
<p>To see this, we apply (2.191) to a single component system and restricting our atten-
</p>
<p>tion to processes in which T and N are held constant:
</p>
<p>dF =&minus;PdV . (2.246)
</p>
<p>Dividing both sides by N,
</p>
<p>dF =&minus;PdV , (2.247)
where we note that (dV )/N = d(V/N) since N is held fixed. Likewise for (dF)/N.
Dividing both sides of (2.247) by dV and recalling that the equation holds only for
</p>
<p>constant T and N processes, we arrive at
</p>
<p>&minus;P =
(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>=
</p>
<p>(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T
</p>
<p>. (2.248)
</p>
<p>In the last step, we used the fact that F is a molar quantity and hence is independent
</p>
<p>of N. Alternatively, we could proceed more mechanically as follows:
</p>
<p>&minus;P =
(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>=
</p>
<p>(
&part;F/N
</p>
<p>&part;V/N
</p>
<p>)
</p>
<p>T,N
</p>
<p>=
</p>
<p>(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>=
</p>
<p>(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T
</p>
<p>. (2.249)
</p>
<p>Equations (2.244) and (2.245) lead to the graphical construction illustrated in
</p>
<p>Fig. 2.15. Specifically, suppose that the isotherm is produced at some temperature
</p>
<p>T of interest. If we draw the tangent line at some particular value of V , say V &lowast;, the
negative of the slope is the pressure P&lowast; at (T,V &lowast;). The intercept of the tangent line
on the F-axis (on which V = 0) is computed as
</p>
<p>F&lowast;&minus;
(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T
</p>
<p>∣∣∣∣
V=V &lowast;
</p>
<p>V &lowast; = F&lowast;+P&lowast;V &lowast; = &micro;&lowast; , (2.250)
</p>
<p>where we used (2.244). That is, the intercept is the chemical potential at (T,V &lowast;).</p>
<p/>
</div>
<div class="page"><p/>
<p>102 2 Thermodynamics
</p>
<p>Fig. 2.15 Graphical construction involving an F versus V isotherm.
</p>
<p>2.16.1.1 Gibbs&ndash;Duhem Relation at Constant T : Interpretation
</p>
<p>Now, let us imagine what happens to the tangent line in Fig. 2.15 if we move V &lowast;
</p>
<p>to the right. We see that, as the tangent line rolls on the curve, the intercept on the
</p>
<p>F-axis (&micro;&lowast;) decreases while the slope (&minus;P&lowast;) increases. In other words, at a given
temperature, a decrease in &micro; is accompanied by a decrease in P. This is actually a
consequence of the Gibbs&ndash;Duhem relation (2.155), which reduces to
</p>
<p>d&micro; =V dP , T const. (2.251)
</p>
<p>for constant T processes in a pure system. In this way, the Gibbs&ndash;Duhem relation
</p>
<p>acquires a graphical interpretation.
</p>
<p>2.16.1.2 &dagger;Gibbs&ndash;Duhem Relation at Constant T : Derivation
</p>
<p>It is somewhat amusing to turn the above observation around and derive (2.251)
</p>
<p>graphically. Let us draw two tangent lines to the F-V isotherm, one at V &lowast; and the
other at V = V &lowast; + dV as shown in Fig. 2.16. The negative of the slope and the
intercept on the F-axis of the second tangent line are the pressure and the chemical
</p>
<p>potential at (T,V &lowast;+dV ), which we denote by P&lowast;+∆P and &micro;&lowast;+∆&micro; , respectively.
If we shift this second tangent line vertically upward till it passes through the
</p>
<p>point of tangent (A1) of the first tangent line at V =V
&lowast;, the intercept of the resulting
</p>
<p>line is
</p>
<p>&micro;&lowast;+∆ &prime;&micro; = F&lowast;+(P&lowast;+∆P)V &lowast; . (2.252)
</p>
<p>Using (2.250), we find that
</p>
<p>∆ &prime;&micro; =V &lowast;∆P . (2.253)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.16 Graphical Methods 103
</p>
<p>Fig. 2.16 Graphical derivation of the Gibbs&ndash;Duhem relation for a constant T process in a pure
</p>
<p>system.
</p>
<p>We now show that the difference between ∆ &prime;&micro; and ∆&micro; is zero to the first order of
dV . In fact,
</p>
<p>∆ &prime;&micro;&minus;∆&micro; = (&micro;&lowast;+∆ &prime;&micro;)&minus; (&micro;&lowast;+∆&micro;) = A1A2 &prop; A3A4 , (2.254)
</p>
<p>where A1A2 is the length of the line segment connecting points A1 and A2. Likewise
</p>
<p>for A3A4. But,
</p>
<p>A3A4 = F(V
&lowast;+dV )&minus;
</p>
<p>[
F(V &lowast;)+
</p>
<p>(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T
</p>
<p>∣∣∣∣
V=V &lowast;
</p>
<p>dV
</p>
<p>]
= h.o. (2.255)
</p>
<p>Thus, retaining up to the first-order term in (2.253), we have
</p>
<p>d&micro; =V &lowast;dP (2.256)
</p>
<p>for a constant T process. Since the equality holds for any V &lowast;, (2.251) is now estab-
lished.
</p>
<p>2.16.2 Binary Mixtures: G Versus x1 (Constant T and P)
</p>
<p>From (2.148) and (2.196), we have
</p>
<p>G :=U &minus;T S+PV = &micro;1N1 +&micro;2N2 (2.257)
</p>
<p>for a binary mixture. Dividing both sides by N := N1 +N2, we obtain
</p>
<p>G = &micro;1x1 +&micro;2x2 , (2.258)</p>
<p/>
</div>
<div class="page"><p/>
<p>104 2 Thermodynamics
</p>
<p>Fig. 2.17 Graphical construction involving a G versus x1 plot at constant (T,P).
</p>
<p>where G := G/N and xi := Ni/N. To see the physical meaning of the slope of the G
versus x1 plot, we recall from Exercise 2.16 that
</p>
<p>dG =&minus;SdT +V dP+&micro;1dN1 +&micro;2dN2 (2.259)
</p>
<p>and consider constant T and P processes in which N :=N1+N2 is also held constant.
This somewhat unusual constraint does not necessarily imply any chemical reaction
</p>
<p>between species 1 and 2. It simply states that, whenever species 1 enters the system,
</p>
<p>the equal number of moles of species 2 needs to be extracted from it. For such
</p>
<p>processes,
</p>
<p>dG = (&micro;1 &minus;&micro;2)dN1 , T,P,N const. (2.260)
Dividing both sides by N,
</p>
<p>dG = (&micro;1 &minus;&micro;2)dx1 , T,P const. (2.261)
</p>
<p>where G :=G/N and x1 :=N1/N. We also dropped the reference to N being constant
since G is a molar quantity and hence, in this case, is a function of T , P, and x1 only.
</p>
<p>Dividing (2.261) by dx1, we find
</p>
<p>(
&part;G
</p>
<p>&part;x1
</p>
<p>)
</p>
<p>T,P
</p>
<p>= &micro;1 &minus;&micro;2 . (2.262)
</p>
<p>This motivates the graphical method illustrated in Fig. 2.17. We draw a tangent
</p>
<p>line to the G versus x1 curve for given (T,P) at the mole fraction x1 = x
&lowast;
1. Its intercept
</p>
<p>on the G-axis (at x1 = 0) is &micro;2 of the mixture at (T,P,x&lowast;1), which we denote by &micro;
&lowast;
2 .
</p>
<p>In fact,
</p>
<p>G&lowast;&minus;
(
&part;G
</p>
<p>&part;x1
</p>
<p>)
</p>
<p>T,P
</p>
<p>∣∣∣∣∣
x1=x
</p>
<p>&lowast;
1
</p>
<p>x&lowast;1 = G
&lowast;&minus; (&micro;&lowast;1 &minus;&micro;&lowast;2 )x&lowast;1 = &micro;&lowast;2 , (2.263)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.16 Graphical Methods 105
</p>
<p>Fig. 2.18 G versus x1 plot at constant (T,P) showing two inflection points indicated by the open
circles.
</p>
<p>where we used (2.258) and the identity x1 + x2 &equiv; 1. Similarly, the intercept on the
vertical line at x1 = 1 is &micro;1 of the same mixture:
</p>
<p>G&lowast;+
</p>
<p>(
&part;G
</p>
<p>&part;x1
</p>
<p>)
</p>
<p>T,P
</p>
<p>∣∣∣∣∣
x1=x
</p>
<p>&lowast;
1
</p>
<p>(1&minus; x&lowast;1) = G&lowast;+(&micro;&lowast;1 &minus;&micro;&lowast;2 )x&lowast;2 = &micro;&lowast;1 . (2.264)
</p>
<p>2.16.2.1 Gibbs&ndash;Duhem Relation at Constant T and P: Interpretation
</p>
<p>As in Sect. 2.16.1.1, the above graphical method provides a graphical interpreta-
</p>
<p>tion of the Gibbs&ndash;Duhem relation applied to constant (T,P) processes in a binary
mixture:
</p>
<p>x1d&micro;1 + x2d&micro;2 = 0 , T,P const. (2.265)
</p>
<p>In fact, imagine how the intercepts at x1 = 0 and x1 = 1 move as the tangent line
rolls on the G versus x1 plot with increasing x1. From the figure, it is clear that &micro;1
increases while &micro;2 decreases. But, that is exactly what (2.265) indicates.
</p>
<p>The G versus x1 plot may contain inflection points as shown in Fig. 2.18. As
</p>
<p>x1 passes through the inflection point at x
a
1 in the figure, &micro;1 goes through a local
</p>
<p>maximum, while &micro;2 goes through a local minimum. Beyond this inflection point,
the curve will be concave down, and &micro;1 now decreases while &micro;2 increases. In this
way, we see that d&micro;1 and d&micro;2 either have opposite signs or they are simultaneously
zero. Again, this behavior is consistent with (2.265).
</p>
<p>As we shall see in Sect. 2.16.2.3, a binary mixture is unstable with respect to
</p>
<p>phase separation if its G versus x1 curve is concave down. Thus, x
a
1 marks the onset
</p>
<p>of instability. In Fig. 2.18, the curve develops another inflection point at xb1. For
</p>
<p>x1 &gt; x
b
1, the curve is concave up and the mixture regains its stability.</p>
<p/>
</div>
<div class="page"><p/>
<p>106 2 Thermodynamics
</p>
<p>Fig. 2.19 Graphical derivation of the Gibbs&ndash;Duhem relation for a constant (T,P) process in a
binary mixture.
</p>
<p>2.16.2.2 &dagger;Gibbs&ndash;Duhem Relation at Constant T and P: Derivation
</p>
<p>As in Sect. 2.16.1.2, let us derive (2.265) graphically. For this purpose, we draw
</p>
<p>another tangent line at x1 = x
&lowast;
1 +dx1 as illustrated in Fig. 2.19. Its intercepts on the
</p>
<p>G-axis is the chemical potential of species 2 at (T,P,x1 + dx
&lowast;
1), which we denote
</p>
<p>by &micro;&lowast;2 +∆&micro;2. The intercept on the vertical line x1 = 1 is the chemical potential of
species 1 at the same condition and is denoted by &micro;&lowast;1 +∆&micro;1.
</p>
<p>Now, we shift the tangent line at x1 = x
&lowast;
1 +dx1 vertically upward so that it passes
</p>
<p>through the point of tangent (A5 in Fig. 2.19) of the tangent line at x1 = x
&lowast;
1. We
</p>
<p>denote the intercepts of this shifted tangent line at x1 = 0 (A2) and x1 = 1 (A3) by
&micro;2(x&lowast;1)+∆
</p>
<p>&prime;&micro;2 and &micro;1(x&lowast;1)+∆
&prime;&micro;1, respectively. From Fig. 2.19, we note that the two
</p>
<p>triangles △A1A2A5 and △A4A3A5 are similar. Thus,
</p>
<p>A1A2
</p>
<p>A3A4
=
</p>
<p>&micro;&lowast;2 &minus; (&micro;&lowast;2 +∆ &prime;&micro;2)
(&micro;&lowast;1 +∆
</p>
<p>&prime;&micro;1)&minus;&micro;&lowast;1
=
</p>
<p>x&lowast;1
1&minus; x&lowast;1
</p>
<p>, (2.266)
</p>
<p>from which we find
</p>
<p>x&lowast;1∆
&prime;&micro;1 + x
</p>
<p>&lowast;
2∆
</p>
<p>&prime;&micro;2 = 0 . (2.267)
</p>
<p>From the figure, we observe that
</p>
<p>∆ &prime;&micro;i &minus;∆&micro;i = A5A6 &prop; A7A8 , i = 1,2 . (2.268)
</p>
<p>But,
</p>
<p>A7A8 = G(x
&lowast;
1 +dx1)&minus;
</p>
<p>⎡
⎣G(x&lowast;1)+
</p>
<p>(
&part;G
</p>
<p>&part;x1
</p>
<p>)
</p>
<p>T,P
</p>
<p>∣∣∣∣∣
x1=x
</p>
<p>&lowast;
1
</p>
<p>dx1
</p>
<p>⎤
⎦= h.o. , (2.269)
</p>
<p>and hence
</p>
<p>∆ &prime;&micro;i = ∆&micro;i +h.o. , i = 1,2 , (2.270)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.16 Graphical Methods 107
</p>
<p>Fig. 2.20 G versus x1 plot at constant T and P.
</p>
<p>which in turn allows us to rewrite (2.267) as
</p>
<p>x&lowast;1∆&micro;1 + x
&lowast;
2∆&micro;2 = h.o. (2.271)
</p>
<p>Retaining the first-order terms only, we obtain
</p>
<p>x&lowast;1d&micro;1 + x
&lowast;
2d&micro;2 = 0 . (2.272)
</p>
<p>Since this equality holds for any x&lowast;1 as long as T and P are fixed, we finally arrive at
(2.265).
</p>
<p>2.16.2.3 Phase Separation
</p>
<p>A binary mixture of a given mole fraction is unstable with respect to phase sepa-
</p>
<p>ration if G versus x1 plot is concave down at that mole fraction. For example, the
</p>
<p>molar Gibbs free energy of a binary mixture at x01 in Fig. 2.20 is G
0. By separating
</p>
<p>into phase A with mole fraction xa1 and phase B with x
b
1, the molar Gibbs free energy
</p>
<p>decreases to G f .
</p>
<p>As you saw in Exercise 2.16, a closed system held at constant T and P evolves
</p>
<p>toward the direction of decreasing G. So, the phase separation we just considered is
</p>
<p>a spontaneous process and the initial phase is unstable.
</p>
<p>As in Exercise 2.11, the graphical construction for G f can be justified as follows.
</p>
<p>First, the Gibbs free energy G f of the composite system consisting of phases A and
</p>
<p>B is given by
</p>
<p>G f = NG f = NaGa +NbGb , (2.273)
</p>
<p>where N is the total number of moles of molecules in the system and includes
</p>
<p>molecules of both species. We denote the total number of moles in phase A by
</p>
<p>Na and that in phase B by Nb. Clearly,
</p>
<p>N = Na +Nb . (2.274)</p>
<p/>
</div>
<div class="page"><p/>
<p>108 2 Thermodynamics
</p>
<p>Fig. 2.21 The common tangent construction to find the mole fractions, xc1 and x
d
1 , at phase coexis-
</p>
<p>tence. The mixture is unstable between two inflection points (open circles) at xa1 and x
b
1.
</p>
<p>Considering the total number of moles of species 1 in the entire system, we obtain
</p>
<p>x01N = x
a
1N
</p>
<p>a + xb1N
b . (2.275)
</p>
<p>From (2.273) and (2.274), we find
</p>
<p>G f =
Na
</p>
<p>N
Ga +
</p>
<p>Nb
</p>
<p>N
Gb = Ga +(Gb &minus;Ga)N
</p>
<p>b
</p>
<p>N
, (2.276)
</p>
<p>while (2.274) and (2.275) gives
</p>
<p>Nb
</p>
<p>N
=
</p>
<p>x01 &minus; xa1
xb1 &minus; xa1
</p>
<p>. (2.277)
</p>
<p>Thus,
</p>
<p>G f = Ga +
Gb &minus;Ga
xb1 &minus; xa1
</p>
<p>(x01 &minus; xa1) (2.278)
</p>
<p>in agreement with the graphical construction.
</p>
<p>Because of the way the diagram is drawn, G versus x1 plot is concave down at
</p>
<p>xa1 and x
b
1. So, both phases A and B are still unstable. If G actually is concave down
</p>
<p>everywhere between x1 = 0 and x1 = 1, then the system will split into two pure
phases. In reality, however, it is likely that the curve will eventually turn around and
</p>
<p>become concave up toward both ends of the x1-axis (at x1 = 0 and x1 = 1).
</p>
<p>2.16.2.4 Phase Coexistence
</p>
<p>If the mole fraction x01 of the system sits between two inflection points of the G
</p>
<p>versus x1 plot as shown in Fig. 2.21, the mixture will separate into two phases.
</p>
<p>The mole fractions of these coexisting phases, which we denote by xc1 and x
d
1 , can</p>
<p/>
</div>
<div class="page"><p/>
<p>2.17 Frequently Used Symbols 109
</p>
<p>be found by means of the common tangent construction illustrated in Fig. 2.21.
</p>
<p>According to this method, we draw a tangent line having two points of contact.
</p>
<p>The mole fractions of the coexisting phases are then given by those of the points of
</p>
<p>tangent.
</p>
<p>Why? Recall that the intercept at x1 = 0 of the tangent line gives you &micro;2, while
that at x1 = 1 gives you &micro;1. So, from the figure, we see that &micro;c1 = &micro;
</p>
<p>d
1 and &micro;
</p>
<p>c
2 = &micro;
</p>
<p>d
2 .
</p>
<p>Since the curve is for given T and P, clearly, T c = T d and Pc =Pd . So, the composite
system consisting of the two phases is in equilibrium. The equilibrium is stable
</p>
<p>because G is concave up at these mole fractions.
</p>
<p>As indicated by the downward arrow in the Fig. 2.21, G decreases upon phase
</p>
<p>separation. The length of this arrow is the driving force of phase separation. As in
</p>
<p>(2.277),
Nd
</p>
<p>N
=
</p>
<p>x01 &minus; xc1
xd1 &minus; xc1
</p>
<p>, (2.279)
</p>
<p>where N = Nc +Nd is the total number of moles in the system. Thus,
</p>
<p>Nc
</p>
<p>N
= 1&minus; Nd
</p>
<p>N
=
</p>
<p>xd1 &minus; x01
xd1 &minus; xc1
</p>
<p>. (2.280)
</p>
<p>Thus,
</p>
<p>Nc(x01 &minus; xc1) = Nd(xd1 &minus; x01) . (2.281)
This result is known as the lever rule. The amount of each phase, as measured by
</p>
<p>the total number of moles, satisfies the condition of the mechanical balance of a
</p>
<p>lever with fulcrum at x01 and loads N
c and Nd at xc1 and x
</p>
<p>d
1 , respectively.
</p>
<p>Between xc1 and x
a
1, the phase is not unstable, but its free energy can be lowered by
</p>
<p>undergoing phase separation (into xc1 and x
d
1). The phase is thus metastable. Likewise
</p>
<p>for a phase between xb1 and x
d
1 .
</p>
<p>2.17 Frequently Used Symbols
</p>
<p>c , the number of species.
</p>
<p>kB , Boltzmann constant, 1.3806&times;10&minus;23 J/K.
ni , Ni/V . We often drop the subscript for a pure system.
s , S/V .
u , U/V .
xi , mole fraction of species i.
</p>
<p>CP , constant pressure heat capacity.
</p>
<p>CP , constant pressure molar heat capacity.
</p>
<p>CV , constant volume heat capacity.
</p>
<p>CV , constant volume molar heat capacity.
</p>
<p>E , total energy.</p>
<p/>
</div>
<div class="page"><p/>
<p>110 2 Thermodynamics
</p>
<p>F , Helmholtz free energy.
</p>
<p>G , Gibbs free energy.
</p>
<p>H , enthalpy.
</p>
<p>Ni , the number of moles of species i. We often drop the subscript for a pure system.
</p>
<p>P , pressure.
</p>
<p>Q , heat.
</p>
<p>R , gas constant. 8.3145 J/mol K
</p>
<p>S , entropy.
</p>
<p>T , absolute temperature.
</p>
<p>U , internal energy.
</p>
<p>V , volume.
</p>
<p>W , work.
</p>
<p>W rev , work done by a reversible work source.
</p>
<p>Xi , ith additional variable needed to specify the state of a system not in
</p>
<p>equilibrium.
</p>
<p>α , coefficient of thermal expansion.
γ , CP/CV .
&micro;i , chemical potential of species i. We often drop the subscript for a pure system.
θ , extensive quantity.
θ , partial molar quantity defined by (&part;θ/&part;Ni)T,P,N j 	=i .
θ , molar quantity defined by θ/&sum;ci=1 Ni.
Ω , grand potential.
</p>
<p>References and Further Reading
</p>
<p>1. Callen H B (1985) Thermodynamics and an Introduction to Thermostatistics, 2nd edn. John
</p>
<p>Wiley &amp; Sons, New York
Our exposition of thermodynamics is motivated, in part, by the axiomatic approach to thermo-
</p>
<p>dynamics by Callen. For a more detailed discussion on thermodynamics, this is probably the
</p>
<p>place to start. Chapters 1-6 should be sufficient for the first reading.
2. Fermi E (1956) Thermodynamics. Dover, New York
</p>
<p>One thing that is missing from Callen as well as from the current chapter is the concept of
</p>
<p>entropy introduced through purely macroscopic considerations. Fermi&rsquo;s book fills this gap very
</p>
<p>nicely. Entropy is covered in the first 60 pages or so. His definition of an adiabatic process
</p>
<p>differs from ours in that he demands the process to be reversible as well.
3. Gibbs J W (1993) The Scientific Papers of J. Willard Gibbs, Volume I. Thermodynamics. Ox
</p>
<p>Bow, Connecticut
Contrary to some uninformed view, the original work by Gibbs still is among the most important
</p>
<p>resources on thermodynamics. This makes a notoriously difficult reading. However, the effort
</p>
<p>is more than justified by generality and rigor of the exposition.
4. Nishioka K (1987) An analysis of the Gibbs theory of infinitesimally discontinuous variation
</p>
<p>in thermodynamics of interface. Scripta Metallurgica 21:789&ndash;792
5. Whitaker S (1992) Introduction to Fluid Mechanics, Krieger Publishing Company, Florida
</p>
<p>Highly readable and thoughtfully written introduction to fluid mechanics.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>Classical Statistical Mechanics
</p>
<p>According to classical mechanics, equations of motion supplemented by initial con-
</p>
<p>ditions uniquely determine the subsequent evolution of a given system. For typical
</p>
<p>systems of our interest, however, the number of mechanical degrees of freedom is
</p>
<p>of the order of 1024. One cannot possibly write down 1024 equations of motion,
</p>
<p>much less solve them. It is also impossible to specify the initial conditions for such
</p>
<p>a system with a required accuracy. Moreover, even if we could somehow accomplish
</p>
<p>all of this, it would be entirely impossible to comprehend the resulting list of coor-
</p>
<p>dinates and momenta at any instant. Despite a hopeless scenario this observation
</p>
<p>might suggest, behavior of a macroscopic system is surprisingly regular as we have
</p>
<p>seen in thermodynamics. It is as if laws governing behavior of a macroscopic system
</p>
<p>are quite different from those governing its behavior at a microscopic level. In this
</p>
<p>chapter, we examine the connection between these two distinct ways of looking at a
</p>
<p>macroscopic system.
</p>
<p>3.1 Macroscopic Measurement
</p>
<p>Suppose that we measure the value of a dynamical variable A of some macroscopic
</p>
<p>body at time t. Is the outcome Aexpt(t) equal to the value of A at time t? That is, can
we write
</p>
<p>Aexpt(t) = A(q
f (t), p f (t), t) ? (3.1)
</p>
<p>For example, we might wish to measure the length of an object consisting of N
</p>
<p>particles (atoms) as in Fig. 3.1. One possible definition for the dynamical variable
</p>
<p>representing the desired length is
</p>
<p>l(rN(t),pN(t)) = max
i, j
</p>
<p>∣∣xi(t)&minus; x j(t)
∣∣ . (3.2)
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 111
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1 3</p>
<p/>
</div>
<div class="page"><p/>
<p>112 3 Classical Statistical Mechanics
</p>
<p>xi t x j t x
</p>
<p>y
</p>
<p>Fig. 3.1 Measurement of the length of a bar.
</p>
<p>That is, we define the instantaneous length of the object as the maximum difference
</p>
<p>in the x coordinates of the particles comprising the object. In analogy to the notation
</p>
<p>q f and p f we introduced in Chap. 1, rN and pN in (3.2) collectively denote the
</p>
<p>position vectors and the linear momenta of particles in the system, respectively.
</p>
<p>If we observe the time evolution of l, however, we will find that l fluctuates as
</p>
<p>a result of particles bouncing around in the object. The characteristic time scale
</p>
<p>for this kind of molecular motion is of the order of 10&minus;12 s. See Example 3.1, for
example. To measure the length, we can simply place a scale bar next to the object
</p>
<p>and read the scale with the naked eye. The characteristic time scale for such a mea-
</p>
<p>surement is of the order of 10&minus;2 s at best. Even if we take a picture to &ldquo;freeze the
motion,&rdquo; the time duration of the measurement will be no shorter than 10&minus;4 s or so.
</p>
<p>Thus, what we obtain as a result of this measurement is not the instantaneous
</p>
<p>value l(t) assumed by the dynamical variable, rather it is a time average of l(t). So,
a proper expression for lexpt(t) is
</p>
<p>lexpt(t) = lim
τ&rarr;&infin;
</p>
<p>1
</p>
<p>τ
</p>
<p>&int; t+τ
</p>
<p>t
l(rN(t &prime;),pN(t &prime;))dt &prime; , (3.3)
</p>
<p>where τ &rarr; &infin; simply means that τ , being comparable with the characteristic time
scale of a macroscopic measurement, is extremely large compared to the character-
</p>
<p>istic time scale of molecular motion.
</p>
<p>For a general dynamical variable A, therefore, we write
</p>
<p>Aexpt(t) = lim
τ&rarr;&infin;
</p>
<p>1
</p>
<p>τ
</p>
<p>&int; t+τ
</p>
<p>t
A(q f (t &prime;), p f (t &prime;), t &prime;)dt &prime; , (3.4)
</p>
<p>Taking an average consolidates a multitude of complexity embodied in the full spec-
</p>
<p>ification of q f (t) and p f (t). The apparent regularity we associate with the behavior
of macroscopic bodies is a result of this consolidation.
</p>
<p>It is instructive to consider a simple, if somewhat artificial, example. Let
</p>
<p>A(q f , p f , t) = A0 +&sum;
i=1
</p>
<p>Ai sin
2πt
</p>
<p>Ti
, (3.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Macroscopic Measurement 113
</p>
<p>where A0, Ai, and Ti are some constants. The second term on the right-hand side is
</p>
<p>a superposition of sine waves each with the period Ti and amplitude Ai. Because of
</p>
<p>this term, A will fluctuate with time in a complex manner. (For an extensive quantity
</p>
<p>pertaining to a macroscopic body, we typically have |Ai/A0| &sim; 1/
&radic;
</p>
<p>N for all i &ge; 1 as
discussed in Sect. 4.1.) Let us calculate the result of measuring this A. From (3.4),
</p>
<p>we obtain
</p>
<p>Aexpt(t) = lim
τ&rarr;&infin;
</p>
<p>1
</p>
<p>τ
</p>
<p>&int; t+τ
</p>
<p>t
</p>
<p>(
A0 +&sum;
</p>
<p>i=1
</p>
<p>Ai sin
2πt &prime;
</p>
<p>Ti
</p>
<p>)
dt &prime;
</p>
<p>= A0 &minus; lim
τ&rarr;&infin;
</p>
<p>1
</p>
<p>τ &sum;
i=1
</p>
<p>AiTi
</p>
<p>2π
</p>
<p>[
cos
</p>
<p>2πt &prime;
</p>
<p>Ti
</p>
<p>]t+τ
</p>
<p>t
</p>
<p>. (3.6)
</p>
<p>Note that the magnitude of cos(2πt &prime;/Ti) is at most 1. Thus, the fluctuating part of
A does not contribute to Aexpt unless Ti is of the order of τ , that is, only extremely
slow modes of fluctuation can survive the time averaging implicit in macroscopic
</p>
<p>measurements and more rapid fluctuations are &ldquo;hidden&rdquo; from macroscopic measure-
</p>
<p>ments.
</p>
<p>Even though these rapid fluctuations are hidden in the sense just indicated, their
</p>
<p>effect may be felt at a macroscopic level. Heat is a notable mechanism through
</p>
<p>which such hidden modes of fluctuation manifest themselves at a macroscopic level.
</p>
<p>In principle, (3.4) provides a prescription for predicting the value of Aexpt(t) for
the system of interest &ldquo;simply&rdquo; by solving the equations of motion. This is the
</p>
<p>basic idea behind the molecular dynamics simulation method, in which equations
</p>
<p>of motion are solved numerically for systems containing a large number of parti-
</p>
<p>cles interacting through effective potentials that are designed to mimic the actual
</p>
<p>molecular interactions. One can easily deal with systems containing 104 &sim; 105 or
even larger number of particles. In pursuing this approach, one must first identify
</p>
<p>appropriate dynamical variables corresponding to various macroscopic quantities
</p>
<p>of interest. For certain quantities, such as temperature, entropy, free energies, and
</p>
<p>chemical potentials, however, it is not immediately obvious what they should be.
</p>
<p>The role of statistical mechanics, in part, is to provide microscopic interpretations
</p>
<p>and microscopic expressions for various quantities we deal with in a macroscopic
</p>
<p>description of our world.
</p>
<p>Example 3.1. Vibration of a monomer in liquid water: To estimate a charac-
</p>
<p>teristic time scale of the vibrational motion of molecules, let us focus on liq-
</p>
<p>uid water and suppose that a molecule in the liquid phase may be regarded
</p>
<p>as a rigid spherical particle confined to a rigid spherical cavity of radius
</p>
<p>Rl = (3/4πnl)
1/3, where nl is the number density of molecules. For liquid
</p>
<p>water at 298.15 K, Rl = 1.928 Å. If we approximate the effective radius Rw
of a water molecule by the Lennard&ndash;Jones radius used in a model potential
</p>
<p>of water, such as the TIP4P model potential, Rw &asymp; 1.577 Å. The magnitude
of the x-component of the velocity, on average, is given by
</p>
<p>&radic;
kBT/m as seen</p>
<p/>
</div>
<div class="page"><p/>
<p>114 3 Classical Statistical Mechanics
</p>
<p>a b c
</p>
<p>Fig. 3.2 a and b are examples of possible phase trajectories, while c is an impossible phase
</p>
<p>trajectory.
</p>
<p>from (3.170). At 298.15 K, this gives vx = 371.0 (m/s). Assuming that a given
molecule is bouncing around between x = Rl and x =&minus;Rl at this velocity, the
period of the vibrational motion is 4(Rl &minus;Rw)/vx = 3.78&times;10&minus;12 s.
</p>
<p>3.2 Phase Space
</p>
<p>The mechanical state of a system is specified by giving numerical values to 2 f vari-
</p>
<p>ables, q f and p f . If we consider a space spanned by the axis q1, . . . , q f and p1, . . . ,
</p>
<p>p f , then the mechanical state of the system will be represented by a point in this 2 f
</p>
<p>dimensional space. This space is called the phase space, and the point representing
</p>
<p>the state of the system is called the phase point. As the mechanical system evolves
</p>
<p>with time, q1, . . . , q f and p1, . . . , p f will also change, and the phase point will move
</p>
<p>along a path, which is referred to as the phase trajectory. Trajectories in Fig. 3.2a,
</p>
<p>b are examples of possible phase trajectories, with the latter representing a periodic
</p>
<p>motion.13 However, trajectory in Fig. 3.2c indicates an impossible phase trajectory.
</p>
<p>According to what we saw in Chap. 1, if the system is at a particular point in the
</p>
<p>phase space at some instant of time t0, its position in this space at any other time
</p>
<p>t 	= t0 is completely determined by the equations of motion. Trajectory c violates
this principle.14
</p>
<p>Example 3.2. Free fall of a particle: Consider a free fall of a particle of mass
</p>
<p>m from the height h at t = 0. Taking the z-axis as vertically upward from the
ground and setting the initial velocity to zero, we find
</p>
<p>z(t) =&minus;1
2
</p>
<p>gt2 +h and ż =&minus;gt . (3.7)
</p>
<p>Thus,
</p>
<p>pz(t) = mż =&minus;mgt . (3.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Ensemble Average 115
</p>
<p>Eliminating t from the equations for z(t) and pz(t), we find
</p>
<p>z =&minus;1
2
</p>
<p>g
</p>
<p>(
&minus; pz
</p>
<p>mg
</p>
<p>)2
+h =&minus; pz
</p>
<p>2
</p>
<p>2m2g
+h . (3.9)
</p>
<p>This is, then, the equation for the phase trajectory. Because the particle is
</p>
<p>moving toward the negative z-direction, only the portion with pz &le; 0 in the
following diagram is relevant.
</p>
<p>. ............. ..............
..............
</p>
<p>..
</p>
<p>...........
......
</p>
<p>.........
.........
</p>
<p>.
</p>
<p>.........
.........
</p>
<p>.....
</p>
<p>.........
.........
</p>
<p>.........
</p>
<p>........
........
........
......
</p>
<p>........
........
........
........
..
</p>
<p>........
........
........
........
.....
</p>
<p>........
........
........
........
........
.
</p>
<p>........
........
........
........
........
....
</p>
<p>z h@ t 0
</p>
<p>z 0 @ t 2h g
</p>
<p>............................................
.................
</p>
<p>..............
.....
</p>
<p>..............
.........
</p>
<p>..............
.............
</p>
<p>.............
.............
....
</p>
<p>.............
.............
........
</p>
<p>.............
.............
...........
</p>
<p>.............
.............
.............
..
</p>
<p>.............
............
.............
......
</p>
<p>m 2gh m 2gh
</p>
<p>z
</p>
<p>pz0
</p>
<p>Setting z = 0 in (3.7), we see that the particle hit the ground at t =
&radic;
</p>
<p>2h/g.
According to (3.8),
</p>
<p>pz =&minus;mg
&radic;
</p>
<p>2h
</p>
<p>g
=&minus;m
</p>
<p>&radic;
2gh (3.10)
</p>
<p>at this very moment.
</p>
<p>Exercise 3.1. A particle is confined to a potential well φ(x) = kx2/2 (k &gt; 0).
Assuming that the particle moves in the x-direction only, draw its phase trajectory.
</p>
<p>///
</p>
<p>3.3 Ensemble Average
</p>
<p>Suppose that we observe a system over a long duration of time that commences at
</p>
<p>time t and ends at t + τ . During τ , the system evolves according to the equations of
motion and passes through various regions in the phase space. As shown in Fig. 3.3
</p>
<p>for the case of f = 1, we take an infinitesimal volume element dq f dp f around a
phase point (q f , p f ) and denote by ∆ t(q f , p f , t) the total amount of time the system
spent in this volume element. Then, we define ρ by
</p>
<p>ρ(q f , p f , t)dq f dp f := lim
τ&rarr;&infin;
</p>
<p>∆ t(q f , p f , t)
</p>
<p>τ
. (3.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>116 3 Classical Statistical Mechanics
</p>
<p>dq
</p>
<p>d
p
</p>
<p>q p
</p>
<p>q
</p>
<p>p
</p>
<p>.
.............................
</p>
<p>..........................
</p>
<p>......................
</p>
<p>...................
................
</p>
<p>...............
.............. ........... ............ .............. ................
</p>
<p>...............
...
</p>
<p>..............
.......
</p>
<p>..............
..........
</p>
<p>..............................................................
.................
</p>
<p>...............
......................................................................... ....... ........ ...... ........
</p>
<p>.........
.............
....................
</p>
<p>.................
..............
............
......... .......... ........... ....... ....... ........ .............. ............ ......... ....... ..... .......... .............
</p>
<p>............... .
.................
..............
............
...........
........
.........................................................................................................................
</p>
<p>..........
............
..................
</p>
<p>...................
</p>
<p>.
</p>
<p>............
.............
.............
..............
..............
............... .
.............
...........
.........
............
.............
</p>
<p>..................
...........
</p>
<p>Fig. 3.3 A phase trajectory passing through a volume element dqdp centered around a phase point
</p>
<p>(q, p) in a two-dimensional phase space during a time duration τ .
</p>
<p>The quantity ρdq f dp f is the probability of finding the system inside the volume
element if it is observed at some instant of time t &prime;, which we choose arbitrarily with
uniform probability between t and t + τ . We can easily see that ρdq f dp f satisfies
the usual requirements of probability. Firstly, because ∆ t &ge; 0, we have ρ &ge; 0 every-
where in the phase space. Secondly, because the system is always found somewhere
</p>
<p>in the phase space, the summation of ∆ t for all the volume elements yields τ:
</p>
<p>&int;
ρ(q f , p f , t)dq f dp f = 1 , (3.12)
</p>
<p>where the integration is over the entire phase space. Because of the probabilistic
</p>
<p>interpretation of ρdq f dp f , we may write the long-time average of the dynamical
variable A(q f , p f , t) as
</p>
<p>Aexpt(t) =
</p>
<p>&int;
A(q f , p f , t)ρ(q f , p f , t)dq f dp f . (3.13)
</p>
<p>For the reason that becomes clear in Sect. 3.5, the expression on the right is referred
</p>
<p>to as the ensemble average, or the thermal average, of the dynamical variable
</p>
<p>A. To help you convince yourself of the validity of (3.13), consider the following
</p>
<p>example.
</p>
<p>Example 3.3. Temperature of a house as experienced by a moving object: Sup-
</p>
<p>pose that you picked up a thermometer and walked around in your house, vis-
</p>
<p>iting multiple times three of the rooms, kept at different temperatures. The
</p>
<p>temperature reading from the thermometer, as a function of time, may look
</p>
<p>like this:</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Ensemble Average 117
</p>
<p>18
</p>
<p>20
</p>
<p>22
</p>
<p>2 6 7 12 13.5 18 t (min)0
</p>
<p>T
(
</p>
<p>C
)
</p>
<p>What is the average temperature Texpt you have experienced? Equation (3.4)
</p>
<p>applied to this problem gives
</p>
<p>Texpt =
1
</p>
<p>18
[(20&times;2)+(18&times;4)+(20&times;1)+(22&times;5)+(18&times;1.5)+(20&times;4.5)] .
</p>
<p>(3.14)
</p>
<p>Evidently, this expression can be rewritten somewhat more compactly as
</p>
<p>Texpt =
1
</p>
<p>18
[18&times; (4+1.5)+20&times; (2+1+4.5)+22&times;5] . (3.15)
</p>
<p>To make it look more like (3.13), let Ti and ∆ ti denote the temperature of
room i and the total amount of time you spent in room i, respectively. From
</p>
<p>the graph shown above, we can construct the following table:
</p>
<p>i Ti (
◦C) ∆ ti (min)
</p>
<p>1 18 5.5
</p>
<p>2 20 7.5
</p>
<p>3 22 5
</p>
<p>Then, the above equation for Texpt becomes
</p>
<p>Texpt =
1
</p>
<p>τ
</p>
<p>3
</p>
<p>&sum;
i=1
</p>
<p>Ti∆ ti =
3
</p>
<p>&sum;
i=1
</p>
<p>Tiρi , (3.16)
</p>
<p>where τ = 18(min) and ρi := ∆ ti/τ . This expression for Texpt should be com-
pared against (3.13).
</p>
<p>We note that the value of ρ , at fixed q f and p f , can depend on t, because ∆ t we
obtain from our measurement may very well depend on when we commence our
</p>
<p>measurement. The explicit time dependence of A indicated in (3.13) deserves some
</p>
<p>comment, however.
</p>
<p>Note that the explicit time dependence can arise in a dynamical variable if the
</p>
<p>system is subject to a time-dependent external field. If the field changes very rapidly
</p>
<p>during the time interval ∆ t, then in general, we cannot expect (3.13) to hold. To see
this, one might consider a somewhat artificial example in which A is independent of</p>
<p/>
</div>
<div class="page"><p/>
<p>118 3 Classical Statistical Mechanics
</p>
<p>q f and p f but depends explicitly on time as
</p>
<p>A(q f , p f , t &prime;) =
</p>
<p>{
1 if t &le; t &prime; &lt; t + ε
0 otherwise,
</p>
<p>(3.17)
</p>
<p>where ε ≪ τ . In this case, Aexpt(t)&asymp; 0 from (3.4). But according to (3.13), in which
the integrand is evaluated at time t, Aexpt(t) = 1. In writing (3.13), therefore, it is
tacitly assumed that the change in A due to its explicit time dependence occurs
</p>
<p>sufficiently slowly during τ . In other words, we are allowing only for external fields
that changes very slowly. On the other hand, rapid variations that may be exhibited
</p>
<p>by A due to its implicit dependence, that is, the change in A due to changes in q f
</p>
<p>and p f with t, is captured by the dependence of ρ on q f and p f .
</p>
<p>3.4 Statistical Equilibrium
</p>
<p>So far, all we have done is to rewrite (3.4) using a newly defined quantity ρ . But, to
compute ρ from (3.11), we still have to solve the equations of motion. So, why do
we even bother with the quantity ρ?
</p>
<p>By expressing Aexpt by means of ρ , we are hoping that we could somehow come
up with an educated guess for the functional form of ρ without ever having to solve
the equations of motion. It will be very difficult to do this for the most general situ-
</p>
<p>ations. However, if we restrict our attention only to systems in equilibrium, maybe
</p>
<p>we can come up with a sensible guess for ρ without too much difficulty.
Of course, you note that we have converted the problem of evaluating the one-
</p>
<p>dimensional integral in (3.4) along with the solution of 2 f -coupled first-order differ-
</p>
<p>ential equations into that of evaluating 2 f -dimensional integral as given by (3.13).
</p>
<p>How should that make the actual calculation of Aexpt any easier? For example, this
</p>
<p>2 f -dimensional integral is often evaluated numerically by means of Monte Carlo
</p>
<p>simulation. The required computational effort is comparable to that of molecular
</p>
<p>dynamics.
</p>
<p>However, and this is the point: By introducing the new quantity ρ , we have com-
pletely changed the nature of the problem. This, in turn, allows for a completely new
</p>
<p>set of logical deductions and physical insights to operate, enabling us to introduce
</p>
<p>reasonable approximations in a manner unimaginable if we insist on solving the
</p>
<p>equations of motion as required by (3.4). Even more importantly, it is now possible
</p>
<p>to find explicit expressions for entropy and free energies in terms of functions of q f
</p>
<p>and p f as we shall see.
</p>
<p>Because we have decided to limit our considerations to systems in equilibrium,
</p>
<p>we should first define precisely what is meant by equilibrium. We say that the system
</p>
<p>is in statistical equilibrium if and only if dAexpt/dt = 0 at any instant of time for any
dynamical variable A that does not depend explicitly on time. For such A, the total
</p>
<p>time derivative of (3.13) can be evaluated as follows. First, we recall the definition</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Statistical Ensemble 119
</p>
<p>of the derivative:
</p>
<p>dAexpt(t)
</p>
<p>dt
= lim
</p>
<p>∆ t&rarr;0
1
</p>
<p>∆ t
</p>
<p>[
Aexpt(t +∆ t)&minus;Aexpt(t)
</p>
<p>]
. (3.18)
</p>
<p>Using (3.13), we find
</p>
<p>dAexpt(t)
</p>
<p>dt
= lim
</p>
<p>∆ t&rarr;0
1
</p>
<p>∆ t
</p>
<p>[&int;
A(q f , p f )ρ(q f , p f , t +∆ t)dq f dp f
</p>
<p>&minus;
&int;
</p>
<p>A(q f , p f )ρ(q f , p f , t)dq f dp f
]
. (3.19)
</p>
<p>Since both integrals are taken over the entire phase space, they can be combined to
</p>
<p>give
</p>
<p>dAexpt(t)
</p>
<p>dt
= lim
</p>
<p>∆ t&rarr;0
1
</p>
<p>∆ t
</p>
<p>&int;
A(q f , p f )
</p>
<p>[
ρ(q f , p f , t +∆ t)&minus;ρ(q f , p f , t)
</p>
<p>]
dq f dp f . (3.20)
</p>
<p>For a small enough ∆ t, the change in ρ during the time duration ∆ t may be written
as
</p>
<p>ρ(q f , p f , t +∆ t)&minus;ρ(q f , p f , t) = &part;ρ(q
f , p f , t)
</p>
<p>&part; t
∆ t (3.21)
</p>
<p>with sufficient accuracy. Introducing this expression in (3.20), we obtain
</p>
<p>dAexpt(t)
</p>
<p>dt
=
</p>
<p>&int;
A(q f , p f )
</p>
<p>&part;ρ(q f , p f , t)
</p>
<p>&part; t
dq f dp f . (3.22)
</p>
<p>At equilibrium, this quantity must vanish at any instant of time for any choice of A.
</p>
<p>Thus, the necessary and sufficient condition for statistical equilibrium is that
</p>
<p>&part;ρ(q f , p f , t)
</p>
<p>&part; t
= 0 (3.23)
</p>
<p>holds everywhere in the phase space at any instant. In other words, ρ does not
depend explicitly on t.
</p>
<p>3.5 Statistical Ensemble
</p>
<p>Recall that we constructed ρ by observing a single system over a long duration
of time starting at time t. This ρ , and hence Aexpt calculated by means of (3.13),
will in general depend on t. When this time dependence is absent for any Aexpt
that corresponds to a dynamical variable A without an explicit time dependence, we
</p>
<p>say that the system is in statistical equilibrium. While this definition of equilibrium
</p>
<p>makes good physical sense, (3.23) that followed from it does not appear to offer
</p>
<p>any useful insight toward an educated guess for the functional form of ρ . For this
reason, we introduce the following construction.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 3 Classical Statistical Mechanics
</p>
<p>First, we create a large number, say N , of copies of the original system.
</p>
<p>Copies have the identical mechanical construction to the original. If there are time-
</p>
<p>dependent external fields, their time dependence are common to all copies. In other
</p>
<p>words, the functional form of the Hamiltonian of a copy is identical to that of the
</p>
<p>original including the explicit time dependence. Such a collection of N copies is
</p>
<p>called the statistical ensemble.
</p>
<p>At a given instant t, each copy of the ensemble has a representative point, as
</p>
<p>specified by (q f (t), p f (t)), in the phase space. While all copies in the ensemble are
characterized by the same functional form of the Hamiltonian, we do not require
</p>
<p>that their mechanical states, and hence the corresponding phase points in the phase
</p>
<p>space, at any instant should coincide. Instead, we construct the statistical ensemble
</p>
<p>so that the number of copies whose mechanical state falls within the infinitesimal
</p>
<p>volume element dq f dp f taken around (q f , p f ) is given by
</p>
<p>N ρ(q f , p f )dq f dp f . (3.24)
</p>
<p>That is, the number density of copies in the phase space at (q f , p f ) is N ρ(q f , p f ).
It should be emphasized that a given copy in the ensemble does not interact with
</p>
<p>the other copies in the ensemble. Each copy has its own representative point in the
</p>
<p>phase space and moves along its own phase trajectory according to the equations of
</p>
<p>motion pertaining only to that copy.
</p>
<p>If we take a fixed control volume in the phase space, the number of copies found
</p>
<p>in it may change with time as some copies leave the control volume while others
</p>
<p>enter it. However, once we put N copies in the phase space and watch what happens
</p>
<p>to them, no copy will simply disappear from the phase space, nor will a new one
</p>
<p>appear spontaneously out of nothing. This implies that the number density N ρ of
the copies satisfies the equivalent of mass balance or the equation of continuity from
</p>
<p>fluid mechanics in the phase space. This observation leads to an important theorem,
</p>
<p>which guide us in our search for the equilibrium distribution as we shall see next.
</p>
<p>3.6 Liouville&rsquo;s Theorem
</p>
<p>Let us start by reviewing the equation of continuity from fluid mechanics in ordinary
</p>
<p>three-dimensional space. (The current derivation is adopted solely for the sake of
</p>
<p>expediency. For a physically more natural derivation of the equation of continuity,
</p>
<p>see Chap. 3 of Ref. [6].)
</p>
<p>Consider a control volume V fixed in space at all time. The total number of
</p>
<p>particles within V at a given moment t is given by
</p>
<p>&int;
</p>
<p>V
ρ(r, t)dr , (3.25)
</p>
<p>where ρ(r, t) temporarily denotes the number density of particles such as molecules
at position r at time t. If we exclude the possibility of chemical reactions, the rate of</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Liouville&rsquo;s Theorem 121
</p>
<p>change of this integral can be expressed in terms of the flux across the boundary A
</p>
<p>of V :
d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>V
ρ(r, t)dr=&minus;
</p>
<p>∮
</p>
<p>A
ρ(r, t)v(r, t) &middot;n(r)dA , (3.26)
</p>
<p>where n is the outward unit normal and v is the average velocity of the particles
</p>
<p>passing through the surface element dA.
</p>
<p>Because our control volume remains fixed at all time, the total time derivative on
</p>
<p>the left-hand side can be brought inside the integral sign, where it becomes a partial
</p>
<p>derivative for the integrand is a function of r as well. Furthermore, by means of the
</p>
<p>divergence theorem, the surface integral in (3.26) may be converted into a volume
</p>
<p>integral, thus yielding &int;
</p>
<p>V
</p>
<p>[
&part;ρ
</p>
<p>&part; t
+&nabla; &middot; (ρv)
</p>
<p>]
dr= 0 . (3.27)
</p>
<p>Because this equation holds for any choice of the control volume V ,
</p>
<p>&part;ρ
</p>
<p>&part; t
+&nabla; &middot; (ρv) = 0 (3.28)
</p>
<p>must hold everywhere at any instant. Writing out the divergence term &nabla; &middot; (ρv) using
a Cartesian coordinate system, we have
</p>
<p>&part;ρ
</p>
<p>&part; t
+
</p>
<p>&part; (ρvx)
</p>
<p>&part;x
+
</p>
<p>&part; (ρvy)
</p>
<p>&part;y
+
</p>
<p>&part; (ρvz)
</p>
<p>&part; z
= 0 . (3.29)
</p>
<p>where the subscripts label the respective components of v.
</p>
<p>In the phase space, the coordinates are q1, . . . , q f and p1, . . . , p f instead of x, y,
</p>
<p>and z. Instead of the components vx, vy, and vz, of the average velocity v, we have q̇1,
</p>
<p>. . . , q̇ f and ṗ1, . . . , ṗ f for the components of the average velocity of phase points.
</p>
<p>Finally, N ρ takes the place of ρ . Thus, by analogy to (3.29), we have
</p>
<p>&part; (N ρ)
</p>
<p>&part; t
+
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>[
&part; (N ρ q̇i)
</p>
<p>&part;qi
+
</p>
<p>&part; (N ρ ṗi)
</p>
<p>&part; pi
</p>
<p>]
= 0 . (3.30)
</p>
<p>Noting that N is a constant independent of q f , p f , and t, and using Hamilton&rsquo;s
</p>
<p>equations of motion (1.162), we find
</p>
<p>0 =
&part;ρ
</p>
<p>&part; t
+
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>[
&part;
</p>
<p>&part;qi
</p>
<p>(
ρ
&part;H
</p>
<p>&part; pi
</p>
<p>)
&minus; &part;
</p>
<p>&part; pi
</p>
<p>(
ρ
&part;H
</p>
<p>&part;qi
</p>
<p>)]
</p>
<p>=
&part;ρ
</p>
<p>&part; t
+
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>(
&part;ρ
</p>
<p>&part;qi
</p>
<p>&part;H
</p>
<p>&part; pi
+ρ
</p>
<p>&part; 2H
</p>
<p>&part;qi&part; pi
&minus; &part;H
</p>
<p>&part;qi
</p>
<p>&part;ρ
</p>
<p>&part; pi
&minus;ρ &part;
</p>
<p>2H
</p>
<p>&part; pi&part;qi
</p>
<p>)
.
</p>
<p>(3.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>122 3 Classical Statistical Mechanics
</p>
<p>Since the second derivatives are independent of the order of differentiation, we
</p>
<p>arrive at
</p>
<p>&part;ρ
</p>
<p>&part; t
+
</p>
<p>f
</p>
<p>&sum;
i=1
</p>
<p>(
&part;ρ
</p>
<p>&part;qi
</p>
<p>&part;H
</p>
<p>&part; pi
&minus; &part;H
</p>
<p>&part;qi
</p>
<p>&part;ρ
</p>
<p>&part; pi
</p>
<p>)
= 0 . (3.32)
</p>
<p>Recalling the definition of the Poisson bracket (1.184), we may rewrite (3.32) as
</p>
<p>dρ
</p>
<p>dt
=
</p>
<p>&part;ρ
</p>
<p>&part; t
+{ρ ,H}= 0 , (3.33)
</p>
<p>where the first equality is an example of (1.185). Equation (3.33) is known as Liou-
</p>
<p>ville&rsquo;s theorem and indicates that ρ is a constant of motion. (Our usage of this
phrase is slightly inappropriate here. Constant of motion usually refers to a dynam-
</p>
<p>ical variable pertaining to a single mechanical system rather than to a collection of
</p>
<p>them.)
</p>
<p>According to (3.23), therefore, the necessary and sufficient condition of statistical
</p>
<p>equilibrium is that ρ be a constant of motion that does not depend explicitly on time.
For this to be the case, it is sufficient (but not necessary) that ρ is a function of
constants of motion that are not, themselves, explicit functions of time. In fact, let
</p>
<p>{A1, . . . ,An} be a set of such constants of motion and suppose that
</p>
<p>ρ = ρ(A1, . . . ,An). (3.34)
</p>
<p>Then, because Ai does not depend explicitly on t,
</p>
<p>&part;ρ
</p>
<p>&part; t
=
</p>
<p>n
</p>
<p>&sum;
i=1
</p>
<p>&part;ρ
</p>
<p>&part;Ai
</p>
<p>&part;Ai
&part; t
</p>
<p>= 0 (3.35)
</p>
<p>and, because Ai is a constant of motion,
</p>
<p>dρ
</p>
<p>dt
=
</p>
<p>n
</p>
<p>&sum;
i=1
</p>
<p>&part;ρ
</p>
<p>&part;Ai
</p>
<p>dAi
</p>
<p>dt
= 0 (3.36)
</p>
<p>as required by (3.23) and (3.33), respectively.
</p>
<p>As we have seen, Liouville&rsquo;s theorem simply states that the number of copies
</p>
<p>in a statistical ensemble is conserved. This is a general requirement ρ must sat-
isfy regardless of whether the system is in statistical equilibrium or not. Statistical
</p>
<p>equilibrium imposes an additional requirement (3.23), which is quite distinct from
</p>
<p>(3.33). Because of (3.33), one often expresses (3.23) as
</p>
<p>{ρ ,H}= 0 . (3.37)
</p>
<p>We observe that any set of two equations taken from (3.23), the second equality in
</p>
<p>(3.33), and (3.37), is equivalent to any other such set.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 &dagger;The Number of Constants of Motion 123
</p>
<p>3.7 Significance of H
</p>
<p>Accepting (3.34) as our guiding principle in our search for the equilibrium distri-
</p>
<p>bution, we still have to decide on the set of dynamical variables. In the absence of
</p>
<p>a time-dependent external field, a system of f mechanical degrees of freedom has
</p>
<p>2 f &minus;1 independent constants of motion, each without an explicit time dependence.
(See the next section.) The mechanical energy E, the total linear momentum P, and
</p>
<p>the total angular momentum M are notable examples of such constants. (See (1.106),
</p>
<p>(1.138), and (1.145) for definitions.) Functions of these constants are also constants
</p>
<p>of motion. Among the multitude of constants, how do we choose our dynamical
</p>
<p>variable in terms of which to express ρ?
In Sect. 1.8, we saw that conservation laws of E, P, and M followed from a
</p>
<p>very general consideration regarding symmetry of space and time. In particular, it
</p>
<p>was unnecessary to refer to any specific details of the mechanical system. As such,
</p>
<p>conservation laws of these quantities are of very general character. In our search for
</p>
<p>a general theory applicable to all systems in equilibrium, the claim that they should
</p>
<p>serve a critical role is extremely compelling.
</p>
<p>Typically, we are interested in macroscopic bodies at rest and E is the only rel-
</p>
<p>evant dynamical variable. Therefore, using H instead of E to emphasize its depen-
</p>
<p>dence on q f and p f , we suppose that
</p>
<p>ρ = ρ(H) . (3.38)
</p>
<p>The justification of this hypothesis ultimately rests on the agreement between pre-
</p>
<p>dictions of our theory and experimental observations.
</p>
<p>3.8 &dagger;The Number of Constants of Motion
</p>
<p>Let us see how many independent constants of motion are associated with a given
</p>
<p>mechanical system. We assume that the system is subject to no time-dependent
</p>
<p>external field.
</p>
<p>We start by noting that the solution of Hamilton&rsquo;s equations of motion may be
</p>
<p>written as
</p>
<p>qi(t) = F qi(t &minus; t0,a f ,b f ) and pi(t) = F pi(t &minus; t0,a f ,b f ) , i = 1, . . . , f ,
(3.39)
</p>
<p>where we introduced a temporary notation F x to indicate functional dependence of
</p>
<p>the quantity x on the variables listed in the brackets. The first equation indicates,
</p>
<p>for example, that qi at time t depends only on the time that has passed since some
</p>
<p>arbitrary chosen instant t0 and the values q
f and p f assumed at t0, which we denote
</p>
<p>by a f and b f , respectively. In the absence of a time-dependent external field, the
</p>
<p>time dependence of q f and p f occurs only through t &minus; t0.</p>
<p/>
</div>
<div class="page"><p/>
<p>124 3 Classical Statistical Mechanics
</p>
<p>If the set of 2 f equations (3.39) is solved for 2 f variables a1, . . . , a f , b1, . . . ,
</p>
<p>b f&minus;1, and t &minus; t0, we obtain
</p>
<p>ai = F ai(q
f , p f ,b f ) , i = 1, . . . , f
</p>
<p>bi = F bi(q
f , p f ,b f ) , i = 1, . . . , f &minus;1
</p>
<p>t0 = t &minus;F t&minus;t0(q f , p f ,b f ) . (3.40)
</p>
<p>Thus, a1, . . . , a f , b1, . . . , b f&minus;1, and t0 are all functions of q f and p f that remain con-
stant, that is, they are constants of motion. Excluding t0, which depends explicitly
</p>
<p>on t, we are left with 2 f &minus;1 constants of motion with no explicit time dependence.
Insofar as their values can be specified independent of each other, F a1 , . . . , F a f
and F b1 , . . . , F b f&minus;1 are independent functions.
</p>
<p>Example 3.4. Free fall of a particle: Let us see how the above general scheme
</p>
<p>works for the problem of free-falling particle in Example 3.2. As the initial
</p>
<p>condition, we suppose that z = z0 and vz = v0 at t = t0. In terms of the notation
adopted above, we have
</p>
<p>q1 = z , p1 = pz , a1 = z0 , and b1 = v0 . (3.41)
</p>
<p>Since f = 1, we expect to find 2&times;1&minus;1 = 1 constant of motion.
Now, in place of (3.39), we have
</p>
<p>z = &minus;1
2
</p>
<p>g(t &minus; t0)2 + v0(t &minus; t0)+ z0
pz = &minus;mg(t &minus; t0)+mv0 . (3.42)
</p>
<p>Solving these equations for z0 and t &minus; t0, we find
</p>
<p>z0 = z+
1
</p>
<p>2m2g
</p>
<p>(
pz
</p>
<p>2 &minus;m2v02
)
</p>
<p>and t &minus; t0 =
mv0 &minus; pz
</p>
<p>mg
. (3.43)
</p>
<p>The first equation gives z0 as a function of the time-dependent coordinate z
</p>
<p>and the momentum pz but displays no explicit time dependence.
</p>
<p>Rearranging the equation a little, we arrive at
</p>
<p>mgz0 +
1
</p>
<p>2
mv0
</p>
<p>2 = mgz+
pz
</p>
<p>2
</p>
<p>2m
, (3.44)
</p>
<p>which is recognized as the mechanical energy E. According to this equation,
</p>
<p>E is also a constant of motion. However, E is given in terms of z0 and hence
</p>
<p>is not independent of z0. So, the number of independent constants of motion
</p>
<p>with no explicit time dependence is just one as advertised.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.9 Canonical Ensemble 125
</p>
<p>3.9 Canonical Ensemble
</p>
<p>At this point, we need to come up with an explicit expression for ρ . Let us suppose
then that
</p>
<p>ρ(q f , p f ) =
1
</p>
<p>C
e&minus;βH(q
</p>
<p>f ,p f ) (3.45)
</p>
<p>and see where it will take us. In Sect. 3.11, we see that there is actually a compelling
</p>
<p>reason for choosing this particular form of ρ . Nevertheless, (3.45) is a hypothesis,
the ultimate justification of which must be sought through experimental scrutiny.
</p>
<p>The constant C is determined by the normalization condition of ρ:
</p>
<p>C =
&int;
</p>
<p>e&minus;βH(q
f ,p f )dq f dp f , (3.46)
</p>
<p>where the integration extends over all phase space. For applications we have in
</p>
<p>mind, there is usually no upper limit to values H can assume while H is bounded
</p>
<p>from below. (The potential energy of interaction between two atoms takes the min-
</p>
<p>imum possible value when the distance between the atoms is of the order of the
</p>
<p>atomic diameter. When they are squeezed closer together, the potential energy
</p>
<p>increases indefinitely.) In order for the integral in (3.46) to exist, therefore, the con-
</p>
<p>stant β must be positive. The statistical ensemble characterized by (3.45) is called
the canonical ensemble.
</p>
<p>Note that copies with different values of energy are present in the canonical
</p>
<p>ensemble. But, ρ was constructed from a phase trajectory along which H is con-
stant. We shall come back to this point in Sect. 3.11. For now, we simply accept the
</p>
<p>canonical ensemble as given and explore its implications. We start by looking for
</p>
<p>physical meaning of β and C in this section.
We define the internal energy U of a system by
</p>
<p>U := 〈H〉=
&int;
</p>
<p>H(q f , p f )ρ(q f , p f )dq f dp f =
1
</p>
<p>C
</p>
<p>&int;
H(q f , p f )e&minus;βH(q
</p>
<p>f ,p f )dq f dp f ,
</p>
<p>(3.47)
</p>
<p>where we introduced a new notation for the ensemble average we encountered
</p>
<p>earlier:
</p>
<p>〈A〉 :=
&int;
</p>
<p>A(q f , p f )ρ(q f , p f )dq f dp f =
1
</p>
<p>C
</p>
<p>&int;
A(q f , p f )e&minus;βH(q
</p>
<p>f ,p f )dq f dp f . (3.48)
</p>
<p>Recalling (3.13), we have
</p>
<p>〈A〉= Aexpt . (3.49)
As with (3.13), this identity is a consequence of our construction of the statistical
</p>
<p>ensemble.
</p>
<p>We recall from thermodynamics that
</p>
<p>dU = T dS+work term. (3.50)</p>
<p/>
</div>
<div class="page"><p/>
<p>126 3 Classical Statistical Mechanics
</p>
<p>x0
</p>
<p>x
</p>
<p>w
</p>
<p>0
</p>
<p>.
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>..
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>........
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>....
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>.
</p>
<p>.........
</p>
<p>.........
</p>
<p>.......
</p>
<p>.........
</p>
<p>.........
</p>
<p>...
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>.....
</p>
<p>..........
.
....................
........
............
</p>
<p>...............
</p>
<p>.
.............
...........
........
............ . . .
</p>
<p>.
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>..
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>........
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>....
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>.
</p>
<p>.........
</p>
<p>.........
</p>
<p>.......
</p>
<p>.........
</p>
<p>.........
</p>
<p>...
</p>
<p>.........
</p>
<p>.........
</p>
<p>.........
</p>
<p>.....
</p>
<p>..........
.
....................
........
............
</p>
<p>...............
</p>
<p>.
.............
...........
........
...............
</p>
<p>Fig. 3.4 Gas particles confined to a cylinder. Each particle is subject to the external field generated
</p>
<p>by the walls of the cylinder. The figure on the right illustrates the net field ψw produced by the
vertical wall on the left and the piston on the right.
</p>
<p>If we can develop an expression for dU based on (3.47) for a process involving
</p>
<p>work, we should be able to relate T and S to statistical mechanical quantities such
</p>
<p>as ρ , β , and C. This forms a basis for exploring a link between thermodynamics and
the microscopic description of matter based on mechanics.
</p>
<p>First, we introduce a few notations. Let Θ = β&minus;1 and rewrite (3.45) as
</p>
<p>H =&minus;Θ lnρ&minus;Θ lnC . (3.51)
</p>
<p>Upon taking the average, we find
</p>
<p>U =&minus;Θ〈lnρ〉&minus;Θ lnC =Θη+α , (3.52)
</p>
<p>where we used the fact that 〈A〉 = A if A is independent of q f and p f . We also
defined
</p>
<p>η :=&minus;〈lnρ〉 and α :=&minus;Θ lnC . (3.53)
From (3.52),
</p>
<p>dU = ηdΘ +Θdη+dα . (3.54)
</p>
<p>Equation (3.54) with three terms on the right cannot be compared directly with
</p>
<p>(3.50). We also need to relate quantities on the right-hand side of (3.54) with the
</p>
<p>work. To this end, let us consider the example shown in Fig. 3.4 illustrating N gas
</p>
<p>particles confined to a cylinder. By moving the piston, we can exert work on the
</p>
<p>gas. Since we require the ability to control λ at will, we choose the gas particles as
our statistical mechanical system and regard the piston as a movable source of an
</p>
<p>external field, which, together with the field generated by the fixed walls, confines
</p>
<p>the particles to the cylinder. So, the value of λ is common to all members of the
statistical ensemble. The Hamiltonian of the system is given by
</p>
<p>H =
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2m
</p>
<p>+φ(rN)+ψ(rN ,λ ) , (3.55)
</p>
<p>where φ is the potential energy due to mutual interaction among the particles and ψ
is the net external field generated by the piston and the other walls of the cylinder.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.9 Canonical Ensemble 127
</p>
<p>The point here is that we can perform work on the system by changing λ and that
the Hamiltonian is a function of λ .
</p>
<p>From (3.46), we see that C is a function of Θ = β&minus;1 and λ , and hence
</p>
<p>dC =
</p>
<p>(
&part;C
</p>
<p>&part;Θ
</p>
<p>)
</p>
<p>λ
</p>
<p>dΘ +
</p>
<p>(
&part;C
</p>
<p>&part;λ
</p>
<p>)
</p>
<p>Θ
</p>
<p>dλ . (3.56)
</p>
<p>Now, using (3.53) and (3.56), we find
</p>
<p>dα =&minus;dΘ lnC&minus;Θ
C
</p>
<p>dC =
</p>
<p>[
α
</p>
<p>Θ
&minus;Θ
</p>
<p>C
</p>
<p>(
&part;C
</p>
<p>&part;Θ
</p>
<p>)
</p>
<p>λ
</p>
<p>]
dΘ &minus;Θ
</p>
<p>C
</p>
<p>(
&part;C
</p>
<p>&part;λ
</p>
<p>)
</p>
<p>Θ
</p>
<p>dλ . (3.57)
</p>
<p>Using (3.46), we can rewrite the partial derivatives in a little more illuminating form:
</p>
<p>1
</p>
<p>C
</p>
<p>(
&part;C
</p>
<p>&part;Θ
</p>
<p>)
</p>
<p>λ
</p>
<p>=
1
</p>
<p>C
</p>
<p>&part;
</p>
<p>&part;Θ
</p>
<p>&int;
e&minus;H/Θdq f dp f . (3.58)
</p>
<p>Since Θ appears only as the denominator in the exponent, &part;/&part;Θ can be brought
inside the integral:
</p>
<p>1
</p>
<p>C
</p>
<p>(
&part;C
</p>
<p>&part;Θ
</p>
<p>)
</p>
<p>λ
</p>
<p>=
1
</p>
<p>C
</p>
<p>&int;
&part;
</p>
<p>&part;Θ
e&minus;H/Θdq f dp f =
</p>
<p>1
</p>
<p>C
</p>
<p>&int;
H
</p>
<p>Θ 2
e&minus;H/Θdq f dp f =
</p>
<p>U
</p>
<p>Θ 2
, (3.59)
</p>
<p>where we used (3.47). Similarly,
</p>
<p>1
</p>
<p>C
</p>
<p>(
&part;C
</p>
<p>&part;λ
</p>
<p>)
</p>
<p>Θ
</p>
<p>=
1
</p>
<p>C
</p>
<p>&part;
</p>
<p>&part;λ
</p>
<p>&int;
e&minus;H/Θdq f dp f =
</p>
<p>1
</p>
<p>C
</p>
<p>&int;
&part;
</p>
<p>&part;λ
e&minus;H/Θdq f dp f
</p>
<p>=
1
</p>
<p>C
</p>
<p>&int;
&minus; 1
Θ
</p>
<p>&part;H
</p>
<p>&part;λ
e&minus;H/Θdq f dp f , (3.60)
</p>
<p>where the partial derivative in the integrand is for fixed q f and p f . Using (3.48),
</p>
<p>1
</p>
<p>C
</p>
<p>(
&part;C
</p>
<p>&part;λ
</p>
<p>)
</p>
<p>Θ
</p>
<p>=&minus; 1
Θ
</p>
<p>&lang;
&part;H
</p>
<p>&part;λ
</p>
<p>&rang;
. (3.61)
</p>
<p>Substituting (3.59) and (3.61) in (3.57) and using (3.52), we obtain
</p>
<p>dα =
1
</p>
<p>Θ
(α&minus;U)dΘ +
</p>
<p>&lang;
&part;H
</p>
<p>&part;λ
</p>
<p>&rang;
dλ =&minus;ηdΘ +
</p>
<p>&lang;
&part;H
</p>
<p>&part;λ
</p>
<p>&rang;
dλ . (3.62)
</p>
<p>This equation can be brought into (3.54) to yield
</p>
<p>dU =Θdη+
</p>
<p>&lang;
&part;H
</p>
<p>&part;λ
</p>
<p>&rang;
dλ . (3.63)
</p>
<p>We identify the term proportional to the displacement, dλ , as the work term.
Because η depends on λ , a term proportional to dλ is implicit in dη . In essence, the
work in thermodynamics is defined to be 〈&part;H/&part;λ 〉dλ . This definition seems very</p>
<p/>
</div>
<div class="page"><p/>
<p>128 3 Classical Statistical Mechanics
</p>
<p>reasonable since &minus;&part;H/&part;λ is the x-component of the force exerted on the piston by
the gas molecules. The appearance of thermal average is also natural provided that
</p>
<p>the characteristic time scale over which the work is performed is considerably larger
</p>
<p>than that of typical molecular motions.
</p>
<p>Upon comparison between (3.50) and (3.63), we finally arrive at
</p>
<p>Θdη = T dS . (3.64)
</p>
<p>This equation implies thatΘ &prop; T and dη &prop; dS. The proportionality constant between
dS and dη is quite arbitrary and the most logical choice for the constant perhaps is
just unity. However, for a historical reason, we introduce the Boltzmann constant kB
and write (3.64) as
</p>
<p>Θdη = kBT d(S/kB) . (3.65)
</p>
<p>This suggests that we set
</p>
<p>T =
Θ
</p>
<p>kB
(3.66)
</p>
<p>and
</p>
<p>S = kBη+ const.=&minus;kB〈lnρ〉+ const. (3.67)
Since we are usually interested only in a entropy difference, we set the constant
</p>
<p>rather arbitrarily to zero. For an in-depth discussion on this choice, see Chap. 3 of
</p>
<p>Ref. [4]. In any case, we now have
</p>
<p>S = kBη =&minus;kB〈lnρ〉 . (3.68)
</p>
<p>Equations (3.66) and (3.68) are, then, the statistical mechanical expressions for T
</p>
<p>and S, respectively.
</p>
<p>Equation (3.68) is due to Gibbs, and is known as Gibbs&rsquo;s entropy formula.
</p>
<p>From (3.66), we note that changing the choice of the constant kB in (3.65) simply
</p>
<p>amounts to changing the scale for measuring the temperature. Since Θ = β&minus;1 and
kB are both positive, T must be positive as well. With the identification
</p>
<p>β :=
1
</p>
<p>kBT
, (3.69)
</p>
<p>the expression e&minus;βH is called the Boltzmann factor.
Equation (3.66) makes it clear that T is not an average of some dynamical vari-
</p>
<p>able. Instead, it is a parameter characterizing the distribution of the members of
</p>
<p>the statistical ensemble over states of different energy values. Later, we see that T
</p>
<p>is proportional to the average kinetic energy per particle. However, this does not
</p>
<p>imply that T is defined in terms of the average kinetic energy. The average becomes
</p>
<p>computable only after we specify T .
</p>
<p>We note that
</p>
<p>α =U &minus;Θη =U &minus;T S (3.70)
is the Helmholtz free energy F . Thus, combining (3.53) and (3.66), we finally arrive
</p>
<p>at
</p>
<p>F =&minus;kBT lnC . (3.71)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Simple Applications of Canonical Ensemble 129
</p>
<p>As exciting all these results are, (3.71) turns out to be not quite correct. To see
</p>
<p>why this is so, however, we have to temporarily accept (3.71) and explore its con-
</p>
<p>sequences. It is only by comparing the predictions of (3.71) against other areas of
</p>
<p>our experience that we are able to uncover the fatal flaw hidden in (3.71). Until
</p>
<p>Sect. 3.12, where we introduce corrections to both (3.68) and (3.71), we pretend as
</p>
<p>if everything is fine and press on.
</p>
<p>Exercise 3.2.
</p>
<p>a. Show that
</p>
<p>U =&minus;&part; lnC
&part;β
</p>
<p>. (3.72)
</p>
<p>This is the statistical mechanical version of the Gibbs&ndash;Helmholtz equation
</p>
<p>derived in Exercise 2.15.
</p>
<p>b. Show that
&part; 2 lnC
</p>
<p>&part;β 2
=
&lang;
(H &minus;〈H〉)2
</p>
<p>&rang;
. (3.73)
</p>
<p>c. Show that
&part; 2 lnC
</p>
<p>&part;β 2
= kBT
</p>
<p>2CV . (3.74)
</p>
<p>Combining parts b and c, we see that CV is related to the characteristic width of
</p>
<p>fluctuation of H. ///
</p>
<p>Exercise 3.3. Using (3.45) and (3.68), derive (3.71). ///
</p>
<p>3.10 Simple Applications of Canonical Ensemble
</p>
<p>To gain some familiarity with the formalism we have developed so far, we shall
</p>
<p>consider several simple model systems and compute a few of their thermodynamic
</p>
<p>properties using the canonical ensemble.
</p>
<p>3.10.1 Rectangular Coordinate System
</p>
<p>Example 3.5. Particle in a box: Consider a particle of mass m confined to a
</p>
<p>rectangular box of dimension Lx &times; Ly &times; Lz. The Hamiltonian of the system
may be written as
</p>
<p>H(r,p) =
p2
</p>
<p>2m
+ψw(r) , (3.75)
</p>
<p>where p2 = ||p||2 and ψw is the potential energy arising from the interaction
between the particle and the wall of the box. In this problem, we assume a</p>
<p/>
</div>
<div class="page"><p/>
<p>130 3 Classical Statistical Mechanics
</p>
<p>simple form for it:
</p>
<p>ψw(r) =
</p>
<p>{
0 if r is in the box,
&infin; otherwise.
</p>
<p>(3.76)
</p>
<p>The corresponding Boltzmann factor is given by
</p>
<p>e&minus;βH(r,p) =
</p>
<p>{
e&minus;β p
</p>
<p>2/2m if r is in the box,
0 otherwise.
</p>
<p>(3.77)
</p>
<p>The normalization constant C is obtained by integrating this expression over
</p>
<p>all phase space:15
</p>
<p>C =
</p>
<p>&int;
e&minus;βHdrdp=
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;βHdxdydzdpxdpydpz .
</p>
<p>(3.78)
</p>
<p>Let us start by considering the innermost integral:
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;βHdx . (3.79)
</p>
<p>Since the integrand vanishes outside the box, it is sufficient to integrate from
</p>
<p>0 to Lx. Over this interval, the integrand is independent of x. Thus,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;βHdx = Lxe
</p>
<p>&minus;βH . (3.80)
</p>
<p>When this expression is brought back into (3.78) and the integration with
</p>
<p>respect to y and z are performed in the similar manner, we find that
</p>
<p>C =V
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;βHdpxdpydpz , (3.81)
</p>
<p>where V := LxLyLz is the volume of the box. Next, we note that
</p>
<p>p2 = px
2 + py
</p>
<p>2 + pz
2 , (3.82)
</p>
<p>and hence
</p>
<p>C =V
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;β px
</p>
<p>2/2me&minus;β py
2/2me&minus;β pz
</p>
<p>2/2mdpxdpydpz . (3.83)
</p>
<p>When performing the innermost integral with respect to px, we may regard py
and pz as constant. Thus,
</p>
<p>C =V
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;β py
</p>
<p>2/2me&minus;β pz
2/2m
</p>
<p>[&int; &infin;
&minus;&infin;
</p>
<p>e&minus;β px
2/2mdpx
</p>
<p>]
dpydpz . (3.84)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Simple Applications of Canonical Ensemble 131
</p>
<p>Since the quantity in the square bracket is independent of py and pz, it can
</p>
<p>be pulled out of the integrals with respect to py and pz. The remaining two-
</p>
<p>dimensional integral can be performed similarly to yield
</p>
<p>C =V
</p>
<p>[&int; &infin;
&minus;&infin;
</p>
<p>e&minus;β px
2/2mdpx
</p>
<p>][&int; &infin;
&minus;&infin;
</p>
<p>e&minus;β py
2/2mdpy
</p>
<p>][&int; &infin;
&minus;&infin;
</p>
<p>e&minus;β pz
2/2mdpz
</p>
<p>]
.
</p>
<p>(3.85)
</p>
<p>Using the formula established in Exercise 3.4 to carry out the integrals, we
</p>
<p>obtain
</p>
<p>C =V
</p>
<p>(
2πm
</p>
<p>β
</p>
<p>)3/2
. (3.86)
</p>
<p>By means of (3.71), we find
</p>
<p>F =&minus;kBT
[
</p>
<p>lnV +
3
</p>
<p>2
ln
</p>
<p>(
2πm
</p>
<p>β
</p>
<p>)]
. (3.87)
</p>
<p>This equation gives F as a function of T , V , and N = 1, and hence is a funda-
mental equation of the system. Other thermodynamic properties of the system
</p>
<p>can be obtained just by taking partial derivatives. For example, the pressure P
</p>
<p>is given by
</p>
<p>P =&minus;
(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T
</p>
<p>=
kBT
</p>
<p>V
, (3.88)
</p>
<p>which is recognized as the ideal gas equation of state applied to a system
</p>
<p>containing only a single particle. The internal energy follows from (3.72):
</p>
<p>U =&minus;&part; lnC
&part;β
</p>
<p>=
3
</p>
<p>2β
=
</p>
<p>3
</p>
<p>2
kBT . (3.89)
</p>
<p>From (2.49),
</p>
<p>CV =
</p>
<p>(
&part;U
</p>
<p>&part;T
</p>
<p>)
</p>
<p>V
</p>
<p>=
3
</p>
<p>2
kB . (3.90)
</p>
<p>These nontrivial results were obtained straightforwardly once the multidimen-
</p>
<p>sional integral in (3.78) was evaluated. It is doubtful that we would be so
</p>
<p>successful if we insisted on solving the equations of motion of the particle
</p>
<p>instead.
</p>
<p>Exercise 3.4. Derive the following equality:
</p>
<p>I(a) :=
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;ax
</p>
<p>2
dx =
</p>
<p>1
</p>
<p>2
</p>
<p>&radic;
π
</p>
<p>a
. (3.91)
</p>
<p>Using this result, show that
</p>
<p>&int; &infin;
</p>
<p>0
x2e&minus;ax
</p>
<p>2
dx =
</p>
<p>1
</p>
<p>4
</p>
<p>&radic;
π
</p>
<p>a3
and
</p>
<p>&int; &infin;
</p>
<p>0
x4e&minus;ax
</p>
<p>2
dx =
</p>
<p>3
</p>
<p>8
</p>
<p>&radic;
π
</p>
<p>a5
. (3.92)
</p>
<p>///</p>
<p/>
</div>
<div class="page"><p/>
<p>132 3 Classical Statistical Mechanics
</p>
<p>Example 3.6. Three-dimensional harmonic oscillator: Consider a particle of
</p>
<p>mass m bound to the origin by a harmonic potential ψ(r) = kr2/2. Using a
Cartesian coordinate system, we specify the position of the particle by x, y, and
</p>
<p>z. The conjugate momenta are just the components of the linear momentum,
</p>
<p>that is, px, py, and pz. Thus, the system Hamiltonian is
</p>
<p>H(r,p) =
1
</p>
<p>2m
</p>
<p>(
px
</p>
<p>2 + py
2 + pz
</p>
<p>2
)
+
</p>
<p>1
</p>
<p>2
k
(
x2 + y2 + z2
</p>
<p>)
=
</p>
<p>p2
</p>
<p>2m
+
</p>
<p>1
</p>
<p>2
kr2 . (3.93)
</p>
<p>The normalization constant C is given by
</p>
<p>C =
</p>
<p>&int;
e&minus;βH(r,p)drdp=
</p>
<p>&int;
e&minus;β p
</p>
<p>2/2me&minus;βkr
2/2drdp . (3.94)
</p>
<p>Noting that the result of carrying out the integration with respect to r is inde-
</p>
<p>pendent of p, we have
</p>
<p>C =
</p>
<p>[&int;
e&minus;β p
</p>
<p>2/2mdp
</p>
<p>][&int;
e&minus;βkr
</p>
<p>2/2dr
</p>
<p>]
. (3.95)
</p>
<p>These integrals can be evaluated using the Cartesian coordinate system as
</p>
<p>in Example 3.5. However, the numerical values of these integrals should be
</p>
<p>independent of the choice of the coordinate system we use. It is actually easier
</p>
<p>to evaluate them using the spherical coordinate system. For example,
</p>
<p>&int;
e&minus;βkr
</p>
<p>2/2dr=
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;βkr
</p>
<p>2/24πr2dr =
</p>
<p>(
2π
</p>
<p>βk
</p>
<p>)3/2
. (3.96)
</p>
<p>The first equality is justified because the integrand depends only on r and
</p>
<p>the volume of the spherical shell defined by two radii r and r+ dr is, to the
first order of dr, given by 4πr2dr. The second equality follows from a formula
from Exercise 3.4. Evaluating the second integral in (3.95) similarly, we arrive
</p>
<p>at
</p>
<p>C =
</p>
<p>(
2πm
</p>
<p>β
</p>
<p>)3/2(
2π
</p>
<p>βk
</p>
<p>)3/2
, (3.97)
</p>
<p>Since C &prop; β&minus;3, we have lnC =&minus;3lnβ + const. Thus,
</p>
<p>U = 3kBT and CV = 3kB . (3.98)
</p>
<p>Exercise 3.5. Suppose that the Hamiltonian H of the system of N particles is given
</p>
<p>by
</p>
<p>H(rN ,pN) =
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2mi
</p>
<p>+φ(rN) . (3.99)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Simple Applications of Canonical Ensemble 133
</p>
<p>Show that the probability ρ(p1)dp1 that particle 1 has the linear momentum within
the volume element dp1 taken around p1 is given by
</p>
<p>ρ(p1)dp1 =
1
</p>
<p>(2πm1kBT )3/2
exp
</p>
<p>(
&minus; ||p1||
</p>
<p>2
</p>
<p>2m1kBT
</p>
<p>)
dp1 . (3.100)
</p>
<p>This is the well-known Maxwell&ndash;Boltzmann distribution, which is more often
</p>
<p>expressed as
</p>
<p>ρ(v1)dv1 =
</p>
<p>(
m1
</p>
<p>2πkBT
</p>
<p>)3/2
exp
</p>
<p>(
&minus;m1||v1||
</p>
<p>2
</p>
<p>2kBT
</p>
<p>)
dv1 , (3.101)
</p>
<p>where we used dp1 = dp1xdp1ydp1z =m1
3dv1xdv1ydv1z =m1
</p>
<p>3dv1. It is worth empha-
</p>
<p>sizing that the Maxwell&ndash;Boltzmann distribution holds regardless of the form of the
</p>
<p>potential energy term φ(rN). ///
</p>
<p>3.10.2 Equipartition Theorem
</p>
<p>Looking back at Examples 3.5 and 3.6, we notice something interesting. In particu-
</p>
<p>lar, each quadratic term, such as x2 and px
2, in the Hamiltonian leads to a factor of
</p>
<p>β&minus;1/2 in C. In view of (3.72), this means that each quadratic term makes a contri-
bution of kBT/2 and kB/2 to U and CV , respectively.
</p>
<p>This observation can be generalized straightforwardly. Thus, suppose that q1
occurs in Hamiltonian as a quadratic term and appears nowhere else:
</p>
<p>H(q f , p f ) = aq1
2 +h(q2, &middot; &middot; &middot; ,q f , p1, &middot; &middot; &middot; , p f ) . (3.102)
</p>
<p>If e&minus;βH decays sufficiently fast with increasing |q1|, the limits of integration for q1
that is occurring in (3.46) can be extended to cover from &minus;&infin; to &infin;. Then, the aq12
term gives rise to a factor of (π/βa)1/2 in C.
</p>
<p>Exercise 3.6. For a system described by the Hamiltonian given by (3.102), show
</p>
<p>that
</p>
<p>〈aq12〉=
1
</p>
<p>2
kBT . (3.103)
</p>
<p>///
</p>
<p>More generally, if
</p>
<p>H(q f , p f ) =
f
</p>
<p>&sum;
i=1
</p>
<p>(
aiqi
</p>
<p>2 +bi pi
2
)
, (3.104)
</p>
<p>and the limits of integration can be similarly extended to from &minus;&infin; to &infin; for all q f
and p f , then,
</p>
<p>U = f kBT, and CV = f kB . (3.105)</p>
<p/>
</div>
<div class="page"><p/>
<p>134 3 Classical Statistical Mechanics
</p>
<p>This is called the equipartition theorem or the principle of equipartition of
</p>
<p>energy to indicate that U , on average, is partitioned equally among each mechanical
</p>
<p>degrees of freedom.
</p>
<p>3.10.3 Spherical Coordinate System
</p>
<p>In Example 3.6, we wrote down the expression for H using the Cartesian coordinate
</p>
<p>system and used the spherical coordinate system only in the last step when comput-
</p>
<p>ing the multidimensional integrals for C. It is instructive to rework the same problem
</p>
<p>using the spherical coordinate system from the beginning.
</p>
<p>Example 3.7. Three-dimensional harmonic oscillator: spherical coordinate
</p>
<p>system: We start from the Lagrangian:
</p>
<p>L(r, ṙ) =
1
</p>
<p>2m
</p>
<p>(
ẋ2 + ẏ2 + ż2
</p>
<p>)
&minus; 1
</p>
<p>2
kr2 . (3.106)
</p>
<p>In a spherical coordinate system,
</p>
<p>x = r sinθ cosφ , y = r sinθ sinφ , z = r cosθ . (3.107)
</p>
<p>Taking the time derivative of these expressions, we find
</p>
<p>ẋ = ṙ sinθ cosφ + rθ̇ cosθ cosφ &minus; rφ̇ sinθ sinφ
ẏ = ṙ sinθ sinφ + rθ̇ cosθ sinφ + rφ̇ sinθ cosφ
</p>
<p>ż = ṙ cosθ &minus; rθ̇ sinθ . (3.108)
</p>
<p>Using these expressions, we find16
</p>
<p>ẋ2 + ẏ2 + ż2 = ṙ2 + r2θ̇
2
+ r2φ̇
</p>
<p>2
sin2 θ . (3.109)
</p>
<p>Thus, in terms of the generalized coordinates r, θ , φ , and the corresponding
generalized velocities ṙ, θ̇ , and φ̇ , the Lagrangian is given by
</p>
<p>L =
1
</p>
<p>2
m
(
</p>
<p>ṙ2 + r2θ̇
2
+ r2φ̇
</p>
<p>2
sin2 θ
</p>
<p>)
&minus; 1
</p>
<p>2
kr2 . (3.110)
</p>
<p>By definition (1.103), the corresponding generalized momenta are
</p>
<p>pr =
&part;L
</p>
<p>&part; ṙ
= mṙ, pθ =
</p>
<p>&part;L
</p>
<p>&part; θ̇
= mr2θ̇ , pφ =
</p>
<p>&part;L
</p>
<p>&part; φ̇
= mr2φ̇ sin2 θ . (3.111)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Simple Applications of Canonical Ensemble 135
</p>
<p>The Hamiltonian follows from (1.156) and is given by
</p>
<p>H =&sum;
i
</p>
<p>piq̇i &minus;L =
pr
</p>
<p>2
</p>
<p>2m
+
</p>
<p>pθ
2
</p>
<p>2mr2
+
</p>
<p>pφ
2
</p>
<p>2mr2 sin2 θ
+
</p>
<p>1
</p>
<p>2
kr2 . (3.112)
</p>
<p>According to (3.46), the normalization constant C is given by
</p>
<p>C =
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; π
</p>
<p>0
</p>
<p>&int; 2π
</p>
<p>0
e&minus;βHdφdθdrdpφdpθdpr , (3.113)
</p>
<p>in which φ , θ , and r are the generalized coordinates in this problem with the
corresponding generalized momentum given by pφ , pθ , and pr, respectively.
</p>
<p>Carrying out the integrations with respect to pφ , pθ , and pr by means of the
</p>
<p>formula from Exercise 3.4, we find
</p>
<p>C =
&int; &infin;
</p>
<p>0
</p>
<p>&int; π
</p>
<p>0
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>(
2πm
</p>
<p>β
</p>
<p>)1/2(
2πmr2
</p>
<p>β
</p>
<p>)1/2(
2πmr2 sin2 θ
</p>
<p>β
</p>
<p>)1/2
</p>
<p>&times;e&minus; 12βkr2dφdθdr . (3.114)
</p>
<p>But, because sinθ &ge; 0 for 0 &le; θ &le; π , we have (sin2 θ)1/2 = sinθ . Thus,
</p>
<p>C =
</p>
<p>(
2πm
</p>
<p>β
</p>
<p>)3/2 &int; &infin;
0
</p>
<p>&int; π
</p>
<p>0
</p>
<p>&int; 2π
</p>
<p>0
e&minus;
</p>
<p>1
2βkr
</p>
<p>2
r2 sinθdφdθdr . (3.115)
</p>
<p>We observe that the Jacobian r2 sinθ of the coordinate transformation from
(x,y,z) to (r,θ ,φ) appeared naturally without us having to put it in by hand.
Evaluating the integrals over φ , θ , and r in this order, we find
</p>
<p>C =
</p>
<p>(
2πm
</p>
<p>β
</p>
<p>)3/2(
2π
</p>
<p>βk
</p>
<p>)3/2
(3.116)
</p>
<p>in agreement with the result we found in Example 3.6.
</p>
<p>Exercise 3.7. Counting the number of quadratic terms in (3.93), we conclude from
</p>
<p>the equipartition theorem that U = 3kBT . Doing the same in (3.112), one might
conclude that U = 2kBT . Resolve this apparent contradiction. ///
</p>
<p>Exercise 3.8. As a model for a diatomic molecule, take a pair of particles of mass
</p>
<p>m1 and m2 connected by a massless rigid rod of length l:
</p>
<p>a. Calculate the normalization constant C for the system consisting of a single
</p>
<p>diatomic molecule confined to a cubic box of volume V . You may assume that
</p>
<p>V 1/3 ≫ l. Why is this assumption convenient?
b. What is the constant volume heat capacity CV of the system?</p>
<p/>
</div>
<div class="page"><p/>
<p>136 3 Classical Statistical Mechanics
</p>
<p>x
</p>
<p>y
</p>
<p>z
</p>
<p>E
</p>
<p>m1, q
</p>
<p>m2, q
</p>
<p>l
</p>
<p>Fig. 3.5 A simple model of a polar molecule.
</p>
<p>c. Replace the rigid rod by a harmonic spring of natural length l and spring constant
</p>
<p>k, and then redo the calculation. To make the computation analytically tractable,
</p>
<p>assume that e&minus;βkl
2/2 &asymp; 0. ///
</p>
<p>Exercise 3.9. Continuing with the rigid diatomic molecule from Exercise 3.8, sup-
</p>
<p>pose that the particles m1 and m2 carry electric charges q and &minus;q, respectively. (See
Fig. 3.5.) In the presence of a static electric field E, the additional term &minus;me &middot;E is
needed in the Hamiltonian of the molecule, where the dipole moment me is a vector
</p>
<p>of length ql pointing from m2 to m1:
</p>
<p>a. Evaluate C of the molecule confined to a cubic box of volume V .
</p>
<p>b. Show that
&part; lnC
</p>
<p>&part;E
= β 〈me〉 . (3.117)
</p>
<p>c. Show that
&part;E
</p>
<p>&part;E
=
</p>
<p>E
</p>
<p>E
, (3.118)
</p>
<p>where E := ||E|| should not be confused with energy.
d. Show that
</p>
<p>〈me〉= meL (βmeE)e , (3.119)
where e is the unit vector pointing in the direction of the field and
</p>
<p>L (x) := cothx&minus; 1
x
=
</p>
<p>coshx
</p>
<p>sinhx
&minus; 1
</p>
<p>x
(3.120)
</p>
<p>is the Langevin function.
</p>
<p>In part a, the computation will be considerably easier if you align the z-axis with
</p>
<p>E as shown in Fig. 3.5. Since our choice of the coordinate system is arbitrary, this
</p>
<p>leads to no loss of generality. ///</p>
<p/>
</div>
<div class="page"><p/>
<p>3.11 Canonical Ensemble and Thermal Contact 137
</p>
<p>3.10.4 &dagger;General Equipartition Theorem
</p>
<p>In this section, we consider a more general form of the equipartition theorem. In
</p>
<p>particular, we establish that
</p>
<p>&lang;
qi
&part;H
</p>
<p>&part;qi
</p>
<p>&rang;
= kBT and
</p>
<p>&lang;
pi
&part;H
</p>
<p>&part; pi
</p>
<p>&rang;
= kBT . (3.121)
</p>
<p>For example, let
</p>
<p>H =
1
</p>
<p>2m
</p>
<p>(
px
</p>
<p>2 + py
2 + pz
</p>
<p>2
)
. (3.122)
</p>
<p>According to (3.121),
</p>
<p>kBT =
</p>
<p>&lang;
px
</p>
<p>&part;H
</p>
<p>&part; px
</p>
<p>&rang;
=
</p>
<p>&lang;
px
</p>
<p>2
</p>
<p>m
</p>
<p>&rang;
(3.123)
</p>
<p>in agreement with the more restricted form of the theorem discussed in Sect. 3.10.2.
</p>
<p>To prove (3.121), let us first consider the following partial derivative:
</p>
<p>&part;
</p>
<p>&part;q1
</p>
<p>(
q1e
</p>
<p>&minus;βH
)
= e&minus;βH &minus;βq1
</p>
<p>&part;H
</p>
<p>&part;q1
e&minus;βH . (3.124)
</p>
<p>Integrating this expression over the entire phase space,
</p>
<p>&int; [
q1e
</p>
<p>&minus;βH
]q1=&infin;
</p>
<p>q1=&minus;&infin;
dq2 &middot; &middot; &middot;dq f dp f =C&minus;β
</p>
<p>&int;
q1
</p>
<p>&part;H
</p>
<p>&part;q1
e&minus;βHdq f dp f . (3.125)
</p>
<p>If the Hamiltonian is such that the Boltzmann factor vanishes for sufficiently large
</p>
<p>values of |q1|, then the left-hand side of this equation is zero, yielding
</p>
<p>kBT =
1
</p>
<p>C
</p>
<p>&int;
q1
</p>
<p>&part;H
</p>
<p>&part;q1
e&minus;βHdq f dp f =
</p>
<p>&lang;
q1
</p>
<p>&part;H
</p>
<p>&part;q1
</p>
<p>&rang;
. (3.126)
</p>
<p>Likewise for the other q&rsquo;s. Carrying out the similar analysis with pi, we obtain the
</p>
<p>second of (3.121).
</p>
<p>3.11 Canonical Ensemble and Thermal Contact
</p>
<p>In this section, we shall return to the question posed at the beginning of Sect. 3.9.
</p>
<p>On the one hand, ρ was determined by observing a single system for a long dura-
tion of time. During such an observation, the energy of the system should remain
</p>
<p>constant. On the other hand, the canonical ensemble contains the copies with var-
</p>
<p>ious values of the energy. Does this mean that the canonical ensemble, despite its
</p>
<p>apparent success we saw in the previous sections, is purely an artificial construction
</p>
<p>with no counterpart in the real-world situation?</p>
<p/>
</div>
<div class="page"><p/>
<p>138 3 Classical Statistical Mechanics
</p>
<p>Let us go back to (3.45). This equation tells us that if we wanted to predict macro-
</p>
<p>scopic behavior of a system by means of a canonical ensemble, we must first specify
</p>
<p>its temperature. Under an observational situation in which we wish to specify and
</p>
<p>control temperature, however, we must allow for an exchange of energy between
</p>
<p>the system and the surroundings. As a result, the energy of the system does not,
</p>
<p>in general, remain constant. So, the fact that the canonical ensemble contains the
</p>
<p>copies with various values of the energy does not invalidate this ensemble. Rather,
</p>
<p>it suggests that the ensemble may be relevant in describing the behavior of a system
</p>
<p>in thermal contact with the surroundings. In fact, under a modest set of assump-
</p>
<p>tions, we can show that the canonical ensemble is the only possible ensemble one
</p>
<p>can construct for such cases.
</p>
<p>First, we need to introduce the notion of a weak interaction. Let us consider
</p>
<p>an isolated system S t , in which we take a subsystem S s enclosed by a rigid wall
</p>
<p>impermeable to particles. The other part of S t will be referred to as the surround-
</p>
<p>ings and denoted by S r. Quite generally then, the Hamiltonian Ht of S t can be
</p>
<p>written as
</p>
<p>Ht(q
m+n, pm+n) = Hs(q
</p>
<p>m, pm)+Hr(q
n, pn)+Hint(q
</p>
<p>m+n, pm+n) , (3.127)
</p>
<p>where m and n denote the numbers of mechanical degrees of freedom of S s and
</p>
<p>S r, respectively. The last term Hint arises from the interaction between S s and
</p>
<p>S r.
</p>
<p>We note that this division of S t into S s and S r would be purely formal and
</p>
<p>quite useless if Hint is of a comparable magnitude to either Hs or Hr. On the other
</p>
<p>hand, if Hint is identically zero, then we might as well imagine S s and S r as sitting
</p>
<p>at the opposite ends of a galaxy, and there will be very little motivation for us to
</p>
<p>study them simultaneously by regarding them as constituting a composite system
</p>
<p>S t . The situation we are interested in is somewhere in between.
</p>
<p>With this in mind, the interaction between S s and S r is said to be weak if (1)
</p>
<p>Hint is of a negligible magnitude compared to Hs and Hr for typical values of their
</p>
<p>arguments to justify the approximation,
</p>
<p>Ht(q
m+n, pm+n)&asymp; Hs(qm, pm)+Hr(qn, pn) (3.128)
</p>
<p>but, at the same time, (2) it is still sufficient to ensure exchange of energy between
</p>
<p>S s and S r over a long duration of time.
</p>
<p>Now, suppose that S s consists of subsystems A and B and denote the numbers
</p>
<p>of mechanical degrees of freedom of these subsystems by ma and mb, respectively.
</p>
<p>Clearly, m = ma +mb. Furthermore, suppose that the interaction between A and B
is also weak in the sense just defined. Then, we may write
</p>
<p>Hs(q
m, pm)&asymp; Ha(qma , pma)+Hb(qmb , pmb) , (3.129)
</p>
<p>where we note that Ha depends only on the generalized coordinates and the con-
</p>
<p>jugate momenta pertaining to the mechanical degrees of freedom of subsystem A.
</p>
<p>Likewise for Hb. Thus, when Hamilton&rsquo;s equations of motion are obtained from Hs,</p>
<p/>
</div>
<div class="page"><p/>
<p>3.11 Canonical Ensemble and Thermal Contact 139
</p>
<p>we should find out that those equations of motion dictating the time evolution of
</p>
<p>(qma , pma) are decoupled from those governing (qmb , pmb).
This approximate independence between the two subsystems implies that
</p>
<p>ρs &asymp; ρaρb , (3.130)
</p>
<p>and hence that
</p>
<p>lnρs &asymp; lnρa + lnρb . (3.131)
We recall from Sect. 3.6 that ρ is a function of constants of motion. Here we see
that lnρ is also additive. These requirements will be met if lnρ is a linear function
of constants of motion which are themselves additive. We note that energy is such
</p>
<p>a quantity and that the linear dependence of lnρ on energy already indicates the
canonical distribution.17
</p>
<p>To see this more explicitly, let us suppose that ρ is a function of H only. If statis-
tical mechanics is to have any predictive ability at all, this function ρ(H) cannot be
specific to each system we happen to choose for our study. Instead, we demand that
</p>
<p>the form of the function ρ(H) is independent of a particular system under consider-
ation. Thus, letting
</p>
<p>ρ = a f (H) , (3.132)
</p>
<p>where a is a system-dependent constant that can be determined by the normalization
</p>
<p>condition of ρ , we write
</p>
<p>as f (Ha +Hb) = aa f (Ha)ab f (Hb) . (3.133)
</p>
<p>Differentiating this equation with respect to Ha, we find
</p>
<p>as f
&prime;(Ha +Hb) = aa f
</p>
<p>&prime;(Ha)ab f (Hb) , (3.134)
</p>
<p>while differentiation with respect to Hb yields
</p>
<p>as f
&prime;(Ha +Hb) = aa f (Ha)ab f
</p>
<p>&prime;(Hb) . (3.135)
</p>
<p>From these two equations,
f &prime;(Ha)
f (Ha)
</p>
<p>=
f &prime;(Hb)
f (Hb)
</p>
<p>, (3.136)
</p>
<p>which must hold regardless of the values of Ha and Hb. Note that the left-hand side
</p>
<p>depends only on Ha, while the right-hand side depends only on Hb, indicating that
</p>
<p>they can only be a constant. For example, we may arbitrarily fix Ha at 1J, and then
</p>
<p>allow Hb to change. Even then the equation must hold true since it holds for any
</p>
<p>values of Ha and Hb. Likewise, we can fix Hb and allow Ha to change. In any event,
</p>
<p>the expressions in (3.136) are equal to some constant, which we denote by &minus;β .
Upon integration, then we find
</p>
<p>f (x) = be&minus;βx , (3.137)</p>
<p/>
</div>
<div class="page"><p/>
<p>140 3 Classical Statistical Mechanics
</p>
<p>where b is a constant. When this expression is substituted into (3.132) and ab is
</p>
<p>determined by the normalization condition of ρ , we find that
</p>
<p>ρa =Ca
&minus;1e&minus;βHa , ρb =Cb
</p>
<p>&minus;1e&minus;βHb , and ρs =Cs
&minus;1e&minus;βHs , (3.138)
</p>
<p>which is the canonical distribution as advertised.
</p>
<p>That β is common to subsystems A and B is quite consistent with our earlier con-
clusion that β = (kBT )&minus;1. At equilibrium, the subsystems A and B both in contact
with the same surroundings should have the same temperature. As noted already in
</p>
<p>Sect. 3.9, β must be positive in order for the integral for C to converge, an observa-
tion consistent with β being (kBT )&minus;1.
</p>
<p>If we substitute (3.138) into (3.130) and use (3.129), we arrive at the conclusion
</p>
<p>that
</p>
<p>Cs =CaCb . (3.139)
</p>
<p>Or, equivalently,
</p>
<p>lnCs = lnCa + lnCb , (3.140)
</p>
<p>which in light of (3.71) indicates that the Helmholtz free energy is an additive quan-
</p>
<p>tity.
</p>
<p>It is significant that the additivity of the free energy relies on the interaction
</p>
<p>between the two subsystems A and B being sufficiently weak. This is the case if A
</p>
<p>and B are both macroscopic and the intermolecular potential between the molecules
</p>
<p>of A and those of B are sufficiently short ranged as to affect only those molecules
</p>
<p>near the boundary of the subsystems. Then, the interaction amounts to a surface
</p>
<p>effect, which is usually negligible for a macroscopic body.
</p>
<p>If you look back at our derivation of Liouville&rsquo;s theorem in Sect. 3.6, which
</p>
<p>guided our search for ρ , you might wonder, however. Did we not write down equa-
tions of motion for the system assuming that it was isolated? Then, in our search
</p>
<p>for ρ of a system in thermal contact with the surroundings, why can we still accept
the conclusion drawn from Liouville&rsquo;s theorem and limit our search of ρ to the
functions of H alone?
</p>
<p>But, we recall that the interaction between the system and the surroundings were
</p>
<p>supposed to be sufficiently weak. So, over a time interval that is not too long, the
</p>
<p>system behaves like an isolated system to a sufficient degree of accuracy. The equa-
</p>
<p>tions of motion obtained by ignoring Hint, and hence Liouville&rsquo;s theorem, will be
</p>
<p>sufficiently accurate over such a time interval. If many segments of phase trajectory
</p>
<p>from many such intervals of time are combined together, we expect to end up with
</p>
<p>a canonical distribution.
</p>
<p>Exercise 3.10. What is the role of the surroundings S r in arriving at (3.138)? ///
</p>
<p>Exercise 3.11. Show that the entropy as given by Gibbs&rsquo;s entropy formula is addi-
</p>
<p>tive. ///</p>
<p/>
</div>
<div class="page"><p/>
<p>3.12 Corrections from Quantum Mechanics 141
</p>
<p>3.12 Corrections from Quantum Mechanics
</p>
<p>As pointed out at the end of Sect. 3.9, our formula (3.71) for F is not quite correct
</p>
<p>as it stands. In this section, we see why this is so and then introduce necessary
</p>
<p>corrections.
</p>
<p>3.12.1 A System of Identical Particles
</p>
<p>Suppose that we have N noninteracting identical particles in a rectangular box of
</p>
<p>dimension Lx &times;Ly &times;Lz. The Hamiltonian is given by
</p>
<p>H(rN ,pN) =
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2m
</p>
<p>+ψw(r
N) , (3.141)
</p>
<p>where ψw is the wall potential as in Example 3.5. With pi
.
= (pix, piy, piz), we have
</p>
<p>||pi||2 = pix2 + piy2 + piz2. Thus,
</p>
<p>C =
</p>
<p>&int;
e&minus;βH(r
</p>
<p>N ,pN)drNdpN =V N
(
</p>
<p>2πm
</p>
<p>β
</p>
<p>)3N/2
. (3.142)
</p>
<p>(If this result is not obvious to you, set N = 2 and evaluate C following Example 3.5.)
Combining (3.71) and (3.142), we obtain
</p>
<p>F =&minus;kBT
[
</p>
<p>3N
</p>
<p>2
ln(2πmkBT )+N lnV
</p>
<p>]
(3.143)
</p>
<p>for the Helmholtz free energy of the system. The pressure of the system follows
</p>
<p>from
</p>
<p>P =&minus;
(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>=
NkBT
</p>
<p>V
, (3.144)
</p>
<p>which is the ideal gas equation of state.
</p>
<p>Our expression for F , however, is not quite correct. To see this, let us rewrite
</p>
<p>(3.143) as
</p>
<p>F = a1(T )N +a2(T )N lnV , (3.145)
</p>
<p>where a1 and a2 are some functions of T . Now, we recall from (3.140) and also from
</p>
<p>thermodynamics that F is extensive, that is,
</p>
<p>F(T,λV,λN) = λF(T,V,N) , (3.146)
</p>
<p>which is in direct conflict with the result just obtained. Equally disturbing is the fact
</p>
<p>that (3.145) fails to satisfy the Euler relation as you can easily verify by writing the
</p>
<p>latter as
</p>
<p>F =&minus;PV +&micro;N . (3.147)</p>
<p/>
</div>
<div class="page"><p/>
<p>142 3 Classical Statistical Mechanics
</p>
<p>Particle 1 at r
</p>
<p>Particle 2 at rp
</p>
<p>p
</p>
<p>Particle 2 at r
</p>
<p>Particle 1 at rp
</p>
<p>p
</p>
<p>a b
</p>
<p>Fig. 3.6 In classical mechanics, a and b represent two distinct microstates (r1,r2,p1,p2) =
(r,r&prime;,p,p&prime;) and (r1,r2,p1,p2) = (r
</p>
<p>&prime;,r,p&prime;,p), respectively. According to quantum mechanics, they
represent the same state.
</p>
<p>Statistical mechanics as we have developed it so far cannot coexist with thermody-
</p>
<p>namics!
</p>
<p>What went wrong? The problem is not with our development of statistical
</p>
<p>mechanics. Rather, it is with our starting point, that is, classical mechanics. The
</p>
<p>difficulty lies in the fact that the behavior of molecules is governed not by classical
</p>
<p>mechanics but by quantum mechanics. To address the difficulty with (3.143), we
</p>
<p>simply borrow a few relevant facts from quantum mechanics.
</p>
<p>In classical mechanics, particles are distinguishable even if they look exactly the
</p>
<p>same. At t = 0, we can mentally label all the molecules in the system and measure
their positions and velocities. Then, the equations of motion will tell us precisely
</p>
<p>which particle is where at any other instant. Thus, for any particle we pick at a later
</p>
<p>time, we know the label we have given it at the earlier time even if we were not
</p>
<p>paying any attention to the system in between. According to quantum mechanics,
</p>
<p>however, this simply cannot be done even in principle. In the quantum mechani-
</p>
<p>cal world, identical particles are fundamentally indistinguishable, that is, neither a
</p>
<p>computational nor an experimental means of distinguishing them can ever be con-
</p>
<p>structed.
</p>
<p>Recall that C was introduced as the normalization constant. By writing
</p>
<p>C =
</p>
<p>&int;
e&minus;βH(r
</p>
<p>N ,pN)drNdpN , (3.148)
</p>
<p>we hoped to sum the unnormalized probability e&minus;βHdrNdpN over all possible states.
According to quantum mechanics, however, this amounts to counting each state
</p>
<p>many times.
</p>
<p>To figure out exactly how many times, consider a system of just two particles. In
</p>
<p>the integral
</p>
<p>C =
</p>
<p>&int;
e&minus;βHdr1dr2dp1dp2 , (3.149)
</p>
<p>the configurations shown in Fig. 3.6a and b are both included. But, quantum
</p>
<p>mechanics tells us to count these classical mechanically distinct two states as a sin-
</p>
<p>gle state. Since these two states have the identical value of the Boltzmann factor,
</p>
<p>they contribute equally to C. The same consideration applies to any other pair of</p>
<p/>
</div>
<div class="page"><p/>
<p>3.12 Corrections from Quantum Mechanics 143
</p>
<p>N Exact (3.152) (3.153)
</p>
<p>1 0 &minus;0.8106 &minus;1
2 0.6931 0.6518 &minus;0.6137
3 1.7917 1.7641 0.29584
</p>
<p>4 3.1781 3.1573 1.5452
</p>
<p>5 4.7875 4.7708 3.0472
</p>
<p>10 15.104 15.096 13.026
</p>
<p>50 148.48 148.48 145.60
</p>
<p>100 363.74 363.74 360.52
</p>
<p>1000 5912.1 5912.1 5907.8
</p>
<p>10000 82109 82109 82103
</p>
<p>Table 3.1 Accuracy of the approximate formulae, (3.152) and (3.153), for computing lnN!.
</p>
<p>classical mechanically distinct, but quantum mechanically identical, states. Thus,
</p>
<p>we should really be writing the normalization constant as
</p>
<p>C&prime; =
1
</p>
<p>2
</p>
<p>&int;
e&minus;βHdr1dr2dp1dp2 . (3.150)
</p>
<p>What happens if we have three particles? To answer this, we note that Fig. 3.6b was
</p>
<p>obtained from Fig. 3.6a simply by switching the labels &ldquo;1&rdquo; and &ldquo;2&rdquo; attached to the
</p>
<p>two particles, and the number 2 which divides C can be understood as the number of
</p>
<p>distinct ways of labeling the particles in the system. When we have three particles,
</p>
<p>there are 3! = 3 &middot;2 &middot;1 = 6 distinct ways of labeling them. (Using the labels 1, 2, and
3 once and only once.)
</p>
<p>Generalizing this result to a system of N identical particles, we conclude that the
</p>
<p>number of distinct permutations of N particles is N! := N(N &minus;1)(N &minus;2) &middot; &middot; &middot;3 &middot;2 &middot;1.
So, we should have defined the normalization factor by
</p>
<p>C&prime; =
1
</p>
<p>N!
</p>
<p>&int;
e&minus;βH(r
</p>
<p>N ,pN)drNdpN . (3.151)
</p>
<p>For a sufficiently large N, the following approximate relation, called Stirling&rsquo;s
</p>
<p>formula, holds.
</p>
<p>lnN! &asymp; N lnN &minus;N + 1
2
</p>
<p>ln(2πN) (3.152)
</p>
<p>&asymp; N lnN &minus;N . (3.153)
</p>
<p>The accuracy of these approximate formulae is illustrated in Table 3.1. In statistical
</p>
<p>mechanics, the typical values of N are in the order of 1024 and (3.153) is seen to be
</p>
<p>quite sufficient.
</p>
<p>Using (3.153), we can show that the Helmholtz free energy defined by
</p>
<p>F :=&minus;kBT lnC&prime; (3.154)</p>
<p/>
</div>
<div class="page"><p/>
<p>144 3 Classical Statistical Mechanics
</p>
<p>is an extensive quantity for a system of N noninteracting identical particles.
</p>
<p>Exercise 3.12. Prove this statement. ///
</p>
<p>Exercise 3.13. Provide a plausibility argument to support (3.153). ///
</p>
<p>What if N is not large enough to ensure the accuracy of Stirling&rsquo;s formula? In
</p>
<p>this case, even with the N! factor, the Helmholtz free energy of the system is not
</p>
<p>extensive. This is not a cause for a concern, though. If two small systems are brought
</p>
<p>together, the energy of their mutual interaction can be comparable with the energy
</p>
<p>of either one of them alone. As discussed in Sect. 3.11, there is no reason to expect
</p>
<p>the free energy to be extensive in such cases.
</p>
<p>As of now, we shall officially retire (3.71). You do not need (3.154) beyond this
</p>
<p>point, either. In its place, we introduce the correct formula for F in (3.167). That is
</p>
<p>the equation you need to remember and use.
</p>
<p>3.12.2 Implication of the Uncertainty Principle
</p>
<p>There is another important quantum mechanical effect, which we have ignored up
</p>
<p>to this point. Contrary to the underlying assumption of classical mechanics, the
</p>
<p>Heisenberg uncertainty principle states that both r and p of a particle cannot
</p>
<p>be determined simultaneously with absolute precision. Instead, quantum mechanics
</p>
<p>places a fundamental limit on the precision that is achievable in our measurement.
</p>
<p>Focusing only on the x-component, let (∆x)QM denote the uncertainty18 associ-
ated with the measured value of x. Likewise, (∆ px)QM is the uncertainty associated
with the measured value of px. Naturally, we would like to make both (∆x)QM and
(∆ px)QM as small as possible. However, the uncertainty principle demands that, for
a simultaneous measurement of x and px, they satisfy
</p>
<p>(∆x)QM(∆ px)QM � h , (3.155)
</p>
<p>where the quantity h= 6.626&times;10&minus;34 (J&middot;s) is the Planck constant having the dimen-
sion of action as in action integral. The similar relations hold for the y- and z- com-
</p>
<p>ponents of r and p. Even though h is very small, it is not zero. If we are to determine
</p>
<p>x with absolute certainty, that is, if (∆x)QM = 0, then we have absolutely no idea
what px is at that very moment. We emphasize that this limitation is not due to prac-
</p>
<p>tical difficulties in manufacturing a measuring device, but that it is a consequence
</p>
<p>of the principles of quantum mechanics.
</p>
<p>To explore the consequence of (3.155), let us consider a system of a single par-
</p>
<p>ticle confined to a one-dimensional container. In its phase space, suppose we take a
</p>
<p>box of area h. Classical mechanically, there are infinitely many distinct states in this
</p>
<p>box, and the integration
</p>
<p>&int; px+(∆ px)QM/2
</p>
<p>px&minus;(∆ px)QM/2
</p>
<p>&int; x+(∆x)QM/2
x&minus;(∆x)QM/2
</p>
<p>e&minus;βH(x,px)dxdpx (3.156)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.12 Corrections from Quantum Mechanics 145
</p>
<p>adds up the Boltzmann factor for all of these states. According to quantum mechan-
</p>
<p>ics, however, it is meaningless to try to distinguish these states. Thus, the integration
</p>
<p>should be replaced by a single Boltzmann factor.
</p>
<p>Because the area h of the box is actually quite small, the Boltzmann factor
</p>
<p>remains essentially constant within the box. (We could of course make (∆x)QM
extremely small causing (∆ px)QM to be extremely large, and vice versa. Such a
choice of a box must be excluded in order for our argument to hold.) So, in place of
</p>
<p>(3.156), we can simply write
</p>
<p>e&minus;βH(x
&lowast;,px&lowast;) (3.157)
</p>
<p>without being precise about the values of x&lowast; and p&lowast; except to note that (x&lowast;, p&lowast;)
should be somewhere in the box of area h:
</p>
<p>x&minus; (∆x)QM
2
</p>
<p>&le; x&lowast; &le; x+ (∆x)QM
2
</p>
<p>and px &minus;
(∆ px)QM
</p>
<p>2
&le; px&lowast; &le; px +
</p>
<p>(∆ px)QM
2
</p>
<p>.
</p>
<p>(3.158)
</p>
<p>Due to the smallness of h, we can replace (3.157) by its average taken inside the
</p>
<p>box:
1
</p>
<p>h
</p>
<p>&int; px+(∆ px)QM/2
</p>
<p>px&minus;(∆ px)QM/2
</p>
<p>&int; x+(∆x)QM/2
</p>
<p>x&minus;(∆x)QM/2
e&minus;βH(x,px)dxdpx (3.159)
</p>
<p>without introducing a noticeable change in the end result. Computationally, this is a
</p>
<p>much easier quantity to handle than (3.157).
</p>
<p>The uncertainty relation holds for any pair of a generalized coordinate and its
</p>
<p>conjugate momentum. Thus, generalizing the above consideration to the case of a
</p>
<p>mechanical system having f degrees of freedom, we may regard a volume element
</p>
<p>of size h f in the phase space as containing a single state, and write the normalization
</p>
<p>constant as
</p>
<p>Z =
1
</p>
<p>h f
</p>
<p>&int;
e&minus;βH(q
</p>
<p>f ,p f )dq f dp f . (3.160)
</p>
<p>As we saw in the previous section, if there are identical particles in the system and
</p>
<p>the integration over q f and p f induces P distinct permutations of identical particles,
</p>
<p>we have
</p>
<p>Z =
1
</p>
<p>h f P
</p>
<p>&int;
e&minus;βH(q
</p>
<p>f ,p f )dq f dp f . (3.161)
</p>
<p>The quantity Z is called the canonical partition function and is dimensionless.
</p>
<p>For a system of N identical particles in a three-dimensional space, we have
</p>
<p>h f P = h3NN!, and hence the partition function is given by
</p>
<p>Z =
1
</p>
<p>h3NN!
</p>
<p>&int;
e&minus;βH(r
</p>
<p>N ,pN)drNdpN . (3.162)
</p>
<p>The actual uncertainty relation reads
</p>
<p>〈∆x〉QM〈∆ px〉QM &ge;
h
</p>
<p>4π
(3.163)</p>
<p/>
</div>
<div class="page"><p/>
<p>146 3 Classical Statistical Mechanics
</p>
<p>as we shall see in Sect. 8.7.2. However, 4π does not factor in here. The 1/h3NN!
factor is chosen so that the quantum mechanical partition function agrees with our
</p>
<p>Z in the so called classical limit. See Sect. 8.16.5 for details.
</p>
<p>We emphasize that the correction factor 1/h f P (or 1/h3NN!) arises from quan-
tum mechanical considerations and is quite foreign to purely classical mechanical
</p>
<p>view of the world. This makes an intuitive interpretation of such hybrid formula as
</p>
<p>(3.161) and (3.162) difficult to obtain. In fact, any such attempt, if taken too lit-
</p>
<p>erally, leads to contradiction with the correct quantum mechanical interpretation.
</p>
<p>Nevertheless, we find it convenient to interpret
&int;
</p>
<p>as &ldquo;the sum over states&rdquo; as before
</p>
<p>and regard
</p>
<p>e&minus;βH(q
f ,p f ) dq
</p>
<p>f dp f
</p>
<p>h f P
, (3.164)
</p>
<p>as the unnormalized probability of finding the system within the infinitesimal vol-
</p>
<p>ume element dq f dp f in the phase space taken around the phase point (q f , p f ).
Dividing (3.164) by the normalization factor Z, we see that
</p>
<p>ρ(q f , p f )dq f dp f =
1
</p>
<p>Z
e&minus;βH(q
</p>
<p>f ,p f ) dq
f dp f
</p>
<p>h f P
(3.165)
</p>
<p>is the normalized probability of finding the system within the infinitesimal volume
</p>
<p>element dq f dp f .
</p>
<p>The breakdown of Z into the sum over states and the unnormalized probability is
</p>
<p>not unique. We note, however, that the probability given by (3.165) is dimensionless
</p>
<p>while the probability density ρ as defined by this equation carries the dimension of
1/(action) f . This is what one should expect.
</p>
<p>The ensemble average 〈A〉 of a dynamical variable A is now given by
</p>
<p>〈A〉= 1
Z
</p>
<p>&int;
A(q f , p f )e&minus;βH(q
</p>
<p>f ,p f ) dq
f dp f
</p>
<p>h f P
. (3.166)
</p>
<p>Since Z =C/h f P , the correction factor h f P drops out when computing the ensem-
ble average and (3.48) requires no modification. However, (3.71) must be corrected.
</p>
<p>Introducing the correction factor h f P into C, we have
</p>
<p>F =&minus;kBT lnZ . (3.167)
</p>
<p>in place of (3.71). As seen from the following exercise, we have
</p>
<p>S =&minus;kB〈ln(h f Pρ)〉 . (3.168)
</p>
<p>The latter is identical to (3.68) except for the h f P factor.
</p>
<p>Exercise 3.14. Using (3.165), (3.167), and the definition F :=U &minus;T S, show that S
is given by (3.168). ///</p>
<p/>
</div>
<div class="page"><p/>
<p>3.12 Corrections from Quantum Mechanics 147
</p>
<p>3.12.3 Applicability of Classical Statistical Mechanics
</p>
<p>We introduced the 1/h3NN! factor based on quantum mechanical considerations.
By simply grafting quantum concepts onto the classical mechanical framework,
</p>
<p>however, we cannot possibly expect to arrive at a theory that agrees with quan-
</p>
<p>tum mechanical predictions on all accounts. It is therefore important for us to know
</p>
<p>when we can expect classical statistical mechanics, with the factor 1/h3NN!, to be
an acceptable mode of description.
</p>
<p>First, we define
</p>
<p>(∆ px)SM :=
&lang;
(px &minus;〈px〉)2
</p>
<p>&rang;1/2
, (3.169)
</p>
<p>which may be considered as the purely statistical mechanical uncertainty in px. For
</p>
<p>a classical mechanical particle in a box,
</p>
<p>1
</p>
<p>2
kBT =
</p>
<p>&lang;
px
</p>
<p>2
</p>
<p>2m
</p>
<p>&rang;
. (3.170)
</p>
<p>as we saw in Example 3.5. You can also easily convince yourself that 〈px〉= 0 since
the probability density ρ is an even function of px. Thus,
</p>
<p>(∆ px)SM =
&radic;
</p>
<p>mkBT . (3.171)
</p>
<p>Given that this statistical mechanical uncertainty is always present in systems
</p>
<p>of our interest, we have no reason to require the quantum mechanical uncertainty
</p>
<p>(∆ px)QM to be any smaller than this. At the same time, we probably do not want
(∆ px)QM to be much larger than (∆ px)SM , either. Otherwise, predictions of classical
statistical mechanics would be of little use. Demanding, therefore, that they are of
</p>
<p>comparable magnitude, we may rewrite (3.155) as
</p>
<p>(∆x)QM &asymp;
h&radic;
</p>
<p>mkBT
&asymp; h&radic;
</p>
<p>2πmkBT
=: Λ , (3.172)
</p>
<p>where the quantity Λ , defined by the expression proceeding it, is known as the ther-
mal wavelength.
</p>
<p>The classical approach would be acceptable if Λ is much smaller than the diam-
eter of particles in the system since in that case, for all practical purposes we know
</p>
<p>&ldquo;exactly&rdquo; where our particles are. Assuming a typical value of 2&times;10&minus;10 m for the
diameter of an atom, this requirement translates to T ≫ 2 K for argon and T ≫ 20 K
for helium.
</p>
<p>Exercise 3.15. By making an appropriate change to the normalization constant C
</p>
<p>you have computed in Exercise 3.8a, compute the partition function Z for the rigid
</p>
<p>diatomic molecule. ///
</p>
<p>Exercise 3.16. As a model for an ideal gas, consider a collection of N noninteracting
</p>
<p>identical particles confined to a container of volume V :</p>
<p/>
</div>
<div class="page"><p/>
<p>148 3 Classical Statistical Mechanics
</p>
<p>a. Compute the partition function Z of the ideal gas.
</p>
<p>b. Derive expressions for internal energy, pressure, and chemical potential of the
</p>
<p>ideal gas. ///
</p>
<p>Exercise 3.17. A binary ideal gas mixture consisting of 2N particles of species
</p>
<p>A and N particles of species B is confined to a container of volume V made of
</p>
<p>diathermal, rigid, and impermeable wall, and is maintained at constant T . Find the
</p>
<p>reversible work required to isothermally separate the mixture into pure A and B
</p>
<p>occupying V/3 and 2V/3, respectively. ///
</p>
<p>3.13 &dagger;A Remark on the Statistical Approach
</p>
<p>In the traditional approach to statistical mechanics, the notion of statistical ensemble
</p>
<p>and that of an ensemble average are introduced independent of a long-time average.
</p>
<p>The equivalence between these two kinds of average,
</p>
<p>〈A〉= Aexpt , (3.173)
</p>
<p>is then adopted as a hypothesis. Any attempt to justifying this hypothesis on the
</p>
<p>basis of classical mechanics leads to a very difficult problem called the ergodic
</p>
<p>problem.
</p>
<p>For us, (3.173) is a consequence of how we constructed our statistical ensemble.
</p>
<p>This, however, does not mean that we have circumvented the ergodic problem. In
</p>
<p>fact, we have not shown how our observation of a single mechanical system over
</p>
<p>the time period τ should lead to ρ that is given by (3.45).
We recall that τ &rarr; &infin; in (3.11) only meant that τ is much longer than the char-
</p>
<p>acteristic time scale of vibrational molecular motion. The actual value of τ we had
in mind is comparable with a duration of a typical measurement, say 10&minus;4 s or
so. Certainly, this is not sufficiently long for a given molecule, in a glass of water
</p>
<p>for example, to fully explore all possible positions (and momentum) accessible to it.
</p>
<p>Diffusive exploration by this molecule throughout the entire glass takes much longer
</p>
<p>than 10&minus;4 s. Nevertheless, the Boltzmann factor e&minus;βH is nonzero for all possible
positions of this molecule with finite H. Despite this contradiction, our statistical
</p>
<p>approach is apparently very successful. Why is that?
</p>
<p>We can offer at least two qualitative explanations. Firstly, our macroscopic mea-
</p>
<p>surement is not fine enough to distinguish two molecules of the same species. Quan-
</p>
<p>tum mechanics actually preclude this possibility. Thus, we are concerned only with
</p>
<p>those dynamical variables that are invariant with respect to permutation of identi-
</p>
<p>cal molecules. The value of a dynamical variable A evaluated at two distinct phase
</p>
<p>points
</p>
<p>(r1,r2,r3, . . . ,rN ,p1,p2,p3, . . . ,pN) and (r2,r1,r3, . . . ,rN ,p2,p1,p3, . . . ,pN) ,
(3.174)
</p>
<p>for example, should be equal.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.14 &Dagger;Expressions for P and &micro; 149
</p>
<p>Secondly, macroscopic measurements are usually not sufficiently sensitive. Thus,
</p>
<p>even if the phase trajectory is perturbed slightly, our measurement lacks the sensi-
</p>
<p>tivity to detect the resulting change in A.
</p>
<p>These observations suggest that the appropriate construction of the probability
</p>
<p>density ρ is not quite what we saw in Sect. 3.3. Instead, we proceed as follows:
Given a phase trajectory of a mechanical system, we consider all possible permuta-
</p>
<p>tions of the coordinates of the identical particles and then &ldquo;smear out&rdquo; the trajecto-
</p>
<p>ries thus obtained to give them a nonzero thickness. Whether our newly constructed
</p>
<p>ρ leads to (3.45) is still an open question, of course.
We shall take a pragmatic point of view in what follows. We accept (3.45) and
</p>
<p>(3.173) as the fundamental hypothesis of our theory. The hypothesis should be
</p>
<p>deemed appropriate if the theory produces predictions in agreement with the out-
</p>
<p>come of experiments. This is how physical theories are constructed elsewhere.
</p>
<p>3.14 &Dagger;Expressions for P and &micro;
</p>
<p>In statistical mechanics, T is a parameter characterizing a statistical ensemble. Nev-
</p>
<p>ertheless, it is possible to express T as a thermal average of a dynamical variable. In
</p>
<p>fact, the equipartition theorem implies that
</p>
<p>T =
2
</p>
<p>3kBN
</p>
<p>&lang;
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2m
</p>
<p>&rang;
(3.175)
</p>
<p>for a system of N identical particles confined to a three-dimensional box. In a similar
</p>
<p>manner, we can express other intensive quantities, P and &micro; , as thermal averages of
some dynamical variables in a canonical ensemble even though they are parameters
</p>
<p>characterizing statistical ensembles as we shall see in Chap. 4.
</p>
<p>In this section, we derive expressions for P and &micro; for a system of N identical
particles with the Hamiltonian
</p>
<p>H =
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2m
</p>
<p>+φ(rN) , (3.176)
</p>
<p>in which φ is the potential energy due to interparticle interactions. The canonical
partition function of the system is given by
</p>
<p>Z =
1
</p>
<p>h3NN!
</p>
<p>&int;
e&minus;βH(r
</p>
<p>N ,pN)drNdpN =
1
</p>
<p>Λ 3NN!
</p>
<p>&int;
e&minus;βφ(r
</p>
<p>N)drN , (3.177)
</p>
<p>where we carried out the momentum integration, that is, the integration with respect
</p>
<p>to pN . The remaining integral
</p>
<p>&int;
e&minus;βφ(r
</p>
<p>N)drN (3.178)
</p>
<p>is called the configurational integral.</p>
<p/>
</div>
<div class="page"><p/>
<p>150 3 Classical Statistical Mechanics
</p>
<p>3.14.1 &Dagger;Pressure
</p>
<p>Recalling (2.193), we have
</p>
<p>P =&minus;
(
&part;F
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>= kBT
</p>
<p>(
&part; lnZ
</p>
<p>&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>. (3.179)
</p>
<p>In (3.178), V appears in the limits of integration with respect to rN . To facilitate the
</p>
<p>computation of the partial derivative, we assume that the system is a cube of side
</p>
<p>length L =V 1/3 and introduce a new set of variables by
</p>
<p>ri =: Lsi , i = 1, . . . ,N . (3.180)
</p>
<p>We note that
</p>
<p>dri = dxidyidzi = L
3dsixdsiydsiz =V dsi (3.181)
</p>
<p>with six denoting the x-component of si and similarly for siy and siz. Thus,
</p>
<p>Z =
V N
</p>
<p>Λ 3NN!
</p>
<p>&int;
e&minus;βφ(Ls1,...,LsN)dsN =:
</p>
<p>V N
</p>
<p>Λ 3NN!
QN , (3.182)
</p>
<p>where the integration is over the unit cube, that is, the cube of side length 1. We see
</p>
<p>that the V dependence of Z is now made explicit in the factor V N and in the potential
</p>
<p>energy φ . From (3.179) and (3.182),
</p>
<p>P = kBT
</p>
<p>[
N
</p>
<p>V
+
</p>
<p>1
</p>
<p>QN
</p>
<p>(
&part;QN
&part;V
</p>
<p>)
</p>
<p>T,N
</p>
<p>]
= Pid &minus;
</p>
<p>&lang;
&part;φ
</p>
<p>&part;V
</p>
<p>&rang;
, (3.183)
</p>
<p>where Pid := kBT N/V is the ideal gas contribution to P and &part;φ/&part;V is evaluated for
fixed sN . Invoking the chain rule, we find
</p>
<p>&part;φ
</p>
<p>&part;V
=
</p>
<p>dL
</p>
<p>dV
</p>
<p>&part;φ
</p>
<p>&part;L
=
</p>
<p>L
</p>
<p>3V
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>&part; ri
&part;L
</p>
<p>&middot;&nabla;iφ(rN) =
1
</p>
<p>3V
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ri &middot;&nabla;iφ(rN) , (3.184)
</p>
<p>where
</p>
<p>&nabla;iφ
.
=
</p>
<p>(
&part;φ
</p>
<p>&part;xi
,
&part;φ
</p>
<p>&part;yi
,
&part;φ
</p>
<p>&part; zi
</p>
<p>)
(3.185)
</p>
<p>is the negative of the net force exerted on the ith particle by the other particles in the
</p>
<p>system. Defining the internal virial by
</p>
<p>V :=&minus;1
3
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ri &middot;&nabla;iφ , (3.186)
</p>
<p>we have
</p>
<p>P = Pid +
〈V 〉
V
</p>
<p>. (3.187)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.14 &Dagger;Expressions for P and &micro; 151
</p>
<p>It is convenient and often acceptable to assume the pairwise additivity of φ .
That is, we assume that φ may be written as
</p>
<p>φ(rN) =
1
</p>
<p>2
</p>
<p>N
</p>
<p>&sum;
j=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>k=1
</p>
<p>v(r jk) , (3.188)
</p>
<p>where v is called the pair potential and &prime; on the second summation sign indicates
that the k = j term is excluded from the sum. We also defined
</p>
<p>r jk := r j &minus; rk (3.189)
</p>
<p>and made the usual assumption that the potential energy of interaction between jth
</p>
<p>and kth particles depends only on the distance r jk := ||r jk|| between them.
In order to compute &nabla;iφ in the internal virial, we must first isolate ri-dependent
</p>
<p>terms in (3.188). These are the terms corresponding to either j = i or k = i. So, the
quantity of our interest is
</p>
<p>1
</p>
<p>2
&sum;
k 	=i
</p>
<p>v(rik)+
1
</p>
<p>2
&sum;
j 	=i
</p>
<p>v(r ji) . (3.190)
</p>
<p>The first term is obtained by retaining only the j = i term in the summation over
j and noting that the condition k 	= j on the second sum translates to k 	= i for this
term. Similarly for the second term. Since k in the first term of (3.190) is just a
</p>
<p>dummy index, it can be replaced by j without affecting its value. We also note that
</p>
<p>r ji = ri j in the second term. So, we can rewrite (3.190) as
</p>
<p>&sum;
j 	=i
</p>
<p>v(ri j) , (3.191)
</p>
<p>which is just the sum of v for all the pairwise interactions involving the ith particle.
</p>
<p>This is exactly what the phrase &ldquo;pairwise additivity&rdquo; suggests. In any case, we now
</p>
<p>have
</p>
<p>&nabla;iφ(r
N) =&sum;
</p>
<p>j 	=i
&nabla;iv(ri j) , (3.192)
</p>
<p>indicating simply that the net force exerted on the ith particle by the rest of the
</p>
<p>particles in the system is given by the sum of the individual contribution from each
</p>
<p>particle.
</p>
<p>The expression &nabla;iv(ri j) can be evaluated as follows. From (3.189) and
</p>
<p>ri j
2 = xi j
</p>
<p>2 + yi j
2 + zi j
</p>
<p>2 , (3.193)
</p>
<p>we see that
</p>
<p>2ri j
&part; ri j
&part;xi
</p>
<p>= 2xi j
&part;xi j
&part;xi
</p>
<p>= 2xi j . (3.194)</p>
<p/>
</div>
<div class="page"><p/>
<p>152 3 Classical Statistical Mechanics
</p>
<p>So,
&part; ri j
&part;xi
</p>
<p>=
xi j
</p>
<p>ri j
. (3.195)
</p>
<p>Noting the similar relations for &part; ri j/&part;yi and &part; ri j/&part; zi, we have
</p>
<p>&nabla;iri j =
ri j
</p>
<p>ri j
, (3.196)
</p>
<p>Thus, by means of the chain rule, we have
</p>
<p>&nabla;iv(ri j) =
dv(ri j)
</p>
<p>dri j
</p>
<p>ri j
</p>
<p>ri j
. (3.197)
</p>
<p>Combining everything,
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ri &middot;&nabla;iφ =
N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>dv(ri j)
</p>
<p>dri j
</p>
<p>ri j
</p>
<p>ri j
&middot; ri =
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>dv(r ji)
</p>
<p>dr ji
</p>
<p>r ji
</p>
<p>r ji
&middot; r j . (3.198)
</p>
<p>Adding the last two alternative expressions for the same quantity and noting that
</p>
<p>r ji =&minus;ri j while ri j = r ji, we find
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ri &middot;&nabla;iφ =
1
</p>
<p>2
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>dv(ri j)
</p>
<p>dri j
ri j . (3.199)
</p>
<p>So, for a pairwise additive potential, the internal virial is
</p>
<p>V =&minus;1
6
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>dv(ri j)
</p>
<p>dri j
ri j (3.200)
</p>
<p>and we finally arrive at the desired expression for P:
</p>
<p>P = Pid &minus; 1
6V
</p>
<p>&lang;
N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>dv(ri j)
</p>
<p>dri j
ri j
</p>
<p>&rang;
. (3.201)
</p>
<p>3.14.2 &Dagger;Chemical Potential
</p>
<p>According to (2.193),
</p>
<p>&micro; =
</p>
<p>(
&part;F
</p>
<p>&part;N
</p>
<p>)
</p>
<p>T,V
</p>
<p>. (3.202)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.14 &Dagger;Expressions for P and &micro; 153
</p>
<p>Since the number of molecules can only be a nonnegative integer, the partial deriva-
</p>
<p>tive with respect to N is not well defined. Since N &asymp; 1024 ≫ 1, we may write
</p>
<p>&micro; = F(T,V,N +1)&minus;F(T,V,N) =&minus;kBT ln
ZN+1
</p>
<p>ZN
, (3.203)
</p>
<p>where the subscripts N+1 and N refer to the number of molecules in the system. (In
Chap. 2, we used N to denote the number of moles. In this chapter, the same symbol
</p>
<p>represents the number of molecules in the system. This difference simply translates
</p>
<p>to the change in units of &micro; . This is why we have kB instead of the gas constant R in
(3.203).)
</p>
<p>To evaluate the ratio ZN+1/ZN , let
</p>
<p>∆(rN+1) := φ(rN+1)&minus;φ(rN) (3.204)
</p>
<p>denote the change in the potential energy when the N +1th particle is added to the
system of N particles at the position rN+1. Then,
</p>
<p>ZN+1
</p>
<p>ZN
=
</p>
<p>1
</p>
<p>Λ 3(N +1)
</p>
<p>&int;
e&minus;βφ(r
</p>
<p>N+1)drN+1
&int;
</p>
<p>e&minus;βφ(rN)drN
</p>
<p>=
1
</p>
<p>Λ 3(N +1)
</p>
<p>&int;
e&minus;βφ(r
</p>
<p>N)
[&int;
</p>
<p>e&minus;β∆drN+1
]
</p>
<p>drN
&int;
</p>
<p>e&minus;βφ(rN)drN
</p>
<p>=
V
</p>
<p>Λ 3(N +1)
</p>
<p>&lang;
1
</p>
<p>V
</p>
<p>&int;
e&minus;β∆drN+1
</p>
<p>&rang;
, (3.205)
</p>
<p>where we suppressed rN+1 dependence of ∆ , a convention we shall continue to
follow throughout the remainder of this section. Using this expression in (3.203),
</p>
<p>we see that
</p>
<p>&micro; =&minus;kBT ln
V
</p>
<p>Λ 3(N +1)
&minus; kBT ln
</p>
<p>&lang;
1
</p>
<p>V
</p>
<p>&int;
e&minus;β∆drN+1
</p>
<p>&rang;
, (3.206)
</p>
<p>in which
</p>
<p>&minus; kBT ln
V
</p>
<p>Λ 3(N +1)
(3.207)
</p>
<p>is the chemical potential of an ideal gas. This expression is obtained by applying
</p>
<p>(3.203) to the ideal gas and should be compared against
</p>
<p>&micro; id =&minus;kBT ln
V
</p>
<p>Λ 3N
(3.208)
</p>
<p>which follows from (3.202). (See Exercise 3.16b.) Equation (3.206) is the basis of
</p>
<p>Widom&rsquo;s test particle insertion method, a common approach for estimating the
</p>
<p>chemical potential in molecular simulation.</p>
<p/>
</div>
<div class="page"><p/>
<p>154 3 Classical Statistical Mechanics
</p>
<p>We now derive an alternative expression for the chemical potential. For this pur-
</p>
<p>pose, let us define
</p>
<p>φ(λ ) := φ(rN)+λ∆ . (3.209)
</p>
<p>Clearly,
</p>
<p>φ(0) = φ(rN) and φ(1) = φ(rN+1) . (3.210)
</p>
<p>If we define
</p>
<p>Z(λ ) :=
1
</p>
<p>Λ 3NN!
</p>
<p>&int;
e&minus;βφ(λ )drN+1 , (3.211)
</p>
<p>we see that
</p>
<p>Z(0) =V ZN and Z(1) =Λ
3(N +1)ZN+1 , (3.212)
</p>
<p>where we used (3.177). It follows that
</p>
<p>ln
ZN+1
</p>
<p>ZN
= ln
</p>
<p>V
</p>
<p>Λ 3(N +1)
+ ln
</p>
<p>Z(1)
</p>
<p>Z(0)
. (3.213)
</p>
<p>But,
</p>
<p>ln
Z(1)
</p>
<p>Z(0)
= lnZ(1)&minus; lnZ(0) =
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>(
&part; lnZ(λ )
</p>
<p>&part;λ
</p>
<p>)
</p>
<p>β ,V,N
</p>
<p>dλ (3.214)
</p>
<p>From (3.211), (
&part; lnZ(λ )
</p>
<p>&part;λ
</p>
<p>)
</p>
<p>β ,V,N
</p>
<p>=&minus;β 〈∆〉λ , (3.215)
</p>
<p>where 〈&middot; &middot; &middot; 〉λ is the thermal average when the interparticle potential energy is φ(λ ).
Using (3.213) &ndash; (3.215) in (3.203), we obtain
</p>
<p>&micro; =&minus;kBT ln
V
</p>
<p>Λ 3(N +1)
+
</p>
<p>&int; 1
</p>
<p>0
〈∆〉λdλ . (3.216)
</p>
<p>This is an example of the thermodynamic integration method, in which a thermo-
</p>
<p>dynamic quantity is expressed in terms of an integration of some thermal average.
</p>
<p>The connection between (3.206) and (3.216) can be made clearer by considering
</p>
<p>an intermediate approach between them. Thus, let
</p>
<p>λi :=
i
</p>
<p>k
, i = 0, . . . ,k (3.217)
</p>
<p>so that λ0 = 0 and λk = 1. Then,
</p>
<p>Z(1)
</p>
<p>Z(0)
=
</p>
<p>Z(λk)
</p>
<p>Z(λ0)
=
</p>
<p>Z(λk)
</p>
<p>Z(λk&minus;1)
Z(λk&minus;1)
Z(λk&minus;2)
</p>
<p>&middot; &middot; &middot; Z(λi+1)
Z(λi)
</p>
<p>&middot; &middot; &middot; Z(λ1)
Z(λ0)
</p>
<p>. (3.218)
</p>
<p>But,
Z(λi+1)
</p>
<p>Z(λi)
=
</p>
<p>&int;
e&minus;βφ(λi+1)drN+1
&int;
</p>
<p>e&minus;βφ(λi)drN+1
= 〈e&minus;β∆/k〉λi . (3.219)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.15 &dagger;Internal Energy 155
</p>
<p>For a sufficiently large k, e&minus;β∆/k &asymp; 1&minus;β∆/k, and hence
</p>
<p>Z(λi+1)
</p>
<p>Z(λi)
&asymp; 1&minus; β
</p>
<p>k
〈∆〉λi . (3.220)
</p>
<p>Because ln(1&minus; x)&asymp;&minus;x if |x| ≪ 1, we have
</p>
<p>ln
Z(1)
</p>
<p>Z(0)
&asymp;
</p>
<p>k&minus;1
&sum;
i=0
</p>
<p>ln
</p>
<p>(
1&minus; β
</p>
<p>k
〈∆〉λi
</p>
<p>)
&asymp;&minus;
</p>
<p>k&minus;1
&sum;
i=0
</p>
<p>β
</p>
<p>k
〈∆〉λi &asymp;&minus;β
</p>
<p>&int; 1
</p>
<p>0
〈∆〉λdλ , (3.221)
</p>
<p>which is just (3.214) and we recover (3.216). One may say that the test particle
</p>
<p>insertion method, if broken down to a series of &ldquo;partial particle insertions,&rdquo; reduces
</p>
<p>to the thermodynamic integration method.
</p>
<p>3.15 &dagger;Internal Energy
</p>
<p>In Sect. 3.9, we identified the average Hamiltonian 〈H〉 with the internal energy
U . However, U is usually defined as a part of 〈H〉 reflecting only the internal state
of the system. The kinetic energy due to its macroscopic translational motion as a
</p>
<p>whole and the potential energy due to its position in an external field are excluded
</p>
<p>in defining U .
</p>
<p>Let us examine in details what is involved in this separation. What we have in
</p>
<p>mind is a collection of N particles that are held together either by intermolecular
</p>
<p>forces or by a container. In the latter case, we shall regard the particles making up
</p>
<p>the container as a part of the system. Then, the system as a whole experiences a
</p>
<p>translational motion under the influence of an external field ψ .
</p>
<p>3.15.1 &dagger;Equilibrium in Motion?
</p>
<p>It might be argued that a system in motion is not in equilibrium. However, a system
</p>
<p>in motion in one frame of reference is also at rest in another. If there is no time-
</p>
<p>dependent external field in the second, the system will eventually reach equilibrium.
</p>
<p>Our goal here is to describe that equilibrium from the first frame of reference with
</p>
<p>respect to which the system is in macroscopic motion.
</p>
<p>Let us choose an inertial frame Oa, with respect to which the system is moving,
</p>
<p>and write down its Lagrangian La as
</p>
<p>La =
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mi||vai ||2 &minus;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ψ(rai ,mi)&minus;φ(ra1, . . . ,raN) , (3.222)</p>
<p/>
</div>
<div class="page"><p/>
<p>156 3 Classical Statistical Mechanics
</p>
<p>where we use the superscript a to signify that the quantities bearing them are evalu-
</p>
<p>ated in Oa. From the outset, we assume that ψ has the following special form
</p>
<p>ψ(ri,mi) =&minus;mib &middot; ri , (3.223)
</p>
<p>where b is a constant. In the case of the gravitational field, b= g, for example.
</p>
<p>Exercise 3.18. Find equations of motion for the particles. ///
</p>
<p>By definition, (1.103), the momentum pai conjugate to r
a
i is given by
</p>
<p>pai :=
&part;La
</p>
<p>&part;vai
= miv
</p>
<p>a
i . (3.224)
</p>
<p>The Hamiltonian follows from (1.156) as
</p>
<p>Ha :=
N
</p>
<p>&sum;
i=1
</p>
<p>pai &middot; vai &minus;La =
N
</p>
<p>&sum;
i=1
</p>
<p>||pai ||2
2mi
</p>
<p>+
N
</p>
<p>&sum;
i=1
</p>
<p>ψ(rai ,mi)+φ(r
a
1, . . . ,r
</p>
<p>a
N) . (3.225)
</p>
<p>Note that the Hamiltonian also bears the superscript a because it is expressed in
</p>
<p>terms of the position and the momenta of particles as measured in Oa.
</p>
<p>If we assume a canonical distribution, can we say that
</p>
<p>ρa =
e&minus;βH
</p>
<p>a
</p>
<p>Za
? (3.226)
</p>
<p>Because of the macroscopic motion with respect to Oa, when our system occu-
</p>
<p>pies a certain region in Oa, it does so only once and only for a brief interval of time.
</p>
<p>From this observation, it might seem that ρa vanishes everywhere in the phase space
because ∆ t in (3.11) becomes vanishingly small in comparison to τ . This is not quite
correct. We must remember that τ is supposed to be very long only in comparison
to the time scale of molecular motion but comparable with the time scale of our
</p>
<p>measurement. If l/||V|| ≫ τ , where l is the characteristic length scale of the sys-
tem and V is the velocity of the system, the system may be regarded as occupying
</p>
<p>essentially the same region in Oa over the duration τ and ρa takes nonzero values
over the corresponding region of the phase space. However, this region of nonzero
</p>
<p>ρa continues to move in the phase space in accordance with the translational motion
of the system in Oa.
</p>
<p>The immediate conclusion is that (3.23), or &part;ρa/&part; t &equiv; 0, is not an appropriate
definition for statistical equilibrium. How do we define statistical equilibrium of our
</p>
<p>system then? It is also unsatisfactory that we have to impose an upper limit on the
</p>
<p>velocity of the macroscopic motion when discussing statistical equilibrium.
</p>
<p>We note that the motion we are considering can be eliminated entirely in a suit-
</p>
<p>ably chosen frame of reference. So, the difficulty we have just described is only
</p>
<p>apparent and can be made to disappear by introducing a new coordinate system O
</p>
<p>that moves with the system. One might say, then, that the system is in statistical
</p>
<p>equilibrium if there is a frame of reference in which &part;ρ/&part; t &equiv; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.15 &dagger;Internal Energy 157
</p>
<p>We expect equilibrium to eventually prevail in O if there is no time-dependent
</p>
<p>field in O . Assuming that there is a thermal contact between the system and its
</p>
<p>surroundings, ρ at equilibrium will be given by the canonical distribution. Thus, we
will first have to find the Hamiltonian H of the system that is appropriate for O .
</p>
<p>Then, the only remaining problem is that of translating ρ in O back to ρa in Oa.
This is the plan we follow.
</p>
<p>3.15.2 &dagger;From a Stationary to a Moving Frame
</p>
<p>We denote the velocity of O with respect to Oa by V(t). The time dependence of V
is included because the system, in general, experiences acceleration with respect to
</p>
<p>O
a in the presence of an external field. This implies that O is not an inertial frame
</p>
<p>in general.
</p>
<p>In order to find the Lagrangian L and the Hamiltonian H written for O , we need
</p>
<p>to figure out the relationship between the position ra and the velocity va of a particle
</p>
<p>in Oa on the one hand and their counterparts, r and v in O , on the other. Clearly,
</p>
<p>ra = r+R , (3.227)
</p>
<p>where R is the position of the origin of O as measured in Oa. To find the relation
</p>
<p>between va and v, suppose that, after a time duration dt, the position of the particle
</p>
<p>in Oa is ra +dra. The location of this same particle in O may be denoted by r+dr.
At the same time, however, O itself has moved by Vdt. Thus, applying (3.227) to
</p>
<p>the varied state,
</p>
<p>ra +dra = (r+dr)+(R+Vdt) . (3.228)
</p>
<p>Subtracting (3.227) from (3.228), and then dividing the resulting equation by dt, we
</p>
<p>find
</p>
<p>va = v+V , (3.229)
</p>
<p>which could have been obtained simply by taking the time derivative of (3.227).
</p>
<p>Equation (3.227) and ta = t taken together is called a Galilean transformation.
Substituting (3.227) and (3.229) into (3.222), we find
</p>
<p>La =
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mi||vi||2 &minus;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ψ(ri,mi)&minus;φ(rN)+V &middot;
N
</p>
<p>&sum;
i=1
</p>
<p>mivi +
1
</p>
<p>2
M||V||2 +Mb &middot;R ,
</p>
<p>(3.230)
</p>
<p>where we used (3.223) to rewrite the external field term. As in Sect. 1.4, M :=&sum;i mi
is the total mass of the system.
</p>
<p>We now assume that R and V may be regarded as explicit functions of t only.
</p>
<p>That is, we suppose that they do not depend on ra1, . . . , v
a
N . Whether this actually
</p>
<p>is the case depends on how we choose R and will be discussed in Sect. 3.15.3.
</p>
<p>Under this assumption, the right-hand side of (3.230) is a function of t and the
</p>
<p>variables pertaining only to O , and hence it may be used as the Lagrangian L in O</p>
<p/>
</div>
<div class="page"><p/>
<p>158 3 Classical Statistical Mechanics
</p>
<p>from which equations of motion appropriate for O can be derived. These equations
</p>
<p>should be equivalent to those obtained from La for Oa. In fact, since La &equiv; L for any
mechanical state of the system at any instant, the action integral S a computed in Oa
</p>
<p>is numerically equal to the action integral S in O . Thus, the stationarity condition
</p>
<p>of the former coincides with that of the latter.
</p>
<p>Exercise 3.19. Verify this statement by deriving equations of motion from (3.230)
</p>
<p>and comparing your results against what you found in Exercise 3.18. ///
</p>
<p>For this choice of L, (1.104) yields
</p>
<p>pi =
&part;L
</p>
<p>&part;vi
= mi(vi +V) . (3.231)
</p>
<p>Recalling (1.156), we have
</p>
<p>H =
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2mi
</p>
<p>+
N
</p>
<p>&sum;
i=1
</p>
<p>ψ(ri,mi)+φ(r
N)&minus;V &middot;P&minus;Mb &middot;R , (3.232)
</p>
<p>where P := &sum;Ni=1 pi is, by (1.138), the total linear momentum of the system. We
observe that unless R is constant and hence V &equiv; 0, this H depends explicitly on
time. It is not at all clear if such a system can reach equilibrium even to an observer
</p>
<p>in O . It is possible, however, to use somewhat different expressions for L and H.
</p>
<p>First, we recall that the last two terms of (3.230) are, by our assumption, functions
</p>
<p>of t only, and hence can be expressed as the total time derivative of some function
</p>
<p>of t. By Exercise 1.8, they can be dropped without affecting the resulting equations
</p>
<p>of motion. Next, we rewrite the term linear in V in (3.230) by noting that
</p>
<p>V &middot; vi =
d
</p>
<p>dt
(V &middot; ri)&minus;
</p>
<p>dV
</p>
<p>dt
&middot; ri . (3.233)
</p>
<p>The first term on the right-hand side can be dropped from the Lagrangian for the
</p>
<p>same reason. In this way, we see that the Lagrangian L may be redefined as
</p>
<p>L :=
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mi||vi||2 &minus;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ψ(ri,mi)&minus;φ(rN)&minus;
dV
</p>
<p>dt
&middot;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>miri . (3.234)
</p>
<p>Exercise 3.20. Derive equations of motion using this Lagrangian. ///
</p>
<p>With this Lagrangian, we have
</p>
<p>pi =
&part;L
</p>
<p>&part;vi
= mivi (3.235)
</p>
<p>and
</p>
<p>H =
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2mi
</p>
<p>+
N
</p>
<p>&sum;
i=1
</p>
<p>ψ(ri,mi)+φ(r
N)+
</p>
<p>dV
</p>
<p>dt
&middot;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>miri . (3.236)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.15 &dagger;Internal Energy 159
</p>
<p>Because of (3.235), the first term of this H is the kinetic energy of the system as
</p>
<p>measured in O . This situation should be contrasted to that in (3.232), for which
</p>
<p>such an interpretation does not hold due to the presence of V in (3.231).
</p>
<p>Up to this point, our choice of R was quite arbitrary. When discussing macro-
</p>
<p>scopic motion of a collection of particles, it is often convenient to place O at the
</p>
<p>center of mass of the system. One often points to this choice of the coordinate sys-
</p>
<p>tem when defining the internal energy of a moving system in thermodynamics.
</p>
<p>In this case, R is equal to the position of the center of mass as measured in Oa,
</p>
<p>which we denote by Rcm:
</p>
<p>R&equiv; Rcm =
1
</p>
<p>M
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mir
a
i =
</p>
<p>1
</p>
<p>M
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mi(ri +Rcm) , (3.237)
</p>
<p>where we recall that a container, if present, must be included in computing Rcm.
</p>
<p>From this equation, it follows that
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>miri &equiv; 0 (3.238)
</p>
<p>and that
N
</p>
<p>&sum;
i=1
</p>
<p>ψ(ri,mi)&equiv; 0 . (3.239)
</p>
<p>When these results are substituted into (3.234) and (3.236), we find
</p>
<p>L =
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mi||vi||2 &minus;φ(rN) and H =
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2mi
</p>
<p>+φ(rN) . (3.240)
</p>
<p>These equations indicate that the effect of the external field vanishes in a coordinate
</p>
<p>system that moves with the center of mass of the system. It should be emphasized
</p>
<p>that we reached this conclusion only for the external field having the form of (3.223)
</p>
<p>and only by making a rather special choice of L in (3.234). Nevertheless, (3.240)
</p>
<p>represent a very satisfying state of affair. Both L and H are well defined without any
</p>
<p>reference to the motion of O relative to some inertial frame Oa that is external to O .
</p>
<p>For a given mechanical state of the system, the value of Ha evaluated by (3.225)
</p>
<p>and that of H given by (3.240) are, in general, different. This is not surprising and,
</p>
<p>in fact, is expected on the basis of Exercise 1.9. It is straightforward to show that
</p>
<p>Ha = H +
1
</p>
<p>2
M||Vcm||2 +ψ(Rcm,M) , (3.241)
</p>
<p>where Vcm := dRcm/dt. In this form, we can clearly see the separation of H
a into
</p>
<p>something &ldquo;intrinsic&rdquo; to the system and the remaining terms due to its macroscopic
</p>
<p>motion.</p>
<p/>
</div>
<div class="page"><p/>
<p>160 3 Classical Statistical Mechanics
</p>
<p>Exercise 3.21. Verify (3.241). ///
</p>
<p>Now that O moves with Rcm, the region of nonzero ρ will be stationary in the
phase space for O and the condition of equilibrium may be expressed as &part;ρ/&part; t &equiv; 0.
For a system in thermal contact with the surroundings, we have ρ &prop; e&minus;βH with H
given by (3.240).
</p>
<p>In writing the statistical distribution ρ for an observer in O , however, we must
remember that the center of mass of the system is fixed at its origin. As a result, rN
</p>
<p>must satisfy (3.238), and hence are not all independent. Because the equation holds
</p>
<p>at all t, it also holds upon taking the time derivative. These constraints are most
</p>
<p>easily imposed by means of the Dirac δ -function:
</p>
<p>ρcm(r
N ,pN) = δ [&middot; &middot; &middot; ] 1
</p>
<p>Zcm
</p>
<p>e&minus;βH
</p>
<p>h3(N&minus;1)P
, (3.242)
</p>
<p>where we introduced a shorthand notation
</p>
<p>δ [&middot; &middot; &middot; ] := δ
(
&sum;Ni=1 miri
</p>
<p>M
</p>
<p>)
δ
</p>
<p>(
N
</p>
<p>&sum;
i=1
</p>
<p>pi
</p>
<p>)
(3.243)
</p>
<p>for the product of two Dirac δ -functions. (See Appendix D.4 for the definition of the
three-dimensional δ -function.) As before, P is the number of distinct permutations
of identical particles if there are any and the subscript cm remind us of the con-
</p>
<p>straints on the center of mass. Because of the constraints, the mechanical degrees of
</p>
<p>freedom of the system is reduced by 3 from the original value of 3N. This accounts
</p>
<p>for the factor h3(N&minus;1) in place of now familiar h3N . Finally, the normalization con-
stant is given by
</p>
<p>Zcm =
1
</p>
<p>h3(N&minus;1)P
</p>
<p>&int;
δ [&middot; &middot; &middot; ]e&minus;βHdrNdpN . (3.244)
</p>
<p>3.15.3 &dagger;Back to the Stationary Frame
</p>
<p>With the expression of ρcm now in hand, we can determine ρacm as follows. We first
note that the quantity
</p>
<p>ρcm(r
N ,pN)drNdpN (3.245)
</p>
<p>represents the probability of finding the system within the infinitesimal volume ele-
</p>
<p>ment drNdpN taken around the phase point (rN ,pN) of the phase space for O . But,
as seen from (3.224), (3.227), (3.229), and (3.235), the Jacobian of transformation
</p>
<p>from (ra1, . . . ,p
a
N) to (r
</p>
<p>N ,pN) is unity, that is, the volume occupied by drNdpN in the
phase space for O is equal to the volume occupied by dra1 &middot; &middot; &middot;dpaN in the phase space
for Oa. It follows that the probability (3.245) can also be written as
</p>
<p>ρacm(r
a
1, . . . ,p
</p>
<p>a
N)dr
</p>
<p>a
1 &middot; &middot; &middot;dpaN (3.246)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.15 &dagger;Internal Energy 161
</p>
<p>with ρacm given simply by ρcm expressed in terms of r
a
1, . . . ,p
</p>
<p>a
N . Thus,
</p>
<p>ρacm(r
a
1, . . . ,p
</p>
<p>a
N) = δ
</p>
<p>a[&middot; &middot; &middot; ] 1
Zacm
</p>
<p>e&minus;β (H
a&minus;Emm)
</p>
<p>h3(N&minus;1)P
, (3.247)
</p>
<p>where we defined
</p>
<p>δ a[&middot; &middot; &middot; ] := δ
(
&sum;Ni=1 mir
</p>
<p>a
i
</p>
<p>M
&minus;Rcm(t)
</p>
<p>)
δ
</p>
<p>(
N
</p>
<p>&sum;
i=1
</p>
<p>pai &minus;Pcm(t)
)
</p>
<p>(3.248)
</p>
<p>with Pcm = MVcm and
</p>
<p>Emm =
1
</p>
<p>2
M||Vcm||2 +ψ(Rcm,M) (3.249)
</p>
<p>is the energy due to macroscopic motion as observed in Oa. The normalization of
</p>
<p>ρacm leads to
</p>
<p>Zacm =
1
</p>
<p>h3(N&minus;1)P
</p>
<p>&int;
δ a[&middot; &middot; &middot; ]e&minus;β (Ha&minus;Emm)dra1 &middot; &middot; &middot;dpaN , (3.250)
</p>
<p>which is equal to Zcm numerically. The δ -functions ensure that only those phase
points consistent with the given functions Rcm(t) and Pcm(t) are accounted for.
</p>
<p>We still have to show that Rcm(t) and Vcm(t) may be regarded as some pre-
scribed function depending only on t. Using the equations of motion derived in
</p>
<p>Exercise 3.18, we obtain
</p>
<p>Vcm(t) = bt + c0 and Rcm(t) =
1
</p>
<p>2
bt2 + c0t + c1 , (3.251)
</p>
<p>where c0 and c1 are constant vectors to be determined by Vcm and Rcm at time t = 0.
Given these initial conditions, the subsequent evolution of Rcm and Vcm is given by
</p>
<p>(3.251), and hence they are explicit functions of t only. This is what we needed to
</p>
<p>show.
</p>
<p>Because of the interaction between the system and the surroundings, the actual
</p>
<p>time evolution of Rcm and Vcm deviates from the predictions of (3.251) in a manner
</p>
<p>that depends on rN and pN . Strictly speaking, therefore, the assumption we intro-
</p>
<p>duced after (3.230) does not hold and the separation of 〈H〉 into Ucm and Emm must
be regarded as an approximation.
</p>
<p>Exercise 3.22. Derive (3.251). ///
</p>
<p>Finally, when the contribution of the macroscopic motion is separated out, we
</p>
<p>define the internal energy Ucm as 〈H〉 computed with respect to ρcm:
</p>
<p>Ucm =&minus;
&part; lnZcm
&part;β
</p>
<p>. (3.252)</p>
<p/>
</div>
<div class="page"><p/>
<p>162 3 Classical Statistical Mechanics
</p>
<p>Since ρcm and ρacm are numerically equal, this may also be written as
</p>
<p>Ucm =&minus;
&part; lnZacm
&part;β
</p>
<p>(3.253)
</p>
<p>We note that Ucm reflects the average behavior of 3(N &minus; 1) mechanical degrees of
freedom. The macroscopic motion of the center of mass accounts for the remaining
</p>
<p>three degrees of freedom.
</p>
<p>3.16 &dagger;Equilibrium of an Accelerating Body
</p>
<p>If we enclose a system of N particles in a rigid container and make the container
</p>
<p>move in the manner we prescribe, the system will, in general, experience accel-
</p>
<p>eration. Insofar as the acceleration is prescribed, it is common to all members of
</p>
<p>the ensemble. If we limit our consideration to a constant linear acceleration, its
</p>
<p>effect will be seen to manifest itself as a time-independent external field. This allows
</p>
<p>for a statistical equilibrium to establish. The same holds for a uniform rotation as
</p>
<p>observed in centrifugation. In this section, we shall derive the appropriate expres-
</p>
<p>sions for the statistical weight ρ for these cases. We attach a moving coordinate
system O to the container. Because the container is then fixed in O , there is no need
</p>
<p>to include the container in our definition of the system.
</p>
<p>3.16.1 &dagger;Linear Translation
</p>
<p>We may take (3.236) as our starting point and rewrite it using (3.223):
</p>
<p>H = H0 +
N
</p>
<p>&sum;
i=1
</p>
<p>ψ (ri,mi)+A &middot;
N
</p>
<p>&sum;
i=1
</p>
<p>miri = H0 +(A&minus;b) &middot;
N
</p>
<p>&sum;
i=1
</p>
<p>miri , (3.254)
</p>
<p>where A := dV/dt is the acceleration of the container and
</p>
<p>H0 :=
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2mi
</p>
<p>+φ
(
rN
)
</p>
<p>(3.255)
</p>
<p>is the part of the Hamiltonian that appears intrinsic to an observer in O as was
</p>
<p>pointed out following (3.236).
</p>
<p>We see from (3.254) that the effect of the acceleration A of the container manifest
</p>
<p>itself as an effective external field and acts only to modify the existing external field
</p>
<p>ψ . If A does not depend on t, that is, for a constant linear acceleration, H has no
explicit time dependence and we expect the system to reach a statistical equilibrium.
</p>
<p>We note that the coordinate system O is attached to the container wall and not to</p>
<p/>
</div>
<div class="page"><p/>
<p>3.16 &dagger;Equilibrium of an Accelerating Body 163
</p>
<p>a, xa
</p>
<p>ya
</p>
<p>xy
</p>
<p>W
ater
</p>
<p>Fig. 3.7 A rotating bucket of water as observed from the inertial frame Oa and the frame O that
</p>
<p>rotates with the bucket.
</p>
<p>the center of mass of the particles. This means that the constraint (3.243) is absent
</p>
<p>here. Instead of (3.242), therefore, we have
</p>
<p>ρ
(
rN ,pN
</p>
<p>)
=
</p>
<p>1
</p>
<p>Z
</p>
<p>e&minus;βH
</p>
<p>h3NP
and Z =
</p>
<p>1
</p>
<p>h3NP
</p>
<p>&int;
e&minus;βHdrNdpN (3.256)
</p>
<p>for a system in thermal contact with the surroundings.
</p>
<p>3.16.2 &dagger;Rotation
</p>
<p>Let us consider the case of rotation without any linear translation. Once again, we
</p>
<p>adopt a frame of reference O that is attached to the container rotating with respect
</p>
<p>to the inertial frame Oa. For simplicity, we assume that these two frames share the
</p>
<p>same origin. Figure 3.7 illustrates the situation.
</p>
<p>The frame O clearly is not an inertial frame: A particle at rest in the rotating
</p>
<p>frame is accelerating with respect to the inertial frame. In general, the effect of
</p>
<p>acceleration is felt as apparent forces. In the case of rotation, they are the centrifu-
</p>
<p>gal force, the Coriolis force, and the Euler force as we shall see in Example 3.8.
</p>
<p>This last terminology is due to Ref. [2].
</p>
<p>It is clear that an external field ψ that is time independent in Oa varies with time
in O in general. In this section, therefore, we assume that the effect of ψ is negligible
in comparison to the intermolecular forces and the apparent forces.
</p>
<p>Again, we need to establish the relationship between the velocity vector of a
</p>
<p>particle as measured in Oa and that in O . For this purpose, suppose that the posi-
</p>
<p>tion vector of a particle, as measured in O , changed from r to r+ dr during a time
duration dt. If we denote the same vectors as observed in Oa by ra and ra + dra,
respectively, then,
</p>
<p>r= ra (3.257)</p>
<p/>
</div>
<div class="page"><p/>
<p>164 3 Classical Statistical Mechanics
</p>
<p>because both r and ra refer to the same arrow in space. But the final vector r+dr is
not equal to ra +dra because of the rotation of O with respect to Oa. We stress that
dra is the change as observed in Oa of the vector ra. This change is brought about
</p>
<p>by two contributions, one is the change dr as observed in O of the vector r and the
</p>
<p>other is the rotation of O with respect to Oa. This is why dra 	= dr.
To make things easier, let us first suppose that r remained constant in O . Denoting
</p>
<p>the rotation of O with respect to Oa by dφ as we have done in Sect. 1.8.2, we see
</p>
<p>that
</p>
<p>dra = dφ&times; r , (3.258)
and hence
</p>
<p>ra +dra = r+dφ&times; r . (3.259)
If r did not remain constant but became r+dr, we have only to replace r by r+dr
and obtain
</p>
<p>ra +dra = (r+dr)+dφ&times; (r+dr) . (3.260)
Now we cancel ra and r using (3.257) and divide the resulting expression by dt to
</p>
<p>arrive at
</p>
<p>va = v+Ω&times; r , (3.261)
where we ignored the higher order term dφ&times; dr. The new vector Ω := dφ/dt is
called the angular velocity of O with respect to Oa.
</p>
<p>Using (3.257) and (3.261) in (3.222) with ψ &equiv; 0, we find
</p>
<p>La =
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mi||vi||2 &minus;φ
</p>
<p>(
rN
)
+
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>mivi &middot;Ω&times; ri +
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mi||Ω&times; ri||2 . (3.262)
</p>
<p>Example 3.8. Apparent forces: Let N = 1 in (3.262):
</p>
<p>La =
1
</p>
<p>2
m||v||2 +mv &middot;Ω&times; r+ 1
</p>
<p>2
m||Ω&times; r||2 , (3.263)
</p>
<p>from which we derive Lagrange&rsquo;s equation of motion of the particle in O . This
</p>
<p>allows us to identify the expressions for the apparent forces.
</p>
<p>From (3.263),
&part;La
</p>
<p>&part;v
= mv+mΩ&times; r . (3.264)
</p>
<p>To compute &part;La/&part; r, we make use of (A.32):
</p>
<p>&part;
</p>
<p>&part; r
(v &middot;Ω&times; r) = &part;
</p>
<p>&part; r
(r &middot; v&times;Ω) = v&times;Ω . (3.265)
</p>
<p>We also note that
</p>
<p>||Ω&times; r||2 = ||Ω||2||r||2 sin2 θ = ||Ω||2||r||2(1&minus; cos2 θ) = ||Ω||2||r||2 &minus; (Ω &middot; r)2 .
(3.266)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.16 &dagger;Equilibrium of an Accelerating Body 165
</p>
<p>Thus,
</p>
<p>1
</p>
<p>2
</p>
<p>&part; ||Ω&times; r||2
&part; r
</p>
<p>= ||Ω||2r&minus; (Ω &middot; r)Ω= ||Ω||2(r&minus; r‖) = ||Ω||2r&perp; , (3.267)
</p>
<p>where we defined
</p>
<p>r‖ :=
Ω
</p>
<p>||Ω|| ||r||cosθ and r&perp; := r&minus; r‖ . (3.268)
</p>
<p>Geometrically, this decomposes r into a part (r‖) that is parallel to Ω and the
other (r&perp;) that is perpendicular to Ω as seen from Fig. 3.8.
</p>
<p>From Lagrange&rsquo;s equation of motion, we obtain
</p>
<p>mv̇+mΩ̇&times; r+mΩ&times; v= mv&times;Ω+m||Ω||2r&perp; . (3.269)
</p>
<p>Solving for mv̇,
</p>
<p>mv̇= 2mv&times;Ω+m||Ω||2r&perp;&minus;mΩ̇&times; r . (3.270)
</p>
<p>In this equation, the terms on the right are the apparent forces. From the left,
</p>
<p>they are the Coriolis force, the centrifugal force, and the Euler force. Being
</p>
<p>perpendicular to v, the Coriolis force does not perform any work on the parti-
</p>
<p>cle.
</p>
<p>Coming back to (3.262), we simply adopt it as the Lagrangian L in O without
</p>
<p>further modifications. Thus,
</p>
<p>pi :=
&part;L
</p>
<p>&part;vi
= mi(vi +Ω&times; ri) = mivai = pai , (3.271)
</p>
<p>and hence
</p>
<p>H :=
N
</p>
<p>&sum;
i=1
</p>
<p>pi &middot; vi &minus;L =
N
</p>
<p>&sum;
i=1
</p>
<p>pai &middot; (vai &minus;Ω&times; ri)&minus;La . (3.272)
</p>
<p>.
...........
</p>
<p>..........
</p>
<p>..........
...........
</p>
<p>......................
</p>
<p>r
</p>
<p>r
</p>
<p>r
</p>
<p>r
c
o
s
</p>
<p>Fig. 3.8 Decomposition of r into r&perp; and r‖.</p>
<p/>
</div>
<div class="page"><p/>
<p>166 3 Classical Statistical Mechanics
</p>
<p>Since the first and the third terms taken together is Ha, it may be replaced by the
</p>
<p>expression given in (3.225), where we recall that ψ &equiv; 0 in this section. Noting once
again that rai = ri and p
</p>
<p>a
i = pi, we find
</p>
<p>H =
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2mi
</p>
<p>+φ(rN)&minus;
N
</p>
<p>&sum;
i=1
</p>
<p>pi &middot;Ω&times; ri . (3.273)
</p>
<p>Using the vector identity (A.32) and the definition (1.145) of the total angular
</p>
<p>momentum, we have
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>pi &middot;Ω&times; ri =Ω &middot;
N
</p>
<p>&sum;
i=1
</p>
<p>ri &times;pi =Ω &middot;M , (3.274)
</p>
<p>and hence
</p>
<p>H =
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2mi
</p>
<p>+φ(rN)&minus;Ω &middot;M . (3.275)
</p>
<p>When Ω is independent of time, the rotation is said to be uniform. For a uniform
</p>
<p>rotation, H has no explicit time dependence and the system is expected to reach
</p>
<p>equilibrium eventually. The statistical distribution is then given by
</p>
<p>ρ(rN ,pN) =
1
</p>
<p>Z
</p>
<p>e&minus;βH
</p>
<p>h3NP
and Z =
</p>
<p>1
</p>
<p>h3NP
</p>
<p>&int;
e&minus;βHdrNdpN . (3.276)
</p>
<p>While pi is the generalized momentum conjugate to ri, it is not the mechanical
</p>
<p>momentum mivi an observer in O would compute based on the values of v
N mea-
</p>
<p>sured in that frame. The situation is entirely analogous to (3.231). To extract from
</p>
<p>H an energy term this observer would compute, we use (3.271) to eliminate pi from
</p>
<p>(3.275) in favor of the mechanical momentum defined by πi := mivi. This leads to
</p>
<p>E =
N
</p>
<p>&sum;
i=1
</p>
<p>||πi||2
2mi
</p>
<p>+φ(rN)&minus;
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mi||Ω&times; ri||2 =: E0 +ψcp , (3.277)
</p>
<p>where
</p>
<p>E0 :=
N
</p>
<p>&sum;
i=1
</p>
<p>||πi||2
2mi
</p>
<p>+φ(rN) and ψcp :=&minus;
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>2
mi||Ω&times; ri||2 . (3.278)
</p>
<p>The expression (3.277) is numerically equal to H in (3.275) but it is not a Hamil-
</p>
<p>tonian since E is given in terms of the mechanical rather than the generalized
</p>
<p>momenta. Nevertheless, the expression is illuminating. In particular, E0 is the energy
</p>
<p>an observer in O would compute based on the values of rN and πN measured in this
</p>
<p>frame without any regard to the fact that O is rotating. Equation (3.277) also makes
</p>
<p>it clear that the effect of rotation of our coordinate system manifests itself only
</p>
<p>through the apparent external field ψcp called the centrifugal potential energy.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.17 Frequently Used Symbols 167
</p>
<p>Exercise 3.23. Verify (3.277). ///
</p>
<p>Because of its intuitive appeal, it will be desirable to use E rather than H to
</p>
<p>express the statistical distribution. To find the functional form of this distribution,
</p>
<p>let
</p>
<p>ρ &prime;(rN ,πN)drNdπN (3.279)
</p>
<p>denote the probability of finding the system of interest within the infinitesimal vol-
</p>
<p>ume element drNdπN taken around the point (rN ,πN) in the space spanned by 2N
vectors rN and πN . From (3.271), we see that the Jacobian of the coordinate trans-
</p>
<p>formation from (rN ,pN) to (rN ,πN) is unity. As in Sect. 3.15.3, this implies that ρ &prime;
</p>
<p>is simply ρ expressed in terms of rN and πN . From (3.276), we arrive at
</p>
<p>ρ &prime;(rN ,πN) =
1
</p>
<p>Z&prime;
e&minus;β (E0+ψcp)
</p>
<p>h3NP
and Z&prime; :=
</p>
<p>1
</p>
<p>h3NP
</p>
<p>&int;
e&minus;β (E0+ψcp)drNdπN .
</p>
<p>(3.280)
</p>
<p>3.17 Frequently Used Symbols
</p>
<p>〈A〉 , ensemble average of a dynamical variable A.
</p>
<p>f , the number of mechanical degrees of freedom.
</p>
<p>h , Planck constant. 6.626&times;10&minus;34 (J&middot;sec).
kB , Boltzmann constant, 1.3806&times;10&minus;23 J/K.
mi , mass of the ith particle.
</p>
<p>pi , generalized momentum conjugate to qi.
</p>
<p>p f , collective notation for p1, . . . , p f .
</p>
<p>pi , linear momentum of the ith particle.
</p>
<p>pN , collective notation for p1, . . . , pN .
</p>
<p>qi , the ith generalized coordinate.
</p>
<p>q f , collective notation for q1, . . . , q f .
</p>
<p>ri , position vector of the ith particle.
</p>
<p>rN , collective notation for r1, . . . , rN .
</p>
<p>t , time.
</p>
<p>vi , velocity vector of the ith particle.
</p>
<p>vN , collective notation for v1, . . . , vN .
</p>
<p>A , a generic dynamical variable.
</p>
<p>C , normalization constant for ρ .
CV , constant volume heat capacity.
</p>
<p>F , Helmholtz free energy.
</p>
<p>H , Hamiltonian.
</p>
<p>L , Lagrangian.
</p>
<p>M , total mass of a many-particle system.</p>
<p/>
</div>
<div class="page"><p/>
<p>168 3 Classical Statistical Mechanics
</p>
<p>N , total number of particles in a system.
</p>
<p>S , entropy.
</p>
<p>T , absolute temperature.
</p>
<p>U , internal energy.
</p>
<p>V , volume.
</p>
<p>Z , canonical partition function.
</p>
<p>N , total number of copies in a statistical ensemble.
</p>
<p>P , the number of permutations of identical particles.
</p>
<p>V , internal virial.
</p>
<p>β , 1/kBT .
δ (x) , Dirac δ -function.
ρ , statistical weight.
π , mechanical momentum.
</p>
<p>φ , potential energy due to interparticle interactions.
ψ , potential energy due to an external field.
</p>
<p>Λ , thermal wavelength h/
&radic;
</p>
<p>2πmkBT of a particle of mass m.
Ω , angular velocity.
</p>
<p>References and Further Reading
</p>
<p>1. Gibbs J W (1981) Elementary principles in statistical mechanics. Ox Bow Press, Connecticut
Our Sect. 3.9 followed Chap. 4 of the book, where the canonical ensemble was introduced. The
</p>
<p>same chapter demonstrates the relationship between statistical mechanics and thermodynamics
</p>
<p>and provides further motivation for the canonical ensemble.
2. Lanczos C (1986) The variational principles of mechanics. Dover, New York
</p>
<p>The phrase &ldquo;Euler force&rdquo; is introduced in p. 103.
3. Landau L D, Lifshitz E M (1980) Statistical physics: Part 1, 3rd edn. Pergamon Press, New
</p>
<p>York
</p>
<p>In developing the basic principles of statistical mechanics, we loosely followed Chap. 1 of the
</p>
<p>book. Our treatment of a rotating body is based on their treatment of the subject, in particular,
</p>
<p>Sects. 26 and 34.
4. Schrödinger E (1989) Statistical thermodynamics. Dover, New York
</p>
<p>Chapter 3 of the book provides a detailed discussion on the meaning of setting the constant
</p>
<p>term in (3.67) to zero. He uses Boltzmann&rsquo;s entropy formula rather than that of Gibbs. But, the
</p>
<p>former can be derived from the latter as we shall see later.
5. Tolman R C (1979) The principles of statistical mechanics. Dover, New York
</p>
<p>For an extended discussion on the statistical approach and its validity, see Chap. 3. A brief
</p>
<p>summary is in Sect. 25.
6. Whitaker S (1992) Introduction to fluid mechanics. Krieger Publishing Company, Florida
</p>
<p>It is far more natural and physically compelling to write down laws of physics, such as the
</p>
<p>conservation of mass, Newton&rsquo;s equation of motion, and the first law of thermodynamics, for
</p>
<p>a (possibly moving and deforming) body than to do so for a control volume fixed in space.
</p>
<p>A derivation of the equation of continuity from this more satisfactory stand point is found in
</p>
<p>Chap. 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>Various Statistical Ensembles
</p>
<p>A canonical ensemble describes a system held at a constant temperature. From the
</p>
<p>canonical partition function follows the Helmholtz free energy. As we saw in ther-
</p>
<p>modynamics, however, it is highly desirable to be able to describe a system held
</p>
<p>under a different set of constraints, such as constant temperature and pressure or
</p>
<p>constant temperature and chemical potentials of some species. These situations call
</p>
<p>for different free energies in thermodynamics, to which correspond different statisti-
</p>
<p>cal ensembles in statistical mechanics. In this chapter, we construct such ensembles
</p>
<p>and illustrate their applications with several simple examples. Our first task is to
</p>
<p>establish the notion of microcanonical ensemble, which is suitable for describing
</p>
<p>an isolated system. The other ensembles of more practical importance, including
</p>
<p>the canonical ensemble, can be derived straightforwardly from the microcanonical
</p>
<p>ensemble.
</p>
<p>4.1 Fluctuations in a Canonical Ensemble
</p>
<p>A system held at a constant temperature is constantly exchanging energy with the
</p>
<p>surroundings, which serves as a heat bath. As a result, the energy of the system will
</p>
<p>fluctuate with time. Let us first figure out the magnitude of this fluctuation. A good
</p>
<p>measure is given by
</p>
<p>∆rmsH :=
&lang;
(H &minus;〈H〉)2
</p>
<p>&rang;1/2
. (4.1)
</p>
<p>In fact, ∆rmsH is zero if and only if H is identically equal to 〈H〉, and increases as
the deviation of H from the average becomes more probable. Because of the square
</p>
<p>root, ∆rmsH also has the dimension of energy.
Recall from Exercise 3.2 that
</p>
<p>〈H〉=&minus;&part; lnZ
&part;β
</p>
<p>and
&lang;
(H &minus;〈H〉)2
</p>
<p>&rang;
=
</p>
<p>&part; 2 lnZ
</p>
<p>&part;β 2
. (4.2)
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 169
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1 4</p>
<p/>
</div>
<div class="page"><p/>
<p>170 4 Various Statistical Ensembles
</p>
<p>For an ideal gas of N noninteracting identical particles, we have
</p>
<p>Z =
V N
</p>
<p>N!Λ 3N
, (4.3)
</p>
<p>where Λ &sim;
&radic;
</p>
<p>β . Thus,
</p>
<p>〈H〉= 3N
2β
</p>
<p>,
&lang;
(H &minus;〈H〉)2
</p>
<p>&rang;
=
</p>
<p>3N
</p>
<p>2β 2
, (4.4)
</p>
<p>and hence
∆rmsH
</p>
<p>〈H〉 &sim;
&radic;
</p>
<p>1
</p>
<p>N
. (4.5)
</p>
<p>For a macroscopic system of N &sim; 1024, this quantity is 10&minus;12. If we let 〈H〉 = 1 J
for the sake of illustration, then the magnitude of fluctuation ∆rmsH is of the order
of 10&minus;12 J. Such an extremely small quantity cannot be measured by an instrument
whose intended range is of the order of 1 J.
</p>
<p>Equations similar to (4.5) hold quite generally for any extensive quantities of a
</p>
<p>homogeneous macroscopic body. To see this, let G be such an extensive quantity
</p>
<p>and denote by N the total number of particles in the body. We divide the body into
</p>
<p>M equal parts and denote the value of the extensive quantity pertaining to the ith
</p>
<p>part by gi. Since G is an extensive quantity, we may write
</p>
<p>G =
M
</p>
<p>&sum;
i=1
</p>
<p>gi . (4.6)
</p>
<p>In writing this equation, we are actually assuming that each part is still sufficiently
</p>
<p>large that the contribution to G from the interaction among adjacent parts can be
</p>
<p>ignored.19 The assumption is justified since we are interested only in macroscopic
</p>
<p>systems here and N can be taken as large as we desire. Taking the average of (4.6),
</p>
<p>we find
</p>
<p>〈G〉=
M
</p>
<p>&sum;
i=1
</p>
<p>〈gi〉=
M
</p>
<p>&sum;
i=1
</p>
<p>g = Mg , (4.7)
</p>
<p>where we used the fact that the original system is homogeneous and hence g := 〈gi〉
is common for all i. Let us next figure out the magnitude of fluctuation of G. First,
</p>
<p>we calculate
</p>
<p>(G&minus;〈G〉)2 =
(
</p>
<p>M
</p>
<p>&sum;
i=1
</p>
<p>gi &minus;Mg
)2
</p>
<p>=
</p>
<p>[
M
</p>
<p>&sum;
i=1
</p>
<p>(gi &minus;g)
]2
</p>
<p>, (4.8)
</p>
<p>where we used (4.7). Now,
</p>
<p>[
M
</p>
<p>&sum;
i=1
</p>
<p>(gi &minus;g)
]2
</p>
<p>=
M
</p>
<p>&sum;
i=1
</p>
<p>(gi &minus;g)
M
</p>
<p>&sum;
j=1
</p>
<p>(g j &minus;g) =
M
</p>
<p>&sum;
i=1
</p>
<p>M
</p>
<p>&sum;
j=1
</p>
<p>(gi &minus;g)(g j &minus;g) . (4.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Microcanonical Ensemble 171
</p>
<p>Taking the average, we find
</p>
<p>(∆rmsG)
2 =
</p>
<p>M
</p>
<p>&sum;
i=1
</p>
<p>M
</p>
<p>&sum;
j=1
</p>
<p>〈(gi &minus;g)(g j &minus;g)〉 . (4.10)
</p>
<p>We have already assumed that the interaction among the adjacent parts is sufficiently
</p>
<p>weak, implying that we may regard each part as statistically independent of the
</p>
<p>adjacent ones. Thus, if i 	= j,
</p>
<p>〈(gi &minus;g)(g j &minus;g)〉= 〈gi &minus;g〉〈g j &minus;g〉= 0 . (4.11)
</p>
<p>However, if i = j,
</p>
<p>〈(gi &minus;g)(g j &minus;g)〉= 〈(gi &minus;g)2〉=: (∆rms g)2 (4.12)
</p>
<p>and (4.10) becomes
</p>
<p>(∆rmsG)
2 = M(∆rms g)
</p>
<p>2 . (4.13)
</p>
<p>Combining (4.7) and (4.13), we arrive at
</p>
<p>∆rmsG
</p>
<p>〈G〉 &sim;
&radic;
</p>
<p>1
</p>
<p>M
. (4.14)
</p>
<p>If we fix the number of particles included in each of the parts, into which we divided
</p>
<p>our original system, then M &sim; N, and hence
</p>
<p>∆rmsG
</p>
<p>〈G〉 &sim;
&radic;
</p>
<p>1
</p>
<p>N
(4.15)
</p>
<p>as advertised.
</p>
<p>4.2 Microcanonical Ensemble
</p>
<p>Equation (4.5) indicates that the distribution of the system energy is sharply peaked
</p>
<p>around 〈H〉 for macroscopic systems with N &sim; 1024. This being the case, the canon-
ical ensemble can be approximated by a new ensemble, called microcanonical
</p>
<p>ensemble, in which all the copies have practically the same energy.
</p>
<p>4.2.1 Expression for ρ
</p>
<p>To define a statistical ensemble precisely, we must give an explicit expression for ρ .
For this purpose, let us start from a canonical ensemble and then imagine selecting
</p>
<p>from the ensemble only those copies whose energy lies within an extremely narrow</p>
<p/>
</div>
<div class="page"><p/>
<p>172 4 Various Statistical Ensembles
</p>
<p>semi-open interval (E&minus;∆E,E]. (That is, H of a copy must satisfy E&minus;∆E &lt;H &le;E.)
Those copies so selected form the microcanonical ensemble. Now, we recall that
</p>
<p>copies in the original canonical ensemble are distributed in the phase space accord-
</p>
<p>ing to the density ρ which depends only on the energy of the system. Since ∆E/E is
extremely small, we can regard ρ within the interval (E &minus;∆E,E] as constant with-
out any noticeable loss of accuracy. The microcanonical ensemble constructed from
</p>
<p>the canonical ensemble is therefore characterized by
</p>
<p>ρ(q f , p f )dq f dp f =
</p>
<p>{
1
</p>
<p>CM
</p>
<p>dq f dp f
</p>
<p>h f P
if E &minus;∆E &lt; H(q f , p f )&le; E
</p>
<p>0 otherwise. (4.16)
</p>
<p>Aside from the normalization factor, this expression for ρ when E &minus;∆E &lt; H &le; E
follows from (3.165) by replacing the Boltzmann factor e&minus;βH by a constant.
</p>
<p>There is nothing special about our choice of the interval for H. We could have
</p>
<p>very well chosen [E &minus;∆E,E) instead. The particular choice we made here, how-
ever, will affect the definition of W (E) and our choice for the step function to be
introduced in what follows. The reason for using a half-open interval becomes clear
</p>
<p>in (4.23).
</p>
<p>If we take a phase point A, the number of copies in the ensemble whose rep-
</p>
<p>resentative phase points fall inside the volume element dq f dp f taken around A is
</p>
<p>given by N ρ(q f , p f )dq f dp f , where N is the total number of copies in the ensem-
ble. According to (4.16), if the dq f dp f lies entirely inside the region E &minus;∆E &lt;
H(q f , p f )&le; E, this number is N dq f dp f /h f PCM regardless of exactly where A is
located. On the other hand, if dq f dp f is taken entirely outside the indicated region,
</p>
<p>the number is strictly zero. Thus, the copies in the ensemble are distributed with
</p>
<p>uniform density in the region of the phase space corresponding to (E &minus;∆E,E]. In
other words, all states satisfying E &minus;∆E &lt; H(q f , p f ) &le; E are equally probable.
This result is known as the principle of equal weight.
</p>
<p>We arrived at this principle by applying a canonical ensemble to a macroscopic
</p>
<p>system. In a more common approach to statistical mechanics, one starts with a
</p>
<p>microcanonical ensemble. In this case, the principle plays a more central role of
</p>
<p>being the logical foundation of statistical mechanics.
</p>
<p>We recall that ρ must be normalized:
</p>
<p>&int;
ρ(q f , p f )dq f dp f = 1 . (4.17)
</p>
<p>Substituting (4.16), we find
</p>
<p>CM =
1
</p>
<p>h f P
</p>
<p>&int;
</p>
<p>E&minus;∆E&lt;H&le;E
dq f dp f . (4.18)
</p>
<p>We refer to CM as the microcanonical partition function. Note that the integral in
</p>
<p>(4.18) is the volume of the region in the phase space (or the phase volume for short)
</p>
<p>that is compatible with E &minus;∆E &lt; H &le; E, while h f P is the phase volume occupied
by a single (quantum mechanically distinct) state. Thus, CM is the number of states
</p>
<p>consistent with the condition that E &minus;∆E &lt; H &le; E.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Microcanonical Ensemble 173
</p>
<p>We introduced a microcanonical ensemble as an approximation to a canonical
</p>
<p>ensemble. Thus, it is sensible to expect that Gibbs&rsquo;s entropy formula continues
</p>
<p>to apply to the microcanonical ensemble. Using (4.16) in (3.168) and noting that
</p>
<p>x lnx &rarr; 0 as x &rarr; 0,20 we have
</p>
<p>&minus;S/kB =
&int;
</p>
<p>ρ ln(h f Pρ)dq f dp f =
&int;
</p>
<p>ρ ln
1
</p>
<p>CM
dq f dp f = ln
</p>
<p>1
</p>
<p>CM
. (4.19)
</p>
<p>The last step follows from (4.17) and the fact that CM is a constant. This is the
</p>
<p>famous Boltzmann&rsquo;s entropy formula:
</p>
<p>S = kB lnCM . (4.20)
</p>
<p>4.2.2 Choice of ∆E
</p>
<p>It is clear from (4.18) that, once the functional form of H is specified and the inte-
</p>
<p>gration is carried out, the resulting CM and hence S depend on f , E, ∆E, and the lim-
its of integrations. Because the integration in (4.18) includes only those microstates
</p>
<p>compatible with E&minus;∆E &lt; H &le; E and ∆E/E is extremely small, the internal energy
may be identified with E:
</p>
<p>U := 〈H〉 &asymp; E . (4.21)
For a system of N particles contained in a three-dimensional box, f = 3N and the
limits of integrations for their coordinates usually show up only in the form of the
</p>
<p>system volume V in the final expression for CM . This being the case,
</p>
<p>S = S(U,V,N,∆E) . (4.22)
</p>
<p>Of all these quantities, upon which S can depend, ∆E alone does not have any cor-
responding quantity in thermodynamics. We now show that the value of ∆E can be
taken quite arbitrarily within a reasonable bound with no quantitative consequence
</p>
<p>to the value of S. Thus, S is practically a function only of U , V , and N.
</p>
<p>To see that S is in fact insensitive to ∆E, let W (E,V,N) denote the number of
states satisfying H &le; E for given values of V and N. Clearly,
</p>
<p>CM(E,V,N,∆E) = W (E,V,N)&minus;W (E &minus;∆E,V,N) . (4.23)
</p>
<p>Because ∆E is extremely small compared to E, we expand the second W into a
Taylor series to obtain
</p>
<p>CM(E,V,N,∆E) &asymp; W (E,V,N)&minus;
[
W (E,V,N)&minus; &part;W
</p>
<p>&part;E
∆E
</p>
<p>]
</p>
<p>=
&part;W
</p>
<p>&part;E
∆E =: Ω(E,V,N)∆E , (4.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>174 4 Various Statistical Ensembles
</p>
<p>where
</p>
<p>Ω(E,V,N) :=
&part;W
</p>
<p>&part;E
(4.25)
</p>
<p>is the density of states. This terminology is quite appropriate since an integration
</p>
<p>of Ω over a certain interval of energy yields the total number of states whose energy
falls within that interval.
</p>
<p>Using (4.20), we obtain
</p>
<p>S = kB lnΩ(E,V,N)∆E , (4.26)
</p>
<p>which is still a function of E, V , N, and ∆E.
Based on the consideration that led us to microcanonical ensemble, it seems quite
</p>
<p>reasonable to set ∆E = E/
&radic;
</p>
<p>N, for which the entropy is evaluated as
</p>
<p>S1 = kB lnΩ(E,V,N)
E&radic;
N
</p>
<p>. (4.27)
</p>
<p>An extreme choice for ∆E would be E itself. For this choice, the Taylor series
expansion used in arriving at (4.24) cannot be justified. Nevertheless, if we proceed
</p>
<p>blindly, we find
</p>
<p>S2 = kB lnΩ(E,V,N)E . (4.28)
</p>
<p>Thus,
S2 &minus;S1
</p>
<p>kBN
=
</p>
<p>1
</p>
<p>2N
lnN . (4.29)
</p>
<p>For a macroscopic body with N &sim; 1024, this quantity is approximately 3&times; 10&minus;23
even though the two choices for ∆E differ by a factor of
</p>
<p>&radic;
N &sim; 1012. For macro-
</p>
<p>scopic systems, therefore, the precise value of ∆E is unimportant. What happens if
N is not large enough? In this case, the use of microcanonical ensemble is of little
</p>
<p>interest. The ensemble was introduced only as an approximation applicable for a
</p>
<p>large N.
</p>
<p>4.2.3 Isolated System
</p>
<p>In a microcanonical ensemble, all copies of the ensemble have nearly the same
</p>
<p>amount of energy. If we recall how ρ was constructed from the long-time behavior
of a single system, this implies that its energy fluctuates very little. In fact, the width
</p>
<p>of this fluctuation relative to the energy itself is of the order of 1/
&radic;
</p>
<p>N and E of
</p>
<p>the system is a constant within the accuracy of any practical means of measuring
</p>
<p>E. We recall that the energy of an isolated system is a constant of motion. Thus,
</p>
<p>a microcanonical ensemble is a natural choice for describing a system that can be
</p>
<p>regarded as isolated within the accuracy of experiments.
</p>
<p>This correspondence between a microcanonical ensemble and an isolated sys-
</p>
<p>tem is quite satisfactory from the point of view of thermodynamics. We recall that
</p>
<p>entropy played a prominent role in the condition of equilibrium of an isolated sys-</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Phase Integral in Microcanonical Ensemble 175
</p>
<p>tem. Moreover, S expressed in terms of (U,V,N) is a fundamental equation of ther-
modynamics. Because S is practically independent of ∆E, (4.22) indicates that S is
obtained directly as a function of just these variables.
</p>
<p>One might argue that ∆E should be identically zero for a truly isolated system.
While this might be so, the system we intend to study must be subjected to experi-
</p>
<p>mental measurements by necessity. Because ∆E is extremely small compared to E
itself, the very attempt to measure something about the system will generally affect
</p>
<p>the system energy by an amount far exceeding ∆E. At the same time, we have no
interest in predicting the behavior of a system if no measurement will ever be per-
</p>
<p>formed on it. In this sense, a truly isolated system never really arises as a subject of
</p>
<p>our study. Instead, ∆E is nonzero for isolated systems of our interest.
</p>
<p>4.3 Phase Integral in Microcanonical Ensemble
</p>
<p>The phase integral for CM extends only over a portion of the phase space consistent
</p>
<p>with the condition E &minus;∆E &lt; H &le; E as we see from (4.18). It is very awkward to
work with a phase integral whose limits are given in terms of the system energy.
</p>
<p>For the subsequent development, it is desirable, and indeed possible, to rewrite the
</p>
<p>phase integral so that the integral is over all phase space.
</p>
<p>We first note that the phase integral
</p>
<p>&int;
</p>
<p>H&le;E
dq f dp f (4.30)
</p>
<p>is the phase volume of the region satisfying H(q f , p f )&le;E. If we divide this quantity
by h f P , which represents the phase volume occupied by a single microstate, we
</p>
<p>obtain the number of states with H(q f , p f )&le; E. That is,
</p>
<p>W (E,V,N) =
1
</p>
<p>h f P
</p>
<p>&int;
</p>
<p>H&le;E
dq f dp f =
</p>
<p>1
</p>
<p>h f P
</p>
<p>&int;
θ
(
E &minus;H(q f , p f )
</p>
<p>)
dq f dp f , (4.31)
</p>
<p>where θ is the step function defined by (D.2). See Fig. 4.1 to justify its use here.
Following the definition of Ω(E) given by (4.24), we now have to differentiate
</p>
<p>W with respect to E. Because the integral is taken over all phase space, the only E
</p>
<p>dependence of W is in the step function θ , giving rise to the Dirac δ -function as
shown in Appendix D:
</p>
<p>Ω(E,V,N) =
&part;W
</p>
<p>&part;E
=
</p>
<p>1
</p>
<p>h f P
</p>
<p>&int;
&part;
</p>
<p>&part;E
θ
(
E &minus;H(q f , p f )
</p>
<p>)
dq f dp f
</p>
<p>=
1
</p>
<p>h f P
</p>
<p>&int;
δ
(
E &minus;H(q f , p f )
</p>
<p>)
dq f dp f . (4.32)</p>
<p/>
</div>
<div class="page"><p/>
<p>176 4 Various Statistical Ensembles
</p>
<p>00 E
</p>
<p>11
</p>
<p>x E H
</p>
<p>x H
</p>
<p>Fig. 4.1 Step function. If H &gt; E, E &minus;H &lt; 0 and hence θ(E &minus;H) = 0. If H &le; E, E &minus;H &ge; 0 and
θ(E &minus;H) = 1.
</p>
<p>Substituting this expression into (4.24), we find that
</p>
<p>CM =
1
</p>
<p>h f P
</p>
<p>&int;
δ
(
E &minus;H(q f , p f )
</p>
<p>)
∆Edq f dp f . (4.33)
</p>
<p>In essence, we replaced the limits of the phase integral in (4.18) by a δ -function in
the integrand.
</p>
<p>Example 4.1. Irreversible expansion of an ideal gas: Consider an isolated sys-
</p>
<p>tem consisting of two compartments of equal volume V that are separated by
</p>
<p>rigid and impermeable partition held fixed in place. Suppose that, at the ini-
</p>
<p>tial state, one of the compartments is filled with N noninteracting identical
</p>
<p>particles while the other compartment is empty. If the partition is removed,
</p>
<p>the gas will expand and fill the entire volume 2V . Insofar as this process is
</p>
<p>irreversible, we expect that the entropy of the system at the final state is larger
</p>
<p>than that at the initial state. Let us compute the change in entropy by means
</p>
<p>of (4.33).
</p>
<p>At the initial state, the Hamiltonian of the system is given by
</p>
<p>H(rN ,pN) =
</p>
<p>{
&sum;Ni=1 |pi|2 /2m if ri &isin;V for all i
&infin; otherwise.
</p>
<p>(4.34)
</p>
<p>Setting P = N! and f = 3N we write (4.33) as
</p>
<p>CiM =
1
</p>
<p>N!h3N
</p>
<p>&int;
δ
(
E &minus;H(rN ,pN)
</p>
<p>)
∆EdrNdpN . (4.35)
</p>
<p>Because we are concerned only with a finite value of E, the integrand is zero
</p>
<p>if H is infinity, that is, if any one of the particle goes outside the volume V . It
</p>
<p>follows that the integration over coordinates should be confined to the volume</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Phase Integral in Microcanonical Ensemble 177
</p>
<p>V and H is then a function of momenta only. Thus,
</p>
<p>CiM =
1
</p>
<p>N!h3N
</p>
<p>&int;
</p>
<p>rN&isin;V
</p>
<p>[&int;
δ
(
E &minus;H(pN)
</p>
<p>)
∆EdpN
</p>
<p>]
drN , (4.36)
</p>
<p>where rN &isin;V denotes the condition that all the particles are inside V . We have
also chosen to perform first the integration with respect to momenta. But, the
</p>
<p>result of this integration is independent of coordinates. So,
</p>
<p>CiM =
V N
</p>
<p>N!h3N
</p>
<p>&int;
δ
(
E &minus;H(pN)
</p>
<p>)
∆EdpN . (4.37)
</p>
<p>In this problem, there is no need to compute this integral as we will see
</p>
<p>shortly. Nevertheless, such a simple system as an ideal gas should surely be
</p>
<p>&ldquo;easy&rdquo; to handle and you are invited to carry out the actual computation in
</p>
<p>Exercise 4.3.
</p>
<p>At the final state, particles can be anywhere inside the volume 2V . Repeat-
</p>
<p>ing the above computation, but with V replaced by 2V , we find
</p>
<p>C
f
M =
</p>
<p>(2V )N
</p>
<p>N!h3N
</p>
<p>&int;
δ
(
E &minus;H(pN)
</p>
<p>)
∆EdpN . (4.38)
</p>
<p>The change in entropy upon removal of the partition, therefore, is given by
</p>
<p>S f &minus;Si = kB lnC fM &minus; kB lnCiM = kB ln
C
</p>
<p>f
M
</p>
<p>CiM
= NkB ln2 , (4.39)
</p>
<p>which is positive as expected.
</p>
<p>Note that C
f
M is the number of distinct microstates consistent with the given
</p>
<p>values of E, N, and the volume 2V . We also recall that all these C
f
M states are
</p>
<p>equally probable. Among these states, however, some have all the particles
</p>
<p>confined to the original volume V . The number of such states is just CiM .
</p>
<p>(To see this more clearly, suppose that the system is a one-dimensional box
</p>
<p>of length 2L and let N = 2. The integration over coordinates is then given by
</p>
<p>&int; L
</p>
<p>&minus;L
</p>
<p>&int; L
</p>
<p>&minus;L
dx1dx2 =
</p>
<p>&int; 0
</p>
<p>&minus;L
</p>
<p>&int; 0
</p>
<p>&minus;L
dx1dx2 +
</p>
<p>&int; 0
</p>
<p>&minus;L
</p>
<p>&int; L
</p>
<p>0
dx1dx2
</p>
<p>+
&int; L
</p>
<p>0
</p>
<p>&int; 0
</p>
<p>&minus;L
dx1dx2 +
</p>
<p>&int; L
</p>
<p>0
</p>
<p>&int; L
</p>
<p>0
dx1dx2 .
</p>
<p>(4.40)
</p>
<p>The first term on the right corresponds to the situation in which both particles
</p>
<p>occupy the left half of the box.)</p>
<p/>
</div>
<div class="page"><p/>
<p>178 4 Various Statistical Ensembles
</p>
<p>Thus, the probability p&lt; that all the particles are found within the original
</p>
<p>volume V in the final state is given by
</p>
<p>p&lt; =
CiM
</p>
<p>C
f
M
</p>
<p>= e(Si&minus;S f )/kB . (4.41)
</p>
<p>This is an example of (2.20). For N = 1024, p&lt; = 2&minus;10
24 &asymp; 10&minus;0.3&times;1024 , an
</p>
<p>extremely small number. For all practical purposes, the process of expansion
</p>
<p>of the gas is irreversible. However, if N = 4, then p&lt; = 2&minus;4 and a sponta-
neous reversal of the process is not too unlikely. In this way, the irreversible
</p>
<p>approach to an equilibrium state acquires a probabilistic interpretation.
</p>
<p>Exercise 4.1. Consider an isolated system consisting of two compartments of equal
</p>
<p>volume that are separated by a diathermal, rigid, impermeable partition held fixed
</p>
<p>in place. Initially, one of the compartments is filled with N noninteracting identi-
</p>
<p>cal particles of species A, while the other is filled with N noninteracting identical
</p>
<p>particles of species B. If the partition is removed and a new state of equilibrium is
</p>
<p>reached, what is the increase in the total entropy? What would be the increase in the
</p>
<p>entropy if species A and B happen to be the same? ///
</p>
<p>In the following two exercises, we find an explicit expression for W and Ω for
an ideal gas. Exercise 4.2 introduces a new mathematical tool, which will be used
</p>
<p>in Exercise 4.3.
</p>
<p>Exercise 4.2. The Gamma function is defined by
</p>
<p>Γ (s) :=
&int; &infin;
</p>
<p>0
xs&minus;1e&minus;xdx (s &gt; 0) . (4.42)
</p>
<p>Prove the following identities:
</p>
<p>a.
</p>
<p>Γ (s+1) = sΓ (s) . (4.43)
</p>
<p>b.
</p>
<p>Γ (n+1) = n! , (4.44)
</p>
<p>where n is a positive integer.
</p>
<p>c.
</p>
<p>Γ
</p>
<p>(
n+
</p>
<p>1
</p>
<p>2
</p>
<p>)
=
</p>
<p>&radic;
π(2n)!
</p>
<p>22nn!
. (4.45)
</p>
<p>///
</p>
<p>Exercise 4.3. Following the steps indicated below, evaluate the partition function
</p>
<p>CM for a system of N noninteracting identical particles confined to a box of volume
</p>
<p>V :</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 &dagger;Adiabatic Reversible Processes 179
</p>
<p>a. Calculate
</p>
<p>In :=
&int; &infin;
</p>
<p>&minus;&infin;
&middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;(x1
</p>
<p>2+&middot;&middot;&middot;+xn2)dx1 &middot; &middot; &middot;dxn . (4.46)
</p>
<p>b. Let Un be the volume of the unit sphere in n-dimensional space. Then, the volume
</p>
<p>of the n-dimensional sphere of radius r is Unr
n. Use this fact to express In in terms
</p>
<p>of Un.
</p>
<p>c. Show that Un is given by
</p>
<p>Un =
πn/2
</p>
<p>Γ (n/2+1)
. (4.47)
</p>
<p>Check the validity of the formula for n = 2 and n = 3.
d. Show that
</p>
<p>W (E,V,N) =
V N
</p>
<p>N!
</p>
<p>(
2mE
</p>
<p>h2
</p>
<p>)3N/2 &int;
θ
</p>
<p>(
1&minus;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>s2i
</p>
<p>)
dsN . (4.48)
</p>
<p>e. Evaluate CM . ///
</p>
<p>4.4 &dagger;Adiabatic Reversible Processes
</p>
<p>Suppose that a system is subject to an external field, such as the one generated by
</p>
<p>a piston in Fig. 3.4, for example. The system is otherwise isolated. We know from
</p>
<p>thermodynamics that, if this external field varies very slowly, the process proceeds
</p>
<p>reversibly and the entropy S of the system remains constant. Our goal in this section
</p>
<p>is to provide a statistical mechanical demonstration of this fact.
</p>
<p>As before, Hamiltonian of the system depends on the parameter λ denoting the
position of the external body. We suppose that λ changes by an infinitesimal amount
∆λ over a long duration of time τ commencing at time t. Then, the total change in
H can be expressed as
</p>
<p>H(t + τ)&minus;H(t) =
&int; t+τ
</p>
<p>t
</p>
<p>dH
</p>
<p>dt &prime;
dt &prime; . (4.49)
</p>
<p>From (1.186), we have
dH
</p>
<p>dt
=
</p>
<p>&part;H
</p>
<p>&part; t
. (4.50)
</p>
<p>For the process we are considering, the explicit time dependence of H arises solely
</p>
<p>from the time dependence of the parameter λ . Thus,
</p>
<p>&part;H
</p>
<p>&part; t
=
</p>
<p>&part;H
</p>
<p>&part;λ
</p>
<p>dλ
</p>
<p>dt
. (4.51)
</p>
<p>Using this expression in (4.49), we obtain
</p>
<p>H(t + τ)&minus;H(t) =
&int; t+τ
</p>
<p>t
</p>
<p>&part;H
</p>
<p>&part;λ
</p>
<p>dλ
</p>
<p>dt &prime;
dt &prime; . (4.52)</p>
<p/>
</div>
<div class="page"><p/>
<p>180 4 Various Statistical Ensembles
</p>
<p>If λ changes at a constant rate, dλ/dt &prime; may be written as ∆λ/τ , thus yielding
</p>
<p>H(t + τ)&minus;H(t) = ∆λ
τ
</p>
<p>&int; t+τ
</p>
<p>t
</p>
<p>&part;H
</p>
<p>&part;λ
dt &prime; . (4.53)
</p>
<p>Because the change is supposed to occur very slowly, we let τ &rarr; &infin; and recall (3.4)
to write21
</p>
<p>H(t + τ)&minus;H(t) = ∆λ
&lang;
&part;H
</p>
<p>&part;λ
</p>
<p>&rang;
. (4.54)
</p>
<p>In a microcanonical ensemble, all copies in the ensemble have nearly identical
</p>
<p>energies. Thus, with very high degree of accuracy, we can write
</p>
<p>H(t + τ) = 〈H(t + τ)〉=U(t + τ) and H(t) = 〈H(t)〉=U(t) . (4.55)
</p>
<p>as in (4.21). Thus, (4.54) finally becomes
</p>
<p>U(t + τ)&minus;U(t) = dλ
&lang;
&part;H
</p>
<p>&part;λ
</p>
<p>&rang;
, (4.56)
</p>
<p>where we replaced ∆λ by the usual notation dλ to indicate the infinitesimal change.
Since the microcanonical ensemble is obtained as an approximation to canonical
</p>
<p>ensemble, there is nothing that prevents us from using (3.63), which in light of
</p>
<p>(4.56) indicates that dS = 0.
The key assumption in arriving at this result was that the process occurs very
</p>
<p>slowly. But, τ &rarr; &infin; only means that τ is much larger compared to the characteristic
time scale of molecular motions. Thus, the very slow process we considered here
</p>
<p>can be quite rapid when measured in a time scale of experiments. In fact, many
</p>
<p>textbooks on thermodynamics contend that dS &asymp; 0 during a very rapid expansion or
compression of a gas. There, the process is said to be rapid if no appreciable heat
</p>
<p>flow is involved between the system and the surroundings. These rapid processes
</p>
<p>are still very slow compared to the characteristic time scale of molecular motions.
</p>
<p>4.5 Canonical Ensemble
</p>
<p>We introduced microcanonical ensemble as an approximation to canonical ensem-
</p>
<p>ble. Historically, though, microcanonical ensemble was the first to be formulated.
</p>
<p>This is because an isolated system is the most natural system to consider in classi-
</p>
<p>cal mechanics. From the point of view of practical applications, an isolated system
</p>
<p>is a rather rare occurrence. So, it is nice to have statistical ensembles suitable for
</p>
<p>describing systems in contact with their surroundings.
</p>
<p>The situation is exactly the same as that in thermodynamics. There, the second
</p>
<p>law was formulated first for an isolated system. Then, various free energies were
</p>
<p>obtained by means of Legendre transformation of the energy representation.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Canonical Ensemble 181
</p>
<p>T
</p>
<p>System A
</p>
<p>Surroundings B
</p>
<p>Isolated system AB
</p>
<p>Fig. 4.2 The system of interest A is held at a constant temperature T by exchanging energy with
</p>
<p>the surroundings B. The composite system AB is isolated from the rest of the universe by means
</p>
<p>of an adiabatic, rigid, and impermeable wall.
</p>
<p>Similarly, various statistical ensembles can be derived from the microcanonical
</p>
<p>ensemble. For this reason, the microcanonical ensemble is important conceptually
</p>
<p>despite the computational awkwardness you saw in Exercise 4.3.
</p>
<p>4.5.1 Closed System Held at a Constant Temperature
</p>
<p>Let us take a closed system A held at a constant temperature T . The number of
</p>
<p>particles N in A and the volume of A are both constant. But the system is allowed
</p>
<p>to exchange energy with the surroundings, which is assumed to be macroscopic. As
</p>
<p>we saw in Sect. 2.13.1, this thermal contact is what maintains the temperature of the
</p>
<p>system at a desired value.
</p>
<p>Due to the energy exchange with the surroundings, the energy Ea of A fluctuates
</p>
<p>over time. Our immediate goal is to find the probability p(Ea)dEa that the energy of
A lies somewhere in the interval (Ea &minus; dEa,Ea]. We use microcanonical ensemble
for this purpose.
</p>
<p>As we have indicated already, a microcanonical ensemble is suitable for describ-
</p>
<p>ing an isolated system, which system A is not. In order to make the microcanonical
</p>
<p>ensemble applicable to the current situation, we construct a composite system AB
</p>
<p>consisting of A and the macroscopic surroundings B, and then isolate AB from the
</p>
<p>rest of the universe by means of an adiabatic, rigid, and impermeable wall as shown
</p>
<p>in Fig. 4.2. We shall assume that the interaction between A and B is sufficiently
</p>
<p>weak in the sense that
</p>
<p>Hab &asymp; Ha +Hb . (4.57)
Because the composite system AB is isolated, the microcanonical ensemble
</p>
<p>applies. As in (4.16), we have
</p>
<p>Eab &minus;∆E &lt; Hab &le; Eab (4.58)
</p>
<p>for the allowed values of Hab.</p>
<p/>
</div>
<div class="page"><p/>
<p>182 4 Various Statistical Ensembles
</p>
<p>Eab
</p>
<p>Eab Ea
</p>
<p>Eab E
</p>
<p>Eab Ea E
</p>
<p>Ea dEa
</p>
<p>Ea0 Ha
</p>
<p>Hb
</p>
<p>Ha Hb Eab
</p>
<p>Ha Hb Eab E
</p>
<p>Fig. 4.3 The parallelogram region defined by four lines Ha +Hb = Eab, Ha +Hb = Eab &minus;∆E,
Ha = Ea, and Ha = Ea &minus; dEa contains all the microstates accessible to the composite system AB
that are consistent with (4.58) and (4.59).
</p>
<p>We recall that all the microstates of AB consistent with this condition on Hab
are equally probable. Thus, to compute p(Ea)dEa, we need to figure out the total
number of microstates of AB that are consistent with both (4.58) and the condition
</p>
<p>Ea &minus;dEa &lt; Ha &le; Ea . (4.59)
</p>
<p>The desired probability is this number divided by the total number of microstates of
</p>
<p>AB, which is simply Ω ab(Eab)∆E.
Now, the number of microstates accessible to A under the condition (4.59) is
</p>
<p>simply
</p>
<p>Ω a(Ea)dEa , (4.60)
</p>
<p>where we suppressed the dependence of Ω a on the system volume and the number
of mechanical degrees of freedom of the system as they are constant in this case.
</p>
<p>But, when A is at a particular microstate with a given energy, B can still be in
</p>
<p>various different microstates that are consistent with the remaining energy left to it.
</p>
<p>As seen in Fig. 4.3, the conditions (4.58) and (4.59) specify the allowed values
</p>
<p>of Hb. The figure indicates that the interval for Hb depends on the exact value of Ha.
</p>
<p>This leads to a complication in figuring out the number of microstates accessible to
</p>
<p>system B. However, for sufficiently small dEa, we may replace the parallelogram
</p>
<p>indicated by thick solid lines by the rectangle defined by Ha = Ea, Ha = Ea &minus; dEa,
Hb = Eab &minus;Ea, and Hb = Eab &minus;Ea &minus;∆E. Thus, we may write
</p>
<p>Eab &minus;Ea &minus;∆E &lt; Hb &le; Eab &minus;Ea . (4.61)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Canonical Ensemble 183
</p>
<p>Accordingly, the number of microstates accessible to B that are consistent with both
</p>
<p>the conditions (4.58) and (4.59) is given by
</p>
<p>Ω b(Eab &minus;Ea)∆E . (4.62)
</p>
<p>The total number of microstates accessible to the composite system AB as a
</p>
<p>whole is then the product of (4.60) and (4.62):
</p>
<p>Ω a(Ea)dEaΩ b(Eab &minus;Ea)∆E . (4.63)
</p>
<p>So, the desired probability is
</p>
<p>p(Ea)dEa =
Ω b(Eab &minus;Ea)∆EΩ a(Ea)dEa
</p>
<p>Ω ab(Eab)∆E
. (4.64)
</p>
<p>The normalization condition of p(Ea) leads to
</p>
<p>Ω ab(Eab)∆E =
&int;
</p>
<p>Ω b(Eab &minus;Ea)∆EΩ a(Ea)dEa , (4.65)
</p>
<p>where the integration is with respect to all possible values of Ea. This result is quite
</p>
<p>natural. In order to find out the total number of microstates accessible to AB, we first
</p>
<p>figure out the number of microstates accessible to AB when Ha is within a certain
</p>
<p>interval indicated by (4.59). This gives (4.63). Once the number of microstates is
</p>
<p>obtained for all the other intervals of width dEa, we can simply add up the results to
</p>
<p>obtain the total number of microstates.
</p>
<p>4.5.2 Canonical Distribution
</p>
<p>As it is, (4.64) is not very useful for practical computations. As we saw in Exer-
</p>
<p>cise 4.3, Ω is a very awkward function to work with. To transform (4.64) a bit
further, we recall Boltzmann&rsquo;s entropy formula (4.26) and write
</p>
<p>Ω b(Eab &minus;Ea)∆E = elnΩb(Eab&minus;Ea)∆E = eSb(Eab&minus;Ea)/kB . (4.66)
</p>
<p>Expanding Sb(Eab &minus;Ea) in a Taylor series, we find
</p>
<p>Sb(Eab &minus;Ea) = Sb0 &minus;
&part;Sb(Eb)
</p>
<p>&part;Eb
</p>
<p>∣∣∣∣
0
</p>
<p>Ea +h.o. = Sb0 &minus;
Ea
</p>
<p>T
+h.o. , (4.67)
</p>
<p>where the subscript 0 on Sb and its derivative indicates that they are to be computed
</p>
<p>under the condition that Eb = Eab, while T defined in the last step is the temperature
of the surroundings B when its energy is Eab.
</p>
<p>At this point, we note that the surroundings B can be chosen arbitrarily large
</p>
<p>compared to A, and hence the numbers of mechanical degrees of freedom of these
</p>
<p>two systems can be made to satisfy fa ≪ fb. This implies that Eab ≫ Ea for all</p>
<p/>
</div>
<div class="page"><p/>
<p>184 4 Various Statistical Ensembles
</p>
<p>reasonable values of Ea and hence T is quite insensitive to the exact value of Ea.
</p>
<p>This being the case, T can be referred to as the temperature of B regardless of the
</p>
<p>exact value of Ea and we can safely ignore the higher order terms in the Taylor series
</p>
<p>expansion of Sb.
22
</p>
<p>Recalling that β := 1/kBT , we can rewrite (4.66) as
</p>
<p>Ω b(Eab &minus;Ea)∆E = eSb0/kBe&minus;βEa , (4.68)
</p>
<p>in terms of which (4.64) and (4.65) become
</p>
<p>p(Ea)dEa =
eSb0/kBe&minus;βEaΩ a(Ea)dEa
</p>
<p>Ω ab(Eab)∆E
(4.69)
</p>
<p>and
</p>
<p>Ω ab(Eab)∆E = e
Sb0/kB
</p>
<p>&int;
Ω a(Ea)e
</p>
<p>&minus;βEa dEa , (4.70)
</p>
<p>respectively.
</p>
<p>In what follows, we will be concerned only with system A. Thus, we drop the
</p>
<p>subscript a without a risk of confusion. Combining the last two equations,
</p>
<p>p(E)dE =
1
</p>
<p>Z
Ω(E)e&minus;βEdE , (4.71)
</p>
<p>in which
</p>
<p>Z :=
&int;
</p>
<p>Ω(E)e&minus;βEdE (4.72)
</p>
<p>is the canonical partition function .23 In Sect. 4.5.3, we see that (4.72) agrees with
</p>
<p>(3.162).
</p>
<p>The free energy associated with this ensemble, by definition, is the Helmholtz
</p>
<p>free energy:
</p>
<p>F =&minus;kBT lnZ . (4.73)
This definition, along with (2.172), leads to Gibbs&rsquo;s entropy formula as you will
</p>
<p>see in Exercise 4.5. Note that Ω is a function of E, the volume V , and the number
of mechanical degrees of freedom f of the system. When it is multiplied by the
</p>
<p>Boltzmann factor and integrated with respect to E, the final answer Z and hence F
</p>
<p>will be functions of T , V , and f . That is, (4.73) is a fundamental equation of the
</p>
<p>system.
</p>
<p>We recall that Ω(E) is the density of states. So, Ω(E)dE is the number of
microstates of system A within the interval (E &minus; dE,E]. We already remarked that
each of these microstates are equally probable.24 Accordingly, (4.71) has the fol-
</p>
<p>lowing interpretation. The factor e&minus;βE/Z is the probability that the system A is at a
particular microstate with energy somewhere in the interval (E &minus;dE,E]. But, there
are Ω(E)dE equally probable microstates within this interval. Therefore, the proba-
bility that the energy of system A is in the interval (E&minus;dE,E], irrespective of which
particular microstate it is in, is given by (4.71).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Canonical Ensemble 185
</p>
<p>Finally, our development up to this point is based entirely on classical mechan-
</p>
<p>ics, except for the 1/h f P factor originating from quantum mechanical considera-
tions. Nevertheless, a fully quantum mechanical formulation also leads to (4.71) and
</p>
<p>(4.72). That is, they are valid regardless of the underlying mechanics. See Chap. 8
</p>
<p>for details.
</p>
<p>Example 4.2. Harmonic oscillator: In Exercise 3.8, you computed the con-
</p>
<p>stant volume heat capacity CV of a diatomic molecule. The result was that, if
</p>
<p>the chemical bond connecting the two atoms is perfectly rigid, then, CV =
5
2
kB.
</p>
<p>The equipartition theorem indicates that, of this amount, 3
2
kB reflects the trans-
</p>
<p>lational degrees of freedom of the center of mass of the molecule, while
</p>
<p>the remaining kB comes from the rotation of the molecule around its cen-
</p>
<p>ter of mass. On the other hand, if the chemical bond is modeled as a har-
</p>
<p>monic spring, CV =
7
2
kB. The additional kB comes from the vibrational motion
</p>
<p>(stretching and compressing) of the spring.
</p>
<p>Experimentally, it is found that CV =
5
2
kB for many (but not all) molecules
</p>
<p>at room temperature. Should we believe, then, that the chemical bond is abso-
</p>
<p>lutely rigid? So, if we are to pull on the atoms, then, the molecule will stay
</p>
<p>rigid until it suddenly snaps? This does not sound reasonable at all. Just as
</p>
<p>puzzling is the following observation: Physically, we expect that the harmonic
</p>
<p>spring will approach the rigid rod if the spring constant k is made infinitely
</p>
<p>large. However,
</p>
<p>lim
k&rarr;&infin;
</p>
<p>7
</p>
<p>2
kB 	=
</p>
<p>5
</p>
<p>2
kB . (4.74)
</p>
<p>This rather unsatisfactory state of affair stems from the fact that the chem-
</p>
<p>ical bond must be described not by classical mechanics but by quantum
</p>
<p>mechanics. (The same remark applies to both translational and rotational
</p>
<p>degrees of freedom of the molecule. At room temperature, however, the classi-
</p>
<p>cal treatment proves sufficiently accurate. For details, see Chap. 8 of Ref. [1],
</p>
<p>for example.)
</p>
<p>According to quantum mechanics, the energy E of the harmonic oscillator,
</p>
<p>a particle of mass m attached to a spring with spring constant k, can take only
</p>
<p>a discrete set of values as prescribed by the formula:
</p>
<p>En = h̄ω
</p>
<p>(
n+
</p>
<p>1
</p>
<p>2
</p>
<p>)
, n = 0,1,2, . . . , (4.75)
</p>
<p>where
</p>
<p>ω :=
</p>
<p>&radic;
k
</p>
<p>m
and h̄ :=
</p>
<p>h
</p>
<p>2π
. (4.76)
</p>
<p>(See Sect. 8.14 for a derivation of this result.) In the case of a diatomic
</p>
<p>molecule consisting of atoms of mass m1 and m2, m := m1m2/(m1 +m2) is
the reduced mass.</p>
<p/>
</div>
<div class="page"><p/>
<p>186 4 Various Statistical Ensembles
</p>
<p>As remarked above, (4.72) is still applicable even for a quantum mechani-
</p>
<p>cal system. To compute Z, then, all we have to do is to figure out Ω(E). It is
actually easier to compute W (E), the number of states whose energy is less
than or equal to E, and then obtain Ω(E) as the partial derivative of W (E)
with respect to E.
</p>
<p>To find W (E), we proceed as follows. If E is less than E0, there is no
quantum mechanical state. Thus W (E) = 0 if E &lt; E0. But, there is a single
quantum state at E = E0 and then another at E = E1. Thus, W (E) = 1 if
E0 &le; E &lt; E1. Likewise, W (E) = 2 if E1 &le; E &lt; E2. In this way, we end up
with
</p>
<p>W (E) =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>θ(E &minus;En) . (4.77)
</p>
<p>The graph of W (E) looks like this:
</p>
<p>0 E
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>E0 E1 E2 E3 E4 E5
</p>
<p>Taking the derivative with respect to E, we find
</p>
<p>Ω(E) =
&part;W (E)
</p>
<p>&part;E
=
</p>
<p>&infin;
</p>
<p>&sum;
n=0
</p>
<p>δ (E &minus;En) . (4.78)
</p>
<p>Substituting this expression in (4.72),
</p>
<p>Z =
&int; &infin;
</p>
<p>0
</p>
<p>&infin;
</p>
<p>&sum;
n=0
</p>
<p>δ (E &minus;En)e&minus;βEdE =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>&int; &infin;
</p>
<p>0
δ (E &minus;En)e&minus;βEdE , (4.79)
</p>
<p>where the lower limit of the integration can be anything as long as it is less
</p>
<p>than E0 since Ω(E)&equiv; 0 for E &lt; E0. Using (D.15), we find
</p>
<p>Z =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>e&minus;βEn =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>e&minus;β h̄ω(n+
1
2 ) = e&minus;β h̄ω/2
</p>
<p>&infin;
</p>
<p>&sum;
n=0
</p>
<p>(
e&minus;β h̄ω
</p>
<p>)n
. (4.80)
</p>
<p>To carry out the indicated summation, we recall (B.25). Setting r = e&minus;β h̄ω ,
which is less than unity since β h̄ω &gt; 0, we find
</p>
<p>Z =
e&minus;β h̄ω/2
</p>
<p>1&minus; e&minus;β h̄ω . (4.81)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Canonical Ensemble 187
</p>
<p>Thus,
</p>
<p>lnZ =&minus;β h̄ω
2
</p>
<p>&minus; ln
(
</p>
<p>1&minus; e&minus;β h̄ω
)
</p>
<p>(4.82)
</p>
<p>and
</p>
<p>U =&minus;&part; lnZ
&part;β
</p>
<p>=
h̄ω
</p>
<p>2
+
</p>
<p>h̄ω
</p>
<p>eβ h̄ω &minus;1 . (4.83)
</p>
<p>So, the heat capacity of the quantum harmonic oscillator is given by
</p>
<p>Cho =
&part;U
</p>
<p>&part;T
=
</p>
<p>dβ
</p>
<p>dT
</p>
<p>&part;U
</p>
<p>&part;β
=&minus; 1
</p>
<p>kBT 2
&minus;(h̄ω)2eβ h̄ω
(
eβ h̄ω &minus;1
</p>
<p>)2 = kB(β h̄ω)
2 e
</p>
<p>β h̄ω
</p>
<p>(
eβ h̄ω &minus;1
</p>
<p>)2 .
</p>
<p>(4.84)
</p>
<p>Let us define Θv by
</p>
<p>β h̄ω =
1
</p>
<p>T
</p>
<p>h̄
</p>
<p>kB
</p>
<p>&radic;
k
</p>
<p>m
=:
</p>
<p>Θv
T
</p>
<p>(4.85)
</p>
<p>and write
</p>
<p>Cho = kB
</p>
<p>(
Θv
T
</p>
<p>)2
eΘv/T
</p>
<p>(
eΘv/T &minus;1
</p>
<p>)2 . (4.86)
</p>
<p>Suppose thatΘv/T ≪ 1, which occurs if T is large, m is large, or k is small.
Then,
</p>
<p>eΘv/T &asymp; 1+Θv
T
</p>
<p>+
1
</p>
<p>2
</p>
<p>(
Θv
T
</p>
<p>)2
, (4.87)
</p>
<p>and hence to the first order of Θv/T ,
</p>
<p>Cho &asymp; kB
(
Θv
T
</p>
<p>)2
1+Θv/T
</p>
<p>[1+Θv/T +1/2(Θv/T )2 &minus;1]2
= kB (4.88)
</p>
<p>in agreement with the classical result. With some extra work, we can find that
</p>
<p>Cho &asymp; kB
[
</p>
<p>1&minus; 1
12
</p>
<p>(
Θv
T
</p>
<p>)2]
. (4.89)
</p>
<p>If, on the other hand, Θv/T ≫ 1, then eΘv/T ≫ 1, and we have
</p>
<p>Cho &asymp; kB
(
Θv
T
</p>
<p>)2
eΘv/T
</p>
<p>(
eΘv/T
</p>
<p>)2 = kB
(
Θv
T
</p>
<p>)2
e&minus;Θv/T , (4.90)
</p>
<p>which becomes vanishingly small in the low T limit. Temperature dependence
</p>
<p>of Cho is illustrated in Fig. 4.4.
</p>
<p>We note that Θv is a material property. Some representative values, along
with the value of Cho/kB computed from (4.86), are given in Table 4.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>188 4 Various Statistical Ensembles
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>1.2
</p>
<p>0 2 4 6 8 10
</p>
<p>C
h
o
</p>
<p>k
B
</p>
<p>v T
</p>
<p>Exact
</p>
<p>High T
</p>
<p>Low T
</p>
<p>Fig. 4.4 Temperature dependence of the heat capacity Cho of a quantum harmonic oscillator.
</p>
<p>&ldquo;Exact, &rdquo; &ldquo;High T ,&rdquo; and &ldquo;Low T &rdquo; refer to (4.86), (4.89), and (4.90), respectively. A classical
</p>
<p>treatment of the harmonic oscillator gives Cho/kB = 1 as you saw in Exercise 3.8c.
</p>
<p>Thus, except for Cl2 and I2, the vibrational motion makes a negligible
</p>
<p>contribution to CV at room temperature. This is why the rigid chemical
</p>
<p>bond model works so well in predicting CV for many (but not all) diatomic
</p>
<p>molecules. At the room temperature, the vibrational degrees of freedom of
</p>
<p>the chemical bond is still frozen.
</p>
<p>Quantum mechanical treatment of the rotational degrees of freedom of a
</p>
<p>symmetric diatomic molecule, such as H2 in the table, requires a special con-
</p>
<p>sideration. See Ref. [1] for details.
</p>
<p>Exercise 4.4. Prove (4.89). ///
</p>
<p>Θv (K) Cho/kB at 300 K
H2 6215 4.32&times;10&minus;7
HCl 4227 1.51&times;10&minus;4
N2 3374 1.65&times;10&minus;3
CO 3100 3.47&times;10&minus;3
Cl2 810 0.563
</p>
<p>I2 310 0.916
</p>
<p>Table 4.1 Representative examples of Θv values and Cho at 300 K. The entries for Θv are adapted
from Statistical Mechanics by Donald McQuarrie, and are reprinted with permission from Univer-
</p>
<p>sity Science Books, all rights reserved [2].</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Canonical Ensemble 189
</p>
<p>4.5.3 Classical Canonical Partition Function
</p>
<p>The quantity Z defined by (4.72) is actually the canonical partition function intro-
</p>
<p>duced earlier. Using the expression for Ω(E) given by (4.32) in (4.72):
</p>
<p>Z =
&int; [
</p>
<p>1
</p>
<p>h f P
</p>
<p>&int;
δ
(
E &minus;H(q f , p f )
</p>
<p>)
dq f dp f
</p>
<p>]
e&minus;βEdE
</p>
<p>=
1
</p>
<p>h f P
</p>
<p>&int; [&int;
δ
(
E &minus;H(q f , p f )
</p>
<p>)
e&minus;βEdE
</p>
<p>]
dq f dp f
</p>
<p>=
1
</p>
<p>h f P
</p>
<p>&int;
e&minus;βH(q
</p>
<p>f ,p f )dq f dp f , (4.91)
</p>
<p>which is (3.161). Since Z is the normalization constant and
&int;
</p>
<p>represents the sum
</p>
<p>over states, this result implies that the probability of finding a system within an
</p>
<p>infinitesimal volume element dq f dp f taken around (q f , p f ) is given by
</p>
<p>ρ(q f , p f )dq f dp f =
1
</p>
<p>Z
e&minus;βH(q
</p>
<p>f ,p f ) dq
f dp f
</p>
<p>h f P
, (4.92)
</p>
<p>which is (3.165). Unlike (4.71) and (4.72), the validity of (4.91) and (4.92) is limited
</p>
<p>to classical mechanical systems only.
</p>
<p>Exercise 4.5. We now have two definitions for the Helmholtz free energy, one is
</p>
<p>(2.172) from thermodynamics and the other is (4.73). Consolidate them using (4.71).
</p>
<p>A similar attempt for (4.92) lead to (3.168) as you saw in Exercise 3.14. ///
</p>
<p>4.5.4 Applicability of Canonical Ensemble
</p>
<p>Looking back at our derivation, we can list two important assumptions made in
</p>
<p>arriving at the canonical ensemble. First, we assumed that the interaction between
</p>
<p>A and B is sufficiently weak. As discussed in Sect. 3.11, an interaction is said to
</p>
<p>be sufficiently weak if it allows exchange of energy between A and B and if the
</p>
<p>Hamiltonian of the composite system AB can be expressed as in (4.57). Second,
</p>
<p>we assumed that the number of the mechanical degrees of freedom of the surround-
</p>
<p>ings fb is much larger than that of the system of interest fa. These are the only
</p>
<p>assumptions we made. Nowhere in our derivation have we assumed that fa is large.
</p>
<p>Provided that these two conditions are met, the canonical ensemble is generally
</p>
<p>applicable even if fa = 1.
Finally, let us remark on the statistical mechanical meaning of T . Because of
</p>
<p>exchange of energy between subsystem A and the surroundings B, the energy of
</p>
<p>system A fluctuates with time. Without the detailed knowledge of this interaction,
</p>
<p>it is not possible to predict how Ea evolves with time. Nevertheless, if the behavior
</p>
<p>of A is observed for a long interval of time, the resulting distribution of the energy</p>
<p/>
</div>
<div class="page"><p/>
<p>190 4 Various Statistical Ensembles
</p>
<p>xw
</p>
<p>PT
</p>
<p>System A
</p>
<p>Surroundings B
</p>
<p>Isolated system AWB
</p>
<p>Fig. 4.5 The system of interest A is held at constant temperature T and pressure P due to exchange
</p>
<p>of energy and repartitioning of volume with the surroundings B. The composite system AWB is
</p>
<p>isolated from the rest of the universe by means of an adiabatic, rigid, and impermeable wall.
</p>
<p>p(Ea) is given by (4.71). The temperature T appears here simply as a parameter
characterizing this distribution.
</p>
<p>4.6 &Dagger;Isothermal&ndash;Isobaric Ensemble
</p>
<p>In this section, we derive an appropriate ensemble for describing a closed sys-
</p>
<p>tem held at a given temperature T and a pressure P. Our system A is the region
</p>
<p>inside a cylinder fitted with a piston. The quantity of interest is the probabil-
</p>
<p>ity p(Ea,Va)dEadVa that A has the energy in (Ea &minus; dEa,Ea] and the volume in
(Va,Va +dVa]. As with our derivation of canonical ensemble, we use a microcanon-
ical ensemble as our starting point. Thus, we consider the composite system AWB
</p>
<p>consisting of system A, the piston W, and the surrounding B, and then isolate AWB
</p>
<p>from the rest of the universe by means of an adiabatic, rigid, and impermeable wall.
</p>
<p>See Fig. 4.5. Under the usual assumption of weak interaction, the Hamiltonian Hawb
of the composite system may be written as
</p>
<p>Hawb = H
&prime;
a +H
</p>
<p>&prime;
b +Kw +φaw +φbw , (4.93)
</p>
<p>where we use &prime; to indicate that H &prime;a contains the generalized coordinates and their
conjugate momenta pertaining only to the degrees of freedom of system A but not
</p>
<p>to the degrees of freedom of the piston. Likewise for H &prime;b. Kw := p
2
w/2mw is the
</p>
<p>kinetic energy of the piston, φaw denotes the potential energy due to the interaction
between system A and the piston, and likewise for φbw. Now, we set Ha := H
</p>
<p>&prime;
a+φaw
</p>
<p>and Hb := H
&prime;
b +φbw and write
</p>
<p>Hawb = Ha +Hb +Kw . (4.94)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 &Dagger;Isothermal&ndash;Isobaric Ensemble 191
</p>
<p>Applying (4.32) to the composite system AWB, we see that the total number of
</p>
<p>microstates accessible to the composite system as a whole is given by
</p>
<p>Ω awb(Eawb)∆E =
&int;
</p>
<p>δ (Eawb &minus;Hawb)∆E
dq fa dp fa
</p>
<p>h faPa
</p>
<p>dxwdpw
</p>
<p>h
</p>
<p>dq fb dp fb
</p>
<p>h fbPb
, (4.95)
</p>
<p>where xw and pw refer to the position and the linear momentum of the piston, respec-
</p>
<p>tively. Using (D.26),
</p>
<p>δ (Eawb &minus;Hawb) = δ (Eawb &minus;Ha &minus;Hb &minus;Kw)
</p>
<p>=
&int;
</p>
<p>δ (Eawb &minus;Ea &minus;Kw &minus;Hb)δ (Ea &minus;Ha)dEa . (4.96)
</p>
<p>Substituting this expression into (4.95), we identify
</p>
<p>Ω a(Ea,xw) =
&int;
</p>
<p>δ (Ea &minus;Ha)
dq fa dp fa
</p>
<p>h faPa
(4.97)
</p>
<p>as the density of states for A. The indicated xw dependence arises through that of
</p>
<p>Ha. Similarly,
</p>
<p>Ω b(Eawb &minus;Ea &minus;Kw,xw) =
&int;
</p>
<p>δ (Eawb &minus;Ea &minus;Kw &minus;Hb)
dq fb dp fb
</p>
<p>h fbPb
. (4.98)
</p>
<p>It follows that
</p>
<p>Ω awb(Eawb)∆E =
&int;&int;&int;
</p>
<p>Ω b(Eawb &minus;Ea &minus;Kw,xw)∆E Ω a(Ea,xw)dEa
dxwdpw
</p>
<p>h
.
</p>
<p>(4.99)
</p>
<p>In this equation, Ω b∆E is the number of microstates for system B when its energy
is in the interval (Eawb &minus; Ea &minus;Kw &minus; ∆E,Eawb &minus; Ea &minus;Kw] and piston W is at xw.
Similarly, Ω adEa is the number of microstates for system A when its energy is in
(Ea&minus;dEa,Ea] with the piston at xw. According to (4.99), dxwdpw/h is then the num-
ber of microstates for the piston when its phase point is somewhere in the infinitesi-
</p>
<p>mal phase volume dxwdpw. This result is consistent with the interpretation we have
</p>
<p>given to the quantum mechanical correction factor due to the Heisenberg uncertainty
</p>
<p>principle.
</p>
<p>As before, we use Boltzmann&rsquo;s entropy formula
</p>
<p>Ω b(Eawb &minus;Ea &minus;Kw,xw)∆E = eSb(Eawb&minus;Ea&minus;Kw,xw)/kB (4.100)
</p>
<p>and expand Sb in Taylor series:
</p>
<p>Sb(Eawb &minus;Ea &minus;Kw,xw) = Sb0 &minus;
&part;Sb
&part;Eb
</p>
<p>∣∣∣∣
0
</p>
<p>(Ea +Kw)+
&part;Sb
&part;xw
</p>
<p>∣∣∣∣
0
</p>
<p>xw +h.o., (4.101)</p>
<p/>
</div>
<div class="page"><p/>
<p>192 4 Various Statistical Ensembles
</p>
<p>where the subscript 0 on Sb and its partial derivatives indicates that they are to be
</p>
<p>computed at Eb = Eawb and xw = 0, that is, system A has zero volume and zero
energy.
</p>
<p>That such a situation is extremely unlikely to occur for nonempty system A is of
</p>
<p>no concern for us here. The indicated state servers only as a reference point around
</p>
<p>which to perform the Taylor series expansion of Sb, the entropy of system B. All we
</p>
<p>need is that Sb is well defined and differentiable at that state.
</p>
<p>The first partial derivative in this equation is just 1/T . In order to transform the
second partial derivative into something familiar, let Aw := Va/xw denote the cross
sectional area of the cylinder. Then,
</p>
<p>&part;Sb
&part;xw
</p>
<p>∣∣∣∣
0
</p>
<p>xw =
&part;Sb
</p>
<p>&part; (Awxw)
</p>
<p>∣∣∣∣
0
</p>
<p>Awxw =
&part;Sb
&part;Va
</p>
<p>∣∣∣∣
0
</p>
<p>Va =&minus;
&part;Sb
&part;Vb
</p>
<p>∣∣∣∣
0
</p>
<p>Va =&minus;
P
</p>
<p>T
Va . (4.102)
</p>
<p>Thus, ignoring the higher order terms, we have
</p>
<p>Sb(Eawb &minus;Ea &minus;Kw,xw) = Sb0 &minus;
Ea +Kw
</p>
<p>T
&minus; PVa
</p>
<p>T
. (4.103)
</p>
<p>Combining the results, we rewrite (4.99) as
</p>
<p>Ω awb(Eawb)∆E =
&int;&int;&int;
</p>
<p>eSb0/kBe&minus;βEa&minus;βKw&minus;βPVaΩ a(Ea,Va)dEa
dxwdpw
</p>
<p>h
. (4.104)
</p>
<p>Performing the integration with respect to pw and replacing dxw by dVa/Aw, we
obtain
</p>
<p>Ω awb(Eawb)∆E = e
Sb0/kB
</p>
<p>&int;
e&minus;βPVa
</p>
<p>&int;
Ω a(Ea,Va)e
</p>
<p>&minus;βEa dEadVa
AwΛw
</p>
<p>, (4.105)
</p>
<p>where Λw := h/
&radic;
</p>
<p>2πmwkBT is the thermal wavelength of the piston W. Because
the 1/Λw factor arose upon the integration of e&minus;βKwdxwdpw/h with respect to pw,
dxw/Λw = dVa/AwΛw may be conveniently interpreted as the number of microstates
accessible to the piston W when it is found somewhere between xw and xw + dxw
regardless of its momentum.
</p>
<p>By carrying out the integrations with respect to Ea and Va only over intervals of
</p>
<p>infinitesimal widths dEa and dVa, we find the number of microstates accessible to
</p>
<p>the composite system AWB when Ha and VA are within these infinitesimal intervals:
</p>
<p>eSb0/kBe&minus;βPVaΩ a(Ea,Va)e
&minus;βEa dEadVa
</p>
<p>AwΛw
. (4.106)
</p>
<p>The desired probability, p(Ea,Va)dEadVa, is obtained by dividing this expression by
(4.105).
</p>
<p>Once again, we can drop the subscript a without a risk of confusion. Thus,
</p>
<p>p(E,V )dEdV =
1
</p>
<p>Y
e&minus;βPVΩ(E,V )e&minus;βE
</p>
<p>dEdV
</p>
<p>AwΛw
, (4.107)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 &Dagger;Isothermal&ndash;Isobaric Ensemble 193
</p>
<p>where we defined the isothermal&ndash;isobaric partition function by
</p>
<p>Y :=
&int;
</p>
<p>e&minus;βPV
&int;
</p>
<p>Ω(E,V )e&minus;βE
dEdV
</p>
<p>AwΛw
. (4.108)
</p>
<p>The free energy associated with this ensemble is, by definition, the Gibbs free
</p>
<p>energy:
</p>
<p>G :=&minus;kBT lnY . (4.109)
Recall that Ω is a function of E, V , and the number of mechanical degrees of free-
dom f . When this is multiplied by the Boltzmann factor and e&minus;βPV and then inte-
grated with respect to E and V , we obtain a function of T , P, and f . The Gibbs free
</p>
<p>energy G also depends on this same set of variables. Equation (4.109) is therefore a
</p>
<p>fundamental equation of the system.
</p>
<p>In (4.108), the integration with respect to V extends over all possible values.
</p>
<p>Because the surrounding can be made arbitrarily large, the upper limit of the inte-
</p>
<p>gration over V is usually set to &infin;. This is acceptable if the integrand vanishes suf-
ficiently fast, which is usually the case if P &gt; 0. On the other hand, the lower limit
is set to zero, the theoretical minimum. Except for an ideal gas, Ω(E,V ) becomes
negligibly small with increasing N/V unless E is extremely large. For example,
imagine a densely packed liquid phase, in which there is little room for a molecule
</p>
<p>to move around. But, if E is very large, the Boltzmann factor effectively kills off the
</p>
<p>integrand and the integration with respect to V from V = 0 to &infin; is finite.
The integral &int;
</p>
<p>Ω(E,V )e&minus;βEdE (4.110)
</p>
<p>in (4.108) is the canonical partition function Z of system A when its volume is V .
</p>
<p>The partition function Y is the Laplace transform of Z.
</p>
<p>Using (4.32) in (4.108), we arrive at
</p>
<p>Y =
1
</p>
<p>h f P
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;βPV
</p>
<p>&int;
e&minus;βH(q
</p>
<p>f ,p f )dq f dp f
dV
</p>
<p>AwΛw
. (4.111)
</p>
<p>Interpreting the integral signs as the sum over microstates, we identify
</p>
<p>ρ(q f , p f ,V )dq f dp f dV =
1
</p>
<p>Y
e&minus;βPV e&minus;βH(q
</p>
<p>f ,p f ) dq
f dp f
</p>
<p>h f P
</p>
<p>dV
</p>
<p>AwΛw
(4.112)
</p>
<p>as the probability that the piston W defines volume between V and V +dV and sys-
tem A has its phase point somewhere in the phase volume dq f dp f centered around
</p>
<p>(q f , p f ).
</p>
<p>Exercise 4.6. Deduce Gibbs entropy formula by combining (2.196), (4.107), and
</p>
<p>(4.109). Do the same using (4.112). ///
</p>
<p>Because of the 1/AwΛw factor, the isothermal-isobaric partition function is
dimensionless. It is customary, however, to avoid an explicit reference to the piston,</p>
<p/>
</div>
<div class="page"><p/>
<p>194 4 Various Statistical Ensembles
</p>
<p>and redefine the partition function Y by
</p>
<p>Y :=
</p>
<p>&int;
e&minus;βPV
</p>
<p>&int;
Ω(E,V )e&minus;βEdEdV . (4.113)
</p>
<p>Using (4.32), we arrive at
</p>
<p>Y =
1
</p>
<p>h f P
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;βPV
</p>
<p>&int;
e&minus;βH(q
</p>
<p>f ,p f )dq f dp f dV . (4.114)
</p>
<p>This partition function has the dimension of volume. When the Gibbs free energy
</p>
<p>is calculated as &minus;kBT lnY using this dimensional Y , its numerical value depends
on the unit used to measure length. A change in the unit of length amounts to an
</p>
<p>addition of a constant to the Gibbs free energy. This is of no consequence when
</p>
<p>discussing the difference in the Gibbs free energy between two states. We also note
</p>
<p>that p(E,V )dEdV given by (4.107) remains unaffected when we drop the 1/AwΛw
factor. We have kept the factor up to this point since such a notion as the number of
</p>
<p>microstates becomes very difficult to conceive of otherwise.
</p>
<p>Exercise 4.7. Find the partition functions Y (without AwΛw) for an ideal gas, mod-
eled as a system of N noninteracting identical particles. You need to recall the defi-
</p>
<p>nition and properties of the Gamma function from Exercise 4.2. ///
</p>
<p>Example 4.3. A system of three rods:25 As shown in Fig. 4.6, a chain of three
</p>
<p>rod-shape molecules is confined to a cylinder of cross sectional area A fitted
</p>
<p>with a piston and is held under a constant temperature T and pressure P. The
</p>
<p>sequence of molecules ABC cannot be changed, but each molecule can take
</p>
<p>two distinct orientations, that is, horizontal (A and C in Fig. 4.6) and vertical
</p>
<p>(B in Fig. 4.6). The length of the molecule is l and the width is w. System
</p>
<p>energy E is always 0 independent of the orientations of the molecules. Let us
</p>
<p>find the equation of state, that is, an equation relating the average length 〈L〉
of the system to T and P.
</p>
<p>L
</p>
<p>l
</p>
<p>w
P
</p>
<p>A B C
</p>
<p>Fig. 4.6 A system of three rods held at a constant temperature and pressure.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 &Dagger;Isothermal&ndash;Isobaric Ensemble 195
</p>
<p>The following orientations of molecules (A,B,C) are possible:
</p>
<p>(w,w,w) L = 3w ,
</p>
<p>(l,w,w) , (w, l,w) , (w,w, l) L = 2w+ l ,
</p>
<p>(l, l,w) , (l,w, l) , (w, l, l) L = w+2l ,
</p>
<p>(l, l, l) L = 3l ,
</p>
<p>where we also included the corresponding values of L. The energy is zero
</p>
<p>regardless of the configuration. Thus, the density of states is given by
</p>
<p>Ω(E,L)
</p>
<p>= δ (E) [δ (L&minus;3w)+3δ (L&minus;2w&minus; l) +3δ (L&minus;w&minus;2l)+δ (L&minus;3l)] .
(4.115)
</p>
<p>To see this, we note that Ω(E,L), upon integration over some domain on
the EL-plane, gives the number of states within that domain. Thus, the point
</p>
<p>(E,L) = (0,3w), if included, should contribute 1 to the integral while the point
(E,L) = (0,2w+ l) should yield 3 upon integration. Equation (4.115) is com-
patible with this requirement.
</p>
<p>Using (4.115) in (4.113),
</p>
<p>Y =
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;βPV
</p>
<p>&int;
Ω(E,L)e&minus;βEdEdV
</p>
<p>=
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;βPAL [δ (L&minus;3w)+3δ (L&minus;2w&minus; l)
</p>
<p>+3δ (L&minus;w&minus;2l)+δ (L&minus;3l)]AdL
= A(x3 +3x2y+3xy2 + y3) = A(x+ y)3 , (4.116)
</p>
<p>where x := e&minus;βPAw and y := e&minus;βPAl . Finally,
</p>
<p>〈V 〉=&minus;
(
&part; lnY
</p>
<p>&part;βP
</p>
<p>)
</p>
<p>β
</p>
<p>=&minus; 3
x+ y
</p>
<p>[(
&part;x
</p>
<p>&part;βP
</p>
<p>)
</p>
<p>β
</p>
<p>+
</p>
<p>(
&part;y
</p>
<p>&part;βP
</p>
<p>)
</p>
<p>β
</p>
<p>]
=
</p>
<p>3A(wx+ ly)
</p>
<p>x+ y
.
</p>
<p>(4.117)
</p>
<p>Dividing by A, we find
</p>
<p>〈L〉= 3(wx+ ly)
x+ y
</p>
<p>, (4.118)
</p>
<p>which is the desired equation of state.
</p>
<p>If you prefer, you can stick to the formalism we used in Example 4.2 and
</p>
<p>work with W (E,L), which is the number of states with the energy less than
or equal to E and the length less than or equal to L. Note that no states are
</p>
<p>allowed unless E = 0. So, W is zero if E &lt; 0 and positive if E &ge; 0. This
implies that W is proportional to θ(E).</p>
<p/>
</div>
<div class="page"><p/>
<p>196 4 Various Statistical Ensembles
</p>
<p>Let w &lt; l and assume that E = 0. If L &lt; 3w, W = 0, while W = 1 if
3w &le; L &lt; 2w+ l. Proceeding similarly with larger values of L, we get
</p>
<p>W (E,L) = θ(E) [θ(L&minus;3w)+3θ(L&minus;2w&minus; l)
+3θ(L&minus;w&minus;2l)+θ(L&minus;3l)] . (4.119)
</p>
<p>Note that the product θ(E)θ(L&minus;3w) is nonzero only if E &ge; 0 and L &ge; 3w. A
similar remark applies to other products in (4.119). Equation (4.115) follows
</p>
<p>from
</p>
<p>Ω(E,L) =
&part; 2W
</p>
<p>&part;E&part;L
. (4.120)
</p>
<p>To see that this is a proper way to compute Ω(E,L), note that
</p>
<p>&int; E2
E1
</p>
<p>&int; L2
L1
</p>
<p>Ω(E,L)dLdE =
&int; E2
</p>
<p>E1
</p>
<p>[
&part;W (E,L2)
</p>
<p>&part;E
&minus; &part;W (E,L1)
</p>
<p>&part;E
</p>
<p>]
dE
</p>
<p>= W (E2,L2)&minus;W (E1,L2)&minus;W (E2,L1)+W (E1,L1) , (4.121)
</p>
<p>which may be recognized as the number of states with E1 &lt; E &le; E2 and L1 &lt;
L &le; L2. You can convince yourself of this by drawing rectangles on the EL-
plane.
</p>
<p>Exercise 4.8. Generalize Example 4.3 for a system of N rods. ///
</p>
<p>4.7 Grand Canonical Ensemble
</p>
<p>In this section, we shall consider an open system which can exchange energy and
</p>
<p>particles of a given species with the surroundings. By means of this exchange, the
</p>
<p>temperature and the chemical potential of that species in the system are held at
</p>
<p>constant values.
</p>
<p>Our formulation generalizes quite straightforwardly to a system open to multi-
</p>
<p>ple species, but the notation becomes quite unmanageable. Thus, we refrain from
</p>
<p>exploring this more general situation here. The formulation we present does not
</p>
<p>exclude mixtures as long as the other species are confined to the system of our
</p>
<p>interest.
</p>
<p>As before, the quantity of interest is the probability p(Ea,Na)dEa that system A
contains Na particles and has the energy in Ea &minus; dEa &lt; Ha &le; Ea. The expression
for the probability will be found using microcanonical ensemble. Once again, we
</p>
<p>construct a composite system AB consisting of the system of interest A and the
</p>
<p>surroundings B, and then isolate AB from the rest of the universe by means of an
</p>
<p>adiabatic, rigid, and impermeable wall as shown in Fig. 4.7.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Grand Canonical Ensemble 197
</p>
<p>T ,
</p>
<p>System A
</p>
<p>Surroundings B
</p>
<p>Isolated system AB
</p>
<p>Fig. 4.7 The system of interest A is held at a constant temperature T and chemical potential &micro; due
to exchange of energy and particles with the surroundings B. The composite system AB is isolated
</p>
<p>from the rest of the universe by means of an adiabatic, rigid, and impermeable wall.
</p>
<p>The argument that led to (4.64) generalizes directly to the current situation simply
</p>
<p>by noting that Ω a, Ω b, and hence p now depends on Na as well as on Ea:
</p>
<p>p(Ea,Na)dEa =
Ω b(Eab &minus;Ea,Nab &minus;Na)∆EΩ a(Ea,Na)dEa
</p>
<p>Ω ab(Eab,Nab)∆E
. (4.122)
</p>
<p>The normalization condition of p(Ea,Na) leads to
</p>
<p>Ω ab(Eab,Nab)∆E =&sum;
Na
</p>
<p>&int;
Ω b(Eab &minus;Ea,Nab &minus;Na)∆EΩ a(Ea,Na)dEa , (4.123)
</p>
<p>where the integral is taken over all possible values of Ea and the summation is over
</p>
<p>all possible values of Na. The bounds on Na will be specified later. We can interpret
</p>
<p>(4.123) by following the argument similar to the one given to (4.65).
</p>
<p>In order to transform (4.122) to a more convenient form, we define the chemical
</p>
<p>potential &micro; of a particle in the surroundings by
</p>
<p>&minus; &micro;
T
</p>
<p>:=
&part;Sb(Eb)
</p>
<p>&part;Nb
</p>
<p>∣∣∣∣
0
</p>
<p>, (4.124)
</p>
<p>where we introduced the subscript 0 to indicate that quantities bearing them are to
</p>
<p>be computed at Eb = Eab and Nb = Nab, that is, the surroundings have all the energy
and particles of the composite system.
</p>
<p>If the number of mechanical degrees of freedom of B is much larger than that
</p>
<p>of A, then the Taylor series expansion of Sb(Eab &minus;Ea,Nab &minus;Na) will be sufficiently
accurate when only the first-order terms in Ea and Na are retained. This gives
</p>
<p>Ω b(Eab &minus;Ea,Nab &minus;Na)∆E = eSb0/kBe&minus;βEa+β&micro;Na , (4.125)</p>
<p/>
</div>
<div class="page"><p/>
<p>198 4 Various Statistical Ensembles
</p>
<p>in terms of which we rewrite (4.123) as
</p>
<p>Ω ab(Eab,Nab)∆E = e
Sb0/kB&sum;
</p>
<p>Na
</p>
<p>eβ&micro;Na
&int;
</p>
<p>Ω a(Ea,Na)e
&minus;βEadEa . (4.126)
</p>
<p>As with other ensembles, we drop the subscript a here and substitute the last two
</p>
<p>equations into (4.122):
</p>
<p>p(E,N)dE =
1
</p>
<p>Ξ
eβ&micro;NΩ(E,N)e&minus;βEdE , (4.127)
</p>
<p>where
</p>
<p>Ξ =&sum;
N
</p>
<p>eβ&micro;N
&int;
</p>
<p>Ω(E,N)e&minus;βEdE (4.128)
</p>
<p>is the grand canonical partition function. The free energy associated with this
</p>
<p>ensemble, by definition, is the grand potential:26
</p>
<p>Ω :=&minus;kBT lnΞ . (4.129)
</p>
<p>Recall that Ω is a function of E, V , N, and the number f of any remaining mechani-
cal degrees of freedom. Any other species, if present in the system, afford an exam-
</p>
<p>ple of such remaining degrees of freedom. Thus, Ξ and Ω are functions of T , V , &micro; ,
and f . Equation (4.129) is then a fundamental equation of the system.
</p>
<p>For a classical mechanical system of identical particles, (4.128) may be written
</p>
<p>as
</p>
<p>Ξ =&sum;
N
</p>
<p>eβ&micro;N
</p>
<p>h3NN!
</p>
<p>&int;
e&minus;βH(r
</p>
<p>N ,pN)drNdpN . (4.130)
</p>
<p>We interpret &sum;N
&int;
</p>
<p>as the sum over states. Then,
</p>
<p>ρ(rN ,pN ,N)drNdpN =
1
</p>
<p>Ξ
eβ&micro;Ne&minus;βH(r
</p>
<p>N ,pN) dr
NdpN
</p>
<p>h3NN!
(4.131)
</p>
<p>is the probability that system A contains N particles and is found inside the phase
</p>
<p>volume drNdpN taken around the phase point (rN ,pN).
</p>
<p>Exercise 4.9. Combine (2.202), (4.127), and (4.129) to deduce Gibbs&rsquo;s entropy for-
</p>
<p>mula. Do the same using (4.131). ///
</p>
<p>In arriving at these results, we assumed that the number of mechanical degrees
</p>
<p>of freedom of system B is much larger than that of system A. This implies that the
</p>
<p>upper limit of the summation over N should be kept sufficiently small compared to
</p>
<p>the total number of particles in the composite system. However, the surroundings
</p>
<p>can be made arbitrarily large. Moreover, for a finite system volume V , Ω(E,N)
becomes negligibly small with increasing N unless E is extremely large, and the
</p>
<p>Boltzmann factor kills off the integrand. Thus, it is customary to set the upper limit
</p>
<p>of the summation to &infin;. Equation (4.128) may be regarded as a discrete version of
the Laplace transform of the canonical partition function Z.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Grand Canonical Ensemble 199
</p>
<p>Let us look at the other end. In principle, system A can become empty. So, the
</p>
<p>lower limit must be set to N = 0. What is the value of the summand in this case?
If we just look at (4.130), the answer is unclear because there is no variable with
</p>
<p>respect to which to perform the integration. Let us step back a little and note that
</p>
<p>&int;
p(E,0)dE (4.132)
</p>
<p>is the probability that system A is empty, which, as we have just noted, can be
</p>
<p>nonzero. On the other hand, we expect that p(E,0) should be zero unless E is equal
to zero (or whatever any other value we assign to the vacuum):
</p>
<p>p(E,0) = 0 if E 	= 0 . (4.133)
</p>
<p>These two equations mean that p(E,0) must be proportional to the δ -function δ (E).
Now, it is quite sensible to demand that, when considering system A alone, the
</p>
<p>number of microstates accessible to A when it is empty should be just one. That is,
</p>
<p>once the system boundary is fixed, there should be only one kind of vacuum:
</p>
<p>&int;
Ω(E,0)dE = 1 . (4.134)
</p>
<p>We see from (4.127) that Ω(E,0) is proportional to p(E,0), and hence to δ (E).
From (4.134), we conclude that
</p>
<p>Ω(E,0) = δ (E) , (4.135)
</p>
<p>which can be used in the N = 0 term in (4.128) to yield
</p>
<p>eβ&micro; &middot;0
&int;
</p>
<p>Ω(E,0)e&minus;βEdE = 1 . (4.136)
</p>
<p>So, the N = 0 term is unity. Because (4.130) was obtained from (4.128), this con-
clusion holds for the N = 0 term in (4.130) as well.
</p>
<p>Example 4.4. Ideal gas: Let us find the partition functions Ξ for a pure ideal
gas, defined as a system of noninteracting identical particles. From (4.130),
</p>
<p>Ξ =
&infin;
</p>
<p>&sum;
N=0
</p>
<p>eβ&micro;N
</p>
<p>h3NN!
</p>
<p>&int;
e&minus;βHdrNdpN =
</p>
<p>&infin;
</p>
<p>&sum;
N=0
</p>
<p>V Neβ&micro;N
</p>
<p>Λ 3NN!
=
</p>
<p>&infin;
</p>
<p>&sum;
N=0
</p>
<p>1
</p>
<p>N!
</p>
<p>(
Veβ&micro;
</p>
<p>Λ 3
</p>
<p>)N
.
</p>
<p>(4.137)
</p>
<p>Using (B.5), we find
</p>
<p>Ξ = exp
</p>
<p>[
Veβ&micro;
</p>
<p>Λ 3
</p>
<p>]
. (4.138)</p>
<p/>
</div>
<div class="page"><p/>
<p>200 4 Various Statistical Ensembles
</p>
<p>The quantity
</p>
<p>z :=
eβ&micro;
</p>
<p>Λ 3
(4.139)
</p>
<p>occurring in (4.138) is referred to as the absolute fugacity. For an ideal gas, this
</p>
<p>quantity is just the number density of particles:
</p>
<p>z =
N
</p>
<p>V
(ideal gas only). (4.140)
</p>
<p>To see this, we recall (2.202) and the Euler relation (2.148) to obtain
</p>
<p>Ω =&minus;PV (homogeneous system only). (4.141)
</p>
<p>Since (2.148) holds only for a homogeneous body, the same restriction applies to
</p>
<p>this identity. Of course, an instantaneous configuration of particles will never be
</p>
<p>homogeneous. But, in writing thermodynamic identity such as (4.141), we are con-
</p>
<p>cerned only with the average behavior over a long duration of time. When inter-
</p>
<p>action between the particles and the system wall can be ignored, and if there is
</p>
<p>no external field, all points in the system are equivalent and a fluid phase will be
</p>
<p>homogeneous, again upon long-time averaging.
</p>
<p>Using (4.138),
</p>
<p>&minus;PV =&minus;kBT lnΞ =&minus;kBT zV . (4.142)
Recalling the ideal gas equation of state, PV = NkBT , we arrive at (4.140). For an
explicit derivation of the equation of state, see Exercise 4.11.
</p>
<p>Example 4.5. Probability of a cavity formation: A gas consisting of a large
</p>
<p>number (N) of noninteracting identical particles is held at a constant temper-
</p>
<p>ature T and occupies a macroscopic volume V . If we focus on a very small
</p>
<p>region of space of volume v taken inside V , where v ≪V , the number of parti-
cles in v will fluctuate with time. Let us find the probability p0 that v contains
</p>
<p>no particle at all.
</p>
<p>Approach 1: The Hamiltonian of the macroscopic V may be written as the
</p>
<p>sum of two Hamiltonians one pertaining only to the particles in v and the
</p>
<p>other only to V &minus;v. This implies that the interaction between regions v and
V &minus; v is sufficiently weak. Because V is much larger than v, region V &minus; v
acts as a reservoir of the energy and the particles for region v, allowing us
</p>
<p>to describe region v by a grand canonical ensemble. The relevant partition
</p>
<p>function is
</p>
<p>Ξ =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>eβ&micro;n
&int;
</p>
<p>Ω(E,n)e&minus;βEdE = exp
</p>
<p>[
veβ&micro;
</p>
<p>Λ 3
</p>
<p>]
. (4.143)
</p>
<p>We recall that the n-th term in the summation is the unnormalized proba-
</p>
<p>bility of finding n particles in v and that Ξ is the normalization constant.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Grand Canonical Ensemble 201
</p>
<p>The desired probability, therefore, is obtained by dividing the n = 0 term
by Ξ . But, the term in question is just one. So,
</p>
<p>p0 =
1
</p>
<p>Ξ
= exp
</p>
<p>[
&minus;ve
</p>
<p>β&micro;
</p>
<p>Λ 3
</p>
<p>]
. (4.144)
</p>
<p>Equation (4.142) applied to region v gives
</p>
<p>&minus;Pv =&minus;kBT lnΞ , (4.145)
</p>
<p>Thus,
</p>
<p>p0 =
1
</p>
<p>Ξ
= e&minus;Pv/kBT , (4.146)
</p>
<p>in which we recognize Pv as the reversible work required to create a cavity
</p>
<p>of volume v in a gas held at the pressure P. This result is an example of
</p>
<p>(2.180).
</p>
<p>Approach 2: We can treat the entire gas phase in V using a canonical ensem-
</p>
<p>ble, for which the relevant partition function is
</p>
<p>Z =
1
</p>
<p>N!
</p>
<p>V N
</p>
<p>Λ 3N
, (4.147)
</p>
<p>where N is the total number of particles in V . Only a subset of all
</p>
<p>microstates embraced by Z is consistent with the condition that v being
</p>
<p>empty. The partition function computed under this condition is
</p>
<p>Z0 =
1
</p>
<p>N!
</p>
<p>(V &minus; v)N
Λ 3N
</p>
<p>. (4.148)
</p>
<p>Thus,
</p>
<p>p0 =
Z0
</p>
<p>Z
=
(
</p>
<p>1&minus; v
V
</p>
<p>)N
(4.149)
</p>
<p>Let us take the limit of N &rarr; &infin; while holding nv := N/V constant. This is
known as the thermodynamic limit. With the help of (B.24), we see that
</p>
<p>p0 approaches
</p>
<p>e&minus;nvv (4.150)
</p>
<p>in this limit. But, this is just (4.146) because of the ideal gas equation of
</p>
<p>state. Note that the use of grand canonical ensemble in Approach 1 implies
</p>
<p>the thermodynamic limit for region V &minus; v.
</p>
<p>Exercise 4.10. For Ξ given by (4.128),
</p>
<p>a. Show that
</p>
<p>〈N〉=
(
&part; lnΞ
</p>
<p>&part;β&micro;
</p>
<p>)
</p>
<p>β ,V
</p>
<p>. (4.151)</p>
<p/>
</div>
<div class="page"><p/>
<p>202 4 Various Statistical Ensembles
</p>
<p>b. Show that
</p>
<p>〈N2〉&minus;〈N〉2 =
(
&part; 2 lnΞ
</p>
<p>&part;β&micro;2
</p>
<p>)
</p>
<p>β ,V
</p>
<p>. (4.152)
</p>
<p>The notation is a bit clumsy here, but the derivative is twice with respect to β&micro; .
c. Show that (
</p>
<p>&part; 2 lnΞ
</p>
<p>&part;β&micro;2
</p>
<p>)
</p>
<p>β ,V
</p>
<p>=
kBTκT 〈N〉2
</p>
<p>V
, (4.153)
</p>
<p>where
</p>
<p>κT =&minus;
1
</p>
<p>V
</p>
<p>(
&part;V
</p>
<p>&part;P
</p>
<p>)
</p>
<p>T,N
</p>
<p>(4.154)
</p>
<p>is the isothermal compressibility. ///
</p>
<p>Combining (4.152) and (4.153), we obtain
</p>
<p>〈N2〉&minus;〈N〉2
〈N〉2 =
</p>
<p>kBT nvκT
〈N〉 , (4.155)
</p>
<p>where nv := 〈N〉/V = 1/V . But, since kBT nvκT is an intensive quantity, (4.155)
implies that
</p>
<p>∆rmsN
</p>
<p>〈N〉 &sim;
&radic;
</p>
<p>1
</p>
<p>〈N〉 , (4.156)
</p>
<p>which is just another example of (4.15).
</p>
<p>Exercise 4.11. In this chapter, we found expressions for partition functions CM , Z,
</p>
<p>Y , and Ξ of a pure ideal gas. Assuming that N is large, find the relationships among:
</p>
<p>a. T , U , and N
</p>
<p>b. T , P V , and N
</p>
<p>c. e&micro;/kBT , V , N, and Λ
</p>
<p>for each ensemble. Verify that, for large N, the results are independent of the choice
</p>
<p>of the ensemble. (Omit Y if you skipped Sect. 4.6.) ///
</p>
<p>Exercise 4.12. So far, we discussed canonical, isothermal&ndash;isobaric (Sect. 4.6),and
</p>
<p>grand canonical ensembles. They are suitable for describing a system held at con-
</p>
<p>stant (T,V,N), (T,P,N), and (T,V,&micro;), respectively. In this problem, you are invited
to explore the possibility of constructing a statistical ensemble suitable for a system
</p>
<p>held at constant (T,P,&micro;).
By a simple generalization of the expressions for Z, Y , and Ξ , it seems reasonable
</p>
<p>to expect that the partition function X for this ensemble is given by
</p>
<p>X(T,P,&micro;) =
&int; &infin;
</p>
<p>0
e&minus;βPV
</p>
<p>&infin;
</p>
<p>&sum;
N=0
</p>
<p>eβ&micro;N
</p>
<p>h3NN!
</p>
<p>&int;
e&minus;βH(r
</p>
<p>N ,pN)drNdpNdV (4.157)
</p>
<p>in the case of a classical system of identical particles.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Frequently Used Symbols 203
</p>
<p>a. What is the probability ρ(rN ,pN ,V,N)drNdpNdV of finding the system with N
particles, with volume in the interval (V,V + dV ], and within an infinitesimal
volume element drNdpN taken around the phase point (rN ,pN)?
</p>
<p>b. Using Gibbs&rsquo;s entropy formula, evaluate the numerical value of &minus;kBT lnX . Is
this ensemble useful at all? Why or why not?
</p>
<p>c. Suppose that the system is an ideal gas. Does X for the ideal gas agree with what
</p>
<p>you found in part b? If not, provide a possible explanation for the disagreement.
</p>
<p>///
</p>
<p>4.8 Frequently Used Symbols
</p>
<p>〈A〉 , ensemble average of a dynamical variable A.
</p>
<p>f , the number of mechanical degrees of freedom.
</p>
<p>h , Planck constant. 6.626&times;10&minus;34(J&middot;s).
h̄ , h/2π .
kB , Boltzmann constant, 1.3806&times;10&minus;23 J/K.
mi , mass of the ith particle.
</p>
<p>pi , generalized momentum conjugate to qi.
</p>
<p>p f , collective notation for p1, . . . , p f .
</p>
<p>pi , linear momentum of the ith particle.
</p>
<p>pN , collective notation for p1, . . . , pN .
</p>
<p>qi , the ith generalized coordinate.
</p>
<p>q f , collective notation for q1, . . . , q f .
</p>
<p>ri , position vector of the ith particle.
</p>
<p>rN , collective notation for r1, . . . , rN .
</p>
<p>vi , velocity vector of the ith particle.
</p>
<p>vN , collective notation for v1, . . . , vN .
</p>
<p>z , absolute fugacity. eβ&micro;/Λ 3.
</p>
<p>A , a generic dynamical variable.
</p>
<p>CM , microcanonical partition function.
</p>
<p>E , energy of the system.
</p>
<p>F , Helmholtz free energy.
</p>
<p>H , Hamiltonian.
</p>
<p>N , total number of particles in a system.
</p>
<p>P , pressure.
</p>
<p>S , entropy.
</p>
<p>T , absolute temperature.
</p>
<p>U , internal energy.
</p>
<p>V , volume.
</p>
<p>Y , isothermal&ndash;isobaric partition function.</p>
<p/>
</div>
<div class="page"><p/>
<p>204 4 Various Statistical Ensembles
</p>
<p>Z , canonical partition function.
</p>
<p>P , the number of permutations of identical particles.
</p>
<p>W (E) , the number of microstates with H &le; E.
</p>
<p>β , 1/kBT .
δ (x) , Dirac δ -function.
ρ , statistical weight.
&micro; , chemical potential.
θ(x) , step function defined by (D.2).
</p>
<p>Λ , thermal wavelength h/
&radic;
</p>
<p>2πmkBT of a particle of mass m.
Ξ , grand canonical partition function.
Ω , density of states.
</p>
<p>References and Further Reading
</p>
<p>1. Hill T L (1986) An Introduction to Statistical Thermodynamics. Dover, New York
</p>
<p>See Chap. 8 for a more detailed treatment of diatomic molecules. Chap. 9 is on polyatomic
</p>
<p>molecules.
</p>
<p>2. McQuarrie D A (2000) Statistical Mechanics. University Science Books, Sausalito, California</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>Simple Models of Adsorption
</p>
<p>As an illustration of canonical and grand canonical ensembles, we discuss a few
</p>
<p>variants of a simple model of adsorption. Despite their simplicity, these models
</p>
<p>provide important insights into diverse phenomena ranging from oxygen binding to
</p>
<p>hemoglobin to vapor&ndash;liquid phase coexistence.
</p>
<p>5.1 Exact Solutions
</p>
<p>Let us first look at simple exactly solvable model. It forms a basis for introducing
</p>
<p>approximations that become necessary when we deal with more complex model
</p>
<p>systems.
</p>
<p>5.1.1 Single Site
</p>
<p>Consider a single adsorption site exposed to a gas phase. We assume that the site
</p>
<p>can accommodate at most a single gas particle. The quantity of our primary interest
</p>
<p>is the average number 〈N〉 of gas particles adsorbed at the site. Before proceeding,
we note that this quantity is actually the probability p1 that the site is occupied by a
</p>
<p>particle since
</p>
<p>〈N〉= 0&times; (1&minus; p1)+1&times; p1 = p1 . (5.1)
If we define our system as in Fig. 5.1, it is an open system, for which the relevant
</p>
<p>ensemble is the grand canonical ensemble with the surrounding gas phase setting
</p>
<p>the temperature T and the chemical potential &micro; for the system.
We recall from Exercise 4.10 that
</p>
<p>〈N〉=
(
&part; lnΞ
</p>
<p>&part;β&micro;
</p>
<p>)
</p>
<p>β ,V
</p>
<p>. (5.2)
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 205
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1 5</p>
<p/>
</div>
<div class="page"><p/>
<p>206 5 Simple Models of Adsorption
</p>
<p>Open system
</p>
<p>Adsorption site
</p>
<p>Gas particle
</p>
<p>Closed system
</p>
<p>........................................................................................................................
..........
</p>
<p>..........
...........
...........
...........
</p>
<p>............................................................................................................... ......... .......................
..........
...........
..........
...........
...........
............
</p>
<p>............................
</p>
<p>Fig. 5.1 A single adsorption site model.
</p>
<p>So, to figure out 〈N〉, our first task is to compute Ξ , for which we need Ω(E,N).
To simplify our analysis, we suppose that the system can take only two distinct
</p>
<p>states: the unoccupied state for which E = 0 and the occupied state for which E =
&minus;ε &lt; 0. We shall refer to ε as the binding energy and take a closer look at it in the
next subsection. In a meanwhile, the following three cases need to be considered:
</p>
<p>a. N = 0. In this case, the energy E of the system can only be zero. Thus, there is no
state if E &lt; 0 or E &gt; 0, but there is a single state if E = 0, leading to the picture
in Fig. 5.2a. Thus,
</p>
<p>W (E,0) = θ(E) (5.3)
</p>
<p>and hence
</p>
<p>Ω(E,0) =
&part;W (E,0)
</p>
<p>&part;E
= δ (E) , (5.4)
</p>
<p>which is nothing but (4.135).
</p>
<p>b. N = 1. Now, the energy can only be &minus;ε and a consideration similar to case a
leads to the picture in Fig. 5.2b, from which we obtain
</p>
<p>W (E,1) = θ(E + ε) (5.5)
</p>
<p>E0
</p>
<p>1
</p>
<p>a. E 0
</p>
<p>E0
</p>
<p>1
</p>
<p>b. E 1
</p>
<p>Fig. 5.2 W (E,N) for N = 0 and N = 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Exact Solutions 207
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>0 2 4 6 8 10
</p>
<p>p
1
</p>
<p>x
</p>
<p>Fig. 5.3 Dependence of p1 on x := e
β&micro;eβε.
</p>
<p>and
</p>
<p>Ω(E,1) =
&part;W (E,1)
</p>
<p>&part;E
= δ (E + ε) . (5.6)
</p>
<p>c. N &ge; 2. In this case, W (E,N)&equiv; 0 and hence Ω(E,N)&equiv; 0.
</p>
<p>Using these results in (4.128), we find
</p>
<p>Ξ = eβ&micro;0
&int;
</p>
<p>δ (E)e&minus;βEdE + eβ&micro;1
&int;
</p>
<p>δ (E + ε)e&minus;βEdE +
&infin;
</p>
<p>&sum;
N=2
</p>
<p>eβ&micro;N
&int;
</p>
<p>0e&minus;βEdE
</p>
<p>= 1+ eβ&micro;eβε = 1+ x , (5.7)
</p>
<p>where we defined x := eβ (&micro;+ε). Using (5.2), we arrive at
</p>
<p>p1 = 〈N〉=
dlnΞ
</p>
<p>dx
</p>
<p>(
&part;x
</p>
<p>&part;β&micro;
</p>
<p>)
</p>
<p>β
</p>
<p>=
x
</p>
<p>1+ x
. (5.8)
</p>
<p>As shown in Fig. 5.3, 〈N〉 or the probability p1 that the site is occupied increases
with increasing x and approaches unity as x tends toward infinity. Note that the
</p>
<p>increase in x can be achieved by either decreasing T = 1/kBβ or increasing &micro; .
</p>
<p>5.1.2 &dagger;Binding Energy
</p>
<p>In the previous subsection, we assumed that the adsorption site can be in either
</p>
<p>of the two states, empty or occupied. To the latter state, we assigned the binding</p>
<p/>
</div>
<div class="page"><p/>
<p>208 5 Simple Models of Adsorption
</p>
<p>energy ε . In reality, however, an adsorbed particle can still move around through a
vibrational motion. Let us see how this aspect can be incorporated into our model.
</p>
<p>This time, we take the entire box shown in Fig. 5.1 as our system. The system
</p>
<p>is closed and held at a given temperature T . So the relevant partition function to
</p>
<p>compute is Z. Let Vt denote the volume of the system. The volume of the open
</p>
<p>system will be indicated by v. We suppose that there are Nt noninteracting identical
</p>
<p>particles in the system and write
</p>
<p>Z =
1
</p>
<p>h3Nt Nt!
</p>
<p>&int;
e&minus;βH(r
</p>
<p>Nt ,pNt )drNt dpNt =
1
</p>
<p>Λ 3Nt Nt !
</p>
<p>&int;
e&minus;βψ(r
</p>
<p>Nt )drNt , (5.9)
</p>
<p>where we carried out the integration with respect to pNt , and denoted the potential
</p>
<p>energy due to the interaction between a particle and the adsorption site by ψ .
We consider the particle to be adsorbed to the site if its center is anywhere in v.
</p>
<p>The site is otherwise empty. We set ψ to zero if no particle occupies v. If the ith
particle occupies the site and no other particle is in v at the same time, we write
</p>
<p>ψ = ψ(ri) to allow for the dependence of ψ on the exact position of the adsorbed
particle. As before, we do not allow for multiple particles to occupy the site simul-
</p>
<p>taneously, so we set ψ to infinity in that case. Thus,
</p>
<p>e&minus;βψ(r
Nt ) =
</p>
<p>⎧
⎨
⎩
</p>
<p>1 if v is empty,
</p>
<p>e&minus;βψ(ri) if v is occupied by the ith particle,
0 if v is occupied by more than one particle.
</p>
<p>(5.10)
</p>
<p>With this much preparations, we can rewrite the integral with respect to rNt as
</p>
<p>&int;
e&minus;βψ(r
</p>
<p>Nt )drNt =
&int;
</p>
<p>rNt &isin;Vt&minus;v
drNt +
</p>
<p>Nt
</p>
<p>&sum;
i=1
</p>
<p>&int;
</p>
<p>ri&isin;v,rNt&minus;1&isin;Vt&minus;v
e&minus;βψ(ri)drNt , (5.11)
</p>
<p>where the subscripts to
&int;
</p>
<p>denote the conditions imposed on the coordinates of par-
</p>
<p>ticles. The first integral is taken under the condition that all the particles are outside
</p>
<p>v, resulting in (Vt &minus;v)Nt . The integrand in the second term depends only on ri. Thus,
&int;
</p>
<p>ri&isin;v,rNt&minus;1&isin;Vt&minus;v
e&minus;βψ(ri)drNt = (Vt &minus; v)Nt&minus;1
</p>
<p>&int;
</p>
<p>ri&isin;v
e&minus;βψ(ri)dri = (Vt &minus; v)Nt&minus;1Ai ,
</p>
<p>(5.12)
</p>
<p>where we defined
</p>
<p>Ai :=
&int;
</p>
<p>ri&isin;v
e&minus;βψ(ri)dri . (5.13)
</p>
<p>However, since ψ(r) is common to all particles, Ai is independent of i. Stated differ-
ently, ri is simply an integration variable in (5.13) and can be replaced by any r j 	=i
without affecting the value of Ai. Thus, dropping the subscript i, we obtain
</p>
<p>Z =
1
</p>
<p>Λ 3Nt Nt !
</p>
<p>[
(Vt &minus; v)Nt +NtA(Vt &minus; v)Nt&minus;1
</p>
<p>]
=
</p>
<p>(Vt &minus; v)Nt
Λ 3Nt Nt !
</p>
<p>(
1+
</p>
<p>Nt
</p>
<p>Vt &minus; v
A
</p>
<p>)
. (5.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Exact Solutions 209
</p>
<p>In this expression, the term proportional to A, that is,
</p>
<p>Z1 :=
(Vt &minus; v)Nt
Λ 3Nt Nt !
</p>
<p>Nt
</p>
<p>Vt &minus; v
A (5.15)
</p>
<p>comes from microstates with the single occupancy of the site. Therefore, the proba-
</p>
<p>bility p1 that the site is occupied is given by
</p>
<p>p1 =
Z1
</p>
<p>Z
=
</p>
<p>Nt
Vt&minus;v A
</p>
<p>1+ Nt
Vt&minus;v A
</p>
<p>. (5.16)
</p>
<p>Ignoring v in comparison to Vt and using the relation
</p>
<p>Nt
</p>
<p>Vt
=
</p>
<p>eβ&micro;
</p>
<p>Λ 3
(5.17)
</p>
<p>which follows from (4.139) and (4.140) for an ideal gas, we rewrite the above
</p>
<p>expression for p1 as
</p>
<p>p1 =
eβ&micro;A/Λ 3
</p>
<p>1+ eβ&micro;A/Λ 3
. (5.18)
</p>
<p>Comparing this expression with (5.8), we find
</p>
<p>eβε =
A
</p>
<p>Λ 3
. (5.19)
</p>
<p>Thus,
</p>
<p>&minus; ε =&minus;kBT ln
[
</p>
<p>1
</p>
<p>Λ 3
</p>
<p>&int;
</p>
<p>r&isin;v
e&minus;βψ(r)dr
</p>
<p>]
, (5.20)
</p>
<p>identifying &minus;ε as the Helmholtz free energy of a particle subject to the external field
ψ(r) generated by the adsorption site.
</p>
<p>By incorporating more realism into our model, we did not materially change the
</p>
<p>behavior of p1. Instead, we obtained an explicit expression for the binding energy.
</p>
<p>In principle, this allows us to predict &minus;ε from a detailed molecular level model of
particles and the adsorption site.
</p>
<p>Exercise 5.1. Analyze the current model using a grand canonical ensemble applied
</p>
<p>to the open system v. ///
</p>
<p>5.1.3 Multiple Independent Sites
</p>
<p>Let us consider a collection of M independent adsorption sites instead of just one.
</p>
<p>By the sites being independent, we mean that whether a given site is occupied or not
</p>
<p>has no impact on any other site being occupied or not. In this case, we expect that
</p>
<p>the probability p1 that a given site is occupied still is given by x/(1+ x). Now that
there are M such sites, we should have 〈N〉 = Mx/(1+ x). Let us try to reach this</p>
<p/>
</div>
<div class="page"><p/>
<p>210 5 Simple Models of Adsorption
</p>
<p>conclusion through an explicit computation. Along the way, we are reminded of the
</p>
<p>notion of the binomial coefficients, which we use later.
</p>
<p>As in Sect. 5.1.1, we take a small volume around each of the adsorption sites. The
</p>
<p>collection of M such volumes forms an open system, which can be treated using a
</p>
<p>grand canonical ensemble. Our starting point still is (4.128), in which we have to
</p>
<p>give an expression for Ω(E,N). We need to consider the following cases:
</p>
<p>a. N = 0. In this case E can only take a single value, zero, and hence
</p>
<p>W (E,0) = θ(E) and Ω(E,0) = δ (E) . (5.21)
</p>
<p>b. N = 1. In this case, E can only be &minus;ε . But there are M different choices for the
site to occupy. Thus,
</p>
<p>W (E,1) = Mθ(E + ε) and Ω(E,1) = Mδ (E + ε) . (5.22)
</p>
<p>c. N = 2. Now, E can only be &minus;2ε . The number of different ways of choosing two
sites to occupy from the M sites is given by M(M &minus; 1)/2. To see this, note that
there are M different ways of choosing the first site to occupy. For each such
</p>
<p>choice, there are M&minus;1 different choices for the second site to occupy. This gives
you M(M&minus;1) options. But, all what matters is which pair of sites is being occu-
pied in the end and not the order in which the two sites were occupied. Reversing
</p>
<p>the order gives you exactly the same state. So, we need to divide M(M&minus;1) by 2.
Therefore,
</p>
<p>W (E,2) =
M(M&minus;1)
</p>
<p>2
θ(E +2ε) and Ω(E,1) =
</p>
<p>M(M&minus;1)
2
</p>
<p>δ (E +2ε) .
</p>
<p>(5.23)
</p>
<p>d. Generalizing the above considerations to N(&le; M) occupied sites, we find
</p>
<p>Ω(E,N) =
M!
</p>
<p>N!(M&minus;N)!δ (E +Nε) (N &le; M) . (5.24)
</p>
<p>To check the validity of this formula, set N = 0,1, and 2. (Note that 0! = 1).
e. Needless to say, Ω(E,N)&equiv; 0 if N &gt; M.
</p>
<p>Before we continue, we recall the standard notation for the binomial coefficient:
</p>
<p>(
M
</p>
<p>N
</p>
<p>)
:=
</p>
<p>M!
</p>
<p>N!(M&minus;N)! , (5.25)
</p>
<p>which gives the number of distinct ways of selecting N objects out of M objects
</p>
<p>without any regard to the particular order in which the selection was made.
</p>
<p>Using (5.24) in (4.128), we find
</p>
<p>Ξ =
M
</p>
<p>&sum;
N=0
</p>
<p>eβ&micro;N
(
</p>
<p>M
</p>
<p>N
</p>
<p>)&int;
δ (E +Nε)e&minus;βEdE =
</p>
<p>M
</p>
<p>&sum;
N=0
</p>
<p>(
M
</p>
<p>N
</p>
<p>)
eβ (&micro;+ε)N =
</p>
<p>M
</p>
<p>&sum;
N=0
</p>
<p>(
M
</p>
<p>N
</p>
<p>)
xN ,
</p>
<p>(5.26)
</p>
<p>where x := eβ (&micro;+ε) as before.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Exact Solutions 211
</p>
<p>To compute the summation explicitly, we recall (B.30) for the binomial expan-
</p>
<p>sion. Then, from (5.26),
</p>
<p>Ξ =
M
</p>
<p>&sum;
N=0
</p>
<p>(
M
</p>
<p>N
</p>
<p>)
xN1M&minus;N = (1+ x)M . (5.27)
</p>
<p>Using (5.2), we obtain the average number 〈N〉 of particles in the entire system as
</p>
<p>〈N〉=
(
&part; lnΞ
</p>
<p>&part;β&micro;
</p>
<p>)
</p>
<p>β ,V
</p>
<p>=
Mx
</p>
<p>1+ x
, (5.28)
</p>
<p>in which V :=Mv is the volume of the system, that is, the collection of M adsorption
sites each with the volume v. Thus,
</p>
<p>p1 :=
〈N〉
M
</p>
<p>=
x
</p>
<p>1+ x
. (5.29)
</p>
<p>Exercise 5.2. Derive (5.29) using (5.9) as the starting point. Assume that Nt ≫ M
meaning there are far more particles in the entire closed system than there are the
</p>
<p>adsorption sites. Note that we tacitly made this approximation in this subsection by
</p>
<p>using a grand canonical ensemble. Without this approximation, &micro; of the gas phase
would depend on the number of adsorbed particles. ///
</p>
<p>5.1.4 Four Sites with Interaction Among Particles
</p>
<p>Consider an array of four adsorption sites placed at the four vertices of a regular
</p>
<p>tetrahedron as shown in Fig. 5.4. We assume that these sites are distinguishable
</p>
<p>Fig. 5.4 Four adsorption sites on the vertices of a regular tetrahedron.</p>
<p/>
</div>
<div class="page"><p/>
<p>212 5 Simple Models of Adsorption
</p>
<p>(That is to say, for example, the state in which only the lower left corner being
</p>
<p>occupied and the state in which only the middle site being occupied are to be con-
</p>
<p>sidered as two distinct states.) and that each site can accommodate up to a single
</p>
<p>particle. The binding energy per site is ε , and the interaction energy between a pair
of adjacent particles is &minus;w. We immerse this array of adsorption sites in a gas of
particles held at constant T and &micro; . Our goal again is to compute p1.
</p>
<p>We note that there are 24 = 16 states the system can take, which are divided
among the following cases:
</p>
<p>a. N = 0 with energy E = 0. As before, we have
</p>
<p>Ω(E,0) = δ (E) . (5.30)
</p>
<p>b. N = 1, and hence E =&minus;ε . Since there are four distinct states (reflecting the four
choices we can make for the occupied site), we have
</p>
<p>Ω(E,1) = 4δ (E + ε) . (5.31)
</p>
<p>c. N = 2, in which case, E =&minus;2ε&minus;w and there are
(
</p>
<p>4
2
</p>
<p>)
= 6 distinct states, leading
</p>
<p>to
</p>
<p>Ω(E,2) = 6δ (E +2ε+w) . (5.32)
</p>
<p>d. N = 3, for which
Ω(E,3) = 4δ (E +3ε+3w) . (5.33)
</p>
<p>e. N = 4, for which
Ω(E,4) = δ (E +4ε+6w) . (5.34)
</p>
<p>Note that the coefficients of the δ -functions add up to 16 as they should. As before,
Ω(E,N)&equiv; 0 for N &ge; 5.
</p>
<p>The grand canonical partition function follows from the above expressions for
</p>
<p>Ω(E,N) and (4.128), and is given by
</p>
<p>Ξ =
&infin;
</p>
<p>&sum;
N=0
</p>
<p>eβ&micro;N
&int;
</p>
<p>Ω(E,N)e&minus;βEdE = 1+4x+6x2y+4x3y3 + x4y6 , (5.35)
</p>
<p>where x := eβ (&micro;+ε) and y := eβw. Then,
</p>
<p>p1 =
〈N〉
</p>
<p>4
=
</p>
<p>1
</p>
<p>4
</p>
<p>(
&part; lnΞ
</p>
<p>&part;β&micro;
</p>
<p>)
</p>
<p>β ,V
</p>
<p>=
x+3x2y+3x3y3 + x4y6
</p>
<p>1+4x+6x2y+4x3y3 + x4y6
. (5.36)
</p>
<p>When βw = 0, the sites behave independent of each other. Noting that y = 1 in this
case, you can verify that this equation reduces to (5.29).
</p>
<p>Figure 5.5 illustrates the dependence of p1 on x for a few values of w. The plot
</p>
<p>for βw = 0 is identical to what was shown in Fig. 5.3. For larger values of w, p1
is seen to change more rapidly over a much smaller range of x compared to the
</p>
<p>plot for w = 0. This is an example of the cooperative phenomena. The presence of</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Exact Solutions 213
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>1.2
</p>
<p>0.01 0.1 1 10 100
</p>
<p>x
</p>
<p>w 0
</p>
<p>w 1
</p>
<p>w 2
</p>
<p>Fig. 5.5 Dependence of p1 on x := e
β (&micro;+ε) for a few values of βw.
</p>
<p>Array 1
</p>
<p>w
</p>
<p>Array 2 Array 3
</p>
<p>Fig. 5.6 Three triangular arrays of adsorption sites. The filled circles represent occupied sites,
</p>
<p>while the open circles are unoccupied. For the configuration shown, we have E =&minus;3ε&minus;w.
</p>
<p>adsorbed particles enhance the probability of further adsorption of another, hence
</p>
<p>the word &ldquo;cooperative.&rdquo;
</p>
<p>The four-site model we just studied is a classical model for hemoglobin. In this
</p>
<p>context, the cooperative behavior accounts for the sensitivity of a hemoglobin in
</p>
<p>regulating the O2 adsorption/desorption in response to a small change in the partial
</p>
<p>pressure of O2.
</p>
<p>Exercise 5.3. Consider three triangular arrays, each carrying three adsorption sites
</p>
<p>as shown in Fig. 5.6. Each site can accommodate up to a single particle. The binding
</p>
<p>energy per site is ε , while the interaction between a pair of adjacent particles, both in
the same array, is &minus;w. There is no interaction among particles adsorbed on different
arrays.</p>
<p/>
</div>
<div class="page"><p/>
<p>214 5 Simple Models of Adsorption
</p>
<p>Now suppose that the arrays are placed in a closed system with only three iden-
</p>
<p>tical particles and held at a constant temperature. While many distinct states are
</p>
<p>accessible to the system, we focus only on the following two situations:
</p>
<p>a. All three particles are adsorbed to a single array.
</p>
<p>b. Each array carries exactly one particle.
</p>
<p>For what range of temperature is situation a more likely than situation b? For the
</p>
<p>sake of this problem, assume that the nine adsorption sites are all distinguishable. ///
</p>
<p>5.2 Mean-Field Approximation
</p>
<p>So far, we have evaluated the partition functions Ξ , Z, and the probability p1 that a
given site is occupied by carrying out the required computations exactly.
</p>
<p>However, it is not hard to see that the computation becomes quickly unmanage-
</p>
<p>able with the increasing number of adsorption sites. There will be too many distinct
</p>
<p>microstates to enumerate, which may or may not have the same energy. In such
</p>
<p>cases, we are forced to introduce some approximation. In this section, we will look
</p>
<p>at one very popular approximation scheme known as the mean-field approxima-
</p>
<p>tion.
</p>
<p>Note that the complication in carrying out the exact computation arises from the
</p>
<p>interaction among adsorbed particles. In fact, M independent site model we saw in
</p>
<p>Sect. 5.1.3 was no more complicated than a single site model of Sect. 5.1.1. But, if
</p>
<p>there is no interaction (w = 0), there is no cooperative behavior either and the model
will not be very interesting to study in the first place.
</p>
<p>The basic idea of a mean-field approximation is to treat each site as if it is inde-
</p>
<p>pendent of the others while at the same time trying to capture some of the effects of
</p>
<p>the actual interactions among adsorbed particles.
</p>
<p>5.2.1 Four Sites with Interaction Among Particles
</p>
<p>We have already obtained the exact expression for p1 for this model. This forms
</p>
<p>a basis for evaluating our approximation scheme. We will consider a much larger
</p>
<p>system in Sect. 5.2.2.
</p>
<p>A key observation is that a particle on a given site may be considered as &ldquo;feeling&rdquo;
</p>
<p>the presence of particles on the other sites through an effective field they generate.
</p>
<p>To illustrate the idea, let us focus on the particle at the central site in Fig. 5.4, which
</p>
<p>we shall refer to as site 1.
</p>
<p>There are three peripheral sites to consider. If only one of them is filled, the
</p>
<p>interaction energy is &minus;w. From the perspective of the particle at site 1, this has the
same effect as sitting alone but with the effective binding energy εeff = ε+w. When
two of the peripheral sites are filled, then εeff felt by the central particle will be</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Mean-Field Approximation 215
</p>
<p>ε+2w. Finally, if all three sites are occupied, εeff = ε+3w. To summarize,
</p>
<p>εeff = ε+Nw , (5.37)
</p>
<p>where N is the total number of particles adsorbed at the peripheral sites.
</p>
<p>So far, everything is exact. The difficulty of continuing with the exact solution
</p>
<p>lies in the fact that εeff at site 1 depends on the value of N, and hence on the state
of the peripheral sites, which may change with time. A simple approximation to
</p>
<p>circumvent this difficulty is to replace the exact N-dependent effective field by its
</p>
<p>average. Noting that the average number of particles in each of the peripheral sites
</p>
<p>is p1, we have
</p>
<p>εeff &asymp; 〈εeff〉= ε+ 〈N〉w = ε+3p1w . (5.38)
Being a thermal average, this expression is independent of the instantaneous state of
</p>
<p>the peripheral sites. According to (5.38), site 1 may be regarded as the binding site
</p>
<p>in the single site model of Sect. 5.1.1 but with the binding energy ε+3p1w.
Under this approximation, we can immediately take over (5.8) and write down
</p>
<p>the probability that site 1 is occupied as
</p>
<p>eβ (&micro;+εeff)
</p>
<p>1+ eβ (&micro;+εeff)
&asymp; e
</p>
<p>β (&micro;+ε+3wp1)
</p>
<p>1+ eβ (&micro;+ε+3wp1)
=
</p>
<p>xy3p1
</p>
<p>1+ xy3p1
. (5.39)
</p>
<p>But, this quantity can only be p1 since all the sites are equivalent and there is nothing
</p>
<p>in the model that serves to single out site 1. Thus,
</p>
<p>p1 =
xy3p1
</p>
<p>1+ xy3p1
. (5.40)
</p>
<p>This is, then, the equation for p1 under the mean-field approximation. The phrase
</p>
<p>&ldquo;mean-field&rdquo; originates from our replacing the actual fluctuating field εeff generated
by particles on the peripheral sites by its average 〈εeff〉.
</p>
<p>For w = 0, we have y = 1 and the approximation is, of course, exact. At βw = 1,
the approximation is reasonably good as seen from Fig. 5.7a. With increasing w,
</p>
<p>however, its prediction starts to deviate from the actual one as indicated by Fig. 5.7b.
</p>
<p>When βw = 2, the mean-field approximation incorrectly predicts a sudden jump in
p1, or a phase transition, with increasing x.
</p>
<p>Exercise 5.4. Consider an array of four adsorption sites placed at four corners of a
</p>
<p>square. We assume that these sites are distinguishable and that each site can accom-
</p>
<p>modate up to a single particle. The binding energy per site is ε , and the interaction
energy between a pair of particles occupying the nearest neighbor sites is &minus;w. There
is no interaction across the diagonally separated particles because they are not a
</p>
<p>nearest neighbor pair. The distance between them is
&radic;
</p>
<p>2 times the lattice constant.
</p>
<p>For example,
</p>
<p>E 2 w E 2</p>
<p/>
</div>
<div class="page"><p/>
<p>216 5 Simple Models of Adsorption
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>1.2
</p>
<p>x
</p>
<p>a. p1 at w 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>1.2
</p>
<p>0.01 0.1 1 10 100 0.01 0.1 1 10 100
</p>
<p>x
</p>
<p>b. p1 at w 2
</p>
<p>Fig. 5.7 Dependence of p1 on x in the four interaction site model. Comparison between the exact
</p>
<p>solution (solid line) and the mean-field approximation (dashed line).
</p>
<p>This array is exposed to a gas phase of particles held at constant T
</p>
<p>and &micro; .
</p>
<p>a. Let w= 0, that is, there is no interaction among adsorbed particles. Find the exact
expression for the average number φ of adsorbed particles per site.
</p>
<p>b. Let w 	= 0. Find the exact expression for φ .
c. Use the mean-field approximation to derive the equation for φ . Rather than com-
</p>
<p>puting the partition function, consider the effective field generated by other par-
</p>
<p>ticles at a given site and then use your answer from part a. ///
</p>
<p>5.2.2 Two-Dimensional Lattice
</p>
<p>Suppose now that the M adsorption sites are arranged on a two-dimensional square
</p>
<p>lattice. We continue to assign a binding energy ε per site and the interaction energy
of &minus;w between a nearest neighbor pair. The diagrams in Exercise 5.4 illustrate the
situation for M = 4.
</p>
<p>We fix the number of particles N(&le; M) and work with the canonical ensemble.
Even before we get started, we see right away that p1 = N/M in this case. So, what
is the point? Why do we even bother with this problem? Well, p1 = N/M is the
probability that a given site is occupied if the system is, on average, homogeneous
</p>
<p>throughout. But, whether the system can remain homogeneous depends on its ther-
</p>
<p>modynamic stability. If the phase is unstable or even metastable for a given value
</p>
<p>of N/M, then the system will eventually separate into two phases, one with p1 less
than N/M and the other with p1 greater than N/M.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Mean-Field Approximation 217
</p>
<p>One way to investigate such a possibility is to compute the free energy of the
</p>
<p>system as a function of p1 assuming that it is homogeneous. The resulting free
</p>
<p>energy function will tell us about the stability and the eventual fate of any given
</p>
<p>homogeneous phase.
</p>
<p>Now, the relevant partition function is given by (4.72). As we saw in Sect. 5.1.3,
</p>
<p>there are
(
</p>
<p>M
N
</p>
<p>)
different ways of putting N particles in M sites. For a given configu-
</p>
<p>ration, the energy of the system may be written as
</p>
<p>E =&minus;εN &minus;wNn.n. , (5.41)
</p>
<p>where Nn.n. is the number of the nearest neighbor pairs in that particular configu-
</p>
<p>rations. Note carefully that Nn.n. depends on the configuration under consideration.
</p>
<p>For example, compare the following two configurations, both with M = 16, N = 4.
</p>
<p>Nn n 4 Nn n 1
</p>
<p>Now, let Γ (Nn.n.) denote the number of configurations containing exactly Nn.n.
nearest neighbor pairs. Clearly, Γ satisfies
</p>
<p>(
M
</p>
<p>N
</p>
<p>)
= &sum;
</p>
<p>Nn.n.
</p>
<p>Γ (Nn.n.) . (5.42)
</p>
<p>In terms of Γ , we can write Ω as
</p>
<p>Ω(E) = &sum;
Nn.n.
</p>
<p>Γ (Nn.n.)δ (E + εN +wNn.n.) . (5.43)
</p>
<p>The canonical partition function is now given by
</p>
<p>Z =
</p>
<p>&int;
&sum;
</p>
<p>Nn.n.
</p>
<p>Γ (Nn.n.)δ (E + εN +wNn.n.)e
&minus;βEdE
</p>
<p>= &sum;
Nn.n.
</p>
<p>Γ (Nn.n.)e
βεNeβwNn.n. = eβεN
</p>
<p>(
M
</p>
<p>N
</p>
<p>)
&sum;
</p>
<p>Nn.n.
</p>
<p>Γ (Nn.n.)(
M
N
</p>
<p>) eβwNn.n. , (5.44)
</p>
<p>in which we identify the ratioΓ (Nn.n.)/
(
</p>
<p>M
N
</p>
<p>)
as the probability of finding Nn.n. nearest
</p>
<p>neighbor pairs in a configuration if it is chosen randomly with an equal probability
</p>
<p>from
(
</p>
<p>M
N
</p>
<p>)
configurations. Denoting the average with respect to this probability by
</p>
<p>〈&middot; &middot; &middot; 〉0, we obtain
Z = eβεN
</p>
<p>(
M
</p>
<p>N
</p>
<p>)&lang;
eβwNn.n.
</p>
<p>&rang;
0
. (5.45)</p>
<p/>
</div>
<div class="page"><p/>
<p>218 5 Simple Models of Adsorption
</p>
<p>Up to this point, everything is exact. But, evaluation of the average in this expres-
</p>
<p>sion is a difficult task. As a simple approximation, let us replace the average of the
</p>
<p>exponential by the exponential of the average:
</p>
<p>Z &asymp; eβεN
(
</p>
<p>M
</p>
<p>N
</p>
<p>)
e〈βwNn.n.〉0 = eβεN
</p>
<p>(
M
</p>
<p>N
</p>
<p>)
eβw〈Nn.n.〉0 . (5.46)
</p>
<p>This step, as carried out at the level of partition function, implements the mean-field
</p>
<p>approximation. We will soon see why this is so.
</p>
<p>Exercise 5.5. In this problem, we investigate how the mean-field approximation
</p>
<p>affects the predicted free energy of the system:
</p>
<p>a. Show that ex &ge; 1+ x for any real number x.
b. Noting that ex = e〈x〉ex&minus;〈x〉 and using the result from part a, show that 〈ex〉 &ge; e〈x〉.
c. What is the implication of the inequality in part b on the free energy of the system
</p>
<p>estimated using the mean-field approximation? ///
</p>
<p>To evaluate 〈Nn.n.〉0, let us focus on a particular site i and its z = 4 nearest neigh-
bor sites as shown below:
</p>
<p>m i
</p>
<p>j
</p>
<p>k
</p>
<p>l
</p>
<p>These sites (i, j, k, l, and m) may or may not be occupied. The probability that i is
</p>
<p>occupied in a particular configuration we choose from
(
</p>
<p>M
N
</p>
<p>)
is simply N/M.27 The
</p>
<p>same applies to site j. Thus, the probability that both i and j are occupied, that is,
</p>
<p>the probability that a nearest neighbor pair exists between sites i and j, is (N/M)2.
Similarly for other pairs involving site i. Thus, the average number of the nearest
</p>
<p>neighbor pairs centered around i is given by (N/M)2z. There is nothing special about
site i and the same consideration applies to other sites as well. This suggests that the
</p>
<p>average number of the nearest neighbor pairs in the system is
</p>
<p>(
N
</p>
<p>M
</p>
<p>)2
zM , (5.47)
</p>
<p>But, counting the pairs in this manner, we have counted each pair twice. When
</p>
<p>counting the number of pairs centered around site i, we counted the i&ndash; j pair once.
</p>
<p>Then, when counting the number of pairs centered around site j, we counted this
</p>
<p>same pair for the second time. Dividing (5.47) by two to correct for the double
</p>
<p>counting, we have
</p>
<p>〈Nn.n.〉0 =
zN2
</p>
<p>2M
. (5.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Mean-Field Approximation 219
</p>
<p>So, (5.46) finally becomes
</p>
<p>Z &asymp; eβεN
(
</p>
<p>M
</p>
<p>N
</p>
<p>)
exp
</p>
<p>(
βwz
</p>
<p>2M
N2
</p>
<p>)
. (5.49)
</p>
<p>Now that we have the partition function, we can compute the Helmholtz free
</p>
<p>energy of the system:
</p>
<p>βF = &minus; lnZ =&minus;βεN &minus; ln
(
</p>
<p>M
</p>
<p>N
</p>
<p>)
&minus; β zw
</p>
<p>2M
N2
</p>
<p>&asymp; N ln N
M
</p>
<p>+(M&minus;N) ln M&minus;N
M
</p>
<p>&minus;βεN &minus; β zw
2M
</p>
<p>N2 , (5.50)
</p>
<p>where we used Stirling&rsquo;s formula (3.153). On per site basis, therefore,
</p>
<p>f :=
βF
</p>
<p>M
= φ lnφ +(1&minus;φ) ln(1&minus;φ)&minus;βεφ &minus; 1
</p>
<p>2
β zwφ 2 , (5.51)
</p>
<p>where we defined φ := N/M. We already remarked at the beginning of this section
that p1 = φ holds only in a homogeneous phase. The quantity f is the (nondimen-
sional) free energy per site of this homogeneous phase.
</p>
<p>Let us assume for a moment that a homogeneous system is stable and find the
</p>
<p>relationship between φ and the chemical potential &micro; of the particle. The latter fol-
lows from the identity:
</p>
<p>β&micro; =
</p>
<p>(
&part;βF
</p>
<p>&part;N
</p>
<p>)
</p>
<p>T,M
</p>
<p>=
</p>
<p>(
&part;βF/M
</p>
<p>&part;N/M
</p>
<p>)
</p>
<p>T,M
</p>
<p>=
</p>
<p>(
&part; f
</p>
<p>&part;φ
</p>
<p>)
</p>
<p>T
</p>
<p>= lnφ &minus; ln(1&minus;φ)&minus;βε&minus;β zwφ , (5.52)
</p>
<p>where we assumed that the lattice constant is held fixed and treated w as a con-
</p>
<p>stant. In this case, holding volume constant is equivalent to fixing M. From (5.52),
</p>
<p>it follows that
φ
</p>
<p>1&minus;φ = e
β (&micro;+ε+wzφ) . (5.53)
</p>
<p>This result should be compared with (5.8), which may be rewritten as
</p>
<p>p1
</p>
<p>1&minus; p1
= eβ (&micro;+ε) . (5.54)
</p>
<p>Our homogeneous system of interacting N particles is seen to be equivalent to a
</p>
<p>single-site model provided that ε in the latter is replaced by the effective binding
energy, or the mean-field, ε+wzφ .
</p>
<p>Let x := eβ (&micro;+ε) and y := eβw as before, and rewrite (5.53) as
</p>
<p>φ =
xyzφ
</p>
<p>1+ xyzφ
. (5.55)</p>
<p/>
</div>
<div class="page"><p/>
<p>220 5 Simple Models of Adsorption
</p>
<p>Fig. 5.8 A two-dimensional triangular lattice. A given site (filled circle) has six nearest neighbor
</p>
<p>sites.
</p>
<p>This is essentially (5.40) with 3 replaced by z. Thus, under the mean-field approx-
</p>
<p>imation, the quantity z, often referred to as the coordination number is the only
</p>
<p>parameter reflecting the lattice structure of the model system. For example, in both
</p>
<p>two-dimensional triangular lattices (see Fig. 5.8.) and three-dimensional cubic lat-
</p>
<p>tices, the mean-field approximation gives (5.55) with z = 6. In reality, these two
systems behave differently especially near the critical point. For details, see Ref. [1].
</p>
<p>Let us now examine the stability of a homogeneous system. The f versus φ plot
is shown in Fig. 5.9 for a few values of w. For simplicity, we set ε = 0. Note that,
as β zw is increased, the graph starts to exhibit two inflection points. Between these
points, the graph is concave down and the system is unstable. That is, the free energy
</p>
<p>of the system can be lowered by splitting into two phases. This mechanism of phase
</p>
<p>separation is known as the spinodal decomposition.
</p>
<p>We recall that the inflection points of the curve are determined by
</p>
<p>&part; 2 f/&part;φ 2 = 0. (5.56)
</p>
<p>According to (5.51), this equation has two real solutions in the interval [0,1] if T is
sufficiently low. In this case, a homogeneous phase is unstable, metastable, or stable
</p>
<p>depending on its value of φ .
Let us denote the two real solutions of (5.56) by φ a and φ b, where φ a &lt; φ b.
</p>
<p>Then, a phase with φ a &le; φ &le; φ b is unstable and will separate into two coexisting
phases one with pc1 and the other with p
</p>
<p>d
1 , where p
</p>
<p>c
1 &lt; φ
</p>
<p>a &lt; φ b &lt; pd1 . If p
c
1 &lt; φ &lt;
</p>
<p>φ a or φ b &lt; φ &lt; pd1 , the phase is metastable and will separate into the two phases
(pc1 and p
</p>
<p>d
1) given a sufficient amount of time. Phases with φ &lt; p
</p>
<p>c
1 or φ &gt; p
</p>
<p>d
1 are
</p>
<p>stable.
</p>
<p>The two solutions φ a and φ b of (5.56) approach each other with increasing T and
eventually merge at the critical temperature Tc. Above Tc, there is no real solution</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Mean-Field Approximation 221
</p>
<p>to (5.56). That is, a homogeneous phase is stable for all values of φ and there will
be no phase separation.
</p>
<p>Exercise 5.6. Show that p1 values at phase coexistence can be determined by means
</p>
<p>of the common tangent construction. In this method, one looks for a tangent line to
</p>
<p>the f versus φ plot having two points of contact. If this common tangent exists,
then the points of contact give the desired p1 values. (See Sect. 2.16.2.4 for another
</p>
<p>example of this graphical method.) ///
</p>
<p>Exercise 5.7.
</p>
<p>a. For w &gt; 0, find the critical temperature below which the system undergoes a
phase separation. Note that the result is independent of ε .
</p>
<p>b. Is there a phase separation if w &lt; 0 ? ///
</p>
<p>Can we approach the same problem using a grand canonical ensemble? Combin-
</p>
<p>ing (4.72) and (4.128), we see that Ξ may be written as
</p>
<p>Ξ =
M
</p>
<p>&sum;
N=0
</p>
<p>eβ&micro;NZ(T,V,N) . (5.57)
</p>
<p>When Z from (5.49) is substituted in this equation, the result is a rather difficult
</p>
<p>expression to evaluate.
</p>
<p>However, based on what we saw in Sect. 4.1, N is expected to be extremely
</p>
<p>sharply peaked around 〈N〉 for large enough M. In this case, the summation in (5.57)
may be replaced by the maximum term in the summand. Moreover, the value of N
</p>
<p>-3.5
</p>
<p>-3
</p>
<p>-2.5
</p>
<p>-2
</p>
<p>-1.5
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>f
</p>
<p>zw 0
</p>
<p>zw 0 5
</p>
<p>zw 1 0
</p>
<p>zw 1 5
</p>
<p>Fig. 5.9 f := βF/M versus φ when ε = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>222 5 Simple Models of Adsorption
</p>
<p>corresponding to this maximum term may safely be identified with its average 〈N〉.
Thus, we write
</p>
<p>Ξ &asymp; eβ&micro;NZ(T,M,N) (5.58)
and then maximize Ξ (or minimize βΩ = &minus; lnΞ ) with respect to N. When we
identify the optimum value of N with 〈N〉, the result is (5.52). In fact, from (5.58),
</p>
<p>&part; lnΞ
</p>
<p>&part;N
</p>
<p>∣∣∣∣
N=〈N〉
</p>
<p>= β&micro;+
&part; lnZ
</p>
<p>&part;N
</p>
<p>∣∣∣∣
N=〈N〉
</p>
<p>= 0 , (5.59)
</p>
<p>which is just the first equality in (5.52).
</p>
<p>Example 5.1. Liquid crystal:28 Simple liquid crystals are systems consisting
</p>
<p>of nonspherical, for example, rod-like, molecules. At high temperatures, the
</p>
<p>orientation of these molecules is random; this is called the isotropic phase.
</p>
<p>At low temperatures, molecules align parallel to each other; this is called the
</p>
<p>nematic phase. The simplest lattice model for this transition is a three-state
</p>
<p>model in which a molecule can take any one of three orthogonal orientations,
</p>
<p>which we designate as x, y, and z. If two nearest molecules lie parallel to each
</p>
<p>other, there is an energy gain of &minus;ε , (ε &gt; 0). If they lie perpendicular, the
energy gain is zero. Assuming single occupancy on each site and no vacancy,
</p>
<p>we may define variables σx(i), σy(i), and σz(i), such that σx(i) = 1 if the
molecule i lies parallel to the x-axis, σx(i) = 0 otherwise and likewise for
other orientations. Of course, σx(i)+σy(i)+σz(i) = 1:
</p>
<p>a. Construct an energy function for the system.
</p>
<p>b. Let
</p>
<p>σx :=
1
</p>
<p>N
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>σx(i) (5.60)
</p>
<p>denote the fraction of molecules pointing in the x-direction. What is 〈σx〉
in the isotropic phase?
</p>
<p>c. Define an order parameter such that it is zero in the isotropic phase,
</p>
<p>nonzero in the nematic phase and reaches a maximum of 1 at zero tem-
</p>
<p>perature.
</p>
<p>d. Find the isotropic&ndash;nematic transition temperature Tt .
</p>
<p>Solution
</p>
<p>a. Let
</p>
<p>σ(i) = σx(i)ex +σy(i)ey +σz(i)ez . (5.61)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Mean-Field Approximation 223
</p>
<p>If σ(i) and σ( j) point in the same direction, σ(i) &middot;σ( j) = 1. The dot prod-
uct is zero otherwise. Thus,
</p>
<p>E =&minus;1
2
ε&sum;
</p>
<p>i
&sum;
j 	=i
</p>
<p>σ(i) &middot;σ( j) . (5.62)
</p>
<p>For given i, the summation over j includes only the nearest neighbors of i.
</p>
<p>The factor 1/2 corrects for the double counting of each nearest neighbor
pair.
</p>
<p>b. In an isotropic phase, each direction is equivalent to any other. Thus, we
</p>
<p>have 〈σx〉= 〈σy〉= 〈σz〉. But, because
</p>
<p>〈σx +σy +σz〉= 〈σx〉+ 〈σy〉+ 〈σz〉= 1 , (5.63)
</p>
<p>we see that
</p>
<p>〈σx〉= 〈σy〉= 〈σz〉=
1
</p>
<p>3
. (5.64)
</p>
<p>c. From (5.61)
</p>
<p>〈σ〉= 〈σx〉ex + 〈σy〉ey + 〈σz〉ez. (5.65)
Because of (5.63), this graphically is a vector pointing from the origin O
</p>
<p>to a point, call it X , on the plane defined by the following three points:
</p>
<p>A
.
= (1,0,0) , B
</p>
<p>.
= (0,1,0) , and C
</p>
<p>.
= (0,0,1) . (5.66)
</p>
<p>Since components 〈σx〉, 〈σy〉, and 〈σz〉 are all nonnegative, the point X is
confined to the triangle ABC. The isotropic phase is represented by the
</p>
<p>point
</p>
<p>S
.
=
</p>
<p>(
1
</p>
<p>3
,
</p>
<p>1
</p>
<p>3
,
</p>
<p>1
</p>
<p>3
</p>
<p>)
. (5.67)
</p>
<p>One possible choice of an order parameter η is the distance |SX| between
S and X, or more conveniently,
</p>
<p>|SX|2 =
(
〈σx〉&minus;
</p>
<p>1
</p>
<p>3
</p>
<p>)2
+
</p>
<p>(
〈σy〉&minus;
</p>
<p>1
</p>
<p>3
</p>
<p>)2
+
</p>
<p>(
〈σz〉&minus;
</p>
<p>1
</p>
<p>3
</p>
<p>)2
</p>
<p>= 〈σx〉2 + 〈σy〉2 + 〈σz〉2 &minus;
1
</p>
<p>3
. (5.68)
</p>
<p>(We want to avoid the square root, which is a bit awkward to handle.) For
</p>
<p>an isotropic phase, this quantity is zero. At zero temperature, the system is
</p>
<p>in a completely ordered state, corresponding to point A, B, or C, for which
</p>
<p>|SA|2 = |SB|2 = |SC|2 = 2
3
. (5.69)</p>
<p/>
</div>
<div class="page"><p/>
<p>224 5 Simple Models of Adsorption
</p>
<p>(This is because the minimum energy state completely dominates the sum-
</p>
<p>mation in Z given below.) Normalizing |SX|2 so that it is unity for A, B,
and C, we arrive at
</p>
<p>η :=
3
</p>
<p>2
</p>
<p>(
〈σx〉2 + 〈σy〉2 + 〈σz〉2
</p>
<p>)
&minus; 1
</p>
<p>2
(5.70)
</p>
<p>d. The canonical partition function is given by
</p>
<p>Z =&sum;e&minus;βE , (5.71)
</p>
<p>where the summation is over all possible orientations of all the molecules
</p>
<p>in the system. To perform this summation, we organize it according to the
</p>
<p>number Nα of molecules pointing in the α-direction. We note that for given
Nα (α = x,y,z), there are
</p>
<p>W (Nx,Ny,Nz) :=
N!
</p>
<p>Nx!Ny!Nz!
(5.72)
</p>
<p>distinct configurations. But, not all of them has the same energy, since the
</p>
<p>latter depends on the number Nαα of nearest neighbor pairs of molecules
</p>
<p>both pointing in the α-direction.
</p>
<p>Let Γ (Nxx,Nyy,Nzz|Nx,Ny,Nz) denote the number of distinct configurations
with given values of Nαα (α = x,y,z) that are consistent with the given
values of Nα . Clearly,
</p>
<p>W (Nx,Ny,Nz) = &sum;
{Nαα |Nα}
</p>
<p>Γ (Nxx,Nyy,Nzz|Nx,Ny,Nz) , (5.73)
</p>
<p>where the summation is over all possible values of Nαα (α = x,y,z) for
given values of Nα . We can also express E in terms of Nαα as
</p>
<p>E =&minus;ε (Nxx +Nyy +Nzz) . (5.74)
</p>
<p>Then,
</p>
<p>Z = &sum;
{Nα}
</p>
<p>&sum;
{Nαα |Nα}
</p>
<p>Γ (Nxx,Nyy,Nzz|Nx,Ny,Nz)eβε(Nxx+Nyy+Nzz) (5.75)
</p>
<p>where the first summation is over all possible values of Nα (α = x,y,z). To
introduce the mean-field approximation, we rewrite this expression as
</p>
<p>Z = &sum;
{Nα}
</p>
<p>W (Nx,Ny,Nz) &sum;
{Nαα |Nα}
</p>
<p>Γ (Nxx,Nyy,Nzz|Nx,Ny,Nz)
W (Nx,Ny,Nz)
</p>
<p>eβε(Nxx+Nyy+Nzz) ,
</p>
<p>(5.76)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Mean-Field Approximation 225
</p>
<p>where the ratio Γ /W is the probability of finding exactly Nαα pairs of
molecules both pointing in the α-direction if we are to choose randomly
with equal probability a single configuration from the total of W configu-
</p>
<p>rations. Denoting the average with respect to this probability by 〈&middot; &middot; &middot; 〉0, we
have
</p>
<p>Z = &sum;
{Nα}
</p>
<p>W (Nx,Ny,Nz)
&lang;
</p>
<p>eβε(Nxx+Nyy+Nzz)
&rang;
</p>
<p>0
</p>
<p>&asymp; &sum;
{Nα}
</p>
<p>W (Nx,Ny,Nz)e
βε〈Nxx+Nyy+Nzz〉0 . (5.77)
</p>
<p>We still have to express 〈Nαα〉0 in terms of Nα . Noting that the desired
average is with respect to the probability Γ /W , which does not take the
Boltzmann factor into account, we have
</p>
<p>〈Nαα〉0 =
1
</p>
<p>2
&times; Nα
</p>
<p>N
&times; Nα
</p>
<p>N
z&times;N = z
</p>
<p>2N
Nα
</p>
<p>2 , (5.78)
</p>
<p>where z is the coordination number. So, we now have
</p>
<p>Z &asymp; &sum;
{Nα}
</p>
<p>W (Nx,Ny,Nz)e
βεz
2N (Nx
</p>
<p>2+Ny
2+Nz
</p>
<p>2) =: &sum;
{Nα}
</p>
<p>q(Nx,Ny,Nz) . (5.79)
</p>
<p>To simplify the computation further, we note that the summand q is pro-
</p>
<p>portional to the probability of finding exactly Nα molecules pointing in the
</p>
<p>α-direction. For a macroscopic system, we expect this probability to be
extremely sharply peaked around the average. This observation allows us
</p>
<p>to replace the summation by the maximum term of the summand:
</p>
<p>Z &asymp; max
{Nα}
</p>
<p>{q(Nx,Ny,Nz)} . (5.80)
</p>
<p>Using Stirling&rsquo;s formula, we have
</p>
<p>lnq =&minus;N(σx lnσx +σy lnσy +σz lnσz)+
1
</p>
<p>2
βεzN
</p>
<p>(
σx
</p>
<p>2 +σy
2 +σz
</p>
<p>2
)
.
</p>
<p>(5.81)
</p>
<p>where σα := Nα/N by (5.60). We need to maximize lnq under the con-
straint that
</p>
<p>{
σx +σy +σz = 1 ,
</p>
<p>0 &le; σx &le; 1 , 0 &le; σy &le; 1 , 0 &le; σz &le; 1 .
(5.82)</p>
<p/>
</div>
<div class="page"><p/>
<p>226 5 Simple Models of Adsorption
</p>
<p>This may also be written as
</p>
<p>{
σz = 1&minus; (σx +σy) ,
0 &le; σx &le; 1 , 0 &le; σy &le; 1 , 0 &le; σx +σy &le; 1 .
</p>
<p>(5.83)
</p>
<p>That is, the degrees of freedom here is just 2 not 3. So, in order to find the
</p>
<p>maximum of lnq, we replace σz in (5.81) by 1&minus;σx &minus;σy and then compute
</p>
<p>&part; lnq
</p>
<p>&part;σx
= N ln
</p>
<p>1&minus;σx &minus;σy
σx
</p>
<p>+βεzN (2σx +σy &minus;1) (5.84)
</p>
<p>and
&part; lnq
</p>
<p>&part;σy
= N ln
</p>
<p>1&minus;σx &minus;σy
σy
</p>
<p>+βεzN (σx +2σy &minus;1) . (5.85)
</p>
<p>(Just as a check, note that the second equation can be obtained by exchang-
</p>
<p>ing x and y in the first. This must be so because the expression for lnq is
</p>
<p>symmetric with respect to such an exchange.)
</p>
<p>The value of σα that maximizes lnq is just 〈σα〉 because q is extremely
sharply peaked around the average. Thus,
</p>
<p>ln
1&minus;〈σx〉&minus;〈σy〉
</p>
<p>〈σx〉
+βεz(2〈σx〉+ 〈σy〉&minus;1) = 0 (5.86)
</p>
<p>and
</p>
<p>ln
1&minus;〈σx〉&minus;〈σy〉
</p>
<p>〈σy〉
+βεz(〈σx〉+2〈σy〉&minus;1) = 0 . (5.87)
</p>
<p>We see that the isotropic phase (at point S)
</p>
<p>(〈σx〉〈σy〉,〈σz〉) .=
(
</p>
<p>1
</p>
<p>3
,
</p>
<p>1
</p>
<p>3
,
</p>
<p>1
</p>
<p>3
</p>
<p>)
(5.88)
</p>
<p>is always a solution. But, there might be other solutions. Physically, we
</p>
<p>expect that, when T &gt; Tt , the entropy dominates the free energy and lnq
is the maximum (the corresponding free energy F &asymp;&minus;kBT lnq being min-
imum) for the isotropic phase. In contrast, if T &lt; Tt , we expect that the
energy dominates the free energy and we have a nematic phase, that is, the
</p>
<p>maximum of lnq should occur elsewhere even though lnq is still extremum
</p>
<p>at point S.
</p>
<p>This suggests that the extremum at S changes from (local) maximum to
</p>
<p>local minimum at T = Tt . To see this transition, we need to expand lnq into
the Taylor series around S and retain at least up to the second-order terms.
</p>
<p>The first-order term, as we have just seen, is zero. The second derivatives</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Frequently Used Symbols 227
</p>
<p>of lnq at S are:
</p>
<p>&part; 2 lnq
</p>
<p>&part;σx
2
</p>
<p>∣∣∣∣∣
S
</p>
<p>= N
</p>
<p>(
&minus; 1
</p>
<p>1&minus;σx &minus;σy
&minus; 1
</p>
<p>σx
</p>
<p>)
+2Nβεz
</p>
<p>∣∣∣∣
S
</p>
<p>= 2N(βεz&minus;3) ,
</p>
<p>&part; 2 lnq
</p>
<p>&part;σy
2
</p>
<p>∣∣∣∣∣
S
</p>
<p>= 2N(βεz&minus;3) , and &part;
2 lnq
</p>
<p>&part;σx&part;σy
</p>
<p>∣∣∣∣∣
S
</p>
<p>= N(βεz&minus;3) . (5.89)
</p>
<p>Retaining up to the second-order terms and using (B.17),
</p>
<p>lnq &asymp; lnqS +N(βεz&minus;3)
[
(∆σx)
</p>
<p>2 +∆σx∆σy +(∆σy)
2
]
, (5.90)
</p>
<p>where qS is the value of q of the isotropic phase. We also defined
</p>
<p>∆σx := σx &minus;
1
</p>
<p>3
and ∆σy := σy &minus;
</p>
<p>1
</p>
<p>3
. (5.91)
</p>
<p>But,
</p>
<p>(∆σx)
2 +∆σx∆σy +(∆σy)
</p>
<p>2 =
</p>
<p>(
∆σx +
</p>
<p>1
</p>
<p>2
∆σy
</p>
<p>)2
+
</p>
<p>3
</p>
<p>4
(∆σy)
</p>
<p>2 (5.92)
</p>
<p>is zero only for the isotropic phase and positive otherwise. If β &lt; 3/εz, lnq
is a (local) maximum at S and the isotropic phase prevails. On the other
</p>
<p>hand, if β &gt; 3/εz, lnq is a local minimum at S. That is, the isotropic phase
is unstable and we have a nematic phase. Thus, the transition temperature
</p>
<p>Tt is εz/3kB.
</p>
<p>5.3 Frequently Used Symbols
</p>
<p>〈A〉 , ensemble average of a dynamical variable A.
</p>
<p>p1 , probability that an adsorption site is occupied.
</p>
<p>pi , linear momentum of the ith particle.
</p>
<p>pN , collective notation for p1, . . . , pN .
</p>
<p>ri , position vector of the ith particle.
</p>
<p>rN , collective notation for r1, . . . , rN .
</p>
<p>w , interaction energy between two nearest neighbor particles.
</p>
<p>z , coordination number.
</p>
<p>A , a generic dynamical variable.
</p>
<p>E , energy of a system.</p>
<p/>
</div>
<div class="page"><p/>
<p>228 5 Simple Models of Adsorption
</p>
<p>F , Helmholtz free energy.
</p>
<p>T , absolute temperature.
</p>
<p>Z , canonical partition function.
</p>
<p>W (E) , the number of microstates with H &le; E.
</p>
<p>β , 1/kBT .
ε , binding energy.
δ (x) , Dirac δ -function.
&micro; , chemical potential.
θ(x) , step function defined by (D.2).
</p>
<p>Λ , thermal wavelength h/
&radic;
</p>
<p>2πmkBT of a particle of mass m.
Ξ , grand canonical partition function.
Ω , density of states.
</p>
<p>Reference
</p>
<p>1. N. Goldenfeld (1992), Lectures on phase transitions and the renormalization group, Addison-
</p>
<p>Wesley, Reading Massachusetts</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>Thermodynamics of Interfaces
</p>
<p>Fundamental equations of thermodynamics are commonly written for homogeneous
</p>
<p>systems without explicitly accounting for effects of the container wall. Similarly, we
</p>
<p>treat a system consisting of multiple coexisting phases as if it is made of homoge-
</p>
<p>neous parts separated by sharp interfaces. This is an acceptable practice provided
</p>
<p>that the number of molecules in the vicinity of the container wall or the inter-
</p>
<p>face is negligibly small compared with the number of the molecules in the bulk.
</p>
<p>The approach becomes inappropriate for microscopic systems in which the major-
</p>
<p>ity of molecules are near a wall or an interface. For example, inhomogeneity extends
</p>
<p>throughout the entire system of interest when a fluid is confined to a narrow pore of
</p>
<p>several atomic diameters. In this case, the phase behavior changes dramatically com-
</p>
<p>pared to that in the bulk. It is also possible that the phenomena of interest are dic-
</p>
<p>tated by the properties of the interface. Examples include formation of microemul-
</p>
<p>sion, wetting of a solid surface, condensation of a vapor phase, and crystallization
</p>
<p>from a melt or a solution. In this chapter, we examine how thermodynamics can be
</p>
<p>extended to explicitly account for effects of interfaces.
</p>
<p>6.1 Interfacial Region
</p>
<p>When studying a system in two-phase coexistence, we usually ignore the micro-
</p>
<p>scopic details of the interface separating the phases. Instead, we simply think of
</p>
<p>the interface as a diathermal and movable wall that is permeable to all species. In
</p>
<p>Chap. 2, this led to the conditions of equilibrium expressed as
</p>
<p>Tα = T β , Pα = Pβ , and &micro;αi = &micro;
β
i , i = 1, . . . ,c , (6.1)
</p>
<p>where the superscripts α and β label the phases.
However, it is unrealistic to expect that the interface is sharply defined at the
</p>
<p>atomistic length scale. For example, the interface between liquid water and its sat-
</p>
<p>urated vapor is an inhomogeneous transition region over which the density changes
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 229
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1 6</p>
<p/>
</div>
<div class="page"><p/>
<p>230 6 Thermodynamics of Interfaces
</p>
<p>Phase
</p>
<p>Phase
</p>
<p>Fig. 6.1 The interfacial region is indicated by a pair of dashed lines and separates two coexisting
</p>
<p>bulk phases α and β .
</p>
<p>Phase
</p>
<p>................................................................................................................................................ ........ ........ ........ ........ ........ ........ ........ ........ ........ ........
........ .
</p>
<p>....... .
....... .
........
.......
........
........
........
........
........
........
........
........
</p>
<p>.........
........
</p>
<p>...............
........................
</p>
<p>..................................................................................................................................................................................................... ........ ........ ........... ................. ........ ........ ......... ................. .
....... ........
</p>
<p>.........
........
.......
........
........
.........
.........
........
........
........
.........
.........
........
........
.........
</p>
<p>........
........
</p>
<p>..............
...................
</p>
<p>........................
</p>
<p>Phase
</p>
<p>...................................................................................... .................. ........ ..................
.........
........
.........
........
.........
..........
</p>
<p>................
.........
</p>
<p>a b
</p>
<p>Fig. 6.2 A droplet (β ) in equilibrium with a supersaturated vapor α . At a lower degree of super-
saturation, the droplet attains bulk-like properties near its center, a situation we indicate with a pair
</p>
<p>of dashed circles in a. At a higher degree of supersaturation, however, it becomes inhomogeneous
</p>
<p>even at its center as indicated by a single dashed circle in b.
</p>
<p>continuously from the bulk liquid value to that of the vapor. Typically, the thickness
</p>
<p>of this transition region is of the order of several atomic diameters. In the case of a
</p>
<p>multicomponent system, the density of certain species within the transition region
</p>
<p>may be significantly higher or lower than its values in the coexisting bulk phases.
</p>
<p>To emphasize the nonzero thickness of the transition region, we use a pair of dashed
</p>
<p>lines to indicate an interface between phases as shown in Fig. 6.1.
</p>
<p>The equilibrium between two phases may involve a curved interface. A familiar
</p>
<p>example is meniscus formation where the interface meets the wall. As we shall see,
</p>
<p>a vapor phase can be compressed beyond its saturation pressure without undergoing
</p>
<p>a phase transition. This supersaturated vapor can coexist with a liquid droplet. Sim-
</p>
<p>ilarly, a bubble can coexists with superheated liquid phase. In these cases also, we
</p>
<p>have a transition region of nonzero thickness, which is again indicated by a pair of
</p>
<p>dashed circles as in Fig. 6.2a.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Defining a System 231
</p>
<p>With the increasing degree of supersaturation, the droplet (in equilibrium with the
</p>
<p>supersaturated vapor) becomes smaller and eventually loses a bulk-like properties
</p>
<p>even at its center. The same applies for the bubble with an increasing degree of
</p>
<p>superheating. This will be indicated as in Fig. 6.2b using a single dashed circle,
</p>
<p>within which the state of the matter is inhomogeneous.
</p>
<p>Following Ref. [12], we develop thermodynamics of interfaces for a system
</p>
<p>containing a spherical droplet. With the assumed spherical symmetry, our theory
</p>
<p>remains applicable even for extremely small droplets illustrated in Fig. 6.2b. No sat-
</p>
<p>isfactory extension of the theory has been developed to cope with the situation in
</p>
<p>Fig. 6.2b without the spherical symmetry. Our formulation carries over to the flat
</p>
<p>interface in Fig. 6.1 simply by increasing the characteristic radius of the interface
</p>
<p>indefinitely.
</p>
<p>6.2 Defining a System
</p>
<p>Let us consider an isolated macroscopic body that contains a small inhomogeneous
</p>
<p>region. For concreteness, we consider a microscopic liquid droplet floating in a
</p>
<p>supersaturated (metastable) vapor phase. We suppose that the droplet is spherically
</p>
<p>symmetric. As shown in Fig. 6.3, we take a spherical region of radius R0 centered
</p>
<p>around the center of symmetry and mentally divide the body into two parts. The
</p>
<p>region inside the sphere will be called region I. The region outside the sphere is
</p>
<p>region II. Then, region I contains the droplet, some portion of the metastable vapor
</p>
<p>phase, and the interface between them. Region II contains only the remaining por-
</p>
<p>tion of the homogeneous vapor phase.
</p>
<p>Because region I contains all that is interesting to us, it will be advantageous to
</p>
<p>take region I as the system,treating region II as the surroundings. But, we must first
</p>
<p>ask if thermodynamic quantities such as the internal energy U I , entropy SI , and the
</p>
<p>number NIi of molecules of species i are well defined for region I. This is because we
</p>
<p>are interested in effects of interfaces in this chapter and we cannot afford to simply
</p>
<p>ignore the interaction between I and II, which is mediated by molecules near the
</p>
<p>boundary at R0.
</p>
<p>R0
</p>
<p>Region I
</p>
<p>Region II
</p>
<p>................................................................
.................
</p>
<p>....
</p>
<p>.............
.......
</p>
<p>...........
.........
</p>
<p>.........
.........
..
</p>
<p>.........
.........
..
</p>
<p>........
........
.....
</p>
<p>........
........
.....
</p>
<p>........
........
.....
</p>
<p>........
</p>
<p>........
</p>
<p>.....
</p>
<p>........
</p>
<p>........
</p>
<p>.....
</p>
<p>........
........
.....
</p>
<p>........
........
.....
</p>
<p>........
........
.....
</p>
<p>.........
.........
</p>
<p>..
</p>
<p>.........
..........
</p>
<p>.
</p>
<p>...........
.........
</p>
<p>.............
.......
</p>
<p>.................
....
</p>
<p>..................... ..................... ..................... ..................... .....................
.....................
</p>
<p>.....................
</p>
<p>....................
</p>
<p>....................
</p>
<p>....................
</p>
<p>....................
</p>
<p>.....................
</p>
<p>.....................
</p>
<p>.....................
</p>
<p>.....................
</p>
<p>.....................
</p>
<p>.....................
</p>
<p>.....................
</p>
<p>.....................
</p>
<p>....................
</p>
<p>....................
</p>
<p>....................
</p>
<p>....................
</p>
<p>.....................
.....................
</p>
<p>..........................................
</p>
<p>................................................................................................ ........ ........ ........ ........ ........ ........ ........
........ .
</p>
<p>....... .
.......
........
........
........
........
........
</p>
<p>.........
........
</p>
<p>.......................
</p>
<p>Fig. 6.3 An isolated system containing a metastable vapor phase and a spherical liquid droplet
</p>
<p>(dashed line). The sphere of radius R0 (solid line) separates the isolated system into regions I and II.</p>
<p/>
</div>
<div class="page"><p/>
<p>232 6 Thermodynamics of Interfaces
</p>
<p>Fig. 6.4 The system defined as the conic region (thick solid lines).
</p>
<p>To define NIi , we adopt the convention that a molecule of species i belongs to
</p>
<p>region I if its center of mass is inside I.29 How about U I? Due to our choice of
</p>
<p>R0, the region of inhomogeneity is contained in region I, and region II is homo-
</p>
<p>geneous. Thus, the internal energy density is uniform throughout region II and its
</p>
<p>internal energy is this energy density multiplied by the volume of region II. Because
</p>
<p>the macroscopic body is isolated, its internal energy is a well-defined quantity. We
</p>
<p>obtain U I as the difference between these energies. The same argument applies to SI .
</p>
<p>Now that region I is shown to have well-defined U I , SI , and NIi , can we choose
</p>
<p>it as our system? Not quite. Recall that thermodynamic quantities are classified
</p>
<p>into either extensive or intensive quantities. This classification plays an essential
</p>
<p>role in thermodynamics and gives rise to such important identities as the Euler and
</p>
<p>the Gibbs&ndash;Duhem relations. When extending thermodynamics to include effects of
</p>
<p>interfaces, it is highly desirable that we maintain this classification. To say that U I is
</p>
<p>extensive, we must show that the internal energy of a system that is λ times region
I is λU I . But, what is that system when λ =
</p>
<p>&radic;
2, for example?
</p>
<p>Instead of region I, we define our system as a conic region with its apex at O and
</p>
<p>a portion of the boundary at R0 as its base as shown in Fig. 6.4. We use ω to denote
the solid angle subtended by the base at O .
</p>
<p>The solid angle subtended by an object at a point is the area occupied by the
</p>
<p>object when it is projected onto a unit sphere centered around the point by means
</p>
<p>of rays of light emanating from or converging onto that point. So, the solid angle
</p>
<p>of a sphere at its center is 4π , while that of the hemisphere is 2π . The solid angle
subtended at the center of a cube by one of its six faces is 4π/6 = 2π/3.30
</p>
<p>Because of the spherical symmetry of region I, the internal energy, entropy, and
</p>
<p>the number of moles of species i in the system are given by
</p>
<p>U =
ω
</p>
<p>4π
U I , S =
</p>
<p>ω
</p>
<p>4π
SI , and Ni =
</p>
<p>ω
</p>
<p>4π
NIi , (6.2)
</p>
<p>respectively. Since U I , SI , and NIi are all well-defined, so are U , S, and Ni for any ω
between 0 and 4π .</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Condition of Equilibrium 233
</p>
<p>6.3 Condition of Equilibrium
</p>
<p>To formulate the condition of equilibrium, we will have to consider variations in
</p>
<p>the state of the system. Here again, a careful analysis is required to account for the
</p>
<p>interaction across the system boundaries. This is the subject of Sect. 6.3.1. The key
</p>
<p>conclusion is that, to the first order of variations, we may regard the variations to
</p>
<p>be affecting the state of the system only, while leaving the surroundings unaffected.
</p>
<p>This is despite the fact that these two parts are in direct contact with each other. If
</p>
<p>you wish to omit Sect. 6.3.1, keep this conclusion in mind and head to Sect. 6.3.2.
</p>
<p>6.3.1 &dagger;Variations in the State of the System
</p>
<p>Because the system is inhomogeneous, densities of the internal energy, entropy, and
</p>
<p>the number of molecules vary across the system, and collectively determine the state
</p>
<p>of the matter. In what follows, we adopt a generic notation ξ (r) for these densities.
At the initial state, ξ depends only on the radial distance r := ||r|| from O and reaches
a constant as R0 is approached from O .
</p>
<p>Let us now consider a variation in the state of the system, and denote by ξ (r)+
δξ (r) the value of ξ in the varied state. Unless the variation is accompanied by a
similar change in the surroundings, ξ (r)+δξ (r) will not be spherically symmetric
over the entire 4π nor is it uniform across R0. The resulting infinitesimal discrepancy
in the state of the matter between the two parts may cause δξ to exhibit a complex r
dependence across the system boundaries, which is very difficult to account for. For
</p>
<p>these more general variations, called discontinuous variation for brevity,31 how do
</p>
<p>we evaluate the values of δU and δS?
The simplest approach is to ignore the complex behavior of δξ (r) across the
</p>
<p>boundaries. When figuring out U and S of the system in its varied state, we pretend
</p>
<p>that the surroundings have experienced a similar variation to ensure the spherical
</p>
<p>symmetry of region I over the entire 4π and the uniformity across R0.32 This is indi-
cated by the dashed line in Fig. 6.5. For the same varied state we are considering,
</p>
<p>the internal energy and entropy of the surroundings are computed by assuming that
</p>
<p>the state of the matter in the surroundings continues beyond the boundaries and that
</p>
<p>the spherical symmetry of region I and the uniformity across R0 still hold. The same
</p>
<p>convention can be adopted when considering simultaneous variations involving both
</p>
<p>the system and the surroundings.
</p>
<p>Variation
Initial
State
</p>
<p>Varied
State
</p>
<p>System Boundary The surroundings
</p>
<p>Fig. 6.5 Discontinuous variation of the system.</p>
<p/>
</div>
<div class="page"><p/>
<p>234 6 Thermodynamics of Interfaces
</p>
<p>BBB
</p>
<p>L R L
</p>
<p>LLL RRR
</p>
<p>a b c
</p>
<p>Fig. 6.6 Three distinct discontinuous variations either in the system (L) or in the surroundings
</p>
<p>(R). In each diagram, the long horizontal solid lines indicates the initial state, while the shorter
</p>
<p>horizontal line continued across the boundary (B) in dashed line indicates the varied state.
</p>
<p>This is how we proceed. As far as the internal energy and entropy of the entire
</p>
<p>isolate system is concerned, the error thus committed is in the higher order terms of
</p>
<p>the variations, and does not affect our discussion of the condition of equilibrium.
</p>
<p>To see this, let us consider the three distinct variations illustrated in Fig. 6.6. In
</p>
<p>Fig. 6.6a, we consider an infinitesimal variation δξ in the state of the system starting
from the initial state specified by ξ . Our method of computing the variation of the
internal energy (or the entropy) in the system (L) and the surroundings (R) leads
</p>
<p>to an error in our estimate of the variation of the internal energy of the composite
</p>
<p>system, which we denote by ∆L(ξ ,δξ ), where the superscript L indicates that the
variation is taken in L. Similarly for the superscript R in the case of variations in R.
</p>
<p>But, in the vicinity of the boundary (B), the varied state in Fig. 6.6a is identical
</p>
<p>to that in Fig. 6.6b. Thus,
</p>
<p>∆L(ξ ,δξ ) = ∆R(ξ +δξ ,&minus;δξ ) . (6.3)
</p>
<p>Now, Fig. 6.6b, c share the identical initial state in which the state of the matter is
</p>
<p>locally symmetric around B. If we focus only on the immediate vicinity of B, their
</p>
<p>varied states are the mirror images of each other. This implies that
</p>
<p>∆R(ξ +δξ ,&minus;δξ ) = ∆L(ξ +δξ ,&minus;δξ ) . (6.4)
</p>
<p>Thus, we have an equality
</p>
<p>∆L(ξ ,δξ ) = ∆L(ξ +δξ ,&minus;δξ ) . (6.5)
</p>
<p>Expanding the expression on the left in the Maclaurin series (see Appendix B.1)
</p>
<p>with respect to the second argument, we have
</p>
<p>∆L(ξ ,δξ ) = ∆L(ξ ,0)+∆L2 (ξ ,0)δξ +h.o. = ∆
L
2 (ξ ,0)δξ +h.o. , (6.6)
</p>
<p>where ∆L2 is the partial derivative of ∆
L with respect to its second argument. We
</p>
<p>also used the fact that ∆L(ξ ,0) &equiv; 0 for any ξ . That is, no error is introduced if no</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Condition of Equilibrium 235
</p>
<p>variation is taken. Likewise, we have
</p>
<p>∆L(ξ +δξ ,&minus;δξ ) =&minus;∆L2 (ξ +δξ ,0)δξ +h.o. =&minus;∆L2 (ξ ,0)δξ +h.o. , (6.7)
</p>
<p>where the last step follows from the Taylor expansion with respect to the first argu-
</p>
<p>ment. Using (6.6) and (6.7) in (6.5), we see that ∆2(ξ ,0) can only be zero. Thus,
</p>
<p>∆L(ξ ,δξ ) = h.o. (6.8)
</p>
<p>For alternative demonstrations of the same idea, see Refs. [4, 11].
</p>
<p>We note that the symmetry around B of the initial state of the matter entered as a
</p>
<p>key ingredient of our analysis. However, because the system boundaries are curved,
</p>
<p>one side of B is not entirely equivalent to the other if we focus on a larger portion
</p>
<p>of the boundary. This is why we have taken a &ldquo;local&rdquo; view when looking for the
</p>
<p>symmetry. Because our system is made of atoms, one may argue that ξ (r) loses
its meaning in an extremely local view. This is not so. In thermodynamics, we are
</p>
<p>interested in thermally averaged quantities, and ξ (r) is well defined at every point
in space.
</p>
<p>If there should remain any doubt about our analysis, it is with variations taken
</p>
<p>near the apex O of the cone. (See Fig. 6.4.) For a sufficiently small ω , a discontin-
uous variation taken in the system affects only those molecules of the surroundings
</p>
<p>near O . A similar discontinuous variation taken in the surroundings may affect all
</p>
<p>the molecules of the system near O . In that case, it seems unreasonable to expect
</p>
<p>(6.4) to hold.
</p>
<p>We can easily circumvent the difficulty indicated here for a nucleus with a homo-
</p>
<p>geneous core. We simply exclude the apex and its vicinity by means of another
</p>
<p>spherical boundary passing through the homogeneous core. For the situation indi-
</p>
<p>cated by Fig. 6.2b, however, this is not possible. Even in such cases, one can insist
</p>
<p>on the formal significance of the theory we develop on the basis of (6.8) provided
</p>
<p>that various physical quantities behave in a physically sensible manner, for example,
</p>
<p>a quantity that must be real and positive remains so.
</p>
<p>Far more satisfactory will be methods based on the principle of statistical
</p>
<p>mechanics that apply regardless of the presence or absence of the homogeneous
</p>
<p>core. Classical density functional theory discussed in Chap. 7 is one such theory.
</p>
<p>Molecular simulation is another satisfactory approach. The important point is that
</p>
<p>thermodynamics of interfaces provides a useful framework when trying to interpret
</p>
<p>the predictions of these statistical mechanical approaches. In fact, no inconsistency
</p>
<p>has ever been discovered to implicate (6.8).
</p>
<p>6.3.2 Fixed System Boundaries
</p>
<p>As we have just demonstrated, we may suppose that the state of the system can be
</p>
<p>varied without affecting the surroundings to the first order of the variation. Thus, if
</p>
<p>we limit our considerations up to this order, the system can be treated as if it is iso-</p>
<p/>
</div>
<div class="page"><p/>
<p>236 6 Thermodynamics of Interfaces
</p>
<p>Fig. 6.7 The composite system consisting of the system and a portion of region II.
</p>
<p>lated. This being case, the condition of its equilibrium is that S takes the maximum
</p>
<p>possible value for given U , N1, . . . ,Nc, and the fixed system boundaries specified by
R0 and ω . It is important to note that specification of the system volume alone is not
sufficient for an inhomogeneous system. We can adjust R0 and ω without changing
the system volume. But this will certainly affect the amount of the interfacial region
</p>
<p>included in our system.
</p>
<p>The condition of equilibrium can be expressed in different ways. For example,
</p>
<p>we may demand that U be minimum for given S, N1, . . . ,Nc, R0, and ω . But, the
most convenient formulation is that of Sect. 2.9.3, according to which the necessary
</p>
<p>condition of equilibrium is that
</p>
<p>δU = A0δS+
c
</p>
<p>&sum;
i=1
</p>
<p>AiδNi , R0,ω const. (6.9)
</p>
<p>holds for any reversible variations with fixed system boundaries.33 Here, A0 and
</p>
<p>A1, . . . ,Ac are constants yet to be determined. We recall from Sect. 2.9.3 that a vari-
ation δXi of some additional variable Xi is said to be reversible if it can take both
positive and negative values.
</p>
<p>To determine the values of A0 and Ai, which are the temperature and chemical
</p>
<p>potentials of species i of the system, we consider a composite system consisting of
</p>
<p>our original system and a portion of region II as shown in Fig. 6.7. The composite
</p>
<p>system itself may be regarded as isolated in the same way the original system was.
</p>
<p>Because the boundary between the original system and region II is purely a construct
</p>
<p>of our imagination, it exerts no physical effect. In other words, it is a diathermal and
</p>
<p>rigid wall permeable to all species. Thus, provided that S and N1, . . . ,Nc of the sys-
tem are all capable of reversible variations, the condition of equilibrium of the com-
</p>
<p>posite system demands that A0 be equal to the temperature T of the surroundings
</p>
<p>and Ai to the chemical potential &micro;i of species i of the surroundings. Accordingly,
(6.9) now reads
</p>
<p>δU = TδS+
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδNi , R0,ω const. (6.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Condition of Equilibrium 237
</p>
<p>.
</p>
<p>..............................
</p>
<p>..............................
</p>
<p>...............................
</p>
<p>...............................
...............................
</p>
<p>..........................................................................................................................
</p>
<p>R0
</p>
<p>.
</p>
<p>..............................
</p>
<p>.............................
</p>
<p>............................
</p>
<p>..............................
</p>
<p>..............................
</p>
<p>...............................
</p>
<p>...............................
</p>
<p>...............................
...............................
</p>
<p>R0
.
</p>
<p>.......................
</p>
<p>.......................
.......................
</p>
<p>....................................................................
</p>
<p>R
</p>
<p>Bulk
</p>
<p>Bulk
</p>
<p>........................
..............................................
</p>
<p>......................................................................................................................................... ........................
..............................................
</p>
<p>.........................................................................................................................................
</p>
<p>...........................................................................................................
..............................
</p>
<p>.......... ...........................................
</p>
<p>a b
</p>
<p>Fig. 6.8 Construction of a reference system. a Actual system. b Reference system with a dividing
</p>
<p>surface at R.
</p>
<p>6.3.3 Reference System
</p>
<p>Before considering the generalization of (6.10) for the case of movable system
</p>
<p>boundaries in Sect. 6.3.4, it is convenient to introduce the reference system, also
</p>
<p>known as the hypothetical system, as illustrated in Fig. 6.8. First, we draw a spher-
</p>
<p>ical surface, called the dividing surface, of radius R (&lt; R0) according to some
arbitrary rule to be decided on later. Then, we fill the space between R and R0 of
</p>
<p>the system by bulk phase α . By &ldquo;bulk phase,&rdquo; we mean that it behaves as if it is a
portion of macroscopic and homogeneous phase α . The space between O and R is
filled with bulk phase β that has the same temperature and chemical potentials as
α phase. This does not imply that phase β is identical to phase α . In other words,
</p>
<p>&micro;αi (T,P
α ,xα1 , . . . ,x
</p>
<p>α
c&minus;1) = &micro;
</p>
<p>β
i (T,P
</p>
<p>β ,x
β
1 , . . . ,x
</p>
<p>β
c&minus;1) , i = 1, . . . ,c (6.11)
</p>
<p>where xi is the mole fraction of species i, has a nontrivial solution with P
α 	= Pβ .
</p>
<p>For these bulk phases, we have
</p>
<p>δUα = TδSα +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδN
α
i and δU
</p>
<p>β = TδSβ +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδN
β
i (6.12)
</p>
<p>for variations that do not affect the system boundaries. Subtracting (6.12) from
</p>
<p>(6.10), we find
</p>
<p>δU s = TδSs +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδN
s
i , R0,ω const. (6.13)
</p>
<p>In this equation, the quantities defined by
</p>
<p>U s :=U &minus; (Uα +Uβ ) , Ss := S&minus; (Sα +Sβ ) , and Nsi := Ni &minus; (Nαi +N
β
i ) .
(6.14)
</p>
<p>are known as the surface excess quantities.</p>
<p/>
</div>
<div class="page"><p/>
<p>238 6 Thermodynamics of Interfaces
</p>
<p>By construction, the reference system has no interface. Thus, any difference
</p>
<p>between the actual and the reference systems can be attributed to the presence of the
</p>
<p>interface. We emphasize that the reference system is purely a theoretical construct
</p>
<p>that cannot be created in practice. The role it plays is entirely analogous to that of
</p>
<p>various ideal systems, such as ideal gas mixture and ideal mixtures, introduced in
</p>
<p>order to characterize the behavior of real mixtures.
</p>
<p>6.3.4 Movable System Boundaries
</p>
<p>Equation (6.10) can be generalized easily for the case of movable system bound-
</p>
<p>aries. Because the system boundary at R0 is passing through a homogeneous region,
</p>
<p>its motion introduces a simple work term that may be written as
</p>
<p>&minus;ωR02PαδR0 , (6.15)
</p>
<p>where ωR02δR0 is the volume swept out by the boundary as it moves to the new
position R0 +δR0. In contrast, a change in ω involves stretching or compressing of
the inhomogeneous region and the associated work term seems difficult to compute.
</p>
<p>We do know, however, that it should be proportional to δω . That is, since U is a
function of S, N1, . . . ,Nc, R0, and ω , the work term in question is (&part;U/&part;ω)S,N,R0δω ,
which we shall denote simply as σδω . Thus,
</p>
<p>δU = TδS+
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδNi &minus;ωR02PαδR0 +σδω . (6.16)
</p>
<p>But, how do we evaluate the value of σ? For our thermodynamic formulation to
be of any use, this quantity must be related to something we can measure at least in
</p>
<p>principle. For this purpose, it proves useful to consider the generalization of (6.13).
</p>
<p>We note that δU s is determined completely if δU , δUα , and δUβ are given. The
latter two quantities refer to the bulk homogeneous phases and are given, respec-
</p>
<p>tively, by
</p>
<p>δUα = TδSα &minus;PαδVα +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδN
α
i (6.17)
</p>
<p>and
</p>
<p>δUβ = TδSβ &minus;PβδV β +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδN
β
i (6.18)
</p>
<p>for movable system boundaries.
</p>
<p>A part of δVα comes from a change in R0 and makes the identical contribution
to δU and δUα , thus dropping out from δU s. The remaining part of δVα , δV β , and
δω are completely determined once δR and δA are given, where A is the area of the
dividing surface, because of the geometric relations
</p>
<p>A = ωR2 , Vα =
ω
</p>
<p>3
</p>
<p>(
R0
</p>
<p>3 &minus;R3
)
, and V β =
</p>
<p>ω
</p>
<p>3
R3 . (6.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Condition of Equilibrium 239
</p>
<p>The validity of these equations is most readily seen by applying them for the entire
</p>
<p>spherical region, for which ω = 4π . It follows that
</p>
<p>δU s = TδSs +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδN
s
i + γδA+CδR . (6.20)
</p>
<p>Because this result holds for any variation and Ss, Ns1, . . . ,N
s
c , A, and R are path-
</p>
<p>independent state functions, we conclude that
</p>
<p>U s =U s(Ss,Ns1, . . . ,N
s
c ,A,R) . (6.21)
</p>
<p>From these two equations, we see that
</p>
<p>C =
</p>
<p>(
&part;U s
</p>
<p>&part;R
</p>
<p>)
</p>
<p>Ss,Ns,A
</p>
<p>=C(Ss,Ns1, . . . ,N
s
c ,A,R) . (6.22)
</p>
<p>Combining (6.17), (6.18), and (6.20), we obtain
</p>
<p>δU = TδS+
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iδNi &minus;PαδVα &minus;PβδV β + γδA+CδR . (6.23)
</p>
<p>Exercise 6.1. Express σ in terms of γ , Pα , Pβ , R and R0. ///
</p>
<p>6.3.5 Laplace Equation
</p>
<p>As we saw in Sect. 6.3.2, the equilibrium of the system demands that its temperature
</p>
<p>and chemical potentials be equal to those in the surrounding phase α . In this section,
we derive an additional condition of equilibrium.
</p>
<p>For variations taken while holding S, N1, . . . ,Nc, R0 and ω constant, we have
</p>
<p>δS = 0 , δN1 = &middot; &middot; &middot;= δNc = 0 ,
δVα = &minus;ωR2δR , δV β = ωR2δR , and δA = 2ωRδR . (6.24)
</p>
<p>Substituting these relations into (6.23), we find
</p>
<p>(δU)S,N,R0,ω =(P
α&minus;Pβ )ωR2δR+2γωRδR+CδR=
</p>
<p>(
Pα &minus;Pβ + 2γ
</p>
<p>R
+ ca
</p>
<p>)
AδR ,
</p>
<p>(6.25)
</p>
<p>where ca :=C/A.
Provided that R &gt; 0, δR can be either positive or negative. Recalling (2.97), we
</p>
<p>conclude that
</p>
<p>Pβ &minus;Pα = 2γ
R
</p>
<p>+ ca . (6.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>240 6 Thermodynamics of Interfaces
</p>
<p>.
...........................
</p>
<p>............................
</p>
<p>............................
</p>
<p>............................
</p>
<p>...........................
</p>
<p>..........................
</p>
<p>..........................
</p>
<p>...........................
............................
</p>
<p>............................
..........................................................................................................................................
</p>
<p>..........................
..
</p>
<p>....................
.......
</p>
<p>.................
.........
</p>
<p>..............
............
</p>
<p>..............
.............
</p>
<p>.............
.............
..
</p>
<p>.............
.............
..
</p>
<p>............
............
....
</p>
<p>...........
</p>
<p>...........
</p>
<p>.....
</p>
<p>P
</p>
<p>P
</p>
<p>Fig. 6.9 The surrounding fluid pushes the hemisphere of radius R downward with the pressure
</p>
<p>πR2Pα , and so does the tension γ with the force 2πRγ . (From (6.20), we see that γ has the dimen-
sion of energy per unit area or force per unit length.) The fluid in the sphere pushes the hemisphere
</p>
<p>upward with the force πR2Pβ . The balance of these forces leads to (6.28).
</p>
<p>So far, we have left unspecified how we choose the radius R of the dividing
</p>
<p>surface for a given state of the system. Accordingly, (6.26) holds for any choice for
</p>
<p>the dividing surface. One common choice is to determine R by
</p>
<p>C = 0 . (6.27)
</p>
<p>For any given state of the system in equilibrium, the quantities Ss, Ns1, . . . ,N
s
c , and
</p>
<p>A in (6.22) all depend only on R. Thus, (6.27) indeed is an equation for R. For a
</p>
<p>sufficiently large droplet (or bubble) shown in Fig. 6.2a, the thickness of the inter-
</p>
<p>facial region is considerably smaller than its radius of curvature. In this case, the
</p>
<p>solution of (6.27) exists and the dividing surface so determined is located within the
</p>
<p>inhomogeneous transition region. An explicit demonstration is found in Ref. [4].
</p>
<p>For the choice of the dividing surface just indicated, (6.26) reduces to
</p>
<p>Pβ &minus;Pα = 2γ
R
</p>
<p>. (6.28)
</p>
<p>This is known as the Laplace equation, and is identical to the condition of mechan-
</p>
<p>ical equilibrium of a membrane of zero thickness having the tension γ but no rigid-
ity while separating a spherical region at Pβ from the surroundings at Pα . This is
</p>
<p>illustrated in Fig. 6.9. For this reason, the dividing surface defined by (6.27) and γ
associated with this dividing surface are called the surface of tension and the sur-
</p>
<p>face tension, respectively. We choose the surface of tension as the dividing surface
</p>
<p>with the expectation that γ may be measurable by some mechanical measurement
technique.
</p>
<p>In the R &rarr; &infin; limit, a spherical interface becomes a flat interface separating two
macroscopic phases in equilibrium. At the same time, (6.28) reduces to Pα = Pβ ,
which is just the second equation in (6.1). In other words, the presence of an inter-
</p>
<p>face does not affect the condition of phase coexistence across a flat interface.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Euler Relation 241
</p>
<p>6.4 Euler Relation
</p>
<p>Applying (6.20) to an infinitesimal process that takes the system from a state of
</p>
<p>equilibrium to another,34 we obtain
</p>
<p>dU s = T dSs +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;idN
s
i + γdA+CdR . (6.29)
</p>
<p>We now integrate (6.29) from ω = 0 to some nonzero ω (&le; 4π) without chang-
ing the intensive state of the system. During such a process, T and &micro;1, . . . ,&micro;c are
constant. If we do not vary the condition to determine the radius of the dividing
</p>
<p>surface, γ and R are also constant. (Because U s and A are both extensive variables,
γ = (&part;U s/&part;A)Ss,Ns,R should be intensive.) Thus,
</p>
<p>U s = T Ss +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iN
s
i + γA , (6.30)
</p>
<p>which is the Euler relation for the inhomogeneous system.
</p>
<p>According to (6.30), γA is the reversible work required to convert the reference
system into the actual system by creating an interface. (See Fig. 6.10.) To see this,
</p>
<p>let [U ] denote the increase in the internal energy of the system upon the reversible
creation of the interface. Because the surroundings can be made arbitrarily large, T
</p>
<p>and &micro;1, . . . ,&micro;c remain constant during the process. Thus, if we regard the system and
its surroundings as forming an isolated system, the increase in the internal energy
</p>
<p>of the surroundings is given by
</p>
<p>&minus;T [S]&minus;
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;i[Ni] , (6.31)
</p>
<p>0 r
</p>
<p>n
</p>
<p>n
</p>
<p>n
</p>
<p>R
</p>
<p>W rev .......... .......... .......... ........... .............
..............
.
..............
...
</p>
<p>.............
......
</p>
<p>....................
..........
</p>
<p>...........
.............
...............
.................
</p>
<p>...................
</p>
<p>0 r
</p>
<p>n
</p>
<p>n
</p>
<p>n
</p>
<p>R
</p>
<p>Fig. 6.10 Introduction of an interface into the reference system by means of a reversible work
</p>
<p>source. The number density of molecules is denoted by n with a superscript indicating its values
</p>
<p>in the bulk reference phases.</p>
<p/>
</div>
<div class="page"><p/>
<p>242 6 Thermodynamics of Interfaces
</p>
<p>where [S] and [Ni] are defined similarly to [U ]. This expression follows from the
fundamental property relation (2.37) applied to the surroundings and the fact that
</p>
<p>the number of molecules of each species is constant in the isolated system (in the
</p>
<p>absence of chemical reactions). Its entropy also remains constant for a reversible
</p>
<p>process.
</p>
<p>In the isolated system, any increase in its internal energy must be due to the
</p>
<p>reversible work source. Recalling that
</p>
<p>[U ] =U &minus; (Uα +Uβ ) =U s , etc. , (6.32)
</p>
<p>we obtain the desired reversible work as
</p>
<p>[U ]&minus;T [S]&minus;
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;i[Ni] =U
s &minus;T Ss &minus;
</p>
<p>c
</p>
<p>&sum;
i=1
</p>
<p>&micro;iN
s
i = γA . (6.33)
</p>
<p>We note that γ must be positive in order for the interface to be stable. Otherwise, the
system can lower its free energy simply by increasing A.35
</p>
<p>6.5 Gibbs&ndash;Adsorption Equation
</p>
<p>Upon differentiation of (6.30), we obtain
</p>
<p>dU s = T dSs +
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;idN
s
i + γdA+S
</p>
<p>sdT +
c
</p>
<p>&sum;
i=1
</p>
<p>Nsi d&micro;i +Adγ . (6.34)
</p>
<p>Substituting (6.29) into this expression, we find
</p>
<p>dγ =&minus;ssdT &minus;
c
</p>
<p>&sum;
i=1
</p>
<p>Γid&micro;i + cadR , (6.35)
</p>
<p>where
</p>
<p>ss :=
Ss
</p>
<p>A
and Γi :=
</p>
<p>Nsi
A
</p>
<p>(6.36)
</p>
<p>are called the superficial densities of the entropy and the number of molecules of
</p>
<p>species i. We observe from (6.35) that
</p>
<p>γ = γ(T,&micro;1, . . . ,&micro;c,R) (6.37)
</p>
<p>and that
</p>
<p>ss =&minus;
(
&part;γ
</p>
<p>&part;T
</p>
<p>)
</p>
<p>&micro; ,R
</p>
<p>, Γi =&minus;
(
</p>
<p>&part;γ
</p>
<p>&part;&micro;i
</p>
<p>)
</p>
<p>T,&micro; j 	=i,R
, and ca =
</p>
<p>(
&part;γ
</p>
<p>&part;R
</p>
<p>)
</p>
<p>T,&micro;
</p>
<p>. (6.38)
</p>
<p>If we fix the condition for locating the dividing surface, R will change in response
</p>
<p>to changes in T and &micro;1, . . . ,&micro;c. For example, the radius of the surface of tension,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Flat Interface 243
</p>
<p>assumed to be finite, is determined by
</p>
<p>ca(T,&micro;1, . . . ,&micro;c,R) = 0 . (6.39)
</p>
<p>The radius R we find by solving this equation, in general, is a function of T and
</p>
<p>&micro;1, . . . ,&micro;c.36 Thus, the partial derivatives in (6.38) are all taken while modifying the
condition for locating the dividing surface. For a reformulation of thermodynamics
</p>
<p>of interfaces emphasizing this perspective, see Ref. [6].
</p>
<p>For the surface of tension, the last equation in (6.38) gives
</p>
<p>ca =
</p>
<p>(
&part;γ
</p>
<p>&part;R
</p>
<p>)
</p>
<p>T,&micro;
</p>
<p>= 0 , (6.40)
</p>
<p>which has the following interpretation. For a given intensive state of the system,
</p>
<p>γ changes depending on our choice for the dividing surface. But, this dependence
is such that γ takes an extremum value for the surface of tension. Using (6.39) in
(6.35), we see that
</p>
<p>dγ =&minus;ssdT &minus;
c
</p>
<p>&sum;
i=1
</p>
<p>Γid&micro;i , (6.41)
</p>
<p>for this dividing surface. This result is known as the Gibbs adsorption equation.
</p>
<p>Instead of (6.38), we now have
</p>
<p>ss =&minus;
(
&part;γ
</p>
<p>&part;T
</p>
<p>)
</p>
<p>&micro;
</p>
<p>and Γi =&minus;
(
</p>
<p>&part;γ
</p>
<p>&part;&micro;i
</p>
<p>)
</p>
<p>T,&micro; j 	=i
</p>
<p>. (6.42)
</p>
<p>In these equations, R is absent from the list of variables that are held fixed. Instead,
</p>
<p>R is a function of T and &micro;1, . . . ,&micro;c, and is allowed to change in response to infinites-
imal changes of these variables so as to satisfy (6.40).
</p>
<p>6.6 Flat Interface
</p>
<p>In contrast to the case of a spherical interface, the value of γ of a flat interface does
not depend on the choice of the dividing surface.
</p>
<p>To see this,we note that the reversible work required to create a unit area of the
</p>
<p>interface from a reference system should remain finite even in the R &rarr; &infin; limit for
any choice of the dividing surface. In this limit, therefore, (6.26) reduces to
</p>
<p>(
&part;γ
</p>
<p>&part;R
</p>
<p>)
</p>
<p>T,&micro;
</p>
<p>= ca = P
β &minus;Pα &minus; 2γ
</p>
<p>R
&rarr; 0 as R &rarr; &infin; , (6.43)
</p>
<p>where we recall that Pα and Pβ are independent of the choice of the dividing surface
</p>
<p>and that Pα = Pβ in the same limit we are considering.
We can reach the same conclusion through a more explicit computation. Let γ&infin;,1
</p>
<p>denote the value of γ of a flat interface for the dividing surface at z1 in Fig. 6.11.</p>
<p/>
</div>
<div class="page"><p/>
<p>244 6 Thermodynamics of Interfaces
</p>
<p>.......... .......... .......... ........... .............
..............
.
..............
...
</p>
<p>.............
......
</p>
<p>....................
..........
</p>
<p>...........
.............
...............
.................
</p>
<p>...................
</p>
<p>0 z1 z2
</p>
<p>z
</p>
<p>z
</p>
<p>n
</p>
<p>n
</p>
<p>n
</p>
<p>Fig. 6.11 The number density profile n(z) of molecules across a flat interface. Two possible
choices are shown for the dividing surface.
</p>
<p>Using (6.30),
</p>
<p>γ&infin;,1A =U &minus; (Uα +Uβ )&minus;T [S&minus; (Sα +Sβ )]&minus;
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;i[Ni &minus; (Nαi +N
β
i )] . (6.44)
</p>
<p>When we move the dividing surface to z2 := z1 +∆z as shown in Fig. 6.11, Uα and
Uβ in (6.44) must be replaced by
</p>
<p>Uα &minus;uαA∆z and Uβ +uβA∆z , (6.45)
</p>
<p>respectively, where uα is the internal energy density of the bulk α phase. Similarly
for uβ . Denoting the entropy density by s and the number density of molecules of
</p>
<p>species i by ni, we have
</p>
<p>γ&infin;,2A = γ&infin;,1A+
</p>
<p>[(
uα &minus;T sα &minus;
</p>
<p>c
</p>
<p>&sum;
i=1
</p>
<p>&micro;in
α
i
</p>
<p>)
&minus;
(
</p>
<p>uβ &minus;T sβ &minus;
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;in
β
i
</p>
<p>)]
A∆z
</p>
<p>= γ&infin;,1A+(P
β &minus;Pα)A∆z = γ&infin;,1A , (6.46)
</p>
<p>where we used Pβ = Pα for the flat interface.
</p>
<p>6.7 W rev as a Measure of Stability
</p>
<p>As an application of thermodynamics of interfaces, let us compute the reversible
</p>
<p>work W rev required to create a spherical fragment of a new phase β within a
metastable phase α . Because our formalism applies only to systems in equilibrium,
we shall limit our consideration to a critical nucleus, which is in equilibrium with
</p>
<p>a given metastable phase.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7 W rev as a Measure of Stability 245
</p>
<p>As we shall see, W rev &gt; 0, implying that the equilibrium is unstable. The system
containing a critical nucleus can lower its free energy not only by shrinking the
</p>
<p>nucleus but also by growing it further until a macroscopic portion of a new phase
</p>
<p>forms. In this sense, formation of a critical nucleus, called nucleation, marks the
</p>
<p>first successful stage of new phase formation, thus the word &ldquo;critical.&rdquo;
</p>
<p>Nucleation is ubiquitous in nature and plays an important role in atmospheric
</p>
<p>science, biological processes, and chemical and pharmaceutical manufacturing. See
</p>
<p>Refs. [16, 19] for recent reviews.
</p>
<p>According to (2.180), nucleation and the subsequent transition to a new phase
</p>
<p>become more probable with decreasing W rev. In this sense, W rev serves as a mea-
</p>
<p>sure of stability of the metastable phase. More quantitatively, the nucleation rate,
</p>
<p>defined as the rate of critical nucleus formation per unit volume of the metastable
</p>
<p>phase, is proportional to e&minus;βW
rev
</p>
<p>. This follows from (2.180) and additional consid-
</p>
<p>erations regarding the kinetics of the process. The details can be found in Ref. [15],
</p>
<p>for example.
</p>
<p>We emphasize that the results of this section apply only to a critical nucleus,
</p>
<p>that is, to a single value of R determined by a specific rule for locating the divid-
</p>
<p>ing surface for a given intensive state of the metastable phase. A generalization
</p>
<p>of our results to noncritical nuclei is possible. An interested reader should consult
</p>
<p>Refs. [1, 2, 3, 8, 13].
</p>
<p>6.7.1 Exact Expression
</p>
<p>Suppose that a supersaturated multicomponent macroscopic phase α is contained
in an adiabatic, rigid, and impermeable wall. We take imaginary boundary B in this
</p>
<p>isolated system and refer to the region inside B as system I. The remaining part of
</p>
<p>the isolated system will be referred to as system II.
</p>
<p>We suppose that a nucleus forms in system I. If B is taken to enclose a sufficiently
</p>
<p>large region compared to the physical extent of the nucleus, B can be made to pass
</p>
<p>through a homogeneous region both before and after the formation of the nucleus.37
</p>
<p>Let [U ] denote the increment of the internal energy of system I upon nucleation.
The corresponding quantity in system II is given by
</p>
<p>&minus;T [S]&minus;&sum;
i
</p>
<p>&micro;i[Ni] (6.47)
</p>
<p>with [S] and [Ni] defined similarly to [U ]. Because the system as a whole is isolated,
the net increase in its internal energy must be due to the reversible work source.
</p>
<p>That is,
</p>
<p>W rev = [U ]&minus;T [S]&minus;&sum;
i
</p>
<p>&micro;i[Ni] . (6.48)
</p>
<p>Initially, system I is filled with uniform bulk phase α . Thus, the number of
molecules of species i in system I is
</p>
<p>nαi (V
α +V β ) , (6.49)</p>
<p/>
</div>
<div class="page"><p/>
<p>246 6 Thermodynamics of Interfaces
</p>
<p>where Vα +V β is the volume of system I expressed in terms of those of the bulk
reference phases. The number of molecules in the final state may be expressed as
</p>
<p>nαi V
α +n
</p>
<p>β
i V
</p>
<p>β +ΓiA . (6.50)
</p>
<p>Thus,
</p>
<p>[Ni] = n
α
i V
</p>
<p>α +n
β
i V
</p>
<p>β +ΓiA&minus;nαi (Vα +V β ) = (n
β
i &minus;nαi )V β +ΓiA . (6.51)
</p>
<p>Using (6.51) and other similar expressions in (6.48), we find
</p>
<p>W rev = (uβ &minus;uα)V β +usA&minus;T [(sβ &minus; sα)V β + ssA]&minus;
c
</p>
<p>&sum;
i=1
</p>
<p>&micro;i[(n
β
i &minus;nαi )V β +ΓiA]
</p>
<p>= &minus;V β (Pβ &minus;Pα)+ γA , (6.52)
</p>
<p>where we used (2.148) and (6.30).
</p>
<p>The term γA is referred to as the surface term and is positive since γ is positive,
while &minus;V β (Pβ &minus;Pα) is called the bulk term and is negative because Pβ &gt; Pα as
seen from (6.28). The balance of these two terms leads to a nonnegative value of
</p>
<p>W rev. In fact, using (6.28) and noting that the nucleus by our assumption is spherical,
</p>
<p>we can rewrite (6.52) as
</p>
<p>W rev =
1
</p>
<p>3
γA =
</p>
<p>1
</p>
<p>2
(Pβ &minus;Pα)V β = 16πγ
</p>
<p>3
</p>
<p>3(Pβ &minus;Pα)2 , (6.53)
</p>
<p>in which each expression is manifestly nonnegative. Thus, nucleation is an unfavor-
</p>
<p>able event. This is the origin of the metastability of phase α .
</p>
<p>Exercise 6.2. Because (6.30) holds for an arbitrary dividing surface, so does (6.52).
</p>
<p>But, W rev has a physical significance, and its value cannot depend on the convention
</p>
<p>we adopt for the dividing surface. Based on this observation, derive (6.26). (In con-
</p>
<p>trast, (6.53) is a result of combining (6.52) and (6.28), and holds only for the surface
</p>
<p>of tension.) ///
</p>
<p>Because every point in the macroscopic metastable phase is equivalent to any
</p>
<p>other, a critical nucleus can form anywhere in the system. As pointed out by Lothe
</p>
<p>and Pound as early as in 1962 [10], it is very unreasonable to expect that these
</p>
<p>&ldquo;translational (and rotational) degrees of freedom&rdquo; are properly reflected in the bulk
</p>
<p>thermodynamic quantities, such as the pressure and the chemical potentials. The
</p>
<p>same concern can be raised against γ . This observation casts a shadow of doubts on
the validity of (6.52), which appears otherwise exact. For a detailed discussion on
</p>
<p>this point, see Ref. [7] and references therein.
</p>
<p>According to the formalism we developed, we compute W rev as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7 W rev as a Measure of Stability 247
</p>
<p>a. For a given intensive state of a metastable phase, as specified by T , Pα , and
</p>
<p>xα1 , . . . ,x
α
c&minus;1, solve
</p>
<p>&micro;αi (T,P
α ,xα1 , . . . ,xc&minus;1
</p>
<p>α) = &micro;βi (T,P
β ,x
</p>
<p>β
1 , . . . ,xc&minus;1
</p>
<p>β ) , i = 1, . . . ,c . (6.54)
</p>
<p>for Pβ and x
β
1 , . . . ,x
</p>
<p>β
c&minus;1.
</p>
<p>b. Solve (6.28) for R, the radius of the surface of tension.
</p>
<p>c. Compute W rev by (6.52) or (6.53).
</p>
<p>Step a requires only the equations of state of the bulk phases. According to (6.41),
</p>
<p>γ = γ(T,&micro;1, . . . ,&micro;c) . (6.55)
</p>
<p>To execute step b, therefore, the explicit form of this function must be known. This is
</p>
<p>often, if not always, a very difficult requirement to meet, and we are forced to intro-
</p>
<p>duce an approximation for γ . In the next subsection, we present one such approxi-
mation scheme.
</p>
<p>6.7.2 &Dagger;Classical Theory Approximations
</p>
<p>In a very popular approximation scheme known as classical theory approxima-
</p>
<p>tion, we simply replace γ by experimentally measurable γ&infin;, the value of the surface
tension for a flat interface at saturation, and obtain
</p>
<p>W rev &asymp;&minus;V β (Pβ &minus;Pα)+ γ&infin;A =
16πγ&infin;3
</p>
<p>3(Pβ &minus;Pα)2 . (6.56)
</p>
<p>If the nucleating phase is incompressible between Pα and Pβ , the bulk term
</p>
<p>&minus;V β (Pβ &minus;Pα) can be expressed in terms of the chemical potentials. To see this,
let us apply the Gibbs&ndash;Duhem relation (2.155) to a constant temperature process:
</p>
<p>V βdPβ =
c
</p>
<p>&sum;
i=1
</p>
<p>N
β
i d&micro;
</p>
<p>β
i , T const. (6.57)
</p>
<p>Here, N
β
i = n
</p>
<p>β
i V
</p>
<p>β is the number of molecules of species i in V β taken inside a
</p>
<p>macroscopic reference phase β . Integrating (6.57) from Pα to Pβ without changing
</p>
<p>T or N
β
1 , . . . ,N
</p>
<p>β
c , we find
</p>
<p>V β (Pβ &minus;Pα) =
c
</p>
<p>&sum;
i=1
</p>
<p>N
β
i ∆&micro;i , (6.58)
</p>
<p>where
</p>
<p>∆&micro;i := &micro;
β
i (T,P
</p>
<p>β ,x
β
1 , . . . ,x
</p>
<p>β
c&minus;1)&minus;&micro;
</p>
<p>β
i (T,P
</p>
<p>α ,x
β
1 , . . . ,x
</p>
<p>β
c&minus;1)
</p>
<p>= &micro;αi (T,P
α ,xα1 , . . . ,x
</p>
<p>α
c&minus;1)&minus;&micro;
</p>
<p>β
i (T,P
</p>
<p>α ,x
β
1 , . . . ,x
</p>
<p>β
c&minus;1) . (6.59)</p>
<p/>
</div>
<div class="page"><p/>
<p>248 6 Thermodynamics of Interfaces
</p>
<p>In the last equality, we used (6.54). Introducing (6.58) into (6.56), we arrive at
</p>
<p>W rev &asymp;&minus;
c
</p>
<p>&sum;
i=1
</p>
<p>N
β
i ∆&micro;i + γ&infin;A =
</p>
<p>16πγ&infin;3
</p>
<p>3(&sum;ci=1 n
β
i ∆&micro;i)
</p>
<p>2
. (6.60)
</p>
<p>If Pβ ≫ Pα , then,
</p>
<p>Pβ &minus;Pα = (Pβ &minus;Psat)&minus; (Pα &minus;Psat)&asymp; Pβ &minus;Psat , (6.61)
</p>
<p>where Psat is the pressure at saturation. Under this approximation, (6.59) gives
</p>
<p>∆&micro;i &asymp; &micro;βi (T,Pβ ,x
β
1 , . . . ,x
</p>
<p>β
c&minus;1)&minus;&micro;
</p>
<p>β
i (T,Psat,x
</p>
<p>β
1 , . . . ,x
</p>
<p>β
c&minus;1)
</p>
<p>= &micro;αi (T,P
α ,xα1 , . . . ,x
</p>
<p>α
c&minus;1)&minus;&micro;αi (T,Psat,xα1,sat, . . . ,xαc&minus;1,sat) , (6.62)
</p>
<p>where we used
</p>
<p>&micro;βi (T,Psat,x
β
1 , . . . ,x
</p>
<p>β
c&minus;1) = &micro;
</p>
<p>α
i (T,Psat,x
</p>
<p>α
1,sat, . . . ,x
</p>
<p>α
c&minus;1,sat) , i = 1, . . . ,c . (6.63)
</p>
<p>at the two-phase coexistence. To apply (6.62) for a given intensive state of the
</p>
<p>metastable phase α , we first find the intensive state of bulk β phase by solving
(6.54). Then, we solve (6.63) to find Psat and x1,sat, . . . ,xc,sat.
</p>
<p>If bulk phase α may be regarded as an ideal gas mixture, (2.201) applies:
</p>
<p>∆&micro;i = kBT ln
xαi P
</p>
<p>α
</p>
<p>xαi,satPsat
, i = 1, . . . ,c . (6.64)
</p>
<p>We replaced the gas constant R in (2.201) by kB since &micro;i in this chapter has the
dimension of energy per molecule. For a single component system, (6.64) reduces
</p>
<p>to
</p>
<p>∆&micro; = kBT ln
Pα
</p>
<p>Psat
, (6.65)
</p>
<p>in which Pα/Psat is commonly called the supersaturation ratio. By means of
(6.65), (6.60) becomes
</p>
<p>W rev &asymp;&minus;Nβ kBT ln
Pα
</p>
<p>Psat
+ γ&infin;A . (6.66)
</p>
<p>6.7.3 &dagger;Thermodynamic Degrees of Freedom
</p>
<p>In step a of Sect. 6.7.1, we took it for granted that (6.54) has a nontrivial solution
</p>
<p>(Pβ 	= Pα ) for given T , xα1 , . . . ,xαc&minus;1, and for any Pα , which we choose between Psat
and the pressure at the onset of instability. According to the Gibbs phase rule we saw
</p>
<p>in Sect. 2.12, however, the thermodynamic degrees of freedom should be just c for
</p>
<p>a c component system in two-phase coexistence. So, all degrees of freedom appear
</p>
<p>to be used up by T and xα1 , . . . ,x
α
c&minus;1. How is it then that we can specify P
</p>
<p>α also?</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7 W rev as a Measure of Stability 249
</p>
<p>We must remember that the Gibbs phase rule was derived on the basis of the
</p>
<p>conditions of equilibrium and the Gibbs&ndash;Duhem relations for macroscopic homo-
</p>
<p>geneous phases. Our attempt to provide an explicit account of the interfacial region
</p>
<p>brought about some modifications to this basic construct. Let us find out how this
</p>
<p>modification impacts the Gibbs phase rule.
</p>
<p>Consider a metastable phase α containing a critical nucleus. As we have seen,
thermodynamic behavior of this inhomogeneous system can be described in terms
</p>
<p>of a composite system consisting of the bulk reference phases and the sharp interface
</p>
<p>located at the surface of tension with the fundamental equation of the interface given
</p>
<p>by either (6.20) or (6.21).38
</p>
<p>At equilibrium, T and &micro;1, . . . ,&micro;c are uniform throughout the system. If the equi-
librium is to be maintained after some perturbation, they must remain so. Thus, dT
</p>
<p>and d&micro;1, . . . ,d&micro;c are subject to the Gibbs&ndash;Duhem relations for the bulk phases
</p>
<p>SαdT &minus;VαdPα +
c
</p>
<p>&sum;
i=1
</p>
<p>Nαi d&micro;i = 0 ,
</p>
<p>SβdT &minus;V βdPβ +
c
</p>
<p>&sum;
i=1
</p>
<p>N
β
i d&micro;i = 0 , (6.67)
</p>
<p>and also to the Gibbs adsorption equation (6.41). In addition, the Laplace equation
</p>
<p>(6.28), providing the condition of mechanical equilibrium, must hold both before
</p>
<p>and after the perturbation, thus leading to
</p>
<p>dPβ &minus;dPα = 2
R
</p>
<p>dγ&minus; 2γ
R2
</p>
<p>dR . (6.68)
</p>
<p>In total, therefore, we have four equations among c+ 5 infinitesimal quantities
dT , d&micro;1, . . . ,d&micro;c, dPα , dPβ , dγ and dR. Accordingly, the thermodynamic degrees of
freedom is c+1 as opposed to just c as might be expected on the basis of the usual
Gibbs phase rule.
</p>
<p>For given values of dT and d&micro;1, . . . ,d&micro;c, for example, the values of dPα , dPβ ,
and dγ are determined uniquely by (6.41) and (6.67). The equilibrium is maintained
by adjusting R according to (6.68). This additional degrees of freedom is absent if
</p>
<p>we limit ourselves to phase coexistence across a flat interface.
</p>
<p>6.7.4 Small Nucleus
</p>
<p>With increasing degree of supersaturation, the metastable phase eventually becomes
</p>
<p>unstable. At the onset of instability, we expect that W rev = 0. (Recall that W rev is
the measure of stability of the supersaturated phase.) Equation (6.53) implies that γ
and R must also vanish at the onset. This is because Pβ &minus;Pα 	= 0 for Pβ determined
by (6.54). Otherwise, we would simply have the phase coexistence across a flat
</p>
<p>interface. In addition, since &micro;αi and &micro;
β
i are finite, P
</p>
<p>β &minus;Pα should remain finite at
the onset as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>250 6 Thermodynamics of Interfaces
</p>
<p>We expect that γ , R, and W rev vary continuously with the degree of supersatu-
ration. Thus, when the degree of supersaturation is increased, a critical nucleus is
</p>
<p>expected to become smaller and gradually lose its homogeneous core.
</p>
<p>In classical theory approximation, the supersaturation dependence of γ is ignored.
Thus, its predictions are likely to worsen with increasing degree of supersaturation.
</p>
<p>How about the exact expression (6.53)? As we remarked in Sect. 6.1, the formal-
</p>
<p>ism we developed remains applicable even in such cases. Nevertheless, its practical
</p>
<p>utility is severely limited due to our inability to experimentally determine the fun-
</p>
<p>damental equation (6.55) in this regime.
</p>
<p>For example, in very small nuclei, such as a liquid water droplet consisting only
</p>
<p>of a few tens of molecules or less, Pβ may no longer be equal to the mechanical
</p>
<p>pressure at the center of the nucleus, and the identification of γ with the mechanical
tension becomes questionable at best. Simultaneously, the thickness of the inter-
</p>
<p>facial region becomes comparable with R itself because the majority of molecules
</p>
<p>are in the interfacial region. This will frustrate any attempt to locate the surface of
</p>
<p>tension precisely.
</p>
<p>Recall that the nucleation rate is proportional to e&minus;βW
rev
</p>
<p>and thus vanishes expo-
</p>
<p>nentially fast with increasing W rev. The implication is that nucleation is likely to
</p>
<p>occur under the condition where classical theory approximation is inadequate but
</p>
<p>the input needed for the exact theory is inaccessible. This is a fundamental challenge
</p>
<p>in thermodynamics of interfaces that can only be addressed by statistical mechan-
</p>
<p>ics. Before we turn to this subject in Chap. 7, we shall discuss an approach that
</p>
<p>lies between pure thermodynamics we have seen so far and full-fledged statistical
</p>
<p>mechanics in the following two optional sections. Two equations, (6.92) and (6.98),
</p>
<p>derived in Sect. 6.9 find their applications in Sect. 7.6.
</p>
<p>6.8 &dagger;Gibbs&ndash;Tolman&ndash;Koenig Equation
</p>
<p>The form of the function (6.55) cannot be determined either within the framework of
</p>
<p>thermodynamics itself or by experiments. Starting with the Gibbs adsorption equa-
</p>
<p>tion, however, we can derive an exact differential equation for this function. The
</p>
<p>differential equation, known as the Gibbs&ndash;Tolman&ndash;Koenig (GTK) equation, was
</p>
<p>first derived by Tolman for single component systems [18] and was almost imme-
</p>
<p>diately generalized to multicomponent systems by Koenig [5]. In this section, we
</p>
<p>derive the GTK equation for single component systems.
</p>
<p>Before we get started, we note that the GTK equation cannot be solved without a
</p>
<p>detailed knowledge regarding the molecular-level structure of the interfacial region.
</p>
<p>Nevertheless, the equation forms a basis for improving the classical approximation
</p>
<p>through the Tolman correction, which we discuss toward the end of this section.
</p>
<p>Writing down (6.41) and (6.67) for a single component system (c = 1) and recall-
ing (6.68), we see that the values of dPα , dPβ , dγ , and d&micro; are completely determined
once the values of dT and dR are given. Thus, we can use T and the curvature
</p>
<p>q := 1/R of the surface of tension as the independent variables instead of T and &micro; .</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8 &dagger;Gibbs&ndash;Tolman&ndash;Koenig Equation 251
</p>
<p>In fact, the dependence of γ on the degree of supersaturation is usually formulated
as its curvature dependence.
</p>
<p>For a constant T process, (6.41) reduces to
</p>
<p>dγ =&minus;Γ d&micro; , T const. (6.69)
</p>
<p>Dividing both sides by dq,
</p>
<p>(
&part;γ
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>=&minus;Γ
(
&part;&micro;
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>. (6.70)
</p>
<p>To rewrite the right-hand side, we recall the Gibbs&ndash;Duhem relation for bulk phase
</p>
<p>α:
dPα = nαd&micro; , T const. (6.71)
</p>
<p>and obtain (
&part;Pα
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>= nα
(
&part;&micro;
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>. (6.72)
</p>
<p>Similarly, (
&part;Pβ
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>= nβ
(
&part;&micro;
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>. (6.73)
</p>
<p>From (6.68), we also have
</p>
<p>dPβ &minus;dPα = 2γdq+2qdγ , (6.74)
</p>
<p>which holds for any process including the constant T process. Thus,
</p>
<p>(
&part;Pβ
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>&minus;
(
&part;Pα
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>= 2γ+2q
</p>
<p>(
&part;γ
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>. (6.75)
</p>
<p>Using (6.72) and (6.73) in (6.75),
</p>
<p>(
&part;&micro;
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>=
2
</p>
<p>nβ &minus;nα
[
γ+q
</p>
<p>(
&part;γ
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>]
. (6.76)
</p>
<p>Substituting this expression into (6.70) and solving the resulting equation for
</p>
<p>(&part;γ/&part;q)T , we find (
&part; lnγ
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>=&minus; 2Γ
nβ &minus;nα +2Γ q . (6.77)
</p>
<p>This is the desired differential equation. But, one commonly eliminates Γ by
introducing an auxiliary surface. For the case of single component systems, this is
</p>
<p>the equimolar dividing surface defined by
</p>
<p>Γe(T,&micro; ,Re) = 0 , (6.78)</p>
<p/>
</div>
<div class="page"><p/>
<p>252 6 Thermodynamics of Interfaces
</p>
<p>where the subscript e refers to the quantities pertaining to this dividing surface. The
</p>
<p>indicated dependence of Γe on T , &micro; , and Re follows from (6.37) and (6.38).
Corresponding to the two choices for the dividing surface, we now have two
</p>
<p>reference systems. Using the surface of tension, we can express the number N of
</p>
<p>molecules in the system as
</p>
<p>N =
1
</p>
<p>3
ωR3nβ +
</p>
<p>1
</p>
<p>3
ω(R0
</p>
<p>3 &minus;R3)nα +ωR2Γ . (6.79)
</p>
<p>The same quantity may be expressed for the equimolar dividing surface as
</p>
<p>N =
1
</p>
<p>3
ωRe
</p>
<p>3nβ +
1
</p>
<p>3
ω(R0
</p>
<p>3 &minus;Re3)nα . (6.80)
</p>
<p>Subtracting (6.80) from (6.79) and solving the resulting equation for Γ , we obtain
</p>
<p>Γ = (nβ &minus;nα)δ
(
</p>
<p>1+δq+
1
</p>
<p>3
δ 2q2
</p>
<p>)
, (6.81)
</p>
<p>where
</p>
<p>δ := Re &minus;R . (6.82)
This distance between the surface of tension and the auxiliary surface is known as
</p>
<p>the Tolman length. By means of (6.81), (6.77) finally becomes
</p>
<p>(
&part; lnγ
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>=&minus; 2δ
(
1+δq+ 1
</p>
<p>3
δ 2q2
</p>
<p>)
</p>
<p>1+2δq
(
1+δq+ 1
</p>
<p>3
δ 2q2
</p>
<p>) , (6.83)
</p>
<p>which is the GTK equation. Interestingly, the GTK equation retains this basic form
</p>
<p>when generalized to multicomponent systems. But the proper choice for the auxil-
</p>
<p>iary surface will have to be modified.
</p>
<p>The GTK equation can be solved only if we know δ as a function of q. As we
shall see in Sect. 6.9, this requires that the (position dependent) free energy density
</p>
<p>be known across the interface. This is where we must resort to statistical mechanics.
</p>
<p>Within the framework of pure thermodynamics, the GTK equation simply replaces
</p>
<p>the difficulty of measuring γ by that of measuring δ .
Nevertheless, recasting of the original problem in the language of GTK equation
</p>
<p>may suggest a different set of approximations that are inconceivable in its absence.
</p>
<p>The famous Tolman correction is one such example. At saturation, q = 0 and the
GTK equation yields
</p>
<p>(
&part;γ
</p>
<p>&part;q
</p>
<p>)
</p>
<p>T
</p>
<p>∣∣∣∣
q=0
</p>
<p>= &minus; 2γδ
(
1+δq+ 1
</p>
<p>3
δ 2q2
</p>
<p>)
</p>
<p>1+2δq
(
1+δq+ 1
</p>
<p>3
δ 2q2
</p>
<p>)
∣∣∣∣∣
q=0
</p>
<p>=&minus;2γ&infin;δ&infin; , (6.84)
</p>
<p>where δ&infin; is the Tolman length in the flat interface. In accordance with (6.82)
for a spherical nucleus, δ&infin; is positive if the surface of tension penetrates deeper
toward the nucleating phase (β ) than does the equimolar dividing surface. Integrat-</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8 &dagger;Gibbs&ndash;Tolman&ndash;Koenig Equation 253
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>0 5 10 15 20
</p>
<p>r
</p>
<p>Fig. 6.12 A numerical solution of the GTK equation under the assumption that δ = δ&infin; &gt; 0. GTK
Gibbs-Tolman-Koenig.
</p>
<p>ing (6.84), we have
</p>
<p>γ &asymp; γ&infin;&minus;2γ&infin;δ&infin;q &asymp;
γ&infin;
</p>
<p>1+2δ&infin;q
(6.85)
</p>
<p>to the first order of q. This result is known as the Tolman correction.
</p>
<p>If we assume that δ &equiv; δ&infin; 	= 0, (6.83) can be written as
(
&part; lnγ&lowast;
</p>
<p>&part;x
</p>
<p>)
</p>
<p>T
</p>
<p>=&minus; 2
(
1+ x+ 1
</p>
<p>3
x2
)
</p>
<p>1+2x
(
1+ x+ 1
</p>
<p>3
x2
) , (6.86)
</p>
<p>where γ&lowast; := γ/γ&infin; and x := δ&infin;q. For a positive value of δ&infin;, x varies from 0 (flat inter-
face) to +&infin; (onset of instability). Equation (6.86) can be integrated either numeri-
cally or analytically [17] over the entire range of x with the result shown in Fig. 6.12.
</p>
<p>For a negative value of δ&infin;, x varies from 0 to &minus;&infin;. But, the denominator on the right-
hand side of (6.86) becomes zero at x &asymp;&minus;1.794, leading to an unphysical behavior
of γ&lowast;. Thus, the assumption of constant δ is untenable if δ&infin; &lt; 0. According to
(6.82), however, if δ&infin; &gt; 0 for droplet formation, then δ&infin; &lt; 0 for bubble formation,
and vice versa.
</p>
<p>As we have remarked already, γ&infin; is often accessible experimentally. In contrast,
the prediction of δ&infin; must rely on a statistical mechanical approach. All we can say
here is that δ&infin; for the case of single component systems is not expected to exceed
a typical thickness of the interfacial region, say several atomic diameters. This is
</p>
<p>because both the surface of tension and the equimolar dividing surface are expected
</p>
<p>to reside within the interfacial region. For multicomponent systems, the same may
</p>
<p>not hold because the equimolar dividing surface is not in general the appropriate
</p>
<p>choice for the auxiliary surface. See Refs. [9, 14], for example.</p>
<p/>
</div>
<div class="page"><p/>
<p>254 6 Thermodynamics of Interfaces
</p>
<p>6.9 &dagger;Interfacial Properties
</p>
<p>As we shall see in Chap. 7, the grand potential of an inhomogeneous system may be
</p>
<p>written as the integral of the grand potential density χ(r) over its volume V :
</p>
<p>Ω =
&int;
</p>
<p>V
χ(r)dr . (6.87)
</p>
<p>In this section, we shall develop expressions for γ&infin; and δ&infin; assuming that the func-
tion χ(r) is known. The method for finding χ(r) will be developed in Chap. 7.
</p>
<p>Let us first recall (6.48), which is an exact expression for the reversible work of
</p>
<p>formation of a spherical critical nucleus inside imaginary boundary B. In Sect. 6.7.1,
</p>
<p>we obtained (6.48) as the increment upon the nucleus formation of the internal
</p>
<p>energy of the isolated system that contains B. But, the same expression can also
</p>
<p>be regarded as the increment of the grand potential inside B. Thus, using a spherical
</p>
<p>coordinate system whose origin coincides with the center of the nucleus, we have
</p>
<p>W rev = 4π
&int; &infin;
</p>
<p>0
[χ(r)+Pα ]r2dr , (6.88)
</p>
<p>where we recognize &minus;Pα as the grand potential density of the homogeneous α
phase. The upper limit &infin; of the integral simply indicates that the integration extends
sufficiently deep into the α phase, where the integrand vanishes. This is an accept-
able convention provided that the integrand approaches zero sufficiently fast to
</p>
<p>ensure the convergence of the integral. In what follows, we shall assume this to
</p>
<p>be the case for all integrals involving &plusmn;&infin; in their limits. (If this is not allowed, the
effect of container walls must be accounted for explicitly.)
</p>
<p>We can separate the integral at the dividing surface at R and rewrite (6.88) as
</p>
<p>W rev = 4π
&int; R
</p>
<p>0
[χ(r)+Pβ ]r2dr+4π
</p>
<p>&int; &infin;
</p>
<p>R
[χ(r)+Pα ]r2dr&minus; 4π
</p>
<p>3
R3(Pβ &minus;Pα) , (6.89)
</p>
<p>which may be compared with (6.52) to yield
</p>
<p>γ =
1
</p>
<p>R2
</p>
<p>{&int; R
0
[χ(r)+Pβ ]r2dr+
</p>
<p>&int; &infin;
</p>
<p>R
[χ(r)+Pα ]r2dr
</p>
<p>}
. (6.90)
</p>
<p>Exercise 6.3. As we saw in Exercise 6.2, (6.52) holds for an arbitrary dividing sur-
</p>
<p>face. The same applies for (6.89) and (6.90). Based on this observation, derive (6.26)
</p>
<p>from (6.90). ///
</p>
<p>The expression for γ&infin; should emerge from (6.90) as its R &rarr; &infin; limit is taken
without changing the convention for the dividing surface. To evaluate this limit, let
</p>
<p>us rewrite (6.90) using a new variable ξ := r&minus;R:
</p>
<p>γ =
&int; 0
</p>
<p>&minus;R
[χ(ξ +R)+Pβ ]
</p>
<p>(
1+
</p>
<p>ξ
</p>
<p>R
</p>
<p>)2
dξ +
</p>
<p>&int; &infin;
</p>
<p>0
[χ(ξ +R)+Pα ]
</p>
<p>(
1+
</p>
<p>ξ
</p>
<p>R
</p>
<p>)2
dξ .
</p>
<p>(6.91)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.9 &dagger;Interfacial Properties 255
</p>
<p>For a sufficiently large R, the nucleus has a homogeneous core, implying that χ+Pβ
</p>
<p>vanishes rapidly once |ξ | exceeds a few times the width of the interfacial thickness.
The same applies to χ +Pα . In the R &rarr; &infin; limit, therefore, 1+ ξ/R may safely be
replaced by unity without worrying about its large |ξ | behavior. Since Pβ &rarr; Pα in
this limit, (6.91) becomes the integral of χ(ξ )+Pα over the interval of ξ that fully
contains the interfacial region. Thus, writing χ(ξ ) for χ(ξ +R) so that χ(0) gives
the grand potential density at r = R, we have
</p>
<p>γ&infin; =
&int; &infin;
</p>
<p>&minus;&infin;
[χ(ξ )+Pα ]dξ , (6.92)
</p>
<p>which is manifestly independent of the choice of the dividing surface. (See Sect. 6.6.)
</p>
<p>At the two-phase coexistence, the two bulk reference phases have the same grand
</p>
<p>potential density &minus;Pα . According to (6.92), γ&infin; is the superficial density of the grand
potential. This is in agreement with the physical interpretation given to γ in Sect. 6.4.
</p>
<p>Using (6.91) in the third equation of (6.38), in which the partial derivative is
</p>
<p>taken while holding the intensive state of the system constant, we find
</p>
<p>ca = &minus;
2
</p>
<p>R2
</p>
<p>&int; 0
</p>
<p>&minus;R
[χ(ξ +R)+Pβ ]ξ
</p>
<p>(
1+
</p>
<p>ξ
</p>
<p>R
</p>
<p>)
dξ +
</p>
<p>&int; &infin;
</p>
<p>&minus;R
</p>
<p>dχ(ξ +R)
</p>
<p>dR
</p>
<p>(
1+
</p>
<p>ξ
</p>
<p>R
</p>
<p>)2
dξ .
</p>
<p>&minus; 2
R2
</p>
<p>&int; &infin;
</p>
<p>0
[χ(ξ +R)+Pα ]ξ
</p>
<p>(
1+
</p>
<p>ξ
</p>
<p>R
</p>
<p>)
dξ (6.93)
</p>
<p>The lower limit &minus;R of the first integral in (6.91) does not contribute to ca because
the integrand is zero at ξ = &minus;R. (For a sufficiently large R, we may first replace
the lower limit by some constant ξc (&minus;R &lt; ξc ≪ 0) without affecting the value of
the integral because the integrand will be zero if ξ &le; ξc. Then, we can take the
derivative to obtain (6.93).)
</p>
<p>The second integral of (6.93) is zero. To see this, we first rewrite it as
</p>
<p>lim
∆R&rarr;0
</p>
<p>1
</p>
<p>∆R
</p>
<p>&int; &infin;
</p>
<p>0
[χ(r+∆R)&minus; χ(r)]
</p>
<p>( r
R
</p>
<p>)2
dr , (6.94)
</p>
<p>where we used the definition of the derivative:
</p>
<p>dχ(ξ +R)
</p>
<p>dR
= lim
</p>
<p>∆R&rarr;0
χ(ξ +R+∆R)&minus; χ(ξ +R)
</p>
<p>∆R
(6.95)
</p>
<p>and reverted to the original variable r = ξ +R. But, the quantity χ(r+∆R)&minus; χ(r)
represents the change in χ(r) that is observed at r when the function χ(r) is shifted
by ∆R in the direction of decreasing r. This same change may also be regarded as
being brought about by an infinitesimal variation in the state of the system. But,
</p>
<p>because the system is in equilibrium initially, the grand potential of the system
</p>
<p>remains unaffected to the first order of such variations. (See Sect. 7.2.1 for details.)
</p>
<p>It follows that the integral in (6.94) is at most second order of ∆R. This proves the
assertion.</p>
<p/>
</div>
<div class="page"><p/>
<p>256 6 Thermodynamics of Interfaces
</p>
<p>Because of the assumption we made earlier, the remaining integrals in (6.93)
</p>
<p>converge and ca vanishes in the R &rarr; &infin; limit as demanded by (6.43). To find the
location of the surface of tension, we recall that C = 4πR2ca and use (6.27), which
now reads
</p>
<p>&int; 0
</p>
<p>&minus;R
[χ(ξ +R)+Pβ ]ξ
</p>
<p>(
1+
</p>
<p>ξ
</p>
<p>R
</p>
<p>)
dξ +
</p>
<p>&int; &infin;
</p>
<p>0
[χ(ξ +R)+Pα ]ξ
</p>
<p>(
1+
</p>
<p>ξ
</p>
<p>R
</p>
<p>)
dξ = 0 .
</p>
<p>(6.96)
</p>
<p>Provided that χ(r) is known, this equation can be solved (numerically in most cases)
for R, which is then the radius of the surface of tension. The R &rarr; &infin; limit of this
equation can be evaluated by repeating the same argument we have given to (6.91).
</p>
<p>In this way, we obtain &int; &infin;
</p>
<p>&minus;&infin;
[χ(ξ )+Pα ]ξdξ = 0 (6.97)
</p>
<p>for a flat interface. We replace the relation ξ = r&minus;R by ξ = z&minus; zs, where the z-axis
is perpendicular to the interface and zs is the location of the surface of tension. Using
</p>
<p>(6.92), we arrive at
</p>
<p>zs =
1
</p>
<p>γ&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
[χ(z)+Pα ]zdz . (6.98)
</p>
<p>Once again, we note that the integrand is zero as we move sufficiently away from
</p>
<p>the interfacial region.
</p>
<p>The location ze of the equimolar dividing surface is determined by
</p>
<p>&int; ze
</p>
<p>&minus;&infin;
[n(z)&minus;nβ ]dz+
</p>
<p>&int; &infin;
</p>
<p>ze
</p>
<p>[n(z)&minus;nα ]dz = 0 , (6.99)
</p>
<p>where n(z) is the density profile, that is, the z-dependent number density of mole-
cules across the system. Then, the Tolman length for the flat interface is given by
</p>
<p>δ&infin; = ze &minus; zs.
Finally, it should be noted that the above elaboration is entirely unnecessary for
</p>
<p>spherical critical nuclei. Once χ(r) is known, we can simply compute W rev using
(6.88). Then, (6.28) and (6.53) serve as the set of simultaneous equations for γ and
R. However, such an approach cannot be applied directly to flat interfaces.
</p>
<p>6.10 Frequently Used Symbols
</p>
<p>[θ ] , increment of an extensive quantity θ during a process under consideration.
θ s , surface excess quantity of an extensive variable θ .
</p>
<p>c , the number of species.
</p>
<p>ca , C/A.
kB , Boltzmann constant, 1.3806&times;10&minus;23 J/K.
ni , the number density of species i.</p>
<p/>
</div>
<div class="page"><p/>
<p>References and Further Reading 257
</p>
<p>q , curvature 1/R of the surface of tension.
ss , Ss/A.
us , U s/A.
xi , mole fraction of species i.
</p>
<p>A , area of a dividing surface.
</p>
<p>C , (&part;U s/&part;R)Ss,Ns,A.
Ni , the number of molecules of species i. We drop the subscript i for a pure system.
</p>
<p>P , pressure.
</p>
<p>R , radius of the dividing surface.
</p>
<p>R0 , radius of the spherical region containing an inhomogeneous region.
</p>
<p>Re , radius of the equimolar dividing surface.
</p>
<p>S , entropy.
</p>
<p>T , absolute temperature.
</p>
<p>U , internal energy.
</p>
<p>V , volume.
</p>
<p>W rev , reversible work of critical nucleus formation.
</p>
<p>α , label for the bulk metastable phase.
β , label for the bulk nucleating phase.
δ , Tolman length.
δ&infin; , Tolman length of a flat interface.
γ , (&part;U s/&part;A)Ss,Ns,R for a generic dividing surface and surface tension if the
dividing surface is the surface of tension.
</p>
<p>γ&infin; , surface tension of a flat interface.
&micro;i , chemical potential of species i. We drop the subscript i for a pure system.
χ , grand potential per unit volume.
ω , solid angle.
</p>
<p>Γi , Nsi /A.
Ω , grand potential.
</p>
<p>References and Further Reading
</p>
<p>1. Buff F P (1951) The spherical interface. I. thermodynamics. J. Chem. Phys. 19:1951&ndash;1954
</p>
<p>2. Debenedetti P G, Reiss H (1998) Reversible work of formation of an embryo of a new phase
</p>
<p>within a uniform macroscopic mother phase. J. Chem. Phys. 108:5498&ndash;5505
</p>
<p>3. Debenedetti P G, Reiss H (1999) Response to &ldquo;Comment on &lsquo;Reversible work of formation
</p>
<p>of an embryo of a new phase within a uniform macroscopic mother phase&rdquo;&rsquo; [J. Chem. Phys.
</p>
<p>111, 3769 (1999)]. J. Chem. Phys. 111:3771&ndash;3772
</p>
<p>4. Gibbs J W (1993) The Scientific Papers of J. Willard Gibbs, Volume I. Thermodynamics. Ox
</p>
<p>Bow, Woodbridge, Connecticut</p>
<p/>
</div>
<div class="page"><p/>
<p>258 6 Thermodynamics of Interfaces
</p>
<p>Thermodynamics of interfaces is developed in pp. 219&ndash;331. Materials covered in this Chapter
</p>
<p>are given in pp. 219&ndash;237 and pp. 252&ndash;258 of the book. Turn to pp. 226&ndash;227 for an explicit
</p>
<p>demonstration that the surface of tension is located within the transition region.
</p>
<p>5. Koenig F O (1950) On the Thermodynamic relation between surface tension and curvature. J.
</p>
<p>Chem. Phys. 18:449&ndash;459
</p>
<p>6. Kondo S (1956) Thermodynamical fundamental equation for spherical interface. J. Chem.
</p>
<p>Phys. 25:662&ndash;669
</p>
<p>7. Kusaka I (2006) Statistical mechanics of nucleation: Incorporating translational and rotational
</p>
<p>free energy into thermodynamics of a microdroplet. Phys. Rev. E. 73:031607
</p>
<p>8. Kusaka I, Oxtoby D W (1999) Comment on &ldquo;Reversible work of formation of an embryo of a
</p>
<p>new phase within a uniform macroscopic mother phase&rdquo; [J. Chem. Phys. 108, 5498(1998)]. J.
</p>
<p>Chem. Phys. 111:3769&ndash;3770
</p>
<p>9. Kusaka I, Talreja M, Tomasko D L (2013) Beyond classical theory: Predicting free energy
</p>
<p>barrier of bubble nucleation in polymer-gas mixture. AIChE J. 59:3042&ndash;3053
</p>
<p>10. Lothe J, Pound G M (1962) Reconsiderations of Nucleation Theory. J. Chem. Phys. 36:2080&ndash;
</p>
<p>2085
</p>
<p>11. Nishioka K (1987) An analysis of the Gibbs theory of infinitesimally discontinuous variation
</p>
<p>in thermodynamics of interface. Scripta Metallurgica 21:789&ndash;792
</p>
<p>12. Nishioka K (1987) Thermodynamic formalism for a liquid microcluster in vapor. Phys. Rev.
</p>
<p>A 36:4845&ndash;4851
</p>
<p>13. Nishioka K, Kusaka I (1992) Thermodynamic formula for the reversible work of forming a
</p>
<p>noncritical cluster from the vapor in multicomponent systems. J. Phys. Chem. 97:6687&ndash;6689
</p>
<p>14. Nishioka K, Tomino H, Kusaka I, Takai T (1989) Curvature dependence of the interfacial
</p>
<p>tension in binary nucleation. Phys. Rev. A 39:772&ndash;782
</p>
<p>15. Oxtoby D W (1992) Homogeneous nucleation: Theory and experiment. J. Phys. Condens.
</p>
<p>Matter 4:7627&ndash;7650
</p>
<p>16. Oxtoby D W (1998) Nucleation of first-order phase transitions. Acc. Chem. Res. 31:91&ndash;97
</p>
<p>17. Rekhviashvili S Sh, Kishtikova E V (2011) On the size dependence of the surface tension.
</p>
<p>Tech. Phys. 56: 143&ndash;146
</p>
<p>18. Tolman R C (1948) Consideration of the Gibbs theory of surface tension. J. Chem. Phys.
</p>
<p>16:758&ndash;774
</p>
<p>19. Xu X, Ting C L, Kusaka I, Wang Z-G (2014) Nucleation in polymers and soft matter. Annu.
</p>
<p>Rev. Phys. Chem. 65:449&ndash;475</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>While thermodynamics of interfaces provides a theoretical foundation for under-
</p>
<p>standing various interfacial phenomena, its application depends on the availability
</p>
<p>of the fundamental equation. That is, the explicit form of the function (6.55) must be
</p>
<p>known. In the absence of experimental access to this information, this is a task best
</p>
<p>left to statistical mechanics. In this chapter, we introduce a powerful method from
</p>
<p>statistical mechanics that allows us to study interfaces and inhomogeneous systems
</p>
<p>in general based on underlying molecular-level models.
</p>
<p>7.1 Functional
</p>
<p>The concept of functionals and their derivatives play a central role in what follows.
</p>
<p>In this section, we introduce the functionals as a generalization of ordinary functions
</p>
<p>and explore the rules of computing with the functionals.
</p>
<p>7.1.1 Definition
</p>
<p>Given a function f (x), we often represent it graphically using the horizontal axis
for the independent variable x and the vertical axis for the dependent variable f .
</p>
<p>For a function f (x,y) of two variables, the graph is a surface in a three-dimensional
space. For a function of three or more variables, such a graphical representation is
</p>
<p>not possible.
</p>
<p>We can devise a far less ambitious approach, in which we use two diagrams, one
</p>
<p>showing only the values assumed by the independent variables and the other show-
</p>
<p>ing the value of the function corresponding to the given values of the independent
</p>
<p>variables.
</p>
<p>As an example, let us consider a function f (u1,u2,u3). A conventional graphical
approach would require a four-dimensional space to plot it. Instead, we show the
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 259
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1 7</p>
<p/>
</div>
<div class="page"><p/>
<p>260 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>9
</p>
<p>5
</p>
<p>14
</p>
<p>ui
</p>
<p>0 i1 2 3
</p>
<p>a b
</p>
<p>f
</p>
<p>f 9 5 14 f0
</p>
<p>Fig. 7.1 The function f (u1,u2,u3) maps the triplet (9,5,14) on a to a real number
f (9,5,14) on b.
</p>
<p>values of u1, u2, and u3 in one diagram as shown in Fig. 7.1a. The value of f corre-
</p>
<p>sponding to those specific u1, u2, and u3 is then shown in another diagram, Fig. 7.1b.
</p>
<p>We can explore the behavior of the function by changing the values of u1, u2, and
</p>
<p>u3, which amounts to vertical movements of the open circles in Fig. 7.1a, and then
</p>
<p>observing the corresponding horizontal movement of the open circle in Fig. 7.1b.
</p>
<p>Admittedly, this new approach does not convey as much information as the con-
</p>
<p>ventional one. But, it has a virtue of being applicable regardless of the number of
</p>
<p>independent variables. Thus, a function f (u1, . . . ,un) of n variables can be expressed
in the same manner even if n = 1000. In this case, we simply have 1000 open cir-
cles in Fig. 7.1a indicating the values of all these independent variables. Figure 7.1b
</p>
<p>will still contain just one open circle showing the value of f corresponding to those
</p>
<p>specific values of the independent variables.
</p>
<p>Coming back to the case of just three independent variables, we note that nothing
</p>
<p>in our approach limits the index i on ui to an integer. We could easily introduce new
</p>
<p>variables, such as u1.2, u&radic;2, or u7/3, and generate a new function
</p>
<p>f (u1,u1.2,u&radic;2,u2,u7/3,u3) . (7.1)
</p>
<p>All we have to do is draw more circles at appropriate positions in Fig. 7.1a. Contin-
</p>
<p>uing in this manner, we can include more and more circles between i = 1 and i = 3
until they form a continuous curve on the iui-plane in this interval. What we have
</p>
<p>then is a rule f of assigning a number to a function u(i) defined for all real numbers
i in the interval 1 &le; i &le; 3. Such a rule of assignment is called a functional.
</p>
<p>More generally, a functional is a rule of assigning a number F to a function u(x)
defined on some interval, such as a &le; x &le; b.39 The functional dependence of F
on the function u(x) is indicated by a pair of square brackets. So, the functional is
denoted by F [u]. One particularly simple example of a functional is
</p>
<p>F 1[u] =
&int; b
</p>
<p>a
u(x)dx . (7.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Functional 261
</p>
<p>A slightly less trivial example is
</p>
<p>F 2[u] =
&int; b
</p>
<p>a
xeu(x)dx . (7.3)
</p>
<p>The action integral S we encountered in Chap. 1 affords yet another example of a
</p>
<p>functional:
</p>
<p>S [q] =
&int; t2
</p>
<p>t1
</p>
<p>L(q, q̇, t)dt , (7.4)
</p>
<p>in which q̇ = dq/dt is a known function of t once q(t) is specified. Perhaps a little
unexpectedly,
</p>
<p>F 3[u] = u(x0) , a &le; x0 &le; b , (7.5)
is also a functional. In fact, F 3 assigns a number u(x0) to the function u(x) defined
on the interval a &le; x &le; b. The point becomes clearer if we rewrite (7.5) as
</p>
<p>F 3[u] =
&int; b
</p>
<p>a
δ (x&minus; x0)u(x)dx , a &le; x0 &le; b (7.6)
</p>
<p>using the Dirac δ -function. (See Appendix D.) In what follows, we shall omit the
explicit reference to the limits of integration unless demanded by the situation.
</p>
<p>7.1.2 Functional Derivative
</p>
<p>Let us consider a functional defined by
</p>
<p>F [u] :=
&int;
</p>
<p>ln[u(x)+1]dx (7.7)
</p>
<p>and compute the difference
</p>
<p>F [u+δu]&minus;F [u] =
&int;
{ln[u(x)+δu(x)+1]&minus; ln[u(x)+1]}dx . (7.8)
</p>
<p>Using (B.6), we note that
</p>
<p>ln(x+a) = lnx+
a
</p>
<p>x
+h.o. (7.9)
</p>
<p>Replacing x by u(x)+1 and a by δu(x) in this formula, we obtain
</p>
<p>ln[u(x)+δu(x)+1]&minus; ln[u(x)+1] = 1
u(x)+1
</p>
<p>δu(x)+h.o. (7.10)
</p>
<p>So,
</p>
<p>F [u+δu]&minus;F [u] =
&int;
</p>
<p>1
</p>
<p>u(x)+1
δu(x)dx+h.o. (7.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>262 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>The coefficient of δu(x) in the integrand, 1/[u(x)+ 1], is called the first functional
derivative of F [u].
</p>
<p>More generally, we define the first functional derivative δF/δu(x) of F [u] by
</p>
<p>F [u+δu]&minus;F [u] =
&int;
</p>
<p>δF
</p>
<p>δu(x)
δu(x)dx+h.o. (7.12)
</p>
<p>We denote the first term on the right-hand side by δF :
</p>
<p>δF :=
&int;
</p>
<p>δF
</p>
<p>δu(x)
δu(x)dx . (7.13)
</p>
<p>These equations generalizes the corresponding relations for ordinary functions,
</p>
<p>f (u1 +δu1, . . . ,un +δun)&minus; f (u1, . . . ,un) =
n
</p>
<p>&sum;
i=1
</p>
<p>&part; f
</p>
<p>&part;ui
δui +h.o. (7.14)
</p>
<p>and
</p>
<p>d f =
n
</p>
<p>&sum;
i=1
</p>
<p>&part; f
</p>
<p>&part;ui
δui , (7.15)
</p>
<p>respectively. When the index i on the variable ui is replaced by x, which changes
</p>
<p>continuously, the summation with respect to i is replaced by the integration with
</p>
<p>respect to x.40
</p>
<p>Example 7.1. Functional derivative:
</p>
<p>a. For the functional F 1 in (7.2),
</p>
<p>F 1[u+δu]&minus;F 1[u] =
&int;
</p>
<p>δu(x)dx , (7.16)
</p>
<p>and hence
δF 1
δu(x)
</p>
<p>= 1 . (7.17)
</p>
<p>b. For the functional F 2 in (7.3),
</p>
<p>F 2[u+δu]&minus;F 2[u] =
&int;
</p>
<p>x[eu(x)+δu(x)&minus; eu(x)]dx =
&int;
</p>
<p>xeu(x)δu(x)dx+h.o.
</p>
<p>(7.18)
</p>
<p>Thus,
δF 2
δu(x)
</p>
<p>= xeu(x) . (7.19)
</p>
<p>c. Noting that (7.5) may be written as (7.6),
</p>
<p>F 3[u+δu]&minus;F 3[u] =
&int;
</p>
<p>δ (x&minus; x0)δu(x)dx . (7.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Functional 263
</p>
<p>So,
δF 3
δu(x)
</p>
<p>=
δu(x0)
</p>
<p>δu(x)
= δ (x&minus; x0) . (7.21)
</p>
<p>d. As a somewhat less trivial example, let
</p>
<p>F 4[u] =
&int; &int;
</p>
<p>φ(x1,x2)u(x1)u(x2)dx1dx2 (7.22)
</p>
<p>and suppose that the limits of integrations are the same for x1 and x2. Not-
</p>
<p>ing that
</p>
<p>[u(x1)+δu(x1)][u(x2)+δu(x2)]&minus;u(x1)u(x2)
= u(x1)δu(x2)+u(x2)δu(x1)+h.o. , (7.23)
</p>
<p>we find
</p>
<p>δF 4[u] =
&int; &int;
</p>
<p>φ(x1,x2)[u(x1)δu(x2)+u(x2)δu(x1)]dx1dx2 . (7.24)
</p>
<p>Since x1 and x2 are just integration variables, they can be replaced by any-
</p>
<p>thing we like. For example, we can use x2 for x1 and x1 for x2, respectively.
</p>
<p>Thus,
</p>
<p>&int; &int;
φ(x1,x2)u(x1)δu(x2)dx1dx2 =
</p>
<p>&int; &int;
φ(x2,x1)u(x2)δu(x1)dx1dx2 .
</p>
<p>(7.25)
</p>
<p>Because the limits of integrations are the same for x1 and x2, we can now
</p>
<p>combine the two terms in (7.24) to find
</p>
<p>δF 4[u] =
&int; {&int;
</p>
<p>[φ(x2,x1)+φ(x1,x2)]u(x2)dx2
</p>
<p>}
δu(x1)dx1 , (7.26)
</p>
<p>So,
δF 4
δu(x1)
</p>
<p>=
&int;
[φ(x2,x1)+φ(x1,x2)]u(x2)dx2 . (7.27)
</p>
<p>If φ(x1,x2) = φ(x2,x1), then,
</p>
<p>δF 4
δu(x1)
</p>
<p>= 2
&int;
</p>
<p>φ(x1,x2)u(x2)dx2 . (7.28)
</p>
<p>Exercise 7.1. Let
</p>
<p>F [u] =
&int;
</p>
<p>f (u(x),x)dx . (7.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>264 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>Show that
δF
</p>
<p>δu(x)
=
</p>
<p>&part;
</p>
<p>&part;u(x)
f (u(x),x) . (7.30)
</p>
<p>The functional derivatives of F 1, F 2, and F 3 we considered in Example 7.1 can
</p>
<p>be found using this formula. ///
</p>
<p>Exercise 7.2. Let
</p>
<p>F [u] =
&int;
</p>
<p>V
f (u(r),&nabla;u(r))dr . (7.31)
</p>
<p>Assuming that δu(r)&equiv; 0 on the surface of V , show that
</p>
<p>δF
</p>
<p>δu(r)
=
</p>
<p>&part; f
</p>
<p>&part;u
&minus;&nabla; &middot; &part; f
</p>
<p>&part;&nabla;u
. (7.32)
</p>
<p>Equations (7.31) and (7.32) should be compared with (1.59) and the integrand of
</p>
<p>(1.71), respectively. ///
</p>
<p>From Example 7.1, we see that δF/δu(x) is either a function of u(x) or a func-
tional of u(x). By (7.6), the former may also be regarded as a functional. Thus, the
first functional derivative is not only a function of x but also a functional of u(x). If
needed, we can express this fact by writing
</p>
<p>F
(1)(x,u] :=
</p>
<p>δF
</p>
<p>δu(x)
. (7.33)
</p>
<p>We can now define the second functional derivative δ 2F/δu(x1)δu(x2) of F by
</p>
<p>F
(1)(x1,u+δu]&minus;F (1)(x1,u] =
</p>
<p>&int;
δ 2F
</p>
<p>δu(x1)δu(x2)
dx2 +h.o . (7.34)
</p>
<p>Example 7.2. Second functional derivative: For F 1 and F 3 we saw in Exam-
</p>
<p>ple 7.1,
</p>
<p>δ 2F 1
δu(x1)δu(x2)
</p>
<p>= 0 and
δ 2F 3
</p>
<p>δu(x1)δu(x2)
= 0 . (7.35)
</p>
<p>Rewriting (7.19) as
</p>
<p>δF 2
δu(x1)
</p>
<p>=
&int;
</p>
<p>x2e
u(x2)δ (x2 &minus; x1)dx2 , (7.36)
</p>
<p>we find
δ 2F 2
</p>
<p>δu(x1)δu(x2)
= x2e
</p>
<p>u(x2)δ (x2 &minus; x1) . (7.37)
</p>
<p>Finally, (7.27) gives
</p>
<p>δ 2F 4
δu(x1)δu(x2)
</p>
<p>= φ(x2,x1)+φ(x1,x2) , (7.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Density Functional Theory 265
</p>
<p>which reduces to 2φ(x1,x2) if φ(x1,x2) = φ(x2,x1). We can reach the same
conclusion if we start from (7.28). So, everything is consistent.
</p>
<p>Recall the Taylor series expansion of a function f of multiple variables u1, . . . ,un:
</p>
<p>f (u1 +δu1, . . . ,un +δun)&minus; f (u1, . . . ,un)
</p>
<p>=
n
</p>
<p>&sum;
i=1
</p>
<p>&part; f
</p>
<p>&part;ui
δui +
</p>
<p>1
</p>
<p>2
</p>
<p>n
</p>
<p>&sum;
i=1
</p>
<p>n
</p>
<p>&sum;
j=1
</p>
<p>&part; 2 f
</p>
<p>&part;ui&part;u j
δuiδu j +h.o. (7.39)
</p>
<p>Corresponding to this formula is the functional Taylor series expansion given by
</p>
<p>F [u+δu]&minus;F [u] =
&int;
</p>
<p>δF
</p>
<p>δu(x1)
δu(x1)dx1
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int;
δ 2F
</p>
<p>δu(x1)δu(x2)
δu(x1)δu(x2)dx1dx2 +h.o. (7.40)
</p>
<p>Finally, if u = u(x1,x2), the first functional derivative of F [u] is defined by
</p>
<p>F [u+δu]&minus;F [u] =
&int; &int;
</p>
<p>δF
</p>
<p>δu(x1,x2)
δu(x1,x2)dx1dx2 +h.o. (7.41)
</p>
<p>For example, if
</p>
<p>F [u] =
&int; &int; &int;
</p>
<p>φ(x1,x2,x3)u(x1,x2)u(x2,x3)u(x3,x1)dx1dx2dx3 , (7.42)
</p>
<p>and φ is invariant with respect to any permutation of the arguments, that is,
φ(x1,x2,x3) = φ(x2,x1,x3) = &middot; &middot; &middot;, then,
</p>
<p>δF
</p>
<p>δu(x1,x2)
= 3
</p>
<p>&int; &int;
φ(x1,x2,x3)u(x2,x3)u(x3,x1)dx3 . (7.43)
</p>
<p>Exercise 7.3. Derive (7.43). ///
</p>
<p>7.2 Density Functional Theory
</p>
<p>For simplicity, let us focus on a single component system and consider an open
</p>
<p>system of volume V . The system may contain an interfacial region or may otherwise
</p>
<p>be inhomogeneous. As we saw in Sect. 6.3.2, the temperature T and the chemical
</p>
<p>potential &micro; will be uniform throughout the system in equilibrium.41 Their values
are imposed by the surroundings serving as a reservoir of heat and molecules. In
</p>
<p>contrast, the number density n of molecules may vary across the system. How is</p>
<p/>
</div>
<div class="page"><p/>
<p>266 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>n determined as a function of position r in a system in equilibrium? How do we
</p>
<p>construct a fundamental equation of the system?
</p>
<p>7.2.1 Equilibrium Density Profile
</p>
<p>Following an analysis similar to what we saw in Sect. 2.13.2, we can deduce the
</p>
<p>condition of equilibrium of an open system:
</p>
<p>Condition of Equilibrium 7 For equilibrium of an open system held at a
</p>
<p>given temperature T , volume V , and chemical potential &micro; , it is necessary and
sufficient that
</p>
<p>(δΩ)T,V,&micro; &ge; 0 (7.44)
holds for any possible variation of the state of the system.
</p>
<p>We recall that variations considered here are brought about through infinitesimal
</p>
<p>changes in the unconstrained variables X1, . . . ,Xm. Thus,
</p>
<p>(δΩ)T,V,&micro; =
m
</p>
<p>&sum;
i=1
</p>
<p>(
&part;Ω
</p>
<p>&part;Xi
</p>
<p>)
</p>
<p>T,V,&micro; ,X j 	=i
</p>
<p>δXi . (7.45)
</p>
<p>Equation (7.44) then determines the equilibrium values of X1, . . . ,Xm.
Instead of X1, . . . ,Xm, we now have a position-dependent function n(r). Its value
</p>
<p>at each r is an unconstrained variable. Since r is capable of continuous variation, the
</p>
<p>sum over i in (7.45) must be replaced by the integration with respect to r. Thus, the
</p>
<p>condition of equilibrium is that
</p>
<p>δΩ =
&int;
</p>
<p>V
</p>
<p>δΩ
</p>
<p>δn(r)
δn(r)dr&ge; 0 (7.46)
</p>
<p>holds for all possible values of δn(r), where the integration is over the system vol-
ume V and we omitted the subscripts T , V , and &micro; .
</p>
<p>Provided that n(r) is positive everywhere, δn(r) is capable of taking negative as
well as positive values. Then, (7.46) leads to
</p>
<p>δΩ
</p>
<p>δn(r)
= 0 . (7.47)
</p>
<p>Given an explicit expression for the functional Ω [n], called a density functional
or a free energy functional, (7.47) leads to an explicit equation for the equilib-
</p>
<p>rium density profile neq(r). Once neq(r) is determined, we can compute the free
energy Ω [neq] of the system, which is then a fundamental equation of the inhomo-
geneous system. This method of computing neq(r) and the grand potential is known
as the statistical mechanical or classical density functional theory (DFT), to be</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Density Functional Theory 267
</p>
<p>distinguished from the quantum mechanical DFT that aims to compute the electron
</p>
<p>density and the energy of quantum mechanical systems.
</p>
<p>7.2.2 Microscopic Definition of Density
</p>
<p>Before turning to the problem of finding the explicit form of Ω [n], we need to be a
little more specific about what is meant by the density profile n(r).
</p>
<p>As we saw in Sect. 3.1, a measurable quantity of our interest is not a dynamical
</p>
<p>variable itself, but its ensemble average. So, what is the dynamical variable whose
</p>
<p>ensemble average gives n(r)?
It is very natural to demand that, for an arbitrary region of volume R,
</p>
<p>&int;
</p>
<p>R
n(r)dr (7.48)
</p>
<p>should give the number of particles inside that region. The density operator defined
</p>
<p>by
</p>
<p>n̂(r,rN) :=
N
</p>
<p>&sum;
i=1
</p>
<p>δ (r&minus; ri) (7.49)
</p>
<p>satisfies this demand. In fact, &int;
</p>
<p>R
δ (r&minus; ri)dr (7.50)
</p>
<p>is unity if the ith particle is inside R and zero otherwise. (See Appendix D.4 on
</p>
<p>the three-dimensional δ -function.) So, the integral of n̂ over R gives the number of
particles in R. Taking the ensemble average, we identify
</p>
<p>n(r) = 〈n̂(r,rN)〉=
&lang;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>δ (r&minus; ri)
&rang;
</p>
<p>(7.51)
</p>
<p>as the density profile.
</p>
<p>Given this definition of the density profile, let us see how it can be related to a
</p>
<p>partition function. For this purpose, consider a system subject to an external field
</p>
<p>ψ(r). Its Hamiltonian is given by
</p>
<p>HN(r
N ,pN) =
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2m
</p>
<p>+φ(rN)+
N
</p>
<p>&sum;
i=1
</p>
<p>ψ(ri) . (7.52)
</p>
<p>The grand canonical partition function of this system is given by
</p>
<p>Ξ =
&infin;
</p>
<p>&sum;
N=0
</p>
<p>eβ&micro;N
</p>
<p>Λ 3NN!
</p>
<p>&int;
e&minus;βφ(r
</p>
<p>N)e&minus;β &sum;
N
i=1 ψ(ri)drN , (7.53)
</p>
<p>in which we carried out the integration with respect to the momenta.</p>
<p/>
</div>
<div class="page"><p/>
<p>268 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>In order to introduce n̂ into our formulation, we rewrite the external field term as
</p>
<p>follows:
N
</p>
<p>&sum;
i=1
</p>
<p>ψ(ri) =
N
</p>
<p>&sum;
i=1
</p>
<p>&int;
</p>
<p>V
δ (r&minus; ri)ψ(r)dr=
</p>
<p>&int;
</p>
<p>V
n̂(r,rN)ψ(r)dr . (7.54)
</p>
<p>Using this expression in (7.53), we see that Ξ is a functional of ψ(r). By considering
an infinitesimal variation of ψ(r), we obtain
</p>
<p>δΞ
</p>
<p>δψ(r)
=&minus;β
</p>
<p>&infin;
</p>
<p>&sum;
N=0
</p>
<p>eβ&micro;N
</p>
<p>Λ 3NN!
</p>
<p>&int;
n̂(r,rN)e&minus;βφ(r
</p>
<p>N)e&minus;β &sum;
N
i=1 ψ(ri)drN . (7.55)
</p>
<p>Thus,
δΩ
</p>
<p>δψ(r)
=&minus;kBT
</p>
<p>δ lnΞ
</p>
<p>δψ(r)
= n(r) . (7.56)
</p>
<p>Because the ensemble average is taken with respect to the equilibrium distribution,
</p>
<p>n(r) computed by means of (7.56) actually is the equilibrium density profile neq(r)
for given T , V , &micro; , and the external field ψ(r).
</p>
<p>Exercise 7.4. Derive (7.55). ///
</p>
<p>Exercise 7.5. Consider an ideal gas in equilibrium in the presence of an external
</p>
<p>field ψ(r). Using a grand canonical ensemble, derive the following results:
</p>
<p>a.
</p>
<p>neq(r) =
eβ [&micro;&minus;ψ(r)]
</p>
<p>Λ 3
. (7.57)
</p>
<p>b.
</p>
<p>Ω [neq] =&minus;kBT
&int;
</p>
<p>V
neq(r)dr . (7.58)
</p>
<p>For a homogeneous phase, Ω =&minus;PV and (7.58) reduces to PV = kBT 〈N〉, the ideal
gas equation of state. ///
</p>
<p>7.2.3 Ω for a Nonequilibrium Density Profile
</p>
<p>Recall that the functional derivative δΩ/δn(r) is defined by the relation
</p>
<p>Ω [n+δn]&minus;Ω [n] =
&int;
</p>
<p>V
</p>
<p>δΩ
</p>
<p>δn(r)
δn(r)dr+h.o. . (7.59)
</p>
<p>Thus, in order to apply (7.47), we need to know Ω [n] not only for the equilibrium
density profile neq(r) but also for an arbitrary density profile n(r). The following
example illustrates the point.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Density Functional Theory 269
</p>
<p>Example 7.3. Ideal gas: Motivated by (7.58), let us suppose that
</p>
<p>Ω [n] =&minus;kBT
&int;
</p>
<p>V
n(r)dr , (7.60)
</p>
<p>for an arbitrary n(r). Then,
</p>
<p>δΩ
</p>
<p>δn(r)
=&minus;kBT . (7.61)
</p>
<p>According to (7.47), the left-hand side of this equation is zero at equilibrium.
</p>
<p>But, because the temperature of an ideal gas can be specified as we please,
</p>
<p>this is a contradiction. What went wrong?
</p>
<p>We must remember that (7.58) applies only to the ideal gas in equilibrium.
</p>
<p>Thus, when we perturbed n(r) in (7.60), we have also changed the external
field so that the varied state is in equilibrium. In contrast, the functional deriva-
</p>
<p>tive in (7.47) must be taken while holding the external field constant along
</p>
<p>with T , V , and &micro; . So, the contradiction is only apparent. But it does illustrate
the need to develop an expression for Ω [n] for an arbitrary n(r).
</p>
<p>For given T , V , &micro; , and the external field, Ω of the system in equilibrium is, at
least conceptually, a well-defined quantity. Thus, let us suppose in this section that
</p>
<p>Ω [neq] is known and establish its relation to Ω [n] for an arbitrary n(r).
For clarity, we denote by na(r) the equilibrium density profile of a system under
</p>
<p>the influence of an external field ψa(r). In general, the value of na at some point in
the system depends on the form of the function ψa(r) throughout the system. That
is, na is not only a function of r but also a functional of ψa. To emphasize this fact,
we write
</p>
<p>na(r) = n
eq(r,ψa] . (7.62)
</p>
<p>Our goal is to express Ω [na,ψb], the grand potential of the system with the
density profile na(r) and the external field ψb(r), in terms of the known quantity
Ω [na,ψa]. To figure out their difference Ω [na,ψb]&minus;Ω [na,ψa], we consider the sys-
tem in equilibrium under ψa and imagine suddenly changing the field to ψb. At this
very moment, that is, before the system starts to respond to the new field, the system
</p>
<p>is characterized by the density profile na and the external field ψb. What would the
value of Ω be? We recall the definition of the grand potential (2.202) and Gibbs&rsquo;s
entropy formula
</p>
<p>S =&minus;kB〈ln(h3NN!ρ)〉 (7.63)
from Exercise 4.9. Then,
</p>
<p>Ω =U &minus;T S&minus;&micro;〈N〉= 〈HN〉+ kBT 〈ln(h3NN!ρ)〉&minus;&micro;
&int;
</p>
<p>V
n(r)dr , (7.64)
</p>
<p>in which ρ is the statistical weight. If we suppose that ρ has yet to adjust itself to the
new field, the only difference between Ω [na,ψa] and Ω [na,ψb] is in 〈HN〉, implying</p>
<p/>
</div>
<div class="page"><p/>
<p>270 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>that
</p>
<p>Ω [na,ψb]&minus;Ω [na,ψa] =
&lang;
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>[ψb(ri)&minus;ψa(ri)]
&rang;
</p>
<p>=
&int;
</p>
<p>V
na(r)[ψb(r)&minus;ψa(r)]dr ,
</p>
<p>(7.65)
</p>
<p>where the last step follows from (7.54). So,
</p>
<p>Ω [na,ψb] =Ω [na,ψa]+
&int;
</p>
<p>V
na(r)[ψb(r)&minus;ψa(r)]dr . (7.66)
</p>
<p>To compute Ω [na,ψb], we need to find the field ψa that produces na as the equi-
librium density profile. Theorem 7.3 in Sect. 7.3 guarantees the existence of such
</p>
<p>ψa.
Gibbs&rsquo;s entropy formula was established only for systems in equilibrium. By
</p>
<p>applying (7.63) to the nonequilibrium state, we have actually adopted a particular
</p>
<p>definition of Ω [na,ψb] among others that may be equally plausible. What matters
here, though, is that (7.66) is compatible with the principle of thermodynamics. That
</p>
<p>is, na = nb is a solution of (7.47) when Ω is given by (7.66). Exercise 7.6 verifies
this statement for the case of an ideal gas. For a more general demonstration, see
</p>
<p>Exercise 7.8. The issue discussed here has a statistical mechanical counterpart. See
</p>
<p>paragraph containing (7.94).
</p>
<p>Exercise 7.6. Let us build on the Exercise 7.5 and examine the density functional
</p>
<p>for an ideal gas:
</p>
<p>a. Show that
</p>
<p>Ω [na,ψb] = kBT
&int;
</p>
<p>V
na(r)
</p>
<p>[
ln
</p>
<p>na(r)
</p>
<p>nb(r)
&minus;1
</p>
<p>]
dr . (7.67)
</p>
<p>b. Show that
</p>
<p>Ω [na,ψb] = kBT
&int;
</p>
<p>V
na(r)[lnΛ
</p>
<p>3na(r)&minus;1]dr&minus;
&int;
</p>
<p>V
na(r)[&micro;&minus;ψb(r)]dr . (7.68)
</p>
<p>c. Use (7.67) in (7.47) to show that na = nb at equilibrium.
d. Use (7.68) in (7.47) to obtain (7.57). ///
</p>
<p>7.2.4 &dagger;A Few Remarks on Ω [na,ψb]
</p>
<p>Taking the functional derivative of (7.66) with respect to ψb while holding na fixed,
we find
</p>
<p>δΩ [na,ψb]
</p>
<p>δψb(r)
= na(r) , fixed na . (7.69)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Density Functional Theory 271
</p>
<p>Similarly, the functional derivative of (7.66) with respect to ψa with na fixed yields
</p>
<p>δΩ [na,ψa]
</p>
<p>δψa(r)
= na(r) , fixed na . (7.70)
</p>
<p>These results appear identical to (7.56). However, (7.56) was obtained without
</p>
<p>imposing any constraint on the density profile. In fact, our derivation of (7.56)
</p>
<p>amounts to comparing the values of Ω of the system in equilibrium under slightly
different external fields.
</p>
<p>Therefore, it is of some interest to derive (7.56) directly from (7.66). Given a
</p>
<p>system in equilibrium under the external field ψa, we perturb the field while allowing
the system to adjust na so as to maintain the equilibrium. We denote the modified
</p>
<p>field by ψa + δψ and the corresponding equilibrium density profile by na + δn.
Then,
</p>
<p>Ω [na +δn,ψa +δψ]&minus;Ω [na,ψa]
=Ω [na +δn,ψa +δψ]&minus;Ω [na +δn,ψa]+Ω [na +δn,ψa]&minus;Ω [na,ψa] .
</p>
<p>(7.71)
</p>
<p>Using (7.66),
</p>
<p>Ω [na +δn,ψa +δψ]&minus;Ω [na +δn,ψa]
</p>
<p>=
&int;
</p>
<p>V
[na(r)+δn(r)]δψ(r)dr=
</p>
<p>&int;
</p>
<p>V
na(r)δψ(r)dr+h.o. , (7.72)
</p>
<p>while
</p>
<p>Ω [na +δn,ψa]&minus;Ω [na,ψa] = h.o. (7.73)
because na is the equilibrium density profile for the field ψa. It follows that
</p>
<p>δΩ =
&int;
</p>
<p>V
na(r)δψ(r)dr (7.74)
</p>
<p>for the perturbation under consideration. Thus,
</p>
<p>δΩ
</p>
<p>δψa(r)
= na(r) , (7.75)
</p>
<p>which is (7.56).
</p>
<p>Equation (7.66) may be rewritten in a physically more illuminating manner. For
</p>
<p>this purpose, let us first define what may be referred to as the intrinsic grand poten-
</p>
<p>tial by
</p>
<p>Ωint[na,ψa] :=Ω [na,ψa]&minus;
&int;
</p>
<p>V
na(r)ψa(r)dr , (7.76)
</p>
<p>which is the part of the grand potential of the system in equilibrium that excludes
</p>
<p>the contribution to 〈HN〉 from the external field.</p>
<p/>
</div>
<div class="page"><p/>
<p>272 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>We also recall from (7.62) that na(r) is uniquely determined by ψa(r). Thus, the
explicit reference to na in Ωint[na,ψa] is redundant and can be dropped. Moreover,
as we shall show in Sect. 7.3, it is also true that na(r) uniquely determines ψa(r).
That is, ψa(r) is a functional of na(r):
</p>
<p>ψa(r) = ψ(r,na] (7.77)
</p>
<p>and the functional dependence on ψa may be replaced by that on na. In summary,
therefore,
</p>
<p>Ωint[na,ψa] =Ωint[ψa] =Ωint[na] . (7.78)
</p>
<p>Using (7.76) and (7.78), we can rewrite (7.66) as
</p>
<p>Ω [na,ψb] =Ωint[na]+
&int;
</p>
<p>V
na(r)ψb(r)dr , (7.79)
</p>
<p>which is the desired expression indicating that Ω [na,ψb] is the sum of two contri-
butions. One is the intrinsic part of the grand potential of the system in equilibrium
</p>
<p>with the density profile na. The other is the potential energy due to the external field
</p>
<p>ψb.
Dropping references to ψa and na, respectively, from Ωint and Ω in (7.76), we
</p>
<p>obtain
</p>
<p>Ωint[na] =Ω [ψa]&minus;
&int;
</p>
<p>V
na(r)ψa(r)dr . (7.80)
</p>
<p>In view of (7.75), Ωint is a Legendre transform of Ω .
42 Accordingly, we anticipate
</p>
<p>that
δΩint
δna(r)
</p>
<p>=&minus;ψa(r) . (7.81)
</p>
<p>Exercise 7.7. Derive (7.81). ///
</p>
<p>Exercise 7.8. Using (7.66), show that nb(r) satisfies (7.47). ///
</p>
<p>7.3 Formal Development
</p>
<p>In Sect. 7.2.1, we introduced the notion of density functional purely on the basis
</p>
<p>of thermodynamics. The subsequent development made only a very modest use of
</p>
<p>the concepts from statistical mechanics. While this may help us develop physical
</p>
<p>intuitions about DFT, the ultimate goal of the theory is to generate the fundamental
</p>
<p>equation of an inhomogeneous system starting from the Hamiltonian. That is, DFT
</p>
<p>is one example among many applications of statistical mechanics, and warrants a
</p>
<p>purely statistical mechanical formulation.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Formal Development 273
</p>
<p>7.3.1 Definitions
</p>
<p>Consider a grand canonical ensemble and let ρ(rN ,pN ,N) denote a probability dis-
tribution over all possible microstates. The equilibrium probability distribution
</p>
<p>ρeq(rN ,pN ,N) =
1
</p>
<p>Ξ eq
eβ [&micro;N&minus;HN(r
</p>
<p>N ,pN)]
</p>
<p>h3NN!
, (7.82)
</p>
<p>in which Ξ eq is the grand canonical partition function, is an example of ρ . In DFT,
however, we consider more general probability distributions. So, the form of the
</p>
<p>function ρ is arbitrary except that it must satisfy the normalization condition:
</p>
<p>Trcl{ρ(rN ,pN ,N)}= 1 , (7.83)
</p>
<p>where Trcl is the so-called classical trace, and is defined by
</p>
<p>Trcl{A(rN ,pN ,N)}=
&infin;
</p>
<p>&sum;
N=0
</p>
<p>&int;
A(rN ,pN ,N)drNdpN . (7.84)
</p>
<p>For example,
</p>
<p>Ξ eq = Trcl
{
</p>
<p>eβ (&micro;N&minus;HN)
</p>
<p>h3NN!
</p>
<p>}
. (7.85)
</p>
<p>We now define a functional of ρ by
</p>
<p>Ω [ρ ] := Trcl
{
ρ
[
HN &minus;&micro;N + kBT ln(h3NN!ρ)
</p>
<p>]}
. (7.86)
</p>
<p>7.3.2 Key Properties of the Density Functional
</p>
<p>Let us record the following three key properties of the functional Ω [ρ ] as theorems
and collect their proofs in Sect. 7.3.3.
</p>
<p>Theorem 7.1.
</p>
<p>Ω [ρeq] =&minus;kBT lnΞ eq , (7.87)
which we recognize as the grand potential.
</p>
<p>Theorem 7.2.
</p>
<p>Ω [ρ ]&ge;Ω [ρeq] , (7.88)
where the equality holds if and only if ρ &equiv; ρeq.
</p>
<p>Theorem 7.3. Suppose that ρ may be written as
</p>
<p>ρ(rN ,pN ,N) =
1
</p>
<p>Ξ
</p>
<p>eβ [&micro;N&minus;H
0
N&minus;&sum;Ni=1 u(ri)]
</p>
<p>h3NN!
(7.89)</p>
<p/>
</div>
<div class="page"><p/>
<p>274 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>with the constant Ξ determined by (7.83). Here,
</p>
<p>H0N :=
N
</p>
<p>&sum;
i=1
</p>
<p>||pi||2
2m
</p>
<p>+φ(rN) (7.90)
</p>
<p>is the Hamiltonian of the system excluding the external field ψ . If we limit our con-
sideration to ρ that can be expressed as (7.89), there is a one-to-one correspondence
between ρ and the density profile
</p>
<p>n(r) = Trcl{ρ n̂} , (7.91)
</p>
<p>where n̂ is the density operator defined by (7.49).
</p>
<p>The actual system of our interest is subject to the external field ψ(r). Thus,
ρ defined by (7.89) is not the equilibrium distribution unless u(r) &equiv; ψ(r). Equa-
tion (7.91) gives the equilibrium density profile of the system if it is subject to the
</p>
<p>external field u(r) in place of ψ(r).
By virtue of Theorem 7.3, Ω may be expressed as a functional of n:
</p>
<p>Ω [ρ ] =Ω [n] . (7.92)
</p>
<p>(It must be clearly understood that the explicit form of the functional Ω [ρ ] differs
from that of Ω [n].) Theorem 7.2 then reads
</p>
<p>Ω [n]&ge;Ω [neq] (7.93)
</p>
<p>with the equality holding if and only if n &equiv; neq. This is a statistical mechanical
version of (7.44).
</p>
<p>Finally, (7.89) does not exhaust all possible forms of ρ . For example,
</p>
<p>ρ(rN ,pN ,N) &prop; exp
{
β
[
&micro;N &minus;H0N(rN ,pN)+ f (pN)
</p>
<p>]}
(7.94)
</p>
<p>cannot be written in the form of (7.89). The same applies if we have f (||r2 &minus; r1||) in
place of f (pN), for example. The point, however, is that ρ in the form of (7.89) is
sufficient to guarantee the existence of the density functional Ω [n] that is compatible
with the principle of thermodynamics embodied in (7.44).
</p>
<p>Example 7.4. Statistical mechanical derivation of (7.66): Consider a system
</p>
<p>with the density profile na(r) in the presence of the external field ub(r). Given
na(r), Theorem 7.3 guarantees the existence of the corresponding ρa (and
hence of ua):
</p>
<p>ρa =
1
</p>
<p>Ξa
</p>
<p>eβ [&micro;N&minus;H
0
N&minus;&sum;Ni=1 ua(ri)]
</p>
<p>h3NN!
. (7.95)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Formal Development 275
</p>
<p>Using this expression in (7.86), we obtain
</p>
<p>Ω [na,ub] = Tr
cl
</p>
<p>{
ρa
</p>
<p>[
H0N +
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ub(ri)&minus;&micro;N + kBT ln(h3NN!ρa)
]}
</p>
<p>(7.96)
</p>
<p>Replacing ub by ua,
</p>
<p>Ω [na,ua] = Tr
cl
</p>
<p>{
ρa
</p>
<p>[
H0N +
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ua(ri)&minus;&micro;N + kBT ln(h3NN!ρa)
]}
</p>
<p>(7.97)
</p>
<p>Subtracting (7.97) from (7.96), we obtain
</p>
<p>Ω [na,ub]&minus;Ω [na,ua] = Trcl
{
ρa
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>[ub(ri)&minus;ua(ri)]
}
</p>
<p>. (7.98)
</p>
<p>Using the density operator,
</p>
<p>Trcl
</p>
<p>{
ρa
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>[ub(ri)&minus;ua(ri)]
}
</p>
<p>= Trcl
{
ρa
</p>
<p>&int;
</p>
<p>V
n̂(r,rN)[ub(r)&minus;ua(r)]dr
</p>
<p>}
</p>
<p>=
</p>
<p>&int;
</p>
<p>V
Trcl{ρan̂}[ub(r)&minus;ua(r)]dr=
</p>
<p>&int;
</p>
<p>V
na(r)[ub(r)&minus;ua(r)]dr , (7.99)
</p>
<p>where we used (7.91). Thus,
</p>
<p>Ω [na,ub]&minus;Ω [na,ua] =
&int;
</p>
<p>V
na(r)[ub(r)&minus;ua(r)]dr , (7.100)
</p>
<p>which is (7.66).
</p>
<p>7.3.3 &dagger;Proofs of Theorems
</p>
<p>Let us prove the theorems just introduced.
</p>
<p>Proof of Theorem 7.1
</p>
<p>We have only to replace ρ in (7.86) by ρeq and substitute (7.82) for the second
ρeq. ⊓⊔</p>
<p/>
</div>
<div class="page"><p/>
<p>276 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>Proof of Theorem 7.2
</p>
<p>Ω [ρ ] = Trcl
{
ρ
[
HN &minus;&micro;N + kBT ln(h3NN!ρ)
</p>
<p>]}
</p>
<p>= Trcl
{
ρ
[
HN &minus;&micro;N + kBT ln(h3NN!ρeq)
</p>
<p>]}
+ kBT Tr
</p>
<p>cl {ρ (lnρ&minus; lnρeq)}
&ge; Trcl
</p>
<p>{
ρ
[
HN &minus;&micro;N + kBT ln(h3NN!ρeq)
</p>
<p>]}
, (7.101)
</p>
<p>where we used the Gibbs&ndash;Bogoliubov inequality discussed in Appendix B.5. The
</p>
<p>equality in (7.101) holds if and only if ρ &equiv; ρeq.
From (7.82),
</p>
<p>kBT ln(h
3NN!ρeq) = &micro;N &minus;HN &minus; kBT lnΞ eq = &micro;N &minus;HN +Ω [ρeq] , (7.102)
</p>
<p>which is now substituted into (7.101) to give
</p>
<p>Ω [ρ ]&ge; Trcl{ρΩ [ρeq]}=Ω [ρeq]Trcl{ρ}=Ω [ρeq] . (7.103)
</p>
<p>The second step is justified because Ω [ρeq], being a quantity obtained after the
classical trace, does not dependent on rN , pN , or N. The last step follows from the
</p>
<p>normalization condition (7.83). ⊓⊔
</p>
<p>Proof of Theorem 7.3
</p>
<p>For a given u(r), ρ is completely determined by (7.89). The corresponding n(r)
follows from (7.91). Pictorially, we have
</p>
<p>u &minus;&rarr; ρ &minus;&rarr; n . (7.104)
</p>
<p>The content of Theorem 7.3 is that, if we limit ourselves to ρ written in the form of
(7.89), there is always one and only one ρ that corresponds to any density n(r) we
prescribe to the system. In view of (7.104), it is sufficient to prove the existence and
</p>
<p>the uniqueness of u(r) that produces the given n(r):
</p>
<p>n &minus;&rarr; u . (7.105)
</p>
<p>By (7.91), n(r) is the equilibrium density profile of the system subject to the field
u(r) instead of ψ(r). Using the notation introduced in (7.62), therefore, we have
</p>
<p>n(r) = neq(r,u] . (7.106)
</p>
<p>The claim we wish to prove is that (7.106), regarded as an equation for u(r), has a
unique solution for any n(r). We prove the uniqueness of u(r) first and then argue
for its existence.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Formal Development 277
</p>
<p>Uniqueness
</p>
<p>The proof is by contradiction. Suppose that
</p>
<p>n(r) = neq(r,ua] = n
eq(r,ub] , (7.107)
</p>
<p>where ua and ub are two distinct external fields, for which (7.89) reads
</p>
<p>ρa =
1
</p>
<p>Ξa
</p>
<p>eβ [&micro;N&minus;H
0
N&minus;&sum;Ni=1 ua(ri)]
</p>
<p>h3NN!
and ρb =
</p>
<p>1
</p>
<p>Ξb
</p>
<p>eβ [&micro;N&minus;H
0
N&minus;&sum;Ni=1 ub(ri)]
</p>
<p>h3NN!
, (7.108)
</p>
<p>respectively. By (7.86) and Theorem 7.2, we have
</p>
<p>Ω [ρa] = Tr
cl
</p>
<p>{
ρa
</p>
<p>[
H0N &minus;&micro;N + kBT ln(h3NN!ρa)+
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ua(ri)
</p>
<p>]}
</p>
<p>&lt; Trcl
</p>
<p>{
ρb
</p>
<p>[
H0N &minus;&micro;N + kBT ln(h3NN!ρb)+
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>ua(ri)
</p>
<p>]}
</p>
<p>= Ω [ρb]+Tr
cl
</p>
<p>{
ρb
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>[ua(ri)&minus;ub(ri)]
}
</p>
<p>. (7.109)
</p>
<p>Using (7.99), we find
</p>
<p>Ω [ρa]&lt;Ω [ρb]+
&int;
</p>
<p>V
n(r)[ua(r)&minus;ub(r)]dr . (7.110)
</p>
<p>Similarly,
</p>
<p>Ω [ρb]&lt;Ω [ρa]+
&int;
</p>
<p>V
n(r)[ub(r)&minus;ua(r)]dr . (7.111)
</p>
<p>Adding these two inequalities, we arrive at the contradiction:
</p>
<p>Ω [ρa]+Ω [ρb]&lt;Ω [ρa]+Ω [ρb] . (7.112)
</p>
<p>Thus, ua &equiv; ub. ⊓⊔
</p>
<p>Existence
</p>
<p>Suppose that a system is under the external field u(r). Left undisturbed, the system
will eventually reach a state of equilibrium, in which
</p>
<p>n(r) = neq(r,u] . (7.113)
</p>
<p>So, at least for this n(r), the corresponding external field does exist.
Now we show that the external field exists also for any density profile that is
</p>
<p>infinitesimally different from n(r). For this purpose, we consider an infinitesimal</p>
<p/>
</div>
<div class="page"><p/>
<p>278 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>perturbation δu(r) to the field. According to the definition of the functional deriva-
tive, the response is given by
</p>
<p>δn(r) =
&int;
</p>
<p>V
</p>
<p>δn(r)
</p>
<p>δu(r&prime;)
δu(r&prime;)dr&prime; (7.114)
</p>
<p>to the first order of the variation.
</p>
<p>As we have seen, u(r) for a given n(r) is unique. So, any distinct perturbations
δu1(r&prime;) and δu2(r&prime;) produce distinct responses δn1(r) and δn2(r), respectively.43
</p>
<p>Conversely, it is clear from (7.89) and (7.91) that distinct responses can result only
</p>
<p>from distinct perturbations of the field. It follows that (7.114) is invertible and we
</p>
<p>can write
</p>
<p>δu(r&prime;) =
&int;
</p>
<p>V
</p>
<p>δu(r&prime;)
δn(r)
</p>
<p>δn(r)dr . (7.115)
</p>
<p>By means of this equation, we can compute δu(r&prime;) that produces any response δn(r)
we desire.44
</p>
<p>What about an arbitrary density profile nb(r) that may differ from the initial equi-
librium profile na(r) = n
</p>
<p>eq(r,ua] by more than an infinitesimal amount? In this case,
we can apply (7.115) repeatedly to a series of infinitesimal variations δn(r) until
we eventually arrive at the desired profile. The corresponding field ub(r
</p>
<p>&prime;) is the ini-
tial field ua(r
</p>
<p>&prime;) plus the sum of all the infinitesimal variations δu(r&prime;) evaluated by
(7.115) along the way. As an example, consider a particular path specified by
</p>
<p>nλ (r) = na(r)+λ [nb(r)&minus;na(r)] , 0 &le; λ &le; 1 . (7.116)
</p>
<p>The thermodynamic integration method described in the next section leads to
</p>
<p>ub(r
&prime;) = ua(r
</p>
<p>&prime;)+
&int; 1
</p>
<p>0
</p>
<p>&int;
</p>
<p>V
</p>
<p>δuλ (r
&prime;)
</p>
<p>δnλ (r)
[nb(r)&minus;na(r)]drdλ , (7.117)
</p>
<p>where nλ (r) = n
eq(r,uλ ]. ⊓⊔
</p>
<p>Substituting (7.115) with r replaced by r&prime;&prime; into (7.114), we see that
</p>
<p>δn(r) =
&int;
</p>
<p>V
</p>
<p>δn(r)
</p>
<p>δu(r&prime;)
</p>
<p>[&int;
</p>
<p>V
</p>
<p>δu(r&prime;)
δn(r&prime;&prime;)
</p>
<p>δn(r&prime;&prime;)dr&prime;&prime;
]
</p>
<p>dr&prime;
</p>
<p>=
&int;
</p>
<p>V
</p>
<p>[&int;
</p>
<p>V
</p>
<p>δn(r)
</p>
<p>δu(r&prime;)
δu(r&prime;)
δn(r&prime;&prime;)
</p>
<p>dr&prime;
]
δn(r&prime;&prime;)dr&prime;&prime; . (7.118)
</p>
<p>It follows that &int;
</p>
<p>V
</p>
<p>δn(r)
</p>
<p>δu(r&prime;)
δu(r&prime;)
δn(r&prime;&prime;)
</p>
<p>dr&prime; = δ (r&minus; r&prime;&prime;) . (7.119)
</p>
<p>The two functional derivatives in the integrand are said to be the functional inverse
</p>
<p>of each other.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 279
</p>
<p>7.4 Construction of a Density Functional
</p>
<p>Before DFT can be utilized for any practical purposes, we must identify the explicit
</p>
<p>form of the density functional. The basic tool we use for this purpose is the ther-
</p>
<p>modynamic integration method. That is, we first compute δΩ in response to a
particular variation either of the external field and/or the intermolecular potential.
</p>
<p>The resulting expression is then integrated from a reference state for which Ω is
known to the actual state of our interest.
</p>
<p>Because the functional derivative, to which δΩ is related, and the subsequent
integration are the opposite operations, it seems strange to suggest that anything
</p>
<p>useful should come out of this. Nevertheless, the functional derivative gives rise to
</p>
<p>quantities which afford direct physical interpretations. This opens up the possibility
</p>
<p>of developing physically meaningful approximations for them even if they may be
</p>
<p>difficult to evaluate exactly.
</p>
<p>7.4.1 Variation of the External Field
</p>
<p>Consider a system in equilibrium subject to the external field ψa. The quantity of
interest is the difference
</p>
<p>Ω [nb,ψb]&minus;Ω [na,ψa] . (7.120)
Since the expression involves only the equilibrium density profiles, this is the
</p>
<p>reversible work required to change the external field from ψa to ψb. To compute
this quantity, we use a reversible work source to generate an external field
</p>
<p>λ [ψb(r)&minus;ψa(r)] (7.121)
</p>
<p>so that the total external field experienced by the system is
</p>
<p>ψλ (r) := ψa(r)+λ [ψb(r)&minus;ψa(r)] . (7.122)
</p>
<p>Clearly, ψλ=0 = ψa and ψλ=1 = ψb. If we change λ very slowly from 0 to 1, the
system will have a sufficient time to establish the equilibrium density profile
</p>
<p>nλ (r) := n
eq(r,ψλ ] (7.123)
</p>
<p>at each value of λ . Such a process is reversible.
Now, recalling the definition of the functional derivative and using (7.56), in
</p>
<p>which n(r) is the equilibrium density profile for a given external field, we have
</p>
<p>δΩ [ψλ ] =
&int;
</p>
<p>V
</p>
<p>δΩ
</p>
<p>δψλ (r)
δψλ (r)dr=
</p>
<p>&int;
</p>
<p>V
nλ (r)δψλ (r)dr . (7.124)
</p>
<p>According to (7.122),
</p>
<p>δψλ (r) = [ψb(r)&minus;ψa(r)]δλ (7.125)</p>
<p/>
</div>
<div class="page"><p/>
<p>280 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>for an infinitesimal change in λ and (7.124) becomes
</p>
<p>δΩ [ψλ ] =
&int;
</p>
<p>V
nλ (r)[ψb(r)&minus;ψa(r)]δλdr . (7.126)
</p>
<p>Dividing both sides by δλ and taking the limit of δλ &rarr; 0, we obtain
</p>
<p>&part;Ω
</p>
<p>&part;λ
=
</p>
<p>&int;
</p>
<p>V
nλ (r)[ψb(r)&minus;ψa(r)]dr . (7.127)
</p>
<p>We now integrate this expression from λ = 0 to λ = 1 and obtain
</p>
<p>Ω [nb,ψb] =Ω [na,ψa]+
&int; 1
</p>
<p>0
</p>
<p>&int;
</p>
<p>V
nλ (r)[ψb(r)&minus;ψa(r)]drdλ . (7.128)
</p>
<p>7.4.2 Variation of the Intermolecular Potential: Case 1
</p>
<p>Consider a system in equilibrium under the external field ψ . This time, the thermo-
dynamic integration involves a continuous change in the intermolecular potential
</p>
<p>φ(rN). But, the external field will be held fixed.
We assume the pairwise additivity of the intermolecular potential and write
</p>
<p>φ(rN) =
1
</p>
<p>2
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>v(ri,r j) , (7.129)
</p>
<p>where v is referred to as the pair potential and &prime; indicates that the j = i term is
excluded from the second sum.
</p>
<p>The quantity of our interest is the difference
</p>
<p>Ω [nb,vb]&minus;Ω [na,va] , (7.130)
</p>
<p>where
</p>
<p>na(r) := n
eq(r,va] . (7.131)
</p>
<p>is the equilibrium density profile when the pair potential is va(r). Similarly for nb.
In order to consider the functional derivative of Ω with respect to v, we define
</p>
<p>Î(r,r&prime;) :=
N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>δ (r&minus; ri)δ (r&prime;&minus; r j) (7.132)
</p>
<p>and rewrite (7.129) as
</p>
<p>φ(rN) =
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
Î(r,r&prime;)v(r,r&prime;)drdr&prime; . (7.133)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 281
</p>
<p>Using (7.133) in (7.53) and recalling (7.41), we observe that
</p>
<p>δΩ
</p>
<p>δv(r,r&prime;)
=
</p>
<p>1
</p>
<p>2
〈Î(r,r&prime;,rN)〉=: 1
</p>
<p>2
n(2)(r,r&prime;) =:
</p>
<p>1
</p>
<p>2
n(r)n(r&prime;)g(r,r&prime;) , (7.134)
</p>
<p>in which n(2)(r,r&prime;) and g(r,r&prime;) are called, respectively, the pair distribution func-
tion and the radial distribution function. Their physical content will be discussed
</p>
<p>in Sect. 7.4.4. Using g, we can write
</p>
<p>δΩ =
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)g(r,r&prime;)δv(r,r&prime;)drdr&prime; , (7.135)
</p>
<p>in which n and g are to be computed for the system in equilibrium before we made
</p>
<p>an infinitesimal change in the pair potential.
</p>
<p>As in the previous subsection, let
</p>
<p>vλ (r,r
&prime;) := va(r,r
</p>
<p>&prime;)+λ [vb(r,r
&prime;)&minus; va(r,r&prime;)] , (7.136)
</p>
<p>which is va if λ = 0 and vb if λ = 1. For this particular choice of vλ ,
</p>
<p>δvλ (r,r
&prime;) = [vb(r,r
</p>
<p>&prime;)&minus; va(r,r&prime;)]δλ . (7.137)
</p>
<p>Substituting this expression into (7.135), dividing the resulting equation by δλ , and
then taking the δλ &rarr; 0 limit, we obtain
</p>
<p>&part;Ω
</p>
<p>&part;λ
=
</p>
<p>1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
nλ (r)nλ (r
</p>
<p>&prime;)gλ (r,r
&prime;)[vb(r,r
</p>
<p>&prime;)&minus; va(r,r&prime;)]drdr&prime; , (7.138)
</p>
<p>where gλ and nλ are, respectively, the radial distribution function and the equi-
</p>
<p>librium density profile in the system in which the pair potential is vλ . Integrating
</p>
<p>(7.138) with respect to λ , we find
</p>
<p>Ω [nb,vb] = Ω [na,va]
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
nλ (r)nλ (r
</p>
<p>&prime;)gλ (r,r
&prime;)[vb(r,r
</p>
<p>&prime;)&minus; va(r,r&prime;)]drdr&prime;dλ .
</p>
<p>(7.139)
</p>
<p>7.4.3 Variation of the Intermolecular Potential: Case 2
</p>
<p>In Sect. 7.4.1, we considered the variation of the external field ψ(r) without chang-
ing the functional form of the pair potential v(r,r&prime;). In Sect. 7.4.2, the functional
form of ψ(r) was fixed while v(r,r&prime;) was varied. In both cases, this meant that the
density profile had to change along the integration path.
</p>
<p>However, Theorem 7.3 guarantees the existence of ψ(r) for any equilibrium den-
sity profile we prescribe to the system. This means that, as we vary v(r,r&prime;), we can</p>
<p/>
</div>
<div class="page"><p/>
<p>282 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>always adjust ψ(r) to maintain the same equilibrium density profile. For the partic-
ular variation given by (7.137), we have
</p>
<p>δΩ =
&int;
</p>
<p>V
n(r)δψλ (r)dr+
</p>
<p>1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)gλ (r,r
</p>
<p>&prime;)[vb(r,r
&prime;)&minus; va(r,r&prime;)]δλdrdr&prime; ,
</p>
<p>(7.140)
</p>
<p>where δψλ (r) is the adjustment that must be made to the external field in order to
keep the density profile unchanged when vλ is varied by an infinitesimal amount.
</p>
<p>Dividing (7.140) by δλ and taking the δλ &rarr; 0 limit, we obtain
</p>
<p>&part;Ω
</p>
<p>&part;λ
=
</p>
<p>&int;
</p>
<p>V
n(r)
</p>
<p>&part;ψλ (r)
</p>
<p>&part;λ
dr+
</p>
<p>1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)gλ (r,r
</p>
<p>&prime;)[vb(r,r
&prime;)&minus; va(r,r&prime;)]drdr&prime; .
</p>
<p>(7.141)
</p>
<p>We integrate this equation with respect to λ and obtain
</p>
<p>Ω [n,vb] = Ω [n,va]+
&int;
</p>
<p>V
n(r)[ψb(r)&minus;ψa(r)]dr
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)gλ (r,r
</p>
<p>&prime;)[vb(r,r
&prime;)&minus; va(r,r&prime;)]drdr&prime;dλ ,
</p>
<p>(7.142)
</p>
<p>where ψa(r) is the external field that yield n(r) as the equilibrium density profile
when the pair potential is va(r,r
</p>
<p>&prime;). Similarly for ψb(r). Using (7.76), we rewrite
(7.142) as
</p>
<p>Ω [n,vb] =Ωint[n,va]+
&int;
</p>
<p>V
n(r)ψb(r)dr
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)gλ (r,r
</p>
<p>&prime;)[vb(r,r
&prime;)&minus; va(r,r&prime;)]drdr&prime;dλ . (7.143)
</p>
<p>We now recall the relation F = Ω + &micro;N and define the intrinsic Helmholtz free
energy by
</p>
<p>Fint[n,va] :=Ωint[n,va]+&micro;
&int;
</p>
<p>V
n(r)dr . (7.144)
</p>
<p>Then, we can rewrite (7.143) as
</p>
<p>Ω [n,vb] = Fint[n,va]&minus;
&int;
</p>
<p>V
n(r)[&micro;&minus;ψb(r)]dr
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)gλ (r,r
</p>
<p>&prime;)[vb(r,r
&prime;)&minus; va(r,r&prime;)]drdr&prime;dλ , (7.145)
</p>
<p>which relates the grand potential of the system of our interest (with vb) to the intrin-
</p>
<p>sic Helmholtz free energy of the reference system (with va).
</p>
<p>Combining (7.68), (7.79), and (7.144), we find
</p>
<p>F idint[n] = kBT
&int;
</p>
<p>V
n(r)[lnΛ 3n(r)&minus;1]dr (7.146)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 283
</p>
<p>for an ideal gas. As with (7.68), this result is exact. We now define the intrinsic
</p>
<p>excess Helmholtz free energy by
</p>
<p>Fexcint [n,va] := Fint[n,va]&minus;F idint[n] (7.147)
</p>
<p>and rewrite (7.145) as
</p>
<p>Ω [n,vb] = kBT
&int;
</p>
<p>V
n(r)[lnΛ 3n(r)&minus;1]dr+Fexcint [n,va]&minus;
</p>
<p>&int;
</p>
<p>V
n(r)[&micro;&minus;ψb(r)]dr
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)gλ (r,r
</p>
<p>&prime;)[vb(r,r
&prime;)&minus; va(r,r&prime;)]drdr&prime;dλ . (7.148)
</p>
<p>This is the desired expression. Of course, we still have to choose the reference pair
</p>
<p>potential va, determine the explicit form of the functional F
exe
int [n,va], and then eval-
</p>
<p>uate gλ (r,r
&prime;) for 0 &le; λ &le; 1.
</p>
<p>7.4.4 Pair Distribution Function
</p>
<p>In this section, we make a few key observations about g of homogeneous systems.
</p>
<p>This will guide our choice for the reference potential va.
</p>
<p>We start by seeking for the physical content of n(2). For this purpose, it is actually
</p>
<p>easier to work with a canonical ensemble. Thus, we define the density profile and
</p>
<p>the pair distribution function by
</p>
<p>nN(r) := 〈n̂(r,rN)〉N (7.149)
</p>
<p>and
</p>
<p>n
(2)
N (r,r
</p>
<p>&prime;) := 〈Î(r,r&prime;,rN)〉N , (7.150)
respectively, where the subscript N reminds us of the canonical ensemble in which
</p>
<p>N is fixed. For the Hamiltonian given by (7.52), we have
</p>
<p>〈δ (r&minus; r1)〉N
</p>
<p>=
1
</p>
<p>Z
</p>
<p>1
</p>
<p>Λ 3NN!
</p>
<p>&int;
δ (r&minus; r1)e&minus;βφ(r
</p>
<p>N)e&minus;β &sum;
N
i=1 ψ(ri)drN
</p>
<p>=
1
</p>
<p>Z
</p>
<p>1
</p>
<p>Λ 3NN!
</p>
<p>&int;
e&minus;βφ(r,r2,...,rN)e&minus;βψ(r)e&minus;β &sum;
</p>
<p>N
i=2 ψ(ri)dr2 &middot; &middot; &middot;drN . (7.151)
</p>
<p>This is a function of r and represents the probability density of finding particle 1 at
</p>
<p>r. Because particle 1 is no different from any other in the system, we have
</p>
<p>〈δ (r&minus; r1)〉N = 〈δ (r&minus; ri)〉N , for i = 2, . . . ,N , (7.152)
</p>
<p>and (7.149) becomes
</p>
<p>nN(r) = N〈δ (r&minus; r1)〉N . (7.153)</p>
<p/>
</div>
<div class="page"><p/>
<p>284 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>Similarly, we have
</p>
<p>〈δ (r&minus; r1)δ (r&prime;&minus; r2)〉N
</p>
<p>=
1
</p>
<p>Z
</p>
<p>1
</p>
<p>Λ 3NN!
</p>
<p>&int;
δ (r&minus; r1)δ (r&prime;&minus; r2)e&minus;βφ(r
</p>
<p>N)e&minus;β &sum;
N
i=1 ψ(ri)drN
</p>
<p>=
1
</p>
<p>Z
</p>
<p>1
</p>
<p>Λ 3NN!
</p>
<p>&int;
e&minus;βφ(r,r
</p>
<p>&prime;,r3,...,rN)e&minus;β [ψ(r)+ψ(r
&prime;)]e&minus;β &sum;
</p>
<p>N
i=3 ψ(ri)dr3 &middot; &middot; &middot;drN , (7.154)
</p>
<p>which is the probability density of finding particle 1 at r and particle 2 at r&prime; regard-
less of the coordinates of other particles. Since the pair of particles 1 and 2 is no
</p>
<p>different from any other pair, (7.150) gives
</p>
<p>n
(2)
N (r,r
</p>
<p>&prime;) = N(N &minus;1)〈δ (r&minus; r1)δ (r&prime;&minus; r2)〉N . (7.155)
</p>
<p>Given the physical content of (7.151) and (7.154), we see that the ratio
</p>
<p>〈δ (r&minus; r1)δ (r&prime;&minus; r2)〉Ndrdr&prime;
〈δ (r&minus; r1)〉Ndr
</p>
<p>(7.156)
</p>
<p>is the conditional probability that particle 2 is within the volume element dr&prime; taken
around r&prime; given that particle 1 is inside dr taken around r. Because particles are
identical, this probability is no different for any other particle (particle 3 through
</p>
<p>particle N in place of particle 2). Thus, on average, the number of particles within
</p>
<p>dr&prime; is given by
</p>
<p>(N &minus;1) 〈δ (r&minus; r1)δ (r
&prime;&minus; r2)〉Ndrdr&prime;
</p>
<p>〈δ (r&minus; r1)〉Ndr
=
</p>
<p>n
(2)
N (r,r
</p>
<p>&prime;)
nN(r)
</p>
<p>dr&prime; . (7.157)
</p>
<p>In other words, n
(2)
N (r,r
</p>
<p>&prime;)/nN(r) is the number density of particles at r&prime; when there
</p>
<p>is a particle at r. This is the physical content of n
(2)
N we seek. We have the factor
</p>
<p>N &minus;1 instead of N because the particle fixed at r is not available to occupy r&prime;.
If the condition that a particle must be at r is disregarded, then the density at r&prime;
</p>
<p>is simply nN(r
&prime;). Dividing (7.157) by nN(r&prime;), therefore, we isolate the effect of the
</p>
<p>particle at r on the density at r&prime;:
</p>
<p>gN(r,r
&prime;) :=
</p>
<p>n
(2)
N (r,r
</p>
<p>&prime;)
nN(r)nN(r&prime;)
</p>
<p>. (7.158)
</p>
<p>This definition of the radial distribution function gN(r,r
&prime;) should be compared
</p>
<p>with (7.134) for the grand canonical ensemble. (Our definition of gN(r,r
&prime;) follows
</p>
<p>the convention adopted in Ref. [9]. The choice is not unique, however. For example,
</p>
<p>gN in Ref. [8] is (N&minus;1)/N times our gN . The latter definition is perhaps more conve-
nient for a homogeneous system, but it leads to a somewhat different interpretation
</p>
<p>of gN .)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 285
</p>
<p>Let us now consider a qualitative behavior of gN in a homogeneous system in the
</p>
<p>absence of any external field. In this case,
</p>
<p>nN(r) = nN(r
&prime;) =
</p>
<p>N
</p>
<p>V
=: n (7.159)
</p>
<p>is a constant. Since every point in the system is equivalent to any other, we have
</p>
<p>gN(r,r
&prime;) = gN(0,r
</p>
<p>&prime;&minus; r) = gN(r&prime;&minus; r) , (7.160)
</p>
<p>where we dropped the explicit reference to 0. We shall also assume that the system
</p>
<p>is isotropic so that gN and n
(2)
N = n
</p>
<p>2gN depend only on r := ||r&prime;&minus; r||. (Notice that
r 	= ||r|| here. Fortunately, we do not have to consider ||r|| in the remainder of this
section.)
</p>
<p>For an ideal gas, the presence of a particle at r does not affect the probability
</p>
<p>density of finding another at r&prime;, which is therefore (N &minus; 1)/V with &minus;1 accounting
for the particle fixed at r and hence cannot simultaneously occupy r&prime;. Division by n
yields
</p>
<p>gN(r) = 1&minus;
1
</p>
<p>N
, (7.161)
</p>
<p>which is essentially unity for a macroscopic system. Alternatively, we notice that
</p>
<p>〈δ (r&minus; r1)δ (r&prime;&minus; r2)〉N = 〈δ (r&minus; r1)〉N〈δ (r&prime;&minus; r2)〉N (7.162)
</p>
<p>for an ideal gas. Thus, (7.157) reduces to
</p>
<p>n
(2)
N (r,r
</p>
<p>&prime;)
nN(r)
</p>
<p>= (N &minus;1)〈δ (r&prime;&minus; r2)〉N =
(
</p>
<p>1&minus; 1
N
</p>
<p>)
nN(r
</p>
<p>&prime;) , (7.163)
</p>
<p>from which we obtain (7.161).
</p>
<p>In the case of a low-density gas, it is rare for a given particle to have another
</p>
<p>nearby, and having two particles nearby is extremely rare. Thus, a particle fixed at r
</p>
<p>serves as the sole source of an external field v(r&prime;&minus; r) for another at r&prime;, implying that
</p>
<p>n
(2)
N (r)
</p>
<p>nN
&prop; e&minus;βv(r) , (7.164)
</p>
<p>where we assumed that v depends only on the distance r between the two particles.
</p>
<p>Typically, v(r) vanishes with increasing r. At the same time, gN should approach the
value for an ideal gas. This is sufficient to fix the proportionality constant in (7.164)
</p>
<p>and leads to
</p>
<p>gN(r) =
</p>
<p>(
1&minus; 1
</p>
<p>N
</p>
<p>)
e&minus;βv(r) . (7.165)</p>
<p/>
</div>
<div class="page"><p/>
<p>286 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>-2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>r d
</p>
<p>vHS r
</p>
<p>2 50 1 3 4
</p>
<p>a
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>0 2 4 6 8
</p>
<p>r d
</p>
<p>1 1
N
</p>
<p>1
gN r
</p>
<p>nd3 0 01
</p>
<p>nd3 0 1
</p>
<p>nd3 0 5
</p>
<p>nd3 1
</p>
<p>b
</p>
<p>Fig. 7.2 a Hard-sphere potential and b its radial distribution functions gN from Monte Carlo
</p>
<p>simulations at different densities nd3 = 0.01, nd3 = 0.1, nd3 = 0.5, and nd3 = 1. In view of (7.161),
gN is multiplied by (1&minus; 1/N)&minus;1. For clarity, the plots for nd3 = 0.1, nd3 = 0.5, and nd3 = 1 are
shifted upward by 1, 2, and 3, respectively. Note that gN of a hard-sphere fluid does not have any
</p>
<p>temperature dependence since vHS(r)/kBT is independent of T . The system was taken as a cubic
box of volume 8000d3 under periodic boundary conditions.
</p>
<p>As an example, consider the hard-sphere potential defined by
</p>
<p>vHS(r) =
</p>
<p>{
&infin; if r &lt; d
</p>
<p>0 otherwise,
(7.166)
</p>
<p>where the parameter d is called the hard-sphere diameter. Equation (7.165) then
</p>
<p>is the step function:
</p>
<p>gN(r) =
</p>
<p>(
1&minus; 1
</p>
<p>N
</p>
<p>)
θ(r&minus;d) :=
</p>
<p>{
0 if r &lt; d
</p>
<p>1&minus; 1
N
</p>
<p>otherwise.
(7.167)
</p>
<p>That gN(r) = 0 for r &lt; d makes perfect sense because the hard-sphere potential pro-
hibits a pair of particles to get any closer than d. Figure 7.2 shows the pair potential
</p>
<p>along with the radial distribution function obtained by Monte Carlo simulations at
</p>
<p>a few values of n. At the lowest density considered, (7.167) indeed is an excellent
</p>
<p>approximation. At the highest density shown in the figure, gN exhibits a consider-
</p>
<p>able oscillatory behavior. What is causing this?
</p>
<p>As the density is increased, a hard-sphere particle at r starts to get surrounded
</p>
<p>by other hard-spheres that are all pushed toward the particle at r. But their centers
</p>
<p>cannot enter the spherical region of diameter d around r, thus leading to formation
</p>
<p>of a layer as illustrated in Fig. 7.3. This explains the first peak of gN(r) observed in</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 287
</p>
<p>........................................................................................ ........ . ....... . ....... . ....... . ....... . ....... . ....... .
....... .
</p>
<p>.......
........
</p>
<p>........
........
........
........
........
</p>
<p>........
........
</p>
<p>........
.........
</p>
<p>...............
</p>
<p>........................................................................................................................
.
........
.
........
.
........
.........
.
........................... ......... ......... ......... . ....... . ........ . ........ ........... . ........ . . ........ . ........ . ........ ..... .... . ........ .
</p>
<p>. ........ .
........ .
</p>
<p>........
..... ....
</p>
<p>. ........
........
</p>
<p>.........
.........
...........
..........
.
........
.
........
.
........
.....
....
.
.........
.........
</p>
<p>.........
.........
</p>
<p>.........
.........
</p>
<p>........
.........
</p>
<p>..........
...........
</p>
<p>...........................
</p>
<p>Fig. 7.3 Formation of coordination shells (gray circles) around a particle at r (black circle). Radii
</p>
<p>of the dashed circles are d and 2d.
</p>
<p>Fig. 7.2b. These spheres in what we might call the &ldquo;first coordination shell,&rdquo; in turn,
</p>
<p>exclude other particles from their cores, leading to formation of the &ldquo;second coordi-
</p>
<p>nation shell&rdquo; and hence to the second peak of gN(r). Continuing in this way, gN(r)
exhibits an oscillatory behavior with the amplitude of oscillation gradually decreas-
</p>
<p>ing with increasing r. For sufficiently large r, (7.162) holds and gN(r) asymptotes
toward 1&minus;1/N. The important thing to remember is that even a homogeneous fluid
has a structure when observed from the perspective of a given particle.
</p>
<p>Essentially, the same behavior is observed for other pair potentials having a less
</p>
<p>harsh but still short-ranged repulsive core.
</p>
<p>As an example, let us take the Lennard&ndash;Jones potential:
</p>
<p>vLJ(r) = 4ε
</p>
<p>[(σ
r
</p>
<p>)12
&minus;
(σ
</p>
<p>r
</p>
<p>)6]
, (7.168)
</p>
<p>where the energy parameter ε is positive and σ is called the Lennard&ndash;Jones diam-
eter. Figure 7.4a illustrates the form of this potential. We note that vLJ(σ) = 0 and
that vLJ takes its minimum value &minus;ε at the distance rmin := 21/6σ .
</p>
<p>It is often more convenient to work with the truncated and shifted Lennard&ndash;
</p>
<p>Jones potential defined by
</p>
<p>vtsLJ(r) =
</p>
<p>{
vLJ(r)&minus; vLJ(rc) if r &lt; rc
0 otherwise.
</p>
<p>(7.169)</p>
<p/>
</div>
<div class="page"><p/>
<p>288 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>-2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>r
</p>
<p>vLJ r
</p>
<p>0 1 2 3 4 5
</p>
<p>a
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>0 2 4 6 8
</p>
<p>r
</p>
<p>1 1
N
</p>
<p>1
gN r
</p>
<p>n 3 0 01
</p>
<p>n 3 0 1
</p>
<p>n 3 0 5
</p>
<p>n 3 1
</p>
<p>b
</p>
<p>Fig. 7.4 a Lennard&ndash;Jones potential and b the radial distribution functions for its truncated and
</p>
<p>shifted version (rc = 2.5σ ) from Monte Carlo simulations at four different densities nσ3 = 0.01,
nσ3 = 0.1, nσ3 = 0.5, and nσ3 = 1. For clarity, the plots for nσ3 = 0.1, nσ3 = 0.5, and nσ3 = 1
are shifted upward by 1, 2, and 3, respectively. The system temperature was T = ε/kB. The system
was taken as a cubic box of volume 8000σ3 under periodic boundary conditions.
</p>
<p>The radial distribution function for vtsLJ(r) with rc = 2.5σ is shown in Fig. 7.4b for a
few values of density. Equation (7.165) is very accurate for the lowest density value
</p>
<p>shown in the figure. We can also see the development of the oscillatory behavior
</p>
<p>with increasing density.
</p>
<p>We note that gN(r) of a hard-sphere fluid captures essential features of gN(r)
of other fluids having a short-ranged repulsive core. This is illustrated in Fig. 7.5,
</p>
<p>in which we compare gN(r) of a hard-sphere fluid against that of a Lennard&ndash;Jones
fluid. At the densities indicated, they look fairly similar with the higher density
</p>
<p>results showing closer resemblance.
</p>
<p>For an open system, fixing a particle at r does not affect the number of particles
</p>
<p>available at r&prime;. Thus, we can simply drop the 1&minus; 1/N factor from (7.161), (7.165),
and (7.167) to obtain the corresponding radial distribution function g(r) for open
systems.
</p>
<p>Before we conclude this section, we note that g(r) is related to the Fourier trans-
form of the so-called static structure factor, which is directly measurable by means
</p>
<p>of neutron or X-ray scattering experiments. Approximate theories for g(r) can also
be developed (For details of these topics, see Chap. 4 of Ref. [8] or Chap. 5 of
</p>
<p>Ref. [9], for example.) As we shall see in Sect. 7.7, a direct link exists between g(r)
and equations of state.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 289
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>r d or r
</p>
<p>0 2 4 6 8
</p>
<p>a
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>0 2 4 6 8
</p>
<p>r d or rb
</p>
<p>Fig. 7.5 Comparison of the radial distribution functions of the hard-sphere fluid (dashed line)
</p>
<p>and the truncated and shifted Lennard&ndash;Jones fluid (solid line) at T = ε/kB. a nd3 = nσ3 = 0.5. b
nd3 = nσ3 = 1. For clarity, the plots for the hard-sphere fluid are shifted upward by one.
</p>
<p>7.4.5 Repulsive Potential
</p>
<p>To construct the density functional by means of (7.148) for a system, in which the
</p>
<p>pair potential is v(r), we must first choose a reference pair potential. How should
we proceed here?
</p>
<p>As we have seen in the previous subsection, g(r) is determined primarily by
the short-ranged repulsive part of the pair potential v(r). This suggest that the λ
dependence of gλ (r) in (7.148) will be small if we take the repulsive part of v(r) as
the reference potential.
</p>
<p>But, how do we separate v(r) into the repulsive part vrep(r) and the rest, which
we refer to as the attractive tail vatt(r)? There is no unique answer to this question,
and merits of each method must be judged by the accuracy of the predictions it
</p>
<p>produces. Here, we mention one particularly useful scheme known as the Weeks-
</p>
<p>Chandler-Andersen (WCA) separation [12]. Taking vLJ(r) as the example, this
scheme separates the potential into
</p>
<p>vrep(r) =
</p>
<p>{
vLJ(r)+ ε if r &le; rmin
0 otherwise
</p>
<p>and vatt(r) =
</p>
<p>{
&minus;ε if r &le; rmin
vLJ(r) otherwise,
</p>
<p>(7.170)
</p>
<p>leading to the graphs in Fig. 7.6.
</p>
<p>Having settled on the reference potential, we move on to the evaluation of the
</p>
<p>excess Helmholtz free energy Fexcint [n,v
rep] of the reference system. A common
</p>
<p>approach for an inhomogeneous system consists of two parts. First, we seek for</p>
<p/>
</div>
<div class="page"><p/>
<p>290 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>-2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>0 1 2 3 4
</p>
<p>r
</p>
<p>vrep
</p>
<p>vatt
</p>
<p>rmin
</p>
<p>Fig. 7.6 WCA separation of the Lennard&ndash;Jones pair potential. WCA Weeks&ndash;Chandler&ndash;Andersen.
</p>
<p>a &ldquo;mapping&rdquo; from a homogeneous reference system to a homogeneous hard-sphere
</p>
<p>(hHS) fluid. This is because properties of a hHS fluid have been studied extensively,
</p>
<p>and a very accurate formula is available for the excess Helmholtz free energy of a
</p>
<p>hard-sphere fluid. Second, we devise a method for incorporating the effect of inho-
</p>
<p>mogeneity.
</p>
<p>The most general mapping from the reference system to a hard-sphere fluid
</p>
<p>would involve two parameters, the hard-sphere diameter d and the density neff of
</p>
<p>the hard-sphere fluid. However, one commonly set n = neff and determine the opti-
mum value of d so as to minimize the error involved in the approximation
</p>
<p>Fexcint [n,v
rep]&asymp; Fexcint [n,vHS] . (7.171)
</p>
<p>The optimum value of d is then a function of T and n in general. However, an
</p>
<p>approximate solution to this optimization problem leads to the expression
</p>
<p>d =
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>[
1&minus; e&minus;βvrep(r)
</p>
<p>]
dr , (7.172)
</p>
<p>which is a function of T only. Equation (7.172) is known as the Barker&ndash;Henderson
</p>
<p>scheme [1, 12] and is derived in Sect. 7.4.7.
</p>
<p>As an illustration, let us consider the truncated and shifted Lennard&ndash;Jones poten-
</p>
<p>tial with rc = 2.5σ . After vrep is determined by the WCA separation, we can compute
d using (7.172). The result is summarized in Fig. 7.7. The T &rarr; 0 limit of d is given
by rmin = 2
</p>
<p>1/6σ . This makes perfect physical sense because βvrep reduces to the
hard-sphere potential with d = rmin in this limit. The observed decrease in d with
increasing T is also very reasonable physically since βvrep becomes less harsh for
higher temperatures.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 291
</p>
<p>0.95
</p>
<p>1
</p>
<p>1.05
</p>
<p>1.1
</p>
<p>1.15
</p>
<p>0 0.5 1 1.5 2
</p>
<p>d
</p>
<p>kBT
</p>
<p>rmin 2
1 6
</p>
<p>Fig. 7.7 Temperature dependence of the hard-sphere diameter as determined by the Barker&ndash;
</p>
<p>Henderson scheme for the truncated and shifted Lennard&ndash;Jones potential (rc = 2.5σ ).
</p>
<p>The simplest approximation to cope with the inhomogeneity is the local density
</p>
<p>approximation (LDA), in which an inhomogeneous system is regarded as a col-
</p>
<p>lection of infinitesimally small homogeneous systems. Within LDA, therefore, we
</p>
<p>write
</p>
<p>Fexcint [n,v
HS]&asymp;
</p>
<p>&int;
</p>
<p>V
f exchHS(n(r))dr , (7.173)
</p>
<p>where f exchHS(n) is the excess Helmholtz free energy per unit volume of a hHS fluid
of density n.
</p>
<p>If the density changes slowly over many particle diameters, LDA is expected to
</p>
<p>perform well. For a system exhibiting more rapid spatial variation of density, how-
</p>
<p>ever, the underlying assumption of LDA does not apply. A common approximation
</p>
<p>scheme is to replace the local density n(r) in (7.173) by the weighted density n(r)
defined by
</p>
<p>n(r) :=
&int;
</p>
<p>V
w(r,r&prime;)n(r&prime;)dr&prime; , (7.174)
</p>
<p>where w(r,r&prime;) is a properly normalized weighting function:
&int;
</p>
<p>V
w(r,r&prime;)dr&prime; = 1 . (7.175)</p>
<p/>
</div>
<div class="page"><p/>
<p>292 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>Various choices for the weighting function are possible, but the resulting approxima-
</p>
<p>tion schemes are collectively called the weighted density approximation (WDA).
</p>
<p>For fluids, w is often chosen as
</p>
<p>w(||r&prime;&minus; r||) =
{
</p>
<p>3
πd4
</p>
<p>(d &minus;||r&prime;&minus; r||) if ||r&prime;&minus; r|| &le; d
0 otherwise.
</p>
<p>(7.176)
</p>
<p>WDA can be developed even for a crystal, which is regarded as a highly inhomoge-
</p>
<p>neous fluid, and provides an accurate estimate for the melting density of hard-sphere
</p>
<p>crystals [4, 5].
</p>
<p>The explicit form of f exehHS can be deduced from an equation of state. The
</p>
<p>Carnahan&ndash;Starling formula [2, 3] is one famous example of highly accurate equa-
</p>
<p>tions of state of hard-sphere fluids. According to this formula,
</p>
<p>βPhHS
n
</p>
<p>=
1+η+η2 &minus;η3
</p>
<p>(1&minus;η)3 , (7.177)
</p>
<p>where
</p>
<p>η :=
π
</p>
<p>6
d3n (7.178)
</p>
<p>is called the packing fraction. Because πd3/6 is the volume of a hard-sphere, η
represents the volume occupied by the spheres per unit volume of the fluid. Spheres
</p>
<p>cannot be packed without leaving any gap among them. So, the maximum packing
</p>
<p>fraction ηmax is less than unity and is given by
</p>
<p>ηmax =
π
</p>
<p>3
&radic;
</p>
<p>2
, (7.179)
</p>
<p>which is realized when the spheres are packed into the face-centered-cubic (fcc)
</p>
<p>structure.
</p>
<p>Exercise 7.9. Confirm (7.179). ///
</p>
<p>To find the expression for the excess free energy of a hard-sphere fluids, we recall
</p>
<p>the Gibbs&ndash;Duhem relation for a constant T process:
</p>
<p>dPhHS = nd&micro;hHS , T const. (7.180)
</p>
<p>Subtracting off the same equation applied to an ideal gas at the same density, we
</p>
<p>find
</p>
<p>dPexchHS = nd&micro;
exc
hHS , T const. (7.181)
</p>
<p>Dividing this equation by dn,
</p>
<p>(
&part;β&micro;exchHS
</p>
<p>&part;n
</p>
<p>)
</p>
<p>T
</p>
<p>=
1
</p>
<p>n
</p>
<p>(
&part;βPexchHS
</p>
<p>&part;n
</p>
<p>)
</p>
<p>T
</p>
<p>. (7.182)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 293
</p>
<p>Now we integrate this equation from some reference state density n0 to the density
</p>
<p>of interest:
</p>
<p>β&micro;exchHS(n)&minus;β&micro;exchHS(n0) =
&int; n
</p>
<p>n0
</p>
<p>1
</p>
<p>n
</p>
<p>(
&part;βPexchHS
</p>
<p>&part;n
</p>
<p>)
</p>
<p>T
</p>
<p>dn =
&int; η
</p>
<p>η0
</p>
<p>1
</p>
<p>η
</p>
<p>(
&part;βPexchHS
</p>
<p>&part;n
</p>
<p>)
</p>
<p>T
</p>
<p>dη .
</p>
<p>(7.183)
</p>
<p>Using the ideal gas equation of state (βPid = n) and (7.177),
</p>
<p>βPexchHS = βPhHS &minus;n = n
4η&minus;2η2
(1&minus;η)3 . (7.184)
</p>
<p>So, (
&part;βPexchHS
</p>
<p>&part;n
</p>
<p>)
</p>
<p>T
</p>
<p>=
4η&minus;2η2
(1&minus;η)3 +η
</p>
<p>d
</p>
<p>dη
</p>
<p>[
4η&minus;2η2
(1&minus;η)3
</p>
<p>]
. (7.185)
</p>
<p>We can now carry out the integration in (7.183) and obtain
</p>
<p>β&micro;exchHS(n)&minus;β&micro;exchHS(n0) =
[
</p>
<p>2
</p>
<p>1&minus;η +
1
</p>
<p>(1&minus;η)2 +
4η&minus;2η2
(1&minus;η)3
</p>
<p>]η
</p>
<p>η0
</p>
<p>. (7.186)
</p>
<p>In the limit of η0 &rarr; 0, the fluid approaches the ideal gas and hence &micro;exchHS(n0)&rarr; 0.
Thus,
</p>
<p>β&micro;exchHS(n) =
2
</p>
<p>1&minus;η +
1
</p>
<p>(1&minus;η)2 +
4η&minus;2η2
(1&minus;η)3 &minus;3 =
</p>
<p>η(3η2 &minus;9η+8)
(1&minus;η)3 .(7.187)
</p>
<p>Recalling that F :=U &minus;T S =&minus;PV +&micro;N, we finally arrive at
</p>
<p>β f exchHS(n) :=
βFexchHS
</p>
<p>V
= n
</p>
<p>η(4&minus;3η)
(1&minus;η)2 . (7.188)
</p>
<p>Even though we introduced a mapping from vrep to the hard-sphere potential,
</p>
<p>this is only for the purpose of computing Fexcint . Thus, we emphasize that the pair
</p>
<p>potential of our reference system is still vrep and vb&minus;va in (7.148) is vatt, not v&minus;vHS.
In various theories of homogeneous fluids with mapping onto hard-spheres, vrep
</p>
<p>still plays an important role, for example, in approximating the radial distribution
</p>
<p>function gλ (r).
</p>
<p>7.4.6 Radial Distribution Function
</p>
<p>The radial distribution function g(r,r&prime;) of an inhomogeneous system is not very well
known even for a hard-sphere fluid. This is not surprising since g(r,r&prime;) depends on
three spatial variables even in the simplest case of a flat interface. Consequently, we
</p>
<p>must adopt a rather drastic approximation for gλ (r,r
&prime;).</p>
<p/>
</div>
<div class="page"><p/>
<p>294 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>For example, we can take over (7.167), without the 1&minus; 1/N factor for an open
system, and write
</p>
<p>gλ (r,r
&prime;)&asymp; θ(||r&prime;&minus; r||&minus;d) . (7.189)
</p>
<p>with d determined by the Barker&ndash;Henderson scheme. Alternative approximation is
</p>
<p>based on (7.165) and is given by
</p>
<p>gλ (r,r
&prime;)&asymp; e&minus;βvrep(||r&prime;&minus;r||) , (7.190)
</p>
<p>where we recall vrep = vλ=0 is the pair potential of the reference system. The sim-
plest approximation is
</p>
<p>gλ (r,r
&prime;)&asymp; 1 , (7.191)
</p>
<p>which is exact only for an ideal gas.
</p>
<p>We are now in position to write down approximate density functionals. The sim-
</p>
<p>plest density functional is based on LDA and (7.191), and is given by
</p>
<p>Ω [n] = kBT
&int;
</p>
<p>V
n(r)[lnΛ 3n(r)&minus;1]dr+
</p>
<p>&int;
</p>
<p>V
f exchHS(n(r))dr&minus;
</p>
<p>&int;
</p>
<p>V
n(r)[&micro;&minus;ψ(r)]dr
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)vatt(||r&prime;&minus; r||)drdr&prime; , (7.192)
</p>
<p>where we dropped the reference to the pair potential v from Ω [n,v].
Equation (7.192) motivates the following definition for the grand potential den-
</p>
<p>sity:
</p>
<p>χ(r,n] := kBT n(r)[lnΛ
3n(r)&minus;1]+ f exchHS(n(r))&minus;n(r)[&micro;&minus;ψ(r)]
</p>
<p>+
1
</p>
<p>2
n(r)
</p>
<p>&int;
</p>
<p>V
n(r&prime;)vatt(||r&prime;&minus; r||)dr&prime; , (7.193)
</p>
<p>in terms of which we may write
</p>
<p>Ω [n] =
&int;
</p>
<p>V
χ(r,n]dr . (7.194)
</p>
<p>We have already made use of (7.194) in Sect. 6.9. A few illustrative applications of
</p>
<p>(7.192) will be given in Sects. 7.5 and 7.6.
</p>
<p>7.4.7 &dagger;Barker&ndash;Henderson Scheme
</p>
<p>The only difference between Fexcint [n,v
rep] and Fexcint [n,v
</p>
<p>HS] is in the pair potentials.
Thus, one possible approach for minimizing the error associated with (7.171) may
</p>
<p>be to expand Fexcint [n,v
rep] into the functional Taylor series around vrep = vHS and then
</p>
<p>choose d so that the first-order term becomes identically zero. If this can be done,
</p>
<p>the error will be in the higher order terms. Glancing at Fig. 7.6, we see immediately
</p>
<p>that vrep and vHS are very different regardless of how we choose d. In fact, for r &lt; d,</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Construction of a Density Functional 295
</p>
<p>-0.8
</p>
<p>-0.6
</p>
<p>-0.4
</p>
<p>-0.2
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0 0.5 1 1.5 2
</p>
<p>e
r
</p>
<p>r
</p>
<p>Fig. 7.8 The blip function. We set σ = d and T = ε/kB.
</p>
<p>their difference is infinitely large. This implies that the higher order terms probably
</p>
<p>cannot be neglected.
</p>
<p>What we need is a better way to characterize the difference between vrep and vHS.
</p>
<p>For this purpose, we define
</p>
<p>e(r) := e&minus;βv(r) . (7.195)
</p>
<p>For the pair potentials at hand,
</p>
<p>erep(r) = e&minus;βv
rep(r) and eHS(r) = e&minus;βv
</p>
<p>HS(r) . (7.196)
</p>
<p>The difference between them
</p>
<p>∆e(r) := erep(r)&minus; eHS(r) (7.197)
</p>
<p>is known as the blip function. The origin of this term is revealed in Fig. 7.8. Unlike
</p>
<p>vrep &minus; vHS, ∆e remains finite for all values of r.
Now, we proceed to evaluate the functional derivative of Fexcint [n,v
</p>
<p>rep] with respect
to erep. Since F idint is independent of v
</p>
<p>rep(r),
</p>
<p>δFexcint [n,v
rep]
</p>
<p>δvrep(r,r&prime;)
=
</p>
<p>δFint[n,v
rep]
</p>
<p>δvrep(r,r&prime;)
. (7.198)
</p>
<p>From (7.76) and (7.144), we observe that
</p>
<p>Fint[n,v] =Ω [n,v]+
&int;
</p>
<p>V
[&micro;&minus;ψ(r)]n(r)dr . (7.199)</p>
<p/>
</div>
<div class="page"><p/>
<p>296 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>Recalling (7.134), we find
</p>
<p>δFexcint [n,v
rep]
</p>
<p>δvrep(r,r&prime;)
=
</p>
<p>1
</p>
<p>2
n(2)(r,r&prime;) , (7.200)
</p>
<p>Note that we replaced the distance r between a pair of particles by their coordinates
</p>
<p>r and r&prime;. It follows from (7.41) that
</p>
<p>δFexcint [n,v
rep] =
</p>
<p>1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(2)(r,r&prime;)δvrep(r,r&prime;)drdr&prime; . (7.201)
</p>
<p>From (7.196), we have
</p>
<p>vrep(r,r&prime;) =&minus;kBT lnerep(r,r&prime;) , (7.202)
</p>
<p>and hence
</p>
<p>δvrep(r,r&prime;) =&minus; kBT
erep(r,r&prime;)
</p>
<p>δerep(r,r&prime;) . (7.203)
</p>
<p>Using this expression in (7.201), we have
</p>
<p>δFexcint [n,v
rep] =&minus;kBT
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
</p>
<p>n(2)(r,r&prime;)
erep(r,r&prime;)
</p>
<p>δerep(r,r&prime;)drdr&prime; . (7.204)
</p>
<p>We now change the variables from (r,r&prime;) to (r,R), where R := r&prime;&minus;r. The pair poten-
tial, by assumption, is a function of R := ||R||. For a homogeneous and isotropic
system, the same applies to n(2) as well. Thus,
</p>
<p>δFexcint [n,v
rep] = &minus;kBT
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
</p>
<p>n(2)(R)
</p>
<p>erep(R)
δerep(R)drdR
</p>
<p>= &minus;kBT
2
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
</p>
<p>n(2)(R)
</p>
<p>erep(R)
δerep(R)dR , (7.205)
</p>
<p>where we note that the determinant of the Jacobian matrix for the change of vari-
</p>
<p>ables from (r,r&prime;) to (r,R) is unity. Using r in place of R, we arrive at45
</p>
<p>δFexcint [n,v
rep]
</p>
<p>δerep(r)
=&minus;kBT
</p>
<p>2
V
</p>
<p>n(2)(r)
</p>
<p>erep(r)
=&minus;kBT
</p>
<p>2
n2V
</p>
<p>grep(r)
</p>
<p>erep(r)
. (7.206)
</p>
<p>Evaluating this expression for erep(r)&equiv; eHS(r),
</p>
<p>δFexcint [n,v
rep]
</p>
<p>δerep(r)
</p>
<p>∣∣∣∣
vrep=vHS
</p>
<p>=&minus;kBT
2
</p>
<p>n2V yHS(r) , (7.207)
</p>
<p>where we defined
</p>
<p>y(r) :=
g(r)
</p>
<p>e(r)
. (7.208)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Hard-Sphere Fluid Under Gravity 297
</p>
<p>We observe that y(r) may be regarded as the correction factor to the low-density
approximation (7.165) without the 1&minus;1/N factor. Thus, the functional Taylor series
expansion of Fexcint [n,v
</p>
<p>rep] around erep(r)&equiv; eHS(r) is given by
</p>
<p>Fexcint [n,v
rep] = Fexcint [n,v
</p>
<p>HS]&minus; 1
2
</p>
<p>kBT n
2V
</p>
<p>&int;
</p>
<p>V
yHS(r)∆e(r)dr+h.o. (7.209)
</p>
<p>The desired equation for d is therefore,
</p>
<p>&int;
</p>
<p>V
yHS(r)∆e(r)dr= 4π
</p>
<p>&int; &infin;
</p>
<p>0
yHS(r)∆e(r)r2dr = 0 . (7.210)
</p>
<p>From Fig. 7.8, we see that the blip function is nonzero only around r = d,
suggesting that r2yHS(r) in (7.210) may be replaced by d2yHS(d) without signifi-
cantly affecting the value of the integral. (We note that y(r) is a continuous function
of r even for a discontinuous v(r), such as the hard-sphere pair potential. For an
explicit demonstration of this fact, see Sect. 5.3 of Ref. [9].) With this approxima-
</p>
<p>tion, (7.210) reduces to
</p>
<p>&int; &infin;
</p>
<p>0
∆e(r)dr =
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>[
erep(r)&minus; eHS(r)
</p>
<p>]
dr = 0 , (7.211)
</p>
<p>which may be rewritten as
</p>
<p>&int; &infin;
</p>
<p>0
[erep(r)&minus;1]dr =
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>[
eHS(r)&minus;1
</p>
<p>]
dr =&minus;d . (7.212)
</p>
<p>But, this is just (7.172).
</p>
<p>7.5 Hard-Sphere Fluid Under Gravity
</p>
<p>As the first example illustrating the use of (7.192), let us look at something simple:
</p>
<p>a hard-sphere fluid in a container under gravity. In this case, we have
</p>
<p>vatt(r)&equiv; 0 . (7.213)
</p>
<p>Taking the z-axis vertically upward from the bottom of the container, the external
</p>
<p>field due to gravity is given by
</p>
<p>ψ(r) = mwgz , (7.214)
</p>
<p>where mw and g are the mass of a hard-sphere and the gravitational acceleration,
</p>
<p>respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>298 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>Using these expressions in (7.192), we see that
</p>
<p>Ω [n] = kBT
&int;
</p>
<p>V
n(r)
</p>
<p>[
lnΛ 3n(r)&minus;1
</p>
<p>]
dr+
</p>
<p>&int;
</p>
<p>V
f exchHS(n(r))dr+
</p>
<p>&int;
</p>
<p>V
n(r)(mwgz&minus;&micro;)dr .
</p>
<p>(7.215)
</p>
<p>The equilibrium density profile is determined by (7.47), which now reads
</p>
<p>δΩ
</p>
<p>δn(r)
= kBT lnΛ
</p>
<p>3n(r)+&micro;exchHS(n(r))+(mwgz&minus;&micro;) = 0 . (7.216)
</p>
<p>Since kBT lnΛ 3n(z) is the ideal gas contribution to the chemical potential, this equa-
tion may be rewritten as
</p>
<p>&micro; = kBT lnΛ
3n(r)+&micro;exehHS(n(r))+mwgz = &micro;hHS(n(r))+mwgz , (7.217)
</p>
<p>indicating that the intrinsic part of the chemical potential and the external field,
</p>
<p>added together, must be constant throughout the system in equilibrium.
</p>
<p>Equation (7.216) indicates that n(r) depends only on z. We can readily solve the
equation for z (but not for n(z)):
</p>
<p>z =
1
</p>
<p>mwg
</p>
<p>[
&micro;&minus;&micro;exchHS(n(z))&minus; kBT lnΛ 3n(z)
</p>
<p>]
. (7.218)
</p>
<p>If we denote the density at z = 0 by n0,
</p>
<p>0 =
1
</p>
<p>mwg
</p>
<p>[
&micro;&minus;&micro;exchHS(n0)&minus; kBT lnΛ 3n0
</p>
<p>]
, (7.219)
</p>
<p>which can be subtracted from (7.218) to give
</p>
<p>z =
1
</p>
<p>mwg
</p>
<p>[
&micro;exchHS(n0)&minus;&micro;exchHS(n(z))+ kBT ln
</p>
<p>n0
</p>
<p>n(z)
</p>
<p>]
. (7.220)
</p>
<p>With n0 set to the maximum possible value of
&radic;
</p>
<p>2/d3, which corresponds to the
hard-spheres packed into the fcc structure, (7.220) leads to the density profile shown
</p>
<p>in Fig. 7.9. Despite its appearance, Fig. 7.9 does not imply vapor&ndash;liquid coexistence
</p>
<p>for the hard-sphere fluid. The attractive part of the pair potential is needed for such
</p>
<p>a phase behavior.
</p>
<p>7.6 Vapor&ndash;Liquid Coexistence
</p>
<p>As a less trivial example, let us examine vapor&ndash;liquid equilibrium in a truncated
</p>
<p>and shifted Lennard&ndash;Jones fluid. For simplicity, we set the external field to zero in
</p>
<p>(7.192). For this potential, WCA separation gives</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Vapor&ndash;Liquid Coexistence 299
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>1.2
</p>
<p>1.4
</p>
<p>1.6
</p>
<p>0 20 40 60 80 100 120 140
</p>
<p>n
d
</p>
<p>3
</p>
<p>mwgz
</p>
<p>Fig. 7.9 Equilibrium density profile of hard-sphere fluids under gravity. The graph was obtained
</p>
<p>by assuming maximum packing fraction at z = 0.
</p>
<p>vatt(r) =
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>&minus;ε&minus; vLJ(rc) if r &le; rmin
vLJ(r)&minus; vLJ(rc) if rmin &lt; r &le; rc
0 otherwise.
</p>
<p>(7.221)
</p>
<p>The repulsive potential vrep(r) is still given by (7.170). In fact, vrep(r) for r &le; rmin is
vtsLJ(r) minus its minimum value at rmin = 2
</p>
<p>1/6σ , both of which received the same
upward shift by vLJ(rc) when defining v
</p>
<p>tsLJ(r) in terms of vLJ(r). In our numerical
computation below, we set rc = 2.5σ .
</p>
<p>7.6.1 Phase Diagram
</p>
<p>It is convenient to work with the Helmholtz free energy. Since F = Ω + &micro;N and
ψ(r)&equiv; 0, (7.192) gives
</p>
<p>F [n] = kBT
&int;
</p>
<p>V
n(r)[lnΛ 3n(r)&minus;1]dr+
</p>
<p>&int;
</p>
<p>V
f exchHS(n(r))dr
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(r)n(r&prime;)vatt(||r&prime;&minus; r||)drdr&prime; . (7.222)</p>
<p/>
</div>
<div class="page"><p/>
<p>300 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>-5
</p>
<p>-4
</p>
<p>-3
</p>
<p>-2
</p>
<p>0 20 40 60 80 100
</p>
<p>V 3
</p>
<p>F
</p>
<p>common tangent
</p>
<p>Fig. 7.10 F versus V plot at T = 0.8ε/kB.
</p>
<p>The Helmholtz free energy of a homogeneous phase follows from (7.222) simply
</p>
<p>by setting n(r) to a constant n:
</p>
<p>f :=
F
</p>
<p>V
= kBT n[lnΛ
</p>
<p>3n&minus;1]+ f exchHS(n)+
1
</p>
<p>2
αattn2 , (7.223)
</p>
<p>which is then a fundamental equation of the homogeneous fluid. Here, αatt is a
constant defined by
</p>
<p>αatt :=
1
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
vatt(||r&prime;&minus; r||)drdr&prime; = 1
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
vatt(R)drdR=
</p>
<p>&int;
</p>
<p>V
vatt(R)dR , (7.224)
</p>
<p>where we defined R := r&prime;&minus; r and R := ||R||. We also made use of the fact that the
determinant of the Jacobian matrix for the change of variables from (r,r&prime;) to (r,R)
is unity. Given the fundamental equation (7.223), we can obtain expressions for P
</p>
<p>and &micro; :
</p>
<p>P = kBT n+P
exc
hHS +
</p>
<p>1
</p>
<p>2
αattn2 (7.225)
</p>
<p>and
</p>
<p>&micro; = kBT lnΛ
3n+&micro;exchHS +α
</p>
<p>attn . (7.226)
</p>
<p>Exercise 7.10. Verify (7.225) and (7.226). ///
</p>
<p>Figure 7.10 shows the dependence of F := F/N on V := V/N at T = 0.8ε/kB.
At this temperature, a portion of the F versus V plot is concave down and the curve
</p>
<p>has a common tangent. We recall from Sect. 2.16.1 that we can read off P and &micro; of a
fluid by drawing a tangent line to the F versus V plot. In particular, its slope and the
</p>
<p>intercept on the F-axis are, respectively, &minus;P and &micro; of the fluid at the point of tangent.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Vapor&ndash;Liquid Coexistence 301
</p>
<p>Therefore, the existence of a common tangent implies the equilibrium between two
</p>
<p>phases, vapor and liquid in this case. The precise values for the densities of the
</p>
<p>coexisting phases are found as the nontrivial solution (for which nl 	= vv) of the
coupled equations:
</p>
<p>P(T,nv) = P(T,nl) and &micro;(T,nv) = &micro;(T,nl), (7.227)
</p>
<p>where the superscripts v and l refer to vapor and liquid phases, respectively. The
</p>
<p>locus of the coexisting bulk phase densities (traced by varying T ) is called a binodal
</p>
<p>line.
</p>
<p>The onset of instability is determined by
</p>
<p>(
&part;P
</p>
<p>&part;n
</p>
<p>)
</p>
<p>T
</p>
<p>= 0 , (7.228)
</p>
<p>the locus of which defines the spinodal line. A bulk phase inside the region enclosed
</p>
<p>by this line is unstable with respect to an infinitesimal density fluctuation and under-
</p>
<p>goes phase separation by a mechanism called spinodal decomposition. (See 5.2.2.)
</p>
<p>Metastable phases reside in the regions between the binodal and spinodal lines with
</p>
<p>the lower density part corresponding to supersaturated vapor and the higher density
</p>
<p>part to superheated (or stretched) liquid. The mechanism of phase separation of a
</p>
<p>metastable phase is nucleation.
</p>
<p>Finally, (7.228) and (
&part; 2P
</p>
<p>&part;n2
</p>
<p>)
</p>
<p>T
</p>
<p>= 0 (7.229)
</p>
<p>determine the critical point.
</p>
<p>Results of these computations are shown in Fig. 7.11 and are compared against
</p>
<p>the results of the Gibbs ensemble simulations [11]. We see that our theory predicts
</p>
<p>qualitatively correct behavior, though it fails at a quantitative level especially for the
</p>
<p>liquid phase densities. A further refinement is expected to be possible by using a
</p>
<p>more accurate mapping scheme between the reference system (with vrep) and hard-
</p>
<p>sphere fluids as well as a better approximation for gλ (r).
The critical point is not particularly well predicted by DFT. This is a generic
</p>
<p>feature of DFT, which does not adequately account for the effect of fluctuations that
</p>
<p>become increasingly important near the critical point. In fact, we see shortly that
</p>
<p>DFT is a form of a mean-field approximation discussed in Sect. 5.2.
</p>
<p>7.6.2 Interfacial Properties
</p>
<p>For our choice of the density functional, (7.47) leads to
</p>
<p>&micro; = kBT lnΛ
3n(r)+&micro;exchHS(n(r))+
</p>
<p>&int;
</p>
<p>V
n(r&prime;)vatt(||r&prime;&minus; r||)dr&prime; . (7.230)</p>
<p/>
</div>
<div class="page"><p/>
<p>302 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>1.2
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>k B
T
</p>
<p>n 3
</p>
<p>Fig. 7.11 Phase diagram of the truncated and shifted Lennard&ndash;Jones fluid (rc = 2.5σ ). The bin-
odal (solid line) and the spinodal (dashed line) lines, both from the density functional theory. The
</p>
<p>coexisting densities (⊙) and the critical point (�), both from Gibbs ensemble simulation involving
the total of 1000 particles.
</p>
<p>For given T and &micro; , this equation determines the equilibrium density profile neq(r).
Comparing (7.217) and (7.230), we see that the last term of (7.230) represents the
</p>
<p>effective field at r that is being generated by the particles in the system. Thus, the
</p>
<p>density functional we constructed, in essence, is a fluid-phase version of the mean-
</p>
<p>field approximation we saw in Sect. 5.2.
</p>
<p>Figure 7.12 exhibits equilibrium density profiles at the vapor&ndash;liquid coexistence
</p>
<p>for a few values of T . Using this information, we computed the surface tension γ&infin; for
a range of temperatures using (7.193) and (6.92). The results are shown in Fig. 7.13
</p>
<p>and is compared against the predictions of Monte Carlo simulations. Despite the
</p>
<p>failure of DFT in accurately predicting the critical point and the liquid phase den-
</p>
<p>sities at coexistence, the DFT predictions of γ&infin; are in reasonably good agreement
with the simulation results. In both cases, γ&infin; decreases monotonically and eventu-
ally vanishes at the critical point where the distinction between the vapor and the
</p>
<p>liquid phases disappears.
</p>
<p>The location of the surface of tension is determined by (6.98) and is shown by the
</p>
<p>curve labeled as zs in Fig. 7.12. The intersection between the curve and the density
</p>
<p>profile at a given temperature gives zs at that T . Similarly for other curves labeled as
</p>
<p>z95 and z05. The former denotes the position at which the density n
eq(z) decreases
</p>
<p>from its bulk liquid value nl by 5% of nl &minus;nv. Likewise, z05 is the position at which
neq(z) decreases to nv +0.05(nl &minus;nv).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Vapor&ndash;Liquid Coexistence 303
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>-20 -15 -10 -5 0 5 10 15 20
</p>
<p>n
eq
</p>
<p>3
</p>
<p>z
</p>
<p>z95
</p>
<p>zs
</p>
<p>z05
</p>
<p>kBT 0 6
</p>
<p>kBT 0 8
</p>
<p>kBT 1 0
</p>
<p>kBT 1 1
</p>
<p>Fig. 7.12 Density profiles across flat interfaces for the truncated and shifted Lennard&ndash;Jones fluid
</p>
<p>(rc = 2.5σ ).
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>0.5 0.6 0.7 0.8 0.9 1 1.1 1.2
</p>
<p>2
</p>
<p>kBT
</p>
<p>DFT
</p>
<p>MC
</p>
<p>Fig. 7.13 Surface tension versus temperature for the truncated and shifted Lennard&ndash;Jones fluid
</p>
<p>(rc = 2.5σ ). Monte Carlo (MC) simulation employed 8000 particles in a rectangular box (dimen-
sion 20σ &times;20σ &times;80σ ) under periodic boundary conditions.
</p>
<p>The thickness of the interfacial region may be conveniently characterized by
</p>
<p>z05 &minus; z95. Our DFT prediction indicates that z05 &minus; z95 = 2.11σ at kBT/ε = 0.5.
The interface becomes more diffuse with increasing temperature. For example,
</p>
<p>z05 &minus; z95 = 5.22σ at kBT/ε = 1.0.</p>
<p/>
</div>
<div class="page"><p/>
<p>304 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>0.5 0.6 0.7 0.8 0.9 1 1.1 1.2
</p>
<p>δ
&infin;
/σ
</p>
<p>kBT/ε
</p>
<p>DFT
</p>
<p>MC
</p>
<p>Fig. 7.14 Tolman length versus temperature for the truncated and shifted Lennard&ndash;Jones fluid
</p>
<p>(rc = 2.5σ ). Monte Carlo (MC) simulation employed 8000 particles in a rectangular box (dimen-
sion 20σ &times;20σ &times;80σ ) under periodic boundary conditions.
</p>
<p>At low temperatures, z95 &lt; zs &lt; z05 in agreement with Gibbs&rsquo;s conclusion that the
surface of tension should be located within the interfacial region. At temperatures
</p>
<p>very close to the critical temperature, the interface becomes very diffuse and the
</p>
<p>arbitrary cut-off at z95 does not mark the end of the interfacial region. At kBT/ε �
1.14, for example, zs &lt; z95.
</p>
<p>According to DFT, the Tolman length δ&infin; is 0.56σ at kBT/ε = 0.5 and increases
with increasing T . At kBT/ε = 1 for example, δ&infin; = 1.6σ . As shown in Fig. 7.14,
the same trend is observed also in Monte Carlo simulations. However, the DFT
</p>
<p>predictions are considerably larger than the results from the simulations. In this
</p>
<p>regard, we note that a recent very-large-scale molecular dynamics study casts some
</p>
<p>doubts on the ability of DFT and small-scale simulations, such as the one included
</p>
<p>in Fig. 7.14, to accurately predict δ&infin; [10].
</p>
<p>7.7 &Dagger;Equations of State from the Radial Distribution Function
</p>
<p>In this section, we shall establish a connection between thermodynamic properties of
</p>
<p>a homogeneous system and the radial distribution function. Since we are interested
</p>
<p>only in homogeneous systems, the external field is set to zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 &Dagger;Equations of State from the Radial Distribution Function 305
</p>
<p>7.7.1 &Dagger;Compressibility Equation of State
</p>
<p>Upon integration over the volume V of the system, (7.51) yields
</p>
<p>&int;
</p>
<p>V
n(r)dr=
</p>
<p>&lang;
N
</p>
<p>&sum;
i=1
</p>
<p>&int;
</p>
<p>V
δ (r&minus; ri)dr
</p>
<p>&rang;
=
</p>
<p>&lang;
N
</p>
<p>&sum;
i=1
</p>
<p>1
</p>
<p>&rang;
= 〈N〉 . (7.231)
</p>
<p>Similarly, integration of n(2) defined by (7.134) gives
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
n(2)(r,r&prime;)drdr&prime; =
</p>
<p>&lang;
N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
δ (r&minus; ri)δ (r&prime;&minus; r j)drdr&prime;
</p>
<p>&rang;
= 〈N(N &minus;1)〉 .
</p>
<p>(7.232)
</p>
<p>Combining these two equations,
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
[n(2)(r,r&prime;)&minus;n(r)n(r&prime;)]drdr&prime; = 〈N2〉&minus;〈N〉&minus;〈N〉2 . (7.233)
</p>
<p>In a homogeneous system, n(r) = n(r&prime;) =: n is a constant. In addition, our choice
for the origin of the coordinate system should be immaterial, leading to
</p>
<p>n(2)(r,r&prime;) = n(2)(0,R) = n2g(0,R) = n2g(R) , (7.234)
</p>
<p>where R := r&prime;&minus; r and we dropped the explicit reference to 0. Using r and R as the
new integration variables, we rewrite the left-hand side of (7.233) as
</p>
<p>n2
&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
[g(R)&minus;1]drdR= n2V
</p>
<p>&int;
</p>
<p>V
[g(R)&minus;1]dR . (7.235)
</p>
<p>On the other hand, (4.155) gives
</p>
<p>〈N2〉&minus;〈N〉&minus;〈N〉2 = 〈N〉(kBT nκT &minus;1) . (7.236)
</p>
<p>Thus, noting that 〈N〉= nV , we arrive at
</p>
<p>kBT nκT = 1+n
&int;
</p>
<p>V
[g(r)&minus;1]dr , (7.237)
</p>
<p>which is known as the compressibility equation of state. For an isotropic system,
</p>
<p>g(r) = g(r). For an ideal gas, g(r) = 1. Thus,
</p>
<p>kBT nκT = 1 , (7.238)
</p>
<p>the validity of which can easily be checked using the ideal gas equation of state.</p>
<p/>
</div>
<div class="page"><p/>
<p>306 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>7.7.2 &Dagger;Virial Equation of State
</p>
<p>Under the assumption of pairwise additivity of the intermolecular potential, we
</p>
<p>obtained a statistical mechanical expression for pressure in a canonical ensemble
</p>
<p>in Sect. 3.14.1. It is possible to rewrite our result (3.201) in terms of gN .
</p>
<p>For this purpose, we note that
</p>
<p>&lang;
N
</p>
<p>&sum;
i=1
</p>
<p>N
</p>
<p>&sum;
&prime;
</p>
<p>j=1
</p>
<p>dv(ri j)
</p>
<p>dri j
ri j
</p>
<p>&rang;
</p>
<p>N
</p>
<p>= N(N &minus;1)
&lang;
</p>
<p>dv(r12)
</p>
<p>dr12
r12
</p>
<p>&rang;
</p>
<p>N
</p>
<p>, (7.239)
</p>
<p>where we introduced the subscript N to indicate ensemble averages in the canonical
</p>
<p>ensemble. But, in the expression
</p>
<p>&lang;
dv(r12)
</p>
<p>dr12
r12
</p>
<p>&rang;
</p>
<p>N
</p>
<p>=
1
</p>
<p>Z
</p>
<p>1
</p>
<p>Λ 3NN!
</p>
<p>&int;
dv(r12)
</p>
<p>dr12
r12
</p>
<p>[&int;
e&minus;βφ(r
</p>
<p>N)dr3, . . . ,drN
</p>
<p>]
dr1dr2 ,
</p>
<p>(7.240)
</p>
<p>r1 and r2 are just the integration variables that can be replaced, respectively, by r
</p>
<p>and r&prime; without affecting the value of the integral. When the resulting expression is
compared against (7.154) with ψ set to zero, we find
</p>
<p>&lang;
dv(r12)
</p>
<p>dr12
r12
</p>
<p>&rang;
</p>
<p>N
</p>
<p>=
&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
</p>
<p>dv(R)
</p>
<p>dR
R〈δ (r&minus; r1)δ (r&prime;&minus; r2)〉Ndrdr&prime; . (7.241)
</p>
<p>Recalling (7.155) and (7.158) with nN(r) = nN(r
&prime;) replaced by n = N/V , we obtain
</p>
<p>N(N &minus;1)
&lang;
</p>
<p>dv(r12)
</p>
<p>dr12
r12
</p>
<p>&rang;
</p>
<p>N
</p>
<p>= n2
&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
</p>
<p>dv(R)
</p>
<p>dR
RgN(r,r
</p>
<p>&prime;)drdr&prime;
</p>
<p>= n2V
&int;
</p>
<p>V
</p>
<p>dv(R)
</p>
<p>dR
RgN(R)dR . (7.242)
</p>
<p>With this expression, (3.201) becomes
</p>
<p>P = Pid &minus; 1
6
</p>
<p>n2
&int;
</p>
<p>V
</p>
<p>dv(r)
</p>
<p>dr
rgN(r)dr , (7.243)
</p>
<p>which is referred to as the virial equation of state. If v(r)&equiv; 0, this equation reduces
to the ideal gas equation of state.
</p>
<p>7.8 Frequently Used Symbols
</p>
<p>F [u] , a functional F of a function u(x).
δF/δu(x) , functional derivative.
Trcl , classical trace.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.8 Frequently Used Symbols 307
</p>
<p>d , hard-sphere diameter.
</p>
<p>f , F/V .
g(r,r&prime;) , radial distribution function in a grand canonical ensemble.
gN(r,r
</p>
<p>&prime;) , radial distribution function in a canonical ensemble.
kB , Boltzmann constant, 1.3806&times;10&minus;23 J/K.
n(r) density profile in a grand canonical ensemble.
nN(r) density profile in a canonical ensemble.
neq(r) , equilibrium density profile.
n(2)(r,r&prime;) , pair distribution function in a grand canonical ensemble.
</p>
<p>n
(2)
N (r,r
</p>
<p>&prime;) , pair distribution function in a canonical ensemble.
n̂(r,rN) , density operator.
pi , linear momentum of the ith particle.
</p>
<p>rc , cut-off radius for the truncated and shifted Lennard&ndash;Jones potential.
</p>
<p>rmin , 2
1/6σ , the distance at which vLJ(r) takes its minimum value &minus;ε .
</p>
<p>ri , position vector of the ith particle.
</p>
<p>v(r,r&prime;) , pair potential, that is, the potential energy due to interaction between two
particles, one at r and the other at r&prime;.
vatt(r) , attractive part of v(r).
vrep(r) , repulsive part of v(r).
vHS(r) , hard-sphere potential.
vLJ(r) , Lennard&ndash;Jones potential.
vtsLJ(r) , truncated and shifted Lennard&ndash;Jones potential.
</p>
<p>Fint , intrinsic Helmholtz free energy.
</p>
<p>Fexcint , intrinsic excess Helmholtz free energy.
</p>
<p>F idint , intrinsic Helmholtz free energy of an ideal gas.
</p>
<p>N , the number of particles.
</p>
<p>P , pressure.
</p>
<p>PhHS , pressure of a homogeneous hard-sphere fluid.
</p>
<p>PexchHS , excess pressure of a homogeneous hard-sphere fluid.
</p>
<p>T , absolute temperature.
</p>
<p>V , volume.
</p>
<p>Xi , the ith additional variable needed to specify the state of a system that is not in
</p>
<p>equilibrium.
</p>
<p>β , 1/kBT .
ε , energy parameter of the Lennard&ndash;Jones potential.
η , packing fraction.
&micro; , chemical potential.
&micro;hHS , chemical potential of a homogeneous hard-sphere fluid.
&micro;exchHS , excess chemical potential of a homogeneous hard-sphere fluid.
ρ , statistical weight.
ρeq , statistical weight at equilibrium.
σ , Lennard&ndash;Jones diameter.
φ(rN) , potential energy due to intermolecular interaction.
ψ(r) , external field.</p>
<p/>
</div>
<div class="page"><p/>
<p>308 7 Statistical Mechanics of Inhomogeneous Fluids
</p>
<p>∆e(r) , blip function.
Λ , thermal wavelength.
Ξ , grand canonical partition function.
Ω , grand potential.
Ωint , intrinsic grand potential.
</p>
<p>References and Further Reading
</p>
<p>1. Barker J A, Henderson D (1967) Perturbation theory and equation of state for fluids. II. A
</p>
<p>successful theory of liquids. J. Chem. Phys. 47:4714&ndash;4721
</p>
<p>2. Carnahan N F, Starling K E (1969) Equation of state for nonattracting rigid spheres. J. Chem.
</p>
<p>Phys. 51:635&ndash;636
</p>
<p>3. Carnahan N F, Starling K E (1970) Thermodynamic properties of a rigid-sphere fluid. J. Chem.
</p>
<p>Phys. 53:600&ndash;603
</p>
<p>4. Curtin W A, Ashcroft N W (1985) Weighted-density-functional theory of inhomogeneous
</p>
<p>liquids and the freezing transition. Phys. Rev. A 32:2909&ndash;2919
</p>
<p>5. Denton A R, Ashcroft N W (1989) Modified weighted-density-functional theory of nonuni-
</p>
<p>form classical liquids. Phys. Rev. A 39:4701&ndash;4708
</p>
<p>6. Español P, Löwen H (2009) Derivation of dynamical density functional theory using the pro-
</p>
<p>jection operator technique. J. Chem. Phys. 131:244101
</p>
<p>7. Evans R (1979) The nature of the liquid&ndash;vapor interface and other topics in the statistical
</p>
<p>mechanics of non-uniform, classical fluids. Adv. Phys. 28:143&ndash;200
</p>
<p>8. Goodstein D L (1985) States of matter. Dover, New York
</p>
<p>An insightful discussion on the theory of liquids is found in Chap. 4.
</p>
<p>9. Hansen J-P, McDonald I R (1986) Theory of Simple Liquids, 2nd edn. Academic Press, San
</p>
<p>Diego
</p>
<p>An excellent resources on the theory of liquids. Density functional theory is discussed in
</p>
<p>Chap. 6.
</p>
<p>10. Lei Y A, Bykov T, Yoo S, Zeng X C (2005) The Tolman length: Is it positive or negative? J.
</p>
<p>Am. Chem. Soc. 127:15346&ndash;15347
</p>
<p>11. Panagiotopoulos A Z, Suter, U W, Reid RC (1986) Phase diagrams of nonideal fluid mixtures
</p>
<p>from Monte Carlo simulation. Ind. Eng. Chem. Fundam. 25:525&ndash;535
</p>
<p>12. Weeks J D, Chandler D, Andersen H C (1971) Role of repulsive forces in determining the
</p>
<p>equilibrium structure of simple liquids. J. Chem. Phys. 54:5237&ndash;5247</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Quantum Formulation
</p>
<p>In this chapter, we present the mathematical formalism used in quantum mechanics
</p>
<p>first and then derive expressions for canonical and microcanonical partition func-
</p>
<p>tions for quantum mechanical systems. This will help you develop familiarity with
</p>
<p>the basic ideas of quantum mechanics and the bra&ndash;ket notation you may encounter
</p>
<p>when consulting more advanced textbooks on statistical mechanics.
</p>
<p>The important conclusion of this chapter is that (4.24) and (4.72) we obtained
</p>
<p>by means of classical statistical mechanics remain applicable in quantum statistical
</p>
<p>mechanics as well. Thus, the distinction between classical and quantum mechani-
</p>
<p>cal versions of statistical mechanics stems from the explicit expressions for Ω(E)
these mechanics predict and from the manner in which a given system populates the
</p>
<p>microstates accessible to it.
</p>
<p>Many optional sections are included to provide explicit derivations of several
</p>
<p>key results from quantum mechanics we have already used in earlier chapters, but
</p>
<p>should probably be omitted upon the first reading. Keeping with our most immedi-
</p>
<p>ate goals, we will not concern ourselves with experimental findings that forced the
</p>
<p>radical departure from classical mechanics and the eventual formulation of quantum
</p>
<p>mechanics in the early twentieth century. Interested readers can find these accounts
</p>
<p>in Refs. [7, 9] as well as in earlier chapters of many textbooks on quantum mechan-
</p>
<p>ics.
</p>
<p>8.1 Vector Space
</p>
<p>In classical mechanics, a microstate of a system with f mechanical degrees of free-
</p>
<p>dom is specified by 2 f variables (q f , p f ). In quantum mechanics, a microstate of a
system is specified by a vector in a complex vector space. In this section, we spend
</p>
<p>some time familiarizing ourselves with the basic properties of a vector space.
</p>
<p>Before proceeding with the abstract vector space, it may be helpful for you
</p>
<p>to review the materials in Appendix A, in which vectors in the ordinary three-
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 309
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1 8</p>
<p/>
</div>
<div class="page"><p/>
<p>310 8 Quantum Formulation
</p>
<p>dimensional space is discussed. This provides a useful analogy as we talk about
</p>
<p>abstract vector space in a more general term.
</p>
<p>8.1.1 Definition
</p>
<p>Let K denote either the set C of all complex numbers or the set R of all real numbers.
</p>
<p>We shall take for granted the rules for addition and multiplication of two numbers
</p>
<p>in K. On this basis, we construct a vector space as follows. Let V be a set of objects
</p>
<p>and suppose that:
</p>
<p>a. There is an operation called addition that assigns for each x &isin; V and for each
y &isin;V another element in V , which we denote by x+ y.
</p>
<p>b. There is an operation called multiplication by a number that assigns for each
</p>
<p>number α &isin;K and each x &isin;V another element in V , which we denote by αx.
When the following properties are satisfied, the set V is said to be a vector space:
</p>
<p>V1: x+ y = y+ x for all x,y &isin;V .
V2: x+(y+ z) = (x+ y)+ z for all x,y,z &isin;V .
V3: There is an element θ &isin;V such that x+θ = x for all x &isin;V .
V4: To each x &isin;V , there corresponds an element x &isin;V such that x+ x = θ .
V5: α(x+ y) = αx+αy for all x,y &isin;V and for all α &isin;K.
V6: (α+β )x = αx+βx for all α,β &isin;K and for all x &isin;V .
V7: α(βx) = (αβ )x for all α,β &isin;K and for all x &isin;V .
V8: 1x = x for all x &isin;V , in which 1 is a number.
</p>
<p>An element of V is called a vector and θ is called the zero vector. If K= R, V is a
real vector space. If K= C instead, V is a complex vector space.
</p>
<p>We note that the standard notation for x appearing in V4 is &minus;x. Thus, x+ y =
x+(&minus;y), which is more commonly written as x&minus;y. We note that &minus;x is a vector that
gives θ when added to another vector x. In contrast, (&minus;1)x is a vector obtained by
multiplying x by a number called &minus;1. Thus, they are conceptually distinct objects.
It is in order to stress this distinction that we use x for &minus;x, even though they turn out
to be the same vector as you can convince yourself in Exercise 8.1d.
</p>
<p>Example 8.1. A real vector space: The set V of all pairs of real numbers, that
</p>
<p>is, V = {(x,y)|x,y &isin; R}, is a real vector space with the addition defined by
</p>
<p>(x1,y1)+(x2,y2) = (x1 + x2,y1 + y2) (8.1)
</p>
<p>and the multiplication by a number defined by
</p>
<p>α(x,y) = (αx,αy) for each α &isin; R . (8.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Vector Space 311
</p>
<p>As a slightly less trivial example, let us consider a set of functions.
</p>
<p>Example 8.2. A real vector space: The set V of all real-valued functions
</p>
<p>defined on the interval [a,b] &sub; R is a vector space with the addition defined
by
</p>
<p>( f +g)(x) := f (x)+g(x) for each f ,g &isin;V (8.3)
and the multiplication by a number defined by
</p>
<p>(α f )(x) := α f (x) for each f &isin;V and each α &isin; R . (8.4)
</p>
<p>It is important to recognize the meaning of (8.3), in which f + g is the
name of a function in V . Being a function, there has to be a rule of assigning
</p>
<p>a real number to each x &isin; R. This rule is given by (8.3) in terms of the rule
for adding two real numbers and the rules that assign real numbers f (x) and
g(x) to each x &isin; R. Likewise, α f on the left-hand side of (8.4) is the name
of a function in V . The rule of assigning a number to each x &isin; R is given in
terms of the rule for multiplying two numbers, α and f (x), and the rule of
assignment for the function f .
</p>
<p>Finally, if we replace α &isin;R in (8.4) by α &isin;C while insisting that V is still
a set of all real-valued functions only, then V is no longer a vector space since
</p>
<p>α f /&isin;V if α /&isin; R.
</p>
<p>An example of a complex vector space can be generated easily with a slight
</p>
<p>modification to the previous one.
</p>
<p>Example 8.3. A complex vector space: The set V of all complex-valued func-
</p>
<p>tions defined on the interval [a,b] &sub; R is a vector space with the addition
defined by
</p>
<p>( f +g)(x) := f (x)+g(x) for each f ,g &isin;V (8.5)
and the multiplication by a number defined by
</p>
<p>(α f )(x) := α f (x) for each f &isin;V and each α &isin; C . (8.6)
</p>
<p>Even if we limit ourselves to α &isin;R, V still is a vector space. According to our
definition, however, V now becomes a real vector space.
</p>
<p>Using the properties of a vector space enumerated above, we can prove various
</p>
<p>useful facts about it.
</p>
<p>Example 8.4. Zero vector is unique: Take property V3, for example. This
</p>
<p>property demands that there must be a zero vector in a vector space. But,</p>
<p/>
</div>
<div class="page"><p/>
<p>312 8 Quantum Formulation
</p>
<p>it does not say how many zero vectors a particular vector space might contain.
</p>
<p>Nevertheless, we can show that, in any vector space V , there is only one zero
</p>
<p>vector. To see this, let both θ1 and θ2 be zero vectors of V . From V3, we have
</p>
<p>x+θ1 = x for any x &isin;V (8.7)
</p>
<p>and
</p>
<p>x+θ2 = x for any x &isin;V . (8.8)
Since (8.7) holds for any x &isin;V , it holds for θ2 &isin;V as well:
</p>
<p>θ2 +θ1 = θ2 . (8.9)
</p>
<p>Using the property V1 of a vector space, we may rewrite this as
</p>
<p>θ1 +θ2 = θ2 . (8.10)
</p>
<p>From (8.8) with x = θ1 &isin;V ,
</p>
<p>θ1 +θ2 = θ1 , (8.11)
</p>
<p>Comparing the last two equations, we find that θ1 = θ2.
</p>
<p>Exercise 8.1. Let V be a vector space:
</p>
<p>a. Suppose that θ &prime; satisfies x+θ &prime; = x for a particular x &isin;V . Show that θ &prime; is a zero
vector of V .
</p>
<p>b. Show that 0x = θ .
c. Show that, if x+ y = θ , then y = x.
d. Show that (&minus;1)x = x.
e. Show that αx = (&minus;α)x for any α &isin;K.
f. Show that αθ = θ for any α &isin;K.
g. Show that αx = αx for any α &isin;K.
h. Show that
</p>
<p>α(x&minus; y) = αx&minus;αy for any α &isin;K (8.12)
and that
</p>
<p>(α&minus;β )x = αx&minus;βx for any α,β &isin;K. (8.13)
Note that we interpret αx&minus;αy as αx+αy and αx&minus;βx as αx+βx. ///</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Vector Space 313
</p>
<p>8.1.2 Linear Independence
</p>
<p>For vectors x1, . . . ,xm &isin;V and the numbers c1, . . . ,cm &isin;K, the sum
</p>
<p>c1x1 + &middot; &middot; &middot;+ cmxm (8.14)
</p>
<p>is called a linear combination of x1, . . . , xm. The set U of all possible linear combi-
</p>
<p>nations of x1, . . . , xm is a vector space called the span of the set {x1, . . . ,xm}, which
is denoted by
</p>
<p>span({x1, . . . ,xm}) . (8.15)
We say that the vectors x1, . . . , xm span this vector space. In the ordinary three-
</p>
<p>dimensional space, for example, two vectors a and b, neither of which is a scalar
</p>
<p>multiple of the other, span a two-dimensional vector space, whose elements are
</p>
<p>vectors on the plane containing a and b.
</p>
<p>Vectors x1, . . . , xm are said to be linearly independent if
</p>
<p>c1x1 + &middot; &middot; &middot;+ cmxm = θ (8.16)
</p>
<p>requires
</p>
<p>ci = 0 for all i = 1, . . . ,m . (8.17)
</p>
<p>Equation (8.17) is clearly sufficient for (8.16) and is referred to as the trivial solu-
</p>
<p>tion of (8.16). The vectors are linearly dependent if there is a nontrivial solution
</p>
<p>of (8.16), for which not every ci is zero.
</p>
<p>Example 8.5. Linear independence:
</p>
<p>a. Two vectors a and b in the ordinary three-dimensional space are linearly
</p>
<p>dependent if one of them is a scalar multiple of the other:
</p>
<p>a= cb for some c &isin; R. (8.18)
</p>
<p>On the other hand, if neither is a scalar multiple of the other, then they are
</p>
<p>linearly independent.
</p>
<p>b. The zero vector, taken by itself, is linearly dependent since
</p>
<p>cθ = θ for any c &isin;K. (8.19)
</p>
<p>c. Vectors θ , x1, . . . , xm are linearly dependent since
</p>
<p>c0θ + c1x1 + &middot; &middot; &middot;+ cmxm = θ (8.20)
</p>
<p>is satisfied for any value of c0 &isin;K if c1 = &middot; &middot; &middot;= cm = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>314 8 Quantum Formulation
</p>
<p>8.1.3 Basis
</p>
<p>A set of vectors
</p>
<p>{φ1, . . . ,φr} , (8.21)
where φi &isin;V for i = 1, . . . ,r, is said to be a basis of V if any x &isin;V can be expressed
as a linear combination of φ1, . . . , φr in one and only one way. A member of the
basis is referred to as a basis vector.
</p>
<p>A vector space V may have multiple bases. However, the number of basis vectors
</p>
<p>is the same for all of them. It is therefore an intrinsic property of V and is called the
</p>
<p>dimension of V . In other words, if the dimension of V is r, a set of m vectors cannot
</p>
<p>be a basis if m &gt; r or m &lt; r. This is established by the following two theorems, the
proofs of which are provided in the next optional subsection.
</p>
<p>Theorem 8.1. A set of vectors B := {φ1, . . . ,φr} is a basis of V if and only if
span(B) =V and φ1, . . . , φr are linearly independent.
</p>
<p>Theorem 8.2. Suppose that B := {φ1, . . . ,φr} is a basis of V .
a. Vectors ψ1, . . . , ψm are not linearly independent if m &gt; r.
b. Vectors ψ1, . . . , ψm does not span V if m &lt; r.
</p>
<p>8.1.4 &dagger;Proofs of Theorems
</p>
<p>For completeness, we prove of the theorems just introduced.
</p>
<p>Proof of Theorem 8.1
</p>
<p>Let us suppose that B is a basis and establish the &ldquo;only if&rdquo; part of the theorem. We
</p>
<p>first show that span(B) = V . If x &isin; V , x may be expressed as a linear combination
of φ1, . . . , φr, and hence x &isin; span(B). On the other hand, if x &isin; span(B), then
x &isin; V . Thus, the membership of span(B) is identical to that of V . So, we have
span(B) =V . To establish that φ1, . . . , φr are linearly independent, suppose that
</p>
<p>c1φ1 + &middot; &middot; &middot;+ crφr = θ . (8.22)
</p>
<p>We note that θ can also be written as
</p>
<p>θ = 0φ1 + &middot; &middot; &middot;+0φr . (8.23)
</p>
<p>But, each vector in V , including θ , has a unique expression when written as a lin-
ear combination of the basis vectors in B. Thus, comparing (8.22) and (8.23), we
</p>
<p>conclude that ci = 0 for all i = 1, . . . ,r.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Vector Space 315
</p>
<p>Conversely, suppose that span(B) = V and that the members of B are linearly
independent. Then, any member x of V belongs to span(B). This guarantees that x
may be expressed as a linear combination of φ1, . . . , φr. If we suppose
</p>
<p>x = c1φ1 + &middot; &middot; &middot;+ crφr = d1φ1 + &middot; &middot; &middot;+drφr , (8.24)
</p>
<p>then,
</p>
<p>(c1 &minus;d1)φ1 + &middot; &middot; &middot;+(cr &minus;dr)φr = θ . (8.25)
Since φ1, . . . , φr are linearly independent, only the trivial solution is possible, that
is, ci = di for all i = 1, . . . ,r. That is, there is only one way to express x as a linear
combination of φ1, . . . , φr. ⊓⊔
</p>
<p>Proof of Theorem 8.2
</p>
<p>We prove each part of the theorem by contradiction. That is, we assume that the
</p>
<p>claim is false and then show that this assumption leads to a contradiction:
</p>
<p>a. Let us suppose that ψ1, . . . , ψm (m &gt; r) are linearly independent. Since B is a
basis,
</p>
<p>ψ1 = c1φ1 + &middot; &middot; &middot;+ crφr , (8.26)
in which at least one of c1, . . . , cr is nonzero. To be concrete, we suppose that c1
is nonzero and solve (8.26) for φ1:
</p>
<p>φ1 =
1
</p>
<p>c1
ψ1 &minus;
</p>
<p>c2
</p>
<p>c1
φ2 &minus;&middot;&middot; &middot;&minus;
</p>
<p>cr
</p>
<p>c1
φr . (8.27)
</p>
<p>There is no loss of generality here since we can always relabel φi&rsquo;s if necessary.
By means of Theorem 8.1, we now show that
</p>
<p>B1 := {ψ1,φ2, . . . ,φr} (8.28)
</p>
<p>is a basis. Since B is a basis, for any x &isin;V , we have
</p>
<p>x= d1φ1+ &middot; &middot; &middot;+drφr =
d1
</p>
<p>c1
ψ1+
</p>
<p>(
d2 &minus;
</p>
<p>d1c2
</p>
<p>c1
</p>
<p>)
φ2+ &middot; &middot; &middot;+
</p>
<p>(
dr &minus;
</p>
<p>d1cr
</p>
<p>c1
</p>
<p>)
φr , (8.29)
</p>
<p>where we used (8.27). So, any x &isin; V can be written as a linear combination of
ψ1,φ2, . . . ,φr. To see that these vectors are linearly independent, let us consider
the equation:
</p>
<p>e1ψ1 + e2φ2 + &middot; &middot; &middot;+ erφr = θ . (8.30)
If e1 	= 0, then ψ1 is a linear combination of φ2, . . . , φr. This is contrary to the
supposition that c1 	= 0 in (8.26). Since by definition of basis, the expression
(8.26) is unique. So, e1 = 0 and (8.30) reduces to
</p>
<p>e2φ2 + &middot; &middot; &middot;+ erφr = θ . (8.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>316 8 Quantum Formulation
</p>
<p>But, since φ2, . . . , φr are linearly independent, the only possible solution is that
e2 = &middot; &middot; &middot;= er = 0. Thus, only the trivial solution can satisfy (8.30).
Now that B1 is shown to be a basis, we can write
</p>
<p>ψ2 = f1ψ1 + f2φ2 + &middot; &middot; &middot;+ frφr . (8.32)
</p>
<p>We observe that at least one of f2, . . . , fr is nonzero. Otherwise, we have
</p>
<p>ψ2 = f1ψ1, which is in odds with our supposition that ψ1, . . . , ψm are linearly
independent. For concreteness, we assume that f2 	= 0 and write
</p>
<p>φ2 =&minus;
f1
</p>
<p>f2
ψ1 +
</p>
<p>1
</p>
<p>f2
ψ2 &minus;
</p>
<p>f3
</p>
<p>f2
φ3 &minus;&middot;&middot; &middot;&minus;
</p>
<p>fr
</p>
<p>f2
φr . (8.33)
</p>
<p>As before, we can show that
</p>
<p>B2 := {ψ1,ψ2,φ3, . . . ,φr} (8.34)
</p>
<p>is a basis. In fact, by means of (8.33), any linear combination of ψ1,φ2, . . . ,φr
may be expressed as a linear combination of ψ1,ψ2,φ3, . . . ,φr. Let
</p>
<p>g1ψ1 +g2ψ2 +g3φ3 + &middot; &middot; &middot;+grφr = θ , (8.35)
</p>
<p>If g2 	= 0, this equation can be solved to express ψ2 as a linear combination of
ψ1,φ3, . . . ,φr, which is in odds with (8.32) with f2 	= 0. So, g2 = 0. It follows that
g1,g3, . . . ,gr are all zero since ψ1,φ3, . . . ,φr are linearly independent as we have
already established below (8.30).
</p>
<p>Proceeding in this manner, we may establish that
</p>
<p>Br := {ψ1, . . . ,ψr} (8.36)
</p>
<p>is a basis. Thus, ψr+1, . . . , ψm can be expressed as linear combinations of ψ1, . . . ,
ψr, which contradicts our assumption that ψ1, . . . , ψm are linearly independent.
⊓⊔
</p>
<p>b. This time, we suppose that m &lt; r but that
</p>
<p>V = span({ψ1, . . . ,ψm}) . (8.37)
</p>
<p>If ψ1, . . . , ψm are linearly independent, then by definition, {ψ1, . . . ,ψm} is a basis
of V . But, then according to part a of the current theorem, φ1, . . . , φr with r &gt; m
cannot be linearly independent, contrary to the supposition that B is a basis. If
</p>
<p>ψ1, . . . , ψm are linearly dependent, then
</p>
<p>c1ψ1 + &middot; &middot; &middot;+ cmψm = θ (8.38)
</p>
<p>and at least one of c1, . . . , cm is nonzero. To be concrete, suppose that c1 	= 0.
Then,
</p>
<p>ψ1 =&minus;
c2
</p>
<p>c1
ψ2 &minus;&middot;&middot; &middot;&minus;
</p>
<p>cm
</p>
<p>c1
ψm , (8.39)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Vector Space 317
</p>
<p>in terms of which any x &isin;V may be expressed as a linear combination of ψ2, . . . ,
ψm:
</p>
<p>x = d1ψ1 + &middot; &middot; &middot;+dmψm =
(
</p>
<p>d2 &minus;
d1c2
</p>
<p>c1
</p>
<p>)
ψ2 + &middot; &middot; &middot;+
</p>
<p>(
dm &minus;
</p>
<p>d1cm
</p>
<p>c1
</p>
<p>)
ψm . (8.40)
</p>
<p>In this way, we can remove ψi&rsquo;s one by one until the remaining ψi&rsquo;s are linearly
independent. But, the span of the set consisting of the remaining ψi&rsquo;s is still V .
As we saw above, this leads to a contradiction. ⊓⊔
</p>
<p>8.1.5 Scalar Product Space
</p>
<p>Let V be a complex vector space. If there is a rule that assigns a number (x,y) &isin; C
for each pair of x,y &isin; V and if the rule satisfies the following properties, then we
call (x,y) the scalar product of x and y and the vector space V a scalar product
space:
</p>
<p>S1: (x+ y,z) = (x,z)+(y,z) for all x,y,z &isin;V .
S2: (y,x) = (x,y)&lowast; for all x,y &isin;V , where * denotes the complex conjugation.
S3: (αx,y) = α(x,y) for all x,y &isin;V and for all α &isin; C.
S4: (x,x)&ge; 0 for all x &isin;V .
S5: (x,x) = 0 if and only if x = θ .
</p>
<p>According to S2, (x,x) is a real number. Because of S4, it is also nonnegative. Thus,
||x|| :=
</p>
<p>&radic;
(x,x), called the norm of the vector x, is a real number. We say that x 	= θ
</p>
<p>and y 	= θ are orthogonal if (x,y) = 0. If V is a real vector space, we demand that
(x,y) &isin; R instead and replace S2 and S3 as follows:
S2&prime; (y,x) = (x,y) for all x,y &isin;V .
S3&prime; (αx,y) = α(x,y) for all x,y &isin;V and for all α &isin; R.
</p>
<p>Example 8.6. Three-dimensional vectors: The vector space V of ordinary
</p>
<p>three-dimensional vectors is a scalar product space if
</p>
<p>(a,b) := a &middot;b . (8.41)
</p>
<p>With this definition, the norm of a is just the length of a, which we have been
</p>
<p>denoting by ||a||.
</p>
<p>The following example illustrates a scalar product defined on a complex vector
</p>
<p>space.</p>
<p/>
</div>
<div class="page"><p/>
<p>318 8 Quantum Formulation
</p>
<p>Example 8.7. Complex-valued functions: Let V be a complex vector space of
</p>
<p>complex-valued functions defined on the interval [a,b]&sub;R. If the membership
of V is such that the integral
</p>
<p>( f ,g) :=
</p>
<p>&int; b
</p>
<p>a
g&lowast;(x) f (x)dx (8.42)
</p>
<p>exists for all f ,g &isin;V , then V is a scalar product space.
</p>
<p>Exercise 8.2. Let V be a scalar product space. Show that
</p>
<p>a.
</p>
<p>(x,αy) = α&lowast;(x,y) for any α &isin; C. (8.43)
b.
</p>
<p>(x,θ) = (θ ,x) = 0 . (8.44)
</p>
<p>c.
</p>
<p>(x,y+ z) = (x,y)+(x,z) . (8.45)
</p>
<p>///
</p>
<p>From (8.43) and (8.45), it follows that
</p>
<p>(x,αy+β z) = α&lowast;(x,y)+β &lowast;(x,z) . (8.46)
</p>
<p>We refer to this result by saying that the scalar product is anti-linear in the second
</p>
<p>argument.
</p>
<p>8.1.6 Orthonormal Basis
</p>
<p>A basis {b1, . . . , br} is said to form an orthonormal basis if
</p>
<p>(bi,b j) = δi j for all i, j = 1, . . . ,r , (8.47)
</p>
<p>where δi j is the Kronecker delta.
Provided that r is finite, an orthonormal basis can be constructed from any basis.
</p>
<p>Given an arbitrary basis {φ1, . . . , φr}, we proceed as
</p>
<p>b1 := φ1/||φ1|| ,
b2 := b
</p>
<p>&prime;
2/||b&prime;2|| , b&prime;2 := φ2 &minus; (φ2,b1)b1 ,
</p>
<p>b3 := b
&prime;
3/||b&prime;3|| , b&prime;3 := φ3 &minus; (φ3,b1)b1 &minus; (φ3,b2)b2 ,
</p>
<p>&middot; &middot; &middot;
</p>
<p>br := b
&prime;
r/||b&prime;r|| , b&prime;r := φr &minus;
</p>
<p>r&minus;1
&sum;
j=1
</p>
<p>(φr,b j)b j . (8.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Vector Space 319
</p>
<p>The newly constructed basis vectors clearly have the unit norm. This construction is
</p>
<p>know as the Gram&ndash;Schmidt orthogonalization.
</p>
<p>Exercise 8.3. Check the orthogonality of the basis {b1, . . . , br}. ///
</p>
<p>8.1.7 Functions
</p>
<p>A complex vector space plays an essential role in quantum mechanics. Accordingly,
</p>
<p>we shall restrict ourselves to such spaces in what follows.
</p>
<p>Consider two sets V and W . A rule that assigns for each x &isin; V precisely one
element y &isin; W is called a function. That F is such a function is indicated by the
symbol F : V &minus;&rarr;W . The element in W assigned to x &isin;V by F is denoted by F(x).
Now, let F : V &minus;&rarr;W and G : V &minus;&rarr;W . If F(x) = G(x) for each x &isin;V , then F and
G refer to one and the same rule of assignment. The functions F and G are said to
</p>
<p>be identical.
</p>
<p>8.1.8 Linear Functional
</p>
<p>The function F : V &minus;&rarr; C on a vector space V is a linear functional if it satisfies:
F1: F(x+ y) = F(x)+F(y)
F2: F(αx) = αF(x)
</p>
<p>for all x,y &isin;V and all α &isin;C. A linear functional is determined completely once the
value of F(x) is given for each x &isin; V . Because any x can be expressed in terms of
an orthonormal basis {b1, . . . , br} of V as46
</p>
<p>x = c1b1 + &middot; &middot; &middot;+ crbr , (8.49)
</p>
<p>we have
</p>
<p>F(x) = c1F(b1)+ &middot; &middot; &middot;+ crF(br) . (8.50)
Thus, F is completely determined by specifying r complex numbers F(b1), . . . ,
F(br).
</p>
<p>A scalar product is an example of a linear functional. Let us pick a particular
</p>
<p>f &isin;V and define F by
F(x) := (x, f ) for all x &isin;V . (8.51)
</p>
<p>Clearly, F(x) satisfies the properties F1 and F2, and hence is a linear functional. It
follows that any vector in V defines a linear functional.
</p>
<p>The converse is also true. That is, to any linear functional F , there corresponds a
</p>
<p>unique vector f &isin;V such that F(x) = (x, f ) for all x &isin;V . In fact, consider
</p>
<p>f = F(b1)
&lowast;b1 + &middot; &middot; &middot;+F(br)&lowast;br . (8.52)</p>
<p/>
</div>
<div class="page"><p/>
<p>320 8 Quantum Formulation
</p>
<p>That is, given F , we first compute r complex numbers F(b1), . . . , F(br). We then
take their complex conjugate and construct f according to (8.52). The resulting
</p>
<p>vector f satisfies (x, f ) = F(x) for all x &isin;V .
Exercise 8.4. Verify this assertion. ///
</p>
<p>Exercise 8.5. Given a linear functional F(x), show that there is only one vector
f &isin;V that satisfies (8.51). ///
</p>
<p>We have just seen that there is a one-to-one correspondence between a vector in
</p>
<p>V and a linear functional on V . Let Ṽ denote a set of all linear functionals on V
</p>
<p>and define the addition of two linear functionals and the multiplication of a linear
</p>
<p>functional by a number as follows:
</p>
<p>D1: (F +G)(x) := F(x)+G(x) for all F,G &isin; Ṽ .
D2: (αF)(x) := αF(x) for all F &isin; Ṽ and for all α &isin; C.
Then, Ṽ is called the dual space of V . We now show that Ṽ is a vector space. First,
</p>
<p>we need to ensure that the following two properties are satisfied by our definitions
</p>
<p>of the addition (D1) and the multiplication by a number (D2):
</p>
<p>a. F +G &isin; Ṽ . That is, F +G is a linear functional, and hence
</p>
<p>(F +G)(x+ y) = (F +G)(x)+(F +G)(y) (8.53)
</p>
<p>and
</p>
<p>(F +G)(αx) = α(F +G)(x) (8.54)
</p>
<p>hold for any x,y &isin;V and α &isin; C.
b. αF &isin; Ṽ . That is, αF is a linear functional, and hence
</p>
<p>(αF)(x+ y) = (αF)(x)+(αF)(y) (8.55)
</p>
<p>and
</p>
<p>(αF)(βx) = β (αF)(x) (8.56)
</p>
<p>hold for any x,y &isin;V and β &isin; C.
</p>
<p>The content of (8.54), which may seem a little obscure at first sight, is that apply-
</p>
<p>ing F +G on αx produces the same result as applying F +G on x first and then
multiplying the result by α . Equation (8.56) indicates that applying the function αF
on βx gives the same result as applying αF on x first and then multiplying the result
by β .
</p>
<p>Exercise 8.6. Prove (8.53) and (8.54) from D1 and (8.55) and (8.56) from D2. ///
</p>
<p>Now we have to show that D1 and D2 are compatible with V1&ndash;V8. For this
</p>
<p>purpose, let f and g in V correspond to F and G in Ṽ , respectively. As we have just
</p>
<p>established, F +G and αF are elements of Ṽ . What are their corresponding vectors
in V ? From D1 and (8.51), we see that
</p>
<p>(F +G)(x) = F(x)+G(x) = (x, f )+(x,g) = (x, f +g) , (8.57)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Kets, Bras, and Operators 321
</p>
<p>where the last step follows from (8.45). Thus, F +G &isin; Ṽ corresponds f + g &isin; V .
Similarly, using D2 and (8.51),
</p>
<p>(αF)(x) = αF(x) = α(x, f ) = (x,α&lowast; f ) , (8.58)
</p>
<p>where the last step follows from (8.43). So, αF &isin; Ṽ corresponds α&lowast; f &isin; V . It is
worth emphasizing that the corresponding vector is α&lowast; f and not α f .
</p>
<p>Exercise 8.7. Show that the dual space is a vector space. ///
</p>
<p>8.1.9 Linear Operator
</p>
<p>Let both V and W be vector spaces. The function T : V &minus;&rarr;W is a linear operator
if T satisfies:
</p>
<p>L1: T (x+ y) = T (x)+T (y) for all x,y &isin;V .
L2: T (αx) = αT (x) for all α &isin; C and for all x &isin;V .
We often write T (x) as T x.
</p>
<p>Example 8.8. Linear operators: The function f (x) = ax, where a,x &isin; C, is a
linear operator from C to C. In contrast, g(x) = ax+b, where b 	= 0, is not.
</p>
<p>Let V be a set of continuously differentiable functions. That is, if f &isin; V ,
then d f/dx exists and d f/dx is continuous. If W is the set of all continuous
functions, then the derivative d/dx is a linear operator from V to W .
</p>
<p>Exercise 8.8. Let T : V &minus;&rarr;W be a linear operator. Show that
a. T (θV ) = θW , where θV is the zero vector in V and θW is the zero vector in W .
b. T (x&minus; y) = T (x)&minus;T (y). ///
</p>
<p>8.2 Kets, Bras, and Operators
</p>
<p>In classical mechanics, a microstate of a system is specified by (q f , p f ). In quantum
mechanics, a microstate is specified by a ket, a vector in a complex vector space
</p>
<p>called a ket space Vk. As in Sect. 8.1.8, we can construct a dual space of Vk through
</p>
<p>the scalar product defined on Vk. The dual space Vb = Ṽ k so constructed is referred
to as a bra space, a member of which is a bra. These vectors and the spaces they
</p>
<p>belong to constitute a fundamental construct of quantum mechanics. In this section,
</p>
<p>we develop rules of computing with bras and kets.</p>
<p/>
</div>
<div class="page"><p/>
<p>322 8 Quantum Formulation
</p>
<p>8.2.1 Bra&ndash;Ket
</p>
<p>A ket is denoted by a symbol | &middot; &middot; &middot; 〉 replacing &ldquo;&middot; &middot; &middot;&rdquo; by an appropriate label. For a
given | f 〉 &isin;Vk,
</p>
<p>F(|x〉) := (|x〉, | f 〉) for all |x〉 &isin;Vk (8.59)
defines a linear functional F . When this is done for all | f 〉 &isin;Vk, we have a set Vb of
linear functionals. A member of Vb is called a bra and is denoted by 〈&middot; &middot; &middot; | with &ldquo;&middot; &middot; &middot;&rdquo;
replaced by an appropriate label. In this notation, the linear functional F derived
</p>
<p>from | f 〉 is simply written as 〈 f |. Thus, in place of (8.59), we write
</p>
<p>〈 f |x〉 := (|x〉, | f 〉) for all |x〉 &isin;Vk. (8.60)
</p>
<p>As we saw in Sect. 8.1.8, this establishes the one-to-one correspondence between
</p>
<p>| f 〉 &isin;Vk and 〈 f | &isin;Vb. Pictorially, we write
</p>
<p>| f 〉 &isin;Vk DC&larr;&rarr; 〈 f | &isin;Vb , (8.61)
</p>
<p>where &ldquo;DC&rdquo; stands for &ldquo;dual-correspondence.&rdquo;
</p>
<p>Now that the correspondence between kets in Vk and bras in Vb is established,
</p>
<p>we can turn Vb into a vector space by defining the addition of two bras and the
</p>
<p>multiplication of a bra by a number in accordance with D1 and D2 from Sect. 8.1.8.
</p>
<p>(See Exercise 8.7.) In terms of the bra&ndash;ket notation, they are given by
</p>
<p>(〈 f |+ 〈g|)|x〉= 〈 f |x〉+ 〈g|x〉 for all 〈 f | ,〈g| &isin;Vb, (8.62)
</p>
<p>and
</p>
<p>(c〈 f |)|x〉= c〈 f |x〉 for all 〈 f | &isin;Vb and for all c &isin; C, (8.63)
respectively. Equation (8.57) now reads
</p>
<p>(〈 f |+ 〈g|)|x〉= (|x〉, | f 〉+ |g〉) , (8.64)
</p>
<p>from which we conclude
</p>
<p>| f 〉+ |g〉 &isin;Vk DC&larr;&rarr; 〈 f |+ 〈g| &isin;Vb . (8.65)
</p>
<p>Similarly, we see from (8.58) that
</p>
<p>(c〈 f |)|x〉= (|x〉,c&lowast;| f 〉) , (8.66)
</p>
<p>and hence
</p>
<p>c&lowast;| f 〉 &isin;Vk DC&larr;&rarr; c〈 f | &isin;Vb . (8.67)
It is often helpful to think of Vb as some sort of a mirror image of Vk with the rule
</p>
<p>of finding a mirror image of a particular object in Vk given by (8.61), (8.65), and
</p>
<p>(8.67). Additional rules will be derived in what follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Kets, Bras, and Operators 323
</p>
<p>Exercise 8.9. Let |θ〉 be the zero vector in a ket space Vk. Show that the correspond-
ing bra 〈θ | is the zero vector in the corresponding bra space Vb. ///
</p>
<p>Because 〈x|y〉 is a scalar product, it satisfies the properties S1&ndash;S5. In the current
notation, they are:
</p>
<p>S1bk: 〈z|(|x〉+ |y〉) = 〈z|x〉+ 〈z|y〉.
S2bk: 〈x|y〉= 〈y|x〉&lowast;.
S3bk: 〈y|(c|x〉) = c〈y|x〉 for all c &isin; C.
S4bk: 〈x|x〉 &ge; 0.
S5bk: 〈x|x〉= 0 if and only if |x〉= |θ〉.
</p>
<p>As with any scalar product space, two kets |x〉 	= |θ〉 and |y〉 	= |θ〉 in Vk are said
to be orthogonal if 〈x|y〉= 0. The quantity
</p>
<p>&radic;
〈x|x〉 is the norm of |x〉:
</p>
<p>|||x〉|| :=
&radic;
〈x|x〉 . (8.68)
</p>
<p>At this point, our terminology &ldquo;bra&rdquo; and &ldquo;ket&rdquo; should appear appropriate:
</p>
<p>〈x
bra
</p>
<p>|
(c)
</p>
<p>y〉
ket
</p>
<p>. (8.69)
</p>
<p>This &ldquo;bra&ndash;ket&rdquo; notation is due to P. A. M. Dirac and allows us to manipulate various
</p>
<p>quantities arising in quantum mechanics rather mechanically.
</p>
<p>Exercise 8.10. Let c &isin; C. Show that
</p>
<p>〈y|(c|x〉) = (〈y|c)|x〉 , (8.70)
</p>
<p>which indicates that brackets are unnecessary and we shall write the product simply
</p>
<p>as 〈y|c|x〉. ///
</p>
<p>Equations (8.62), (8.63), and those occurring S1bk&ndash;S5bk are simply a matter of
</p>
<p>definitions. However, (8.62) and S1bk taken together indicates that multiplication
</p>
<p>between a ket and a bra is distributive over addition. According to (8.70), the multi-
</p>
<p>plication of a bra, a number, and a ket, written in this order, is associative. S3bk then
</p>
<p>allows us to pull out the complex number placed between a bra and a ket, leaving
</p>
<p>an uninterrupted bra&ndash;ket. Without the benefit of the bra&ndash;ket notation, we have to
</p>
<p>express this fact as
</p>
<p>(c|x〉, |y〉) = (|x〉,c&lowast;|y〉) = c(|x〉, |y〉) . (8.71)
</p>
<p>Dirac&rsquo;s bra&ndash;ket notation expresses various definitions and properties pertaining to a
</p>
<p>scalar product space and its dual space through intuitively transparent distributive
</p>
<p>and associative laws. We shall encounter a few more such instances that illustrates
</p>
<p>the power of his notation.</p>
<p/>
</div>
<div class="page"><p/>
<p>324 8 Quantum Formulation
</p>
<p>8.2.2 Operator and Adjoint
</p>
<p>Let X̂ denote a linear operator from Vk to Vk. Then, we may write
</p>
<p>|β 〉= X̂ |α〉 . (8.72)
</p>
<p>to express the fact that, when the operator X̂ acts on |α〉, it produces another ket |β 〉.
Even though X̂ is a linear operator on Vk, it is actually possible to define its action
</p>
<p>on a bra and hence think of X̂ as a linear operator from Vb to Vb.
</p>
<p>By convention, an operator acting on a bra is written to the right of the bra as
</p>
<p>in 〈α|X̂ . This is a new construct, to which we have complete freedom to assign
any meaning we wish. Taking advantage of this freedom, we declare that 〈α|X̂ be a
functional on Vk, that is, it acts on a ket in Vk and produces a complex number. But,
</p>
<p>a functional is defined completely once its action on every |β 〉 &isin; Vk is specified.
Accordingly, we define 〈α|X̂ by
</p>
<p>(〈α|X̂)|β 〉 := 〈α|(X̂ |β 〉) for all |β 〉 &isin;Vk . (8.73)
</p>
<p>Since X̂ , when regarded as acting on Vk, is a linear operator, this definition implies
</p>
<p>that the functional 〈α|X̂ is linear. In fact,
</p>
<p>(〈α|X̂)(|β 〉+ |γ〉) = 〈α|[X̂(|β 〉+ |γ〉)] = 〈α|(X̂ |β 〉+ X̂ |γ〉)
= 〈α|(X̂ |β 〉)+ 〈α|(X̂ |γ〉) = (〈α|X̂)|β 〉+(〈α|X̂)|γ〉 , (8.74)
</p>
<p>where the third equality follows from S1bk. Also,
</p>
<p>(〈α|X̂)(c|β 〉) = 〈α|[X̂(c|β 〉)] = 〈α|(cX̂ |β 〉) = c〈α|(X̂ |β 〉) = c(〈α|X̂)|β 〉 , (8.75)
</p>
<p>where we used S3bk in the third equality.
</p>
<p>Since 〈α|X̂ is a linear functional on Vk, it is nothing but a bra in Vb. So, we are
justified in writing
</p>
<p>〈α|X̂ = 〈γα | &isin;Vb , (8.76)
where we use the subscript α to indicate the 〈α| dependence of the resulting bra.
That is, X̂ acts on a particular bra 〈α| and produces yet another bra 〈γα |, which in
turn is completely specified by means of (8.73). If 〈α|X̂ is defined for all 〈α| &isin; Vb
by (8.73), the end result is a function X̂ on Vb.
</p>
<p>We note that X̂ acting on Vb is a linear operator, that is,
</p>
<p>(〈α|+ 〈β |)X̂ = 〈α|X̂ + 〈β |X̂ , (8.77)
</p>
<p>and
</p>
<p>(c〈α|)X̂ = c(〈α|X̂) for all c &isin; C. (8.78)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Kets, Bras, and Operators 325
</p>
<p>Exercise 8.11. Prove (8.77) and (8.78). ///
</p>
<p>We recall from Sect. 8.1.8 that, for each bra in Vb, there corresponds a ket in Vk.
</p>
<p>We denote the ket corresponding to 〈α|X̂ by X̂&dagger;|α〉:
</p>
<p>〈α|X̂ DC&larr;&rarr; X̂&dagger;|α〉 . (8.79)
</p>
<p>As a consequence of (8.77) and (8.78), X̂
&dagger;
</p>
<p>acting on Vk is a linear operator. To see
</p>
<p>this, let |γ〉= |α〉+ |β 〉. Then,
</p>
<p>X̂
&dagger;
(|α〉+ |β 〉) = X̂&dagger;|γ〉 DC&larr;&rarr; 〈γ |X̂ = (〈α|+ 〈β |)X̂ , (8.80)
</p>
<p>where we used (8.65) in the last step. On the other hand, (8.77) gives
</p>
<p>(〈α|+ 〈β |)X̂ = 〈α|X̂ + 〈β |X̂ DC&larr;&rarr; X̂&dagger;|α〉+ X̂&dagger;|β 〉 , (8.81)
</p>
<p>where the last step also follows from (8.65). Since dual correspondence is one-to-
</p>
<p>one, we conclude that
</p>
<p>X̂
&dagger;
(|α〉+ |β 〉) = X̂&dagger;|α〉+ X̂&dagger;|β 〉 . (8.82)
</p>
<p>Exercise 8.12. Show that
</p>
<p>X̂
&dagger;
(c|α〉) = c(X̂&dagger;|α〉) for all c &isin; C, (8.83)
</p>
<p>thus completing the demonstration that X̂
&dagger;
</p>
<p>on Vk is linear. We have already seen
</p>
<p>how the linearity of X̂ on Vk implied that of X̂ on Vb. It follows that X̂
&dagger;
</p>
<p>on Vb is also
</p>
<p>linear. ///
</p>
<p>In view of (8.73), the multiplication of a bra, an operator, and a ket, written in this
</p>
<p>order, is associative. Thus, the brackets are unnecessary and we may simply write
</p>
<p>〈α|X̂ |β 〉 for the product. The content of (8.73) is far from trivial. Written by means
of the earlier notation for a scalar product, it reads
</p>
<p>(|β 〉, X̂&dagger;|α〉) = (X̂ |β 〉, |α〉) . (8.84)
</p>
<p>Dirac&rsquo;s notation allows us to perform complex operations such as this one effort-
</p>
<p>lessly and is extremely useful for computation.
</p>
<p>We now show that
</p>
<p>〈α|X̂&dagger;|β 〉= 〈β |X̂ |α〉&lowast; . (8.85)
From what was just said about (8.73),
</p>
<p>〈α|X̂&dagger;|β 〉= 〈α|(X̂&dagger;|β 〉) . (8.86)</p>
<p/>
</div>
<div class="page"><p/>
<p>326 8 Quantum Formulation
</p>
<p>Using (8.60) and S2,
</p>
<p>〈α|(X̂&dagger;|β 〉) = (X̂&dagger;|β 〉, |α〉) = (|α〉, X̂&dagger;|β 〉)&lowast; . (8.87)
</p>
<p>But, since the bra corresponding to the ket X̂
&dagger;|β 〉 is just 〈β |X̂ , we have
</p>
<p>(|α〉, X̂&dagger;|β 〉) = (〈β |X̂)|α〉= 〈β |X̂ |α〉 . (8.88)
</p>
<p>Combining everything, we arrive at (8.85).
</p>
<p>Exercise 8.13. Show that
</p>
<p>(X̂
&dagger;
)&dagger; = X̂ . (8.89)
</p>
<p>///
</p>
<p>Exercise 8.14. Show that
</p>
<p>(cX̂)&dagger; = c&lowast;X̂
&dagger;
, (8.90)
</p>
<p>where
</p>
<p>(cX̂)|α〉 := c(X̂ |α〉) for all c &isin; C. (8.91)
///
</p>
<p>The linear operator X̂
&dagger;
</p>
<p>is called the adjoint of X̂ . In general,
</p>
<p>X̂
&dagger;|α〉 	= X̂ |α〉 , (8.92)
</p>
<p>and hence X̂
&dagger; 	= X̂ . If X̂&dagger; = X̂ , then the operator X̂ is said to be Hermitian.
</p>
<p>8.2.3 Addition and Multiplication of Operators
</p>
<p>Addition of two operators is defined by
</p>
<p>(X̂ + Ŷ )|α〉 := X̂ |α〉+ Ŷ |α〉 for all |α〉 &isin;Vk, (8.93)
</p>
<p>while multiplication of two operators is defined by
</p>
<p>(X̂Ŷ )|α〉 := X̂(Ŷ |α〉) for all |α〉 &isin;Vk. (8.94)
</p>
<p>Repeated application of (8.73) combined with (8.94) yields
</p>
<p>〈β |(X̂Ŷ ) = (〈β |X̂)Ŷ . (8.95)
</p>
<p>In fact, for any |α〉 &isin;Vk,
</p>
<p>[〈β |(X̂Ŷ )]|α〉= 〈β |[(X̂Ŷ )|α〉] = 〈β |[X̂(Ŷ |α〉)] = (〈β |X̂)(Ŷ |α〉) = [(〈β |X̂)Ŷ ]|α〉 .
(8.96)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Kets, Bras, and Operators 327
</p>
<p>Since this holds for any |α〉, we arrive at (8.95).
</p>
<p>Exercise 8.15. Prove the following relations:
</p>
<p>a.
</p>
<p>X̂(Ŷ + Ẑ) = X̂Ŷ + Ŷ Ẑ . (8.97)
</p>
<p>b.
</p>
<p>(X̂Ŷ )Ẑ = X̂(Ŷ Ẑ) , (8.98)
</p>
<p>c.
</p>
<p>(X̂Ŷ )&dagger; = Ŷ
&dagger;
X̂
</p>
<p>&dagger;
. (8.99)
</p>
<p>///
</p>
<p>Equation (8.98) indicates that a multiplication of operators is associative, and
</p>
<p>hence the product may be written simply as X̂Ŷ Ẑ. Multiplication, however, is not
</p>
<p>necessarily commutative:
</p>
<p>X̂Ŷ 	= Ŷ X̂ . (8.100)
The difference between these two products defines a commutator between X̂ and
</p>
<p>Ŷ :
</p>
<p>[X̂ ,Ŷ ] := X̂Ŷ &minus; Ŷ X̂ , (8.101)
which is another operator. The following set of identities can be verified easily:
</p>
<p>[X̂ , X̂ ] = 0 , (8.102)
</p>
<p>[c, X̂ ] = 0 , c &isin; C (8.103)
[X̂ ,Ŷ + Ẑ] = [X̂ ,Ŷ ]+ [X̂ , Ẑ] , (8.104)
</p>
<p>[X̂ ,Ŷ Ẑ] = Ŷ [X̂ , Ẑ]+ [X̂ ,Ŷ ]Ẑ , (8.105)
</p>
<p>and
</p>
<p>[X̂ , [Ŷ , Ẑ]]+ [Ŷ , [Ẑ, X̂ ]]+ [Ẑ, [X̂ ,Ŷ ]] = 0 . (8.106)
</p>
<p>The last equation is the quantum mechanical version of Jacobi&rsquo;s identity to be
</p>
<p>compared with (1.196).
</p>
<p>8.2.4 Unitary Operator
</p>
<p>Let Î denote the unit operator defined by
</p>
<p>Î|α〉= |α〉 for all |α〉 &isin;Vk . (8.107)
</p>
<p>An operator Û satisfying
</p>
<p>ÛÛ
&dagger;
= Û
</p>
<p>&dagger;
Û = Î (8.108)</p>
<p/>
</div>
<div class="page"><p/>
<p>328 8 Quantum Formulation
</p>
<p>is called a unitary operator.
</p>
<p>Since (Û
&dagger;
)&dagger; = Û , (8.108) indicates that V̂ = Û
</p>
<p>&dagger;
is unitary if Û is. Unitary oper-
</p>
<p>ators do not affect the norm of a ket:
</p>
<p>||Û |α〉||= |||α〉|| . (8.109)
</p>
<p>This follows immediately from (8.108):
</p>
<p>||Û |α〉||=
&radic;
</p>
<p>〈α|Û&dagger;Û |α〉=
&radic;
〈α|Î|α〉=
</p>
<p>&radic;
〈α|α〉 . (8.110)
</p>
<p>Exercise 8.16. Show that X̂ Î = ÎX̂ = X̂ . ///
</p>
<p>8.2.5 Outer Product
</p>
<p>As we have seen, the scalar product 〈α|β 〉 is a complex number. Here, we define
the so-called outer product of |α〉 and 〈β | by the equation
</p>
<p>(|α〉〈β |)|γ〉 := |α〉(〈β |γ〉) for all |γ〉 &isin;Vk . (8.111)
</p>
<p>On the right-hand side, we have a complex number multiplying a ket, the result
</p>
<p>being another ket in Vk. That is, the outer product |α〉〈β | acting on a ket produces a
new ket proportional to |α〉. Because of the linearity of the scalar product, the outer
product is a linear operator. As shown in Exercise 8.17, however, the outer product
</p>
<p>|α〉〈β | is not a Hermitian operator unless |α〉 = |β 〉. Finally, (8.111) indicates that
the multiplication of a ket, a bra, and a ket, written in this order, is associative. Thus,
</p>
<p>the brackets are unnecessary and we shall write the product simply as |α〉〈β |γ〉.
</p>
<p>Exercise 8.17.
</p>
<p>a. Let X̂=|α〉〈β |. Show that X̂&dagger; = |β 〉〈α|.
b. Let Ŷ =&minus;i|α〉〈β | and find Ŷ &dagger;. ///
</p>
<p>If a given pair of expressions, each including bras, kets, operators, or complex
</p>
<p>numbers, are in dual correspondence with each other, they are said to be the adjoint
</p>
<p>of each other. Table 8.2.5 summarizes useful rules and formulae for finding the
</p>
<p>adjoint of a given expression.
</p>
<p>8.3 Eigenkets and Eigenvalues
</p>
<p>Eigenvalues of operators and their corresponding kets play a crucial role in quantum
</p>
<p>mechanics. Accordingly, we shall summarize their properties in this section.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Eigenkets and Eigenvalues 329
</p>
<p>Dual correspondence Equation Identity Equation
</p>
<p>| f 〉 DC&larr;&rarr; 〈 f | (8.61) 〈α |X̂&dagger;|β 〉= 〈β |X̂ |α〉&lowast; (8.85)
| f 〉+ |g〉 DC&larr;&rarr; 〈 f |+ 〈g| (8.65) (X̂&dagger;)&dagger; = X̂ (8.89)
</p>
<p>c&lowast;| f 〉 DC&larr;&rarr; c〈 f | (8.67) (cX̂)&dagger; = c&lowast;X̂&dagger; (8.90)
X̂
</p>
<p>&dagger;| f 〉 DC&larr;&rarr; 〈 f |X̂ (8.79) (X̂Ŷ )&dagger; = Ŷ &dagger;X̂&dagger; (8.99)
c|α〉〈β | DC&larr;&rarr; c&lowast;|β 〉〈α | Exercise 8.17
</p>
<p>Table 8.1 A list of useful rules and formulae for finding the adjoint of a given expression.
</p>
<p>8.3.1 Definition
</p>
<p>In general, an operator X̂ acting on a ket produces another ket different from the
</p>
<p>original one. However, there may be a ket |α〉 for which
</p>
<p>X̂ |α〉= λ |α〉 , λ &isin; C (8.112)
</p>
<p>holds. If this happens, λ and |α〉 are called, respectively, an eigenvalue of X̂ and the
eigenket corresponding (or belonging) to λ . The set of all eigenvalues is called the
spectrum of X̂ .
</p>
<p>In view of Exercise 8.8a,
</p>
<p>X̂ |θ〉= |θ〉= λ |θ〉 for any λ &isin; C (8.113)
</p>
<p>for any linear operator X̂ . That is, |θ〉 satisfies (8.112) for arbitrary λ &isin; C. By con-
vention, however, the zero vector is not an eigenket. According to the definition, if
</p>
<p>|α〉 is an eigenket, so is c|α〉, where c is a nonzero complex number.
For a given eigenvalue, there may be multiple corresponding linearly indepen-
</p>
<p>dent eigenkets. Such an eigenvalue is said to be degenerate. If the number of lin-
</p>
<p>early independent eigenkets corresponding to this eigenvalue is s, we say that the
</p>
<p>eigenvalue is s-fold degenerate or that its degeneracy is s.
</p>
<p>Suppose that the eigenvalue λ of X̂ is s-fold degenerate and denotes the cor-
responding s linearly independent eigenkets by |α1〉, . . . , |αs〉. Then, their linear
combination
</p>
<p>|φ〉 :=
s
</p>
<p>&sum;
k=1
</p>
<p>ck|αk〉 (8.114)
</p>
<p>is also an eigenket of X̂ corresponding to λ . In fact,
</p>
<p>X̂ |φ〉=
s
</p>
<p>&sum;
k=1
</p>
<p>ckX̂ |αk〉= λ
s
</p>
<p>&sum;
k=1
</p>
<p>ck|αk〉= λ |φ〉 . (8.115)
</p>
<p>The set of vectors {|α1〉, . . . , |αs〉} spans a s-dimensional vector space, called the
eigensubspace of the eigenvalue λ .</p>
<p/>
</div>
<div class="page"><p/>
<p>330 8 Quantum Formulation
</p>
<p>Example 8.9. Rotation: If we take an ordinary three-dimensional vector a and
</p>
<p>rotate it around a given axis by an angle θ (0 &lt; θ &lt; 2π), the result is another
vector b. The rotation is a linear operator, which we denote by R̂θ . While b 	= a
in general,
</p>
<p>R̂θa= λa , λ &isin; R (8.116)
holds with λ = 1 for vectors that are either parallel or antiparallel to the axis of
rotation. Then, λ = 1 is a nondegenerate eigenvalue of R̂θ with the said vectors
all belonging to this eigenvalue. Geometrically, it is clear that there is no other
</p>
<p>eigenvalue for a general value of θ . However, R̂π has another eigenvalue λ =
&minus;1. The vectors perpendicular to the axis of rotation are the corresponding
eigenvectors. Because these vectors lie on a plane, the degeneracy of λ =&minus;1
is two.
</p>
<p>8.3.2 Closure relation
</p>
<p>A Hermitian operator X̂ defined on a ket space Vk is said to be an observable if a
</p>
<p>set of all linearly independent eigenkets of X̂ forms a basis of Vk. It can be shown
</p>
<p>that a Hermitian operator always is an observable if the dimension of Vk is finite. In
</p>
<p>what follows, we denote generic observables by the symbols Â, B̂, Ĉ, and so on to
</p>
<p>distinguish them clearly from more general linear operators, which we continue to
</p>
<p>indicate by X̂ , Ŷ , Ẑ, etc.
</p>
<p>Let Â be an observable and denote its eigenvalues by a1, . . . , ar. We label an
</p>
<p>eigenket by the eigenvalue to which it belongs. For example,
</p>
<p>Â|a1〉= a1|a1〉 , . . . , Â|ar〉= ar|ar〉 . (8.117)
</p>
<p>If there is a degenerate eigenvalue, some of ai&rsquo;s are equal. For example, if a1 = 5 is
twofold degenerate, a1 = a2 = 5 and both |a1〉 and |a2〉 would become |5〉, requir-
ing additional label to distinguish them. This may be the case especially during a
</p>
<p>solution of a concrete numerical problem. However, since we do not need to refer
</p>
<p>to actual numerical values of ai&rsquo;s in what follows, the notation we have just adopted
</p>
<p>will be sufficient.
</p>
<p>Theorem 8.3. Let Â be an observable. Then,
</p>
<p>a. The eigenvalues of Â are real.
</p>
<p>b. The eigenkets of Â corresponding to different eigenvalues are orthogonal.
</p>
<p>Proof. Let
</p>
<p>Â|ai〉= ai|ai〉 and Â|a j〉= a j|a j〉 . (8.118)
Multiplying the first equation by 〈a j| from the left,
</p>
<p>〈a j|Â|ai〉= ai〈a j|ai〉 . (8.119)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Eigenkets and Eigenvalues 331
</p>
<p>Taking the adjoint of the second equation and multiplying the resulting equation by
</p>
<p>|ai〉 from the right,
〈a j|Â|ai〉= a j&lowast;〈a j|ai〉 , (8.120)
</p>
<p>where we note that Â
&dagger;
= Â. Subtracting (8.120) from (8.119),
</p>
<p>(ai &minus;a j&lowast;)〈a j|ai〉= 0 . (8.121)
</p>
<p>Let ai = a j. Since |ai〉 	= |θ〉 by convention, (8.121) implies that ai = ai&lowast;. This
proves a. Because of a, (8.121) becomes
</p>
<p>(ai &minus;a j)〈a j|ai〉= 0 . (8.122)
</p>
<p>If ai 	= a j, then 〈a j|ai〉= 0, thus establishing b. ⊓⊔
</p>
<p>What happens if an eigenvalue is s-fold degenerate? According to this theorem,
</p>
<p>these s eigenkets are certainly orthogonal to eigenkets belonging to other eigenval-
</p>
<p>ues. However, the theorem does not tell us if they are orthogonal among themselves.
</p>
<p>Fortunately, the Gram&ndash;Schmidt orthogonalization we saw in Sect. 8.1.6 allows us
</p>
<p>to construct s mutually orthogonal eigenkets as linear combinations of the original
</p>
<p>eigenkets. With a proper normalization, therefore, a set of all linearly independent
</p>
<p>eigenkets of Â can be made orthonormal:
</p>
<p>〈ai|a j〉= δi j . (8.123)
</p>
<p>In what follows, we shall assume that this is done.
</p>
<p>Using an orthonormal basis {|a1〉, . . . , |ar〉} so constructed, we may write any
ket |α〉 &isin;Vk as
</p>
<p>|α〉=
r
</p>
<p>&sum;
i=1
</p>
<p>Cai |ai〉 . (8.124)
</p>
<p>Multiplying (8.124) by 〈a j|, we find
</p>
<p>〈a j|α〉=
r
</p>
<p>&sum;
i=1
</p>
<p>Cai〈a j|ai〉=
r
</p>
<p>&sum;
i=1
</p>
<p>Caiδ ji =Ca j . (8.125)
</p>
<p>When this is substituted back into (8.124), we obtain
</p>
<p>|α〉=
r
</p>
<p>&sum;
i=1
</p>
<p>(〈ai|α〉) |ai〉=
r
</p>
<p>&sum;
i=1
</p>
<p>|ai〉(〈ai|α〉) =
r
</p>
<p>&sum;
i=1
</p>
<p>(|ai〉〈ai|) |α〉=
[
</p>
<p>r
</p>
<p>&sum;
i=1
</p>
<p>|ai〉〈ai|
]
|α〉 .
</p>
<p>(8.126)
</p>
<p>Since this holds for any |α〉 &isin;Vk, we conclude that
r
</p>
<p>&sum;
i=1
</p>
<p>|ai〉〈ai|= Î , (8.127)</p>
<p/>
</div>
<div class="page"><p/>
<p>332 8 Quantum Formulation
</p>
<p>which is to be compared with (A.21). Equation (8.127) is known as the closure
</p>
<p>relation and will be in frequent use in what follows.
</p>
<p>Exercise 8.18. Rewrite (8.52) using the bra&ndash;ket notation. ///
</p>
<p>Exercise 8.19. Show that
</p>
<p>Â =
r
</p>
<p>&sum;
i=1
</p>
<p>|ai〉ai〈ai| . (8.128)
</p>
<p>///
</p>
<p>A function f (Â) of the operator Â is a yet another operator and is defined by its
Maclaurin expansion. (See Sect. B.1.) That is,
</p>
<p>f (Â) = f (0)Î + f &prime;(0)Â+
1
</p>
<p>2
f &prime;&prime;(0)Â
</p>
<p>2
+
</p>
<p>1
</p>
<p>3
f &prime;&prime;&prime;(0)Â
</p>
<p>3
+ &middot; &middot; &middot; . (8.129)
</p>
<p>Exercise 8.20. Prove the following relations:
</p>
<p>a.
</p>
<p>f (Â)|ai〉= f (ai)|ai〉 . (8.130)
b.
</p>
<p>f (Â) =
r
</p>
<p>&sum;
i=1
</p>
<p>|ai〉 f (ai)〈ai| . (8.131)
</p>
<p>///
</p>
<p>For certain functions, (8.129) fails to define f (Â). For example,
&radic;
</p>
<p>x does not
</p>
<p>have the Maclaurin expansion and
&radic;
</p>
<p>Â cannot be defined by means of (8.129). More
</p>
<p>generally, we can define a function of an operator by (8.130). In fact, for any |α〉 &isin;
Vk, (8.130) assigns a unique ket in Vk:
</p>
<p>f (Â)|α〉=
r
</p>
<p>&sum;
i=1
</p>
<p>f (Â)|ai〉〈ai|α〉=
r
</p>
<p>&sum;
i=1
</p>
<p>f (ai)|ai〉〈ai|α〉 , (8.132)
</p>
<p>where we used the closure relation in the first equality.
</p>
<p>8.3.3 Matrix Representation
</p>
<p>Using the closure relation twice, we have
</p>
<p>X̂ =&sum;
i
&sum;
</p>
<p>j
</p>
<p>|ai〉〈ai|X̂ |a j〉〈a j| , (8.133)
</p>
<p>where 〈ai|X̂ |a j〉 is called a matrix element of X̂ in the A-representation with A
referring to the fact that we have used the set of all eigenkets {|a1〉, . . . , |ar〉} of</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Eigenkets and Eigenvalues 333
</p>
<p>Â as the basis. Evidently, numerical values of the matrix elements depend on the
</p>
<p>representation, that is, the choice of the basis kets.
</p>
<p>Let&rsquo;s consider the product Ẑ = X̂Ŷ :
</p>
<p>〈ai|Ẑ|a j〉= 〈ai|X̂Ŷ |a j〉=&sum;
k
</p>
<p>〈ai|X̂ |ak〉〈ak|Ŷ |a j〉 . (8.134)
</p>
<p>With an abbreviation Xi j := 〈ai|X̂ |a j〉, this equation may be written as
</p>
<p>Zi j =&sum;
k
</p>
<p>XikYk j , (8.135)
</p>
<p>or more explicitly,
</p>
<p>⎛
⎜⎝
</p>
<p>Z11 Z12 &middot; &middot; &middot;
Z21 Z22 &middot; &middot; &middot;
</p>
<p>...
...
</p>
<p>. . .
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>X11 X12 &middot; &middot; &middot;
X21 X22 &middot; &middot; &middot;
</p>
<p>...
...
</p>
<p>. . .
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
</p>
<p>Y11 Y12 &middot; &middot; &middot;
Y21 Y22 &middot; &middot; &middot;
</p>
<p>...
...
</p>
<p>. . .
</p>
<p>⎞
⎟⎠ . (8.136)
</p>
<p>The term &ldquo;matrix element&rdquo; is seen to be quite appropriate.
</p>
<p>We now turn to the equation:
</p>
<p>|β 〉= X̂ |α〉 . (8.137)
</p>
<p>Multiplying this equation by 〈ai| from the left, we find
</p>
<p>〈ai|β 〉= 〈ai|X̂ |α〉=
r
</p>
<p>&sum;
j=1
</p>
<p>〈ai|X̂ |a j〉〈a j|α〉 , i = 1, . . . ,r , (8.138)
</p>
<p>which may be written as
</p>
<p>⎛
⎜⎝
</p>
<p>〈a1|β 〉
〈a2|β 〉
</p>
<p>...
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>X11 X12 &middot; &middot; &middot;
X21 X22 &middot; &middot; &middot;
</p>
<p>...
...
</p>
<p>. . .
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
</p>
<p>〈a1|α〉
〈a2|α〉
</p>
<p>...
</p>
<p>⎞
⎟⎠ . (8.139)
</p>
<p>So, |α〉 can be represented by a column matrix:
</p>
<p>|α〉 .=
</p>
<p>⎛
⎜⎝
</p>
<p>〈a1|α〉
〈a2|α〉
</p>
<p>...
</p>
<p>⎞
⎟⎠ . (8.140)
</p>
<p>Let us see what happens with
</p>
<p>〈β |= 〈α|X̂ . (8.141)
Multiplying this equation by |ai〉 from the right, we find
</p>
<p>〈β |ai〉= 〈α|X̂ |ai〉=
r
</p>
<p>&sum;
j=1
</p>
<p>〈α|a j〉〈a j|X̂ |ai〉 , i = 1, . . . ,r . (8.142)</p>
<p/>
</div>
<div class="page"><p/>
<p>334 8 Quantum Formulation
</p>
<p>This may be written as
</p>
<p>(
〈β |a1〉 〈β |a2〉 &middot; &middot; &middot;
</p>
<p>)
=
(
〈α|a1〉 〈α|a2〉 &middot; &middot; &middot;
</p>
<p>)
⎛
⎜⎝
</p>
<p>X11 X12 &middot; &middot; &middot;
X21 X22 &middot; &middot; &middot;
</p>
<p>...
...
</p>
<p>. . .
</p>
<p>⎞
⎟⎠ , (8.143)
</p>
<p>indicating that 〈α| can be represented by a row matrix:
</p>
<p>〈α| .=
(
〈α|a1〉 〈α|a2〉 &middot; &middot; &middot;
</p>
<p>)
=
(
〈a1|α〉&lowast; 〈a2|α〉&lowast; &middot; &middot; &middot;
</p>
<p>)
. (8.144)
</p>
<p>Exercise 8.21. Express 〈α|β 〉 using matrices. ///
</p>
<p>The trace of an operator Â is defined as the sum of all diagonal elements of the
</p>
<p>matrix that represents Â:
</p>
<p>Tr{Â} :=
r
</p>
<p>&sum;
i=1
</p>
<p>〈ai|Â|ai〉 . (8.145)
</p>
<p>Tr{Â} is independent of the basis. To see this, let {|a1〉, . . . , |ar〉} and {|a&prime;1〉, . . . , |a&prime;r〉}
be distinct bases. Then,
</p>
<p>Tr{Â}=&sum;
i
</p>
<p>〈ai|Â|ai〉=&sum;
i, j
</p>
<p>〈ai|a&prime;j〉〈a&prime;j|Â|ai〉=&sum;
i, j
</p>
<p>〈a&prime;j|Â|ai〉〈ai|a&prime;j〉=&sum;
j
</p>
<p>〈a&prime;j|Â|a&prime;j〉 .
</p>
<p>(8.146)
</p>
<p>Exercise 8.22. Show that
</p>
<p>Tr{ÂB̂}= Tr{B̂Â} . (8.147)
///
</p>
<p>8.3.4 Commuting Observables
</p>
<p>An observable Â is diagonal in the A-representation. That is,
</p>
<p>〈ai|Â|a j〉= 〈ai|a j|a j〉= a j〈ai|a j〉= a jδi j , (8.148)
</p>
<p>where the last step follows from (8.123). Because the basis {|a1〉, . . . , |ar〉} is
orthonormal by construction, this equation holds even if some of the eigenvalues are
</p>
<p>degenerate. What can we say about the A-representation of some other observable
</p>
<p>B̂ that acts in Vk? The following theorem provides a partial answer to this question.
</p>
<p>Theorem 8.4. Let Â and B̂ be observables and suppose that none of the eigenvalues
</p>
<p>of Â is degenerate. If Â and B̂ commute, that is, if [Â, B̂] = 0, B̂ is diagonal in the
A-representation
</p>
<p>Proof.
</p>
<p>0 = 〈ai|[Â, B̂]|a j〉= 〈ai|(ÂB̂&minus; B̂Â)|a j〉= 〈ai|ÂB̂|a j〉&minus;〈ai|B̂Â|a j〉 . (8.149)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Eigenkets and Eigenvalues 335
</p>
<p>We recall that |ai〉 is an eigenket of Â corresponding to the eigenvalue ai:
</p>
<p>Â|ai〉= ai|ai〉 , (8.150)
</p>
<p>the adjoint of which reads
</p>
<p>〈ai|Â = ai〈ai| , (8.151)
where we used the fact that Â is Hermitian and that a j is a real number. It follows
</p>
<p>that
</p>
<p>0 = 〈ai|[Â, B̂]|a j〉= ai〈ai|B̂|a j〉&minus;〈ai|B̂|a j〉a j = (ai &minus;a j)〈ai|B̂|a j〉 . (8.152)
</p>
<p>Because the eigenvalues of Â are nondegenerate by assumption, i 	= j implies ai 	= a j
and hence 〈ai|B̂|a j〉= 0. ⊓⊔
</p>
<p>But, if B̂ is diagonal in the A-representation, the effect of B̂ on a basis ket |ai〉
should be just a multiplication by a number, and hence we have the following theo-
</p>
<p>rem:
</p>
<p>Theorem 8.5. Let Â and B̂ be observables satisfying [Â, B̂] = 0 and suppose that the
eigenvalues of Â are all nondegenerate. Then, |ai〉 is also an eigenket of B̂.
</p>
<p>Proof. Using the closure relation,
</p>
<p>B̂|ai〉=&sum;
j
</p>
<p>|a j〉〈a j|B̂|ai〉 . (8.153)
</p>
<p>Since the eigenvalues of Â are nondegenerate, Theorem 8.4 applies and we obtain
</p>
<p>B̂|ai〉= |ai〉〈ai|B̂|ai〉 . (8.154)
</p>
<p>That is, |ai〉 is an eigenket of B̂ corresponding to the eigenvalue 〈ai|B̂|ai〉. ⊓⊔
</p>
<p>Let us summarize the content of Theorems 8.3&ndash;8.5 here. In quantum mechanics,
</p>
<p>we are interested in an observable whose eigenkets span the vector space in which
</p>
<p>it acts. The eigenkets may be made orthonormal using Gram&ndash;Schmidt orthogonal-
</p>
<p>ization scheme if necessary. If none of the eigenvalues of Â are degenerate, the basis
</p>
<p>kets so constructed are simultaneous eigenkets of Â and all other operators that
</p>
<p>commute with Â. These operators, including Â, are diagonal in the A-representation.
</p>
<p>What happens to this very nice result if there is a degeneracy? Even then, by
</p>
<p>means of what is called a complete set of commuting observables, the simultane-
</p>
<p>ous eigenkets can still be found and the orthonormal basis can still be constructed
</p>
<p>using them. The observable in the set are all diagonal in this basis. This is the topic
</p>
<p>of next optional subsection. No harm will come from omitting it if you choose to
</p>
<p>accept the claim just made.</p>
<p/>
</div>
<div class="page"><p/>
<p>336 8 Quantum Formulation
</p>
<p>8.3.5 &dagger;Degenerate Eigenvalues
</p>
<p>Now, suppose that Â has an s-fold degenerate eigenvalue a1 and that other eigenval-
</p>
<p>ues as+1, . . . , ar are nondegenerate. A generalization to cases with multiple degen-
</p>
<p>erate eigenvalues will be trivial. If [Â, B̂] = 0, how does the matrix of B̂ look like in
the A-representation? We label the eigenkets corresponding to a1 as |a1〉, . . . , |as〉
and those corresponding to as+1, . . . , ar as |as+1〉, . . . , |ar〉. We suppose that these
kets are, by construction, orthonormal. Looking at the last step in the proof of The-
</p>
<p>orem 8.4, we see that
</p>
<p>〈ai|B̂|a j〉= 0 if i 	= j , 〈ai|B̂|a j〉= 0 , and 〈ai|B̂|a j〉= 0 . (8.155)
</p>
<p>However, the theorem is silent about 〈ai|B̂|a j〉.
For various computations, it would be very convenient if B̂ is fully diagonal in
</p>
<p>the A-representation. Can we somehow accomplish this by properly choosing an
</p>
<p>orthonormal basis? We would still like to keep using the eigenkets of Â as the basis
</p>
<p>kets, of course, since at least Â is diagonal then.
</p>
<p>We observe that (8.155) still holds even if we replace |a1〉, . . . , |as〉 by any of their
linear combinations. By Theorem 8.3b, |a j〉 is orthogonal to any linear combination
of |a1〉, . . . , |as〉. If we choose appropriate linear combinations and use them with
|as+1〉, . . . , |ar〉 as the basis kets, B̂ might become fully diagonal.
</p>
<p>To examine this possibility, we confine our attention to the eigensubspace V1k
of a1, that is, the vector space spanned by eigenkets |a1〉, . . . , |as〉 corresponding
to the eigenvalue a1. We recall that, by construction, the set {|a1〉, . . . , |as〉} is an
orthonormal basis in Vk1. As we just saw, we are free to choose any other orthonor-
</p>
<p>mal basis without affecting (8.155). Let us denote this new orthonormal basis in Vk1
by {|a1〉, . . . , |as〉}. Our goal is to construct this new basis from the old. But, this
can be accomplished by a linear operator Û acting in Vk1:
</p>
<p>Û :=
s
</p>
<p>&sum;
k=1
</p>
<p>|ak〉〈ak| . (8.156)
</p>
<p>In fact,
</p>
<p>Û |ai〉=
s
</p>
<p>&sum;
k=1
</p>
<p>|ak〉〈ak|ai〉=
s
</p>
<p>&sum;
k=1
</p>
<p>|ak〉δki = |ai〉 (8.157)
</p>
<p>as desired.
</p>
<p>Because we insist that new and old bases are both orthonormal, Û satisfies
</p>
<p>(8.108), and hence is a unitary operator. We can easily confirm this:
</p>
<p>ÛÛ
&dagger;
=
</p>
<p>s
</p>
<p>&sum;
i, j=1
</p>
<p>|ai〉〈ai|a j〉〈a j|=
s
</p>
<p>&sum;
i, j=1
</p>
<p>|ai〉δi j〈a j|=
s
</p>
<p>&sum;
i=1
</p>
<p>|ai〉〈ai|= Î , (8.158)
</p>
<p>where we used the closure relation in Vk1. Similarly for Û
&dagger;
Û = Î.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Eigenkets and Eigenvalues 337
</p>
<p>Of course, |ak〉 occurring in the definition for Û is the very unknown we are
trying to find. So, (8.156) is not an explicit prescription for finding Û .
</p>
<p>How do we find Û then? What we wish to accomplish with this Û is to find the
</p>
<p>new basis in which B̂ is diagonal:
</p>
<p>〈ai|B̂|a j〉= λδi j = λ 〈ai|a j〉 , i, j = 1, . . . ,s . (8.159)
</p>
<p>in which λ is also unknown at this point. Because this equation must hold for any
〈ai|, our goal is equivalent to ensuring
</p>
<p>B̂|a j〉= λ |a j〉 . (8.160)
</p>
<p>That is, the new basis kets we seek are all eigenvectors of B̂.
</p>
<p>The method for finding eigenvalues and the corresponding eigenkets are as fol-
</p>
<p>lows. We multiply (8.160) by 〈ai| from the left and rewrite the resulting equation
using the closure relations:
</p>
<p>s
</p>
<p>&sum;
k=1
</p>
<p>〈ai|B̂|ak〉〈ak|a j〉= λ 〈ai|a j〉 . (8.161)
</p>
<p>Written in terms of matrices, this reads
</p>
<p>⎛
⎜⎝
</p>
<p>B11 &middot; &middot; &middot; B1s
...
</p>
<p>. . .
...
</p>
<p>Bs1 &middot; &middot; &middot; Bss
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
</p>
<p>〈a1|a j〉
...
</p>
<p>〈as|a j〉
</p>
<p>⎞
⎟⎠= λ
</p>
<p>⎛
⎜⎝
</p>
<p>〈a1|a j〉
...
</p>
<p>〈as|a j〉
</p>
<p>⎞
⎟⎠ . (8.162)
</p>
<p>So that this equation has the nontrivial solutions, that is, for the solutions other than
</p>
<p>〈ai|a j〉= 0 for all i to exist, the determinant of the operator B̂&minus;λ Î must vanish:
∣∣∣∣∣∣∣
</p>
<p>B11 &minus;λ &middot; &middot; &middot; B1s
...
</p>
<p>. . .
...
</p>
<p>Bs1 &middot; &middot; &middot; Bss &minus;λ
</p>
<p>∣∣∣∣∣∣∣
= 0 . (8.163)
</p>
<p>The determinant is the sth order polynomial of λ , from which we find s roots
λ = b1, . . . ,bs. For each λ = b j so determined, we solve (8.162) to find the eigen-
vector ⎛
</p>
<p>⎜⎝
〈a1|a j〉
</p>
<p>...
</p>
<p>〈as|a j〉
</p>
<p>⎞
⎟⎠ . (8.164)
</p>
<p>When this is completed for each b j ( j = 1, . . . ,s), we have the matrix representation
of Û :
</p>
<p>〈ai|Û |a j〉=
s
</p>
<p>&sum;
k=1
</p>
<p>〈ai|ak〉〈ak|a j〉 . (8.165)</p>
<p/>
</div>
<div class="page"><p/>
<p>338 8 Quantum Formulation
</p>
<p>We recall that the newly determined basis vectors span Vk1 and they are all eigen-
</p>
<p>vectors of Â corresponding to a1. Now, by construction, |a j〉 &isin;Vk1 is also an eigen-
vector of B̂ corresponding to b j. If s solutions of (8.163) are all distinct, then |a1〉,
. . . , |as〉 are orthogonal by Theorem 8.3b applied to B̂. Under the same condition,
the pair of numbers a1 and b j, taken together, specifies the eigenket completely,
</p>
<p>which is then denoted by |a1b j〉. We say that B̂ resolves the s-fold degeneracy of Â
completely.
</p>
<p>The eigenkets |as+1〉, . . . , |ar〉 are also eigenkets of B̂ by Theorem 8.5 and may
be denoted in the same manner, even though the label b j ( j = s+1, . . . ,r) will be
redundant for them. Thus, when s-fold degeneracy of Â is completely resolved by
</p>
<p>B̂, we have the orthonormal basis
</p>
<p>{|a1b1〉, . . . , |a1bs〉, |as+1bs+1〉, . . . , |arbr〉} (8.166)
</p>
<p>of the original vector space Vk that consists of the simultaneous eigenkets of Â and
</p>
<p>B̂.
</p>
<p>If not all of s solutions of (8.163) are distinct, then there will be multiple eigen-
</p>
<p>kets corresponding to a given pair of numbers, a1 and one of b1, . . . , bs. In this case,
</p>
<p>we can introduce additional operators that commute with both Â, B̂, and with each
</p>
<p>other to further resolve the degeneracy. When all degeneracy is resolved in this man-
</p>
<p>ner, we have what is called a complete set of commuting observables Â1, . . . , Âc,
</p>
<p>for which [Âi, Â j] = 0 for all pairs of i, j = 1, . . . ,c, and their simultaneous eigenkets
form an orthonormal basis of Vk.
</p>
<p>8.4 Postulates of Quantum Mechanics
</p>
<p>Now we are finally ready to state the fundamental postulates of quantum mechanics.
</p>
<p>Postulate 1: The state of the system is represented by the &ldquo;direction&rdquo; of a ket, that
</p>
<p>is, |φ〉 and c|φ〉 (c 	= 0) represent the same state.
Postulate 2: A dynamical variable A, such as energy, position, and momentum, is
</p>
<p>represented by an observable Â.
</p>
<p>Postulate 3: A measurement of A yields one of the eigenvalues a1, a2, &middot; &middot; &middot;, an,
of Â. Equation (8.130) implies that, if a measurement of A yields ai, then the
</p>
<p>measurement of f (A) yields f (ai), which is a very natural thing to demand of
our theory.
</p>
<p>Postulate 4: If the system is in a state |φ〉, the probability of obtaining the value
ai upon measuring A is given by
</p>
<p>p(ai) = |〈ai|φ〉|2 (8.167)
</p>
<p>provided that ai is nondegenerate and |φ〉 is normalized. If |φ〉 is not normalized,
this expression must be replaced by
</p>
<p>p(ai) =
|〈ai|φ〉|2
〈φ |φ〉 . (8.168)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Postulates of Quantum Mechanics 339
</p>
<p>Note that the value of p(ai) is independent of whether |φ〉 is normalized or not,
as is demanded by Postulate 1. In what follows, state kets are assumed to be
</p>
<p>normalized.
</p>
<p>If ai is s-fold degenerate, then
</p>
<p>p(ai) =
s
</p>
<p>&sum;
k=1
</p>
<p>|〈ai,k|φ〉|2 , (8.169)
</p>
<p>where |ai,k〉 is the kth eigenket corresponding to ai.
</p>
<p>Example 8.10. Eigenstate and the outcome of a measurement: Suppose that
</p>
<p>the system is in state |φ〉. A measurement of a dynamical variable A of the
system yields ai with certainty if and only if |φ〉= |ai〉.
</p>
<p>To see this, let us suppose first that |φ〉= |ai〉 in (8.167). Then,
</p>
<p>p(ai) = |〈ai|ai〉|2 = 1 (8.170)
</p>
<p>since the eigenkets are normalized. On the other hand, if |φ〉 	= |ai〉, then
</p>
<p>|φ〉=Ci|ai〉+&sum;
j 	=i
</p>
<p>C j|a j〉 , (8.171)
</p>
<p>where C j 	= 0 for at least one C j 	=i. Since |φ〉 is normalized,
</p>
<p>〈φ |φ〉= |Ci|2 +&sum;
j 	=i
</p>
<p>∣∣C j
∣∣2 = 1 . (8.172)
</p>
<p>Thus,
</p>
<p>p(ai) = |Ci|2 = 1&minus;&sum;
j 	=i
</p>
<p>∣∣C j
∣∣2 &lt; 1 . (8.173)
</p>
<p>This proves the assertion.
</p>
<p>Now, let us suppose we measured A of a system in state |φ〉 and found the value
a j. If we wait around too long, the state of the system may evolve with time. But,
</p>
<p>if we measure A immediately after the first measurement, the system has no time
</p>
<p>to evolve. In this case, it seems natural to expect that we should obtain the same
</p>
<p>result a j with certainty. It follows from Example 8.10 that the state of the system
</p>
<p>immediately after the measurement which yielded a j must be |a j〉. One may say
that the state of the system has changed abruptly from |φ〉 to |a j〉 by the very act
of measuring A. This is the so-called collapse postulate of quantum mechanics. For
</p>
<p>alternative views, see Refs. [1, 8].
</p>
<p>Exercise 8.23. Suppose that we measure A right after preparing the system in the
</p>
<p>state |φ〉. If we repeat this procedure many times, including the preparation of the</p>
<p/>
</div>
<div class="page"><p/>
<p>340 8 Quantum Formulation
</p>
<p>system in the state |φ〉, we can find the average value of our many measurements,
which we denote by 〈A〉QM. Show that
</p>
<p>〈A〉QM = 〈φ |Â|φ〉 , (8.174)
</p>
<p>where |φ〉 is normalized. ///
</p>
<p>8.5 &Dagger;Uncertainty Principle
</p>
<p>Let us consider two observables Â and Ĉ. We assume that Â and Ĉ commute, that
</p>
<p>is, [Â,Ĉ] = 0 and that Â is nondegenerate. As we saw in Sect. 8.3.4, Â and Ĉ has the
simultaneous eigenkets that form an orthonormal basis {|a1c1〉, . . . , |arcr〉} of the
vector space Vk.
</p>
<p>The state ket |φ〉 &isin;Vk of the system can be written using this basis as
</p>
<p>|φ〉=
r
</p>
<p>&sum;
i=1
</p>
<p>λi|aici〉. (8.175)
</p>
<p>What would the outcome be of a measurement of the dynamical variable A? Accord-
</p>
<p>ing to the postulate of quantum mechanics, the outcome is one of ai (i = 1, . . . ,r)
with probability |λi|2. The state ket right after the measurement that gave ai is |aici〉.
Thus, if we measure C now, the outcome is ci with certainty and the state ket after
</p>
<p>this second measurement again is |aici〉. The subsequent measurements of Â and Ĉ
in any order will give ai and ci with certainty, provided that all measurements are
</p>
<p>done in rapid succession before the state ket has a time to evolve. We may say that
</p>
<p>the values of A and C can be determined simultaneously.
</p>
<p>The situation is quite different if we consider the measurements of A and B when
</p>
<p>[Â, B̂] 	= 0. In this case, the measurement of A gives ai and the state ket right after this
measurement is still |aici〉 as before. If we measure A again, we are certain to get ai.
However, |aici〉 is not an eigenket of B̂. Denoting the orthonormal basis consisting
of the eigenkets of B̂ by {|b1〉, . . . , |br〉}, we write
</p>
<p>|aici〉=
r
</p>
<p>&sum;
i=1
</p>
<p>λ &prime;i |bi〉 . (8.176)
</p>
<p>Thus, the subsequent measurement will give one of bi (i = 1, . . . ,r) with probability
|λ &prime;i |2. Still, the resulting ket |bi〉, though an eigenket of B̂, is not an eigenket of Â:
</p>
<p>|bi〉=
r
</p>
<p>&sum;
i=1
</p>
<p>λ &prime;&prime;i |aici〉 (8.177)
</p>
<p>and we are no longer certain of the outcome of the future measurement of A. Thus,
</p>
<p>if [Â, B̂] 	= 0, no amount of repeated measurements of A and B will ever give us</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 &Dagger;Uncertainty Principle 341
</p>
<p>any certainty regarding the future measurements of both of these quantities. That is,
</p>
<p>values of A and B cannot be determined simultaneously.
</p>
<p>The measurements of A and C are said to be compatible, while those of A and B
</p>
<p>are incompatible. From many repeated measurements of A and those of B, all done
</p>
<p>on the identically prepared state |φ〉, however, we can still compute the average
values, 〈A〉QM and 〈B〉QM, and the probability distributions of all possible outcomes
around these averages.
</p>
<p>To characterize the widths of these distributions, let us take Â and define
</p>
<p>Â∆ := Â&minus;〈A〉QMÎ . (8.178)
</p>
<p>Then,
</p>
<p>Â
2
∆ = Â
</p>
<p>2 &minus;2〈A〉QMÂ+ 〈A〉QM2Î , (8.179)
and hence
</p>
<p>〈A∆ 2〉QM = 〈φ |Â∆
2|φ〉= 〈A2〉QM &minus;〈A〉QM2 . (8.180)
</p>
<p>This quantity is called the dispersion of A and serves as a measure for the width of
</p>
<p>the distribution for A. Similarly for B̂.
</p>
<p>The dispersions of noncommuting observables are subject to a fundamental lim-
</p>
<p>itation imposed by the Heisenberg uncertainty principle:
</p>
<p>〈A∆ 2〉QM〈B∆ 2〉QM &ge;
1
</p>
<p>4
|〈[Â, B̂]〉QM|2 . (8.181)
</p>
<p>Equation (3.155) responsible for the 1/h3N factor for a classical statistical mechan-
ical partition function stems from this principle as we shall see in Sect. 8.7.2.
</p>
<p>The proof of the uncertainty principle requires three ingredients:
</p>
<p>a. The Schwarz inequality
</p>
<p>|〈α|β 〉|2 &le; 〈α|α〉〈β |β 〉 , (8.182)
</p>
<p>the proof of which is similar to that given in Appendix A.5 for the case of ordi-
</p>
<p>nary vectors in three-dimensional space. In particular, let |γ〉 := |α〉+λ |β 〉 and
note that
</p>
<p>〈γ |γ〉= 〈α|α〉+λ 〈α|β 〉+λ &lowast;〈β |α〉+λλ &lowast;〈β |β 〉 &ge; 0 (8.183)
</p>
<p>for any λ &isin; C. Setting λ =&minus;〈β |α〉/〈β |β 〉 we obtain the inequality in question.
b. The average of a Hermitian operator Â is a real number as is obvious from The-
</p>
<p>orem 8.3.
</p>
<p>c. An operator satisfying X̂
&dagger;
= &minus;X̂ is said to be anti-Hermitian. The average of
</p>
<p>an anti-Hermitian operator is purely imaginary. In fact, using (8.85),
</p>
<p>〈φ |X̂ |φ〉&lowast; = 〈φ |X̂&dagger;|φ〉=&minus;〈φ |X̂ |φ〉 . (8.184)</p>
<p/>
</div>
<div class="page"><p/>
<p>342 8 Quantum Formulation
</p>
<p>Let α = Â∆ |φ〉 and β = B̂∆ |φ〉 in the Schwarz inequality. This gives
</p>
<p>〈A∆ 2〉QM〈B∆ 2〉QM &ge;
∣∣〈φ |Â∆ B̂∆ |φ〉
</p>
<p>∣∣2 . (8.185)
</p>
<p>To transform the right-hand side, we write
</p>
<p>Â∆ B̂∆ =
1
</p>
<p>2
[Â∆ , B̂∆ ]&minus;+
</p>
<p>1
</p>
<p>2
[Â∆ , B̂∆ ]+ , (8.186)
</p>
<p>where
</p>
<p>[X̂ ,Ŷ ]&plusmn; := X̂Ŷ &plusmn; Ŷ X̂ . (8.187)
The subscript &ldquo;&minus;&rdquo; temporarily denotes the now familiar commutator, while the sub-
script &ldquo;+&rdquo; defines the so-called anti-commutator.
</p>
<p>It is straightforward to show that [Â∆ , B̂∆ ]&minus; = [Â, B̂]&minus; is anti-Hermitian, while
[Â∆ , B̂∆ ]+ is Hermitian. In view of the items b and c above, we conclude that
</p>
<p>〈A∆ 2〉QM〈B∆ 2〉QM &ge;
1
</p>
<p>4
</p>
<p>∣∣〈φ |[Â, B̂]&minus;|φ〉
∣∣2 + 1
</p>
<p>4
</p>
<p>∣∣〈φ |[Â∆ , B̂∆ ]+|φ〉
∣∣2
</p>
<p>&ge; 1
4
</p>
<p>∣∣〈φ |[Â, B̂]&minus;|φ〉
∣∣2 , (8.188)
</p>
<p>which is just (8.181).
</p>
<p>8.6 &Dagger;Operator with Continuous Spectrum
</p>
<p>Certain quantities, such as the position and the momentum of a particle, are expected
</p>
<p>to be capable of continuous variation. In accordance with the postulates of quantum
</p>
<p>mechanics, the observables corresponding to these quantities must possess continu-
</p>
<p>ous spectrum of eigenvalues. Let ξ̂ denote such an observable. Then,
</p>
<p>ξ̂ |ξ 〉= ξ |ξ 〉 , (8.189)
</p>
<p>where ξ is the eigenvalue of ξ̂ and |ξ 〉 is the corresponding eigenket. As it turns
out, the norm of |ξ 〉 is infinite if ξ belongs to a continuous spectrum, and hence |ξ 〉
cannot be normalized. Instead of now familiar
</p>
<p>〈ai|a j〉= δi j (8.190)
</p>
<p>we had for eigenkets belonging to eigenvalues of discrete spectrum, the orthonor-
</p>
<p>mality condition becomes
</p>
<p>〈ξ &prime;|ξ &prime;&prime;〉= δ (ξ &prime;&minus;ξ &prime;&prime;) , (8.191)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 &Dagger;Operator with Continuous Spectrum 343
</p>
<p>where δ is the Dirac δ -function. The closure relation must be replaced by
</p>
<p>&int;
|ξ 〉〈ξ |dξ = Î . (8.192)
</p>
<p>In this book, we shall take these results for granted. See Ref. [1] for a brief discus-
</p>
<p>sion of a more satisfactory treatment.
</p>
<p>Using (8.192), we see immediately that
</p>
<p>|α〉=
&int;
</p>
<p>|ξ 〉〈ξ |α〉dξ , (8.193)
</p>
<p>and that
</p>
<p>〈α|β 〉= 〈α|
[&int;
</p>
<p>|ξ 〉〈ξ |dξ
]
|β 〉=
</p>
<p>&int;
〈α|ξ 〉〈ξ |β 〉dξ . (8.194)
</p>
<p>From (8.189) and (8.191), we have
</p>
<p>〈ξ &prime;|ξ̂ |ξ &prime;&prime;〉= ξ &prime;&prime;δ (ξ &prime;&minus;ξ &prime;&prime;) . (8.195)
</p>
<p>In place of (8.167), we have
</p>
<p>|〈ξ |φ〉|2dξ (8.196)
as the probability that the measurement of ξ yields the value between ξ and ξ +dξ .
</p>
<p>8.6.1 &Dagger;Position Operator and Position Eigenkets
</p>
<p>Consider a particle confined to a one-dimensional space and place the x-axis along
</p>
<p>this space. In accordance with the postulates of quantum mechanics, the position is
</p>
<p>represented by an observable, which we call a position operator x̂. If the particle
</p>
<p>is exactly at x, the measurement of its position should give us x. Thus,
</p>
<p>x̂|x〉= x|x〉 . (8.197)
</p>
<p>This concept of position operator and its eigenkets may be extended to a three-
</p>
<p>dimensional space. In doing so, we assume the existence of a simultaneous eigenket
</p>
<p>|xyz〉 of position operators x̂, ŷ, and ẑ so that three components of the position vector
of the particle can be determined simultaneously. (See Sect. 8.5.)
</p>
<p>In what follows, it proves convenient to use x1, x2, x3 in place of more familiar x,
</p>
<p>y, z. In this new notation, we have
</p>
<p>[x̂i, x̂ j] = 0 i, j = 1,2,3 (8.198)
</p>
<p>and
</p>
<p>x̂1|x1x2x3〉= x1|x1x2x3〉 , x̂2|x1x2x3〉= x2|x1x2x3〉 , x̂3|x1x2x3〉= x3|x1x2x3〉 .
(8.199)</p>
<p/>
</div>
<div class="page"><p/>
<p>344 8 Quantum Formulation
</p>
<p>These three equations may be written more compactly as
</p>
<p>x̂|x〉= x|x〉 . (8.200)
</p>
<p>Because x̂ is assumed to be an observable, {|x〉} forms a basis and hence the closure
relation &int;
</p>
<p>|x〉〈x|dx = Î (8.201)
</p>
<p>holds. As a result, any state ket can be expressed as a linear combination of |x〉:
</p>
<p>|α〉=
&int;
</p>
<p>|x〉〈x|α〉dx , (8.202)
</p>
<p>in which the complex-valued function φα(x) := 〈x|α〉 of x is called the wave func-
tion. A scalar product between two kets is now expressed as
</p>
<p>〈β |α〉=
&int;
</p>
<p>〈β |x〉〈x|α〉dx=
&int;
</p>
<p>φ &lowast;β (x)φα(x)dx . (8.203)
</p>
<p>This is the scalar product between two complex-valued function we saw in Exam-
</p>
<p>ple 8.7.
</p>
<p>8.7 &dagger;Linear Translation
</p>
<p>Suppose that a system is in the state represented by |α〉 &isin; Vk. The system may be
just a particle, for example. If we take this system and move it by ∆x as a whole,
the resulting ket, let us call it |α &prime;〉 &isin;Vk, in general will be different from the original
one. Insofar as we may perform such operation for any |α〉 &isin;Vk, there should be an
operator T̂∆x representing this linear translation. Thus,
</p>
<p>|α &prime;〉= T̂∆x|α〉 . (8.204)
</p>
<p>We call T̂∆x the translation operator and demand that it is linear. In this section,
</p>
<p>we seek for the explicit expression for this operator and study its connection to x̂.
</p>
<p>8.7.1 &dagger;Properties of Linear Translation
</p>
<p>On physical grounds, we expect T̂∆x to satisfy the following conditions:
</p>
<p>a. T̂∆x preserves the norm of |α〉. Thus,
</p>
<p>〈α &prime;|α &prime;〉= 〈α|T̂ &dagger;∆xT̂∆x|α〉= 〈α|α〉 , (8.205)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 &dagger;Linear Translation 345
</p>
<p>for which it is sufficient that
</p>
<p>T̂
&dagger;
∆xT̂∆x = Î . (8.206)
</p>
<p>To understand the reason for demanding this property, recall that
</p>
<p>〈α|α〉=
r
</p>
<p>&sum;
i=1
</p>
<p>〈α|ai〉〈ai|α〉=
r
</p>
<p>&sum;
i=1
</p>
<p>|〈ai|α〉|2 (8.207)
</p>
<p>is the probability that the measurement of some dynamical variable A gives any
</p>
<p>of its possible values as an outcome. This probability has to be one and should
</p>
<p>not change upon linear translation of the system.
</p>
<p>b. Two successive linear translations, ∆x1 followed by ∆x2, should produce a state
identical to the one produced by a single linear translation by ∆x1 +∆x2. Thus,
</p>
<p>T̂∆x2 T̂∆x1 = T̂∆x1+∆x2 . (8.208)
</p>
<p>c. The state of the system will not change at all if no translation is performed. We
</p>
<p>also do not expect any discontinuity to be present at ∆x= 0. Thus,
</p>
<p>lim
∆x&rarr;0
</p>
<p>T̂∆x = T̂∆x=0 = Î . (8.209)
</p>
<p>Let us explore some immediate consequence of these requirements. Since ∆x1 +
∆x2 = ∆x2 +∆x1, (8.208) implies that
</p>
<p>T̂∆x2 T̂∆x1 = T̂∆x1 T̂∆x2 , (8.210)
</p>
<p>that is,
</p>
<p>[T̂∆x1 , T̂∆x2 ] = 0 . (8.211)
</p>
<p>Let ∆x2 = ∆x and ∆x1 =&minus;∆x in (8.208), and apply (8.209). We find
</p>
<p>T̂∆xT̂&minus;∆x = Î . (8.212)
</p>
<p>Multiplying this equation by T̂
&dagger;
∆x from the left and using (8.206), we arrive at
</p>
<p>T̂&minus;∆x = T̂
&dagger;
∆x . (8.213)
</p>
<p>These properties of T̂∆x guide us in our search for a reasonable guess on its form.
</p>
<p>To avoid unnecessary complications, let us confine our attention to an infinitesi-
</p>
<p>mal linear translation by δx so that higher order terms in δx can be safely ignored.
Then, all of the properties we listed for the linear translation operator are satisfied
</p>
<p>by the following form of T̂ δx:
</p>
<p>T̂ δx = Î &minus; iK̂ &middot;δx , (8.214)
</p>
<p>where K̂ is a Hermitian operator and
</p>
<p>K̂ &middot;δx := K̂1δx1 + K̂2δx2 + K̂3δx3 . (8.215)</p>
<p/>
</div>
<div class="page"><p/>
<p>346 8 Quantum Formulation
</p>
<p>We note that T̂ δx should not change the dimension of a ket it is acting on. Conse-
</p>
<p>quently, K̂ has the dimension of inverse length.
</p>
<p>Exercise 8.24. Prove that (8.214) satisfies (8.206), (8.208), and (8.209) to the first
</p>
<p>order of δx. ///
</p>
<p>8.7.2 &dagger;Commutation Relations
</p>
<p>What effect would the linear translation have on the position eigenket |x〉? By
answering this question, we will be lead to discover the commutation relation
</p>
<p>between the position operator x̂ and K̂. Let |α &prime;〉 denote the state ket obtained by
linearly translating |α〉 by δx. According to the definition given in Sect. 8.6.1, the
corresponding wave functions are given by
</p>
<p>φα(x) = 〈x|α〉 and φα &prime;(x) = 〈x|α &prime;〉= 〈x|T̂ δx|α〉 , (8.216)
</p>
<p>respectively. Because the two states, |α〉 and |α &prime;〉, are related by the displacement
δx, it seems utterly natural to expect that their corresponding wave functions should
also be so related. That is, the value of φα &prime; at x is equal to that of φα at x&minus;δx:
</p>
<p>φα &prime;(x) = φα(x&minus;δx) . (8.217)
</p>
<p>Using (8.216), we rewrite this equation as
</p>
<p>〈x|T̂ δx|α〉= 〈x&minus;δx|α〉 . (8.218)
</p>
<p>Since |α〉 &isin;Vk is arbitrary, we conclude that
</p>
<p>〈x|T̂ δx = 〈x&minus;δx| . (8.219)
</p>
<p>Taking the adjoint,
</p>
<p>T̂
&dagger;
δx|x〉= |x&minus;δx〉 . (8.220)
</p>
<p>Recalling (8.213),
</p>
<p>T̂&minus;δx|x〉= |x&minus;δx〉 . (8.221)
But because this equality holds for any δx, it should hold when we replace δx by
&minus;δx:
</p>
<p>T̂ δx|x〉= |x+δx〉 , (8.222)
Multiplying both sides of this equation by x̂, we find
</p>
<p>x̂T̂ δx|x〉= x̂|x+δx〉= (x+δx)|x+δx〉 , (8.223)
</p>
<p>where we used (8.200). On the other hand,
</p>
<p>T̂ δxx̂|x〉= T̂ δxx|x〉= xT̂ δx|x〉= x|x+δx〉 . (8.224)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 &dagger;Linear Translation 347
</p>
<p>The second equality holds because x is simply a vector having real numbers as its
</p>
<p>components. The last step follows from (8.222). The last two equations indicate
</p>
<p>[x̂, T̂ δx]|x〉= δx|x+δx〉 . (8.225)
</p>
<p>Expanding |x+δx〉 around δx= 0, and retaining the leading term only, we find
</p>
<p>[x̂, T̂ δx]|x〉= δx|x〉 . (8.226)
</p>
<p>If a Taylor series expansion of a ket leaves you uncomfortable, we can proceed as
</p>
<p>follows:
</p>
<p>〈φ |[x̂, T̂ δx]|x〉= 〈φ |δx|x+δx〉= δxφ(x+δx)&asymp; δxφ(x) = δx〈φ |x〉 . (8.227)
</p>
<p>But since φ in this equation is arbitrary, this result implies (8.226). Furthermore,
since {|x〉} forms a basis, (8.226) holds even if |x〉 is replaced by an arbitrary ket.
So, we conclude that
</p>
<p>[x̂, T̂ δx] = δx . (8.228)
</p>
<p>Using (8.214), we transform this equation as follows:
</p>
<p>x̂(Î &minus; iK̂ &middot;δx)&minus; (Î &minus; iK̂ &middot;δx)x̂ = δx
&minus;ix̂(K̂ &middot;δx)+ i(K̂ &middot;δx)x̂ = δx
</p>
<p>x̂(K̂ &middot;δx)&minus; (K̂ &middot;δx)x̂ = iδx . (8.229)
</p>
<p>The first (or the x1-) component of this equation reads
</p>
<p>x̂1(K̂1δx1 + K̂2δx2 + K̂3δx3)&minus; (K̂1δx1 + K̂2δx2 + K̂3δx3)x̂1 = iδx1 . (8.230)
</p>
<p>Since δx is arbitrary, we conclude that
</p>
<p>x̂1K̂1 &minus; K̂1x̂1 = i , x̂1K̂2 &minus; K̂2x̂1 = 0 , and x̂1K̂3 &minus; K̂3x̂1 = 0 . (8.231)
</p>
<p>Six additional equations follow from the other components of (8.229). The resulting
</p>
<p>(total of nine) equations can be summarized compactly as
</p>
<p>[x̂i, K̂ j] = iδi j . (8.232)
</p>
<p>We introduce a Hermitian operator defined by
</p>
<p>p̂= h̄K̂ (8.233)
</p>
<p>and call it the momentum operator, whose eigenvalue, by definition, is the
</p>
<p>momentum.
</p>
<p>Justification of this seemingly arbitrary definition of momentum must be sought
</p>
<p>among the physical attributes of the eigenvalues predicted by the theory. See
</p>
<p>Sect. 8.15, for example. Here, we simply remark that p̂ does have the correct dimen-</p>
<p/>
</div>
<div class="page"><p/>
<p>348 8 Quantum Formulation
</p>
<p>sion of momentum because of the factor h̄ := h/2π , in which h is the Planck con-
stant.
</p>
<p>Now, we rewrite (8.232) in terms of the momentum operator as
</p>
<p>[x̂i, p̂ j] = ih̄δi j . (8.234)
</p>
<p>The appearance of δi j is quite reasonable on physical grounds if we note that p̂ is
a part of the linear translation operator. When you measure the xi coordinate of a
</p>
<p>particle, it certainly matters whether or not you move the particle in the xi-direction
</p>
<p>prior to the measurement, while moving it in the x j-direction (i 	= j) should not have
any effect. Substituting (8.234) into (8.181), we arrive at
</p>
<p>&radic;
〈xi∆ 2〉QM〈p j∆ 2〉QM &ge;
</p>
<p>h̄
</p>
<p>2
δi j , (8.235)
</p>
<p>which is essentially (3.155).
</p>
<p>The commutation relations among p̂&rsquo;s are obtained from (8.211) and (8.214):
</p>
<p>[ p̂i, p̂ j] = 0 . (8.236)
</p>
<p>Equations (8.198), (8.234), and (8.236) should be contrasted against their classical
</p>
<p>counterparts given in Exercise 1.17.
</p>
<p>8.7.3 &dagger;Momentum Eigenket
</p>
<p>Combining (8.214), (8.216), (8.233), we find
</p>
<p>φα &prime;(x) =
</p>
<p>&lang;
x
</p>
<p>∣∣∣∣Î &minus;
i
</p>
<p>h̄
p̂ &middot;δx
</p>
<p>∣∣∣∣α
&rang;
= φα(x)&minus;
</p>
<p>i
</p>
<p>h̄
δx &middot; 〈x|p̂|α〉 . (8.237)
</p>
<p>Recall that we kept only up to the first order terms of δx in arriving at this equation.
Under the same approximation, (8.217) gives
</p>
<p>φα &prime;(x) = φα(x)&minus;δx &middot;&nabla;φα(x) . (8.238)
</p>
<p>Comparing these two equations, and noting that δx is arbitrary, we find that
</p>
<p>〈x|p̂|α〉=&minus;ih̄&nabla;〈x|α〉 . (8.239)
</p>
<p>On the right-hand side, 〈x|α〉 is the wave function for state |α〉. On the left, we
have the wave function corresponding to the ket p̂|α〉, that is, the state |α〉 after
it is modified by the operator p̂. Equation (8.239) indicates that the effect of the
</p>
<p>momentum operator p̂ in the x-representation is to take the derivative of the wave
</p>
<p>function. More pictorially,
</p>
<p>p̂ : φα(x)&minus;&rarr;&minus;ih̄&nabla;φα(x) . (8.240)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 &dagger;Linear Translation 349
</p>
<p>Using (8.239), we have
</p>
<p>〈β |p̂|α〉=
&int;
</p>
<p>〈β |x〉〈x|p̂|α〉dx=
&int;
</p>
<p>φ &lowast;β (x)(&minus;ih̄&nabla;)φα(x)dx , (8.241)
</p>
<p>which is one of the key equations in quantum mechanics.
</p>
<p>Other useful results follow from (8.239). Because |α〉 is arbitrary, we can sub-
stitute any ket in its place. In particular, let us use |p〉, a momentum eigenket corre-
sponding to the eigenvalue p:
</p>
<p>p̂|p〉= p|p〉 . (8.242)
This gives
</p>
<p>p〈x|p〉=&minus;ih̄&nabla;〈x|p〉 . (8.243)
But, this is just a differential equation for 〈x|p〉, with the solution
</p>
<p>〈x|p〉= ce ih̄ p&middot;x . (8.244)
</p>
<p>So, the momentum eigenket in the x-representation is a plane wave.
</p>
<p>Exercise 8.25. Verify that (8.244) satisfies (8.243). ///
</p>
<p>How do we determine the constant c? As with x̂, we assume that p̂ is an observ-
</p>
<p>able and hence {|p〉} forms a basis, that is,
</p>
<p>|α〉=
&int;
</p>
<p>|p〉〈p|α〉dp (8.245)
</p>
<p>for any |α〉. From (D.53), we see that
&int;
</p>
<p>eik&middot;xdx= (2π)3δ (k) , (8.246)
</p>
<p>where δ (k) := δ (k1)δ (k2)δ (k3) by definition. By means of this identity, we find
</p>
<p>δ (x&minus;x&prime;) = 〈x|x&prime;〉=
&int;
</p>
<p>〈x|p〉〈p|x&prime;〉dp= |c|2(2π h̄)3δ (x&minus;x&prime;) . (8.247)
</p>
<p>While c can be any complex number satisfying this equation, the convention dictates
</p>
<p>that we take the real positive root. Thus,
</p>
<p>c =
1
</p>
<p>(2π h̄)3/2
. (8.248)
</p>
<p>Now that we have a complete expression for 〈x|p〉, we can immediately deduce
the following relationship between the coordinate-space wave function φα(x) =
〈x|α〉 and the momentum&ndash;space wave function φα(p) := 〈p|α〉:
</p>
<p>φα(x) = 〈x|α〉=
&int;
</p>
<p>〈x|p〉〈p|α〉dp= 1
(2π h̄)3/2
</p>
<p>&int;
φα(p)e
</p>
<p>i
h̄
p&middot;xdp (8.249)</p>
<p/>
</div>
<div class="page"><p/>
<p>350 8 Quantum Formulation
</p>
<p>and
</p>
<p>φα(p) = 〈p|α〉=
&int;
</p>
<p>〈p|x〉〈x|α〉dx= 1
(2π h̄)3/2
</p>
<p>&int;
φα(x)e
</p>
<p>&minus; i
h̄
p&middot;xdx . (8.250)
</p>
<p>In other words, φα(x) and φα(p) are the Fourier transform of each other.
Finally, by setting |α〉= |x&prime;〉 in (8.239), we obtain
</p>
<p>〈x|p̂|x&prime;〉=&minus;ih̄&nabla;〈x|x&prime;〉=&minus;ih̄&nabla;δ (x&minus;x&prime;) , (8.251)
</p>
<p>where we note that the derivative is with respect to x and not x&prime;. This is the matrix
element of p̂ in the x-representation.
</p>
<p>Exercise 8.26. Show that
</p>
<p>x̂|p〉=&minus;ih̄d|p〉
dp
</p>
<p>(8.252)
</p>
<p>for a particle in a one-dimensional space. ///
</p>
<p>8.8 &dagger;Time Evolution Operator
</p>
<p>Now, we have to figure out how the state of a given system evolves with time. What
</p>
<p>we need is a linear operator that acts on a state ket |φ , t〉 at time t and converts it
to a new one |φ , t +δ t〉 at a later time t + δ t. As with the linear translation, it is
sufficient to consider an infinitesimally small interval of time. That is, we wish to
</p>
<p>find the time evolution operator Ûδ t(t) satisfying
</p>
<p>|φ , t +δ t〉= Ûδ t(t)|φ , t〉 , (8.253)
</p>
<p>where we allowed for the explicit dependence of Ûδ t(t) on t. As in the case of the
linear translation, we list a few reasonable demands to be imposed on Ûδ t(t):
</p>
<p>a. Ûδ t(t) preserves the norm of |φ , t〉:
</p>
<p>〈φ , t +δ t|φ , t +δ t〉= 〈φ , t|Ûδ t(t)&dagger;Ûδ t(t)|φ , t〉= 〈φ , t|φ , t〉 , (8.254)
</p>
<p>for which it is sufficient that
</p>
<p>Ûδ t(t)
&dagger;Ûδ t(t) = Î . (8.255)
</p>
<p>b. Two successive time translations, δ t1 followed by another δ t2, should produce a
state identical to that produced by a single time translation by δ t1 +δ t2. Thus,
</p>
<p>Ûδ t2(t +δ t1)Ûδ t1(t) = Ûδ t1+δ t2(t) . (8.256)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.9 &dagger;Û t is Unitary 351
</p>
<p>c. The state of the system will not change at all if no time has passed. We also do
</p>
<p>not expect any discontinuity to be present at δ t = 0. Thus,
</p>
<p>lim
δ t&rarr;0
</p>
<p>Ûδ t = Ûδ t=0 = Î . (8.257)
</p>
<p>Except for the use of δ t in place of ∆x, these demands are identical to what we saw
in Sect. 8.7.1. So, we can immediately write down the desired expression for Ûδ t :
</p>
<p>Ûδ t(t) = Î &minus;
i
</p>
<p>h̄
Ĥ(t)δ t , (8.258)
</p>
<p>in which Ĥ, by definition, is called Hamiltonian operator and its eigenvalues are
</p>
<p>called energy of the system.
</p>
<p>It is straightforward to verify that (8.258) is compatible with the demands we
</p>
<p>have listed. Unlike x, there is no operator corresponding to t. As in classical mechan-
</p>
<p>ics, t is a label indicating when things are happening. So, there is no commutation
</p>
<p>relation to speak of here.
</p>
<p>From here, the celebrated Schrödinger equation is just a stone&rsquo;s through away.
</p>
<p>Using (8.258) in (8.253),
</p>
<p>|φ , t +δ t〉=
[
</p>
<p>Î &minus; i
h̄
</p>
<p>Ĥ(t)δ t
</p>
<p>]
|φ , t〉 . (8.259)
</p>
<p>Moving Î|φ , t〉= |φ , t〉 to the left and dividing the resulting expression by δ t,
</p>
<p>|φ , t +δ t〉&minus; |φ , t〉
δ t
</p>
<p>=&minus; i
h̄
</p>
<p>Ĥ(t)|φ , t〉 . (8.260)
</p>
<p>Taking the δ t &rarr; 0 limit, we find
</p>
<p>ih̄
d|φ , t〉
</p>
<p>dt
= Ĥ(t)|φ , t〉 , (8.261)
</p>
<p>which is the Schrödinger equation written for a state ket. The form this equation
</p>
<p>takes in r-representation is given in Sect. 8.13.
</p>
<p>8.9 &dagger;Û t is Unitary
</p>
<p>From (8.258), we see immediately that the infinitesimal time evolution operator
</p>
<p>Ûδ t(t) is unitary:
</p>
<p>Û
&dagger;
δ t(t)Ûδ t(t) = Ûδ t(t)Û
</p>
<p>&dagger;
δ t(t) = Î . (8.262)</p>
<p/>
</div>
<div class="page"><p/>
<p>352 8 Quantum Formulation
</p>
<p>Since any finite duration of time can be divided into many infinitesimal time inter-
</p>
<p>vals, (8.262) can be used to establish
</p>
<p>Û
&dagger;
t Û t = Û tÛ
</p>
<p>&dagger;
t = Î , (8.263)
</p>
<p>where Û t := Û t(0) is the time evolution operator that converts the state ket at t = 0
to that at time t. To see this more explicitly, let
</p>
<p>∆ t :=
t
</p>
<p>n
and ti := i∆ t , i = 0, . . . ,n&minus;1 (8.264)
</p>
<p>and write
</p>
<p>Û t = Û∆ t(tn&minus;1)Û∆ t(tn&minus;2) &middot; &middot; &middot;Û∆ t(t1)Û∆ t(t0) . (8.265)
Taking the adjoint of this equation, we obtain
</p>
<p>Û
&dagger;
t = Û
</p>
<p>&dagger;
∆ t(t0) ,Û
</p>
<p>&dagger;
∆ t(t1) &middot; &middot; &middot;Û
</p>
<p>&dagger;
∆ t(tn&minus;2)Û
</p>
<p>&dagger;
∆ t(tn&minus;1) , (8.266)
</p>
<p>In the limit of n &rarr; &infin;, ∆ t becomes infinitesimally small and (8.262) applies:
</p>
<p>Û
&dagger;
∆ t(ti)Û∆ t(ti) = Û∆ t(ti)Û
</p>
<p>&dagger;
∆ t(ti) = Î , i = 0, . . . ,n&minus;1 . (8.267)
</p>
<p>Combining (8.265), (8.266), and (8.267), we arrive at (8.263).
</p>
<p>8.10 &Dagger;Formal Solution of the Schrödinger Equation
</p>
<p>In a purely formal manner, (8.261) can be solved for a general Hamiltonian Ĥ.
</p>
<p>We say that the solution is only formal in the sense that it may be very difficult to
</p>
<p>evaluate the resulting expression for |φ , t〉. Nevertheless, the formal solution is often
very useful as it allows us to expresses |φ , t〉 very compactly.
</p>
<p>8.10.1 &Dagger;Time-Independent Ĥ
</p>
<p>Let us first assume that Ĥ is independent of t. To find the formal solution of (8.261),
</p>
<p>we first expand |φ , t〉 into the Taylor series around t = 0:
</p>
<p>|φ , t〉= |φ ,0〉+ d
dt
|φ , t〉
</p>
<p>∣∣∣∣
t=0
</p>
<p>t +
1
</p>
<p>2
</p>
<p>d2
</p>
<p>dt2
|φ , t〉
</p>
<p>∣∣∣∣∣
t=0
</p>
<p>t2 + &middot; &middot; &middot; . (8.268)
</p>
<p>From (8.261),
d
</p>
<p>dt
|φ , t〉=&minus; i
</p>
<p>h̄
Ĥ|φ , t〉. (8.269)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.10 &Dagger;Formal Solution of the Schrödinger Equation 353
</p>
<p>Applying this formula to d|φ , t〉/dt, which is just a ket, we find
</p>
<p>d2
</p>
<p>dt2
|φ , t〉=&minus; i
</p>
<p>h̄
Ĥ
</p>
<p>[
d
</p>
<p>dt
|φ , t〉
</p>
<p>]
=&minus; i
</p>
<p>h̄
Ĥ
</p>
<p>[
&minus; i
</p>
<p>h̄
Ĥ|φ , t〉
</p>
<p>]
=
</p>
<p>(
&minus; i
</p>
<p>h̄
Ĥ
</p>
<p>)2
|φ , t〉 . (8.270)
</p>
<p>Continuing in this manner, we see that
</p>
<p>dn
</p>
<p>dtn
|φ , t〉=
</p>
<p>(
&minus; i
</p>
<p>h̄
Ĥ
</p>
<p>)n
|φ , t〉 , n = 1,2, . . . (8.271)
</p>
<p>Setting t = 0 in this equation and using the resulting expression in (8.268), we have
</p>
<p>|φ , t〉=
[
</p>
<p>&infin;
</p>
<p>&sum;
n=0
</p>
<p>1
</p>
<p>n!
</p>
<p>(
&minus; i
</p>
<p>h̄
Ĥt
</p>
<p>)n]
|φ ,0〉 , (8.272)
</p>
<p>which is the desired formal solution. This equation shows that
</p>
<p>Û t := Û t(0) =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>1
</p>
<p>n!
</p>
<p>(
&minus; i
</p>
<p>h̄
Ĥt
</p>
<p>)n
(8.273)
</p>
<p>for a time-independent Ĥ. Using this result, it is straightforward to show that
</p>
<p>dÛ t
</p>
<p>dt
=&minus; i
</p>
<p>h̄
ĤÛ t , (8.274)
</p>
<p>which is the Schrödinger equation written for Û t . Multiplying both sides of (8.274)
</p>
<p>from the right by |φ ,0〉 and noting that this ket is independent of t, we recover
(8.261).
</p>
<p>Exercise 8.27. Derive (8.274) from (8.273). ///
</p>
<p>As we saw in Sect. 8.3.2, a function of an operator is defined in terms of the
</p>
<p>Maclaurin expansion of the function. Recalling (B.5), we see that
</p>
<p>Û t = e
&minus; i
</p>
<p>h̄
Ĥt , (8.275)
</p>
<p>and hence
</p>
<p>|φ , t〉= e&minus; ih̄ Ĥt |φ ,0〉 . (8.276)
</p>
<p>8.10.2 &dagger;Time-Dependent Ĥ
</p>
<p>Equation (8.276) cannot be true if Ĥ depends on t. This becomes immediately clear
</p>
<p>if we ask ourselves at which instant Ĥ in the exponent should be evaluated. We must
</p>
<p>also allow for the possibility that Ĥ evaluated at different instances may not even
</p>
<p>commute.</p>
<p/>
</div>
<div class="page"><p/>
<p>354 8 Quantum Formulation
</p>
<p>To make a progress, we first integrate (8.261) from t = 0 to t:
</p>
<p>|φ , t〉= |φ ,0〉&minus; i
h̄
</p>
<p>&int; t
</p>
<p>0
Ĥ(t1)|φ , t1〉dt1 . (8.277)
</p>
<p>This is not a particularly useful form of expressing |φ , t〉 since the right-hand side
still contains the unknown ket |φ , t1〉. But, the ket can be expressed using (8.277) as
</p>
<p>|φ , t1〉= |φ ,0〉&minus;
i
</p>
<p>h̄
</p>
<p>&int; t1
0
</p>
<p>Ĥ(t2)|φ , t2〉dt2 . (8.278)
</p>
<p>Substituting this expression into (8.277), we obtain
</p>
<p>|φ , t〉= |φ ,0〉+
[
&minus; i
</p>
<p>h̄
</p>
<p>&int; t
</p>
<p>0
Ĥ(t1)dt1
</p>
<p>]
|φ ,0〉+
</p>
<p>(
&minus; i
</p>
<p>h̄
</p>
<p>)2 &int; t
0
</p>
<p>&int; t1
0
</p>
<p>Ĥ(t1)Ĥ(t2)|φ , t2〉dt2dt1 ,
(8.279)
</p>
<p>where we pulled |φ ,0〉 out of the integral sign because it is independent of t. Now,
we can use (8.277) once again to express |φ , t2〉 as in (8.278). Continuing in this
manner, we see that the time evolution operator for a time-dependent Ĥ is given by
</p>
<p>Û t =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>(
&minus; i
</p>
<p>h̄
</p>
<p>)n &int; t
0
</p>
<p>&int; t1
0
</p>
<p>&middot; &middot; &middot;
&int; tn&minus;1
</p>
<p>0
Ĥ(t1)Ĥ(t2) &middot; &middot; &middot; Ĥ(tn)dtn &middot; &middot; &middot;dt2dt1 , (8.280)
</p>
<p>in which n = 0 term in the summation is the identity operator Î. This result is known
as the Neumann series.
</p>
<p>Iterated integrals in which a limit of an integral depends on the values of the
</p>
<p>other integration variables is rather tedious to handle. However, we can circumvent
</p>
<p>this difficulty through a clever trick. To this end, let us consider the integrals for the
</p>
<p>n = 2 term:
</p>
<p>I2 :=
&int; t
</p>
<p>0
</p>
<p>&int; t1
0
</p>
<p>Ĥ(t1)Ĥ(t2)dt2dt1 . (8.281)
</p>
<p>Because t1 and t2 are simply the integration variables, we have
</p>
<p>I2 =
&int; t
</p>
<p>0
</p>
<p>&int; t2
0
</p>
<p>Ĥ(t2)Ĥ(t1)dt1dt2 . (8.282)
</p>
<p>In (8.281), the integral over t2 runs from 0 to t1. Thus, 0 &le; t2 &le; t1. On the other hand,
we have 0 &le; t1 &le; t2 in (8.282).
</p>
<p>Let us now define the time-ordering operator T̂ &gt; which acts on a product of
</p>
<p>time-dependent operators and rearranges them so that the time decreases from left
</p>
<p>to right. Thus, if ta &lt; tb, we have
</p>
<p>T̂ &gt;
</p>
<p>[
Â(ta)B̂(tb)
</p>
<p>]
= T̂ &gt;
</p>
<p>[
B̂(tb)Â(ta)
</p>
<p>]
= B̂(tb)Â(ta) . (8.283)
</p>
<p>Similarly if we have a product of more than two time-dependent operators. By
</p>
<p>means of T̂ &gt;, we can rewrite (8.281) and (8.282) as</p>
<p/>
</div>
<div class="page"><p/>
<p>8.10 &Dagger;Formal Solution of the Schrödinger Equation 355
</p>
<p>t
</p>
<p>t
</p>
<p>t1
</p>
<p>t2
</p>
<p>0
</p>
<p>A
</p>
<p>B
</p>
<p>t 2
</p>
<p>t 1
</p>
<p>Fig. 8.1 The integral (8.284) is carried out over the lower triangle (△A) while (8.285) is over the
upper triangle (△B). Taken together, they cover the square.
</p>
<p>I2 =
&int; t
</p>
<p>0
</p>
<p>&int; t1
0
</p>
<p>T̂ &gt;
</p>
<p>[
Ĥ(t1)Ĥ(t2)
</p>
<p>]
dt2dt1 (8.284)
</p>
<p>and
</p>
<p>I2 =
&int; t
</p>
<p>0
</p>
<p>&int; t2
0
</p>
<p>T̂ &gt;
</p>
<p>[
Ĥ(t1)Ĥ(t2)
</p>
<p>]
dt1dt2 , (8.285)
</p>
<p>respectively. These two integrals share the same integrand. From Fig. 8.1, we see
</p>
<p>that these two integrals, taken together, cover the square region defined by 0 &le; t1 &le; t
and 0 &le; t2 &le; t. Thus, adding (8.284) and (8.285), we arrive at
</p>
<p>I2 =
1
</p>
<p>2
</p>
<p>&int; t
</p>
<p>0
</p>
<p>&int; t
</p>
<p>0
T̂ &gt;
</p>
<p>[
Ĥ(t1)Ĥ(t2)
</p>
<p>]
dt1dt2 . (8.286)
</p>
<p>In a similar manner, it is possible to show that the n-dimensional integral in (8.280)
</p>
<p>can be written as
</p>
<p>In =
1
</p>
<p>n!
</p>
<p>&int; t
</p>
<p>0
&middot; &middot; &middot;
</p>
<p>&int; t
</p>
<p>0
T̂ &gt;
</p>
<p>[
Ĥ(t1) &middot; &middot; &middot; Ĥ(tn)
</p>
<p>]
dt1 &middot; &middot; &middot;dtn . (8.287)
</p>
<p>In this way, we arrive at
</p>
<p>Û t =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>1
</p>
<p>n!
</p>
<p>(
&minus; i
</p>
<p>h̄
</p>
<p>)n &int; t
0
&middot; &middot; &middot;
</p>
<p>&int; t
</p>
<p>0
T̂ &gt;
</p>
<p>[
Ĥ(t1) &middot; &middot; &middot; Ĥ(tn)
</p>
<p>]
dt1 &middot; &middot; &middot;dtn , (8.288)
</p>
<p>which is known as the Dyson series. As a short-hand notation, it is common to
</p>
<p>express this result in terms of the time-ordered exponential as
</p>
<p>Û t = T̂ &gt; exp
</p>
<p>[
&minus; i
</p>
<p>h̄
</p>
<p>&int; t
</p>
<p>0
Ĥ(t &prime;)dt &prime;
</p>
<p>]
. (8.289)</p>
<p/>
</div>
<div class="page"><p/>
<p>356 8 Quantum Formulation
</p>
<p>Exercise 8.28. Show that Û t given by (8.288) satisfies the Schrödinger equation:
</p>
<p>dÛ t
</p>
<p>dt
=&minus; i
</p>
<p>h̄
Ĥ(t)Û t (8.290)
</p>
<p>for the time evolution operator Û t . ///
</p>
<p>If Ĥ(ta) and Ĥ(tb) commutes for any ta and tb in the interval [0, t], that is, if
</p>
<p>Ĥ(ta)Ĥ(tb) = Ĥ(tb)Ĥ(ta) for any t1, t2 &isin; [0, t] (8.291)
</p>
<p>then we have
</p>
<p>T̂ &gt;
</p>
<p>[
Ĥ(t1) &middot; &middot; &middot; Ĥ(tn)
</p>
<p>]
= Ĥ(t1) &middot; &middot; &middot; Ĥ(tn) , (8.292)
</p>
<p>that is, a product of Ĥ&rsquo;s evaluated at different instances defines an unique operator
</p>
<p>regardless of the order in which Ĥ&rsquo;s are multiplied. In this case, (8.287) reduces to
</p>
<p>In =
1
</p>
<p>n!
</p>
<p>&int; t
</p>
<p>0
&middot; &middot; &middot;
</p>
<p>&int; t
</p>
<p>0
Ĥ(t1) &middot; &middot; &middot; Ĥ(tn)dt1 &middot; &middot; &middot;dtn =
</p>
<p>1
</p>
<p>n!
</p>
<p>[&int; t
0
</p>
<p>Ĥ(t &prime;)dt &prime;
]n
</p>
<p>(8.293)
</p>
<p>and we now have
</p>
<p>Û t = exp
</p>
<p>[
&minus; i
</p>
<p>h̄
</p>
<p>&int; t
</p>
<p>0
Ĥ(t &prime;)dt &prime;
</p>
<p>]
. (8.294)
</p>
<p>in place of (8.289). If Ĥ is actually independent of t, this expression reduces to
</p>
<p>(8.275).
</p>
<p>8.11 &Dagger;Heisenberg&rsquo;s Equation of Motion
</p>
<p>Thus far, we have taken a point of view that the state of a quantum mechanical
</p>
<p>system is represented by a state ket |φ , t〉 with its time evolution dictated by the
Schrödinger equation. The time dependence of the average value 〈A〉QM(t) of a
dynamical variable A is a result of this time evolution and a possible explicit time
</p>
<p>dependence of Â. This view point is called the Schrödinger picture.
</p>
<p>An alternative view point emerges through the use of the time evolution operator
</p>
<p>Û t :
</p>
<p>〈A〉QM(t) = 〈φ , t|Â|φ , t〉= 〈φ ,0|Û&dagger;t ÂÛ t |φ ,0〉 , (8.295)
</p>
<p>where we used the formal solution,
</p>
<p>|φ , t〉= Û t |φ ,0〉 , (8.296)
</p>
<p>of the Schrödinger equation from Sect. 8.10 and and its adjoint:
</p>
<p>〈φ , t|= 〈φ ,0|Û&dagger;t . (8.297)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.11 &Dagger;Heisenberg&rsquo;s Equation of Motion 357
</p>
<p>We now interpret (8.295) as indicating that the time dependence of 〈A〉QM(t) arises
entirely from that of the time-dependent operator
</p>
<p>ÂH(t) := Û
&dagger;
t ÂÛ t , (8.298)
</p>
<p>while the state ket |φ ,0〉 remains frozen in the ket space. This point of view is called
the Heisenberg picture. By construction, both pictures lead to the same prediction
</p>
<p>for 〈A〉QM(t).
Because Û t=0 = Î, the newly defined operator ÂH(t) satisfies the initial condition
</p>
<p>that
</p>
<p>ÂH(0) = Â . (8.299)
</p>
<p>What is the differential equation governing its time evolution? To find out, let us
</p>
<p>take the time derivative of (8.298):
</p>
<p>dÂH
</p>
<p>dt
=
</p>
<p>dÛ
&dagger;
t
</p>
<p>dt
ÂÛ t +Û
</p>
<p>&dagger;
t Â
</p>
<p>dÛ t
</p>
<p>dt
+Û
</p>
<p>&dagger;
t
</p>
<p>&part; Â
</p>
<p>&part; t
Û t . (8.300)
</p>
<p>As we shall see below, the right-hand side can be rewritten to highlight the for-
</p>
<p>mal similarity between this equation from quantum mechanics and the equation of
</p>
<p>motion (1.185) from classical mechanics.
</p>
<p>8.11.1 &Dagger;Time-Independent Ĥ
</p>
<p>For time-independent Hamiltonian, (8.274) and its adjoint
</p>
<p>dÛ
&dagger;
t
</p>
<p>dt
=
</p>
<p>i
</p>
<p>h̄
Û
</p>
<p>&dagger;
t Ĥ (8.301)
</p>
<p>give
</p>
<p>dÂH
</p>
<p>dt
=
</p>
<p>i
</p>
<p>h̄
Û
</p>
<p>&dagger;
t ĤÂÛ t &minus;
</p>
<p>i
</p>
<p>h̄
Û
</p>
<p>&dagger;
t ÂĤÛ t +Û
</p>
<p>&dagger;
t
</p>
<p>&part; Â
</p>
<p>&part; t
Û t . (8.302)
</p>
<p>Following (8.298), we write
</p>
<p>(
&part; Â
</p>
<p>&part; t
</p>
<p>)
</p>
<p>H
</p>
<p>:= Û
&dagger;
t
</p>
<p>&part; Â
</p>
<p>&part; t
Û t . (8.303)
</p>
<p>Since Û t and Û
&dagger;
t are both functions of Ĥ and Ĥ is independent of time, we have
</p>
<p>[Ĥ,Û t ] = 0 and [Ĥ,Û
&dagger;
t ] = 0 . (8.304)</p>
<p/>
</div>
<div class="page"><p/>
<p>358 8 Quantum Formulation
</p>
<p>This allows us to rewrite (8.302) as
</p>
<p>dÂH
</p>
<p>dt
=
</p>
<p>i
</p>
<p>h̄
ĤÂH(t)&minus;
</p>
<p>i
</p>
<p>h̄
ÂH(t)Ĥ +
</p>
<p>(
&part; Â
</p>
<p>&part; t
</p>
<p>)
</p>
<p>H
</p>
<p>=
1
</p>
<p>ih̄
[ÂH(t), Ĥ]+
</p>
<p>(
&part; Â
</p>
<p>&part; t
</p>
<p>)
</p>
<p>H
</p>
<p>. (8.305)
</p>
<p>This is Heisenberg&rsquo;s equation of motion to be contrasted against (1.185).
</p>
<p>8.11.2 &dagger;Time-Dependent Ĥ
</p>
<p>Let us turn to a time-dependent Ĥ next. In this case, we must use (8.290) and its
</p>
<p>adjoint in place of (8.274) and (8.301). This gives (8.302) with Ĥ replaced by Ĥ(t).
However, (8.304) can be used only if (8.291) holds. What do we do if it does not?
</p>
<p>This more general situations can be handled by means of (8.263), which allows
</p>
<p>us to rewrite (8.302), with Ĥ replaced by Ĥ(t), as
</p>
<p>dÂH
</p>
<p>dt
=
</p>
<p>i
</p>
<p>h̄
Û
</p>
<p>&dagger;
t Ĥ(t)Û tÛ
</p>
<p>&dagger;
t ÂÛ t &minus;
</p>
<p>i
</p>
<p>h̄
Û
</p>
<p>&dagger;
t ÂÛ tÛ
</p>
<p>&dagger;
t Ĥ(t)Û t +
</p>
<p>(
&part; Â
</p>
<p>&part; t
</p>
<p>)
</p>
<p>H
</p>
<p>=
i
</p>
<p>h̄
ĤH(t)ÂH(t)&minus;
</p>
<p>i
</p>
<p>h̄
ÂH(t)ĤH(t)+
</p>
<p>(
&part; Â
</p>
<p>&part; t
</p>
<p>)
</p>
<p>H
</p>
<p>=
1
</p>
<p>ih̄
[ÂH(t), ĤH(t)]+
</p>
<p>(
&part; Â
</p>
<p>&part; t
</p>
<p>)
</p>
<p>H
</p>
<p>, (8.306)
</p>
<p>where, following (8.298), we defined
</p>
<p>ĤH(t) := Û
&dagger;
t Ĥ(t)Û t . (8.307)
</p>
<p>If (8.291) holds, then so does (8.304) and we obtain
</p>
<p>ĤH(t) = Ĥ(t)Û
&dagger;
t Û t = Ĥ(t) . (8.308)
</p>
<p>In this case, (8.306) becomes
</p>
<p>dÂH
</p>
<p>dt
=
</p>
<p>1
</p>
<p>ih̄
[ÂH(t), Ĥ(t)]+
</p>
<p>(
&part; Â
</p>
<p>&part; t
</p>
<p>)
</p>
<p>H
</p>
<p>, (8.309)
</p>
<p>which reduces to (8.305) if Ĥ(t) is actually independent of time.
</p>
<p>8.12 Eigenstates of Ĥ
</p>
<p>As shown in Sect. 8.8, the time evolution of a state ket is determined by the
</p>
<p>Schrödinger equation (8.261), in which Ĥ is the Hamiltonian operator. An eigen-
</p>
<p>value of the Hamiltonian and the corresponding eigenket are called, respectively,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.12 Eigenstates of Ĥ 359
</p>
<p>the energy eigenvalue and the energy eigenket. The entire collection of eigenval-
</p>
<p>ues may form either a continuum spectrum, a discrete spectrum, or a combination
</p>
<p>thereof. However, for a system of particles confined to a finite region of space, the
</p>
<p>spectrum is discrete. (See Example 8.13 and Sect. 8.14.) This is the case for sys-
</p>
<p>tems of our primary interest and we shall pretend that the spectrum is discrete in
</p>
<p>what follows. Thus, let En and |n〉 denote an energy eigenvalue and the correspond-
ing eigenket:
</p>
<p>Ĥ|n〉= En|n〉 , n = 0,1, &middot; &middot; &middot; (8.310)
with the label n distinguishing one eigenvalue (or eigenket) from others.
</p>
<p>For our purposes, it is sufficient to consider a time-independent Ĥ. Then, |n〉 is
also independent of time. In this case, (8.276) applies and we have
</p>
<p>|φ , t〉= e&minus; ih̄ Ĥt |φ ,0〉=&sum;
n
</p>
<p>e&minus;
i
h̄
</p>
<p>Ĥt |n〉〈n|φ ,0〉=&sum;
n
</p>
<p>Cne
&minus; i
</p>
<p>h̄
Ent |n〉 , (8.311)
</p>
<p>in which
</p>
<p>Cn := 〈n|φ ,0〉 (8.312)
is a constant determined by initial conditions. Equation (8.311) expresses the time
</p>
<p>dependent |φ , t〉 as a linear combination of energy eigenkets. It follows that the
time evolution of the state ket |φ , t〉 is completely determined by the solution to the
eigenvalue problem (8.310) and initial conditions.
</p>
<p>Exercise 8.29. Suppose that the state ket |φ , t〉 coincided with one of the energy
eigenkets, say |m〉 at t = 0. Being an eigenket of a time-independent Ĥ, |m〉 remains
constant. But, |φ , t〉 evolves with time. Show that the expectation value 〈A〉QM of
an observable Â remains constant. For this reason, the energy eigenkets are called
</p>
<p>stationary states. Carry out the similar calculation for more general case, in which
</p>
<p>|φ〉 is given by (8.311). Under what condition is 〈A〉QM constant? ///
</p>
<p>Exercise 8.30. Suppose that Ĥ of the system is time independent. By means of a
</p>
<p>direct calculation, show that the expectation value of the system energy 〈φ , t|Ĥ|φ , t〉
remains constant despite the fact that the state of the system |φ , t〉 evolves with time.
</p>
<p>///
</p>
<p>Example 8.11. Two state system: Let
</p>
<p>Ĥ = c[|φ1〉〈φ2|+ |φ2〉〈φ1|] , (8.313)
</p>
<p>where c is a real nonzero constant having the dimension of energy and
</p>
<p>{|φ1〉, |φ2〉} is an orthonormal basis. Let us find
a. The eigenvalues of Ĥ.
</p>
<p>b. The corresponding eigenkets.
</p>
<p>c. The time evolution of a stateket |φ , t〉 if |φ ,0〉= |φ1〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>360 8 Quantum Formulation
</p>
<p>a. Let Ĥ|α〉= λ |α〉. Using the closure relation,
</p>
<p>&sum;
j
</p>
<p>Ĥ|φ j〉〈φ j|α〉= λ |α〉 . (8.314)
</p>
<p>Multiplying this equation by 〈φi|, we find
</p>
<p>&sum;
j
</p>
<p>〈φi|Ĥ|φ j〉〈φ j|α〉= λ 〈φi|α〉 . (8.315)
</p>
<p>For Ĥ given by (8.313), we have
</p>
<p>〈φ1|Ĥ|φ1〉= 〈φ2|Ĥ|φ2〉= 0 and 〈φ1|Ĥ|φ2〉= 〈φ2|Ĥ|φ1〉= c . (8.316)
</p>
<p>Thus, (8.315) may be written more explicitly as
</p>
<p>(
0 c
</p>
<p>c 0
</p>
<p>)(
α1
α2
</p>
<p>)
= λ
</p>
<p>(
α1
α2
</p>
<p>)
, (8.317)
</p>
<p>or, (
&minus;λ c
</p>
<p>c &minus;λ
</p>
<p>)(
α1
α2
</p>
<p>)
=
</p>
<p>(
0
</p>
<p>0
</p>
<p>)
, (8.318)
</p>
<p>where
</p>
<p>αi := 〈φi|α〉 . (8.319)
Equation (8.318) has a nontrivial solution if and only if
</p>
<p>∣∣∣∣
&minus;λ c
</p>
<p>c &minus;λ
</p>
<p>∣∣∣∣= 0 , (8.320)
</p>
<p>that is, λ 2 &minus; c2 = 0. So, the eigenvalues are λ =&plusmn;c.
</p>
<p>b. Let λ = c. Then, (8.318) becomes
</p>
<p>(
&minus;c c
c &minus;c
</p>
<p>)(
α1
α2
</p>
<p>)
=
</p>
<p>(
0
</p>
<p>0
</p>
<p>)
, (8.321)
</p>
<p>from which we find α1 = α2. So,
</p>
<p>|c〉=&sum;
i
</p>
<p>|φi〉〈φi|c〉= α1[|φ1〉+ |φ2〉] . (8.322)
</p>
<p>Upon normalization, we find
</p>
<p>|c〉= 1&radic;
2
[|φ1〉+ |φ2〉] . (8.323)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.13 &Dagger;Schrödinger Wave Equation 361
</p>
<p>Similarly, the eigenket corresponding to the eigenvalue &minus;c is found to be
</p>
<p>|&minus; c〉= 1&radic;
2
[|φ1〉&minus; |φ2〉] . (8.324)
</p>
<p>c. According to (8.311), the general solution of the Schrödinger equation is
</p>
<p>the linear combination of |c〉 and |&minus; c〉:
</p>
<p>|φ , t〉=C1e&minus;
i
h̄
</p>
<p>ct |c〉+C2e
i
h̄
</p>
<p>ct |&minus; c〉 . (8.325)
</p>
<p>From the initial condition, we have
</p>
<p>|φ ,0〉=C1|c〉+C2|&minus; c〉= |φ1〉 . (8.326)
</p>
<p>Multiplying this equation by 〈φ1| from the left, we find
</p>
<p>1&radic;
2
(C1 +C2) = 1 . (8.327)
</p>
<p>Multiplying by 〈φ2| from the left, we have
</p>
<p>1&radic;
2
(C1 &minus;C2) = 0 . (8.328)
</p>
<p>Thus, C1 =C2 = 1/
&radic;
</p>
<p>2 and
</p>
<p>|φ , t〉= 1&radic;
2
</p>
<p>[
e&minus;
</p>
<p>i
h̄
</p>
<p>ct |c〉+ e ih̄ ct |&minus; c〉
]
. (8.329)
</p>
<p>Exercise 8.31. Let
</p>
<p>Ĥ = a [|φ1〉〈φ1|+ i|φ1〉〈φ2|+ i|φ2〉〈φ1|+ |φ2〉〈φ2|] , (8.330)
</p>
<p>where a is a real nonzero constant having a dimension of energy. Can this be the
</p>
<p>Hamiltonian of some two-state system? Why or why not? ///
</p>
<p>8.13 &Dagger;Schrödinger Wave Equation
</p>
<p>Given a physical system of interest, how do we find its Hamiltonian? As an example,
</p>
<p>let us consider a particle moving in a one-dimensional space.
</p>
<p>Classical mechanically, the Hamiltonian of such a system is
</p>
<p>H =
p2
</p>
<p>2m
+ψ(x) , (8.331)</p>
<p/>
</div>
<div class="page"><p/>
<p>362 8 Quantum Formulation
</p>
<p>where m is the mass of the particle and ψ is an external field. In quantum mechan-
ics, we simply adopt this Hamiltonian and replace p and x by the corresponding
</p>
<p>Hermitian operators:
</p>
<p>Ĥ =
p̂2
</p>
<p>2m
+ψ(x̂) . (8.332)
</p>
<p>Accordingly, the Schrödinger equation (8.261) becomes
</p>
<p>ih̄
d|φ , t〉
</p>
<p>dt
=
</p>
<p>[
p̂2
</p>
<p>2m
+ψ(x̂)
</p>
<p>]
|φ , t〉 . (8.333)
</p>
<p>Let us multiply this equation by 〈x| from the left and consider what happens to
each terms. The end result is a differential equation for the wave function φ(x, t) :=
〈x|φ , t〉.
</p>
<p>Because 〈x| is a basis bra in the x-representation, it does not depend on time and
hence &lang;
</p>
<p>x
</p>
<p>∣∣∣∣ih̄
d
</p>
<p>dt
</p>
<p>∣∣∣∣φ , t
&rang;
= ih̄
</p>
<p>&part;
</p>
<p>&part; t
〈x|φ , t〉= ih̄&part;φ(x, t)
</p>
<p>&part; t
. (8.334)
</p>
<p>Note that we replaced the total time derivative by the partial derivative since the
</p>
<p>wave function is a function of both x and t. Using (8.130), we find
</p>
<p>ψ(x̂)|x〉= ψ(x)|x〉 . (8.335)
</p>
<p>Since Ĥ is Hermitian, so is ψ(x̂). Thus, the adjoint of this equation reads
</p>
<p>〈x|ψ(x̂) = 〈x|ψ(x) , (8.336)
</p>
<p>and we arrive at
</p>
<p>〈x|ψ(x̂)|φ , t〉= 〈x|ψ(x)|φ , t〉= ψ(x)〈x|φ , t〉= ψ(x)φ(x, t) . (8.337)
</p>
<p>The remaining term takes a little more effort. Using the closure relation (8.201)
</p>
<p>written for the one-dimensional case, we write
</p>
<p>〈x| p̂2|φ , t〉=
&int;
</p>
<p>〈x| p̂|x&prime;〉〈x&prime;| p̂|φ , t〉dx&prime; . (8.338)
</p>
<p>Using (8.239) and (8.251),
</p>
<p>〈x|p̂2|φ , t〉= (&minus;ih̄)2
&int;
</p>
<p>&part;
</p>
<p>&part;x
δ (x&minus; x&prime;) &part;
</p>
<p>&part;x&prime;
φ(x&prime;, t)dx&prime; . (8.339)
</p>
<p>Since the integration is with respect to x&prime;, &part;/&part;x can be pulled out of the integral
sign, thus leading to
</p>
<p>〈x| p̂2|φ , t〉=&minus;h̄2 &part;
&part;x
</p>
<p>&int;
δ (x&minus; x&prime;) &part;
</p>
<p>&part;x&prime;
φ(x&prime;, t)dx&prime; =&minus;h̄2 &part;
</p>
<p>2
</p>
<p>&part;x2
φ(x, t) . (8.340)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.13 &Dagger;Schrödinger Wave Equation 363
</p>
<p>Combining everything, we arrive at
</p>
<p>ih̄
&part;
</p>
<p>&part; t
φ(x, t) =
</p>
<p>[
&minus; h̄
</p>
<p>2
</p>
<p>2m
</p>
<p>&part; 2
</p>
<p>&part;x2
+ψ(x)
</p>
<p>]
φ(x, t) , (8.341)
</p>
<p>which is the famous time-dependent Schrödinger wave equation applied to the
</p>
<p>one-dimensional system.
</p>
<p>As we have seen in Sect. 8.12, the problem of finding |φ , t〉 reduces to that of
solving the eigenvalue problem (8.310). Its x-representation for the present problem
</p>
<p>is [
&minus; h̄
</p>
<p>2
</p>
<p>2m
</p>
<p>d2
</p>
<p>dx2
+ψ(x)
</p>
<p>]
un(x) = Enun(x) , (8.342)
</p>
<p>where un(x) := 〈x|n〉 is the energy eigenfunction corresponding to the energy
eigenvalue En. Equation (8.342) is the time-independent Schrödinger wave equa-
</p>
<p>tion. The general solution of (8.341) is obtained by simply expressing (8.311) in
</p>
<p>the x-representation:
</p>
<p>φ(x, t) =&sum;
n
</p>
<p>Cne
&minus; i
</p>
<p>h̄
Entun(x) . (8.343)
</p>
<p>As in Sect. 8.12, the solution to the eigenvalue problem (8.342) and initial conditions
</p>
<p>completely determine the time evolution of the wave function φ(x, t). One can also
obtain (8.342) and (8.343) directly from (8.341) by means of separation of variables,
</p>
<p>in which one assumes that the solution is of the form of φ(x, t) = θ(t)u(x).
In accordance with the postulates of quantum mechanics,
</p>
<p>|〈x|φ , t〉|2dx = φ &lowast;(x, t)φ(x, t)dx (8.344)
</p>
<p>is the probability of finding the particle within the interval dx taken around the point
</p>
<p>x. Because of this probabilistic interpretation, φ(x, t) cannot diverge. We also expect
φ(x, t) to be a continuous function. If the particle is confined to a finite region of
space, these conditions demand that the energy eigenvalues form a discrete spec-
</p>
<p>trum. For a general discussion on this point, see Chap. 3 of Ref. [6]. Specific exam-
</p>
<p>ples are given in Example 8.13 and Sect. 8.14.
</p>
<p>Example 8.12. Free particle: For a free particle moving in a one-dimensional
</p>
<p>space,
</p>
<p>Ĥ =
p̂2
</p>
<p>2m
, (8.345)
</p>
<p>whose energy eigenfunction is the one-dimensional version of (8.244):
</p>
<p>〈x|p〉=Ce ih̄ px . (8.346)</p>
<p/>
</div>
<div class="page"><p/>
<p>364 8 Quantum Formulation
</p>
<p>To see this, we note that
</p>
<p>d
</p>
<p>dx
〈x|p〉= i
</p>
<p>h̄
pCe
</p>
<p>i
h̄
</p>
<p>px =
i
</p>
<p>h̄
p〈x|p〉 (8.347)
</p>
<p>and
d2
</p>
<p>dx2
〈x|p〉=
</p>
<p>(
i
</p>
<p>h̄
p
</p>
<p>)2
〈x|p〉=&minus;
</p>
<p>( p
h̄
</p>
<p>)2
〈x|p〉 . (8.348)
</p>
<p>Thus,
</p>
<p>&minus; h̄
2
</p>
<p>2m
</p>
<p>d2
</p>
<p>dx2
〈x|p〉= p
</p>
<p>2
</p>
<p>2m
〈x|p〉 . (8.349)
</p>
<p>This is just (8.342) written for the free particle, for which ψ(x) &equiv; 0. That
is, 〈x|p〉 is an eigenfunction of the Hamiltonian (8.345) and belongs to the
eigenvalue p2/2m.
</p>
<p>Insofar as we have called (though rather arbitrarily) the eigenvalue of p̂
</p>
<p>momentum in Sect. 8.7, it is quite appropriate that we have identified an eigen-
</p>
<p>value E of Ĥ as the energy in Sect. 8.8. Finally, we note that (8.349) holds for
</p>
<p>any real number p. In the absence of an external field that confines the par-
</p>
<p>ticle to a finite region of space, the energy of the system forms a continuous
</p>
<p>spectrum.
</p>
<p>It is straightforward to generalize (8.341) and (8.342) for a system of N particles
</p>
<p>in a three-dimensional space. The results are
</p>
<p>ih̄
&part;
</p>
<p>&part; t
φ(rN , t) =
</p>
<p>[
N
</p>
<p>&sum;
i=1
</p>
<p>&minus; h̄
2
</p>
<p>2mi
&nabla;i
</p>
<p>2 +ψ(rN)
</p>
<p>]
φ(rN , t) , (8.350)
</p>
<p>where &nabla;i acts only on the coordinates of the ith particle, and
</p>
<p>[
N
</p>
<p>&sum;
i=1
</p>
<p>&minus; h̄
2
</p>
<p>2mi
&nabla;i
</p>
<p>2 +ψ(rN)
</p>
<p>]
φ(rN , t) = Eφ(rN , t) , (8.351)
</p>
<p>respectively.
</p>
<p>Example 8.13. Particle in a box: Consider a particle of mass m confined to a
</p>
<p>rectangular box of dimension Lx &times;Ly &times;Lz. The classical mechanical Hamil-
tonian of the particle is given by (3.75), in which ψw(r) is the wall potential
specified by (3.76). The corresponding time-independent Schrödinger wave
</p>
<p>equation reads
</p>
<p>&minus; h̄
2
</p>
<p>2m
</p>
<p>[
&part; 2u(r)
</p>
<p>&part;x2
+
</p>
<p>&part; 2u(r)
</p>
<p>&part;y2
+
</p>
<p>&part; 2u(r)
</p>
<p>&part; z2
</p>
<p>]
+ψw(r)u(r) = Eu(r) . (8.352)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.13 &Dagger;Schrödinger Wave Equation 365
</p>
<p>Outside the box, ψw is &infin;. Provided that E, u, and its second partial derivatives
are all finite, we have u = 0 outside the box. So, let us focus on the region
inside the box, in which ψw(r) = 0, and hence
</p>
<p>&minus; h̄
2
</p>
<p>2m
</p>
<p>[
&part; 2u(r)
</p>
<p>&part;x2
+
</p>
<p>&part; 2u(r)
</p>
<p>&part;y2
+
</p>
<p>&part; 2u(r)
</p>
<p>&part; z2
</p>
<p>]
= Eu(r) . (8.353)
</p>
<p>Suppose that u(r) = X(x)Y (y)Z(z). Substituting this expression into (8.353)
and dividing the resulting equation by u, we find
</p>
<p>1
</p>
<p>X
</p>
<p>d2X
</p>
<p>dx2
+
</p>
<p>1
</p>
<p>Y
</p>
<p>d2Y
</p>
<p>dy2
+
</p>
<p>1
</p>
<p>Z
</p>
<p>d2Z
</p>
<p>dz2
=&minus;2mE
</p>
<p>h̄2
. (8.354)
</p>
<p>By rewriting this as
</p>
<p>1
</p>
<p>X
</p>
<p>d2X
</p>
<p>dx2
=&minus;2mE
</p>
<p>h̄2
&minus; 1
</p>
<p>Y
</p>
<p>d2Y
</p>
<p>dy2
&minus; 1
</p>
<p>Z
</p>
<p>d2Z
</p>
<p>dz2
(8.355)
</p>
<p>we see that the left-hand side is a function only of x, while the right-hand side
</p>
<p>is a function only of y and z. Because the equality holds for any x, y, and z,
</p>
<p>they must equal a constant, which we call &minus;kx2.47 This gives
</p>
<p>d2X
</p>
<p>dx2
+ kx
</p>
<p>2X = 0 and
1
</p>
<p>Y
</p>
<p>d2Y
</p>
<p>dy2
=&minus;2mE
</p>
<p>h̄2
&minus; 1
</p>
<p>Z
</p>
<p>d2Z
</p>
<p>dz2
+ kx
</p>
<p>2 . (8.356)
</p>
<p>Again, we see that the left-hand side of the second equation must be a con-
</p>
<p>stant, which we denote by &minus;ky2 and obtain
</p>
<p>d2Y
</p>
<p>dy2
+ ky
</p>
<p>2Y = 0 and
d2Z
</p>
<p>dz2
+ kz
</p>
<p>2Z = 0 , (8.357)
</p>
<p>where we defined kz by
</p>
<p>kx
2 + ky
</p>
<p>2 + kz
2 =
</p>
<p>2mE
</p>
<p>h̄2
. (8.358)
</p>
<p>So far, we have not placed any restriction on kx, ky, or kz. They may even
</p>
<p>be a complex number. The ordinary differential equation for X in (8.356) has
</p>
<p>the following solution:
</p>
<p>X(x) = Ax coskxx+Bx sinkxx , (8.359)
</p>
<p>where Ax and Bx are constants to be determined by the boundary conditions.
</p>
<p>Because u(r) is zero outside the box, u(r) = 0 if x &lt; 0 or x &gt; Lx. But, we
also expect that u(r) is continuous at the walls of the box. This means that</p>
<p/>
</div>
<div class="page"><p/>
<p>366 8 Quantum Formulation
</p>
<p>X(0) = X(Lx) = 0. The first equality gives Ax = 0, while the second one gives
Bx sinkxLx = 0. If Bx = 0, then u(r)&equiv; 0, which is not a solution we seek. So,
sinkxLx = 0, indicating that
</p>
<p>kx =
nxπ
</p>
<p>Lx
, nx = 1,2,3, . . . (8.360)
</p>
<p>We exclude nx = 0, for which u(r) &equiv; 0. The negative integers (nx =
&minus;1,&minus;2,&minus;3, . . .) does not generate any new solution because Bx sin(&minus;kxx) =
&minus;Bx sin(kxx). A similar procedure leads to
</p>
<p>Y (y) = By sinkyy ,ky =
nyπ
</p>
<p>Ly
, ny = 1,2,3, . . . (8.361)
</p>
<p>and
</p>
<p>Z(z) = Bz sinkzz ,kz =
nzπ
</p>
<p>Lz
, nz = 1,2,3, . . . (8.362)
</p>
<p>From (8.358), we see that the energy eigenvalue E is determined by
</p>
<p>8mE
</p>
<p>h2
=
</p>
<p>(
nx
</p>
<p>Lx
</p>
<p>)2
+
</p>
<p>(
ny
</p>
<p>Ly
</p>
<p>)2
+
</p>
<p>(
nz
</p>
<p>Lz
</p>
<p>)2
, (8.363)
</p>
<p>in which nx, ny, and nz are positive integers and we used h = 2π h̄. In contrast
to the case of a free particle, energy spectrum is discrete. The corresponding
</p>
<p>energy eigenfunction is now labeled by three indices nx, ny, and nz:
</p>
<p>unx,ny,nz(r) = Bsin
</p>
<p>(
πnx
Lx
</p>
<p>x
</p>
<p>)
sin
</p>
<p>(
πny
Ly
</p>
<p>y
</p>
<p>)
sin
</p>
<p>(
πnz
Lz
</p>
<p>z
</p>
<p>)
. (8.364)
</p>
<p>The remaining constant B may be determined by the normalization condition.
</p>
<p>Recalling that sin2 θ = (1&minus; cos2θ)/2, we find
&int; Lx
</p>
<p>0
sin2
</p>
<p>(
πnx
Lx
</p>
<p>x
</p>
<p>)
dx =
</p>
<p>Lx
</p>
<p>πnx
</p>
<p>&int; πnx
</p>
<p>0
sin2 θdθ =
</p>
<p>Lx
</p>
<p>2
. (8.365)
</p>
<p>Similarly for the other factors in (8.364). Thus,
</p>
<p>unx,ny,nz(r) =
</p>
<p>&radic;
8
</p>
<p>V
sin
</p>
<p>(
πnx
Lx
</p>
<p>x
</p>
<p>)
sin
</p>
<p>(
πny
Ly
</p>
<p>y
</p>
<p>)
sin
</p>
<p>(
πnz
Lz
</p>
<p>z
</p>
<p>)
, (8.366)
</p>
<p>where V = LxLyLz and we chose B to be a real positive number.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.14 &Dagger;Harmonic Oscillator 367
</p>
<p>8.14 &Dagger;Harmonic Oscillator
</p>
<p>For a one-dimensional harmonic oscillator, the Hamiltonian is given by
</p>
<p>Ĥ =
p̂2
</p>
<p>2m
+
</p>
<p>1
</p>
<p>2
kx̂2 . (8.367)
</p>
<p>The primary goal of this section is to find the expression for the energy eigenvalues
</p>
<p>for this Hamiltonian. We have already made use of the result in Example 4.2.
</p>
<p>With Ĥ given by (8.367), the time-independent Schrödinger wave equation
</p>
<p>(8.342) reads [
&minus; h̄
</p>
<p>2
</p>
<p>2m
</p>
<p>d2
</p>
<p>dx2
+
</p>
<p>1
</p>
<p>2
kx2
</p>
<p>]
un(x) = Enun(x) , (8.368)
</p>
<p>which can be solved directly to yield both En and un(x). See Ref. [6] for details. As
an alternative, we shall follow an elegant operator method.
</p>
<p>8.14.1 &Dagger;Operator Method
</p>
<p>Let ω :=
&radic;
</p>
<p>k/m and rewrite (8.367) as
</p>
<p>Ĥ =
p̂2
</p>
<p>2m
+
</p>
<p>mω2
</p>
<p>2
x̂2 =
</p>
<p>mω2
</p>
<p>2
</p>
<p>(
x̂2 +
</p>
<p>p̂2
</p>
<p>m2ω2
</p>
<p>)
. (8.369)
</p>
<p>Because x̂ and p̂ do not commute, we cannot just factorize this as
</p>
<p>h̄ω
</p>
<p>&radic;
mω
</p>
<p>2h̄
</p>
<p>(
x̂+
</p>
<p>ip̂
</p>
<p>mω
</p>
<p>)&radic;
mω
</p>
<p>2h̄
</p>
<p>(
x̂&minus; ip̂
</p>
<p>mω
</p>
<p>)
= h̄ω ââ&dagger; , (8.370)
</p>
<p>where we defined
</p>
<p>â :=
</p>
<p>&radic;
mω
</p>
<p>2h̄
</p>
<p>(
x̂+
</p>
<p>ip̂
</p>
<p>mω
</p>
<p>)
(8.371)
</p>
<p>and hence
</p>
<p>â&dagger; =
</p>
<p>&radic;
mω
</p>
<p>2h̄
</p>
<p>(
x̂&minus; ip̂
</p>
<p>mω
</p>
<p>)
. (8.372)
</p>
<p>By means of direct computations, however, we obtain
</p>
<p>ââ&dagger; =
mω
</p>
<p>2h̄
</p>
<p>(
x̂2 &minus; i
</p>
<p>mω
x̂ p̂+
</p>
<p>i
</p>
<p>mω
p̂x̂+
</p>
<p>p̂2
</p>
<p>m2ω2
</p>
<p>)
(8.373)
</p>
<p>and
</p>
<p>â&dagger;â =
mω
</p>
<p>2h̄
</p>
<p>(
x̂2 +
</p>
<p>i
</p>
<p>mω
x̂ p̂&minus; i
</p>
<p>mω
p̂x̂+
</p>
<p>p̂2
</p>
<p>m2ω2
</p>
<p>)
. (8.374)</p>
<p/>
</div>
<div class="page"><p/>
<p>368 8 Quantum Formulation
</p>
<p>It follows that
</p>
<p>[â, â&dagger;] =&minus; i
h̄
[x̂, p̂] = 1 (8.375)
</p>
<p>where the last equality follows from (8.234). In contrast,
</p>
<p>ââ&dagger; + â&dagger;â =
mω
</p>
<p>h̄
</p>
<p>(
x̂2 +
</p>
<p>p̂2
</p>
<p>m2ω2
</p>
<p>)
=
</p>
<p>2
</p>
<p>h̄ω
Ĥ . (8.376)
</p>
<p>Eliminating ââ&dagger; in (8.376) by means of (8.375), we obtain
</p>
<p>Ĥ = h̄ω
</p>
<p>(
N̂ +
</p>
<p>1
</p>
<p>2
</p>
<p>)
, (8.377)
</p>
<p>where
</p>
<p>N̂ := â&dagger;â (8.378)
</p>
<p>is known as the number operator.
</p>
<p>Let n denote the eigenvalue of N̂ and write the corresponding eigenket as |n〉:
</p>
<p>N̂|n〉= n|n〉 . (8.379)
</p>
<p>From (8.377), it is clear that
</p>
<p>[Ĥ, N̂] = 0 . (8.380)
</p>
<p>By Theorem 8.5, the eigenkets of N̂ are also energy eigenkets. But,
</p>
<p>Ĥ|n〉= h̄ω
(
</p>
<p>N̂ +
1
</p>
<p>2
</p>
<p>)
|n〉= h̄ω
</p>
<p>(
n+
</p>
<p>1
</p>
<p>2
</p>
<p>)
|n〉 . (8.381)
</p>
<p>So, |n〉 belongs to the energy eigenvalue h̄ω(n+1/2).
It might appear that (4.75) follows directly from (8.381). But, we have yet to
</p>
<p>show that n can only be a nonnegative integer. Since N̂ is Hermitian, we know
</p>
<p>at least that n must be a real number. In addition, n must be nonnegative. This is
</p>
<p>because the norm of a ket cannot be negative:
</p>
<p>0 &le; ||(â|n〉)||2 = 〈n|â&dagger;â|n〉= n〈n|n〉 . (8.382)
</p>
<p>But, |n〉 is not a zero ket due to the convention mentioned in Sect. 8.3.1. So, 〈n|n〉&gt; 0
and we conclude that n &ge; 0.
</p>
<p>In order to further narrow down the possible values n can take, we evaluate
</p>
<p>N̂â|n〉= (â&dagger;â)â|n〉= (ââ&dagger; &minus;1)â|n〉= â(â&dagger;â&minus;1)|n〉= (n&minus;1)â|n〉 , (8.383)
</p>
<p>where we used (8.375) to transform the second expression to the third. Equa-
</p>
<p>tion (8.383) indicates that â|n〉 is an eigenket of N̂ corresponding to the eigenvalue
n&minus;1. That is,
</p>
<p>â|n〉= c|n&minus;1〉 . (8.384)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.14 &Dagger;Harmonic Oscillator 369
</p>
<p>The constant c may be determined by demanding the normalization of the energy
</p>
<p>eigenkets. Noting that
</p>
<p>|c|2〈n&minus;1|n&minus;1〉= 〈n|â&dagger;â|n〉= n〈n|n〉= n , (8.385)
</p>
<p>we have |c|2 = n. By convention, we choose a real positive root for c, that is
</p>
<p>c =
&radic;
</p>
<p>n . (8.386)
</p>
<p>and write (8.384) as
</p>
<p>â|n〉=
&radic;
</p>
<p>n|n&minus;1〉 . (8.387)
The operator â, when acting on |n〉, reduces the value of n by one. Since this corre-
sponds to the decrease in the energy eigenvalue by one quantum unit of energy h̄ω ,
â is known as the annihilation operator.
</p>
<p>Repeated applications of â on |n〉 give the following sequence of kets:
</p>
<p>â|n〉 =
&radic;
</p>
<p>n|n&minus;1〉 ,
â2|n〉 =
</p>
<p>&radic;
n(n&minus;1)|n&minus;2〉 ,
</p>
<p>â3|n〉 =
&radic;
</p>
<p>n(n&minus;1)(n&minus;2)|n&minus;3〉 , &middot; &middot; &middot; (8.388)
</p>
<p>As we have seen above, however, n&minus;m &ge; 0 for any eigenket |n&minus;m〉 of N̂. This
means that the sequence (8.388) must terminate before n&minus;m becomes negative.
This happens if n is a nonnegative integer because
</p>
<p>ân|n〉=
&radic;
</p>
<p>n(n&minus;1)(n&minus;2) &middot; &middot; &middot;1|0〉 (8.389)
</p>
<p>and an additional application of â produces the zero ket |θ〉, which will remain
unchanged upon further applications of â. (See Exercises 8.1b and 8.8a.) Thus, the
</p>
<p>energy eigenvalues form a discrete set:
</p>
<p>En = h̄ω
</p>
<p>(
n+
</p>
<p>1
</p>
<p>2
</p>
<p>)
, n = 0,1,2 . . . , (8.390)
</p>
<p>which is (4.75) we encountered before.
</p>
<p>It is important to distinguish |θ〉 and |0〉. The former is the zero ket, the norm
of which is zero. The latter, in contrast, is a simultaneous eigenket of N̂ and Ĥ cor-
</p>
<p>responding to the lowest possible energy h̄ω/2 the harmonic oscillator can take. In
other words, |0〉 is the state ket of the so-called ground state. Classical mechani-
cally, the lowest energy occurs when (x, p) = (0,0). Quantum mechanically, such a
state is in violation of the Heisenberg uncertainty principle as we saw in Sect. 8.5.
</p>
<p>Given the ground state ket |0〉, â&dagger; allows us to systematically generate the other
eigenstates of N̂. To see this, we evaluate
</p>
<p>N̂â&dagger;|n〉= â&dagger;(ââ&dagger;)|n〉= â&dagger;(â&dagger;â+1)|n〉= â&dagger;(n+1)|n〉= (n+1)â&dagger;|n〉 , (8.391)</p>
<p/>
</div>
<div class="page"><p/>
<p>370 8 Quantum Formulation
</p>
<p>where we used (8.375). According to (8.391), â&dagger;|n〉 is an eigenket of N̂ correspond-
ing to the eigenvalue n+1:
</p>
<p>â&dagger;|n〉= c|n+1〉 , (8.392)
where c is a constant. The effect of â&dagger; on |n〉 is to increase the value of n by one and
hence the energy eigenvalue by one quantum unit of energy h̄ω . So, the operator â&dagger;
</p>
<p>is referred to as the creation operator.
</p>
<p>Because the effect of â on |n〉 is known and â&dagger; is the adjoint of â, the formalism
we have developed so far should be sufficient to determine the value of c. In fact,
</p>
<p>multiplying (8.392) by â from the left, we find
</p>
<p>ââ&dagger;|n〉 = câ|n+1〉(
â&dagger;â+1
</p>
<p>)
|n〉 = c
</p>
<p>&radic;
n+1|n〉
</p>
<p>(n+1)|n〉 = c
&radic;
</p>
<p>n+1|n〉 . (8.393)
</p>
<p>Multiplying both sides by 〈n| from the left, we obtain c =
&radic;
</p>
<p>n+1. Thus,
</p>
<p>â&dagger;|n〉=
&radic;
</p>
<p>n+1|n+1〉 . (8.394)
</p>
<p>Repeated applications of â&dagger; on |0〉 give
</p>
<p>â&dagger;|0〉 = |1〉
(â&dagger;)2|0〉 = â&dagger;|1〉=
</p>
<p>&radic;
2|2〉
</p>
<p>(â&dagger;)3|0〉 = â&dagger;
&radic;
</p>
<p>2|2〉=
&radic;
</p>
<p>3 &middot;2|3〉
(â&dagger;)4|0〉 = â&dagger;
</p>
<p>&radic;
3 &middot;2|3〉=
</p>
<p>&radic;
4 &middot;3 &middot;2|4〉 . (8.395)
</p>
<p>Proceeding in this manner, we find
</p>
<p>|n〉= (â
&dagger;)n&radic;
n!
</p>
<p>|0〉 . (8.396)
</p>
<p>8.14.2 &Dagger;Energy Eigenfunctions
</p>
<p>It is now straightforward to find the wave function un(x) := 〈x|n〉. We start from
</p>
<p>â|0〉= |θ〉 , (8.397)
</p>
<p>and hence
</p>
<p>〈x|â|0〉= 〈x|θ〉= 0 . (8.398)
Using (8.371) and recalling (8.239), we have
</p>
<p>(
x+
</p>
<p>h̄
</p>
<p>mω
</p>
<p>d
</p>
<p>dx
</p>
<p>)
u0 = 0 , (8.399)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.14 &Dagger;Harmonic Oscillator 371
</p>
<p>which can be solved to give
</p>
<p>u0(x) = ce
&minus; 12αx2 , (8.400)
</p>
<p>where α = mω/h̄. Upon normalization under the convention that c be a real positive
number, we have
</p>
<p>u0(x) =
(α
π
</p>
<p>)1/4
e&minus;αx
</p>
<p>2/2 . (8.401)
</p>
<p>Substituting this result into (8.368), we find that E0 = h̄ω/2, which is in agreement
with (8.390).
</p>
<p>From (8.396),
</p>
<p>un(x) =
1&radic;
n!
</p>
<p>&lang;
x
∣∣(â&dagger;)n
</p>
<p>∣∣0
&rang;
. (8.402)
</p>
<p>Once again, we use (8.372) and (8.239) and find
</p>
<p>un(x) =
1&radic;
n!
</p>
<p>(α
2
</p>
<p>)n/2(
x&minus; 1
</p>
<p>α
</p>
<p>d
</p>
<p>dx
</p>
<p>)n
u0(x)
</p>
<p>=
1&radic;
2nn!
</p>
<p>(&radic;
αx&minus; 1&radic;
</p>
<p>α
</p>
<p>d
</p>
<p>dx
</p>
<p>)n(α
π
</p>
<p>)1/4
exp
</p>
<p>[
&minus;1
</p>
<p>2
</p>
<p>(&radic;
αx
</p>
<p>)2
]
</p>
<p>=: Nn
</p>
<p>(
ξ &minus; d
</p>
<p>dξ
</p>
<p>)n
e&minus;ξ
</p>
<p>2/2 , (8.403)
</p>
<p>where ξ :=
&radic;
αx and
</p>
<p>Nn :=
1&radic;
2nn!
</p>
<p>(α
π
</p>
<p>)1/4
. (8.404)
</p>
<p>More explicitly, the energy eigenfunctions are given by
</p>
<p>u0(ξ ) = N0e
&minus;ξ 2/2 ,
</p>
<p>u1(ξ ) = N1(2ξ )e
&minus;ξ 2/2 ,
</p>
<p>u2(ξ ) = N2(4ξ
2 &minus;2)e&minus;ξ 2/2 ,
</p>
<p>u3(ξ ) = N3(8ξ
3 &minus;12ξ )e&minus;ξ 2/2 ,
</p>
<p>u4(ξ ) = N4(16ξ
4 &minus;48ξ 2 +12)e&minus;ξ 2/2 ,
</p>
<p>u5(ξ ) = N5(32ξ
5 &minus;160ξ 3 +120ξ )e&minus;ξ 2/2 , &middot; &middot; &middot; . (8.405)
</p>
<p>The polynomials in these equations, that is,
</p>
<p>H0(ξ ) = 1 ,
</p>
<p>H1(ξ ) = 2ξ ,
</p>
<p>H2(ξ ) = 4ξ
2 &minus;2 ,
</p>
<p>H3(ξ ) = 8ξ
3 &minus;12ξ ,
</p>
<p>H4(ξ ) = 16ξ
4 &minus;48ξ 2 +12 ,
</p>
<p>H5(ξ ) = 32ξ
5 &minus;160ξ 3 +120ξ , &middot; &middot; &middot; . (8.406)</p>
<p/>
</div>
<div class="page"><p/>
<p>372 8 Quantum Formulation
</p>
<p>are known as the Hermit polynomials. For various properties of this mathematical
</p>
<p>object, see Ref. [6].
</p>
<p>Classical mechanically, the energy E of the one-dimensional harmonic oscillator
</p>
<p>is given by
</p>
<p>E =
p2
</p>
<p>2m
+
</p>
<p>1
</p>
<p>2
kx2 =
</p>
<p>p2
</p>
<p>2m
+
</p>
<p>1
</p>
<p>2
h̄ωξ 2 . (8.407)
</p>
<p>Thus,
1
</p>
<p>2
h̄ωξ 2 = E &minus; p
</p>
<p>2
</p>
<p>2m
&le; E , (8.408)
</p>
<p>and the motion of the particle is confined to the region defined by
</p>
<p>|ξ | &le;
&radic;
</p>
<p>2E
</p>
<p>h̄ω
. (8.409)
</p>
<p>|ξ | &le;
&radic;
</p>
<p>2n+1 , n = 0,1,2, . . . (8.410)
</p>
<p>Equation (8.405) indicates, however, that the probability, u&lowast;n(x)un(x)dx, of finding a
particle in the classical mechanically forbidden region (|ξ | &gt;
</p>
<p>&radic;
2n+1) is nonzero.
</p>
<p>This is illustrated in Fig. 8.2 for several values of n.
</p>
<p>8.15 &dagger;Ehrenfest&rsquo;s Theorem
</p>
<p>In Sect. 8.11, we derived Heisenberg&rsquo;s equation of motion, which may be regarded
</p>
<p>as the quantum mechanical counterpart of (1.185) in classical mechanics. Insofar as
</p>
<p>(1.185) contains Hamilton&rsquo;s and therefore Newton&rsquo;s equations of motion, it is nat-
</p>
<p>ural to ask if their quantum mechanical counterparts can be derived from Heisen-
</p>
<p>berg&rsquo;s equation of motion. This is indeed the case as we shall see in this section.
</p>
<p>For simplicity, we restrict ourselves to a one-dimensional system described by
</p>
<p>the Hamiltonian:
</p>
<p>Ĥ(t) =
p̂2
</p>
<p>2m
+ψ(x̂, t) . (8.411)
</p>
<p>Applying (8.306) to x̂H(t) and noting that x̂ (an operator in the Schrödinger picture)
is independent of time,
</p>
<p>dx̂H
</p>
<p>dt
=
</p>
<p>1
</p>
<p>ih̄
[x̂H(t), ĤH(t)] =
</p>
<p>1
</p>
<p>ih̄
Û
</p>
<p>&dagger;
t [x̂, Ĥ(t)]Û t , (8.412)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.15 &dagger;Ehrenfest&rsquo;s Theorem 373
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>n 0
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>n 1
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>n 2
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>n 3
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>n 4
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>-6 -4 -2 0 6 -6 -4 -2 0 2 4 6
</p>
<p>-6 -4 -2 0 6 -6 -4 -2 0 2 4 6
</p>
<p>-6 -4 -2 0
</p>
<p>2 4
</p>
<p>2 4
</p>
<p>2 4 6 -6 -4 -2 0 2 4 6
</p>
<p>n 5
</p>
<p>Fig. 8.2 Energy eigenfunctions (α/π)&minus;1/4un(ξ ) (solid line) and classically forbidden regions
(thick horizontal lines) for several values of n. The horizontal axis is ξ .
</p>
<p>where we used (8.298) in the last step. But,
</p>
<p>[x̂, Ĥ(t)] =
</p>
<p>[
x̂,
</p>
<p>p̂2
</p>
<p>2m
+ψ(x̂, t)
</p>
<p>]
=
</p>
<p>1
</p>
<p>2m
[x̂, p̂2] . (8.413)</p>
<p/>
</div>
<div class="page"><p/>
<p>374 8 Quantum Formulation
</p>
<p>Using (8.105) and (8.234),
</p>
<p>[x̂, p̂2] = p̂[x̂, p̂]+ [x̂, p̂] p̂ = 2ih̄p̂ . (8.414)
</p>
<p>Thus,
dx̂H
</p>
<p>dt
=
</p>
<p>1
</p>
<p>m
Û
</p>
<p>&dagger;
t p̂Û t =
</p>
<p>p̂H(t)
</p>
<p>m
. (8.415)
</p>
<p>This is to be contrasted with one of Hamilton&rsquo;s equations of motion:
</p>
<p>ẋ =
&part;H
</p>
<p>&part; p
. (8.416)
</p>
<p>For the classical mechanical Hamiltonian corresponding to (8.411), this gives
</p>
<p>dx
</p>
<p>dt
=
</p>
<p>p
</p>
<p>m
. (8.417)
</p>
<p>In view of (8.415) and (8.417), the name momentum operator we have given to p̂ is
</p>
<p>quite appropriate.
</p>
<p>In a similar manner,
</p>
<p>d p̂H
dt
</p>
<p>=
1
</p>
<p>ih̄
[ p̂H(t), ĤH(t)] =
</p>
<p>1
</p>
<p>ih̄
Û
</p>
<p>&dagger;
t [ p̂, Ĥ(t)]Û t =
</p>
<p>1
</p>
<p>ih̄
Û
</p>
<p>&dagger;
t [ p̂,ψ(x̂, t)]Û t . (8.418)
</p>
<p>If ψ(x, t) has the Maclaurin expansion, the operator ψ(x̂, t) is given by
</p>
<p>ψ(x̂, t) = ψ(0, t)+ψ &prime;(0, t)x̂+
1
</p>
<p>2
ψ &prime;&prime;(0, t)x̂2 +
</p>
<p>1
</p>
<p>3!
ψ &prime;&prime;&prime;(0, t)x̂3 + &middot; &middot; &middot; , (8.419)
</p>
<p>where &prime; indicates the partial derivative with respect to x. With repeated applications
of (8.105) and (8.234), we find
</p>
<p>[ p̂,1] = 0 , [ p̂, x̂] =&minus;ih̄ , [ p̂, x̂2] = x̂[ p̂, x̂]+ [p̂, x̂]x̂ =&minus;2ih̄x̂ , (8.420)
</p>
<p>and
</p>
<p>[ p̂, x̂3] = x̂[ p̂, x̂2]+ [p̂, x̂]x̂2 =&minus;2ih̄x̂2 &minus; ih̄x̂2 =&minus;3ih̄x̂2 . (8.421)
Proceeding in this manner, we obtain
</p>
<p>[ p̂, x̂n] =&minus;ih̄nx̂n&minus;1 , n = 1,2, . . . (8.422)
</p>
<p>Thus,
</p>
<p>[ p̂,ψ(x̂, t)] =&minus;ih̄ψ &prime;(x̂, t) , (8.423)
and hence
</p>
<p>d p̂H
dt
</p>
<p>=&minus;Û&dagger;t ψ &prime;(x̂, t)Û t . (8.424)
</p>
<p>Noting that
</p>
<p>Û
&dagger;
t x̂
</p>
<p>nÛ t = [x̂H(t)]
n , (8.425)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.15 &dagger;Ehrenfest&rsquo;s Theorem 375
</p>
<p>as can be seen by inserting Û tÛ
&dagger;
t = 1 between x̂&rsquo;s, we arrive at
</p>
<p>d p̂H
dt
</p>
<p>=&minus;ψ &prime;(x̂H(t), t) . (8.426)
</p>
<p>This result should be compared with its classical mechanical counterpart:
</p>
<p>dp
</p>
<p>dt
=&minus;ψ &prime;(x, t) , (8.427)
</p>
<p>which follows from the remaining half of Hamilton&rsquo;s equations of motion:
</p>
<p>ṗ =&minus;&part;H
&part;x
</p>
<p>. (8.428)
</p>
<p>Combining (8.415) and (8.426), we find
</p>
<p>m
d2x̂H
</p>
<p>dt2
=&minus;ψ &prime;(x̂H(t), t) . (8.429)
</p>
<p>This is the quantum mechanical counterpart of Newton&rsquo;s equation of motion.
</p>
<p>As seen from (8.295) and (8.298), the expectation value of an operator in the
</p>
<p>Heisenberg picture, such as x̂H and p̂H , must be evaluated using the state ket at
</p>
<p>t = 0. With (8.415), this leads to
</p>
<p>d〈x〉QM
dt
</p>
<p>=
〈p〉QM
</p>
<p>m
. (8.430)
</p>
<p>For the other half of the equations of motion, it is easier to work with (8.424) than
</p>
<p>with (8.426). The result is
</p>
<p>d〈p〉QM
dt
</p>
<p>=&minus;〈ψ &prime;(x̂, t)〉QM , (8.431)
</p>
<p>If the particle is well localized, the approximate relation:
</p>
<p>〈ψ &prime;(x̂, t)〉QM &asymp; ψ &prime;(〈x〉QM, t) (8.432)
</p>
<p>holds. In fact, following Exercise 8.23, we have
</p>
<p>〈ψ &prime;(x̂, t)〉QM = 〈φ |ψ &prime;(x̂)|φ〉=
&int;
</p>
<p>〈φ |ψ &prime;(x̂)|x〉〈x|φ〉dx =
&int;
</p>
<p>ψ &prime;(x)〈φ |x〉〈x|φ〉dx
</p>
<p>=
&int;
</p>
<p>ψ &prime;(x)|〈x|φ〉|2dx , (8.433)</p>
<p/>
</div>
<div class="page"><p/>
<p>376 8 Quantum Formulation
</p>
<p>in which |〈x|φ〉|2dx is the probability that the particle is found between x and x+dx
as seen from (8.344). If this probability is sharply peaked around the average 〈x〉QM,
then
</p>
<p>〈ψ &prime;(x̂, t)〉QM &asymp; ψ &prime;(〈x〉QM)
&int;
</p>
<p>|〈x|φ〉|2dx = ψ &prime;(〈x〉QM) . (8.434)
</p>
<p>Within this approximation,
</p>
<p>d〈p〉QM
dt
</p>
<p>=&minus;ψ &prime;(〈x〉QM, t) . (8.435)
</p>
<p>Equations (8.430) and (8.435) are known as Ehrenfest&rsquo;s theorem.
</p>
<p>Some functions, such as ψ(x, t) = xn (n &lt; 1), do not have the Maclaurin expan-
sion. Even then, (8.423) still holds provided that ψ &prime;(x, t) exists. One way to see this
is to expand ψ(x̂, t) into a Taylor series around x = a 	= 0 and note that a, being a
number, commutes with p̂. Alternatively, we start by writing
</p>
<p>[ p̂,ψ(x̂)] = p̂ψ(x̂)&minus;ψ(x̂) p̂ =
&int; &infin;
</p>
<p>&minus;&infin;
[ p̂|x〉〈x|ψ(x̂)&minus;ψ(x̂)|x〉〈x| p̂]dx
</p>
<p>=
&int; &infin;
</p>
<p>&minus;&infin;
[ p̂|x〉ψ(x)〈x|&minus; |x〉ψ(x)〈x| p̂]dx , (8.436)
</p>
<p>where we used the closure relation and (8.130). Using the closure relation one more
</p>
<p>time,
</p>
<p>p̂|x〉=
&int; &infin;
</p>
<p>&minus;&infin;
|x1〉〈x1| p̂|x〉dx1 =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
|x1〉(&minus;ih̄)
</p>
<p>d
</p>
<p>dx1
δ (x&minus; x1)dx1 , (8.437)
</p>
<p>where we made use of (8.251). Let X := x&minus; x1 and use the chain rule:
</p>
<p>d
</p>
<p>dx1
δ (x&minus; x1) =
</p>
<p>dX
</p>
<p>dx1
</p>
<p>d
</p>
<p>dX
δ (X) =&minus; d
</p>
<p>dX
δ (X) =&minus; d
</p>
<p>dx
δ (x&minus; x1) , (8.438)
</p>
<p>Because x is not the integration variable, we have
</p>
<p>p̂|x〉= ih̄ d
dx
</p>
<p>&int;
|x1〉δ (x&minus; x1)dx1 = ih̄
</p>
<p>d|x〉
dx
</p>
<p>. (8.439)
</p>
<p>By means of this equation and its adjoint, we may rewrite (8.436) as
</p>
<p>[ p̂,ψ(x̂)] = ih̄
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>[
d|x〉
dx
</p>
<p>ψ(x)〈x|+ |x〉ψ(x)d〈x|
dx
</p>
<p>]
dx
</p>
<p>= ih̄
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>{
d
</p>
<p>dx
[|x〉ψ(x)〈x|]&minus;|x〉ψ &prime;(x)〈x|
</p>
<p>}
dx , (8.440)
</p>
<p>When we compute the expectation value of this expression with respect to the state
</p>
<p>ket |φ , t〉, the first term in the integrand can be dropped provided that ψ(x) becomes
much larger than the system energy in the |x| &rarr; &infin; limit. This is because 〈φ , t|x〉
vanishes rapidly as |x| &rarr; &infin; for such ψ(x). (See Fig. 8.2 for example.) More explic-</p>
<p/>
</div>
<div class="page"><p/>
<p>8.16 Quantum Statistical Mechanics 377
</p>
<p>itly,
</p>
<p>〈φ , t|
{&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>d
</p>
<p>dx
[|x〉ψ(x)〈x|]dx
</p>
<p>}
|φ , t〉 =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>d
</p>
<p>dx
[〈φ , t|x〉ψ(x)〈x|φ , t〉]dx
</p>
<p>= 〈φ , t|x〉ψ(x)〈x|φ , t〉|&infin;&minus;&infin; = 0 . (8.441)
</p>
<p>As far as the expectation value is concerned, therefore, we may write
</p>
<p>[ p̂,ψ(x̂)] =&minus;ih̄
&int; &infin;
</p>
<p>&minus;&infin;
|x〉ψ &prime;(x)〈x|dx =&minus;ih̄ψ &prime;(x̂) , (8.442)
</p>
<p>where the last step follows from (8.131).
</p>
<p>Finally, exchanging the role of p̂ and x̂ in (8.423) and noting that [x̂, p̂] =
&minus;[p̂, x̂] = ih̄, we find
</p>
<p>[x̂, f ( p̂)] = ih̄ f &prime;( p̂) . (8.443)
</p>
<p>This formula, too, can be extended for f that does not have the Maclaurin expansion.
</p>
<p>Exercise 8.32. Using (8.252), show that (8.443) holds even for a function that does
</p>
<p>not have the Maclaurin expansion. Assume that
</p>
<p>lim
p&rarr;&plusmn;&infin;
</p>
<p>〈φ , t|p〉 f (p)〈p|φ , t〉= 0 . (8.444)
</p>
<p>///
</p>
<p>8.16 Quantum Statistical Mechanics
</p>
<p>In this final section, we build on the key concepts of quantum mechanics we have
</p>
<p>reviewed so far and formulate statistical mechanics using the language of quantum
</p>
<p>mechanics.
</p>
<p>8.16.1 Density Matrix
</p>
<p>In classical mechanics, we considered the ensemble average given by (3.48):
</p>
<p>〈A〉=
&int;
</p>
<p>A(q f , p f )ρ(q f , p f )dq f dp f , (8.445)
</p>
<p>which was identified with the result of a measurement of some dynamical variable
</p>
<p>A. We suppose that an analogous relation holds for quantum mechanical systems
</p>
<p>and write
</p>
<p>〈A〉=&sum;
α
</p>
<p>ωα〈φα |Â|φα〉 , (8.446)</p>
<p/>
</div>
<div class="page"><p/>
<p>378 8 Quantum Formulation
</p>
<p>where 〈φα |Â|φα〉 is the expectation value for the outcome of the measurement of A
when the system is in the microstate α for which the state ket is |φα〉. Note that this
averaging is a part of the postulate of quantum mechanics and that it was absent in
</p>
<p>classical mechanics. The coefficient ωα is the quantum counterpart of ρ(q f , p f )
in classical statistical mechanics and is the probability that the system is in the
</p>
<p>microstate α . Being a probability, ωα must satisfy the normalization condition:
</p>
<p>&sum;
α
</p>
<p>ωα = 1 . (8.447)
</p>
<p>We emphasize that |φα〉 (α = 1,2, . . .) do not necessarily form a basis, nor are they
orthogonal. In what follows, however, we assume that they are normalized. Using
</p>
<p>(8.128), we can rewrite (8.446) as
</p>
<p>〈A〉 =&sum;
α ,i
</p>
<p>ωα〈φα |ai〉ai〈ai|φα〉=&sum;
α ,i
</p>
<p>〈ai|φα〉ωα〈φα |ai〉ai =&sum;
α ,i
</p>
<p>〈ai|φα〉ωα〈φα |Â|ai〉
</p>
<p>=&sum;
i
</p>
<p>〈ai|
[
&sum;
α
</p>
<p>|φα〉ωα〈φα |
]
</p>
<p>Â|ai〉=:&sum;
i
</p>
<p>〈ai|ρ̂ t Â|ai〉= Tr{ρ̂ t Â} , (8.448)
</p>
<p>where we defined the density operator ρ̂ t by
</p>
<p>ρ̂ t :=&sum;
α
</p>
<p>|φα〉ωα〈φα | . (8.449)
</p>
<p>In terms of the density operator, (8.447) is written as
</p>
<p>Tr{ρ̂ t}= 1 . (8.450)
</p>
<p>In fact,
</p>
<p>Tr{ρ̂ t}=&sum;
i,α
</p>
<p>〈ai|φα〉ωα〈φα |ai〉=&sum;
i,α
</p>
<p>ωα〈φα |ai〉〈ai|φα〉=&sum;
α
</p>
<p>ωα〈φα |φα〉=&sum;
α
</p>
<p>ωα ,
</p>
<p>(8.451)
</p>
<p>where we used the closure relation and the fact that |φα〉 is normalized.
</p>
<p>8.16.2 Statistical Equilibrium
</p>
<p>As is the case with classical mechanics, we first define statistical equilibrium by
</p>
<p>demanding that
d〈A〉
</p>
<p>dt
&equiv; 0 (8.452)
</p>
<p>hold for any observable Â that is independent of time. To express this in a more
</p>
<p>informative fashion, let us first rewrite the trace using some time-independent basis</p>
<p/>
</div>
<div class="page"><p/>
<p>8.16 Quantum Statistical Mechanics 379
</p>
<p>{|b1〉, . . . , |br〉}:
</p>
<p>〈A〉= Tr{ρ̂ t Â}=&sum;
i, j
</p>
<p>〈bi|ρ̂ t |b j〉〈b j|Â|bi〉 . (8.453)
</p>
<p>Because Â is time independent, the matrix element 〈b j|Â|bi〉 is independent of time
and the only time dependence of 〈A〉 is in ρ̂ t . Thus,
</p>
<p>d〈A〉
dt
</p>
<p>=&sum;
i, j
</p>
<p>&lang;
bi
</p>
<p>∣∣∣∣
dρ̂ t
dt
</p>
<p>∣∣∣∣b j
&rang;
〈b j|Â|bi〉 . (8.454)
</p>
<p>We see shortly that this expression vanishes for arbitrary Â if and only if all the
</p>
<p>matrix elements of dρ̂ t/dt vanish in the B-representation:
</p>
<p>&lang;
bi
</p>
<p>∣∣∣∣
dρ̂ t
dt
</p>
<p>∣∣∣∣b j
&rang;
&equiv; 0 (8.455)
</p>
<p>which is true if and only if
dρ̂ t
dt
</p>
<p>&equiv; 0 . (8.456)
</p>
<p>This is then the necessary and sufficient condition for statistical equilibrium.
</p>
<p>Equation (8.455) is evidently sufficient for (8.452). To see its necessity, suppose
</p>
<p>that
</p>
<p>〈bβ |Â|bα〉 	= 0 . (8.457)
</p>
<p>Because Â is Hermitian,
</p>
<p>〈bα |Â|bβ 〉= 〈bβ |Â|bα〉&lowast; 	= 0 . (8.458)
</p>
<p>If these are the only nonzero matrix elements of Â in the B-representation, (8.454)
</p>
<p>becomes
</p>
<p>d〈A〉
dt
</p>
<p>=
</p>
<p>&lang;
bα
</p>
<p>∣∣∣∣
dρ̂ t
dt
</p>
<p>∣∣∣∣bβ
&rang;
〈bβ |Â|bα〉+
</p>
<p>&lang;
bβ
</p>
<p>∣∣∣∣
dρ̂ t
dt
</p>
<p>∣∣∣∣bα
&rang;
〈bα |Â|bβ 〉 . (8.459)
</p>
<p>Because ωα in (8.449) is a real number, we see that ρ̂ t is a Hermitian at all time.
Thus,
</p>
<p>dρ̂ t
dt
</p>
<p>= lim
∆ t&rarr;0
</p>
<p>ρ̂ t+∆ t &minus; ρ̂ t
∆ t
</p>
<p>(8.460)
</p>
<p>is also a Hermitian and we can write
</p>
<p>d〈A〉
dt
</p>
<p>=
</p>
<p>&lang;
bα
</p>
<p>∣∣∣∣
dρ̂ t
dt
</p>
<p>∣∣∣∣bβ
&rang;
〈bα |Â|bβ 〉&lowast;+
</p>
<p>&lang;
bα
</p>
<p>∣∣∣∣
dρ̂ t
dt
</p>
<p>∣∣∣∣bβ
&rang;&lowast;
</p>
<p>〈bα |Â|bβ 〉 . (8.461)
</p>
<p>which is twice the real part of the expression
</p>
<p>&lang;
bα
</p>
<p>∣∣∣∣
dρ̂ t
dt
</p>
<p>∣∣∣∣bβ
&rang;
〈bα |Â|bβ 〉&lowast; . (8.462)</p>
<p/>
</div>
<div class="page"><p/>
<p>380 8 Quantum Formulation
</p>
<p>Equation (8.452) thus demands that the expression (8.462) be purely imaginary. But,
</p>
<p>since Â is arbitrary, and hence 〈bα |Â|bβ 〉 is a complex number in general, this will
be the case only if &lang;
</p>
<p>bα
</p>
<p>∣∣∣∣
dρ̂ t
dt
</p>
<p>∣∣∣∣bβ
&rang;
= 0 . (8.463)
</p>
<p>8.16.3 Liouville&rsquo;s Theorem
</p>
<p>Equation (8.456) can be transformed into a more revealing form by means of the
</p>
<p>quantum mechanical version of Liouville&rsquo;s theorem. The situation parallels what
</p>
<p>we saw in classical statistical mechanics.
</p>
<p>Before proceeding to the proof of the theorem, we must point out that the ket
</p>
<p>|φα〉 in ρ̂ t , in general, evolves with time. There is nothing in our definition of ρ̂ t
to exclude the possibility that the ket evolves to become yet another ket already
</p>
<p>included in the summation over α or otherwise become something that was not in
that summation initially. So, we need to be a little more precise about what is meant
</p>
<p>by the sum over α in (8.449). The density matrix is constructed at some fixed point
in time, say at t = 0, using the state kets |φ1,0〉, |φ2,0〉,. . .. These kets may evolve
with time and, at a later time t, become |φ1, t〉, |φ2, t〉,. . ., respectively. Then, at time
t, our ρ̂ t will be given by
</p>
<p>ρ̂ t =&sum;
α
</p>
<p>|φα , t〉ωα〈φα , t| . (8.464)
</p>
<p>That is, ωα does not change with time because this refers to the fraction of copies
in microstate |φα〉 at time t = 0 in the statistical ensemble that was created at that
time.
</p>
<p>We now proceed to evaluate dρ̂ t/dt. Taking the time derivative of (8.464), we
have
</p>
<p>dρ̂ t
dt
</p>
<p>=&sum;
α
</p>
<p>[
d|φα , t〉
</p>
<p>dt
ωα〈φα , t|+ |φα , t〉ωα
</p>
<p>d〈φα , t|
dt
</p>
<p>]
. (8.465)
</p>
<p>Recall the Schrödinger equation (8.261) and take its adjoint:
</p>
<p>d|φα , t〉
dt
</p>
<p>=
Ĥ(t)
</p>
<p>ih̄
|φα , t〉 and
</p>
<p>d〈φα , t|
dt
</p>
<p>=&minus;〈φα , t|
Ĥ(t)
</p>
<p>ih̄
, (8.466)
</p>
<p>where we allowed for the time dependence of Ĥ for now. Using these equations in
</p>
<p>(8.465), we arrive at the quantum mechanical version of Liouville&rsquo;s theorem:
</p>
<p>dρ̂ t
dt
</p>
<p>+
1
</p>
<p>ih̄
[ρ̂ t , Ĥ(t)] = 0 . (8.467)
</p>
<p>It should be emphasized that (8.467) is a condition any density operator must
</p>
<p>satisfy regardless of whether the system is in statistical equilibrium or not. This is
</p>
<p>quite analogous to what was said about (3.33) in classical statistical mechanics.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.16 Quantum Statistical Mechanics 381
</p>
<p>So, the necessary and sufficient condition of statistical equilibrium is that both
</p>
<p>(8.456) and (8.467) hold. But, this is equivalent to demanding both (8.456) and
</p>
<p>[ρ̂ t , Ĥ(t)] = 0 . (8.468)
</p>
<p>This equation is the quantum mechanical version of (3.37) and, in view of Theo-
</p>
<p>rem 8.4, implies that ρ̂ t is diagonal in the H-representation. For this to be the case,
it is sufficient that ρ̂ t be a function of Ĥ only:
</p>
<p>ρ̂ t = f (Ĥ(t)) . (8.469)
</p>
<p>Equation (8.456) then demands that Ĥ be independent of time.
</p>
<p>Exercise 8.33. If you have read Sects. 8.10 and 8.11, this exercise provides an alter-
</p>
<p>native derivation of (8.467) and (8.468).
</p>
<p>a. Show that
</p>
<p>ρ̂ t = Û t ρ̂0Û
&dagger;
t . (8.470)
</p>
<p>b. Using (8.290) and (8.470), derive (8.467).
</p>
<p>c. Recall (8.298) and show that
</p>
<p>Tr{ρ̂ t Â}= Tr{ρ̂0ÂH(t)} . (8.471)
</p>
<p>d. Using (8.306), show that
</p>
<p>d〈A〉
dt
</p>
<p>= Tr{[Ĥ(t), ρ̂ t ]Â} , (8.472)
</p>
<p>where Â is a Hermitian operator that is independent of time. Demanding that
</p>
<p>(8.452) holds for any Â, we arrive at (8.468). ///
</p>
<p>8.16.4 Canonical Ensemble
</p>
<p>Restricting ourselves for systems with time-independent Ĥ, we define quantum
</p>
<p>mechanical canonical ensemble by
</p>
<p>ρ̂ =
1
</p>
<p>Z
e&minus;β Ĥ , (8.473)
</p>
<p>where we dropped the subscript t. Since any function of Ĥ commutes with Ĥ, this
</p>
<p>expression clearly satisfies (8.468) and is an example of (8.469).
</p>
<p>Using the orthonormal basis of energy eigenkets, that is, the H-representation,
</p>
<p>and recalling (8.131), we can express ρ̂ as
</p>
<p>ρ̂ =
1
</p>
<p>Z
&sum;
n
</p>
<p>|n〉e&minus;βEn〈n| . (8.474)</p>
<p/>
</div>
<div class="page"><p/>
<p>382 8 Quantum Formulation
</p>
<p>Comparing this expression with (8.449), we see that the probability in this ensemble
</p>
<p>that a given copy is in the energy eigenstate |n〉 belonging to the eigenvalue En is
e&minus;βEn/Z.
</p>
<p>The normalization constant Z is the canonical partition function and is given
</p>
<p>by
</p>
<p>Z = Tr{e&minus;β Ĥ} . (8.475)
In the H-representation, this reads
</p>
<p>Z =&sum;
n
</p>
<p>〈n|e&minus;β Ĥ |n〉=&sum;
n
</p>
<p>e&minus;βEn , (8.476)
</p>
<p>where the sum is over all energy eigenstates labeled by n. We can also write Z as
</p>
<p>Z =&sum;
En
</p>
<p>g(En)e
&minus;βEn , (8.477)
</p>
<p>where the sum is now over all distinct energy eigenvalues and g(En) is the degen-
eracy of the eigenvalue En, that is, the number of linearly independent eigenkets
</p>
<p>corresponding to En.
</p>
<p>Using the Dirac δ -function, we can rewrite (8.477) as
</p>
<p>Z =&sum;
En
</p>
<p>g(En)
&int;
</p>
<p>δ (E &minus;En)e&minus;βEdE =
&int;
&sum;
En
</p>
<p>g(En)δ (E &minus;En)e&minus;βEdE . (8.478)
</p>
<p>As we have seen in Chaps. 4 and 5,
</p>
<p>Ω(E) :=&sum;
En
</p>
<p>g(En)δ (E &minus;En) (8.479)
</p>
<p>is nothing but the density of states when the possible values of the system energy
</p>
<p>E form a discrete spectrum. This observation allows us to write (8.477) as
</p>
<p>Z =
</p>
<p>&int;
Ω(E)e&minus;βEdE , (8.480)
</p>
<p>in agreement with our earlier result of classical statistical mechanics. See (4.72) in
</p>
<p>particular.
</p>
<p>Exercise 8.34. The Hamiltonian of a two-state system is given by
</p>
<p>Ĥ = ia [|φ1〉〈φ2|&minus; |φ2〉〈φ1|] , (8.481)
</p>
<p>where 〈φi|φ j〉= δi j and a is a positive constant having the dimension of energy:
a. Find the energy eigenvalues and the corresponding eigenkets as linear combina-
</p>
<p>tions of |φ1〉 and |φ2〉.
b. Suppose that you have an ensemble of the identical systems. At some instant,
</p>
<p>30 % of them are in the state |φ1〉 and the rest is in the state |φ2〉. Construct the
density matrix and calculate the average energy for the ensemble at that instant.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.16 Quantum Statistical Mechanics 383
</p>
<p>c. Find the matrix representation of ρ̂ in the H-representation.
d. The system is brought to contact with a thermal bath at temperature T , thereby
</p>
<p>changing ρ̂ . After a sufficient time has passed, the system has reached statisti-
cal equilibrium. Find the Helmholtz free energy and the internal energy of the
</p>
<p>system. ///
</p>
<p>8.16.5 Ideal Gas and Classical Limit
</p>
<p>Let us compute the canonical partition function for a particle of mass m confined to
</p>
<p>a rectangular box of dimension Lx &times;Ly &times;Lz. As shown in Example 8.13, the energy
eigenvalues are determined by (8.363).
</p>
<p>For a given value of E, (8.363) defines an ellipsoid with semi-principal axes of
</p>
<p>length
&radic;
</p>
<p>8mE/h2Lx,
&radic;
</p>
<p>8mE/h2Ly, and
&radic;
</p>
<p>8mE/h2Lz in the three-dimensional space
defined by nx-, ny-, nz-axes. (If Lx = Ly = Lz = L, the ellipsoid becomes a sphere
</p>
<p>of radius
&radic;
</p>
<p>8mE/h2L.) Now, each point in the nxnynz-space whose coordinates are
positive integers represents a distinct energy eigenstate. The number of such points
</p>
<p>on or inside the ellipsoid gives the number W (E) of distinct energy eigenstates
whose energy is equal to or less than E. But, because each point occupies a unit
</p>
<p>volume, W (E) is approximately 1/8 of the volume of the ellipsoid. In the factor
1/8 = (1/2)3, the first factor of 1/2 removes the half of the ellipsoid, which corre-
sponds to nx &le; 0. The second 1/2 removes the half of the remaining portion of the
ellipsoid, for which ny &le; 0, and so on.
</p>
<p>We note that the ellipsoid with semi-principal axes of length a, b, and c is
</p>
<p>obtained from the unit sphere by stretching it by factors of a, b, and c in x-, y-, and
</p>
<p>z-directions, respectively. So, the volume of the ellipsoid is 4πabc/3. This leads to
</p>
<p>W (E)&asymp; π
6
</p>
<p>(
8mE
</p>
<p>h2
</p>
<p>)3/2
LxLyLz =
</p>
<p>π
</p>
<p>6
</p>
<p>(
8mE
</p>
<p>h2
</p>
<p>)3/2
V , (8.482)
</p>
<p>where V = LxLyLz is the volume of the box. We can now compute the density of
states as
</p>
<p>Ω(E) =
&part;W
</p>
<p>&part;E
&asymp; π
</p>
<p>4
</p>
<p>(
8m
</p>
<p>h2
</p>
<p>)3/2
V E1/2 . (8.483)
</p>
<p>Using this expression in (8.480) and recalling the formula (3.92), we arrive at
</p>
<p>Z =
V
</p>
<p>Λ 3
(8.484)
</p>
<p>in agreement with the prediction of the classical statistical mechanics, that is, (3.86)
</p>
<p>with the correction factor h&minus;3.
Suppose now that we have N noninteracting identical particles in the box. In this
</p>
<p>case, (8.363) is replaced by</p>
<p/>
</div>
<div class="page"><p/>
<p>384 8 Quantum Formulation
</p>
<p>8mE
</p>
<p>h2
=
</p>
<p>N
</p>
<p>&sum;
i=1
</p>
<p>[(
nxi
</p>
<p>Lx
</p>
<p>)2
+
</p>
<p>(
nyi
</p>
<p>Ly
</p>
<p>)2
+
</p>
<p>(
nzi
</p>
<p>Lz
</p>
<p>)2]
(8.485)
</p>
<p>with subscript i labeling a particle. This equation defines an ellipsoid in a 3N-
</p>
<p>dimensional space. Recalling (4.47), which gives the volume of a n-dimensional
</p>
<p>unit sphere, we find
</p>
<p>W (E) =
1
</p>
<p>8NN!
</p>
<p>π3N/2
</p>
<p>Γ (3N/2+1)
</p>
<p>(
8mE
</p>
<p>h2
</p>
<p>)3N/2
V N (8.486)
</p>
<p>in place of (8.482). Here, the 1/8N factor selects out the portion of the ellipsoid
corresponding to the positive values of nx1, . . . ,nzN and the 1/N! factor arises from
the indistinguishability of identical particles. Thus,
</p>
<p>Ω(E) =
1
</p>
<p>8N!
</p>
<p>π3N/2
</p>
<p>Γ (3N/2+1)
</p>
<p>(
8m
</p>
<p>h2
</p>
<p>)3N/2
V N
</p>
<p>3N
</p>
<p>2
E3N/2&minus;1 . (8.487)
</p>
<p>With the aid of (4.42) and (4.43), this leads to
</p>
<p>Z =
V N
</p>
<p>Λ 3NN!
(8.488)
</p>
<p>in agreement with what you saw in Exercise 3.16 for the classical mechanical sys-
</p>
<p>tem.
</p>
<p>According to quantum mechanics, a particle is either a fermion following a
</p>
<p>Fermi&ndash;Dirac statistics or a boson following a Bose&ndash;Einstein statistics. No more
</p>
<p>than one fermion can occupy the same quantum state (specified by the triplet of
</p>
<p>numbers nx, ny, and nz for the ideal gas under consideration) at any instant. No such
</p>
<p>restriction applies to bosons. Yet, (8.488) fails for bosons at high densities. This is
</p>
<p>because the 1/N! factor can no longer properly account for the indistinguishabil-
ity of identical particles if more than one particle occupies a given quantum state.
</p>
<p>These are the results of the so-called exchange effect, which is present even if the
</p>
<p>Hamiltonian has no potential energy term due to interaction among particles, and
</p>
<p>lead to net repulsion among fermions and net attraction among bosons.
</p>
<p>If the system is sufficiently dilute, the exchange effect can safely be ignored since
</p>
<p>there will be far more quantum states than there are particles and the probability
</p>
<p>that any given quantum state is occupied is very small. This is called the classical
</p>
<p>limit. Because our treatment of the ideal gas ignores the exchange effect, (8.488)
</p>
<p>is applicable only in this limit. Further details on Fermi&ndash;Dirac and Bose&ndash;Einstein
</p>
<p>statistics can be found in Refs. [2, 3, 5].</p>
<p/>
</div>
<div class="page"><p/>
<p>8.16 Quantum Statistical Mechanics 385
</p>
<p>8.16.6 Microcanonical Ensemble
</p>
<p>Following the approach we took in Chap. 4, we introduce a microcanonical ensem-
</p>
<p>ble as an approximation to a canonical ensemble when the system under considera-
</p>
<p>tion is macroscopic.
</p>
<p>In a macroscopic system, the difference between two consecutive energy eigen-
</p>
<p>values is very small. This being the case, it makes sense to lump together the densely
</p>
<p>distributed δ -functions in (8.479) and regard Ω(E) as a continuous function of E.
More explicitly, (8.479) may be replaced by the approximate expression
</p>
<p>Ω(E)&asymp; 1
δE
</p>
<p>&int; E
</p>
<p>E&minus;δE
&sum;
En
</p>
<p>g(En)δ (E &minus;En)dE , (8.489)
</p>
<p>without incurring any sensible loss of accuracy in (8.480), where δE is a small inter-
val of energy that is nevertheless much larger than the difference in two consecutive
</p>
<p>energy eigenvalues.
</p>
<p>This modified density of states is usually a very rapidly increasing function of E.
</p>
<p>When such a function is multiplied by the very rapidly decreasing function e&minus;βE ,
where β &gt; 0, the result is often a very sharply peaked function of E.48 If this hap-
pens, we can take an small interval (E &minus;∆E,E] that contains the sharp peak and
safely assume that essentially all of the copies in the canonical ensemble belongs to
</p>
<p>this narrow interval of the energy. For sufficiently small ∆E, all states in the inter-
val E &minus;∆E &lt; En &le; E may be regarded as equally probable. This then defines our
microcanonical ensemble. In the H-representation, therefore,
</p>
<p>ρ̂ =
1
</p>
<p>CM
&sum;
n
</p>
<p>|n〉 f (En)〈n| , (8.490)
</p>
<p>where
</p>
<p>f (En) =
</p>
<p>{
1 if E &minus;∆E &lt; En &le; E
0 otherwise.
</p>
<p>(8.491)
</p>
<p>The quantity CM is the normalization constant and is determined by the normaliza-
</p>
<p>tion condition (8.450), which now yields the microcanonical partition function
</p>
<p>CM =&sum;
n
</p>
<p>f (En) , (8.492)
</p>
<p>where the summation is over all energy eigenstates. Because of (8.491), CM is the
</p>
<p>total number of energy eigenstates corresponding to the energy eigenvalues that fall
</p>
<p>into the interval E &minus;∆E &lt; En &le; E. Thus, using ∆E for δE in (8.489),
</p>
<p>CM =
&int; E
</p>
<p>E&minus;∆E
&sum;
En
</p>
<p>g(En)δ (E &minus;En)dE =Ω(E)∆E , (8.493)</p>
<p/>
</div>
<div class="page"><p/>
<p>386 8 Quantum Formulation
</p>
<p>which is nothing but (4.24).
</p>
<p>As we saw in Chap. 4, other statistical ensembles can be derived from (8.493) and
</p>
<p>the physical interpretation given to Ω(E)∆E. Our derivation in that chapter made no
reference to the underlying mechanics except in the case of the isothermal&ndash;isobaric
</p>
<p>ensemble.49 Any difference between the classical and the quantum mechanical for-
</p>
<p>mulations of statistical mechanics is thus seen to arise in part from the form of Ω(E)
prescribed by the underlying mechanics. We have already encountered a profound
</p>
<p>implication of this difference in Example 4.2. The other difference stems from the
</p>
<p>exchange effect that impacts how a quantum mechanical system populates those
</p>
<p>microstates enumerated by Ω(E). This is then a logical place for us to end this
brief excursion into quantum mechanics and its application in formulating statisti-
</p>
<p>cal mechanics.
</p>
<p>8.17 Frequently Used Symbols
</p>
<p>&dagger; , adjoint.
</p>
<p>c&lowast; , complex conjugate of c.
〈A〉 , ensemble average of a dynamical variable A.
〈A〉QM , quantum mechanical average of A.
|n〉 , energy eigenket corresponding to the nthe energy eigenvalue En.
|p〉 , momentum eigenket.
|x〉 , position eigenket.
[X̂ ,Ŷ ] , commutator defined by X̂Ŷ &minus; Ŷ X̂ .
[X̂ ,Ŷ ]+ , anti-commutator defined by X̂Ŷ + Ŷ X̂ .
Tr{Â} , trace of Â.
</p>
<p>â , annihilation operator.
</p>
<p>â&dagger; , creation operator.
</p>
<p>g(En) , degeneracy of the energy eigenvalue En.
h̄ , h/2π , where h = 6.626&times;10&minus;34 (J&middot;s) is the Planck constant.
i , imaginary unit
</p>
<p>&radic;
&minus;1.
</p>
<p>m , mass of a particle.
</p>
<p>p̂ , momentum operator.
</p>
<p>r , dimension of a ket space Vk.
</p>
<p>s , degeneracy of an eigenvalue.
</p>
<p>x̂ , position operator.
</p>
<p>Â , B̂ ,. . . , observables.
</p>
<p>CM , microcanonical partition function.
</p>
<p>En , the nth energy eigenvalue.
</p>
<p>Ĥ , Hamiltonian operator.
</p>
<p>Î , identity operator.
</p>
<p>N̂ , number operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>References and Further Reading 387
</p>
<p>Û t , time evolution operator that advances the state ket from t = 0 to t.
X̂ , Ŷ ,. . . , generic linear operator.
</p>
<p>Z , canonical partition function.
</p>
<p>δ (x) , the Dirac δ -function.
ρ̂ t , density operator at time t.
Ω , density of states.
</p>
<p>References and Further Reading
</p>
<p>1. Ballentine L E (1998) Quantum Mechanics: A Modern Development. World Scientific, River
</p>
<p>Edge, New Jersey
</p>
<p>For a more careful discussion of infinite dimensional vector space and operators with a contin-
</p>
<p>uous spectrum, see Sects. 1.3 and 1.4.
</p>
<p>2. Goodstein D L (1985) States of matter. Dover, New York
</p>
<p>Chapter 2 provides a highly readable account of Fermi&ndash;Dirac and Bose&ndash;Einstein statistics.
</p>
<p>3. Landau L D, Lifshitz E M (1980) Statistical physics: Part 1, 3rd edn. Pergamon Press, New
</p>
<p>York
</p>
<p>A detailed treatment of Fermi&ndash;Dirac and Bose&ndash;Einstein statistics is found in Chap. 5.
</p>
<p>4. Macdonald A (2011) Linear and Geometric Algebra. Printed by CreateSpace
</p>
<p>The first four chapters offer a very compact and highly accessible introduction to linear algebra
</p>
<p>in just 70 pages.
</p>
<p>5. Pathria R K (1972) Statistical Mechanics. Pergamon, Elmsford, New York
</p>
<p>For a detailed consideration of classical limit, see Sect. 5.5.
</p>
<p>6. Pauling L, Wilson, Jr. E B (1985) Introduction to Quantum Mechanics with Applications to
</p>
<p>Chemistry. Dover, New York
</p>
<p>A very lucid and detailed treatment of the one-dimensional harmonic oscillator is found in
</p>
<p>Chap. 3.
</p>
<p>7. Sakurai J J (1985) Modern Quantum Mechanics. Addison-Wesley, Reading, Massachusetts
</p>
<p>The need for defining a quantum mechanical state as a vector in a complex vector space is
</p>
<p>demonstrated using the Stern-Garlach experiment at the very beginning of the book. Our own
</p>
<p>outline of quantum mechanics followed the first two chapters of the book, but focusing only on
</p>
<p>those topics essential for our purpose. The density matrix is treated in some detail in Sect. 3.4
</p>
<p>of the book.
</p>
<p>8. Schlosshauer M (2007) Decoherence and the Quantum-to-Classical Transition. Springer-
</p>
<p>Verlag, Heidelberg
</p>
<p>9. Tolman R C (1979) The principles of statistical mechanics. Dover, New York
</p>
<p>Chapters 7 and 8 provides a very thorough introduction to quantum mechanics. For a summary
</p>
<p>of relevant experimental discoveries that lead to quantum mechanics, see Sect. 52.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A
</p>
<p>Vectors in Three-Dimensional Space
</p>
<p>In this appendix, we review a few key facts about the algebra of ordinary three-
</p>
<p>dimensional vectors. In doing so, we recall a few frequently used identities and
</p>
<p>obtain a useful analogy that will help us in our exploration of a more general vector
</p>
<p>space in Chap. 8.
</p>
<p>A.1 Arrow in Space
</p>
<p>A vector a, as we were told in our first encounter with it, is an arrow. As an arrow,
</p>
<p>it has the length and the direction it points to. However, we assign no significance
</p>
<p>to the absolute location of a in space. Thus, a and a&prime;, the latter being obtained by
translating a without changing its length or direction, are actually the same vector.
</p>
<p>Given a positive number α , we may construct another vector that points to the
same direction as a but with its length given by α times that of a. This vector is
denoted by αa. When α &lt; 0, we agree to denote by αa a vector pointing to the
opposite direction with its length &minus;α(&gt; 0) times that of a. If α = 0, we say that αa
is the zero vector and denote it by 0. The length of 0 is zero. We do not associate
</p>
<p>with 0 any direction as we have no need to do so. Addition of two vectors a and b is
</p>
<p>defined by means of the parallelogram construction.
</p>
<p>A.2 Components of a Vector
</p>
<p>It becomes quite tedious, however, to draw arrows and parallelograms each time
</p>
<p>we refer to vectors and their additions. We need a more efficient way of handling
</p>
<p>vectors. This is accomplished by representing a vector with the help of a coordinate
</p>
<p>system.
</p>
<p>Given a vector a, its components can be found as follows. For simplicity, let us
</p>
<p>first suppose that the vector is on the xy-plane. Figure A.1 illustrates the process.
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 389
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1</p>
<p/>
</div>
<div class="page"><p/>
<p>390 A Vectors in Three-Dimensional Space
</p>
<p>ax
</p>
<p>ay
</p>
<p>bx
</p>
<p>by
</p>
<p>a
</p>
<p>b
</p>
<p>ex
</p>
<p>ey
</p>
<p>x
</p>
<p>y
</p>
<p>Fig. A.1 Components of two-dimensional vectors a and b.
</p>
<p>We move the vector, without changing its length or direction, so that the arrow
</p>
<p>starts from the origin. Then, the x-coordinate of the tip of the arrow, denoted by
</p>
<p>ax in Fig. A.1, is the x-component of a. Likewise, the y-coordinate of the tip is the
</p>
<p>y-component of a. As is the case with b, a component of a vector can be a negative
</p>
<p>number.
</p>
<p>Let ex be the vector whose x- and y- components are 1 and 0, respectively. Simi-
</p>
<p>larly, we denote by ey the vector whose x- and y- components are, respectively, 0 and
</p>
<p>1. Then, from what has been said about the multiplication of a vector a by a num-
</p>
<p>ber α , we see that axex is a vector represented by an arrow starting from the origin
and ending at the point (ax,0) on the x-axis. Likewise, ayey is a vector represented
by an arrow from the origin to the point (0,ay) on the y-axis. By the parallelogram
construction for adding two vectors, we see that
</p>
<p>a= axex +ayey , (A.1)
</p>
<p>which represents the vector a in terms of its components. (See Fig. A.1.)
</p>
<p>Equation (A.1) generalizes naturally to three-dimensional space:
</p>
<p>a= axex +ayey +azez . (A.2)
</p>
<p>We shall often express this relation by
</p>
<p>a
.
= (ax,ay,az) . (A.3)
</p>
<p>The symbol &ldquo;
.
=&rdquo; is used here instead of &ldquo;=&rdquo; since the quantity on the left is a vector,
</p>
<p>while the expression on the right is a list of its components. These two things are
</p>
<p>conceptually different. In fact, a, being an arrow in three-dimensional space, does
</p>
<p>not depend on how the coordinate system is set up, while its components do.
</p>
<p>We observe from Fig. A.2 that the x-component of a vector αa is αax while
Fig. A.3 indicates that the x-component of a+ b is ax + bx. Similarly for their y-
components.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 Dot Product 391
</p>
<p>ax ax
</p>
<p>ay
</p>
<p>ay
</p>
<p>a
</p>
<p>a
</p>
<p>x
</p>
<p>y
</p>
<p>Fig. A.2 Components of a and multiplication of a vector a by a number α .
</p>
<p>ax bx ax bx
</p>
<p>a
</p>
<p>b
</p>
<p>a b
</p>
<p>x
</p>
<p>y
</p>
<p>Fig. A.3 Addition of two vectors a and b.
</p>
<p>A.3 Dot Product
</p>
<p>Let a be a vector and denote its x-, y-, and z-components by ax, ay, and az, respec-
</p>
<p>tively. Likewise for b. The dot product between these two vectors is defined by
</p>
<p>a &middot;b := axbx +ayby +azbz . (A.4)
</p>
<p>Example A.1. Simple identities involving dot product between ei&rsquo;s: We first
</p>
<p>note that
</p>
<p>ex
.
= (1,0,0) , ey
</p>
<p>.
= (0,1,0) , and ez
</p>
<p>.
= (0,0,1) . (A.5)
</p>
<p>Then,
</p>
<p>ex &middot; ey = 1&times;0+0&times;1+0&times;0 = 0 . (A.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>392 A Vectors in Three-Dimensional Space
</p>
<p>Likewise, ey &middot; ez = ez &middot; ex = 0. Now,
</p>
<p>ex &middot; ex = 1&times;1+0&times;0+0&times;0 = 1 . (A.7)
</p>
<p>Likewise, ey &middot; ey = ez &middot; ez = 1.
</p>
<p>The length of the vector a is denoted by ||a|| and is defined by
</p>
<p>||a|| :=
&radic;
a &middot;a=
</p>
<p>&radic;
ax2 +ay2 +az2 . (A.8)
</p>
<p>To lighten the notation, we often use a for ||a||.
The above definitions allow us to define the angle θ(a,b) between the two vectors
</p>
<p>a and b by the following relations:
</p>
<p>cosθ(a,b) =
a &middot;b
</p>
<p>||a||||b|| , where 0 &le; θ(a,b)&le; π . (A.9)
</p>
<p>Since &minus;1 &le; cosθ &le; 1, you can find θ(a,b) from this formula if and only if
</p>
<p>&minus;1 &le; a &middot;b||a||||b|| &le; 1 . (A.10)
</p>
<p>That this is always the case follows from the Schwartz inequality proved in Sect. A.5.
</p>
<p>Note also that the angle is undefined if either ||a|| or ||b|| is zero. This is to be
expected since the zero vector does not have a direction associated with it.
</p>
<p>Example A.2. Angle between two vectors:
</p>
<p>a.
</p>
<p>cosθ(ex,ey) =
ex &middot; ey
</p>
<p>||ex||||ey||
= 0 . (A.11)
</p>
<p>Thus,
</p>
<p>θ(ex,ey) =
π
</p>
<p>2
. (A.12)
</p>
<p>b. Let a= axex +ayey so that a
.
= (ax,ay,0). Then,
</p>
<p>cosθ(a,ex) =
a &middot; ex
</p>
<p>||a||||ex||
=
</p>
<p>ax&radic;
ax2 +ay2
</p>
<p>. (A.13)
</p>
<p>These two examples indicate that θ(a,b) defined by (A.9) agrees with what
we expect on the basis of geometry.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.5 &dagger;Schwarz Inequality 393
</p>
<p>A.4 Unit Operator
</p>
<p>From (A.2), we see that
</p>
<p>a &middot; ex = ax , a &middot; ey = ay , and a &middot; ez = az . (A.14)
</p>
<p>Substituting these equations back to (A.2), we find
</p>
<p>a= (a &middot; ex)ex +(a &middot; ey)ey +(a &middot; ez)ez . (A.15)
</p>
<p>Note that (a &middot; b)c, being a scalar (a &middot; b) times a vector (c) is a vector. If we agree to
define
</p>
<p>a &middot; (bc) := (a &middot;b)c , (A.16)
we may rewrite (A.15) as
</p>
<p>a= a &middot; (exex)+a &middot; (eyey)+a &middot; (ezez) = a &middot; (exex + eyey + ezez) . (A.17)
</p>
<p>Since αa= aα and a &middot;b= b &middot;a, (A.15) can also be written as
</p>
<p>a= ex(ex &middot;a)+ ey(ey &middot;a)+ ez(ez &middot;a) . (A.18)
</p>
<p>With the definition
</p>
<p>(ab) &middot; c := a(b &middot; c) , (A.19)
we can write
</p>
<p>a= (exex) &middot;a+(eyey) &middot;a+(ezez) &middot;a= (exex + eyey + ezez) &middot;a . (A.20)
</p>
<p>Equations (A.17) and (A.20) indicate that
</p>
<p>Î := exex + eyey + ezez (A.21)
</p>
<p>is a unit operator in the sense that
</p>
<p>a &middot; Î = Î &middot;a= a (A.22)
</p>
<p>for any vector a.
</p>
<p>A.5 &dagger;Schwarz Inequality
</p>
<p>This inequality states that
</p>
<p>(a &middot;b)2 &le; ||a||2||b||2 (A.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>394 A Vectors in Three-Dimensional Space
</p>
<p>for any a and b. To prove it, let c= a+λb, where λ is a real number. Since ||c||2 =
cx
</p>
<p>2 + cy
2 + cz
</p>
<p>2, we have
</p>
<p>||c||2 &ge; 0 (A.24)
for any λ . We compute ||c|| as follows:
</p>
<p>||c||2 = c &middot; c= (a+λb) &middot; (a+λb) = a &middot;a+λa &middot;b+λb &middot;a+λ 2b &middot;b
= ||a||2 +2λa &middot;b+λ 2||b||2 . (A.25)
</p>
<p>So, we have
</p>
<p>||a||2 +2λa &middot;b+λ 2||b||2 &ge; 0 . (A.26)
As indicated above, this inequality holds for any real number λ . In particular, it will
hold if we happen to choose λ as
</p>
<p>λ =&minus; a &middot;b||b||2 , (A.27)
</p>
<p>which, of course, is a real number. With this choice of λ , (A.26) becomes
</p>
<p>||a||2 &ge; (a &middot;b)
2
</p>
<p>||b||2 . (A.28)
</p>
<p>Since ||b||2 is a positive number, this is equivalent to (A.23) and the proof is com-
plete.
</p>
<p>A.6 Cross Product
</p>
<p>Consider two (nonzero) vectors b and c. If neither is a scalar multiple of the other,
</p>
<p>then, these two vectors are not parallel to each other and hence define a plane. We
</p>
<p>define the cross product b&times;c between two vectors b and c as a vector perpendicular
to the plane defined by b and c. By definition, b&times; c points toward the direction a
right-handed screw advances if it is rotated to bring b toward c by closing the angle
</p>
<p>θ (0 &lt; θ &lt; π) between them. Moreover,
</p>
<p>||b&times; c|| := ||b||||c||sinθ . (A.29)
</p>
<p>Since sin0 = sinπ = 0, no ambiguity arises in the above definition regarding the
direction of b&times; c even if θ = 0 or π , for which b&times; c= 0.
</p>
<p>Let us now introduce another vector a and consider the expression
</p>
<p>a &middot;b&times; c . (A.30)
</p>
<p>Since a &middot; b is a scalar, the cross product sign in this expression makes sense only if
we interpret this expression as a &middot; (b&times; c).</p>
<p/>
</div>
<div class="page"><p/>
<p>A.6 Cross Product 395
</p>
<p>If the set of three vectors a, b, and c taken in this order forms a right-handed
</p>
<p>system as in the x-, y-, and z-axes in a right-handed coordinate system, the expres-
</p>
<p>sion (A.30) is the volume V of the parallelepiped whose edges are defined by these
</p>
<p>vectors. In fact, if we denote by φ the angle between the two vectors a and b&times;c, we
have
</p>
<p>a &middot;b&times; c= ||a||||b&times; c||cosφ = ||a||||b||||c||sinθ cosφ , (A.31)
in which ||b||||c||sinθ is the area of the base of the parallelepiped defined by b and
c, while ||a||cosφ is the height of the parallelepiped.
</p>
<p>We are certainly free to consider c and a (or a and b) as a pair of vectors defining
</p>
<p>the base of the parallelepiped, leading to a different way of computing the same
</p>
<p>volume V of the parallelepiped. In this way, we arrive at the identity
</p>
<p>a &middot;b&times; c= b &middot; c&times;a= c &middot;a&times;b . (A.32)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B
</p>
<p>Useful Formulae
</p>
<p>This appendix is meant to serve as a duct-tape to hold your prior knowledge on
</p>
<p>calculus. Other useful computational tools needed in the book are included for con-
</p>
<p>venience.
</p>
<p>B.1 Taylor Series Expansion
</p>
<p>In the main body of this book, we frequently use Taylor series expansion. Accord-
</p>
<p>ingly, we summarize a few key formulae here.
</p>
<p>B.1.1 Function of a Single Variable
</p>
<p>Let f (x) be a function of a single variable x and suppose that f (x) is differentiable
at x0 however many times we need. Then,
</p>
<p>f (x0 +a) = f (x0)+
d f
</p>
<p>dx
</p>
<p>∣∣∣∣
x=x0
</p>
<p>a+
1
</p>
<p>2!
</p>
<p>d2 f
</p>
<p>dx2
</p>
<p>∣∣∣∣∣
x=x0
</p>
<p>a2 +
1
</p>
<p>3!
</p>
<p>d3 f
</p>
<p>dx3
</p>
<p>∣∣∣∣∣
x=x0
</p>
<p>a3 + &middot; &middot; &middot; , (B.1)
</p>
<p>where the symbol dn f/dxn|x=x0 (n = 1,2,3 . . .) indicates that the value of the func-
tion dn f/dxn of x is to be evaluated at x = x0. Let x = x0 +a and rewrite (B.1) as
</p>
<p>f (x) = f (x0)+
d f
</p>
<p>dx
</p>
<p>∣∣∣∣
x=x0
</p>
<p>(x&minus; x0)+
1
</p>
<p>2!
</p>
<p>d2 f
</p>
<p>dx2
</p>
<p>∣∣∣∣∣
x=x0
</p>
<p>(x&minus; x0)2 + &middot; &middot; &middot; . (B.2)
</p>
<p>The expression on the right-hand side of this equation is called the Taylor series
</p>
<p>expansion of f (x) around x = x0. If x0 = 0, the series is also referred to as the
Maclaurin series.
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 397
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1</p>
<p/>
</div>
<div class="page"><p/>
<p>398 B Useful Formulae
</p>
<p>Example B.1. Exponential function: One famous example of the Maclaurin
</p>
<p>series that shows up from time to time is
</p>
<p>ex = 1+ x+
1
</p>
<p>2!
x2 +
</p>
<p>1
</p>
<p>3!
x3 + &middot; &middot; &middot; . (B.3)
</p>
<p>In order to arrive at this result, we recalled that
</p>
<p>dnex
</p>
<p>dxn
</p>
<p>∣∣∣∣
x=0
</p>
<p>= ex|x=0 = 1 . (B.4)
</p>
<p>Since 0! = 1 by definition, (B.3) can be written more compactly as
</p>
<p>ex =
&infin;
</p>
<p>&sum;
n=0
</p>
<p>xn
</p>
<p>n!
. (B.5)
</p>
<p>Note that (B.1) holds for any value of x0 provided that f (x) is differentiable at
x0 as many times as we desire. This means that the application of (B.1) need not be
</p>
<p>restricted to a particular value of x0, and hence we may rewrite it more simply as
</p>
<p>f (x+a) = f (x)+
d f
</p>
<p>dx
a+
</p>
<p>1
</p>
<p>2!
</p>
<p>d2 f
</p>
<p>dx2
a2 +
</p>
<p>1
</p>
<p>3!
</p>
<p>d3 f
</p>
<p>dx3
a3 + &middot; &middot; &middot; . (B.6)
</p>
<p>When a is replaced by an infinitesimal quantity dx, the second term on the right-
</p>
<p>hand side of (B.6) is often written as
</p>
<p>d f :=
d f
</p>
<p>dx
dx . (B.7)
</p>
<p>The motivation behind this notation becomes clearer if we rewrite this as
</p>
<p>d f =
</p>
<p>(
dx
</p>
<p>d
</p>
<p>dx
</p>
<p>)
f . (B.8)
</p>
<p>That is, we may regard d := dx(d/dx) as a differential operator acting on f . By
analogy, we write
</p>
<p>d2 f =
d2 f
</p>
<p>dx2
(dx)2 =
</p>
<p>(
dx
</p>
<p>d
</p>
<p>dx
</p>
<p>)2
f , d3 f =
</p>
<p>d3 f
</p>
<p>dx3
(dx)3 =
</p>
<p>(
dx
</p>
<p>d
</p>
<p>dx
</p>
<p>)3
f , . . . . (B.9)
</p>
<p>With this notation, we can write
</p>
<p>∆ f := f (x+dx)&minus; f (x) = d f + 1
2!
</p>
<p>d2 f +
1
</p>
<p>3!
d3 f + &middot; &middot; &middot; . (B.10)
</p>
<p>The nth term on the right is referred to as the nth order term. Note that, unless
</p>
<p>dn f &equiv; 0 for all n &ge; 2, ∆ f 	= d f in general.</p>
<p/>
</div>
<div class="page"><p/>
<p>B.1 Taylor Series Expansion 399
</p>
<p>More often than not, we are interested only in the most significant contribution
</p>
<p>(d f ) to ∆ f . In such a situation, we suppress d2 f/2!, d3 f/3!, . . ., and write
</p>
<p>∆ f = d f +h.o. (B.11)
</p>
<p>We say that ∆ f is given by d f to the first order of dx. The symbol h.o. stands for the
higher order terms.
</p>
<p>It is somewhat amusing to note that (B.10) can be rewritten, in a purely formal
</p>
<p>manner, as follows:
</p>
<p>f (x+dx) =
</p>
<p>(
1+d+
</p>
<p>1
</p>
<p>2!
d2 +
</p>
<p>1
</p>
<p>3!
d3 + &middot; &middot; &middot;
</p>
<p>)
f (x) = ed f (x) , (B.12)
</p>
<p>indicating that the effect of the operator ed is to &ldquo;advance&rdquo; x by dx, thus converting
</p>
<p>f (x) into f (x+ dx). If we were to define dn f by (n!)&minus;1dn f/dxn, this expression
would not follow. It should also be noted that, in writing this equation, we have
</p>
<p>assumed that f is differentiable as many times as we desire.
</p>
<p>Suppose that f (x) is continuously differentiable, that is d f/dx exists and is con-
tinuous. Then, for f (x) to take an extremum value at x = x0, it is necessary and
sufficient that
</p>
<p>d f =
d f
</p>
<p>dx
</p>
<p>∣∣∣∣
x=x0
</p>
<p>dx = 0 (B.13)
</p>
<p>for any dx. We often express this fact by saying that &ldquo; f (x0 + dx)&minus; f (x0) is zero to
the first order of dx.&rdquo; Because dx is arbitrary, this is equivalent to
</p>
<p>d f
</p>
<p>dx
</p>
<p>∣∣∣∣
x=x0
</p>
<p>= 0 . (B.14)
</p>
<p>Since d f/dx is a function of x, this is an equation for x0.
</p>
<p>B.1.2 Function of Multiple Variables
</p>
<p>The similar results hold when f is a scalar-valued function of multiple variables. For
</p>
<p>example, (B.10) remains valid for f = f (x,y) if the differential operator d is defined
by
</p>
<p>d := dx
&part;
</p>
<p>&part;x
+dy
</p>
<p>&part;
</p>
<p>&part;y
. (B.15)
</p>
<p>In particular, (B.8) generalizes to
</p>
<p>d f =
</p>
<p>(
dx
</p>
<p>&part;
</p>
<p>&part;x
+dy
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>)
f =
</p>
<p>&part; f
</p>
<p>&part;x
dx+
</p>
<p>&part; f
</p>
<p>&part;y
dy , (B.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>400 B Useful Formulae
</p>
<p>while
</p>
<p>d2 f =
</p>
<p>(
dx
</p>
<p>&part;
</p>
<p>&part;x
+dy
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>)2
f =
</p>
<p>&part; 2 f
</p>
<p>&part;x2
(dx)2 +2
</p>
<p>&part; 2 f
</p>
<p>&part;x&part;y
dxdy+
</p>
<p>&part; 2 f
</p>
<p>&part;y2
(dy)2 . (B.17)
</p>
<p>The condition that f (x,y) takes an extremum value at (x,y) = (x0,y0) is a simple
generalization of (B.14) and is given by
</p>
<p>&part; f
</p>
<p>&part;x
</p>
<p>∣∣∣∣
(x,y)=(x0,y0)
</p>
<p>= 0 and
&part; f
</p>
<p>&part;y
</p>
<p>∣∣∣∣
(x,y)=(x0,y0)
</p>
<p>= 0 , (B.18)
</p>
<p>from which we can determine x0 and y0.
</p>
<p>B.2 Exponential
</p>
<p>By definition,
</p>
<p>e = lim
n&rarr;&infin;
</p>
<p>(
1+
</p>
<p>1
</p>
<p>n
</p>
<p>)n
. (B.19)
</p>
<p>This relation holds with n replaced by a real number x. In fact,
</p>
<p>lim
x&rarr;&infin;
</p>
<p>(
1&plusmn; 1
</p>
<p>x
</p>
<p>)x
= e&plusmn;1 . (B.20)
</p>
<p>The simplest way to verify this identity perhaps is to use the Taylor series expansion
</p>
<p>of ln(1+ x):
</p>
<p>ln(1+ x) = x&minus; 1
2
</p>
<p>x2 +
1
</p>
<p>3
x3 + &middot; &middot; &middot; . (B.21)
</p>
<p>Now, let
</p>
<p>f (x) :=
</p>
<p>(
1&plusmn; 1
</p>
<p>x
</p>
<p>)x
. (B.22)
</p>
<p>Using (B.21), we find
</p>
<p>ln f (x) = x ln
</p>
<p>(
1&plusmn; 1
</p>
<p>x
</p>
<p>)
&asymp; x
</p>
<p>(
&plusmn;1
</p>
<p>x
&minus; 1
</p>
<p>2x2
&plusmn; 1
</p>
<p>3x3
&minus;&middot;&middot; &middot;
</p>
<p>)
. (B.23)
</p>
<p>Since this expression approaches &plusmn;1 in the x &rarr;&infin; limit, we have established (B.20).
It follows that
</p>
<p>lim
x&rarr;&infin;
</p>
<p>(
1&plusmn; a
</p>
<p>x
</p>
<p>)x
= lim
</p>
<p>y&rarr;&infin;
</p>
<p>[(
1&plusmn; 1
</p>
<p>y
</p>
<p>)y]a
= e&plusmn;a , (B.24)
</p>
<p>where y := x/a.</p>
<p/>
</div>
<div class="page"><p/>
<p>B.4 Binomial Expansion 401
</p>
<p>B.3 Summation of a Geometric Series
</p>
<p>The following result may be familiar to you:
</p>
<p>&infin;
</p>
<p>&sum;
i=1
</p>
<p>ri =
1
</p>
<p>1&minus; r if |r|&lt; 1 . (B.25)
</p>
<p>To see why this is so, let
</p>
<p>Sn := 1+ r+ r
2 + &middot; &middot; &middot;+ rn . (B.26)
</p>
<p>Then,
</p>
<p>rSn = r+ r
2 + &middot; &middot; &middot;+ rn + rn+1 . (B.27)
</p>
<p>Subtracting (B.27) from (B.26), we find
</p>
<p>(1&minus; r)Sn = 1&minus; rn+1 , (B.28)
</p>
<p>and hence
</p>
<p>Sn =
1&minus; rn+1
</p>
<p>1&minus; r . (B.29)
</p>
<p>If |r|&lt; 1, limn&rarr;&infin; rn = 0. This gives (B.25).
</p>
<p>B.4 Binomial Expansion
</p>
<p>The following identity holds:
</p>
<p>(a+b)M =
M
</p>
<p>&sum;
N=0
</p>
<p>(
M
</p>
<p>N
</p>
<p>)
aNbM&minus;N , (B.30)
</p>
<p>in which (
M
</p>
<p>N
</p>
<p>)
:=
</p>
<p>M!
</p>
<p>N!(M&minus;N)! (B.31)
</p>
<p>is the binomial coefficient. To see this, let us imagine computing (a+b)M by writ-
ing out M factors of (a+b):
</p>
<p>(a+b)&times; (a+b)&times;&middot;&middot; &middot;&times; (a+b) . (B.32)
</p>
<p>We choose from each pair of brackets, either a or b, and form their product. Each
</p>
<p>distinct set of choices we make for the brackets yields a product that will show up
</p>
<p>in the expansion of (a+b)M . There are 2M distinct set of choices we can make, and
we get 2M products. But, not all of them have different values.
</p>
<p>For example, if we always pick a from each pair of brackets, then, we get aM .
</p>
<p>There is only one way to do this. So the coefficient of aM is just one. If we choose</p>
<p/>
</div>
<div class="page"><p/>
<p>402 B Useful Formulae
</p>
<p>b only once and a for the remaining M&minus;1 times, then, their product is aM&minus;1b. But,
since there are M distinct ways of choosing the pair of brackets from which to pick
</p>
<p>b, the coefficient of aM&minus;1b is M, and so on. The coefficient
(
</p>
<p>M
N
</p>
<p>)
of aNbM&minus;N is the
</p>
<p>number of distinct ways of choosing N pairs of brackets from which we pick a when
</p>
<p>there are M pairs of brackets in total.
</p>
<p>B.5 Gibbs&ndash;Bogoliubov Inequality
</p>
<p>Let f (x) and g(x) be integrable positive functions satisfying
</p>
<p>&int; b
</p>
<p>a
f (x)dx =
</p>
<p>&int; b
</p>
<p>a
g(x)dx . (B.33)
</p>
<p>Then, &int; b
</p>
<p>a
f (x) ln f (x)dx &ge;
</p>
<p>&int; b
</p>
<p>a
f (x) lng(x)dx , (B.34)
</p>
<p>where the equality holds only when f (x)&equiv; g(x). This result is known as the Gibbs&ndash;
Bogoliubov inequality.
</p>
<p>To prove the inequality, we first compute
</p>
<p>&int; b
</p>
<p>a
f (x) ln f (x)dx&minus;
</p>
<p>&int; b
</p>
<p>a
f (x) lng(x)dx
</p>
<p>=
&int; b
</p>
<p>a
f (x) ln
</p>
<p>f (x)
</p>
<p>g(x)
dx =
</p>
<p>&int; b
</p>
<p>a
g(x)
</p>
<p>[
f (x)
</p>
<p>g(x)
ln
</p>
<p>f (x)
</p>
<p>g(x)
&minus; f (x)
</p>
<p>g(x)
+1
</p>
<p>]
dx , (B.35)
</p>
<p>where the last equality follows from (B.33). Now, we note that
</p>
<p>h(x) := x lnx+1&minus; x &ge; 0 (B.36)
</p>
<p>for all x &gt; 0 with the equality holding only at x = 1. The validity of this inequality
can be easily established graphically. In fact,
</p>
<p>dh
</p>
<p>dx
= lnx , (B.37)
</p>
<p>which is negative if 0&lt; x&lt; 1, zero at x= 1, and positive if x&gt; 1. Thus, the minimum
value of h(x) occurs at x = 1 and it is zero. Replacing x by f (x)/g(x), we see from
(B.35) that &int; b
</p>
<p>a
f (x) ln f (x)dx&minus;
</p>
<p>&int; b
</p>
<p>a
f (x) lng(x)dx &ge; 0 , (B.38)
</p>
<p>where the equality holds if and only if f (x)/g(x)&equiv; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C
</p>
<p>Legendre Transformation
</p>
<p>This appendix presents a basic idea of Legendre transformation used in Chaps. 1
</p>
<p>and 2.
</p>
<p>C.1 Legendre Transformation
</p>
<p>The problem we wish to answer is the following: Given a function
</p>
<p>y = y(x) , (C.1)
</p>
<p>how do we construct a function
</p>
<p>w = w(p) (C.2)
</p>
<p>containing just as much information as (C.1), where
</p>
<p>p :=
dy
</p>
<p>dx
? (C.3)
</p>
<p>By saying &ldquo;(C.1) and (C.2) contain the same information,&rdquo; we mean that, given
</p>
<p>(C.1) alone, we can find (C.2) and also that we can find (C.1) using (C.2) alone.
</p>
<p>Let us first take a very simple-minded approach. As we shall see shortly, this
</p>
<p>leads to a failure. However, an approach very similar to this one will work. So, our
</p>
<p>effort will not be wasted.
</p>
<p>a. Given (C.1), take the derivative, which will give us p as a function of x:
</p>
<p>p =
dy
</p>
<p>dx
= p(x) . (C.4)
</p>
<p>b. Invert this equation to express x as a function of p. Note that this is always pos-
</p>
<p>sible provided that p = dy/dx is a monotonic function of x, that is, d2y/dx2 does
not change its sign. See Fig. C.1 to understand this point.
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 403
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1</p>
<p/>
</div>
<div class="page"><p/>
<p>404 C Legendre Transformation
</p>
<p>p
</p>
<p>x
</p>
<p>p
</p>
<p>x
</p>
<p>a b
</p>
<p>Fig. C.1 The function p = p(x) can be inverted to give x = x(p) if p = p(x) is monotonic as
indicated in a. If this is not the case, there may be multiple values of x for a given value of p as in
</p>
<p>b and the function p = p(x) cannot be inverted.
</p>
<p>c. Substitute this function x = x(p) into (C.1). The end result is y expressed as a
function of p. Identify the function so obtained as w in (C.2).
</p>
<p>As an example of this approach, let us take y0 = x
2, for which p = 2x, and hence
</p>
<p>x = p/2. So, w0 = p
2/4. This approach, however, has a serious flaw. To see this, let
</p>
<p>us take another function ya = (x&minus;a)2, where a is a constant. If you go through the
same procedure, you will find that wa = p
</p>
<p>2/4. So, two different functions y0 and ya
both give you the same function for w. Thus, the proposed transformation has no
</p>
<p>unique inverse.
</p>
<p>C.1.1 Representation of a Curve
</p>
<p>Let us step back a little and take a little more graphical look at the problem.
</p>
<p>By writing down an explicit expression for a function y = y(x), such as y = x2
</p>
<p>above, we are actually specifying, in a very compact way, a set
</p>
<p>{(xi,yi)|yi = y(xi)} , (C.5)
</p>
<p>that is, the set of all pairs of numbers (xi,yi) for which yi = y(xi) holds. Now, you
are certainly familiar with a practice of showing a pair of numbers, say (x1,y1) as a
point on a piece of paper. If this is done for all the members of the set, the resulting
</p>
<p>collection of points is a curve representing the function y = y(x).
Now, there is an interesting way to draw a curve. You might have tried this back in
</p>
<p>your elementary school days, perhaps out of boredom during a class. Draw a number
</p>
<p>of lines as shown in Fig. C.2. What do you see? Do you see a curve emerging from
</p>
<p>the set of lines? These lines are a set of tangent lines to that curve. So, a curve can</p>
<p/>
</div>
<div class="page"><p/>
<p>C.1 Legendre Transformation 405
</p>
<p>x
</p>
<p>y
</p>
<p>Fig. C.2 A curve emerges from a set of lines.
</p>
<p>be specified by a set of tangent lines. But, each line is determined by its slope p and
</p>
<p>the intercept w on the y-axis. Finally, specifying a set of pairs of numbers (p,w) is
equivalent to specifying a function w = w(p). (Provided, of course, that for a given
value of p, there is a unique value of w, that is, if (p1,w1) and (p1,w2) are both in
the set, then w1 = w2. This demands that dp/dx = d
</p>
<p>2y/dx2 be of a definite sign.)
Given a pair of numbers (xi,yi) on a curve y = y(x), it is a simple matter to find
</p>
<p>the corresponding pair (pi,wi) for the tangent line of y = y(x) at x = xi:
</p>
<p>pi =
dy
</p>
<p>dx
</p>
<p>∣∣∣∣
x=xi
</p>
<p>and wi = y(xi)&minus; pixi . (C.6)
</p>
<p>In this way, a point (xi,yi) on the curve on the xy-plane is mapped to a point (pi,wi)
on the pw-plane. By repeating this process for each point in the set (C.5), we end up
</p>
<p>with a function w = w(p), which is then an alternative representation of the original
function y = y(x), but now the independent variable is p instead of x.
</p>
<p>C.1.2 Legendre Transformation
</p>
<p>The above consideration leads to the following procedure called the Legendre
</p>
<p>transformation:
</p>
<p>a. Given (C.1), compute p as
</p>
<p>p =
dy
</p>
<p>dx
, (C.7)
</p>
<p>which is a function of x.</p>
<p/>
</div>
<div class="page"><p/>
<p>406 C Legendre Transformation
</p>
<p>b. Solve (C.7) to express x in terms of p.
</p>
<p>x = x(p) , (C.8)
</p>
<p>c. Substitute (C.8) into the relation
</p>
<p>w = y(x)&minus; px , (C.9)
</p>
<p>which is now expressed in terms of p. The resulting function w = w(p) is
called the Legendre transform of y.
</p>
<p>Going back to the earlier example, let y0 = x
2, for which p = 2x and hence x =
</p>
<p>p/2. Then, w0 = y0 &minus; px = &minus;p2/4. On the other hand, if ya = (x&minus; a)2, then p =
2(x&minus;a) and x = p/2+a, from which you find wa = ya &minus; px =&minus;p2/4&minus;ap. Unlike
the earlier approach, w0 	= wa.
</p>
<p>At this point, you might wonder why the earlier approach failed. There, we
</p>
<p>expressed y as a function of p and called the resulting function w. This latter func-
</p>
<p>tion can therefore be written as y = w(dy/dx). But, this is a differential equation
for y, and hence cannot determine y uniquely without specifying a boundary condi-
</p>
<p>tion, that is, a pair of numbers such as (x1,y1) through which the curve must pass.
But, what we wanted was a new equation that is equivalent to the original one. In
</p>
<p>constructing this new function, we do not wish to carry around the extra piece of
</p>
<p>information that y1 = y(x1), which pertains to the old one.
</p>
<p>C.1.3 Inverse Legendre Transformation
</p>
<p>The remaining question now is whether we can start from the function w0 (or wa)
and recover the original function y0 (or ya). If we can, then y and w are both satis-
</p>
<p>factory representation of the same information. Now, given w = w(p), how do we
reconstruct y = y(x)? From (C.9), we have
</p>
<p>y = w(p)+ px . (C.10)
</p>
<p>In order to obtain y(x), all we need to do is to express p as a function of x. Again
from (C.9),
</p>
<p>dw = dy&minus; pdx&minus; xdp . (C.11)
But, since p = dy/dx, we have dy = pdx. So, we conclude that
</p>
<p>dw =&minus;xdp , (C.12)
</p>
<p>and hence
</p>
<p>x =&minus;dw
dp
</p>
<p>. (C.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>C.1 Legendre Transformation 407
</p>
<p>Thus, we can proceed as follows:
</p>
<p>a. Given w = w(p), compute the negative of its derivative. This gives x as a
function of p.
</p>
<p>b. Solve this equation to express p as a function of x.
</p>
<p>c. Substitute the result into (C.10) to obtain y as a function of x.
</p>
<p>From (C.10) and (C.13), we see that the procedure is just the Legendre trans-
</p>
<p>formation of w = w(p).
</p>
<p>Let us see how this goes for w0 = &minus;p2/4, for which x = &minus;dw/dp = p/2. So
p = 2x, and hence y0 = w0 + px =&minus;(2x)2/4+2x&times; x = x2.
</p>
<p>Exercise C.1. Repeat the same process for wa =&minus;p2/4&minus;ap to reconstruct ya. ///
</p>
<p>Exercise C.2. Given a function
</p>
<p>y = x lnx , (C.14)
</p>
<p>a. Perform its Legendre transformation to obtain the function w = w(p).
b. Perform the inverse Legendre transformation to recover the function y = y(x). ///
</p>
<p>Exercise C.3. Given a function
</p>
<p>z = x2ey , (C.15)
</p>
<p>a. Perform its Legendre transformation to obtain the function w = w(x, p), where
p := &part; z/&part;y.
</p>
<p>b. Perform the inverse Legendre transformation to recover the function z = z(x,y).
</p>
<p>In this problem, treat x as if it is a constant. ///</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D
</p>
<p>Dirac δ -Function
</p>
<p>Dirac originally defined the δ -function that bears his name through the following
properties[2]:
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
δ (x)dx = 1 and δ (x) = 0 for x 	= 0 . (D.1)
</p>
<p>No ordinary function we know from our calculus courses satisfies these properties.
</p>
<p>Nevertheless, the δ -function permeates our mathematical description of the physical
world. We have also made use of this function in Sect. 3.15 and, more extensively,
</p>
<p>in Chap. 4. This appendix summarizes important facts about the Dirac δ -function.
</p>
<p>D.1 Definition of δ (x)
</p>
<p>Let θ(x) denote a step function:
</p>
<p>θ(x) =
</p>
<p>{
1 if x &ge; 0
0 if x &lt; 0 .
</p>
<p>(D.2)
</p>
<p>Because θ(x) is discontinuous at x = 0, it is not differentiable there. If we denote its
derivative by δ (x):
</p>
<p>δ (x) :=
dθ(x)
</p>
<p>dx
, (D.3)
</p>
<p>it is not well-defined at x = 0. Nevertheless, it is possible to assign a meaning to a
definite integral &int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x)dx , (D.4)
</p>
<p>which contains the dangerous point x = 0.
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 409
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1</p>
<p/>
</div>
<div class="page"><p/>
<p>410 D Dirac δ -Function
</p>
<p>0 0
</p>
<p>1 1
</p>
<p>gn x x
</p>
<p>x x1 n
</p>
<p>....... ...... .......
.........
...........
.............
...............
.................
.
................
...............
.............
...........
.........
........ ....... .......
</p>
<p>Fig. D.1 Behavior of gn(x) and θ(x) := limn&rarr;&infin; gn(x).
</p>
<p>Let gn(x) (n = 1,2, . . .) denote a sequence of sufficiently smooth functions that
approaches θ(x) as n tends toward infinity:
</p>
<p>lim
n&rarr;&infin;
</p>
<p>gn(x) = θ(x) . (D.5)
</p>
<p>Since gn(x) is sufficiently smooth, the integral
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>dgn(x)
</p>
<p>dx
dx (D.6)
</p>
<p>exists for all n provided that f (x) behaves nicely enough. If the limit
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>dgn(x)
</p>
<p>dx
dx (D.7)
</p>
<p>exists also, then, we adopt
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x)dx or
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>dθ(x)
</p>
<p>dx
dx (D.8)
</p>
<p>as a compact notation for that limit. Equation (D.7) should be contrasted against the
</p>
<p>mathematically ill-defined expression
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>d
</p>
<p>dx
</p>
<p>[
lim
n&rarr;&infin;
</p>
<p>gn(x)
]
</p>
<p>dx , (D.9)
</p>
<p>which is suggested by a more literal interpretation of (D.4).
</p>
<p>As an example of gn(x), let us consider the function depicted in Fig. D.1. More
precisely, we set
</p>
<p>gn(x) =
</p>
<p>{
1 if x &ge; 0
0 if x &le;&minus;1/n , (D.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>D.1 Definition of δ (x) 411
</p>
<p>0 0
</p>
<p>1
</p>
<p>gn x dgn x dx
</p>
<p>x x1 n 1 n
</p>
<p>....... ...... .......
.........
...........
.............
...............
.................
.
................
...............
.............
...........
.........
........ ....... .......
</p>
<p>.................
.........
............
..............
................
</p>
<p>..................
</p>
<p>.
.....................
</p>
<p>...................
</p>
<p>.................
</p>
<p>...............
.............
...........
.........
..............................................
</p>
<p>.............
.............
..
</p>
<p>............
</p>
<p>.....
</p>
<p>............
</p>
<p>.......
</p>
<p>............
</p>
<p>.........
</p>
<p>.
</p>
<p>............
</p>
<p>......
</p>
<p>............
</p>
<p>....
.............
.
.....................................
</p>
<p>Fig. D.2 Behavior of gn(x) and dgn(x)/dx.
</p>
<p>in which n is a positive constant. In between x = &minus;1/n and x = 0, we demand that
gn(x) is sufficiently smooth and monotonic. For convenience, we also suppose that
the inflection point, at which d2gn(x)/dx
</p>
<p>2 = 0, occurs only once at x =&minus;1/2n. For
such gn(x), the derivative dgn(x)/dx is nonzero only for &minus;1/n &lt; x &lt; 0 and it takes
the maximum value only once at x =&minus;1/2n. This is shown in Fig. D.2. As a result,
we can rewrite the integral in (D.6) as
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>dgn(x)
</p>
<p>dx
dx =
</p>
<p>&int; 0
</p>
<p>&minus;1/n
f (x)
</p>
<p>dgn(x)
</p>
<p>dx
dx . (D.11)
</p>
<p>Integrating by parts, we obtain
</p>
<p>&int; 0
</p>
<p>&minus;1/n
f (x)
</p>
<p>dgn(x)
</p>
<p>dx
dx = [ f (x)gn(x)]
</p>
<p>0
&minus;1/n &minus;
</p>
<p>&int; 0
</p>
<p>&minus;1/n
</p>
<p>d f (x)
</p>
<p>dx
gn(x)dx
</p>
<p>= f (0)&minus;
&int; 0
</p>
<p>&minus;1/n
</p>
<p>d f (x)
</p>
<p>dx
gn(x)dx . (D.12)
</p>
<p>But, the last term of this equation becomes vanishingly small with increasing n. So,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>dgn(x)
</p>
<p>dx
dx = f (0) . (D.13)
</p>
<p>In terms of the compact notation we introduced, this may be written as
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x)dx = f (0) , (D.14)
</p>
<p>which is the desired result.
</p>
<p>Exercise D.1. Show that
&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x&minus;a)dx = f (a) . (D.15)
</p>
<p>///</p>
<p/>
</div>
<div class="page"><p/>
<p>412 D Dirac δ -Function
</p>
<p>It is certainly nice not having to write limn&rarr;&infin; all the time. But, does our notation
make good sense at all? By concealing the limiting process, which we used to get
</p>
<p>rid of the last term in (D.12), do we not run a risk of making some serious mistakes?
</p>
<p>So, let us see what happens if we ignore the fact that δ (x) = dθ(x)/dx does not exist
at x = 0 and proceed with (D.8). Since θ(x) = 0 for x &lt; 0,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x)dx =
</p>
<p>&int; +&infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>dθ(x)
</p>
<p>dx
dx = [ f (x)θ(x)]+&infin;&minus;&infin;&minus;
</p>
<p>&int; +&infin;
</p>
<p>&minus;&infin;
</p>
<p>d f (x)
</p>
<p>dx
θ(x)dx
</p>
<p>= f (+&infin;)&minus;
&int; +&infin;
</p>
<p>0
</p>
<p>d f (x)
</p>
<p>dx
dx = f (+&infin;)&minus; f (+&infin;)+ f (0) = f (0)
</p>
<p>(D.16)
</p>
<p>in agreement with (D.14). This demonstrates that our compact notation, in which
</p>
<p>(D.7) is written as (D.8), is perfectly acceptable.
</p>
<p>It is instructive to seek for an alternative and more intuitive justification of (D.14).
</p>
<p>To begin with, we see immediately that
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>dgn(x)
</p>
<p>dx
dx = [gn(x)]
</p>
<p>&infin;
&minus;&infin; = 1&minus;0 = 1 . (D.17)
</p>
<p>This relation holds for any n. Thus, going to the large n limit, we find
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
δ (x)dx = 1 . (D.18)
</p>
<p>According to (D.18), &int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x)dx (D.19)
</p>
<p>may be considered as a weighted average of f (x) with δ (x) playing the role of the
weighting function. If we regard δ (x) as dgn(x)/dx in the limit of n &rarr;&infin;, then, δ (x)
is seen to be extremely sharply peaked near x = 0. If f (x) is continuous and varies
slowly in this critical region, then, f (x) under the integral sign may be replaced by
f (0) and taken outside:
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x)dx = f (0)
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
δ (x)dx = f (0) , (D.20)
</p>
<p>where the last equality follows from (D.18).
</p>
<p>To summarize, the Dirac δ -function arises whenever we differentiate a discontin-
uous function such as θ(x). Even though a function is not differentiable at the point
of discontinuity, we can ignore this inconvenient fact and carry out the computation
</p>
<p>consistently provided that δ (x) occurs only inside the integral.
In essence, we pretend that any discontinuity we encounter in our theoretical
</p>
<p>description of the physical world, such as θ(x), is an idealization of what is really a
continuous transition as described by gn(x). The alternative approach of not differ-
entiating any function at its point of discontinuity is too restrictive for many of our
</p>
<p>purposes.</p>
<p/>
</div>
<div class="page"><p/>
<p>D.2 Basic Properties of the δ -Function 413
</p>
<p>D.2 Basic Properties of the δ -Function
</p>
<p>The δ -function satisfies a number of useful identities. Here, we list several famous
examples:
</p>
<p>a.
</p>
<p>δ (&minus;x) = δ (x) . (D.21)
b.
</p>
<p>xδ (x) = 0 . (D.22)
</p>
<p>c.
</p>
<p>δ (&plusmn;ax) = 1
a
δ (x) (a &gt; 0) , (D.23)
</p>
<p>which is often expressed as
</p>
<p>δ (ax) =
1
</p>
<p>|a|δ (x) (a 	= 0) . (D.24)
</p>
<p>d.
</p>
<p>δ (x2 &minus;a2) = 1
2|a| [δ (x&minus;a)+δ (x+a)] . (D.25)
</p>
<p>e. &int; &infin;
</p>
<p>&minus;&infin;
δ (a&minus; x)δ (b&minus; x)dx = δ (a&minus;b) . (D.26)
</p>
<p>f.
</p>
<p>f (x)δ (x&minus;a) = f (a)δ (x&minus;a) . (D.27)
g.
</p>
<p>δ (g(x)) =&sum;
i
</p>
<p>δ (x&minus;ai)
|g&prime;(ai)|
</p>
<p>, (D.28)
</p>
<p>where ai is the ith root of g(x). It is assumed that there are no coincident roots,
that is, g&prime;(ai) 	= 0.
Equation (D.21) might appear surprising in view of the graph for dgn(x)/dx
</p>
<p>shown in Fig. D.2. We must remember, however, that δ (x) is defined only in terms
of an integration in which δ (x) occurs as a factor in the integrand. Thus, (D.21)
should really be interpreted as
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x)dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (&minus;x)dx . (D.29)
</p>
<p>The same remark applies to other identities.
</p>
<p>Exercise D.2. Prove (D.21) &ndash; (D.28). ///
</p>
<p>Despite the wild variation of δ (x) at x = 0, it is still meaningful to talk about its
derivative δ &prime;(x) provided that it occurs only inside an integral. As with δ (x), the key</p>
<p/>
</div>
<div class="page"><p/>
<p>414 D Dirac δ -Function
</p>
<p>is in the proper definition of such integrals. By definition then,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ &prime;(x)dx := lim
</p>
<p>n&rarr;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>d2gn(x)
</p>
<p>dx2
dx . (D.30)
</p>
<p>By applying integration by parts, either to the expression on the left or to that on the
</p>
<p>right, it is straightforward to show that
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ &prime;(x)dx =&minus; f &prime;(0) , (D.31)
</p>
<p>where f &prime;(x) = d f (x)/dx.
</p>
<p>Exercise D.3. Prove (D.31). ///
</p>
<p>Exercise D.4. Prove the following identities
</p>
<p>a.
</p>
<p>xδ &prime;(x) =&minus;δ (x) . (D.32)
b.
</p>
<p>δ &prime;(x&minus; y) =&minus;δ &prime;(y&minus; x) . (D.33)
The second identity implies that δ &prime;(x) is an odd function. This is what we expect
since δ (x) is an even function as indicated in (D.21). (Recall cosθ and sinθ .) ///
</p>
<p>Higher derivatives of the δ -function can be defined analogously. Thus,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ &prime;&prime;(x)dx := lim
</p>
<p>n&rarr;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>d3gn(x)
</p>
<p>dx3
dx ,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ &prime;&prime;&prime;(x)dx := lim
</p>
<p>n&rarr;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>d4gn(x)
</p>
<p>dx4
dx , (D.34)
</p>
<p>and so on. Combined with (B.6), this leads to the following Taylor series expansion
</p>
<p>of the δ -function:
</p>
<p>δ (x+a) = δ (x)+δ &prime;(x)a+
1
</p>
<p>2!
δ &prime;&prime;(x)a2 +
</p>
<p>1
</p>
<p>3!
δ &prime;&prime;&prime;(x)a3 + &middot; &middot; &middot; . (D.35)
</p>
<p>At first sight, the formula seems incorrect because δ (x+a) = 0 for all x 	= &minus;a and
the right-hand side is zero except at x = 0. However, identities involving the δ -
function and its derivatives must be understood under the integral sign. The limits
</p>
<p>of integration is to some extent arbitrary, but it must include all the critical points
</p>
<p>of the integrand, which are x =&minus;a and x = 0 in this case. On the left-hand side, we
have &int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x+a)dx = f (&minus;a) . (D.36)</p>
<p/>
</div>
<div class="page"><p/>
<p>D.3 Weak Versus Strong Definitions of the δ -Function 415
</p>
<p>On the right, we have several terms:
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ (x)dx = f (0) ,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ &prime;(x)dx =&minus; f &prime;(0) ,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ &prime;&prime;(x)dx = f &prime;&prime;(0) ,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)δ &prime;&prime;&prime;(x)dx =&minus; f &prime;&prime;&prime;(0) , . . . (D.37)
</p>
<p>where the last two equalities can be established by integration by parts. Thus,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)[R.H.S.]dx = f (0)&minus;a f &prime;(0)+ 1
</p>
<p>2!
a2 f &prime;&prime;(0)&minus; 1
</p>
<p>3!
a3 f &prime;&prime;&prime;(0)+ &middot; &middot; &middot; . (D.38)
</p>
<p>But, this is just the Taylor series expansion of f (&minus;a). In other words,
&int; &infin;
</p>
<p>&minus;&infin;
f (x)[R.H.S.]dx = f (&minus;a) (D.39)
</p>
<p>Comparing (D.36) and (D.39), we see that (D.35) is valid.
</p>
<p>D.3 Weak Versus Strong Definitions of the δ -Function
</p>
<p>Dirac himself defined the δ -function by means of (D.1). The δ -function we con-
structed by means of gn(x) is consistent with this definition.
</p>
<p>50 However, there are
</p>
<p>other ways of constructing a function that satisfies these defining properties.
</p>
<p>As an illustration, let us consider the following step function:
</p>
<p>η(x) =
</p>
<p>⎧
⎨
⎩
</p>
<p>1 if x &gt; 0
1
2
</p>
<p>if x = 0
0 if x &lt; 0 ,
</p>
<p>(D.40)
</p>
<p>which may be regarded as the large n limit of a smooth and monotonically increasing
</p>
<p>function hn(x) satisfying
</p>
<p>hn(x) =
</p>
<p>⎧
⎨
⎩
</p>
<p>1 if x &gt; 1/2n
1
2
</p>
<p>if x = 0
0 if x &lt;&minus;1/2n .
</p>
<p>(D.41)
</p>
<p>Because &int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>dhn(x)
</p>
<p>dx
dx = 1 , (D.42)
</p>
<p>we see that δ (x) := dη/dx satisfies (D.18). It is also clear that this δ (x) is zero
except at x = 0. So, this δ (x) is consistent with (D.1). Moreover, an analysis similar
to what followed (D.11) indicates that
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)
</p>
<p>dhn(x)
</p>
<p>dx
dx = f (0) . (D.43)</p>
<p/>
</div>
<div class="page"><p/>
<p>416 D Dirac δ -Function
</p>
<p>We emphasize that the two versions of δ -function we considered are nonetheless
not entirely identical. To see this, we note that
</p>
<p>&int; 0
</p>
<p>&minus;&infin;
</p>
<p>dgn(x)
</p>
<p>dx
dx = 1 and
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>dgn(x)
</p>
<p>dx
dx = 0 , (D.44)
</p>
<p>and taking the limit n &rarr; &infin;,
&int; 0
</p>
<p>&minus;&infin;
δ (x)dx = 1 and
</p>
<p>&int; &infin;
</p>
<p>0
δ (x)dx = 0 . (D.45)
</p>
<p>In contrast, for hn(x), we have
</p>
<p>&int; 0
</p>
<p>&minus;&infin;
</p>
<p>dhn(x)
</p>
<p>dx
dx =
</p>
<p>1
</p>
<p>2
and
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>dhn(x)
</p>
<p>dx
dx =
</p>
<p>1
</p>
<p>2
, (D.46)
</p>
<p>and hence &int; 0
</p>
<p>&minus;&infin;
δ (x)dx =
</p>
<p>1
</p>
<p>2
and
</p>
<p>&int; &infin;
</p>
<p>0
δ (x)dx =
</p>
<p>1
</p>
<p>2
. (D.47)
</p>
<p>Neither (D.45) nor (D.47) is a part of the defining properties of δ (x). The orig-
inal definition due to Dirac is called the weak definition of δ (x). When it is sup-
plemented by (D.45), (D.47), or any other such relations, the definition is said to be
</p>
<p>strong. For our particular purposes, we find it more convenient to work with θ(x)
as the step function rather than η(x). Consequently, our δ -function satisfies (D.45)
but not (D.47).
</p>
<p>D.4 Three-Dimensional δ -Function
</p>
<p>Let r be a position vector and (x,y,z) denote its components in a Cartesian coordi-
nate system. Then, δ (r) is a short-hand notation expressing the following product
of three one-dimensional δ -functions.
</p>
<p>δ (r) := δ (x)δ (y)δ (z) . (D.48)
</p>
<p>Evidently, &int;
</p>
<p>V
δ (r)dr=
</p>
<p>&int; &int; &int;
δ (x)δ (y)δ (z)dxdydz (D.49)
</p>
<p>is unity if the origin (r = 0) is inside V and zero if it is outside V . If the origin is
on the boundary of V , the value of this integral depends on the strong definition
</p>
<p>being adopted for the one-dimensional δ -function. Insofar as (D.49) is dimension-
less, δ (r) has the dimension of the reciprocal volume.</p>
<p/>
</div>
<div class="page"><p/>
<p>References and Further Reading 417
</p>
<p>D.5 &dagger;Representation of the δ -Function
</p>
<p>Let us consider a function given by
</p>
<p>hn(x) =
</p>
<p>&radic;
n
</p>
<p>π
e&minus;nx
</p>
<p>2
. (D.50)
</p>
<p>Using (3.91), we see that
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
hn(x)dx = 1 and
</p>
<p>&int; 0
</p>
<p>&minus;&infin;
hn(x)dx =
</p>
<p>&int; &infin;
</p>
<p>0
hn(x)dx =
</p>
<p>1
</p>
<p>2
(D.51)
</p>
<p>for any n. Moreover,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>hn(x) = 0 if x 	= 0 . (D.52)
</p>
<p>Thus, the sequence of functions hn(x) defines the Dirac δ -function that is compatible
with (D.47).
</p>
<p>Another frequently encountered representation of the δ -function is given by
</p>
<p>δ (x) =
1
</p>
<p>2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
eikxdk , (D.53)
</p>
<p>where i :=
&radic;
&minus;1 is the imaginary unit. To see this, let
</p>
<p>fn(x) :=
1
</p>
<p>2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;
</p>
<p>k2
</p>
<p>4n +ikxdk . (D.54)
</p>
<p>In the limit of n &rarr; &infin;, the right-hand side approaches the integral in (D.53). On
the other hand, fn(x) can be evaluated explicitly as follows. We first rewrite the
exponent as
</p>
<p>&minus; k
2
</p>
<p>4n
+ ikx =&minus;1
</p>
<p>4
</p>
<p>(
k&radic;
n
&minus;2i
</p>
<p>&radic;
nx
</p>
<p>)2
&minus;nx2 . (D.55)
</p>
<p>Using a new variable s := (k/
&radic;
</p>
<p>n&minus;2i&radic;nx)/2, we find51
</p>
<p>fn(x) =
</p>
<p>&radic;
n
</p>
<p>π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;s
</p>
<p>2
e&minus;nx
</p>
<p>2
ds =
</p>
<p>&radic;
n
</p>
<p>π
e&minus;nx
</p>
<p>2 &rarr; δ (x) as n &rarr; &infin; . (D.56)
</p>
<p>References and Further Reading
</p>
<p>1. Barton B (1989) Elements of Green&rsquo;s Functions and Propagation: Potentials, Diffusion and
</p>
<p>Waves. Oxford University Press, New York
A thorough, yet highly accessible, account of the δ -function is in Chap. 1.
</p>
<p>2. Dirac P A M (1958) The Principles of Quantum Mechanics, 4th edn. Oxford University Press,
</p>
<p>New York
It is still very instructive to see how Dirac himself explained his δ -function. See Sect. 15 of the
book.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix E
</p>
<p>Where to Go from Here
</p>
<p>Most of the following textbooks will be accessible to you by now, and should be
</p>
<p>consulted according to your interests and needs. I have only included textbooks I
</p>
<p>know reasonably well and feel comfortable recommending to students who have
</p>
<p>mastered the content of this book.
</p>
<p>Statistical Mechanics in General
</p>
<p>Chandler D (1987) Introduction to Modern Statistical Mechanics. Oxford, New
</p>
<p>York
</p>
<p>A very concise yet extremely insightful account of statistical mechanics. Some
</p>
<p>familiarity with quantum mechanics will be useful in parts of the book.
</p>
<p>Hill T L (1986) An Introduction to Statistical Thermodynamics. Dover, New York
</p>
<p>Relatively accessible textbook on statistical mechanics with various applications.
</p>
<p>Hill T L (1986) Statistical Mechanics, Principles and Selected Applications.
</p>
<p>Dover, New York
</p>
<p>More advanced textbook by the same author and publisher as above.
</p>
<p>Computer Simulation
</p>
<p>Allen M P and Tildesley D J (1987) Computer Simulation of Liquids. Oxford,
</p>
<p>New York
</p>
<p>Short overview of statistical mechanics followed by an in-depth exposition of
</p>
<p>the inner working of molecular level simulation. If you plan to do any kind of
</p>
<p>molecular simulation, this will be the best place to start.
</p>
<p>Frenkel D and Smit B (2001) Understanding Molecular Simulation, 2nd edn.
</p>
<p>Academic Press, San Diego
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 419
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1</p>
<p/>
</div>
<div class="page"><p/>
<p>420 E Where to Go from Here
</p>
<p>Discussion on statistical mechanics is very short, but covers various advanced
</p>
<p>algorithm developed in recent years. My recommendation is that you&rsquo;d read this
</p>
<p>after Allen &amp; Tildesley.
</p>
<p>Tuckerman M E (2010) Statistical Mechanics: Theory and Molecular Simulation.
</p>
<p>Oxford University Press, New York
</p>
<p>Geared towards those with physics background. Nevertheless, chapters on molec-
</p>
<p>ular dynamics, free energy calculations, time-dependent phenomena will be
</p>
<p>accessible and may be of interest to you.
</p>
<p>Theory of Liquids, Density Functional Theory
</p>
<p>Goodstein D L (1985) States of Matter. Dover, New York
</p>
<p>Chapter 4 provides a very nice introduction to integral equation theories of simple
</p>
<p>liquids.
</p>
<p>Evans R (1979) The nature of the liquid-vapor interface and other topics in the
</p>
<p>statistical mechanics of non-uniform, classical fluids. Adv. Phys. 28:143&ndash;200
</p>
<p>This isn&rsquo;t a book. But, it is probably the most cited article on the subject of
</p>
<p>statistical mechanical density functional theory.
</p>
<p>Hansen J-P and McDonald I R (1986) Theory of Simple Liquids, 2nd edn. Aca-
</p>
<p>demic Press, San Diego
</p>
<p>Theories of liquids are treated in great depth.
</p>
<p>Mean-Field Theory
</p>
<p>Goldenfeld N (1992) Lectures on Phase Transitions and the Renormalization
</p>
<p>Group. Addison-Wesley, Reading, Massachusetts
</p>
<p>Highly accessible book on phase transition. Though the focus is on critical phe-
</p>
<p>nomena, that is, phase behavior near the critical point, the first several chapters
</p>
<p>provide a systematic treatment of the mean-field theory.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F
</p>
<p>List of Greek Letters
</p>
<p>Greek letters are frequently used in this book, and are listed here for convenience.
</p>
<p>Name Uppercase Lowercase Name Uppercase Lowercase
</p>
<p>alpha A α nu N ν
beta B β xi Ξ ξ
</p>
<p>gamma Γ γ omicron O o
delta ∆ δ pi Π π
</p>
<p>epsilon E ε rho P ρ
zeta Z ζ sigma Σ σ
eta H η tau T τ
</p>
<p>theta Θ θ upsilon ϒ υ
iota I ι phi Φ φ
</p>
<p>kappa K κ chi X χ
lambda Λ λ psi Ψ ψ
</p>
<p>mu M &micro; omega Ω ω
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 421
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix G
</p>
<p>Hints to Selected Exercises
</p>
<p>Chapter 1
</p>
<p>1.1 Since the work required to bring a particle from rA to rA&prime; is independent of the
</p>
<p>path taken, we have
</p>
<p>ψ(rA,rA&prime;) = ψ(rA,r)+ψ(r,rA&prime;) = ψ(rA,r)&minus;ψ(rA&prime; ,r) . (G1.1)
</p>
<p>1.3 Note that
</p>
<p>&sum;
i
</p>
<p>mivi
&prime; =
</p>
<p>d
</p>
<p>dt
&sum;
</p>
<p>i
</p>
<p>miri
&prime; =
</p>
<p>d
</p>
<p>dt
&sum;
</p>
<p>i
</p>
<p>mi(ri &minus;R) = 0 . (G1.2)
</p>
<p>1.5 Eliminating T from (1.85), we find
</p>
<p>m(ẍcosθ + ÿsinθ) =&minus;mgsinθ . (G1.3)
</p>
<p>But, ẍ and ÿ can be computed by taking the time derivative of (1.76).
</p>
<p>1.6
</p>
<p>a. The work required to stretch (or compress) the spring from its natural length l0
to r is given by
</p>
<p>W =&minus;
&int; r
</p>
<p>l0
</p>
<p>F &middot;dr , (G1.4)
</p>
<p>in which dr = drer for both stretching and compression. This work is stored in
the harmonic spring as its potential energy.
</p>
<p>b.
</p>
<p>L =
1
</p>
<p>2
m[ṙ2 +(rθ̇)2]+mgr cosθ &minus; 1
</p>
<p>2
k(r&minus; l0)2 . (G1.5)
</p>
<p>1.7 Start from
</p>
<p>L =
1
</p>
<p>2
m1ṙ1
</p>
<p>2 +
1
</p>
<p>2
m2ṙ2
</p>
<p>2 &minus;φ(r1 &minus; r2) (G1.6)
</p>
<p>and use the definitions for R and r to express r1 and r2 in terms of R and r.
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 423
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1</p>
<p/>
</div>
<div class="page"><p/>
<p>424 G Hints to Selected Exercises
</p>
<p>Derive Lagrange&rsquo;s equations of motion to see why using R and r is more advan-
</p>
<p>tageous than using r1 and r2.
</p>
<p>1.8 Since F is a function of qi&rsquo;s and t only,
</p>
<p>dF
</p>
<p>dt
=
</p>
<p>f
</p>
<p>&sum;
j=1
</p>
<p>&part;F
</p>
<p>&part;q j
q̇ j +
</p>
<p>&part;F
</p>
<p>&part; t
, (G1.7)
</p>
<p>from which we obtain
&part;
</p>
<p>&part; q̇i
</p>
<p>dF
</p>
<p>dt
=
</p>
<p>&part;F
</p>
<p>&part;qi
(G1.8)
</p>
<p>and
</p>
<p>&part;
</p>
<p>&part;qi
</p>
<p>dF
</p>
<p>dt
=
</p>
<p>f
</p>
<p>&sum;
j=1
</p>
<p>&part; 2F
</p>
<p>&part;qi&part;q j
q̇ j +
</p>
<p>&part; 2F
</p>
<p>&part;qi&part; t
. (G1.9)
</p>
<p>1.10 Take the origin to coincide with the source of the field and note that M := r&times;p
is perpendicular to r.
</p>
<p>1.13 From (1.156),
</p>
<p>(
&part;H
</p>
<p>&part; t
</p>
<p>)
</p>
<p>q,p
</p>
<p>=&sum;
i
</p>
<p>pi
</p>
<p>(
&part; q̇i
&part; t
</p>
<p>)
</p>
<p>q,p
</p>
<p>&minus;
(
&part;L
</p>
<p>&part; t
</p>
<p>)
</p>
<p>q,p
</p>
<p>, (G1.10)
</p>
<p>where the subscripts q, p indicate that all of q1, . . . , q f and p1, . . . , p f are held fixed
when evaluating the partial derivatives. Evaluate (&part;L/&part; t)q,p from
</p>
<p>dL =&sum;
i
</p>
<p>[(
&part;L
</p>
<p>&part;qi
</p>
<p>)
</p>
<p>q j 	=i,q̇
dqi +
</p>
<p>(
&part;L
</p>
<p>&part; q̇i
</p>
<p>)
</p>
<p>q,q̇ j 	=i
</p>
<p>dq̇i
</p>
<p>]
+
</p>
<p>(
&part;L
</p>
<p>&part; t
</p>
<p>)
</p>
<p>q,q̇
</p>
<p>dt , (G1.11)
</p>
<p>where the subscripts q j 	=i, q̇ indicates that all of q1, . . . , q f except for qi and all of
q̇1, . . . , q̇ f are held fixed.
</p>
<p>1.18 Since {A,B} is a dynamical variable, (1.185) applies:
</p>
<p>d
</p>
<p>dt
{A,B}= {{A,B},H}+ &part;
</p>
<p>&part; t
{A,B} . (G1.12)
</p>
<p>Chapter 2
</p>
<p>2.1
</p>
<p>c. For an adiabatic reversible expansion, dS = 0.
d. For the process under consideration, both S and N remain constant. From (2.39),
</p>
<p>we see that
</p>
<p>U2
3/2V2 =U1
</p>
<p>3/2V1 , (G2.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>G Hints to Selected Exercises 425
</p>
<p>which may be solved for U2. Alternatively,
</p>
<p>U2 &minus;U1 =W =&minus;
&int; V2
</p>
<p>V1
</p>
<p>PdV , (G2.2)
</p>
<p>in which P may be expressed as a function of V by means of (2.42).
</p>
<p>2.2
</p>
<p>a. Note that S, U , V , and N are all extensive quantities. When U , V , N are doubled,
</p>
<p>S must double as well.
</p>
<p>2.3
</p>
<p>a. For a constant V and N1, . . . , Nc process,
</p>
<p>d̄Q = dU = T dS . (G2.3)
</p>
<p>2.5 For the process under consideration, dS = 0 and hence
</p>
<p>dU =&minus;PdV = CV
NR
</p>
<p>(PdV +VdP) . (G2.4)
</p>
<p>2.7
</p>
<p>a. The fundamental equation of pure i is obtained by setting N j 	=i to zero:
</p>
<p>S = aNi +NiR
</p>
<p>(
3
</p>
<p>2
lnU + lnV &minus; 5
</p>
<p>2
lnNi
</p>
<p>)
. (G2.5)
</p>
<p>After equilibrium is established,
</p>
<p>T af = T
b
f . (G2.6)
</p>
<p>In view of (2.26) and (G2.5), this is an equation for Uaf and U
b
f , the final values
</p>
<p>of the internal energy in compartments A and B, respectively. We need one more
</p>
<p>equation for these two unknowns. But, since the composite system is isolated,
</p>
<p>Uaf +U
b
f =U
</p>
<p>a
0 +U
</p>
<p>b
0 , (G2.7)
</p>
<p>where the subscript 0 refers to the initial state. From (G2.6) and (G2.7), we find
</p>
<p>that the final temperature is 328.6 K.
</p>
<p>b. In addition to (G2.6) and (G2.7), we have
</p>
<p>&micro;a1 f = &micro;
b
1 f , (G2.8)
</p>
<p>and
</p>
<p>Na1 f +N
b
1 f = N1 = 10 (mol). (G2.9)
</p>
<p>These four equations lead to Na1 f = N
b
1 f = 5 mol and 328.6 K for the final tem-
</p>
<p>perature.</p>
<p/>
</div>
<div class="page"><p/>
<p>426 G Hints to Selected Exercises
</p>
<p>2.10
</p>
<p>a. Because the system is isolated, Ni can change only through the chemical reaction.
</p>
<p>From the stoichiometry of the reaction, we have
</p>
<p>δN1 =&minus;
c1
</p>
<p>c3
δN3 and δN2 =&minus;
</p>
<p>c2
</p>
<p>c3
δN3 . (G2.10)
</p>
<p>Use these relations in the expression for δS.
</p>
<p>2.11 Show that
</p>
<p>s f = s1 +
s2 &minus; s1
u2 &minus;u1
</p>
<p>(u0 &minus;u1) (G2.11)
</p>
<p>and interpret this equation graphically. Note also that
</p>
<p>s fV = s1V1 + s2V2 and u0V = u1V1 +u2V2 , (G2.12)
</p>
<p>where Vi denotes the volume of the part having the internal energy density ui.
</p>
<p>2.14 Recall that the molar internal energy U := U/N of an ideal gas is a function
only of T and hence remains constant for an isothermal process. In addition, from
</p>
<p>(2.146), P is constant for processes in which T and &micro; are both constant as in steps a
and c. (This holds true for a single component system in general as we shall see in
</p>
<p>Sect. 2.11.)
</p>
<p>You will also have to choose a convention for the definition of heat. Adopting the
</p>
<p>Convention 2 in Sect. 2.6.5, we have the following results for each step, where ∆U ,
W , and Q are, respectively, the change in the internal energy of the working fluid,
</p>
<p>work done on the working fluid, and the heat received by the fluid:
</p>
<p>a. ∆U =U∆N, W =&minus;RT∆N, and Q = (U +RT )∆N.
b. ∆U = 0, W =&minus;(N +∆N)(&micro;h &minus;&micro;l), and Q = (N +∆N)(&micro;h &minus;&micro;l).
c. ∆U =&minus;U∆N, W = RT∆N, and Q =&minus;(U +RT )∆N.
d. ∆U = 0, W = N(&micro;h &minus;&micro;l), and Q =&minus;N(&micro;h &minus;&micro;l).
2.17
</p>
<p>a. 2 mol.
</p>
<p>b. 3 mol. In part c, you will see that this corresponds to the minimum of G even
</p>
<p>though &micro;a1 	= &micro;b1 .
2.20 At given T and &micro;1, . . . , &micro;c, Ω of a homogeneous system is a linear function
of V .
</p>
<p>2.21
</p>
<p>a. Looking at the variables with respect to which derivatives are computed and the
</p>
<p>list of variables being fixed, we see that T , V , and N1, . . . , Nc are used as the
</p>
<p>independent variables in this problem. The Helmholtz free energy F becomes a
</p>
<p>fundamental equation when expressed in terms of these variables. So, you can
</p>
<p>probably do something with F .</p>
<p/>
</div>
<div class="page"><p/>
<p>G Hints to Selected Exercises 427
</p>
<p>2.22 The number of Maxwell relations we have is
</p>
<p>(c+2)(c+1)
</p>
<p>2
(2c+2 &minus;1) . (G2.13)
</p>
<p>For c = 1, this amounts to 21. For c = 2, the number of Maxwell relations is 90. Of
course, not all of them are of any use. This little computation also tells you that you
</p>
<p>should never attempt to memorize Maxwell relations. Knowing how to derive one
</p>
<p>when needed is more important.
</p>
<p>2.23 Note that dU = T dS&minus;PdV holds for any process in which N1, . . . , Nc are held
fixed. In particular, it holds for a process in which T is also fixed.
</p>
<p>Chapter 3
</p>
<p>3.2
</p>
<p>a. Note that
</p>
<p>&part;C
</p>
<p>&part;β
=
</p>
<p>&int;
&part;
</p>
<p>&part;β
e&minus;βH(q
</p>
<p>f ,p f )dq f dp f =
&int;
(&minus;H)e&minus;βH(q f ,p f )dq f dp f . (G3.1)
</p>
<p>b. Note that
</p>
<p>&lang;
(H &minus;〈H〉)2
</p>
<p>&rang;
=
</p>
<p>&lang;
H2 &minus;2H〈H〉+ 〈H〉2
</p>
<p>&rang;
= 〈H2〉&minus;2〈H〉〈H〉+ 〈H〉2
</p>
<p>= 〈H2〉&minus;〈H〉2 . (G3.2)
</p>
<p>c. In evaluating the partial derivative of C, the limits of integrals and the number of
</p>
<p>mechanical degrees of freedom f are both treated as constant. This implies that
</p>
<p>the system volume and the number of particles in it are both fixed.
</p>
<p>3.4 Evaluate
</p>
<p>[I(a)]2 =
</p>
<p>[&int; &infin;
0
</p>
<p>e&minus;ax
2
dx
</p>
<p>]
&times;
[&int; &infin;
</p>
<p>0
e&minus;ay
</p>
<p>2
dy
</p>
<p>]
(G3.3)
</p>
<p>using polar coordinates. To do this, replace dxdy by rdθdr. Other equations are
obtained by repeatedly differentiating I(a) with respect to a.
</p>
<p>3.5 The probability we seek is the integration of e&minus;βHdrNdpN/C with respect to rN
</p>
<p>and p2, . . . , pN .
</p>
<p>3.8
</p>
<p>a. We need to find the Hamiltonian first. Recall Exercise 1.7, where we consid-
</p>
<p>ered the Lagrangian of two particles interacting through the interparticle potential
</p>
<p>φ(r). Here, r := r1 &minus; r2 is the vector pointing from m2 to m1. Now, φ is usually a
function only of ||r||. In this part of the problem, ||r||= l is constant and φ can be</p>
<p/>
</div>
<div class="page"><p/>
<p>428 G Hints to Selected Exercises
</p>
<p>dropped from the Lagrangian. Thus,
</p>
<p>L =
1
</p>
<p>2
M||Ṙ||2 + 1
</p>
<p>2
&micro; ||ṙ||2 . (G3.4)
</p>
<p>Note that, even though ||r|| is constant, ṙ is not necessarily zero since r can still
change its orientation.
</p>
<p>b. CV = 5kB/2.
c. CV = 7kB/2. Note that
</p>
<p>&int; &infin;
</p>
<p>&minus;l
(x+ l)2e&minus;βkx
</p>
<p>2/2dx &asymp;
&int; &infin;
</p>
<p>&minus;&infin;
(x+ l)2e&minus;βkx
</p>
<p>2/2dx =
&int; &infin;
</p>
<p>&minus;&infin;
(x2 + l2)e&minus;βkx
</p>
<p>2/2dx ,
</p>
<p>(G3.5)
</p>
<p>where x := r&minus; l.
3.9
</p>
<p>b. When considering &part;H/&part;E, recall that
</p>
<p>&part;H
</p>
<p>&part;E
.
=
</p>
<p>(
&part;H
</p>
<p>&part;Ex
,
&part;H
</p>
<p>&part;Ey
,
&part;H
</p>
<p>&part;Ez
</p>
<p>)
(G3.6)
</p>
<p>and that
</p>
<p>me &middot;E= mexEx +meyEy +mezEz . (G3.7)
c. Take appropriate partial derivatives of E2 = Ex
</p>
<p>2 +Ey
2 +Ez
</p>
<p>2.
</p>
<p>3.10 Which step of the derivation breaks down in the absence of the surroundings?
</p>
<p>3.11 Consider a system AB consisting of two subsystems A and B. Assuming that
</p>
<p>the interaction between A and B is sufficiently weak, show that
</p>
<p>&minus;Sab/kB = 〈lnρab〉ab = 〈lnρa〉a + 〈lnρb〉b , (G3.8)
</p>
<p>where, the thermal average 〈X〉ab is defined by
</p>
<p>〈X〉ab :=
&int;
</p>
<p>Xe&minus;βHab dqmdpmdqndpn
&int;
</p>
<p>e&minus;βHabdqmdpmdqndpn
(G3.9)
</p>
<p>for any dynamical variable X , where (qm, pm) denotes the generalized coordinates
and momenta of subsystem A, while (qn, pn) denotes those of subsystem B. Simi-
larly,
</p>
<p>〈Y 〉a :=
&int;
</p>
<p>Ye&minus;βHadqmdpm
&int;
</p>
<p>e&minus;βHadqmdpm
(G3.10)
</p>
<p>and likewise for the thermal averages for subsystem B.
</p>
<p>3.12 Compute C&prime; and show that F/N is independent of the size of the system using
(3.153).</p>
<p/>
</div>
<div class="page"><p/>
<p>G Hints to Selected Exercises 429
</p>
<p>3.13 Draw a graph for lnx. Then, note that
</p>
<p>lnN! =
N
</p>
<p>&sum;
i=1
</p>
<p>ln i (G3.11)
</p>
<p>and interpret the right-hand side graphically.
</p>
<p>3.15 Recall that the number of mechanical degrees of freedom is the number of
</p>
<p>variables we need to specify in order to uniquely determine the configuration of the
</p>
<p>system. This number is 5.
</p>
<p>For example, we can give the Cartesian coordinates (x2,y2,z2) of the particle of
mass m2 and then specify the orientation of the bond connecting the two particles
</p>
<p>by giving two polar angles θ and φ . The former can be replaced by (X ,Y,Z), the
coordinates of the center of mass of the molecule. But, the degrees of freedom still
</p>
<p>is 5.
</p>
<p>Alternatively, we can argue as follows. The positions of particles can be speci-
</p>
<p>fied by 6 variables (x1,y1,z1) and (x2,y2,z2). However, these variables are not all
independent. Instead, they are subject to the constraint that
</p>
<p>(x1 &minus; x2)2 +(y1 &minus; y2)2 +(z1 &minus; z2)2 = l2 . (G3.12)
</p>
<p>Thus, the number of independent variables, or the number of variables we need to
</p>
<p>specify, is 6&minus;1 = 5.
3.16
</p>
<p>a.
</p>
<p>Z =
V N
</p>
<p>Λ 3NN!
, (G3.13)
</p>
<p>where Λ := h/
&radic;
</p>
<p>2πmkBT .
</p>
<p>3.17 From (2.174), W rev = Ff &minus;Fi = kBT lnZi/Z f for an isothermal process, where
subscripts i and f refer to the initial and final state, respectively.
</p>
<p>3.18
</p>
<p>mi
d2rai
</p>
<p>dt2
= mib&minus;
</p>
<p>&part;φ
</p>
<p>&part; rai
. (G3.14)
</p>
<p>3.19 Lagrange&rsquo;s equations of motion lead to
</p>
<p>mi
d
</p>
<p>dt
(vi +V) = mib&minus;
</p>
<p>&part;φ
</p>
<p>&part; ri
. (G3.15)
</p>
<p>To see that this reduces to (G3.14), recall (3.229) and note that
</p>
<p>φ(r1, . . . ,rN) = φ(r1 +R, . . . ,rN +R) = φ(r
a
1, . . . ,r
</p>
<p>a
N) (G3.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>430 G Hints to Selected Exercises
</p>
<p>since φ is a function only of the relative position of the particles. Using &part; rai /&part; ri = Î,
where Î is the unit matrix, we have
</p>
<p>&part;φ(r1, . . . ,rN)
</p>
<p>&part; ri
=
</p>
<p>&part;φ(ra1, . . . ,r
a
N)
</p>
<p>&part; rai
. (G3.17)
</p>
<p>3.20 From Lagrange&rsquo;s equations of motion, we find
</p>
<p>mi
dvi
</p>
<p>dt
= mi
</p>
<p>(
b&minus; dV
</p>
<p>dt
</p>
<p>)
&minus; &part;φ
</p>
<p>&part; ri
. (G3.18)
</p>
<p>Moving the midV/dt term to the left, we see that this is noting but (G3.15). We
also see that the force exerted on the ith particle by the external field is modified by
</p>
<p>&minus;dV/dt. In particular, if dV/dt = b, then, the particle &ldquo;feels&rdquo; no external field at all.
If this seems surprising, recall how your body feels lighter when an elevator starts
</p>
<p>to go down. Extrapolate that feeling to a free fall, for which dV/dt = g.
</p>
<p>3.22 Use (G3.14). Since φ depends only on the relative position of particles, we
have
</p>
<p>0 &equiv; φ(ra1 +dr, . . . ,raN +dr)&minus;φ(ra1, . . . ,raN) =
N
</p>
<p>&sum;
i=1
</p>
<p>&part;φ
</p>
<p>&part; rai
&middot;dr+h.o. (G3.19)
</p>
<p>for any dr, implying that
N
</p>
<p>&sum;
i=1
</p>
<p>&part;φ
</p>
<p>&part; rai
&equiv; 0 . (G3.20)
</p>
<p>3.23 In (3.274),
</p>
<p>pi &middot;Ω&times; ri = πi &middot; (Ω&times; ri)+mi||Ω&times; ri||2 . (G3.21)
</p>
<p>Chapter 4
</p>
<p>4.1 If A and B are distinct species, then,
</p>
<p>S f &minus;Si = kB lnC fM &minus; kB lnCiM = 2NkB ln2 . (G4.1)
</p>
<p>If A and B are identical species, then,
</p>
<p>S f &minus;Si = kB ln
[
</p>
<p>22N
(N!)2
</p>
<p>(2N)!
</p>
<p>]
. (G4.2)
</p>
<p>Using (3.153),
</p>
<p>S f &minus;Si &asymp; 0 (G4.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>G Hints to Selected Exercises 431
</p>
<p>for a large enough N. Using (3.152) instead, we see that
</p>
<p>S f &minus;Si
kBN
</p>
<p>&asymp; 1
2N
</p>
<p>ln(πN) , (G4.4)
</p>
<p>which vanishes with increasing N. This is what one expects since, macroscopically,
</p>
<p>the initial and the final states are identical.
</p>
<p>What about small N? For N = 3, for example, (S f &minus; Si)/kB = ln(16/5), which
is not negligible. This should not be a surprise, though. With the partition now
</p>
<p>removed, the system can explore a much larger number of microstates, such as those
</p>
<p>in which all the particles occupy the same half of the total available volume 2V . Such
</p>
<p>states were not accessible when the partition was in place. As N increases, however,
</p>
<p>microstates are completely dominated by those with equal (or nearly equal) parti-
</p>
<p>tioning of particles between the two compartments.
</p>
<p>4.2
</p>
<p>b. Γ (1) = 1.
c. Γ (1/2) =
</p>
<p>&radic;
π .
</p>
<p>4.3
</p>
<p>b. We divide the n-dimensional space into concentric spherical shells of width dr
</p>
<p>centered around the origin. Ignoring the higher order terms, the volume dSn of
</p>
<p>the spherical shell can be written as
</p>
<p>dSn = Sn(r+dr)&minus;Sn(r) =
dSn
</p>
<p>dr
dr , (G4.5)
</p>
<p>where Sn(r) :=Unr
n. Thus,
</p>
<p>dSn = nUnr
n&minus;1dr (G4.6)
</p>
<p>and In may be written as
</p>
<p>In =
&int; &infin;
</p>
<p>0
nUnr
</p>
<p>n&minus;1e&minus;r
2
dr . (G4.7)
</p>
<p>d. Change variables by
</p>
<p>si :=
pi&radic;
2mE
</p>
<p>. (G4.8)
</p>
<p>Note that this is a vector equation and that dsi, for example, represents a volume
</p>
<p>element in three-dimensional space. Thus,
</p>
<p>dsi = dsixdsiydsiz =
dpixdpiydpiz
</p>
<p>(2mE)3/2
=
</p>
<p>dpi
</p>
<p>(2mE)3/2
. (G4.9)
</p>
<p>Also,
</p>
<p>H = E
N
</p>
<p>&sum;
i=1
</p>
<p>||si||2 (G4.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>432 G Hints to Selected Exercises
</p>
<p>4.5
</p>
<p>S =&minus;kB〈ln(p/Ω)〉 . (G4.11)
We note that p(E)dE is the probability of finding the system in the energy interval
(E &minus; dE,E]. The number of microstates within this interval is given by Ω(E)dE.
Thus, p/Ω is the probability of find the system at a particular microstate. If
microstates can be counted as in quantum mechanical systems, then, (G4.11) may
</p>
<p>be written as
</p>
<p>S =&minus;kB&sum;
i
</p>
<p>pi ln pi , (G4.12)
</p>
<p>where the summation is over all microstates that are accessible to the system.
</p>
<p>4.6
</p>
<p>S =&minus;kB〈ln(AwΛw p/Ω)〉 . (G4.13)
The expression in the natural logarithm can be interpreted as follows. Let us first
</p>
<p>rewrite it as
AwΛw p(E,V )
</p>
<p>Ω(E,V )
=
</p>
<p>p(E,V )dEdV
</p>
<p>Ω(E,V )dE &times; dV
AwΛw
</p>
<p>. (G4.14)
</p>
<p>In this expression, p(E,V )dEdV is the probability that the system is found with its
energy and volume in their respective intervals (E &minus; dE,E] and (V,V + dV ]. The
factor Ω(E,V )dE is the number of microstates consistent with the given interval of
E when the system volume is exactly equal to V . From the discussion below (4.105),
</p>
<p>dV/AwΛw may be regarded as &ldquo;the number of states&rdquo; accessible to the piston when
the system volume is allowed to be anywhere in (V,V + dV ]. The denominator is
then the number of microstates accessible to the system+piston consistent with the
</p>
<p>above intervals for E and V . Note that we may safely ignore here the change in Ω
due to the infinitesimal change in V as it leads a higher order term in the denominator
</p>
<p>of (G4.14). It follows that (G4.14) gives the probability of finding the system at a
</p>
<p>particular microstate.
</p>
<p>4.7
</p>
<p>Y =
1
</p>
<p>Λ 3N
</p>
<p>(
kBT
</p>
<p>P
</p>
<p>)N+1
. (G4.15)
</p>
<p>4.10
</p>
<p>c. You will need to prove two identities:
</p>
<p>(
&part;&micro;
</p>
<p>&part;N
</p>
<p>)
</p>
<p>T,V
</p>
<p>=
1
</p>
<p>N
</p>
<p>(
&part;P
</p>
<p>&part;nv
</p>
<p>)
</p>
<p>T
</p>
<p>and
</p>
<p>(
&part;nv
&part;P
</p>
<p>)
</p>
<p>T
</p>
<p>=
κT
V
</p>
<p>. (G4.16)
</p>
<p>The first one follows from the Gibbs&ndash;Duhem relation.
</p>
<p>4.11
</p>
<p>kBT =
2U
</p>
<p>3N
, P =
</p>
<p>NkBT
</p>
<p>V
, and
</p>
<p>eβ&micro;
</p>
<p>Λ 3
=
</p>
<p>N
</p>
<p>V
. (G4.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>G Hints to Selected Exercises 433
</p>
<p>4.12
</p>
<p>b. &minus;kBT lnX = 0.
c.
</p>
<p>X =
&int; &infin;
</p>
<p>0
e&minus;βPVΞdV =
</p>
<p>&int; &infin;
</p>
<p>0
dV 	= 1 . (G4.18)
</p>
<p>Chapter 5
</p>
<p>5.2 The integral &int;
e&minus;βψ(r
</p>
<p>Nt )drNt (G5.1)
</p>
<p>in (5.9) may be separated into terms depending on the number N of adsorbed parti-
</p>
<p>cles. Denoting by IN the value of the integral obtained under the constraint that there
</p>
<p>are N adsorbed particles, we have
</p>
<p>IN = (V &minus; v)Nt&minus;N
(
</p>
<p>M
</p>
<p>N
</p>
<p>)
Nt !
</p>
<p>(Nt &minus;N)!
AN , (G5.2)
</p>
<p>where A is defined by (5.13). The coefficient
(
</p>
<p>M
N
</p>
<p>)
is the number of distinct ways of
</p>
<p>choosing N sites among M sites. Then, Nt !/(Nt &minus;N)! gives the number of distinct
ways of occupying these sites with N distinguishable particles chosen from the total
</p>
<p>of Nt particles. Note that the correction due to indistinguishability of identical par-
</p>
<p>ticles is dealt with by the Nt ! factor in the partition function Z and plays no role at
</p>
<p>this stage. Here, we are simply trying to evaluate the integral given above. Suppose,
</p>
<p>for example, that N = 1. As we allow each of r1, . . . , rNt to move through the entire
system under the constraint that N = 1, each of them will be found inside a given
adsorption site.
</p>
<p>5.3 T &lt; 3w/(kB ln9).
</p>
<p>5.4
</p>
<p>a. φ = x/(1+ x), where x := eβ (&micro;+ε).
b.
</p>
<p>φ =
〈N〉
</p>
<p>4
=
</p>
<p>x+ x2 +2x2y+3x3y2 + x4y4
</p>
<p>1+4x+2x2 +4x2y+4x3y2 + x4y4
, (G5.3)
</p>
<p>where y := eβw.
c. φ = xy2φ/(1+ xy2φ ).
</p>
<p>5.5
</p>
<p>a. Compare graphs for ex and 1+ x.
b. ex = e〈x〉ex&minus;〈x〉.
c. Fexact &le; F . So, the free energy obtained under the mean-field approximation is
</p>
<p>an upper bound to the exact free energy.</p>
<p/>
</div>
<div class="page"><p/>
<p>434 G Hints to Selected Exercises
</p>
<p>5.6 Note that β&micro; = (&part; f/&part;φ)T as we saw in (5.52). Also, show that the intercept
(at φ = 0) of the tangent line is proportional to the pressure P of the two phases.
</p>
<p>5.7
</p>
<p>a. Tc = zw/4kB.
</p>
<p>Chapter 6
</p>
<p>6.1 Using (6.19), find the expressions for δA, δVα , and δV β . Use them in (6.23).
</p>
<p>Chapter 7
</p>
<p>7.2 Recall the vector identity
</p>
<p>&nabla; &middot; [a(x) f (x)] = f (x)&nabla; &middot;a(x)+a(x) &middot;&nabla; f (x) (G6.1)
</p>
<p>where a is a vector-valued function of x while f is a scalar-valued function of x.
</p>
<p>You will also need to use the divergence theorem:
</p>
<p>&int;
</p>
<p>V
&nabla; &middot;a(x)dr=
</p>
<p>&int;
</p>
<p>A
n &middot;a(x)dA , (G6.2)
</p>
<p>where A is the surface of V and n is the outward unit normal on A.
</p>
<p>7.5 Show that
</p>
<p>Ξ = exp
</p>
<p>[
eβ&micro;
</p>
<p>Λ 3
</p>
<p>&int;
</p>
<p>V
e&minus;βψ(r)dr
</p>
<p>]
. (G6.3)
</p>
<p>7.6
</p>
<p>a. Using (7.57), show that na(r) = nb(r)e
β (ψb&minus;ψa), where we suppressed the r
</p>
<p>dependence of ψ&rsquo;s in the exponent.
</p>
<p>7.7 Subtract (7.76) from the same equation applied to the perturbed state, in which
</p>
<p>the external field is ψa + δψ and the corresponding equilibrium density profile is
na +δn.
</p>
<p>7.8 Compute Ω [nb +δn,ψb]&minus;Ω [nb,ψb].
7.10 Note that
</p>
<p>&micro; =
</p>
<p>(
&part;F
</p>
<p>&part;N
</p>
<p>)
</p>
<p>T,V
</p>
<p>=
</p>
<p>(
&part; f
</p>
<p>&part;n
</p>
<p>)
</p>
<p>T
</p>
<p>and P =&minus; f +&micro;n . (G6.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>G Hints to Selected Exercises 435
</p>
<p>Chapter 8
</p>
<p>8.1
</p>
<p>a. Adding x to the both sides of x+θ &prime; = x, you get x+(x+θ &prime;) = x+ x. Then, use
V2, V1, V4, V1, and V3 in this order to show that θ &prime; = θ .
</p>
<p>b. For the particular element x that is appearing in 0x, show that x+0x = x and then
use part a.
</p>
<p>c. Adding x on both sides of x+ y = θ , you get x = x+(x+ y). Using V2, V1, V4,
V1, and V3 in this order, show that the right-hand side is y.
</p>
<p>d. Show that x+(&minus;1)x = 0x and use parts b and c.
e. Using part d, αx = α[(&minus;1)x].
f. Let x &isin;V . Then, for a particular vector αx &isin;V , show that αx+αθ = αx and use
</p>
<p>part a.
</p>
<p>g. Show that αx+αx = αθ = θ and use part c.
</p>
<p>8.2
</p>
<p>b. Set α = 0 in (8.43) and notice that 0y = θ from Exercise 8.1b.
</p>
<p>8.3 Use induction. That is, first establish the orthogonality of b1 and b2 by directly
</p>
<p>computing (b1,b
&prime;
2). Then, assuming that the vectors in the set {b1, . . . ,bm} (m &lt; r)
</p>
<p>are orthogonal to each other, consider the scalar product between bi (i &le; m) and
b&prime;m+1.
</p>
<p>8.4 Since {b1, . . . ,br} forms a basis of V , we may write x = α1b1 + &middot; &middot; &middot;+αrbr,
where, from S1 and the orthonormality of the basis, αi = (x,bi).
</p>
<p>8.5 Suppose that F(x) = (x, f ) = (x,g) for al x &isin;V and deduce f = g.
8.6
</p>
<p>(8.53): Use D1, F1, and D1 in this order.
</p>
<p>(8.54): Use D1, F2, and D1 in this order.
</p>
<p>8.7
</p>
<p>Ṽ3: Show that a linear function Θ defined by Θ(x) := (x,θ) serves as the zero
vector in Ṽ , where θ is the zero vector in V .
</p>
<p>Ṽ4: Define F &isin; Ṽ by F(x) :=(x, f ) for all x&isin;V and show that (F+F)(x)=Θ(x).
8.8
</p>
<p>a. Show that T (x)+T (θV ) = T (x) for x &isin;V .
8.9 Show that 〈γ|+ 〈θ |= 〈γ | for 〈γ | &isin;Vk.
8.11 Use (8.62), (8.63), and (8.73).
</p>
<p>8.12 Let |γ〉 := c|α〉 and consider the adjoint of X̂&dagger;|γ〉. You will need (8.67) and
(8.79).
</p>
<p>8.13 Use (8.85) twice to compute 〈α|(X̂&dagger;)&dagger;|β 〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>436 G Hints to Selected Exercises
</p>
<p>8.17
</p>
<p>c. Show that 〈δ |X̂&dagger;|γ〉= 〈γ |X̂ |δ 〉&lowast; = 〈δ |β 〉〈α|γ〉.
8.23 Start from 〈φ |Â|φ〉 and use a closure relation.
8.26 Using the closure relation,
</p>
<p>x̂|p〉=
&int;
</p>
<p>x̂|x〉〈x|p〉dx =
&int;
</p>
<p>x|x〉〈x|p〉dx . (G8.1)
</p>
<p>8.29 The time dependence drops out if Â is diagonal in the energy representation or
</p>
<p>equivalently, [Â, Ĥ] = 0.
</p>
<p>8.33
</p>
<p>a. Note that Û
&dagger;
t |φα , t〉= Û
</p>
<p>&dagger;
t Û t |φα ,0〉= |φα ,0〉.
</p>
<p>c. Recall (8.147).
</p>
<p>Appendix D
</p>
<p>D.2
</p>
<p>a. Let y =&minus;x.
c. From part a, δ (&minus;ax) = δ (ax). Thus, it is sufficient to show that δ (ax) = δ (x)/a.
</p>
<p>Let y = ax.
d. Let s = x2 &minus;a2. Then, x =
</p>
<p>&radic;
s+a2 if x &gt; 0 and x =&minus;
</p>
<p>&radic;
s+a2 if x &lt; 0.
</p>
<p>g. In the vicinity of x = ai, we can write g(x) = g
&prime;(ai)(x&minus;ai).
</p>
<p>D.4
</p>
<p>b. Apply integration by parts. In doing so, note that δ (x&minus; y) is zero as x &rarr;&plusmn;&infin; for
any fixed value of y. For the integral involving δ &prime;(y&minus; x), use the chain rule:
</p>
<p>d
</p>
<p>dx
δ (y&minus; x) =&minus;δ &prime;(y&minus; x) . (GD.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>NOTES 437
</p>
<p>Notes
</p>
<p>1 Lanczos C (1997) Linear Differential Operators. Dover, New York.
2 The statement does not hold if F depends on the third or higher time derivatives
</p>
<p>of r.
3 If a particle of mass 1 kg is accelerated by 1 m/s2, the magnitude of the force
</p>
<p>acting on it is 1 N, where N is a unit of force called Newton.
4 The symbol &ldquo;
</p>
<p>.
=&rdquo; stands for &ldquo;has the components given by.&rdquo; Note that mg is
</p>
<p>a vector, while (0,0,&minus;mg) is its representation in terms of components. These two
things are conceptually different. In fact, g, being an arrow with a certain length,
</p>
<p>exists independently of the choice of the coordinate system, while its components
</p>
<p>do not. To emphasize this distinction, we used &ldquo;
.
=&rdquo; instead of &ldquo;=.&rdquo;
</p>
<p>5 More precisely, F=&minus;Fc+ǫ. For a reversed path considered just above (1.21),
F=&minus;Fc &minus;ǫ. Since W is linear in F, however, the contribution from ǫ drops out in
the limit of ||ǫ|| &rarr; 0.
</p>
<p>6 If this expectation turned out to be false, we would have to believe, at least
</p>
<p>within the framework of classical mechanics, that the mechanical degrees of free-
</p>
<p>dom is greater than f , requiring more variables than those in the set (q1, . . . , q f , q̇1,
</p>
<p>. . . , q̇ f ) for the complete specification of the mechanical state of the system.
7 This problem is motivated by Problem 2.8-1 of Ref. [1] of Chap. 2.
8 This brings forward a close analogy between thermodynamics and the anal-
</p>
<p>ysis of mechanical systems at rest. A mechanical system is in equilibrium if its
</p>
<p>potential energy is stationary with respect to &ldquo;virtual displacements&rdquo; of its various
</p>
<p>constituent parts. As in variations considered in thermodynamics, the virtual dis-
</p>
<p>placements must satisfy any mechanical constraints imposed on the system. A gen-
</p>
<p>eralization of this principle to mechanical systems in motion leads to Hamilton&rsquo;s
</p>
<p>principle. In all cases, perturbations are denoted by the same symbol δ .
9 If this sounds too abstract, go to the produce section during your next visit to a
</p>
<p>grocery store and look for a fine plastic net containing either garlic bulbs or onions.
</p>
<p>That net makes a fairly accurate representation of the partition considered here.
10 One such example is found in the footnote straddling pages 257 and 258 of
</p>
<p>Ref. [3] of Chap. 2.
11 This formulation of the second law is due to Nishioka, who attributed it to
</p>
<p>Gibbs.
12 To appreciate the necessity of the condition just indicated, recall the definition
</p>
<p>of the partial derivative of f (x,y) with respect to x:
</p>
<p>&part; f
</p>
<p>&part;x
= lim
</p>
<p>ε&rarr;0
f (x+ ε ,y)&minus; f (x,y)
</p>
<p>ε
.
</p>
<p>Thus, &part; f/&part;x = &part;g/&part;x holds only if f (x,y) = g(x,y) within an interval containing
the x value at which the derivative is evaluated.
</p>
<p>13 Allow for the three-dimensional perspective in Fig. 3.2a, even though the
</p>
<p>dimension of a phase space is an even number.</p>
<p/>
</div>
<div class="page"><p/>
<p>438 NOTES
</p>
<p>14 That is, if there is no time-dependent external field. If such a field is present,
</p>
<p>then we cannot predict the future or the past of a given mechanical system based
</p>
<p>solely on the knowledge of q f and p f at some instant. We also need to know the
</p>
<p>exact time dependence of the external field.
15 It might be argued that there is a more sensible choice for the bounds on the
</p>
<p>components of momentum. According to the special theory of relativity, the speed
</p>
<p>of an object v cannot exceed that of the light c. Moreover, as v approaches c, our
</p>
<p>formula for H is no longer valid. However, for common systems of our interest, the
</p>
<p>values of β = (kBT )&minus;1 and m are such that the Boltzmann factor becomes negligible
well before the relativistic effects become significant. This being the case, what
</p>
<p>bounds we place on the components of momentum or whether we replace H by its
</p>
<p>relativistic counterpart has no bearing on our results.
16 Alternatively, we note that
</p>
<p>v2 =
</p>
<p>(
dl
</p>
<p>dt
</p>
<p>)2
=
</p>
<p>(dl)2
</p>
<p>(dt)2
,
</p>
<p>where dl is the magnitude of an infinitesimal displacement during the infinitesimal
</p>
<p>time interval dt. Since the r-, θ - and φ - axes are locally orthogonal to each other,
the Pythagorean theorem applies, leading to
</p>
<p>(dl)2 = (dr)2 +(rdθ)2 +(r sinθdφ)2 ,
</p>
<p>which yields the same result upon division by (dt)2.
17 In addition, the total linear momentum P and the total angular momentum M
</p>
<p>are such quantities.
18 More precisely, this is the standard deviation of the x values obtained through
</p>
<p>repeated measurements performed on a system prepared in an identical quantum
</p>
<p>mechanical state. See Sects. 8.5 and 8.7.2 for details.
19For a homogeneous system, the contribution from the interaction between the
</p>
<p>adjacent pair of parts, say i and j, can be regarded as being shared equally between
</p>
<p>these parts. One can then regard gi and g j as including this contribution. However,
</p>
<p>the subsequent argument will not hold unless one assumes the statistical indepen-
</p>
<p>dence between the different parts, which requires that the interaction between them
</p>
<p>be sufficiently weak.
20 Let y = 1/x. Then, x lnx =&minus;(lny)/y &rarr; 0 as y &rarr;+&infin;.
21 Because H has the explicit λ dependence, the statistical weight of a given
</p>
<p>microstate changes with time. We can safely ignore this time dependence because
</p>
<p>∆λ is infinitesimally small.
22 To see this more explicitly, we consider the ratio between
</p>
<p>1
</p>
<p>2
</p>
<p>&part; 2Sb
</p>
<p>&part;Eb
2
</p>
<p>∣∣∣∣∣
0
</p>
<p>Ea
2 and
</p>
<p>&part;Sb
&part;Eb
</p>
<p>∣∣∣∣
0
</p>
<p>Ea ,</p>
<p/>
</div>
<div class="page"><p/>
<p>NOTES 439
</p>
<p>which is given by
</p>
<p>&minus; Ea
2T
</p>
<p>&part;T
</p>
<p>&part;Eb
</p>
<p>∣∣∣∣
0
</p>
<p>.
</p>
<p>The quantity &minus;Ea&part;T/&part;Eb|0 is the change in T of system B when its energy changes
from Eab to Eab &minus;Ea. Provided that fa ≪ fb, this should be negligibly small com-
pared to T itself.
</p>
<p>23 According to (4.72), Z is the Laplace transform of Ω(E).
24 The number of microstates accessible to system B is given by (4.62) irrespec-
</p>
<p>tive of the precise value H assumes within the interval (E &minus;dE,E].
25This is a simplified version of the problem discussed in p.132 of Kubo R (1965),
</p>
<p>Statistical Mechanics. North-Holland Personal Library, Amsterdam.
26 If there are more than one species present in the system, this should be the
</p>
<p>semi-grand potential as the system we are considering is open only to one of the
</p>
<p>species. The corresponding ensemble is the semi-grand canonical ensemble.
27 Let us see this through an explicit computation. Suppose that none of the M
</p>
<p>sites is initially occupied and we place N particles one by one. The probability that
</p>
<p>the first particle is placed on the ith site is 1/M. The probability that the second
particle is placed on the site is
</p>
<p>M&minus;1
M
</p>
<p>&times; 1
M&minus;1 =
</p>
<p>1
</p>
<p>M
</p>
<p>with (M&minus;1)/M representing the probability that the first particle is placed on a site
other than the ith one while 1/(M &minus; 1) is the probability that the second particle is
placed on the ith site. Similarly, the probability that the nth particle is placed on the
</p>
<p>ith site is
M&minus;1
</p>
<p>M
&times; M&minus;2
</p>
<p>M&minus;1 &times;&middot;&middot; &middot;&times;
M&minus;n+1
M&minus;n+2 &times;
</p>
<p>1
</p>
<p>M&minus;n+1 =
1
</p>
<p>M
,
</p>
<p>which is independent of n. The probability that the ith site is occupied at some
</p>
<p>point in the process of placing N particles on M sites is obtained by adding these
</p>
<p>probabilities from n = 1 to N. This gives N/M.
28 Courtesy of Professor Zhen-Gang Wang.
29 More precisely, NIi is the long-time average of the instantaneous number of
</p>
<p>molecules in region I. Even then, NIi is well defined when the instantaneous value is
</p>
<p>defined by this convention.
30 A solid angle is measured by steradians, sr for short. We shall omit sr in what
</p>
<p>follows.
31 Because molecules interact at least over a distance of a few atomic diameters,
</p>
<p>the energy and entropy densities are expected to change continuously across the
</p>
<p>boundaries. In contrast, the number density can be made to vary discontinuously.
</p>
<p>The point is that δξ (r) in the varied state exhibits the r dependence that is difficult
to predict.
</p>
<p>32 This implies that the actual variation of the system under consideration is
</p>
<p>such that ξ (r) + δξ (r) is spherically symmetric within the solid angle ω except</p>
<p/>
</div>
<div class="page"><p/>
<p>440 NOTES
</p>
<p>for the region right next to the boundaries, where the effect of the surroundings in
</p>
<p>an infinitesimally different state of the matter cannot be ignored.
33 The condition is necessary but not sufficient because we have considered
</p>
<p>reversible variations only. In other words, if there are additional variables incapable
</p>
<p>of a reversible variation, they are held fixed when computing δU . The system may
not remain in equilibrium if we allow these variables to change as well.
</p>
<p>34 In Sect. 2.8.1, we called such a process differentiation.
35 In making this claim, we are ignoring the effect on γ of bending the interface.
</p>
<p>This is acceptable for flat or nearly flat interfaces.
36 If R is infinite as is the case for flat interfaces, we have to go back to the original
</p>
<p>definition (6.27) instead of (6.39). The latter is always satisfied as seen from (6.43).
37 Even though we have no prior knowledge on where the nucleus might form,
</p>
<p>we can always make a proper choice for B after the fact. This is permissible since B
</p>
<p>is purely a theoretical construct introduced entirely for the convenience of compu-
</p>
<p>tation.
38 This is often taken as the starting assumption when developing thermodynam-
</p>
<p>ics of interfaces. As we have seen, however, this is one of the consequences of
</p>
<p>thermodynamics of interfaces. The distinction is of paramount importance.
39 Open or half-open intervals, i.e., a &lt; x &lt; b, a &lt; x &le; b, or a &le; x &lt; b are also
</p>
<p>possible.
40 The partial derivative of f with respect to ui is defined by
</p>
<p>&part; f
</p>
<p>&part;ui
= lim
</p>
<p>ε&rarr;0
f (u1, . . . ,ui&minus;1,ui + ε ,ui+1, . . . ,un)&minus; f (u1, . . . ,un)
</p>
<p>ε
.
</p>
<p>This suggests that we define the functional derivative of F with respect to u(x) as
</p>
<p>δF
</p>
<p>δu(x)
= lim
</p>
<p>ε&rarr;0
F [u(x&prime;)+ εδ (x&prime;&minus; x)]&minus;F [u(x&prime;)]
</p>
<p>ε
.
</p>
<p>This alternative definition is consistent with the one we adopted. In fact, applying
</p>
<p>(7.12) to the numerator on the right with δu(x) replaced by εδ (x&prime;&minus; x), we find
</p>
<p>F [u(x&prime;)+ εδ (x&prime;&minus; x)]&minus;F [u(x&prime;)] =
&int;
</p>
<p>δF
</p>
<p>δu(x&prime;)
εδ (x&prime;&minus; x)dx&prime;+h.o. = ε δF
</p>
<p>δu(x)
+h.o.
</p>
<p>When this expression is substituted into the above definition, we obtain an equality
</p>
<p>between δF/δu(x) computed according to the two definitions.
41 Strictly speaking, we have only shown that a scalar Ai, to be interpreted as
</p>
<p>the chemical potential of species i of the interfacial region, is equal to the chemical
</p>
<p>potential of the same species in the surroundings. This itself does not imply unifor-
</p>
<p>mity of the chemical potential across the interfacial region. The same applies to the
</p>
<p>temperature. These remarks do not affect the development of our theory, however.
42 As far as known to me, this observation is due to Ref. [6] of Chap. 7, which
</p>
<p>refers to Ωint as the density functional and Ω as the grand potential.</p>
<p/>
</div>
<div class="page"><p/>
<p>NOTES 441
</p>
<p>43 Is it possible that the difference between neq(r,u] and neq(r,u+δu] is entirely
in the higher order terms and thus δn(r)&equiv; 0 for all r? To see how this might happen,
consider the integral
</p>
<p>&int;
</p>
<p>V
δn(r)δu(r)dr=
</p>
<p>&int;
</p>
<p>V
</p>
<p>&int;
</p>
<p>V
</p>
<p>δn(r)
</p>
<p>δu(r&prime;)
δu(r)δu(r&prime;)drdr&prime; .
</p>
<p>Using (7.55) and (7.56), we can show that
</p>
<p>&minus;kBT
δn(r)
</p>
<p>δu(r&prime;)
=&minus;kBT
</p>
<p>δ 2Ω
</p>
<p>δu(r)δu(r&prime;)
= 〈n̂(r,rN)n̂(r&prime;,rN)〉&minus;〈n̂(r,rN)〉〈n̂(r&prime;,rN)〉 ,
</p>
<p>which is a covariance matrix and hence is positive semi-definite. (To see a matrix
</p>
<p>here, imagine dividing up V into many tiny volume elements and labeling each of
</p>
<p>them. If r and r&prime; are in the ith and jth volume elements, respectively, δ 2Ω/δu(r)δu(r&prime;)
may be regarded as the i jth element of the matrix. In this picture, an integration over
</p>
<p>V becomes the sum over one of the indices on the matrix.) It follows that
</p>
<p>&int;
</p>
<p>V
δn(r)δu(r)dr&le; 0
</p>
<p>for any δu(r). If δu(r) is not identically zero, the equality holds only in the absence
of any fluctuation of n̂. Excluding this situation, therefore, δn(r) is no-zero at least
for some r.
</p>
<p>44 We note that δn in (7.114) is limited to only those variations that can be pro-
duced by some perturbation δu and we have argued for the invertibility of (7.114)
only for such δn. Could there be δn that cannot be produced by any δu? The
assumption is that there is none. That is, when δu exhausts all possible scalar-valued
functions on V , so does the corresponding δn given by (7.114). This will be the case
if we divide up the volume V into many tiny volume elements and replace δn(r) and
δu(r) by (δn1, . . . ,δnM) and (δu1, . . . ,δuM), respectively. Here M is the number of
the volume elements and a quantity bearing the subscript i is to be evaluated inside
</p>
<p>the ith volume element.
45 Even though erep(r) = erep(r),
</p>
<p>δFexcint [n,v
rep]
</p>
<p>δerep(r)
	= δF
</p>
<p>exc
int [n,v
</p>
<p>rep]
</p>
<p>δerep(r)
.
</p>
<p>In fact,
</p>
<p>δFexcint [n,v
rep] =&minus;kBT
</p>
<p>2
V
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>n(2)(r)
</p>
<p>erep(r)
δerep(r)4πr2dr .
</p>
<p>Thus,
δFexcint [n,v
</p>
<p>rep]
</p>
<p>δerep(r)
=&minus;kBT
</p>
<p>2
V
</p>
<p>n(2)(r)
</p>
<p>erep(r)
4πr2 = 4πr2
</p>
<p>δFexcint [n,v
rep]
</p>
<p>δerep(r)
.
</p>
<p>46 We assume that r is finite in order to avoid the question of convergence of the
</p>
<p>sum.</p>
<p/>
</div>
<div class="page"><p/>
<p>442 NOTES
</p>
<p>47 Taking a partial derivative of the equation with respect to x, we find
</p>
<p>d
</p>
<p>dx
</p>
<p>(
1
</p>
<p>X
</p>
<p>d2X
</p>
<p>dx2
</p>
<p>)
= 0 ,
</p>
<p>which indicates that the quantity in the brackets is a constant.
48 As an example, plot xne&minus;x (n = 1,2,3, . . .) for increasingly larger values of n
</p>
<p>for x &gt; 0. However, the statement is by no means universally true. For example, the
product between a rapidly increasing function ex and a rapidly decreasing function
</p>
<p>e&minus;x is a constant.
49 Based on the physical interpretation we have given to (4.99), it is tempting to
</p>
<p>believe that the equation holds even for quantum mechanical systems provided only
</p>
<p>that the piston can be regarded as a classical mechanical object. Because the piston
</p>
<p>is usually a macroscopic object, no significant error is expected to arise from such a
</p>
<p>treatment.
50 Technically dgn(x)/dx will be zero at x = 0 since we demanded smoothness of
</p>
<p>gn(x). This does not affect our practical use of δ (x) because dgn(x)/dx is nonzero
only in the immediate left of x = 0 and we are only concerned with nice enough
f (x).
</p>
<p>51 Since s is now a complex variable, we are abusing (3.91). The procedure can
</p>
<p>be justified by using a contour integration in the complex plane.</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
</p>
<p>Absolute fugacity, 200
</p>
<p>Absolute temperature, 48
</p>
<p>Acceleration, 4
</p>
<p>Action integral, 15
</p>
<p>Adiabatic process, 48
</p>
<p>Adjoint, 326, 328
</p>
<p>Angular momentum
</p>
<p>total, 32
</p>
<p>Angular velocity, 164
</p>
<p>Annihilation operator. See Operator
</p>
<p>Anti-commutator, 342
</p>
<p>Anti-hermitian. See Operator
</p>
<p>Anti-linearity, 318
</p>
<p>Apparent force, 163
</p>
<p>Auxiliary surface, 251
</p>
<p>B
</p>
<p>Barker-Henderson scheme, 290
</p>
<p>Basis, 314
</p>
<p>Basis vector, 314
</p>
<p>Binding energy, 206
</p>
<p>Binodal line, 301
</p>
<p>Binomial coefficient, 210, 401
</p>
<p>Blip function, 295
</p>
<p>Body. See Closed system
</p>
<p>Boltzmann constant, 50
</p>
<p>Boltzmann factor, 128
</p>
<p>Boltzmann&rsquo;s entropy formula. See
</p>
<p>Entropy formula
</p>
<p>Bose-Einstein statistics, 384
</p>
<p>Boson, 384
</p>
<p>Bra, 321
</p>
<p>Bra space, 321
</p>
<p>C
</p>
<p>Canonical ensemble. See Statistical ensemble
</p>
<p>Canonical partition function. See
</p>
<p>Partition function
</p>
<p>Carnahan-Starling formula, 292
</p>
<p>Carnot cycle. See Cycle
</p>
<p>Center of mass, 13
</p>
<p>Centrifugal force, 163
</p>
<p>Centrifugal potential energy, 166
</p>
<p>Chemical potential, 53
</p>
<p>Classical limit, 384
</p>
<p>Classical mechanical device, 46
</p>
<p>Classical theory approximation, 247
</p>
<p>Classical trace. See Trace
</p>
<p>Closed system, 43
</p>
<p>Closure relation, 332
</p>
<p>Coefficient of thermal expansion, 56
</p>
<p>Common tangent construction, 109
</p>
<p>Commutator, 327
</p>
<p>Compatible measurement, 341
</p>
<p>Complete set of commuting observables,
</p>
<p>335, 338
</p>
<p>Compressibility equation of state, 305
</p>
<p>Condition of equilibrium
</p>
<p>constant temperature system, 88
</p>
<p>energy representation, 69
</p>
<p>entropy representation, 64
</p>
<p>isolated system, 50, 63, 69
</p>
<p>open system, 266
</p>
<p>perfect equilibrium, 72
</p>
<p>reversible work source, 71
</p>
<p>Configurational integral, 149
</p>
<p>Conservative force, 6
</p>
<p>Conserved quantity, 11. See also Constant
</p>
<p>of motion
</p>
<p>Constant of motion, 11, 28, 39
</p>
<p>Cooperative phenomena, 212
</p>
<p>c&copy; Springer International Publishing Switzerland 2015 443
</p>
<p>I. Kusaka, Statistical Mechanics for Engineers,
</p>
<p>DOI 10.1007/978-3-319-13809-1</p>
<p/>
</div>
<div class="page"><p/>
<p>444 Index
</p>
<p>Coordination number, 220
</p>
<p>Coriolis force, 163
</p>
<p>Creation operator. See Operator
</p>
<p>Critical nucleus, 244
</p>
<p>Critical temperature, 220
</p>
<p>Cross product, 394
</p>
<p>Cycle, 77
</p>
<p>Carnot, 80
</p>
<p>D
</p>
<p>Degeneracy of an eigen value, 329
</p>
<p>Degenerate eigen value, 329
</p>
<p>Density functional, 266
</p>
<p>Density functional theory, 266
</p>
<p>Density of states, 174, 382
</p>
<p>Density operator, 267, 378
</p>
<p>Density profile, 267, 283
</p>
<p>Diathermal, 62
</p>
<p>Differentiation, 63
</p>
<p>Dirac δ -function, 409
strong definition, 416
</p>
<p>weak definition, 416
</p>
<p>Discontinuous variation, 233
</p>
<p>Dispersion, 341
</p>
<p>Dividing surface, 237
</p>
<p>Dot product, 391
</p>
<p>Driving force of phase separation, 109
</p>
<p>Dual space, 320
</p>
<p>Dynamical variables, 39
</p>
<p>Dyson series, 355
</p>
<p>E
</p>
<p>Efficiency of a heat engine, 80
</p>
<p>Ehrenfest&rsquo;s theorem, 376
</p>
<p>Eigenket, 329
</p>
<p>energy, 359
</p>
<p>Eigensubspace, 329
</p>
<p>Eigenvalue, 329
</p>
<p>energy, 359
</p>
<p>Energy, 11, 351
</p>
<p>Energy eigenfunction, 363
</p>
<p>Energy eigenket. See Eigenket
</p>
<p>Energy eigenvalue. See Eigenvalue
</p>
<p>Energy function, 25
</p>
<p>Engine
</p>
<p>heat, 79
</p>
<p>material, 79
</p>
<p>Ensemble. See Statistical ensemble
</p>
<p>Ensemble average, 116, 125
</p>
<p>Enthalpy, 56, 94
</p>
<p>Entropy, 48
</p>
<p>additivity of, 49
</p>
<p>extensivity of, 49
</p>
<p>Entropy formula
</p>
<p>Boltzmann, 173
</p>
<p>Gibbs, 128
</p>
<p>Equilibrium
</p>
<p>metastable, 64
</p>
<p>neutral, 64
</p>
<p>perfect, 72
</p>
<p>stable, 64
</p>
<p>thermodynamic, 49
</p>
<p>unstable, 64
</p>
<p>Equimolar dividing surface, 251
</p>
<p>Equipartition theorem, 134
</p>
<p>Ergodic problem, 148
</p>
<p>Euler force, 163
</p>
<p>Euler relation, 82
</p>
<p>generalized, 99
</p>
<p>Excess Helmholtz free energy, 283
</p>
<p>Exchange effect, 384
</p>
<p>Explicit time dependence, 20
</p>
<p>Extensive quantity, 45
</p>
<p>F
</p>
<p>Fermi-Dirac statistics, 384
</p>
<p>Fermion, 384
</p>
<p>First law of thermodynamics, 44
</p>
<p>Fluctuation
</p>
<p>probability of, 50
</p>
<p>Free energy functional. See Density functional
</p>
<p>Functional, 260
</p>
<p>Functional derivative, 262
</p>
<p>Functional inverse, 278
</p>
<p>Fundamental equation, 51
</p>
<p>Fundamental property relation, 54
</p>
<p>G
</p>
<p>Galilean transformation, 157
</p>
<p>Gamma function, 178
</p>
<p>Gas constant, 58
</p>
<p>Generalized coordinate, 20
</p>
<p>Generalized momentum, 25
</p>
<p>Generalized velocity, 20
</p>
<p>Gibbs adsorption equation, 243
</p>
<p>Gibbs free energy, 93, 193
</p>
<p>Gibbs phase rule, 86
</p>
<p>Gibbs&rsquo;s entropy formula. See Entropy formula
</p>
<p>Gibbs-Bogoliubov inequality, 402
</p>
<p>Gibbs-Duhem relation, 84
</p>
<p>generalized, 100
</p>
<p>Gibbs-Helmholtz equation, 93, 129
</p>
<p>Gibbs-Tolman-Koenig equation, 250
</p>
<p>Gradient of a function, 8
</p>
<p>Gram-Schmidt orthogonalization, 319
</p>
<p>Grand canonical partition function. See
</p>
<p>Partition function</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 445
</p>
<p>Grand potential, 94, 198
</p>
<p>Ground state, 370
</p>
<p>H
</p>
<p>Hamilton&rsquo;s equations of motion, 36
</p>
<p>Hamilton&rsquo;s principle, 15
</p>
<p>Hamiltonian, 35
</p>
<p>Hamiltonian operator. See Operator
</p>
<p>Hard sphere potential, 286
</p>
<p>Hard-sphere diameter, 286
</p>
<p>Heat
</p>
<p>definition, 44
</p>
<p>open system, 59
</p>
<p>Heat bath, 87
</p>
<p>Heat capacity
</p>
<p>constant pressure, 56
</p>
<p>constant volume, 55
</p>
<p>Heat engine. See Engine
</p>
<p>Heisenberg picture, 357
</p>
<p>Heisenberg uncertainty principle, 144, 341
</p>
<p>Heisenberg&rsquo;s equation of motion, 358
</p>
<p>Helmholtz free energy, 88, 184
</p>
<p>Hermit polynomials, 372
</p>
<p>Hermitian, 326
</p>
<p>Higher order terms, 399
</p>
<p>Homogeneity
</p>
<p>of space, 29
</p>
<p>of time, 33
</p>
<p>Hypothetical system. See Reference system
</p>
<p>I
</p>
<p>Ideal gas, 56
</p>
<p>Ideal gas mixture, 94
</p>
<p>Incompatible measurement, 341
</p>
<p>Inertial frame, 1
</p>
<p>Intensive quantity, 49
</p>
<p>Internal energy, 45
</p>
<p>additivity of, 45
</p>
<p>extensivity of, 45
</p>
<p>Internal virial. See Virial
</p>
<p>Intrinsic grand potential, 271
</p>
<p>Intrinsic Helmholtz free energy, 282
</p>
<p>Isothermal compressibility, 202
</p>
<p>Isothermal-isobaric partition function. See
</p>
<p>Partition function
</p>
<p>Isotropy of space, 31
</p>
<p>J
</p>
<p>Jacobi&rsquo;s identity, 40, 327
</p>
<p>K
</p>
<p>Ket, 321
</p>
<p>Ket space, 321
</p>
<p>Kinetic energy
</p>
<p>of a many particle system, 13
</p>
<p>of a particle, 10
</p>
<p>Kronecker delta, 40
</p>
<p>L
</p>
<p>Lagrange&rsquo;s equations of motion, 17
</p>
<p>Lagrangian, 14
</p>
<p>Langevin function, 136
</p>
<p>Laplace equation, 240
</p>
<p>Legendre transform, 35, 92, 406
</p>
<p>Legendre transformation, 34, 405
</p>
<p>inverse, 406
</p>
<p>Lennard-Jones diameter, 287
</p>
<p>Lennard-Jones potential, 287
</p>
<p>truncated and shifted, 287
</p>
<p>Lever rule, 109
</p>
<p>Linear combination, 313
</p>
<p>Linear functional, 319
</p>
<p>Linear independence, 313
</p>
<p>Linear momentum, 25
</p>
<p>total, 30
</p>
<p>Linear operator. See Operator
</p>
<p>Linearly dependent, 313
</p>
<p>Liouville&rsquo;s theorem, 122, 380
</p>
<p>Local density approximation, 291
</p>
<p>M
</p>
<p>Maclaurin series, 397
</p>
<p>Macrostate, 45
</p>
<p>Massieu function, 95
</p>
<p>Material engine. See Engine
</p>
<p>Material point, 2
</p>
<p>Matrix element, 332
</p>
<p>Maximum work principle, 77
</p>
<p>Maxwell relation, 96
</p>
<p>Maxwell-Boltzmann distribution, 133
</p>
<p>Mean-field approximation, 214, 218, 302
</p>
<p>Mechanical degrees of freedom, 20
</p>
<p>Mechanical energy
</p>
<p>of a many particle system, 13
</p>
<p>of a particle, 11
</p>
<p>Mechanical momentum, 166
</p>
<p>Mechanical state
</p>
<p>of f mechanical degrees of freedom, 20
</p>
<p>of a many particle system, 11
</p>
<p>of a particle, 4
</p>
<p>Metastable phase, 50
</p>
<p>Microcanonical ensemble. See Statistical
</p>
<p>ensemble
</p>
<p>Microcanonical partition function. See
</p>
<p>Partition function
</p>
<p>Microstate, 45
</p>
<p>Molar heat capacity
</p>
<p>constant pressure, 56</p>
<p/>
</div>
<div class="page"><p/>
<p>446 Index
</p>
<p>constant volume, 56
</p>
<p>Momentum, 25, 347
</p>
<p>Momentum operator. See Operator
</p>
<p>N
</p>
<p>Neumann series, 354
</p>
<p>Newton&rsquo;s equations of motion
</p>
<p>of a particle, 2
</p>
<p>of a many particle system, 11
</p>
<p>Norm, 317, 323
</p>
<p>Nucleation, 245, 301
</p>
<p>Nucleation rate, 245
</p>
<p>Number operator. See Operator
</p>
<p>O
</p>
<p>Observable, 330
</p>
<p>Open system, 43
</p>
<p>Operator
</p>
<p>annihilation, 369
</p>
<p>anti-Hermitian, 341
</p>
<p>creation, 370
</p>
<p>Hamiltonian, 351
</p>
<p>linear operator, 321
</p>
<p>momentum, 347
</p>
<p>number, 368
</p>
<p>position, 343
</p>
<p>time evolution, 350
</p>
<p>time ordering, 354
</p>
<p>translation, 344
</p>
<p>unit, 327, 393
</p>
<p>unitary, 328
</p>
<p>Orthogonality
</p>
<p>of kets, 323
</p>
<p>Orthogonality of a vector, 317
</p>
<p>Orthonormal basis, 318
</p>
<p>Outer product, 328
</p>
<p>P
</p>
<p>Packing fraction, 292
</p>
<p>Pair distribution function, 281, 283
</p>
<p>Pair potential, 151, 280
</p>
<p>Pairwise additivity, 151, 280
</p>
<p>Partial molar quantity, 98
</p>
<p>Particle, 2
</p>
<p>Partition function
</p>
<p>canonical, 145, 184, 382
</p>
<p>grand canonical, 198
</p>
<p>isothermal-isobaric, 193
</p>
<p>microcanonical, 172, 385
</p>
<p>Path function, 45
</p>
<p>Phase, 85
</p>
<p>Phase point, 114
</p>
<p>Phase space, 114
</p>
<p>Phase trajectory, 114
</p>
<p>Planck constant, 144
</p>
<p>Poisson bracket, 39
</p>
<p>Poisson theorem, 40
</p>
<p>Position operator. See Operator
</p>
<p>Potential energy
</p>
<p>of a particle, 7
</p>
<p>due to gravity, 6
</p>
<p>inter-particle interaction, 12
</p>
<p>Principle of equal weight, 172
</p>
<p>Principle of equipartition of energy. See
</p>
<p>Equipartition theorem
</p>
<p>Q
</p>
<p>Quasi-static chemical work, 59
</p>
<p>R
</p>
<p>Radial distribution function, 281, 284
</p>
<p>Reduced mass, 23
</p>
<p>Reference system, 237
</p>
<p>Reversible process, 48
</p>
<p>Reversible variation, 74
</p>
<p>Reversible work source, 71
</p>
<p>Routhian, 37
</p>
<p>S
</p>
<p>Scalar product, 317
</p>
<p>Scalar product space, 317
</p>
<p>Schrödinger equation, 351
</p>
<p>time dependent wave equation, 363
</p>
<p>time independent wave equation, 363
</p>
<p>Schrödinger picture, 356
</p>
<p>Second law of thermodynamics, 48
</p>
<p>Semi-grand potential, 94
</p>
<p>Solid angle, 232
</p>
<p>Span, 313
</p>
<p>Specific heat, 56
</p>
<p>Spectrum, 329
</p>
<p>Speed, 4
</p>
<p>Spinodal decomposition, 220, 301
</p>
<p>State function, 45
</p>
<p>Stationary state, 359
</p>
<p>Statistical ensemble, 120
</p>
<p>canonical, 125, 381
</p>
<p>microcanonical, 171, 385
</p>
<p>Statistical equilibrium, 118
</p>
<p>Stirling&rsquo;s formula, 143
</p>
<p>Superficial density, 242
</p>
<p>Supersaturation ratio, 248
</p>
<p>Surface excess quantity, 237
</p>
<p>Surface of tension, 240
</p>
<p>Surface tension, 240
</p>
<p>T
</p>
<p>Taylor series expansion, 397</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 447
</p>
<p>functional, 265
</p>
<p>Thermal average. See Ensemble average
</p>
<p>Thermal wavelength, 147
</p>
<p>Thermodynamic integration method, 154, 279
</p>
<p>Thermodynamic limit, 201
</p>
<p>Time dependent Schrödinger wave equation.
</p>
<p>See Schrödinger equation
</p>
<p>Time evolution operator. See Operator
</p>
<p>Time independent Schrödinger wave equation.
</p>
<p>See Schrödinger equation
</p>
<p>Time ordered exponential, 355
</p>
<p>Time ordering operator. See Operator
</p>
<p>Tolman correction, 250, 253
</p>
<p>Tolman length, 252
</p>
<p>Trace, 334
</p>
<p>classical, 273
</p>
<p>Translation operator. See Operator
</p>
<p>Trivial solution, 313
</p>
<p>Truncated and shifted Lennard-Jones potential.
</p>
<p>See Lennard-Jones potential
</p>
<p>U
</p>
<p>Uncertainty principle. See Heisenberg
</p>
<p>uncertainty principle
</p>
<p>Unit operator. See Operator
</p>
<p>Unitary operator. See Operator
</p>
<p>V
</p>
<p>Variation, 63
</p>
<p>Vector, 310
</p>
<p>Vector space, 310
</p>
<p>complex, 310
</p>
<p>dimension of, 314
</p>
<p>real, 310
</p>
<p>Velocity, 3
</p>
<p>Virial, 150
</p>
<p>Virial equation of state, 306
</p>
<p>W
</p>
<p>Wave function, 344
</p>
<p>coordinate-space, 349
</p>
<p>momentum-space, 349
</p>
<p>WCA separation, 289
</p>
<p>Weak interaction, 138
</p>
<p>Weighted density approximation, 292
</p>
<p>Widom&rsquo;s test particle insertion method, 153
</p>
<p>Work
</p>
<p>macroscopic, 44
</p>
<p>mechanical, 4
</p>
<p>Z
</p>
<p>Zero vector, 310, 389</p>
<p/>
</div>
<ul>	<li>Preface</li>
	<li>Acknowledgments</li>
	<li>Contents</li>
	<li>1 Classical Mechanics</li>
<ul>	<li>1.1 Inertial Frame</li>
	<li>1.2 Mechanics of a Single Particle</li>
<ul>	<li>1.2.1 Newton's Equation of Motion</li>
	<li>1.2.2 Work</li>
	<li>1.2.3 Potential Energy</li>
	<li>1.2.4 Kinetic Energy</li>
	<li>1.2.5 Conservation of Energy</li>
</ul>
	<li>1.3 Mechanics of Many Particles</li>
<ul>	<li>1.3.1 Mechanical Energy of a Many-Particle System</li>
</ul>
	<li>1.4 Center of Mass</li>
	<li>1.5 Hamilton's Principle</li>
<ul>	<li>1.5.1 Lagrangian and Action</li>
	<li>1.5.2 Generalized Coordinates</li>
	<li>1.5.3 Many Mechanical Degrees of Freedom</li>
</ul>
	<li>1.6 Momentum and Energy: Definitions</li>
	<li>1.7 &dagger;Energy Function and Energy</li>
	<li>1.8 Conservation Laws and Symmetry</li>
<ul>	<li>1.8.1 Conservation of Linear Momentum</li>
	<li>1.8.2 &Dagger;Conservation of Angular Momentum</li>
	<li>1.8.3 Conservation of Energy</li>
</ul>
	<li>1.9 Hamilton's Equations of Motion</li>
	<li>1.10 &dagger;Routhian</li>
	<li>1.11 Poisson Bracket</li>
	<li>1.12 Frequently Used Symbols</li>
	<li>References and Further Reading</li>
</ul>
	<li>2 Thermodynamics</li>
<ul>	<li>2.1 The First Law of Thermodynamics</li>
	<li>2.2 Quantifying Heat</li>
	<li>2.3 &Dagger;A Typical Expression for d&macr;W&#13;&#13;</li>
	<li>2.4 The Second Law of Thermodynamics</li>
	<li>2.5 Equilibrium of an Isolated System</li>
	<li>2.6 Fundamental Equations</li>
<ul>	<li>2.6.1 Closed System of Fixed Composition</li>
	<li>2.6.2 Open System</li>
	<li>2.6.3 Heat Capacities</li>
	<li>2.6.4 &Dagger;Ideal Gas</li>
	<li>2.6.5 &dagger;Heat Flow into an Open System</li>
</ul>
	<li>2.7 Role of Additional Variables</li>
	<li>2.8 Entropy Representation</li>
<ul>	<li>2.8.1 Condition of Equilibrium</li>
	<li>2.8.2 Equality of Temperature</li>
	<li>2.8.3 Direction of a Spontaneous Process</li>
	<li>2.8.4 &dagger;Very Short Remark on the Stability of Equilibrium</li>
</ul>
	<li>2.9 Energy Representation</li>
<ul>	<li>2.9.1 Condition of Equilibrium</li>
	<li>2.9.2 Reversible Work Source</li>
	<li>2.9.3 &dagger;Condition of Perfect Equilibrium</li>
	<li>2.9.4 &dagger;Closed System with a Chemical Reaction</li>
	<li>2.9.5 &dagger;Maximum Work Principle</li>
</ul>
	<li>2.10 Euler Relation</li>
	<li>2.11 Gibbs&ndash;Duhem Relation</li>
	<li>2.12 &Dagger;Gibbs Phase Rule</li>
	<li>2.13 Free Energies</li>
<ul>	<li>2.13.1 Fixing Temperature</li>
	<li>2.13.2 Condition of Equilibrium</li>
	<li>2.13.3 Direction of a Spontaneous Process</li>
	<li>2.13.4 &dagger;Wrev and a Spontaneous Process</li>
	<li>2.13.5 Fundamental Equation</li>
	<li>2.13.6 Other Free Energies</li>
</ul>
	<li>2.14 &Dagger;Maxwell Relation</li>
	<li>2.15 &Dagger;Partial Molar Quantities</li>
	<li>2.16 Graphical Methods</li>
<ul>	<li>2.16.1 Pure Systems: F Versus V (Constant T)</li>
	<li>2.16.2 Binary Mixtures: G Versus x1 (Constant T and P)</li>
</ul>
	<li>2.17 Frequently Used Symbols</li>
	<li>References and Further Reading</li>
</ul>
	<li>3 Classical Statistical Mechanics</li>
<ul>	<li>3.1 Macroscopic Measurement</li>
	<li>3.2 Phase Space</li>
	<li>3.3 Ensemble Average</li>
	<li>3.4 Statistical Equilibrium</li>
	<li>3.5 Statistical Ensemble</li>
	<li>3.6 Liouville's Theorem</li>
	<li>3.7 Significance of H</li>
	<li>3.8 &dagger;The Number of Constants of Motion</li>
	<li>3.9 Canonical Ensemble</li>
	<li>3.10 Simple Applications of Canonical Ensemble</li>
<ul>	<li>3.10.1 Rectangular Coordinate System</li>
	<li>3.10.2 Equipartition Theorem</li>
	<li>3.10.3 Spherical Coordinate System</li>
	<li>3.10.4 &dagger;General Equipartition Theorem</li>
</ul>
	<li>3.11 Canonical Ensemble and Thermal Contact</li>
	<li>3.12 Corrections from Quantum Mechanics</li>
<ul>	<li>3.12.1 A System of Identical Particles</li>
	<li>3.12.2 Implication of the Uncertainty Principle</li>
	<li>3.12.3 Applicability of Classical Statistical Mechanics</li>
</ul>
	<li>3.13 &dagger;A Remark on the Statistical Approach</li>
	<li>3.14 &Dagger;Expressions for P and μ&#13;</li>
<ul>	<li>3.14.1 &Dagger;Pressure</li>
	<li>3.14.2 &Dagger;Chemical Potential</li>
</ul>
	<li>3.15 &dagger;Internal Energy</li>
<ul>	<li>3.15.1 &dagger;Equilibrium in Motion?</li>
	<li>3.15.2 &dagger;From a Stationary to a Moving Frame</li>
	<li>3.15.3 &dagger;Back to the Stationary Frame</li>
</ul>
	<li>3.16 &dagger;Equilibrium of an Accelerating Body</li>
<ul>	<li>3.16.1 &dagger;Linear Translation</li>
	<li>3.16.2 &dagger;Rotation</li>
</ul>
	<li>3.17 Frequently Used Symbols</li>
	<li>References and Further Reading</li>
</ul>
	<li>4 Various Statistical Ensembles</li>
<ul>	<li>4.1 Fluctuations in a Canonical Ensemble</li>
	<li>4.2 Microcanonical Ensemble</li>
<ul>	<li>4.2.1 Expression for ρ&#13;&#13;</li>
	<li>4.2.2 Choice of  ΔE&#13;</li>
	<li>4.2.3 Isolated System</li>
</ul>
	<li>4.3 Phase Integral in Microcanonical Ensemble</li>
	<li>4.4 &dagger;Adiabatic Reversible Processes</li>
	<li>4.5 Canonical Ensemble</li>
<ul>	<li>4.5.1 Closed System Held at a Constant Temperature</li>
	<li>4.5.2 Canonical Distribution</li>
	<li>4.5.3 Classical Canonical Partition Function</li>
	<li>4.5.4 Applicability of Canonical Ensemble</li>
</ul>
	<li>4.6 &Dagger;Isothermal&ndash;Isobaric Ensemble</li>
	<li>4.7 Grand Canonical Ensemble</li>
	<li>4.8 Frequently Used Symbols</li>
	<li>References and Further Reading</li>
</ul>
	<li>5 Simple Models of Adsorption</li>
<ul>	<li>5.1 Exact Solutions</li>
<ul>	<li>5.1.1 Single Site</li>
	<li>5.1.2 &dagger;Binding Energy</li>
	<li>5.1.3 Multiple Independent Sites</li>
	<li>5.1.4 Four Sites with Interaction Among Particles</li>
</ul>
	<li>5.2 Mean-Field Approximation</li>
<ul>	<li>5.2.1 Four Sites with Interaction Among Particles</li>
	<li>5.2.2 Two-Dimensional Lattice</li>
</ul>
	<li>5.3 Frequently Used Symbols</li>
	<li>Reference</li>
</ul>
	<li>6 Thermodynamics of Interfaces</li>
<ul>	<li>6.1 Interfacial Region</li>
	<li>6.2 Defining a System</li>
	<li>6.3 Condition of Equilibrium</li>
<ul>	<li>6.3.1 &dagger;Variations in the State of the System</li>
	<li>6.3.2 Fixed System Boundaries</li>
	<li>6.3.3 Reference System</li>
	<li>6.3.4 Movable System Boundaries</li>
	<li>6.3.5 Laplace Equation</li>
</ul>
	<li>6.4 Euler Relation</li>
	<li>6.5 Gibbs&ndash;Adsorption Equation</li>
	<li>6.6 Flat Interface</li>
	<li>6.7 &#13;Wrev as a Measure of Stability&#13;</li>
<ul>	<li>6.7.1 Exact Expression</li>
	<li>6.7.2 &Dagger;Classical Theory Approximations</li>
	<li>6.7.3 &dagger;Thermodynamic Degrees of Freedom</li>
	<li>6.7.4 Small Nucleus</li>
</ul>
	<li>6.8 &dagger;Gibbs&ndash;Tolman&ndash;Koenig Equation</li>
	<li>6.9 &dagger;Interfacial Properties</li>
	<li>6.10 Frequently Used Symbols</li>
	<li>References and Further Reading</li>
</ul>
	<li>7 Statistical Mechanics of Inhomogeneous Fluids</li>
<ul>	<li>7.1 Functional</li>
<ul>	<li>7.1.1 Definition</li>
	<li>7.1.2 Functional Derivative</li>
</ul>
	<li>7.2 Density Functional Theory</li>
<ul>	<li>7.2.1 Equilibrium Density Profile</li>
	<li>7.2.2 Microscopic Definition of Density</li>
	<li>7.2.3  Ω for a Nonequilibrium Density Profile&#13;</li>
	<li>7.2.4 &dagger;A Few Remarks on Ω[na,ψb]�</li>
</ul>
	<li>7.3 Formal Development</li>
<ul>	<li>7.3.1 Definitions</li>
	<li>7.3.2 Key Properties of the Density Functional</li>
	<li>7.3.3 &dagger;Proofs of Theorems</li>
</ul>
	<li>7.4 Construction of a Density Functional</li>
<ul>	<li>7.4.1 Variation of the External Field</li>
	<li>7.4.2 Variation of the Intermolecular Potential: Case 1</li>
	<li>7.4.3 Variation of the Intermolecular Potential: Case 2</li>
	<li>7.4.4 Pair Distribution Function</li>
	<li>7.4.5 Repulsive Potential</li>
	<li>7.4.6 Radial Distribution Function</li>
	<li>7.4.7 &dagger;Barker&ndash;Henderson Scheme</li>
</ul>
	<li>7.5 Hard-Sphere Fluid Under Gravity</li>
	<li>7.6 Vapor&ndash;Liquid Coexistence</li>
<ul>	<li>7.6.1 Phase Diagram</li>
	<li>7.6.2 Interfacial Properties</li>
</ul>
	<li>7.7 &Dagger;Equations of State from the Radial Distribution Function</li>
<ul>	<li>7.7.1 &Dagger;Compressibility Equation of State</li>
	<li>7.7.2 &Dagger;Virial Equation of State</li>
</ul>
	<li>7.8 Frequently Used Symbols</li>
	<li>References and Further Reading</li>
</ul>
	<li>8 Quantum Formulation</li>
<ul>	<li>8.1 Vector Space</li>
<ul>	<li>8.1.1 Definition</li>
	<li>8.1.2 Linear Independence</li>
	<li>8.1.3 Basis</li>
	<li>8.1.4 &dagger;Proofs of Theorems</li>
	<li>8.1.5 Scalar Product Space</li>
	<li>8.1.6 Orthonormal Basis</li>
	<li>8.1.7 Functions</li>
	<li>8.1.8 Linear Functional</li>
	<li>8.1.9 Linear Operator</li>
</ul>
	<li>8.2 Kets, Bras, and Operators</li>
<ul>	<li>8.2.1 Bra&ndash;Ket</li>
	<li>8.2.2 Operator and Adjoint</li>
	<li>8.2.3 Addition and Multiplication of Operators</li>
	<li>8.2.4 Unitary Operator</li>
	<li>8.2.5 Outer Product</li>
</ul>
	<li>8.3 Eigenkets and Eigenvalues</li>
<ul>	<li>8.3.1 Definition</li>
	<li>8.3.2 Closure relation</li>
	<li>8.3.3 Matrix Representation</li>
	<li>8.3.4 Commuting Observables</li>
	<li>8.3.5 &dagger;Degenerate Eigenvalues</li>
</ul>
	<li>8.4 Postulates of Quantum Mechanics</li>
	<li>8.5 &Dagger;Uncertainty Principle</li>
	<li>8.6 &Dagger;Operator with Continuous Spectrum</li>
<ul>	<li>8.6.1 &Dagger;Position Operator and Position Eigenkets</li>
</ul>
	<li>8.7 &dagger;Linear Translation</li>
<ul>	<li>8.7.1 &dagger;Properties of Linear Translation</li>
	<li>8.7.2 &dagger;Commutation Relations</li>
	<li>8.7.3 &dagger;Momentum Eigenket</li>
</ul>
	<li>8.8 &dagger;Time Evolution Operator</li>
	<li>8.9 &dagger;ˆUt is Unitary&#13;</li>
	<li>8.10 &Dagger;Formal Solution of the Schr&ouml;dinger Equation</li>
<ul>	<li>8.10.1 &Dagger;Time-Independent ˆH&#13;</li>
	<li>8.10.2 &dagger;Time-Dependent ˆH&#13;&#13;</li>
</ul>
	<li>8.11 &Dagger;Heisenberg's Equation of Motion</li>
<ul>	<li>8.11.1 &Dagger;Time-Independent ˆH&#13;</li>
	<li>8.11.2 &dagger;Time-Dependent ˆH&#13;</li>
</ul>
	<li>8.12 Eigenstates of ˆH&#13;</li>
	<li>8.13 &Dagger;Schr&ouml;dinger Wave Equation</li>
	<li>8.14 &Dagger;Harmonic Oscillator</li>
<ul>	<li>8.14.1 &Dagger;Operator Method</li>
	<li>8.14.2 &Dagger;Energy Eigenfunctions</li>
</ul>
	<li>8.15 &dagger;Ehrenfest's Theorem</li>
	<li>8.16 Quantum Statistical Mechanics</li>
<ul>	<li>8.16.1 Density Matrix</li>
	<li>8.16.2 Statistical Equilibrium</li>
	<li>8.16.3 Liouville's Theorem</li>
	<li>8.16.4 Canonical Ensemble</li>
	<li>8.16.5 Ideal Gas and Classical Limit</li>
	<li>8.16.6 Microcanonical Ensemble</li>
</ul>
	<li>8.17 Frequently Used Symbols</li>
	<li>References and Further Reading</li>
</ul>
	<li>A Vectors in Three-Dimensional Space</li>
<ul>	<li>A.1 Arrow in Space</li>
	<li>A.2 Components of a Vector</li>
	<li>A.3 Dot Product</li>
	<li>A.4 Unit Operator</li>
	<li>A.5 &dagger;Schwarz Inequality</li>
	<li>A.6 Cross Product</li>
</ul>
	<li>B Useful Formulae</li>
<ul>	<li>B.1 Taylor Series Expansion</li>
<ul>	<li>B.1.1 Function of a Single Variable</li>
	<li>B.1.2 Function of Multiple Variables</li>
</ul>
	<li>B.2 Exponential</li>
	<li>B.3 Summation of a Geometric Series</li>
	<li>B.4 Binomial Expansion</li>
	<li>B.5 Gibbs&ndash;Bogoliubov Inequality</li>
</ul>
	<li>C Legendre Transformation</li>
<ul>	<li>C.1 Legendre Transformation</li>
<ul>	<li>C.1.1 Representation of a Curve</li>
	<li>C.1.2 Legendre Transformation</li>
	<li>C.1.3 Inverse Legendre Transformation</li>
</ul>
</ul>
	<li>D Dirac δ-Function&#13;</li>
<ul>	<li>D.1 Definition of δ(x)&#13;</li>
	<li>D.2 Basic Properties of the δ-Function&#13;</li>
	<li>D.3 Weak Versus Strong Definitions of the δ-Function&#13;</li>
	<li>D.4 Three-Dimensional δ-Function&#13;</li>
	<li>D.5 &dagger;Representation of the δ-Function&#13;</li>
	<li>References and Further Reading</li>
</ul>
	<li>E Where to Go from Here</li>
	<li>F List of Greek Letters</li>
	<li>G Hints to Selected Exercises</li>
	<li>Index</li>
</ul>
</body></html>