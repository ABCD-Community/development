<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
</div>
<div class="page"><p/>
<p>Statistics and Computing
Series Editors:
</p>
<p>J. Chambers
</p>
<p>D. Hand
</p>
<p>W. Härdle</p>
<p/>
</div>
<div class="page"><p/>
<p>Statistics and Computing
</p>
<p>Brusco/Stahl: Branch and Bound Applications in Combinatorial
</p>
<p>Data Analysis
</p>
<p>Chambers: Software for Data Analysis: Programming with R
Dalgaard: Introductory Statistics with R, 2nd ed.
Gentle: Elements of Computational Statistics
</p>
<p>Gentle: Numerical Linear Algebra for Applications in Statistics
</p>
<p>Gentle: Random Number Generation and Monte
</p>
<p>Carlo Methods, 2nd ed.
</p>
<p>Härdle/Klinke/Turlach: XploRe: An Interactive Statistical
</p>
<p>Computing Environment
</p>
<p>Hörmann/Leydold/Derflinger: Automatic Nonuniform Random
</p>
<p>Variate Generation
</p>
<p>Krause/Olson: The Basics of S-PLUS, 4th ed.
</p>
<p>Lange: Numerical Analysis for Statisticians
</p>
<p>Lemmon/Schafer: Developing Statistical Software in Fortran 95
</p>
<p>Loader: Local Regression and Likelihood
</p>
<p>Marasinghe/Kennedy: SAS for Data Analysis: Intermediate
</p>
<p>Statistical Methods
</p>
<p>Ó Ruanaidh/Fitzgerald: Numerical Bayesian Methods Applied to
</p>
<p>Signal Processing
</p>
<p>Pannatier: VARIOWIN: Software for Spatial Data Analysis in 2D
</p>
<p>Pinheiro/Bates: Mixed-Effects Models in S and S-PLUS
</p>
<p>Unwin/Theus/Hofmann: Graphics of Large Datasets:
</p>
<p>Visualizing a Million
</p>
<p>Venables/Ripley:Modern Applied Statistics with S, 4th ed.
</p>
<p>Venables/Ripley: S Programming
</p>
<p>Wilkinson: The Grammar of Graphics, 2nd ed.</p>
<p/>
</div>
<div class="page"><p/>
<p>Peter Dalgaard
</p>
<p>Introductory Statistics with R
</p>
<p>Second Edition
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>Peter Dalgaard
Department of Biostatistics
University of Copenhagen
Denmark
p.dalgaard@biostat.ku.dk
</p>
<p>ISSN: 1431-8784
ISBN: 978-0-387-79053-4 e-ISBN: 978-0-387-79054-1
DOI: 10.1007/978-0-387-79054-1
</p>
<p>Library of Congress Control Number: 2008932040
</p>
<p>c&copy; 2008 Springer Science+Business Media, LLC
All rights reserved. This work may not be translated or copied in whole or in part without the written
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York,
NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use
in connection with any form of information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they
are not identified as such, is not to be taken as an expression of opinion as to whether or not they are
subject to proprietary rights.
</p>
<p>Printed on acid-free paper
</p>
<p>springer.com</p>
<p/>
</div>
<div class="page"><p/>
<p>To Grete, for putting up with me for so long</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>R is a statistical computer program made available through the Internet
under the General Public License (GPL). That is, it is supplied with a li-
cense that allows you to use it freely, distribute it, or even sell it, as long as
the receiver has the same rights and the source code is freely available. It
exists for Microsoft Windows XP or later, for a variety of Unix and Linux
platforms, and for Apple Macintosh OS X.
</p>
<p>R provides an environment in which you can perform statistical analysis
and produce graphics. It is actually a complete programming language,
although that is only marginally described in this book. Here we content
ourselves with learning the elementary concepts and seeing a number of
cookbook examples.
</p>
<p>R is designed in such a way that it is always possible to do further
computations on the results of a statistical procedure. Furthermore, the
design for graphical presentation of data allows both no-nonsense meth-
ods, for example plot(x,y), and the possibility of fine-grained control
of the output&rsquo;s appearance. The fact that R is based on a formal computer
language gives it tremendous flexibility. Other systems present simpler
interfaces in terms of menus and forms, but often the apparent user-
friendliness turns into a hindrance in the longer run. Although elementary
statistics is often presented as a collection of fixed procedures, analysis
of moderately complex data requires ad hoc statistical model building,
which makes the added flexibility of R highly desirable.</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Preface
</p>
<p>R owes its name to typical Internet humour. You may be familiar with
the programming language C (whose name is a story in itself). Inspired
by this, Becker and Chambers chose in the early 1980s to call their newly
developed statistical programming language S. This language was further
developed into the commercial product S-PLUS, which by the end of the
decade was in widespread use among statisticians of all kinds. Ross Ihaka
and Robert Gentleman from the University of Auckland, New Zealand,
chose to write a reduced version of S for teaching purposes, and what was
more natural than choosing the immediately preceding letter? Ross&rsquo; and
Robert&rsquo;s initials may also have played a role.
</p>
<p>In 1995, Martin Maechler persuaded Ross and Robert to release the source
code forR under the GPL. This coincidedwith the upsurge inOpen Source
software spurred by the Linux system. R soon turned out to fill a gap for
people like me who intended to use Linux for statistical computing but
had no statistical package available at the time. A mailing list was set up
for the communication of bug reports and discussions of the development
of R.
</p>
<p>In August 1997, I was invited to join an extended international core team
whose members collaborate via the Internet and that has controlled the
development of R since then. The core team was subsequently expanded
several times and currently includes 19 members. On February 29, 2000,
version 1.0.0 was released. As of this writing, the current version is 2.6.2.
</p>
<p>This book was originally based upon a set of notes developed for the
course in Basic Statistics for Health Researchers at the Faculty of Health
Sciences of the University of Copenhagen. The course had a primary tar-
get of students for the Ph.D. degree in medicine. However, the material
has been substantially revised, and I hope that it will be useful for a larger
audience, although some biostatistical bias remains, particularly in the
choice of examples.
</p>
<p>In later years, the course in Statistical Practice in Epidemiology, which has
been held yearly in Tartu, Estonia, has been a major source of inspiration
and experience in introducing young statisticians and epidemiologists to
R.
</p>
<p>This book is not a manual for R. The idea is to introduce a number of basic
concepts and techniques that should allow the reader to get started with
practical statistics.
</p>
<p>In terms of the practical methods, the book covers a reasonable curriculum
for first-year students of theoretical statistics as well as for engineering
students. These groups will eventually need to go further and study
more complex models as well as general techniques involving actual
programming in the R language.</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface ix
</p>
<p>For fields where elementary statistics is taught mainly as a tool, the book
goes somewhat further than what is commonly taught at the under-
graduate level. Multiple regression methods or analysis of multifactorial
experiments are rarely taught at that level but may quickly become essen-
tial for practical research. I have collected the simpler methods near the
beginning to make the book readable also at the elementary level. How-
ever, in order to keep technical material together, Chapters 1 and 2 do
include material that some readers will want to skip.
</p>
<p>The book is thus intended to be useful for several groups, but I will not
pretend that it can stand alone for any of them. I have included brief
theoretical sections in connection with the various methods, but more
than as teaching material, these should serve as reminders or perhaps as
appetizers for readers who are new to the world of statistics.
</p>
<p>Notes on the 2nd edition
</p>
<p>The original first chapter was expanded and broken into two chapters,
and a chapter on more advanced data handling tasks was inserted after
the coverage of simpler statistical methods. There are also two new chap-
ters on statistical methodology, covering Poisson regression and nonlinear
curve fitting, and a few items have been added to the section on de-
scriptive statistics. The original methodological chapters have been quite
minimally revised, mainly to ensure that the text matches the actual out-
put of the current version of R. The exercises have been revised, and
solution sketches now appear in Appendix D.
</p>
<p>Acknowledgements
</p>
<p>Obviously, this book would not have been possible without the efforts of
my friends and colleagues on the R Core Team, the authors of contributed
packages, and many of the correspondents of the e-mail discussion lists.
</p>
<p>I am deeply grateful for the support of my colleagues and co-teachers
Lene Theil Skovgaard, Bendix Carstensen, Birthe Lykke Thomsen, Helle
Rootzen, Claus Ekstr&oslash;m, Thomas Scheike, and from the Tartu course
Krista Fischer, Esa L&auml;&auml;ra, Martyn Plummer, Mark Myatt, and Michael
Hills, as well as the feedback from several students. In addition, sev-
eral people, including Bill Venables, Brian Ripley, and David James, gave
valuable advice on early drafts of the book.
</p>
<p>Finally, profound thanks are due to the free software community at large.
The R project would not have been possible without their effort. For the</p>
<p/>
</div>
<div class="page"><p/>
<p>x Preface
</p>
<p>typesetting of this book, TEX, LATEX, and the consolidating efforts of the
LATEX2e project have been indispensable.
</p>
<p>Peter Dalgaard
Copenhagen
</p>
<p>April 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>Preface vii
</p>
<p>1 Basics 1
1.1 First steps . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.1.1 An overgrown calculator . . . . . . . . . . . . . . 3
1.1.2 Assignments . . . . . . . . . . . . . . . . . . . . . 3
1.1.3 Vectorized arithmetic . . . . . . . . . . . . . . . . 4
1.1.4 Standard procedures . . . . . . . . . . . . . . . . 6
1.1.5 Graphics . . . . . . . . . . . . . . . . . . . . . . . 7
</p>
<p>1.2 R language essentials . . . . . . . . . . . . . . . . . . . . 9
1.2.1 Expressions and objects . . . . . . . . . . . . . . . 9
1.2.2 Functions and arguments . . . . . . . . . . . . . 11
1.2.3 Vectors . . . . . . . . . . . . . . . . . . . . . . . . 12
1.2.4 Quoting and escape sequences . . . . . . . . . . 13
1.2.5 Missing values . . . . . . . . . . . . . . . . . . . . 14
1.2.6 Functions that create vectors . . . . . . . . . . . . 14
1.2.7 Matrices and arrays . . . . . . . . . . . . . . . . . 16
1.2.8 Factors . . . . . . . . . . . . . . . . . . . . . . . . 18
1.2.9 Lists . . . . . . . . . . . . . . . . . . . . . . . . . . 19
1.2.10 Data frames . . . . . . . . . . . . . . . . . . . . . 20
1.2.11 Indexing . . . . . . . . . . . . . . . . . . . . . . . 21
1.2.12 Conditional selection . . . . . . . . . . . . . . . . 22
1.2.13 Indexing of data frames . . . . . . . . . . . . . . 23
1.2.14 Grouped data and data frames . . . . . . . . . . 25</p>
<p/>
</div>
<div class="page"><p/>
<p>xii Contents
</p>
<p>1.2.15 Implicit loops . . . . . . . . . . . . . . . . . . . . 26
1.2.16 Sorting . . . . . . . . . . . . . . . . . . . . . . . . 27
</p>
<p>1.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
</p>
<p>2 The R environment 31
2.1 Session management . . . . . . . . . . . . . . . . . . . . 31
</p>
<p>2.1.1 The workspace . . . . . . . . . . . . . . . . . . . . 31
2.1.2 Textual output . . . . . . . . . . . . . . . . . . . . 32
2.1.3 Scripting . . . . . . . . . . . . . . . . . . . . . . . 33
2.1.4 Getting help . . . . . . . . . . . . . . . . . . . . . 34
2.1.5 Packages . . . . . . . . . . . . . . . . . . . . . . . 35
2.1.6 Built-in data . . . . . . . . . . . . . . . . . . . . . 35
2.1.7 attach and detach . . . . . . . . . . . . . . 36
2.1.8 subset, transform, and within . . . . . . . . 37
</p>
<p>2.2 The graphics subsystem . . . . . . . . . . . . . . . . . . . 39
2.2.1 Plot layout . . . . . . . . . . . . . . . . . . . . . . 39
2.2.2 Building a plot from pieces . . . . . . . . . . . . . 40
2.2.3 Using par . . . . . . . . . . . . . . . . . . . . . . 42
2.2.4 Combining plots . . . . . . . . . . . . . . . . . . . 42
</p>
<p>2.3 R programming . . . . . . . . . . . . . . . . . . . . . . . 44
2.3.1 Flow control . . . . . . . . . . . . . . . . . . . . . 44
2.3.2 Classes and generic functions . . . . . . . . . . . 46
</p>
<p>2.4 Data entry . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.4.1 Reading from a text file . . . . . . . . . . . . . . . 47
2.4.2 Further details on read.table . . . . . . . . . . 50
2.4.3 The data editor . . . . . . . . . . . . . . . . . . . 51
2.4.4 Interfacing to other programs . . . . . . . . . . . 52
</p>
<p>2.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>3 Probability and distributions 55
3.1 Random sampling . . . . . . . . . . . . . . . . . . . . . . 55
3.2 Probability calculations and combinatorics . . . . . . . . 56
3.3 Discrete distributions . . . . . . . . . . . . . . . . . . . . 57
3.4 Continuous distributions . . . . . . . . . . . . . . . . . . 58
3.5 The built-in distributions in R . . . . . . . . . . . . . . . 59
</p>
<p>3.5.1 Densities . . . . . . . . . . . . . . . . . . . . . . . 59
3.5.2 Cumulative distribution functions . . . . . . . . 62
3.5.3 Quantiles . . . . . . . . . . . . . . . . . . . . . . . 63
3.5.4 Random numbers . . . . . . . . . . . . . . . . . . 64
</p>
<p>3.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
</p>
<p>4 Descriptive statistics and graphics 67
4.1 Summary statistics for a single group . . . . . . . . . . . 67
4.2 Graphical display of distributions . . . . . . . . . . . . . 71
</p>
<p>4.2.1 Histograms . . . . . . . . . . . . . . . . . . . . . . 71</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xiii
</p>
<p>4.2.2 Empirical cumulative distribution . . . . . . . . 73
4.2.3 Q&ndash;Q plots . . . . . . . . . . . . . . . . . . . . . . 74
4.2.4 Boxplots . . . . . . . . . . . . . . . . . . . . . . . 75
</p>
<p>4.3 Summary statistics by groups . . . . . . . . . . . . . . . 75
4.4 Graphics for grouped data . . . . . . . . . . . . . . . . . 79
</p>
<p>4.4.1 Histograms . . . . . . . . . . . . . . . . . . . . . . 79
4.4.2 Parallel boxplots . . . . . . . . . . . . . . . . . . . 80
4.4.3 Stripcharts . . . . . . . . . . . . . . . . . . . . . . 81
</p>
<p>4.5 Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.5.1 Generating tables . . . . . . . . . . . . . . . . . . 83
4.5.2 Marginal tables and relative frequency . . . . . . 87
</p>
<p>4.6 Graphical display of tables . . . . . . . . . . . . . . . . . 89
4.6.1 Barplots . . . . . . . . . . . . . . . . . . . . . . . . 89
4.6.2 Dotcharts . . . . . . . . . . . . . . . . . . . . . . . 91
4.6.3 Piecharts . . . . . . . . . . . . . . . . . . . . . . . 92
</p>
<p>4.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
</p>
<p>5 One- and two-sample tests 95
5.1 One-sample t test . . . . . . . . . . . . . . . . . . . . . . 95
5.2 Wilcoxon signed-rank test . . . . . . . . . . . . . . . . . 99
5.3 Two-sample t test . . . . . . . . . . . . . . . . . . . . . . 100
5.4 Comparison of variances . . . . . . . . . . . . . . . . . . 103
5.5 Two-sample Wilcoxon test . . . . . . . . . . . . . . . . . 103
5.6 The paired t test . . . . . . . . . . . . . . . . . . . . . . . 104
5.7 The matched-pairs Wilcoxon test . . . . . . . . . . . . . 106
5.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
</p>
<p>6 Regression and correlation 109
6.1 Simple linear regression . . . . . . . . . . . . . . . . . . . 109
6.2 Residuals and fitted values . . . . . . . . . . . . . . . . . 113
6.3 Prediction and confidence bands . . . . . . . . . . . . . . 117
6.4 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . 120
</p>
<p>6.4.1 Pearson correlation . . . . . . . . . . . . . . . . . 121
6.4.2 Spearman&rsquo;s ρ . . . . . . . . . . . . . . . . . . . . . 123
6.4.3 Kendall&rsquo;s τ . . . . . . . . . . . . . . . . . . . . . . 124
</p>
<p>6.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
</p>
<p>7 Analysis of variance and the Kruskal&ndash;Wallis test 127
7.1 One-way analysis of variance . . . . . . . . . . . . . . . 127
</p>
<p>7.1.1 Pairwise comparisons and multiple testing . . . 131
7.1.2 Relaxing the variance assumption . . . . . . . . . 133
7.1.3 Graphical presentation . . . . . . . . . . . . . . . 134
7.1.4 Bartlett&rsquo;s test . . . . . . . . . . . . . . . . . . . . . 136
</p>
<p>7.2 Kruskal&ndash;Wallis test . . . . . . . . . . . . . . . . . . . . . 136
7.3 Two-way analysis of variance . . . . . . . . . . . . . . . 137</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Contents
</p>
<p>7.3.1 Graphics for repeated measurements . . . . . . . 140
7.4 The Friedman test . . . . . . . . . . . . . . . . . . . . . . 141
7.5 The ANOVA table in regression analysis . . . . . . . . . 141
7.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
</p>
<p>8 Tabular data 145
8.1 Single proportions . . . . . . . . . . . . . . . . . . . . . . 145
8.2 Two independent proportions . . . . . . . . . . . . . . . 147
8.3 k proportions, test for trend . . . . . . . . . . . . . . . . . 149
8.4 r&times; c tables . . . . . . . . . . . . . . . . . . . . . . . . . . 151
8.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
</p>
<p>9 Power and the computation of sample size 155
9.1 The principles of power calculations . . . . . . . . . . . 155
</p>
<p>9.1.1 Power of one-sample and paired t tests . . . . . . 156
9.1.2 Power of two-sample t test . . . . . . . . . . . . . 158
9.1.3 Approximate methods . . . . . . . . . . . . . . . 158
9.1.4 Power of comparisons of proportions . . . . . . . 159
</p>
<p>9.2 Two-sample problems . . . . . . . . . . . . . . . . . . . . 159
9.3 One-sample problems and paired tests . . . . . . . . . . 161
9.4 Comparison of proportions . . . . . . . . . . . . . . . . . 161
9.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
</p>
<p>10 Advanced data handling 163
10.1 Recoding variables . . . . . . . . . . . . . . . . . . . . . . 163
</p>
<p>10.1.1 The cut function . . . . . . . . . . . . . . . . . . 163
10.1.2 Manipulating factor levels . . . . . . . . . . . . . 165
10.1.3 Working with dates . . . . . . . . . . . . . . . . . 166
10.1.4 Recoding multiple variables . . . . . . . . . . . . 169
</p>
<p>10.2 Conditional calculations . . . . . . . . . . . . . . . . . . 170
10.3 Combining and restructuring data frames . . . . . . . . 171
</p>
<p>10.3.1 Appending frames . . . . . . . . . . . . . . . . . 172
10.3.2 Merging data frames . . . . . . . . . . . . . . . . 173
10.3.3 Reshaping data frames . . . . . . . . . . . . . . . 175
</p>
<p>10.4 Per-group and per-case procedures . . . . . . . . . . . . 178
10.5 Time splitting . . . . . . . . . . . . . . . . . . . . . . . . . 179
10.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
</p>
<p>11 Multiple regression 185
11.1 Plotting multivariate data . . . . . . . . . . . . . . . . . . 185
11.2 Model specification and output . . . . . . . . . . . . . . 187
11.3 Model search . . . . . . . . . . . . . . . . . . . . . . . . . 190
11.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 193</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xv
</p>
<p>12 Linear models 195
12.1 Polynomial regression . . . . . . . . . . . . . . . . . . . . 196
12.2 Regression through the origin . . . . . . . . . . . . . . . 198
12.3 Design matrices and dummy variables . . . . . . . . . . 200
12.4 Linearity over groups . . . . . . . . . . . . . . . . . . . . 202
12.5 Interactions . . . . . . . . . . . . . . . . . . . . . . . . . . 206
12.6 Two-way ANOVA with replication . . . . . . . . . . . . 207
12.7 Analysis of covariance . . . . . . . . . . . . . . . . . . . 208
</p>
<p>12.7.1 Graphical description . . . . . . . . . . . . . . . . 209
12.7.2 Comparison of regression lines . . . . . . . . . . 212
</p>
<p>12.8 Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . 218
12.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
</p>
<p>13 Logistic regression 227
13.1 Generalized linear models . . . . . . . . . . . . . . . . . 228
13.2 Logistic regression on tabular data . . . . . . . . . . . . 229
</p>
<p>13.2.1 The analysis of deviance table . . . . . . . . . . . 234
13.2.2 Connection to test for trend . . . . . . . . . . . . 235
</p>
<p>13.3 Likelihood profiling . . . . . . . . . . . . . . . . . . . . . 237
13.4 Presentation as odds-ratio estimates . . . . . . . . . . . . 239
13.5 Logistic regression using raw data . . . . . . . . . . . . . 239
13.6 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
13.7 Model checking . . . . . . . . . . . . . . . . . . . . . . . 242
13.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
</p>
<p>14 Survival analysis 249
14.1 Essential concepts . . . . . . . . . . . . . . . . . . . . . . 249
14.2 Survival objects . . . . . . . . . . . . . . . . . . . . . . . 250
14.3 Kaplan&ndash;Meier estimates . . . . . . . . . . . . . . . . . . . 251
14.4 The log-rank test . . . . . . . . . . . . . . . . . . . . . . . 254
14.5 The Cox proportional hazards model . . . . . . . . . . . 256
14.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
</p>
<p>15 Rates and Poisson regression 259
15.1 Basic ideas . . . . . . . . . . . . . . . . . . . . . . . . . . 259
</p>
<p>15.1.1 The Poisson distribution . . . . . . . . . . . . . . 260
15.1.2 Survival analysis with constant hazard . . . . . . 260
</p>
<p>15.2 Fitting Poisson models . . . . . . . . . . . . . . . . . . . 262
15.3 Computing rates . . . . . . . . . . . . . . . . . . . . . . . 266
15.4 Models with piecewise constant intensities . . . . . . . . 270
15.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
</p>
<p>16 Nonlinear curve fitting 275
16.1 Basic usage . . . . . . . . . . . . . . . . . . . . . . . . . . 276
16.2 Finding starting values . . . . . . . . . . . . . . . . . . . 278</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Contents
</p>
<p>16.3 Self-starting models . . . . . . . . . . . . . . . . . . . . . 284
16.4 Profiling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
16.5 Finer control of the fitting algorithm . . . . . . . . . . . 287
16.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
</p>
<p>A Obtaining and installing R and the ISwR package 289
</p>
<p>B Data sets in the ISwR package 293
</p>
<p>C Compendium 325
</p>
<p>D Answers to exercises 337
</p>
<p>Bibliography 355
</p>
<p>Index 357</p>
<p/>
</div>
<div class="page"><p/>
<p>1
Basics
</p>
<p>The purpose of this chapter is to get you started using R. It is assumed that
you have a working installation of the software and of the ISwR package
that contains the data sets for this book. Instructions for obtaining and
installing the software are given in Appendix A.
</p>
<p>The text that follows describes R version 2.6.2. As of this writing, that is
the latest version of R. As far as possible, I present the issues in a way
that is independent of the operating system in use and assume that the
reader has the elementary operational knowledge to select from menus,
move windows around, etc. I do, however, make exceptions where I am
aware of specific difficulties with a particular platform or specific features
of it.
</p>
<p>1.1 First steps
</p>
<p>This section gives an introduction to the R computing environment and
walks you through its most basic features.
</p>
<p>Starting R is straightforward, but the method will depend on your com-
puting platform. You will be able to launch it from a system menu, by
double-clicking an icon, or by entering the command &ldquo;R&rdquo; at the system
command line. This will either produce a console window or cause R
to start up as an interactive program in the current terminal window. In
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_1, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1. Basics
</p>
<p>Figure 1.1. Screen image of R for Windows.
</p>
<p>either case, R works fundamentally by the question-and-answer model:
You enter a line with a command and press Enter (&larr;֓ ). Then the program
does something, prints the result if relevant, and asks for more input.
When R is ready for input, it prints out its prompt, a &ldquo;&gt;&rdquo;. It is possi-
ble to use R as a text-only application, and also in batch mode, but for
the purposes of this chapter, I assume that you are sitting at a graphical
workstation.
</p>
<p>All the examples in this book should run if you type them in exactly as
printed, provided that you have the ISwR package not only installed but
also loaded into your current search path. This is done by entering
</p>
<p>&gt; library(ISwR)
</p>
<p>at the command prompt. You do not need to understand what the
command does at this point. It is explained in Section 2.1.5.
</p>
<p>For a first impression of what R can do, try typing the following:
</p>
<p>&gt; plot(rnorm(1000))
</p>
<p>This command draws 1000 numbers at random from the normal distri-
bution (rnorm = random normal) and plots them in a pop-up graphics
window. The result on a Windows machine can be seen in Figure 1.1.
</p>
<p>Of course, you are not expected at this point to guess that you would ob-
tain this result in that particular way. The example is chosen because it
shows several components of the user interface in action. Before the style</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 First steps 3
</p>
<p>of commands will fall naturally, it is necessary to introduce some concepts
and conventions through simpler examples.
</p>
<p>UnderWindows, the graphics windowwill have taken the keyboard focus
at this point. Click on the console to make it accept further commands.
</p>
<p>1.1.1 An overgrown calculator
</p>
<p>One of the simplest possible tasks in R is to enter an arithmetic expression
and receive a result. (The second line is the answer from the machine.)
</p>
<p>&gt; 2 + 2
</p>
<p>[1] 4
</p>
<p>So the machine knows that 2 plus 2 makes 4. Of course, it also knows how
to do other standard calculations. For instance, here is how to compute
e&minus;2 :
</p>
<p>&gt; exp(-2)
</p>
<p>[1] 0.1353353
</p>
<p>The [1] in front of the result is part of R&rsquo;s way of printing numbers and
vectors. It is not useful here, but it becomes so when the result is a longer
vector. The number in brackets is the index of the first number on that
line. Consider the case of generating 15 random numbers from a normal
distribution:
</p>
<p>&gt; rnorm(15)
</p>
<p>[1] -0.18326112 -0.59753287 -0.67017905 0.16075723 1.28199575
</p>
<p>[6] 0.07976977 0.13683303 0.77155246 0.85986694 -1.01506772
</p>
<p>[11] -0.49448567 0.52433026 1.07732656 1.09748097 -1.09318582
</p>
<p>Here, for example, the [6] indicates that 0.07976977 is the sixth element in
the vector. (For typographical reasons, the examples in this book are made
with a shortened line width. If you try it on your own machine, you will
see the values printedwith six numbers per line rather than five. The num-
bers themselves will also be different since random number generation is
involved.)
</p>
<p>1.1.2 Assignments
</p>
<p>Even on a calculator, you will quickly need some way to store intermedi-
ate results, so that you do not have to key them in over and over again.
R, like other computer languages, has symbolic variables, that is names that</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1. Basics
</p>
<p>can be used to represent values. To assign the value 2 to the variable x,
you can enter
</p>
<p>&gt; x &lt;- 2
</p>
<p>The two characters &lt;- should be read as a single symbol: an arrow point-
ing to the variable to which the value is assigned. This is known as the
assignment operator. Spacing around operators is generally disregarded
by R, but notice that adding a space in the middle of a &lt;- changes the
meaning to &ldquo;less than&rdquo; followed by &ldquo;minus&rdquo; (conversely, omitting the
space when comparing a variable to a negative number has unexpected
consequences!).
</p>
<p>There is no immediately visible result, but from now on, x has the value 2
and can be used in subsequent arithmetic expressions.
</p>
<p>&gt; x
</p>
<p>[1] 2
</p>
<p>&gt; x + x
</p>
<p>[1] 4
</p>
<p>Names of variables can be chosen quite freely in R. They can be built from
letters, digits, and the period (dot) symbol. There is, however, the limita-
tion that the name must not start with a digit or a period followed by a
digit. Names that start with a period are special and should be avoided.
A typical variable name could be height.1yr, which might be used to
describe the height of a child at the age of 1 year. Names are case-sensitive:
WT and wt do not refer to the same variable.
</p>
<p>Some names are already used by the system. This can cause some con-
fusion if you use them for other purposes. The worst cases are the
single-letter names c, q, t, C, D, F, I, and T, but there are also diff, df,
and pt, for example. Most of these are functions and do not usually cause
trouble when used as variable names. However, F and T are the standard
abbreviations for FALSE and TRUE and no longer work as such if you
redefine them.
</p>
<p>1.1.3 Vectorized arithmetic
</p>
<p>You cannot do much statistics on single numbers! Rather, you will look at
data from a group of patients, for example. One strength of R is that it can
handle entire data vectors as single objects. A data vector is simply an array
of numbers, and a vector variable can be constructed like this:
</p>
<p>&gt; weight &lt;- c(60, 72, 57, 90, 95, 72)
</p>
<p>&gt; weight
</p>
<p>[1] 60 72 57 90 95 72</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 First steps 5
</p>
<p>The construct c(...) is used to define vectors. The numbers are made
up but might represent the weights (in kg) of a group of normal men.
</p>
<p>This is neither the only way to enter data vectors into R nor is it gen-
erally the preferred method, but short vectors are used for many other
purposes, and the c(...) construct is used extensively. In Section 2.4,
we discuss alternative techniques for reading data. For now, we stick to a
single method.
</p>
<p>You can do calculations with vectors just like ordinary numbers, as long
as they are of the same length. Suppose that we also have the heights that
correspond to the weights above. The body mass index (BMI) is defined
for each person as the weight in kilograms divided by the square of the
height in meters. This could be calculated as follows:
</p>
<p>&gt; height &lt;- c(1.75, 1.80, 1.65, 1.90, 1.74, 1.91)
</p>
<p>&gt; bmi &lt;- weight/height^2
</p>
<p>&gt; bmi
</p>
<p>[1] 19.59184 22.22222 20.93664 24.93075 31.37799 19.73630
</p>
<p>Notice that the operation is carried out elementwise (that is, the first value
of bmi is 60/1.752 and so forth) and that the ^ operator is used for raising
a value to a power. (On some keyboards, ^ is a &ldquo;dead key&rdquo; and you will
have to press the spacebar afterwards to make it show.)
</p>
<p>It is in fact possible to perform arithmetic operations on vectors of differ-
ent length. We already used that when we calculated the height^2 part
above since 2 has length 1. In such cases, the shorter vector is recycled.
This is mostly used with vectors of length 1 (scalars) but sometimes also
in other cases where a repeating pattern is desired. A warning is issued if
the longer vector is not a multiple of the shorter in length.
</p>
<p>These conventions for vectorized calculations make it very easy to specify
typical statistical calculations. Consider, for instance, the calculation of the
mean and standard deviation of the weight variable.
</p>
<p>First, calculate the mean, x̄ = &sum; xi/n:
</p>
<p>&gt; sum(weight)
</p>
<p>[1] 446
</p>
<p>&gt; sum(weight)/length(weight)
</p>
<p>[1] 74.33333
</p>
<p>Then save the mean in a variable xbar and proceed with the calculation
of SD =
</p>
<p>&radic;
</p>
<p>(&sum;(xi &minus; x̄)2)/(n&minus; 1). We do this in steps to see the individual
components. The deviations from the mean are
</p>
<p>&gt; xbar &lt;- sum(weight)/length(weight)
</p>
<p>&gt; weight - xbar</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1. Basics
</p>
<p>[1] -14.333333 -2.333333 -17.333333 15.666667 20.666667
</p>
<p>[6] -2.333333
</p>
<p>Notice how xbar, which has length 1, is recycled and subtracted from
each element of weight. The squared deviations will be
</p>
<p>&gt; (weight - xbar)^2
</p>
<p>[1] 205.444444 5.444444 300.444444 245.444444 427.111111
</p>
<p>[6] 5.444444
</p>
<p>Since this command is quite similar to the one before it, it is convenient
to enter it by editing the previous command. On most systems running R,
the previous command can be recalled with the up-arrow key.
</p>
<p>The sum of squared deviations is similarly obtained with
</p>
<p>&gt; sum((weight - xbar)^2)
</p>
<p>[1] 1189.333
</p>
<p>and all in all the standard deviation becomes
</p>
<p>&gt; sqrt(sum((weight - xbar)^2)/(length(weight) - 1))
</p>
<p>[1] 15.42293
</p>
<p>Of course, since R is a statistical program, such calculations are already
built into the program, and you get the same results just by entering
</p>
<p>&gt; mean(weight)
</p>
<p>[1] 74.33333
</p>
<p>&gt; sd(weight)
</p>
<p>[1] 15.42293
</p>
<p>1.1.4 Standard procedures
</p>
<p>As a slightly more complicated example of what R can do, consider the
following: The rule of thumb is that the BMI for a normal-weight indi-
vidual should be between 20 and 25, and we want to know if our data
deviate systematically from that. You might use a one-sample t test to as-
sess whether the six persons&rsquo; BMI can be assumed to havemean 22.5 given
that they come from a normal distribution. To this end, you can use the
function t.test. (You might not know the theory of the t test yet. The
example is included here mainly to give some indication of what &ldquo;real&rdquo;
statistical output looks like. A thorough description of t.test is given in
Chapter 5.)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 First steps 7
</p>
<p>&gt; t.test(bmi, mu=22.5)
</p>
<p>One Sample t-test
</p>
<p>data: bmi
</p>
<p>t = 0.3449, df = 5, p-value = 0.7442
</p>
<p>alternative hypothesis: true mean is not equal to 22.5
</p>
<p>95 percent confidence interval:
</p>
<p>18.41734 27.84791
</p>
<p>sample estimates:
</p>
<p>mean of x
</p>
<p>23.13262
</p>
<p>The argument mu=22.5 attaches a value to the formal argument mu,
which represents the Greek letter &micro; conventionally used for the theoret-
ical mean. If this is not given, t.testwould use the default mu=0, which
is not of interest here.
</p>
<p>For a test like this, we get a more extensive printout than in the earlier
examples. The details of the output are explained in Chapter 5, but you
might focus on the p-value which is used for testing the hypothesis that
the mean is 22.5. The p-value is not small, indicating that it is not at all un-
likely to get data like those observed if the meanwere in fact 22.5. (Loosely
speaking; actually p is the probability of obtaining a t value bigger than
0.3449 or less than&minus;0.3449.) However, youmight also look at the 95% con-
fidence interval for the true mean. This interval is quite wide, indicating
that we really have very little information about the true mean.
</p>
<p>1.1.5 Graphics
</p>
<p>One of the most important aspects of the presentation and analysis of data
is the generation of proper graphics. R &mdash; like S before it &mdash; has a model
for constructing plots that allows simple production of standard plots as
well as fine control over the graphical components.
</p>
<p>If you want to investigate the relation between weight and height, the
first idea is to plot one versus the other. This is done by
</p>
<p>&gt; plot(height,weight)
</p>
<p>leading to Figure 1.2.
</p>
<p>You will often want to modify the drawing in various ways. To that end,
there are a wealth of plotting parameters that you can set. As an example,
let us try changing the plotting symbol using the keyword pch (&ldquo;plotting
character&rdquo;) like this:
</p>
<p>&gt; plot(height, weight, pch=2)</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1. Basics
</p>
<p>1.65 1.70 1.75 1.80 1.85 1.90
</p>
<p>6
0
</p>
<p>7
0
</p>
<p>8
0
</p>
<p>9
0
</p>
<p>height
</p>
<p>w
e
ig
</p>
<p>h
t
</p>
<p>Figure 1.2. A simple x&ndash;y plot.
</p>
<p>This gives the plot in Figure 1.3, with the points now marked with little
triangles.
</p>
<p>The idea behind the BMI calculation is that this value should be inde-
pendent of the person&rsquo;s height, thus giving you a single number as an
indication of whether someone is overweight and by how much. Since
a normal BMI should be about 22.5, you would expect that weight &asymp;
22.5 &times; height2. Accordingly, you can superimpose a curve of expected
weights at BMI 22.5 on the figure:
</p>
<p>&gt; hh &lt;- c(1.65, 1.70, 1.75, 1.80, 1.85, 1.90)
</p>
<p>&gt; lines(hh, 22.5 * hh^2)
</p>
<p>yielding Figure 1.4. The function lines will add (x, y) values joined by
straight lines to an existing plot.
</p>
<p>The reason for defining a new variable (hh) with heights rather than using
the original height vector is twofold. First, the relation between height
andweight is a quadratic one and hence nonlinear, although it can be diffi-
cult to see on the plot. Since we are approximating a nonlinear curve with
a piecewise linear one, it will be better to use points that are spread evenly
along the x-axis than to rely on the distribution of the original data. Sec-</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 9
</p>
<p>ond, since the values of height are not sorted, the line segments would
not connect neighbouring points but would run back and forth between
distant points.
</p>
<p>1.2 R language essentials
</p>
<p>This section outlines the basic aspects of the R language. It is necessary
to do this in a slightly superficial manner, with some of the finer points
glossed over. The emphasis is on items that are useful to know in interac-
tive usage as opposed to actual programming, although a brief section on
programming is included.
</p>
<p>1.2.1 Expressions and objects
</p>
<p>The basic interaction mode in R is one of expression evaluation. The user
enters an expression; the system evaluates it and prints the result. Some
expressions are evaluated not for their result but for side effects such as
</p>
<p>1.65 1.70 1.75 1.80 1.85 1.90
</p>
<p>6
0
</p>
<p>7
0
</p>
<p>8
0
</p>
<p>9
0
</p>
<p>height
</p>
<p>w
e
ig
</p>
<p>h
t
</p>
<p>Figure 1.3. Plot with pch = 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1. Basics
</p>
<p>1.65 1.70 1.75 1.80 1.85 1.90
</p>
<p>6
0
</p>
<p>7
0
</p>
<p>8
0
</p>
<p>9
0
</p>
<p>height
</p>
<p>w
e
ig
</p>
<p>h
t
</p>
<p>Figure 1.4. Superimposed reference curve, using lines(...).
</p>
<p>putting up a graphics window or writing to a file. All R expressions return
a value (possibly NULL), but sometimes it is &ldquo;invisible&rdquo; and not printed.
</p>
<p>Expressions typically involve variable references, operators such as +, and
function calls, as well as some other items that have not been introduced
yet.
</p>
<p>Expressions work on objects. This is an abstract term for anything that can
be assigned to a variable. R contains several different types of objects. So
far, we have almost exclusively seen numeric vectors, but several other
types are introduced in this chapter.
</p>
<p>Although objects can be discussed abstractly, it would make a rather bor-
ing read without some indication of how to generate them and what to do
with them. Conversely, much of the expression syntax makes little sense
without knowledge of the objects on which it is intended to work. There-
fore, the subsequent sections alternate between introducing new objects
and introducing new language elements.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 11
</p>
<p>1.2.2 Functions and arguments
</p>
<p>At this point, you have obtained an impression of the way R works, and
we have already used some of the special terminology when talking about
the plot function, etc. That is exactly the point: Many things in R are done
using function calls, commands that look like an application of a math-
ematical function of one or several variables; for example, log(x) or
plot(height, weight).
</p>
<p>The format is that a function name is followed by a set of parentheses con-
taining one or more arguments. For instance, in plot(height,weight)
the function name is plot and the arguments are height and weight.
These are the actual arguments, which apply only to the current call. A func-
tion also has formal arguments, which get connected to actual arguments in
the call.
</p>
<p>When youwrite plot(height, weight), R assumes that the first argu-
ment corresponds to the x-variable and the second one to the y-variable.
This is known as positional matching. This becomes unwieldy if a func-
tion has a large number of arguments since you have to supply every
one of them and remember their position in the sequence. Fortunately,
R has methods to avoid this: Most arguments have sensible defaults and
can be omitted in the standard cases, and there are nonpositional ways of
specifying them when you need to depart from the default settings.
</p>
<p>The plot function is in fact an example of a function that has a large
selection of arguments in order to be able to modify symbols, line
widths, titles, axis type, and so forth. We used the alternative form of
specifying arguments when setting the plot symbol to triangles with
plot(height, weight, pch=2).
</p>
<p>The pch=2 form is known as a named actual argument, whose name can
be matched against the formal arguments of the function and thereby
allow keyword matching of arguments. The keyword pch was used to
say that the argument is a specification of the plotting character. This
type of function argument can be specified in arbitrary order. Thus, you
can write plot(y=weight,x=height) and get the same plot as with
plot(x=height,y=weight).
</p>
<p>The two kinds of argument specification &mdash; positional and named &mdash; can
be mixed in the same call.
</p>
<p>Even if there are no arguments to a function call, you have to write, for
example, ls() for displaying the contents of the workspace. A common
error is to leave off the parentheses, which instead results in the display of
a piece of R code since ls entered by itself indicates that you want to see
the definition of the function rather than execute it.</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1. Basics
</p>
<p>The formal arguments of a function are part of the function definition. The
set of formal arguments to a function, for instance plot.default (which
is the function that gets called when you pass plot an x argument for
which no special plot method exists), may be seen with
</p>
<p>&gt; args(plot.default)
</p>
<p>function (x, y = NULL, type = "p", xlim = NULL, ylim = NULL,
</p>
<p>log = "", main = NULL, sub = NULL, xlab = NULL, ylab = NULL,
</p>
<p>ann = par("ann"), axes = TRUE, frame.plot = axes,
</p>
<p>panel.first = NULL, panel.last = NULL, asp = NA, ...)
</p>
<p>Notice that most of the arguments have defaults, meaning that if you do
not specify (say) the type argument, the function will behave as if you
had passed type="p". The NULL defaults for many of the arguments re-
ally serve as indicators that the argument is unspecified, allowing special
behaviour to be defined inside the function. For instance, if they are not
specified, the xlab and ylab arguments are constructed from the actual
arguments passed as x and y. (There are some very fine points associated
with this, but we do not go further into the topic.)
</p>
<p>The triple-dot (...) argument indicates that this function will accept
additional arguments of unspecified name and number. These are of-
ten meant to be passed on to other functions, although some functions
treat it specially. For instance, in data.frame and c, the names of the
...-arguments become the names of the elements of the result.
</p>
<p>1.2.3 Vectors
</p>
<p>We have already seen numeric vectors. There are two further types,
character vectors and logical vectors.
</p>
<p>A character vector is a vector of text strings, whose elements are specified
and printed in quotes:
</p>
<p>&gt; c("Huey","Dewey","Louie")
</p>
<p>[1] "Huey" "Dewey" "Louie"
</p>
<p>It does not matter whether you use single- or double-quote symbols, as
long as the left quote is the same as the right quote:
</p>
<p>&gt; c(&rsquo;Huey&rsquo;,&rsquo;Dewey&rsquo;,&rsquo;Louie&rsquo;)
</p>
<p>[1] "Huey" "Dewey" "Louie"
</p>
<p>However, you should avoid the acute accent key (&acute;), which is present on
some keyboards. Double quotes are used throughout this book to prevent
mistakes. Logical vectors can take the value TRUE or FALSE (or NA; see
below). In input, youmay use the convenient abbreviations T and F (if you</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 13
</p>
<p>are careful not to redefine them). Logical vectors are constructed using the
c function just like the other vector types:
</p>
<p>&gt; c(T,T,F,T)
</p>
<p>[1] TRUE TRUE FALSE TRUE
</p>
<p>Actually, you will not often have to specify logical vectors in the manner
above. It is much more common to use single logical values to turn an
option on or off in a function call. Vectors of more than one value most
often result from relational expressions:
</p>
<p>&gt; bmi &gt; 25
</p>
<p>[1] FALSE FALSE FALSE FALSE TRUE FALSE
</p>
<p>We return to relational expressions and logical operations in the context
of conditional selection in Section 1.2.12.
</p>
<p>1.2.4 Quoting and escape sequences
</p>
<p>Quoted character strings require some special considerations: How, for
instance, do you put a quote symbol inside a string? And what about spe-
cial characters such as newlines? This is done using escape sequences. We
shall look at those in a moment, but first it will be useful to observe the
following.
</p>
<p>There is a distinction between a text string and theway it is printed.When,
for instance, you give the string "Huey", it is a string of four characters,
not six. The quotes are not actually part of the string, they are just there
so that the system can tell the difference between a string and a variable
name.
</p>
<p>If you print a character vector, it usually comes out with quotes added to
each element. There is a way to avoid this, namely to use the cat function.
For instance,
</p>
<p>&gt; cat(c("Huey","Dewey","Louie"))
</p>
<p>Huey Dewey Louie&gt;
</p>
<p>This prints the strings without quotes, just separated by a space character.
There is no newline following the string, so the prompt (&gt;) for the next
line of input follows directly at the end of the line. (Notice that when the
character vector is printed by cat there is no way of telling the difference
from the single string "Huey Dewey Louie".)
</p>
<p>To get the system prompt onto the next line, you must include a newline
character</p>
<p/>
</div>
<div class="page"><p/>
<p>14 1. Basics
</p>
<p>&gt; cat("Huey","Dewey","Louie", "\n")
</p>
<p>Huey Dewey Louie
</p>
<p>&gt;
</p>
<p>Here, \n is an example of an escape sequence. It actually represents a sin-
gle character, the linefeed (LF), but is represented as two. The backslash
(\) is known as the escape character. In a similar vein, you can insert quote
characters with \", as in
</p>
<p>&gt; cat("What is \"R\"?\n")
</p>
<p>What is "R"?
</p>
<p>There are also ways to insert other control characters and special glyphs,
but it would lead us too far astray to discuss it in full detail. One impor-
tant thing, though: What about the escape character itself? This, too, must
be escaped, so to put a backslash in a string, you must double it. This
is important to know when specifying file paths on Windows, see also
Section 2.4.1.
</p>
<p>1.2.5 Missing values
</p>
<p>In practical data analysis, a data point is frequently unavailable (the pa-
tient did not show up, an experiment failed, etc.). Statistical software
needs ways to deal with this. R allows vectors to contain a special NA
value. This value is carried through in computations so that operations on
NA yield NA as the result. There are some special issues associated with the
handling of missing values; we deal with them as we encounter them (see
&ldquo;missing values&rdquo; in the index).
</p>
<p>1.2.6 Functions that create vectors
</p>
<p>Here we introduce three functions, c, seq, and rep, that are used to create
vectors in various situations.
</p>
<p>The first of these, c, has already been introduced. It is short for &ldquo;con-
catenate&rdquo;, joining items end to end, which is exactly what the function
does:
</p>
<p>&gt; c(42,57,12,39,1,3,4)
</p>
<p>[1] 42 57 12 39 1 3 4
</p>
<p>You can also concatenate vectors of more than one element as in
</p>
<p>&gt; x &lt;- c(1, 2, 3)
</p>
<p>&gt; y &lt;- c(10, 20)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 15
</p>
<p>&gt; c(x, y, 5)
</p>
<p>[1] 1 2 3 10 20 5
</p>
<p>However, you do not need to use c to create vectors of length 1. People
sometimes type, for example, c(1), but it is the same as plain 1.
</p>
<p>It is also possible to assign names to the elements. This modifies the way
the vector is printed and is often used for display purposes.
</p>
<p>&gt; x &lt;- c(red="Huey", blue="Dewey", green="Louie")
</p>
<p>&gt; x
</p>
<p>red blue green
</p>
<p>"Huey" "Dewey" "Louie"
</p>
<p>(In this case, it does of course make sense to use c even for single-element
vectors.)
</p>
<p>The names can be extracted or set using names:
</p>
<p>&gt; names(x)
</p>
<p>[1] "red" "blue" "green"
</p>
<p>All elements of a vector have the same type. If you concatenate vectors of
different types, they will be converted to the least &ldquo;restrictive&rdquo; type:
</p>
<p>&gt; c(FALSE, 3)
</p>
<p>[1] 0 3
</p>
<p>&gt; c(pi, "abc")
</p>
<p>[1] "3.14159265358979" "abc"
</p>
<p>&gt; c(FALSE, "abc")
</p>
<p>[1] "FALSE" "abc"
</p>
<p>That is, logical values may be converted to 0/1 or "FALSE"/"TRUE" and
numbers converted to their printed representations.
</p>
<p>The second function, seq (&ldquo;sequence&rdquo;), is used for equidistant series of
numbers. Writing
</p>
<p>&gt; seq(4,9)
</p>
<p>[1] 4 5 6 7 8 9
</p>
<p>yields, as shown, the integers from 4 to 9. If you want a sequence in jumps
of 2, write
</p>
<p>&gt; seq(4,10,2)
</p>
<p>[1] 4 6 8 10
</p>
<p>This kind of vector is frequently needed, particularly for graphics. For ex-
ample, we previously used c(1.65,1.70,1.75,1.80,1.85,1.90) to
define the x-coordinates for a curve, something that could also have been</p>
<p/>
</div>
<div class="page"><p/>
<p>16 1. Basics
</p>
<p>written seq(1.65,1.90,0.05) (the advantage of using seqmight have
been more obvious if the heights had been in steps of 1 cm rather than
5 cm!).
</p>
<p>The case with step size equal to 1 can also be written using a special
syntax:
</p>
<p>&gt; 4:9
</p>
<p>[1] 4 5 6 7 8 9
</p>
<p>The above is exactly the same as seq(4,9), only easier to read.
</p>
<p>The third function, rep (&ldquo;replicate&rdquo;), is used to generate repeated values.
It is used in two variants, depending on whether the second argument is
a vector or a single number:
</p>
<p>&gt; oops &lt;- c(7,9,13)
</p>
<p>&gt; rep(oops,3)
</p>
<p>[1] 7 9 13 7 9 13 7 9 13
</p>
<p>&gt; rep(oops,1:3)
</p>
<p>[1] 7 9 9 13 13 13
</p>
<p>The first of the function calls above repeats the entire vector oops three
times. The second call has the number 3 replaced by a vector with the
three values (1, 2, 3); these values correspond to the elements of the oops
vector, indicating that 7 should be repeated once, 9 twice, and 13 three
times. The rep function is often used for things such as group codes: If it
is known that the first 10 observations are men and the last 15 are women,
you can use
</p>
<p>&gt; rep(1:2,c(10,15))
</p>
<p>[1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
</p>
<p>to form a vector that for each observation indicates whether it is from a
man or a woman.
</p>
<p>The special case where there are equally many replications of each value
can be obtained using the each argument. E.g., rep(1:2,each=10) is
the same as rep(1:2,c(10,10)).
</p>
<p>1.2.7 Matrices and arrays
</p>
<p>A matrix in mathematics is just a two-dimensional array of numbers. Ma-
trices are used for many purposes in theoretical and practical statistics,
but it is not assumed that the reader is familiar with matrix algebra,
so many special operations on matrices, including matrix multiplication,
are skipped. (The document &ldquo;An Introduction to R&rdquo;, which comes with</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 17
</p>
<p>the installation, outlines these items quite well.) However, matrices and
also higher-dimensional arrays do get used for simpler purposes as well,
mainly to hold tables, so an elementary description is in order.
</p>
<p>In R, the matrix notion is extended to elements of any type, so you could
have, for instance, a matrix of character strings. Matrices and arrays are
represented as vectors with dimensions:
</p>
<p>&gt; x &lt;- 1:12
</p>
<p>&gt; dim(x) &lt;- c(3,4)
</p>
<p>&gt; x
</p>
<p>[,1] [,2] [,3] [,4]
</p>
<p>[1,] 1 4 7 10
</p>
<p>[2,] 2 5 8 11
</p>
<p>[3,] 3 6 9 12
</p>
<p>The dim assignment function sets or changes the dimension attribute of x,
causing R to treat the vector of 12 numbers as a 3&times; 4 matrix. Notice that
the storage is column-major; that is, the elements of the first column are
followed by those of the second, etc.
</p>
<p>A convenient way to create matrices is to use the matrix function:
</p>
<p>&gt; matrix(1:12,nrow=3,byrow=T)
</p>
<p>[,1] [,2] [,3] [,4]
</p>
<p>[1,] 1 2 3 4
</p>
<p>[2,] 5 6 7 8
</p>
<p>[3,] 9 10 11 12
</p>
<p>Notice how the byrow=T switch causes thematrix to be filled in a rowwise
fashion rather than columnwise.
</p>
<p>Useful functions that operate on matrices include rownames, colnames,
and the transposition function t (notice the lowercase t as opposed to
uppercase T for TRUE), which turns rows into columns and vice versa:
</p>
<p>&gt; x &lt;- matrix(1:12,nrow=3,byrow=T)
</p>
<p>&gt; rownames(x) &lt;- LETTERS[1:3]
</p>
<p>&gt; x
</p>
<p>[,1] [,2] [,3] [,4]
</p>
<p>A 1 2 3 4
</p>
<p>B 5 6 7 8
</p>
<p>C 9 10 11 12
</p>
<p>&gt; t(x)
</p>
<p>A B C
</p>
<p>[1,] 1 5 9
</p>
<p>[2,] 2 6 10
</p>
<p>[3,] 3 7 11
</p>
<p>[4,] 4 8 12</p>
<p/>
</div>
<div class="page"><p/>
<p>18 1. Basics
</p>
<p>The character vector LETTERS is a built-in variable that contains the cap-
ital letters A&ndash;Z. Similar useful vectors are letters, month.name, and
month.abbwith lowercase letters, month names, and abbreviated month
names.
</p>
<p>You can &ldquo;glue&rdquo; vectors together, columnwise or rowwise, using the cbind
and rbind functions.
</p>
<p>&gt; cbind(A=1:4,B=5:8,C=9:12)
</p>
<p>A B C
</p>
<p>[1,] 1 5 9
</p>
<p>[2,] 2 6 10
</p>
<p>[3,] 3 7 11
</p>
<p>[4,] 4 8 12
</p>
<p>&gt; rbind(A=1:4,B=5:8,C=9:12)
</p>
<p>[,1] [,2] [,3] [,4]
</p>
<p>A 1 2 3 4
</p>
<p>B 5 6 7 8
</p>
<p>C 9 10 11 12
</p>
<p>We return to table operations in Section 4.5, which discusses tabulation of
variables in a data set.
</p>
<p>1.2.8 Factors
</p>
<p>It is common in statistical data to have categorical variables, indicating
some subdivision of data, such as social class, primary diagnosis, tu-
mor stage, Tanner stage of puberty, etc. Typically, these are input using
a numeric code.
</p>
<p>Such variables should be specified as factors in R. This is a data structure
that (among other things) makes it possible to assign meaningful names
to the categories.
</p>
<p>There are analyses where it is essential for R to be able to distinguish
between categorical codes and variables whose values have a direct
numerical meaning (see Chapter 7).
</p>
<p>The terminology is that a factor has a set of levels&mdash;say four levels for con-
creteness. Internally, a four-level factor consists of two items: (a) a vector of
integers between 1 and 4 and (b) a character vector of length 4 containing
strings describing what the four levels are. Let us look at an example:
</p>
<p>&gt; pain &lt;- c(0,3,2,2,1)
</p>
<p>&gt; fpain &lt;- factor(pain,levels=0:3)
</p>
<p>&gt; levels(fpain) &lt;- c("none","mild","medium","severe")</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 19
</p>
<p>The first command creates a numeric vector pain, encoding the pain lev-
els of five patients. We wish to treat this as a categorical variable, so we
create a factor fpain from it using the function factor. This is called
with one argument in addition to pain, namely levels=0:3, which in-
dicates that the input coding uses the values 0&ndash;3. The latter can in principle
be left out since R by default uses the values in pain, suitably sorted, but
it is a good habit to retain it; see below. The effect of the final line is that
the level names are changed to the four specified character strings.
</p>
<p>The result should be apparent from the following:
</p>
<p>&gt; fpain
</p>
<p>[1] none severe medium medium mild
</p>
<p>Levels: none mild medium severe
</p>
<p>&gt; as.numeric(fpain)
</p>
<p>[1] 1 4 3 3 2
</p>
<p>&gt; levels(fpain)
</p>
<p>[1] "none" "mild" "medium" "severe"
</p>
<p>The function as.numeric extracts the numerical coding as numbers
1&ndash;4 and levels extracts the names of the levels. Notice that the origi-
nal input coding in terms of numbers 0&ndash;3 has disappeared; the internal
representation of a factor always uses numbers starting at 1.
</p>
<p>R also allows you to create a special kind of factor in which the lev-
els are ordered. This is done using the ordered function, which works
similarly to factor. These are potentially useful in that they distinguish
nominal and ordinal variables from each other (and arguably text.pain
above ought to have been an ordered factor). Unfortunately, R defaults
to treating the levels as if they were equidistant in the modelling code (by
generating polynomial contrasts), so it may be better to ignore ordered
factors at this stage.
</p>
<p>1.2.9 Lists
</p>
<p>It is sometimes useful to combine a collection of objects into a larger
composite object. This can be done using lists.
</p>
<p>You can construct a list from its components with the function list.
</p>
<p>As an example, consider a set of data from Altman (1991, p. 183) concern-
ing pre- and postmenstrual energy intake in a group of women. We can
place these data in two vectors as follows:
</p>
<p>&gt; intake.pre &lt;- c(5260,5470,5640,6180,6390,
</p>
<p>+ 6515,6805,7515,7515,8230,8770)
</p>
<p>&gt; intake.post &lt;- c(3910,4220,3885,5160,5645,
</p>
<p>+ 4680,5265,5975,6790,6900,7335)</p>
<p/>
</div>
<div class="page"><p/>
<p>20 1. Basics
</p>
<p>Notice how input lines can be broken and continue on the next line. If
you press the Enter key while an expression is syntactically incomplete, R
will assume that the expression continues on the next line and will change
its normal &gt; prompt to the continuation prompt +. This often happens in-
advertently due to a forgotten parenthesis or a similar problem; in such
cases, either complete the expression on the next line or press ESC (Win-
dows andMacintosh) or Ctrl-C (Unix). The &ldquo;Stop&rdquo; button can also be used
under Windows.
</p>
<p>To combine these individual vectors into a list, you can say
</p>
<p>&gt; mylist &lt;- list(before=intake.pre,after=intake.post)
</p>
<p>&gt; mylist
</p>
<p>$before
</p>
<p>[1] 5260 5470 5640 6180 6390 6515 6805 7515 7515 8230 8770
</p>
<p>$after
</p>
<p>[1] 3910 4220 3885 5160 5645 4680 5265 5975 6790 6900 7335
</p>
<p>The components of the list are named according to the argument names
used in list. Named components may be extracted like this:
</p>
<p>&gt; mylist$before
</p>
<p>[1] 5260 5470 5640 6180 6390 6515 6805 7515 7515 8230 8770
</p>
<p>Many ofR&rsquo;s built-in functions computemore than a single vector of values
and return their results in the form of a list.
</p>
<p>1.2.10 Data frames
</p>
<p>A data frame corresponds to what other statistical packages call a &ldquo;data
matrix&rdquo; or a &ldquo;data set&rdquo;. It is a list of vectors and/or factors of the same
length that are related &ldquo;across&rdquo; such that data in the same position come
from the same experimental unit (subject, animal, etc.). In addition, it has
a unique set of row names.
</p>
<p>You can create data frames from preexisting variables:
</p>
<p>&gt; d &lt;- data.frame(intake.pre,intake.post)
</p>
<p>&gt; d
</p>
<p>intake.pre intake.post
</p>
<p>1 5260 3910
</p>
<p>2 5470 4220
</p>
<p>3 5640 3885
</p>
<p>4 6180 5160
</p>
<p>5 6390 5645
</p>
<p>6 6515 4680
</p>
<p>7 6805 5265</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 21
</p>
<p>8 7515 5975
</p>
<p>9 7515 6790
</p>
<p>10 8230 6900
</p>
<p>11 8770 7335
</p>
<p>Notice that these data are paired, that is, the same woman has an intake
of 5260 kJ premenstrually and 3910 kJ postmenstrually.
</p>
<p>As with lists, components (i.e., individual variables) can be accessed using
the $ notation:
</p>
<p>&gt; d$intake.pre
</p>
<p>[1] 5260 5470 5640 6180 6390 6515 6805 7515 7515 8230 8770
</p>
<p>1.2.11 Indexing
</p>
<p>If you need a particular element in a vector, for instance the premenstrual
energy intake for woman no. 5, you can do
</p>
<p>&gt; intake.pre[5]
</p>
<p>[1] 6390
</p>
<p>The brackets are used for selection of data, also known as indexing or sub-
setting. This also works on the left-hand side of an assignment (so that you
can say, for instance, intake.pre[5] &lt;- 6390) if you want to modify
elements of a vector.
</p>
<p>If you want a subvector consisting of data for more than one woman, for
instance nos. 3, 5, and 7, you can index with a vector:
</p>
<p>&gt; intake.pre[c(3,5,7)]
</p>
<p>[1] 5640 6390 6805
</p>
<p>Note that it is necessary to use the c(...)-construction to define the vec-
tor consisting of the three numbers 3, 5, and 7. intake.pre[3,5,7]
would mean something completely different. It would specify indexing
into a three-dimensional array.
</p>
<p>Of course, indexing with a vector also works if the index vector is stored
in a variable. This is useful when you need to index several variables in
the same way.
</p>
<p>&gt; v &lt;- c(3,5,7)
</p>
<p>&gt; intake.pre[v]
</p>
<p>[1] 5640 6390 6805
</p>
<p>It is also worth noting that to get a sequence of elements, for instance the
first five, you can use the a:b notation:</p>
<p/>
</div>
<div class="page"><p/>
<p>22 1. Basics
</p>
<p>&gt; intake.pre[1:5]
</p>
<p>[1] 5260 5470 5640 6180 6390
</p>
<p>A neat feature of R is the possibility of negative indexing. You can get all
observations except nos. 3, 5, and 7 by writing
</p>
<p>&gt; intake.pre[-c(3,5,7)]
</p>
<p>[1] 5260 5470 6180 6515 7515 7515 8230 8770
</p>
<p>It is not possible to mix positive and negative indices. That would be
highly ambiguous.
</p>
<p>1.2.12 Conditional selection
</p>
<p>We saw in Section 1.2.11 how to extract data using one or several indices.
In practice, you often need to extract data that satisfy certain criteria, such
as data from the males or the prepubertal or those with chronic diseases,
etc. This can be done simply by inserting a relational expression instead
of the index,
</p>
<p>&gt; intake.post[intake.pre &gt; 7000]
</p>
<p>[1] 5975 6790 6900 7335
</p>
<p>yielding the postmenstrual energy intake for the four women who had an
energy intake above 7000 kJ premenstrually.
</p>
<p>Of course, this kind of expression makes sense only if the variables that go
into the relational expression have the same length as the variable being
indexed.
</p>
<p>The comparison operators available are &lt; (less than), &gt; (greater than), ==
(equal to), &lt;= (less than or equal to), &gt;= (greater than or equal to), and !=
(not equal to). Notice that a double equal sign is used for testing equality.
This is to avoid confusionwith the = symbol used tomatch keywords with
function arguments. Also, the != operator is new to some; the ! symbol
indicates negation. The same operators are used in the C programming
language.
</p>
<p>To combine several expressions, you can use the logical operators &amp; (log-
ical &ldquo;and&rdquo;), | (logical &ldquo;or&rdquo;), and ! (logical &ldquo;not&rdquo;). For instance, we find
the postmenstrual intake for women with a premenstrual intake between
7000 and 8000 kJ with
</p>
<p>&gt; intake.post[intake.pre &gt; 7000 &amp; intake.pre &lt;= 8000]
</p>
<p>[1] 5975 6790</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 23
</p>
<p>There are also &amp;&amp; and ||, which are used for flow control in R
programming. However, their use is beyond what we discuss here.
</p>
<p>It may be worth taking a closer look at what actually happens when you
use a logical expression as an index. The result of the logical expression is
a logical vector as described in Section 1.2.3:
</p>
<p>&gt; intake.pre &gt; 7000 &amp; intake.pre &lt;= 8000
</p>
<p>[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE
</p>
<p>[11] FALSE
</p>
<p>Indexing with a logical vector implies that you pick out the values where
the logical vector is TRUE, so in the preceding example we got the 8th and
9th values in intake.post.
</p>
<p>If missing values (NA; see Section 1.2.5) appear in an indexing vector, then
R will create the corresponding elements in the result but set the values to
NA.
</p>
<p>In addition to the relational and logical operators, there are a series of
functions that return a logical value. A particularly important one is
is.na(x), which is used to find out which elements of x are recorded
as missing (NA).
</p>
<p>Notice that there is a real need for is.na because you cannot make
comparisons of the form x==NA. That simply gives NA as the result for
any value of x. The result of a comparison with an unknown value is
unknown!
</p>
<p>1.2.13 Indexing of data frames
</p>
<p>We have already seen how it is possible to extract variables from a
data frame by typing, for example, d$intake.post. However, it is also
possible to use a notation that uses the matrix-like structure directly:
</p>
<p>&gt; d &lt;- data.frame(intake.pre,intake.post)
</p>
<p>&gt; d[5,1]
</p>
<p>[1] 6390
</p>
<p>gives fifth row, first column (that is, the &ldquo;pre&rdquo; measurement for woman
no. 5), and
</p>
<p>&gt; d[5,]
</p>
<p>intake.pre intake.post
</p>
<p>5 6390 5645
</p>
<p>gives allmeasurements for woman no. 5. Notice that the comma in d[5,]
is required; without the comma, for example d[2], you get the data frame</p>
<p/>
</div>
<div class="page"><p/>
<p>24 1. Basics
</p>
<p>consisting of the second column of d (that is, more like d[,2], which is the
column itself).
</p>
<p>Other indexing techniques also apply. In particular, it can be useful to ex-
tract all data for cases that satisfy some criterion, such as women with a
premenstrual intake above 7000 kJ:
</p>
<p>&gt; d[d$intake.pre&gt;7000,]
</p>
<p>intake.pre intake.post
</p>
<p>8 7515 5975
</p>
<p>9 7515 6790
</p>
<p>10 8230 6900
</p>
<p>11 8770 7335
</p>
<p>Here we extracted the rows of the data frame where intake.pre&gt;7000.
Notice that the row names are those of the original data frame.
</p>
<p>If you want to understand the details of this, it may be a little easier if it is
divided into smaller steps. It could also have been done like this:
</p>
<p>&gt; sel &lt;- d$intake.pre&gt;7000
</p>
<p>&gt; sel
</p>
<p>[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE
</p>
<p>[11] TRUE
</p>
<p>&gt; d[sel,]
</p>
<p>intake.pre intake.post
</p>
<p>8 7515 5975
</p>
<p>9 7515 6790
</p>
<p>10 8230 6900
</p>
<p>11 8770 7335
</p>
<p>What happens is that sel (select) becomes a logical vector with the value
TRUE for to the four women consumingmore than 7000 kJ premenstrually.
Indexing as d[sel,] yields data from the rows where sel is TRUE and
from all columns because of the empty field after the comma.
</p>
<p>It is often convenient to look at the first few cases in a data set. This can be
done with indexing, like this:
</p>
<p>&gt; d[1:2,]
</p>
<p>intake.pre intake.post
</p>
<p>1 5260 3910
</p>
<p>2 5470 4220
</p>
<p>This is such a frequent occurrence that a convenience function called head
exists. By default, it shows the first six lines.
</p>
<p>&gt; head(d)
</p>
<p>intake.pre intake.post
</p>
<p>1 5260 3910
</p>
<p>2 5470 4220</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 25
</p>
<p>3 5640 3885
</p>
<p>4 6180 5160
</p>
<p>5 6390 5645
</p>
<p>6 6515 4680
</p>
<p>Similarly, tail shows the last part.
</p>
<p>1.2.14 Grouped data and data frames
</p>
<p>The natural way of storing grouped data in a data frame is to have the data
themselves in one vector and parallel to that have a factor telling which
data are from which group. Consider, for instance, the following data set
on energy expenditure for lean and obese women.
</p>
<p>&gt; energy
</p>
<p>expend stature
</p>
<p>1 9.21 obese
</p>
<p>2 7.53 lean
</p>
<p>3 7.48 lean
</p>
<p>4 8.08 lean
</p>
<p>5 8.09 lean
</p>
<p>6 10.15 lean
</p>
<p>7 8.40 lean
</p>
<p>8 10.88 lean
</p>
<p>9 6.13 lean
</p>
<p>10 7.90 lean
</p>
<p>11 11.51 obese
</p>
<p>12 12.79 obese
</p>
<p>13 7.05 lean
</p>
<p>14 11.85 obese
</p>
<p>15 9.97 obese
</p>
<p>16 7.48 lean
</p>
<p>17 8.79 obese
</p>
<p>18 9.69 obese
</p>
<p>19 9.68 obese
</p>
<p>20 7.58 lean
</p>
<p>21 9.19 obese
</p>
<p>22 8.11 lean
</p>
<p>This is a convenient format since it generalizes easily to data classified
by multiple criteria. However, sometimes it is desirable to have data in a
separate vector for each group. Fortunately, it is easy to extract these from
the data frame:
</p>
<p>&gt; exp.lean &lt;- energy$expend[energy$stature=="lean"]
</p>
<p>&gt; exp.obese &lt;- energy$expend[energy$stature=="obese"]
</p>
<p>Alternatively, you can use the split function, which generates a list of
vectors according to a grouping.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 1. Basics
</p>
<p>&gt; l &lt;- split(energy$expend, energy$stature)
</p>
<p>&gt; l
</p>
<p>$lean
</p>
<p>[1] 7.53 7.48 8.08 8.09 10.15 8.40 10.88 6.13 7.90 7.05
</p>
<p>[11] 7.48 7.58 8.11
</p>
<p>$obese
</p>
<p>[1] 9.21 11.51 12.79 11.85 9.97 8.79 9.69 9.68 9.19
</p>
<p>1.2.15 Implicit loops
</p>
<p>The looping constructs ofR are described in Section 2.3.1. For the purposes
of this book, you can largely ignore their existence. However, there is a
group of R functions that it will be useful for you to know about.
</p>
<p>A common application of loops is to apply a function to each element of
a set of values or vectors and collect the results in a single structure. In
R this is abstracted by the functions lapply and sapply. The former
always returns a list (hence the &lsquo;l&rsquo;), whereas the latter tries to simplify
(hence the &lsquo;s&rsquo;) the result to a vector or a matrix if possible. So, to compute
the mean of each variable in a data frame of numeric vectors, you can do
the following:
</p>
<p>&gt; lapply(thuesen, mean, na.rm=T)
</p>
<p>$blood.glucose
</p>
<p>[1] 10.3
</p>
<p>$short.velocity
</p>
<p>[1] 1.325652
</p>
<p>&gt; sapply(thuesen, mean, na.rm=T)
</p>
<p>blood.glucose short.velocity
</p>
<p>10.300000 1.325652
</p>
<p>Notice how both forms attach meaningful names to the result, which
is another good reason to prefer to use these functions rather than ex-
plicit loops. The second argument to lapply/sapply is the function that
should be applied, here mean. Any further arguments are passed on to the
function; in this case we pass na.rm=T to request that missing values be
removed (see Section 4.1).
</p>
<p>Sometimes you just want to repeat something a number of times but still
collect the results as a vector. Obviously, this makes sense only when the
repeated computations actually give different results, the common case
being simulation studies. This can be done using sapply, but there is a
simplified version called replicate, in which you just have to give a
count and the expression to evaluate:</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 R language essentials 27
</p>
<p>&gt; replicate(10,mean(rexp(20)))
</p>
<p>[1] 1.0677019 1.2166898 0.8923216 1.1281207 0.9636017 0.8406877
</p>
<p>[7] 1.3357814 0.8249408 0.9488707 0.5724575
</p>
<p>A similar function, apply, allows you to apply a function to the rows
or columns of a matrix (or over indices of a multidimensional array in
general) as in
</p>
<p>&gt; m &lt;- matrix(rnorm(12),4)
</p>
<p>&gt; m
</p>
<p>[,1] [,2] [,3]
</p>
<p>[1,] -2.5710730 0.2524470 -0.16886795
</p>
<p>[2,] 0.5509498 1.5430648 0.05359794
</p>
<p>[3,] 2.4002722 0.1624704 -1.23407417
</p>
<p>[4,] 1.4791103 0.9484525 -0.84670929
</p>
<p>&gt; apply(m, 2, min)
</p>
<p>[1] -2.5710730 0.1624704 -1.2340742
</p>
<p>The second argument is the index (or vector of indices) that defines what
the function is applied to; in this case we get the columnwise minima.
</p>
<p>Also, the function tapply allows you to create tables (hence the &lsquo;t&rsquo;) of the
value of a function on subgroups defined by its second argument, which
can be a factor or a list of factors. In the latter case a cross-classified table
is generated. (The grouping can also be defined by ordinary vectors. They
will be converted to factors internally.)
</p>
<p>&gt; tapply(energy$expend, energy$stature, median)
</p>
<p>lean obese
</p>
<p>7.90 9.69
</p>
<p>1.2.16 Sorting
</p>
<p>It is trivial to sort a vector. Just use the sort function. (We use the built-
in data set intake here; it contains the same data that were used in
Section 1.2.9.)
</p>
<p>&gt; intake$post
</p>
<p>[1] 3910 4220 3885 5160 5645 4680 5265 5975 6790 6900 7335
</p>
<p>&gt; sort(intake$post)
</p>
<p>[1] 3885 3910 4220 4680 5160 5265 5645 5975 6790 6900 7335
</p>
<p>(intake$pre could not be used for this example since it is sorted
already!)
</p>
<p>However, sorting a single vector is not always what is required. Often
you need to sort a series of variables according to the values of some other
variables &mdash; blood pressures sorted by sex and age, for instance. For this</p>
<p/>
</div>
<div class="page"><p/>
<p>28 1. Basics
</p>
<p>purpose, there is a construction that may look somewhat abstract at first
but is really very powerful. You first compute an ordering of a variable.
</p>
<p>&gt; order(intake$post)
</p>
<p>[1] 3 1 2 6 4 7 5 8 9 10 11
</p>
<p>The result is the numbers 1 to 11 (or whatever the length of the vec-
tor is), sorted according to the size of the argument to order (here
intake$post). Interpreting the result of order is a bit tricky&mdash; it should
be read as follows: You sort intake$post by placing its values in the
order no. 3, no. 1, no. 2, no. 6, etc.
</p>
<p>The point is that, by indexing with this vector, other variables can be
sorted by the same criterion. Note that indexing with a vector containing
the numbers from 1 to the number of elements exactly once corresponds
to a reordering of the elements.
</p>
<p>&gt; o &lt;- order(intake$post)
</p>
<p>&gt; intake$post[o]
</p>
<p>[1] 3885 3910 4220 4680 5160 5265 5645 5975 6790 6900 7335
</p>
<p>&gt; intake$pre[o]
</p>
<p>[1] 5640 5260 5470 6515 6180 6805 6390 7515 7515 8230 8770
</p>
<p>What has happened here is that intake$post has been sorted &mdash; just as
in sort(intake$post) &mdash; while intake$pre has been sorted by the
size of the corresponding intake$post.
</p>
<p>It is of course also possible to sort the entire data frame intake
</p>
<p>&gt; intake.sorted &lt;- intake[o,]
</p>
<p>Sorting by several criteria is done simply by having several arguments to
order; for instance, order(sex,age)will give amain division intomen
and women, and within each sex an ordering by age. The second variable
is used when the order cannot be decided from the first variable. Sorting
in reverse order can be handled by, for example, changing the sign of the
variable.
</p>
<p>1.3 Exercises
</p>
<p>1.1 How would you check whether two vectors are the same if they
may contain missing (NA) values? (Use of the identical function is
considered cheating!)
</p>
<p>1.2 If x is a factor with n levels and y is a length n vector, what happens
if you compute y[x]?</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Exercises 29
</p>
<p>1.3 Write the logical expression to use to extract girls between 7 and 14
years of age in the juul data set.
</p>
<p>1.4 What happens if you change the levels of a factor (with levels) and
give the same value to two or more levels?
</p>
<p>1.5 On p. 27, replicate was used to simulate the distribution of the
mean of 20 random numbers from the exponential distribution by re-
peating the operation 10 times. How would you do the same thing with
sapply?</p>
<p/>
</div>
<div class="page"><p/>
<p>2
The R environment
</p>
<p>This chapter collects some practical aspects of working with R. It de-
scribes issues regarding the structure of the workspace, graphical devices
and their parameters, and elementary programming, and includes a fairly
extensive, although far from complete, discussion of data entry.
</p>
<p>2.1 Session management
</p>
<p>2.1.1 The workspace
</p>
<p>All variables created in R are stored in a commonworkspace. To see which
variables are defined in the workspace, you can use the function ls (list).
It should look as follows if you have run all the examples in the preceding
chapter:
</p>
<p>&gt; ls()
</p>
<p>[1] "bmi" "d" "exp.lean"
</p>
<p>[4] "exp.obese" "fpain" "height"
</p>
<p>[7] "hh" "intake.post" "intake.pre"
</p>
<p>[10] "intake.sorted" "l" "m"
</p>
<p>[13] "mylist" "o" "oops"
</p>
<p>[16] "pain" "sel" "v"
</p>
<p>[19] "weight" "x" "xbar"
</p>
<p>[22] "y"
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_2, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>32 2. The R environment
</p>
<p>Remember that you cannot omit the parentheses in ls().
</p>
<p>If at some point things begin to look messy, you can delete some of the
objects. This is done using rm (remove), so that
</p>
<p>&gt; rm(height, weight)
</p>
<p>deletes the variables height and weight.
</p>
<p>The entire workspace can be cleared using rm(list=ls()) and also via
the &ldquo;Remove all objects&rdquo; or &ldquo;Clear Workspace&rdquo; menu entries in the Win-
dows and Macintosh GUIs. This does not remove variables whose name
begins with a dot because they are not listed by ls()&mdash; you would need
ls(all=T) for that, but it could be dangerous because such names are
used for system purposes.
</p>
<p>If you are acquainted with the Unix operating system, for which the S lan-
guage, which preceded R, was originally written, then you will know that
the commands for listing and removing files in Unix are called precisely
ls and rm.
</p>
<p>It is possible to save the workspace to a file at any time. If you just write
</p>
<p>save.image()
</p>
<p>then it will be saved to a file called .RData in your working directory.
The Windows version also has this on the File menu. When you exit R,
you will be asked whether to save the workspace image; if you accept,
the same thing will happen. It is also possible to specify an alternative
filename (within quotes). You can also save selected objects with save.
The .RData file is loaded by default when R is started in its directory.
Other save files can be loaded into your current workspace using load.
</p>
<p>2.1.2 Textual output
</p>
<p>It is important to note that the workspace consists only of R objects, not of
any of the output that you have generated during a session. If you want
to save your output, use &ldquo;Save to File&rdquo; from the File menu in Windows or
use standard cut-and-paste facilities. You can also use ESS (Emacs Speaks
Statistics), which works on all platforms. It is a &ldquo;mode&rdquo; for the Emacs
editor where you can run your entire session in an Emacs buffer. You can
get ESS and installation instructions for it from CRAN (see Appendix A).
</p>
<p>An alternative way of diverting output to a file is to use the sink func-
tion. This is largely a relic from the days of the 80&times; 25 computer terminal,
where cut-and-paste techniques were not available, but it can still be use-</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Session management 33
</p>
<p>ful at times. In particular, it can be used in batch processing. The way it
works is as follows:
</p>
<p>&gt; sink("myfile")
</p>
<p>&gt; ls()
</p>
<p>No output appears! This is because the output goes into the file myfile in
the current directory. The system will remain in a state where commands
are processed, but the output (apparently) goes into the drain until the
normal state of affairs is reestablished by
</p>
<p>&gt; sink()
</p>
<p>The current working directory can be obtained by getwd() and changed
by setwd(mydir), where mydir is a character string. The initial working
directory is system-dependent; for instance, the Windows GUI sets it to
the user&rsquo;s home directory, and command line versions use the directory
from which you start R.
</p>
<p>2.1.3 Scripting
</p>
<p>Beyond a certain level of complexity, you will not want to work with R on
a line-by-line basis. For instance, if you have entered an 8&times; 8 matrix over
eight lines and realize that you made a mistake, you will find yourself
using the up-arrow key 64 times to reenter it! In such cases, it is better to
work with R scripts, collections of lines of R code stored either in a file or
in computer memory somehow.
</p>
<p>One option is to use the source function, which is sort of the opposite of
sink. It takes the input (i.e., the commands from a file) and runs them.
Notice, though, that the entire file is syntax-checked before anything is
executed. It is often useful to set echo=T in the call so that commands are
printed along with the output.
</p>
<p>Another option is more interactive in nature. You can work with a script
editor window, which allows you to submit one or more lines of the script
to a running R, which will then behave as if the same lines had been
entered at the prompt. The Windows and Macintosh versions of R have
simple scripting windows built-in, and a number of text editors also have
features for sending commands to R; popular choices on Windows in-
clude TINN-R and WinEdt. This is also available as part of ESS (see the
preceding section).
</p>
<p>The history of commands entered in a session can be saved and reloaded
using the savehistory and loadhistory commands, which are also
mapped to menu entries in Windows. Saved histories can be useful as a</p>
<p/>
</div>
<div class="page"><p/>
<p>34 2. The R environment
</p>
<p>starting point for writing scripts; notice also that the history() function
will show the last commands entered at the console (up to a maximum of
25 lines by default).
</p>
<p>2.1.4 Getting help
</p>
<p>R can do a lot more than what a typical beginner can be expected to need
or even understand. This book is written so that most of the code you are
likely to need in relation to the statistical procedures is described in the
text, and the compendium in Appendix C is designed to provide a basic
overview. However, it is obviously not possible to cover everything.
</p>
<p>R also comes with extensive online help in text form as well as in the form
of a series of HTML files that can be read using a Web browser such as
Netscape or Internet Explorer. The help pages can be accessed via &ldquo;help&rdquo;
in the menu bar on Windows and by entering help.start() on any
platform. You will find that the pages are of a technical nature. Preci-
sion and conciseness here take precedence over readability and pedagogy
(something one learns to appreciate after exposure to the opposite).
</p>
<p>From the command line, you can always enter help(aggregate) to get
help on the aggregate function or use the prefix form ?aggregate. If
the HTML viewer is running, then the help page is shown there. Other-
wise it is shown as text either through a pager to the terminal window or
in a separate window.
</p>
<p>Notice that the HTML version of the help system features a very use-
ful &ldquo;Search Engine and Keywords&rdquo; and that the apropos function
allows you to get a list of command names that contain a given pat-
tern. The function help.search is similar but uses fuzzy matching and
searches deeper into the help pages, so that it will be able to locate,
for example, Kendall&rsquo;s correlation coefficient in cor.test if you use
help.search("kendal").
</p>
<p>Also available with the R distributions is a set of documents in various
formats. Of particular interest is &ldquo;An Introduction to R&rdquo;, originally based
on a set of notes for S-PLUS by Bill Venables and David Smith and modi-
fied for R by various people. It contains an introduction to the R language
and environment in a rather more language-centric fashion than this book.
On the Windows platform, you can choose to install PDF documents as
part of the installation procedure so that &mdash; provided the Adobe Acrobat
Reader program is also installed &mdash; it can be accessed via the Help menu.
An HTML version (without pictures) can be accessed via the browser
interface on all platforms.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Session management 35
</p>
<p>2.1.5 Packages
</p>
<p>An R installation contains one or more libraries of packages. Some of these
packages are part of the basic installation. Others can be downloaded from
CRAN (see Appendix A), which currently hosts over 1000 packages for
various purposes. You can even create your own packages.
</p>
<p>A library is generally just a folder on your disk. A system library is created
when R is installed. In some installations, users may be prohibited from
modifying the system library. It is possible to set up private user libraries;
see help(".Library") for details.
</p>
<p>A package can contain functions written in the R language, dynamically
loaded libraries of compiled code (written in C or Fortran mostly), and
data sets. It generally implements functionality that most users will prob-
ably not need to have loaded all the time. A package is loaded into R using
the library command, so to load the survival package you should
enter
</p>
<p>&gt; library(survival)
</p>
<p>The loaded packages are not considered part of the user workspace. If
you terminate your R session and start a new session with the saved
workspace, then you will have to load the packages again. For the same
reason, it is rarely necessary to remove a package that you have loaded,
but it can be done if desired with
</p>
<p>&gt; detach("package:survival")
</p>
<p>(see also Section 2.1.7).
</p>
<p>2.1.6 Built-in data
</p>
<p>Many packages, both inside and outside the standard R distribution, come
with built-in data sets. Such data sets can be rather large, so it is not a
good idea to keep them all in computer memory at all times. A mecha-
nism for on-demand loading is required. In many packages, this works
via a mechanism called lazy loading, which allows the system to &ldquo;pretend&rdquo;
that the data are in memory, but in fact they are not loaded until they are
referenced for the first time.
</p>
<p>With this mechanism, data are &ldquo;just there&rdquo;. For example, if you type &ldquo;thue-
sen&rdquo;, the data frame of that name is displayed. Some packages still require
explicit calls to the data function. Most often, this loads a data frame with
the name that its argument specifies; data(thuesen) will, for instance,
load the thuesen data frame.</p>
<p/>
</div>
<div class="page"><p/>
<p>36 2. The R environment
</p>
<p>What data does is to go through the data directories associated with each
package (see Section 2.1.5) and look for files whose basename matches the
given name. Depending on the file extension, several things can then hap-
pen. Files with a .tab extension are read using read.table (Section 2.4),
whereas files with a .R extension are executed as source files (and could,
in general, do anything!), to give two common examples.
</p>
<p>If there is a subdirectory of the current directory called data, then it
is searched as well. This can be quite a handy way of organizing your
personal projects.
</p>
<p>2.1.7 attach and detach
</p>
<p>The notation for accessing variables in data frames gets rather heavy if
you repeatedly have to write longish commands like
</p>
<p>plot(thuesen$blood.glucose,thuesen$short.velocity)
</p>
<p>Fortunately, you can make R look for objects among the variables in a
given data frame, for example thuesen. You write
</p>
<p>&gt; attach(thuesen)
</p>
<p>and then thuesen&rsquo;s data are available without the clumsy $-notation:
</p>
<p>&gt; blood.glucose
</p>
<p>[1] 15.3 10.8 8.1 19.5 7.2 5.3 9.3 11.1 7.5 12.2 6.7 5.2
</p>
<p>[13] 19.0 15.1 6.7 8.6 4.2 10.3 12.5 16.1 13.3 4.9 8.8 9.5
</p>
<p>What happens is that the data frame thuesen is placed in the system&rsquo;s
search path. You can view the search path with search:
</p>
<p>&gt; search()
</p>
<p>[1] ".GlobalEnv" "thuesen" "package:ISwR"
</p>
<p>[4] "package:stats" "package:graphics" "package:grDevices"
</p>
<p>[7] "package:utils" "package:datasets" "package:methods"
</p>
<p>[10] "Autoloads" "package:base"
</p>
<p>Notice that thuesen is placed as no. 2 in the search path. .GlobalEnv
is the workspace and package:base is the system library where
all standard functions are defined. Autoloads is not described here.
package:stats and onwards contains the basic statistical routines such
as the Wilcoxon test, and the other packages similarly contain vari-
ous functions and data sets. (The package system is modular, and you
can run R with a minimal set of packages for specific uses.) Finally,
package:ISwR contains the data sets used for this book.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Session management 37
</p>
<p>There may be several objects of the same name in different parts of the
search path. In that case, R chooses the first one (that is, it searches first in
.GlobalEnv, then in thuesen, and so forth). For this reason, you need
to be a little careful with &ldquo;loose&rdquo; objects that are defined in the workspace
outside a data frame since they will be used before any vectors and factors
of the same name in an attached data frame. For the same reason, it is not a
good idea to give a data frame the same name as one of the variables inside
it. Note also that changing a data frame after attaching it will not affect the
variables available since attach involves a (virtual) copy operation of the
data frame.
</p>
<p>It is not possible to attach data frames in front of .GlobalEnv or fol-
lowing package:base. However, it is possible to attach more than one
data frame. New data frames are inserted into position 2 by default, and
everything except .GlobalEnv moves one step to the right. It is, how-
ever, possible to specify that a data frame should be searched before
.GlobalEnv by using constructions of the form
</p>
<p>with(thuesen, plot(blood.glucose, short.velocity))
</p>
<p>In some contexts, R uses a slightly different method when looking for ob-
jects. If looking for a variable of a specific type (usually a function), R will
skip those of other types. This is what saves you from the worst conse-
quences of accidentally naming a variable (say) c, even though there is a
system function of the same name.
</p>
<p>You can remove a data frame from the search path with detach. If no
arguments are given, the data frame in position 2 is removed, which is
generally what is desired. .GlobalEnv and package:base cannot be
detach&rsquo;ed.
</p>
<p>&gt; detach()
</p>
<p>&gt; search()
</p>
<p>[1] ".GlobalEnv" "package:ISwR" "package:stats"
</p>
<p>[4] "package:graphics" "package:grDevices" "package:utils"
</p>
<p>[7] "package:datasets" "package:methods" "Autoloads"
</p>
<p>[10] "package:base"
</p>
<p>2.1.8 subset, transform, and within
</p>
<p>You can attach a data frame to avoid the cumbersome indexing of every
variable inside of it. However, this is less helpful for selecting subsets of
data and for creating new data frames with transformed variables. A cou-
ple of functions exist to make these operations easier. They are used as
follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>38 2. The R environment
</p>
<p>&gt; thue2 &lt;- subset(thuesen,blood.glucose&lt;7)
</p>
<p>&gt; thue2
</p>
<p>blood.glucose short.velocity
</p>
<p>6 5.3 1.49
</p>
<p>11 6.7 1.25
</p>
<p>12 5.2 1.19
</p>
<p>15 6.7 1.52
</p>
<p>17 4.2 1.12
</p>
<p>22 4.9 1.03
</p>
<p>&gt; thue3 &lt;- transform(thuesen,log.gluc=log(blood.glucose))
</p>
<p>&gt; thue3
</p>
<p>blood.glucose short.velocity log.gluc
</p>
<p>1 15.3 1.76 2.727853
</p>
<p>2 10.8 1.34 2.379546
</p>
<p>3 8.1 1.27 2.091864
</p>
<p>4 19.5 1.47 2.970414
</p>
<p>5 7.2 1.27 1.974081
</p>
<p>...
</p>
<p>22 4.9 1.03 1.589235
</p>
<p>23 8.8 1.12 2.174752
</p>
<p>24 9.5 1.70 2.251292
</p>
<p>Notice that the variables used in the expressions for new variables or for
subsetting are evaluated with variables taken from the data frame.
</p>
<p>subset also works on single vectors. This is nearly the same as indexing
with a logical vector (such as short.velocity[blood.glucose&lt;7]),
except that observations with missing values in the selection criterion are
excluded.
</p>
<p>subset also has a select argument which can be used to extract
variables from the data frame. We shall return to this in Section 10.3.1.
</p>
<p>The transform function has a couple of drawbacks, the most serious of
which is probably that it does not allow chained calculations where some
of the new variables depend on the others. The = signs in the syntax are
not assignments, but indicate names, which are assigned to the computed
vectors in the last step.
</p>
<p>An alternative to transform is the within function, which can be used
like this:
</p>
<p>&gt; thue4 &lt;- within(thuesen,{
</p>
<p>+ log.gluc &lt;- log(blood.glucose)
</p>
<p>+ m &lt;- mean(log.gluc)
</p>
<p>+ centered.log.gluc &lt;- log.gluc - m
</p>
<p>+ rm(m)
</p>
<p>+ })
</p>
<p>&gt; thue4
</p>
<p>blood.glucose short.velocity centered.log.gluc log.gluc
</p>
<p>1 15.3 1.76 0.481879807 2.727853
</p>
<p>2 10.8 1.34 0.133573113 2.379546</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 The graphics subsystem 39
</p>
<p>3 8.1 1.27 -0.154108960 2.091864
</p>
<p>4 19.5 1.47 0.724441444 2.970414
</p>
<p>5 7.2 1.27 -0.271891996 1.974081
</p>
<p>...
</p>
<p>22 4.9 1.03 -0.656737817 1.589235
</p>
<p>23 8.8 1.12 -0.071221300 2.174752
</p>
<p>24 9.5 1.70 0.005318777 2.251292
</p>
<p>Notice that the second argument is an arbitrary expression (here a com-
pound expression, see p. 45). The function is similar to with, but instead
of just returning the computed value, it collects all new and modified
variables into a modified data frame, which is then returned. As shown,
variables containing intermediate results can be discarded with rm. (It is
particularly important to do this if the contents are incompatible with the
data frame.)
</p>
<p>2.2 The graphics subsystem
</p>
<p>In Section 1.1.5, we saw how to generate a simple plot and superimpose
a curve on it. It is quite common in statistical graphics for you to want to
create a plot that is slightly different from the default: Sometimes you will
want to add annotation, sometimes you want the axes to be different &mdash;
labels instead of numbers, irregular placement of tick marks, etc. All these
things can be obtained in R. The methods for doing themmay feel slightly
unusual at first, but offers a very flexible and powerful approach.
</p>
<p>In this section, we look deeper into the structure of a typical plot and give
some indication of how you can work with plots to achieve your desired
results. Beware, though, that this is a large and complex area and it is not
within the scope of this book to cover it completely. In fact, we completely
ignore important newer tools in the grid and lattice packages.
</p>
<p>2.2.1 Plot layout
</p>
<p>In the graphics model that R uses, there is (for a single plot) a figure region
containing a central plotting region surrounded by margins. Coordinates
inside the plotting region are specified in data units (the kind generally
used to label the axes). Coordinates in the margins are specified in lines
of text as you move in a direction perpendicular to a side of the plotting
region but in data units as you move along the side. This is useful since
you generally want to put text in the margins of a plot.
</p>
<p>A standard x&ndash;y plot has an x and a y title label generated from the ex-
pressions being plotted. You may, however, override these labels and also</p>
<p/>
</div>
<div class="page"><p/>
<p>40 2. The R environment
</p>
<p>add two further titles, a main title above the plot and a subtitle at the very
bottom, in the plot call.
</p>
<p>&gt; x &lt;- runif(50,0,2)
</p>
<p>&gt; y &lt;- runif(50,0,2)
</p>
<p>&gt; plot(x, y, main="Main title", sub="subtitle",
</p>
<p>+ xlab="x-label", ylab="y-label")
</p>
<p>Inside the plotting region, you can place points and lines that are either
specified in the plot call or added later with points and lines. You
can also place a text with
</p>
<p>&gt; text(0.6,0.6,"text at (0.6,0.6)")
</p>
<p>&gt; abline(h=.6,v=.6)
</p>
<p>Here, the abline call is just to show how the text is centered on the point
(0.6, 0.6). (Normally, abline plots the line y = a+ bxwhen given a and b
as arguments, but it can also be used to draw horizontal and vertical lines
as shown.)
</p>
<p>The margin coordinates are used by the mtext function. They can be
demonstrated as follows:
</p>
<p>&gt; for (side in 1:4) mtext(-1:4,side=side,at=.7,line=-1:4)
</p>
<p>&gt; mtext(paste("side",1:4), side=1:4, line=-1,font=2)
</p>
<p>The for loop (see Section 2.3.1) places the numbers &minus;1 to 4 on corre-
sponding lines in each of the four margins at an off-center position of 0.7
measured in user coordinates. The subsequent call places a label on each
side, giving the side number. The argument font=2means that a boldface
font is used. Notice in Figure 2.1 that not all the margins are wide enough
to hold all the numbers and that it is possible to use negative line numbers
to place text within the plotting region.
</p>
<p>2.2.2 Building a plot from pieces
</p>
<p>High-level plots are composed of elements, each of which can also be
drawn separately. The separate drawing commands often allow finer con-
trol of the element, so a standard strategy to achieve a given effect is first
to draw the plot without that element and add the element subsequently.
As an extreme case, the following command will plot absolutely nothing:
</p>
<p>&gt; plot(x, y, type="n", xlab="", ylab="", axes=F)
</p>
<p>Here type="n" causes the points not to be drawn. axes=F suppresses
the axes and the box around the plot, and the x and y title labels are set to
empty strings.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 The graphics subsystem 41
</p>
<p>0.0 0.5 1.0 1.5 2.0
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>1
.5
</p>
<p>2
.0
</p>
<p>Main title
</p>
<p>subtitle
</p>
<p>x&minus;label
</p>
<p>y
&minus;
</p>
<p>la
b
e
l
</p>
<p>text at (0.6,0.6)
</p>
<p>&minus;1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>&minus;
10123
</p>
<p>&minus;1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>&minus;
1 0 1
</p>
<p>side 1
</p>
<p>s
id
</p>
<p>e
 2
</p>
<p>side 3
</p>
<p>s
id
</p>
<p>e
 4
</p>
<p>Figure 2.1. The layout of a standard plot.
</p>
<p>However, the fact that nothing is plotted does not mean that nothing hap-
pened. The command sets up the plotting region and coordinate systems
just as if it had actually plotted the data. To add the plot elements, evaluate
the following:
</p>
<p>&gt; points(x,y)
</p>
<p>&gt; axis(1)
</p>
<p>&gt; axis(2,at=seq(0.2,1.8,0.2))
</p>
<p>&gt; box()
</p>
<p>&gt; title(main="Main title", sub="subtitle",
</p>
<p>+ xlab="x-label", ylab="y-label")
</p>
<p>Notice how the second axis call specifies an alternative set of tick marks
(and labels). This is a common technique used to create special axes on a
plot and might also be used to create nonequidistant axes as well as axes
with nonnumeric labelling.
</p>
<p>Plotting with type="n" is sometimes a useful technique because it has
the side effect of dimensioning the plot area. For instance, to create a plot
with different colours for different groups, you could first plot all data
with type="n", ensuring that the plot region is large enough, and then</p>
<p/>
</div>
<div class="page"><p/>
<p>42 2. The R environment
</p>
<p>add the points for each group using points. (Passing a vector argument
for col is more expedient in this particular case.)
</p>
<p>2.2.3 Using par
</p>
<p>The par function allows incredibly fine control over the details of a plot,
although it can be quite confusing to the beginner (and even to experi-
enced users at times). The best strategy for learning it may well be simply
to try and pick up a few useful tricks at a time and once in a while try to
solve a particular problem by poring over the help page.
</p>
<p>Some of the parameters, but not all, can also be set via arguments to plot-
ting functions, which also have some arguments that cannot be set by par.
When a parameter can be set by both methods, the difference is generally
that if something is set via par, then it stays set subsequently.
</p>
<p>The par settings allow you to control line width and type, character size
and font, colour, style of axis calculation, size of the plot and figure re-
gions, clipping, etc. It is possible to divide a figure into several subfigures
by using the mfrow and mfcol parameters.
</p>
<p>For instance, the default margin sizes are just over 5, 4, 4, and 2 lines.
You might set par(mar=c(4,4,2,2)+0.1) before plotting. This shaves
one line off the bottom margin and two lines off the top margin of the
plot, which will reduce the amount of unused whitespace when there is
no main title or subtitle. If you look carefully, you will in fact notice that
Figure 2.1 has a somewhat smaller plotting region than the other plots in
this book. This is because the other plots have been made with reduced
margins for typesetting reasons.
</p>
<p>However, it is quite pointless to describe the graphics parameters com-
pletely at this point. Instead, we return to them as they are used for specific
plots.
</p>
<p>2.2.4 Combining plots
</p>
<p>Some special considerations arise when you wish to put several elements
together in the same plot. Consider overlaying a histogram with a normal
density (see Sections 4.2 and 4.4.1 for information on histograms and Sec-
tion 3.5.1 for density). The following is close, but only nearly good enough
(figure not shown).
</p>
<p>&gt; x &lt;- rnorm(100)
</p>
<p>&gt; hist(x,freq=F)
</p>
<p>&gt; curve(dnorm(x),add=T)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 The graphics subsystem 43
</p>
<p>The freq=F argument to hist ensures that the histogram is in terms of
densities rather than absolute counts. The curve function graphs an ex-
pression (in terms of x) and its add=T allows it to overplot an existing
plot. So things are generally set up correctly, but sometimes the top of the
density function gets chopped off. The reason is of course that the height
of the normal density played no role in the setting of the y-axis for the his-
togram. It will not help to reverse the order and draw the curve first and
add the histogram because then the highest bars might get clipped.
</p>
<p>The solution is first to get hold of the magnitude of the y values for both
plot elements and make the plot big enough to hold both (Figure 2.2):
</p>
<p>&gt; h &lt;- hist(x, plot=F)
</p>
<p>&gt; ylim &lt;- range(0, h$density, dnorm(0))
</p>
<p>&gt; hist(x, freq=F, ylim=ylim)
</p>
<p>&gt; curve(dnorm(x), add=T)
</p>
<p>Histogram of x
</p>
<p>x
</p>
<p>D
e
n
s
it
y
</p>
<p>&minus;2 &minus;1 0 1 2 3
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>Figure 2.2. Histogram with normal density overlaid.
</p>
<p>When called with plot=F, hist will not plot anything, but it will re-
turn a structure containing the bar heights on the density scale. This and
the fact that the maximum of dnorm(x) is dnorm(0) allows us to cal-
culate a range covering both the bars and the normal density. The zero in</p>
<p/>
</div>
<div class="page"><p/>
<p>44 2. The R environment
</p>
<p>the range call ensures that the bottom of the bars will be in range, too.
The range of y values is then passed to the hist function via the ylim
argument.
</p>
<p>2.3 R programming
</p>
<p>It is possible to write your own R functions. In fact, this is a major as-
pect and attraction of working with the system in the long run. This book
largely avoids the issue in favour of covering a larger set of basic statistical
procedures that can be executed from the command line. However, to give
you a feel for what can be done, consider the following function, which
wraps the code from the example of Section 2.2.4 so that you can just
say hist.with.normal(rnorm(200)). It has been slightly extended
so that it now uses the empirical mean and standard deviation of the data
instead of just 0 and 1.
</p>
<p>&gt; hist.with.normal &lt;- function(x, xlab=deparse(substitute(x)),...)
</p>
<p>+ {
</p>
<p>+ h &lt;- hist(x, plot=F, ...)
</p>
<p>+ s &lt;- sd(x)
</p>
<p>+ m &lt;- mean(x)
</p>
<p>+ ylim &lt;- range(0,h$density,dnorm(0,sd=s))
</p>
<p>+ hist(x, freq=F, ylim=ylim, xlab=xlab, ...)
</p>
<p>+ curve(dnorm(x,m,s), add=T)
</p>
<p>+ }
</p>
<p>Notice the use of a default argument for xlab. If xlab is not specified,
then it is obtained from this expression, which evaluates to a character
form of the expression given for x; that is, if you pass rnorm(100) for
x, then the x label becomes &ldquo;rnorm(100)&rdquo;. Notice also the use of a ...
argument, which collects any additional arguments and passes them on
to hist in the two calls.
</p>
<p>You can learn more about programming in R by studying the built-in
functions, starting with simple ones like log10 or weighted.mean.
</p>
<p>2.3.1 Flow control
</p>
<p>Until now, we have seen components of the R language that cause evalua-
tion of single expressions. However, R is a true programming language
that allows conditional execution and looping constructs as well. Con-
sider, for instance, the following code. (The code implements a version
of Newton&rsquo;s method for calculating the square root of y.)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 R programming 45
</p>
<p>&gt; y &lt;- 12345
</p>
<p>&gt; x &lt;- y/2
</p>
<p>&gt; while (abs(x*x-y) &gt; 1e-10) x &lt;- (x + y/x)/2
</p>
<p>&gt; x
</p>
<p>[1] 111.1081
</p>
<p>&gt; x^2
</p>
<p>[1] 12345
</p>
<p>Notice the while(condition) expression construction, which says
that the expression should be evaluated as long as the condition is TRUE.
The test occurs at the top of the loop, so the expression might never be
evaluated.
</p>
<p>A variation of the same algorithm with the test at the bottom of the loop
can be written with a repeat construction:
</p>
<p>&gt; x &lt;- y/2
</p>
<p>&gt; repeat{
</p>
<p>+ x &lt;- (x + y/x)/2
</p>
<p>+ if (abs(x*x-y) &lt; 1e-10) break
</p>
<p>+ }
</p>
<p>&gt; x
</p>
<p>[1] 111.1081
</p>
<p>This also illustrates three other flow control structures: (a) a compound ex-
pression, several expressions held together between curly braces; (b) an if
construction for conditional execution; and (c) a break expression, which
causes the enclosing loop to exit.
</p>
<p>Incidentally, the loop could allow for y being a vector simply by changing
the termination condition to
</p>
<p>if (all(abs(x*x - y) &lt; 1e-10)) break
</p>
<p>This would iterate excessively for some elements, but the vectorized
arithmetic would likely more than make up for that.
</p>
<p>However, the most frequently used looping construct is for, which loops
over a fixed set of values as in the following example, which plots a set of
power curves on the unit interval.
</p>
<p>&gt; x &lt;- seq(0, 1,.05)
</p>
<p>&gt; plot(x, x, ylab="y", type="l")
</p>
<p>&gt; for ( j in 2:8 ) lines(x, x^j)
</p>
<p>Notice the loop variable j, which in turn takes the values of the given
sequence when used in the lines call.</p>
<p/>
</div>
<div class="page"><p/>
<p>46 2. The R environment
</p>
<p>2.3.2 Classes and generic functions
</p>
<p>Object-oriented programming is about creating coherent systems of data
and methods that work upon them. One purpose is to simplify programs
by accommodating the fact that you will have conceptually similar meth-
ods for different types of data, even though the implementations will have
to be different. A prototype example is the print method: It makes sense
to print many kinds of data objects, but the print layout will depend on
what the data object is. You will generally have a class of data objects and
a print method for that class. There are several object-oriented languages
implementing these ideas in different ways.
</p>
<p>Most of the basic parts of R use the same object system as S version 3. An
alternative object system similar to that of S version 4 has been developed
in recent years. The new system has several advantages over the old one,
but we shall restrict attention to the latter. The S3 object system is a sim-
ple system in which an object has a class attribute, which is simply a
character vector. One example of this is that all the return values of the
classical tests such as t.test have class "htest", indicating that they
are the result of a hypothesis test. When these objects are printed, it is
done by print.htest, which creates the nice layout (see Chapter 5 for
examples). However, from a programmatic viewpoint, these objects are
just lists, and you can, for instance, extract the p-value by writing
</p>
<p>&gt; t.test(bmi, mu=22.5)$p.value
</p>
<p>[1] 0.7442183
</p>
<p>The function print is a generic function, one that acts differently depend-
ing on its argument. These generally look like this:
</p>
<p>&gt; print
</p>
<p>function (x, ...)
</p>
<p>UseMethod("print")
</p>
<p>&lt;environment: namespace:base&gt;
</p>
<p>What UseMethod("print") means is that R should pass control to a
function named according to the object class (print.htest for objects of
class "htest", etc.) or, if this is not found, to print.default. To see all
the methods available for print, type methods(print) (there are 138
of them in R 2.6.2, so the output is not shown here).
</p>
<p>2.4 Data entry
</p>
<p>Data sets do not have to be very large before it becomes impractical to type
them in with c(...). Most of the examples in this book use data sets in-</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Data entry 47
</p>
<p>cluded in the ISwR package, made available to you by library(ISwR).
However, as soon as you wish to apply the methods to your own data,
you will have to deal with data file formats and the specification thereof.
</p>
<p>In this section we discuss how to read data files and how to use the data
editor module in R. The text has some bias toward Windows systems,
mainly because of some special issues that need to be mentioned for that
platform.
</p>
<p>2.4.1 Reading from a text file
</p>
<p>The most convenient way of reading data into R is via the function called
read.table. It requires that data be in &ldquo;ASCII format&rdquo;; that is, a &ldquo;flat
file&rdquo; as createdwithWindows&rsquo; NotePad or any plain-text editor. The result
of read.table is a data frame, and it expects to find data in a corre-
sponding layout where each line in the file contains all data from one
subject (or rat or . . . ) in a specific order, separated by blanks or, option-
ally, some other separator. The first line of the file can contain a header
giving the names of the variables, a practice that is highly recommended.
</p>
<p>Table 11.6 in Altman (1991) contains an example on ventricular circum-
ferential shortening velocity versus fasting blood glucose by Thuesen et
al. We used those data to illustrate subsetting and use them again in the
chapter on correlation and regression. They are among the built-in data
sets in the ISwR package and available as the data frame thuesen, but
the point here is to show how to read them from a plain-text file.
</p>
<p>Assume that the data are contained in the file thuesen.txt, which looks
as follows:
</p>
<p>blood.glucose short.velocity
</p>
<p>15.3 1.76
</p>
<p>10.8 1.34
</p>
<p>8.1 1.27
</p>
<p>19.5 1.47
</p>
<p>7.2 1.27
</p>
<p>5.3 1.49
</p>
<p>9.3 1.31
</p>
<p>11.1 1.09
</p>
<p>7.5 1.18
</p>
<p>12.2 1.22
</p>
<p>6.7 1.25
</p>
<p>5.2 1.19
</p>
<p>19.0 1.95
</p>
<p>15.1 1.28
</p>
<p>6.7 1.52
</p>
<p>8.6 NA
</p>
<p>4.2 1.12</p>
<p/>
</div>
<div class="page"><p/>
<p>48 2. The R environment
</p>
<p>10.3 1.37
</p>
<p>12.5 1.19
</p>
<p>16.1 1.05
</p>
<p>13.3 1.32
</p>
<p>4.9 1.03
</p>
<p>8.8 1.12
</p>
<p>9.5 1.70
</p>
<p>To enter the data into the file, you could start up Windows&rsquo; NotePad
or any other plain-text editor, such as those discussed in Section 2.1.3.
Unix/Linux users should just use a standard editor, such as emacs or
vi. If you must, you can even use a word processing program with a little
care.
</p>
<p>You should simply type in the data as shown. Notice that the columns
are separated by an arbitrary number of blanks and that NA represents a
missing value.
</p>
<p>At the end, you should save the data to a text file. Notice that word pro-
cessors require special actions in order to save as text. Their normal save
format is difficult to read from other programs.
</p>
<p>Assuming further that the file is in the ISwR folder on the N: drive, the
data can be read using
</p>
<p>&gt; thuesen2 &lt;- read.table("N:/ISwR/thuesen.txt",header=T)
</p>
<p>Notice header=T specifying that the first line is a header containing
the names of variables contained in the file. Also note that you use for-
ward slashes (/), not backslashes (\), in the filename, even on a Windows
system.
</p>
<p>The reason for avoiding backslashes in Windows filenames is that the
symbol is used as an escape character (see Section 1.2.4) and therefore
needs to be doubled. You could have used N:\\ISwR\\thuesen.txt.
</p>
<p>The result is a data frame, which is assigned to the variable thuesen2
and looks as follows:
</p>
<p>&gt; thuesen2
</p>
<p>blood.glucose short.velocity
</p>
<p>1 15.3 1.76
</p>
<p>2 10.8 1.34
</p>
<p>3 8.1 1.27
</p>
<p>4 19.5 1.47
</p>
<p>5 7.2 1.27
</p>
<p>6 5.3 1.49
</p>
<p>7 9.3 1.31
</p>
<p>8 11.1 1.09
</p>
<p>9 7.5 1.18
</p>
<p>10 12.2 1.22</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Data entry 49
</p>
<p>11 6.7 1.25
</p>
<p>12 5.2 1.19
</p>
<p>13 19.0 1.95
</p>
<p>14 15.1 1.28
</p>
<p>15 6.7 1.52
</p>
<p>16 8.6 NA
</p>
<p>17 4.2 1.12
</p>
<p>18 10.3 1.37
</p>
<p>19 12.5 1.19
</p>
<p>20 16.1 1.05
</p>
<p>21 13.3 1.32
</p>
<p>22 4.9 1.03
</p>
<p>23 8.8 1.12
</p>
<p>24 9.5 1.70
</p>
<p>To read in factor variables (see Section 1.2.8), the easiest way may be to
encode them using a textual representation. The read.table function
autodetects whether a vector is text or numeric and converts it to a factor
in the former case (but makes no attempt to recognize numerically coded
factors). For instance, the secretin built-in data set is read from a file
that begins like this:
</p>
<p>gluc person time repl time20plus time.comb
</p>
<p>1 92 A pre a pre pre
</p>
<p>2 93 A pre b pre pre
</p>
<p>3 84 A 20 a 20+ 20
</p>
<p>4 88 A 20 b 20+ 20
</p>
<p>5 88 A 30 a 20+ 30+
</p>
<p>6 90 A 30 b 20+ 30+
</p>
<p>7 86 A 60 a 20+ 30+
</p>
<p>8 89 A 60 b 20+ 30+
</p>
<p>9 87 A 90 a 20+ 30+
</p>
<p>10 90 A 90 b 20+ 30+
</p>
<p>11 85 B pre a pre pre
</p>
<p>12 85 B pre b pre pre
</p>
<p>13 74 B 20 a 20+ 20
</p>
<p>....
</p>
<p>This file can be read directly by read.table with no arguments other
than the filename. It will recognize the case where the first line is one item
shorter than the rest and will interpret that layout to imply that the first
line contains a header and the first value on all subsequent lines is a row
label &mdash; that is, exactly the layout generated when printing a data frame.
</p>
<p>Reading factors like this may be convenient, but there is a drawback: The
level order is alphabetic, so for instance
</p>
<p>&gt; levels(secretin$time)
</p>
<p>[1] "20" "30" "60" "90" "pre"</p>
<p/>
</div>
<div class="page"><p/>
<p>50 2. The R environment
</p>
<p>If this is not what you want, then you may have to manipulate the factor
levels; see Section 10.1.2.
</p>
<p>A technical note: The files referenced above are contained in the ISwR
package in the subdirectory (folder) rawdata. Exactly where the file is
located on your system will depend on where the ISwR package was
installed. You can find this out as follows:
</p>
<p>&gt; system.file("rawdata", "thuesen.txt", package="ISwR")
</p>
<p>[1] "/home/pd/Rlibrary/ISwR/rawdata/thuesen.txt"
</p>
<p>2.4.2 Further details on read.table
</p>
<p>The read.table function is a very flexible tool that is controlled bymany
options. We shall not attempt a full description here but just give some
indication of what it can do.
</p>
<p>File format details
</p>
<p>We have already seen the use of header=T. A couple of other options
control the detailed format of the input file:
</p>
<p>Field separator. This can be specified using sep. Notice that when this is
used, as opposed to the default use of whitespace, there must be ex-
actly one separator between data fields. Two consecutive separators
will imply that there is a missing value in between. Conversely, it
is necessary to use specific codes to represent missing values in the
default format and also to use some form of quoting for strings that
contain embedded spaces.
</p>
<p>NA strings. You can specify which strings represent missing values via
na.strings. There can be several different strings, although not
different strings for different columns. For print files from the SAS
program, you would use na.strings=".".
</p>
<p>Quotes and comments. By default, R-style quotes can be used to delimit
character strings, and parts of files following the comment character
# are ignored. These features can be modified or removed via the
quote and comment.char arguments.
</p>
<p>Unequal field count. It is normally considered an error if not all lines con-
tain the same number of values (the first line can be one item short,
as described above for the secretin data). The fill and flush
arguments can be used in case lines vary in length.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Data entry 51
</p>
<p>Delimited file types
</p>
<p>Applications such as spreadsheets and databases produce text files in for-
mats that require multiple options to be adjusted. For such purposes, there
exist &ldquo;precooked&rdquo; variants of read.table. Two of these are intended
to handle CSV files and are called read.csv and read.csv2. The for-
mer assumes that fields are separated by a comma, and the latter assumes
that they are separated by semicolons but use a comma as the decimal
point (this format is often generated in European locales). Both formats
have header=T as the default. Further variants are read.delim and
read.delim2 for reading delimited files (by default, Tab-delimited files).
</p>
<p>Conversion of input
</p>
<p>It can be desirable to override the default conversion mechanisms in
read.table. By default, nonnumeric input is converted to factors, but
it does not always make sense. For instance, names and addresses typi-
cally should not be converted. This can be modified either for all columns
using stringsAsFactors or on a per-item basis using as.is.
</p>
<p>Automatic conversion is often convenient, but it is inefficient in terms
of computer time and storage; in order to read a numeric column,
read.table first reads it as character data, checks whether all elements
can be converted to numeric, and only then performs the conversion. The
colClasses argument allows you to bypass the mechanism by explic-
itly specifying which columns are of which class (the standard classes
"character", "numeric", etc., get special treatment). You can also skip
unwanted columns by specifying "NULL" as the class.
</p>
<p>2.4.3 The data editor
</p>
<p>R lets you edit data frames using a spreadsheet-like interface. The
interface is a bit rough but quite useful for small data sets.
</p>
<p>To edit a data frame, you can use the edit function:
</p>
<p>&gt; aq &lt;- edit(airquality)
</p>
<p>This brings up a spreadsheet-like editor with a column for each vari-
able in the data frame. The airquality data set is built into R; see
help(airquality) for its contents. Inside the editor, you can move
around with the mouse or the cursor keys and edit the current cell by typ-
ing in data. The type of variable can be switched between real (numeric)
and character (factor) by clicking on the column header, and the name of</p>
<p/>
</div>
<div class="page"><p/>
<p>52 2. The R environment
</p>
<p>the variable can be changed similarly. Note that there is (as of R 2.6.2) no
way to delete rows and columns and that new data can be entered only at
the end.
</p>
<p>When you close the data editor, the edited data frame is assigned to aq.
The original airquality is left intact. Alternatively, if you do not mind
overwriting the original data frame, you can use
</p>
<p>&gt; fix(aq)
</p>
<p>This is equivalent to aq &lt;- edit(aq).
</p>
<p>To enter data into a blank data frame, use
</p>
<p>&gt; dd &lt;- data.frame()
</p>
<p>&gt; fix(dd)
</p>
<p>An alternative would be dd &lt;- edit(data.frame()), which works
fine except that beginners tend to reexecute the commandwhen they need
to edit dd, which of course destroys all data. It is necessary in either case
to start with an empty data frame since by default edit expects you to
want to edit a user-defined function and would bring up a text editor if
you started it as edit().
</p>
<p>2.4.4 Interfacing to other programs
</p>
<p>Sometimes you will want to move data between R and other statistical
packages or spreadsheets. A simple fallback approach is to request that
the package in question export data as a text file of some sort and use
read.table, read.csv, read.csv2, read.delim, or read.delim2,
as previously described.
</p>
<p>The foreign package is one of the packages labelled &ldquo;recommended&rdquo;
and should therefore be available with binary distributions of R. It
contains routines to read files in several formats, including those from
SPSS (.sav format), SAS (export libraries), Epi-Info (.rec), Stata, Systat,
Minitab, and some S-PLUS version 3 dump files.
</p>
<p>Unix/Linux users sometimes find themselves with data sets written on
Windows machines. The foreign package will work there as well for
those formats that it supports. Notice that ordinary SAS data sets are
not among the supported formats. These have to be converted to ex-
port libraries on the originating system. Data that have been entered
into Microsoft Excel spreadsheets are most conveniently extracted using a
compatible application such as OOo (OpenOffice.org).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Exercises 53
</p>
<p>An expedient technique is to read from the system clipboard. Say, high-
light a rectangular region in a spreadsheet, press Ctrl-C (if on Windows),
and inside R use
</p>
<p>read.table("clipboard", header=T)
</p>
<p>This does require a little caution, though. It may result in loss of accu-
racy since you only transfer the data as they appear on the screen. This is
mostly a concern if you have data to many significant digits.
</p>
<p>For data stored in databases, there exist a number of interface packages on
CRAN. Of particular interest on Windows and with some Unix databases
is the RODBC package because you can set up ODBC (&ldquo;Open Database
Connectivity&rdquo;) connections to data stored by common applications, in-
cluding Excel and Access. Some Unix databases (e.g., PostgreSQL) also
allow ODBC connections.
</p>
<p>For up-to-date information on these matters, consult the &ldquo;R Data Im-
port/Export&rdquo; manual that comes with the system.
</p>
<p>2.5 Exercises
</p>
<p>2.1 Describe how to insert a value between two elements of a vector at a
given position by using the append function (use the help system to find
out). Without append, how would you do it?
</p>
<p>2.2 Write the built-in data set thuesen to a Tab-separated text file with
write.table. View it with a text editor (depending on your system).
Change the NA value to . (period), and read the changed file back into R
with a suitable command. Also try importing the data into other applica-
tions of your choice and exporting them to a new file after editing. You
may have to remove row names to make this work.</p>
<p/>
</div>
<div class="page"><p/>
<p>3
Probability and distributions
</p>
<p>The concepts of randomness and probability are central to statistics. It
is an empirical fact that most experiments and investigations are not
perfectly reproducible. The degree of irreproducibility may vary: Some
experiments in physics may yield data that are accurate to many decimal
places, whereas data on biological systems are typically much less reli-
able. However, the view of data as something coming from a statistical
distribution is vital to understanding statistical methods. In this section,
we outline the basic ideas of probability and the functions that R has for
random sampling and handling of theoretical distributions.
</p>
<p>3.1 Random sampling
</p>
<p>Much of the earliest work in probability theorywas about games and gam-
bling issues, based on symmetry considerations. The basic notion then is
that of a random sample: dealing from a well-shuffled pack of cards or
picking numbered balls from a well-stirred urn.
</p>
<p>In R, you can simulate these situations with the sample function. If you
want to pick five numbers at random from the set 1:40, then you can
write
</p>
<p>&gt; sample(1:40,5)
</p>
<p>[1] 4 30 28 40 13
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_3, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>56 3. Probability and distributions
</p>
<p>The first argument (x) is a vector of values to be sampled and the second
(size) is the sample size. Actually, sample(40,5) would suffice since
a single number is interpreted to represent the length of a sequence of
integers.
</p>
<p>Notice that the default behaviour of sample is sampling without replace-
ment. That is, the samples will not contain the same number twice, and
size obviously cannot be bigger than the length of the vector to be sam-
pled. If you want sampling with replacement, then you need to add the
argument replace=TRUE.
</p>
<p>Sampling with replacement is suitable for modelling coin tosses or throws
of a die. So, for instance, to simulate 10 coin tosses we could write
</p>
<p>&gt; sample(c("H","T"), 10, replace=T)
</p>
<p>[1] "T" "T" "T" "T" "T" "H" "H" "T" "H" "T"
</p>
<p>In fair coin-tossing, the probability of heads should equal the probability
of tails, but the idea of a random event is not restricted to symmetric cases.
It could be equally well applied to other cases, such as the successful out-
come of a surgical procedure. Hopefully, there would be a better than 50%
chance of this. You can simulate data with nonequal probabilities for the
outcomes (say, a 90% chance of success) by using the prob argument to
sample, as in
</p>
<p>&gt; sample(c("succ", "fail"), 10, replace=T, prob=c(0.9, 0.1))
</p>
<p>[1] "succ" "succ" "succ" "succ" "succ" "succ" "succ" "succ"
</p>
<p>[9] "succ" "succ"
</p>
<p>This may not be the best way to generate such a sample, though. See the
later discussion of the binomial distribution.
</p>
<p>3.2 Probability calculations and combinatorics
</p>
<p>Let us return to the case of sampling without replacement, specifically
sample(1:40, 5). The probability of obtaining a given number as the
first one of the sample should be 1/40, the next one 1/39, and so forth. The
probability of a given sample should then be 1/(40&times; 39&times; 38&times; 37&times; 36).
In R, use the prod function, which calculates the product of a vector of
numbers
</p>
<p>&gt; 1/prod(40:36)
</p>
<p>[1] 1.266449e-08</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Discrete distributions 57
</p>
<p>However, notice that this is the probability of getting given numbers in a
given order. If this were a Lotto-like game, then youwould rather be inter-
ested in the probability of guessing a given set of five numbers correctly.
Thus you need also to include the cases that give the same numbers in
a different order. Since obviously the probability of each such case is go-
ing to be the same, all we need to do is to figure out how many such cases
there are andmultiply by that. There are five possibilities for the first num-
ber, and for each of these there are four possibilities for the second, and so
forth; that is, the number is 5&times; 4&times; 3&times; 2&times; 1. This number is also written
as 5! (5 factorial). So the probability of a &ldquo;winning Lotto coupon&rdquo; would be
</p>
<p>&gt; prod(5:1)/prod(40:36)
</p>
<p>[1] 1.519738e-06
</p>
<p>There is another way of arriving at the same result. Notice that since the
actual set of numbers is immaterial, all sets of five numbers must have the
same probability. So all we need to do is to calculate the number of ways
to choose 5 numbers out of 40. This is denoted
</p>
<p>(
40
5
</p>
<p>)
</p>
<p>=
40!
5!35!
</p>
<p>= 658008
</p>
<p>In R, the choose function can be used to calculate this number, and the
probability is thus
</p>
<p>&gt; 1/choose(40,5)
</p>
<p>[1] 1.519738e-06
</p>
<p>3.3 Discrete distributions
</p>
<p>When looking at independent replications of a binary experiment, you
would not usually be interested in whether each case is a success or a fail-
ure but rather in the total number of successes (or failures). Obviously,
this number is random since it depends on the individual random out-
comes, and it is consequently called a random variable. In this case it is a
discrete-valued random variable that can take values 0, 1, . . . , n, where n is
the number of replications. Continuous random variables are encountered
later.
</p>
<p>A random variable X has a probability distribution that can be described
using point probabilities f (x) = P(X = x) or the cumulative distribution
function F(x) = P(X &le; x). In the case at hand, the distribution can be
worked out as having the point probabilities
</p>
<p>f (x) =
</p>
<p>(
n
</p>
<p>x
</p>
<p>)
</p>
<p>px(1&minus; p)n&minus;x</p>
<p/>
</div>
<div class="page"><p/>
<p>58 3. Probability and distributions
</p>
<p>This is known as the binomial distribution, and the (nx) are known as bino-
mial coefficients. The parameter p is the probability of a successful outcome
in an individual trial. A graph of the point probabilities of the binomial
distribution appears in Figure 3.2 ahead.
</p>
<p>We delay describing the R functions related to the binomial distribution
until we have discussed continuous distributions so that we can present
the conventions in a unified manner.
</p>
<p>Many other distributions can be derived from simple probability models.
For instance, the geometric distribution is similar to the binomial distri-
bution but records the number of failures that occur before the first
success.
</p>
<p>3.4 Continuous distributions
</p>
<p>Some data arise from measurements on an essentially continuous scale,
for instance temperature, concentrations, etc. In practice, they will be
recorded to a finite precision, but it is useful to disregard this in the
modelling. Such measurements will usually have a component of random
variation, which makes them less than perfectly reproducible. However,
these random fluctuations will tend to follow patterns; typically they will
cluster around a central value, with large deviations being more rare than
smaller ones.
</p>
<p>In order to model continuous data, we need to define random variables
that can obtain the value of any real number. Because there are infinitely
many numbers infinitely close, the probability of any particular value will
be zero, so there is no such thing as a point probability as for discrete-
valued random variables. Instead we have the concept of a density. This is
the infinitesimal probability of hitting a small region around x divided by
the size of the region. The cumulative distribution function can be defined
as before, and we have the relation
</p>
<p>F(x) =
&int; x
</p>
<p>&minus;&infin;
</p>
<p>f (x) dx
</p>
<p>There are a number of standard distributions that come up in statistical
theory and are available in R. It makes little sense to describe them in
detail here except for a couple of examples.
</p>
<p>The uniform distribution has a constant density over a specified interval (by
default [0, 1]).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 The built-in distributions in R 59
</p>
<p>The normal distribution (also known as the Gaussian distribution) has
density
</p>
<p>f (x) =
1&radic;
2πσ
</p>
<p>exp
(
</p>
<p>&minus; (x&minus; &micro;)
2
</p>
<p>2σ2
</p>
<p>)
</p>
<p>depending on its mean &micro; and standard deviation σ. The normal distribu-
tion has a characteristic bell shape (Figure 3.1), and modifying &micro; and σ
simply translates and widens the distribution. It is a standard building
block in statistical models, where it is commonly used to describe error
variation. It also comes up as an approximating distribution in several
contexts; for instance, the binomial distribution for large sample sizes can
be well approximated by a suitably scaled normal distribution.
</p>
<p>3.5 The built-in distributions in R
</p>
<p>The standard distributions that turn up in connectionwithmodel building
and statistical tests have been built into R, and it can therefore completely
replace traditional statistical tables. Here we look only at the normal dis-
tribution and the binomial distribution, but other distributions follow
exactly the same pattern.
</p>
<p>Four fundamental items can be calculated for a statistical distribution:
</p>
<p>&bull; Density or point probability
</p>
<p>&bull; Cumulated probability, distribution function
</p>
<p>&bull; Quantiles
</p>
<p>&bull; Pseudo-random numbers
</p>
<p>For all distributions implemented in R, there is a function for each of the
four items listed above. For example, for the normal distribution, these are
named dnorm, pnorm, qnorm, and rnorm (density, probability, quantile,
and random, respectively).
</p>
<p>3.5.1 Densities
</p>
<p>The density for a continuous distribution is a measure of the relative prob-
ability of &ldquo;getting a value close to x&rdquo;. The probability of getting a value in
a particular interval is the area under the corresponding part of the curve.</p>
<p/>
</div>
<div class="page"><p/>
<p>60 3. Probability and distributions
</p>
<p>For discrete distributions, the term &ldquo;density&rdquo; is used for the point proba-
bility &mdash; the probability of getting exactly the value x. Technically, this is
correct: It is a density with respect to counting measure.
</p>
<p>&minus;4 &minus;2 0 2 4
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>x
</p>
<p>d
n
o
rm
</p>
<p>(x
)
</p>
<p>Figure 3.1. Density of normal distribution.
</p>
<p>The density function is likely the one of the four function types that is least
used in practice, but if for instance it is desired to draw the well-known
bell curve of the normal distribution, then it can be done like this:
</p>
<p>&gt; x &lt;- seq(-4,4,0.1)
</p>
<p>&gt; plot(x,dnorm(x),type="l")
</p>
<p>(Notice that this is the letter &lsquo;l&rsquo;, not the digit &lsquo;1&rsquo;).
</p>
<p>The function seq (see p. 15) is used to generate equidistant values, here
from &minus;4 to 4 in steps of 0.1; that is, (&minus;4.0,&minus;3.9,&minus;3.8, . . . , 3.9, 4.0). The use
of type="l" as an argument to plot causes the function to draw lines
between the points rather than plotting the points themselves.
</p>
<p>An alternative way of creating the plot is to use curve as follows:
</p>
<p>&gt; curve(dnorm(x), from=-4, to=4)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 The built-in distributions in R 61
</p>
<p>This is often a more convenient way of making graphs, but it does require
that the y-values can be expressed as a simple functional expression in x.
</p>
<p>For discrete distributions, where variables can take on only distinct values,
it is preferable to draw a pin diagram, here for the binomial distribution
with n = 50 and p = 0.33 (Figure 3.2):
</p>
<p>&gt; x &lt;- 0:50
</p>
<p>&gt; plot(x,dbinom(x,size=50,prob=.33),type="h")
</p>
<p>0 10 20 30 40 50
</p>
<p>0
.0
</p>
<p>0
0
.0
</p>
<p>2
0
.0
</p>
<p>4
0
.0
</p>
<p>6
0
.0
</p>
<p>8
0
.1
</p>
<p>0
0
.1
</p>
<p>2
</p>
<p>x
</p>
<p>d
b
in
</p>
<p>o
m
</p>
<p>(x
, 
s
iz
</p>
<p>e
 =
</p>
<p> 5
0
, 
p
ro
</p>
<p>b
 =
</p>
<p> 0
.3
</p>
<p>3
)
</p>
<p>Figure 3.2. Point probabilities in binom(50, 0.33).
</p>
<p>Notice that there are three arguments to the &ldquo;d-function&rdquo; this time. In
addition to x, you have to specify the number of trials n and the proba-
bility parameter p. The distribution drawn corresponds to, for example,
the number of 5s or 6s in 50 throws of a symmetrical die. Actually, dnorm
also takes more than one argument, namely the mean and standard devia-
tion, but they have default values of 0 and 1, respectively, since most often
it is the standard normal distribution that is requested.
</p>
<p>The form 0:50 is a short version of seq(0,50,1): the whole numbers
from 0 to 50 (see p. 15). It is type="h" (as in histogram-like) that causes
the pins to be drawn.</p>
<p/>
</div>
<div class="page"><p/>
<p>62 3. Probability and distributions
</p>
<p>3.5.2 Cumulative distribution functions
</p>
<p>The cumulative distribution function describes the probability of &ldquo;hitting&rdquo;
x or less in a given distribution. The corresponding R functions begin with
a &lsquo;p&rsquo; (for probability) by convention.
</p>
<p>Just as you can plot densities, you can of course also plot cumulative dis-
tribution functions, but that is usually not very informative. More often,
actual numbers are desired. Say that it is known that some biochemical
measure in healthy individuals is well described by a normal distribution
with a mean of 132 and a standard deviation of 13. Then, if a patient has a
value of 160, there is
</p>
<p>&gt; 1-pnorm(160,mean=132,sd=13)
</p>
<p>[1] 0.01562612
</p>
<p>or only about 1.5% of the general population, that has that value or higher.
The function pnorm returns the probability of getting a value smaller
than its first argument in a normal distribution with the given mean and
standard deviation.
</p>
<p>Another typical application occurs in connection with statistical tests.
Consider a simple sign test: Twenty patients are given two treatments each
(blindly and in randomized order) and then askedwhether treatment A or
B worked better. It turned out that 16 patients liked A better. The question
is then whether this can be taken as sufficient evidence that A actually is
the better treatment or whether the outcomemight as well have happened
by chance even if the treatments were equally good. If there was no dif-
ference between the two treatments, then we would expect the number of
people favouring treatment A to be binomially distributed with p = 0.5
and n = 20. How (im)probable would it then be to obtain what we have
observed? As in the normal distribution, we need a tail probability, and
the immediate guess might be to look at
</p>
<p>&gt; pbinom(16,size=20,prob=.5)
</p>
<p>[1] 0.9987116
</p>
<p>and subtract it from 1 to get the upper tail &mdash; but this would be an error!
Whatwe need is the probability of the observed or more extreme, and pbinom
is giving the probability of 16 or less. We need to use &ldquo;15 or less&rdquo; instead.
</p>
<p>&gt; 1-pbinom(15,size=20,prob=.5)
</p>
<p>[1] 0.005908966
</p>
<p>If you want a two-tailed test because you have no prior idea about which
treatment is better, then you will have to add the probability of obtaining
equally extreme results in the opposite direction. In the present case, that</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 The built-in distributions in R 63
</p>
<p>means the probability that four or fewer people prefer A, giving a total
probability of
</p>
<p>&gt; 1-pbinom(15,20,.5)+pbinom(4,20,.5)
</p>
<p>[1] 0.01181793
</p>
<p>(which is obviously exactly twice the one-tailed probability).
</p>
<p>As can be seen from the last command, it is not strictly necessary to use
the size and prob keywords as long as the arguments are given in the
right order (positional matching; see Section 1.2.2).
</p>
<p>It is quite confusing to keep track of whether or not the observation itself
needs to be counted. Fortunately, the function binom.test keeps track
of such formalities and performs the correct binomial test. This is further
discussed in Chapter 8.
</p>
<p>3.5.3 Quantiles
</p>
<p>The quantile function is the inverse of the cumulative distribution func-
tion. The p-quantile is the value with the property that there is probability
p of getting a value less than or equal to it. The median is by definition the
50% quantile.
</p>
<p>Some details concerning the definition in the case of discontinuous distri-
butions are glossed over here. You can fairly easily deduce the behaviour
by experimenting with the R functions.
</p>
<p>Tables of statistical distributions are almost always given in terms of quan-
tiles. For a fixed set of probabilities, the table shows the boundary that a
test statistic must cross in order to be considered significant at that level.
This is purely for operational reasons; it is almost superfluous when you
have the option of computing p exactly.
</p>
<p>Theoretical quantiles are commonly used for the calculation of confi-
dence intervals and for power calculations in connection with designing
and dimensioning experiments (see Chapter 9). A simple example of a
confidence interval can be given here (see also Chapter 5).
</p>
<p>If we have n normally distributed observations with the same mean &micro;
and standard deviation σ, then it is known that the average x̄ is normally
distributed around &micro; with standard deviation σ/
</p>
<p>&radic;
n. A 95% confidence
</p>
<p>interval for &micro; can be obtained as
</p>
<p>x̄ + σ/
&radic;
n&times; N0.025 &le; &micro; &le; x̄ + σ/
</p>
<p>&radic;
n&times; N0.975
</p>
<p>where N0.025 is the 2.5% quantile in the normal distribution. If σ = 12 and
we have measured n = 5 persons and found an average of x̄ = 83, then</p>
<p/>
</div>
<div class="page"><p/>
<p>64 3. Probability and distributions
</p>
<p>we can compute the relevant quantities as (&ldquo;sem&rdquo; means standard error of
the mean)
</p>
<p>&gt; xbar &lt;- 83
</p>
<p>&gt; sigma &lt;- 12
</p>
<p>&gt; n &lt;- 5
</p>
<p>&gt; sem &lt;- sigma/sqrt(n)
</p>
<p>&gt; sem
</p>
<p>[1] 5.366563
</p>
<p>&gt; xbar + sem * qnorm(0.025)
</p>
<p>[1] 72.48173
</p>
<p>&gt; xbar + sem * qnorm(0.975)
</p>
<p>[1] 93.51827
</p>
<p>and thus find a 95% confidence interval for &micro; going from 72.48 to 93.52.
(Notice that this is based on the assumption that σ is known. This is some-
times reasonable in process control applications. The more common case
of estimating σ from the data leads to confidence intervals based on the t
distribution and is discussed in Chapter 5.)
</p>
<p>Since it is known that the normal distribution is symmetric, so that
N0.025 = &minus;N0.975, it is common to write the formula for the confidence in-
terval as x̄&plusmn;σ/&radic;n&times;N0.975. The quantile itself is often written Φ&minus;1(0.975),
where Φ is standard notation for the cumulative distribution function of
the normal distribution (pnorm).
</p>
<p>Another application of quantiles is in connection with Q&ndash;Q plots (see
Section 4.2.3), which can be used to assess whether a set of data can
reasonably be assumed to come from a given distribution.
</p>
<p>3.5.4 Random numbers
</p>
<p>To many people, it sounds like a contradiction in terms to generate
random numbers on a computer since its results are supposed to be pre-
dictable and reproducible. What is in fact possible is to generate sequences
of &ldquo;pseudo-random&rdquo; numbers, which for practical purposes behave as if
they were drawn randomly.
</p>
<p>Here random numbers are used to give the reader a feeling for the way in
which randomness affects the quantities that can be calculated from a set
of data. In professional statistics, they are used to create simulated data
sets in order to study the accuracy of mathematical approximations and
the effect of assumptions being violated.
</p>
<p>The use of the functions that generate random numbers is straightfor-
ward. The first argument specifies the number of random numbers to
compute, and the subsequent arguments are similar to those for other
functions related to the same distributions. For instance,</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Exercises 65
</p>
<p>&gt; rnorm(10)
</p>
<p>[1] -0.2996466 -0.1718510 -0.1955634 1.2280843 -2.6074190
</p>
<p>[6] -0.2999453 -0.4655102 -1.5680666 1.2545876 -1.8028839
</p>
<p>&gt; rnorm(10)
</p>
<p>[1] 1.7082495 0.1432875 -1.0271750 -0.9246647 0.6402383
</p>
<p>[6] 0.7201677 -0.3071239 1.2090712 0.8699669 0.5882753
</p>
<p>&gt; rnorm(10,mean=7,sd=5)
</p>
<p>[1] 8.934983 8.611855 4.675578 3.670129 4.223117 5.484290
</p>
<p>[7] 12.141946 8.057541 -2.893164 13.590586
</p>
<p>&gt; rbinom(10,size=20,prob=.5)
</p>
<p>[1] 12 11 10 8 11 8 11 8 8 13
</p>
<p>3.6 Exercises
</p>
<p>3.1 Calculate the probability for each of the following events: (a) A
standard normally distributed variable is larger than 3. (b) A normally
distributed variable with mean 35 and standard deviation 6 is larger than
42. (c) Getting 10 out of 10 successes in a binomial distribution with prob-
ability 0.8. (d) X &lt; 0.9 when X has the standard uniform distribution. (e)
X &gt; 6.5 in a χ2 distribution with 2 degrees of freedom.
</p>
<p>3.2 A rule of thumb is that 5% of the normal distribution lies outside an
interval approximately &plusmn;2s about the mean. To what extent is this true?
Where are the limits corresponding to 1%, 0.5%, and 0.1%? What is the
position of the quartiles measured in standard deviation units?
</p>
<p>3.3 For a disease known to have a postoperative complication frequency
of 20%, a surgeon suggests a new procedure. He tests it on 10 patients
and there are no complications. What is the probability of operating on 10
patients successfully with the traditional method?
</p>
<p>3.4 Simulated coin-tossing can be done using rbinom instead of sample.
How exactly would you do that?</p>
<p/>
</div>
<div class="page"><p/>
<p>4
Descriptive statistics and graphics
</p>
<p>Before going into the actual statistical modelling and analysis of a data
set, it is often useful to make some simple characterizations of the data in
terms of summary statistics and graphics.
</p>
<p>4.1 Summary statistics for a single group
</p>
<p>It is easy to calculate simple summary statistics with R. Here is how to
calculate the mean, standard deviation, variance, and median.
</p>
<p>&gt; x &lt;- rnorm(50)
</p>
<p>&gt; mean(x)
</p>
<p>[1] 0.03301363
</p>
<p>&gt; sd(x)
</p>
<p>[1] 1.069454
</p>
<p>&gt; var(x)
</p>
<p>[1] 1.143731
</p>
<p>&gt; median(x)
</p>
<p>[1] -0.08682795
</p>
<p>Notice that the example starts with the generation of an artificial data
vector x of 50 normally distributed observations. It is used in examples
throughout this section. When reproducing the examples, you will not get
exactly the same results since your random numbers will differ.
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_4, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>68 4. Descriptive statistics and graphics
</p>
<p>Empirical quantiles may be obtained with the function quantile like
this:
</p>
<p>&gt; quantile(x)
</p>
<p>0% 25% 50% 75% 100%
</p>
<p>-2.60741896 -0.54495849 -0.08682795 0.70018536 2.98872414
</p>
<p>As you see, by default you get the minimum, the maximum, and the
three quartiles &mdash; the 0.25, 0.50, and 0.75 quantiles &mdash; so named because
they correspond to a division into four parts. Similarly, we have deciles for
0.1, 0.2, . . . , 0.9, and centiles or percentiles. The difference between the first
and third quartiles is called the interquartile range (IQR) and is sometimes
used as a robust alternative to the standard deviation.
</p>
<p>It is also possible to obtain other quantiles; this is done by adding an argu-
ment containing the desired percentage points. This, for example, is how
to get the deciles:
</p>
<p>&gt; pvec &lt;- seq(0,1,0.1)
</p>
<p>&gt; pvec
</p>
<p>[1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
</p>
<p>&gt; quantile(x,pvec)
</p>
<p>0% 10% 20% 30% 40%
</p>
<p>-2.60741896 -1.07746896 -0.70409272 -0.46507213 -0.29976610
</p>
<p>50% 60% 70% 80% 90%
</p>
<p>-0.08682795 0.19436950 0.49060129 0.90165137 1.31873981
</p>
<p>100%
</p>
<p>2.98872414
</p>
<p>Be aware that there are several possible definitions of empirical quantiles.
The oneR uses by default is based on a sumpolygonwhere the ith ranking
observation is the (i&minus; 1)/(n&minus; 1) quantile and intermediate quantiles are
obtained by linear interpolation. It sometimes confuses students that in
a sample of 10 there will be 3 observations below the first quartile with
this definition. Other definitions are available via the type argument to
quantile.
</p>
<p>If there are missing values in data, things become a bit more complicated.
For illustration, we use the following example.
</p>
<p>The data set juul contains variables from an investigation performed by
Anders Juul (Rigshospitalet, Department for Growth and Reproduction)
concerning serum IGF-I (insulin-like growth factor) in a group of healthy
humans, primarily schoolchildren. The data set is contained in the ISwR
package and contains a number of variables, of which we only use igf1
(serum IGF-I) for now, but later in the chapter we also use tanner (Tan-
ner stage of puberty, a classification into five groups based on appearance</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Summary statistics for a single group 69
</p>
<p>of primary and secondary sexual characteristics), sex, and menarche
(indicating whether or not a girl has had her first period).
</p>
<p>Attempting to calculate the mean of igf1 reveals a problem.
</p>
<p>&gt; attach(juul)
</p>
<p>&gt; mean(igf1)
</p>
<p>[1] NA
</p>
<p>R will not skip missing values unless explicitly requested to do so. The
mean of a vector with an unknown value is unknown. However, you can
give the na.rm argument (not available, remove) to request that missing
values be removed:
</p>
<p>&gt; mean(igf1,na.rm=T)
</p>
<p>[1] 340.168
</p>
<p>There is one slightly annoying exception: The length function will not
understand na.rm, so we cannot use it to count the number of nonmissing
measurements of igf1. However, you can use
</p>
<p>&gt; sum(!is.na(igf1))
</p>
<p>[1] 1018
</p>
<p>The construction above uses the fact that if logical values are used in
arithmetic, then TRUE is converted to 1 and FALSE to 0.
</p>
<p>A nice summary display of a numeric variable is obtained from the
summary function:
</p>
<p>&gt; summary(igf1)
</p>
<p>Min. 1st Qu. Median Mean 3rd Qu. Max. NA&rsquo;s
</p>
<p>25.0 202.2 313.5 340.2 462.8 915.0 321.0
</p>
<p>The 1st Qu. and 3rd Qu. refer to the empirical quartiles (0.25 and 0.75
quantiles).
</p>
<p>In fact, it is possible to summarize an entire data frame with
</p>
<p>&gt; summary(juul)
</p>
<p>age menarche sex
</p>
<p>Min. : 0.170 Min. : 1.000 Min. :1.000
</p>
<p>1st Qu.: 9.053 1st Qu.: 1.000 1st Qu.:1.000
</p>
<p>Median :12.560 Median : 1.000 Median :2.000
</p>
<p>Mean :15.095 Mean : 1.476 Mean :1.534
</p>
<p>3rd Qu.:16.855 3rd Qu.: 2.000 3rd Qu.:2.000
</p>
<p>Max. :83.000 Max. : 2.000 Max. :2.000
</p>
<p>NA&rsquo;s : 5.000 NA&rsquo;s :635.000 NA&rsquo;s :5.000
</p>
<p>igf1 tanner testvol
</p>
<p>Min. : 25.0 Min. : 1.000 Min. : 1.000
</p>
<p>1st Qu.:202.2 1st Qu.: 1.000 1st Qu.: 1.000</p>
<p/>
</div>
<div class="page"><p/>
<p>70 4. Descriptive statistics and graphics
</p>
<p>Median :313.5 Median : 2.000 Median : 3.000
</p>
<p>Mean :340.2 Mean : 2.640 Mean : 7.896
</p>
<p>3rd Qu.:462.8 3rd Qu.: 5.000 3rd Qu.: 15.000
</p>
<p>Max. :915.0 Max. : 5.000 Max. : 30.000
</p>
<p>NA&rsquo;s :321.0 NA&rsquo;s :240.000 NA&rsquo;s :859.000
</p>
<p>The data set has menarche, sex, and tanner coded as numeric variables
even though they are clearly categorical. This can be mended as follows:
</p>
<p>&gt; detach(juul)
</p>
<p>&gt; juul$sex &lt;- factor(juul$sex,labels=c("M","F"))
</p>
<p>&gt; juul$menarche &lt;- factor(juul$menarche,labels=c("No","Yes"))
</p>
<p>&gt; juul$tanner &lt;- factor(juul$tanner,
</p>
<p>+ labels=c("I","II","III","IV","V"))
</p>
<p>&gt; attach(juul)
</p>
<p>&gt; summary(juul)
</p>
<p>age menarche sex igf1
</p>
<p>Min. : 0.170 No :369 M :621 Min. : 25.0
</p>
<p>1st Qu.: 9.053 Yes :335 F :713 1st Qu.:202.2
</p>
<p>Median :12.560 NA&rsquo;s:635 NA&rsquo;s: 5 Median :313.5
</p>
<p>Mean :15.095 Mean :340.2
</p>
<p>3rd Qu.:16.855 3rd Qu.:462.8
</p>
<p>Max. :83.000 Max. :915.0
</p>
<p>NA&rsquo;s : 5.000 NA&rsquo;s :321.0
</p>
<p>tanner testvol
</p>
<p>I :515 Min. : 1.000
</p>
<p>II :103 1st Qu.: 1.000
</p>
<p>III : 72 Median : 3.000
</p>
<p>IV : 81 Mean : 7.896
</p>
<p>V :328 3rd Qu.: 15.000
</p>
<p>NA&rsquo;s:240 Max. : 30.000
</p>
<p>NA&rsquo;s :859.000
</p>
<p>Notice how the display changes for the factor variables. Note also that
juulwas detached and reattached after the modification. This is because
modifying a data frame does not affect any attached version. It was not
strictly necessary to do it here because summary works directly on the
data frame whether attached or not.
</p>
<p>In the above, the variables sex, menarche, and tanner were converted
to factors with suitable level names (in the raw data these are represented
using numeric codes). The converted variables were put back into the
data frame juul, replacing the original sex, tanner, and menarche
variables. Wemight also have used the transform function (or within):
</p>
<p>&gt; juul &lt;- transform(juul,
</p>
<p>+ sex=factor(sex,labels=c("M","F")),
</p>
<p>+ menarche=factor(menarche,labels=c("No","Yes")),
</p>
<p>+ tanner=factor(tanner,labels=c("I","II","III","IV","V")))</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Graphical display of distributions 71
</p>
<p>Histogram of x
</p>
<p>x
</p>
<p>F
re
</p>
<p>q
u
e
n
c
y
</p>
<p>&minus;3 &minus;2 &minus;1 0 1 2 3
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>1
5
</p>
<p>2
0
</p>
<p>Figure 4.1. Histogram.
</p>
<p>4.2 Graphical display of distributions
</p>
<p>4.2.1 Histograms
</p>
<p>You can get a reasonable impression of the shape of a distribution by
drawing a histogram; that is, a count of howmany observations fall within
specified divisions (&ldquo;bins&rdquo;) of the x-axis (Figure 4.1).
</p>
<p>&gt; hist(x)
</p>
<p>By specifying breaks=n in the hist call, you get approximately n bars in
the histogram since the algorithm tries to create &ldquo;pretty&rdquo; cutpoints. You
can have full control over the interval divisions by specifying breaks as
a vector rather than as a number. Altman (1991, pp. 25&ndash;26) contains an
example of accident rates by age group. These are given as a count in age
groups 0&ndash;4, 5&ndash;9, 10&ndash;15, 16, 17, 18&ndash;19, 20&ndash;24, 25&ndash;59, and 60&ndash;79 years of age.
The data can be entered as follows:
</p>
<p>&gt; mid.age &lt;- c(2.5,7.5,13,16.5,17.5,19,22.5,44.5,70.5)
</p>
<p>&gt; acc.count &lt;- c(28,46,58,20,31,64,149,316,103)
</p>
<p>&gt; age.acc &lt;- rep(mid.age,acc.count)</p>
<p/>
</div>
<div class="page"><p/>
<p>72 4. Descriptive statistics and graphics
</p>
<p>Histogram of age.acc
</p>
<p>age.acc
</p>
<p>D
e
n
s
it
y
</p>
<p>0 20 40 60 80
</p>
<p>0
.0
</p>
<p>0
0
.0
</p>
<p>1
0
.0
</p>
<p>2
0
.0
</p>
<p>3
0
.0
</p>
<p>4
</p>
<p>Figure 4.2. Histogram with unequal divisions.
</p>
<p>&gt; brk &lt;- c(0,5,10,16,17,18,20,25,60,80)
</p>
<p>&gt; hist(age.acc,breaks=brk)
</p>
<p>Here the first three lines generate pseudo-data from the table in the book.
For each interval, the relevant number of &ldquo;observations&rdquo; is generated with
an age set to the midpoint of the interval; that is, 28 2.5-year-olds, 46 7.5-
year-olds, etc. Then a vector brk of cutpoints is defined (note that the
extremes need to be included) and used as the breaks argument to hist,
yielding Figure 4.2.
</p>
<p>Notice that you automatically got the &ldquo;correct&rdquo; histogram where the area
of a column is proportional to the number. The y-axis is in density units
(that is, proportion of data per x unit), so that the total area of the his-
togramwill be 1. If, for some reason, you want the (misleading) histogram
where the column height is the raw number in each interval, then it can
be specified using freq=T. For equidistant breakpoints, that is the default
(because then you can see how many observations have gone into each
column), but you can set freq=F to get densities displayed. This is really
just a change of scale on the y-axis, but it has the advantage that it be-
comes possible to overlay the histogram with a corresponding theoretical
density function.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Graphical display of distributions 73
</p>
<p>&minus;2 &minus;1 0 1 2 3
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>sort(x)
</p>
<p>(1
:n
</p>
<p>)/
n
</p>
<p>Figure 4.3. Empirical cumulative distribution function.
</p>
<p>4.2.2 Empirical cumulative distribution
</p>
<p>The empirical cumulative distribution function is defined as the fraction of
data smaller than or equal to x. That is, if x is the kth smallest observation,
then the proportion k/n of the data is smaller than or equal to x (7/10
if x is no. 7 of 10). The empirical cumulative distribution function can be
plotted as follows (see Figure 4.3) where x is the simulated data vector
from Section 4.1:
</p>
<p>&gt; n &lt;- length(x)
</p>
<p>&gt; plot(sort(x),(1:n)/n,type="s",ylim=c(0,1))
</p>
<p>The plotting parameter type="s" gives a step function where (x, y) is the
left end of the steps and ylim is a vector of two elements specifying the
extremes of the y-coordinates on the plot. Recall that c(...) is used to
create vectors.
</p>
<p>Some more elaborate displays of empirical cumulative distribution func-
tions are available via the ecdf function. This is also more precise
regarding the mathematical definition of the step function.</p>
<p/>
</div>
<div class="page"><p/>
<p>74 4. Descriptive statistics and graphics
</p>
<p>&minus;2 &minus;1 0 1 2
</p>
<p>&minus;
2
</p>
<p>&minus;
1
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>Normal Q&minus;Q Plot
</p>
<p>Theoretical Quantiles
</p>
<p>S
a
m
</p>
<p>p
le
</p>
<p> Q
u
a
n
ti
le
</p>
<p>s
</p>
<p>Figure 4.4. Q&ndash;Q plot using qqnorm(x).
</p>
<p>4.2.3 Q&ndash;Q plots
</p>
<p>One purpose of calculating the empirical cumulative distribution function
(c.d.f.) is to see whether data can be assumed normally distributed. For
a better assessment, you might plot the kth smallest observation against
the expected value of the kth smallest observation out of n in a standard
normal distribution. The point is that in this way you would expect to
obtain a straight line if data come from a normal distribution with any
mean and standard deviation.
</p>
<p>Creating such a plot is slightly complicated. Fortunately, there is a built-
in function for doing it, qqnorm. The result of using it can be seen in
Figure 4.4. You only have to write
</p>
<p>&gt; qqnorm(x)
</p>
<p>As the title of the plot indicates, plots of this kind are also called &ldquo;Q&ndash;Q
plots&rdquo; (quantile versus quantile). Notice that x and y are interchanged rel-
ative to the empirical c.d.f. &mdash; the observed values are now drawn along
the y-axis. You should notice that with this convention the distribution has
heavy tails if the outer parts of the curve are steeper than the middle part.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Summary statistics by groups 75
</p>
<p>Some readers will have been taught &ldquo;probability plots&rdquo;, which are similar
but have the axes interchanged. It can be argued that the way R draws the
plot is the better one since the theoretical quantiles are known in advance,
while the empirical quantiles depend on data. Youwould normally choose
to draw fixed values horizontally and variable values vertically.
</p>
<p>4.2.4 Boxplots
</p>
<p>A &ldquo;boxplot&rdquo;, or more descriptively a &ldquo;box-and-whiskers plot&rdquo;, is a graph-
ical summary of a distribution. Figure 4.5 shows boxplots for IgM and its
logarithm; see the example on page 23 in Altman (1991).
</p>
<p>Here is how a boxplot is drawn in R. The box in the middle indicates
&ldquo;hinges&rdquo; (nearly quartiles; see the help page for boxplot.stats) and
median. The lines (&ldquo;whiskers&rdquo;) show the largest or smallest observation
that falls within a distance of 1.5 times the box size from the nearest hinge.
If any observations fall farther away, the additional points are considered
&ldquo;extreme&rdquo; values and are shown separately.
</p>
<p>The practicalities are these:
</p>
<p>&gt; par(mfrow=c(1,2))
</p>
<p>&gt; boxplot(IgM)
</p>
<p>&gt; boxplot(log(IgM))
</p>
<p>&gt; par(mfrow=c(1,1))
</p>
<p>A layout with two plots side by side is specified using the mfrow graph-
ical parameter. It should be read as &ldquo;multif rame, rowwise, 1&times; 2 layout&rdquo;.
Individual plots are organized in one row and two columns. As youmight
guess, there is also an mfcol parameter to plot columnwise. In a 2&times; 2 lay-
out, the difference is whether plot no. 2 is drawn in the top right or bottom
left corner.
</p>
<p>Notice that it is necessary to reset the layout parameter to c(1,1) at the
end unless you also want two plots side by side subsequently.
</p>
<p>4.3 Summary statistics by groups
</p>
<p>When dealing with grouped data, you will often want to have vari-
ous summary statistics computed within groups; for example, a table of
means and standard deviations. To this end, you can use tapply (see Sec-
tion 1.2.15). Here is an example concerning the folate concentration in red
blood cells according to three types of ventilation during anesthesia (Alt-</p>
<p/>
</div>
<div class="page"><p/>
<p>76 4. Descriptive statistics and graphics
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
</p>
<p>&minus;
2
</p>
<p>&minus;
1
</p>
<p>0
1
</p>
<p>Figure 4.5. Boxplots for IgM and log IgM.
</p>
<p>man, 1991, p. 208). We return to this example in Section 7.1, which also
contains the explanation of the category names.
</p>
<p>&gt; attach(red.cell.folate)
</p>
<p>&gt; tapply(folate,ventilation,mean)
</p>
<p>N2O+O2,24h N2O+O2,op O2,24h
</p>
<p>316.6250 256.4444 278.0000
</p>
<p>The tapply call takes the folate variable, splits it according to
ventilation, and computes the mean for each group. In the same way,
standard deviations and the number of observations in the groups can be
computed.
</p>
<p>&gt; tapply(folate,ventilation,sd)
</p>
<p>N2O+O2,24h N2O+O2,op O2,24h
</p>
<p>58.71709 37.12180 33.75648
</p>
<p>&gt; tapply(folate,ventilation,length)
</p>
<p>N2O+O2,24h N2O+O2,op O2,24h
</p>
<p>8 9 5
</p>
<p>Try something like this for a nicer display:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Summary statistics by groups 77
</p>
<p>&gt; xbar &lt;- tapply(folate, ventilation, mean)
</p>
<p>&gt; s &lt;- tapply(folate, ventilation, sd)
</p>
<p>&gt; n &lt;- tapply(folate, ventilation, length)
</p>
<p>&gt; cbind(mean=xbar, std.dev=s, n=n)
</p>
<p>mean std.dev n
</p>
<p>N2O+O2,24h 316.6250 58.71709 8
</p>
<p>N2O+O2,op 256.4444 37.12180 9
</p>
<p>O2,24h 278.0000 33.75648 5
</p>
<p>For the juul data, we might want the mean igf1 by tanner group, but
of course we run into the problem of missing values again:
</p>
<p>&gt; tapply(igf1, tanner, mean)
</p>
<p>I II III IV V
</p>
<p>NA NA NA NA NA
</p>
<p>We need to get tapply to pass na.rm=T as a parameter to mean to make
it exclude the missing values. This is achieved simply by passing it as an
additional argument to tapply.
</p>
<p>&gt; tapply(igf1, tanner, mean, na.rm=T)
</p>
<p>I II III IV V
</p>
<p>207.4727 352.6714 483.2222 513.0172 465.3344
</p>
<p>The functions aggregate and by are variations on the same topic. The
former is very much like tapply, except that it works on an entire data
frame and presents its results as a data frame. This is useful for presenting
many variables at once; e.g.,
</p>
<p>&gt; aggregate(juul[c("age","igf1")],
</p>
<p>+ list(sex=juul$sex), mean, na.rm=T)
</p>
<p>sex age igf1
</p>
<p>1 M 15.38436 310.8866
</p>
<p>2 F 14.84363 368.1006
</p>
<p>Notice that the grouping argument in this case must be a list, even when
it is one-dimensional, and that the names of the list elements get used
as column names in the output. Notice also that since the function is ap-
plied to all columns of the data frame, you may have to choose a subset of
columns, in this case the numeric variables.
</p>
<p>The indexing variable is not necessarily part of the data frame that is being
aggregated, and there is no attempt at &ldquo;smart evaluation&rdquo; as there is in
subset, so you have to spell out juul$sex. You can also use the fact
that data frames are list-like and say
</p>
<p>&gt; aggregate(juul[c("age","igf1")], juul["sex"], mean, na.rm=T)
</p>
<p>sex age igf1
</p>
<p>1 M 15.38436 310.8866
</p>
<p>2 F 14.84363 368.1006</p>
<p/>
</div>
<div class="page"><p/>
<p>78 4. Descriptive statistics and graphics
</p>
<p>(the &ldquo;trick&rdquo; being that indexing a data frame with single brackets yields a
data frame as the result).
</p>
<p>The by function is again similar, but different. The difference is that the
function now takes an entire (sub-) data frame as its argument, so that
you can for instance summarize the Juul data by sex as follows:
</p>
<p>&gt; by(juul, juul["sex"], summary)
</p>
<p>sex: M
</p>
<p>age menarche sex igf1 tanner
</p>
<p>Min. : 0.17 No : 0 M:621 Min. : 29.0 I :291
</p>
<p>1st Qu.: 8.85 Yes : 0 F: 0 1st Qu.:176.0 II : 55
</p>
<p>Median :12.38 NA&rsquo;s:621 Median :280.0 III : 34
</p>
<p>Mean :15.38 Mean :310.9 IV : 41
</p>
<p>3rd Qu.:16.77 3rd Qu.:430.2 V :124
</p>
<p>Max. :83.00 Max. :915.0 NA&rsquo;s: 76
</p>
<p>NA&rsquo;s :145.0
</p>
<p>testvol
</p>
<p>Min. : 1.000
</p>
<p>1st Qu.: 1.000
</p>
<p>Median : 3.000
</p>
<p>Mean : 7.896
</p>
<p>3rd Qu.: 15.000
</p>
<p>Max. : 30.000
</p>
<p>NA&rsquo;s :141.000
</p>
<p>-------------------------------------------------
</p>
<p>sex: F
</p>
<p>age menarche sex igf1 tanner
</p>
<p>Min. : 0.25 No :369 M: 0 Min. : 25.0 I :224
</p>
<p>1st Qu.: 9.30 Yes :335 F:713 1st Qu.:233.0 II : 48
</p>
<p>Median :12.80 NA&rsquo;s: 9 Median :352.0 III : 38
</p>
<p>Mean :14.84 Mean :368.1 IV : 40
</p>
<p>3rd Qu.:16.93 3rd Qu.:483.0 V :204
</p>
<p>Max. :75.12 Max. :914.0 NA&rsquo;s:159
</p>
<p>NA&rsquo;s :176.0
</p>
<p>testvol
</p>
<p>Min. : NA
</p>
<p>1st Qu.: NA
</p>
<p>Median : NA
</p>
<p>Mean :NaN
</p>
<p>3rd Qu.: NA
</p>
<p>Max. : NA
</p>
<p>NA&rsquo;s :713
</p>
<p>The result of the call to by is actually a list of objects that has has been
wrapped as an object of class "by" and printed using a print method for
that class. You can assign the result to a variable and access the result for
each subgroup using standard list indexing.
</p>
<p>The same technique can also be used to generate more elaborate statistical
analyses for each group.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Graphics for grouped data 79
</p>
<p>4.4 Graphics for grouped data
</p>
<p>In dealing with grouped data, it is important to be able not only to create
plots for each group but also to compare the plots between groups. In
this section we review some general graphical techniques that allow us to
display similar plots for several groups on the same page. Some functions
have specific features for displaying data from more than one group.
</p>
<p>4.4.1 Histograms
</p>
<p>We have already seen in Section 4.2.1 how to obtain a histogram simply by
typing hist(x), where x is the variable containing the data. R will then
choose a number of groups so that a reasonable number of data points fall
in each bin while at the same time ensuring that the cutpoints are &ldquo;pretty&rdquo;
numbers on the x-axis.
</p>
<p>It is also mentioned there that an alternative number of intervals can be
set via the argument breaks, although you do not always get exactly
the number you asked for since R reserves the right to choose &ldquo;pretty&rdquo;
column boundaries. For instance, multiples of 0.5 MJ are chosen in the
following example using the energy data introduced in Section 1.2.14 on
the 24-hour energy expenditure for two groups of women.
</p>
<p>In this example, some further techniques of general use are illustrated.
The end result is seen in Figure 4.6, but first we must fetch the data:
</p>
<p>&gt; attach(energy)
</p>
<p>&gt; expend.lean &lt;- expend[stature=="lean"]
</p>
<p>&gt; expend.obese &lt;- expend[stature=="obese"]
</p>
<p>Notice howwe separate the expend vector in the energy data frame into
two vectors according to the value of the factor stature.
</p>
<p>Now we do the actual plotting:
</p>
<p>&gt; par(mfrow=c(2,1))
</p>
<p>&gt; hist(expend.lean,breaks=10,xlim=c(5,13),ylim=c(0,4),col="white")
</p>
<p>&gt; hist(expend.obese,breaks=10,xlim=c(5,13),ylim=c(0,4),col="grey")
</p>
<p>&gt; par(mfrow=c(1,1))
</p>
<p>We set par(mfrow=c(2,1)) to get the two histograms in the same plot.
In the hist commands themselves, we used the breaks argument as
already mentioned and col, whose effect should be rather obvious. We
also used xlim and ylim to get the same x and y axes in the two plots.
However, it is a coincidence that the columns have the same width.</p>
<p/>
</div>
<div class="page"><p/>
<p>80 4. Descriptive statistics and graphics
</p>
<p>Histogram of expend.lean
</p>
<p>expend.lean
</p>
<p>F
re
</p>
<p>q
u
e
n
c
y
</p>
<p>6 8 10 12
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
</p>
<p>Histogram of expend.obese
</p>
<p>expend.obese
</p>
<p>F
re
</p>
<p>q
u
e
n
c
y
</p>
<p>6 8 10 12
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
</p>
<p>Figure 4.6. Histograms with refinements.
</p>
<p>As a practical remark, when working with plots like the above, where
more than a single line of code is required, it gets cumbersome to use com-
mand recall in the R console window every time something needs to be
changed. A better idea may be to start up a script window or a plain-text
editor and cut and paste entire blocks of code from there (see Section 2.1.3).
You might also take it as an incentive to start writing simple functions.
</p>
<p>4.4.2 Parallel boxplots
</p>
<p>You might want a set of boxplots from several groups in the same frame.
boxplot can handle this both when data are given in the form of sepa-
rate vectors from each group and when data are in one long vector and
a parallel vector or factor defines the grouping. To illustrate the latter, we
use the energy data introduced in Section 1.2.14.
</p>
<p>Figure 4.7 is created as follows:
</p>
<p>&gt; boxplot(expend ~ stature)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Graphics for grouped data 81
</p>
<p>lean obese
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>1
0
</p>
<p>1
1
</p>
<p>1
2
</p>
<p>1
3
</p>
<p>Figure 4.7. Parallel boxplot.
</p>
<p>We could also have based the plot on the separate vectors expend.lean
and expend.obese. In that case, a syntax is used that specifies the
vectors as two separate arguments:
</p>
<p>&gt; boxplot(expend.lean,expend.obese)
</p>
<p>The plot is not shown here, but the only difference lies in the labelling
of the x-axis. There is also a third form, where data are given as a single
argument that is a list of vectors.
</p>
<p>The bottom plot has been made using the complete expend vector and
the grouping variable fstature.
</p>
<p>Notation of the type y ~ x should be read &ldquo;y described using x&rdquo;. This is
the first example we see of a model formula. We see many more examples
of model formulas later on.
</p>
<p>4.4.3 Stripcharts
</p>
<p>The boxplots made in the preceding section show a &ldquo;Laurel &amp; Hardy&rdquo;
effect that is not really well founded in the data. The cause is that the in-</p>
<p/>
</div>
<div class="page"><p/>
<p>82 4. Descriptive statistics and graphics
</p>
<p>6 7 8 9 10 11 12 13
</p>
<p>le
a
n
</p>
<p>o
b
</p>
<p>e
s
e
</p>
<p>6 7 8 9 10 11 12 13
</p>
<p>le
a
n
</p>
<p>o
b
</p>
<p>e
s
e
</p>
<p>6 7 8 9 10 11 12 13
</p>
<p>le
a
n
</p>
<p>o
b
e
s
e
</p>
<p>6 7 8 9 10 11 12 13
</p>
<p>le
a
n
</p>
<p>o
b
e
s
e
</p>
<p>Figure 4.8. Stripcharts in four variations.
</p>
<p>terquartile range is quite a bit larger in one group than in the other, making
the boxplot appear &ldquo;fatter&rdquo;. With groups as small as these, the quartiles
will be quite inaccurately determined, and it may therefore be more desir-
able to plot the raw data. If you were to do this by hand, youmight draw a
dot diagram where every number is marked with a dot on a number line.
R&rsquo;s automated variant of this is the function stripchart. Four variants
of stripcharts are shown in Figure 4.8.
</p>
<p>The four plots were created as follows:
</p>
<p>&gt; opar &lt;- par(mfrow=c(2,2), mex=0.8, mar=c(3,3,2,1)+.1)
</p>
<p>&gt; stripchart(expend ~ stature)
</p>
<p>&gt; stripchart(expend ~ stature, method="stack")
</p>
<p>&gt; stripchart(expend ~ stature, method="jitter")
</p>
<p>&gt; stripchart(expend ~ stature, method="jitter", jitter=.03)
</p>
<p>&gt; par(opar)
</p>
<p>Notice that a little parmagic was used to reduce the spacing between the
four plots. The mex setting reduces the interline distance, and mar reduces
the number of lines that surround the plot region. This can be done for
these plots since they have neither main title, subtitle, nor x and y labels.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Tables 83
</p>
<p>All the original values of the changed settings can be stored in a variable
(here opar) and reestablished with par(opar).
</p>
<p>The first plot is a standard stripchart, where the points are simply plotted
on a line. The problem with this is that some points can become invisible
because they are overplotted. This is why there is a method argument,
which can be set to either "stack" or "jitter".
</p>
<p>The former method stacks points with identical values, but it only does
so if data are completely identical, so in the upper right plot, it is only the
two replicates of 7.48 that get stacked, whereas 8.08, 8.09, and 8.11 are still
plotted in almost the same spot.
</p>
<p>The &ldquo;jitter&rdquo; method offsets all points a random amount vertically. The
standard jittering on plot no. 3 (bottom left) is a bit large; it may be prefer-
able to make it clearer that data are placed along a horizontal line. For that
purpose, you can set jitter lower than the default of 0.1, which is done
in the fourth plot.
</p>
<p>In this example we have not bothered to specify data in several forms as
we did for boxplot but used expend~stature throughout. We could
also have written
</p>
<p>stripchart(list(lean=expend.lean, obese=expend.obese))
</p>
<p>but stripchart(expend.lean, expend.obese) cannot be used.
</p>
<p>4.5 Tables
</p>
<p>Categorical data are usually described in the form of tables. This section
outlines how you can create tables from your data and calculate relative
frequencies.
</p>
<p>4.5.1 Generating tables
</p>
<p>We deal mainly with two-way tables. In the first example, we enter a table
directly, as is required for tables taken from a book or a journal article.
</p>
<p>A two-way table can be entered as a matrix object (Section 1.2.7). Altman
(1991, p. 242) contains an example on caffeine consumption by marital
status among women giving birth. That table may be input as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>84 4. Descriptive statistics and graphics
</p>
<p>&gt; caff.marital &lt;- matrix(c(652,1537,598,242,36,46,38,21,218
</p>
<p>+ ,327,106,67),
</p>
<p>+ nrow=3,byrow=T)
</p>
<p>&gt; caff.marital
</p>
<p>[,1] [,2] [,3] [,4]
</p>
<p>[1,] 652 1537 598 242
</p>
<p>[2,] 36 46 38 21
</p>
<p>[3,] 218 327 106 67
</p>
<p>The matrix function needs an argument containing the table values as
a single vector and also the number of rows in the argument nrow. By
default, the values are entered columnwise; if rowwise entry is desired,
then you need to specify byrow=T.
</p>
<p>You might also give the number of columns instead of rows using ncol.
If exactly one of ncol and nrow is given, R will compute the other one so
that it fits the number of values. If both ncol and nrow are given and it
does not fit the number of values, the values will be &ldquo;recycled&rdquo;, which in
some (other!) circumstances can be useful.
</p>
<p>To get readable printouts, you can add row and column names to the
matrices.
</p>
<p>&gt; colnames(caff.marital) &lt;- c("0","1-150","151-300","&gt;300")
</p>
<p>&gt; rownames(caff.marital) &lt;- c("Married","Prev.married","Single")
</p>
<p>&gt; caff.marital
</p>
<p>0 1-150 151-300 &gt;300
</p>
<p>Married 652 1537 598 242
</p>
<p>Prev.married 36 46 38 21
</p>
<p>Single 218 327 106 67
</p>
<p>Furthermore, you can name the row and column names as follows. This
is particularly useful if you are generating many tables with similar
classification criteria.
</p>
<p>&gt; names(dimnames(caff.marital)) &lt;- c("marital","consumption")
</p>
<p>&gt; caff.marital
</p>
<p>consumption
</p>
<p>marital 0 1-150 151-300 &gt;300
</p>
<p>Married 652 1537 598 242
</p>
<p>Prev.married 36 46 38 21
</p>
<p>Single 218 327 106 67
</p>
<p>Actually, I glossed over something. Tables are not completely equivalent
to matrices. There is a "table" class for which special methods exist,
and you can convert to that class using as.table(caff.marital). The
table function below returns an object of class "table".</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Tables 85
</p>
<p>Formost elementary purposes, you can usematrices where two-dimensio-
nal tables are expected. One important case where you do need as.table
is when converting a table to a data frame of counts:
</p>
<p>&gt; as.data.frame(as.table(caff.marital))
</p>
<p>marital consumption Freq
</p>
<p>1 Married 0 652
</p>
<p>2 Prev.married 0 36
</p>
<p>3 Single 0 218
</p>
<p>4 Married 1-150 1537
</p>
<p>5 Prev.married 1-150 46
</p>
<p>6 Single 1-150 327
</p>
<p>7 Married 151-300 598
</p>
<p>8 Prev.married 151-300 38
</p>
<p>9 Single 151-300 106
</p>
<p>10 Married &gt;300 242
</p>
<p>11 Prev.married &gt;300 21
</p>
<p>12 Single &gt;300 67
</p>
<p>In practice, the more frequent case is that you have a data frame with
variables for each person in a data set. In that case, you should do the
tabulation with table, xtabs, or ftable. These functions will gener-
ally work for tabulating numeric vectors as well as factor variables, but
the latter will have their levels used for row and column names automati-
cally. Hence, it is recommended to convert numerically coded categorical
data into factors. The table function is the oldest and most basic of the
three. The two others offer formula-based interfaces and better printing of
multiway tables.
</p>
<p>The data set juul was introduced on p. 68. Here we look at some other
variables in that data set, namely sex and menarche; the latter indicates
whether or not a girl has had her first period.We can generate some simple
tables as follows:
</p>
<p>&gt; table(sex)
</p>
<p>sex
</p>
<p>M F
</p>
<p>621 713
</p>
<p>&gt; table(sex,menarche)
</p>
<p>menarche
</p>
<p>sex No Yes
</p>
<p>M 0 0
</p>
<p>F 369 335
</p>
<p>&gt; table(menarche,tanner)
</p>
<p>tanner
</p>
<p>menarche I II III IV V
</p>
<p>No 221 43 32 14 2
</p>
<p>Yes 1 1 5 26 202</p>
<p/>
</div>
<div class="page"><p/>
<p>86 4. Descriptive statistics and graphics
</p>
<p>Of course, the table of menarche versus sex is just a check on internal con-
sistency of the data. The table of menarche versus Tanner stage of puberty
is more interesting.
</p>
<p>There are also tables with more than two sides, but not many simple sta-
tistical functions use them. Briefly, to tabulate such data, just write, for
example, table(factor1,factor2,factor3). To input a table of cell
counts, use the array function (an analogue of matrix).
</p>
<p>The xtabs function is quite similar to table except that it uses a model
formula interface. This most often uses a one-sided formula where you
just list the classification variables separated by +.
</p>
<p>&gt; xtabs(~ tanner + sex, data=juul)
</p>
<p>sex
</p>
<p>tanner M F
</p>
<p>I 291 224
</p>
<p>II 55 48
</p>
<p>III 34 38
</p>
<p>IV 41 40
</p>
<p>V 124 204
</p>
<p>Notice how the interface allows you to refer to variables in a data frame
without attaching it. The empty left-hand side can be replaced by a vector
of counts in order to handle pretabulated data.
</p>
<p>The formatting of multiway tables from table or xtabs is not really nice;
e.g.,
</p>
<p>&gt; xtabs(~ dgn + diab + coma, data=stroke)
</p>
<p>, , coma = No
</p>
<p>diab
</p>
<p>dgn No Yes
</p>
<p>ICH 53 6
</p>
<p>ID 143 21
</p>
<p>INF 411 64
</p>
<p>SAH 38 0
</p>
<p>, , coma = Yes
</p>
<p>diab
</p>
<p>dgn No Yes
</p>
<p>ICH 19 1
</p>
<p>ID 23 3
</p>
<p>INF 23 2
</p>
<p>SAH 9 0
</p>
<p>As you add dimensions, you get more of these two-sided subtables and
it becomes rather easy to lose track. This is where ftable comes in. This
function creates &ldquo;flat&rdquo; tables; e.g., like this:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Tables 87
</p>
<p>&gt; ftable(coma + diab ~ dgn, data=stroke)
</p>
<p>coma No Yes
</p>
<p>diab No Yes No Yes
</p>
<p>dgn
</p>
<p>ICH 53 6 19 1
</p>
<p>ID 143 21 23 3
</p>
<p>INF 411 64 23 2
</p>
<p>SAH 38 0 9 0
</p>
<p>That is, variables on the left-hand side tabulate across the page and those
on the right tabulate downwards. ftable works on raw data as shown,
but its data argument can also be a table as generated by one of the other
functions.
</p>
<p>Like any matrix, a table can be transposed with the t function:
</p>
<p>&gt; t(caff.marital)
</p>
<p>marital
</p>
<p>consumption Married Prev.married Single
</p>
<p>0 652 36 218
</p>
<p>1-150 1537 46 327
</p>
<p>151-300 598 38 106
</p>
<p>&gt;300 242 21 67
</p>
<p>For multiway tables, exchanging indices (generalized transposition) is
done by aperm.
</p>
<p>4.5.2 Marginal tables and relative frequency
</p>
<p>It is often desired to compute marginal tables; that is, the sums of the
counts along one or the other dimension of a table. Due to missing val-
ues, this might not coincide with just tabulating a single factor. This is
done fairly easily using the apply function (Section 1.2.15), but there is
also a simplified version called margin.table, described below.
</p>
<p>First, we need to generate the table itself:
</p>
<p>&gt; tanner.sex &lt;- table(tanner,sex)
</p>
<p>(tanner.sex is an arbitrarily chosen name for the crosstable.)
</p>
<p>&gt; tanner.sex
</p>
<p>sex
</p>
<p>tanner M F
</p>
<p>I 291 224
</p>
<p>II 55 48
</p>
<p>III 34 38
</p>
<p>IV 41 40
</p>
<p>V 124 204</p>
<p/>
</div>
<div class="page"><p/>
<p>88 4. Descriptive statistics and graphics
</p>
<p>Then we compute the marginal tables:
</p>
<p>&gt; margin.table(tanner.sex,1)
</p>
<p>tanner
</p>
<p>I II III IV V
</p>
<p>515 103 72 81 328
</p>
<p>&gt; margin.table(tanner.sex,2)
</p>
<p>sex
</p>
<p>M F
</p>
<p>545 554
</p>
<p>The second argument to margin.table is the number of the marginal
index: 1 and 2 give row and column totals, respectively.
</p>
<p>Relative frequencies in a table are generally expressed as proportions of
the row or column totals. Tables of relative frequencies can be constructed
using prop.table as follows:
</p>
<p>&gt; prop.table(tanner.sex,1)
</p>
<p>sex
</p>
<p>tanner M F
</p>
<p>I 0.5650485 0.4349515
</p>
<p>II 0.5339806 0.4660194
</p>
<p>III 0.4722222 0.5277778
</p>
<p>IV 0.5061728 0.4938272
</p>
<p>V 0.3780488 0.6219512
</p>
<p>Note that the rows (1st index) sum to 1. If a table of percentages is desired,
just multiply the entire table by 100.
</p>
<p>prop.table cannot be used to express the numbers relative to the grand
total of the table, but you can of course always write
</p>
<p>&gt; tanner.sex/sum(tanner.sex)
</p>
<p>sex
</p>
<p>tanner M F
</p>
<p>I 0.26478617 0.20382166
</p>
<p>II 0.05004550 0.04367607
</p>
<p>III 0.03093722 0.03457689
</p>
<p>IV 0.03730664 0.03639672
</p>
<p>V 0.11282985 0.18562329
</p>
<p>The functions margin.table and prop.table also work on multiway
tables &mdash; the margin argument can be a vector if the relevant margin has
two or more dimensions.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Graphical display of tables 89
</p>
<p>0 1&minus;150 151&minus;300 &gt;300
</p>
<p>0
5
0
0
</p>
<p>1
0
0
0
</p>
<p>1
5
0
0
</p>
<p>Figure 4.9. Simple barplot of total caffeine consumption.
</p>
<p>4.6 Graphical display of tables
</p>
<p>For presentation purposes, it may be desirable to display a graph rather
than a table of counts or percentages. In this section, the main methods for
doing this are described.
</p>
<p>4.6.1 Barplots
</p>
<p>Barplots are made using barplot. This function takes an argument,
which can be a vector or a matrix. The simplest variant goes as follows
(Figure 4.9):
</p>
<p>&gt; total.caff &lt;- margin.table(caff.marital,2)
</p>
<p>&gt; total.caff
</p>
<p>consumption
</p>
<p>0 1-150 151-300 &gt;300
</p>
<p>906 1910 742 330
</p>
<p>&gt; barplot(total.caff, col="white")</p>
<p/>
</div>
<div class="page"><p/>
<p>90 4. Descriptive statistics and graphics
</p>
<p>0 1&minus;150 &gt;300
</p>
<p>0
5
</p>
<p>0
0
</p>
<p>1
5
</p>
<p>0
0
</p>
<p>Married Single
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>0
2
</p>
<p>0
0
</p>
<p>0
3
</p>
<p>0
0
</p>
<p>0
</p>
<p>Married Single
</p>
<p>0
4
0
0
</p>
<p>8
0
0
</p>
<p>1
4
0
0
</p>
<p>Married Single
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>Figure 4.10. Four variants of barplot on a two-way table.
</p>
<p>Without the col="white" argument, the plot comes out in colour, but
this is not suitable for a black and white book illustration.
</p>
<p>If the argument is a matrix, then barplot creates by default a &ldquo;stacked
barplot&rdquo;, where the columns are partitioned according to the contri-
butions from different rows of the table. If you want to place the
row contributions beside each other instead, you can use the argu-
ment beside=T. A series of variants is found in Figure 4.10, which is
constructed as follows:
</p>
<p>&gt; par(mfrow=c(2,2))
</p>
<p>&gt; barplot(caff.marital, col="white")
</p>
<p>&gt; barplot(t(caff.marital), col="white")
</p>
<p>&gt; barplot(t(caff.marital), col="white", beside=T)
</p>
<p>&gt; barplot(prop.table(t(caff.marital),2), col="white", beside=T)
</p>
<p>&gt; par(mfrow=c(1,1))
</p>
<p>In the last three plots, we switched rows and columns with the trans-
position function t. In the very last one, the columns are expressed as
proportions of the total number in the group. Thus, information is lost on
the relative sizes of the marital status groups, but the group of previously
married women (recall that the data set deals with women giving birth)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Graphical display of tables 91
</p>
<p>Married Prev.married Single
</p>
<p>0
</p>
<p>1&minus;150
</p>
<p>151&minus;300
</p>
<p>&gt;300
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>0
.5
</p>
<p>Figure 4.11. Bar plot with specified colours and legend.
</p>
<p>is so small that it otherwise becomes almost impossible to compare their
caffeine consumption profile with those of the other groups.
</p>
<p>As usual, there are a multitude of ways to &ldquo;prettify&rdquo; the plots. Here is one
possibility (Figure 4.11):
</p>
<p>&gt; barplot(prop.table(t(caff.marital),2),beside=T,
</p>
<p>+ legend.text=colnames(caff.marital),
</p>
<p>+ col=c("white","grey80","grey50","black"))
</p>
<p>Notice that the legend overlaps the top of one of the columns. R is not
designed to be able to find a clear area in which to place the legend.
However, you can get full control of the legend&rsquo;s position if you insert
it explicitly with the legend function. For that purpose, it will be help-
ful to use locator(), which allows you to click a mouse button over the
plot and have the coordinates returned. See p. 209 for more about this.
</p>
<p>4.6.2 Dotcharts
</p>
<p>The Cleveland dotcharts, named after William S. Cleveland (1994), can be
employed to study a table from both sides at the same time. They contain</p>
<p/>
</div>
<div class="page"><p/>
<p>92 4. Descriptive statistics and graphics
</p>
<p>0
</p>
<p>1&minus;150
</p>
<p>151&minus;300
</p>
<p>&gt;300
</p>
<p>0
</p>
<p>1&minus;150
</p>
<p>151&minus;300
</p>
<p>&gt;300
</p>
<p>0
</p>
<p>1&minus;150
</p>
<p>151&minus;300
</p>
<p>&gt;300
</p>
<p>Married
</p>
<p>Prev.married
</p>
<p>Single
</p>
<p>0 500 1000 1500
</p>
<p>Figure 4.12. Dotchart of caffeine consumption.
</p>
<p>the same information as barplots with beside=T but give quite a differ-
ent visual impression. We content ourselves with a single example here
(Figure 4.12):
</p>
<p>&gt; dotchart(t(caff.marital), lcolor="black")
</p>
<p>(The line colour was changed from the default "gray" because it tends to
be hard to see in print.)
</p>
<p>4.6.3 Piecharts
</p>
<p>Piecharts are traditionally frowned upon by statisticians because they are
so often used to make trivial data look impressive and are difficult to
decode for the human mind. They very rarely contain information that
would not have been at least as effectively conveyed in a barplot. Once
in a while they are useful, though, and it is no problem to get R to draw
them. Here is a way to represent the table of caffeine consumption versus
marital status (Figure 4.13; see Section 4.4.3 for an explanation of the &ldquo;par
magic&rdquo; used to reduce the space between the subplots):</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Exercises 93
</p>
<p>0
</p>
<p>1&minus;150
</p>
<p>151&minus;300
</p>
<p>&gt;300
</p>
<p>Married
</p>
<p>0
</p>
<p>1&minus;150
</p>
<p>151&minus;300
</p>
<p>&gt;300
</p>
<p>Previously married
</p>
<p>0
</p>
<p>1&minus;150
</p>
<p>151&minus;300
</p>
<p>&gt;300
</p>
<p>Single
</p>
<p>Figure 4.13. Pie charts of caffeine consumption according to marital status.
</p>
<p>&gt; opar &lt;- par(mfrow=c(2,2),mex=0.8, mar=c(1,1,2,1))
</p>
<p>&gt; slices &lt;- c("white","grey80","grey50","black")
</p>
<p>&gt; pie(caff.marital["Married",], main="Married", col=slices)
</p>
<p>&gt; pie(caff.marital["Prev.married",],
</p>
<p>+ main="Previously married", col=slices)
</p>
<p>&gt; pie(caff.marital["Single",], main="Single", col=slices)
</p>
<p>&gt; par(opar)
</p>
<p>The col argument sets the colour of the pie slices.
</p>
<p>There are more possibilities with piechart. The help page for pie con-
tains an illustrative example concerning the distribution of pie sales (!) by
pie type.
</p>
<p>4.7 Exercises
</p>
<p>4.1 Explore the possibilities for different kinds of line and point plots.
Vary the plot symbol, line type, line width, and colour.</p>
<p/>
</div>
<div class="page"><p/>
<p>94 4. Descriptive statistics and graphics
</p>
<p>4.2 If you make a plot like plot(rnorm(10),type="o") with over-
plotted lines and points, the lines will be visible inside the plotting
symbols. How can this be avoided?
</p>
<p>4.3 How can you overlay two qqnorm plots in the same plotting area?
What goes wrong if you try to generate the plot using type="l", and
how do you avoid that?
</p>
<p>4.4 Plot a histogram for the react data set. Since these data are highly
discretized, the histogram will be biased. Why? You may want to try
truehist from the MASS package as a replacement.
</p>
<p>4.5 Generate a sample vector z of five random numbers from the uni-
form distribution, and plot quantile(z,x) as a function of x (use
curve, for instance).</p>
<p/>
</div>
<div class="page"><p/>
<p>5
One- and two-sample tests
</p>
<p>Most of the rest of this book describes applications of R for actual sta-
tistical analysis. The focus to some extent shifts from explanation of the
syntax to description of the output and specific arguments to the relevant
functions.
</p>
<p>Some of the most basic statistical tests deal with comparing continuous
data either between two groups or against an a priori stipulated value.
This is the topic for this chapter.
</p>
<p>Two functions are introduced here, namely t.test and wilcox.test
for t tests and Wilcoxon tests, respectively. Both can be applied to one-
and two-sample problems as well as paired data. Notice that the &ldquo;two-
sample Wilcoxon test&rdquo; is the same as the one called the &ldquo;Mann&ndash;Whitney
test&rdquo; in many textbooks.
</p>
<p>5.1 One-sample t test
</p>
<p>The t tests are based on an assumption that data come from the nor-
mal distribution. In the one-sample case we thus have data x1, . . . , xn
assumed to be independent realizations of random variables with distri-
bution N(&micro;, σ2), which denotes the normal distribution with mean &micro; and
variance σ2, and we wish to test the null hypothesis that &micro; = &micro;0. We can
estimate the parameters &micro; and σ by the empirical mean x̄ and standard
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_5, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>96 5. One- and two-sample tests
</p>
<p>deviation s, although we must realize that we could never pinpoint their
values exactly.
</p>
<p>The key concept is that of the standard error of the mean, or SEM. This de-
scribes the variation of the average of n random values with mean &micro; and
variance σ2. This value is
</p>
<p>SEM = σ/
&radic;
n
</p>
<p>and means that if you were to repeat the entire experiment several times
and calculate an average for each experiment, then these averages would
follow a distribution that is narrower than that of the original distribu-
tion. The crucial point is that even based on a single sample, it is possible
to calculate an empirical SEM as s/
</p>
<p>&radic;
n using the empirical standard de-
</p>
<p>viation of the sample. This value will tell us how far the observed mean
may reasonably have strayed from its true value. For normally distributed
data, the rule of thumb is that there is a 95% probability of staying within
&micro;&plusmn; 2σ, so we would expect that if &micro;0 were the true mean, then x̄ should
be within 2 SEMs of it. Formally, you calculate
</p>
<p>t =
x̄&minus; &micro;0
SEM
</p>
<p>and see whether this falls within an acceptance region outside which t
should fall with probability equal to a specified significance level. This is
often chosen as 5%, in which case the acceptance region is almost, but not
exactly, the interval from &minus;2 to 2.
In small samples, it is necessary to correct for the fact that an empirical
SEM is used and that the distribution of t therefore has somewhat &ldquo;heavier
tails&rdquo; than the N(0, 1): Large deviations happen more frequently than in
the normal distribution since they can result from normalizing with an
SEM that is too small. The correct values for the acceptance region can
be looked up as quantiles in the t distribution with f = n&minus; 1 degrees of
freedom.
</p>
<p>If t falls outside the acceptance region, then we reject the null hypothesis
at the chosen significance level. Alternatively (and equivalently), you can
calculate the p-value, which is the probability of obtaining a value as nu-
merically large as or larger than the observed t and reject the hypothesis
if the p-value is less than the significance level.
</p>
<p>Sometimes you have prior information on the direction of an effect; for
instance, that all plausible mechanisms that would cause &micro; not to equal &micro;0
would tend to make it bigger. In those cases, you may choose to reject the
hypothesis only if t falls in the upper tail of the distribution. This is known
as testing against a one-sided alternative. Since removing the lower tail from
the rejection region effectively halves the significance level, a one-sided
test at a given level will have a smaller cutoff point. Similarly, p-values</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 One-sample t test 97
</p>
<p>are calculated as the probability of a larger value than observed rather
than a numerically larger one, effectively halving the p-value as long as
the observed effect is in the stipulated direction. One-sided tests should
be used with some care, preferably only when there is a clear statement
of the intent to use them in the study protocol. Switching to a one-sided
test to make an otherwise nonsignificant result significant could easily be
regarded as dishonest.
</p>
<p>Here is an example concerning daily energy intake in kJ for 11 women
(Altman, 1991, p. 183). First, the values are placed in a data vector:
</p>
<p>&gt; daily.intake &lt;- c(5260,5470,5640,6180,6390,6515,
</p>
<p>+ 6805,7515,7515,8230,8770)
</p>
<p>Let us first look at some simple summary statistics, even though these are
hardly necessary for such a small data set:
</p>
<p>&gt; mean(daily.intake)
</p>
<p>[1] 6753.636
</p>
<p>&gt; sd(daily.intake)
</p>
<p>[1] 1142.123
</p>
<p>&gt; quantile(daily.intake)
</p>
<p>0% 25% 50% 75% 100%
</p>
<p>5260 5910 6515 7515 8770
</p>
<p>You might wish to investigate whether the women&rsquo;s energy intake devi-
ates systematically from a recommended value of 7725 kJ. Assuming that
data come from a normal distribution, the object is to test whether this
distribution might have mean &micro; = 7725. This is done with t.test as
follows:
</p>
<p>&gt; t.test(daily.intake,mu=7725)
</p>
<p>One Sample t-test
</p>
<p>data: daily.intake
</p>
<p>t = -2.8208, df = 10, p-value = 0.01814
</p>
<p>alternative hypothesis: true mean is not equal to 7725
</p>
<p>95 percent confidence interval:
</p>
<p>5986.348 7520.925
</p>
<p>sample estimates:
</p>
<p>mean of x
</p>
<p>6753.636
</p>
<p>This is an example of the exact same type as used in the introductory Sec-
tion 1.1.4. The description of the output is quite superficial there. Here it
is explained more thoroughly.
</p>
<p>The layout is common to many of the standard statistical tests, and a
&ldquo;dissection&rdquo; is given in the following:</p>
<p/>
</div>
<div class="page"><p/>
<p>98 5. One- and two-sample tests
</p>
<p>One Sample t-test
</p>
<p>This should be self-explanatory. It is simply a description of the test that
we have asked for. Notice that, by looking at the format of the function
call, t.test has automatically found out that a one-sample test is desired.
</p>
<p>data: daily.intake
</p>
<p>This tells us which data are being tested. Of course, this will be obvious
unless output has been separated from the command that generated it.
This can happen, for example, when using the source function to read
commands from an external file.
</p>
<p>t = -2.8208, df = 10, p-value = 0.01814
</p>
<p>This is where it begins to get interesting. We get the t statistic, the asso-
ciated degrees of freedom, and the exact p-value. We do not need to use
a table of the t distribution to look up which quantiles the t-value can be
found between. You can immediately see that p &lt; 0.05 and thus that (us-
ing the customary 5% level of significance) data deviate significantly from
the hypothesis that the mean is 7725.
</p>
<p>alternative hypothesis: true mean is not equal to 7725
</p>
<p>This contains two important pieces of information: (a) the value we
wanted to test whether the mean could be equal to (7725 kJ) and (b) that
the test is two-sided (&ldquo;not equal to&rdquo;).
</p>
<p>95 percent confidence interval:
</p>
<p>5986.348 7520.925
</p>
<p>This is a 95% confidence interval for the true mean; that is, the set of (hy-
pothetical) mean values from which the data do not deviate significantly.
It is based on inverting the t test by solving for the values of &micro;0 that cause
t to lie within its acceptance region. For a 95% confidence interval, the
solution is
</p>
<p>x̄&minus; t0.975( f )&times; SEM &lt; &micro; &lt; x̄ + t0.975( f )&times; SEM
</p>
<p>sample estimates:
</p>
<p>mean of x
</p>
<p>6753.636
</p>
<p>This final item is the observed mean; that is, the (point) estimate of the
true mean.
</p>
<p>The function t.test has a number of optional arguments, three of which
are relevant in one-sample problems. We have already seen the use of mu</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Wilcoxon signed-rank test 99
</p>
<p>to specify the mean value &micro; under the null hypothesis (default is mu=0). In
addition, you can specify that a one-sided test is desired against alterna-
tives greater than &micro; by using alternative="greater" or alternatives
less than &micro; using alternative="less". The third item that can be spec-
ified is the confidence level used for the confidence intervals; you would
write conf.level=0.99 to get a 99% interval.
</p>
<p>Actually, it is often allowable to abbreviate a longish argument specifica-
tion; for instance, it is sufficient to write alt="g" to get the test against
greater alternatives.
</p>
<p>5.2 Wilcoxon signed-rank test
</p>
<p>The t tests are fairly robust against departures from the normal distribu-
tion especially in larger samples, but sometimes youwish to avoidmaking
that assumption. To this end, the distribution-free methods are convenient.
These are generally obtained by replacing data with corresponding order
statistics.
</p>
<p>For the one-sample Wilcoxon test, the procedure is to subtract the the-
oretical &micro;0 and rank the differences according to their numerical value,
ignoring the sign, and then calculate the sum of the positive or negative
ranks. The point is that, assuming only that the distribution is symmetric
around &micro;0, the test statistic corresponds to selecting each number from 1
to n with probability 1/2 and calculating the sum. The distribution of the
test statistic can be calculated exactly, at least in principle. It becomes com-
putationally excessive in large samples, but the distribution is then very
well approximated by a normal distribution.
</p>
<p>Practical application of the Wilcoxon signed-rank test is done almost
exactly like the t test:
</p>
<p>&gt; wilcox.test(daily.intake, mu=7725)
</p>
<p>Wilcoxon signed rank test with continuity correction
</p>
<p>data: daily.intake
</p>
<p>V = 8, p-value = 0.0293
</p>
<p>alternative hypothesis: true location is not equal to 7725
</p>
<p>Warning message:
</p>
<p>In wilcox.test.default(daily.intake, mu = 7725) :
</p>
<p>cannot compute exact p-value with ties
</p>
<p>There is not quite as much output as from t.test due to the fact that
there is no such thing as a parameter estimate in a nonparametric test and
therefore no confidence limits, etc., either. It is, however, possible under</p>
<p/>
</div>
<div class="page"><p/>
<p>100 5. One- and two-sample tests
</p>
<p>some assumptions to define a location measure and calculate confidence
intervals for it. See the help files for wilcox.test for details.
</p>
<p>The relative merits of distribution-free (or nonparametric) versus para-
metric methods such as the t test are a contentious issue. If the model
assumptions of the parametric test are fulfilled, then it will be somewhat
more efficient, on the order of 5% in large samples, although the difference
can be larger in small samples. Notice, for instance, that unless the sample
size is 6 or above, the signed-rank test simply cannot become significant at
the 5% level. This is probably not too important, though; what is more im-
portant is that the apparent lack of assumptions for these tests sometimes
misleads people into using them for data where the observations are not
independent or where a comparison is biased by an important covariate.
</p>
<p>TheWilcoxon tests are susceptible to the problem of ties, where several ob-
servations share the same value. In such cases, you simply use the average
of the tied ranks; for example, if there are four identical values corre-
sponding to places 6 to 9, they will all be assigned the value 7.5. This is
not a problem for the large-sample normal approximations, but the exact
small-sample distributions become much more difficult to calculate and
wilcox.test cannot do so.
</p>
<p>The test statistic V is the sum of the positive ranks. In the example, the
p-value is computed from the normal approximation because of the tie at
7515.
</p>
<p>The function wilcox.test takes arguments mu and alternative,
just like t.test. In addition, it has correct, which turns a continu-
ity correction on or off (the default is &ldquo;on&rdquo;, as seen from the output
title; correct=F turns it off), and exact, which specifies whether exact
tests should be calculated. Recall that &ldquo;on/off&rdquo; options such as these are
specified using logical values that can be either TRUE or FALSE.
</p>
<p>5.3 Two-sample t test
</p>
<p>The two-sample t test is used to test the hypothesis that two samples may
be assumed to come from distributions with the same mean.
</p>
<p>The theory for the two-sample t test is not very different in principle from
that of the one-sample test. Data are now from two groups, x11, . . . , x1n1
and x21, . . . , x2n2 , which we assume are sampled from the normal distribu-
tions N(&micro;1, σ21 ) and N(&micro;2, σ
</p>
<p>2
2 ), and it is desired to test the null hypothesis
</p>
<p>&micro;1 = &micro;2. You then calculate
</p>
<p>t =
x̄2 &minus; x̄1
SEDM</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Two-sample t test 101
</p>
<p>where the standard error of difference of means is
</p>
<p>SEDM =
&radic;
</p>
<p>SEM21 + SEM
2
2
</p>
<p>There are two ways of calculating the SEDM depending on whether or not
you assume that the two groups have the same variance. The &ldquo;classical&rdquo;
approach is to assume that the variances are identical. With this approach,
you first calculate a pooled s based on the standard deviations from the
two groups and plug that value into the SEM. Under the null hypothesis,
the t value will follow a t distribution with n1 + n2&minus; 2 degrees of freedom.
An alternative procedure due to Welch is to calculate the SEMs from the
separate group standard deviations s1 and s2. With this procedure, t is
actually not t-distributed, but its distribution may be approximated by a
t distribution with a number of degrees of freedom that can be calculated
from s1, s2, and the group sizes. This is generally not an integer.
</p>
<p>The Welch procedure is generally considered the safer one. Usually, the
two procedures give very similar results unless both the group sizes and
the standard deviations are very different.
</p>
<p>We return to the daily energy expenditure data (see Section 1.2.14) and
consider the problem of comparing energy expenditures between lean and
obese women.
</p>
<p>&gt; attach(energy)
</p>
<p>&gt; energy
</p>
<p>expend stature
</p>
<p>1 9.21 obese
</p>
<p>2 7.53 lean
</p>
<p>3 7.48 lean
</p>
<p>...
</p>
<p>20 7.58 lean
</p>
<p>21 9.19 obese
</p>
<p>22 8.11 lean
</p>
<p>Notice that the necessary information is contained in two parallel columns
of a data frame. The factor stature contains the group and the numeric
variable expend the energy expenditure in mega-Joules. R allows data in
this format to be analyzed by t.test and wilcox.test using a model
formula specification. An older format (still available) requires you to
specify data from each group in a separate variable, but the newer for-
mat is much more convenient for data that are kept in data frames and
is also more flexible if you later want to group the same response data
according to other criteria.
</p>
<p>The object is to seewhether there is a shift in level between the two groups,
so we apply a t test as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>102 5. One- and two-sample tests
</p>
<p>&gt; t.test(expend~stature)
</p>
<p>Welch Two Sample t-test
</p>
<p>data: expend by stature
</p>
<p>t = -3.8555, df = 15.919, p-value = 0.001411
</p>
<p>alternative hypothesis: true difference in means is not equal to 0
</p>
<p>95 percent confidence interval:
</p>
<p>-3.459167 -1.004081
</p>
<p>sample estimates:
</p>
<p>mean in group lean mean in group obese
</p>
<p>8.066154 10.297778
</p>
<p>Notice the use of the tilde (~) operator to specify that expend is described
by stature.
</p>
<p>The output is not much different from that of the one-sample test. The
confidence interval is for the difference in means and does not contain 0,
which is in accordance with the p-value indicating a significant difference
at the 5% level.
</p>
<p>It isWelch&rsquo;s variant of the t test that is calculated by default. This is the test
where you do not assume that the variance is the same in the two groups,
which (among other things) results in the fractional degrees of freedom.
</p>
<p>To get the usual (textbook) t test, you must specify that you are willing
to assume that the variances are the same. This is done via the optional
argument var.equal=T; that is:
</p>
<p>&gt; t.test(expend~stature, var.equal=T)
</p>
<p>Two Sample t-test
</p>
<p>data: expend by stature
</p>
<p>t = -3.9456, df = 20, p-value = 0.000799
</p>
<p>alternative hypothesis: true difference in means is not equal to 0
</p>
<p>95 percent confidence interval:
</p>
<p>-3.411451 -1.051796
</p>
<p>sample estimates:
</p>
<p>mean in group lean mean in group obese
</p>
<p>8.066154 10.297778
</p>
<p>Notice that the degrees of freedom now has become a whole number,
namely 13 + 9&minus; 2 = 20. The p-value has dropped slightly (from 0.14%
to 0.08%) and the confidence interval is a little narrower, but overall the
changes are slight.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Comparison of variances 103
</p>
<p>5.4 Comparison of variances
</p>
<p>Even though it is possible in R to perform the two-sample t test without
the assumption that the variances are the same, youmay still be interested
in testing that assumption, and R provides the var.test function for that
purpose, implementing an F test on the ratio of the group variances. It is
called the same way as t.test:
</p>
<p>&gt; var.test(expend~stature)
</p>
<p>F test to compare two variances
</p>
<p>data: expend by stature
</p>
<p>F = 0.7844, num df = 12, denom df = 8, p-value = 0.6797
</p>
<p>alternative hypothesis: true ratio of variances is not equal to 1
</p>
<p>95 percent confidence interval:
</p>
<p>0.1867876 2.7547991
</p>
<p>sample estimates:
</p>
<p>ratio of variances
</p>
<p>0.784446
</p>
<p>The test is not significant, so there is no evidence against the assumption
that the variances are identical. However, the confidence interval is very
wide. For small data sets such as this one, the assumption of constant vari-
ance is largely a matter of belief. It may also be noted that this test is not
robust against departures from a normal distribution. The stats package
contains several alternative tests for variance homogeneity, each with its
own assumptions, benefits, and drawbacks, but we shall not discuss them
at length.
</p>
<p>Notice that the test is based on the assumption that the groups are
independent. You should not apply this test to paired data.
</p>
<p>5.5 Two-sample Wilcoxon test
</p>
<p>You might prefer a nonparametric test if you doubt the normal distribu-
tion assumptions of the t test. The two-sample Wilcoxon test is based on
replacing the data by their rank (without regard to grouping) and calcu-
lating the sum of the ranks in one group, thus reducing the problem to one
of sampling n1 values without replacement from the numbers 1 to n1 + n2.
</p>
<p>This is done using wilcox.test, which behaves similarly to t.test:</p>
<p/>
</div>
<div class="page"><p/>
<p>104 5. One- and two-sample tests
</p>
<p>&gt; wilcox.test(expend~stature)
</p>
<p>Wilcoxon rank sum test with continuity correction
</p>
<p>data: expend by stature
</p>
<p>W = 12, p-value = 0.002122
</p>
<p>alternative hypothesis: true location shift is not equal to 0
</p>
<p>Warning message:
</p>
<p>In wilcox.test.default(x = c(7.53, 7.48, 8.08, 8.09, 10.15, 8.4, :
</p>
<p>cannot compute exact p-value with ties
</p>
<p>The test statisticW is the sum of ranks in the first groupminus its theoreti-
cal minimum (i.e., it is zero if all the smallest values fall in the first group).
Some textbooks use a statistic that is the sum of ranks in the smallest group
with no minimum correction, which is of course equivalent. Notice that,
as in the one-sample example, we are having problems with ties and rely
on the approximate normal distribution ofW.
</p>
<p>5.6 The paired t test
</p>
<p>Paired tests are used when there are two measurements on the same ex-
perimental unit. The theory is essentially based on taking differences and
thus reducing the problem to that of a one-sample test. Notice, though,
that it is implicitly assumed that such differences have a distribution that
is independent of the level. A useful graphical check is to make a scatter-
plot of the pairs with the line of identity added or to plot the difference
against the average of the pair (sometimes called a Bland&ndash;Altman plot).
If there seems to be a tendency for the dispersion to change with the
level, then it may be useful to transform the data; frequently the stan-
dard deviation is proportional to the level, in which case a logarithmic
transformation is useful.
</p>
<p>The data on pre- and postmenstrual energy intake in a group of women
are considered several times in Chapter 1 (and youmay notice that the first
column is identical to daily.intake, which was used in Section 5.1).
There data are entered from the command line, but they are also available
as a data set in the ISwR package:
</p>
<p>&gt; attach(intake)
</p>
<p>&gt; intake
</p>
<p>pre post
</p>
<p>1 5260 3910
</p>
<p>2 5470 4220
</p>
<p>3 5640 3885
</p>
<p>4 6180 5160</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 The paired t test 105
</p>
<p>5 6390 5645
</p>
<p>6 6515 4680
</p>
<p>7 6805 5265
</p>
<p>8 7515 5975
</p>
<p>9 7515 6790
</p>
<p>10 8230 6900
</p>
<p>11 8770 7335
</p>
<p>The point is that the same 11women aremeasured twice, so it makes sense
to look at individual differences:
</p>
<p>&gt; post - pre
</p>
<p>[1] -1350 -1250 -1755 -1020 -745 -1835 -1540 -1540 -725 -1330
</p>
<p>[11] -1435
</p>
<p>It is immediately seen that they are all negative. All the women have a
lower energy intake postmenstrually than premenstrually. The paired t
test is obtained as follows:
</p>
<p>&gt; t.test(pre, post, paired=T)
</p>
<p>Paired t-test
</p>
<p>data: pre and post
</p>
<p>t = 11.9414, df = 10, p-value = 3.059e-07
</p>
<p>alternative hypothesis: true difference in means is not equal to 0
</p>
<p>95 percent confidence interval:
</p>
<p>1074.072 1566.838
</p>
<p>sample estimates:
</p>
<p>mean of the differences
</p>
<p>1320.455
</p>
<p>There is not much new to say about the output; it is virtually identical to
that of a one-sample t test on the elementwise differences.
</p>
<p>Notice that you have to specify paired=T explicitly in the call, indicat-
ing that you want a paired test. In the old-style interface for the unpaired
t test, the two groups are specified as separate vectors and you would
request that analysis by omitting paired=T. If data are actually paired,
then it would be seriously inappropriate to analyze them without taking
the pairing into account.
</p>
<p>Even though it might be considered pedagogically dubious to show what
you should not do, the following shows the results of an unpaired t test
on the same data for comparison:</p>
<p/>
</div>
<div class="page"><p/>
<p>106 5. One- and two-sample tests
</p>
<p>&gt; t.test(pre, post) #WRONG!
</p>
<p>Welch Two Sample t-test
</p>
<p>data: pre and post
</p>
<p>t = 2.6242, df = 19.92, p-value = 0.01629
</p>
<p>alternative hypothesis: true difference in means is not equal to 0
</p>
<p>95 percent confidence interval:
</p>
<p>270.5633 2370.3458
</p>
<p>sample estimates:
</p>
<p>mean of x mean of y
</p>
<p>6753.636 5433.182
</p>
<p>The number symbol (or &ldquo;hash&rdquo;) # introduces a comment in R. The rest of
the line is skipped.
</p>
<p>It is seen that t has become considerably smaller, although still significant
at the 5% level. The confidence interval has become almost four times
wider than in the correct paired analysis. Both illustrate the loss of ef-
ficiency caused by not using the information that the &ldquo;pre&rdquo; and &ldquo;post&rdquo;
measurements are from the same person. Alternatively, you could say that
it demonstrates the gain in efficiency obtained by planning the experi-
ment with two measurements on the same person, rather than having two
independent groups of pre- and postmenstrual women.
</p>
<p>5.7 The matched-pairs Wilcoxon test
</p>
<p>The paired Wilcoxon test is the same as a one-sample Wilcoxon signed-
rank test on the differences. The call is completely analogous to t.test:
</p>
<p>&gt; wilcox.test(pre, post, paired=T)
</p>
<p>Wilcoxon signed rank test with continuity correction
</p>
<p>data: pre and post
</p>
<p>V = 66, p-value = 0.00384
</p>
<p>alternative hypothesis: true location shift is not equal to 0
</p>
<p>Warning message:
</p>
<p>In wilcox.test.default(pre, post, paired = T) :
</p>
<p>cannot compute exact p-value with ties
</p>
<p>The result does not show any material difference from that of the t test.
The p-value is not quite so extreme, which is not too surprising since the
Wilcoxon rank sum cannot get any larger than it does when all differences
have the same sign, whereas the t statistic can become arbitrarily extreme.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Exercises 107
</p>
<p>Again, we have trouble with tied data invalidating the exact p calcula-
tions. This time it is the two identical differences of &minus;1540.
In the present case it is actually very easy to calculate the exact p-value
for the Wilcoxon test. It is the probability of 11 positive differences + the
probability of 11 negative ones, 2&times; (1/2)11 = 1/1024 = 0.00098, so the
approximate p-value is almost four times too large.
</p>
<p>5.8 Exercises
</p>
<p>5.1 Do the values of the react data set (notice that this is a single vector,
not a data frame) look reasonably normally distributed? Does the mean
differ significantly from zero according to a t test?
</p>
<p>5.2 In the data set vitcap, use a t test to compare the vital capacity for
the two groups. Calculate a 99% confidence interval for the difference. The
result of this comparison may be misleading. Why?
</p>
<p>5.3 Perform the analyses of the react and vitcap data using nonpara-
metric techniques.
</p>
<p>5.4 Perform graphical checks of the assumptions for a paired t test in the
intake data set.
</p>
<p>5.5 The function shapiro.test computes a test of normality based on
the degree of linearity of the Q&ndash;Q plot. Apply it to the react data. Does
it help to remove the outliers?
</p>
<p>5.6 The crossover trial in ashina can be analyzed for a drug effect in
a simple way (how?) if you ignore a potential period effect. However,
you can do better. Hint: Consider the intra-individual differences; if there
were only a period effect present, how should the differences behave in the
two groups? Compare the results of the simple method and the improved
method.
</p>
<p>5.7 Perform 10 one-sample t tests on simulated normally distributed
data sets of 25 observations each. Repeat the experiment, but instead sim-
ulate samples from a different distribution; try the t distribution with 2
degrees of freedom and the exponential distribution (in the latter case,
test for the mean being equal to 1). Can you find a way to automate this
so that you can have a larger number of replications?</p>
<p/>
</div>
<div class="page"><p/>
<p>6
Regression and correlation
</p>
<p>The main object of this chapter is to show how to perform basic regression
analyses, including plots for model checking and display of confidence
and prediction intervals. Furthermore, we describe the related topic of
correlation in both its parametric and nonparametric variants.
</p>
<p>6.1 Simple linear regression
</p>
<p>We consider situations where you want to describe the relation be-
tween two variables using linear regression analysis. You may, for
instance, be interested in describing short.velocity as a function of
blood.glucose. This section deals only with the very basics, whereas
several more complicated issues are postponed until Chapter 12.
</p>
<p>The linear regression model is given by
</p>
<p>yi = α + βxi + ǫi
</p>
<p>in which the ǫi are assumed independent and N(0, σ2). The nonrandom
part of the equation describes the yi as lying on a straight line. The slope
of the line (the regression coefficient) is β, the increase per unit change in x.
The line intersects the y-axis at the intercept α.
</p>
<p>The parameters α, β, and σ2 can be estimated using the method of least
squares. Find the values of α and β that minimize the sum of squared
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_6, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>110 6. Regression and correlation
</p>
<p>residuals
</p>
<p>SSres = &sum;
i
</p>
<p>(yi &minus; (α + βxi))2
</p>
<p>This is not actually done by trial and error. One can find closed-form
expressions for the choice of parameters that gives the smallest value of
SSres:
</p>
<p>β̂ =
&sum;(xi &minus; x̄)(yi &minus; ȳ)
</p>
<p>&sum;(xi &minus; x̄)2
α̂ = ȳ&minus; β̂x̄
</p>
<p>The residual variance is estimated as SSres/(n &minus; 2), and the residual
standard deviation is of course the square root of that.
</p>
<p>The empirical slope and intercept will deviate somewhat from the true
values due to sampling variation. If you were to generate several sets of yi
at the same set of xi, you would observe a distribution of empirical slopes
and intercepts. Just as you could calculate the SEM to describe the vari-
ability of the empirical mean, it is also possible from a single sample of
(xi, yi) to calculate the standard error of the computed estimates, s.e.(α̂)
and s.e.(β̂). These standard errors can be used to compute confidence in-
tervals for the parameters and tests for whether a parameter has a specific
value.
</p>
<p>It is usually of prime interest to test the null hypothesis that β = 0 since
that would imply that the line was horizontal and thus that the ys have a
distribution that is the same, whatever the value of x. You can compute a
t test for that hypothesis simply by dividing the estimate by its standard
error
</p>
<p>t =
β̂
</p>
<p>s.e.(β̂)
</p>
<p>which follows a t distribution on n&minus; 2 degrees of freedom if the true β is
zero. A similar test can be calculated for whether the intercept is zero, but
you should be aware that it is often a meaningless hypothesis either be-
cause there is no natural reason to believe that the line should go through
the origin or because it would involve an extrapolation far outside the
range of data.
</p>
<p>For the example in this section, we need the data frame thuesen, which
we attach with
</p>
<p>&gt; attach(thuesen)
</p>
<p>For linear regression analysis, the function lm (linear model) is used:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Simple linear regression 111
</p>
<p>&gt; lm(short.velocity~blood.glucose)
</p>
<p>Call:
</p>
<p>lm(formula = short.velocity ~ blood.glucose)
</p>
<p>Coefficients:
</p>
<p>(Intercept) blood.glucose
</p>
<p>1.09781 0.02196
</p>
<p>The argument to lm is amodel formula in which the tilde symbol (~) should
be read as &ldquo;described by&rdquo;. This was seen several times earlier, both in
connection with boxplots and stripcharts and with the t and Wilcoxon
tests.
</p>
<p>The lm function handles much more complicated models than simple lin-
ear regression. There can be many other things besides a dependent and a
descriptive variable in a model formula. Amultiple linear regression anal-
ysis (which we discuss in Chapter 11) of, for example, y on x1, x2, and x3
is specified as y ~ x1 + x2 + x3.
</p>
<p>In its raw form, the output of lm is very brief. All you see is the estimated
intercept (α) and the estimated slope (β). The best-fitting straight line is
seen to be short.velocity = 1.098 + 0.0220&times; blood.glucose, but
for instance no tests of significance are given.
</p>
<p>The result of lm is a model object. This is a distinctive concept of the S lan-
guage (of which R is a dialect). Whereas other statistical systems focus on
generating printed output that can be controlled by setting options, you
get instead the result of a model fit encapsulated in an object from which
the desired quantities can be obtained using extractor functions. An lm ob-
ject does in fact contain much more information than you see when it is
printed.
</p>
<p>A basic extractor function is summary:
</p>
<p>&gt; summary(lm(short.velocity~blood.glucose))
</p>
<p>Call:
</p>
<p>lm(formula = short.velocity ~ blood.glucose)
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-0.40141 -0.14760 -0.02202 0.03001 0.43490
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 1.09781 0.11748 9.345 6.26e-09 ***
</p>
<p>blood.glucose 0.02196 0.01045 2.101 0.0479 *
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1</p>
<p/>
</div>
<div class="page"><p/>
<p>112 6. Regression and correlation
</p>
<p>Residual standard error: 0.2167 on 21 degrees of freedom
</p>
<p>(1 observation deleted due to missingness)
</p>
<p>Multiple R-squared: 0.1737, Adjusted R-squared: 0.1343
</p>
<p>F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479
</p>
<p>The format above looks more like what other statistical packages would
output. The following is a &ldquo;dissection&rdquo; of the output:
</p>
<p>Call:
</p>
<p>lm(formula = short.velocity ~ blood.glucose)
</p>
<p>As in t.test, etc., the output starts with something that is essentially
a repeat of the function call. This is not very interesting when one has
just given it as a command to R, but it is useful if the result is saved in a
variable that is printed later.
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-0.40141 -0.14760 -0.02202 0.03001 0.43490
</p>
<p>This gives a superficial view of the distribution of the residuals that may
be used as a quick check of the distributional assumptions. The average
of the residuals is zero by definition, so the median should not be far from
zero, and the minimum and maximum should be roughly equal in ab-
solute value. In the example, it can be noticed that the third quartile is
remarkably close to zero, but in view of the small number of observations,
this is not really something to worry about.
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 1.09781 0.11748 9.345 6.26e-09 ***
</p>
<p>blood.glucose 0.02196 0.01045 2.101 0.0479 *
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Herewe see the regression coefficient and the intercept again, but this time
with accompanying standard errors, t tests, and p-values. The symbols
to the right are graphical indicators of the level of significance. The line
below the table shows the definition of these indicators; one star means
0.01 &lt; p &lt; 0.05.
</p>
<p>The graphical indicators have been the target of some controversy. Some
people like to have the possibility of seeing at a glance whether there is
&ldquo;anything interesting&rdquo; in an analysis, whereas others feel that the indica-
tors too often correspond to meaningless tests. For instance, the intercept
in the analysis above is hardly a meaningful quantity at all, and the three-
star significance of it is certainly irrelevant. If you are bothered by the
stars, turn them off with options(show.signif.stars=FALSE).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Residuals and fitted values 113
</p>
<p>Residual standard error: 0.2167 on 21 degrees of freedom
</p>
<p>(1 observation deleted due to missingness)
</p>
<p>This is the residual variation, an expression of the variation of the ob-
servations around the regression line, estimating the model parameter
σ. The model is not fitted to the entire data set because one value of
short.velocity is missing.
</p>
<p>Multiple R-squared: 0.1737, Adjusted R-squared: 0.1343
</p>
<p>The first item above is R2, which in a simple linear regression may be rec-
ognized as the squared Pearson correlation coefficient (see Section 6.4.1);
that is, R2 = r2. The other one is the adjusted R2; if you multiply it by
100%, it can be interpreted as &ldquo;% variance reduction&rdquo; (this can, in fact,
become negative).
</p>
<p>F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479
</p>
<p>This is an F test for the hypothesis that the regression coefficient is zero.
This test is not really interesting in a simple linear regression analysis since
it just duplicates information already given&mdash; it becomes more interesting
when there is more than one explanatory variable. Notice that it gives the
exact same result as the t test for a zero slope. In fact, the F test is identical
to the square of the t test: 4.414 = (2.101)2. This is true in any model with
1 degree of freedom.
</p>
<p>We will see later how to draw residual plots and plots of data with confi-
dence and prediction limits. First, we draw just the points and the fitted
line. Figure 6.1 has been constructed as follows:
</p>
<p>&gt; plot(blood.glucose,short.velocity)
</p>
<p>&gt; abline(lm(short.velocity~blood.glucose))
</p>
<p>abline, meaning (a, b)-line, draws lines based on the intercept and
slope, a and b, respectively. It can be used with scalar values as in
abline(1.1,0.022), but conveniently it can also extract the informa-
tion from a linear model fitted to data with lm.
</p>
<p>6.2 Residuals and fitted values
</p>
<p>We have seen how summary can be used to extract information about
the results of a regression analysis. Two further extraction functions are
fitted and resid. They are used as follows. For convenience, we
first store the value returned by lm under the name lm.velo (short for
&ldquo;velocity&rdquo;, but you could of course use any other name).</p>
<p/>
</div>
<div class="page"><p/>
<p>114 6. Regression and correlation
</p>
<p>5 10 15 20
</p>
<p>1
.0
</p>
<p>1
.2
</p>
<p>1
.4
</p>
<p>1
.6
</p>
<p>1
.8
</p>
<p>blood.glucose
</p>
<p>s
h
o
rt
</p>
<p>.v
e
lo
</p>
<p>c
it
y
</p>
<p>Figure 6.1. Scatterplot with regression line.
</p>
<p>&gt; lm.velo &lt;- lm(short.velocity~blood.glucose)
</p>
<p>&gt; fitted(lm.velo)
</p>
<p>1 2 3 4 5 6 7
</p>
<p>1.433841 1.335010 1.275711 1.526084 1.255945 1.214216 1.302066
</p>
<p>8 9 10 11 12 13 14
</p>
<p>1.341599 1.262534 1.365758 1.244964 1.212020 1.515103 1.429449
</p>
<p>15 17 18 19 20 21 22
</p>
<p>1.244964 1.190057 1.324029 1.372346 1.451411 1.389916 1.205431
</p>
<p>23 24
</p>
<p>1.291085 1.306459
</p>
<p>&gt; resid(lm.velo)
</p>
<p>1 2 3 4 5
</p>
<p>0.326158532 0.004989882 -0.005711308 -0.056084062 0.014054962
</p>
<p>6 7 8 9 10
</p>
<p>0.275783754 0.007933665 -0.251598875 -0.082533795 -0.145757649
</p>
<p>11 12 13 14 15
</p>
<p>0.005036223 -0.022019994 0.434897199 -0.149448964 0.275036223
</p>
<p>17 18 19 20 21
</p>
<p>-0.070057471 0.045971143 -0.182346406 -0.401411486 -0.069916424
</p>
<p>22 23 24
</p>
<p>-0.175431237 -0.171085074 0.393541161
</p>
<p>The function fitted returns fitted values &mdash; the y-values that you
would expect for the given x-values according to the best-fitting straight</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Residuals and fitted values 115
</p>
<p>line; in the present case, 1.098+0.0220*blood.glucose. The resid-
uals shown by resid is the difference between this and the observed
short.velocity.
</p>
<p>Note that the fitted values and residuals are labelled with the row names
of the thuesen data frame. Notice in particular that they do not contain
observation no. 16, which had a missing value in the response variable.
</p>
<p>It is necessary to discuss some awkward aspects that arise when there are
missing values in data.
</p>
<p>To put the fitted line on the plot, you might, although it is easier to use
abline(lm.velo), get the idea of doing it with lines, but
</p>
<p>&gt; plot(blood.glucose,short.velocity)
</p>
<p>&gt; lines(blood.glucose,fitted(lm.velo))
</p>
<p>Error in xy.coords(x, y) : &rsquo;x&rsquo; and &rsquo;y&rsquo; lengths differ
</p>
<p>Calls: lines -&gt; lines.default -&gt; plot.xy -&gt; xy.coords
</p>
<p>which is true. There are 24 observations but only 23 fitted values because
one of the short.velocity values is NA. Notice, incidentally, that the
error occurs within a series of nested function calls, which are being listed
along with the error message to reduce confusion.
</p>
<p>What we needed was blood.glucose, but only for those patients whose
short.velocity has been recorded.
</p>
<p>&gt; lines(blood.glucose[!is.na(short.velocity)],fitted(lm.velo))
</p>
<p>Recall that the is.na function yields a vector that is TRUE wherever the
argument is NA (missing). One advantage to this method is that the fitted
line does not extend beyond the range of data. The technique works but
becomes clumsy if there are missing values in several variables:
</p>
<p>...blood.glucose[!is.na(short.velocity) &amp; !is.na(blood.glucose)]...
</p>
<p>It becomes easier with the function complete.cases, which can find
observations that are nonmissing on several variables or across an entire
data frame.
</p>
<p>&gt; cc &lt;- complete.cases(thuesen)
</p>
<p>We could then attach thuesen[cc,] and work on from there. How-
ever, there is a better alternative available: You can use the na.exclude
method for NA handling. This can be set either as an argument to lm or as
an option; that is,
</p>
<p>&gt; options(na.action=na.exclude)
</p>
<p>&gt; lm.velo &lt;- lm(short.velocity~blood.glucose)</p>
<p/>
</div>
<div class="page"><p/>
<p>116 6. Regression and correlation
</p>
<p>5 10 15 20
</p>
<p>1
.0
</p>
<p>1
.2
</p>
<p>1
.4
</p>
<p>1
.6
</p>
<p>1
.8
</p>
<p>blood.glucose
</p>
<p>s
h
o
rt
</p>
<p>.v
e
lo
</p>
<p>c
it
y
</p>
<p>Figure 6.2. Scatterplot of short.velocity versus blood.glucose with fitted
line and residual line segments.
</p>
<p>&gt; fitted(lm.velo)
</p>
<p>1 2 3 4 5 6 7
</p>
<p>1.433841 1.335010 1.275711 1.526084 1.255945 1.214216 1.302066
</p>
<p>8 9 10 11 12 13 14
</p>
<p>1.341599 1.262534 1.365758 1.244964 1.212020 1.515103 1.429449
</p>
<p>15 16 17 18 19 20 21
</p>
<p>1.244964 NA 1.190057 1.324029 1.372346 1.451411 1.389916
</p>
<p>22 23 24
</p>
<p>1.205431 1.291085 1.306459
</p>
<p>Notice how the missing observation, no. 16, now appears in the fitted val-
ues with a missing fitted value. It is necessary to recalculate the lm.velo
object after changing the option.
</p>
<p>To create a plot where residuals are displayed by connecting observations
to corresponding points on the fitted line, you can do the following. The
final result will look like Figure 6.2. segments draws line segments; its
arguments are the endpoint coordinates in the order (x1, y1, x2, y2).
</p>
<p>&gt; segments(blood.glucose,fitted(lm.velo),
</p>
<p>+ blood.glucose,short.velocity)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Prediction and confidence bands 117
</p>
<p>1.20 1.25 1.30 1.35 1.40 1.45 1.50
</p>
<p>&minus;
0
.4
</p>
<p>&minus;
0
.2
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>fitted(lm.velo)
</p>
<p>re
s
id
</p>
<p>(l
m
</p>
<p>.v
e
lo
</p>
<p>)
</p>
<p>Figure 6.3. short.velocity and blood.glucose: residuals versus fitted value.
</p>
<p>A simple plot of residuals versus fitted values is obtained as (Figure 6.3)
</p>
<p>&gt; plot(fitted(lm.velo),resid(lm.velo))
</p>
<p>and we can get an indication of whether residuals might have come from
a normal distribution by checking for a straight line on a Q&ndash;Q plot (see
Section 4.2.3) as follows (Figure 6.4):
</p>
<p>&gt; qqnorm(resid(lm.velo))
</p>
<p>6.3 Prediction and confidence bands
</p>
<p>Fitted lines are often presented with uncertainty bands around them.
There are two kinds of bands, often referred to as the &ldquo;narrow&rdquo; and
&ldquo;wide&rdquo; limits.
</p>
<p>The narrow bands, confidence bands, reflect the uncertainty about the line
itself, like the SEM expresses the precision with which a mean is known.
If there are many observations, the bands will be quite narrow, reflecting</p>
<p/>
</div>
<div class="page"><p/>
<p>118 6. Regression and correlation
</p>
<p>&minus;2 &minus;1 0 1 2
</p>
<p>&minus;
0
.4
</p>
<p>&minus;
0
.2
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>Normal Q&minus;Q Plot
</p>
<p>Theoretical Quantiles
</p>
<p>S
a
m
</p>
<p>p
le
</p>
<p> Q
u
a
n
ti
le
</p>
<p>s
</p>
<p>Figure 6.4. short.velocity and blood.glucose: Q&ndash;Q plot of residuals.
</p>
<p>a well-determined line. These bands often show a marked curvature since
the line is better determined near the center of the point cloud. This is a
fact that can be shown mathematically, but you may also understand it
intuitively as follows: The predicted value at x̄ will be ȳ, whatever the
slope is, and hence the standard error of the fitted value at that point is
the SEM of the ys. At other values of x, there will also be a contribution
from the variability of the estimated slope, having increasing influence as
you move away from x̄. Technically, you also need to establish that ȳ and
β̂ are uncorrelated.
</p>
<p>The wide bands, prediction bands, include the uncertainty about future
observations. These bands should capture the majority of the observed
points and will not collapse to a line as the number of observations in-
creases. Rather, the limits approach the true line &plusmn;2 standard deviations
(for 95% limits). In smaller samples, the bands do curve since they include
uncertainty about the line itself, but not as markedly as the confidence
bands. Obviously, these limits rely strongly on the assumption of nor-
mally distributed errors with a constant variance, so you should not
use such limits unless you believe that the assumption is a reasonable
approximation for the data at hand.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Prediction and confidence bands 119
</p>
<p>Predicted values, with or without prediction and confidence bands, may
be extracted with the function predict. With no arguments, it just gives
the fitted values:
</p>
<p>&gt; predict(lm.velo)
</p>
<p>1 2 3 4 5 6 7
</p>
<p>1.433841 1.335010 1.275711 1.526084 1.255945 1.214216 1.302066
</p>
<p>8 9 10 11 12 13 14
</p>
<p>1.341599 1.262534 1.365758 1.244964 1.212020 1.515103 1.429449
</p>
<p>15 16 17 18 19 20 21
</p>
<p>1.244964 NA 1.190057 1.324029 1.372346 1.451411 1.389916
</p>
<p>22 23 24
</p>
<p>1.205431 1.291085 1.306459
</p>
<p>If you add interval="confidence" or interval="prediction",
then you get the vector of predicted values augmented with limits. The
arguments can be abbreviated:
</p>
<p>&gt; predict(lm.velo,int="c")
</p>
<p>fit lwr upr
</p>
<p>1 1.433841 1.291371 1.576312
</p>
<p>2 1.335010 1.240589 1.429431
</p>
<p>...
</p>
<p>23 1.291085 1.191084 1.391086
</p>
<p>24 1.306459 1.210592 1.402326
</p>
<p>&gt; predict(lm.velo,int="p")
</p>
<p>fit lwr upr
</p>
<p>1 1.433841 0.9612137 1.906469
</p>
<p>2 1.335010 0.8745815 1.795439
</p>
<p>...
</p>
<p>23 1.291085 0.8294798 1.752690
</p>
<p>24 1.306459 0.8457315 1.767186
</p>
<p>Warning message:
</p>
<p>In predict.lm(lm.velo, int = "p") :
</p>
<p>Predictions on current data refer to _future_ responses
</p>
<p>fit denotes the expected values, here identical to the fitted values
(they need not be; read on). lwr and upr are the lower and upper
confidence limits for the expected values, respectively, the prediction
limits for short.velocity for new persons with these values of
blood.glucose. The warning in this case does not really mean that any-
thing is wrong, but there is a pitfall: The limits should not be used for
evaluating the observed data to which the line has been fitted. These will
tend to lie closer to the line for the extreme x values because those data
points are the more influential; that is, the prediction bands curve the
wrong way.
</p>
<p>The best way to add prediction and confidence intervals to a scatterplot is
to use the matlines function, which plots the columns of amatrix against
a vector.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 6. Regression and correlation
</p>
<p>There are a few snags to this, however: (a) The blood.glucose values
are in random order; we do not want line segments connecting points
haphazardly along the confidence curves; (b) the prediction limits, partic-
ularly the lower one, extend outside the plot region; and (c) the matlines
command needs to be prevented from cycling through line styles and
colours. Notice that the na.exclude setting (p. 115) prevents us from
also having an observation omitted from the predicted values.
</p>
<p>The solution is to predict in a new data frame containing suitable x values
(here blood.glucose) at which to predict. It is done as follows:
</p>
<p>&gt; pred.frame &lt;- data.frame(blood.glucose=4:20)
</p>
<p>&gt; pp &lt;- predict(lm.velo, int="p", newdata=pred.frame)
</p>
<p>&gt; pc &lt;- predict(lm.velo, int="c", newdata=pred.frame)
</p>
<p>&gt; plot(blood.glucose,short.velocity,
</p>
<p>+ ylim=range(short.velocity, pp, na.rm=T))
</p>
<p>&gt; pred.gluc &lt;- pred.frame$blood.glucose
</p>
<p>&gt; matlines(pred.gluc, pc, lty=c(1,2,2), col="black")
</p>
<p>&gt; matlines(pred.gluc, pp, lty=c(1,3,3), col="black")
</p>
<p>What happens is that we create a new data frame in which the variable
blood.glucose contains the values at which we want predictions to be
made. pp and pc are then made to contain the result of predict for the
new data in pred.frame with prediction limits and confidence limits,
respectively.
</p>
<p>For the plotting, we first create a standard scatterplot, except that we en-
sure that it has enough room for the prediction limits. This is obtained by
setting ylim=range(short.velocity, pp, na.rm=T). The func-
tion range returns a vector of length 2 containing the minimum and
maximum values of its arguments. We need the na.rm=T argument to
cause missing values to be skipped for the range computation; notice that
short.velocity is included to ensure that points outside the predic-
tion limits are not missed (although in this case there are none). Finally,
the curves are added, using as x-values the blood.glucose used for the
prediction and setting the line types and colours to more sensible values.
The final result is seen in Figure 6.5.
</p>
<p>6.4 Correlation
</p>
<p>A correlation coefficient is a symmetric, scale-invariant measure of associ-
ation between two random variables. It ranges from &minus;1 to +1, where the
extremes indicate perfect correlation and 0 means no correlation. The sign
is negative when large values of one variable are associated with small
values of the other and positive if both variables tend to be large or small</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Correlation 121
</p>
<p>5 10 15 20
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>1
.2
</p>
<p>1
.4
</p>
<p>1
.6
</p>
<p>1
.8
</p>
<p>2
.0
</p>
<p>blood.glucose
</p>
<p>s
h
o
rt
</p>
<p>.v
e
lo
</p>
<p>c
it
y
</p>
<p>Figure 6.5. Plot with confidence and prediction bands.
</p>
<p>simultaneously. The reader should be warned that there are many incor-
rect uses of correlation coefficients, particularly when they are used in
regression-type settings.
</p>
<p>This section describes the computation of parametric and nonparametric
correlation measures in R.
</p>
<p>6.4.1 Pearson correlation
</p>
<p>The Pearson correlation is rooted in the two-dimensional normal distri-
bution where the theoretical correlation describes the contour ellipses for
the density. If both variables are scaled to have a variance of 1, then a
correlation of zero corresponds to circular contours, whereas the ellipses
become narrower and finally collapse into a line segment as the correlation
approaches &plusmn;1.
The empirical correlation coefficient is
</p>
<p>r =
&sum;(xi &minus; x̄)(yi &minus; ȳ)
</p>
<p>&radic;
</p>
<p>&sum;(xi &minus; x̄)2 &sum;(yi &minus; ȳ)2</p>
<p/>
</div>
<div class="page"><p/>
<p>122 6. Regression and correlation
</p>
<p>It can be shown that |r| will be less than 1 unless there is a perfect linear
relation between xi and yi, and for that reason the Pearson correlation is
sometimes called the &ldquo;linear correlation&rdquo;.
</p>
<p>It is possible to test the significance of the correlation by transforming it to
a t-distributed variable (the formula is not particularly elucidating so we
skip it here), which will be identical with the test obtained from testing the
significance of the slope of either the regression of y on x or vice versa.
</p>
<p>The function cor can be used to compute the correlation between two
or more vectors. However, if it is naively applied to the two vectors in
thuesen, the following happens:
</p>
<p>&gt; cor(blood.glucose,short.velocity)
</p>
<p>Error in cor(blood.glucose, short.velocity) :
</p>
<p>missing observations in cov/cor
</p>
<p>All the elementary statistical functions in R require either that all values
be nonmissing or that you explicitly state what should be done with the
cases withmissing values. For mean, var, sd, and similar one-vector func-
tions, you can give the argument na.rm=T to indicate that missing values
should be removed before the computation. For cor, you can write
</p>
<p>&gt; cor(blood.glucose,short.velocity,use="complete.obs")
</p>
<p>[1] 0.4167546
</p>
<p>The reason that cor does not use na.rm=T like the other functions is
that there are more possibilities than just removing incomplete cases or
failing. If more than two variables are in play, it is also possible to use in-
formation from all nonmissing pairs of measurements (this might result in
a correlation matrix that is not positive definite, though).
</p>
<p>You can obtain the entire matrix of correlations between all variables in a
data frame by saying, for instance,
</p>
<p>&gt; cor(thuesen,use="complete.obs")
</p>
<p>blood.glucose short.velocity
</p>
<p>blood.glucose 1.0000000 0.4167546
</p>
<p>short.velocity 0.4167546 1.0000000
</p>
<p>Of course, this is more interesting when the data frame contains more than
two vectors!
</p>
<p>However, the calculations above give no indication of whether the correla-
tion is significantly different from zero. To that end, you need cor.test.
It works simply by specifying the two variables:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Correlation 123
</p>
<p>&gt; cor.test(blood.glucose,short.velocity)
</p>
<p>Pearson&rsquo;s product-moment correlation
</p>
<p>data: blood.glucose and short.velocity
</p>
<p>t = 2.101, df = 21, p-value = 0.0479
</p>
<p>alternative hypothesis: true correlation is not equal to 0
</p>
<p>95 percent confidence interval:
</p>
<p>0.005496682 0.707429479
</p>
<p>sample estimates:
</p>
<p>cor
</p>
<p>0.4167546
</p>
<p>We also get a confidence interval for the true correlation. Notice that it is
exactly the same p-value as in the regression analysis in Section 6.1 and
also that based on the ANOVA table for the regression model, which is
described in Section 7.5.
</p>
<p>6.4.2 Spearman&rsquo;s ρ
</p>
<p>As with the one- and two-sample problems, you may be interested in
nonparametric variants. These have the advantage of not depending
on the normal distribution and, indeed, being invariant to monotone
transformations of the coordinates. The main disadvantage is that its in-
terpretation is not quite clear. A popular and simple choice is Spearman&rsquo;s
rank correlation coefficient ρ. This is obtained quite simply by replac-
ing the observations by their rank and computing the correlation. Under
the null hypothesis of independence between the two variables, the exact
distribution of ρ can be calculated.
</p>
<p>Unlike group comparisons where there is essentially one function per
named test, correlation tests are all grouped into cor.test. There is no
special spearman.test function. Instead, the test is considered one of
several possibilities for testing correlations and is therefore specified via
an option to cor.test:
</p>
<p>&gt; cor.test(blood.glucose,short.velocity,method="spearman")
</p>
<p>Spearman&rsquo;s rank correlation rho
</p>
<p>data: blood.glucose and short.velocity
</p>
<p>S = 1380.364, p-value = 0.1392
</p>
<p>alternative hypothesis: true rho is not equal to 0
</p>
<p>sample estimates:
</p>
<p>rho
</p>
<p>0.318002
</p>
<p>Warning message:</p>
<p/>
</div>
<div class="page"><p/>
<p>124 6. Regression and correlation
</p>
<p>In cor.test.default(blood.glucose, short.velocity, method="spearman"):
</p>
<p>Cannot compute exact p-values with ties
</p>
<p>6.4.3 Kendall&rsquo;s τ
</p>
<p>The third correlation method that you can choose is Kendall&rsquo;s τ, which is
based on counting the number of concordant and discordant pairs. A pair
of points is concordant if the difference in the x-coordinate is of the same
sign as the difference in the y-coordinate. For a perfect monotone rela-
tion, either all pairs will be concordant or all pairs will be discordant.
Under independence, there should be as many concordant pairs as there
are discordant ones.
</p>
<p>Since there are many pairs of points to check, this is quite a computation-
ally intensive procedure compared with the two others. In small data sets
such as the present one, it does notmatter at all, though, and the procedure
is generally usable up to at least 5000 observations.
</p>
<p>The τ coefficient has the advantage of a more direct interpretation over
Spearman&rsquo;s ρ, but apart from that there is little reason to prefer one over
the other.
</p>
<p>&gt; cor.test(blood.glucose,short.velocity,method="kendall")
</p>
<p>Kendall&rsquo;s rank correlation tau
</p>
<p>data: blood.glucose and short.velocity
</p>
<p>z = 1.5604, p-value = 0.1187
</p>
<p>alternative hypothesis: true tau is not equal to 0
</p>
<p>sample estimates:
</p>
<p>tau
</p>
<p>0.2350616
</p>
<p>Warning message:
</p>
<p>In cor.test.default(blood.glucose, short.velocity, method="kendall"):
</p>
<p>Cannot compute exact p-value with ties
</p>
<p>Notice that neither of the two nonparametric correlations is significant
at the 5% level, which the Pearson correlation is, albeit only borderline
significant.
</p>
<p>6.5 Exercises
</p>
<p>6.1 With the rmr data set, plot metabolic rate versus body weight. Fit
a linear regression model to the relation. According to the fitted model,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Exercises 125
</p>
<p>what is the predicted metabolic rate for a body weight of 70 kg? Give a
95% confidence interval for the slope of the line.
</p>
<p>6.2 In the juul data set, fit a linear regression model for the square root
of the IGF-I concentration versus age to the group of subjects over 25 years
old.
</p>
<p>6.3 In the malaria data set, analyze the log-transformed antibody level
versus age. Make a plot of the relation. Do you notice anything peculiar?
</p>
<p>6.4 One can generate simulated data from the two-dimensional normal
distribution with a correlation of ρ by the following technique: (a) Gen-
erate X as a normal variate with mean 0 and standard deviation 1; (b)
generate Y with mean ρX and standard deviation
</p>
<p>&radic;
</p>
<p>1&minus; ρ2. Use this to
create scatterplots of simulated data with a given correlation. Compute
the Spearman and Kendall statistics for some of these data sets.</p>
<p/>
</div>
<div class="page"><p/>
<p>7
Analysis of variance and the
Kruskal&ndash;Wallis test
</p>
<p>In this section, we consider comparisons among more than two groups
parametrically, using analysis of variance, as well as nonparametrically,
using the Kruskal&ndash;Wallis test. Furthermore, we look at two-way analysis
of variance in the case of one observation per cell.
</p>
<p>7.1 One-way analysis of variance
</p>
<p>We start this section with a brief sketch of the theory underlying the one-
way analysis of variance. A little bit of notation is necessary. Let xij denote
observation no. j in group i, so that x35 is the fifth observation in group
3; x̄i is the mean for group i, and x̄. is the grand mean (average of all
observations).
</p>
<p>We can decompose the observations as
</p>
<p>xij = x̄. + (x̄i &minus; x̄.)
︸ ︷︷ ︸
</p>
<p>deviation of
group mean from
grand mean
</p>
<p>+ (xij &minus; x̄i)
︸ ︷︷ ︸
</p>
<p>deviation of
observation from
group mean
</p>
<p>informally corresponding to the model
</p>
<p>Xij = &micro; + αi + ǫij, ǫij &sim; N(0, σ2)
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_7, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>128 7. Analysis of variance and the Kruskal&ndash;Wallis test
</p>
<p>in which the hypothesis that all the groups are the same implies that all
αi are zero. Notice that the error terms ǫij are assumed to be independent
and have the same variance.
</p>
<p>Now consider the sums of squares of the underbraced terms, known as
variation within groups
</p>
<p>SSDW = &sum;
i
</p>
<p>&sum;
j
</p>
<p>(xij &minus; x̄i)2
</p>
<p>and variation between groups
</p>
<p>SSDB = &sum;
i
</p>
<p>&sum;
j
</p>
<p>(x̄i &minus; x̄.)2 = &sum;
i
</p>
<p>ni(x̄i &minus; x̄.)2
</p>
<p>It is possible to prove that
</p>
<p>SSDB + SSDW = SSDtotal = &sum;
i
</p>
<p>&sum;
j
</p>
<p>(xij &minus; x̄.)2
</p>
<p>That is, the total variation is split into a term describing differences be-
tween group means and a term describing differences between individual
measurements within the groups. One says that the grouping explains
part of the total variation, and obviously an informative grouping will
explain a large part of the variation.
</p>
<p>However, the sums of squares can only be positive, so even a completely
irrelevant grouping will always &ldquo;explain&rdquo; some part of the variation. The
question is how small an amount of explained variation can be before it
might as well be due to chance. It turns out that in the absence of any
systematic differences between the groups, you should expect the sum of
squares to be partitioned according to the degrees of freedom for each
term, k&minus; 1 for SSDB and N&minus; k for SSDW , where k is the number of groups
and N is the total number of observations.
</p>
<p>Accordingly, you can normalize the sums of squares by calculating mean
squares:
</p>
<p>MSW = SSDW/(N &minus; k)
MSB = SSDB/(k&minus; 1)
</p>
<p>MSW is the pooled variance obtained by combining the individual group
variances and thus an estimate of σ2. In the absence of a true group effect,
MSB will also be an estimate of σ2, but if there is a group effect, then the
differences between group means and hence MSB will tend to be larger.
Thus, a test for significant differences between the group means can be
performed by comparing two variance estimates. This is why the proce-
dure is called analysis of variance even though the objective is to compare
the group means.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 One-way analysis of variance 129
</p>
<p>A formal test needs to account for the fact that random variationwill cause
some difference in the mean squares. You calculate
</p>
<p>F = MSB/MSW
</p>
<p>so that F is ideally 1, but some variation around that value is expected.
The distribution of F under the null hypothesis is an F distribution with
k&minus; 1 and N &minus; k degrees of freedom. You reject the hypothesis of identical
means if F is larger than the 95% quantile in that F distribution (if the
significance level is 5%). Notice that this test is one-sided; a very small F
would occur if the group means were very similar, and that will of course
not signify a difference between the groups.
</p>
<p>Simple analyses of variance can be performed in R using the func-
tion lm, which is also used for regression analysis. For more elaborate
analyses, there are also the functions aov and lme (linear mixed ef-
fects models, from the nlme package). An implementation of Welch&rsquo;s
procedure, relaxing the assumption of equal variances and generaliz-
ing the unequal-variance t test, is implemented in oneway.test (see
Section 7.1.2).
</p>
<p>The main example in this section is the &ldquo;red cell folate&rdquo; data from Alt-
man (1991, p. 208). To use lm, it is necessary to have the data values in
one vector and a factor variable (see Section 1.2.8) describing the division
into groups. The red.cell.folate data set contains a data frame in the
proper format.
</p>
<p>&gt; attach(red.cell.folate)
</p>
<p>&gt; summary(red.cell.folate)
</p>
<p>folate ventilation
</p>
<p>Min. :206.0 N2O+O2,24h:8
</p>
<p>1st Qu.:249.5 N2O+O2,op :9
</p>
<p>Median :274.0 O2,24h :5
</p>
<p>Mean :283.2
</p>
<p>3rd Qu.:305.5
</p>
<p>Max. :392.0
</p>
<p>Recall that summary applied to a data frame gives a short summary of
the distribution of each of the variables contained in it. The format of the
summary is different for numeric vectors and factors, so that provides a
check that the variables are defined correctly.
</p>
<p>The category names for ventilationmean &ldquo;N2O and O2 for 24 hours&rdquo;,
&ldquo;N2O and O2 during operation&rdquo;, and &ldquo;only O2 for 24 hours&rdquo;.
</p>
<p>In the following, the analysis of variance is demonstrated first and then a
couple of useful techniques for the presentation of grouped data as tables
and graphs are shown.</p>
<p/>
</div>
<div class="page"><p/>
<p>130 7. Analysis of variance and the Kruskal&ndash;Wallis test
</p>
<p>The specification of a one-way analysis of variance is analogous to a re-
gression analysis. The only difference is that the descriptive variable needs
to be a factor and not a numeric variable.We calculate amodel object using
lm and extract the analysis of variance table with anova.
</p>
<p>&gt; anova(lm(folate~ventilation))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: folate
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>ventilation 2 15516 7758 3.7113 0.04359 *
</p>
<p>Residuals 19 39716 2090
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Here we have SSDB and MSB in the top line and SSDW and MSW in the
second line.
</p>
<p>In statistics textbooks, the sums of squares are most often labelled &ldquo;be-
tween groups&rdquo; and &ldquo;within groups&rdquo;. Like most other statistical software,
R uses slightly different labelling. Variation between groups is labelled
by the name of the grouping factor (ventilation), and variation within
groups is labelled Residual. ANOVA tables can be used for a wide range
of statistical models, and it is convenient to use a format that is less linked
to the particular problem of comparing groups.
</p>
<p>For a further example, consider the data set juul, introduced in Sec-
tion 4.1. Notice that the tanner variable in this data set is a numeric
vector and not a factor. For purposes of tabulation, this makes little dif-
ference, but it would be a serious error to use it in this form in an analysis
of variance:
</p>
<p>&gt; attach(juul)
</p>
<p>&gt; anova(lm(igf1~tanner)) ## WRONG!
</p>
<p>Analysis of Variance Table
</p>
<p>Response: igf1
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>tanner 1 10985605 10985605 686.07 &lt; 2.2e-16 ***
</p>
<p>Residuals 790 12649728 16012
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>This does not describe a grouping of data but a linear regression on the
group number! Notice the telltale 1 DF for the effect of tanner.
</p>
<p>Things can be fixed as follows:
</p>
<p>&gt; juul$tanner &lt;- factor(juul$tanner,
</p>
<p>+ labels=c("I","II","III","IV","V"))</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 One-way analysis of variance 131
</p>
<p>&gt; detach(juul)
</p>
<p>&gt; attach(juul)
</p>
<p>&gt; summary(tanner)
</p>
<p>I II III IV V NA&rsquo;s
</p>
<p>515 103 72 81 328 240
</p>
<p>&gt; anova(lm(igf1~tanner))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: igf1
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>tanner 4 12696217 3174054 228.35 &lt; 2.2e-16 ***
</p>
<p>Residuals 787 10939116 13900
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>We needed to reattach the juul data frame in order to use the changed
definition. An attached data frame is effectively a separate copy of it
(although it does not take up extra space as long as the original is
unchanged). The Df column now has an entry of 4 for tanner, as it
should.
</p>
<p>7.1.1 Pairwise comparisons and multiple testing
</p>
<p>If the F test shows that there is a difference between groups, the ques-
tion quickly arises of where the difference lies. It becomes necessary to
compare the individual groups.
</p>
<p>Part of this information can be found in the regression coefficients. You can
use summary to extract regression coefficients with standard errors and t
tests. These coefficients do not have their usual meaning as the slope of a
regression line but have a special interpretation, which is described below.
</p>
<p>&gt; summary(lm(folate~ventilation))
</p>
<p>Call:
</p>
<p>lm(formula = folate ~ ventilation)
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-73.625 -35.361 -4.444 35.625 75.375
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 316.62 16.16 19.588 4.65e-14 ***
</p>
<p>ventilationN2O+O2,op -60.18 22.22 -2.709 0.0139 *
</p>
<p>ventilationO2,24h -38.62 26.06 -1.482 0.1548
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 45.72 on 19 degrees of freedom
</p>
<p>Multiple R-squared: 0.2809, Adjusted R-squared: 0.2052
</p>
<p>F-statistic: 3.711 on 2 and 19 DF, p-value: 0.04359</p>
<p/>
</div>
<div class="page"><p/>
<p>132 7. Analysis of variance and the Kruskal&ndash;Wallis test
</p>
<p>The interpretation of the estimates is that the intercept is the mean in the
first group (N2O+O2,24h), whereas the two others describe the difference
between the relevant group and the first one.
</p>
<p>There are multiple ways of representing the effect of a factor variable in
linearmodels (and one-way analysis of variance is the simplest example of
a linear model with a factor variable). The representations are in terms of
contrasts, the choice of which can be controlled either by global options or
as part of the model formula. We do not go deeply into this but just men-
tion that the contrasts used by default are the so-called treatment contrasts,
in which the first group is treated as a baseline and the other groups are
given relative to that. Concretely, the analysis is performed as a multiple
regression analysis (see Chapter 11) by introducing two dummy variables,
which are 1 for observations in the relevant group and 0 elsewhere.
</p>
<p>Among the t tests in the table, you can immediately find a test for the hy-
pothesis that the first two groups have the same true mean (p = 0.0139)
and also whether the first and the third might be identical (p = 0.1548).
However, a comparison of the last two groups cannot be found. This can
be overcome by modifying the factor definition (see the help page for
relevel), but that gets tedious when there are more than a few groups.
</p>
<p>If we want to compare all groups, we ought to correct for multiple testing.
Performing many tests will increase the probability of finding one of them
to be significant; that is, the p-values tend to be exaggerated. A common
adjustment method is the Bonferroni correction, which is based on the fact
that the probability of observing at least one of n events is less than the
sum of the probabilities for each event. Thus, by dividing the significance
level by the number of tests or, equivalently, multiplying the p-values, we
obtain a conservative test where the probability of a significant result is less
than or equal to the formal significance level.
</p>
<p>A function called pairwise.t.test computes all possible two-group
comparisons. It is also capable of making adjustments for multiple
comparisons and works like this:
</p>
<p>&gt; pairwise.t.test(folate, ventilation, p.adj="bonferroni")
</p>
<p>Pairwise comparisons using t tests with pooled SD
</p>
<p>data: folate and ventilation
</p>
<p>N2O+O2,24h N2O+O2,op
</p>
<p>N2O+O2,op 0.042 -
</p>
<p>O2,24h 0.464 1.000
</p>
<p>P value adjustment method: bonferroni</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 One-way analysis of variance 133
</p>
<p>The output is a table of p-values for the pairwise comparisons. Here, the
p-values have been adjusted by the Bonferroni method, where the unad-
justed values have beenmultiplied by the number of comparisons, namely
3. If that results in a value bigger than 1, then the adjustment procedure
sets the adjusted p-value to 1.
</p>
<p>The default method for pairwise.t.test is actually not the Bonferroni
correction but a variant due to Holm. In this method, only the smallest p
needs to be corrected by the full number of tests, the second smallest is
corrected by n &minus; 1, etc., unless that would make it smaller than the pre-
vious one, since the order of the p-values should be unaffected by the
adjustment.
</p>
<p>&gt; pairwise.t.test(folate,ventilation)
</p>
<p>Pairwise comparisons using t tests with pooled SD
</p>
<p>data: folate and ventilation
</p>
<p>N2O+O2,24h N2O+O2,op
</p>
<p>N2O+O2,op 0.042 -
</p>
<p>O2,24h 0.310 0.408
</p>
<p>P value adjustment method: holm
</p>
<p>7.1.2 Relaxing the variance assumption
</p>
<p>The traditional one-way ANOVA requires an assumption of equal vari-
ances for all groups. There is, however, an alternative procedure that
does not require that assumption. It is due to Welch and similar to the
unequal-variances t test. This has been implemented in the oneway.test
function:
</p>
<p>&gt; oneway.test(folate~ventilation)
</p>
<p>One-way analysis of means (not assuming equal variances)
</p>
<p>data: folate and ventilation
</p>
<p>F = 2.9704, num df = 2.000, denom df = 11.065, p-value = 0.09277
</p>
<p>In this case, the p-value increased to a nonsignificant value, presumably
related to the fact that the group that seems to differ from the two others
also has the largest variance.
</p>
<p>It is also possible to perform the pairwise t tests so that they do not use
a common pooled standard deviation. This is controlled by the argument
pool.sd.</p>
<p/>
</div>
<div class="page"><p/>
<p>134 7. Analysis of variance and the Kruskal&ndash;Wallis test
</p>
<p>&gt; pairwise.t.test(folate,ventilation,pool.sd=F)
</p>
<p>Pairwise comparisons using t tests with non-pooled SD
</p>
<p>data: folate and ventilation
</p>
<p>N2O+O2,24h N2O+O2,op
</p>
<p>N2O+O2,op 0.087 -
</p>
<p>O2,24h 0.321 0.321
</p>
<p>P value adjustment method: holm
</p>
<p>Again, it is seen that the significance disappears as we remove the
constraint on the variances.
</p>
<p>7.1.3 Graphical presentation
</p>
<p>Of course, there are many ways to present grouped data. Here we create
a somewhat elaborate plot where the raw data are plotted as a stripchart
and overlaid with an indication of means and SEMs (Figure 7.1):
</p>
<p>&gt; xbar &lt;- tapply(folate, ventilation, mean)
</p>
<p>&gt; s &lt;- tapply(folate, ventilation, sd)
</p>
<p>&gt; n &lt;- tapply(folate, ventilation, length)
</p>
<p>&gt; sem &lt;- s/sqrt(n)
</p>
<p>&gt; stripchart(folate~ventilation, method="jitter",
</p>
<p>+ jitter=0.05, pch=16, vert=T)
</p>
<p>&gt; arrows(1:3,xbar+sem,1:3,xbar-sem,angle=90,code=3,length=.1)
</p>
<p>&gt; lines(1:3,xbar,pch=4,type="b",cex=2)
</p>
<p>Here we used pch=16 (small plotting dots) in stripchart and put
vertical=T to make the &ldquo;strips&rdquo; vertical.
</p>
<p>The error bars have been made with arrows, which adds arrows to a
plot. We slightly abuse the fact that the angle of the arrowhead is ad-
justable to create the little crossbars at either end. The first four arguments
specify the endpoints, (x1, y1, x2, y2); the angle argument gives the an-
gle between the lines of the arrowhead and shaft, here set to 90◦; and
length is the length of the arrowhead (in inches on a printout). Finally,
code=3means that the arrow should have a head at both ends. Note that
the x-coordinates of the stripcharts are simply the group numbers.
</p>
<p>The indication of averages and the connecting lines are done with lines,
where type="b" (both) means that both points and lines are printed,
leaving gaps in the lines to make room for the symbols. pch=4 is a cross,
and cex=2 requests that the symbols be drawn in double size.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 One-way analysis of variance 135
</p>
<p>N2O+O2,24h N2O+O2,op O2,24h
</p>
<p>2
0
0
</p>
<p>2
5
0
</p>
<p>3
0
0
</p>
<p>3
5
0
</p>
<p>Figure 7.1. &ldquo;Red cell folate&rdquo; data with x̄&plusmn; 1 SEM.
</p>
<p>It is debatable whether you should draw the plot using 1 SEM as is done
here orwhether perhaps it is better to draw proper confidence intervals for
the means (approximately 2 SEM), or maybe even SD instead of SEM. The
latter point has to do with whether the plot is to be used in a descriptive
or an analytical manner. Standard errors of the mean are not useful for
describing the distributions in the groups; they only say how precisely
the mean is determined. On the other hand, SDs do not enable the reader
to see at a glance which groups are significantly different.
</p>
<p>In many fields it appears to have become the tradition to use 1 SEM
&ldquo;because they are the smallest&rdquo;; that is, it makes differences look more
dramatic. Probably, the best thing to do is to follow the traditions in the
relevant field and &ldquo;calibrate your eyeballs&rdquo; accordingly.
</p>
<p>Oneword of warning, though: At small group sizes, the rule of thumb that
the confidence interval is the mean&plusmn; 2 SEM becomes badly misleading.
At a group size of 2, it actually has to be 12.7 SEM! That is a correction
heavily dependent on data having the normal distribution. If you have
such small groups, it may be advisable to use a pooled SD for the entire
data set rather than the group-specific SDs. This does, of course, require</p>
<p/>
</div>
<div class="page"><p/>
<p>136 7. Analysis of variance and the Kruskal&ndash;Wallis test
</p>
<p>that you can reasonably assume that the true standard deviation actually
is the same in all groups.
</p>
<p>7.1.4 Bartlett&rsquo;s test
</p>
<p>Testing whether the distribution of a variable has the same variance in
all groups can be done using Bartlett&rsquo;s test, although like the F test for
comparing two variances, it is rather nonrobust against departures from
the assumption of normal distributions. As in var.test, it is assumed
that the data are from independent groups. The procedure is performed
as follows:
</p>
<p>&gt; bartlett.test(folate~ventilation)
</p>
<p>Bartlett test of homogeneity of variances
</p>
<p>data: folate by ventilation
</p>
<p>Bartlett&rsquo;s K-squared = 2.0951, df = 2, p-value = 0.3508
</p>
<p>That is, in this case, nothing in the data contradicts the assumption of
equal variances in the three groups.
</p>
<p>7.2 Kruskal&ndash;Wallis test
</p>
<p>A nonparametric counterpart of a one-way analysis of variance is the
Kruskal&ndash;Wallis test. As in the Wilcoxon two-sample test (see Section 5.5),
data are replaced with their ranks without regard to the grouping, only
this time the test is based on the between-group sum of squares calcu-
lated from the average ranks. Again, the distribution of the test statistic
can be worked out based on the idea that, under the hypothesis of irrel-
evant grouping, the problem reduces to a combinatorial one of sampling
the within-group ranks from a fixed set of numbers.
</p>
<p>You can make R calculate the Kruskal&ndash;Wallis test as follows:
</p>
<p>&gt; kruskal.test(folate~ventilation)
</p>
<p>Kruskal-Wallis rank sum test
</p>
<p>data: folate by ventilation
</p>
<p>Kruskal-Wallis chi-squared = 4.1852, df = 2, p-value = 0.1234
</p>
<p>It is seen that there is no significant difference using this test. This should
not be too surprising in view of the fact that the F test in the one-way anal-
ysis of variance was only borderline significant. Also, the Kruskal&ndash;Wallis</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Two-way analysis of variance 137
</p>
<p>test is less efficient than its parametric counterpart if the assumptions
hold, although it does not invariably give a larger p-value.
</p>
<p>7.3 Two-way analysis of variance
</p>
<p>One-way analysis of variance deals with one-way classifications of data.
It is also possible to analyze data that are cross-classified according to
several criteria. When a cross-classified design is balanced, then you can
almost read the entire statistical analysis from a single analysis of vari-
ance table, and that table generally consists of items that are simple to
compute, which was very important before the computer era. Balanced-
ness is a concept that is hard to define exactly; for a two-way classification,
a sufficient condition is that the cell counts be equal, but there are other
balanced designs.
</p>
<p>Here we restrict ourselves to the case of a single observation per cell.
This typically arises from having multiple measurements on the same
experimental unit and in this sense generalizes the paired t test.
</p>
<p>Let xij denote the observation in row i and column j of them&times;n table. This
is similar to the notation used for one-way analysis of variance, but notice
that there is now a connection between observations with the same j, so
that it makes sense to look at both row averages x̄i&middot; and column averages
x̄
&middot;j.
</p>
<p>Consequently, it now makes sense to look at both variation between rows
</p>
<p>SSDR = n&sum;
i
</p>
<p>(x̄i&middot; &minus; x̄..)2
</p>
<p>and variation between columns
</p>
<p>SSDC = m&sum;
j
</p>
<p>(x̄
&middot;j &minus; x̄..)2
</p>
<p>Subtracting these two from the total variation leaves the residual variation,
which works out as
</p>
<p>SSDres = &sum;
i
</p>
<p>&sum;
j
</p>
<p>(xij &minus; x̄i&middot; &minus; x̄&middot;j + x̄..)2
</p>
<p>This corresponds to a statistical model in which it is assumed that the
observations are composed of a general level, a row effect, and a column
effect plus a noise term:
</p>
<p>Xij = &micro; + αi + β j + ǫij ǫij &sim; N(0, σ2)</p>
<p/>
</div>
<div class="page"><p/>
<p>138 7. Analysis of variance and the Kruskal&ndash;Wallis test
</p>
<p>The parameters of this model are not uniquely defined unless we impose
some restriction on the parameters. If we impose &sum; αi = 0 and &sum; β j = 0,
then the estimates of αi, β j, and &micro; turn out to be x̄i&middot; &minus; x̄.., x̄&middot;j &minus; x̄.., and x̄...
Dividing the sums of squares by their respective degrees of freedomm&minus; 1
for SSDR, n &minus; 1 for SSDC, and (m &minus; 1)(n &minus; 1) for SSDres, we get a set of
mean squares. F tests for no row and column effect can be carried out by
dividing the respective mean squares by the residual mean square.
</p>
<p>It is important to notice that this works out so nicely only because of the
balanced design. If you have a table with &ldquo;holes&rdquo; in it, the analysis is con-
siderably more complicated. The simple formulas for the sum of squares
are no longer valid and, in particular, the order independence is lost, so
that there is no longer a single SSDC but ones with and without adjusting
for row effects.
</p>
<p>To perform a two-way ANOVA, it is necessary to have data in one vec-
tor, with the two classifying factors parallel to it. We consider an example
concerning heart rate after administration of enalaprilate (Altman, 1991,
p. 327). Data are found in this form in the heart.rate data set:
</p>
<p>&gt; attach(heart.rate)
</p>
<p>&gt; heart.rate
</p>
<p>hr subj time
</p>
<p>1 96 1 0
</p>
<p>2 110 2 0
</p>
<p>3 89 3 0
</p>
<p>4 95 4 0
</p>
<p>5 128 5 0
</p>
<p>6 100 6 0
</p>
<p>7 72 7 0
</p>
<p>8 79 8 0
</p>
<p>9 100 9 0
</p>
<p>10 92 1 30
</p>
<p>11 106 2 30
</p>
<p>12 86 3 30
</p>
<p>13 78 4 30
</p>
<p>14 124 5 30
</p>
<p>15 98 6 30
</p>
<p>16 68 7 30
</p>
<p>17 75 8 30
</p>
<p>18 106 9 30
</p>
<p>19 86 1 60
</p>
<p>20 108 2 60
</p>
<p>21 85 3 60
</p>
<p>22 78 4 60
</p>
<p>23 118 5 60
</p>
<p>24 100 6 60
</p>
<p>25 67 7 60
</p>
<p>26 74 8 60
</p>
<p>27 104 9 60</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Two-way analysis of variance 139
</p>
<p>28 92 1 120
</p>
<p>29 114 2 120
</p>
<p>30 83 3 120
</p>
<p>31 83 4 120
</p>
<p>32 118 5 120
</p>
<p>33 94 6 120
</p>
<p>34 71 7 120
</p>
<p>35 74 8 120
</p>
<p>36 102 9 120
</p>
<p>If you look inside the heart.rate.R file in the data directory of the
ISwR package, you will see that the actual definition of the data frame is
</p>
<p>heart.rate &lt;- data.frame(hr = c(96,110,89,95,128,100,72,79,100,
</p>
<p>92,106,86,78,124,98,68,75,106,
</p>
<p>86,108,85,78,118,100,67,74,104,
</p>
<p>92,114,83,83,118,94,71,74,102),
</p>
<p>subj=gl(9,1,36),
</p>
<p>time=gl(4,9,36,labels=c(0,30,60,120)))
</p>
<p>The gl (generate levels) function is specially designed for generating pat-
terned factors for balanced experimental designs. It has three arguments:
the number of levels, the block length (how many times each level should
repeat), and the total length of the result. The two patterns in the data
frame are thus
</p>
<p>&gt; gl(9,1,36)
</p>
<p>[1] 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4
</p>
<p>[32] 5 6 7 8 9
</p>
<p>Levels: 1 2 3 4 5 6 7 8 9
</p>
<p>&gt; gl(4,9,36,labels=c(0,30,60,120))
</p>
<p>[1] 0 0 0 0 0 0 0 0 0 30 30 30 30 30 30
</p>
<p>[16] 30 30 30 60 60 60 60 60 60 60 60 60 120 120 120
</p>
<p>[31] 120 120 120 120 120 120
</p>
<p>Levels: 0 30 60 120
</p>
<p>Once the variables have been defined, the two-way analysis of variance is
specified simply by
</p>
<p>&gt; anova(lm(hr~subj+time))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: hr
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>subj 8 8966.6 1120.8 90.6391 4.863e-16 ***
</p>
<p>time 3 151.0 50.3 4.0696 0.01802 *
</p>
<p>Residuals 24 296.8 12.4
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1</p>
<p/>
</div>
<div class="page"><p/>
<p>140 7. Analysis of variance and the Kruskal&ndash;Wallis test
</p>
<p>7
0
</p>
<p>8
0
</p>
<p>9
0
</p>
<p>1
0
0
</p>
<p>1
1
0
</p>
<p>1
2
0
</p>
<p>1
3
0
</p>
<p>time
</p>
<p>m
e
a
n
 o
</p>
<p>f 
 h
</p>
<p>r
</p>
<p>0 30 60 120
</p>
<p>   subj
</p>
<p>5
</p>
<p>2
</p>
<p>9
</p>
<p>6
</p>
<p>1
</p>
<p>3
</p>
<p>4
</p>
<p>8
</p>
<p>7
</p>
<p>Figure 7.2. Interaction plot of heart-rate data.
</p>
<p>Interchanging subj and time in the model formula (hr~time+subj)
yields exactly the same analysis except for the order of the rows of the
ANOVA table. This is because we are dealing with a balanced design (a
complete two-way table with no missing values). In unbalanced cases, the
factor order will matter.
</p>
<p>7.3.1 Graphics for repeated measurements
</p>
<p>At least for your own use, it is useful to plot a &ldquo;spaghettigram&rdquo; of the data;
that is, a plot where data from the same subject are connected with lines.
To this end, you can use the function interaction.plot, which graphs
the values against one factor while connecting data for the other factor
with line segments to form traces.
</p>
<p>&gt; interaction.plot(time, subj, hr)
</p>
<p>In fact there is a fourth argument, which specifies what should be done in
case, there is more than one observation per cell. By default, the mean is
taken, which is the reason why the y-axis in Figure 7.2 reads &ldquo;mean of hr&rdquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 The Friedman test 141
</p>
<p>If you prefer to have the values plotted according to the times of measure-
ment (which are not equidistant in this example), you could instead write
(resulting plot not shown)
</p>
<p>&gt; interaction.plot(ordered(time),subj,hr)
</p>
<p>7.4 The Friedman test
</p>
<p>A nonparametric counterpart of two-way analysis of variance exists for
the case with one observation per cell. Friedman&rsquo;s test is based on ranking
observationswithin each row assuming that if there is no column effect then
all orderings should be equally likely. A test statistic based on the column
sum of squares can be calculated and normalized to give a χ2-distributed
test statistic.
</p>
<p>In the case of two columns, the Friedman test is equivalent to the sign test,
in which one uses the binomial distribution to test for equal probabili-
ties of positive and negative differences within pairs. This is a rather less
sensitive test than the Wilcoxon signed-rank test discussed in Section 5.2.
</p>
<p>Practical application of the Friedman test is as follows:
</p>
<p>&gt; friedman.test(hr~time|subj,data=heart.rate)
</p>
<p>Friedman rank sum test
</p>
<p>data: hr and time and subj
</p>
<p>Friedman chi-squared = 8.5059, df = 3, p-value = 0.03664
</p>
<p>Notice that the blocking factor is specified in a model formula using the
vertical bar, which may be read as &ldquo;time within subj&rdquo;. It is seen that the
test is not quite as strongly significant as the parametric counterpart. This
is unsurprising since the latter test is more powerful when its assumptions
are met.
</p>
<p>7.5 The ANOVA table in regression analysis
</p>
<p>We have seen the use of analysis of variance tables in grouped and cross-
classified experimental designs. However, their use is not restricted to
these designs but applies to the whole class of linear models (more on this
in Chapter 12).</p>
<p/>
</div>
<div class="page"><p/>
<p>142 7. Analysis of variance and the Kruskal&ndash;Wallis test
</p>
<p>The variation between and within groups for a one-way analysis of
variance generalizes to model variation and residual variation
</p>
<p>SSDmodel = &sum;
i
</p>
<p>(ŷi &minus; ȳ.)2
</p>
<p>SSDres = &sum;
i
</p>
<p>(yi &minus; ŷi)2
</p>
<p>which partition the total variation &sum;i(yi &minus; ȳ.)2. This applies only when the
model contains an intercept; see Section 12.2. The role of the group means
in the one-way classification is taken over by the fitted values ŷi in the
more general linear model.
</p>
<p>An F test for significance of the model is available in direct analogy with
Section 7.1. In simple linear regression, this test is equivalent to testing
that the regression coefficient is zero.
</p>
<p>The analysis of variance table corresponding to a regression analysis
can be extracted with the function anova, just as for one- and two-way
analyses of variance. For the thuesen example, it will look like this:
</p>
<p>&gt; attach(thuesen)
</p>
<p>&gt; lm.velo &lt;- lm(short.velocity~blood.glucose)
</p>
<p>&gt; anova(lm.velo)
</p>
<p>Analysis of Variance Table
</p>
<p>Response: short.velocity
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>blood.glucose 1 0.20727 0.20727 4.414 0.0479 *
</p>
<p>Residuals 21 0.98610 0.04696
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Notice that the F test gives the same p-value as the t test for a zero slope
from Section 6.1. It is the same F test that gets printed at the end of the
summary output:
</p>
<p>...
</p>
<p>Residual standard error: 0.2167 on 21 degrees of freedom
</p>
<p>Multiple R-Squared: 0.1737, Adjusted R-squared: 0.1343
</p>
<p>F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479
</p>
<p>The remaining elements of the three output lines above may also be de-
rived from the ANOVA table. &ldquo;Residual standard error&rdquo; is the square root
of &ldquo;Residual mean squares&rdquo;, namely 0.2167 =
</p>
<p>&radic;
0.04696. R2 is the propor-
</p>
<p>tion of the total sum of squares explained by the regression line, 0.1737 =
0.2073/(0.2073 + 0.9861); and, finally, the adjusted R2 is the relative im-
provement of the residual variance, 0.1343 = (v &minus; 0.04696)/v, where
v = (0.2073 + 0.9861)/22 = 0.05425 is the variance of short.velocity
if the glucose values are not taken into account.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Exercises 143
</p>
<p>7.6 Exercises
</p>
<p>7.1 The zelazo data are in the form of a list of vectors, one for each of
the four groups. Convert the data to a form suitable for the use of lm, and
calculate the relevant test. Consider t tests comparing selected subgroups
or obtained by combining groups.
</p>
<p>7.2 In the lung data, do the three measurement methods give systemat-
ically different results? If so, which ones appear to be different?
</p>
<p>7.3 Repeat the previous exercises using the zelazo and lung data with
the relevant nonparametric tests.
</p>
<p>7.4 The igf1 variable in the juul data set is arguably skewed and has
different variances across Tanner groups. Try to compensate for this us-
ing logarithmic and square-root transformations, and use the Welch test.
However, the analysis is still problematic &mdash; why?</p>
<p/>
</div>
<div class="page"><p/>
<p>8
Tabular data
</p>
<p>This chapter describes a series of functions designed to analyze tabular
data. Specifically, we look at the functions prop.test, binom.test,
chisq.test, and fisher.test.
</p>
<p>8.1 Single proportions
</p>
<p>Tests of single proportions are generally based on the binomial distribu-
tion (see Section 3.3) with size parameter N and probability parameter p.
For large sample sizes, this can be well approximated by a normal distri-
bution with mean Np and variance Np(1 &minus; p). As a rule of thumb, the
approximation is satisfactory when the expected numbers of &ldquo;successes&rdquo;
and &ldquo;failures&rdquo; are both larger than 5.
</p>
<p>Denoting the observed number of &ldquo;successes&rdquo; by x, the test for the
hypothesis that p = p0 can be based on
</p>
<p>u =
x&minus; Np0
</p>
<p>&radic;
</p>
<p>Np0(1&minus; p0)
which has an approximate normal distribution with mean zero and stan-
dard deviation 1, or on u2, which has an approximate χ2 distribution with
1 degree of freedom.
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_8, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>146 8. Tabular data
</p>
<p>The normal approximation can be somewhat improved by the Yates correc-
tion, which shrinks the observed value by half a unit towards the expected
value when calculating u.
</p>
<p>We consider an example (Altman, 1991, p. 230) where 39 of 215 randomly
chosen patients are observed to have asthma and one wants to test the
hypothesis that the probability of a &ldquo;random patient&rdquo; having asthma is
0.15. This can be done using prop.test:
</p>
<p>&gt; prop.test(39,215,.15)
</p>
<p>1-sample proportions test with continuity correction
</p>
<p>data: 39 out of 215, null probability 0.15
</p>
<p>X-squared = 1.425, df = 1, p-value = 0.2326
</p>
<p>alternative hypothesis: true p is not equal to 0.15
</p>
<p>95 percent confidence interval:
</p>
<p>0.1335937 0.2408799
</p>
<p>sample estimates:
</p>
<p>p
</p>
<p>0.1813953
</p>
<p>The three arguments to prop.test are the number of positive outcomes,
the total number, and the (theoretical) probability parameter that you
want to test for. The latter is 0.5 by default, which makes sense for sym-
metrical problems, but this is not the case here. The amount 15% is a bit
synthetic since it is rarely the case that one has a specific a priori value to
test for. It is usually more interesting to compute a confidence interval for
the probability parameter, such as is given in the last part of the output.
Notice that we have a slightly unfortunate double usage of the symbol p
as the probability parameter of the binomial distribution and as the test
probability or p-value.
</p>
<p>You can also use binom.test to obtain a test in the binomial distribution.
In that way, you get an exact test probability, so it is generally preferable
to using prop.test, but prop.test can do more than testing single
proportions. The procedure to obtain the p-value is to calculate the point
probabilities for all the possible values of x and sum those that are less
than or equal to the point probability of the observed x.
</p>
<p>&gt; binom.test(39,215,.15)
</p>
<p>Exact binomial test
</p>
<p>data: 39 and 215
</p>
<p>number of successes = 39, number of trials = 215, p-value = 0.2135
</p>
<p>alternative hypothesis: true probability ... not equal to 0.15
</p>
<p>95 percent confidence interval:
</p>
<p>0.1322842 0.2395223
</p>
<p>sample estimates:</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Two independent proportions 147
</p>
<p>probability of success
</p>
<p>0.1813953
</p>
<p>The &ldquo;exact&rdquo; confidence intervals at the 0.05 level are actually constructed
from the two one-sided tests at the 0.025 level. Finding an exact confi-
dence interval using two-sided tests is not a well-defined problem (see
Exercise 8.5).
</p>
<p>8.2 Two independent proportions
</p>
<p>The function prop.test can also be used to compare two or more pro-
portions. For that purpose, the arguments should be given as two vectors,
where the first contains the number of positive outcomes and the second
the total number for each group.
</p>
<p>The theory is similar to that for a single proportion. Consider the dif-
ference in the two proportions d = x1/N1 &minus; x2/N2, which will be
approximately normally distributedwithmean zero and varianceVp(d) =
(1/N1 + 1/N2) &times; p(1 &minus; p) if the counts are binomially distributed with
the same p parameter. So to test the hypothesis that p1 = p2, plug the
common estimate p̂ = (x1 + x2)/(n1 + n2) into the variance formula and
</p>
<p>look at u = d/
&radic;
</p>
<p>Vp̂(d), which approximately follows a standard normal
</p>
<p>distribution, or look at u2, which is approximately χ2(1)-distributed. A
Yates-type correction is possible, but we skip the details.
</p>
<p>For illustration, we use an example originally due to Lewitt and Machin
(Altman, 1991, p. 232):
</p>
<p>&gt; lewitt.machin.success &lt;- c(9,4)
</p>
<p>&gt; lewitt.machin.total &lt;- c(12,13)
</p>
<p>&gt; prop.test(lewitt.machin.success,lewitt.machin.total)
</p>
<p>2-sample test for equality of proportions with continuity
</p>
<p>correction
</p>
<p>data: lewitt.machin.success out of lewitt.machin.total
</p>
<p>X-squared = 3.2793, df = 1, p-value = 0.07016
</p>
<p>alternative hypothesis: two.sided
</p>
<p>95 percent confidence interval:
</p>
<p>0.01151032 0.87310506
</p>
<p>sample estimates:
</p>
<p>prop 1 prop 2
</p>
<p>0.7500000 0.3076923</p>
<p/>
</div>
<div class="page"><p/>
<p>148 8. Tabular data
</p>
<p>The confidence interval given is for the difference in proportions. The the-
ory behind its calculation is similar to that of the test, but there are some
technical complications, and a different approximation is used.
</p>
<p>You can also perform the test without the Yates continuity correction.
This is done by adding the argument correct=F. The continuity cor-
rection makes the confidence interval somewhat wider than it would
otherwise be, but notice that it nevertheless does not contain zero. Thus,
the confidence interval is contradicting the test, which says that there
is no significant difference between the two groups with a two-sided
test. The explanation lies in the different approximations, which becomes
important for tables as sparse as the present one.
</p>
<p>If you want to be sure that at least the p-value is correct, you can use
Fisher&rsquo;s exact test. We illustrate this using the same data as in the preced-
ing section. The test works by making the calculations in the conditional
distribution of the 2&times; 2 table given both the row and column marginals.
This can be difficult to envision, but think of it like this: Take 13 white
balls and 12 black balls (success and failure, respectively), and sample the
balls without replacement into two groups of sizes 12 and 13. The num-
ber of white balls in the first group obviously defines the whole table, and
the point is that its distribution can be found as a purely combinatorial
problem. The distribution is known as the hypergeometric distribution.
</p>
<p>The relevant function is fisher.test, which requires that data be given
in matrix form. This is obtained as follows:
</p>
<p>&gt; matrix(c(9,4,3,9),2)
</p>
<p>[,1] [,2]
</p>
<p>[1,] 9 3
</p>
<p>[2,] 4 9
</p>
<p>&gt; lewitt.machin &lt;- matrix(c(9,4,3,9),2)
</p>
<p>&gt; fisher.test(lewitt.machin)
</p>
<p>Fisher&rsquo;s Exact Test for Count Data
</p>
<p>data: lewitt.machin
</p>
<p>p-value = 0.04718
</p>
<p>alternative hypothesis: true odds ratio is not equal to 1
</p>
<p>95 percent confidence interval:
</p>
<p>0.9006803 57.2549701
</p>
<p>sample estimates:
</p>
<p>odds ratio
</p>
<p>6.180528
</p>
<p>Notice that the second column of the table needs to be the number of
negative outcomes, not the total number of observations.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 k proportions, test for trend 149
</p>
<p>Notice also that the confidence interval is for the odds ratio; that is, for the
estimate of (p1/(1&minus; p1))/(p2/(1&minus; p2)). One can show that if the ps are
not identical, then the conditional distribution of the table depends only
on the odds ratio, so it is the natural measure of association in connection
with the Fisher test. The exact distribution of the test can be worked out
also when the odds ratio differs from 1, but there is the same complication
as with binom.test that a two-sided 95% confidence interval must be
pasted together from two one-sided 97.5% intervals. This leads to the op-
posite inconsistency as with prop.test: The test is (barely) significant,
but the confidence interval for the odds ratio includes 1.
</p>
<p>The standard χ2 test (see also Section 8.4) in chisq.test works with
data in matrix form, like fisher.test does. For a 2&times; 2 table, the test is
exactly equivalent to prop.test.
</p>
<p>&gt; chisq.test(lewitt.machin)
</p>
<p>Pearson&rsquo;s Chi-squared test with Yates&rsquo; continuity
</p>
<p>correction
</p>
<p>data: lewitt.machin
</p>
<p>X-squared = 3.2793, df = 1, p-value = 0.07016
</p>
<p>8.3 k proportions, test for trend
</p>
<p>Sometimes you want to compare more than two proportions. In that
case, the categories are often ordered so that you would expect to find a
decreasing or increasing trend in the proportions with the group number.
</p>
<p>The example used in this section concerns data from a group of women
giving birth where it was recorded whether the child was delivered by
caesarean section and what shoe size the mother used (Altman, 1991,
p. 229).
</p>
<p>The table looks like this:
</p>
<p>&gt; caesar.shoe
</p>
<p>&lt;4 4 4.5 5 5.5 6+
</p>
<p>Yes 5 7 6 7 8 10
</p>
<p>No 17 28 36 41 46 140
</p>
<p>To compare k &gt; 2 proportions, another test based on the normal approx-
imation is available. It consists of the calculation of a weighted sum of
squared deviations between the observed proportions in each group and
the overall proportion for all groups. The test statistic has an approximate
χ2 distribution with k&minus; 1 degrees of freedom.</p>
<p/>
</div>
<div class="page"><p/>
<p>150 8. Tabular data
</p>
<p>To use prop.test on a table like caesar.shoe, we need to convert it to
a vector of &ldquo;successes&rdquo; (which in this case is close to being the opposite)
and a vector of &ldquo;trials&rdquo;. The two vectors can be computed like this:
</p>
<p>&gt; caesar.shoe.yes &lt;- caesar.shoe["Yes",]
</p>
<p>&gt; caesar.shoe.total &lt;- margin.table(caesar.shoe,2)
</p>
<p>&gt; caesar.shoe.yes
</p>
<p>&lt;4 4 4.5 5 5.5 6+
</p>
<p>5 7 6 7 8 10
</p>
<p>&gt; caesar.shoe.total
</p>
<p>&lt;4 4 4.5 5 5.5 6+
</p>
<p>22 35 42 48 54 150
</p>
<p>Thereafter it is easy to perform the test:
</p>
<p>&gt; prop.test(caesar.shoe.yes,caesar.shoe.total)
</p>
<p>6-sample test for equality of proportions without
</p>
<p>continuity correction
</p>
<p>data: caesar.shoe.yes out of caesar.shoe.total
</p>
<p>X-squared = 9.2874, df = 5, p-value = 0.09814
</p>
<p>alternative hypothesis: two.sided
</p>
<p>sample estimates:
</p>
<p>prop 1 prop 2 prop 3 prop 4 prop 5 prop 6
</p>
<p>0.22727273 0.20000000 0.14285714 0.14583333 0.14814815 0.06666667
</p>
<p>Warning message:
</p>
<p>In prop.test(caesar.shoe.yes, caesar.shoe.total) :
</p>
<p>Chi-squared approximation may be incorrect
</p>
<p>It is seen that the test comes out nonsignificant, but the subdivision is re-
ally unreasonably fine in view of the small number of caesarean sections.
Notice, by the way, the warning about the χ2 approximation being dubi-
ous, which is prompted by some cells having an expected count less than
5.
</p>
<p>You can test for a trend in the proportions using prop.trend.test. It
takes three arguments: x, n, and score. The first two of these are exactly
as in prop.test, whereas the last one is the score given to the groups,
by default simply 1, 2, . . . , k. The basis of the test is essentially a weighted
linear regression of the proportions on the group scores, where we test for
a zero slope, which becomes a χ2 test on 1 degree of freedom.
</p>
<p>&gt; prop.trend.test(caesar.shoe.yes,caesar.shoe.total)
</p>
<p>Chi-squared Test for Trend in Proportions
</p>
<p>data: caesar.shoe.yes out of caesar.shoe.total ,
</p>
<p>using scores: 1 2 3 4 5 6
</p>
<p>X-squared = 8.0237, df = 1, p-value = 0.004617</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 r&times; c tables 151
</p>
<p>So if we assume that the effect of shoe size is linear in the group score,
then we can see a significant difference. This kind of assumption should
not be thought of as something that must hold for the test to be valid.
Rather, it indicates the rough type of alternative to which the test should
be sensitive.
</p>
<p>The effect of using a trend test can be viewed as an approximate subdivi-
sion of the test for equal proportions (χ2 = 9.29) into a contribution from
the linear effect (χ2 = 8.02) on 1 degree of freedom and a contribution
from deviations from the linear trend (χ2 = 1.27) on 4 degrees of freedom.
So you could say that the test for equal proportions is being diluted or
wastes degrees of freedom on testing for deviations in a direction we are
not really interested in.
</p>
<p>8.4 r&times; c tables
</p>
<p>For the analysis of tables with more than two classes on both sides, you
can use chisq.test or fisher.test, although you should note that
the latter can be very computationally demanding if the cell counts are
large and there are more than two rows or columns. We have already seen
chisq.test in a simple example, but with larger tables, some additional
features are of interest.
</p>
<p>An r&times; c table looks like this:
n11 n12 &middot; &middot; &middot; n1c n1&middot;
n21 n22 &middot; &middot; &middot; n2c n2&middot;
...
</p>
<p>...
...
</p>
<p>...
nr1 nr2 &middot; &middot; &middot; nrc nr&middot;
n&middot;1 n&middot;2 &middot; &middot; &middot; n&middot;c n&middot;&middot;
</p>
<p>Such a table can arise from several different sampling plans, and the
notion of &ldquo;no relation between rows and columns&rdquo; is correspondingly dif-
ferent. The total in each row might be fixed in advance, and you would be
interested in testing whether the distribution over columns is the same
for each row, or vice versa if the column totals were fixed. It might also
be the case that only the total number is chosen and the individuals are
grouped randomly according to the row and column criteria. In the latter
case, you would be interested in testing the hypothesis of statistical inde-
pendence, that the probability of an individual falling into the ijth cell is
the product pi&middot;p&middot;j of the marginal probabilities. However, the analysis of
the table turns out to be the same in all cases.</p>
<p/>
</div>
<div class="page"><p/>
<p>152 8. Tabular data
</p>
<p>If there is no relation between rows and columns, then you would expect
to have the following cell values:
</p>
<p>Eij =
ni&middot; &times; n&middot;j
</p>
<p>n&middot;&middot;
</p>
<p>This can be interpreted as distributing each row total according to the pro-
portions in each column (or vice versa) or as distributing the grand total
according to the products of the row and column proportions.
</p>
<p>The test statistic
</p>
<p>X2 = &sum;
(O&minus; E)2
</p>
<p>E
</p>
<p>has an approximate χ2 distribution with (r&minus; 1) &times; (c&minus; 1) degrees of free-
dom. Here the sum is over the entire table and the ij indices have been
omitted. O denotes the observed values and E the expected values as
described above.
</p>
<p>We consider the table with caffeine consumption and marital status from
Section 4.5 and compute the χ2 test:
</p>
<p>&gt; caff.marital &lt;- matrix(c(652,1537,598,242,36,46,38,21,218
</p>
<p>+ ,327,106,67),
</p>
<p>+ nrow=3,byrow=T)
</p>
<p>&gt; colnames(caff.marital) &lt;- c("0","1-150","151-300","&gt;300")
</p>
<p>&gt; rownames(caff.marital) &lt;- c("Married","Prev.married","Single")
</p>
<p>&gt; caff.marital
</p>
<p>0 1-150 151-300 &gt;300
</p>
<p>Married 652 1537 598 242
</p>
<p>Prev.married 36 46 38 21
</p>
<p>Single 218 327 106 67
</p>
<p>&gt; chisq.test(caff.marital)
</p>
<p>Pearson&rsquo;s Chi-squared test
</p>
<p>data: caff.marital
</p>
<p>X-squared = 51.6556, df = 6, p-value = 2.187e-09
</p>
<p>The test is highly significant, so we can safely conclude that the data con-
tradict the hypothesis of independence. However, you would generally
also like to know the nature of the deviations. To that end, you can look at
some extra components of the return value of chisq.test.
</p>
<p>Notice that chisq.test (just like lm) actually returns more information
than what is commonly printed:</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Exercises 153
</p>
<p>&gt; chisq.test(caff.marital)$expected
</p>
<p>0 1-150 151-300 &gt;300
</p>
<p>Married 705.83179 1488.01183 578.06533 257.09105
</p>
<p>Prev.married 32.85648 69.26698 26.90895 11.96759
</p>
<p>Single 167.31173 352.72119 137.02572 60.94136
</p>
<p>&gt; chisq.test(caff.marital)$observed
</p>
<p>0 1-150 151-300 &gt;300
</p>
<p>Married 652 1537 598 242
</p>
<p>Prev.married 36 46 38 21
</p>
<p>Single 218 327 106 67
</p>
<p>These two tables may then be scrutinized to see where the differences lie.
It is often useful to look at a table of the contributions from each cell to the
total χ2. Such a table cannot be directly extracted, but it is easy to calculate:
</p>
<p>&gt; E &lt;- chisq.test(caff.marital)$expected
</p>
<p>&gt; O &lt;- chisq.test(caff.marital)$observed
</p>
<p>&gt; (O-E)^2/E
</p>
<p>0 1-150 151-300 &gt;300
</p>
<p>Married 4.1055981 1.612783 0.6874502 0.8858331
</p>
<p>Prev.married 0.3007537 7.815444 4.5713926 6.8171090
</p>
<p>Single 15.3563704 1.875645 7.0249243 0.6023355
</p>
<p>There are some large contributions, particularly from too many &ldquo;abstain-
ing&rdquo; singles, and the distribution among previously married is shifted
in the direction of a larger intake &mdash; insofar as they consume caffeine at
all. Still, it is not easy to find a simple description of the deviation from
independence in these data.
</p>
<p>You can also use chisq.test directly on raw (untabulated) data, here
using the juul data set from Section 4.5:
</p>
<p>&gt; attach(juul)
</p>
<p>&gt; chisq.test(tanner,sex)
</p>
<p>Pearson&rsquo;s Chi-squared test
</p>
<p>data: tanner and sex
</p>
<p>X-squared = 28.8672, df = 4, p-value = 8.318e-06
</p>
<p>It may not really be relevant to test for independence between these par-
ticular variables. The definition of Tanner stages is gender-dependent by
nature.
</p>
<p>8.5 Exercises
</p>
<p>8.1 Reconsider the situation of Exercise 3.3, where 10 consecutive pa-
tients had operations without complications and the expected rate was</p>
<p/>
</div>
<div class="page"><p/>
<p>154 8. Tabular data
</p>
<p>20%. Calculate the relevant one-sided test in the binomial distribution.
How large a sample (still with zero complications) would be necessary to
obtain statistical significance?
</p>
<p>8.2 In 747 cases of &ldquo;Rocky Mountain spotted fever&rdquo; from the west-
ern United States, 210 patients died. Out of 661 cases from the eastern
United States, 122 died. Is the difference statistically significant? (See also
Exercise 13.4.)
</p>
<p>8.3 Two drugs for the treatment of peptic ulcer were compared (Camp-
bell and Machin, 1993, p. 72). The results were as follows:
</p>
<p>Healed Not Healed Total
Pirenzepine 23 7 30
Trithiozine 18 13 31
Total 41 20 61
</p>
<p>Compute the χ2 test and Fisher&rsquo;s exact test and discuss the difference.
Find an approximate 95% confidence interval for the difference in healing
probability.
</p>
<p>8.4 (From &ldquo;Mathematics 5&rdquo; exam, University of Copenhagen, Summer
1969.) From September 20, 1968, to February 1, 1969, an instructor con-
sumed 254 eggs. Every day, he recorded how many eggs broke during
boiling so that the white ran out and how many cracked so that the white
did not run out. Additionally, he recorded whether the eggs were size A
or size B. From February 4, 1969, until April 10, 1969, he consumed 130
eggs, but this time he used a &ldquo;piercer&rdquo; to create a small hole in the egg to
prevent breaking and cracking. The results were as follows:
</p>
<p>Period Size Total Broken Cracked
Sept. 20&ndash;Feb. 1 A 54 4 8
Sept. 20&ndash;Feb. 1 B 200 15 28
Feb. 4&ndash;Apr. 10 A 60 4 9
Feb. 4&ndash;Apr. 10 B 70 1 7
</p>
<p>Investigate whether or not the piercer seems to have had an effect.
</p>
<p>8.5 Make a plot of the two-sided p-value for testing that the probability
parameter is x when the observations are 3 successes in 15 trials for x
varying from 0 to 1 in steps of 0.001. Explain what makes the definition of
a two-sided confidence interval difficult.</p>
<p/>
</div>
<div class="page"><p/>
<p>9
Power and the computation of
sample size
</p>
<p>A statistical test will not be able to detect a true difference if the sample
size is too small compared with the magnitude of the difference. When
designing experiments, the experimenter should try to ensure that a suf-
ficient amount of data are collected to be reasonably sure that a difference
of a specified size will be detected. R has methods for doing these calcu-
lations in the simple cases of comparing means using one- or two-sample
t tests and comparing two proportions.
</p>
<p>9.1 The principles of power calculations
</p>
<p>This section outlines the theory of power calculations and sample-size
choice. If you are practically inclined and just need to find the necessary
sample size in a particular situation, you can safely skim this section and
move quickly to subsequent sections that contain the actual R calls.
</p>
<p>The basic idea of a hypothesis test should be clear by now. A test statistic is
defined, and its value is used to decide whether or not you can accept the
(null) hypothesis. Acceptance and rejection regions are set up so that the
probability of getting a test statistic that falls into the rejection region is a
specified significance level (α) if the null hypothesis is true. In the present
context, it is useful to stick to this formulation (as opposed to the use of
p-values), as rigid as it might be.
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_9, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>156 9. Power and the computation of sample size
</p>
<p>Since data are sampled at random, there is always a risk of reaching a
wrong conclusion, and things can go wrong in two ways:
</p>
<p>&bull; The hypothesis is correct, but the test rejects it (type I error).
</p>
<p>&bull; The hypothesis is wrong, but the test accepts it (type II error).
</p>
<p>The risk of a type I error is the significance level. The risk of a type II
error will depend on the size and nature of the deviation you are trying
to detect. If there is very little difference, then you do not have much of
a chance of detecting it. For this reason, some statisticians disapprove of
terms like &ldquo;acceptance region&rdquo; because you can never prove that there is
no difference &mdash; you can only fail to prove that there is one.
</p>
<p>The probability of rejecting a false hypothesis is called the power of the
test, and methods exist for calculating or approximating the power in the
most important practical situations. It is inconvenient to talk further about
these matters in the abstract, so let us move on to some concrete examples.
</p>
<p>9.1.1 Power of one-sample and paired t tests
</p>
<p>Consider the case of the comparison of a sample mean to a given value.
For example, in a matched trial we wish to test whether the difference be-
tween treatment A and treatment B is zero using a paired t test (described
in Chapter 5).
</p>
<p>We call the true difference δ. Even if the null hypothesis is not true, we can
still work out the distribution of the test statistic, provided the othermodel
assumptions hold. It is called the noncentral t distribution and depends on
a noncentrality parameter as well as the usual degrees of freedom. For the
paired t test, the noncentrality parameter ν is a function of δ, the standard
deviation of differences σ, and the sample size n and equals
</p>
<p>ν =
δ
</p>
<p>σ/
&radic;
n
</p>
<p>That is, it is simply the true difference divided by the standard error of the
mean.
</p>
<p>The cumulative noncentral t distribution is available in R simply by
adding an ncp argument to the pt function. Figure 9.1 shows a plot of
pt with ncp=3 and df=25. A vertical line indicates the upper end of the
acceptance region for a two-sided test at the 0.05 significance level. The
plot was created as follows:
</p>
<p>&gt; curve(pt(x,25,ncp=3), from=0, to=6)
</p>
<p>&gt; abline(v=qt(.975,25))</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 The principles of power calculations 157
</p>
<p>0 1 2 3 4 5 6
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>x
</p>
<p>p
t(
</p>
<p>x
, 
2
5
, 
n
c
p
 =
</p>
<p> 3
)
</p>
<p>Figure 9.1. The cumulative noncentral t distribution with ν = 3 and 25 degrees of
freedom. The vertical line marks the upper significance limit for a two-sided test
at the 0.05 level.
</p>
<p>The plot shows the main part of the distribution falling in the rejection
region. The probability of getting a value in the acceptance region can be
seen from the graph as the intersection between the curve and the vertical
line. (Almost! See Exercise 9.4.) This value is easily calculated as
</p>
<p>&gt; pt(qt(.975,25),25,ncp=3)
</p>
<p>[1] 0.1779891
</p>
<p>or roughly 0.18. The power of the test is the opposite, the probability of
getting a significant result. In this case it is 0.82, and it is of course desirable
to have the power as close to 1 as possible.
</p>
<p>Notice that the power (traditionally denoted β) depends on four quanti-
ties: δ, σ, n, and α. If we fix any three of these, we can adjust the fourth to
achieve a given power. This can be used to determine the necessary sam-
ple size for an experiment: You need to specify a desired power (β = 0.80
and β = 0.90 are common choices), the significance level (usually given by
convention as α = 0.05), a guess of the standard deviation, and δ, which
is known as the &ldquo;minimal relevant difference&rdquo; (MIREDIF) or &ldquo;smallest
meaningful difference&rdquo; (SMD). This gives an equation that you can solve</p>
<p/>
</div>
<div class="page"><p/>
<p>158 9. Power and the computation of sample size
</p>
<p>for n. The result will generally be a fractional number, which should of
course be rounded up.
</p>
<p>You can also work on the opposite problem and answer the following
question: Given a feasible sample size, how large a difference should you
reasonably be able to detect?
</p>
<p>Sometimes a shortcut is made by expressing δ relative to the standard
deviation, in which case you would simply set σ to 1.
</p>
<p>9.1.2 Power of two-sample t test
</p>
<p>Procedures for two-sample t tests are essentially the same as for the one-
sample case, except for the calculation of the noncentrality parameter,
which is calculated as
</p>
<p>ν =
δ
</p>
<p>σ
&radic;
</p>
<p>1/n1 + 1/n2
</p>
<p>It is generally assumed that the variance is the same in the two groups;
that is, using the Welch procedure is not considered. In sample-size calcu-
lations, one usually assumes that the group sizes are the same since that
gives the optimal power for a given total number of observations.
</p>
<p>9.1.3 Approximate methods
</p>
<p>For hand calculations, the power calculations can be considerably simpli-
fied by assuming that the standard deviation is known, so that the t test
is replaced by a test in the standard normal distribution. The practical ad-
vantage is that the approximate formula for the power is easily inverted
to give an explicit formula for n. For the one- and two-sample cases, this
works out as
</p>
<p>n =
</p>
<p>(
Φα/2 + Φβ
</p>
<p>δ/σ
</p>
<p>)2
</p>
<p>one-sample
</p>
<p>n = 2&times;
(
</p>
<p>Φα/2 + Φβ
δ/σ
</p>
<p>)2
</p>
<p>two-sample, each group
</p>
<p>with the Φx denoting quantiles on the normal distribution. This is for two-
sided tests. For one-sided tests, use α instead of α/2.
</p>
<p>These formulas are often found in textbooks, and some computer pro-
grams implement them rather than the more accurate method described
earlier. They do have the advantage of more clearly displaying theoretical
properties such as the proportionality of δ and 1/
</p>
<p>&radic;
n for a given power.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Two-sample problems 159
</p>
<p>However, they become numerically unsatisfactory when the degrees of
freedom falls below 20 or so.
</p>
<p>9.1.4 Power of comparisons of proportions
</p>
<p>Suppose you wish to compare the morbidity between two populations
and have to decide the number of persons to sample from each pop-
ulation. That is, you plan to perform a comparison of two binomial
distributions as in Section 8.2 using prop.test or chisq.test.
</p>
<p>For binomial comparisons, exact power calculations become unwieldy,
so we rely on normal approximations to the binomial distribution. The
power will depend on the probabilities in both groups, not just their dif-
ference. As for the t test, the group sizes are assumed to be equal. The
theoretical derivation of the power proceeds along the same lines as before
by calculating the distribution of p̂1 &minus; p̂2 when p1 6= p2 and the probabil-
ity that it falls outside the range of values compatible with the hypothesis
p1 = p2. Assuming equal numbers in the two groups, this leads to the
sample-size formula
</p>
<p>n =
</p>
<p>(
</p>
<p>Φα/2
&radic;
</p>
<p>2p(1&minus; p) + Φβ
&radic;
</p>
<p>p1(1&minus; p1) + p2(1&minus; p2)
|p2 &minus; p1|
</p>
<p>)2
</p>
<p>in which p = (p1 + p2)/2.
</p>
<p>Since the method is only approximate, the results are not reliable unless
the expected number in each of the four cells in the 2&times; 2 table is greater
than 5.
</p>
<p>9.2 Two-sample problems
</p>
<p>The following example is from Altman (1991, p. 457) and concerns the
influence of milk on growth. Two groups are to be given different diets,
and their growth will be measured. We wish to compute the sample size
that with a power of 90%, using a two-sided test at the 1% level, can find
a difference of 0.5 cm in a distribution with a standard deviation of 2 cm.
This is done as follows:
</p>
<p>&gt; power.t.test(delta=0.5, sd=2, sig.level = 0.01, power=0.9)
</p>
<p>Two-sample t test power calculation
</p>
<p>n = 477.8021
</p>
<p>delta = 0.5</p>
<p/>
</div>
<div class="page"><p/>
<p>160 9. Power and the computation of sample size
</p>
<p>sd = 2
</p>
<p>sig.level = 0.01
</p>
<p>power = 0.9
</p>
<p>alternative = two.sided
</p>
<p>NOTE: n is number in *each* group
</p>
<p>delta stands for the &ldquo;true difference&rdquo;, and sd is the standard deviation.
As is seen, the calculation may return a fractional number of experimental
units. This would, of course, in practice be rounded up to 478. In the orig-
inal reference, a method employing nomograms (a graphical technique) is
used and the value obtained is 450. The difference is probably due to diffi-
culty in reading the value off the nomogram scale. To know which power
you would actually obtain with 450 in each group, you would enter
</p>
<p>&gt; power.t.test(n=450, delta=0.5, sd=2, sig.level = 0.01)
</p>
<p>Two-sample t test power calculation
</p>
<p>n = 450
</p>
<p>delta = 0.5
</p>
<p>sd = 2
</p>
<p>sig.level = 0.01
</p>
<p>power = 0.8784433
</p>
<p>alternative = two.sided
</p>
<p>NOTE: n is number in *each* group
</p>
<p>The system is that exactly four out of five arguments (power, sig.level,
delta, sd, and n) are given, and the function computes the missing one
(defaults exist to set sd=1 and sig.level=0.05 &mdash; if you wish to have
those calculated, explicitly pass them as NULL). In addition, there are two
optional arguments: alternative, which can be used to specify one-
sided tests; and type, which can be used to specify that you want to
handle a one-sample problem. An example of the former is
</p>
<p>&gt; power.t.test(delta=0.5, sd=2, sig.level = 0.01, power=0.9,
</p>
<p>+ alt="one.sided")
</p>
<p>Two-sample t test power calculation
</p>
<p>n = 417.898
</p>
<p>delta = 0.5
</p>
<p>sd = 2
</p>
<p>sig.level = 0.01
</p>
<p>power = 0.9
</p>
<p>alternative = one.sided
</p>
<p>NOTE: n is number in *each* group</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 One-sample problems and paired tests 161
</p>
<p>9.3 One-sample problems and paired tests
</p>
<p>One-sample problems are handled by adding type="one.sample" in
the call to power.t.test. Similarly, paired tests are specified with
type="paired"; although these reduce to one-sample tests by forming
differences, the printout will be slightly different.
</p>
<p>One pitfall when planning a study with paired data is that the literature
sometimes gives the intra-individual variation as &ldquo;standard deviation of
repeated measurements on the same person&rdquo; or similar. These may be cal-
culated by measuring a number of persons several times and computing
a common standard deviation within persons. This needs to be multiplied
by
</p>
<p>&radic;
2 to get the standard deviation of differences, which power.t.test
</p>
<p>requires for paired data. If, for instance, it is known that the standard de-
viation within persons is about 10, and you want to use a paired test at the
5% level to detect a difference of 10 with a power of 85%, then you should
enter
</p>
<p>&gt; power.t.test(delta=10, sd=10*sqrt(2), power=0.85, type="paired")
</p>
<p>Paired t test power calculation
</p>
<p>n = 19.96892
</p>
<p>delta = 10
</p>
<p>sd = 14.14214
</p>
<p>sig.level = 0.05
</p>
<p>power = 0.85
</p>
<p>alternative = two.sided
</p>
<p>NOTE: n is number of *pairs*, sd is std.dev. of
</p>
<p>*differences* within pairs
</p>
<p>Notice that sig.level=0.05was taken as the default.
</p>
<p>9.4 Comparison of proportions
</p>
<p>To calculate sample sizes and related quantities for comparisons of
proportions, you should use power.prop.test. This is based on ap-
proximations with the normal distribution, so do not trust the results if
any of the expected cell counts drop below 5.
</p>
<p>The use of power.prop.test is analogous to power.t.test, although
delta and sd are replaced by the hypothesized probabilities in the two
groups, p1 and p2. Currently, it is not possible to specify that one wants
to consider a one-sample problem.</p>
<p/>
</div>
<div class="page"><p/>
<p>162 9. Power and the computation of sample size
</p>
<p>An example is given in Altman (1991, p. 459) in which two groups are
administered or not administered nicotine chewing gum and the binary
outcome is smoking cessation. The stipulated values are p1 = 0.15 and
p2 = 0.30. We want a power of 85%, and the significance level is the
traditional 5%. Inserting these values yields
</p>
<p>&gt; power.prop.test(power=.85,p1=.15,p2=.30)
</p>
<p>Two-sample comparison of proportions power calculation
</p>
<p>n = 137.6040
</p>
<p>p1 = 0.15
</p>
<p>p2 = 0.3
</p>
<p>sig.level = 0.05
</p>
<p>power = 0.85
</p>
<p>alternative = two.sided
</p>
<p>NOTE: n is number in *each* group
</p>
<p>9.5 Exercises
</p>
<p>9.1 The ashina trial was designed to have 80% power if the true
treatment difference was 15% and the standard deviation of differences
within a person was 20%. Comment on the sample size chosen. (The
power calculation was originally done using the approximative formula.
The imbalance between the group sizes is due to the use of an open
randomization procedure.)
</p>
<p>9.2 In a trial comparing a binary outcome between two groups, find the
required number of patients to find an increase in the success rate from
60% to 75% with a power of 90%. What happens if we reduce the power
requirement to 80%?
</p>
<p>9.3 Plot the density of the noncentral t distribution for ncp=3 and df=25
and compare it with the distribution of t + 3, where t has a central t
distribution with df=25.
</p>
<p>9.4 In two-sided tests, there is also a risk of falling into the rejection
region on the opposite side of the true value. The power calculations
in R only take this into account if you set strict=TRUE. Discuss the
consequences.
</p>
<p>9.5 It is occasionally suggested to choose n to &ldquo;make the true differ-
ence significant&rdquo;. What power would result from choosing n by such a
procedure?</p>
<p/>
</div>
<div class="page"><p/>
<p>10
Advanced data handling
</p>
<p>In the preceding text, we have covered a basic set of elementary statistical
procedures. In the chapters that follow, we begin to discussmore elaborate
statistical modelling.
</p>
<p>This is also a natural point to discuss some data handling techniques that
are useful in the practical analysis of data but were too advanced to cover
in the first two chapters of the book.
</p>
<p>10.1 Recoding variables
</p>
<p>This section describes some techniques that are used to construct derived
variables: grouping quantitative data, combining and renaming factor
levels, and handling date values.
</p>
<p>10.1.1 The cut function
</p>
<p>You may need to convert a quantitative variable to a grouping factor. For
instance, you may wish to present your data in terms of age in 5-year
groups, but age is in the data set as a quantitative variable, recorded as
whole years or perhaps to a finer resolution. This is what the cut function
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_10, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>164 10. Advanced data handling
</p>
<p>is for. The basic principles are quite simple, although there are some fine
points to be aware of.
</p>
<p>The function has two basic arguments: a numeric vector and a vector of
breakpoints. The latter defines a set of intervals into which the variable is
grouped. You have to specify both ends of all intervals; that is, the total
number of break points must be one more than the number of intervals.
It is a common mistake to believe that the outer breakpoints can be omit-
ted but the result for a value outside all intervals is set to NA. The outer
breakpoints can be chosen as -Inf and Inf, though.
</p>
<p>The intervals are left-open, right-closed by default. That is, they include
the breakpoint at the right end of each interval. The lowest breakpoint
is not included unless you set include.lowest=TRUE, making the first
interval closed at both ends.
</p>
<p>In (e.g.) epidemiology, you are more likely to want groupings like &ldquo;40&ndash;
49 years of age&rdquo;. This opposite convention can be obtained by setting
right=FALSE.
</p>
<p>Of course, as you switch to left-closed, right-open intervals, the issue of
losing the extreme interval endpoint shifts to the other end of the scale.
In that case, include.lowest actually includes the highest value! In the
example below, the difference lies in the inclusion of two subjects who
were exactly 16 years old.
</p>
<p>&gt; age &lt;- subset(juul, age &gt;= 10 &amp; age &lt;= 16)$age
</p>
<p>&gt; range(age)
</p>
<p>[1] 10.01 16.00
</p>
<p>&gt; agegr &lt;- cut(age, seq(10,16,2), right=F, include.lowest=T)
</p>
<p>&gt; length(age)
</p>
<p>[1] 502
</p>
<p>&gt; table(agegr)
</p>
<p>agegr
</p>
<p>[10,12) [12,14) [14,16]
</p>
<p>190 168 144
</p>
<p>&gt; agegr2 &lt;- cut(age, seq(10,16,2), right=F)
</p>
<p>&gt; table(agegr2)
</p>
<p>agegr2
</p>
<p>[10,12) [12,14) [14,16)
</p>
<p>190 168 142
</p>
<p>It is sometimes desired to split data into roughly equal-sized groups. This
can be achieved by using breakpoints computed by quantile, whichwas
described in Section 4.1. For instance, you could do
</p>
<p>&gt; q &lt;- quantile(age, c(0, .25, .50, .75, 1))
</p>
<p>&gt; q
</p>
<p>0% 25% 50% 75% 100%
</p>
<p>10.0100 11.3825 12.6400 14.2275 16.0000</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Recoding variables 165
</p>
<p>&gt; ageQ &lt;- cut(age, q, include.lowest=T)
</p>
<p>&gt; table(ageQ)
</p>
<p>ageQ
</p>
<p>[10,11.4] (11.4,12.6] (12.6,14.2] (14.2,16]
</p>
<p>126 125 125 126
</p>
<p>The level names resulting from cut turn out rather ugly at times. Fortu-
nately they are easily changed. You can modify each of the factors created
above as follows:
</p>
<p>&gt; levels(ageQ) &lt;- c("1st", "2nd", "3rd", "4th")
</p>
<p>&gt; levels(agegr) &lt;- c("10-11", "12-13", "14-15")
</p>
<p>Frank Harrell&rsquo;s Hmisc package contains the cut2 function, which simpli-
fies some of these matters.
</p>
<p>10.1.2 Manipulating factor levels
</p>
<p>In Section 1.2.8, we used levels(f)&lt;- .... to change the level set of a
factor. Some related tasks will be discussed in this section.
</p>
<p>First, notice that the conversion from numeric input and renaming of
levels can be done in one operation:
</p>
<p>&gt; pain &lt;- c(0,3,2,2,1)
</p>
<p>&gt; fpain &lt;- factor(pain,levels=0:3,
</p>
<p>+ labels=c("none","mild","medium","severe"))
</p>
<p>Beware the slightly confusing distinction between levels and labels.
The latter end up being the levels of the result, whereas the former refers
to the coding of the input vector (pain in this case). That is, levels refers
to the input and labels to the output.
</p>
<p>If you do not specify a levels argument, the levels will be the sorted,
unique values represented in the vector. This is not always desirable when
dealing with text variables since the sorting is alphabetical. Consider, for
instance,
</p>
<p>&gt; text.pain &lt;- c("none","severe", "medium", "medium", "mild")
</p>
<p>&gt; factor(text.pain)
</p>
<p>[1] none severe medium medium mild
</p>
<p>Levels: medium mild none severe
</p>
<p>Another reason for specifying levels is that the default levels, obvi-
ously, do not include values that are not present in data. This may or may
not be a problem, but it has consequences for later analyses; for instance,
whether tables contain zero entries or whether barplots leave space for the
empty columns.</p>
<p/>
</div>
<div class="page"><p/>
<p>166 10. Advanced data handling
</p>
<p>The factor function works on factors as if they were character vectors,
so you can reorder the levels as follows
</p>
<p>&gt; ftpain &lt;- factor(text.pain)
</p>
<p>&gt; ftpain2 &lt;- factor(ftpain,
</p>
<p>+ levels=c("none", "mild", "medium", "severe"))
</p>
<p>Another typical task is to combine two or more levels. This is often done
when groups would otherwise be too small for valid statistical analysis.
Say you wish to combine the levels "medium" and "mild" into a sin-
gle "intermediate" level. For this purpose, the assignment form of
levels allows the right-hand side to be a list:
</p>
<p>&gt; ftpain3 &lt;- ftpain2
</p>
<p>&gt; levels(ftpain3) &lt;- list(
</p>
<p>+ none="none",
</p>
<p>+ intermediate=c("mild","medium"),
</p>
<p>+ severe="severe")
</p>
<p>&gt; ftpain3
</p>
<p>[1] none severe intermediate intermediate
</p>
<p>[5] intermediate
</p>
<p>Levels: none intermediate severe
</p>
<p>However, it is often easier just to change the level names and give the
same name to several groups:
</p>
<p>&gt; ftpain4 &lt;- ftpain2
</p>
<p>&gt; levels(ftpain4) &lt;- c("none","intermediate","intermediate","severe")
</p>
<p>&gt; ftpain4
</p>
<p>[1] none severe intermediate intermediate
</p>
<p>[5] intermediate
</p>
<p>Levels: none intermediate severe
</p>
<p>The latter method is not quite as general as the former, though. It gives
less control over the final ordering of levels.
</p>
<p>10.1.3 Working with dates
</p>
<p>In epidemiology and survival data, you often deal with time in the form
of dates in calendar format. Different formats are used in different places
of the world, and the files you have to read were not necessarily written
in the same region as the one you are currently in. The "Date" class and
associated conversion routines exist to help you deal with the complexity.
</p>
<p>As an example, consider the Estonian stroke study, a preprocessed version
of which is contained in the data frame stroke. The raw data files can be
found in the rawdata directory of the ISwR package and read using the
following code:</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Recoding variables 167
</p>
<p>&gt; stroke &lt;- read.csv2(
</p>
<p>+ system.file("rawdata","stroke.csv", package="ISwR"),
</p>
<p>+ na.strings=".")
</p>
<p>&gt; names(stroke) &lt;- tolower(names(stroke))
</p>
<p>&gt; head(stroke)
</p>
<p>sex died dstr age dgn coma diab minf han
</p>
<p>1 1 7.01.1991 2.01.1991 76 INF 0 0 1 0
</p>
<p>2 1 &lt;NA&gt; 3.01.1991 58 INF 0 0 0 0
</p>
<p>3 1 2.06.1991 8.01.1991 74 INF 0 0 1 1
</p>
<p>4 0 13.01.1991 11.01.1991 77 ICH 0 1 0 1
</p>
<p>5 0 23.01.1996 13.01.1991 76 INF 0 1 0 1
</p>
<p>6 1 13.01.1991 13.01.1991 48 ICH 1 0 0 1
</p>
<p>(You can of course also just substitute the full path to stroke.csv instead
of using the system.file construction.)
</p>
<p>In this data set, the two date variables died and dstr (date of stroke) ap-
pear as factor variables, which is the standard behaviour of read.table.
To convert them to class "Date", we use the function as.Date. This is
straightforward but requires some attention to the date format. The for-
mat used here is (day, month, year) separated by a period (dot character),
with year given as four digits. This is not a standard format, so we need
to specify it explicitly.
</p>
<p>&gt; stroke &lt;- transform(stroke,
</p>
<p>+ died = as.Date(died, format="%d.%m.%Y"),
</p>
<p>+ dstr = as.Date(dstr, format="%d.%m.%Y"))
</p>
<p>Notice the use of &ldquo;percent-codes&rdquo; to represent specific parts of the date:
%d indicates the day of the month, %mmeans the month as a number, and
%Y means that a four-digit year is used (notice the uppercase Y). The full
set of codes is documented on the help page for strptime.
</p>
<p>Internally, dates are represented as the number of days before or after a
given point in time, known as the epoch. Specifically, the epoch is January
1, 1970, although this is an implementation detail that should not be relied
upon.
</p>
<p>It is possible to perform arithmetic on dates; that is, they behave mostly
like numeric vectors:
</p>
<p>&gt; summary(stroke$died)
</p>
<p>Min. 1st Qu. Median Mean 3rd Qu.
</p>
<p>"1991-01-07" "1992-03-14" "1993-01-23" "1993-02-15" "1993-11-04"
</p>
<p>Max.
</p>
<p>"1996-02-22"
</p>
<p>&gt; summary(stroke$dstr)
</p>
<p>Min. 1st Qu. Median Mean 3rd Qu.
</p>
<p>"1991-01-02" "1991-11-08" "1992-08-12" "1992-07-27" "1993-04-30"
</p>
<p>Max.
</p>
<p>"1993-12-31"</p>
<p/>
</div>
<div class="page"><p/>
<p>168 10. Advanced data handling
</p>
<p>&gt; summary(stroke$died - stroke$dstr)
</p>
<p>Min. 1st Qu. Median Mean 3rd Qu. Max. NA&rsquo;s
</p>
<p>0.0 8.0 28.0 225.7 268.5 1836.0 338.0
</p>
<p>&gt; head(stroke$died - stroke$dstr)
</p>
<p>Time differences in days
</p>
<p>[1] 5 NA 145 2 1836 0
</p>
<p>Notice that means and quantiles are displayed in date format (even if they
are nonintegers). The count of NA values is not displayed for date variables
even though the date of death is unknown for quite a few patients; this is
a bit unfortunate, but it would conflict with a convention that numerical
summaries have the same class as the object that is summarized (so you
would get the count displayed as a date!).
</p>
<p>The vector of differences between the two dates is actually an object of
class "difftime". Such objects can have different units &mdash; when based
on dates, it will always be "days", but for other kinds of time vari-
ables it can be "hours" or "seconds". Accordingly, it is somewhat bad
practice just to treat the vector of differences as a numeric variable. The
recommended procedure is to use as.numeric with an explicit units
argument.
</p>
<p>In the data file, NA for a death date means that the patient did not die be-
fore the end of the study on January 1, 1996. Six patients were recorded as
having died after this date, but since there may well be unrecorded deaths
among the remaining patients, we have to discard these death dates and
just record the patients as alive at the end of the study.
</p>
<p>We shall transform the data so that all patients have an end date plus an
indicator of what happened at the end date: died or survived.
</p>
<p>&gt; stroke &lt;- transform(stroke,
</p>
<p>+ end = pmin(died, as.Date("1996-1-1"), na.rm = T),
</p>
<p>+ dead = !is.na(died) &amp; died &lt; as.Date("1996-1-1"))
</p>
<p>&gt; head(stroke)
</p>
<p>sex died dstr age dgn coma diab minf han
</p>
<p>1 1 1991-01-07 1991-01-02 76 INF 0 0 1 0
</p>
<p>2 1 &lt;NA&gt; 1991-01-03 58 INF 0 0 0 0
</p>
<p>3 1 1991-06-02 1991-01-08 74 INF 0 0 1 1
</p>
<p>4 0 1991-01-13 1991-01-11 77 ICH 0 1 0 1
</p>
<p>5 0 1996-01-23 1991-01-13 76 INF 0 1 0 1
</p>
<p>6 1 1991-01-13 1991-01-13 48 ICH 1 0 0 1
</p>
<p>end dead
</p>
<p>1 1991-01-07 TRUE
</p>
<p>2 1996-01-01 FALSE
</p>
<p>3 1991-06-02 TRUE
</p>
<p>4 1991-01-13 TRUE
</p>
<p>5 1996-01-01 FALSE
</p>
<p>6 1991-01-13 TRUE</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Recoding variables 169
</p>
<p>The pmin function calculates the minimum, but unlike the min function,
which returns a single number, it does so in parallel across multiple vec-
tors. The na.rm argument allows NA values to be ignored, so the result
is that wherever died is missing or later than 1996-01-01, the end date
becomes 1996-01-01 and the actual date of death otherwise.
</p>
<p>The expression for dead is straightforward, although you should check
that missing values are treated correctly. (They are. The &amp; operator handles
missingness such that if one argument is FALSE the result is FALSE, even
if the other is NA.)
</p>
<p>Finally, to obtain the observation time for all individuals, we can do
</p>
<p>&gt; stroke &lt;- transform(stroke,
</p>
<p>+ obstime = as.numeric(end - dstr, units="days")/365.25)
</p>
<p>in which we pragmatically convert to &ldquo;epidemiological years&rdquo; of average
length. (This cannot be done just by setting units="years". Objects of
class "difftime" can only have units of "weeks" or less.)
</p>
<p>Notice that we performed the transformations in three separate calls to
transform. This was not just for the flow of the presentation; each of
the last two calls refers to variables that were not defined previously. The
transform function does not allow references to variables defined in the
same call (we could have used within, though; see Section 2.1.8).
</p>
<p>Further time classes
</p>
<p>R also has classes that represent time to a granularity finer than 1 day.
The "POSIXct" class (calendar time according to the POSIX standards)
is similar to "Date" except that it counts seconds rather than days, and
"POSIXlt" (local time) represents date and time using a structure that
consists of fields for various components: year, month, day of month,
hours, minutes, seconds, and more. Working with such objects involves,
by and large, the same issues as for the "Date" class, although with a
couple of extra twists related to time zones and Daylight Savings Time.
We shall not go deeper into this area here.
</p>
<p>10.1.4 Recoding multiple variables
</p>
<p>In the previous sections, we had some cases where essentially the same
transformation had to be applied to several variables. The solution in
those cases was simply to repeat the operation, but it can happen that a
data set contains many similar variables that all need to be recoded (ques-
tionnaire data may, for instance, have dozens of items rated on the same</p>
<p/>
</div>
<div class="page"><p/>
<p>170 10. Advanced data handling
</p>
<p>five-point scale). In such cases, you can make use of the fact that data
frames are fundamentally lists and that lapply and indexing work on
them. For instance, in dealing with the raw stroke data, we could have
done the date handling as follows:
</p>
<p>&gt; rawstroke &lt;- read.csv2(
</p>
<p>+ system.file("rawdata","stroke.csv", package="ISwR"),
</p>
<p>+ na.strings=".")
</p>
<p>&gt; ix &lt;- c("DSTR", "DIED")
</p>
<p>&gt; rawstroke[ix] &lt;- lapply(rawstroke[ix],
</p>
<p>+ as.Date, format="%d.%m.%Y")
</p>
<p>&gt; head(rawstroke)
</p>
<p>SEX DIED DSTR AGE DGN COMA DIAB MINF HAN
</p>
<p>1 1 1991-01-07 1991-01-02 76 INF 0 0 1 0
</p>
<p>2 1 &lt;NA&gt; 1991-01-03 58 INF 0 0 0 0
</p>
<p>3 1 1991-06-02 1991-01-08 74 INF 0 0 1 1
</p>
<p>4 0 1991-01-13 1991-01-11 77 ICH 0 1 0 1
</p>
<p>5 0 1996-01-23 1991-01-13 76 INF 0 1 0 1
</p>
<p>6 1 1991-01-13 1991-01-13 48 ICH 1 0 0 1
</p>
<p>Similarly, the four binary variables could be converted to &ldquo;No/Yes&rdquo;
factors in a single operation.
</p>
<p>&gt; ix &lt;- 6:9
</p>
<p>&gt; rawstroke[ix] &lt;- lapply(rawstroke[ix],
</p>
<p>+ factor, levels=0:1, labels=c("No","Yes"))
</p>
<p>10.2 Conditional calculations
</p>
<p>The ifelse function lets you apply different calculations to different
parts of data. For illustration, we use a subset of the stroke data dis-
cussed in Section 10.1.3, but we use the &ldquo;cooked&rdquo; version contained in the
ISwR package.
</p>
<p>&gt; strokesub &lt;- ISwR::stroke[1:10,2:3]
</p>
<p>&gt; strokesub
</p>
<p>died dstr
</p>
<p>1 1991-01-07 1991-01-02
</p>
<p>2 &lt;NA&gt; 1991-01-03
</p>
<p>3 1991-06-02 1991-01-08
</p>
<p>4 1991-01-13 1991-01-11
</p>
<p>5 &lt;NA&gt; 1991-01-13
</p>
<p>6 1991-01-13 1991-01-13
</p>
<p>7 1993-12-01 1991-01-14
</p>
<p>8 1991-12-12 1991-01-14
</p>
<p>9 &lt;NA&gt; 1991-01-15
</p>
<p>10 1993-11-10 1991-01-15</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Combining and restructuring data frames 171
</p>
<p>To compute the time on study and the event/censoring indicator needed
for survival models, we can do as follows:
</p>
<p>&gt; strokesub &lt;- transform(strokesub,
</p>
<p>+ event = !is.na(died))
</p>
<p>&gt; strokesub &lt;- transform(strokesub,
</p>
<p>+ obstime = ifelse(event, died-dstr, as.Date("1996-1-1") - dstr))
</p>
<p>&gt; strokesub
</p>
<p>died dstr event obstime
</p>
<p>1 1991-01-07 1991-01-02 TRUE 5
</p>
<p>2 &lt;NA&gt; 1991-01-03 FALSE 1824
</p>
<p>3 1991-06-02 1991-01-08 TRUE 145
</p>
<p>4 1991-01-13 1991-01-11 TRUE 2
</p>
<p>5 &lt;NA&gt; 1991-01-13 FALSE 1814
</p>
<p>6 1991-01-13 1991-01-13 TRUE 0
</p>
<p>7 1993-12-01 1991-01-14 TRUE 1052
</p>
<p>8 1991-12-12 1991-01-14 TRUE 332
</p>
<p>9 &lt;NA&gt; 1991-01-15 FALSE 1812
</p>
<p>10 1993-11-10 1991-01-15 TRUE 1030
</p>
<p>The way ifelse works is that it takes three arguments: test, yes, and
no. All three are vectors of the same length (if not, they will be made so by
recycling). The answer is &ldquo;stitched together&rdquo; of pieces of yes and no in
the sense that the yes element is selected wherever test is TRUE and the
no element where it is FALSE. When the condition is NA, so is the result.
</p>
<p>Notice that both alternatives are computed (exceptions are made for the
cases where the condition is all TRUE or all FALSE). This is not usually
a problem in terms of speed, but it does mean that ifelse is not the
right tool to use if you want to avoid, for example, taking the logarithm
of negative values. Also notice that ifelse discards attributes, including
the class, so that obstime is not of class "difftime" even though both
the yes and the no part are. This sometimes makes using ifelse more
trouble than it is worth, and it can be preferable simply to use explicit
subsetting operations.
</p>
<p>10.3 Combining and restructuring data frames
</p>
<p>In this section, we discuss ways of joining data frames either &ldquo;vertically&rdquo;
(adding records) or &ldquo;horizontally&rdquo; (adding variables). We also look at
the issue of converting data with repeated measurements of the same
variables between the &ldquo;long&rdquo; and the &ldquo;wide&rdquo; formats.</p>
<p/>
</div>
<div class="page"><p/>
<p>172 10. Advanced data handling
</p>
<p>10.3.1 Appending frames
</p>
<p>Sometimes data are received from multiple sources and you need to com-
bine them to form one bigger data set. In this subsection, we consider the
case where data are combined by &ldquo;vertical stacking&rdquo;; that is, you start out
with data frames which refer to separate rows of the result &mdash; typically
different subjects. It is required that the data frames contain the same vari-
ables, although not necessarily in the same order (this is unlike some other
statistical systems, which will simply insert missing values for variables
that are absent in a data set).
</p>
<p>To simulate such a situation, suppose that the juul data set had been
collected separately for boys and girls. In that case, the data frames might
not contain the variable sex, since this is the same for everyone in the
same data frame, and variables that only make sense for one gender may
also have been omitted for the other group.
</p>
<p>&gt; juulgrl &lt;- subset(juul, sex==2, select=-c(testvol,sex))
</p>
<p>&gt; juulboy &lt;- subset(juul, sex==1, select=-c(menarche,sex))
</p>
<p>Notice the use of the select argument to subset. The processing of this
argument replaces column names by column numbers, and the resulting
expression is used to index the data frame. The net effect of the negative
indices is to remove, for example, testvol and sex from juulgrl.
</p>
<p>To put the data frames back together, you must first add in the missing
variables
</p>
<p>&gt; juulgrl$sex &lt;- factor("F")
</p>
<p>&gt; juulgrl$testvol &lt;- NA
</p>
<p>&gt; juulboy$sex &lt;- factor("M")
</p>
<p>&gt; juulboy$menarche &lt;- NA
</p>
<p>and then it is just a matter of using the rbindmethod for data frames:
</p>
<p>&gt; juulall &lt;- rbind(juulboy, juulgrl)
</p>
<p>&gt; names(juulall)
</p>
<p>[1] "age" "igf1" "tanner" "testvol" "sex"
</p>
<p>[6] "menarche"
</p>
<p>Notice that rbind uses the column names (so that it does not concatenate
unrelated variables even though the order of columns differs in the two
data frames) and that the order of variables in the first data frame &ldquo;wins&rdquo;:
The result has the variables in the same order as juulboy. Notice also
that rbind is being smart about factor levels:
</p>
<p>&gt; levels(juulall$sex)
</p>
<p>[1] "M" "F"</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Combining and restructuring data frames 173
</p>
<p>10.3.2 Merging data frames
</p>
<p>Just as you may have different groups of subjects collected in separate
data sets, you may also have different sorts of data on the same patients
collected separately. For example, you could have one data frame with
registry data, one with clinical biochemistry data, and one with question-
naire data. It may work to use cbind to stick the data frames together
side-by-side, but it could be dangerous: What if the data are not complete
in all data frames or out of sequence? You typically have to work with a
unique subject identification code to avoid mistakes of this sort.
</p>
<p>The merge function deals with these issues. It works by matching on one
or several variables from each data frame. By default, this is the set of
variables that have the same name in both frames (typically, there is a
variable called something like ID, which holds the subject identification).
Assuming that this default works and that the two data frames are called
respectively dfx and dfy, the merged frame is computed simply as
</p>
<p>merge(dfx, dfy)
</p>
<p>However, there may be variables of the same name in both frames. In such
cases, you can add a by argument, which contains the variable name or
names to match on as in
</p>
<p>merge(dfx, dfy, by="ID")
</p>
<p>Any other variables that appear in both frames will have .x or .y ap-
pended to their name in the result. It is recommended to use this format in
any case as a safeguard and for readability and explicitness. If the match-
ing variable(s) have different names in the two data frames, you can use
by.x and by.y.
</p>
<p>Matching is not necessarily one-to-one. One of the data sets might for in-
stance hold tabular material corresponding to the study population. The
common example is mortality tables. In such cases, there is generally a
many-to-one relationship between the data frames. More than one subject
in the study population will belong to the table entry for 40&ndash;49 year-olds,
and the rows of the table will have to be duplicated accordingly during
the merge.
</p>
<p>To illustrate these concepts, we use the data set nickel. This describes a
cohort of nickel smelting workers in South Wales. The data set ewrates
contains a table of the population mortality by year and age group in five-
year intervals.
</p>
<p>&gt; head(nickel)
</p>
<p>id icd exposure dob age1st agein ageout
</p>
<p>1 3 0 5 1889.019 17.4808 45.2273 92.9808</p>
<p/>
</div>
<div class="page"><p/>
<p>174 10. Advanced data handling
</p>
<p>2 4 162 5 1885.978 23.1864 48.2684 63.2712
</p>
<p>3 6 163 10 1881.255 25.2452 52.9917 54.1644
</p>
<p>4 8 527 9 1886.340 24.7206 47.9067 69.6794
</p>
<p>5 9 150 0 1879.500 29.9575 54.7465 76.8442
</p>
<p>6 10 163 2 1889.915 21.2877 44.3314 62.5413
</p>
<p>&gt; head(ewrates)
</p>
<p>year age lung nasal other
</p>
<p>1 1931 10 1 0 1269
</p>
<p>2 1931 15 2 0 2201
</p>
<p>3 1931 20 6 0 3116
</p>
<p>4 1931 25 14 0 3024
</p>
<p>5 1931 30 30 1 3188
</p>
<p>6 1931 35 68 1 4165
</p>
<p>Suppose we wish to merge these two data sets according to the values
at entry into the study population. This age is contained in agein, and
the date of entry is computed as dob + agein. You can compute group
codes corresponding to ewrates as follows:
</p>
<p>&gt; nickel &lt;- transform(nickel,
</p>
<p>+ agr = trunc(agein/5)*5,
</p>
<p>+ ygr = trunc((dob+agein-1)/5)*5+1)
</p>
<p>The trunc function rounds values towards zero. Notice that the age
groups start on values that are evenly divisible by 5, whereas the year
groups end on such values; this is why the expression for ygr subtracts 1
and adds it back after truncation. (Actually this does not matter because
all enrollment dates were April 1 of 1934, 1939, 1944, or 1949.) Notice also
that we do not use the same variable names as in ewrates. We could have
done so, but the names age and yearwould be unintuitive in the context
of the nickel data.
</p>
<p>With the age and year groups defined, it is an easy matter to perform the
merge. We just need to account for the fact that we have used different
variable names in the two data frames.
</p>
<p>&gt; mrg &lt;- merge(nickel, ewrates,
</p>
<p>+ by.x=c("agr","ygr"), by.y=c("age","year"))
</p>
<p>&gt; head(mrg,10)
</p>
<p>agr ygr id icd exposure dob age1st agein ageout
</p>
<p>1 20 1931 273 154 0 1909.500 14.6913 24.7465 55.9302
</p>
<p>2 20 1931 213 162 0 1910.129 14.2018 24.1177 63.0493
</p>
<p>3 20 1931 546 0 0 1909.500 14.4945 24.7465 72.5000
</p>
<p>4 20 1931 574 491 0 1909.729 14.0356 24.5177 70.6592
</p>
<p>5 20 1931 110 0 0 1909.247 14.0302 24.9999 72.7534
</p>
<p>6 20 1931 325 434 0 1910.500 14.0737 23.7465 43.0343
</p>
<p>7 25 1931 56 502 2 1904.500 18.2917 29.7465 51.5847
</p>
<p>8 25 1931 690 420 0 1906.500 17.2206 27.7465 55.1219
</p>
<p>9 25 1931 443 420 0 1905.326 14.5562 28.9204 65.7616
</p>
<p>10 25 1931 137 465 0 1905.386 19.0808 28.8601 74.2794
</p>
<p>lung nasal other</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Combining and restructuring data frames 175
</p>
<p>1 6 0 3116
</p>
<p>2 6 0 3116
</p>
<p>3 6 0 3116
</p>
<p>4 6 0 3116
</p>
<p>5 6 0 3116
</p>
<p>6 6 0 3116
</p>
<p>7 14 0 3024
</p>
<p>8 14 0 3024
</p>
<p>9 14 0 3024
</p>
<p>10 14 0 3024
</p>
<p>We have only described the main function of merge. There are also op-
tions to include rows that only exist in one of the two frames (all, all.x,
all.y), and it may also be useful to know that the pseudo-variable
row.nameswill allow matching on row names.
</p>
<p>We have discussed the cases of one-to-one and many-to-one matching.
Many-to-many is possible but rarely useful. What happens in that case
is that the &ldquo;Cartesian product&rdquo; is formed by generating all combinations
of rows from the two frames within each matching set. The extreme case
of many-to-many matching occurs if the by set is empty, which gives a
result with as many rows as the product of the row counts. This sometimes
surprises people who expect that the row number will act as an implicit
ID.
</p>
<p>10.3.3 Reshaping data frames
</p>
<p>Longitudinal data come in two different forms: a &ldquo;wide&rdquo; format, where
there is a separate column for each time point but only one record per
case; and a &ldquo;long&rdquo; format, where there are multiple records for each case,
one for each time point. The long format is more general since it does not
need to assume that the cases are recorded at the same set of times, but
when applicable it may be easier to work with data in the wide format,
and some statistical functions expect it that way. Other functions expect to
find data in the long format. Either way, there is a need to convert from
one format to another. This is what the reshape function does.
</p>
<p>Consider the following data from a randomized study of bonemetabolism
data during Tamoxifen treatment after breast cancer. The concentration
of alkaline phosphatase is recorded at baseline and 3, 6, 9, 12, 18, and
24 months after treatment start.
</p>
<p>&gt; head(alkfos)
</p>
<p>grp c0 c3 c6 c9 c12 c18 c24
</p>
<p>1 1 142 140 159 162 152 175 148
</p>
<p>2 1 120 126 120 146 134 119 116
</p>
<p>3 1 175 161 168 164 213 194 221</p>
<p/>
</div>
<div class="page"><p/>
<p>176 10. Advanced data handling
</p>
<p>4 1 234 203 174 197 289 174 189
</p>
<p>5 1 94 107 146 124 128 98 114
</p>
<p>6 1 128 97 113 203 NA NA NA
</p>
<p>In the simplest uses of reshape, the function will assume that the vari-
able names encode the information necessary for reshaping to the long
format. By default, it assumes that variable names are separated from time
ofmeasurement by a "." (dot), sowemight oblige bymodifying the name
format.
</p>
<p>&gt; a2 &lt;- alkfos
</p>
<p>&gt; names(a2) &lt;- sub("c", "c.", names(a2))
</p>
<p>&gt; names(a2)
</p>
<p>[1] "grp" "c.0" "c.3" "c.6" "c.9" "c.12" "c.18" "c.24"
</p>
<p>The sub function does substitutions within character strings, in this case
replacing the string "c" with "c.". Alternatively, the original name for-
mat (c0, . . . , c24) can be handled by adding sep="" to the reshape
call.
</p>
<p>Once we have the variable naming in place, the only things we need to
specify are the direction of the reshape and the set of variables to be con-
sidered time-varying. As a convenience feature, the latter can be specified
by index rather than by name.
</p>
<p>&gt; a.long &lt;- reshape(a2, varying=2:8, direction="long")
</p>
<p>&gt; head(a.long)
</p>
<p>grp time c id
</p>
<p>1.0 1 0 142 1
</p>
<p>2.0 1 0 120 2
</p>
<p>3.0 1 0 175 3
</p>
<p>4.0 1 0 234 4
</p>
<p>5.0 1 0 94 5
</p>
<p>6.0 1 0 128 6
</p>
<p>&gt; tail(a.long)
</p>
<p>grp time c id
</p>
<p>38.24 2 24 95 38
</p>
<p>39.24 2 24 NA 39
</p>
<p>40.24 2 24 192 40
</p>
<p>41.24 2 24 94 41
</p>
<p>42.24 2 24 194 42
</p>
<p>43.24 2 24 129 43
</p>
<p>Notice that the sort order of the result is that id varies within time. This
is the most convenient format to generate technically, but if you prefer the
opposite sort order, just use
</p>
<p>&gt; o &lt;- with(a.long, order(id, time))
</p>
<p>&gt; head(a.long[o,], 10)
</p>
<p>grp time c id</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Combining and restructuring data frames 177
</p>
<p>1.0 1 0 142 1
</p>
<p>1.3 1 3 140 1
</p>
<p>1.6 1 6 159 1
</p>
<p>1.9 1 9 162 1
</p>
<p>1.12 1 12 152 1
</p>
<p>1.18 1 18 175 1
</p>
<p>1.24 1 24 148 1
</p>
<p>2.0 1 0 120 2
</p>
<p>2.3 1 3 126 2
</p>
<p>2.6 1 6 120 2
</p>
<p>To demonstrate the reverse procedure, we use the same data, in the long
format. Actually, this is a bit too easy because reshape has inserted
enough information in its output to let you convert to the wide format just
by saying reshape(a.long). To simulate the situation where the orig-
inal data are given in the long format, we remove the "reshapeLong"
attribute, which holds these data. Furthermore, we remove the records for
which we have missing data by using na.omit.
</p>
<p>&gt; a.long2 &lt;- na.omit(a.long)
</p>
<p>&gt; attr(a.long2, "reshapeLong") &lt;- NULL
</p>
<p>To convert a.long2 to the wide format, use
</p>
<p>&gt; a.wide2 &lt;- reshape(a.long2, direction="wide", v.names="c",
</p>
<p>+ idvar="id", timevar="time")
</p>
<p>&gt; head(a.wide2)
</p>
<p>grp id c.0 c.3 c.6 c.9 c.12 c.18 c.24
</p>
<p>1.0 1 1 142 140 159 162 152 175 148
</p>
<p>2.0 1 2 120 126 120 146 134 119 116
</p>
<p>3.0 1 3 175 161 168 164 213 194 221
</p>
<p>4.0 1 4 234 203 174 197 289 174 189
</p>
<p>5.0 1 5 94 107 146 124 128 98 114
</p>
<p>6.0 1 6 128 97 113 203 NA NA NA
</p>
<p>Notice that NA values are filled in for patient no. 6, for whom only the first
four observations are available.
</p>
<p>The arguments idvar and timevar specify the names of the variables
that contain the ID and the time for each observation. It is not strictly
necessary to specify them if they have their default names, but it is good
practice to do so. The argument v.names specifies the time-varying vari-
ables; notice that if it were omitted, then the grp variable would also be
treated as time-varying.</p>
<p/>
</div>
<div class="page"><p/>
<p>178 10. Advanced data handling
</p>
<p>10.4 Per-group and per-case procedures
</p>
<p>A specific data management task involves operations within subsets of
a data frame, particularly those where there are multiple records for
each individual. Examples include calculation of cumulative dosage in a
pharmacokinetic experiment and various methods of normalization and
standardization.
</p>
<p>A nice general approach to such tasks is first to split the data into a list of
groups, operate on each group, and then put the pieces back together.
</p>
<p>Consider the task of normalizing the values of alkaline phosphatase in
a.long to their baseline values. The split function can be used to
generate a list of the individual time courses:
</p>
<p>&gt; l &lt;- split(a.long$c, a.long$id)
</p>
<p>&gt; l[1:3]
</p>
<p>$&lsquo;1&lsquo;
</p>
<p>[1] 142 140 159 162 152 175 148
</p>
<p>$&lsquo;2&lsquo;
</p>
<p>[1] 120 126 120 146 134 119 116
</p>
<p>$&lsquo;3&lsquo;
</p>
<p>[1] 175 161 168 164 213 194 221
</p>
<p>Next, we apply a function to each element of the list and collect the results
using lapply.
</p>
<p>&gt; l2 &lt;- lapply(l, function(x) x / x[1])
</p>
<p>Finally, we put the pieces back together using unsplit, which is the
reverse operation of split. Notice that a.long has id varying within
time, so this is not just a matter of concatenating the elements of l2. The
data for the first patient are now
</p>
<p>&gt; a.long$c.adj &lt;- unsplit(l2, a.long$id)
</p>
<p>&gt; subset(a.long, id==1)
</p>
<p>grp time c id c.adj
</p>
<p>1.0 1 0 142 1 1.0000000
</p>
<p>1.3 1 3 140 1 0.9859155
</p>
<p>1.6 1 6 159 1 1.1197183
</p>
<p>1.9 1 9 162 1 1.1408451
</p>
<p>1.12 1 12 152 1 1.0704225
</p>
<p>1.18 1 18 175 1 1.2323944
</p>
<p>1.24 1 24 148 1 1.0422535
</p>
<p>In fact, there is a function that formalizes this sort of split-modify-unsplit
operation. It is called ave because the default use is to replace data with</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Time splitting 179
</p>
<p>group averages, but it can also be used for more general transformations.
The following is an alternative way of doing the same computation as
above:
</p>
<p>&gt; a.long$c.adj &lt;- ave(a.long$c, a.long$id,
</p>
<p>+ FUN = function(x) x / x[1])
</p>
<p>In the preceding code, we worked on the single vector a.long$c.
Alternatively, we can split the entire data frame and use code like
</p>
<p>&gt; l &lt;- split(a.long, a.long$id)
</p>
<p>&gt; l2 &lt;- lapply(l, transform, c.adj = c / c[1])
</p>
<p>&gt; a.long2 &lt;- unsplit(l2, a.long$id)
</p>
<p>Notice how the last argument to lapply is passed on to transform, so
that you effectively call transform(x, c.adj = c / c[1]) for each
data frame x in the list l. This procedure is somewhat less efficient than
the first one because there is more copying of data, but it generalizes to
more complex transformations.
</p>
<p>10.5 Time splitting
</p>
<p>This section is rather advanced, and the beginner may want to skip it on
the first read. Understanding the contents is not crucial for the later parts
of the book. On the other hand, apart from solving the particular problem,
this is also a rather nice first example of the use of ad hoc programming in
R and also of the &ldquo;lateral thinking&rdquo; that is sometimes required.
</p>
<p>The merge operation of the nickel and ewrates data in Section 10.3.2
does not really make sense statistically: We merged in the mortality table
corresponding to the age at the time of entry into the study population.
However, the data set is about cancer, a slow disease, and an exposure
that perhaps leads to an increased risk 20 ormore years later. If the subjects
typically die around age 50, the population mortality for people of age 30
is hardly relevant.
</p>
<p>A sensible statistical analysis needs to consider the population mortality
during the entire follow-up period. One way to handle this issue is to split
the individuals into multiple &ldquo;sub-individuals&rdquo;.
</p>
<p>In the data set, the first six observations are (after the merge in Sec-
tion 10.3.2)
</p>
<p>&gt; head(nickel)
</p>
<p>id icd exposure dob age1st agein ageout agr ygr
</p>
<p>1 3 0 5 1889.019 17.4808 45.2273 92.9808 45 1931</p>
<p/>
</div>
<div class="page"><p/>
<p>180 10. Advanced data handling
</p>
<p>2 4 162 5 1885.978 23.1864 48.2684 63.2712 45 1931
</p>
<p>3 6 163 10 1881.255 25.2452 52.9917 54.1644 50 1931
</p>
<p>4 8 527 9 1886.340 24.7206 47.9067 69.6794 45 1931
</p>
<p>5 9 150 0 1879.500 29.9575 54.7465 76.8442 50 1931
</p>
<p>6 10 163 2 1889.915 21.2877 44.3314 62.5413 40 1931
</p>
<p>Consider the individual with id == 4; this person entered the study at
the age of 48.2684 and died (from lung cancer) at the age of 63.2712 (apolo-
gies for the excess precision). The time-splitting method treats this subject
as four separate subjects, one entering the study at age 48.2684 and leav-
ing at age 50 (on his 50th birthday) and the others covering the intervals
50&ndash;55, 55&ndash;60, and 60&ndash;63.2712. The first three are censored observations, as
the subject did not die.
</p>
<p>If we merge these data with the population tables, then we can compute
the expected number of deaths in a given age interval and compare that
with the actual number of deaths.
</p>
<p>Taking advantage of the vectorized nature of computations in R, the
nice way of doing this is to loop over age intervals, &ldquo;trimming&rdquo; every
observation period to each interval.
</p>
<p>To trim the observation periods to ages between (say) 60 and 65, the entry
and exit times should be adjusted to the interval if they fall outside of it,
cases that are unobserved during the interval should be removed, and if
the subject did not die inside the interval, icd should be set to 0.
</p>
<p>The easiest procedure is to &ldquo;shoot first and ask later&rdquo;. The adjusted entry
and exit times are
</p>
<p>&gt; entry &lt;- pmax(nickel$agein, 60)
</p>
<p>&gt; exit &lt;- pmin(nickel$ageout, 65)
</p>
<p>or rather they would be if there were always a suitable overlap between
the observation period and the target age interval. However, there are peo-
ple leaving the study population before age 60 (by death or otherwise) and
people entering the study after age 65. In either case, what goes wrong is
that entry &gt;= exit, and we can check for such cases by calculating
</p>
<p>&gt; valid &lt;- (entry &lt; exit)
</p>
<p>&gt; entry &lt;- entry[valid]
</p>
<p>&gt; exit &lt;- exit[valid]
</p>
<p>The censoring indicator for valid cases is
</p>
<p>&gt; cens &lt;- (nickel$ageout[valid] &gt; 65)
</p>
<p>(We might have used cens &lt;- (exit == 65), but it is a good rule to
avoid testing floating point data for equality.)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Time splitting 181
</p>
<p>The trimmed data set can then be obtained as
</p>
<p>&gt; nickel60 &lt;- nickel[valid,]
</p>
<p>&gt; nickel60$icd[cens] &lt;- 0
</p>
<p>&gt; nickel60$agein &lt;- entry
</p>
<p>&gt; nickel60$ageout &lt;- exit
</p>
<p>&gt; nickel60$agr &lt;- 60
</p>
<p>&gt; nickel60$ygr &lt;- with(nickel60, trunc((dob+agein-1)/5)*5+1)
</p>
<p>and the first lines of the result are
</p>
<p>&gt; head(nickel60)
</p>
<p>id icd exposure dob age1st agein ageout agr ygr
</p>
<p>1 3 0 5 1889.019 17.4808 60 65.0000 60 1946
</p>
<p>2 4 162 5 1885.978 23.1864 60 63.2712 60 1941
</p>
<p>4 8 0 9 1886.340 24.7206 60 65.0000 60 1946
</p>
<p>5 9 0 0 1879.500 29.9575 60 65.0000 60 1936
</p>
<p>6 10 163 2 1889.915 21.2877 60 62.5413 60 1946
</p>
<p>7 15 334 0 1890.500 23.2836 60 62.0000 60 1946
</p>
<p>A couple of fine points: If someone dies exactly at age 65, they are counted
as dying inside the age interval. Conversely, we do not include people
dying exactly at age 60; they belong in the interval 55&ndash;60 (for purposes
like those of Chapter 15, one should avoid observation intervals of length
zero). It was also necessary to recompute ygr since this was based on the
original agein.
</p>
<p>To get the fully expanded data set, you could repeat the above for each age
interval (20&ndash;25, . . . , 95&ndash;100) and append the resulting 16 data frames with
rbind. However, this gets rather long-winded, and there is a substantial
risk of copy-paste errors. Instead, you can do a little programming. First,
wrap up the procedure for one group as a function:
</p>
<p>&gt; trim &lt;- function(start)
</p>
<p>+ {
</p>
<p>+ end &lt;- start + 5
</p>
<p>+ entry &lt;- pmax(nickel$agein, start)
</p>
<p>+ exit &lt;- pmin(nickel$ageout, end)
</p>
<p>+ valid &lt;- (entry &lt; exit)
</p>
<p>+ cens &lt;- (nickel$ageout[valid] &gt; end)
</p>
<p>+ result &lt;- nickel[valid,]
</p>
<p>+ result$icd[cens] &lt;- 0
</p>
<p>+ result$agein &lt;- entry[valid]
</p>
<p>+ result$ageout &lt;- exit[valid]
</p>
<p>+ result$agr &lt;- start
</p>
<p>+ result$ygr &lt;- with(result, trunc((dob+agein-1)/5)*5+1)
</p>
<p>+ result
</p>
<p>+ }
</p>
<p>(In practice, you should not type all this at the command line but use a
script window or an editor; see Section 2.1.3.)</p>
<p/>
</div>
<div class="page"><p/>
<p>182 10. Advanced data handling
</p>
<p>This is typical ad hoc programming. The function is far from general
since it relies on knowing various names, and it also hardcodes the in-
terval length as 5. However, more generality is not required for a one-off
calculation. The important thing for the purpose at hand is to make the
dependence on start explicit so that we can loop over it.
</p>
<p>With this definition, trim(60) is equivalent to the nickel60 we
computed earlier:
</p>
<p>&gt; head(trim(60))
</p>
<p>id icd exposure dob age1st agein ageout agr ygr
</p>
<p>1 3 0 5 1889.019 17.4808 60 65.0000 60 1946
</p>
<p>2 4 162 5 1885.978 23.1864 60 63.2712 60 1941
</p>
<p>4 8 0 9 1886.340 24.7206 60 65.0000 60 1946
</p>
<p>5 9 0 0 1879.500 29.9575 60 65.0000 60 1936
</p>
<p>6 10 163 2 1889.915 21.2877 60 62.5413 60 1946
</p>
<p>7 15 334 0 1890.500 23.2836 60 62.0000 60 1946
</p>
<p>To get results for all intervals, do the following:
</p>
<p>&gt; nickel.expand &lt;- do.call("rbind", lapply(seq(20,95,5), trim))
</p>
<p>&gt; head(nickel.expand)
</p>
<p>id icd exposure dob age1st agein ageout agr ygr
</p>
<p>84 110 0 0 1909.247 14.0302 24.9999 25 20 1931
</p>
<p>156 213 0 0 1910.129 14.2018 24.1177 25 20 1931
</p>
<p>197 273 0 0 1909.500 14.6913 24.7465 25 20 1931
</p>
<p>236 325 0 0 1910.500 14.0737 23.7465 25 20 1931
</p>
<p>384 546 0 0 1909.500 14.4945 24.7465 25 20 1931
</p>
<p>400 574 0 0 1909.729 14.0356 24.5177 25 20 1931
</p>
<p>The do.call construct works by creating a call to rbind with a given
argument list, which in this case is the return value from lapply, which
in turn has applied the trim function to each of the values 20, 25, . . . 95.
That is, the whole thing is equivalent to
</p>
<p>rbind(trim(20), trim(25), ......, trim(95))
</p>
<p>Displaying the result for a single subject yields, for example,
</p>
<p>&gt; subset(nickel.expand, id==4)
</p>
<p>id icd exposure dob age1st agein ageout agr ygr
</p>
<p>2 4 0 5 1885.978 23.1864 48.2684 50.0000 45 1931
</p>
<p>2100 4 0 5 1885.978 23.1864 50.0000 55.0000 50 1931
</p>
<p>2102 4 0 5 1885.978 23.1864 55.0000 60.0000 55 1936
</p>
<p>2104 4 162 5 1885.978 23.1864 60.0000 63.2712 60 1941
</p>
<p>(The strange row names occur because multiple data frames with the
same row names are being rbind-ed together and data frames must have
unique row names.)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Exercises 183
</p>
<p>A weakness of the ygr computation is that since ygr refers to the calen-
dar time group at agein, it may be off by up to 5 years. However, lung
cancer death rates by age do not change that quickly, so we leave it at this.
A more careful procedure, and in fact the common practice in epidemiol-
ogy, is to split on both age and calendar time. The Epi package contains
generalized time-splitters splitLexis and cutLexis, which are useful
for this purpose and also for handling the related case of splitting time
based on individual events (e.g., childbirth).
</p>
<p>As a final step, we can merge in the mortality table as we did in
Section 10.3.2.
</p>
<p>&gt; nickel.expand &lt;- merge(nickel.expand, ewrates,
</p>
<p>+ by.x=c("agr","ygr"), by.y=c("age","year"))
</p>
<p>&gt; head(nickel.expand)
</p>
<p>agr ygr id icd exposure dob age1st agein ageout lung
</p>
<p>1 20 1931 325 0 0 1910.500 14.0737 23.7465 25 6
</p>
<p>2 20 1931 273 0 0 1909.500 14.6913 24.7465 25 6
</p>
<p>3 20 1931 110 0 0 1909.247 14.0302 24.9999 25 6
</p>
<p>4 20 1931 574 0 0 1909.729 14.0356 24.5177 25 6
</p>
<p>5 20 1931 213 0 0 1910.129 14.2018 24.1177 25 6
</p>
<p>6 20 1931 546 0 0 1909.500 14.4945 24.7465 25 6
</p>
<p>nasal other
</p>
<p>1 0 3116
</p>
<p>2 0 3116
</p>
<p>3 0 3116
</p>
<p>4 0 3116
</p>
<p>5 0 3116
</p>
<p>6 0 3116
</p>
<p>For later use, the expanded data set is made available &ldquo;precooked&rdquo; in the
ISwR package under the name nickel.expand. We return to the data
set in connection with the analysis of rates in Chapter 15.
</p>
<p>10.6 Exercises
</p>
<p>10.1 Create a factor in which the blood.glucose variable in the
thuesen data is divided into the intervals (4, 7], (7, 9], (9, 12], and (12, 20].
Change the level names to &ldquo;low&rdquo;, &ldquo;intermediate&rdquo;, &ldquo;high&rdquo;, and &ldquo;very
high&rdquo;.
</p>
<p>10.2 In the bcmort data set, the four-level factor cohort can be consid-
ered the product of two two-level factors, say period and area. How
can you generate them?</p>
<p/>
</div>
<div class="page"><p/>
<p>184 10. Advanced data handling
</p>
<p>10.3 Convert the ashina data to the long format. Consider how to
encode whether the vas measurement is from the first or the second
measurement session.
</p>
<p>10.4 Split the stroke data according to obsmonths into time intervals
0&ndash;0.5, 0.5&ndash;2, 2&ndash;12, and 12+ months after stroke.</p>
<p/>
</div>
<div class="page"><p/>
<p>11
Multiple regression
</p>
<p>This chapter discusses the case of regression analysis with multiple pre-
dictors. There is not really much new here since model specification and
output do not differ a lot from what has been described for regression
analysis and analysis of variance. The news is mainly the model search
aspect, namely among a set of potential descriptive variables to look for a
subset that describes the response sufficiently well.
</p>
<p>The basic model for multiple regression analysis is
</p>
<p>y = β0 + β1x1 + &middot; &middot; &middot;+ βkxk + ǫ
where x1, . . . xk are explanatory variables (also called predictors) and the
parameters β1, . . . , βk can be estimated using the method of least squares
(see Section 6.1). A closed-form expression for the estimates can be derived
using matrix calculus, but we do not go into the details of that here.
</p>
<p>11.1 Plotting multivariate data
</p>
<p>As an example in this chapter, we use a study concerning lung function in
patients with cystic fibrosis in Altman (1991, p. 338). The data are in the
cystfibr data frame in the ISwR package.
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_11, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>186 11. Multiple regression
</p>
<p>age
</p>
<p>0.0 0.6 20 50 20 40 100 200 60 120 200
</p>
<p>1
0
</p>
<p>2
0
</p>
<p>0
.0
</p>
<p>0
.6
</p>
<p>sex
</p>
<p>height
</p>
<p>1
1
0
</p>
<p>1
5
</p>
<p>0
</p>
<p>2
0
</p>
<p>5
0
</p>
<p>weight
</p>
<p>bmp
</p>
<p>6
5
</p>
<p>8
0
</p>
<p>9
5
</p>
<p>2
0
</p>
<p>4
0
</p>
<p>fev1
</p>
<p>rv
</p>
<p>1
5
0
</p>
<p>3
0
0
</p>
<p>4
5
0
</p>
<p>1
0
0
</p>
<p>2
0
0
</p>
<p>frc
</p>
<p>tlc
</p>
<p>8
0
</p>
<p>1
1
0
</p>
<p>10 20
</p>
<p>6
0
</p>
<p>1
2
0
</p>
<p>2
0
0
</p>
<p>110 150 65 80 95 150 300 450 80 110
</p>
<p>pemax
</p>
<p>Figure 11.1. Pairwise plots for cystic fibrosis data.
</p>
<p>You can obtain pairwise scatterplots between all the variables in the data
set. This is done using the function pairs. To get Figure 11.1, you simply
write
</p>
<p>&gt; par(mex=0.5)
</p>
<p>&gt; pairs(cystfibr, gap=0, cex.labels=0.9)
</p>
<p>The arguments gap and cex.labels control the visual appearance by
removing the space between subplots and decreasing the font size. The
mex graphics parameter reduces the interline distance in the margins.
</p>
<p>A similar plot is obtained by simply saying plot(cystfibr) since the
plot function is generic and behaves differently depending on the class
of its arguments (see Section 2.3.2). Here the argument is a data frame and
a pairs plot is a fairly reasonable thing to get when asking for a plot of an</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Model specification and output 187
</p>
<p>entire data frame (although you might equally reasonably have expected
a histogram or a barchart of each variable instead).
</p>
<p>The individual plots do get rather small, probably not suitable for di-
rect publication, but such plots are quite an effective way of obtaining
an overview of multidimensional issues. For example, the close relations
among age, height, and weight appear clearly on the plot.
</p>
<p>In order to be able to refer directly to the variables in cystfibr, we add
it to the search path (a harmless warning about masking of tlc ensues at
this point):
</p>
<p>&gt; attach(cystfibr)
</p>
<p>Because this data set contains common variable names such as age,
height, and weight, it is a good idea to ensure that you do not have
identically named variables in the workspace at this point. In particular,
such names were used in the introductory session.
</p>
<p>11.2 Model specification and output
</p>
<p>Specification of a multiple regression analysis is done by setting up a
model formula with + between the explanatory variables:
</p>
<p>lm(pemax~age+sex+height+weight+bmp+fev1+rv+frc+tlc)
</p>
<p>which is meant to be read as &ldquo;pemax is described using a model that
is additive in age, sex, and so forth.&rdquo; (pemax is the maximal expira-
tory pressure. See Appendix B for a description of the other variables in
cystfibr.)
</p>
<p>As usual, there is not much output from lm itself, but with the aid of
summary you can obtain some more interesting output:
</p>
<p>&gt; summary(lm(pemax~age+sex+height+weight+bmp+fev1+rv+frc+tlc))
</p>
<p>Call:
</p>
<p>lm(formula = pemax ~ age + sex + height + weight + bmp + fev1 +
</p>
<p>rv + frc + tlc)
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-37.338 -11.532 1.081 13.386 33.405
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 176.0582 225.8912 0.779 0.448</p>
<p/>
</div>
<div class="page"><p/>
<p>188 11. Multiple regression
</p>
<p>age -2.5420 4.8017 -0.529 0.604
</p>
<p>sex -3.7368 15.4598 -0.242 0.812
</p>
<p>height -0.4463 0.9034 -0.494 0.628
</p>
<p>weight 2.9928 2.0080 1.490 0.157
</p>
<p>bmp -1.7449 1.1552 -1.510 0.152
</p>
<p>fev1 1.0807 1.0809 1.000 0.333
</p>
<p>rv 0.1970 0.1962 1.004 0.331
</p>
<p>frc -0.3084 0.4924 -0.626 0.540
</p>
<p>tlc 0.1886 0.4997 0.377 0.711
</p>
<p>Residual standard error: 25.47 on 15 degrees of freedom
</p>
<p>Multiple R-squared: 0.6373, Adjusted R-squared: 0.4197
</p>
<p>F-statistic: 2.929 on 9 and 15 DF, p-value: 0.03195
</p>
<p>The layout should be well known by now. Notice that there is not one
single significant t value, but the joint F test is nevertheless significant,
so there must be an effect somewhere. The reason is that the t tests only
say something about what happens if you remove one variable and leave
in all the others. You cannot see whether a variable would be statistically
significant in a reduced model; all you can see is that no variable must be
included.
</p>
<p>Note further that there is quite a large difference between the unadjusted
and the adjusted R2, which is due to the large number of variables relative
to the number of degrees of freedom for the variance. Recall that the for-
mer is the change in residual sum of squares relative to an empty model,
whereas the latter is the similar change in residual variance:
</p>
<p>&gt; 1-25.5^2/var(pemax)
</p>
<p>[1] 0.4183949
</p>
<p>The 25.5 comes from &ldquo;residual standard error&rdquo; in the summary output.
</p>
<p>The ANOVA table for a multiple regression analysis is obtained using
anova and gives a rather different picture:
</p>
<p>&gt; anova(lm(pemax~age+sex+height+weight+bmp+fev1+rv+frc+tlc))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: pemax
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>age 1 10098.5 10098.5 15.5661 0.001296 **
</p>
<p>sex 1 955.4 955.4 1.4727 0.243680
</p>
<p>height 1 155.0 155.0 0.2389 0.632089
</p>
<p>weight 1 632.3 632.3 0.9747 0.339170
</p>
<p>bmp 1 2862.2 2862.2 4.4119 0.053010 .
</p>
<p>fev1 1 1549.1 1549.1 2.3878 0.143120
</p>
<p>rv 1 561.9 561.9 0.8662 0.366757
</p>
<p>frc 1 194.6 194.6 0.2999 0.592007
</p>
<p>tlc 1 92.4 92.4 0.1424 0.711160
</p>
<p>Residuals 15 9731.2 648.7</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Model specification and output 189
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Note that, except for the very last line (&ldquo;tlc&rdquo;), there is practically no
correspondence between these F tests and the t tests from summary. In
particular, the effect of age is now significant. That is because these tests
are successive; they correspond to (reading upward from the bottom) a
stepwise removal of terms from the model until finally only age is left.
During the process, bmp came close to the magical 5% limit, but in view of
the number of tests, this is hardly noteworthy.
</p>
<p>The probability that one out of eight independent tests gives a p-value of
0.053 or below is actually just over 35%! The tests in the ANOVA table are
not completely independent, but the approximation should be good.
</p>
<p>The ANOVA table indicates that there is no significant improvement of
the model once age is included. It is possible to perform a joint test for
whether all the other variables can be removed by adding up the sums of
squares contributions and using the sum for an F test; that is,
</p>
<p>&gt; 955.4+155.0+632.3+2862.2+1549.1+561.9+194.6+92.4
</p>
<p>[1] 7002.9
</p>
<p>&gt; 7002.9/8
</p>
<p>[1] 875.3625
</p>
<p>&gt; 875.36/648.7
</p>
<p>[1] 1.349407
</p>
<p>&gt; 1-pf(1.349407,8,15)
</p>
<p>[1] 0.2935148
</p>
<p>This corresponds to collapsing the eight lines of the table so that it would
look like this:
</p>
<p>Df Sum Sq Mean Sq F Pr(&gt;F)
</p>
<p>age 1 10098.5 10098.5 15.566 0.00130
</p>
<p>others 8 7002.9 875.4 1.349 0.29351
</p>
<p>Residual 15 9731.2 648.7
</p>
<p>(Note that this is &ldquo;cheat output&rdquo;, in which we have manually inserted the
numbers computed above.)
</p>
<p>A procedure leading directly to the result is
</p>
<p>&gt; m1&lt;-lm(pemax~age+sex+height+weight+bmp+fev1+rv+frc+tlc)
</p>
<p>&gt; m2&lt;-lm(pemax~age)
</p>
<p>&gt; anova(m1,m2)
</p>
<p>Analysis of Variance Table
</p>
<p>Model 1: pemax ~ age + sex + height + weight + bmp + fev1 + rv +
</p>
<p>frc + tlc
</p>
<p>Model 2: pemax ~ age</p>
<p/>
</div>
<div class="page"><p/>
<p>190 11. Multiple regression
</p>
<p>Res.Df RSS Df Sum of Sq F Pr(&gt;F)
</p>
<p>1 15 9731.2
</p>
<p>2 23 16734.2 -8 -7002.9 1.3493 0.2936
</p>
<p>which gives the appropriate F test with no manual computation.
</p>
<p>Notice, however, that you need to be careful to ensure that the twomodels
are actually nested. R does not check this, although it does verify that
the number of response observations is the same to safeguard against the
more obvious mistakes. (When there are missing values in the descriptive
variables, it&rsquo;s easy for the smaller model to contain more data points.)
</p>
<p>From the ANOVA table, we can thus see that it is allowable to remove all
variables except age. However, that this particular variable is left in the
model is primarily due to the fact that it was mentioned first in the model
specification, as we see below.
</p>
<p>11.3 Model search
</p>
<p>R has the step() function for performing model searches by the Akaike
information criterion. Since that is well beyond the scope of this book, we
use simple manual variants of backwards elimination.
</p>
<p>In the following, we go through a practical model reduction for the exam-
ple data. Notice that the output has been slightly edited to take up less
space.
</p>
<p>&gt; summary(lm(pemax~age+sex+height+weight+bmp+fev1+rv+frc+tlc))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 176.0582 225.8912 0.779 0.448
</p>
<p>age -2.5420 4.8017 -0.529 0.604
</p>
<p>sex -3.7368 15.4598 -0.242 0.812
</p>
<p>height -0.4463 0.9034 -0.494 0.628
</p>
<p>weight 2.9928 2.0080 1.490 0.157
</p>
<p>bmp -1.7449 1.1552 -1.510 0.152
</p>
<p>fev1 1.0807 1.0809 1.000 0.333
</p>
<p>rv 0.1970 0.1962 1.004 0.331
</p>
<p>frc -0.3084 0.4924 -0.626 0.540
</p>
<p>tlc 0.1886 0.4997 0.377 0.711
</p>
<p>...
</p>
<p>One advantage of doing model reductions by hand is that you may im-
pose some logical structure on the process. In the present case, it may, for
instance, be natural to try to remove other lung function indicators first.
</p>
<p>&gt; summary(lm(pemax~age+sex+height+weight+bmp+fev1+rv+frc))</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Model search 191
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 221.8055 185.4350 1.196 0.2491
</p>
<p>age -3.1346 4.4144 -0.710 0.4879
</p>
<p>sex -4.6933 14.8363 -0.316 0.7558
</p>
<p>height -0.5428 0.8428 -0.644 0.5286
</p>
<p>weight 3.3157 1.7672 1.876 0.0790 .
</p>
<p>bmp -1.9403 1.0047 -1.931 0.0714 .
</p>
<p>fev1 1.0183 1.0392 0.980 0.3417
</p>
<p>rv 0.1857 0.1887 0.984 0.3396
</p>
<p>frc -0.2605 0.4628 -0.563 0.5813
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~age+sex+height+weight+bmp+fev1+rv))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 166.71822 154.31294 1.080 0.2951
</p>
<p>age -1.81783 3.66773 -0.496 0.6265
</p>
<p>sex 0.10239 11.89990 0.009 0.9932
</p>
<p>height -0.40981 0.79257 -0.517 0.6118
</p>
<p>weight 2.87386 1.55120 1.853 0.0814 .
</p>
<p>bmp -1.94971 0.98415 -1.981 0.0640 .
</p>
<p>fev1 1.41526 0.74788 1.892 0.0756 .
</p>
<p>rv 0.09567 0.09798 0.976 0.3425
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~age+sex+height+weight+bmp+fev1))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 260.6313 120.5215 2.163 0.0443 *
</p>
<p>age -2.9062 3.4898 -0.833 0.4159
</p>
<p>sex -1.2115 11.8083 -0.103 0.9194
</p>
<p>height -0.6067 0.7655 -0.793 0.4384
</p>
<p>weight 3.3463 1.4719 2.273 0.0355 *
</p>
<p>bmp -2.3042 0.9136 -2.522 0.0213 *
</p>
<p>fev1 1.0274 0.6329 1.623 0.1219
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~age+sex+height+weight+bmp))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 280.4482 124.9556 2.244 0.0369 *
</p>
<p>age -3.0750 3.6352 -0.846 0.4081
</p>
<p>sex -11.5281 10.3720 -1.111 0.2802
</p>
<p>height -0.6853 0.7962 -0.861 0.4001
</p>
<p>weight 3.5546 1.5281 2.326 0.0312 *
</p>
<p>bmp -1.9613 0.9263 -2.117 0.0476 *
</p>
<p>...
</p>
<p>As is seen, there was no obstacle to removing the four lung function
variables. Next we try to reduce among the variables that describe the
patient&rsquo;s state of physical development or size. Initially, we avoid remov-
ing weight and bmp since they appear to be close to the 5% significance
limit.</p>
<p/>
</div>
<div class="page"><p/>
<p>192 11. Multiple regression
</p>
<p>&gt; summary(lm(pemax~age+height+weight+bmp))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 274.5307 125.5745 2.186 0.0409 *
</p>
<p>age -3.0832 3.6566 -0.843 0.4091
</p>
<p>height -0.6985 0.8008 -0.872 0.3934
</p>
<p>weight 3.6338 1.5354 2.367 0.0282 *
</p>
<p>bmp -1.9621 0.9317 -2.106 0.0480 *
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~height+weight+bmp))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 245.3936 119.8927 2.047 0.0534 .
</p>
<p>height -0.8264 0.7808 -1.058 0.3019
</p>
<p>weight 2.7717 1.1377 2.436 0.0238 *
</p>
<p>bmp -1.4876 0.7375 -2.017 0.0566 .
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~weight+bmp))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 124.8297 37.4786 3.331 0.003033 **
</p>
<p>weight 1.6403 0.3900 4.206 0.000365 ***
</p>
<p>bmp -1.0054 0.5814 -1.729 0.097797 .
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~weight))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 63.5456 12.7016 5.003 4.63e-05 ***
</p>
<p>weight 1.1867 0.3009 3.944 0.000646 ***
</p>
<p>...
</p>
<p>Notice that, once age and heightwere removed, bmpwas no longer sig-
nificant. In the original reference (Altman, 1991), weight, fev1, and bmp
all ended up with p-values below 5%. However, far from all elimination
procedures lead to that result.
</p>
<p>It is also a good idea to pay close attention to the age, weight, and
height variables, which are heavily correlated since we are dealing with
children and adolescents.
</p>
<p>&gt; summary(lm(pemax~age+weight+height))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 64.65555 82.40935 0.785 0.441
</p>
<p>age 1.56755 3.14363 0.499 0.623
</p>
<p>weight 0.86949 0.85922 1.012 0.323
</p>
<p>height -0.07608 0.80278 -0.095 0.925
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~age+height))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 17.8600 68.2493 0.262 0.796</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Exercises 193
</p>
<p>age 2.7178 2.9325 0.927 0.364
</p>
<p>height 0.3397 0.6900 0.492 0.627
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~age))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 50.408 16.657 3.026 0.00601 **
</p>
<p>age 4.055 1.088 3.726 0.00111 **
</p>
<p>...
</p>
<p>&gt; summary(lm(pemax~height))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) -33.2757 40.0445 -0.831 0.41453
</p>
<p>height 0.9319 0.2596 3.590 0.00155 **
</p>
<p>...
</p>
<p>As it turns out, there is really no reason to prefer one of the three variables
over the two others. The fact that an elimination method ends up with a
model containing only weight is essentially a coincidence. You can easily
be misled by model search procedures that end up with one highly sig-
nificant variable &mdash; it is far from certain that the same variable would be
chosen if you were to repeat the analysis on a new, similar data set.
</p>
<p>What you may reasonably conclude is that there is probably a connection
with the patient&rsquo;s physical development or size, which may be described
in terms of age, height, or weight. Which description to use is arbitrary. If
you want to choose one over the others, a decision cannot be based on the
data, although possibly on theoretical considerations and/or results from
previous investigations.
</p>
<p>11.4 Exercises
</p>
<p>11.1 The secher data are best analyzed after log-transforming birth
weight as well as the abdominal and biparietal diameters. Fit a prediction
equation for birth weight. Howmuch is gained by using both diameters in
a prediction equation? The sum of the two regression coefficients is almost
exactly 3 &mdash; can this be given a nice interpretation?
</p>
<p>11.2 The tlc data set contains a variable also called tlc. This is not in
general a good idea; explain why. Describe tlc using the other variables
in the data set and discuss the validity of the model.
</p>
<p>11.3 The analyses of cystfibr involve sex, which is a binary variable.
How would you interpret the results for this variable?
</p>
<p>11.4 Consider the juul2 data set and select the group of those over 25
years old. Perform a regression analysis of
</p>
<p>&radic;
igf1 on age, and extend</p>
<p/>
</div>
<div class="page"><p/>
<p>194 11. Multiple regression
</p>
<p>the model by including height and weight. Generate the analysis of
variance table for the extendedmodel. What is the surprise, and why does
it happen?
</p>
<p>11.5 Analyze and interpret the effect of explanatory variables on themilk
intake in the kfm data set using a multiple regression model. Notice that
sex is a factor here; what does that imply for the analyses?</p>
<p/>
</div>
<div class="page"><p/>
<p>12
Linear models
</p>
<p>Many data sets are inherently too complex to be handled adequately by
standard procedures and thus require the formulation of ad hoc models.
The class of linear models provides a flexible framework into which many
&mdash; although not all &mdash; of these cases can be fitted.
</p>
<p>Youmay have noticed that the lm function is applied to data classified into
groups (Chapter 7) as well as to (multiple) linear regression (Chapters 6
and 11) problems, even though the theory for these procedures appears
to be quite different. However, they are, in fact, special cases of the same
general model.
</p>
<p>The basic point is that a multiple regression model can describe a wide va-
riety of situations if you choose the explanatory variables suitably. There
is no requirement that the explanatory variables should follow a normal
distribution, or any continuous distribution for that matter. One simple
example (which we use without comment in Chapter 11) is that a group-
ing into two categories can be coded as a 0/1 variable and used in a
regression analysis. The regression coefficient in that case corresponds to
a difference between two groups rather than the slope of an actual line. To
encode a grouping with more than two categories, you can use multiple
0/1 variables.
</p>
<p>Generating these dummy variables becomes tedious, but it can be auto-
mated by the use of model formulas. Among other things, such formulas
provide a convenient abstraction by treating classification variables (fac-
tors) and continuous variables symmetrically. You will need to learn
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_12, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>196 12. Linear models
</p>
<p>exactly what model formulas do in order to become able to express your
own modelling ideas.
</p>
<p>This chapter contains a collection of models and their handling by lm,
mainly in the form of relatively minor extensions and modifications of
methods described earlier. It is meant only to give you a feel for the scope
of possibilities and does not pretend to be complete.
</p>
<p>12.1 Polynomial regression
</p>
<p>One basic observation showing that multiple regression analysis can do
more than meets the eye is that you can include second-order and higher
powers of a variable in the model along with the original linear term. That
is, you can have a model like
</p>
<p>y = α + β1x + β2x
2 + &middot; &middot; &middot;+ βkxk + ǫ
</p>
<p>This obviously describes a nonlinear relation between y and x, but that
does not matter; the model is still a linear model. What does matter is that
the relation between the parameters and the expected observations is lin-
ear. It also does not matter that there is a deterministic relation between
the regression variables x, x2, x3, . . . , as long as there is no linear relation
between them. However, fitting high-degree polynomials can be diffi-
cult because near-collinearity between terms makes the fit numerically
unstable.
</p>
<p>We return to the cystic fibrosis data set for an example. The plot of pemax
and height in Figure 11.1 may suggest that the relation is not quite linear.
One way to test this is to try to add a term that is the square of the height.
</p>
<p>&gt; attach(cystfibr)
</p>
<p>&gt; summary(lm(pemax~height+I(height^2)))
</p>
<p>...
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 615.36248 240.95580 2.554 0.0181 *
</p>
<p>height -8.08324 3.32052 -2.434 0.0235 *
</p>
<p>I(height^2) 0.03064 0.01126 2.721 0.0125 *
</p>
<p>...
</p>
<p>Notice that the computed height2 in the model formula needs to be &ldquo;pro-
tected&rdquo; by I(...). This technique is often used to prevent any special
interpretation of operators in a model formula. Such an interpretation will
not take place inside a function call, and I is the identity function that
returns its argument unaltered.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 Polynomial regression 197
</p>
<p>We find a significant deviation from linearity. However, considering the
process that led to doing this particular analysis, the p-values have to
be taken with more than a grain of salt. This is getting dangerously
close to &ldquo;data dredging&rdquo;, fishing expeditions in data. Consider it more
an illustration of a technique than an exemplary data analysis.
</p>
<p>To draw a plot of the fitted curve with prediction and confidence bands,
we can use predict. To avoid problems caused by data not being sorted
by height, we use newdata, which allows the prediction of values for a
chosen set of predictors. Here we choose a set of heights between 110 and
180 cm in steps of 2 cm:
</p>
<p>&gt; pred.frame &lt;- data.frame(height=seq(110,180,2))
</p>
<p>&gt; lm.pemax.hq &lt;- lm(pemax~height+I(height^2))
</p>
<p>&gt; predict(lm.pemax.hq,interval="pred",newdata=pred.frame)
</p>
<p>fit lwr upr
</p>
<p>1 96.90026 37.94461 155.8559
</p>
<p>2 94.33611 36.82985 151.8424
</p>
<p>3 92.01705 35.73077 148.3033
</p>
<p>...
</p>
<p>34 141.68922 88.70229 194.6761
</p>
<p>35 147.21294 93.51117 200.9147
</p>
<p>36 152.98174 98.36718 207.5963
</p>
<p>Based on these predicted data, Figure 12.1 is obtained as follows:
</p>
<p>&gt; pp &lt;- predict(lm.pemax.hq,newdata=pred.frame,interval="pred")
</p>
<p>&gt; pc &lt;- predict(lm.pemax.hq,newdata=pred.frame,interval="conf")
</p>
<p>&gt; plot(height,pemax,ylim=c(0,200))
</p>
<p>&gt; matlines(pred.frame$height,pp,lty=c(1,2,2),col="black")
</p>
<p>&gt; matlines(pred.frame$height,pc,lty=c(1,3,3),col="black")
</p>
<p>It is seen that the fitted curve is slightly decreasing for small heights. This
is probably an artifact caused by the choice of a second-order polynomial
to fit data. More likely, the reality is that pemax is relatively constant up
to about 150 cm, after which it increases quickly with height. Note also
that there seems to be a discrepancy between the prediction limits and the
actual distribution of data for the smaller heights. The standard deviation
might be larger for larger heights, but it is not impossible to obtain a sim-
ilar distribution of points by coincidence, and there is also an issue with
potential overfitting to the observed data. It is really not advisable to con-
struct prediction intervals based on data as limited as these unless you are
sure that the model is correct.</p>
<p/>
</div>
<div class="page"><p/>
<p>198 12. Linear models
</p>
<p>110 120 130 140 150 160 170 180
</p>
<p>0
5
0
</p>
<p>1
0
0
</p>
<p>1
5
0
</p>
<p>2
0
0
</p>
<p>height
</p>
<p>p
e
m
</p>
<p>a
x
</p>
<p>Figure 12.1. Quadratic regression with confidence and prediction limits.
</p>
<p>12.2 Regression through the origin
</p>
<p>It sometimes makes sense to assume that a regression line passes through
(0, 0) &mdash; that the intercept of the regression line is zero. This can be spec-
ified in the model formula by adding the term -1 (&ldquo;minus intercept&rdquo;) to
the right-hand side: y ~ x - 1.
</p>
<p>The logic of the notation can be seen by writing the linear regression
model as y = α&times; 1 + β&times; x + ǫ. The intercept corresponds to having an
extra descriptive variable, which is the constant 1. Removing this variable
yields regression through the origin.
</p>
<p>This is a simulated example of a linear relationship through the origin
(y = 2x + ǫ):
</p>
<p>&gt; x &lt;- runif(20)
</p>
<p>&gt; y &lt;- 2*x+rnorm(20,0,0.3)
</p>
<p>&gt; summary(lm(y~x))
</p>
<p>Call:
</p>
<p>lm(formula = y ~ x)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Regression through the origin 199
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-0.50769 -0.08766 0.03802 0.14512 0.26358
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) -0.14896 0.08812 -1.69 0.108
</p>
<p>x 2.39772 0.15420 15.55 7.05e-12 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 0.2115 on 18 degrees of freedom
</p>
<p>Multiple R-squared: 0.9307, Adjusted R-squared: 0.9269
</p>
<p>F-statistic: 241.8 on 1 and 18 DF, p-value: 7.047e-12
</p>
<p>&gt; summary(lm(y~x-1))
</p>
<p>Call:
</p>
<p>lm(formula = y ~ x - 1)
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-0.62178 -0.16855 -0.04019 0.12044 0.27346
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>x 2.17778 0.08669 25.12 4.87e-16 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 0.2216 on 19 degrees of freedom
</p>
<p>Multiple R-squared: 0.9708, Adjusted R-squared: 0.9692
</p>
<p>F-statistic: 631.1 on 1 and 19 DF, p-value: 4.873e-16
</p>
<p>In the first analysis, the intercept is not significant, which is, of course,
not surprising. In the second analysis we force the intercept to be zero,
resulting in a slope estimate with a substantially improved accuracy.
</p>
<p>Comparison of the R2-values in the two analyses shows something that
occasionally causes confusion: R2 is much larger in the model with no in-
tercept! This does not, however, mean that the relation is &ldquo;more linear&rdquo;
when the intercept is not included or that more of the variation is ex-
plained. What is happening is that the definition of R2 itself is changing.
It is most easily seen from the ANOVA tables in the two cases:
</p>
<p>&gt; anova(lm(y~x))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: y
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>x 1 10.8134 10.8134 241.80 7.047e-12 ***
</p>
<p>Residuals 18 0.8050 0.0447</p>
<p/>
</div>
<div class="page"><p/>
<p>200 12. Linear models
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>&gt; anova(lm(y~x-1))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: y
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>x 1 30.9804 30.9804 631.06 4.873e-16 ***
</p>
<p>Residuals 19 0.9328 0.0491
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Notice that the total sum of squares and the total number of degrees of
freedom is not the same in the two analyses. In the model with an inter-
cept, there are 19 DF in all and the total sum of squares is &sum;(yi &minus; ȳ)2, while
the model without an intercept has a total of 20 DF and the total sum of
squares is defined as &sum; y2i . Unless ȳ is close to zero, the latter &ldquo;total SS&rdquo;
will be much larger than the former, so if the residual variance is similar,
R2 will be much closer to 1.
</p>
<p>The reason for defining the total sum of squares like this for models with-
out intercepts is that it has to correspond to the residual sum of squares
in a minimal model. The minimal model has to be a submodel of the re-
gression model; otherwise the ANOVA table simply does not make sense.
In an ordinary regression analysis, the minimal model is y = α + ǫ, but
when the regression model does not include α, the only sensible minimal
model is y = 0+ ǫ.
</p>
<p>12.3 Design matrices and dummy variables
</p>
<p>The function model.matrix gives the design matrix for a given model. It
can look like this:
</p>
<p>&gt; model.matrix(pemax~height+weight)
</p>
<p>(Intercept) height weight
</p>
<p>1 1 109 13.1
</p>
<p>2 1 112 12.9
</p>
<p>3 1 124 14.1
</p>
<p>4 1 125 16.2
</p>
<p>...
</p>
<p>24 1 175 51.1
</p>
<p>25 1 179 71.5
</p>
<p>attr(,"assign")
</p>
<p>[1] 0 1 2
</p>
<p>(The cystfibr data set was attached previously.)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Design matrices and dummy variables 201
</p>
<p>You should not worry about the "assign" attribute at this stage, but the
three columns are important. If you add them together, weighted by the
corresponding regression coefficients, you get exactly the fitted values.
Notice that the intercept enters as the coefficient to a column of ones.
</p>
<p>If the same is attempted for a model containing a factor, the following
happens. (We return to the anesthetic ventilation example from p. 129.)
</p>
<p>&gt; attach(red.cell.folate)
</p>
<p>&gt; model.matrix(folate~ventilation)
</p>
<p>(Intercept) ventilationN2O+O2,op ventilationO2,24h
</p>
<p>1 1 0 0
</p>
<p>2 1 0 0
</p>
<p>...
</p>
<p>16 1 1 0
</p>
<p>17 1 1 0
</p>
<p>18 1 0 1
</p>
<p>19 1 0 1
</p>
<p>20 1 0 1
</p>
<p>21 1 0 1
</p>
<p>22 1 0 1
</p>
<p>attr(,"assign")
</p>
<p>[1] 0 1 1
</p>
<p>attr(,"contrasts")
</p>
<p>attr(,"contrasts")$ventilation
</p>
<p>[1] "contr.treatment"
</p>
<p>The two columns of zeros and ones are sometimes called dummy variables.
They are interpreted exactly as above: Multiplying them by the respective
regression coefficients and adding the results yields the fitted value. No-
tice that, for example, the second column is 1 for observations in group 2
and 0 otherwise; that is, the corresponding regression coefficient describes
something that is added to the intercept for observations in that particular
group. Both columns have zeros for observations from the first group, the
mean value of which is described by the intercept (β0) alone. The regres-
sion coefficient β1 thus describes the difference in means between groups 1
and 2, and β2 between groups 1 and 3.
</p>
<p>You may be confused by the use of the term &ldquo;regression coefficients&rdquo; even
though no regression lines are present in models like that above. The point
is that you formally rewrite a model for groups as a multiple regression
model, so that you can use the same software. As is seen, there is a unique
correspondence between the formal regression coefficients and the group
means.
</p>
<p>You can define dummy variables in several different ways to describe a
grouping. This particular scheme is called treatment contrasts because if
the first group is &ldquo;no treatment&rdquo; then the coefficients immediately give
the treatment effects for each of the other groups. We do not discuss other</p>
<p/>
</div>
<div class="page"><p/>
<p>202 12. Linear models
</p>
<p>choices here; see Venables and Ripley (2002) for a much deeper discussion.
Note only that contrast type can be set on a per-term basis and that this is
what is reflected in the "contrasts" attribute of the design matrix.
</p>
<p>For completeness, the "assign" attribute indicates which columns be-
long together. When, for instance, you request an analysis of variance
using anova, the sum of squares for ventilationwill have 2 degrees of
freedom, corresponding to the removal of both columns simultaneously.
</p>
<p>Removing the intercept from a model containing a factor term will not
correspond to a model in which a particular group has mean zero since
such models are usually nonsensical. Instead, R generates a simpler set of
dummy variables, which are indicator variables of the levels of the factor.
This corresponds to the same model as when the intercept is included (the
fitted values are identical), but the regression coefficients have a different
interpretation.
</p>
<p>12.4 Linearity over groups
</p>
<p>Sometimes data are grouped according to a division of a continuous scale
(e.g., by age group), or an experiment was designed to take several mea-
surements at each of a fixed set of x-values. In both cases it is relevant
to compare the results of a linear regression with those of an analysis of
variance.
</p>
<p>In the case of grouped x-values, you might take a central value as rep-
resentative for everyone in a given group, for instance formally letting
everyone in a &ldquo;20&ndash;29-year&rdquo; category be 25 years old. If individual x-
values are available, they may of course be used in a linear regression,
but it makes the analysis a little more complicated, so we discuss only the
situation where that is not the case.
</p>
<p>We thus have two alternative models for the same data. Both belong to the
class of linear models that lm is capable of handling. The linear regression
model is a submodel of the model for one-way analysis of variance because
the former can be obtained by placing restrictions on the parameters of
the latter (namely that the true group means lie on a straight line).
</p>
<p>It is possible to test whether or not a model reduction is allowable by com-
paring the reduction in the amount of variation explained to the residual
variation in the larger model, resulting in an F test.
</p>
<p>In the following example on trypsin concentrations in age groups (Alt-
man, 1991, p. 212), data are given as the mean and SD within each of six
groups. This is a kind of data that R is not quite prepared to handle, and it</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Linearity over groups 203
</p>
<p>has therefore been necessary to create &ldquo;fake&rdquo; data giving the same means
and SDs. These can be obtained via
</p>
<p>&gt; attach(fake.trypsin)
</p>
<p>The actual results of the analysis of variance depend only on the means
and SDs and are therefore independent of the faking. Readers inter-
ested in how to perform the actual faking should take a look at the file
fake.trypsin.R in the rawdata directory of the ISwR package.
</p>
<p>The fake.trypsin data frame contains three variables, as seen by
</p>
<p>&gt; summary(fake.trypsin)
</p>
<p>trypsin grp grpf
</p>
<p>Min. :-39.96 Min. :1.000 1: 32
</p>
<p>1st Qu.:119.52 1st Qu.:2.000 2:137
</p>
<p>Median :167.59 Median :2.000 3: 38
</p>
<p>Mean :168.68 Mean :2.583 4: 44
</p>
<p>3rd Qu.:213.98 3rd Qu.:3.000 5: 16
</p>
<p>Max. :390.13 Max. :6.000 6: 4
</p>
<p>Notice that there are both grp, which is a numeric vector, and grpf,
which is a factor with six levels.
</p>
<p>Performing a one-way analysis of variance on the fake data gives the
following ANOVA table:
</p>
<p>&gt; anova(lm(trypsin~grpf))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: trypsin
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>grpf 5 224103 44821 13.508 9.592e-12 ***
</p>
<p>Residuals 265 879272 3318
</p>
<p>If you had used grp instead of grpf in the model formula, you would
have obtained a linear regression on the group number instead. In some
circumstances, that would have been a serious error, but here it actually
makes sense. The midpoints of the age intervals are equidistant, so the
model is equivalent to assuming a linear development with age (the in-
terpretation of the regression coefficient requires some care, though). The
ANOVA table looks as follows:
</p>
<p>&gt; anova(lm(trypsin~grp))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: trypsin
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>grp 1 206698 206698 62.009 8.451e-14 ***
</p>
<p>Residuals 269 896677 3333</p>
<p/>
</div>
<div class="page"><p/>
<p>204 12. Linear models
</p>
<p>Notice that the residual mean squares did not change very much, indicat-
ing that the two models describe the data nearly equally well. If you want
to have a formal test of the simple linear model against the model where
there is a separate mean for each group, it can be done easily as follows:
</p>
<p>&gt; model1 &lt;- lm(trypsin~grp)
</p>
<p>&gt; model2 &lt;- lm(trypsin~grpf)
</p>
<p>&gt; anova(model1,model2)
</p>
<p>Analysis of Variance Table
</p>
<p>Model 1: trypsin ~ grp
</p>
<p>Model 2: trypsin ~ grpf
</p>
<p>Res.Df RSS Df Sum of Sq F Pr(&gt;F)
</p>
<p>1 269 896677
</p>
<p>2 265 879272 4 17405 1.3114 0.2661
</p>
<p>So we see that the model reduction has a nonsignificant p-value and hence
that model2 does not fit data significantly better than model1.
</p>
<p>This technique works only when one model is a submodel of the other,
which is the case here since the linear model is defined by a restriction on
the group means.
</p>
<p>Another way to achieve the same result is to add the two models together
formally as follows:
</p>
<p>&gt; anova(lm(trypsin~grp+grpf))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: trypsin
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>grp 1 206698 206698 62.2959 7.833e-14 ***
</p>
<p>grpf 4 17405 4351 1.3114 0.2661
</p>
<p>Residuals 265 879272 3318
</p>
<p>This model is exactly the same as when only grpf was included. How-
ever, the ANOVA table now contains a subdivision of the model sum of
squares in which the grpf line describes the change incurred by expand-
ing the model from one to five parameters. The ANOVA table in Altman
(1991, p. 213) is different, erroneously.
</p>
<p>The plot in Figure 12.2 is made like this:
</p>
<p>&gt; xbar.trypsin &lt;- tapply(trypsin,grpf,mean)
</p>
<p>&gt; stripchart(trypsin~grp, method="jitter",
</p>
<p>+ jitter=.1, vertical=T, pch=20)
</p>
<p>&gt; lines(1:6,xbar.trypsin,type="b",pch=4,cex=2,lty=2)
</p>
<p>&gt; abline(lm(trypsin~grp))
</p>
<p>The graphical techniques used here are essentially identical to those used
for Figure 7.1, so we do not go into further details.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Linearity over groups 205
</p>
<p>1 2 3 4 5 6
</p>
<p>0
1
0
0
</p>
<p>2
0
0
</p>
<p>3
0
0
</p>
<p>4
0
0
</p>
<p>Figure 12.2. &ldquo;Fake&rdquo; data for the trypsin example with fitted line and empirical
means.
</p>
<p>Notice that the fakeness of the data is exposed by a point showing a neg-
ative trypsin concentration! The original data are unavailable but would
likely show a distribution skewed slightly upwards.
</p>
<p>Actually, it is possible to analyze the data in R without generating fake
data. A weighted regression analysis of the group means, with weights
equal to the number of observations in each group, will yield the first two
lines of the ANOVA table, and the last one can be computed from the SDs.
The details are as follows:
</p>
<p>&gt; n &lt;- c(32,137, 38,44,16,4)
</p>
<p>&gt; tryp.mean &lt;- c(128,152,194,207,215,218)
</p>
<p>&gt; tryp.sd &lt;-c(50.9,58.5,49.3,66.3,60,14)
</p>
<p>&gt; gr&lt;-1:6
</p>
<p>&gt; anova(lm(tryp.mean~gr+factor(gr),weights=n))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: tryp.mean
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>gr 1 206698 206698
</p>
<p>factor(gr) 4 17405 4351
</p>
<p>Residuals 0 0</p>
<p/>
</div>
<div class="page"><p/>
<p>206 12. Linear models
</p>
<p>Notice that the &ldquo;Residuals&rdquo; line is zero and that the F tests are not cal-
culated. Omitting the factor(gr) term will cause that line to go into
Residuals and be treated as an estimate of the error variation, but that
is not what you want since it does not include the information about
the variation within groups. Instead, you need to fill in the missing
information computed from the group standard deviations and sizes.
The following gives the residual sum of squares and the corresponding
degrees of freedom and mean squares:
</p>
<p>&gt; sum(tryp.sd^2*(n-1))
</p>
<p>[1] 879271.9
</p>
<p>&gt; sum(n-1)
</p>
<p>[1] 265
</p>
<p>&gt; sum(tryp.sd^2*(n-1))/sum(n-1)
</p>
<p>[1] 3318.007
</p>
<p>There is no simple way of updating the ANOVA table with an external
variance estimate, but it is easy enough to do the computations directly:
</p>
<p>&gt; 206698/3318.007 # F statistic for gr
</p>
<p>[1] 62.29583
</p>
<p>&gt; 1-pf(206698/3318.007,1,265) # p-value
</p>
<p>[1] 7.838175e-14
</p>
<p>&gt; 4351/3318.007 # F statistic for factor(gr)
</p>
<p>[1] 1.311329
</p>
<p>&gt; 1-pf(4351/3318.007,4,265) # p-value
</p>
<p>[1] 0.2660733
</p>
<p>12.5 Interactions
</p>
<p>A basic assumption in a multiple regression model is that terms act ad-
ditively on the response. However, this does not mean that linear models
cannot describe nonadditivity. You can add special interaction terms that
specify that the effect of one term is modified according to the level of
another. In the model formulas in R, such terms are generated using
the colon operator; for example, a:b. Usually, you will also include the
terms a and b, and R allows the notation a*b for a+b+a:b. Higher-order
interactions among three or more variables are also possible.
</p>
<p>The exact definition of the interaction terms and the interpretation of their
associated regression coefficients can be elusive. Some peculiar things
happen if an interaction term is present but one or more of the main
effects are missing. The full details are probably best revealed through
experimentation. However, depending on the nature of the terms a and b
as factors or numeric variables, the overall effect of including interaction
terms can be described as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Two-way ANOVA with replication 207
</p>
<p>&bull; Interaction between two factors. This is conceptually the simplest case.
The model with interaction corresponds to having different levels
for all possible combinations of levels of the two factors.
</p>
<p>&bull; Interaction between a factor and a numeric variable. In this case, the
model with interaction contains linear effects of the continuous vari-
able but with different slopes within each group defined by the
factor.
</p>
<p>&bull; Interaction between two continuous variables. This gives a slightly pecu-
liar model containing a new regression variable that is the product
of the two. The interpretation is that you have a linear effect of vary-
ing one variable while keeping the other constant, but with a slope
that changes as you vary the other variable.
</p>
<p>12.6 Two-way ANOVA with replication
</p>
<p>The coking data set comes from Johnson (1994, Section 13.1). The time
required to make coke from coal is analyzed in a 2&times; 3 experiment varying
the oven temperature and the oven width. There were three replications
at each combination.
</p>
<p>&gt; attach(coking)
</p>
<p>&gt; anova(lm(time~width*temp))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: time
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>width 2 123.143 61.572 222.102 3.312e-10 ***
</p>
<p>temp 1 17.209 17.209 62.076 4.394e-06 ***
</p>
<p>width:temp 2 5.701 2.851 10.283 0.002504 **
</p>
<p>Residuals 12 3.327 0.277
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>We see that the interaction term is significant. If we take a look at the cell
means, we can get an idea of why this happens:
</p>
<p>&gt; tapply(time,list(width,temp),mean)
</p>
<p>1600 1900
</p>
<p>4 3.066667 2.300000
</p>
<p>8 7.166667 5.533333
</p>
<p>12 10.800000 7.333333
</p>
<p>The difference between high and low temperatures increases with oven
width, making an additive model inadequate. When this is the case, the
individual tests for the two factors make no sense. If the interaction had</p>
<p/>
</div>
<div class="page"><p/>
<p>208 12. Linear models
</p>
<p>not been significant, then we would have been able to perform separate F
tests for the two factors.
</p>
<p>12.7 Analysis of covariance
</p>
<p>As the example in this section, we use a data set concerning growth condi-
tions of Tetrahymena cells, collected by Per Hellung-Larsen. Data are from
two groups of cell cultures where glucose was either added or not added
to the growth medium. For each culture, the average cell diameter (&micro;) and
cell concentration (count per ml) were recorded. The cell concentration
was set at the beginning of the experiment, and there is no systematic dif-
ference in cell concentration between the two glucose groups. However, it
is expected that the cell diameter is affected by the presence of glucose in
the medium.
</p>
<p>Data are in the data frame hellung, which can be loaded and viewed like
this:
</p>
<p>&gt; hellung
</p>
<p>glucose conc diameter
</p>
<p>1 1 631000 21.2
</p>
<p>2 1 592000 21.5
</p>
<p>3 1 563000 21.3
</p>
<p>4 1 475000 21.0
</p>
<p>...
</p>
<p>49 2 14000 24.4
</p>
<p>50 2 13000 24.3
</p>
<p>51 2 11000 24.2
</p>
<p>The coding of glucose is such that 1 and 2 mean yes and no, respectively.
There are no missing values.
</p>
<p>Summarizing the data frame yields
</p>
<p>&gt; summary(hellung)
</p>
<p>glucose conc diameter
</p>
<p>Min. :1.000 Min. : 11000 Min. :19.20
</p>
<p>1st Qu.:1.000 1st Qu.: 27500 1st Qu.:21.40
</p>
<p>Median :1.000 Median : 69000 Median :23.30
</p>
<p>Mean :1.373 Mean :164325 Mean :23.00
</p>
<p>3rd Qu.:2.000 3rd Qu.:243000 3rd Qu.:24.35
</p>
<p>Max. :2.000 Max. :631000 Max. :26.30
</p>
<p>Notice that the distribution of the concentrations is strongly right-skewed
with a mean more than twice as big as the median. Note also that glucose
is regarded as a numeric vector by summary, even though it has only two
different values.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Analysis of covariance 209
</p>
<p>It will be more convenient to have glucose as a factor, so it is recoded as
shown below. Recall that to change a variable inside a data frame, you use
$-notation (p. 21) to specify the component you want to change:
</p>
<p>&gt; hellung$glucose &lt;- factor(hellung$glucose, labels=c("Yes","No"))
</p>
<p>&gt; summary(hellung)
</p>
<p>glucose conc diameter
</p>
<p>Yes:32 Min. : 11000 Min. :19.20
</p>
<p>No :19 1st Qu.: 27500 1st Qu.:21.40
</p>
<p>Median : 69000 Median :23.30
</p>
<p>Mean :164325 Mean :23.00
</p>
<p>3rd Qu.:243000 3rd Qu.:24.35
</p>
<p>Max. :631000 Max. :26.30
</p>
<p>It is convenient to be able to refer to the variables of hellungwithout the
hellung$ prefix, so we put hellung in the search path.
</p>
<p>&gt; attach(hellung)
</p>
<p>12.7.1 Graphical description
</p>
<p>First, we plot the raw data (Figure 12.3):
</p>
<p>&gt; plot(conc,diameter,pch=as.numeric(glucose))
</p>
<p>By calculating as.numeric(glucose), we convert the factor glucose
to the underlying codes, 1 and 2. The specification of pch thus implies that
group 1 (&ldquo;Yes&rdquo;) is drawn using plotting character 1 (circles) and group 2
with plotting character 2 (triangles).
</p>
<p>To get different plotting symbols, you must first create a vector containing
the symbol numbers and give that as the pch argument. The following
form yields open and filled circles: c(1,16)[glucose]. It looks a bit
cryptic at first, but it is really just a consequence of R&rsquo;s way of indexing.
For indexing purposes, a factor like glucose behaves as a vector of 1s
and 2s, so you get the first element of c(1,16), namely 1, whenever an
observation is from group 1; when the observation is from group 2, you
similarly get 16.
</p>
<p>The explanatory text is inserted with legend like this:
</p>
<p>&gt; legend(locator(n=1),legend=c("glucose","no glucose"),pch=1:2)
</p>
<p>Notice that both the function and one of its arguments are named legend.
</p>
<p>The function locator returns the coordinates of a point on a plot. It
works so that the function awaits a click with a mouse button and then re-
turns the cursor position. You may want to call locator() directly from</p>
<p/>
</div>
<div class="page"><p/>
<p>210 12. Linear models
</p>
<p>0e+00 1e+05 2e+05 3e+05 4e+05 5e+05 6e+05
</p>
<p>1
9
</p>
<p>2
0
</p>
<p>2
1
</p>
<p>2
2
</p>
<p>2
3
</p>
<p>2
4
</p>
<p>2
5
</p>
<p>2
6
</p>
<p>conc
</p>
<p>d
ia
</p>
<p>m
e
te
</p>
<p>r
</p>
<p>glucose
</p>
<p>no glucose
</p>
<p>Figure 12.3. Plot of diameter versus concentration for Tetrahymena data.
</p>
<p>the command line to see the effect. Notice that if you do not specify a value
for n, then you need to right-click when you are done selecting points.
</p>
<p>The plot shows a clear inverse and nonlinear relation between concentra-
tion and cell diameter. Further, it is seen that the cultures without glucose
are systematically below cultures with added glucose.
</p>
<p>You get a much nicer plot (Figure 12.4) by using a logarithmic x-axis:
</p>
<p>&gt; plot(conc,diameter,pch=as.numeric(glucose),log="x")
</p>
<p>Now the relation suddenly looks linear!
</p>
<p>You could also try a log-log plot (shown in Figure 12.5 with regression
lines as described below):
</p>
<p>&gt; plot(conc,diameter,pch=as.numeric(glucose),log="xy")
</p>
<p>As is seen, this really does not change much, but it was nevertheless
decided to analyze data with both diameter and concentration log-
transformed because a power-law relation was expected (y = αxβ, which
gives a straight line on a log-log plot).</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Analysis of covariance 211
</p>
<p>1e+04 2e+04 5e+04 1e+05 2e+05 5e+05
</p>
<p>1
9
</p>
<p>2
0
</p>
<p>2
1
</p>
<p>2
2
</p>
<p>2
3
</p>
<p>2
4
</p>
<p>2
5
</p>
<p>2
6
</p>
<p>conc
</p>
<p>d
ia
</p>
<p>m
e
te
</p>
<p>r
</p>
<p>Figure 12.4. Tetrahymena data with logarithmic x-axis.
</p>
<p>When adding regression lines to a log plot or log-log plot, you should
notice that abline interprets them as lines in the coordinate system ob-
tained after taking (base-10) logarithms. Thus, you can add a line for
each group with abline applied to the results of a regression analysis
of log10(diameter) on log10(conc). First, however, it is convenient
to define data frames corresponding to the two glucose groups:
</p>
<p>&gt; tethym.gluc &lt;- hellung[glucose=="Yes",]
</p>
<p>&gt; tethym.nogluc &lt;- hellung[glucose=="No",]
</p>
<p>Notice that you have to use the names, not the numbers, of the factor
levels.
</p>
<p>Since we only need the two data frames for adding lines to the figure,
it would be cumbersome to add them in turn to the search path with
attach, do the plotting, and then use detach to remove them. It is eas-
ier to use the data argument to lm; this allows you to explicitly specify
the data frame in which to look for variables. The two regression lines are
drawn with
</p>
<p>&gt; lm.nogluc &lt;- lm(log10(diameter)~ log10(conc),data=tethym.nogluc)
</p>
<p>&gt; lm.gluc &lt;- lm(log10(diameter)~ log10(conc),data=tethym.gluc)</p>
<p/>
</div>
<div class="page"><p/>
<p>212 12. Linear models
</p>
<p>1e+04 2e+04 5e+04 1e+05 2e+05 5e+05
</p>
<p>1
9
</p>
<p>2
0
</p>
<p>2
1
</p>
<p>2
2
</p>
<p>2
3
</p>
<p>2
4
</p>
<p>2
5
</p>
<p>2
6
</p>
<p>conc
</p>
<p>d
ia
</p>
<p>m
e
te
</p>
<p>r
</p>
<p>Figure 12.5. Tetrahymena data, log-log plot with regression lines.
</p>
<p>&gt; abline(lm.nogluc)
</p>
<p>&gt; abline(lm.gluc)
</p>
<p>after which the plot looks like Figure 12.5. It is seen that the lines fit the
data quite well and that they are almost, but not perfectly, parallel. The
question is whether the difference in slope is statistically significant. This
is the topic of the next section.
</p>
<p>12.7.2 Comparison of regression lines
</p>
<p>Corresponding to the two lines from before, we have the following
regression analyses:
</p>
<p>&gt; summary(lm(log10(diameter)~ log10(conc), data=tethym.gluc))
</p>
<p>Call:
</p>
<p>lm(formula = log10(diameter) ~ log10(conc), data = tethym.gluc)
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-0.0267219 -0.0043361 0.0006891 0.0035489 0.0176077</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Analysis of covariance 213
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 1.63134 0.01345 121.29 &lt;2e-16 ***
</p>
<p>log10(conc) -0.05320 0.00272 -19.56 &lt;2e-16 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 0.008779 on 30 degrees of freedom
</p>
<p>Multiple R-squared: 0.9273, Adjusted R-squared: 0.9248
</p>
<p>F-statistic: 382.5 on 1 and 30 DF, p-value: &lt; 2.2e-16
</p>
<p>&gt; summary(lm(log10(diameter)~ log10(conc), data=tethym.nogluc))
</p>
<p>Call:
</p>
<p>lm(formula = log10(diameter) ~ log10(conc), data = tethym.nogluc)
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-2.192e-02 -4.977e-03 5.598e-05 5.597e-03 1.663e-02
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 1.634761 0.020209 80.89 &lt; 2e-16 ***
</p>
<p>log10(conc) -0.059677 0.004125 -14.47 5.48e-11 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 0.009532 on 17 degrees of freedom
</p>
<p>Multiple R-squared: 0.9249, Adjusted R-squared: 0.9205
</p>
<p>F-statistic: 209.3 on 1 and 17 DF, p-value: 5.482e-11
</p>
<p>Notice that you can use arithmetic expressions in the model formula
[here log10(...)]. There are limitations, though, because, for example,
z~x+ymeans a model where z is described by an additive model in x and
y, which is not the same as a regression analysis on the sum of the two.
The latter may be specified using z~I(x+y) (I for &ldquo;identity&rdquo;).
</p>
<p>A quick assessment of the significance of the difference between the
slopes of the two lines can be obtained as follows: The difference be-
tween the slope estimates is 0.0065, and the standard error of that is&radic;
0.00412 + 0.00272 = 0.0049. Since t = 0.0065/0.0049 = 1.3, it would
</p>
<p>seem that we are allowed to assume that the slopes are the same.
</p>
<p>It is, however, preferable to fit a model to the entire data set and test the
hypothesis of equal slopes in that model. One reason that this approach
is preferable is that it can be generalized to more complicated models.
Another reason is that even though there is nothing seriously wrong with
the simple test for equal slopes, that procedure gives you little information
on how to proceed. If the slopes are the same, you would naturally want</p>
<p/>
</div>
<div class="page"><p/>
<p>214 12. Linear models
</p>
<p>to find an estimate of the common slope and the distance between the
parallel lines.
</p>
<p>First, we set up a model that allows the relation between concentration
and cell diameter to have different slopes and intercepts in the two glucose
groups:
</p>
<p>&gt; summary(lm(log10(diameter)~log10(conc)*glucose))
</p>
<p>Call:
</p>
<p>lm(formula = log10(diameter) ~ log10(conc) * glucose)
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-2.672e-02 -4.888e-03 5.598e-05 3.767e-03 1.761e-02
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 1.631344 0.013879 117.543 &lt;2e-16 ***
</p>
<p>log10(conc) -0.053196 0.002807 -18.954 &lt;2e-16 ***
</p>
<p>glucoseNo 0.003418 0.023695 0.144 0.886
</p>
<p>log10(conc):glucoseNo -0.006480 0.004821 -1.344 0.185
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 0.009059 on 47 degrees of freedom
</p>
<p>Multiple R-squared: 0.9361, Adjusted R-squared: 0.9321
</p>
<p>F-statistic: 229.6 on 3 and 47 DF, p-value: &lt; 2.2e-16
</p>
<p>These regression coefficients should be read as follows. The expected
value of the log cell diameter for an observation with cell concentration
C is obtained as the sum of the following four quantities:
</p>
<p>1. The intercept, 1.6313
</p>
<p>2. &minus;0.0532&times; log10 C
3. 0.0034, but only for a culture without glucose
</p>
<p>4. &minus;0.0065&times; log10 C, but only for cultures without glucose
</p>
<p>Accordingly, for cell cultures with glucose, we have the linear relation
</p>
<p>log10 D = 1.6313&minus; 0.0532&times; log10 C
and for cultures without glucose we have
</p>
<p>log10 D = (1.6313+ 0.0034)&minus; (0.0532+ 0.0065)&times; log10 C
Put differently, the first two coefficients in the joint model can be inter-
preted as the estimates for intercept and slope in group 1, whereas the
latter two are the differences between group 1 and group 2 in intercept
and slope, respectively. Comparison with the separate regression analyses</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Analysis of covariance 215
</p>
<p>shows that slopes and intercepts are the same as in the joint analysis. The
standard errors differ a little from the separate analyses because a pooled
variance estimate is now used. Notice that the rough test of difference in
slope outlined above is essentially the t test for the last coefficient.
</p>
<p>Notice also that the glucose and log10(conc).glucose terms indi-
cate items to be added for cultures without glucose. This is because the
factor levels are ordered yes = 1 and no = 2, and the base level is the first
group.
</p>
<p>Fitting an additive model, we get
</p>
<p>&gt; summary(lm(log10(diameter)~log10(conc)+glucose))
</p>
<p>...
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 1.642132 0.011417 143.83 &lt; 2e-16 ***
</p>
<p>log10(conc) -0.055393 0.002301 -24.07 &lt; 2e-16 ***
</p>
<p>glucoseNo -0.028238 0.002647 -10.67 2.93e-14 ***
</p>
<p>...
</p>
<p>Here the interpretation of the coefficients is that the estimated relation for
cultures with glucose is
</p>
<p>log10 D = 1.6421&minus; 0.0554&times; log10 C
and for cultures without glucose it is
</p>
<p>log10 D = (1.6421&minus; 0.0282)&minus; 0.0554&times; log10 C
That is, the lines for the two cultures are parallel, but the log diame-
ters for cultures without glucose are 0.0282 below those with glucose. On
the original (nonlogarithmic) scale, this means that the former are 6.3%
lower (a constant absolute difference on a logarithmic scale corresponds
to constant relative differences on the original scale and 10&minus;0.0282 = 0.937).
</p>
<p>The joint analysis presumes that the variance around the regression line
is the same in the two groups. This assumption should really have been
tested before embarking on the analysis above. A formal test can be
performed with var.test, which conveniently allows a pair of linear
models as arguments instead of a model formula or two group vectors:
</p>
<p>&gt; var.test(lm.gluc,lm.nogluc)
</p>
<p>F test to compare two variances
</p>
<p>data: lm.gluc and lm.nogluc
</p>
<p>F = 0.8482, num df = 30, denom df = 17, p-value = 0.6731
</p>
<p>alternative hypothesis: true ratio of variances is not equal to 1
</p>
<p>95 percent confidence interval:
</p>
<p>0.3389901 1.9129940
</p>
<p>sample estimates:</p>
<p/>
</div>
<div class="page"><p/>
<p>216 12. Linear models
</p>
<p>ratio of variances
</p>
<p>0.8481674
</p>
<p>When there are more than two groups, Bartlett&rsquo;s test can be used. It, too,
allows linear models to be compared. The reservations about robustness
against nonnormality apply here, too.
</p>
<p>It is seen that it is possible to assume that the lines have the same slope and
that they have the same intercept, but &mdash; as we will see below &mdash; not both
at once. The hypothesis of a common intercept is silly anyway unless the
slopes are also identical: The intercept is by definition the y-value at x = 0,
which because of the log scale corresponds to a cell concentration of 1.
That is far outside the region the data cover, and it is a completely arbitrary
point that will change if the concentrations aremeasured in different units.
</p>
<p>The ANOVA table for the model is
</p>
<p>&gt; anova(lm(log10(diameter)~ log10(conc)*glucose))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: log10(diameter)
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>log10(conc) 1 0.046890 0.046890 571.436 &lt; 2.2e-16 ***
</p>
<p>glucose 1 0.009494 0.009494 115.698 2.89e-14 ***
</p>
<p>log10(conc):glucose 1 0.000148 0.000148 1.807 0.1853
</p>
<p>Residuals 47 0.003857 0.000082
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>The model formula a*b, where in the present case a is log10(conc)
and b is glucose, is a short form for a + b + a:b, which is read &ldquo;effect
of a plus effect of b plus interaction&rdquo;. The F test in the penultimate line
of the ANOVA table is a test for the hypothesis that the last term (a:b)
can be omitted, reducing the model to be additive in log10(conc) and
glucose, which corresponds to the parallel regression lines. The F test
one line earlier indicates whether you can subsequently remove glucose
and the one in the first line to whether you can additionally remove
log10(conc), leaving an empty model.
</p>
<p>Alternatively, you can read the table from top to bottom as adding terms
describing more and more of the total sum of squares. To those familiar
with the SAS system, this kind of ANOVA table is known as type I sums
of squares.
</p>
<p>The p-value for log10(conc):glucose can be recognized as that of the
t test for the coefficient labelled log10(conc).glucose in the previous
output. The F statistic is exactly the square of t as well. However, this is
true only because there are just two groups. Had there been three or more,
there would have been several regression coefficients and the F test would</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Analysis of covariance 217
</p>
<p>have tested them all against zero simultaneously, just like when all groups
are tested equal in a one-way analysis of variance.
</p>
<p>Note that the test for removing log10(conc) does not make sense be-
cause you would have to remove glucose first, which is &ldquo;forbidden&rdquo;
when glucose has a highly significant effect. It makes perfectly good
sense to test log10(conc) without removing glucose &mdash; which corre-
sponds to testing that the two parallel regression lines can be assumed
horizontal &mdash; but that test is not found in the ANOVA table. You can
get the right test by changing the order of terms in the model formula;
compare, for instance, these two regression analyses:
</p>
<p>&gt; anova(lm(log10(diameter)~glucose+log10(conc)))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: log10(diameter)
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>glucose 1 0.008033 0.008033 96.278 4.696e-13 ***
</p>
<p>log10(conc) 1 0.048351 0.048351 579.494 &lt; 2.2e-16 ***
</p>
<p>Residuals 48 0.004005 0.000083
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>&gt; anova(lm(log10(diameter)~log10(conc)+ glucose))
</p>
<p>Analysis of Variance Table
</p>
<p>Response: log10(diameter)
</p>
<p>Df Sum Sq Mean Sq F value Pr(&gt;F)
</p>
<p>log10(conc) 1 0.046890 0.046890 561.99 &lt; 2.2e-16 ***
</p>
<p>glucose 1 0.009494 0.009494 113.78 2.932e-14 ***
</p>
<p>Residuals 48 0.004005 0.000083
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>They both describe exactly the same model, as is indicated by the resid-
ual sum of squares being identical. The partitioning of the sum of squares
is not the same, though &mdash; and the difference may be much more dra-
matic than it is here. The difference is whether log10(conc) is added to
a model already containing glucose or vice versa. Since the second F test
in both tables is highly significant, no model reduction is possible and the
F test in the line above it is irrelevant.
</p>
<p>If you go back and look at the regression coefficients in the model with
parallel regression lines, you will see that the squares of the t tests are
579.49 and 113.8, precisely the last F test in the two tables above.
</p>
<p>It is informative to compare the covariance analysis above with the
simpler analysis in which the effect of cell concentration is ignored:
</p>
<p>&gt; t.test(log10(diameter)~glucose)</p>
<p/>
</div>
<div class="page"><p/>
<p>218 12. Linear models
</p>
<p>Welch Two Sample t-test
</p>
<p>data: log10(diameter) by glucose
</p>
<p>t = 2.7037, df = 36.31, p-value = 0.01037
</p>
<p>alternative hypothesis: true difference in means is not equal to 0
</p>
<p>95 percent confidence interval:
</p>
<p>0.006492194 0.045424241
</p>
<p>sample estimates:
</p>
<p>mean in group Yes mean in group No
</p>
<p>1.370046 1.344088
</p>
<p>Notice that the p-value is much less extreme. It is still significant in this
case, but in smaller data sets the statistical significance could easily disap-
pear completely. The difference in means between the two groups is 0.026,
which is comparable to the 0.028 that was the glucose effect in the analysis
of covariance. However, the confidence interval goes from 0.006 to 0.045,
whereas the analysis of covariance had 0.023 to 0.034 [0.0282&plusmn; t.975(48)&times;
0.0026], which is almost four times as narrow, obviously a substantial gain
in efficiency.
</p>
<p>12.8 Diagnostics
</p>
<p>Regression diagnostics are used to evaluate the model assumptions and in-
vestigate whether or not there are observations with a large influence
on the analysis. A basic set of these is available via the plot method for
lm objects. Four of them are displayed in a 2&times; 2 layout (Figure 12.6) as
follows:
</p>
<p>&gt; attach(thuesen)
</p>
<p>&gt; options(na.action="na.exclude")
</p>
<p>&gt; lm.velo &lt;- lm(short.velocity~blood.glucose)
</p>
<p>&gt; opar &lt;- par(mfrow=c(2,2), mex=0.6, mar=c(4,4,3,2)+.3)
</p>
<p>&gt; plot(lm.velo, which=1:4)
</p>
<p>&gt; par(opar)
</p>
<p>The par commands set up for a 2&times;2 layout with compressedmargin texts
and go back to normal after plotting.
</p>
<p>The top left panel shows residuals versus fitted values. The top right panel
is a Q&ndash;Q normal distribution plot of standardized residuals. Notice that
there are residuals and standardized residuals; the latter have been cor-
rected for differences in the SD of residuals depending on their position in
the design. (Residuals corresponding to extreme x-values generally have
a lower SD due to overfitting.) The third plot is of the square root of the
absolute value of the standardized residuals; this reduces the skewness
of the distribution and makes it much easier to detect if there might be a</p>
<p/>
</div>
<div class="page"><p/>
<p>12.8 Diagnostics 219
</p>
<p>1.20 1.30 1.40 1.50
</p>
<p>&minus;
0
.4
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>Fitted values
</p>
<p>R
e
</p>
<p>s
id
</p>
<p>u
a
</p>
<p>ls
Residuals vs Fitted
</p>
<p>13
</p>
<p>20
</p>
<p>24
</p>
<p>&minus;2 &minus;1 0 1 2
</p>
<p>&minus;
2
</p>
<p>&minus;
1
</p>
<p>0
1
</p>
<p>2
</p>
<p>Theoretical Quantiles
</p>
<p>S
ta
</p>
<p>n
d
</p>
<p>a
rd
</p>
<p>iz
e
</p>
<p>d
 r
</p>
<p>e
s
id
</p>
<p>u
a
</p>
<p>ls
</p>
<p>Normal Q&minus;Q
</p>
<p>13
</p>
<p>20
</p>
<p>24
</p>
<p>1.20 1.30 1.40 1.50
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>1
.5
</p>
<p>Fitted values
</p>
<p>S
ta
</p>
<p>n
d
a
rd
</p>
<p>iz
e
d
 r
</p>
<p>e
s
id
</p>
<p>u
a
ls
</p>
<p>Scale&minus;Location
13
</p>
<p>2024
</p>
<p>5 10 15 20
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>Obs. number
</p>
<p>C
o
o
k
&rsquo;s
</p>
<p> d
is
</p>
<p>ta
n
c
e
</p>
<p>Cook&rsquo;s distance
13
</p>
<p>20
</p>
<p>1
</p>
<p>Figure 12.6. Regression diagnostics.
</p>
<p>trend in the dispersion. The fourth plot is of &ldquo;Cook&rsquo;s distance&rdquo;, which is a
measure of the influence of each observation on the regression coefficients.
We will return to Cook&rsquo;s distance shortly. Actually, this is not the default
set of plots; the default replaces the fourth plot by a plot that contains the
two components that enter into the calculation of Cook&rsquo;s distance, but this
is harder to explain at this level.
</p>
<p>The plots for the thuesen data show observation no. 13 as extreme in
several respects. It has the largest residual as well as a prominent spike in
the Cook&rsquo;s distance plot. Observation no. 20 also has a large residual, but
not quite as conspicuous a Cook&rsquo;s distance.
</p>
<p>&gt; opar &lt;- par(mfrow=c(2,2), mex=0.6, mar=c(4,4,3,2)+.3)
</p>
<p>&gt; plot(rstandard(lm.velo))
</p>
<p>&gt; plot(rstudent(lm.velo))
</p>
<p>&gt; plot(dffits(lm.velo),type="l")
</p>
<p>&gt; matplot(dfbetas(lm.velo),type="l", col="black")
</p>
<p>&gt; lines(sqrt(cooks.distance(lm.velo)), lwd=2)
</p>
<p>&gt; par(opar)
</p>
<p>It is also possible to obtain individual diagnostics; a selection is shown
in Figure 12.7. The function rstandard gives the standardized residuals</p>
<p/>
</div>
<div class="page"><p/>
<p>220 12. Linear models
</p>
<p>5 10 15 20
</p>
<p>&minus;
2
</p>
<p>&minus;
1
</p>
<p>0
1
</p>
<p>2
</p>
<p>Index
</p>
<p>rs
ta
</p>
<p>n
d
</p>
<p>a
rd
</p>
<p>(l
m
</p>
<p>.v
e
</p>
<p>lo
)
</p>
<p>5 10 15 20
</p>
<p>&minus;
2
</p>
<p>&minus;
1
</p>
<p>0
1
</p>
<p>2
</p>
<p>Index
</p>
<p>rs
tu
</p>
<p>d
e
</p>
<p>n
t(
</p>
<p>lm
.v
</p>
<p>e
lo
</p>
<p>)
</p>
<p>5 10 15 20
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>Index
</p>
<p>d
ff
</p>
<p>it
s
(l
m
</p>
<p>.v
e
lo
</p>
<p>)
</p>
<p>5 10 15 20
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>d
fb
</p>
<p>e
ta
</p>
<p>s
(l
m
</p>
<p>.v
e
lo
</p>
<p>)
</p>
<p>Figure 12.7. Further regression diagnostics.
</p>
<p>discussed above. There is also rstudent, which gives leave-out-one residu-
als, in which the fitted value is calculated omitting the current point; if the
model is correct, then these will follow a (Student&rsquo;s) t distribution. (Unfor-
tunately, some texts use &ldquo;studentized residuals&rdquo; for residuals divided by
their standard deviation; i.e., what rstandard calculates in R.) It takes a
keen eye to see the difference between the two types of residuals, but the
extreme residuals tend to be a little further out in the case of rstudent.
</p>
<p>The function dffits expresses how much an observation affects the as-
sociated fitted value. As with the residuals, observations 13 and maybe
20 seem to stick out. Notice that there is a gap in the line. This is due
to the missing observation 16 and the use of na.exclude. This looks
a little awkward but has the advantage of making the x-axis match the
observation number.
</p>
<p>The function dfbetas gives the change in the estimated parameters if
an observation is excluded relative to its standard error. It is a matrix, so
matplot is useful to plot them all in one plot. Notice that observation 13
affects both α (the solid line) and β by nearly one standard error.
</p>
<p>The name dfbetas refers to its use in multiple regression analysis, where
you write the model as y = β0 + β1x1 + β2x2 + &middot; &middot; &middot; . This gets a little con-</p>
<p/>
</div>
<div class="page"><p/>
<p>12.8 Diagnostics 221
</p>
<p>fusing in a simple regression analysis, where the intercept is otherwise
called α.
</p>
<p>Cook&rsquo;s distance D calculated by cooks.distance is essentially a joint
measure of the components of dfbetas. The exact procedure is to take
the unnormalized change in coefficients and use the norm defined by the
estimated covariance matrix for β̂ and then divide by the number of co-
efficients.
</p>
<p>&radic;
D is on the same scale as dfbetas and was added to that
</p>
<p>plot as a double-width line. (If you look inside the R functions for some
of these quantities, you will find them apparently quite different from the
descriptions above, but they are in fact the same, only computationally
more efficient.)
</p>
<p>Thus, the picture is that observation 13 seems to be influential. Let us look
at the analysis without this observation.
</p>
<p>We use the subset argument to lm, which, like other indexing operations,
can be used with negative numbers to remove observations.
</p>
<p>&gt; summary(lm(short.velocity~blood.glucose, subset=-13))
</p>
<p>Call:
</p>
<p>lm(formula = short.velocity ~ blood.glucose, subset = -13)
</p>
<p>Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-0.31346 -0.11136 -0.01247 0.06043 0.40794
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>(Intercept) 1.18929 0.11061 10.752 9.22e-10 ***
</p>
<p>blood.glucose 0.01082 0.01029 1.052 0.305
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 0.193 on 20 degrees of freedom
</p>
<p>(1 observation deleted due to missingness)
</p>
<p>Multiple R-squared: 0.05241, Adjusted R-squared: 0.005026
</p>
<p>F-statistic: 1.106 on 1 and 20 DF, p-value: 0.3055
</p>
<p>The relation practically vanished in thin air! The whole analysis actually
hinges on a single observation. If the data and model are valid, then of
course the original p-value is correct, and perhaps you could also say that
there will always be influential observations in small data sets, but some
caution in the interpretation does seem advisable.
</p>
<p>The methods for finding influential observations and outliers are even
more important in regression analysis with multiple descriptive variables.
One of the big problems is how to present the quantities graphically in a
sensible way. This might be done using three-dimensional plots (the add-</p>
<p/>
</div>
<div class="page"><p/>
<p>222 12. Linear models
</p>
<p>110 120 130 140 150 160 170 180
</p>
<p>2
0
</p>
<p>3
0
</p>
<p>4
0
</p>
<p>5
0
</p>
<p>6
0
</p>
<p>7
0
</p>
<p>height
</p>
<p>w
e
ig
</p>
<p>h
t
</p>
<p>Figure 12.8. Cook&rsquo;s distance (colour coded) in pemax ~ height + weight.
</p>
<p>on package scatterplot3d makes this possible), but you can get quite
far using colour coding.
</p>
<p>Here, we see how to display the value of Cook&rsquo;s distance (which is always
positive) graphically for a model where pemax is described using height
and weight, as in Figure 12.8:
</p>
<p>&gt; cookd &lt;- cooks.distance(lm(pemax~height+weight))
</p>
<p>&gt; cookd &lt;- cookd/max(cookd)
</p>
<p>&gt; cook.colors &lt;- gray(1-sqrt(cookd))
</p>
<p>&gt; plot(height,weight,bg=cook.colors,pch=21,cex=1.5)
</p>
<p>&gt; points(height,weight,pch=1,cex=1.5)
</p>
<p>The first line computes Cook&rsquo;s distance and the second scales it to a value
between 0 and 1. Thereafter, a colour coding of the values in cookd is
made with the function gray. The latter interprets its argument as the de-
gree of whiteness, so if you want a large distance represented as black,
you need to subtract the value from 1. Furthermore, it is convenient to
take the square root of cookd because it is a quadratic distance measure
(which in practice shows up in the form of too many white or nearly white
points). Then a scatterplot of height versus weight is drawn with the cho-</p>
<p/>
</div>
<div class="page"><p/>
<p>12.8 Diagnostics 223
</p>
<p>70 80 90 100 110 120 130
</p>
<p>7
0
</p>
<p>8
0
</p>
<p>9
0
</p>
<p>1
0
0
</p>
<p>ad
</p>
<p>b
p
d
</p>
<p>Figure 12.9. Studentized residuals in the Secher data, colour coded. Positive values
are marked with upward-pointing triangles; negative ones point down.
</p>
<p>sen colours. A filled plotting symbol in enlarged symbol size is used to get
the grayscale to stand out more clearly.
</p>
<p>You can use similar techniques to describe other influence measures. In
the case of signed measures, you might use different symbols for positive
and negative values. Here is an example on Studentized residuals in a
data set describing birth weight as a function of abdominal and biparietal
diameters determined by ultrasonography of the fetus immediately before
birth, also used in Exercise 11.1 (Figure 12.9):
</p>
<p>&gt; attach(secher)
</p>
<p>&gt; rst &lt;- rstudent(lm(log10(bwt)~log10(ad)+log10(bpd)))
</p>
<p>&gt; range(rst)
</p>
<p>[1] -3.707509 3.674050
</p>
<p>&gt; rst &lt;- rst/3.71
</p>
<p>&gt; plot(ad,bpd,log="xy",bg=gray(1-abs(rst)),
</p>
<p>+ pch=ifelse(rst&gt;0,24,25), cex=1.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>224 12. Linear models
</p>
<p>12.9 Exercises
</p>
<p>12.1 Set up an additive model for the ashina data (see Exercise 5.6) con-
taining additive effects of subjects, period, and treatment. Compare the
results with those obtained from t tests.
</p>
<p>12.2 Perform a two-way analysis of variance on the tb.dilute data.
Modify the model to have a dose effect that is linear in log dose. Compute
a confidence interval for the slope. An alternative approach could be to
calculate a slope for each animal and perform a test based on them. Com-
pute a confidence interval for the mean slope, and compare it with the
preceding result.
</p>
<p>12.3 Consider the following definitions:
</p>
<p>a &lt;- gl(2, 2, 8)
</p>
<p>b &lt;- gl(2, 4, 8)
</p>
<p>x &lt;- 1:8
</p>
<p>y &lt;- c(1:4,8:5)
</p>
<p>z &lt;- rnorm(8)
</p>
<p>Generate the model matrices for models z ~ a*b, z ~ a:b, etc. Dis-
cuss the implications. Carry out the model fits, and notice which models
contain singularities.
</p>
<p>12.4 (Advanced) In the secretin experiment, you may expect to find
inter-individual differences not only in the level of glucose but also in
the change induced by the injection of secretin. The factor time.comb
combines time values at 30, 60, and 90 minutes. The factor time20plus
combines all values from 20 minutes onward. Discuss the differences and
relations among the following linear models:
</p>
<p>attach(secretin)
</p>
<p>model1 &lt;- lm(gluc ~ person * time)
</p>
<p>model2 &lt;- lm(gluc ~ person + time)
</p>
<p>model3 &lt;- lm(gluc ~ person * time20plus + time)
</p>
<p>model4 &lt;- lm(gluc ~ person * time20plus + time.comb)
</p>
<p>12.5 Analyze the blood pressure in the bp.obese data set as a function
of obesity and gender.
</p>
<p>12.6 Analyze the vitcap2 data set using analysis of covariance. Revisit
Exercise 5.2 and compare the conclusions. Try using the drop1 function
with test="F" instead of summary in this model.
</p>
<p>12.7 In the juul data setmake regression analyses for prepubescent chil-
dren (Tanner stage 1) of
</p>
<p>&radic;
igf1 versus age separately for boys and girls.
</p>
<p>Compare the two regression lines.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.9 Exercises 225
</p>
<p>12.8 Try step on the kfm data and discuss the result. One observation
appears to be influential on the diagnostic plot for this model &mdash; explain
why. What happens if you reduce the model further?
</p>
<p>12.9 For the juul data, fit a model for igf1 with interactions between
age, sex, and Tanner stage for those under 25 years old. Explain the inter-
pretation of this model. Hint: A plot of the fitted values against age should
be helpful. Use diagnostic plots to evaluate possible transformations of the
dependent variable: untransformed, log, or square root.</p>
<p/>
</div>
<div class="page"><p/>
<p>13
Logistic regression
</p>
<p>Sometimes you wish to model binary outcomes, variables that can have
only two possible values: diseased or nondiseased, and so forth. For in-
stance, you want to describe the risk of getting a disease depending on
various kinds of exposures. Chapter 8 discusses some simple techniques
based on tabulation, but you might also want to model dose-response re-
lationships (where the predictor is a continuous variable) or model the
effect of multiple variables simultaneously. It would be very attractive to
be able to use the same modelling techniques as for linear models.
</p>
<p>However, it is not really attractive to use additive models for probabili-
ties since they have a limited range and regression models could predict
off-scale values below zero or above 1. It makes better sense to model
the probabilities on a transformed scale; this is what is done in logistic
regression analysis.
</p>
<p>A linear model for transformed probabilities can be set up as
</p>
<p>logit p = β0 + β1x1 + β2x2 + . . . βkxk
</p>
<p>in which logit p = log[p/(1&minus; p)] is the log odds. A constant additive ef-
fect on the logit scale corresponds to a constant odds ratio. The choice of
the logit function is not the only one possible, but it has some mathemat-
ically convenient properties. Other choices do exist; the probit function
(the quantile function of the normal distribution) or log(&minus; log p), which
has a connection to survival analysis models.
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_13, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>228 13. Logistic regression
</p>
<p>One thing to notice about the logistic model is that there is no error term as
in linearmodels.We aremodelling the probability of an event directly, and
that in itself will determine the variability of the binary outcome. There is
no variance parameter as in the normal distribution.
</p>
<p>The parameters of the model can be estimated by the method of maximum
likelihood. This is a quite general technique, similar to the least-squares
method in that it finds a set of parameters that optimizes a goodness-of-
fit criterion (in fact, the least-squares method itself is a slightly modified
maximum-likelihood procedure). The likelihood function L(β) is simply the
probability of the entire observed data set for varying parameters.
</p>
<p>The deviance is the difference between the maximized value of &minus;2 log L
and the similar quantity under a &ldquo;maximal model&rdquo; that fits data perfectly.
Changes in deviance caused by a model reduction will be approximately
χ2-distributed with degrees of freedom equal to the change in the number
of parameters.
</p>
<p>In this chapter, we see how to perform logistic regression analysis in R.
There naturally is quite a large overlap with the material on linear models
since the description of models is quite similar, but there are also some
special issues concerning deviance tables and the specification of models
for pretabulated data.
</p>
<p>13.1 Generalized linear models
</p>
<p>Logistic regression analysis belongs to the class of generalized linear models.
These models are characterized by their response distribution (here the
binomial distribution) and a link function, which transfers the mean value
to a scale in which the relation to background variables is described as
linear ans additive. In a logistic regression analysis, the link function is
logit p = log[p/(1&minus; p)].
There are several other examples of generalized linear models; for in-
stance, analysis of count data is often handled by the multiplicative
Poisson model, where the link function is logλ, with λ the mean of the
Poisson-distributed observation. All of these models can be handled using
the same algorithm, which also allows the user some freedom to define his
or her own models by defining suitable link functions.
</p>
<p>In R generalized linear models are handled by the glm function. This
function is very similar to lm, which we have used many times for lin-
ear normal models. The two functions use essentially the same model
formulas and extractor functions (summary, etc.), but glm also needs to
have specified which generalized linear model is desired. This is done via</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Logistic regression on tabular data 229
</p>
<p>the family argument. To specify a binomial model with logit link (i.e.,
logistic regression analysis), you write family=binomial("logit").
</p>
<p>13.2 Logistic regression on tabular data
</p>
<p>In this section, we analyze the example concerning hypertension from
Altman (1991, p. 353). First, we need to enter data, which is done as
follows:
</p>
<p>&gt; no.yes &lt;- c("No","Yes")
</p>
<p>&gt; smoking &lt;- gl(2,1,8,no.yes)
</p>
<p>&gt; obesity &lt;- gl(2,2,8,no.yes)
</p>
<p>&gt; snoring &lt;- gl(2,4,8,no.yes)
</p>
<p>&gt; n.tot &lt;- c(60,17,8,2,187,85,51,23)
</p>
<p>&gt; n.hyp &lt;- c(5,2,1,0,35,13,15,8)
</p>
<p>&gt; data.frame(smoking,obesity,snoring,n.tot,n.hyp)
</p>
<p>smoking obesity snoring n.tot n.hyp
</p>
<p>1 No No No 60 5
</p>
<p>2 Yes No No 17 2
</p>
<p>3 No Yes No 8 1
</p>
<p>4 Yes Yes No 2 0
</p>
<p>5 No No Yes 187 35
</p>
<p>6 Yes No Yes 85 13
</p>
<p>7 No Yes Yes 51 15
</p>
<p>8 Yes Yes Yes 23 8
</p>
<p>The gl function to &ldquo;generate levels&rdquo; was briefly introduced in Section 7.3.
The first three arguments to gl are, respectively, the number of levels,
the repeat count of each level, and the total length of the vector. A fourth
argument can be used to specify the level names of the resulting factor.
The result is apparent from the printout of the generated variables. They
were put together in a data frame to get a nicer layout. Another way of
generating a regular pattern like this is to use expand.grid:
</p>
<p>&gt; expand.grid(smoking=no.yes, obesity=no.yes, snoring=no.yes)
</p>
<p>smoking obesity snoring
</p>
<p>1 No No No
</p>
<p>2 Yes No No
</p>
<p>3 No Yes No
</p>
<p>4 Yes Yes No
</p>
<p>5 No No Yes
</p>
<p>6 Yes No Yes
</p>
<p>7 No Yes Yes
</p>
<p>8 Yes Yes Yes
</p>
<p>R is able to fit logistic regression analyses for tabular data in two different
ways. You have to specify the response as a matrix, where one column is</p>
<p/>
</div>
<div class="page"><p/>
<p>230 13. Logistic regression
</p>
<p>the number of &ldquo;diseased&rdquo; and the other is the number of &ldquo;healthy&rdquo; (or
&ldquo;success&rdquo; and &ldquo;failure&rdquo;, depending on context).
</p>
<p>&gt; hyp.tbl &lt;- cbind(n.hyp,n.tot-n.hyp)
</p>
<p>&gt; hyp.tbl
</p>
<p>n.hyp
</p>
<p>[1,] 5 55
</p>
<p>[2,] 2 15
</p>
<p>[3,] 1 7
</p>
<p>[4,] 0 2
</p>
<p>[5,] 35 152
</p>
<p>[6,] 13 72
</p>
<p>[7,] 15 36
</p>
<p>[8,] 8 15
</p>
<p>The cbind function (&ldquo;c&rdquo; for &ldquo;column&rdquo;) is used to bind variables together,
columnwise, to form a matrix. Note that it would be a horrible mistake to
use the total count for column 2 instead of the number of failures.
</p>
<p>Then, you can specify the logistic regression model as
</p>
<p>&gt; glm(hyp.tbl~smoking+obesity+snoring,family=binomial("logit"))
</p>
<p>Actually, "logit" is the default for binomial and the family argument
is the second argument to glm, so it suffices to write
</p>
<p>&gt; glm(hyp.tbl~smoking+obesity+snoring,binomial)
</p>
<p>The other way to specify a logistic regression model is to give the
proportion of diseased in each cell:
</p>
<p>&gt; prop.hyp &lt;- n.hyp/n.tot
</p>
<p>&gt; glm.hyp &lt;- glm(prop.hyp~smoking+obesity+snoring,
</p>
<p>+ binomial,weights=n.tot)
</p>
<p>It is necessary to give weights because R cannot see how many
observations a proportion is based on.
</p>
<p>As output, you get in either case (except for minor details)
</p>
<p>Call: glm(formula = hyp.tbl ~ smoking + obesity + snoring, ...
</p>
<p>Coefficients:
</p>
<p>(Intercept) smokingYes obesityYes snoringYes
</p>
<p>-2.37766 -0.06777 0.69531 0.87194
</p>
<p>Degrees of Freedom: 7 Total (i.e. Null); 4 Residual
</p>
<p>Null Deviance: 14.13
</p>
<p>Residual Deviance: 1.618 AIC: 34.54</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Logistic regression on tabular data 231
</p>
<p>which is in a minimal style similar to that used for printing lm objects.
Also in the result of glm is some nonvisible information, which may be
extracted with particular functions. You can, for instance, save the result
of a fit of a generalized linear model in a variable and obtain a table of
regression coefficients and so forth using summary:
</p>
<p>&gt; glm.hyp &lt;- glm(hyp.tbl~smoking+obesity+snoring,binomial)
</p>
<p>&gt; summary(glm.hyp)
</p>
<p>Call:
</p>
<p>glm(formula = hyp.tbl ~ smoking + obesity + snoring, family ...
</p>
<p>Deviance Residuals:
</p>
<p>1 2 3 4 5 6
</p>
<p>-0.04344 0.54145 -0.25476 -0.80051 0.19759 -0.46602
</p>
<p>7 8
</p>
<p>-0.21262 0.56231
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -2.37766 0.38018 -6.254 4e-10 ***
</p>
<p>smokingYes -0.06777 0.27812 -0.244 0.8075
</p>
<p>obesityYes 0.69531 0.28509 2.439 0.0147 *
</p>
<p>snoringYes 0.87194 0.39757 2.193 0.0283 *
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>(Dispersion parameter for binomial family taken to be 1)
</p>
<p>Null deviance: 14.1259 on 7 degrees of freedom
</p>
<p>Residual deviance: 1.6184 on 4 degrees of freedom
</p>
<p>AIC: 34.537
</p>
<p>Number of Fisher Scoring iterations: 4
</p>
<p>In the following, we go through the components of summary output for
generalized linear models:
</p>
<p>Call:
</p>
<p>glm(formula = hyp.tbl ~ smoking + obesity + snoring, family = ...
</p>
<p>As usual, we start off with a repeat of the model specification. Obviously,
more interesting is when the output is not viewed in connection with the
function call that generated it.
</p>
<p>Deviance Residuals:
</p>
<p>1 2 3 4 5 6
</p>
<p>-0.04344 0.54145 -0.25476 -0.80051 0.19759 -0.46602
</p>
<p>7 8
</p>
<p>-0.21262 0.56231</p>
<p/>
</div>
<div class="page"><p/>
<p>232 13. Logistic regression
</p>
<p>This is the contribution of each cell of the table to the deviance of the
model (the deviance corresponds to the sum of squares in linear normal
models), with a sign according to whether the observation is larger or
smaller than expected. They can be used to pinpoint cells that are par-
ticularly poorly fitted, but you have to be wary of the interpretation in
sparse tables.
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -2.37766 0.38018 -6.254 4e-10 ***
</p>
<p>smokingYes -0.06777 0.27812 -0.244 0.8075
</p>
<p>obesityYes 0.69531 0.28509 2.439 0.0147 *
</p>
<p>snoringYes 0.87194 0.39757 2.193 0.0283 *
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>(Dispersion parameter for binomial family taken to be 1)
</p>
<p>This is the table of primary interest. Here, we get estimates of the re-
gression coefficients, standard errors of same, and tests for whether each
regression coefficient can be assumed to be zero. The layout is nearly
identical to the corresponding part of the lm output.
</p>
<p>The note about the dispersion parameter is related to the fact that the bino-
mial variance depends entirely on the mean. There is no scale parameter
like the variance in the normal distribution.
</p>
<p>Null deviance: 14.1259 on 7 degrees of freedom
</p>
<p>Residual deviance: 1.6184 on 4 degrees of freedom
</p>
<p>AIC: 34.537
</p>
<p>&ldquo;Residual deviance&rdquo; corresponds to the residual sum of squares in ordi-
nary regression analyses which is used to estimate the standard deviation
about the regression line. In binomial models, however, the standard devi-
ation of the observations is known, and you can therefore use the deviance
in a test for model specification. The AIC (Akaike information criterion) is
a measure of goodness of fit that takes the number of fitted parameters
into account.
</p>
<p>R is reluctant to associate a p-value with the deviance. This is just as well
because no exact p-value can be found, only an approximation that is valid
for large expected counts. In the present case, there are actually a couple
of places where the expected cell count is rather small.
</p>
<p>The asymptotic distribution of the residual deviance is a χ2 distribution
with the stated degrees of freedom, so even though the approximation
may be poor, nothing in the data indicates that the model is wrong (the
5% significance limit is at 9.49 and the value found here is 1.62).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Logistic regression on tabular data 233
</p>
<p>The null deviance is the deviance of a model that contains only the in-
tercept (that is, describes a fixed probability, here for hypertension, in all
cells). What you would normally be interested in is the difference from the
residual deviance, here 14.13&minus; 1.62 = 12.51, which can be used for a joint
test for whether any effects are present in the model. In the present case, a
p-value of approximately 0.6% is obtained.
</p>
<p>Number of Fisher Scoring iterations: 4
</p>
<p>This refers to the actual fitting procedure and is a purely technical item.
There is no statistical information in it, but you should keep an eye on
whether the number of iterations becomes too large because that might be
a sign that the model is too complex to fit based on the available data. Nor-
mally, glm halts the fitting procedure if the number of iterations exceeds
25, but it is possible to configure the limit.
</p>
<p>The fitting procedure is iterative in that there is no explicit formula that
can be used to compute the estimates, only a set of equations that they
should satisfy. However, there is an approximate solution of the equations
if you supply an initial guess at the solution. This solution is then used as
a starting point for an improved solution, and the procedure is repeated
until the guesses are sufficiently stable.
</p>
<p>A table of correlations between parameter estimates can be obtained via
the optional argument corr=T to summary (this also works for linear
models). It looks like this:
</p>
<p>Correlation of Coefficients:
</p>
<p>(Intercept) smokingYes obesityYes
</p>
<p>smokingYes -0.1520
</p>
<p>obesityYes -0.1361 -9.499e-05
</p>
<p>snoringYes -0.8965 -6.707e-02 -0.07186
</p>
<p>It is seen that the correlation between the estimates is fairly small, so that it
may be expected that removing a variable from themodel does not change
the coefficients and p-values for other variables much. (The correlations
between the regression coefficients and intercept are not very informative;
they mostly relate to whether the variable in question has many or few
observations in the &ldquo;Yes&rdquo; category.)
</p>
<p>The z test in the table of regression coefficients immediately shows that
the model can be simplified by removing smoking. The result then looks
as follows (abbreviated):
</p>
<p>&gt; glm.hyp &lt;- glm(hyp.tbl~obesity+snoring,binomial)
</p>
<p>&gt; summary(glm.hyp)
</p>
<p>...</p>
<p/>
</div>
<div class="page"><p/>
<p>234 13. Logistic regression
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -2.3921 0.3757 -6.366 1.94e-10 ***
</p>
<p>obesityYes 0.6954 0.2851 2.440 0.0147 *
</p>
<p>snoringYes 0.8655 0.3967 2.182 0.0291 *
</p>
<p>13.2.1 The analysis of deviance table
</p>
<p>Deviance tables correspond to ANOVA tables for multiple regression
analyses and are generated like these with the anova function:
</p>
<p>&gt; glm.hyp &lt;- glm(hyp.tbl~smoking+obesity+snoring,binomial)
</p>
<p>&gt; anova(glm.hyp, test="Chisq")
</p>
<p>Analysis of Deviance Table
</p>
<p>Model: binomial, link: logit
</p>
<p>Response: hyp.tbl
</p>
<p>Terms added sequentially (first to last)
</p>
<p>Df Deviance Resid. Df Resid. Dev P(&gt;|Chi|)
</p>
<p>NULL 7 14.1259
</p>
<p>smoking 1 0.0022 6 14.1237 0.9627
</p>
<p>obesity 1 6.8274 5 7.2963 0.0090
</p>
<p>snoring 1 5.6779 4 1.6184 0.0172
</p>
<p>Notice that the Deviance column gives differences between models as
variables are added to the model in turn. The deviances are approximately
χ2-distributed with the stated degrees of freedom. It is necessary to add
the test="chisq" argument to get the approximate χ2 tests.
</p>
<p>Since the snoring variable on the last line is significant, it may not be
removed from the model and we cannot use the table to justify model
reductions. If, however, the terms are rearranged so that smoking comes
last, we get a deviance-based test for removal of that variable:
</p>
<p>&gt; glm.hyp &lt;- glm(hyp.tbl~snoring+obesity+smoking,binomial)
</p>
<p>&gt; anova(glm.hyp, test="Chisq")
</p>
<p>...
</p>
<p>Df Deviance Resid. Df Resid. Dev P(&gt;|Chi|)
</p>
<p>NULL 7 14.1259
</p>
<p>snoring 1 6.7887 6 7.3372 0.0092
</p>
<p>obesity 1 5.6591 5 1.6781 0.0174
</p>
<p>smoking 1 0.0597 4 1.6184 0.8069
</p>
<p>From this you can read that smoking is removable, whereas obesity is
not, after removal of smoking.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Logistic regression on tabular data 235
</p>
<p>For good measure, you should also set up the analysis with the two re-
maining explanatory variables interchanged, so that you get a test of
whether snoring may be removed from a model that also contains
obesity:
</p>
<p>&gt; glm.hyp &lt;- glm(hyp.tbl~obesity+snoring,binomial)
</p>
<p>&gt; anova(glm.hyp, test="Chisq")
</p>
<p>...
</p>
<p>Df Deviance Resid. Df Resid. Dev P(&gt;|Chi|)
</p>
<p>NULL 7 14.1259
</p>
<p>obesity 1 6.8260 6 7.2999 0.0090
</p>
<p>snoring 1 5.6218 5 1.6781 0.0177
</p>
<p>An alternative method is to use drop1 to try removing one term at a time:
</p>
<p>&gt; drop1(glm.hyp, test="Chisq")
</p>
<p>Single term deletions
</p>
<p>Model:
</p>
<p>hyp.tbl ~ obesity + snoring
</p>
<p>Df Deviance AIC LRT Pr(Chi)
</p>
<p>&lt;none&gt; 1.678 32.597
</p>
<p>obesity 1 7.337 36.256 5.659 0.01737 *
</p>
<p>snoring 1 7.300 36.219 5.622 0.01774 *
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Here LRT is the likelihood ratio test, another name for the deviance
change.
</p>
<p>The information in the deviance tables is fundamentally the same as that
given by the z tests in the table of regression coefficients. The results may
differ due to the use of different approximations, though. From theoretical
considerations, the deviance test is preferable, but in practice the differ-
ence is often small because of the large-sample approximation χ2 &asymp; z2
for tests with a single degree of freedom. However, to test factors with
more than two categories, you have to use the deviance table because the
z tests only relate to some of the possible group comparisons. Also, the
small-sample situation requires special attention; see the next section.
</p>
<p>13.2.2 Connection to test for trend
</p>
<p>In Chapter 8, we considered tests for comparing relative frequencies using
prop.test and prop.trend.test, in particular the example of cae-
sarean section versus shoe size. This example can also be analyzed as a
logistic regression analysis on a &ldquo;shoe score&rdquo;, which &mdash; for want of a bet-
ter idea &mdash; may be chosen as the group number. This gives essentially the
same analysis in the sense that the same models are involved.</p>
<p/>
</div>
<div class="page"><p/>
<p>236 13. Logistic regression
</p>
<p>&gt; caesar.shoe
</p>
<p>&lt;4 4 4.5 5 5.5 6+
</p>
<p>Yes 5 7 6 7 8 10
</p>
<p>No 17 28 36 41 46 140
</p>
<p>&gt; shoe.score &lt;- 1:6
</p>
<p>&gt; shoe.score
</p>
<p>[1] 1 2 3 4 5 6
</p>
<p>&gt; summary(glm(t(caesar.shoe)~shoe.score,binomial))
</p>
<p>...
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -0.87058 0.40506 -2.149 0.03161 *
</p>
<p>shoe.score -0.25971 0.09361 -2.774 0.00553 **
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>(Dispersion parameter for binomial family taken to be 1)
</p>
<p>Null deviance: 9.3442 on 5 degrees of freedom
</p>
<p>Residual deviance: 1.7845 on 4 degrees of freedom
</p>
<p>AIC: 27.616
</p>
<p>...
</p>
<p>Notice that caesar.shoe had to be transposed with t(...), so that the
matrix was &ldquo;stood on its end&rdquo; in order to be used as the response variable
by glm.
</p>
<p>You can also write the results in a deviance table
</p>
<p>&gt; anova(glm(t(caesar.shoe)~shoe.score,binomial))
</p>
<p>...
</p>
<p>Df Deviance Resid. Df Resid. Dev
</p>
<p>NULL 5 9.3442
</p>
<p>shoe.score 1 7.5597 4 1.7845
</p>
<p>from the last line of which you see that there is no significant deviation
from linearity (1.78 on 4 degrees of freedom), whereas shoe.score has a
significant contribution.
</p>
<p>For comparison, the previous analyses using standard tests are repeated:
</p>
<p>&gt; caesar.shoe.yes &lt;- caesar.shoe["Yes",]
</p>
<p>&gt; caesar.shoe.no &lt;- caesar.shoe["No",]
</p>
<p>&gt; caesar.shoe.total &lt;- caesar.shoe.yes+caesar.shoe.no
</p>
<p>&gt; prop.trend.test(caesar.shoe.yes,caesar.shoe.total)
</p>
<p>Chi-squared Test for Trend in Proportions
</p>
<p>...
</p>
<p>X-squared = 8.0237, df = 1, p-value = 0.004617
</p>
<p>&gt; prop.test(caesar.shoe.yes,caesar.shoe.total)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Likelihood profiling 237
</p>
<p>6-sample test for equality of proportions without
</p>
<p>continuity correction
</p>
<p>...
</p>
<p>X-squared = 9.2874, df = 5, p-value = 0.09814
</p>
<p>...
</p>
<p>Warning message:
</p>
<p>In prop.test(caesar.shoe.yes, caesar.shoe.total) :
</p>
<p>Chi-squared approximation may be incorrect
</p>
<p>The 9.29 from prop.test corresponds to the 9.34 in residual deviance
from a NULL model, whereas the 8.02 in the trend test corresponds to
the 7.56 in the test of significance of shoe.score. Thus, the tests do not
give exactly the same result but generally almost the same. Theoretical
considerations indicate that the specialized trend test is probably slightly
better than the regression-based test. However, testing the linearity by
subtracting the two χ2 tests is definitely not as good as the real test for
linearity.
</p>
<p>13.3 Likelihood profiling
</p>
<p>The z tests in the summary output are based on the Wald approximation,
which calculates what the approximate standard error of the parameter
estimate would be if the true values of the parameters were equal to the es-
timates. In large data sets, this is fine because the result is nearly the same
for all parameter values that fit the data reasonably well. In smaller data
sets, however, the difference between the Wald tests and the likelihood
ratio test can be considerable.
</p>
<p>This also affects the calculation of confidence intervals since these are
based on inverting the tests, giving a set of parameter values that are
not rejected by a statistical test. As an alternative to the Wald-based
&plusmn;1.96 &times; s.e. technique, the MASS package allows you to compute inter-
vals that are based on inverting the likelihood ratio test. In practice, this
works like this
</p>
<p>&gt; confint(glm.hyp)
</p>
<p>Waiting for profiling to be done...
</p>
<p>2.5 % 97.5 %
</p>
<p>(Intercept) -3.2102369 -1.718143
</p>
<p>obesityYes 0.1254382 1.246788
</p>
<p>snoringYes 0.1410865 1.715860
</p>
<p>The standard type of result can be obtained using confint.default.
The difference in this case is not very large, although visible in the lines
relating to snoring and the intercept:</p>
<p/>
</div>
<div class="page"><p/>
<p>238 13. Logistic regression
</p>
<p>&minus;4.0 &minus;3.0 &minus;2.0 &minus;1.0
</p>
<p>&minus;
4
</p>
<p>&minus;
2
</p>
<p>0
2
</p>
<p>4
</p>
<p>(Intercept)
</p>
<p>ta
u
</p>
<p>&minus;0.5 0.0 0.5 1.0 1.5
</p>
<p>&minus;
4
</p>
<p>&minus;
2
</p>
<p>0
2
</p>
<p>obesityYes
</p>
<p>ta
u
</p>
<p>&minus;0.5 0.5 1.0 1.5 2.0 2.5
</p>
<p>&minus;
4
</p>
<p>&minus;
2
</p>
<p>0
2
</p>
<p>snoringYes
</p>
<p>ta
u
</p>
<p>Figure 13.1. Profile plot for hypertension model.
</p>
<p>&gt; confint.default(glm.hyp)
</p>
<p>2.5 % 97.5 %
</p>
<p>(Intercept) -3.12852108 -1.655631
</p>
<p>obesityYes 0.13670388 1.254134
</p>
<p>snoringYes 0.08801498 1.642902
</p>
<p>The way this works is via likelihood profiling. For a set of trial values of the
parameter, the likelihood is maximized over the other parameters in the
model. The result can be displayed in a profile plot as follows:
</p>
<p>&gt; library(MASS)
</p>
<p>&gt; plot(profile(glm.hyp))
</p>
<p>Notice that we need to load the MASS package at this point. (The function
was used by confint earlier on, but without putting it on the search
path.)
</p>
<p>The plots require a little explanation. The quantity on the y-axis, labelled
tau, is the signed square root of the likelihood ratio test.
</p>
<p>τ(β) = sgn(β&minus; β̂)
&radic;
</p>
<p>&minus;2(ℓ(β)&minus; ℓ(β̂))</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Presentation as odds-ratio estimates 239
</p>
<p>Here ℓ denotes the profile log-likelihood. The main idea is that when the
profile likelihood function is approximately quadratic, τ(β) is approxi-
mately linear. Conversely, likelihood functions not well approximated by
a quadratic show up as nonlinear profile plots.
</p>
<p>One important thing to notice, though, is that although the profiling
method will capture nonquadratic behaviour of the likelihood function,
confidence intervals based on the likelihood ratio test will always be
limited in accuracy by the approximation of the distribution of the test.
</p>
<p>13.4 Presentation as odds-ratio estimates
</p>
<p>In parts of the epidemiological literature, it has become traditional to
present logistic regression analyses in terms of odds ratios. In the case
of a quantitative covariate, this means odds ratio per unit change in the
covariate. That is, the antilogarithm (exp) of the regression coefficients is
given instead of the coefficients themselves. Since standard errors make
little sense after the transformation, it is also customary to give confidence
intervals instead. This can be obtained quite easily as follows:
</p>
<p>&gt; exp(cbind(OR=coef(glm.hyp), confint(glm.hyp)))
</p>
<p>Waiting for profiling to be done...
</p>
<p>OR 2.5 % 97.5 %
</p>
<p>(Intercept) 0.09143963 0.04034706 0.1793989
</p>
<p>obesityYes 2.00454846 1.13364514 3.4791490
</p>
<p>snoringYes 2.37609483 1.15152424 5.5614585
</p>
<p>The (Intercept) is really the odds of hypertension (for the not snoring
non-obese) and not an odds ratio.
</p>
<p>13.5 Logistic regression using raw data
</p>
<p>In this section, we again use Anders Juul&rsquo;s data (see p. 85). For easy ref-
erence, here is how to read data and convert the variables that describe
groupings into factors (this time slightly simplified):
</p>
<p>&gt; juul$menarche &lt;- factor(juul$menarche, labels=c("No","Yes"))
</p>
<p>&gt; juul$tanner &lt;- factor(juul$tanner)
</p>
<p>In the following, we look at menarche as the response variable. This vari-
able indicates for each girl whether or not she has had her first period. It
is coded 1 for &ldquo;no&rdquo; and 2 for &ldquo;yes&rdquo;. It is convenient to look at a subset of
data consisting of 8&ndash;20-year-old girls. This can be extracted as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>240 13. Logistic regression
</p>
<p>&gt; juul.girl &lt;- subset(juul,age&gt;8 &amp; age&lt;20 &amp;
</p>
<p>+ complete.cases(menarche))
</p>
<p>&gt; attach(juul.girl)
</p>
<p>For obvious reasons, no boys have a nonmissing menarche, so it is not
necessary to select on gender explicitly.
</p>
<p>Then you can analyze menarche as a function of age like this:
</p>
<p>&gt; summary(glm(menarche~age,binomial))
</p>
<p>Call:
</p>
<p>glm(formula = menarche ~ age, family = binomial)
</p>
<p>Deviance Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-2.32759 -0.18998 0.01253 0.12132 2.45922
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -20.0132 2.0284 -9.867 &lt;2e-16 ***
</p>
<p>age 1.5173 0.1544 9.829 &lt;2e-16 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>(Dispersion parameter for binomial family taken to be 1)
</p>
<p>Null deviance: 719.39 on 518 degrees of freedom
</p>
<p>Residual deviance: 200.66 on 517 degrees of freedom
</p>
<p>AIC: 204.66
</p>
<p>Number of Fisher Scoring iterations: 7
</p>
<p>The response variable menarche is a factor with two levels, where the last
level is considered the event. It also works to use a variable that has the
values 0 and 1 (but not, for instance, 1 and 2!).
</p>
<p>Notice that from this model you can estimate the median menarcheal age
as the age where logit p = 0. A little thought (solve &minus;20.0132 + 1.5173&times;
age = 0) reveals that it is 20.0132/1.5173 = 13.19 years.
</p>
<p>You should not pay too much attention to the deviance residuals in this
case since they automatically become large in every case where the fitted
probability &ldquo;goes against&rdquo; the observations (which is bound to happen in
some cases). The residual deviance is also difficult to interpret when there
is only one observation per cell.
</p>
<p>A hint of a more complicated analysis is obtained by including the Tan-
ner stage of puberty in the model. You should be warned that the exact
interpretation of such an analysis is quite tricky and qualitatively different
from the analysis of menarche as a function of age. It can be used for pre-
diction purposes (although asking the girl whether she has had her first</p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Prediction 241
</p>
<p>period would likely be much easier than determining her Tanner stage!),
but the interpretation of the terms is not clear-cut.
</p>
<p>&gt; summary(glm(menarche~age+tanner,binomial))
</p>
<p>...
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -13.7758 2.7630 -4.986 6.17e-07 ***
</p>
<p>age 0.8603 0.2311 3.723 0.000197 ***
</p>
<p>tanner2 -0.5211 1.4846 -0.351 0.725609
</p>
<p>tanner3 0.8264 1.2377 0.668 0.504313
</p>
<p>tanner4 2.5645 1.2172 2.107 0.035132 *
</p>
<p>tanner5 5.1897 1.4140 3.670 0.000242 ***
</p>
<p>...
</p>
<p>Notice that there is no joint test for the effect of tanner. There are a cou-
ple of significant z-values, so you would expect that the tanner variable
has some effect (which, of course, you would probably expect even in the
absence of data!). The formal test, however, must be obtained from the
deviances:
</p>
<p>&gt; drop1(glm(menarche~age+tanner,binomial),test="Chisq")
</p>
<p>...
</p>
<p>Df Deviance AIC LRT Pr(Chi)
</p>
<p>&lt;none&gt; 106.599 118.599
</p>
<p>age 1 124.500 134.500 17.901 2.327e-05 ***
</p>
<p>tanner 4 161.881 165.881 55.282 2.835e-11 ***
</p>
<p>...
</p>
<p>Clearly, both terms are highly significant.
</p>
<p>13.6 Prediction
</p>
<p>The predict function works for generalized linear models, too. Let us
first consider the hypertension example, where data were given in tabular
form:
</p>
<p>&gt; predict(glm.hyp)
</p>
<p>1 2 3 4 5 6
</p>
<p>-2.3920763 -2.3920763 -1.6966575 -1.6966575 -1.5266180 -1.5266180
</p>
<p>7 8
</p>
<p>-0.8311991 -0.8311991
</p>
<p>Recall that smoking was eliminated from the model, which is why the
expected values come in identical pairs.
</p>
<p>These numbers are on the logit scale, which reveals the additive structure.
Notice that 2.392&minus; 1.697 = 1.527&minus; 0.831 = 0.695 (except for roundoff er-</p>
<p/>
</div>
<div class="page"><p/>
<p>242 13. Logistic regression
</p>
<p>ror), which is exactly the regression coefficient to obesity. Likewise, the
regression coefficient to snoring is obtained by looking at the differences
2.392&minus; 1.527 = 1.697&minus; 0.831 = 0.866.
To get predicted values on the response scale (i.e., probabilities), use the
type="response" argument to predict:
</p>
<p>&gt; predict(glm.hyp, type="response")
</p>
<p>1 2 3 4 5 6
</p>
<p>0.08377892 0.08377892 0.15490233 0.15490233 0.17848906 0.17848906
</p>
<p>7 8
</p>
<p>0.30339158 0.30339158
</p>
<p>These may also be obtained using fitted, although you then cannot use
the techniques for predicting on new data, etc.
</p>
<p>In the analysis of menarche, the primary interest is probably in seeing a
plot of the expected probabilities versus age (Figure 13.2). A crude plot
could be obtained using something like
</p>
<p>plot(age, fitted(glm(menarche~age,binomial)))
</p>
<p>(it will look better if a different plotting symbol in a smaller size, using the
pch and cex arguments, is used) but here is a more ambitious plan:
</p>
<p>&gt; glm.menarche &lt;- glm(menarche~age, binomial)
</p>
<p>&gt; Age &lt;- seq(8,20,.1)
</p>
<p>&gt; newages &lt;- data.frame(age=Age)
</p>
<p>&gt; predicted.probability &lt;- predict(glm.menarche,
</p>
<p>+ newages,type="resp")
</p>
<p>&gt; plot(predicted.probability ~ Age, type="l")
</p>
<p>This is Figure 13.2. Recall that seq generates equispaced vectors, here ages
from 8 to 20 in steps of 0.1, so that connecting the points with lines will
give a nearly smooth curve.
</p>
<p>13.7 Model checking
</p>
<p>For tabular data it is obvious to try to compare observed and fitted
proportions. In the hypertension example you get
</p>
<p>&gt; fitted(glm.hyp)
</p>
<p>1 2 3 4 5 6
</p>
<p>0.08377892 0.08377892 0.15490233 0.15490233 0.17848906 0.17848906
</p>
<p>7 8
</p>
<p>0.30339158 0.30339158
</p>
<p>&gt; prop.hyp</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Model checking 243
</p>
<p>8 10 12 14 16 18 20
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>Age
</p>
<p>p
re
</p>
<p>d
ic
</p>
<p>te
d
.p
</p>
<p>ro
b
a
b
ili
</p>
<p>ty
</p>
<p>Figure 13.2. Fitted probability of menarche having occurred.
</p>
<p>[1] 0.08333333 0.11764706 0.12500000 0.00000000 0.18716578
</p>
<p>[6] 0.15294118 0.29411765 0.34782609
</p>
<p>The problem with this is that you get no feeling for how well the rela-
tive frequencies are determined. It can be better to look at observed and
expected counts instead. The former can be computed as
</p>
<p>&gt; fitted(glm.hyp)*n.tot
</p>
<p>1 2 3 4 5 6
</p>
<p>5.0267351 1.4242416 1.2392186 0.3098047 33.3774535 15.1715698
</p>
<p>7 8
</p>
<p>15.4729705 6.9780063
</p>
<p>and to get a nice print for the comparison, you can use
</p>
<p>&gt; data.frame(fit=fitted(glm.hyp)*n.tot,n.hyp,n.tot)
</p>
<p>fit n.hyp n.tot
</p>
<p>1 5.0267351 5 60
</p>
<p>2 1.4242416 2 17
</p>
<p>3 1.2392186 1 8
</p>
<p>4 0.3098047 0 2
</p>
<p>5 33.3774535 35 187
</p>
<p>6 15.1715698 13 85</p>
<p/>
</div>
<div class="page"><p/>
<p>244 13. Logistic regression
</p>
<p>8 10 12 14 16 18 20
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>Age
</p>
<p>p
re
</p>
<p>d
ic
</p>
<p>te
d
.p
</p>
<p>ro
b
a
b
ili
</p>
<p>ty
</p>
<p>Figure 13.3. Fitted probability for menarche having occurred and observed
proportion in age groups.
</p>
<p>7 15.4729705 15 51
</p>
<p>8 6.9780063 8 23
</p>
<p>Notice that the discrepancy in cell 4 between 15% expected and 0% ob-
served really is that there are 0 hypertensives out of 2 in a cell where the
model yields an expectation of 0.3 hypertensives!
</p>
<p>For complex models with continuous background variables, it becomes
more difficult to perform an adequate model check. It is especially a
hindrance that nothing really corresponds to a residual plot when the
observations have only two different values.
</p>
<p>Consider the example of the probability of menarche as a function of age.
The problem here is whether the relation can really be assumed linear on
the logit scale. For this case, you might try subdividing the x-axis in a
number of intervals and see how the counts in each interval fit with the
expected probabilities. This is presented graphically in Figure 13.3. Notice
that the code adds points to Figure 13.2, which you are assumed not to
have deleted at this point.
</p>
<p>&gt; age.group &lt;- cut(age,c(8,10,12,13,14,15,16,18,20))</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Model checking 245
</p>
<p>&gt; tb &lt;- table(age.group,menarche)
</p>
<p>&gt; tb
</p>
<p>menarche
</p>
<p>age.group No Yes
</p>
<p>(8,10] 100 0
</p>
<p>(10,12] 97 4
</p>
<p>(12,13] 32 21
</p>
<p>(13,14] 22 20
</p>
<p>(14,15] 5 36
</p>
<p>(15,16] 0 31
</p>
<p>(16,18] 0 105
</p>
<p>(18,20] 0 46
</p>
<p>&gt; rel.freq &lt;- prop.table(tb,1)[,2]
</p>
<p>&gt; rel.freq
</p>
<p>(8,10] (10,12] (12,13] (13,14] (14,15] (15,16]
</p>
<p>0.00000000 0.03960396 0.39622642 0.47619048 0.87804878 1.00000000
</p>
<p>(16,18] (18,20]
</p>
<p>1.00000000 1.00000000
</p>
<p>&gt; points(rel.freq ~ c(9,11,12.5,13.5,14.5,15.5,17,19),pch=5)
</p>
<p>The technique used above probably requires some explanation. First, cut
is used to define the factor age.group, which describes a grouping
into age intervals. Then a crosstable tb is formed from menarche and
age.group. Using prop.table, the numbers are expressed relative to
the row total, and column 2 of the resulting table is extracted. This con-
tains the relative proportion in each age group of girls for whommenarche
has occurred. Finally, a plot of expected probabilities is made, overlaid by
the observed proportions.
</p>
<p>The plot looks reasonable on the whole, although the observed proportion
among 12&ndash;13-year-olds appears a bit high and the proportion among 13&ndash;
14-year-olds is a bit too low.
</p>
<p>But how do you evaluate whether the deviation is larger than what can
be expected from the statistical variation? One thing to try is to extend the
model with a factor that describes a division into intervals. It is not prac-
tical to use the full division of age.group because there are cells where
either none or all of the girls have had their menarche.
</p>
<p>We therefore try a division into four groups, with cutpoints at 12, 13, and
14 years, and add this factor to the model containing a linear age effect.
</p>
<p>&gt; age.gr &lt;- cut(age,c(8,12,13,14,20))
</p>
<p>&gt; summary(glm(menarche~age+age.gr,binomial))
</p>
<p>...
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -21.5683 5.0645 -4.259 2.06e-05 ***
</p>
<p>age 1.6250 0.4416 3.680 0.000233 ***
</p>
<p>age.gr(12,13] 0.7296 0.7856 0.929 0.353024
</p>
<p>age.gr(13,14] -0.5219 1.1184 -0.467 0.640765</p>
<p/>
</div>
<div class="page"><p/>
<p>246 13. Logistic regression
</p>
<p>8 10 12 14 16 18 20
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>Age
</p>
<p>p
re
</p>
<p>d
ic
</p>
<p>te
d
.p
</p>
<p>ro
b
a
b
ili
</p>
<p>ty
</p>
<p>Figure 13.4. Logit-cubical fit of menarche data.
</p>
<p>age.gr(14,20] 0.2751 1.6065 0.171 0.864053
</p>
<p>...
</p>
<p>&gt; anova(glm(menarche~age+age.gr,binomial))
</p>
<p>...
</p>
<p>Df Deviance Resid. Df Resid. Dev
</p>
<p>NULL 518 719.39
</p>
<p>age 1 518.73 517 200.66
</p>
<p>age.gr 3 8.06 514 192.61
</p>
<p>&gt; 1-pchisq(8.058,3)
</p>
<p>[1] 0.04482811
</p>
<p>That is, the addition of the grouping actually does give a significantly
better deviance. The effect is not highly significant, but since the devia-
tion concerns the ages where &ldquo;much happens&rdquo;, you should probably be
cautious about postulating a logit-linear age effect.
</p>
<p>Another possibility is to try a polynomial regression model. Here you
need at least a third-degree polynomial to describe the apparent stagna-
tion of the curve around 13 years of age. We do not look at this in great
detail, but just show part of the output and in Figure 13.4 a graphical
presentation of the model.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8 Exercises 247
</p>
<p>&gt; anova(glm(menarche~age+I(age^2)+I(age^3)+age.gr,binomial))
</p>
<p>...
</p>
<p>Df Deviance Resid. Df Resid. Dev
</p>
<p>NULL 518 719.39
</p>
<p>age 1 518.73 517 200.66
</p>
<p>I(age^2) 1 0.05 516 200.61
</p>
<p>I(age^3) 1 8.82 515 191.80
</p>
<p>age.gr 3 3.34 512 188.46
</p>
<p>Warning messages:
</p>
<p>1: In glm.fit(x = X, y = Y, weights = weights, .... :
</p>
<p>fitted probabilities numerically 0 or 1 occurred
</p>
<p>2: In method(x = x[, varseq &lt;= i, drop = FALSE], .... :
</p>
<p>fitted probabilities numerically 0 or 1 occurred
</p>
<p>&gt; glm.menarche &lt;- glm(menarche~age+I(age^2)+I(age^3), binomial)
</p>
<p>Warning message:
</p>
<p>In glm.fit(x = X, y = Y, weights = weights, start = start, .... :
</p>
<p>fitted probabilities numerically 0 or 1 occurred
</p>
<p>&gt; predicted.probability &lt;-
</p>
<p>+ predict(glm.menarche, newages, type="resp")
</p>
<p>&gt; plot(predicted.probability ~ Age, type="l")
</p>
<p>&gt; points(rel.freq~c(9,11,12.5,13.5,14.5,15.5,17,19), pch=5)
</p>
<p>The warnings about fitted probabilities of 0 or 1 occur because the cubic
term makes the logit tend much faster to &plusmn;&infin; than the linear model did.
There are two occurrences for the anova call because two of the models
include the cubic term.
</p>
<p>The thing to note in the deviance table is that the cubic term gives a sub-
stantial improvement of the deviance, but once that is included, the age
grouping gives no additional improvement. The plot should speak for
itself.
</p>
<p>13.8 Exercises
</p>
<p>13.1 In the malaria data set, analyze the risk of malaria with age and
log-transformed antibody level as explanatory variables.
</p>
<p>13.2 Fit a logistic regression model to the graft.vs.host data set, pre-
dicting the gvhd response. Use different transformations of the index
variable. Reduce the model using backwards elimination.
</p>
<p>13.3 In the analyses of the malaria and graft.vs.host data, try us-
ing the confint function to find improved confidence intervals for the
regression coefficients.
</p>
<p>13.4 Following up on Exercise 8.2 about &ldquo;Rocky Mountain spotted
fever&rdquo;, splitting the data by age groups gives the table below. Does this</p>
<p/>
</div>
<div class="page"><p/>
<p>248 13. Logistic regression
</p>
<p>confirm the earlier analysis?
</p>
<p>Western Type Eastern Type
Age Group Total Fatal Total Fatal
Under 15 108 13 310 40
15&ndash;39 264 40 189 21
40 or above 375 157 162 61
</p>
<p>747 210 661 122
</p>
<p>13.5 A probit regression is just like a logistic regression but uses a differ-
ent link function. Try the analysis of the menarche variable in the juul
data set with this link. Does the fit improve?</p>
<p/>
</div>
<div class="page"><p/>
<p>14
Survival analysis
</p>
<p>The analysis of lifetimes is an important topic within biology and
medicine in particular but also in reliability analysis with engineering ap-
plications. Such data are often highly nonnormally distributed, so that the
use of standard linear models is problematic.
</p>
<p>Lifetime data are often censored: You do not know the exact lifetime, only
that it is longer than a given value. For instance, in a cancer trial, some
people are lost to follow-up or simply live beyond the study period. It
is an error to ignore the censoring in the statistical analysis, sometimes
with extreme consequences. Consider, for instance, the case where a new
treatment is introduced towards the end of the study period, so that nearly
all the observed lifetimes will be cut short.
</p>
<p>14.1 Essential concepts
</p>
<p>Let X be the true lifetime and T a censoring time. What you observe is
the minimum of X and T together with an indication of whether it is one
or the other. T can be a random variable or a fixed time depending on
context, but if it is random, then it should generally be noninformative for
the methods we describe here to be applicable. Sometimes &ldquo;dead from
other causes&rdquo; is considered a censoring event for the mortality of a given
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_14, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>250 14. Survival analysis
</p>
<p>disease, and in those cases it is particularly important to ensure that these
other causes are unassociated with the disease state.
</p>
<p>The survival function S(t) measures the probability of being alive at a given
time. It is really just 1 minus the cumulative distribution function for X,
1&minus; F(t).
The hazard function or force of mortality h(t) measures the (infinitesimal)
risk of dying within a short interval of time t, given that the subject is alive
at time t. If the lifetime distribution has density f , then h(t) = f (t)/S(t).
This is often considered a more fundamental quantity than (say) the mean
or median of the survival distribution and is used as a basis for modelling.
</p>
<p>14.2 Survival objects
</p>
<p>We use the package survival, written by Terry Therneau and ported
to R by Thomas Lumley. The package implements a large number of ad-
vanced techniques. For the present purposes, we use only a small subset
of it.
</p>
<p>To load survival, use
</p>
<p>&gt; library(survival)
</p>
<p>(This may produce a harmless warning about masking the lung data set
from the ISwR package.)
</p>
<p>The routines in survival work with objects of class "Surv", which is
a data structure that combines times and censoring information. Such ob-
jects are constructed using the Surv function, which takes two arguments:
an observation time and an event indicator. The latter can be coded as a
logical variable, a 0/1 variable, or a 1/2 variable. The latter coding is not
recommended since Survwill assume 0/1 coding if all values are 1.
</p>
<p>Actually, Surv can also be used with three arguments for dealing with
data that have a start time as well as an end time (&ldquo;staggered entry&rdquo;) and
also interval censored data (where you know that an event happened be-
tween two dates, as happens, for instance, in repeated testing for a disease)
can be handled.
</p>
<p>We use the data set melanom collected by K. T. Drzewiecki and re-
produced in Andersen et al. (1991). The data become accessible as
follows:
</p>
<p>&gt; attach(melanom)
</p>
<p>&gt; names(melanom)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Kaplan&ndash;Meier estimates 251
</p>
<p>[1] "no" "status" "days" "ulc" "thick" "sex"
</p>
<p>The variable status is an indicator of the patient&rsquo;s status by the end of
the study: 1 means &ldquo;dead from malignant melanoma&rdquo;, 2 means &ldquo;alive on
January 1, 1978&rdquo;, and 3 means &ldquo;dead from other causes&rdquo;. The variable
days is the observation time in days, ulc indicates (1 for present and 2
for absent) whether the tumor was ulcerated, thick is the thickness in
1/100 mm, and sex contains the gender of the patient (1 for women and
2 for men).
</p>
<p>We want to create a Surv object in which we consider the values 2 and 3
of the status variable as censorings. This is done as follows:
</p>
<p>&gt; Surv(days, status==1)
</p>
<p>[1] 10+ 30+ 35+ 99+ 185 204 210 232 232+ 279
</p>
<p>[11] 295 355+ 386 426 469 493+ 529 621 629 659
</p>
<p>[21] 667 718 752 779 793 817 826+ 833 858 869
</p>
<p>...
</p>
<p>[181] 3476+ 3523+ 3667+ 3695+ 3695+ 3776+ 3776+ 3830+ 3856+ 3872+
</p>
<p>[191] 3909+ 3968+ 4001+ 4103+ 4119+ 4124+ 4207+ 4310+ 4390+ 4479+
</p>
<p>[201] 4492+ 4668+ 4688+ 4926+ 5565+
</p>
<p>Associated with the Surv objects is a print method that displays the ob-
jects in the format above, with a &lsquo;+&rsquo; marking censored observations. For
example, 10+ means that the patient did not die from melanoma within
10 days and was then unavailable for further study (in fact, he died from
other causes), whereas 185means that the patient died from the disease a
little over half a year after his operation.
</p>
<p>Notice that the second argument to Surv is a logical vector; status==1
is TRUE for those who died of malignant melanoma and FALSE otherwise.
</p>
<p>14.3 Kaplan&ndash;Meier estimates
</p>
<p>The Kaplan&ndash;Meier estimator allows the computation of an estimated sur-
vival function in the presence of right-censoring. It is also called the
product-limit estimator because one way of describing the procedure is
that it multiplies together conditional survival curves for intervals in
which there are either no censored observations or no deaths. This be-
comes a step function where the estimated survival is reduced by a factor
(1&minus; 1/Rt) if there is a death at time t and a population of Rt is still alive
and uncensored at that time.
</p>
<p>Computing the Kaplan&ndash;Meier estimator for the survival function is done
with a function called survfit. In its simplest form, it takes just a single</p>
<p/>
</div>
<div class="page"><p/>
<p>252 14. Survival analysis
</p>
<p>argument, namely a Surv object. It returns a survfit object. As de-
scribed above, we consider &ldquo;dead from other causes&rdquo; a kind of censoring
and do as follows:
</p>
<p>&gt; survfit(Surv(days,status==1))
</p>
<p>Call: survfit(formula = Surv(days, status == 1))
</p>
<p>n events median 0.95LCL 0.95UCL
</p>
<p>205 57 Inf Inf Inf
</p>
<p>As is seen, using survfit by itself is not very informative (just as the
printed output of a &ldquo;bare&rdquo; lm is not). You get a couple of summary statis-
tics and an estimate of the median survival, and in this case the latter is
not even interesting because the estimate is infinite. The survival curve
does not cross the 50% mark before all patients are censored.
</p>
<p>To see the actual Kaplan&ndash;Meier estimate, use summary on the survfit
object. We first save the survfit object into a variable, here named
surv.all because it contains the raw survival function for all patients
without regard to patient characteristics.
</p>
<p>&gt; surv.all &lt;- survfit(Surv(days,status==1))
</p>
<p>&gt; summary(surv.all)
</p>
<p>Call: survfit(formula = Surv(days, status == 1))
</p>
<p>time n.risk n.event survival std.err lower 95% CI upper 95% CI
</p>
<p>185 201 1 0.995 0.00496 0.985 1.000
</p>
<p>204 200 1 0.990 0.00700 0.976 1.000
</p>
<p>210 199 1 0.985 0.00855 0.968 1.000
</p>
<p>232 198 1 0.980 0.00985 0.961 1.000
</p>
<p>279 196 1 0.975 0.01100 0.954 0.997
</p>
<p>295 195 1 0.970 0.01202 0.947 0.994
</p>
<p>...
</p>
<p>2565 63 1 0.689 0.03729 0.620 0.766
</p>
<p>2782 57 1 0.677 0.03854 0.605 0.757
</p>
<p>3042 52 1 0.664 0.03994 0.590 0.747
</p>
<p>3338 35 1 0.645 0.04307 0.566 0.735
</p>
<p>This contains the values of the survival function at the event times. The
censoring times are not displayed but are contained in the survfit object
and can be obtained by passing censored=T to summary (see the help
page for summary.survfit for such details).
</p>
<p>The Kaplan&ndash;Meier estimate is the step function whose jump points are
given in time andwhose values right after a jump are given in survival.
Additionally, both an estimate of the standard error of the curve and a
(pointwise) confidence interval for the true curve are given.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Kaplan&ndash;Meier estimates 253
</p>
<p>0 1000 2000 3000 4000 5000
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>Figure 14.1. Kaplan&ndash;Meier plot for melanoma data (all observations).
</p>
<p>Normally, you would be more interested in showing the Kaplan&ndash;Meier
estimate graphically than numerically. To do this (Figure 14.1), you simply
write
</p>
<p>&gt; plot(surv.all)
</p>
<p>The markings on the curve indicate censoring times, and the bands give
approximate confidence intervals. If you look closely, you will see that the
bands are not symmetrical around the estimate. They are constructed as a
symmetric interval on the log scale and transformed back to the original
scale.
</p>
<p>It is often useful to plot two or more survival functions on the same plot
so that they can be directly compared (Figure 14.2). To obtain survival
functions split by gender, do the following:
</p>
<p>&gt; surv.bysex &lt;- survfit(Surv(days,status==1)~sex)
</p>
<p>&gt; plot(surv.bysex)
</p>
<p>That is, you use a model formula as in lm and glm, specifying that the
survival object generated from day and status should be described by
sex. Notice that there are no confidence intervals on the curves. These are</p>
<p/>
</div>
<div class="page"><p/>
<p>254 14. Survival analysis
</p>
<p>0 1000 2000 3000 4000 5000
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>Figure 14.2. Kaplan&ndash;Meier plots for melanoma data, grouped by gender.
</p>
<p>turned off when there are two or more curves because the display easily
becomes confusing. They can be turned on again by passing conf.int=T
to plot, in which case it can be recommended to use separate colours for
the curves, as in
</p>
<p>&gt; plot(surv.bysex, conf.int=T, col=c("black","gray"))
</p>
<p>Similarly, you can avoid plotting the confidence bands in the single-
sample case by setting conf.int=F. If you want the bands but at a 99%
confidence level, you should pass conf.int=0.99 to survfit. Notice
that the level of confidence is an argument to the fitting function (which
needs it to compute the confidence limits), whereas the decision to plot
the bands is controlled by a similarly named argument to plot.
</p>
<p>14.4 The log-rank test
</p>
<p>The log-rank test is used to test whether two or more survival curves are
identical. It is based on looking at the population at each death time and
computing the expected number of deaths in proportion to the number of</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The log-rank test 255
</p>
<p>individuals at risk in each group. This is then summed over all death times
and compared with the observed number of deaths by a procedure similar
(but not identical) to the χ2 test. Notice that the interpretation of &ldquo;ex-
pected&rdquo; and &ldquo;observed&rdquo; is slightly peculiar: If the difference in mortality
is sufficiently large, then you can easily &ldquo;expect&rdquo; the same individuals to
die several times over the course of the trial. If the population is observed
to extinction with no censoring, then the observed number of deaths will
equal the group size by definition and the expected values will contain all
the random variation.
</p>
<p>The log-rank test is formally nonparametric since the distribution of the
test statistic depends only on the assumption that the groups have the
same survival function. However, it can also be viewed as a model-based
test under the assumption of proportional hazards (see Section 14.1). You can
set up a semiparametric model in which the hazard itself is unspecified
but it is assumed that the hazards are proportional between groups. Test-
ing that the proportionality factors are all unity then leads to a log-rank
test. The log-rank test will work best against this class of alternatives.
</p>
<p>Computation of the log-rank test is done by the function survdiff.
This actually implements a whole family of tests specified by a param-
eter ρ, allowing various nonproportional hazards alternatives to the null
hypothesis, but the default value of ρ = 0 gives the log-rank test.
</p>
<p>&gt; survdiff(Surv(days,status==1)~sex)
</p>
<p>Call:
</p>
<p>survdiff(formula = Surv(days, status == 1) ~ sex)
</p>
<p>N Observed Expected (O-E)^2/E (O-E)^2/V
</p>
<p>sex=1 126 28 37.1 2.25 6.47
</p>
<p>sex=2 79 29 19.9 4.21 6.47
</p>
<p>Chisq= 6.5 on 1 degrees of freedom, p= 0.011
</p>
<p>The specification is using a model formula as for linear and generalized
linear models. However, the test can deal only with grouped data, so if
you specify multiple variables on the right-hand side it will work on the
grouping of data generated by all combinations of predictor variables. It
also makes no distinction between factors and numerical codes. The same
is true of survfit.
</p>
<p>It is also possible to specify stratified analyses, in which the observed and
expected value calculations are carried out separately within a stratifica-
tion of the data set. For instance, you can compute the log-rank test for a
gender effect stratified by ulceration as follows:
</p>
<p>&gt; survdiff(Surv(days,status==1)~sex+strata(ulc))
</p>
<p>Call:</p>
<p/>
</div>
<div class="page"><p/>
<p>256 14. Survival analysis
</p>
<p>survdiff(formula = Surv(days, status == 1) ~ sex + strata(ulc))
</p>
<p>N Observed Expected (O-E)^2/E (O-E)^2/V
</p>
<p>sex=1 126 28 34.7 1.28 3.31
</p>
<p>sex=2 79 29 22.3 1.99 3.31
</p>
<p>Chisq= 3.3 on 1 degrees of freedom, p= 0.0687
</p>
<p>Notice that this makes the effect of sex appear less significant. A possible
explanation might be that males seek treatment when the disease is in
a more advanced state than women do, so that the gender difference is
reduced when adjusted for a measure of disease progression.
</p>
<p>14.5 The Cox proportional hazards model
</p>
<p>The proportional hazards model allows the analysis of survival data by
regression models similar to those of lm and glm. The scale on which
linearity is assumed is the log-hazard scale. Models can be fitted via the
maximization of Cox&rsquo;s likelihood, which is not a true likelihood but it can
be shown that it may be used as one. It is calculated in a manner similar
to that of the log-rank test, as the product of conditional likelihoods of the
observed death at each death time.
</p>
<p>As a first example, consider a model with the single regressor sex:
</p>
<p>&gt; summary(coxph(Surv(days,status==1)~sex))
</p>
<p>Call:
</p>
<p>coxph(formula = Surv(days, status == 1) ~ sex)
</p>
<p>n= 205
</p>
<p>coef exp(coef) se(coef) z p
</p>
<p>sex 0.662 1.94 0.265 2.5 0.013
</p>
<p>exp(coef) exp(-coef) lower .95 upper .95
</p>
<p>sex 1.94 0.516 1.15 3.26
</p>
<p>Rsquare= 0.03 (max possible= 0.937 )
</p>
<p>Likelihood ratio test= 6.15 on 1 df, p=0.0131
</p>
<p>Wald test = 6.24 on 1 df, p=0.0125
</p>
<p>Score (logrank) test = 6.47 on 1 df, p=0.0110
</p>
<p>The coef is the estimated logarithm of the hazard ratio between the two
groups, which for convenience is also given as the actual hazard ratio
exp(coef). The line following that also gives the inverted ratio (swap-
ping the groups) and confidence intervals for the hazard ratio. Finally,
three overall tests for significant effects in the model are given. These are
all equivalent in large samples but may differ somewhat in small-sample</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 The Cox proportional hazards model 257
</p>
<p>cases. Notice that the Wald test is identical to the z test based on the es-
timated coefficient divided by its standard error, whereas the score test is
equivalent to the log-rank test (as long as themodel involves only a simple
grouping).
</p>
<p>A more elaborate example, involving a continuous covariate and a
stratification variable, is
</p>
<p>&gt; summary(coxph(Surv(days,status==1)~sex+log(thick)+strata(ulc)))
</p>
<p>Call:
</p>
<p>coxph(formula = Surv(days, status == 1) ~ sex + log(thick) +
</p>
<p>strata(ulc))
</p>
<p>n= 205
</p>
<p>coef exp(coef) se(coef) z p
</p>
<p>sex 0.36 1.43 0.270 1.33 0.1800
</p>
<p>log(thick) 0.56 1.75 0.178 3.14 0.0017
</p>
<p>exp(coef) exp(-coef) lower .95 upper .95
</p>
<p>sex 1.43 0.698 0.844 2.43
</p>
<p>log(thick) 1.75 0.571 1.234 2.48
</p>
<p>Rsquare= 0.063 (max possible= 0.9 )
</p>
<p>Likelihood ratio test= 13.3 on 2 df, p=0.00130
</p>
<p>Wald test = 12.9 on 2 df, p=0.00160
</p>
<p>Score (logrank) test = 13.0 on 2 df, p=0.00152
</p>
<p>It is seen that the significance of the sex variable has been further reduced.
</p>
<p>The Cox model assumes an underlying baseline hazard function with a
corresponding survival curve. In a stratified analysis, there will be one
such curve for each stratum. They can be extracted by using survfit on
the output of coxph and of course be plotted using the plot method for
survfit objects (Figure 14.3):
</p>
<p>&gt; plot(survfit(coxph(Surv(days,status==1)~
</p>
<p>+ log(thick)+sex+strata(ulc))))
</p>
<p>Be aware that the default for survfit is to generate curves for a pseudo-
individual for which the covariates are at their mean values. In the present
case, that would correspond to a tumor thickness of 1.86 mm and a gen-
der of 1.39 (!). Notice that we have been sloppy in not defining sex as a
factor variable, but that would not actually give a different result (coxph
subtracts the means of the regressors before fitting, so a 1/2 coding is the
same as 0/1, which is what a factor with treatment contrasts gives you).
However, you can use the newdata argument of survfit to specify a
data frame for which you want to calculate survival curves.</p>
<p/>
</div>
<div class="page"><p/>
<p>258 14. Survival analysis
</p>
<p>0 500 1000 1500 2000 2500 3000
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>Figure 14.3. Baseline survival curves (ulcerated and nonulcerated tumors) in
stratified Cox regression.
</p>
<p>14.6 Exercises
</p>
<p>14.1 In the graft.vs.host data set, estimate the survival function for
patients with or without GVHD. Test the hypothesis that the survival
is the same in both groups. Extend the analysis by including the other
explanatory variables.
</p>
<p>14.2 With the Cox model in the last section of the text, generate a plot
with estimated survival curves for men with nonulcerated tumors of
thicknesses 0.1, 0.2, and 0.5 mm (three curves in one plot). Hint: survfit
objects can be indexed with [] to extract individual strata.
</p>
<p>14.3 Fit Cox models to the stroke data with age and sex as predictors
and with sex alone. Explain the difference.
</p>
<p>14.4 With the split data from Exercise 10.4, you can fit a Cox model with
delayed entry to the stroke data; help(Surv) shows how to set up the
Surv object in that case. Refit the model(s) from the previous exercise.</p>
<p/>
</div>
<div class="page"><p/>
<p>15
Rates and Poisson regression
</p>
<p>Epidemiological studies often involve the calculation of rates, typically
rates of death or incidence rates of a chronic or acute disease. This is
based upon counts of events occurring within a certain amount of time.
The Poisson regression method is often employed for the statistical analy-
sis of such data. However, data that are not actually counts of events but
rather measurements of time until an event (or nonevent) can be analyzed
by a technique which is formally equivalent.
</p>
<p>15.1 Basic ideas
</p>
<p>The data that we wish to analyze can be in one of two forms. They can
be in aggregate form as an observed count x based on a number of person-
years T. Often the latter is an approximation based on tables of population
size. There may of course be more than one group, and we may wish to
formulate various models describing the rates in different groups.
</p>
<p>Wemay also have individual-level data, in which for each subject we have
a time under observation Ti and a 0/1 indicator xi of whether the subject
has had an event. The aggregate data can be thought of as being x = &sum; xi
and T = &sum; Ti, where the sums are over all individuals in the group.
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_15, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>260 15. Rates and Poisson regression
</p>
<p>15.1.1 The Poisson distribution
</p>
<p>The Poisson distribution can be described as the limiting case of the bino-
mial distributions when the size parameter N increases while the expected
number of successes λ = Np is fixed. This is useful to describe rare event
in large populations. The resulting distribution has point probabilities
</p>
<p>f (x) =
λx
</p>
<p>x!
e&minus;λ x = 0, 1, . . .
</p>
<p>The distribution is theoretically unbounded, although the probabilities for
large xwill be very small. In R, the Poisson distribution is available via the
functions dpois, ppois, etc.
</p>
<p>In the context of epidemiological data, the parameter of interest is usually
the expected counts per unit of observed time; i.e., the rate at which events
occur. This enables the comparison of populations that may be of differ-
ent size or observed for different lengths of time. Accordingly, we may
parameterize the Poisson distribution using
</p>
<p>ρ = λ/T
</p>
<p>Notice that parts of the literature use λ to denote the rate. The notation
used here is chosen so as to stay compatible with the argument name in
dpois.
</p>
<p>The Poisson likelihood
</p>
<p>Models for Poisson data can be fitted by the method of maximum
likelihood. If we parameterize in terms of ρ, the log-likelihood becomes
</p>
<p>l(ρ) = constant+ x log ρ&minus; ρT
which is maximized when ρ = x/T. The log-likelihood can be generalized
to models involving several counts by summing terms of the same form.
</p>
<p>15.1.2 Survival analysis with constant hazard
</p>
<p>In this section, for convenience, we use terminology appropriate for mor-
tality studies, although the event may bemany things other than the death
of the subject.
</p>
<p>Individual-level data are essentially survival data as described in Chap-
ter 14, except for changes in notation. One difference, though, is that in
the analysis of rates it is often reasonable to assume that the hazard does
not change over time, or at least not abruptly so. Rates tend to be obtained
over rather short individual time periods, and the origin of the timescale</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Basic ideas 261
</p>
<p>is not usually keyed to a life-changing event such as disease onset or major
surgery.
</p>
<p>If the hazard is constant, then the distribution of the lifetime is the
exponential distributionwith density ρe&minus;ρt and survival function e&minus;ρt.
</p>
<p>Likelihood analysis
</p>
<p>Likelihoods for censored data can be constructed using terms that are
either the probability density at the time of death or the survival prob-
ability in the case of censoring. In the constant-hazard case, the two kinds
of terms differ only in the presence of the factor ρ, which we may conve-
niently encode using the event indicator xi so that the log-likelihood terms
are
</p>
<p>l(ρ) = xi log ρ&minus; ρTi
</p>
<p>Except for the constant, which does not depend on ρ, these terms are for-
mally identical to a Poisson likelihood, where the count is 1 (death) or
zero (censoring). This is the crucial &ldquo;trick&rdquo; that allows survival data with
constant hazard to be analyzed by Poisson regression methods.
</p>
<p>The trick can be extended to hazards that are only piecewise constant.
</p>
<p>Suppose the lifetime of an individual is subdivided as Ti = T
(1)
i + &middot; &middot; &middot; +
</p>
<p>T
(k)
i , where the hazard is assumed constant during each section of time.
</p>
<p>The corresponding log-likelihood term is
</p>
<p>l(ρ1, . . . , ρk) =
k
</p>
<p>&sum;
j=1
</p>
<p>(x
(j)
i log ρj &minus; ρjT
</p>
<p>(j)
i )
</p>
<p>in which the first k &minus; 1 of the x(j)i will be 0, and only the last one, x
(k)
i ,
</p>
<p>can be either 0 or one. The point of writing it in this elaborate form is
that it then becomes obvious that the likelihood contribution might as well
have come from k different individuals where the first k&minus; 1 had censored
observations.
</p>
<p>This is the rationale behind time-splitting techniques where the obser-
vation time of one subject is divided into observations for multiple
pseudo-individuals.
</p>
<p>It should be noted that although themodels with (piecewise) constant haz-
ard can be fitted and analyzed by likelihood techniques, pretending that
the data have come from a Poisson distribution, this does not extend to
all aspects of the model. For instance following a cohort to extinction will
lead to a fixed total number of events by definition, whereas the corre-
sponding Poisson model implies that the total event count has a Poisson</p>
<p/>
</div>
<div class="page"><p/>
<p>262 15. Rates and Poisson regression
</p>
<p>distribution. Both types of models deal in rates, counts per time, but the
difference is towhat extent the random variation lies in the counts or in the
amount of time. When data are frequently censored (i.e., the event is rare),
the survival model becomes well approximated by the Poisson model.
</p>
<p>15.2 Fitting Poisson models
</p>
<p>The class of generalized linear models (see Section 13.1) also includes the
Poisson distribution, which by default uses a log link function. This is the
mathematically convenient option and also a quite natural choice since it
allows the linear predictor to span the entire real line. We can use this to
formulate models for the log rates of the form
</p>
<p>log ρ = β0 + β1x1 + β2x2 + . . . βkxk
</p>
<p>or, since glm needs a model for the expected counts rather than rates,
</p>
<p>logλ = β0 + β1x1 + β2x2 + . . . βkxk + log T
</p>
<p>A feature of many Poisson models is that the model contains an offset in
the linear predictor, log T in this case. Notice that this is not the same as
including the term as a regression variable since the regression coefficient
is fixed at 1.
</p>
<p>The following example was used by Erling B. Andersen in 1977. It in-
volves the rates of lung cancer by age in four Danish cities and may be
found as eba1977 in the ISwR package.
</p>
<p>&gt; names(eba1977)
</p>
<p>[1] "city" "age" "pop" "cases"
</p>
<p>&gt; attach(eba1977)
</p>
<p>To fit a model that has multiplicative effects of age and city on the
rate of lung cancer cases, we use the glm function in much the same
way as in logistic regression. Of course, we need to change the family
argument to accommodate Poisson-distributed data. We also need to in-
corporate an offset to account for the different sizes and age structures of
the populations in the four cities.
</p>
<p>&gt; fit &lt;- glm(cases~city+age+offset(log(pop)), family=poisson)
</p>
<p>&gt; summary(fit)
</p>
<p>Call:
</p>
<p>glm(formula = cases ~ city + age + offset(log(pop)), family=poisson)
</p>
<p>Deviance Residuals:
</p>
<p>Min 1Q Median 3Q Max
</p>
<p>-2.63573 -0.67296 -0.03436 0.37258 1.85267</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Fitting Poisson models 263
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -5.6321 0.2003 -28.125 &lt; 2e-16 ***
</p>
<p>cityHorsens -0.3301 0.1815 -1.818 0.0690 .
</p>
<p>cityKolding -0.3715 0.1878 -1.978 0.0479 *
</p>
<p>cityVejle -0.2723 0.1879 -1.450 0.1472
</p>
<p>age55-59 1.1010 0.2483 4.434 9.23e-06 ***
</p>
<p>age60-64 1.5186 0.2316 6.556 5.53e-11 ***
</p>
<p>age65-69 1.7677 0.2294 7.704 1.31e-14 ***
</p>
<p>age70-74 1.8569 0.2353 7.891 3.00e-15 ***
</p>
<p>age75+ 1.4197 0.2503 5.672 1.41e-08 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>(Dispersion parameter for poisson family taken to be 1)
</p>
<p>Null deviance: 129.908 on 23 degrees of freedom
</p>
<p>Residual deviance: 23.447 on 15 degrees of freedom
</p>
<p>AIC: 137.84
</p>
<p>Number of Fisher Scoring iterations: 5
</p>
<p>The offset was included in the model formula in this case. Alternatively, it
could have been given as a separate argument as in
</p>
<p>glm(cases~city+age, offset = log(pop), family=poisson)
</p>
<p>The table labelled &ldquo;Coefficients:&rdquo; contains regression coefficients for the
linear predictor along with standard errors and z tests. These can be in-
terpreted in the same way as in ordinary multiple regression or logistic
regression. Since both variables are factors and we are using treatment
contrasts (see Section 12.3), the coefficients indicate differences in the log
rate (i.e., the log of the rate ratio) compared with the city of Fredericia and
with the 50&ndash;54-year-olds, respectively.
</p>
<p>The intercept term refers to the log rate for the group of 50&ndash;54-year-olds
in Fredericia. Notice that because we used the population size rather than
the number of person-years in the offset and the data cover the years 1968&ndash;
1971, this rate will effectively be per 4 person-years.
</p>
<p>A goodness-of-fit statistic is provided by comparing the residual deviance
to a χ2 distribution on the stated degrees of freedom. This statistic is gen-
erally considered valid if the expected count in all cells is larger than 5.
Accordingly,
</p>
<p>&gt; min(fitted(fit))
</p>
<p>[1] 6.731286
</p>
<p>&gt; pchisq(deviance(fit), df.residual(fit), lower=F)
</p>
<p>[1] 0.07509017</p>
<p/>
</div>
<div class="page"><p/>
<p>264 15. Rates and Poisson regression
</p>
<p>and we see that the model fits the data acceptably. Of course, we could
also just have read off the residual deviance and degrees of freedom from
the summary output:
</p>
<p>&gt; pchisq(23.45, 15, lower=F)
</p>
<p>[1] 0.07504166
</p>
<p>From the coefficient table, it is obvious that there is an age effect, but it is
less clear whether there is a city effect. We can perform χ2 tests for each
term by using drop1 and looking at the changes in the deviance.
</p>
<p>&gt; drop1(fit, test="Chisq")
</p>
<p>Single term deletions
</p>
<p>Model:
</p>
<p>cases ~ city + age + offset(log(pop))
</p>
<p>Df Deviance AIC LRT Pr(Chi)
</p>
<p>&lt;none&gt; 23.447 137.836
</p>
<p>city 3 28.307 136.695 4.859 0.1824
</p>
<p>age 5 126.515 230.903 103.068 &lt;2e-16 ***
</p>
<p>...
</p>
<p>We see that the age term is significant, hardly surprisingly, but the city
term apparently is not. However, if you can argue a priori that Fredericia
could be expected to have a higher cancer rate than the three other cities,
then it could be warranted to combine the three other cities into one and
perform an analysis as below.
</p>
<p>&gt; fit2 &lt;- glm(cases~(city=="Fredericia")+age+offset(log(pop)),
</p>
<p>+ family=poisson)
</p>
<p>&gt; anova(fit, fit2, test="Chisq")
</p>
<p>Analysis of Deviance Table
</p>
<p>Model 1: cases ~ city + age + offset(log(pop))
</p>
<p>Model 2: cases ~ (city == "Fredericia") + age + offset(log(pop))
</p>
<p>Resid. Df Resid. Dev Df Deviance P(&gt;|Chi|)
</p>
<p>1 15 23.4475
</p>
<p>2 17 23.7001 -2 -0.2526 0.8814
</p>
<p>&gt; drop1(fit2, test="Chisq")
</p>
<p>Single term deletions
</p>
<p>Model:
</p>
<p>cases ~ (city == "Fredericia") + age + offset(log(pop))
</p>
<p>Df Deviance AIC LRT Pr(Chi)
</p>
<p>&lt;none&gt; 23.700 134.088
</p>
<p>city == "Fredericia" 1 28.307 136.695 4.606 0.03185 *
</p>
<p>age 5 127.117 227.505 103.417 &lt; 2e-16 ***
</p>
<p>...
</p>
<p>According to this, you may combine the three cities other than Fredericia,
and, once this is done, Fredericia does indeed appear to be significantly</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Fitting Poisson models 265
</p>
<p>different from the others. Alternatively, you can look at the coefficients in
fit2 directly
</p>
<p>&gt; summary(fit2)
</p>
<p>...
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) -5.9589 0.1809 -32.947 &lt; 2e-16 ***
</p>
<p>city == "Fredericia"TRUE 0.3257 0.1481 2.200 0.0278 *
</p>
<p>age55-59 1.1013 0.2483 4.436 9.17e-06 ***
</p>
<p>age60-64 1.5203 0.2316 6.564 5.23e-11 ***
</p>
<p>age65-69 1.7687 0.2294 7.712 1.24e-14 ***
</p>
<p>age70-74 1.8592 0.2352 7.904 2.71e-15 ***
</p>
<p>age75+ 1.4212 0.2502 5.680 1.34e-08 ***
</p>
<p>...
</p>
<p>and see the p-value of 0.0278. This agrees with the 0.03185 from drop1;
you cannot expect the two p-values to be perfectly equal since they rely
on different asymptotic approximations. If you really push it, you can ar-
gue that a one-sided test with half the p-value is appropriate since you
would only expect Fredericia to be more harmful than the others, not less.
However, the argumentation becomes tenuous, and in his paper Ander-
sen outlines the possibility of testing Fredericia against the other cities but
stops short of providing any p-value, stating that in his opinion &ldquo;there is
no reason to believe a priori that Fredericia is the more dangerous city&rdquo;.
</p>
<p>It is sometimes preferred to state the results of Poisson regression analy-
sis in terms of rate ratios by taking exp() of the estimates (this parallels
the presentation of logistic regression analysis in terms of odds ratios in
Section 13.4). The intercept term is not really a ratio but a rate, and for
nonfactor covariates it should be understood that the coefficient is the rel-
ative change per unit change in the covariate. Because of the nonlinear
transformation, standard errors are not useful; instead one can calculate
confidence intervals for the coefficients as follows:
</p>
<p>&gt; cf &lt;- coefficients(summary(fit2))
</p>
<p>&gt; est &lt;- cf[,1]
</p>
<p>&gt; s.e. &lt;- cf[,2]
</p>
<p>&gt; rr &lt;- exp(cbind(est, est - s.e.*qnorm(.975), est
</p>
<p>+ + s.e.*qnorm(.975) ))
</p>
<p>&gt; colnames(rr) &lt;- c("RateRatio", "CI.lo","CI.hi")
</p>
<p>&gt; rr
</p>
<p>RateRatio CI.lo CI.hi
</p>
<p>(Intercept) 0.002582626 0.001811788 0.003681423
</p>
<p>city == "Fredericia"TRUE 1.384992752 1.036131057 1.851314957
</p>
<p>age55-59 3.008134852 1.849135187 4.893571521
</p>
<p>age60-64 4.573665854 2.904833526 7.201245496
</p>
<p>age65-69 5.863391064 3.740395488 9.191368903
</p>
<p>age70-74 6.418715646 4.047748963 10.178474731
</p>
<p>age75+ 4.142034525 2.536571645 6.763637070</p>
<p/>
</div>
<div class="page"><p/>
<p>266 15. Rates and Poisson regression
</p>
<p>Actually, we can do better by using the confint function. This calcu-
lates confidence intervals by profiling the likelihood function instead of
using the approximation with the normal distribution inherent in the use
of asymptotic standard errors. This is done like this:
</p>
<p>&gt; exp(cbind(coef(fit2), confint(fit2)))
</p>
<p>Waiting for profiling to be done...
</p>
<p>2.5 % 97.5 %
</p>
<p>(Intercept) 0.002582626 0.001776461 0.003617228
</p>
<p>city == "Fredericia"TRUE 1.384992752 1.029362341 1.841224091
</p>
<p>age55-59 3.008134852 1.843578634 4.902339637
</p>
<p>age60-64 4.573665854 2.912314045 7.248143959
</p>
<p>age65-69 5.863391064 3.752718226 9.256907108
</p>
<p>age70-74 6.418715646 4.053262281 10.234338998
</p>
<p>age75+ 4.142034525 2.527117848 6.771833979
</p>
<p>In the present case, we are well within the regime where the asymptotic
normal approximation works well, so there is little difference between the
two displays. However, in some cases where some expected cell counts are
low and one or several coefficients are poorly determined, the difference
can be substantial.
</p>
<p>15.3 Computing rates
</p>
<p>We return to theWelsh nickel worker data discussed in Chapter 10. In that
section, we discussed how to split the individual lifetime data into smaller
pieces that could reasonably be merged with the standard mortality table
in the ewrates data.
</p>
<p>The result of this initial data restructuring is in the nickel.expand data
set. It contains data from a lot of short time intervals like this:
</p>
<p>&gt; head(nickel.expand)
</p>
<p>agr ygr id icd exposure dob age1st agein ageout lung
</p>
<p>1 20 1931 325 0 0 1910.500 14.0737 23.7465 25 6
</p>
<p>2 20 1931 273 0 0 1909.500 14.6913 24.7465 25 6
</p>
<p>3 20 1931 110 0 0 1909.247 14.0302 24.9999 25 6
</p>
<p>4 20 1931 574 0 0 1909.729 14.0356 24.5177 25 6
</p>
<p>5 20 1931 213 0 0 1910.129 14.2018 24.1177 25 6
</p>
<p>6 20 1931 546 0 0 1909.500 14.4945 24.7465 25 6
</p>
<p>nasal other
</p>
<p>1 0 3116
</p>
<p>2 0 3116
</p>
<p>3 0 3116
</p>
<p>4 0 3116
</p>
<p>5 0 3116
</p>
<p>6 0 3116</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Computing rates 267
</p>
<p>The same individuals reappear later in the data at older ages. For example,
all data for the individual with id number 325 are
</p>
<p>&gt; subset(nickel.expand, id==325)
</p>
<p>agr ygr id icd exposure dob age1st agein ageout lung
</p>
<p>1 20 1931 325 0 0 1910.5 14.0737 23.7465 25.0000 6
</p>
<p>13 25 1931 325 0 0 1910.5 14.0737 25.0000 30.0000 14
</p>
<p>172 30 1936 325 0 0 1910.5 14.0737 30.0000 35.0000 30
</p>
<p>391 35 1941 325 0 0 1910.5 14.0737 35.0000 40.0000 81
</p>
<p>728 40 1946 325 434 0 1910.5 14.0737 40.0000 43.0343 236
</p>
<p>nasal other
</p>
<p>1 0 3116
</p>
<p>13 0 3024
</p>
<p>172 1 3188
</p>
<p>391 1 3549
</p>
<p>728 3 3643
</p>
<p>Accordingly, this subject enters the study at age 23.7 and we follow him
through five age groups until his death at age 43.
</p>
<p>The variable ygr reflects the year of entry into the interval, so even though
the subject dies in 1953, the last record is coded as belonging to the years
1946&ndash;1950.
</p>
<p>Subject no. 325 has the icd code 434 in his last record. This refers to the
International Classification of Diseases (version 7) and indicates &ldquo;Other
and unspecified diseases of the heart&rdquo; as the cause of death. For the pur-
poses of this chapter, we are primarily interested in lung cancer, which has
codes 162 and 163, so we define a variable to indicate whether this is the
cause of death. (Expect a warning about masking the lung data set upon
attaching.)
</p>
<p>&gt; nickel.expand &lt;- within(nickel.expand,
</p>
<p>+ lung.cancer &lt;- as.numeric(icd %in% c(162,163)))
</p>
<p>&gt; attach(nickel.expand)
</p>
<p>The %in% operator returns a logical vector that is TRUE when the corre-
sponding element of the operand on the left is contained in the vector that
is the operand on the right and FALSE in all other cases. Use of this op-
erator is slightly dangerous in the case of an NA element in icd, but in
these particular data, there are none. We convert the result to zero or one
since we are going to pretend that it is a Poisson count later on (this is not
strictly necessary). Notice that by using lung.cancer as the endpoint,
we treat death from all other causes, including &ldquo;unknown&rdquo;, as censoring.
</p>
<p>Each record provides ageout - agein person-years of risk time, so to
tabulate the risk times, we can just do as follows:
</p>
<p>&gt; pyr &lt;- tapply(ageout-agein,list(ygr,agr), sum)
</p>
<p>&gt; print(round(pyr), na.print="-")</p>
<p/>
</div>
<div class="page"><p/>
<p>268 15. Rates and Poisson regression
</p>
<p>20 25 30 35 40 45 50 55 60 65 70 75 80
</p>
<p>1931 3 86 268 446 446 431 455 323 159 23 4 - -
</p>
<p>1936 - - 100 327 504 512 503 472 314 130 20 5 -
</p>
<p>1941 - - 0 105 336 481 482 445 368 235 80 14 3
</p>
<p>1946 - - - - 102 335 461 404 369 263 157 43 10
</p>
<p>1951 - - - - - 95 299 415 334 277 181 92 31
</p>
<p>1956 - - - - - - 89 252 364 257 181 101 52
</p>
<p>1961 - - - - - - - 71 221 284 150 104 44
</p>
<p>1966 - - - - - - - - 66 168 208 93 51
</p>
<p>1971 - - - - - - - - - 57 133 131 54
</p>
<p>1976 - - - - - - - - - - 31 68 53
</p>
<p>Notice that there are many NA entries in cells that no subject ever entered.
The subjects in the study were born between 1864 and 1910, so there is a
large block missing in the lower left and a smaller block in the upper right.
The na.print option to print allows you to represent these missing
values by a string that is less visually imposing than the default "NA".
</p>
<p>The corresponding counts of lung cancer cases are obtained as
</p>
<p>&gt; count &lt;- tapply(lung.cancer, list(ygr, agr), sum)
</p>
<p>&gt; print(count, na.print="-")
</p>
<p>20 25 30 35 40 45 50 55 60 65 70 75 80
</p>
<p>1931 0 0 0 0 0 4 2 2 2 0 0 - -
</p>
<p>1936 - - 0 0 2 3 4 6 5 1 0 0 -
</p>
<p>1941 - - 0 0 0 3 7 5 6 3 2 0 0
</p>
<p>1946 - - - - 0 0 8 7 6 2 2 0 0
</p>
<p>1951 - - - - - 0 3 3 9 6 1 0 0
</p>
<p>1956 - - - - - - 0 4 3 6 1 2 0
</p>
<p>1961 - - - - - - - 0 1 1 3 2 1
</p>
<p>1966 - - - - - - - - 2 0 0 1 0
</p>
<p>1971 - - - - - - - - - 0 0 2 2
</p>
<p>1976 - - - - - - - - - - 0 1 1
</p>
<p>and the cancer rates can be obtained as the ratio of the counts to the risk
time. These are small, so we multiply by 1000 to get rates per 1000 person-
years.
</p>
<p>&gt; print(round(count/pyr*1000, 1), na.print="-")
</p>
<p>20 25 30 35 40 45 50 55 60 65 70 75 80
</p>
<p>1931 0 0 0 0 0 9.3 4.4 6.2 12.6 0.0 0.0 - -
</p>
<p>1936 - - 0 0 4 5.9 7.9 12.7 15.9 7.7 0.0 0.0 -
</p>
<p>1941 - - 0 0 0 6.2 14.5 11.2 16.3 12.8 25.0 0.0 0.0
</p>
<p>1946 - - - - 0 0.0 17.4 17.3 16.3 7.6 12.8 0.0 0.0
</p>
<p>1951 - - - - - 0.0 10.0 7.2 27.0 21.7 5.5 0.0 0.0
</p>
<p>1956 - - - - - - 0.0 15.9 8.2 23.4 5.5 19.8 0.0
</p>
<p>1961 - - - - - - - 0.0 4.5 3.5 19.9 19.3 22.8
</p>
<p>1966 - - - - - - - - 30.1 0.0 0.0 10.7 0.0
</p>
<p>1971 - - - - - - - - - 0.0 0.0 15.2 36.8
</p>
<p>1976 - - - - - - - - - - 0.0 14.6 19.0</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Computing rates 269
</p>
<p>Comparison of these rates with those in ewrates suggests that they are
very high. However, this kind of display has the disadvantage that it hides
the actual counts on which the rates are based. For instance, the lower
part of the column for 80&ndash;84-year-olds jumps by roughly 20 units for each
additional case since there are only about 50 person-years per cell.
</p>
<p>It may be better to compute the expected counts in each cell based on
the standard mortality table and then compare that to the actual counts.
Since we have already merged in the ewrates data, this is just a matter
of multiplying each piece of risk time by the rate. We need to divide by
1e6 (i.e., 106 = 1000000) since the standard rates are given per million
person-years.
</p>
<p>&gt; expect.count &lt;- tapply(lung/1e6*(ageout-agein),
</p>
<p>+ list(ygr,agr), sum)
</p>
<p>&gt; print(round(expect.count, 1), na.print="-")
</p>
<p>20 25 30 35 40 45 50 55 60 65 70 75 80
</p>
<p>1931 0 0 0 0 0.1 0.1 0.2 0.2 0.1 0.0 0.0 - -
</p>
<p>1936 - - 0 0 0.1 0.1 0.2 0.3 0.2 0.1 0.0 0.0 -
</p>
<p>1941 - - 0 0 0.1 0.2 0.3 0.4 0.4 0.2 0.1 0.0 0.0
</p>
<p>1946 - - - - 0.0 0.2 0.4 0.5 0.6 0.5 0.2 0.0 0.0
</p>
<p>1951 - - - - - 0.1 0.4 0.8 0.9 0.8 0.5 0.2 0.0
</p>
<p>1956 - - - - - - 0.1 0.6 1.2 1.0 0.7 0.3 0.1
</p>
<p>1961 - - - - - - - 0.2 0.8 1.4 0.7 0.5 0.1
</p>
<p>1966 - - - - - - - - 0.2 0.9 1.3 0.6 0.2
</p>
<p>1971 - - - - - - - - - 0.3 0.9 1.0 0.3
</p>
<p>1976 - - - - - - - - - - 0.2 0.6 0.4
</p>
<p>The observed counts are clearly much larger than expected. We can sum-
marize them by calculating the overall SMR (standardized mortality rate),
which is simply the ratio of the total number of cases to the total expected
number of cases.
</p>
<p>&gt; expect.tot &lt;- sum(lung/1e6*(ageout-agein))
</p>
<p>&gt; expect.tot
</p>
<p>[1] 24.19893
</p>
<p>&gt; count.tot &lt;- sum(lung.cancer)
</p>
<p>&gt; count.tot
</p>
<p>[1] 137
</p>
<p>&gt; count.tot/expect.tot
</p>
<p>[1] 5.661408
</p>
<p>That is, this data set has almost six times as many cancer deaths as you
would expect from the mortality of the general population.</p>
<p/>
</div>
<div class="page"><p/>
<p>270 15. Rates and Poisson regression
</p>
<p>15.4 Models with piecewise constant intensities
</p>
<p>We can formulate the SMR analysis as a &ldquo;Poisson&rdquo; regression model in the
sense of Section 15.1.2. The assumption behind the SMR is that there is a
constant rate ratio to the standard mortality, so we can fit a model with
only an intercept while having an offset, which is the log of the expected
count. This is not really different from modelling rates &mdash; the population
mortality ρi is just absorbed into the offset, log ρi + log Ti = log ρiTi.
</p>
<p>&gt; fit &lt;- glm(lung.cancer ~ 1, poisson,
</p>
<p>+ offset = log((ageout-agein)*lung/1e6))
</p>
<p>&gt; summary(fit)
</p>
<p>...
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) 1.73367 0.08544 20.29 &lt;2e-16 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>(Dispersion parameter for poisson family taken to be 1)
</p>
<p>Null deviance: 1175.6 on 3723 degrees of freedom
</p>
<p>Residual deviance: 1175.6 on 3723 degrees of freedom
</p>
<p>AIC: 1451.6
</p>
<p>Number of Fisher Scoring iterations: 7
</p>
<p>Notice that this is based on individual data; the dependent variable
lung.cancer is zero or one. We could have aggregated the data accord-
ing to the cross-classification of agr and ygr and analyzed the number of
cases in each cell. This would have allowed glm to run much faster, but on
the other hand it would then not be possible to add individual covariates
such as age at first exposure.
</p>
<p>In this case, we cannot use the deviances for model checking both because
the expected counts per cell are very small and because we do not actu-
ally have Poisson-distributed data. However, the standard error and the
p-value should be reliable if the assumptions hold.
</p>
<p>The connection between this analysis and the SMR can be seen immedi-
ately from
</p>
<p>&gt; exp(coef(fit))
</p>
<p>(Intercept)
</p>
<p>5.661408
</p>
<p>This value is exactly the SMR value from the previous section.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Models with piecewise constant intensities 271
</p>
<p>We can analyze the data more thoroughly using regression methods. As a
first approach, we investigate whether the SMR is constant over year and
age groups using a multiplicative Poisson model.
</p>
<p>We need to simplify the groupings because some of the groups contain
very few cases. By calculating the marginal tables of counts, we get some
idea of what to do.
</p>
<p>&gt; tapply(lung.cancer, agr, sum)
</p>
<p>20 25 30 35 40 45 50 55 60 65 70 75 80
</p>
<p>0 0 0 0 2 10 24 27 34 19 9 8 4
</p>
<p>&gt; tapply(lung.cancer, ygr, sum)
</p>
<p>1931 1936 1941 1946 1951 1956 1961 1966 1971 1976
</p>
<p>10 21 26 25 22 16 8 3 4 2
</p>
<p>To get at least 10 cases per level, we combine all values of agr up to 45 (i.e.,
ages less than 50) and also those from 70 and up. Similarly, we combine
all values of ygr for the periods from 1961 onwards.
</p>
<p>&gt; detach()
</p>
<p>&gt; nickel.expand &lt;- within(nickel.expand,{
</p>
<p>+ A &lt;- factor(agr)
</p>
<p>+ Y &lt;- factor(ygr)
</p>
<p>+ lv &lt;- levels(A)
</p>
<p>+ lv[1:6] &lt;- "&lt; 50"
</p>
<p>+ lv[11:13] &lt;- "70+"
</p>
<p>+ levels(A) &lt;- lv
</p>
<p>+ lv &lt;- levels(Y)
</p>
<p>+ lv[7:10] &lt;- "1961ff"
</p>
<p>+ levels(Y) &lt;- lv
</p>
<p>+ rm(lv)
</p>
<p>+ })
</p>
<p>&gt; attach(nickel.expand)
</p>
<p>Notice that this is a case where the within function (see Section 2.1.8)
works better than transform because it allowsmore flexibility, including
the creation of temporary variables such as lv.
</p>
<p>We can analyze the effect of A and Y on the mortality ratio by building
a log-additive model in the usual way. Notice that we still use the orig-
inal grouping in the calculation of the offset; it is only the SMR that is
assumed to be the same for everyone below 50, etc. We use drop1 to test
the significance of the two factors.
</p>
<p>&gt; fit &lt;- glm(lung.cancer ~ A + Y, poisson,
</p>
<p>+ offset=log((ageout-agein)*lung/1e6))
</p>
<p>&gt; drop1(fit, test="Chisq")
</p>
<p>Single term deletions
</p>
<p>Model:
</p>
<p>lung.cancer ~ A + Y</p>
<p/>
</div>
<div class="page"><p/>
<p>272 15. Rates and Poisson regression
</p>
<p>Df Deviance AIC LRT Pr(Chi)
</p>
<p>&lt;none&gt; 1069.73 1367.73
</p>
<p>A 5 1073.81 1361.81 4.08 0.5376
</p>
<p>Y 6 1118.50 1404.50 48.77 8.29e-09 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>So it seems that we do not need the age grouping in the model, but
the year grouping is needed. Accordingly, we fit a model with Y alone,
and by dropping the intercept, we get a parameterization with a separate
intercept for each level of Y.
</p>
<p>&gt; fit &lt;- glm(lung.cancer ~ Y - 1, poisson,
</p>
<p>+ offset=log((ageout-agein)*lung/1e6))
</p>
<p>&gt; summary(fit)
</p>
<p>...
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>Y1931 2.6178 0.3162 8.279 &lt; 2e-16 ***
</p>
<p>Y1936 3.0126 0.2182 13.805 &lt; 2e-16 ***
</p>
<p>Y1941 2.7814 0.1961 14.182 &lt; 2e-16 ***
</p>
<p>Y1946 2.2787 0.2000 11.394 &lt; 2e-16 ***
</p>
<p>Y1951 1.8038 0.2132 8.461 &lt; 2e-16 ***
</p>
<p>Y1956 1.3698 0.2500 5.479 4.27e-08 ***
</p>
<p>Y1961ff 0.4746 0.2425 1.957 0.0504 .
</p>
<p>....
</p>
<p>The regression coefficients may again be recognized as log-SMR values, as
the following demonstrates:
</p>
<p>&gt; round(exp(coef(fit)), 1)
</p>
<p>Y1931 Y1936 Y1941 Y1946 Y1951 Y1956 Y1961ff
</p>
<p>13.7 20.3 16.1 9.8 6.1 3.9 1.6
</p>
<p>&gt; expect.count &lt;- tapply(lung/1e6*(ageout-agein), Y, sum)
</p>
<p>&gt; count &lt;- tapply(lung.cancer, Y, sum)
</p>
<p>&gt; cbind(count=count, expect=round(expect.count,1),
</p>
<p>+ SMR= round(count/expect.count, 1))
</p>
<p>count expect SMR
</p>
<p>1931 10 0.7 13.7
</p>
<p>1936 21 1.0 20.3
</p>
<p>1941 26 1.6 16.1
</p>
<p>1946 25 2.6 9.8
</p>
<p>1951 22 3.6 6.1
</p>
<p>1956 16 4.1 3.9
</p>
<p>1961ff 17 10.6 1.6
</p>
<p>The advantage of using the regression approach is that it provides a frame-
work in which you can formulate statistical tests and investigate the effect
of multiple regression variables simultaneously.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Models with piecewise constant intensities 273
</p>
<p>Breslow and Day analyzed the nickel data in their seminal book (Bres-
low and Day, 1987) on the analysis of cohort studies. In their analysis,
they split the individual risk times according to three criteria, two of them
being age and period, to match the standard mortality table, but they
also treat time from employment as a time-dependent covariate with a
piecewise constant effect, which requires that the person-year be split fur-
ther according to the interval boundaries. They then represent time effects
using three variables: time since, age at, and year of first employment,
TFE, AFE, and YFE, respectively. In addition, they include a measure of
exposure level.
</p>
<p>The following analysis roughly reproduces the Breslow and Day analysis.
It is not completely similar because we settle for splitting time accord-
ing to agr only and use the age at entry into each interval to define the
TFE variable as well as for choosing the relevant standard mortality rates.
However, to enable some comparison of results, we define cut groups in
a manner that is similar to that of Breslow and Day.
</p>
<p>&gt; detach()
</p>
<p>&gt; nickel.expand &lt;- within(nickel.expand,{
</p>
<p>+ TFE &lt;- cut(agein-age1st, c(0,20,30,40,50,100), right=F)
</p>
<p>+ AFE &lt;- cut(age1st, c(0, 20, 27.5, 35, 100), right=F)
</p>
<p>+ YFE &lt;- cut(dob + age1st, c(0, 1910, 1915, 1920, 1925),right=F)
</p>
<p>+ EXP &lt;- cut(exposure, c(0, 0.5, 4.5, 8.5, 12.5, 25), right=F)
</p>
<p>+ })
</p>
<p>&gt; attach(nickel.expand)
</p>
<p>Some relabelling of group levels might be called for &mdash; e.g., the levels for
EXP are really 0, 0.5&ndash;4, 4.5&ndash;8, 8.5&ndash;12, 12.5+ &mdash; but let us not make more of
it than necessary.
</p>
<p>We fit a multiplicative model and test the significance of the individual
terms as follows:
</p>
<p>&gt; fit &lt;- glm(lung.cancer ~ TFE + AFE + YFE + EXP, poisson,
</p>
<p>+ offset=log((ageout-agein)*lung/1e6))
</p>
<p>&gt; drop1(fit, test="Chisq")
</p>
<p>Single term deletions
</p>
<p>Model:
</p>
<p>lung.cancer ~ TFE + AFE + YFE + EXP
</p>
<p>Df Deviance AIC LRT Pr(Chi)
</p>
<p>&lt;none&gt; 1052.91 1356.91
</p>
<p>TFE 4 1107.33 1403.33 54.43 4.287e-11 ***
</p>
<p>AFE 3 1054.99 1352.99 2.08 0.5560839
</p>
<p>YFE 3 1058.06 1356.06 5.15 0.1608219
</p>
<p>EXP 4 1071.98 1367.98 19.07 0.0007606 ***
</p>
<p>This suggests that the two major terms are TFE and EXP, whereas AFE
and YFE could be taken out of the model. Notice, though, that it cannot be</p>
<p/>
</div>
<div class="page"><p/>
<p>274 15. Rates and Poisson regression
</p>
<p>concluded from the above that both can be removed. In principle, one of
them could become significant when the other is removed. This does not
happen in this case, though.
</p>
<p>The table of coefficients looks like this:
</p>
<p>&gt; summary(fit)
</p>
<p>...
</p>
<p>Coefficients:
</p>
<p>Estimate Std. Error z value Pr(&gt;|z|)
</p>
<p>(Intercept) 2.36836 0.55716 4.251 2.13e-05 ***
</p>
<p>TFE[20,30) -0.21788 0.36022 -0.605 0.545284
</p>
<p>TFE[30,40) -0.77184 0.36529 -2.113 0.034605 *
</p>
<p>TFE[40,50) -1.87583 0.41707 -4.498 6.87e-06 ***
</p>
<p>TFE[50,100) -2.22142 0.55068 -4.034 5.48e-05 ***
</p>
<p>AFE[20,27.5) 0.28506 0.31524 0.904 0.365868
</p>
<p>AFE[27.5,35) 0.21961 0.34011 0.646 0.518462
</p>
<p>AFE[35,100) -0.10818 0.44412 -0.244 0.807556
</p>
<p>YFE[1910,1915) 0.04826 0.27193 0.177 0.859137
</p>
<p>YFE[1915,1920) -0.56397 0.37585 -1.501 0.133483
</p>
<p>YFE[1920,1925) -0.42520 0.30017 -1.417 0.156614
</p>
<p>EXP[0.5,4.5) 0.58373 0.21200 2.753 0.005897 **
</p>
<p>EXP[4.5,8.5) 1.03175 0.28364 3.638 0.000275 ***
</p>
<p>EXP[8.5,12.5) 1.18345 0.37406 3.164 0.001557 **
</p>
<p>EXP[12.5,25) 1.28601 0.48236 2.666 0.007674 **
</p>
<p>...
</p>
<p>A dose-response pattern and a declining effect of time since first employ-
ment seem to be present.
</p>
<p>The results may be more readily interpreted if they are given in terms of
ratios and confidence intervals. These can be obtained in exactly the same
way as in the analysis of the eba1977 data.
</p>
<p>15.5 Exercises
</p>
<p>15.1 In the bcmort data set, we defined the period and area factors
in Exercise 10.2. Fit a Poisson regression model to the data with age,
period, and area as descriptors, as well as the three two-factor interac-
tion terms. The interaction between period and area can be interpreted
as the effect of screening.
</p>
<p>15.2 With the split stroke data from Exercise 10.4, fit a Poisson regres-
sion model corresponding to a constant hazard in each interval and with
multiplicative effects of age and sex.</p>
<p/>
</div>
<div class="page"><p/>
<p>16
Nonlinear curve fitting
</p>
<p>Curve fitting problems occur in many scientific areas. The typical case
is that you wish to fit the relation between some response y and a
one-dimensional predictor x, by adjusting a (possibly multidimensional)
parameter β. That is,
</p>
<p>y = f (x; β) + error
</p>
<p>in which the &ldquo;error&rdquo; term is usually assumed to contain independent nor-
mally distributed terms with a constant standard deviation σ. The class of
models can be easily extended to multivariate x and somewhat less easily
to models with nonconstant error variation, but we settle for the simple
case.
</p>
<p>Chapter 6 described the special case of a linear relation
</p>
<p>y = β0 + β1x + error
</p>
<p>and we discussed the fitting of polynomials by including quadratic and
higher-order terms in Section 12.1. There are other techniques, notably
trigonometric regression and spline regression, that can also be formu-
lated in linear form and handled by software for multiple regression
analysis like lm.
</p>
<p>However, sometimes linear methods are inadequate. The common case is
that you have a priori knowledge of the form of the function. This may
come from theoretical analysis of an underlying physical and chemical
</p>
<p>P. Dalgaard, Introductory Statistics with R,
DOI: 10.1007/978-0-387-79054-1_16, &copy; Springer Science+Business Media, LLC 2008</p>
<p/>
</div>
<div class="page"><p/>
<p>276 16. Nonlinear curve fitting
</p>
<p>system, and the parameters of the relation have a specific meaning in that
theory.
</p>
<p>The method of least squares makes good sense even when the relation
between data and parameters is not linear. That is, we can estimate β by
minimizing
</p>
<p>SSD(β) = &sum;(y&minus; f (x; β))2
</p>
<p>There is no explicit formula for the location of the minimum, but the min-
imization can be performed numerically by algorithms that we describe
only superficially here. This general technique is also known as nonlin-
ear regression analysis. For an in-depth treatment of the topic, a standard
reference is Bates and Watts (1988).
</p>
<p>If the model is &ldquo;well-behaved&rdquo; (to use a deliberately vague term), then the
model can be approximated by a linear model in the vicinity of the opti-
mum, and it then makes sense to calculate approximate standard errors
for the parameter estimates.
</p>
<p>Most of the available optimization algorithms build on the same idea of
linearization; i.e.,
</p>
<p>y&minus; f (x; β + δ) &asymp; y&minus; f (x; β) + D f δ
in which D f denotes the gradient matrix of derivatives of f with respect to
β. This effectively becomes a design matrix of a linear model, and you can
proceed from a starting guess at β to find an approximate least squares
fit of δ. Then you replace β by β + δ and repeat until convergence. Varia-
tions on this basic algorithm include numerical computation of D f and
techniques to avoid instability if the starting guess is too far from the
optimum.
</p>
<p>To perform the optimization in R, you can use the nls function, which is
broadly similar to lm and glm.
</p>
<p>16.1 Basic usage
</p>
<p>In this section, we use a simulated data set just so that we know what we
are doing. The model is a simple exponential decay.
</p>
<p>&gt; t &lt;- 0:10
</p>
<p>&gt; y &lt;- rnorm(11, mean=5*exp(-t/5), sd=.2)
</p>
<p>&gt; plot(y ~ t)
</p>
<p>The simulated data can be seen in Figure 16.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 Basic usage 277
</p>
<p>0 2 4 6 8 10
</p>
<p>1
2
</p>
<p>3
4
</p>
<p>5
</p>
<p>t
</p>
<p>y
</p>
<p>Figure 16.1. Simulated exponential decay.
</p>
<p>We now fit the model to data using nls. Unlike lm and glm, the model
formula for nls does not use the special codings for linear terms, grouping
factors, interactions, etc. Instead, the right-hand side is an explicit expres-
sion to calculate the expected value of the left-hand side. This can depend
on external variables as well as the parameters, so we need to specify
which is which. The simplest way to do this is to specify a named vector
(or a named list) of starting values.
</p>
<p>&gt; nlsout &lt;- nls(y ~ A*exp(-alpha*t), start=c(A=2, alpha=0.05))
</p>
<p>&gt; summary(nlsout)
</p>
<p>Formula: y ~ A * exp(-alpha * t)
</p>
<p>Parameters:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>A 4.97204 0.21766 22.84 2.80e-09 ***
</p>
<p>alpha 0.20793 0.01572 13.23 3.35e-07 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 0.2805 on 9 degrees of freedom
</p>
<p>Number of iterations to convergence: 5</p>
<p/>
</div>
<div class="page"><p/>
<p>278 16. Nonlinear curve fitting
</p>
<p>Achieved convergence tolerance: 2.223e-06
</p>
<p>Notice that nls treats t as a variable and not a parameter because it is not
mentioned in the start argument. Whenever the fitting algorithm needs
to evaluate A * exp(-alpha * t), t is taken from the variable in the
global environment, whereas A and alpha are varied by the algorithm.
</p>
<p>The general form of the output is quite similar to that of glm, so we shall
not dwell too long upon it. One thing that might be noted is that the t test
and p-value stated for each parameter are tests for a hypothesis that the
parameter is zero, which is often quite meaningless for nonlinear models.
</p>
<p>16.2 Finding starting values
</p>
<p>In the previous section, we had quite fast convergence, even though the
initial guess of parameters was (deliberately) rather badly off. Unfortu-
nately, things are not always that simple; convergence of nonlinear models
can depend critically on having good starting values. Even when the al-
gorithm is fairly robust, we at least need to get the order of magnitude
right.
</p>
<p>Methods for obtaining starting values will most often rely on an analy-
sis of the functional form; common techniques involve transformation to
linearity and the estimation of &ldquo;landmarks&rdquo; such asymptotes, maximum
points, and initial slopes.
</p>
<p>To illustrate this, we again consider the Juul data. This time we focus on
the relation between age and height. To obtain a reasonably homogeneous
data set, we look at males only and subset the data to the ages between 5
and 20.
</p>
<p>&gt; attach(subset(juul2, age&lt;20 &amp; age&gt;5 &amp; sex==1))
</p>
<p>&gt; plot(height ~ age)
</p>
<p>Aplot of the data is shown in Figure 16.2. The plot looks linear over a large
portion of its domain, but there is some levelling off at the right end and
of course it is basic human biology that we stop growing at some point in
the later teens.
</p>
<p>The Gompertz curve is often used to describe growth. It can be expressed
in the following form:
</p>
<p>y = αe&minus;βe
&minus;γx</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Finding starting values 279
</p>
<p>5 10 15 20
</p>
<p>1
2
0
</p>
<p>1
4
0
</p>
<p>1
6
0
</p>
<p>1
8
0
</p>
<p>2
0
0
</p>
<p>age
</p>
<p>h
e
ig
</p>
<p>h
t
</p>
<p>Figure 16.2. Relationship between age and height in juul2 data set.
</p>
<p>The curve has a sigmoid shape, approaching a constant level α as x in-
creases and (in principle) zero for large negative x. The β and γ parameters
determine the location and steepness of the transition.
</p>
<p>To obtain starting values for a nonlinear fit, one approach is to notice that
the relation between y and x is something like log-log linear. Specifically,
we can rewrite the relation as
</p>
<p>log y = log α&minus; βe&minus;γx
</p>
<p>whichwemay rearrange and take logarithms on both sides again, yielding
</p>
<p>log(log α&minus; log y) = log β&minus; γx
</p>
<p>That means that if we can come up with a guess for α, then we can guess
the two other parameters by a linear fit to transformed data. Since α is
the asymptotic maximum, a guess of α = 200 could be reasonable. With
this guess, we can make a plot that should show an approximately linear
relationship (log 200 &asymp; 5.3):</p>
<p/>
</div>
<div class="page"><p/>
<p>280 16. Nonlinear curve fitting
</p>
<p>5 10 15 20
</p>
<p>&minus;
4
.0
</p>
<p>&minus;
3
.5
</p>
<p>&minus;
3
.0
</p>
<p>&minus;
2
.5
</p>
<p>&minus;
2
.0
</p>
<p>&minus;
1
.5
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>age
</p>
<p>lo
g
(5
</p>
<p>.3
 &minus;
</p>
<p> l
o
g
(h
</p>
<p>e
ig
</p>
<p>h
t)
</p>
<p>)
</p>
<p>Figure 16.3. Linearized plot of the Gompertz relation when assuming α &asymp; 200.
</p>
<p>&gt; plot(log(5.3-log(height))~age)
</p>
<p>Warning message:
</p>
<p>In log(5.3 - log(height)) : NaNs produced
</p>
<p>Notice that we got a warning that an NaN (Not a Number) value was pro-
duced. This is because one individual was taller than 200 cm, and we
therefore tried to take the logarithm of a negative value. The linearized
plot shows a clearly nonconstant variance and probably also some asym-
metry of the residual distribution, so the assumptions for linear regression
analysis are clearly violated. However, it is good enough for our purpose,
and a linear fit gives
</p>
<p>&gt; lm(log(5.3-log(height))~age)
</p>
<p>Call:
</p>
<p>lm(formula = log(5.3 - log(height)) ~ age)
</p>
<p>Coefficients:
</p>
<p>(Intercept) age
</p>
<p>0.4200 -0.1538
</p>
<p>Warning message:
</p>
<p>In log(5.3 - log(height)) : NaNs produced</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Finding starting values 281
</p>
<p>Accordingly, an initial guess of the parameters is
</p>
<p>log α = 5.3
</p>
<p>log β = 0.42
</p>
<p>γ = 0.15
</p>
<p>Supplying these guesses to nls and fitting the Gompertz curve yields
</p>
<p>&gt; fit &lt;- nls(height~alpha*exp(-beta*exp(-gamma*age)),
</p>
<p>+ start=c(alpha=exp(5.3),beta=exp(0.42),gamma=0.15))
</p>
<p>&gt; summary(fit)
</p>
<p>Formula: height ~ alpha * exp(-beta * exp(-gamma * age))
</p>
<p>Parameters:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>alpha 2.428e+02 1.157e+01 20.978 &lt;2e-16 ***
</p>
<p>beta 1.176e+00 1.892e-02 62.149 &lt;2e-16 ***
</p>
<p>gamma 7.903e-02 8.569e-03 9.222 &lt;2e-16 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 6.811 on 499 degrees of freedom
</p>
<p>Number of iterations to convergence: 8
</p>
<p>Achieved convergence tolerance: 5.283e-06
</p>
<p>(3 observations deleted due to missingness)
</p>
<p>The final estimates are quite a bit different from the starting values. This
reflects the crudeness of the estimation methods used for the starting
values. In particular, we used transformations that were based on the
mathematical form of the function but did not take the structure of the er-
ror variation into account. Also, the important parameter α was obtained
by eye.
</p>
<p>Looking at the fitted model, however, it is not reassuring that the final
estimate for α suggests that boys would continue growing until they are
243 cm tall (for readers in nonmetric countries, that is almost eight feet!).
Possibly, the Gompertz curve is just not a good fit for these data.
</p>
<p>We can overlay the original data with the fitted curve as follows
(Figure 16.4)
</p>
<p>&gt; plot(age, height)
</p>
<p>&gt; newage &lt;- seq(5,20,length=500)
</p>
<p>&gt; lines(newage, predict(fit,newdata=data.frame(age=newage)),lwd=2)</p>
<p/>
</div>
<div class="page"><p/>
<p>282 16. Nonlinear curve fitting
</p>
<p>5 10 15 20
</p>
<p>1
2
0
</p>
<p>1
4
0
</p>
<p>1
6
0
</p>
<p>1
8
0
</p>
<p>2
0
0
</p>
<p>age
</p>
<p>h
e
ig
</p>
<p>h
t
</p>
<p>Figure 16.4. The fitted Gompertz curve.
</p>
<p>The plot suggests that there is a tendency for the dispersion to increase
with increasing fitted values, so we attempt a log-scale fit. This can be
done expediently by transforming both sides of the model formula.
</p>
<p>&gt;
</p>
<p>&gt; fit &lt;- nls(log(height)~log(alpha*exp(-beta*exp(-gamma*age))),
</p>
<p>+ start=c(alpha=exp(5.3),beta=exp(.12),gamma=.12))
</p>
<p>&gt; summary(fit)
</p>
<p>Formula: log(height) ~ log(alpha * exp(-beta * exp(-gamma * age)))
</p>
<p>Parameters:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>alpha 255.97694 15.03920 17.021 &lt;2e-16 ***
</p>
<p>beta 1.18949 0.02971 40.033 &lt;2e-16 ***
</p>
<p>gamma 0.07033 0.00811 8.673 &lt;2e-16 ***
</p>
<p>---
</p>
<p>Signif. codes: 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1
</p>
<p>Residual standard error: 0.04307 on 499 degrees of freedom
</p>
<p>Number of iterations to convergence: 8
</p>
<p>Achieved convergence tolerance: 2.855e-06
</p>
<p>(3 observations deleted due to missingness)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Finding starting values 283
</p>
<p>5 10 15 20
</p>
<p>4
.7
</p>
<p>4
.8
</p>
<p>4
.9
</p>
<p>5
.0
</p>
<p>5
.1
</p>
<p>5
.2
</p>
<p>5
.3
</p>
<p>age
</p>
<p>lo
g
(h
</p>
<p>e
ig
</p>
<p>h
t)
</p>
<p>Figure 16.5. Fitted Gompertz curve on log scale.
</p>
<p>&gt; plot(age, log(height))
</p>
<p>&gt; lines(newage, predict(fit,newdata=data.frame(age=newage)),lwd=2)
</p>
<p>On the log-scale plot (Figure 16.5), the distribution around the curve ap-
pears to be more stable. The parameter estimates did not change much,
although the maximum height is now increased by a further 13 cm
(5 inches) and the γ parameter is reduced to compensate.
</p>
<p>Closer inspection of the plots (whether on log scale or not), however, re-
veals that the Gompertz curve tends to overshoot the data points at the
right end, where a much flatter curve would fit the data in the range from
15 years upwards. Although visually there is a nice overall fit, this is not
hard to obtain for a three-parameter family of curves, and the Gompertz
curves seem unable to fit the characteristic patterns of human growth.</p>
<p/>
</div>
<div class="page"><p/>
<p>284 16. Nonlinear curve fitting
</p>
<p>16.3 Self-starting models
</p>
<p>Finding starting values is an art rather than a craft, but once a stable
method has been found, it may be reasonable to assume that will apply
to most data sets from a given model. nls allows the nice feature that
the procedure for calculating starting values can be embodied in the ex-
pressions that are used on the right-hand side of the model formula. Such
functions are by convention named starting with &ldquo;SS&rdquo;, and R 2.6.2 comes
with 10 of these built-in. In particular, there is in fact an SSgompertz
function, so we could have saved ourselves much of the trouble of the
previous section by just writing
</p>
<p>&gt; summary(nls(height~SSgompertz(age, Asym, b2, b3)))
</p>
<p>Formula: height ~ SSgompertz(age, Asym, b2, b3)
</p>
<p>Parameters:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>Asym 2.428e+02 1.157e+01 20.98 &lt;2e-16 ***
</p>
<p>b2 1.176e+00 1.892e-02 62.15 &lt;2e-16 ***
</p>
<p>b3 9.240e-01 7.918e-03 116.69 &lt;2e-16 ***
</p>
<p>...
</p>
<p>Notice, though, that the parameterization is different: The parameter b3
is actually eγ, whereas the two other parameters are recognized as α and
β.
</p>
<p>One minor drawback of self-starting models is that you cannot just trans-
form them if you want to see if the model fits better on, for example, a log
scale. In other words, this fails:
</p>
<p>&gt; nls(log(height) ~ log(SSgompertz(age, Asym, b2, b3)))
</p>
<p>Error in nlsModel(formula, mf, start, wts) :
</p>
<p>singular gradient matrix at initial parameter estimates
</p>
<p>Calls: nls -&gt; switch -&gt; nlsModel
</p>
<p>In addition: Warning message:
</p>
<p>In nls(log(height) ~ log(SSgompertz(age, Asym, b2, b3))) :
</p>
<p>No starting values specified for some parameters.
</p>
<p>Intializing &lsquo;Asym&rsquo;, &lsquo;b2&rsquo;, &lsquo;b3&rsquo; to &rsquo;1.&rsquo;.
</p>
<p>Consider specifying &rsquo;start&rsquo; or using a selfStart model
</p>
<p>The error message means, in essence, that the self-start machinery is
turned off, so nls tries a wild guess, setting all parameters to 1, and then
fails to converge from that starting point.
</p>
<p>Using expression log(SSgompertz(age, Asym, b2, b3)) to com-
pute the expected value of log(height) is not a problem (in itself). We
can take the starting values from the untransformed fit but this is still not
enough to make things work.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 Profiling 285
</p>
<p>There is a hitch: SSgompertz returns a gradient attribute along with the
fitted values. This is the derivative of the fitted value with respect to each
of the model parameters. This speeds up the convergence process for the
original model but is plainly wrong for the transformed model, where it
causes convergence failure. We could patch this up by calculating the cor-
rect gradient, but it is expedient simply to discard the attribute by taking
as.vector.
</p>
<p>&gt; cf &lt;- coef(nls(height ~ SSgompertz(age, Asym, b2, b3)))
</p>
<p>&gt; summary(nls(log(height) ~
</p>
<p>+ log(as.vector(SSgompertz(age,Asym, b2, b3))),
</p>
<p>+ start=as.list(cf)))
</p>
<p>Formula: log(height) ~ log(as.vector(SSgompertz(age, Asym, b2, b3)))
</p>
<p>Parameters:
</p>
<p>Estimate Std. Error t value Pr(&gt;|t|)
</p>
<p>Asym 2.560e+02 1.504e+01 17.02 &lt;2e-16 ***
</p>
<p>b2 1.189e+00 2.971e-02 40.03 &lt;2e-16 ***
</p>
<p>b3 9.321e-01 7.559e-03 123.31 &lt;2e-16 ***
</p>
<p>...
</p>
<p>It is possible to write your own self-starting models. It is not hard once
you have some experience with R programming, but we shall not go
into details here. The essence is that you need two basic items: the
model expression and a function that calculates the starting values. You
must ensure that these adhere to some formal requirements, and then
a constructor function selfStart can be called to create the actual
self-starting function.
</p>
<p>16.4 Profiling
</p>
<p>We discussed profiling before in connection with glm and logistic re-
gression in Section 13.3. For nonlinear regression, there are some slight
differences: The function that is being profiled is not the likelihood func-
tion but the sum of squared deviations, and the approximate confidence
intervals are based on the t distribution rather than the normal distribu-
tion. Also, the plotting method does not by default use the signed version
of the profile, just the square root of the difference in the sum of squared
deviations.
</p>
<p>Profiling is designed to eliminate parameter curvature. The same model can
be formulated using different parameterizations (such as when Gompertz
curves could be defined using γ or b3 = eγ). The choice of parameteri-
zation can have a substantial influence on whether the distribution of the</p>
<p/>
</div>
<div class="page"><p/>
<p>286 16. Nonlinear curve fitting
</p>
<p>240 260 280 300 320
</p>
<p>0
.0
</p>
<p>1
.0
</p>
<p>2
.0
</p>
<p>alpha
</p>
<p>τ
</p>
<p>1.15 1.20 1.25 1.30
</p>
<p>0
.0
</p>
<p>1
.0
</p>
<p>2
.0
</p>
<p>beta
</p>
<p>τ
</p>
<p>0.05 0.06 0.07 0.08 0.09
</p>
<p>0
.0
</p>
<p>1
.0
</p>
<p>2
.0
</p>
<p>gamma
</p>
<p>τ
</p>
<p>Figure 16.6. Parameter profiles of the Gompertz fit on log scale.
</p>
<p>estimate is approximately normal or not, and this in turn means that the
use of symmetric confidence intervals based on the standard errors from
the model summary can be misleading. Profile-based confidence intervals
do not depend on parameterization &mdash; if you transform a parameter, the
ends of the confidence interval are just transformed in the same way.
</p>
<p>There is, however, also intrinsic curvature of the models. This describes
how far the model is from an approximating linear model. This kind of
curvature is independent of parameterization and is harder to adjust for
than parameter curvature. The effect of intrinsic curvature is that the t
distribution used for the calculation of profile-based confidence intervals
is not exactly the right distribution to use. Experience suggests that this
effect is usually much smaller than the distortions caused by parameter
curvature.
</p>
<p>For the Gompertz fit (after log transformation), we get the plots shown in
Figure 16.6.
</p>
<p>&gt; par(mfrow=c(3,1))
</p>
<p>&gt; plot(profile(fit))</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Finer control of the fitting algorithm 287
</p>
<p>The plots show that there is a marked curvature for the α and β pa-
rameters, reflected in the curved and asymmetric profiles, whereas the γ
profile is more linear and symmetric. This is also seen when comparing
the profile-based confidence intervals with those of confint.default,
which uses the normal approximation and the approximate standard
errors.
</p>
<p>&gt; confint(fit)
</p>
<p>Waiting for profiling to be done...
</p>
<p>2.5% 97.5%
</p>
<p>alpha 233.49688706 294.76696435
</p>
<p>beta 1.14429894 1.27416518
</p>
<p>gamma 0.05505754 0.08575007
</p>
<p>&gt; confint.default(fit)
</p>
<p>2.5 % 97.5 %
</p>
<p>alpha 226.50064512 285.45322721
</p>
<p>beta 1.13125578 1.24772846
</p>
<p>gamma 0.05443819 0.08622691
</p>
<p>16.5 Finer control of the fitting algorithm
</p>
<p>The Juul example that has been used in this chapter has been quite benign
because there are a large number of observations and an objective func-
tion that is relatively smooth as a function of the parameters. However,
convergence problems easily come up in less nice examples. Nonlinear
optimization is simply a tricky topic, to which we have no chance of do-
ing justice in this short chapter. The algorithms have several parameters
that can be adjusted in order to help convergence, but since we are not
describing the algorithms, it is hardly possible to give more than a feeling
for what can be done.
</p>
<p>The possibility of supplying a gradient of the fitted curve with respect
to parameters was mentioned earlier. If the curve is given by a simple
mathematical expression, then the deriv function can even be used to
generate the gradient automatically. If a gradient is not available, then the
algorithm will estimate it numerically; in practice, this often turns out to
be equally fast.
</p>
<p>The nls function features a trace argument that, if set to TRUE, allows
you to follow the parameters and the SSD iteration by iteration. This
is sometimes useful to get a handle on what is happening, for instance
whether the algorithm is making unreasonably large jumps. To actually
modify the behaviour, there is a single control argument, which can be
set to the return value of nls.control, which in turn has arguments to
set iteration limits and tolerances (and more).</p>
<p/>
</div>
<div class="page"><p/>
<p>288 16. Nonlinear curve fitting
</p>
<p>You can switch out the entire fitting method by using the algorithm
argument. Apart from the default algorithm, this allows the settings
"plinear" and "port". The former allows models of the form
</p>
<p>y = &sum;
i
</p>
<p>αi fi(x; βi)
</p>
<p>that are partially linear since the αi can be determined by multiple lin-
ear regression if the βi are considered fixed. To specify models with more
than one term, you let the expression on the right-hand side of the model
formula return a matrix instead of a vector. The latter algorithm uses a
routine from the PORT library from Lucent Technologies; this in partic-
ular allows you to set contraints on parameters by using the upper and
lower arguments to nls.
</p>
<p>It should be noted that all the available algorithms operate under the im-
plicit assumption that the SSD(β) is fairly smooth and well behaved, with
a well-defined global minimum and no other local minima nearby. There
are cases where this assumption is not warranted. In such cases, youmight
attack the minimization problem directly using the optim function.
</p>
<p>16.6 Exercises
</p>
<p>16.1 Try fitting the Gompertz model for girls in the Juul data. How
would you go about testing whether the same model fits both genders?
</p>
<p>16.2 The philion data contain four small-sample EC50 experiments
that are somewhat tricky to handle. We suggest the model y = ymax/(1+
(x/β)α). It may be useful to transform y by the square root since the data
are counts, and this stabilizes the variance of the Poisson distribution.
Consider how to obtain starting values for the model, and fit it with nls.
The "port" algorithm seems more stable for these data. For profiling and
confidence intervals, it seems to help if you set the alphamax argument
to 0.2.
</p>
<p>16.3 (Theoretical) Continuing with the philion data, consider what
happens if you modify the model to be y = ymax/(1+ x/β)α.</p>
<p/>
</div>
<div class="page"><p/>
<p>A
Obtaining and installing R and the
ISwR package
</p>
<p>The way to obtain R is to download it from one of the CRAN (Compre-
hensive R Archive Network) sites. The main site is
</p>
<p>http://cran.r-project.org/
</p>
<p>It has a number of mirror sites worldwide that may be closer to you and
give faster download times.
</p>
<p>Installation details tend to vary over time, so you should read the
accompanying documents and any other information offered on CRAN.
</p>
<p>Binary distributions
</p>
<p>As of this writing, the version for recent variants of Microsoft Win-
dows comes as a single R-2.6.2-win32.exe file, on which you simply
double-click with the mouse and then follow the on-screen instructions.
When the process is completed, you will have an entry under Programs
on the Start menu for invoking R, as well as a desktop icon.
</p>
<p>For Linux distributions that use the RPM package format (primarily Red-
Hat, Fedora, and SUSE), .rpm files of R and the recommended add-on
packages can be installed using the rpm command and the respective
system software management tools. Fedora now has R in its standard
repositories, and it is also in the repository of openSUSE.org. Debian</p>
<p/>
</div>
<div class="page"><p/>
<p>290 Appendix A. Obtaining and installing R and the ISwR package
</p>
<p>packages can be accessed through APT, the Debian package maintenance
tool, as can packages for Ubuntu (in both cases, make sure that you get
the r-recommended package). Further details are in the FAQ.
</p>
<p>For the Macintosh platforms, only OS X 10.2 and above are supported.
Installation is by downloading the disk image R-2.6.2.dmg and double-
clicking the &ldquo;R.mpkg&rdquo; icon found inside it.
</p>
<p>Installation from source
</p>
<p>Installation of Rfrom source code is possible on all supported platforms,
although not quite trivial on Windows, mainly because the build envi-
ronment is not part of the system. On Unix-like systems (Macintosh OS
X included), the process can be as simple as unpacking the sources and
writing
</p>
<p>./configure
</p>
<p>make
</p>
<p>make install
</p>
<p>The above works on widely used platforms, provided that the relevant
compilers and support libraries are installed. If your system is more eso-
teric or you want to use special compilers or libraries, then you may need
to dig deeper.
</p>
<p>For Windows, the directory src/gnuwin32 has an INSTALL file with
detailed information about the procedure to follow.
</p>
<p>Package installation
</p>
<p>To work through the examples and exercises in this book, you should
install the ISwR package, which contains the data sets.
</p>
<p>Assuming that you are connected to the Internet, you can start R and
from the Windows and Macintosh versions use their convenient menu
interfaces.
</p>
<p>On other platforms, you can type
</p>
<p>install.packages("ISwR")
</p>
<p>This will give off a harmless warning and install the package in the default
location.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A. Obtaining and installing R and the ISwR package 291
</p>
<p>OnUnix and Linux systems youwill need superuser permissions to install
in the system location. Similarly, you may require administrator access on
some Windows versions.
</p>
<p>Otherwise you can set up a private library directory and install into
that. Set the R_LIBS environment variable to use your private library
subsequently. Further details can be found on the help page for library.
</p>
<p>If your R machine is not connected to the Internet, you can also download
the package as a file via a different computer. For Windows and Macin-
tosh, you should get the binary package (.zip or .tgz extension) and
then installation from a local file is possible via a menu entry. For Unix
and Linux, you can issue the following at the shell prompt (the -l option
allows you to give a private library if needed):
</p>
<p>R CMD INSTALL ISwR
</p>
<p>More information
</p>
<p>Information and further Internet resources for R can be obtained from
CRAN and the R homepage at
</p>
<p>http://www.r-project.org
</p>
<p>Notice in particular the mailing lists, the user-contributed documents, and
the FAQs.</p>
<p/>
</div>
<div class="page"><p/>
<p>B
Data sets in the ISwR package1
</p>
<p>IgM Immunoglobulin G
</p>
<p>Description
</p>
<p>Serum IgM in 298 children aged 6 months to 6 years.
</p>
<p>Usage
</p>
<p>IgM
</p>
<p>Format
</p>
<p>A single numeric vector (g/l).
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 3.2,
Chapman &amp; Hall.
</p>
<p>1Reproduced with permission from the documentation files in the ISwR package.</p>
<p/>
</div>
<div class="page"><p/>
<p>294 Appendix B. Data sets in the ISwR package
</p>
<p>Examples
</p>
<p>stripchart(IgM,method="stack")
</p>
<p>alkfos Alkaline phosphatase data
</p>
<p>Description
</p>
<p>Repeated measurements of alkaline phosphatase in a randomized
trial of Tamoxifen treatment of breast cancer patients.
</p>
<p>Usage
</p>
<p>alkfos
</p>
<p>Format
</p>
<p>A data frame with 43 observations on the following 8 variables.
</p>
<p>grp a numeric vector, group code (1=placebo, 2=Tamoxifen).
c0 a numeric vector, concentration at baseline.
c3 a numeric vector, concentration after 3 months.
c6 a numeric vector, concentration after 6 months.
c9 a numeric vector, concentration after 9 months.
c12 a numeric vector, concentration after 12 months.
c18 a numeric vector, concentration after 18 months.
c24 a numeric vector, concentration after 24 months.
</p>
<p>Source
</p>
<p>Original data.
</p>
<p>References
</p>
<p>B. Kristensen et al. (1994), Tamoxifen and bone metabolism in post-
menopausal low-risk breast cancer patients: a randomized study.
Journal of Clinical Oncology, 12(2):992&ndash;997.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 295
</p>
<p>ashina Ashina&rsquo;s crossover trial
</p>
<p>Description
</p>
<p>The ashina data frame has 16 rows and 3 columns. It contains data
from a crossover trial for the effect of an NO synthase inhibitor on
headaches. Visual analog scale recordings of pain levels were made at
baseline and at five time points after infusion of the drug or placebo.
A score was calculated as the sum of the differences from baseline.
Data were recorded during two sessions for each patient. Six patients
were given treatment on the first occasion and the placebo on the sec-
ond. Ten patients had placebo first and then treatment. The order of
treatment and the placebo was randomized.
</p>
<p>Usage
</p>
<p>ashina
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>vas.active a numeric vector, summary score when given active
substance.
</p>
<p>vas.plac a numeric vector, summary score when given placebo
treatment.
</p>
<p>grp a numeric vector code, 1: placebo first, 2: active first.
</p>
<p>Source
</p>
<p>Original data.
</p>
<p>References
</p>
<p>M.Ashina et al. (1999), Effect of inhibition of nitric oxide synthase on
chronic tension-type headache: a randomised crossover trial. Lancet
353, 287&ndash;289
</p>
<p>Examples
</p>
<p>plot(vas.active~vas.plac,pch=grp,data=ashina)
</p>
<p>abline(0,1)</p>
<p/>
</div>
<div class="page"><p/>
<p>296 Appendix B. Data sets in the ISwR package
</p>
<p>bcmort Breast cancer mortality
</p>
<p>Description
</p>
<p>Danish study on the effect of screening for breast cancer.
</p>
<p>Usage
</p>
<p>bcmort
</p>
<p>Format
</p>
<p>A data frame with 24 observations on the following 4 variables.
</p>
<p>age a factor with levels 50-54, 55-59, 60-64, 65-69, 70-74, and
75-79.
</p>
<p>cohort a factor with levels Study gr., Nat.ctr., Hist.ctr.,
and Hist.nat.ctr..
</p>
<p>bc.deaths a numeric vector, number of breast cancer deaths.
p.yr a numeric vector, person-years under study.
</p>
<p>Details
</p>
<p>Four cohorts were collected. The &ldquo;study group&rdquo; consists of the pop-
ulation of women in the appropriate age range in Copenhagen and
Frederiksberg after the introduction of routine mammography screen-
ing. The &ldquo;national control group&rdquo; consisted of the population in the
parts of Denmark in which routine mammography screening was
not available. These two groups were both collected in the years
1991&ndash;2001. The &ldquo;historical control group&rdquo; and the &ldquo;historical national
control group&rdquo; are similar cohorts from 10 years earlier (1981&ndash;1991),
before the introduction of screening in Copenhagen and Frederiks-
berg. The study group comprises the entire population, not just those
accepting the invitation to be screened.
</p>
<p>Source
</p>
<p>A.H. Olsen et al. (2005), Breast cancer mortality in Copenhagen after
introduction of mammography screening. British Medical Journal, 330:
220&ndash;222.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 297
</p>
<p>bp.obese Obesity and blood pressure
</p>
<p>Description
</p>
<p>The bp.obese data frame has 102 rows and 3 columns. It contains
data from a random sample of Mexican-American adults in a small
California town.
</p>
<p>Usage
</p>
<p>bp.obese
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>sex a numeric vector code, 0: male, 1: female.
obese a numeric vector, ratio of actual weight to ideal weight from
</p>
<p>New York Metropolitan Life Tables.
bp a numeric vector,systolic blood pressure (mm Hg).
</p>
<p>Source
</p>
<p>B.W. Brown and M. Hollander (1977), Statistics: A Biomedical Introduc-
tion,Wiley.
</p>
<p>Examples
</p>
<p>plot(bp~obese,pch = ifelse(sex==1, "F", "M"), data = bp.obese)
</p>
<p>caesarean Caesarean section and maternal shoe size
</p>
<p>Description
</p>
<p>The table caesar.shoe contains the relation between caesarean
section and maternal shoe size (UK sizes!).
</p>
<p>Usage
</p>
<p>caesar.shoe</p>
<p/>
</div>
<div class="page"><p/>
<p>298 Appendix B. Data sets in the ISwR package
</p>
<p>Format
</p>
<p>A matrix with two rows and six columns.
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 10.1,
Chapman &amp; Hall.
</p>
<p>Examples
</p>
<p>prop.trend.test(caesar.shoe["Yes",],margin.table(caesar.shoe,2))
</p>
<p>coking Coking data
</p>
<p>Description
</p>
<p>The coking data frame has 18 rows and 3 columns. It contains
the time to coking in an experiment where the oven width and
temperature were varied.
</p>
<p>Usage
</p>
<p>coking
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>width a factor with levels 4, 8, and 12, giving the oven width in
inches.
</p>
<p>temp a factor with levels 1600 and 1900, giving the temperature in
Fahrenheit.
</p>
<p>time a numeric vector, time to coking.
</p>
<p>Source
</p>
<p>R.A. Johnson (1994), Miller and Freund&rsquo;s Probability and Statistics for
Engineers, 5th ed., Prentice-Hall.
</p>
<p>Examples
</p>
<p>attach(coking)
</p>
<p>matplot(tapply(time,list(width,temp),mean))
</p>
<p>detach(coking)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 299
</p>
<p>cystfibr Cystic fibrosis lung function data
</p>
<p>Description
</p>
<p>The cystfibr data frame has 25 rows and 10 columns. It contains
lung function data for cystic fibrosis patients (7&ndash;23 years old).
</p>
<p>Usage
</p>
<p>cystfibr
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>age a numeric vector, age in years.
sex a numeric vector code, 0: male, 1:female.
height a numeric vector, height (cm).
weight a numeric vector, weight (kg).
bmp a numeric vector, body mass (% of normal).
fev1 a numeric vector, forced expiratory volume.
rv a numeric vector, residual volume.
frc a numeric vector, functional residual capacity.
tlc a numeric vector, total lung capacity.
pemax a numeric vector, maximum expiratory pressure.
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 12.11,
Chapman &amp; Hall.
</p>
<p>References
</p>
<p>O&rsquo;Neill et al. (1983), The effects of chronic hyperinflation, nutritional
status, and posture on respiratory muscle strength in cystic fibrosis,
Am. Rev. Respir. Dis., 128:1051&ndash;1054.</p>
<p/>
</div>
<div class="page"><p/>
<p>300 Appendix B. Data sets in the ISwR package
</p>
<p>eba1977 Lung cancer incidence in four Danish cities 1968&ndash;
1971
</p>
<p>Description
</p>
<p>This data set contains counts of incident lung cancer cases and
population size in four neighbouring Danish cities by age group.
</p>
<p>Usage
</p>
<p>eba1977
</p>
<p>Format
</p>
<p>A data frame with 24 observations on the following 4 variables:
</p>
<p>city a factor with levels Fredericia, Horsens, Kolding, and
Vejle.
</p>
<p>age a factor with levels 40-54, 55-59, 60-64, 65-69, 70-74, and
75+.
</p>
<p>pop a numeric vector, number of inhabitants.
cases a numeric vector, number of lung cancer cases.
</p>
<p>Details
</p>
<p>These data were &ldquo;at the center of public interest in Denmark in 1974&rdquo;,
according to Erling Andersen&rsquo;s paper. The city of Fredericia has a
substantial petrochemical industry in the harbour area.
</p>
<p>Source
</p>
<p>E.B. Andersen (1977), Multiplicative Poissonmodels with unequal cell
rates, Scandinavian Journal of Statistics, 4:153&ndash;158.
</p>
<p>References
</p>
<p>J. Clemmensen et al. (1974), Ugeskrift for L&aelig;ger, pp. 2260&ndash;2268.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 301
</p>
<p>energy Energy expenditure
</p>
<p>Description
</p>
<p>The energy data frame has 22 rows and 2 columns. It contains data
on the energy expenditure in groups of lean and obese women.
</p>
<p>Usage
</p>
<p>energy
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>expend a numeric vector, 24 hour energy expenditure (MJ).
stature a factor with levels lean and obese.
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 9.4,
Chapman &amp; Hall.
</p>
<p>Examples
</p>
<p>plot(expend~stature,data=energy)
</p>
<p>ewrates Rates of lung and nasal cancer mortality, and total
mortality.
</p>
<p>Description
</p>
<p>England andWales mortality rates from lung cancer, nasal cancer, and
all causes, 1936&ndash;1980. The 1936 rates are repeated as 1931 rates in order
to accommodate follow-up for the nickel study.
</p>
<p>Usage
</p>
<p>ewrates</p>
<p/>
</div>
<div class="page"><p/>
<p>302 Appendix B. Data sets in the ISwR package
</p>
<p>Format
</p>
<p>A data frame with 150 observations on the following 5 variables:
</p>
<p>year calendar period, 1931: 1931&ndash;35, 1936: 1936&ndash;40, . . . .
age age class, 10: 10&ndash;14, 15:15&ndash;19, . . . .
lung lung cancer mortality rate per 1 million person-years
nasal nasal cancer mortality rate per 1 million person-years
other all cause mortality rate per 1 million person-years
</p>
<p>Details
</p>
<p>Taken from the &ldquo;Epi&rdquo; package by Bendix Carstensen et al.
</p>
<p>Source
</p>
<p>N.E. Breslow, and N. Day (1987). Statistical Methods in Cancer Research.
Volume II: The Design and Analysis of Cohort Studies, Appendix IX. IARC
Scientific Publications, Lyon.
</p>
<p>fake.trypsin Trypsin by age groups
</p>
<p>Description
</p>
<p>The trypsin data frame has 271 rows and 3 columns. Serum levels
of immunoreactive trypsin in healthy volunteers (faked!).
</p>
<p>Usage
</p>
<p>fake.trypsin
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>trypsin a numeric vector, serum-trypsin in ng/ml.
grp a numeric vector, age coding. See below.
grpf a factor with levels 1: age 10&ndash;19, 2: age 20&ndash;29, 3: age 30&ndash;39, 4:
</p>
<p>age 40&ndash;49, 5: age 50&ndash;59, and 6: age 60&ndash;69.
</p>
<p>Details
</p>
<p>Data have been simulated to match given group means and SD.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 303
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 9.12,
Chapman &amp; Hall.
</p>
<p>Examples
</p>
<p>plot(trypsin~grp, data=fake.trypsin)
</p>
<p>graft.vs.host Graft versus host disease
</p>
<p>Description
</p>
<p>The gvhd data frame has 37 rows and 7 columns. It contains data
from patients receiving a nondepleted allogenic bone marrow trans-
plant with the purpose of finding variables associated with the
development of acute graft-versus-host disease.
</p>
<p>Usage
</p>
<p>graft.vs.host
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>pnr a numeric vector patient number.
rcpage a numeric vector, age of recipient (years).
donage a numeric vector, age of donor (years).
type a numeric vector, type of leukaemia coded 1: AML, 2: ALL, 3:
</p>
<p>CML for acute myeloid, acute lymphatic, and chronic myeloid
leukaemia.
</p>
<p>preg a numeric vector code indicating whether donor has been
pregnant. 0: no, 1: yes.
</p>
<p>index a numeric vector giving an index of mixed epidermal cell-
lymphocyte reactions.
</p>
<p>gvhd a numeric vector code, graft-versus-host disease, 0: no, 1: yes.
time a numeric vector, follow-up time
dead a numeric vector code, 0: no (censored), 1: yes
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Exercise
12.3, Chapman &amp; Hall.</p>
<p/>
</div>
<div class="page"><p/>
<p>304 Appendix B. Data sets in the ISwR package
</p>
<p>Examples
</p>
<p>plot(jitter(gvhd,0.2)~index,data=graft.vs.host)
</p>
<p>heart.rate Heart rates after enalaprilat
</p>
<p>Description
</p>
<p>The heart.rate data frame has 36 rows and 3 columns. It contains
data for nine patients with congestive heart failure before and shortly
after administration of enalaprilat, in a balanced two-way layout.
</p>
<p>Usage
</p>
<p>heart.rate
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>hr a numeric vector, heart rate in beats per minute.
subj a factor with levels 1 to 9.
time a factor with levels 0 (before), 30, 60, and 120 (minutes after
</p>
<p>administration).
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 12.2,
Chapman &amp; Hall.
</p>
<p>Examples
</p>
<p>evalq(interaction.plot(time,subj,hr), heart.rate)
</p>
<p>hellung Growth of Tetrahymena cells
</p>
<p>Description
</p>
<p>The hellung data frame has 51 rows and 3 columns. diameter and
concentration of Tetrahymena cells with and without glucose added to
growth medium.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 305
</p>
<p>Usage
</p>
<p>hellung
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>glucose a numeric vector code, 1: yes, 2: no.
conc a numeric vector, cell concentration (counts/ml).
diameter a numeric vector, cell diameter (&micro;m).
</p>
<p>Source
</p>
<p>D. Kronborg and L.T. Skovgaard (1990), Regressionsanalyse, Table 1.1,
FADLs Forlag (in Danish).
</p>
<p>Examples
</p>
<p>plot(diameter~conc,pch=glucose,log="xy",data=hellung)
</p>
<p>intake Energy intake
</p>
<p>Description
</p>
<p>The intake data frame has 11 rows and 2 columns. It contains paired
values of energy intake for 11 women.
</p>
<p>Usage
</p>
<p>intake
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>pre a numeric vector, premenstrual intake (kJ).
post a numeric vector, postmenstrual intake (kJ).
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 9.3,
Chapman &amp; Hall.</p>
<p/>
</div>
<div class="page"><p/>
<p>306 Appendix B. Data sets in the ISwR package
</p>
<p>Examples
</p>
<p>plot(intake$pre, intake$post)
</p>
<p>juul Juul&rsquo;s IGF data
</p>
<p>Description
</p>
<p>The juul data frame has 1339 rows and 6 columns. It contains a ref-
erence sample of the distribution of insulin-like growth factor (IGF-I),
one observation per subject in various ages, with the bulk of the data
collected in connection with school physical examinations.
</p>
<p>Usage
</p>
<p>juul
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>age a numeric vector (years).
menarche a numeric vector. Has menarche occurred (code 1: no, 2:
</p>
<p>yes)?
sex a numeric vector (1: boy, 2: girl).
igf1 a numeric vector, insulin-like growth factor (&micro;g/l).
tanner a numeric vector, codes 1&ndash;5: Stages of puberty ad modum
</p>
<p>Tanner.
testvol a numeric vector, testicular volume (ml).
</p>
<p>Source
</p>
<p>Original data.
</p>
<p>Examples
</p>
<p>plot(igf1~age, data=juul)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 307
</p>
<p>juul2 Juul&rsquo;s IGF data, extended version
</p>
<p>Description
</p>
<p>The juul2 data frame has 1339 rows and 8 columns; extended version
of juul.
</p>
<p>Usage
</p>
<p>juul2
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>age a numeric vector (years).
height a numeric vector (cm).
menarche a numeric vector. Has menarche occurred (code 1: no, 2:
</p>
<p>yes)?
sex a numeric vector (1: boy, 2: girl).
igf1 a numeric vector, insulin-like growth factor (&micro;g/l).
tanner a numeric vector, codes 1&ndash;5: Stages of puberty ad modum
</p>
<p>Tanner.
testvol a numeric vector, testicular volume (ml).
weight a numeric vector, weight (kg).
</p>
<p>Source
</p>
<p>Original data.
</p>
<p>Examples
</p>
<p>plot(igf1~age, data=juul2)
</p>
<p>kfm Breast-feeding data
</p>
<p>Description
</p>
<p>The kfm data frame has 50 rows and 7 columns. It was collected by
Kim Fleischer Michaelsen and contains data for 50 infants of age ap-
proximately 2 months. They were weighed immediately before and</p>
<p/>
</div>
<div class="page"><p/>
<p>308 Appendix B. Data sets in the ISwR package
</p>
<p>after each breast feeding. and the measured intake of breast milk was
registered along with various other data.
</p>
<p>Usage
</p>
<p>kfm
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>no a numeric vector, identification number.
dl.milk a numeric vector, breast-milk intake (dl/24h).
sex a factor with levels boy and girl.
weight a numeric vector, weight of child (kg).
ml.suppl a numeric vector, supplementarymilk substitute (ml/24h).
mat.weight a numeric vector, weight of mother (kg).
mat.height a numeric vector, height of mother (cm).
</p>
<p>Note
</p>
<p>The amount of supplementarymilk substitute refers to a period before
the data collection.
</p>
<p>Source
</p>
<p>Original data.
</p>
<p>Examples
</p>
<p>plot(dl.milk~mat.height,pch=c(1,2)[sex],data=kfm)
</p>
<p>lung Methods for determining lung volume
</p>
<p>Description
</p>
<p>The lung data frame has 18 rows and 3 columns. It contains data on
three different methods of determining human lung volume.
</p>
<p>Usage
</p>
<p>lung</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 309
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>volume a numeric vector, measured lung volume.
method a factor with levels A, B, and C.
subject a factor with levels 1&ndash;6.
</p>
<p>Source
</p>
<p>Anon. (1977), Exercises in Applied Statistics, Exercise 4.15, Dept. of
Theoretical Statistics, Aarhus University.
</p>
<p>malaria Malaria antibody data
</p>
<p>Description
</p>
<p>The malaria data frame has 100 rows and 4 columns.
</p>
<p>Usage
</p>
<p>malaria
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>subject subject code.
age age in years.
ab antibody level.
mal a numeric vector code, Malaria: 0: no, 1: yes.
</p>
<p>Details
</p>
<p>A random sample of 100 children aged 3&ndash;15 years from a village in
Ghana. The children were followed for a period of 8 months. At the
beginning of the study, values of a particular antibody were assessed.
Based on observations during the study period, the children were cat-
egorized into two groups: individuals with and without symptoms of
malaria.
</p>
<p>Source
</p>
<p>Unpublished data.</p>
<p/>
</div>
<div class="page"><p/>
<p>310 Appendix B. Data sets in the ISwR package
</p>
<p>Examples
</p>
<p>summary(malaria)
</p>
<p>melanom Survival after malignant melanoma
</p>
<p>Description
</p>
<p>The melanom data frame has 205 rows and 7 columns. It contains
data relating to the survival of patients after an operation for ma-
lignant melanoma, collected at Odense University Hospital by K.T.
Drzewiecki.
</p>
<p>Usage
</p>
<p>melanom
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>no a numeric vector, patient code.
status a numeric vector code, survival status; 1: dead frommelanoma,
</p>
<p>2: alive, 3: dead from other cause.
days a numeric vector, observation time.
ulc a numeric vector code, ulceration; 1: present, 2: absent.
thick a numeric vector, tumor thickness (1/100 mm).
sex a numeric vector code; 1: female, 2: male.
</p>
<p>Source
</p>
<p>P.K. Andersen, &Oslash;. Borgan, R.D. Gill, and N. Keiding (1991), Statistical
Models Based on Counting Processes, Appendix 1, Springer-Verlag.
</p>
<p>Examples
</p>
<p>require(survival)
</p>
<p>plot(survfit(Surv(days,status==1),data=melanom))</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 311
</p>
<p>nickel Nickel smelters in South Wales
</p>
<p>Description
</p>
<p>The data concern a cohort of nickel smelting workers in South Wales,
with information on exposure, follow-up period, and cause of death.
</p>
<p>Usage
</p>
<p>nickel
</p>
<p>Format
</p>
<p>A data frame containing 679 observations of the following 7 variables:
</p>
<p>id subject identifier (numeric).
icd ICD cause of death if dead, 0 otherwise (numeric).
exposure exposure index for workplace (numeric)
dob date of birth (numeric).
age1st age at first exposure (numeric).
agein age at start of follow-up (numeric).
ageout age at end of follow-up (numeric).
</p>
<p>Details
</p>
<p>Taken from the &ldquo;Epi&rdquo; package by Bendix Carstensen et al. For com-
parison purposes, England and Wales mortality rates (per 1,000,000
per annum) from lung cancer (ICDs 162 and 163), nasal cancer (ICD
160), and all causes, by age group and calendar period, are supplied
in the data set ewrates.
</p>
<p>Source
</p>
<p>N.E. Breslow and N. Day (1987). Statistical Methods in Cancer Research.
Volume II: The Design and Analysis of Cohort Studies, IARC Scientific
Publications, Lyon.</p>
<p/>
</div>
<div class="page"><p/>
<p>312 Appendix B. Data sets in the ISwR package
</p>
<p>nickel.expand Nickel smelters in South Wales, expanded
</p>
<p>Description
</p>
<p>The data concern a cohort of nickel smelting workers in South Wales,
with information on exposure, follow-up period, and cause of death,
as in the nickel data. This version has follow-up times split ac-
cording to age groups and is merged with the mortality rates in
ewrates.
</p>
<p>Usage
</p>
<p>nickel.expand
</p>
<p>Format
</p>
<p>A data frame with 3724 observations on the following 12 variables:
</p>
<p>agr age class: 10: 10&ndash;14, 15: 15&ndash;19, . . . .
ygr calendar period, 1931: 1931&ndash;35, 1936: 1936&ndash;40, . . . .
id subject identifier (numeric).
icd ICD cause of death if dead, 0 otherwise (numeric).
exposure exposure index for workplace (numeric).
dob date of birth (numeric).
age1st age at first exposure (numeric).
agein age at start of follow-up (numeric).
ageout age at end of follow-up (numeric).
lung lung cancer mortality rate per 1 million person-years.
nasal nasal cancer mortality rate per 1 million person-years.
other all cause mortality rate per 1 million person-years.
</p>
<p>Source
</p>
<p>Computed from nickel and ewrates data sets.
</p>
<p>philion Dose response data
</p>
<p>Description
</p>
<p>Four small experiments with the purpose of estimating the EC50 of a
biological dose-response relation.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 313
</p>
<p>Usage
</p>
<p>philion
</p>
<p>Format
</p>
<p>A data frame with 30 observations on the following 3 variables:
</p>
<p>experiment a numeric vector; codes 1 through 4 denote the experi-
ment number.
</p>
<p>dose a numeric vector, the dose.
response a numeric vector, the response (counts).
</p>
<p>Details
</p>
<p>These data were discussed on the R mailing lists, initially suggest-
ing a log-linear Poisson regression, but actually a relation like y =
ymax/(1+ (x/β)α) is more suitable.
</p>
<p>Source
</p>
<p>Original data from Vincent Philion, IRDA, Qu&eacute;bec.
</p>
<p>References
</p>
<p>http://tolstoy.newcastle.edu.au/R/help/03b/1121.html
</p>
<p>react Tuberculin reactions
</p>
<p>Description
</p>
<p>The numeric vector react contains differences between two nurses&rsquo;
determinations of 334 tuberculin reaction sizes.
</p>
<p>Usage
</p>
<p>react
</p>
<p>Format
</p>
<p>A single vector, differences between reaction sizes in mm.</p>
<p/>
</div>
<div class="page"><p/>
<p>314 Appendix B. Data sets in the ISwR package
</p>
<p>Source
</p>
<p>Anon. (1977), Exercises in Applied Statistics, Exercise 2.9, Dept. of
Theoretical Statistics, Aarhus University.
</p>
<p>Examples
</p>
<p>hist(react) # not good because of discretization effects...
</p>
<p>plot(density(react))
</p>
<p>red.cell.folate Red cell folate data
</p>
<p>Description
</p>
<p>The folate data frame has 22 rows and 2 columns. It contains data
on red cell folate levels in patients receiving three different methods
of ventilation during anesthesia.
</p>
<p>Usage
</p>
<p>red.cell.folate
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>folate a numeric vector, folate concentration (&micro;g/l).
ventilation a factor with levels N2O+O2,24h: 50% nitrous oxide
</p>
<p>and 50% oxygen, continuously for 24 hours; N2O+O2,op: 50%
nitrous oxide and 50% oxygen, only during operation; O2,24h:
no nitrous oxide but 35%&ndash;50% oxygen for 24 hours.
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 9.10,
Chapman &amp; Hall.
</p>
<p>Examples
</p>
<p>plot(folate~ventilation,data=red.cell.folate)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 315
</p>
<p>rmr Resting metabolic rate
</p>
<p>Description
</p>
<p>The rmr data frame has 44 rows and 2 columns. It contains the resting
metabolic rate and body weight data for 44 women.
</p>
<p>Usage
</p>
<p>rmr
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>body.weight a numeric vector, body weight (kg).
metabolic.rate a numeric vector, metabolic rate (kcal/24hr).
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Exercise
11.2, Chapman &amp; Hall.
</p>
<p>Examples
</p>
<p>plot(metabolic.rate~body.weight,data=rmr)
</p>
<p>secher Birth weight and ultrasonography
</p>
<p>Description
</p>
<p>The secher data frame has 107 rows and 4 columns. It contains ultra-
sonographic measurements of fetuses immediately before birth and
their subsequent birth weight.
</p>
<p>Usage
</p>
<p>secher</p>
<p/>
</div>
<div class="page"><p/>
<p>316 Appendix B. Data sets in the ISwR package
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>bwt a numeric vector, birth weight (g).
bpd a numeric vector, biparietal diameter (mm).
ad a numeric vector, abdominal diameter (mm).
no a numeric vector, observation number.
</p>
<p>Source
</p>
<p>D. Kronborg and L.T. Skovgaard (1990), Regressionsanalyse, Table 3.1,
FADLs Forlag (in Danish).
Secher et al. (1987), European Journal of Obstetrics, Gynecology, and
Reproductive Biology, 24: 1&ndash;11.
</p>
<p>Examples
</p>
<p>plot(bwt~ad, data=secher, log="xy")
</p>
<p>secretin Secretin-induced blood glucose changes
</p>
<p>Description
</p>
<p>The secretin data frame has 50 rows and 6 columns. It contains data
from a glucose response experiment.
</p>
<p>Usage
</p>
<p>secretin
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>gluc a numeric vector, blood glucose level.
person a factor with levels A&ndash;E.
time a factor with levels 20, 30, 60, 90 (minutes since injection), and
</p>
<p>pre (before injection).
repl a factor with levels a: 1st sample; b: 2nd sample.
time20plus a factor with levels 20+: 20 minutes or longer since
</p>
<p>injection; pre: before injection.
time.comb a factor with levels 20: 20 minutes since injection; 30+:
</p>
<p>30 minutes or longer since injection; pre: before injection.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 317
</p>
<p>Details
</p>
<p>Secretin is a hormone of the duodenal mucous membrane. An extract
was administered to five patients with arterial hypertension. Primary
registrations (double determination) of blood glucose were on graph
paper and later quantified with the smallest of the two measurements
recorded first.
</p>
<p>Source
</p>
<p>Anon. (1977), Exercises in Applied Statistics, Exercise 5.8, Dept. of
Theoretical Statistics, Aarhus University.
</p>
<p>stroke Estonian stroke data
</p>
<p>Description
</p>
<p>All cases of stroke in Tartu, Estonia, during the period 1991&ndash;1993, with
follow-up until January 1, 1996.
</p>
<p>Usage
</p>
<p>stroke
</p>
<p>Format
</p>
<p>A data frame with 829 observations on the following 10 variables.
</p>
<p>sex a factor with levels Female and Male.
died a Date, date of death.
dstr a Date, date of stroke.
age a numeric vector, age at stroke.
dgn a factor, diagnosis, with levels ICH (intracranial haemorrhage),
</p>
<p>ID (unidentified). INF (infarction, ischaemic), SAH (subarchnoid
haemorrhage).
</p>
<p>coma a factor with levels No and Yes, indicating whether patient was
in coma after the stroke.
</p>
<p>diab a factor with levels No and Yes, history of diabetes.
minf a factor with levels No and Yes, history of myocardial infarc-
</p>
<p>tion.
han a factor with levels No and Yes, history of hypertension.
obsmonths a numeric vector, observation times in months (set to 0.1
</p>
<p>for patients dying on the same day as the stroke).
dead a logical vector, whether patient died during the study.</p>
<p/>
</div>
<div class="page"><p/>
<p>318 Appendix B. Data sets in the ISwR package
</p>
<p>Source
</p>
<p>Original data.
</p>
<p>References
</p>
<p>J. Korv, M. Roose, and A.E. Kaasik (1997). Stroke Registry of Tartu,
Estonia, from 1991 through 1993. Cerebrovascular Disorders 7:154&ndash;
162.
</p>
<p>tb.dilute Tuberculin dilution assay
</p>
<p>Description
</p>
<p>The tb.dilute data frame has 18 rows and 3 columns. It contains
data from a drug test involving dilutions of tuberculin.
</p>
<p>Usage
</p>
<p>tb.dilute
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>reaction a numeric vector, reaction sizes (average of diameters) for
tuberculin skin pricks.
</p>
<p>animal a factor with levels 1&ndash;6.
logdose a factor with levels 0.5, 0, and -0.5.
</p>
<p>Details
</p>
<p>The actual dilutions were 1:100, 1:100
&radic;
10, 1:1000. Setting the middle
</p>
<p>one to 1 and using base-10 logarithms gives the logdose values.
</p>
<p>Source
</p>
<p>Anon. (1977), Exercises in Applied Statistics, part of Exercise 4.15, Dept.
of Theoretical Statistics, Aarhus University.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 319
</p>
<p>thuesen Ventricular shortening velocity
</p>
<p>Description
</p>
<p>The thuesen data frame has 24 rows and 2 columns. It contains
ventricular shortening velocity and blood glucose for type 1 diabetic
patients.
</p>
<p>Usage
</p>
<p>thuesen
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>blood.glucose a numeric vector, fasting blood glucose (mmol/l).
short.velocity a numeric vector, mean circumferential shorten-
</p>
<p>ing velocity (%/s).
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Table 11.6,
Chapman &amp; Hall.
</p>
<p>Examples
</p>
<p>plot(short.velocity~blood.glucose, data=thuesen)
</p>
<p>tlc Total lung capacity
</p>
<p>Description
</p>
<p>The tlc data frame has 32 rows and 4 columns. It contains data on
pretransplant total lung capacity (TLC) for recipients of heart-lung
transplants by whole-body plethysmography.
</p>
<p>Usage
</p>
<p>tlc</p>
<p/>
</div>
<div class="page"><p/>
<p>320 Appendix B. Data sets in the ISwR package
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>age a numeric vector, age of recipient (years).
sex a numeric vector code, female: 1, male: 2.
height a numeric vector, height of recipient (cm).
tlc a numeric vector, total lung capacity (l).
</p>
<p>Source
</p>
<p>D.G. Altman (1991), Practical Statistics for Medical Research, Exercise
12.5, 10.1, Chapman &amp; Hall.
</p>
<p>Examples
</p>
<p>plot(tlc~height,data=tlc)
</p>
<p>vitcap Vital capacity
</p>
<p>Description
</p>
<p>The vitcap data frame has 24 rows and 3 columns. It contains data
on vital capacity for workers in the cadmium industry. It is a subset of
the vitcap2 data set.
</p>
<p>Usage
</p>
<p>vitcap
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>group a numeric vector; group codes are 1: exposed &gt; 10 years, 3: not
exposed.
</p>
<p>age a numeric vector, age in years.
vital.capacity a numeric vector, vital capacity (a measure of
</p>
<p>lung volume) in liters.
</p>
<p>Source
</p>
<p>P. Armitage and G. Berry (1987), Statistical Methods in Medical Research,
2nd ed., Blackwell, p.286.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 321
</p>
<p>Examples
</p>
<p>plot(vital.capacity~age, pch=group, data=vitcap)
</p>
<p>vitcap2 Vital capacity, full data set
</p>
<p>Description
</p>
<p>The vitcap2 data frame has 84 rows and 3 columns. Age and vital
capacity for workers in the cadmium industry.
</p>
<p>Usage
</p>
<p>vitcap2
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>group a numeric vector; group codes are 1: exposed &gt; 10 years, 2:
exposed &lt; 10 years, 3: not exposed.
</p>
<p>age a numeric vector, age in years.
vital.capacity a numeric vector, vital capacity (a measure of
</p>
<p>lung volume) (l).
</p>
<p>Source
</p>
<p>P. Armitage and G. Berry (1987), Statistical Methods in Medical Research,
2nd ed., Blackwell, p.286.
</p>
<p>Examples
</p>
<p>plot(vital.capacity~age, pch=group, data=vitcap2)
</p>
<p>wright Comparison of Wright peak-flow meters
</p>
<p>Description
</p>
<p>The wright data frame has 17 rows and 2 columns. It contains data
on peak expiratory flow rate with two different flow meters on each
of 17 subjects.</p>
<p/>
</div>
<div class="page"><p/>
<p>322 Appendix B. Data sets in the ISwR package
</p>
<p>Usage
</p>
<p>wright
</p>
<p>Format
</p>
<p>This data frame contains the following columns:
</p>
<p>std.wright a numeric vector, data from large flow meter (l/min).
mini.wright a numeric vector, data from mini flow meter (l/min).
</p>
<p>Source
</p>
<p>J.M. Bland and D.G. Altman (1986), Statistical methods for assess-
ing agreement between two methods of clinical measurement, Lancet,
1:307&ndash;310.
</p>
<p>Examples
</p>
<p>plot(wright)
</p>
<p>abline(0,1)
</p>
<p>zelazo Age at walking
</p>
<p>Description
</p>
<p>The zelazo object is a list with four components.
</p>
<p>Usage
</p>
<p>zelazo
</p>
<p>Format
</p>
<p>This is a list containing data on age at walking (in months) for four
groups of infants:
</p>
<p>active test group receiving active training; these children had their
walking and placing reflexes trained during four three-minute
sessions that took place every day from their second to their
eighth week of life.
</p>
<p>passive passive training group; these children received the same
types of social and gross motor stimulation, but did not have
their specific walking and placing reflexes trained.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B. Data sets in the ISwR package 323
</p>
<p>none no training; these children had no special training, but were
tested along with the children who underwent active or passive
training.
</p>
<p>ctr.8w eighth-week controls; these children had no training and
were only tested at the age of 8 weeks.
</p>
<p>Note
</p>
<p>When asked to enter these data from a text source, many students will
use one vector per group and will need to reformat data into a data
frame for some uses. The rather unusual format of this data set mimics
that situation.
</p>
<p>Source
</p>
<p>P.R. Zelazo, N.A. Zelazo, and S. Kolb (1972), &ldquo;Walking&rdquo; in the
newborn, Science, 176: 314&ndash;315.</p>
<p/>
</div>
<div class="page"><p/>
<p>C
Compendium
</p>
<p>Elementary
</p>
<p>Commands
</p>
<p>ls() or objects() List objects in workspace
rm(object) Delete object
search() Search path
</p>
<p>Variable names
</p>
<p>Combinations of letters, digits, and period. Must not start with a
digit. Avoid starting with period.
</p>
<p>Assignments
</p>
<p>&lt;- Assign value to variable
-&gt; Assignment &ldquo;to the right&rdquo;
&lt;&lt;- Global assignment (in functions)</p>
<p/>
</div>
<div class="page"><p/>
<p>326 Appendix C. Compendium
</p>
<p>Operators
</p>
<p>Arithmetic
</p>
<p>+ Addition
- Subtraction, sign
* Multiplication
/ Division
^ Raise to power
%/% Integer division
%% Remainder from integer division
</p>
<p>Logical and relational
</p>
<p>== Equal to
!= Not equal to
&lt; Less than
&gt; Greater than
&lt;= Less than or equal to
&gt;= Greater than or equal to
</p>
<p>is.na(x) Missing?
&amp; Logical AND
| Logical OR
! Logical NOT
</p>
<p>&amp; and | are elementwise. See &ldquo;Programming&rdquo; (p. 336) for &amp;&amp; and ||.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C. Compendium 327
</p>
<p>Vectors and data types
</p>
<p>Generating
</p>
<p>numeric(25) 25 zeros
character(25) 25 &times; ""
logical(25) 25 &times; FALSE
</p>
<p>seq(-4,4,0.1) Sequence: &minus;4.0, &minus;3.9, 3.8, . . . , 3.9, 4.0
1:10 Same as seq(1,10,1)
</p>
<p>c(5,7,9,13,1:5) Concatenation: 5 7 9 13 1 2 3 4 5
rep(1,10) 1 1 1 1 1 1 1 1 1 1
gl(3,2,12) Factor with 3 levels, repeat each level in blocks
</p>
<p>of 2, up to length 12 (i.e., 1 1 2 2 3 3 1 1 2 2 3 3)
</p>
<p>Coercion
</p>
<p>as.numeric(x) Convert to numeric
as.character(x) Convert to text string
as.logical(x) Convert to logical
factor(x) Create factor from vector x
</p>
<p>For factors, see also &ldquo;Tabulation, grouping, and recoding&rdquo; (p. 331).
</p>
<p>Data frames
</p>
<p>data.frame(height =
</p>
<p>c(165,185), weight =
</p>
<p>c(90,65))
</p>
<p>Data frame with two named vectors
</p>
<p>data.frame(height,
</p>
<p>weight)
</p>
<p>Collect vectors into data frame
</p>
<p>dfr$var Select vector var in data frame dfr
attach(dfr) Put data frame in search path
detach() &mdash; and remove it from path
Attached data frames always come after .GlobalEnv in the search path.
</p>
<p>Attached data frames are copies; subsequent changes to dfr have no effect.</p>
<p/>
</div>
<div class="page"><p/>
<p>328 Appendix C. Compendium
</p>
<p>Numerical functions
</p>
<p>Mathematical
</p>
<p>log(x) Logarithm of x, natural (base-e)
logarithm
</p>
<p>log10(x) Base-10 logarithm
exp(x) Exponential function ex
</p>
<p>sin(x) Sine
cos(x) Cosine
tan(x) Tangent
asin(x) Arcsin (inverse sine)
acos(x)
</p>
<p>atan(x)
</p>
<p>min(x) Smallest value in vector
min(x1,x2,...) Minimum over several vectors (one
</p>
<p>number)
max(x) Largest value in vector
range(x) Like c(min(x),max(x))
</p>
<p>pmin(x1,x2,...) Parallel (elementwise) minimum
over multiple equally long vectors
</p>
<p>pmax(x1,x2,...) Parallel maximum
length(x) Number of elements in vector
</p>
<p>sum(complete.cases(x)) Number of nonmissing elements in
vector
</p>
<p>Statistical
</p>
<p>mean(x) Average
sd(x) Standard deviation
var(x) Variance
</p>
<p>median(x) Median
quantile(x,p) Quantiles
</p>
<p>cor(x,y) Correlation</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C. Compendium 329
</p>
<p>Indexing/selection
</p>
<p>x[1] First element
x[1:5] Subvector containing first five
</p>
<p>elements
x[c(2,3,5,7,11)] Element nos. 2, 3, 5, 7, and 11
</p>
<p>x[y&lt;=30] Selection by logical expression
x[sex=="male"] Selection by factor variable
</p>
<p>i &lt;- c(2,3,5,7,11); x[i] Selection by numeric variable
l &lt;- (y&lt;=30); x[l] Selection by logical variable
</p>
<p>Matrices and data frames
</p>
<p>m[4,] Fourth row
m[,3] Third column
</p>
<p>dfr[dfr$var&lt;=30,] Partial data frame
subset(dfr,var&lt;=30) Same, often simpler
</p>
<p>Input of data
</p>
<p>data(name) Built-in data set
read.table("filename") Read from external file
</p>
<p>Common arguments to read.table
</p>
<p>header=TRUE First line has variable names
sep="," Data are separated by commas
dec="," Decimal point is comma
</p>
<p>na.strings="." Missing value is dot
</p>
<p>Variants of read.table
</p>
<p>read.csv("filename") Comma-separated
read.delim("filename") Tab-delimited
read.csv2("filename") Semicolon-separated, comma
</p>
<p>decimal point
read.delim2("filename") Tab-delimited, comma decimal
</p>
<p>point
These all set header=TRUE.</p>
<p/>
</div>
<div class="page"><p/>
<p>330 Appendix C. Compendium
</p>
<p>Missing values
</p>
<p>Functions
</p>
<p>is.na(x) Logical vector. TRUEwhere x
has NA.
</p>
<p>complete.cases(x1,x2,...) Missing neither in x1, nor x2,
nor. . . .
</p>
<p>Arguments to other functions
</p>
<p>na.rm= In statistical functions, remove
missing if TRUE, return NA if
FALSE.
</p>
<p>na.last= In sort; TRUE, FALSE and NA
mean, respectively, &ldquo;last&rdquo;,
&ldquo;first&rdquo;, and &ldquo;throw away&rdquo;.
</p>
<p>na.action= In lm, etc., values na.fail,
na.omit, na.exclude; also in
options("na.action").
</p>
<p>na.print= In summary and
print.default; how to
represent NA in output.
</p>
<p>na.strings= In read.table(); code(s) for
NA in input.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C. Compendium 331
</p>
<p>Tabulation, grouping, and recoding
</p>
<p>table(f1,...) (Cross)-tabulation
xtabs(~ f1 + ...) ditto, formula interface
</p>
<p>ftable(f1 ~ f2 + ...) &ldquo;Flat&rdquo; tables
tapply(x,f,mean) Table of means
</p>
<p>aggregate(df,list(f),mean) Means for several variables
by(df, list(f), summary) Summarize data frame by group
</p>
<p>factor(x) Convert vector to factor
cut(x,breaks) Groups from cutpoints for
</p>
<p>continuous variable
</p>
<p>Arguments to factor
</p>
<p>levels Values of x to code. Use if some
values are not present in data or
if the order would be wrong.
</p>
<p>labels Values associated with factor
levels.
</p>
<p>exclude Values to exclude. Default NA.
Set to NULL to have missing
values included as a level.
</p>
<p>Arguments to cut
</p>
<p>breaks Cutpoints. Note that values of x
outside breaks give NA.
</p>
<p>labels Names for groups. Default is
(0,30], etc.
</p>
<p>right Right endpoint included?
(FALSE: left)
</p>
<p>Recoding factors
</p>
<p>levels(f) &lt;- names New level names
levels(f) &lt;- list( Combining levels
</p>
<p>new1=c("old1","old2")
</p>
<p>new2="old3")</p>
<p/>
</div>
<div class="page"><p/>
<p>332 Appendix C. Compendium
</p>
<p>Statistical distributions
</p>
<p>Normal distribution
</p>
<p>dnorm(x) Density
pnorm(x) Cumulative distribution
</p>
<p>function, P(X &le; x)
qnorm(p) p-quantile, x : P(X &le; x) = p
rnorm(n) n (pseudo-)random normally
</p>
<p>distributed numbers
</p>
<p>Distributions
</p>
<p>pnorm(x,mean,sd) Normal
plnorm(x,mean,sd) Lognormal
</p>
<p>pt(x,df) Student&rsquo;s t
pf(x,n1,n2) F distribution
pchisq(x,df) χ2
</p>
<p>pbinom(x,n,p) Binomial
ppois(x,lambda) Poisson
punif(x,min,max) Uniform
pexp(x,rate) Exponential
</p>
<p>pgamma(x,shape,scale) Gamma
pbeta(x,a,b) Beta
</p>
<p>Same convention (d-q-r) for density, quantiles, and random numbers as for normal
distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C. Compendium 333
</p>
<p>Statistical standard methods
</p>
<p>Continuous response
</p>
<p>t.test One- and two-sample t tests
pairwise.t.test Pairwise comparisons
</p>
<p>cor.test Correlation
var.test Comparison of two variances
</p>
<p>(F test)
lm(y ~ x) Regression analysis
lm(y ~ f) One-way analysis of variance
</p>
<p>lm(y ~ f1 + f2) Two-way analysis of variance
lm(y ~ f + x) Analysis of covariance
</p>
<p>lm(y ~ x1 + x2 + x3) Multiple regression analysis
bartlett.test Bartlett&rsquo;s test (k variances)
</p>
<p>Nonparametric:
wilcox.test One- and two-sample
</p>
<p>Wilcoxon tests
kruskal.test Kruskal&ndash;Wallis test
friedman.test Friedman&rsquo;s two-way analysis
</p>
<p>of variance
cor.test variants:
</p>
<p>method="kendall" Kendall&rsquo;s τ
method="spearman" Spearman&rsquo;s ρ
</p>
<p>Discrete response
</p>
<p>binom.test Binomial test (incl. sign test)
prop.test Comparison of proportions
</p>
<p>prop.trend.test Test for trend in relative
proportions
</p>
<p>fisher.test Exact test in small tables
chisq.test χ2 test
</p>
<p>glm(y ~ x1+x2+x3, binomial) Logistic regression</p>
<p/>
</div>
<div class="page"><p/>
<p>334 Appendix C. Compendium
</p>
<p>Models
</p>
<p>Model formulas
</p>
<p>~ Described by
+ Additive effects
: Interaction
* Main effects + interaction
</p>
<p>(a*b = a + b + a:b)
-1 Remove intercept
</p>
<p>Classifications are represented by descriptive variable being a factor.
</p>
<p>Linear, nonlinear, and generalized linear models
</p>
<p>lm.out &lt;- lm(y ~ x) Fit model and save result
summary(lm.out) Coefficients, etc.
anova(lm.out) Analysis of variance table
fitted(lm.out) Fitted values
resid(lm.out) Residuals
</p>
<p>predict(lm.out, newdata) Predictions for new data frame
glm(y ~ x, binomial) Logistic regression
glm(y ~ x, poisson) Poisson regression
nls(y ~ a*exp(-b*x), Nonlinear regression
start=c(a=5, b=.2))
</p>
<p>Diagnostics
</p>
<p>rstudent(lm.out) Studentized residuals
dfbetas(lm.out) Change in β if obs. removed
dffits(lm.out) Change in fit if obs. removed
</p>
<p>Survival analysis
</p>
<p>S &lt;- Surv(time, ev) Create survival object
survfit(S) Kaplan&ndash;Meier estimate
</p>
<p>plot(survfit(S)) Survival curve
survdiff(S ~ g) (Log-rank) test for equal
</p>
<p>survival curves
coxph(S ~ x1 + x2) Cox&rsquo;s proportional hazards
</p>
<p>model</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C. Compendium 335
</p>
<p>Graphics
</p>
<p>Standard plots
</p>
<p>plot() Scatterplot (and more)
hist() Histogram
</p>
<p>boxplot() Box-and-whiskers plot
stripplot() Stripplot
barplot() Bar diagram
dotplot() Dot diagram
piechart() Cakes. . .
</p>
<p>interaction.plot() Interaction plot
</p>
<p>Plotting elements
</p>
<p>lines() Lines
abline() Line given by intercept and slope
</p>
<p>(and more)
points() Points
</p>
<p>segments() Line segments
arrows() Arrows (N.B.: angle=90 for error
</p>
<p>bars)
axis() Axis
box() Frame around plot
</p>
<p>title() Title (above plot)
text() Text in plot
mtext() Text in margin
legend() List of symbols
</p>
<p>These are all added to existing plots.
</p>
<p>Graphical parameters
</p>
<p>pch Symbol (plotting character)
mfrow, mfcol Several plots on one (multif rame)
xlim, ylim Plot limits
lty,lwd Line type/width
</p>
<p>col Colour
cex, mex Character size and line spacing in
</p>
<p>margins
See the help page for par for more details.</p>
<p/>
</div>
<div class="page"><p/>
<p>336 Appendix C. Compendium
</p>
<p>Programming
</p>
<p>Conditional execution
if(p&lt;0.05)
</p>
<p>print("Hooray!")
</p>
<p>&mdash; with alternative
</p>
<p>if(p&lt;0.05)
</p>
<p>print("Hooray!")
</p>
<p>else
</p>
<p>print("Bah.")
</p>
<p>Loop over list
for(i in 1:10)
</p>
<p>print(i)
</p>
<p>Loop
</p>
<p>i &lt;- 1
</p>
<p>while(i&lt;10) {
</p>
<p>print(i)
</p>
<p>i &lt;- i + 1
</p>
<p>}
</p>
<p>User-defined function
</p>
<p>f &lt;- function(a,b,doit=FALSE){
</p>
<p>if (doit)
</p>
<p>a + b
</p>
<p>else
</p>
<p>0
</p>
<p>}
</p>
<p>In flow control, one uses a &amp;&amp; b and a || b, where b is only
computed if necessary; that is, if a then b else FALSE and
if a then TRUE else b.</p>
<p/>
</div>
<div class="page"><p/>
<p>D
Answers to exercises
</p>
<p>1.1 One possibility is
</p>
<p>x &lt;- y &lt;- c(7, 9, NA, NA, 13)
</p>
<p>all(is.na(x) == is.na(y)) &amp; all((x == y)[!is.na(x)])
</p>
<p>Notice that FALSE &amp; NA is FALSE, so the case of different NA patterns is
handled correctly.
</p>
<p>1.2 Factor x gets treated as if it contained the integer codes.
</p>
<p>x &lt;- factor(c("Huey", "Dewey", "Louie", "Huey"))
</p>
<p>y &lt;- c("blue", "red", "green")
</p>
<p>x
</p>
<p>y[x]
</p>
<p>(This is useful, e.g., when selecting plot symbols.)
</p>
<p>1.3
</p>
<p>juul.girl &lt;- juul[juul$age &gt;=7 &amp; juul$age &lt; 14 &amp; juul$sex == 2,]
</p>
<p>summary(juul.girl)
</p>
<p>1.4 The levels with the same name are collapsed into one.
</p>
<p>1.5 sapply(1:10, function(i) mean(rexp(20)))
</p>
<p>2.1 To insert 1.23 between x[7] and x[8]:</p>
<p/>
</div>
<div class="page"><p/>
<p>338 Appendix D. Answers to exercises
</p>
<p>x &lt;- 1:10
</p>
<p>z &lt;- append(x, 1.23, after=7)
</p>
<p>z
</p>
<p>Otherwise, consider
</p>
<p>z &lt;- c(x[1:7],1.23,x[8:10])
</p>
<p>z
</p>
<p>or, more generally, to insert v just after index k (the boundary cases require
some care),
</p>
<p>v &lt;- 1.23; k &lt;- 7
</p>
<p>i &lt;- seq(along=x)
</p>
<p>z &lt;- c(x[i &lt;= k], v, x[i &gt; k])
</p>
<p>z
</p>
<p>2.2 (First part only) Use
</p>
<p>write.table(thuesen, file="foo.txt")
</p>
<p># edit the file
</p>
<p>read.table("foo.txt", na.strings=".")
</p>
<p>or
</p>
<p>write.table(thuesen, file="foo.txt", na=".")
</p>
<p>read.table("foo.txt", na.strings=".")
</p>
<p>(Notice that if you do not edit the file in the first case, then the second
column gets read as a character vector.)
</p>
<p>3.1
</p>
<p>1 - pnorm(3)
</p>
<p>1 - pnorm(42, mean=35, sd=6)
</p>
<p>dbinom(10, size=10, prob=0.8)
</p>
<p>punif(0.9) # this one is obvious...
</p>
<p>1 - pchisq(6.5, df=2)
</p>
<p>It might be better to use lower.tail=FALSE instead of subtracting from
1 in (a), (b), and (e). Notice that question (c) is about a point probability,
whereas the others involve the cumulative distribution function.
</p>
<p>3.2 Evaluate each of the following. Notice that the standard normal can
be used for all questions.
</p>
<p>pnorm(-2) * 2
</p>
<p>qnorm(1-.01/2)
</p>
<p>qnorm(1-.005/2)
</p>
<p>qnorm(1-.001/2)
</p>
<p>qnorm(.25)
</p>
<p>qnorm(.75)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D. Answers to exercises 339
</p>
<p>Again, lower.tail can be used in some cases.
</p>
<p>3.3 dbinom(0, size=10, prob=.2)
</p>
<p>3.4 Either of the following should work:
</p>
<p>rbinom(10, 1, .5)
</p>
<p>ifelse(rbinom(10, 1, .5) == 1, "H", "T")
</p>
<p>c("H", "T")[1 + rbinom(10, 1, .5)]
</p>
<p>The first one gives a 0/1 result, the two others H/T like the sample exam-
ple in the text. One advantage of using rbinom is that its prob argument
can be a vector, so you can have different probabilities of success for each
element of the result.
</p>
<p>4.1 For example,
</p>
<p>x &lt;- 1:5 ; y &lt;- rexp(5,1) ; opar &lt;- par(mfrow=c(2,2))
</p>
<p>plot(x, y, pch=15) # filled square
</p>
<p>plot(x, y, type="b", lty="dotted")
</p>
<p>plot(x, y, type="b", lwd=3)
</p>
<p>plot(x, y, type="o", col="blue")
</p>
<p>par(opar)
</p>
<p>4.2 Use a filled symbol, and set the fill colour equal to the plot
background:
</p>
<p>plot(rnorm(10),type="o", pch=21, bg="white")
</p>
<p>4.3 You can use qqnorm with plot.it=F and get a return value from
which you can extract the range information (you could of course also get
this &ldquo;by eye&rdquo;).
</p>
<p>x1 &lt;- rnorm(20)
</p>
<p>x2 &lt;- rnorm(10)+1
</p>
<p>q1 &lt;- qqnorm(x1, plot.it=F)
</p>
<p>q2 &lt;- qqnorm(x2, plot.it=F)
</p>
<p>xr &lt;- range(q1$x, q2$x)
</p>
<p>yr &lt;- range(q1$y, q2$y)
</p>
<p>qqnorm(x1, xlim=xr, ylim=yr)
</p>
<p>points(q2, col="red")
</p>
<p>Here, qqnorm is used for the basic plot to get the labels right. Then
points is used with q2 for the overlay.
</p>
<p>Setting type="l" gives a messy plot because the values are not plotted
in order. The remedy is to use sort(x1) and sort(x2).
</p>
<p>4.4 The breaks occur at integer values, as do the data. Data on the bound-
ary are counted in the column to the left of it, effectively shifting the</p>
<p/>
</div>
<div class="page"><p/>
<p>340 Appendix D. Answers to exercises
</p>
<p>histogram half a unit left. The truehist function allows you to specify a
better set of breaks.
</p>
<p>hist(react)
</p>
<p>library(MASS)
</p>
<p>truehist(react,h=1,x0=.5)
</p>
<p>4.5 The thing to notice is the linear interpolation between data points:
</p>
<p>z &lt;- runif(5)
</p>
<p>curve(quantile(z,x), from=0, to=1)
</p>
<p>5.1 The distribution appears reasonably normal, with some discretiza-
tion effect and two weak outliers, one at each end. There is a significant
difference from zero (t = &minus;7.75, p = 1.1&times; 10&minus;13).
qqnorm(react)
</p>
<p>t.test(react)
</p>
<p>5.2 t.test(vital.capacity~group,conf=0.99,data=vitcap).
The fact that age also differs by group may cause bias.
</p>
<p>5.3 This is quite parallel to t.test usage
</p>
<p>wilcox.test(react)
</p>
<p>wilcox.test(vital.capacity~group, data=vitcap)
</p>
<p>5.4 The following builds a post-vs.-pre plot, a difference-vs.-average)
(Bland-Altman) plot, and a histogram and a Q-Q plot of the differences.
</p>
<p>attach(intake) ; opar &lt;- par(mfrow=c(2,2))
</p>
<p>plot(post ~ pre) ; abline(0,1)
</p>
<p>plot((post+pre)/2, post - pre,
</p>
<p>ylim=range(0,post-pre)); abline(h=0)
</p>
<p>hist(post-pre)
</p>
<p>qqnorm(post-pre)
</p>
<p>detach(intake)
</p>
<p>par(opar)
</p>
<p>5.5 The outliers are the first and last observations in the (sorted) data
vector and can be removed as follows
</p>
<p>shapiro.test(react)
</p>
<p>shapiro.test(react[-c(1,334)])
</p>
<p>qqnorm(react[-c(1,334)])
</p>
<p>The test comes out highly significant even with outliers removed because
it picks up the discretization effect in the otherwise nearly straight-line
qqnorm plot.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D. Answers to exercises 341
</p>
<p>5.6 A paired t test is appropriate if there is no period effect. However,
even with a period effect (assumed additive), you would expect the dif-
ference between the two periods to be the same in both groups if there
were no effect of treatment. This can be used to test for a treatment effect.
</p>
<p>attach(ashina)
</p>
<p>t.test(vas.active, vas.plac, paired=TRUE)
</p>
<p>t.test((vas.active-vas.plac)[grp==1],
</p>
<p>(vas.plac-vas.active)[grp==2])
</p>
<p>Notice that the subtraction is reversed in one group. Observe that the
confidence interval in the second case is for twice the treatment effect.
</p>
<p>5.7 This is the sort of thing replicate is for. The plot at the end shows
a P-P plot with logarithmic axes, showing that extreme p-values tend to
be exaggerated.
</p>
<p>t.test(rnorm(25))$p.value #repeat 10x
</p>
<p>t.test(rt(25,df=2))$p.value #repeat 10x
</p>
<p>t.test(rexp(25), mu=1)$p.value #repeat 10x
</p>
<p>x &lt;- replicate(5000, t.test(rexp(25), mu=1)$p.value)
</p>
<p>qqplot(sort(x),ppoints(5000),type="l",log="xy")
</p>
<p>6.1 The following gives both elementary and more general answers.
Notice the use of confint.
</p>
<p>fit &lt;- lm(metabolic.rate ~ body.weight, data=rmr)
</p>
<p>summary(fit)
</p>
<p>811.2267 + 7.0595 * 70 # , or:
</p>
<p>predict(fit, newdata=data.frame(body.weight=70))
</p>
<p>qt(.975,42)
</p>
<p>7.0595 + c(-1,1) * 2.018 * 0.9776 # , or:
</p>
<p>confint(fit)
</p>
<p>6.2 summary(lm(sqrt(igf1)~age,data=juul,subset=age&gt;25))
</p>
<p>6.3 We can fit a linear model and plot the data as follows:
</p>
<p>summary(lm(log(ab)~age, data=malaria))
</p>
<p>plot(log(ab)~age, data=malaria)
</p>
<p>The plot appears to show a cyclic pattern. It is unclear whether it reflects
a significant departure from the model, though. Malaria is a disease with
epidemic behaviour, so cycles are plausible.
</p>
<p>6.4 (This could be elaborated by wrapping the random number genera-
tion in a function, etc.)
</p>
<p>rho &lt;- .90 ; n &lt;- 100
</p>
<p>x &lt;- rnorm(n)</p>
<p/>
</div>
<div class="page"><p/>
<p>342 Appendix D. Answers to exercises
</p>
<p>y &lt;- rnorm(n, rho * x, sqrt(1 - rho^2))
</p>
<p>plot(x, y)
</p>
<p>cor.test(x, y)
</p>
<p>cor.test(x, y, method="spearman")
</p>
<p>cor.test(x, y, method="kendall")
</p>
<p>You will most likely find that the Kendall correlation is somewhat smaller
than the two others.
</p>
<p>7.1
</p>
<p>walk &lt;- unlist(zelazo) # or c(..,recursive=TRUE)
</p>
<p>group &lt;- factor(rep(1:4,c(6,6,6,5)), labels=names(zelazo))
</p>
<p>summary(lm(walk ~ group))
</p>
<p>t.test(zelazo$active,zelazo$ctr.8w) # first vs. last
</p>
<p>t.test(zelazo$active,unlist(zelazo[-1])) # first vs. rest
</p>
<p>7.2 A and C differ with B intermediate, not significantly different from
either. (The B&ndash;C comparison is not available from the summary, but due
to the balanced design, the standard error of that difference is 0.16656 like
the two others.)
</p>
<p>fit &lt;- lm(volume~method+subject, data=lung)
</p>
<p>anova(fit)
</p>
<p>summary(fit)
</p>
<p>7.3
</p>
<p>kruskal.test(walk ~ group)
</p>
<p>wilcox.test(zelazo$active,zelazo$ctr.8w) # first vs. last
</p>
<p>wilcox.test(zelazo$active,unlist(zelazo[-1])) # first vs. rest
</p>
<p>friedman.test(volume ~ method | subject, data=lung)
</p>
<p>wilcox.test(lung$volume[lung$method=="A"],
</p>
<p>lung$volume[lung$method=="C"], paired=TRUE) # etc.
</p>
<p>7.4 (Only the square-root transform is shown; you can do the same for
log-transformed and untransformed data.)
</p>
<p>attach(juul)
</p>
<p>tapply(sqrt(igf1),tanner, sd, na.rm=TRUE)
</p>
<p>plot(sqrt(igf1)~jitter(tanner))
</p>
<p>oneway.test(sqrt(igf1)~tanner)
</p>
<p>The square root looks nice, logarithms become skewed in the opposite
direction. The transformations do not make much of a difference for the
test. It is, however, a problem that strong age effects are being ignored,
particularly within Tanner stage 1.
</p>
<p>8.1 With 10 patients, p = 0.1074. Fourteen or more are needed for
significance at level 0.05.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D. Answers to exercises 343
</p>
<p>binom.test(0, 10, p=.20, alt="less")
</p>
<p>binom.test(0, 13, p=.20, alt="less")
</p>
<p>binom.test(0, 14, p=.20, alt="less")
</p>
<p>8.2 Yes, it is highly significant.
</p>
<p>prop.test(c(210,122),c(747,661))
</p>
<p>8.3 The confidence interval (from prop.test) is (&minus;0.085, 0.507)
M &lt;- matrix(c(23,7,18,13),2,2)
</p>
<p>chisq.test(M)
</p>
<p>fisher.test(M)
</p>
<p>prop.test(M)
</p>
<p>8.4 The following is a simplified analysis, which uses fisher.test
because of the small cell counts:
</p>
<p>tbl &lt;- c(42, 157, 47, 62, 4, 15, 4, 1, 8, 28, 9, 7)
</p>
<p>dim(tbl) &lt;- c(2,2,3)
</p>
<p>dimnames(tbl) &lt;- list(c("A","B"),
</p>
<p>c("not pierced","pierced"),
</p>
<p>c("ok","broken","cracked"))
</p>
<p>ftable(tbl)
</p>
<p>fisher.test(tbl["B",,]) # slice analysis
</p>
<p>fisher.test(tbl["A",,])
</p>
<p>fisher.test(margin.table(tbl,2:3)) # marginal
</p>
<p>You may wish to check that there is little or no effect of egg size on
breakage, so that the marginal analysis is defensible. You could also try
collapsing the &ldquo;broken&rdquo; and &ldquo;cracked&rdquo; categories.
</p>
<p>8.5 The curve shows substantial discontinuities where probability mass
is shifted from one tail to the other and also a number of local minima. A
confidence region could be defined as those p against which there is no
significant evidence at level α, but for some α that is not an interval.
</p>
<p>p &lt;- seq(0,1,0.001)
</p>
<p>pval &lt;- sapply(p,function(p)binom.test(3,15,p=p)$p.value)
</p>
<p>plot(p,pval,type="l")
</p>
<p>9.1 The estimated sample size is 6.29 or 8.06 per group depending on
whether you use one- or two-sided testing. The approximate formula
gives 6.98 for the two-sided case. The reduction in power due to the un-
balanced sampling can be accounted for by reducing delta by the ratio
of the two SEDM.
</p>
<p>power.t.test(power=.8,delta=.30,sd=.20)
</p>
<p>power.t.test(power=.8,delta=.30,sd=.20,alt="one.sided")</p>
<p/>
</div>
<div class="page"><p/>
<p>344 Appendix D. Answers to exercises
</p>
<p>(qnorm(.975)+qnorm(.8))^2*2*(.2/.3)^2 # approx. formula
</p>
<p>power.t.test(n=8, delta=.30, sd=.20) # power with eq.size
</p>
<p>d2 &lt;- .30 * sqrt(2/8) / sqrt(1/6+1/10) # corr.f.uneq. size
</p>
<p>power.t.test(n=8, delta=d2, sd=.20)
</p>
<p>9.2 This is straightforward:
</p>
<p>power.prop.test(power=.9, p1=.6, p2=.75)
</p>
<p>power.prop.test(power=.8, p1=.6, p2=.75)
</p>
<p>9.3 Notice that the noncentral t distribution is asymmetric, with a rather
heavy right tail.
</p>
<p>curve(dt(x-3, 25), from=0, to=5)
</p>
<p>curve(dt(x, 25, 3), add=TRUE)
</p>
<p>9.4 This causes the &ldquo;power&rdquo; at zero effect size (i.e., under the null hy-
pothesis) to be half the significance level, in contradiction to theory. For
any relevant true effect size, the difference is immaterial.
</p>
<p>9.5 The power in that case is approximately 0.50; exactly so if the
variance is assumed known.
</p>
<p>10.1
</p>
<p>attach(thuesen)
</p>
<p>f &lt;- cut(blood.glucose, c(4, 7, 9, 12, 20))
</p>
<p>levels(f) &lt;- c("low", "intermediate", "high", "very high")
</p>
<p>10.2
</p>
<p>bcmort2 &lt;- within(bcmort,{
</p>
<p>period &lt;- area &lt;- cohort
</p>
<p>levels(period) &lt;- rep(c("1991-2001","1981-1991"), each=2)
</p>
<p>levels(area) &lt;- rep(c("Cph+Frb","Nat"),2)
</p>
<p>})
</p>
<p>summary(bcmort2)
</p>
<p>10.3 One way is the following (for later use, we also make sure that
variables are converted to factors):
</p>
<p>ashina.long &lt;- reshape(ashina, direction="long",
</p>
<p>varying=1:2, timevar="treat")
</p>
<p>ashina.long &lt;- within(ashina.long, {
</p>
<p>m &lt;- matrix(c(2,1,1,2),2)
</p>
<p>id &lt;- factor(id)
</p>
<p>treat &lt;- factor(treat)
</p>
<p>grp &lt;- factor(grp)
</p>
<p>period &lt;- factor(m[cbind(grp,treat)])</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D. Answers to exercises 345
</p>
<p>rm(m)
</p>
<p>})
</p>
<p>Notice the use of array indexing. Alternatively, an ifelse construct can be
used; e.g., the following (notice that (3 - grp) is 2 when grp is 1 and
vice versa):
</p>
<p>within(ashina.long,
</p>
<p>period2 &lt;- ifelse(treat != "active",
</p>
<p>as.numeric(grp), 3 - as.numeric(grp))
</p>
<p>)
</p>
<p>Arithmetic involving grp does not work after it was converted to a factor,
hence the conversion with as.numeric.
</p>
<p>10.4 This can be done a little more easily than in the nickel example by
using subset and transform. It also helps that all observation periods
start at time zero in this case.
</p>
<p>stroke.trim &lt;- function(t1, t2)
</p>
<p>subset(transform(stroke,
</p>
<p>entry=t1, exit=pmin(t2, obsmonths),
</p>
<p>dead=dead &amp; obsmonths &lt;= t2),
</p>
<p>entry &lt; exit)
</p>
<p>stroke2 &lt;- do.call(rbind, mapply(stroke.trim,
</p>
<p>c(0,0.5,2,12), c(0.5,2,12,Inf), SIMPLIFY=F))
</p>
<p>table(stroke$dead)
</p>
<p>table(stroke2$dead)
</p>
<p>Notice the use of mapply here. This is like sapply and lapply but al-
lows the function to have multiple arguments. Alternatively, one could
arrange for stroke.trim to have a single interval argument and use
lapply on a list of such intervals.
</p>
<p>The tabulation at the end is a &ldquo;sanity check&rdquo; to show that we have the
same number of deaths but manymore censored cases after time-splitting.
</p>
<p>11.1 The model with both diameters has a residual error of 0.107, com-
pared with 0.128 using abdominal diameter alone and 0.281 with no
predictors at all. If a fetus is scaled isotropically, a cubic relation with
weight is expected, and you could speculate that this is reflected in the
sum of coefficients when using log scales.
</p>
<p>summary(lm(log(bwt) ~ log(bpd) + log(ad), data=secher))
</p>
<p>summary(lm(log(bwt) ~ log(ad), data=secher))
</p>
<p>11.2 If you use attach(tlc), the tlc variable will mask the data frame
of the same name, which makes it awkward to access the data frame if
you need to. If the data frame is in the global environment rather than in</p>
<p/>
</div>
<div class="page"><p/>
<p>346 Appendix D. Answers to exercises
</p>
<p>a package, you get the opposite problem, masking of the variable by the
data frame. The simplest workaround is to avoid attach.
</p>
<p>pairs(tlc)
</p>
<p>summary(lm(log(tlc) ~ ., data=tlc))
</p>
<p>opar &lt;- par(mfrow=c(2,2))
</p>
<p>plot(lm(log(tlc) ~ ., data=tlc), which=1:4)
</p>
<p>drop1(lm(log(tlc) ~ ., data=tlc))
</p>
<p>drop1(lm(log(tlc) ~ . - age, data=tlc))
</p>
<p>par(mfrow=c(1,1))
</p>
<p>plot(log(tlc) ~ height, data=tlc)
</p>
<p>par(mfrow=c(2,2))
</p>
<p>plot(lm(tlc ~ ., data=tlc), which=1:4) # slightly worse
</p>
<p>par(opar)
</p>
<p>Some new variations of model formulas were introduced above. A dot
on the right-hand side in this context means &ldquo;everything not used on the
left-hand side&rdquo; within the scope of the data frame. A minus term is re-
moved from the model. In other words, ... ~ . - age is the same as
... ~ sex + height.
</p>
<p>11.3 The regression coefficient describes a value to be added for females.
</p>
<p>11.4 age is highly significant in the first analysis but only borderline
significant (p = 0.06) in the second analysis after removing height and
weight. Youwould expect similar results, but the number of observations
differs in the two cases, due to missing observations.
</p>
<p>summary(lm(sqrt(igf1) ~ age, data=juul2, subset=(age &gt;= 25)))
</p>
<p>anova(lm(sqrt(igf1) ~ age + weight + height,
</p>
<p>data=juul2, subset=(age &gt;= 25)))
</p>
<p>11.5 sex is treated as a binary indicator for girls. Notice that there are
effects both of the mother&rsquo;s and the child&rsquo;s size. The reason why height
rather than weight of the mother enters into the equation is somewhat
obscure, but one could speculate that weight is an unreliable indicator
shortly after pregnancy.
</p>
<p>summary(lm(dl.milk ~ . - no, data=kfm))
</p>
<p>summary(lm(dl.milk ~ . - no - mat.weight, data=kfm))
</p>
<p>summary(lm(dl.milk ~ . - no - mat.weight - sex, data=kfm))
</p>
<p>summary(lm(dl.milk ~ weight + mat.height, data=kfm))
</p>
<p>The variations on model formulas used here were described in the
solution to Exercise 11.2.
</p>
<p>12.1 Using ashina.long from Exercise 10.3,</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D. Answers to exercises 347
</p>
<p>fit.ashina &lt;- lm(vas ~ id + period + treat, data=ashina.long)
</p>
<p>drop1(fit.ashina, test="F")
</p>
<p>anova(fit.ashina)
</p>
<p>attach(ashina)
</p>
<p>dd &lt;- vas.active - vas.plac
</p>
<p>t.test(dd[grp==1], -dd[grp==2], var.eq=T)
</p>
<p>t.test(dd[grp==1], dd[grp==2], var.eq=T)
</p>
<p>Notice that the imbalance in group sizes makes the tests for period and
treatment effects order-dependent. The t tests are equivalent to the F tests
from drop1 but not those from anova.
</p>
<p>12.2
</p>
<p>attach(tb.dilute)
</p>
<p>anova(lm(reaction ~ animal + logdose))
</p>
<p>ld &lt;- c(0.5, 0, -0.5)[logdose]
</p>
<p>anova(lm(reaction ~ animal + ld))
</p>
<p>summary(lm(reaction ~ animal + ld))
</p>
<p>4.7917 + 0.6039 * qt(c(.025,.975), 11)
</p>
<p># or:
</p>
<p>confint(lm(reaction ~ animal + ld))["ld",]
</p>
<p>slopes &lt;- reaction[logdose==0.5] - reaction[logdose==-0.5]
</p>
<p>t.test(slopes)
</p>
<p>anova(lm(reaction ~ animal*ld))
</p>
<p>Notice that the formula for the fitted slope is β̂ = &sum; xy/ &sum; x2 since x̄ = 0,
which reduces to taking differences. (The calculation does rely on data
being in the right order.)
</p>
<p>The confidence interval is wider in the t test, reflecting that slopes
may vary between rats and that there are fewer degrees of freedom for
estimating the variation.
</p>
<p>The final ANOVA contains a test for parallel slopes, and the F statistic is
less than one, so in these data the slopes vary less than expected and the
DF must be the important issue for the confidence interval.
</p>
<p>12.3 This can be varied indefinitely, but consider these examples:
</p>
<p>model.matrix(~ a:b) ; lm(z ~ a:b)
</p>
<p>model.matrix(~ a * b) ; lm(z ~ a * b)
</p>
<p>model.matrix(~ a:x) ; lm(z ~ a:x)
</p>
<p>model.matrix(~ a * x) ; lm(z ~ a * x)
</p>
<p>model.matrix(~ b * (x + y)) ; lm(z ~ b * (x + y))
</p>
<p>The first model is singular because indicator variables are created for all
four groups, but the intercept is not removed. R will only reduce the set
of design variables for an interaction term between categorical variables</p>
<p/>
</div>
<div class="page"><p/>
<p>348 Appendix D. Answers to exercises
</p>
<p>when one of the main effects is present. There are no singularities in either
of the two cases involving a categorical and a continuous variable, but the
first one has one parameter less (common-intercept model).
</p>
<p>The last example has a &ldquo;coincidental&rdquo; singularity (x and y are propor-
tional within each level of b) that R has no chance of detecting.
</p>
<p>12.4 The models can be illustrated by plotting the fitted values against
time with separate symbols for each person; e.g.,
</p>
<p>tt &lt;- c(20,30,60,90,0)[time]
</p>
<p>plot(fitted(model4)~tt,pch=as.character(person))
</p>
<p>With model1 there is no imposed structure, model2 is completely addi-
tive so that the individual traces are parallel to each other, model3 allows
the jump from the &ldquo;pre&rdquo; value to the value at 20 minutes to vary between
individuals, and finally model4 is like model3 except that there is no
change after 30 minutes (traces become horizontal). So model3 is nested
in model1 and both model2 and model4 are nested in model3, but there
is no nesting relation between model2 and model4.
</p>
<p>12.5
</p>
<p>bp.obese &lt;- transform(bp.obese,sex=factor(sex, labels=c("M","F")))
</p>
<p>plot(log(bp) ~ log(obese), pch=c(20,21)[sex], data=bp.obese)
</p>
<p>summary(lm(log(bp) ~ sex, data=bp.obese))
</p>
<p>summary(lm(log(bp) ~ sex + log(obese), data=bp.obese))
</p>
<p>summary(lm(log(bp) ~ sex*log(obese), data=bp.obese))
</p>
<p>12.6
</p>
<p>vitcap2 &lt;- transform(vitcap2,group=factor(group,
</p>
<p>labels=c("exp&gt;10",
</p>
<p>"exp&lt;10", "unexp")))
</p>
<p>attach(vitcap2)
</p>
<p>plot(vital.capacity~age, pch=(20:22)[group])
</p>
<p>vit.fit &lt;- lm(vital.capacity ~ age*group)
</p>
<p>summary(vit.fit)
</p>
<p>drop1(vit.fit, test="F")
</p>
<p>for (i in 1:3) abline(lm(vital.capacity ~ age,
</p>
<p>subset=as.numeric(group)==i), lty=i)
</p>
<p>legend(20, 3.5 ,legend=levels(group), pch=20:22, lty=1:3)
</p>
<p>Notice that there is a significant interaction; i.e., the lines are not parallel.
</p>
<p>12.7
</p>
<p>juul.prepub &lt;- subset(juul, tanner==1)
</p>
<p>summary(lm(sqrt(igf1)~age, data=juul.prepub, subset= sex==1))
</p>
<p>summary(lm(sqrt(igf1)~age, data=juul.prepub, subset= sex==2))</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D. Answers to exercises 349
</p>
<p>summary(lm(sqrt(igf1)~age*factor(sex), data=juul.prepub))
</p>
<p>summary(lm(sqrt(igf1)~age+factor(sex), data=juul.prepub))
</p>
<p>12.8
</p>
<p>summary(fit.aicopt &lt;- step(lm(dl.milk ~ . - no, data=kfm)))
</p>
<p>opar &lt;- par(mfrow=c(2,2))
</p>
<p>plot(fit.aicopt, which=1:4)
</p>
<p>kfm[32,]
</p>
<p>summary(kfm)
</p>
<p>summary(update(fit.aicopt, ~ . - sex))
</p>
<p>plot(update(fit.aicopt, ~ . - sex - ml.suppl), which=1:4)
</p>
<p>par(opar)
</p>
<p>Observation 32 contains an extremely large value of ml.suppl and there-
fore has a large influence on its regression coefficient. Without ml.suppl
in the model, the Cook&rsquo;s distances are much smaller.
</p>
<p>12.9
</p>
<p>juulyoung &lt;- subset(juul, age &lt; 25)
</p>
<p>juulyoung &lt;- transform(juulyoung,
</p>
<p>sex=factor(sex), tanner=factor(tanner))
</p>
<p>fit.untf &lt;- lm(igf1 ~ age * sex * tanner, data=juulyoung,
</p>
<p>na.action=na.exclude)
</p>
<p>plot(fitted(fit.untf) ~ age, data=juulyoung,
</p>
<p>col=c("red","green")[sex])
</p>
<p>fit.log &lt;- update(fit.untf, log(igf1) ~ .)
</p>
<p>fit.sqrt &lt;- update(fit.untf, sqrt(igf1) ~ .)
</p>
<p>opar &lt;- par(mfrow=c(2,2))
</p>
<p>plot(fit.untf, which=1:4)
</p>
<p>plot(fit.log, which=1:4)
</p>
<p>plot(fit.sqrt, which=1:4)
</p>
<p>par(opar)
</p>
<p>13.1
</p>
<p>summary(glm(mal~age+log(ab), binomial, data=malaria))
</p>
<p>(You may also want to check for interaction.)
</p>
<p>13.2
</p>
<p>attach(graft.vs.host)
</p>
<p>type &lt;- factor(type,labels=c("AML", "ALL", "CML"))
</p>
<p>m1 &lt;- glm(gvhd~rcpage+donage+type+preg+log(index), binomial)
</p>
<p>m1a &lt;- glm(gvhd~rcpage+donage+type+preg+index, binomial)
</p>
<p>summary(m1)
</p>
<p>summary(m1a)</p>
<p/>
</div>
<div class="page"><p/>
<p>350 Appendix D. Answers to exercises
</p>
<p>The coefficient to log(index) is more significant, but the model with
index has a slightly better deviance. There is little hard evidence for ei-
ther. The log-transform has the advantage that it reduces the influence of
two very large values of index.
</p>
<p>drop1(m1, test="Chisq")
</p>
<p>drop1(update(m1, ~ . - rcpage), test="Chisq")
</p>
<p>drop1(update(m1, ~ . - rcpage - type), test="Chisq")
</p>
<p>drop1(update(m1, ~ . - rcpage - type - preg), test="Chisq")
</p>
<p>summary(m2 &lt;- glm(gvhd~donage + log(index), binomial))
</p>
<p>Notice that except for log(index) it is essentially an arbitrary decision
which variables to put in the final model. Altman (1991) treats the type
classification as separate binary variables and gets a final model where
ALL and AML are combined into one group and includes preg but not
donage.
</p>
<p>13.3 For example,
</p>
<p>confint(m2)
</p>
<p>## normal approximation:
</p>
<p>est &lt;- coefficients(summary(m2))[,1]
</p>
<p>se &lt;- coefficients(summary(m2))[,2]
</p>
<p>est + cbind(qnorm(.025)*se, qnorm(.975)*se)
</p>
<p>confint.default(m2)
</p>
<p>Notice that the confint-generated intervals lie asymmetrically around
the estimate. In this case, both ends of the interval are shifted away from
zero, in accordance with the fact that the deviance-based tests from drop1
have lower p-values than the approximate t tests in summary.
</p>
<p>13.4 The model can be fitted as follows
</p>
<p>counts &lt;- c(13,40,157,40,21,61)
</p>
<p>total &lt;- c(108,264,375,310,181,162)
</p>
<p>age &lt;- gl(3,1,6)
</p>
<p>type &lt;- gl(2,3,6)
</p>
<p>anova(glm(counts/total~age+type,weights=total, binomial),
</p>
<p>test="Chisq")
</p>
<p>The effect of type vanished once age was included, suggesting that it
really is the same disease, which has affected mostly younger (and fitter)
subjects in the Eastern region.
</p>
<p>13.5
</p>
<p>juul.girl &lt;- transform(subset(juul,age&gt;8 &amp; age&lt;20 &amp;
</p>
<p>complete.cases(menarche)),
</p>
<p>menarche=factor(menarche))
</p>
<p>logit.menarche &lt;- glm(menarche~age+I(age^2)+I(age^3),
</p>
<p>binomial, data=juul.girl)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D. Answers to exercises 351
</p>
<p>probit.menarche &lt;- glm(menarche~age+I(age^2)+I(age^3),
</p>
<p>binomial(probit), data=juul.girl)
</p>
<p>summary(logit.menarche)
</p>
<p>summary(probit.menarche)
</p>
<p>Age=seq(8,20,.1)
</p>
<p>newages &lt;- data.frame(age=Age)
</p>
<p>p.logit &lt;- predict(logit.menarche,newdata=newages,type="resp")
</p>
<p>p.probit &lt;- predict(probit.menarche,newdata=newages,type="resp")
</p>
<p>matplot(Age,cbind(p.probit,p.logit),type="l")
</p>
<p>14.1
</p>
<p>attach(graft.vs.host)
</p>
<p>plot(survfit(Surv(time,dead)~gvhd))
</p>
<p>survdiff(Surv(time,dead)~gvhd)
</p>
<p>summary(coxph(Surv(time,dead) ~ gvhd)) # for comparison
</p>
<p>summary(coxph(Surv(time,dead) ~
</p>
<p>gvhd + log(index) + donage + rcpage + preg))
</p>
<p>Subsequent elimination suggests that preg might be a better predictor
than gvhd.
</p>
<p>14.2
</p>
<p>attach(melanom)
</p>
<p>cox1 &lt;- coxph(Surv(days, status==1) ~
</p>
<p>log(thick) + sex + strata(ulc))
</p>
<p>new &lt;- data.frame(sex=2, thick=c(0.1, 0.2, 0.5))
</p>
<p>svfit &lt;- survfit(cox1,newdata=new)
</p>
<p>plot(svfit[2], ylim=c(.985, 1))
</p>
<p>14.3
</p>
<p>summary(coxph(Surv(obsmonths, dead)~age+sex, data=stroke))
</p>
<p>summary(coxph(Surv(obsmonths, dead)~sex, data=stroke))
</p>
<p>with(stroke, tapply(age,sex,mean))
</p>
<p>Themenwere considerably younger than the womenwhen they had their
stroke, which may explain their apparently better survival.
</p>
<p>14.4 Using stroke2 from Exercise 10.4,
</p>
<p>summary(coxph(Surv(entry, exit, dead)~age+sex, data=stroke2))
</p>
<p>Notice that the result is essentially the same as in the unsplit analysis; only
n and Rsquare are changed.
</p>
<p>15.1 Using bcmort2 from Exercise 10.2,
</p>
<p>bcfit &lt;- glm(bc.deaths ~ (age + period + area)^2, poisson,
</p>
<p>offset=log(p.yr), data=bcmort2)</p>
<p/>
</div>
<div class="page"><p/>
<p>352 Appendix D. Answers to exercises
</p>
<p>summary(bcfit)
</p>
<p>drop1(bcfit, test="Chisq")
</p>
<p>confint(bcfit, parm="period1981-1991:areaNat")
</p>
<p>15.2 Continuing with stroke2 from Exercise 10.4, the only slight com-
plication is to convert entry to a factor to specify the relevant time
interval.
</p>
<p>summary(glm(dead~sex+age+factor(entry), poisson,
</p>
<p>offset=log(exit-entry), data=stroke2))
</p>
<p>Notice how similar the results are to the Cox analysis in Exercise 14.3.
</p>
<p>16.1 To fit to the data for girls, just copy the procedure for boys. Even
though the growth curves differ, there is no real reason to redo the starting
value calculation, sowe can fit themodel to boys, girls, and both as follows
</p>
<p>girls &lt;- subset(juul2, age&lt;20 &amp; age&gt;5 &amp; sex==2)
</p>
<p>boys &lt;- subset(juul2, age&lt;20 &amp; age&gt;5 &amp; sex==1)
</p>
<p>young &lt;- subset(juul2, age&lt;20 &amp; age&gt;5)
</p>
<p>stval &lt;- c(alpha=exp(5.3),beta=exp(0.42),gamma=0.15)
</p>
<p>fit.boys &lt;- nls(height~alpha*exp(-beta*exp(-gamma*age)),
</p>
<p>start=stval, data=boys)
</p>
<p>fit.girls &lt;- nls(height~alpha*exp(-beta*exp(-gamma*age)),
</p>
<p>start=stval, data=girls)
</p>
<p>fit.young &lt;- nls(height~alpha*exp(-beta*exp(-gamma*age)),
</p>
<p>start=stval, data=young)
</p>
<p>To test whether we can use the same model for boys and girls, there are
two approaches. One is to make an F test based on the three fits above:
</p>
<p>ms.pooled &lt;- (deviance(fit.boys) + deviance(fit.girls))/(499+625)
</p>
<p>ms.diff &lt;- (deviance(fit.young) -
</p>
<p>deviance(fit.boys) - deviance(fit.girls))/3
</p>
<p>ms.diff/ms.pooled
</p>
<p>This gives F = 90.58 on 3 and 1124 degrees of freedom, which is of course
highly significant.
</p>
<p>Alternatively, we can set up the joint model with separate parameters for
boys and girls and test whether it fits the data better than the model with
the same parameters, like this:
</p>
<p>fit.young2 &lt;- nls(height~(alpha+da*(sex==1))*
</p>
<p>exp(-(beta+db*(sex==1))*
</p>
<p>exp(-(gamma+dg*(sex==1))*age)),
</p>
<p>start=c(alpha=exp(5.3),beta=exp(0.42),gamma=0.15,
</p>
<p>da=0, db=0, dg=0), data=young)
</p>
<p>summary(fit.young2)
</p>
<p>anova(fit.young, fit.young2)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D. Answers to exercises 353
</p>
<p>Notice that da, db, and dg represent differences between the parameters
for the two genders. The term sex==1 is 0 for girls and 1 for boys.
</p>
<p>16.2 We consider experiment 1 only. Starting values can be eyeballed by
using the observation at zero dose for ymax and the x (dose) value at
approximately ymax/2 as β. The value of α can be guessed as the fixed
constant 1. Below, we use α via its logarithm, called la.
</p>
<p>e1 &lt;- subset(philion, experiment==1)
</p>
<p>fit &lt;- nls(sqrt(response) ~ sqrt(ymax / (1 + (dose/ec50)^exp(la))),
</p>
<p>start=list(ymax=28, ec50=.3, la=0), data=e1,
</p>
<p>lower=c(.1,.0001,-Inf), algorithm="port")
</p>
<p>summary(fit)
</p>
<p>confint(fit)
</p>
<p>p &lt;- profile(fit, alphamax=.2)
</p>
<p>par(mfrow=c(3,1))
</p>
<p>plot(p)
</p>
<p>confint(p)
</p>
<p>16.3 The alternative model has similar tail behaviour but behaves differ-
ently when x is close to zero. (In particular, the original model has a term
proportional to &minus;xα&minus;1 in its derivative. At zero, this is &minus;&infin; when α &lt; 1
and 0 when α &gt; 1, so the model describes curves that are either very steep
or very flat near zero.) Notice that, in the modified model, β is no longer
the EC50; the latter is now the solution for x of (1 + x/β)α = 2. The two
are connected by a factor of 21/α &minus; 1.
e1 &lt;- subset(philion, experiment==1)
</p>
<p>fit1 &lt;- nls(sqrt(response) ~ sqrt(ymax / (1 + dose/b)^exp(la)),
</p>
<p>start=list(ymax=28, b=.3, la=0), data=e1,
</p>
<p>lower=c(.1,.0001,-Inf), algorithm="port")
</p>
<p>summary(fit1)
</p>
<p>fit2 &lt;- nls(sqrt(response) ~ sqrt(ymax / (1 +
</p>
<p>dose/(d50/(2^(1/exp(la))-1)))^exp(la)),
</p>
<p>start=list(ymax=28, d50=.3, la=0), data=e1,
</p>
<p>lower=c(.1,.0001,-Inf), algorithm="port")
</p>
<p>summary(fit2)
</p>
<p>Here, fit1 and fit2 are equivalent models, except that the latter is pa-
rameterized in terms of ec50. We can compare the fitted curve with the
model from the previous exercise as follows:
</p>
<p>dd &lt;- seq(0,1,,200)
</p>
<p>yy &lt;- predict(fit, newdata=data.frame(dose=dd))
</p>
<p>y1 &lt;- predict(fit2, newdata=data.frame(dose=dd))
</p>
<p>matplot(dd,cbind(yy,y1)^2, type="l")
</p>
<p>(Notice that the fitted values should be squared because of the square-root
transformation in the models.)</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography
</p>
<p>Agresti, A. (1990), Categorical Data Analysis, JohnWiley &amp; Sons, New York.
</p>
<p>Altman, D. G. (1991), Practical Statistics for Medical Research, Chapman &amp;
Hall, London.
</p>
<p>Andersen, P. K., Borgan, &Oslash;., Gill, R. D., and Keiding, N. (1991), Statistical
Models Based on Counting Processes, Springer-Verlag, New York.
</p>
<p>Armitage, P. and Berry, G. (1994), Statistical Methods in Medical Research,
3rd ed., Blackwell, Oxford.
</p>
<p>Bates, D. M. and Watts, D. G. (1988), Nonlinear regression analysis and its
applications, John Wiley &amp; Sons, New York.
</p>
<p>Becker, R. A., Chambers, J. M., and Wilks, A. R. (1988), The NEW S
Language, Chapman &amp; Hall, London.
</p>
<p>Breslow, N. E. and Day, N. (1987), Statistical Methods in Cancer Research.
Volume II: The Design and Analysis of Cohort Studies, IARC Scientific
Publications, Lyon.
</p>
<p>Campbell, M. J. and Machin, D. (1993), Medical Statistics. A Commonsense
Approach, 2nd ed., John Wiley &amp; Sons, Chichester.
</p>
<p>Chambers, J. M. and Hastie, T. J. (1992), Statistical Models in S, Chapman &amp;
Hall, London.
</p>
<p>Clayton, D. and Hills, M. (1993), Statistical Models in Epidemiology, Oxford
University Press, Oxford.</p>
<p/>
</div>
<div class="page"><p/>
<p>356 Bibliography
</p>
<p>Cleveland, W. S. (1994), The Elements of Graphing Data, Hobart Press, New
Jersey.
</p>
<p>Cochran, W. G. and Cox, G. M. (1957), Experimental Designs, 2nd ed., John
Wiley &amp; Sons, New York.
</p>
<p>Cox, D. R. (1970), Analysis of Binary Data, Chapman &amp; Hall, London.
</p>
<p>Cox, D. R. and Oakes, D. (1984), Analysis of Survival Data, Chapman &amp;
Hall, London.
</p>
<p>Everitt, B. S. (1994), A Handbook of Statistical Analyses Using S-PLUS,
Chapman &amp; Hall, London.
</p>
<p>H&aacute;jek, J., Šid&aacute;k, Z., and Sen, P. K. (1999), Theory of Rank Tests, 2nd ed.,
Academic Press, San Diego.
</p>
<p>Hald, A. (1952), Statistical Theory with Engineering Applications, John Wiley
&amp; Sons, New York.
</p>
<p>Hosmer, D. W. and Lemeshow, S. (2000), Applied Logistic Regression, 2nd
ed., John Wiley &amp; Sons, New York.
</p>
<p>Johnson, R. A. (1994),Miller &amp; Freund&rsquo;s Probability &amp; Statistics for Engineers,
5th ed., Prentice-Hall, Englewood Cliffs, NJ.
</p>
<p>Kalbfleisch, J. D. and Prentice, R. L. (1980), The Statistical Analysis of Failure
Time Data, John Wiley &amp; Sons, New York.
</p>
<p>Lehmann, E. L. (1975), Nonparametrics, Statistical Methods Based on Ranks,
McGraw-Hill, New York.
</p>
<p>Matthews, D. E. and Farewell, V. T. (1988),Using and UnderstandingMedical
Statistics, 2nd ed., Karger, Basel.
</p>
<p>McCullagh, P. and Nelder, J. A. (1989), Generalized Linear Models, 2nd ed.,
Chapman &amp; Hall, London.
</p>
<p>Murrell, P. (2005), R Graphics, Chapman &amp;Hall/CRC, Boca Raton, Florida.
</p>
<p>Siegel, S. (1956), Nonparametric Statistics for the Behavioral Sciences,
McGraw-Hill International, Auckland.
</p>
<p>Venables, W. N. and Ripley, B. D. (2000), S Programming, Springer-Verlag,
New York.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002), Modern Applied Statistics with S,
4th ed., Springer-Verlag, New York.
</p>
<p>Weisberg, S. (1985), Applied Linear Regression, 2nd ed., John Wiley &amp; Sons,
New York.
</p>
<p>Zar, J. H. (1999), Biostatistical Analysis, Prentice Hall, Englewood Cliffs, NJ.</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>abline, 113
and logarithmic scales, 211
</p>
<p>acceptance region, 96
aggregate, 77
alternative, 96, 98
</p>
<p>one-sided, 96
two-sided, 98
</p>
<p>analysis of covariance, 208
analysis of deviance, 234
analysis of variance, 127
</p>
<p>one-way, 127
unequal variances, 133
</p>
<p>two-way, 137
with replications, 207
</p>
<p>ANOVA, see analysis of variance
anova, 130, 139, 141, 188, 216
ANOVA table
</p>
<p>for covariance analysis, 216
for multiple regression, 188
in regression analysis, 141
</p>
<p>apply, 27
arrays, 16
arrows, 134
as.Date, 167
as.numeric, 19
ASCII files, 47
assignment, 3
</p>
<p>assignment operator, 4
attach, 36
ave, 178
average, 67
</p>
<p>bar plot
grouped, 90
stacked, 90
</p>
<p>barplot, 89
barplot
</p>
<p>legend.text, 89, 91
bartlett.test, 136
binom.test, 146
binomial coefficients, 58
Bland&ndash;Altman plot, 104
Bonferroni correction, 132
boxplot, 75
</p>
<p>parallel, 80
by, 78
</p>
<p>c, 14
calculator, overgrown, 3
cat, 13
categorical variables, 18
cbind, 18
censoring, 249
chisq.test, 149, 151</p>
<p/>
</div>
<div class="page"><p/>
<p>358 Index
</p>
<p>expected, 152
observed, 152
</p>
<p>choose, 57
classes, 46
combinatorics, 56
comment (#), 106
comparison operators, 22
complete.cases, 115
concatenation, 14
conditional calculation, 170
confidence bands, 117
confidence interval, 63, 98
confint, 237, 287
confint.default, 237
Conradsen, Knut, 154
console window, 1
contingency tables, see tables
contrasts, 132
Cook&rsquo;s distance, 218, 222
cor, 122
correlation, 120
</p>
<p>between parameter estimates, 233
Kendall&rsquo;s τ, 124
multiple, 113
</p>
<p>adjusted, 113
Pearson-, 121
Spearman&rsquo;s ρ, 123
</p>
<p>count data, 259
Cox model, 256
curve, 43, 60
curve fitting, 275
cut, 163, 245
</p>
<p>data, 35
data editor, 51
data entry, 46
data frames, 20
</p>
<p>appending, 172
components of, 21
encoding grouped data, 25
merging, 173
reshaping, 175
splitting, 179
</p>
<p>data from other programs, 52
data sets, 35
data.frame, 20
dates, 166
</p>
<p>format of, 167
decile, 68
</p>
<p>degrees of freedom, 102
fractional, 102
</p>
<p>deleting objects, 32
density, 42, 58, 59
design, see sample size
design matrix, 200
design variables, 200
detach, 36
deviance, 228
</p>
<p>null, 233
residual, 232, 240
</p>
<p>dfbetas, 219
dffits, 219
difftime object, 168
dim, 17
dimension, 17
distribution, 55
</p>
<p>binomial, 58, 61
normal approximation, 145
</p>
<p>continuous, 58
cumulative distribution function,
</p>
<p>57, 62
empirical, 73
</p>
<p>density, 59
discrete, 57
empirical description of, 71
exponential, 261
Gaussian, see normal
geometric, 58
hypergeometric, 148
noncentral t, 156
normal, 42, 58, 60
point probabilities, 57, 59
Poisson, 260
quantiles, 63
</p>
<p>empirical, 67
random numbers, 64
uniform, 58
</p>
<p>distribution-free methods, 99
do.call, 182
dotchart, 91
drop1, 235, 271
dummy variables, 132, 195, 200, 201
</p>
<p>edit, 51
eggs, 154
error bars, 134
escape character, 48
escape sequence, 13</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 359
</p>
<p>ESS (Emacs Speaks Statistics), 32
expand.grid, 229
expressions, 9
extractor function, 111
</p>
<p>F test
for comparison of variances, 103
in ANOVA table, 142
in regression analysis, 113
</p>
<p>factor, 19, 165
levels, 19
</p>
<p>factors, 18
describing regular pattern, 139
levels of, 18
manipulating levels of, 165
ordered, 19
reading, 49
regular pattern, 229
</p>
<p>FALSE, 12
figure region, 39
filename
</p>
<p>of file in package, 50
filenames, 48
Fisher&rsquo;s exact test, 148
fisher.test, 148
fitted, 113
fitted line, 113
fitted values, 113
fix, 52
flow control, 44
break, 45
compound expression, 45
for, 45
if, 45
repeat, 45
while, 45
</p>
<p>force of mortality, 250
frequency, 145
Friedman&rsquo;s test, 141
ftable, 86
functions, 11
</p>
<p>arguments to, 11
actual, 11
default, 44
formal, 11
keyword matching, 11
named, 11
positional matching, 11
</p>
<p>generic, 46
</p>
<p>generalized linear models, 228
gl, 139, 229
glm, 229
family, 229
weights, 230
</p>
<p>graphics, 7, 79
colour coding, 222
for grouped data, 134
for repeated measurements, 140
pairwise plots, 185
</p>
<p>graphics window, 2
</p>
<p>hazard function, 250
constant, 261
piecewise constant, 261
</p>
<p>head, 24
header, 48
help, 34
help functions, 34
help.search, 34
help.start, 34
hist, 42, 71, 79
breaks, 79
freq, 72
</p>
<p>histogram, 42, 71, 79
Holm correction, 133
</p>
<p>I, 196, 213
ifelse, 171
indexing, 21
</p>
<p>negative, 22
of data frames, 23
with a vector, 21
with logical vector, 23
with relational expression, 22
</p>
<p>input
from file, 47
</p>
<p>interaction, 140, 206, 216
interaction.plot, 140
intercept, 109, 112, 198
interquartile range (IQR), 68
ISwR package, 2
</p>
<p>Kaplan&ndash;Meier estimator, 251
Kruskal&ndash;Wallis test, 136
kruskal.test, 136
</p>
<p>lapply, 26, 170
lazy loading, 35</p>
<p/>
</div>
<div class="page"><p/>
<p>360 Index
</p>
<p>least squares, 109, 276
legend, 91
legend, 91, 209
length, 73
levels, 19, 165, 166
library, 35
lifetime data, 249
likelihood function, 228
</p>
<p>&ldquo;Poisson&rdquo; (constant hazard
survival), 261
</p>
<p>Poisson, 260
linear regression, 109
linearity
</p>
<p>grouped data, 202
lines, 8, 115
link function, 228
list, 19
lists, 19
</p>
<p>components of, 20
lm, 110, 129, 139
load, 32
locator, 91, 209
log odds, 227
log-rank test, 254
logarithmic scale, 210
logical expressions, 22
</p>
<p>combination of, 22
logical operators, 22
logistic regression, 227
logit, 227
loops
</p>
<p>implicit, 26
ls, 31
</p>
<p>Mann&ndash;Whitney test, see Wilcoxon test,
two-sample
</p>
<p>margin coordinates, 39
margin.table, 87
matlines, 119
matrices, 16, 83
</p>
<p>binding together, 18
row and column names, 17, 84
transposition, 17
</p>
<p>matrix, 17, 83
byrow, 83
ncol, 83
nrow, 83
</p>
<p>maximum likelihood, 228
mean, 5
</p>
<p>mean, 67
mean squares, 128
median, 67
merge, 173
missing values, 14, 23, 68, 115
is.na, 23, 115
na.exclude, 115
na.rm, 69
na.rm argument, 122
use argument, 122
</p>
<p>model formulas, 81, 111, 187, 216
arithmetic expressions in, 213
interaction terms, 216
</p>
<p>model object, 111
model search, 190
model.matrix, 200
multiframe graphics layout, 75, 79
multiple comparisons, 131
multiple regression, 185
</p>
<p>NA, see missing values
nls, 276
nonparametric tests, see distribution-
</p>
<p>free methods
normal distribution, 63, 74
null hypothesis, 95
number of observations, 73
</p>
<p>objects, 9
listing of, 31
removing, 32
</p>
<p>odds ratio, 148, 227, 239
one-sided test, 96
oneway.test, 133
online help, 34
</p>
<p>search engine, 34
order, 28
ordered, 19
</p>
<p>p-value, 96
packages, 35
pairs, 185
pairwise comparisons, 131
pairwise.t.test, 132
par, 42, 79
percentile, 68
person-years, 259
pie, 92
pin diagram, 61</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 361
</p>
<p>plot, 7
plot layout, 39
plot region, 39
plot symbol, 7
plots
</p>
<p>adding to
abline, 40
axis, 41
box, 41
lines, 8, 40, 115
mtext, 40
points, 40
text, 40
title, 41
</p>
<p>combining, 42
plotting parameters
</p>
<p>axis labels (xlab and ylab), 39
axis limits (xlim and ylim), 43, 73,
</p>
<p>79
colour (col), 79
empty plot (type="n"), 40
heading (main), 39, 93
line plot (type="l"), 60
logarithmic axes (log), 210
margin expansion (mex), 42
margin sizes (mar), 42
multiframe layout (mfrow and
</p>
<p>mfcol), 75, 79
no axes (axes=F), 40
pin diagram (type="h"), 61
setting with par, 42
step function (type="s"), 73
subtitle (sub), 39
symbol (pch), 7, 209
text font (font), 40
</p>
<p>pmax, 180
pmin, 168, 180
polynomial regression, 196
power, 155, 156
power.prop.test, 155
power.t.test, 155
predict, 119, 197
prediction, 197, 241
prediction bands, 117
prediction in new data frame, 120
print methods, 46
probability, 56
probability distribution, see
</p>
<p>distribution
</p>
<p>probability plot, 74
product-limit estimator, see
</p>
<p>Kaplan&ndash;Meier estimator
profile, 238, 286
profile plot, 238, 286
profiling, 237
glm, 238
nls, 285
</p>
<p>programming, 44
prompt, 2
</p>
<p>continuation, 20
prop.table, 87
prop.test, 146
prop.trend.test, 150
proportional hazards model, 256
proportions, 145
</p>
<p>comparison of k groups, 149
comparison of two groups, 147
</p>
<p>power of, 159
test for trend, 149, 235
test of simple hypothesis, 145
</p>
<p>Q&ndash;Q plot, 74, 218
qqnorm, 74, 117
quantile, 67, 164
quartile, 68
quote symbols, 12
</p>
<p>R2, 113
in model without intercept, 199
</p>
<p>random numbers, 2, 64
random sample, 55
</p>
<p>with and without replacement, 56
range, 43
rate ratio, 265
rates, 259, 266
rbind, 18, 172
read.csv, 51
read.csv2, 51
read.delim, 51
read.delim2, 51
read.table, 47
colClasses, 51
comments, 50
conversion, 51
field separator, 50
header, 48
NA strings, 50
quotes, 50</p>
<p/>
</div>
<div class="page"><p/>
<p>362 Index
</p>
<p>with unequal field count, 50
reading data, 47
regression
</p>
<p>nonlinear, 276
self-starting models, 284
</p>
<p>regression analysis, 195
diagnostics, 218
line through origin, 198
linear, 109
logistic, 227
</p>
<p>for raw data, 239
for tabular data, 229
</p>
<p>multiple, 185
polynomial, 196
</p>
<p>regression coefficients, 109, 112, 201
interpretation for factors, 132
interpretation in covariance
</p>
<p>analysis, 214
regression lines
</p>
<p>comparison of, 212
relational expressions, 13
rep, 16
replicate, 26
replication, 16
reshape, 175
resid, 113
residual variation, 113
residuals, 109, 112, 113
</p>
<p>deviance, 231, 240
leave-out-one, 219
standardized, 218
</p>
<p>rm, 32
rnorm, 2
rstandard, 219
rstudent, 219
</p>
<p>sample, 55
sample size, 155
</p>
<p>comparison of proportions, 161
one-sample problems, 161
paired tests, 161
two-sample problems, 159
</p>
<p>sampling, see random sample
sapply, 26
save, 32
save.image, 32
saving
</p>
<p>command history, 33
workspace, 32
</p>
<p>scripts, 33
windows for editing, 33
</p>
<p>sd, 67
search, 36
search path, 36
SEDM, 101
segments, 116
selection, 22
SEM, 96
seq, 15, 60
sequences of numbers, 15, 60
sign test, 62
signed-rank test, 99
significance level, 96, 156
significance stars, 112
sink, 32
slope, 109
SMR (standardized mortality rate),
</p>
<p>269
sort, 27
sorting, 27
</p>
<p>by another variable, 28
source, 33
spaghettigram, 140
split, 25, 178
staggered entry, 250
standard deviation, 5, 67
standard error
</p>
<p>of differences of means, 101
of regression coefficients, 110
of the mean, 96
</p>
<p>stripchart, 82, 134
jitter, 82
method, 82
</p>
<p>subset, 37
select argument, 172
</p>
<p>subsetting, 21, 37
summary
</p>
<p>corr argument, 233
of data frame, 69
of glm object, 231
of lm object, 111, 131
of numeric variable, 69
of survfit object, 252
</p>
<p>summary statistics, 67, 97
tables of, 75
</p>
<p>Surv objects, 250
survdiff, 255
survfit, 252</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 363
</p>
<p>plot of, 253
survival function, 250, 251
system.file, 50
</p>
<p>t test, 6, 95
approximate power, 158
one-sample, 6, 95
</p>
<p>power of, 156
paired, 104
</p>
<p>power of, 156
two-sample, 100
</p>
<p>power of, 158
same variance, 102
</p>
<p>t.test, 97
alternative, 99
mu, 97
paired, 105
var.equal, 102
</p>
<p>tables, 83
graphical display, 89
marginals, 87
r&times; c, 151
relative frequencies, 87
statistical analysis, 145
</p>
<p>tail, 24
tapply, 27, 75
ties, 100
time splitting, 179
transform, 37
transposition, 87
treatment contrasts, 132, 201
TRUE, 12
two-sided test, 98
type I and type II errors, 156
</p>
<p>var, 67
var.test, 103
</p>
<p>variable names, 4
variance, 67
</p>
<p>comparison of, 103, 136
between linear models, 215
</p>
<p>variation
between rows and columns, 137
due to model, 141
residual, 141
within and between groups, 128
</p>
<p>vectors, 4, 12
calculations with, 5
logical, 12
numeric, 4, 12
recycling of, 5
text- (character), 12
</p>
<p>Welch&rsquo;s test, 101
more than two categories, 133
</p>
<p>wilcox.test, 99
Wilcoxon test, 95
</p>
<p>matched-pairs, 106
one-sample, 99
two-sample, 103
</p>
<p>Windows, 2
with, 37
within, 37
working directory, 33
workspace, 31
</p>
<p>clearing, 32
</p>
<p>χ2 test, 149, 151
components of, 152
</p>
<p>xtabs, 86
</p>
<p>Yates correction, 145, 148</p>
<p/>
</div>
<div class="page"><p/>
<p> 
</p>
<p>springer.com 
</p>
<p>Software for Data Analysis 
Programming with R 
 
John M. Chambers 
</p>
<p> 
</p>
<p>This book guides the reader through programming with R, beginning 
with simple interactive use and progressing by gradual stages,  
starting with simple functions. More advanced programming  
techniques can be added as needed, allowing users to grow into 
software contributors, benefiting their careers and the community. R 
packages provide a powerful mechanism for contributions to be  
organized and communicated. 
 
2008. Approx. 510 pp. (Statistics and Computing) Hardcover  
ISBN 978-0-387-75935-7 
</p>
<p> 
</p>
<p>Time Series Analysis  
with Applications in R 
 
</p>
<p>Jonathan D. Cryer and Kung-Sik Chan  
 
</p>
<p>Time Series Analysis With Applications in R, Second Ed., presents 
an accessible approach to understanding time series models and 
their applications. Although the emphasis is on time domain ARIMA 
models and their analysis, the new edition devotes two chapters to 
the frequency domain and three to time series regression models, 
models for heteroscedasticty, and threshold models. All of the ideas 
and methods are illustrated with both real and simulated data sets. A 
unique feature of this edition is its integration with the R computing 
environment.  
 
2008. 2nd Ed., 494 pp. (Springer Texts in Statistics) Hardcover  
ISBN 0-387-75958-6 
</p>
<p> 
</p>
<p>Data Manipulation with R 
 
Phil Spector 
</p>
<p> 
This book presents a wide array of methods applicable for reading 
data into R, and efficiently manipulating that data. In addition to the 
built-in functions, a number of readily available packages from CRAN 
(the Comprehensive R Archive Network) are also covered. All of the 
methods presented take advantage of the core features of R:  
vectorization, efficient use of subscripting, and the proper use of the 
varied functions in R that are provided for common data management 
tasks. 
 
</p>
<p>2008. 164 pp. (Use R) Softcover  
ISBN 978-0-387-74730-9 
</p>
<p> 
</p>
<p>Easy Ways to Order► Call: Toll-Free 1-800-SPRINGER ▪ E-mail: orders-ny@springer.com ▪ Write: 
Springer, Dept. S8113, PO Box 2485, Secaucus, NJ 07096-2485 ▪ Visit: Your 
local scientific bookstore or urge your librarian to order.  </p>
<p/>
</div>
<ul>	<li>Preface</li>
	<li>Contents</li>
	<li>1 Basics</li>
<ul>	<li>1.1 First steps</li>
<ul>	<li>1.1.1 An overgrown calculator</li>
	<li>1.1.2 Assignments</li>
	<li>1.1.3 Vectorized arithmetic</li>
	<li>1.1.4 Standard procedures</li>
	<li>1.1.5 Graphics</li>
</ul>
	<li>1.2 language essentials</li>
<ul>	<li>1.2.1 Expressions and objects</li>
	<li>1.2.2 Functions and arguments</li>
	<li>1.2.3 Vectors</li>
	<li>1.2.4 Quoting and escape sequences</li>
	<li>1.2.5 Missing values</li>
	<li>1.2.6 Functions that create vectors</li>
	<li>1.2.7 Matrices and arrays</li>
	<li>1.2.8 Factors</li>
	<li>1.2.9 Lists</li>
	<li>1.2.10 Data frames</li>
	<li>1.2.11 Indexing</li>
	<li>1.2.12 Conditional selection</li>
	<li>1.2.13 Indexing of data frames</li>
	<li>1.2.14 Grouped data and data frames</li>
	<li>1.2.15 Implicit loops</li>
	<li>1.2.16 Sorting</li>
</ul>
	<li>1.3 Exercises</li>
</ul>
	<li>2 The R environment</li>
<ul>	<li>2.1 Session management</li>
<ul>	<li>2.1.1 The workspace</li>
	<li>2.1.2 Textual output</li>
	<li>2.1.3 Scripting</li>
	<li>2.1.4 Getting help</li>
	<li>2.1.5 Packages</li>
	<li>2.1.6 Built-in data</li>
	<li>2.1.7 attach and detach</li>
	<li>2.1.8 subset, transform, and within</li>
</ul>
	<li>2.2 The graphics subsystem</li>
<ul>	<li>2.2.1 Plot layout</li>
	<li>2.2.2 Building a plot from pieces</li>
	<li>2.2.3 Using par</li>
	<li>2.2.4 Combining plots</li>
</ul>
	<li>2.3 R programming</li>
<ul>	<li>2.3.1 Flow control</li>
	<li>2.3.2 Classes and generic functions</li>
</ul>
	<li>2.4 Data entry</li>
<ul>	<li>2.4.1 Reading from a text file</li>
	<li>2.4.2 Further details on read.table</li>
	<li>2.4.3 The data editor</li>
	<li>2.4.4 Interfacing to other programs</li>
</ul>
	<li>2.5 Exercises</li>
</ul>
	<li>3 Probability and distributions</li>
<ul>	<li>3.1 Random sampling</li>
	<li>3.2 Probability calculations and combinatorics</li>
	<li>3.3 Discrete distributions</li>
	<li>3.4 Continuous distributions</li>
	<li>3.5 The built-in distributions in R</li>
<ul>	<li>3.5.1 Densities</li>
	<li>3.5.2 Cumulative distribution functions</li>
	<li>3.5.3 Quantiles</li>
	<li>3.5.4 Random numbers</li>
</ul>
	<li>3.6 Exercises</li>
</ul>
	<li>4 Descriptive statistics and graphics</li>
<ul>	<li>4.1 Summary statistics for a single group</li>
	<li>4.2 Graphical display of distributions</li>
<ul>	<li>4.2.1 Histograms</li>
	<li>4.2.2 Empirical cumulative distribution</li>
	<li>4.2.3 Q&ndash;Q plots</li>
	<li>4.2.4 Boxplots</li>
</ul>
	<li>4.3 Summary statistics by groups</li>
	<li>4.4 Graphics for grouped data</li>
<ul>	<li>4.4.1 Histograms</li>
	<li>4.4.2 Parallel boxplots</li>
	<li>4.4.3 Stripcharts</li>
</ul>
	<li>4.5 Tables</li>
<ul>	<li>4.5.1 Generating tables</li>
	<li>4.5.2 Marginal tables and relative frequency</li>
</ul>
	<li>4.6 Graphical display of tables</li>
<ul>	<li>4.6.1 Barplots</li>
	<li>4.6.2 Dotcharts</li>
	<li>4.6.3 Piecharts</li>
</ul>
	<li>4.7 Exercises</li>
</ul>
	<li>5 Oneand two-sample tests</li>
<ul>	<li>5.1 One-sample t test</li>
	<li>5.2 Wilcoxon signed-rank test</li>
	<li>5.3 Two-sample t test</li>
	<li>5.4 Comparison of variances</li>
	<li>5.5 Two-sample Wilcoxon test</li>
	<li>5.6 The paired t test</li>
	<li>5.7 The matched-pairs Wilcoxon test</li>
	<li>5.8 Exercises</li>
</ul>
	<li>6 Regression and correlation</li>
<ul>	<li>6.1 Simple linear regression</li>
	<li>6.2 Residuals and fitted values</li>
	<li>6.3 Prediction and confidence bands</li>
	<li>6.4 Correlation</li>
<ul>	<li>6.4.1 Pearson correlation</li>
	<li>6.4.2 Spearman&rsquo;s p</li>
	<li>6.4.3 Kendall&rsquo;s t</li>
</ul>
	<li>6.5 Exercises</li>
</ul>
	<li>7 Analysis of variance and the Kruskal&ndash;Wallis test</li>
<ul>	<li>7.1 One-way analysis of variance</li>
<ul>	<li>7.1.1 Pairwise comparisons and multiple testing</li>
	<li>7.1.2 Relaxing the variance assumption</li>
	<li>7.1.3 Graphical presentation</li>
	<li>7.1.4 Bartlett&rsquo;s test</li>
</ul>
	<li>7.2 Kruskal&ndash;Wallis test</li>
	<li>7.3 Two-way analysis of variance</li>
<ul>	<li>7.3.1 Graphics for repeated measurements</li>
</ul>
	<li>7.4 The Friedman test</li>
	<li>7.5 The ANOVA table in regression analysis</li>
	<li>7.6 Exercises</li>
</ul>
	<li>8 Tabular data</li>
<ul>	<li>8.1 Single proportions</li>
	<li>8.2 Two independent proportions</li>
	<li>8.3 k proportions, test for trend</li>
	<li>8.4 r &times; c tables</li>
	<li>8.5 Exercises</li>
</ul>
	<li>9 Power and the computation of sample size</li>
<ul>	<li>9.1 The principles of power calculations</li>
<ul>	<li>9.1.1 Power of one-sample and paired t tests</li>
	<li>9.1.2 Power of two-sample t test</li>
	<li>9.1.3 Approximate methods</li>
	<li>9.1.4 Power of comparisons of proportions</li>
</ul>
	<li>9.2 Two-sample problems</li>
	<li>9.3 One-sample problems and paired tests</li>
	<li>9.4 Comparison of proportions</li>
	<li>9.5 Exercises</li>
</ul>
	<li>10 Advanced data handling</li>
<ul>	<li>10.1 Recoding variables</li>
<ul>	<li>10.1.1 The cut function</li>
	<li>10.1.2 Manipulating factor levels</li>
	<li>10.1.3 Working with dates</li>
	<li>10.1.4 Recoding multiple variables</li>
</ul>
	<li>10.2 Conditional calculations</li>
	<li>10.3 Combining and restructuring data frames</li>
<ul>	<li>10.3.1 Appending frames</li>
	<li>10.3.2 Merging data frames</li>
	<li>10.3.3 Reshaping data frames</li>
</ul>
	<li>10.4 Per-group and per-case procedures</li>
	<li>10.5 Time splitting</li>
	<li>10.6 Exercises</li>
</ul>
	<li>11 Multiple regression</li>
<ul>	<li>11.1 Plotting multivariate data</li>
	<li>11.2 Model specification and output</li>
	<li>11.3 Model search</li>
	<li>11.4 Exercises</li>
</ul>
	<li>12 Linear models</li>
<ul>	<li>12.1 Polynomial regression</li>
	<li>12.2 Regression through the origin</li>
	<li>12.3 Design matrices and dummy variables</li>
	<li>12.4 Linearity over groups</li>
	<li>12.5 Interactions</li>
	<li>12.6 Two-way ANOVA with replication</li>
	<li>12.7 Analysis of covariance</li>
<ul>	<li>12.7.1 Graphical description</li>
	<li>12.7.2 Comparison of regression lines</li>
</ul>
	<li>12.8 Diagnostics</li>
	<li>12.9 Exercises</li>
</ul>
	<li>13 Logistic regression</li>
<ul>	<li>13.1 Generalized linear models</li>
	<li>13.2 Logistic regression on tabular data</li>
<ul>	<li>13.2.1 The analysis of deviance table</li>
	<li>13.2.2 Connection to test for trend</li>
</ul>
	<li>13.3 Likelihood profiling</li>
	<li>13.4 Presentation as odds-ratio estimates</li>
	<li>13.5 Logistic regression using raw data</li>
	<li>13.6 Prediction</li>
	<li>13.7 Model checking</li>
	<li>13.8 Exercises</li>
</ul>
	<li>14 Survival analysis</li>
<ul>	<li>14.1 Essential concepts</li>
	<li>14.2 Survival objects</li>
	<li>14.3 Kaplan&ndash;Meier estimates</li>
	<li>14.4 The log-rank test</li>
	<li>14.5 The Cox proportional hazards model</li>
	<li>14.6 Exercises</li>
</ul>
	<li>15 Rates and Poisson regression</li>
<ul>	<li>15.1 Basic ideas</li>
<ul>	<li>15.1.1 The Poisson distribution</li>
	<li>15.1.2 Survival analysis with constant hazard</li>
</ul>
	<li>15.2 Fitting Poisson models</li>
	<li>15.3 Computing rates</li>
	<li>15.4 Models with piecewise constant intensities</li>
	<li>15.5 Exercises</li>
</ul>
	<li>16 Nonlinear curve fitting</li>
<ul>	<li>16.1 Basic usage</li>
	<li>16.2 Finding starting values</li>
	<li>16.3 Self-starting models</li>
	<li>16.4 Profiling</li>
	<li>16.5 Finer control of the fitting algorithm</li>
	<li>16.6 Exercises</li>
</ul>
	<li>A Obtaining and installing</li>
	<li>B Data sets in the ISwR package1</li>
	<li>C Compendium</li>
	<li>D Answers to exercises</li>
	<li>Bibliography</li>
	<li>Index</li>
</ul>
</body></html>